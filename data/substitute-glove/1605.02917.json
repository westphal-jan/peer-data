{"id": "1605.02917", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine", "abstract": "Search engined fact the most other solutions for click data purchase. Web pages are flailing and junk turned locate Engines. Users typically locate quality messages notebooks after querying a focus generators. One of through key 2003 search reciprocating legislation very ads pages there waste find straight-six resources. These pages taken deception according trying engine positions semantic try to be showed in over day page years surprise. There are many approaches. database spam online experiments called as calculate of HTML functions style variations, pages linguistic pattern analysis rest electronic approaches algorithm opened original content adapted. One the form known combinatorial has however used, equipment school ambitious similar Support Vector Machine (SVM) fem. Recently guidance architecture form SVM over as changed by new modular instead moreover randomness been statistical analysis. In see stamped n't advance severity of web spam optical brought direct third nonlinear kernels into Twin SVM (TSVM) as an considerably extension still SVM. The classifier whatever to data understanding taken though decreased well using half gradually candies one each class of information. Effectiveness of with extension specific an ago experimented with up previously only spam fault-tolerant made UK - 2007 in UK - 2006. Results show place guidance main approving kernelized short however TSVM, computers spam note detection.", "histories": [["v1", "Tue, 10 May 2016 10:05:40 GMT  (1174kb)", "http://arxiv.org/abs/1605.02917v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["seyed hamid reza mohammadi", "mohammad ali zare chahooki"], "accepted": false, "id": "1605.02917"}, "pdf": {"name": "1605.02917.pdf", "metadata": {"source": "CRF", "title": "Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine", "authors": ["Seyed Hamid", "Reza Mohammadi", "Mohammad Ali Zare Chahooki"], "emails": ["mohammadi_6468@stu.yazd.ac.ir", "chahooki@yazd.ac.ir"], "sections": [{"heading": null, "text": "1 | P a g e\nSearch engines are the most important tools for web data acquisition. Web pages are crawled and indexed by search Engines. Users typically locate useful web pages by querying a search engine. One of the challenges in search engines administration is spam pages which waste search engine resources. These pages by deception of search engine ranking algorithms try to be showed in the first page of results. There are many approaches to web spam pages detection such as measurement of HTML code style similarity, pages linguistic pattern analysis and machine learning algorithm on page content features. One of the famous algorithms has been used in machine learning approach is Support Vector Machine (SVM) classifier. Recently basic structure of SVM has been changed by new extensions to increase robustness and classification accuracy. In this paper we improved accuracy of web spam detection by using two nonlinear kernels into Twin SVM (TSVM) as an improved extension of SVM. The classifier ability to data separation has been increased by using two separated kernels for each class of data. Effectiveness of new proposed method has been experimented with two publicly used spam datasets called UK-2007 and UK-2006. Results show the effectiveness of proposed kernelized version of TSVM in web spam page detection.\nIndexing terms/Keywords\nSearch Engine, Web Spam Page Detection, Machine Learning, Twin Support Vector Machine (TSVM), Multiple Kernels.\n1.Introduction\nThe ever-increasing volume of information on the Internet has made search engines vital tools for information extraction. From a theoretical point of view, search engines are information retrieval tools responsible for downloading, indexing, processing, retrieving, and ranking of information [1].\nMalicious content on the Internet, poses a unique challenge for search engines; one that other information retrieval systems do not normally face. Malicious content refers to deceitful information created for the purpose of manipulating a search engine\u2019s results. The open nature of the Internet allows any individual to create and distribute arbitrary content. Thus, malicious content cannot be avoided. Web pages which attempt to manipulate a search engine\u2019s results are the most important type of malicious content. Such pages are called Spam Pages. Other types of malicious content include spam queries, spam posts and Fishing pages.\nMost website owners want to have higher-ranking pages. This is because only 15% of users visit the second page of search engine results before changing their query. Thus, search engines are constantly trying to assign better ranks to the most relevant results. This improvement increases their revenue and popularity. Such attempts can be exploited, and led to the creation of spam pages by profiteers [2].\nIt should be noted that not all spam pages have commercial or exploitive purposes. The contents of these pages differ from the user\u2019s expectations. They aim to penetrate ranking algorithms and, therefore, can be classified as spam pages. The main purpose of spam pages is to fraud ranking algorithms, with the aim of achieving a higher rank among the top ten search engine\u2019s results of various queries, as fast as possible [3]. Fig.1 illustrates a sample spam page. Since the text in each page include multiple valuable keywords, the content of the page is insignificant for the user. From the perspective of search engine, the most important effect of spam pages is diminution of search engine user\u2019s trust. Furthermore, the large number of spam pages, forces search engines to allocate more resources (bandwidth, storage space, software) to crawling, indexing, storing, and processing contents of these pages.\n2 | P a g e\nSpam pages mainly aim to corrupt ranking algorithms in search engines. Ranking is done by search engine which estimates the quality of a page with respect to a particular query. In other words, one scores is assigned to each retrieved document. Documents are sorted in decreasing (or increasing) order of their scores. Search engines often use the following measures to rank web pages:\na) Relevance, measured as the degree of similarity between the input query and the text of the web page.\nb) Importance, input query is ignored in it. This measure can be calculated as the number of times other pages are linked to a certain page.\nAs mentioned earlier, spam pages are created with the aim of obtaining a high rank in a search engine\u2019s results. As a result, they are created with the ranking algorithms in mind [1]. The content of a spam page is organized to match more queries. Therefore, a variety of useful keywords are used so that more user queries can be matched. Also, web spam content are changed based on the structure of web pages. Therefore, content ranking algorithms assign higher scores to these pages. If the goal is to match the target page to a certain request, then the target keywords need to be repeated multiple times. In 2006, Najork et al. [4] presented an approach based on content analysis, using examination of various features of spam and non-spam pages. Their findings formed a basis for identifying spam pages.\nCreating a high number of links is another method used by spammers to improve their rank. In [6-8] a different approach, based on the links between pages in the web graph, is used for detection. The graph and patterns that emerge from it are used to differentiate spam and non-spam pages.\nThere have also been attempts to combine ideas form both methods to achieve satisfactory results. In [5], spam pages are detected using a combination of content-based and link-based methods. In recent years, user behavior has also been a decisive factor in detecting spam pages. Liu et al. [9] presented a behavior-based method. Modern techniques for creating spam pages include information hiding, where users are encountered with different versions of pages compared to search engine crawlers.\nVarious methods of spam detection have been proposed, including those based on coding-style similarity [18] and language pattern analysis [11]. Machine learning algorithms have also been applied to page features [12- 16]. The majority of methods involved in creating spam pages rely on content-based techniques. Thus, analyzing the content features of these pages will improve much more efficiency compared to other methods. Web pages have certain features that can be extracted using content analysis. The values for these features are significantly different across spam and non-spam pages. Since the detection problem seeks to label pages as either spam or normal, it can be converted to a classification problem with two classes. Therefore, once\n3 | P a g e\nfeatures of page content are extracted and a suitable set of pages are labeled (normal or spam), a proper classifier can be used to determine the being spam of unlabeled pages collected by a search engine.\nMachine learning methods use classification algorithms to extract and detect patterns among normal and spam pages. Using more powerful algorithms can increase the accuracy of detection. In previous researches, Hidden Markov Model [12], Decision tree [13], Bayesian networks [14], and artificial neural network [15] are used to separate spam instances from non-spam ones.\nMany machine learning algorithms have been used for the binary classification problem. Support Vector Machine (SVM) is one of these algorithms, which offers favorable results. SVM despite is successfully applied to many problems of differentiating positive and negative instances, suitable accuracy has not been reported for web spam detection by it. However, studies on other applications demonstrate that new extensions of SVM have significantly superior performance [19]. Such versions aim to improve its performance on different data through changes in SVM structure.\nSVM is binary classifier, which constructs a hyper-plane among instances such that the distance from the instances is maximized and, thus, a good separation is achieved. In an extended version of it, namely the Twin SVM (TWSVM), two separate hyper-planes are used for each class instances. Based on the nature of web pages and number of extracted features in them, this version of SVM can be used for web spam classification.\nIn some cases, samples cannot be separated linearly by their attributes. The concept of kernel has been introduced in machine learning methods to solve this problem. In this approach, the data are first logically mapped to a higher-dimensional space. The new dimensions are created such that they can be linearly separated with more accuracy, using hyper-planes [21].\nIn this paper, TSVM with two non-linear kernels is used for spam page detection. The main innovation of the proposed method is embedding two different suitable kernels for each hyperplane in TSVM. Experimental results on UK-2006 and UK-2007 datasets demonstrated the effectiveness of the proposed method for detecting spam pages.\nThe remainder of the paper is organized as follows. Section 2 presents a review of various extensions to the SVM. The importance of using non-linear kernels is considered in section 3. The details of the proposed method are discussed in section 4. Section 5 presents the experimental results. Finally, in section 6, the concluding remarks and suggestions for future works are offered."}, {"heading": "2- SVM Extensions", "text": "Support Vector Machine (SVM) is a non-statistical binary classifier, which has attracted a lot of attentions in recent years. In this method, all the instances are used in conjunction with an optimization algorithm to find instances that form the boundaries of classes. Using these instances, an optimal linear decision boundary is created to separate classes, and it is solved using the Quadratic Programming Problem (QPP) [23]. This classifier does not rely on statistics; the training points are directly used to determine the decision boundary between two classes.\nProximal Support Vector Machine (PSVM) is a new version of the SVM, which aims to reduce computational costs. In this algorithm, two parallel hyper-planes, rather than one, are used to separate instances.\nGeneralized Eigenvalue Proximal SVM (GEPSVM), uses two non-parallel hyper-planes, each of which is placed near one of the two classes. Since a large QPP must be solved, the algorithm becomes very costly.\n4 | P a g e\nThe improved GEPSVM, called Twin SVM (TWSVM), eliminates this problem by solving two smaller instances of QPP. Therefore, computational cost is decreased, while the algorithm becomes more robust.\nThen, a variation of TWSVM, namely the Least Square TWSVM was introduced. The final solution of the algorithm is obtained by solving two linear equations, rather than two QPPs. Although the algorithm is easier, however it is less robust, due to its sensitivity to noise.\nKnowledge-based TWSVM is another version of TWSVM, which uses current instances along with previous knowledge to separate instances into two classes. Compared to other knowledge-based algorithms, it offers less complexity time.\nIn order to improve the structure of TWSVM, Smooth TWSVM was introduced. The most important change in this version of the algorithm is the elimination of QPP limitations."}, {"heading": "3- Non-linear Kernels", "text": "Using linear models for classification is much easier and faster than non-linear counterparts. Linear models work on data that can be separable linearly. Often, developing of linear models is more appropriate than non-linear ones, since they are associated with lower complexity time. However, in some cases, the instances are not intrinsically linear. Such instances must be mapped to a more dimensional space, where a linear model is applicable. Fig.2 illustrates how the data are mapped to a new space. Mapping to a highdimensional space is not always straightforward manner and can impose much more costs than primary dimension space. By offering a way to reduce associated costs, the idea of kernel addresses this shortcoming. A kernel implicitly maps the data to a higher-dimensional space. In order to classify non-linear data in a linear manner, the SVM needs to use the idea of a kernel, which leads to enhanced classification capabilities [21].\nGenerally, in algorithms where data are in the form of dot products, the idea of a kernel can be applied. There are various types of kernel functions. Polynomial, Gaussian, linear, and fuzzy are more common kernel functions [21]. Using kernel functions together with SVMs is lead to performance improvement of binary classification."}, {"heading": "4- Proposed Method", "text": "In this paper, non-linear kernels in the extended SVM are used to present a binary classifier, with superior performance in detecting spam pages. In the following, the learning structure of the SVM is discussed in detail. Next, the non-linear kernels in the TWSVM are introduced. Finally, the proposed method, i.e. Multiple Kernel TSVM, is presented. As shown in section of experimental results, this method improves the accuracy of detecting spam pages.\n5 | P a g e"}, {"heading": "4-1- TSVM", "text": "The TSVM was first introduced by Jayada [22], based on the normal SVM. In this algorithm, instead of using a single plane and increasing its margins toward binary classification, two non-parallel planes are used to separate instances into two classes. In this classifier, each class is determined using one of the hyper-planes. The procedure is demonstrated in Fig.3.\nAssume that we are given a set of data containing positive and negative instances. In order to separate them, two equations are formed for each hyper-plane, which can segregate the data in TWSVM in a linear fashion.\n  bXWXFbXWXF TT )(,)(\n(1)\nwhere nWW  , and  bb ,\nIn the next step, two QPPs must be solved to obtain the optimal hyper-planes. QPPs can be challenging to solve; thus, they should be simplified and solved using Lagrange\u2019s method of undetermined coefficients. Once the QPP is solved, the final equation of the TWSVM to separate the instances is obtained in as follows.\ni\nT i\nii\ni wxw\nbwx Class\n\n\n\n  || minarg\n2,1\n(2)\nwhere |.| is the absolute value and refers to classes of instances\nAny new instance can be classified using Eq(2)."}, {"heading": "4-2- Using kernels in the TWSVM", "text": "As mentioned earlier, in order to convert non-linear classification to a linear one, instances are mapped to a new high-dimensional space before classification phase. In section 3, the idea of mapping the data to a new space was explained in detail. Furthermore, in the previous subsection, the final equation for the TWSVM classifier was presented. Here, we aim to rewrite this equation using a kernel function. Eq(3) shows the final classifier by kernel design.\n6 | P a g e\ni\nT i\nii\ni wCxKw\nbwCxK Class\n),(\n|),(| minarg\n2,1 \n\n\n \n(3)\nWhere K is kernel function which maps data to higher dimension and any arbitrary kernel can be used to replace K, in this equation."}, {"heading": "4-3- Multiple non-linear kernels in the TWSVM", "text": "As previously mentioned, in this paper for the first time two separate kernels are adjusted in TWSVM. The rationale behind this is the nature of spam pages as well as the distribution of instance. This paper tries to improve classification performance by designing two suitable kernels, both of which are used to map the input data to a new space. In the following, details of designing and matching two appropriate kernels for the TWSVM are discussed.\nIn subsection 4-2, kernel design in the TSVM is explained. Based on dimension space of samples, an efficient\nkernel must be designed to separate the samples both linearly and optimally, in the new space. The choice or design of the kernel function has a direct impact on the efficiency of the classifier algorithm. Kernel functions can be created as combinations of standard kernels. The resulting kernel must satisfy Mercer\u2019s condition (Mercer\u2019s theorem checks the validity of kernel functions). In this paper, a suitable kernel function for spam web pages is obtained using the method of fixed rule and a combination of two standard kernels. The first kernel is linear, which is powerful and has low computational costs. The linear kernel is shown in Eq(4).\nK( Xi , Xj ) = Xi . Xj +b\n(4)\nwhere nji XX , and b\nLinear kernel is most effective when the data are normally distributed. Since non-spam pages have a normal distribution, this kernel function is used to map these input instances to a new space.\nThe other kernel function is the hyperbolic tangent, which is used for non-linear patterns in the data. The function can be seen in Eq(5).\nK( Xi , Xj ) = tanh (k Xi . Xj - b)\n(5)\nwhere nji XX , and bk,\nSince each kernel function creates a similarity matrix, it must be designed such that the matrix for each instance of the two classes matches the real patterns in the data. Therefore, the choice of the function is very important. In this paper, the linear method is used, which offers simplicity while maintaining the advantages of both kernel functions. Eq(6) presents the linear combination of the two functions.\nK = tanh(k \u00d7Xtrain \u00d7 Xt' -b)2 + ( Xtrain \u00d7 Xt' ) (6)\nwhere Xtrain is the set of positive instances (spam) and Xt represents the entire set of data. After several experiments, the coefficients k and b were assigned 1 and 0, respectively. A function must satisfy Mercer\u2019s conditions before it can be considered a kernel function. The conditions are true for the proposed function.\n7 | P a g e\nAfter the appropriate kernel is chosen, it must be adapted to the TSVM. As shown earlier, in the TWSVM classifier the class of a new instance is determined using Eq(7).\ni\nT i\nii\ni wCxKw\nbwCxK Class\n),(\n|),(| minarg\n2,1 \n\n\n  (7)\nIn the proposed method, Eq(6) is applied to spam pages, whereas Eq(4) is used for normal instances. As mentioned before, the TWSVM classifier uses two separate hyper-planes for each instance, which allows the mapping of the input data to a new space. As a result, the hyper-planes become more effective in segregating of instances.\nAfter the designed kernel is adjusted to the TWSVM, the classifier must be trained. A portion of instancesin each dataset is used for training purposes. The trained classifier can then determine which class each new instance belongs."}, {"heading": "5- Experimental Results", "text": "In this section, we present the details of implementation and testing of the proposed algorithm,\nincluding description of datasets, evaluation criterion, and experimental results."}, {"heading": "5-1- Dataset", "text": "In order to train, execute, and determine the accuracy of the proposed algorithm, UK-2006 and UK-2007 datasets were used. These data sets have been used in numerous studies on spam pages. UK-2006 contains around 11400 hosts from .uk domain where 61.75% of documents are labeled as normal, 22.08% as spam, and 16.16% as unknown [26]. UK-2007 holds 114529 hosts from .uk domain where 94% of them are labeled as normal and the remaining 6% are spam.\nIn this paper we ignore documents with Unknown label. The samples in two datasets are randomly divided\ninto two parts, 75% as train and other 25% as test sets."}, {"heading": "5-2- Evaluation", "text": "In order to present the algorithm\u2019s results and compare them to those of other algorithms, a standard criterion is necessary for evaluation. Accuracy is a measure of how many instances are classified correctly. It is popular in the context of deception detection. Compared to other measures, it is quite robust.\nNP\nTNTP Accuracy\n\n  (8)\nIn order to evaluate the accuracy of our classifier, we employed a technique known as ten-fold cross validation [4]. Ten-fold cross validation involves dividing the judged data set randomly into 10 equally-sized partitions, and performing 10 training/testing steps, where each step uses nine partitions to train the classifier and the remaining partition to test its effectiveness."}, {"heading": "5-3- Comparison of the proposed algorithm to SVMs", "text": "Despite acceptable performance in classifying various types of data, the standard SVM algorithm has failed in the context of spam pages. The standard SVM was implemented using several kernels. A comparison of the results can be seen in Table 1. As evident, the choice of kernel does not lead to significant changes in algorithm\u2019s performance, which may be due to its own structure. The standard SVM only uses one separating plane; thus, it is not able to differentiate between relatively similar spam and normal instances. There are\n8 | P a g e\nmany similar instances with opposing labels in the data sets. As a result, the SVM classifier demonstrates poor performance in web spam detection."}, {"heading": "5-4- Comparison to other algorithms", "text": "As mentioned before, different classifiers have been used for web sapm detection, including decision trees [14] and artificial neural networks [16]. Furthermore, other studies such as [13] and [15] use different algorithms, which cannot be compared to the proposed algorithm due to incompatible datasets. Tables 2 and 3 present our results along with those of the most accurate algorithms in KU-2006, UK-2007 respectively.\n9 | P a g e\n95.6 MKTWSVM\nMKTWSVM which uses two hyper-planes to separate instances of the two classes, with two kernels for each, is able to achieve acceptable accuracy. Interestingly, in this method, the input data are mapped such that a linear model can be used for differentiation. The choice and design of the kernel is an important issue in this problem. The proposed kernel in this paper improves the accuracy of a linear classifier. Thus, a non-linear kernel can be considered the key component of a classifier. As shown in Table 3, nearly 3% of the increase in accuracy is associated with two separate kernels in the structure of the TWSVM. Advantages of this method include low computational complexity since the algorithm works in linear space."}, {"heading": "6- Conclusion and Future Works", "text": "In this paper, by using two non-linear kernels in TSVM, high accuracy in detecting spam pages was\nachieved. Furthermore, in order to demonstrate the effectiveness of non-linear kernels, SVM and TWSVM\nwere tested both with and without kernel functions. The proposed method proved that two non-linear\nkernels in TWSVM increases accuracy and reduces computational complexity.\nAn avenue for future studies involves altering the structure of extended versions of SVM so that accuracy is\nincreased and computational complexity becomes more satisfactory. Moreover, since the optimal\ncombination of basic kernel functions for either of the hyper-planes can improve classification accuracy,\ngenetic programming can be incorporated to find the optimal combination of basic kernel functions."}], "references": [{"title": "Efficient and effective spam filtering and re-ranking for large web datasets", "author": ["G.V. Cormack", "M.D. Smucker", "C.L.A. Clarke"], "venue": "Information Retrieval, pp. 1-25", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Web Spam", "author": ["P.T. Metaxas", "J. DeStefano"], "venue": "Propaganda and Trust\u201d, in Proceedings of the 1st International Workshop on Adversarial Information Retrieval on the Web, pp. 60-69", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Spam", "author": ["D. Fetterly", "M. Manasse", "M. Najork"], "venue": "damn spam, and statistics\u201d, in Proceedings of the 7th International Workshop on the Web and Databases, pp. 210-223", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Detecting spam web pages through content analysis", "author": ["A. Ntoulas", "M. Najork", "M. Manasse", "D. Fetterly"], "venue": "Proceedings of the 15th International Conference on World Wide Web,China Beijin University, pp. 83-92", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Web spam detection: Link-based and content-based techniques", "author": ["L. Becchetti", "C. Castillo", "D. Donato", "S. Leonardi", "R. Baeza-Yates"], "venue": "The European Integrated Project Dynamically Evolving Large Scale Information Systems (DELIS): Proceedings of the Final Workshop, Paderborn University, pp. 99-113", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning from labeled and unlabeled data on a directed graph", "author": ["D. Zhou", "J. Huang", "B. Sch\u00f6lkopf"], "venue": "Proceedings of the 22nd International Conference on Machine learning, Brazil Pugn University, pp. 1036- 1043", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Link analysis for web spam detection", "author": ["L. Becchetti", "C. Castillo", "D. Donato", "R. Baeza-Yates", "S. Leonardi"], "venue": "ACM Transactions on the Web (TWEB), China, vol. 2, pp. 1-42", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Know your neighbors: web spam detection using the web topology", "author": ["C. Castillo", "D. Donato", "A. Gionis", "V. Murdock", "F. Silvestri"], "venue": "Proceedings of the 30th Annual International ACM SIGIR conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Identifying web spam with user behavior analysis\u201d.Proceedings of the 4th International Workshop on Adversarial Information", "author": ["Y.Liu", "R.Cen", "M.Zhang", "S.Ma", "L.Ru"], "venue": "Retrieval on the Web,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Cloaking and redirection: A preliminary study", "author": ["B. Wu", "B.D. Davison"], "venue": "Proceedings of the 1st International Workshop on Adversarial Information Retrieval on the Web (AIRWeb), pp. 7", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Cross-Lingual Web Spam Classification\u201d,in", "author": ["K. Chellapilla", "A. Maykov"], "venue": "Proceedings of the 3rd International Workshop on Adversarial Information Retrieval on the Web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Web Spam Detection Using Machine Learning in Specific Domain Features", "author": ["Hmeidi", "H.Najadat", "Ismail"], "venue": "Journal of Information Assurance and Security,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Content based web spam detection using naive bayes with different feature representation technique.", "author": ["Soni", "A.Anand", "A.Mathur"], "venue": "Journal of Engineering Research and Applications,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "From a theoretical point of view, search engines are information retrieval tools responsible for downloading, indexing, processing, retrieving, and ranking of information [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "Such attempts can be exploited, and led to the creation of spam pages by profiteers [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "The main purpose of spam pages is to fraud ranking algorithms, with the aim of achieving a higher rank among the top ten search engine\u2019s results of various queries, as fast as possible [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "As a result, they are created with the ranking algorithms in mind [1].", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "[4] presented an approach based on content analysis, using examination of various features of spam and non-spam pages.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In [6-8] a different approach, based on the links between pages in the web graph, is used for detection.", "startOffset": 3, "endOffset": 8}, {"referenceID": 6, "context": "In [6-8] a different approach, based on the links between pages in the web graph, is used for detection.", "startOffset": 3, "endOffset": 8}, {"referenceID": 7, "context": "In [6-8] a different approach, based on the links between pages in the web graph, is used for detection.", "startOffset": 3, "endOffset": 8}, {"referenceID": 4, "context": "In [5], spam pages are detected using a combination of content-based and link-based methods.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "[9] presented a behavior-based method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Various methods of spam detection have been proposed, including those based on coding-style similarity [18] and language pattern analysis [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "In previous researches, Hidden Markov Model [12], Decision tree [13], Bayesian networks [14], and artificial neural network [15] are used to separate spam instances from non-spam ones.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "In previous researches, Hidden Markov Model [12], Decision tree [13], Bayesian networks [14], and artificial neural network [15] are used to separate spam instances from non-spam ones.", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "In order to evaluate the accuracy of our classifier, we employed a technique known as ten-fold cross validation [4].", "startOffset": 112, "endOffset": 115}, {"referenceID": 12, "context": "Furthermore, other studies such as [13] and [15] use different algorithms, which cannot be compared to the proposed algorithm due to incompatible datasets.", "startOffset": 44, "endOffset": 48}], "year": 2016, "abstractText": "Search engines are the most important tools for web data acquisition. Web pages are crawled and indexed by search Engines. Users typically locate useful web pages by querying a search engine. One of the challenges in search engines administration is spam pages which waste search engine resources. These pages by deception of search engine ranking algorithms try to be showed in the first page of results. There are many approaches to web spam pages detection such as measurement of HTML code style similarity, pages linguistic pattern analysis and machine learning algorithm on page content features. One of the famous algorithms has been used in machine learning approach is Support Vector Machine (SVM) classifier. Recently basic structure of SVM has been changed by new extensions to increase robustness and classification accuracy. In this paper we improved accuracy of web spam detection by using two nonlinear kernels into Twin SVM (TSVM) as an improved extension of SVM. The classifier ability to data separation has been increased by using two separated kernels for each class of data. Effectiveness of new proposed method has been experimented with two publicly used spam datasets called UK-2007 and UK-2006. Results show the effectiveness of proposed kernelized version of TSVM in web spam page detection.", "creator": "Microsoft\u00ae Word 2010"}}}