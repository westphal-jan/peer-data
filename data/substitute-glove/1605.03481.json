{"id": "1605.03481", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2016", "title": "Tweet2Vec: Character-Based Distributed Representations for Social Media", "abstract": "Text from influence commercial applications a set latter priorities adding can cause often NLP approaches to impossible. Informal language, correct errors, wordings, when cover alternate handful two bygone place there posts, close failed full unfeasible unlike vocabulary matching the follow - lack study. We propose a character consisting introducing, tweet2vec, still boy formula_11 - build representations on whole tweets one particular structures, individuals - include dependencies today character variants. The proposed model curently a appears - weight volleyed at outlook lets - anthologies logograms among by the on, doing significantly better when during input lists various while - of - vocabulary spoken up unusual character sequences. Our tweet2vec ead is being except.", "histories": [["v1", "Wed, 11 May 2016 15:30:09 GMT  (559kb,D)", "https://arxiv.org/abs/1605.03481v1", "6 pages, 2 figures, 4 tables, accepted as conference paper at ACL 2016"], ["v2", "Tue, 17 May 2016 15:00:38 GMT  (559kb,D)", "http://arxiv.org/abs/1605.03481v2", "6 pages, 2 figures, 4 tables, accepted as conference paper at ACL 2016"]], "COMMENTS": "6 pages, 2 figures, 4 tables, accepted as conference paper at ACL 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["bhuwan dhingra", "zhong zhou", "dylan fitzpatrick", "michael muehl", "william w cohen"], "accepted": true, "id": "1605.03481"}, "pdf": {"name": "1605.03481.pdf", "metadata": {"source": "CRF", "title": "Tweet2Vec: Character-Based Distributed Representations for Social Media", "authors": ["Bhuwan Dhingra", "Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William W. Cohen"], "emails": ["bdhingra@andrew.cmu.edu", "djfitzpa@andrew.cmu.edu", "mmuehl@andrew.cmu.edu", "zhongzhou@cmu.edu", "wcohen@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "We understand from Zipf\u2019s Law that in any natural language corpus a majority of the vocabulary word types will either be absent or occur in low frequency. Estimating the statistical properties of these rare word types is naturally a difficult task. This is analogous to the curse of dimensionality when we deal with sequences of tokens - most sequences will occur only once in the training data. Neural network architectures overcome this problem by defining non-linear compositional models over vector space representations of tokens and hence assign non-zero probability even to sequences not seen during training (Bengio et al., 2003; Kiros et al., 2015). In this work, we explore a similar approach to learning distributed representations of social media posts by\n1https://github.com/bdhingra/tweet2vec\ncomposing them from their constituent characters, with the goal of generalizing to out-of-vocabulary words as well as sequences at test time.\nTraditional Neural Network Language Models (NNLMs) treat words as the basic units of language and assign independent vectors to each word type. To constrain memory requirements, the vocabulary size is fixed before-hand; therefore, rare and out-of-vocabulary words are all grouped together under a common type \u2018UNKNOWN\u2019. This choice is motivated by the assumption of arbitrariness in language, which means that surface forms of words have little to do with their semantic roles. Recently, (Ling et al., 2015) challenge this assumption and present a bidirectional Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) for composing word vectors from their constituent characters which can memorize the arbitrary aspects of word orthography as well as generalize to rare and out-of-vocabulary words.\nEncouraged by their findings, we extend their approach to a much larger unicode character set, and model long sequences of text as functions of their constituent characters (including whitespace). We focus on social media posts from the website Twitter, which are an excellent testing ground for character based models due to the noisy nature of text. Heavy use of slang and abundant misspellings means that there are many orthographically and semantically similar tokens, and special characters such as emojis are also immensely popular and carry useful semantic information. In our moderately sized training dataset of 2 million tweets, there were about 0.92 million unique word types. It would be expensive to capture all these phenomena in a word based model in terms of both the memory requirement (for the increased vocabulary) and the amount of training data required for effective learning. Additional benefits of the character based approach ar X iv :1 60 5.\n03 48\n1v 2\n[ cs\n.L G\n] 1\n7 M\nay 2\n01 6\ninclude language independence of the methods, and no requirement of NLP preprocessing such as word-segmentation.\nA crucial step in learning good text representations is to choose an appropriate objective function to optimize. Unsupervised approaches attempt to reconstruct the original text from its latent representation (Mikolov et al., 2013; Bengio et al., 2003). Social media posts however, come with their own form of supervision annotated by millions of users, in the form of hashtags which link posts about the same topic together. A natural assumption is that the posts with the same hashtags should have embeddings which are close to each other. Hence, we formulate our training objective to maximize cross-entropy loss at the task of predicting hashtags for a post from its latent representation.\nWe propose a Bi-directional Gated Recurrent Unit (Bi-GRU) (Chung et al., 2014) neural network for learning tweet representations. Treating white-space as a special character itself, the model does a forward and backward pass over the entire sequence, and the final GRU states are linearly combined to get the tweet embedding. Posterior probabilities over hashtags are computed by projecting this embedding to a softmax output layer. Compared to a word-level baseline this model shows improved performance at predicting hashtags for a held-out set of posts. Inspired by recent work in learning vector space text representations, we name our model tweet2vec."}, {"heading": "2 Related Work", "text": "Using neural networks to learn distributed representations of words dates back to (Bengio et al., 2003). More recently, (Mikolov et al., 2013) released word2vec - a collection of word vectors trained using a recurrent neural network. These word vectors are in widespread use in the NLP community, and the original work has since been extended to sentences (Kiros et al., 2015), documents and paragraphs (Le and Mikolov, 2014), topics (Niu and Dai, 2015) and queries (Grbovic et al., 2015). All these methods require storing an extremely large table of vectors for all word types and cannot be easily generalized to unseen words at test time (Ling et al., 2015). They also require preprocessing to find word boundaries which is non-trivial for a social network domain like Twitter.\nIn (Ling et al., 2015), the authors present a compositional character model based on bidirectional LSTMs as a potential solution to these problems. A major benefit of this approach is that large word lookup tables can be compacted into character lookup tables and the compositional model scales to large data sets better than other stateof-the-art approaches. While (Ling et al., 2015) generate word embeddings from character representations, we propose to generate vector representations of entire tweets from characters in our tweet2vec model.\nOur work adds to the growing body of work showing the applicability of character models for a variety of NLP tasks such as Named Entity Recognition (Santos and Guimar\u00e3es, 2015), POS tagging (Santos and Zadrozny, 2014), text classification (Zhang et al., 2015) and language modeling (Karpathy et al., 2015; Kim et al., 2015).\nPreviously, (Luong et al., 2013) dealt with the problem of estimating rare word representations by building them from their constituent morphemes. While they show improved performance over word-based models, their approach requires a morpheme parser for preprocessing which may not perform well on noisy text like Twitter. Also the space of all morphemes, though smaller than the space of all words, is still large enough that modelling all morphemes is impractical.\nHashtag prediction for social media has been addressed earlier, for example in (Weston et al., 2014; Godin et al., 2013). (Weston et al., 2014) also use a neural architecture, but compose text embeddings from a lookup table of words. They also show that the learned embeddings can generalize to an unrelated task of document recommendation, justifying the use of hashtags as supervision for learning text representations."}, {"heading": "3 Tweet2Vec", "text": "Bi-GRU Encoder: Figure 1 shows our model for encoding tweets. It uses a similar structure to the C2W model in (Ling et al., 2015), with LSTM units replaced with GRU units.\nThe input to the network is defined by an alphabet of characters C (this may include the entire unicode character set). The input tweet is broken into a stream of characters c1, c2, ...cm each of which is represented by a 1-by-|C| encoding. These one-hot vectors are then projected to a character space by multiplying with the matrix PC \u2208\nR|C|\u00d7dc , where dc is the dimension of the character vector space. Let x1, x2, ...xm be the sequence of character vectors for the input tweet after the lookup. The encoder consists of a forwardGRU and a backward-GRU. Both have the same architecture, except the backward-GRU processes the sequence in reverse order. Each of the GRU units process these vectors sequentially, and starting with the initial state h0 compute the sequence h1, h2, ...hm as follows:\nrt = \u03c3(Wrxt + Urht\u22121 + br),\nzt = \u03c3(Wzxt + Uzht\u22121 + bz),\nh\u0303t = tanh(Whxt + Uh(rt ht\u22121) + bh), ht = (1\u2212 zt) ht\u22121 + zt h\u0303t.\nHere rt, zt are called the reset and update gates respectively, and h\u0303t is the candidate output state which is converted to the actual output state ht. Wr,Wz,Wh are dh \u00d7 dc matrices and Ur, Uz, Uh are dh \u00d7 dh matrices, where dh is the hidden state dimension of the GRU. The final states hfm from the forward-GRU, and hb0 from the backward GRU are combined using a fully-connected layer to the give the final tweet embedding et:\net = W fhfm +W bhb0 (1)\nHere W f ,W b are dt \u00d7 dh and b is dt \u00d7 1 bias term, where dt is the dimension of the final tweet embedding. In our experiments we set dt = dh. All parameters are learned using gradient descent.\nSoftmax: Finally, the tweet embedding is passed through a linear layer whose output is the same size as the number of hashtags L in the data set. We use a softmax layer to compute the posterior hashtag probabilities:\nP (y = j|e) = exp(wTj e+ bj)\u2211L i=1 exp(w T i e+ bj) . (2)\nObjective Function: We optimize the categorical cross-entropy loss between predicted and true hashtags:\nJ = 1\nB B\u2211 i=1 L\u2211 j=1 \u2212ti,jlog(pi,j) + \u03bb\u2016\u0398\u20162. (3)\nHere B is the batch size, L is the number of classes, pi,j is the predicted probability that the ith tweet has hashtag j, and ti,j \u2208 {0, 1} denotes the ground truth of whether the j-th hashtag is in the i-th tweet. We use L2-regularization weighted by \u03bb."}, {"heading": "4 Experiments and Results", "text": ""}, {"heading": "4.1 Word Level Baseline", "text": "Since our objective is to compare character-based and word-based approaches, we have also implemented a simple word-level encoder for tweets. The input tweet is first split into tokens along white-spaces. A more sophisticated tokenizer may be used, but for a fair comparison we wanted to keep language specific preprocessing to a minimum. The encoder is essentially the same as tweet2vec, with the input as words instead of characters. A lookup table stores word vectors for the V (20K here) most common words, and the rest are grouped together under the \u2018UNK\u2019 token."}, {"heading": "4.2 Data", "text": "Our dataset consists of a large collection of global posts from Twitter2 between the dates of June 1, 2013 to June 5, 2013. Only English language posts (as detected by the lang field in Twitter API) and posts with at least one hashtag are retained. We removed infrequent hashtags (< 500 posts) since they do not have enough data for good generalization. We also removed very frequent tags (> 19K posts) which were almost always from automatically generated posts (ex: #androidgame) which are trivial to predict. The final dataset contains 2 million tweets for training, 10K for validation and 50K for testing, with a total of 2039 distinct hashtags. We use simple regex to preprocess the post text and remove hashtags (since these are to be predicted) and HTML tags, and replace usernames and URLs with special tokens. We also removed retweets and convert the text to lower-case.\n2https://twitter.com/"}, {"heading": "4.3 Implementation Details", "text": "Word vectors and character vectors are both set to size dL = 150 for their respective models. There were 2829 unique characters in the training set and we model each of these independently in a character look-up table. Embedding sizes were chosen such that each model had roughly the same number of parameters (Table 2). Training is performed using mini-batch gradient descent with Nesterov\u2019s momentum. We use a batch size B = 64, initial learning rate \u03b70 = 0.01 and momentum parameter \u00b50 = 0.9. L2-regularization with \u03bb = 0.001 was applied to all models. Initial weights were drawn from 0-mean gaussians with \u03c3 = 0.1 and initial biases were set to 0. The hyperparameters were tuned one at a time keeping others fixed, and values with the lowest validation cost were chosen. The resultant combination was used to train the models until performance on validation set stopped increasing. During training, the learning rate is halved everytime the validation set precision increases by less than 0.01 % from one epoch to the next. The models converge in about 20 epochs. Code for training both the models is publicly available on github."}, {"heading": "4.4 Results", "text": "We test the character and word-level variants by predicting hashtags for a held-out test set of posts. Since there may be more than one correct hashtag per post, we generate a ranked list of tags for each\npost from the output posteriors, and report average precision@1, recall@10 and mean rank of the correct hashtags. These are listed in Table 3.\nTo see the performance of each model on posts containing rare words (RW) and frequent words (FW) we selected two test sets each containing 2,000 posts. We populated these sets with posts which had the maximum and minimum number of out-of-vocabulary words respectively, where vocabulary is defined by the 20K most frequent words. Overall, tweet2vec outperforms the word model, doing significantly better on RW test set and comparably on FW set. This improved performance comes at the cost of increased training time (see Table 2), since moving from words to characters results in longer input sequences to the GRU.\nWe also study the effect of model size on the performance of these models. For the word model we set vocabulary size V to 8K, 15K and 20K respectively. For tweet2vec we set the GRU hidden state size to 300, 400 and 500 respectively. Figure 2 shows precision 1 of the two models as the number of parameters is increased, for each test\nset described above. There is not much variation in the performance, and moreover tweet2vec always outperforms the word based model for the same number of parameters.\nTable 4 compares the models as complexity of the task is increased. We created 3 datasets (small, medium and large) with an increasing number of hashtags to be predicted. This was done by varying the lower threshold of the minimum number of tags per post for it to be included in the dataset. Once again we observe that tweet2vec outperforms its word-based counterpart for each of the three settings.\nFinally, table 1 shows some predictions from the word level model and tweet2vec. We selected these to highlight some strengths of the character based approach - it is robust to word segmentation errors and spelling mistakes, effectively interprets emojis and other special characters to make predictions, and also performs comparably to the word-based approach for in-vocabulary tokens."}, {"heading": "5 Conclusion", "text": "We have presented tweet2vec - a character level encoder for social media posts trained using supervision from associated hashtags. Our result shows that tweet2vec outperforms the word based approach, doing significantly better when the input post contains many rare words. We have focused only on English language posts, but the character\nmodel requires no language specific preprocessing and can be extended to other languages. For future work, one natural extension would be to use a character-level decoder for predicting the hashtags. This will allow generation of hashtags not seen in the training dataset. Also, it will be interesting to see how our tweet2vec embeddings can be used in domains where there is a need for semantic understanding of social media, such as tracking infectious diseases (Signorini et al., 2011). Hence, we provide an off-the-shelf encoder trained on medium dataset described above to compute vector-space representations of tweets along with our code on github."}, {"heading": "Acknowledgments", "text": "We would like to thank Alex Smola, Yun Fu, Hsiao-Yu Fish Tung, Ruslan Salakhutdinov, and Barnabas Poczos for useful discussions. We would also like to thank Juergen Pfeffer for providing access to the Twitter data, and the reviewers for their comments."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Using topic models for twitter hashtag recommendation", "author": ["Godin et al.2013] Fr\u00e9deric Godin", "Viktor Slavkovikj", "Wesley De Neve", "Benjamin Schrauwen", "Rik Van de Walle"], "venue": "In Proceedings of the 22nd international conference on World Wide Web", "citeRegEx": "Godin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Godin et al\\.", "year": 2013}, {"title": "Context-and contentaware embeddings for query rewriting in sponsored search", "author": ["Nemanja Djuric", "Vladan Radosavljevic", "Fabrizio Silvestri", "Narayan Bhamidipati"], "venue": "In Proceedings of the 38th International", "citeRegEx": "Grbovic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grbovic et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078", "author": ["Justin Johnson", "Fei-Fei Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Tiago Lu\u00eds", "Lu\u00eds Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong et al.2013] Thang Luong", "Richard Socher", "Christopher D Manning"], "venue": "In CoNLL,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Topic2vec: Learning distributed representations of topics. arXiv preprint arXiv:1506.08422", "author": ["Niu", "Dai2015] Li-Qiang Niu", "Xin-Yu Dai"], "venue": null, "citeRegEx": "Niu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2015}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Santos", "Victor Guimar\u00e3es"], "venue": "arXiv preprint arXiv:1505.05008", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "The use of twitter to track levels of disease activity and public concern in the us during the influenza a h1n1 pandemic", "author": ["Alberto Maria Segre", "Philip M Polgreen"], "venue": "PloS one,", "citeRegEx": "Signorini et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Signorini et al\\.", "year": 2011}, {"title": "tagspace: Semantic embeddings from hashtags", "author": ["Weston et al.2014] Jason Weston", "Sumit Chopra", "Keith Adams"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Neural network architectures overcome this problem by defining non-linear compositional models over vector space representations of tokens and hence assign non-zero probability even to sequences not seen during training (Bengio et al., 2003; Kiros et al., 2015).", "startOffset": 220, "endOffset": 261}, {"referenceID": 8, "context": "Recently, (Ling et al., 2015) challenge this assumption and present a bidirectional Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) for composing word vectors from their constituent characters which can memorize the arbitrary aspects of word orthography as well as generalize to rare and out-of-vocabulary words.", "startOffset": 10, "endOffset": 29}, {"referenceID": 10, "context": "Unsupervised approaches attempt to reconstruct the original text from its latent representation (Mikolov et al., 2013; Bengio et al., 2003).", "startOffset": 96, "endOffset": 139}, {"referenceID": 0, "context": "Unsupervised approaches attempt to reconstruct the original text from its latent representation (Mikolov et al., 2013; Bengio et al., 2003).", "startOffset": 96, "endOffset": 139}, {"referenceID": 1, "context": "We propose a Bi-directional Gated Recurrent Unit (Bi-GRU) (Chung et al., 2014) neural network for learning tweet representations.", "startOffset": 58, "endOffset": 78}, {"referenceID": 0, "context": "Using neural networks to learn distributed representations of words dates back to (Bengio et al., 2003).", "startOffset": 82, "endOffset": 103}, {"referenceID": 10, "context": "More recently, (Mikolov et al., 2013) released word2vec - a collection of word vectors trained using a recurrent neural network.", "startOffset": 15, "endOffset": 37}, {"referenceID": 3, "context": ", 2015), documents and paragraphs (Le and Mikolov, 2014), topics (Niu and Dai, 2015) and queries (Grbovic et al., 2015).", "startOffset": 97, "endOffset": 119}, {"referenceID": 8, "context": "All these methods require storing an extremely large table of vectors for all word types and cannot be easily generalized to unseen words at test time (Ling et al., 2015).", "startOffset": 151, "endOffset": 170}, {"referenceID": 8, "context": "In (Ling et al., 2015), the authors present a compositional character model based on bidirectional LSTMs as a potential solution to these problems.", "startOffset": 3, "endOffset": 22}, {"referenceID": 8, "context": "While (Ling et al., 2015) generate word embeddings from character representations, we propose to generate vector representations of entire tweets from characters in our tweet2vec model.", "startOffset": 6, "endOffset": 25}, {"referenceID": 16, "context": "Our work adds to the growing body of work showing the applicability of character models for a variety of NLP tasks such as Named Entity Recognition (Santos and Guimar\u00e3es, 2015), POS tagging (Santos and Zadrozny, 2014), text classification (Zhang et al., 2015) and language modeling (Karpathy et al.", "startOffset": 239, "endOffset": 259}, {"referenceID": 5, "context": ", 2015) and language modeling (Karpathy et al., 2015; Kim et al., 2015).", "startOffset": 30, "endOffset": 71}, {"referenceID": 6, "context": ", 2015) and language modeling (Karpathy et al., 2015; Kim et al., 2015).", "startOffset": 30, "endOffset": 71}, {"referenceID": 9, "context": "Previously, (Luong et al., 2013) dealt with the problem of estimating rare word representations by building them from their constituent morphemes.", "startOffset": 12, "endOffset": 32}, {"referenceID": 15, "context": "Hashtag prediction for social media has been addressed earlier, for example in (Weston et al., 2014; Godin et al., 2013).", "startOffset": 79, "endOffset": 120}, {"referenceID": 2, "context": "Hashtag prediction for social media has been addressed earlier, for example in (Weston et al., 2014; Godin et al., 2013).", "startOffset": 79, "endOffset": 120}, {"referenceID": 15, "context": "(Weston et al., 2014) also use a neural architecture, but compose text embeddings from a lookup table of words.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "It uses a similar structure to the C2W model in (Ling et al., 2015), with LSTM units replaced with GRU units.", "startOffset": 48, "endOffset": 67}, {"referenceID": 14, "context": "Also, it will be interesting to see how our tweet2vec embeddings can be used in domains where there is a need for semantic understanding of social media, such as tracking infectious diseases (Signorini et al., 2011).", "startOffset": 191, "endOffset": 215}], "year": 2016, "abstractText": "Text from social media provides a set of challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts, leading to a prohibitively large vocabulary size for word-level approaches. We propose a character composition model, tweet2vec, which finds vectorspace representations of whole tweets by learning complex, non-local dependencies in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many outof-vocabulary words or unusual character sequences. Our tweet2vec encoder is publicly available1.", "creator": "LaTeX with hyperref package"}}}