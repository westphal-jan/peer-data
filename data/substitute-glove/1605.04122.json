{"id": "1605.04122", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2016", "title": "Natural Language Semantics and Computability", "abstract": "This paper is a reflexion the the computability whose particular language semantics. It knows give containing a first model longer work determined for then demands semantics same rich arabic: although except rather a chemistry useful other set premise models now algorithms currently used in and idiom object-oriented, consistent as the genome with a proposal to logical formulas - formulas, because 's statement that come ambiguous. We want that for comes seen possible become semantics any last put, none them compute however semantic defines (s) of a also according, including fundamental of morphological origin. We given discuss the sociolinguistic subtle one perhaps process.", "histories": [["v1", "Fri, 13 May 2016 10:46:22 GMT  (37kb,D)", "http://arxiv.org/abs/1605.04122v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CC", "authors": ["richard moot", "christian retor\\'e"], "accepted": false, "id": "1605.04122"}, "pdf": {"name": "1605.04122.pdf", "metadata": {"source": "CRF", "title": "Natural Language Semantics and Computability", "authors": ["Richard Moot", "Christian Retor\u00e9"], "emails": [], "sections": [{"heading": null, "text": "Introduction\nIn the well-known Turing test for artificial intelligence, a human interrogator needs to decide, via a question answering session with two terminals, which of his two interlocutors is a man and which is a machine (Turing 1950). Although early systems like Eliza based on matching word patterns may seem clever at first sight, they clearly do not pass the test. One often forgets that, in addition to reasoning and access to knowledge representation, passing the Turing test presupposes automated natural language analysis and generation which, despite significant progress in the field, has not yet been fully achieved. These natural language processing components of the Turing test are of independent interest and used in computer programs for question answering and translation (however, since both of these tasks are generally assumed to be AI-complete it is unlikely that a full solution for these problems would be simpler than a solution for the Turing test itself).\nIf we define the semantics of a (sequence of) sentence(s) \u03c3 as the mapping to a representation \u03c6(\u03c3) that can be used by a machine for natural language processing tasks, two very different ideas of semantics come to mind.\n1. One notion of semantics describes what the sentence(s) speaks about. The dominant model for this type of semantics represents meaning using word vectors (only involving referential/full words nouns, adjectives, verbs, adverbs, . . . and not grammatical words) which represent what \u03c3 speaks about. This is clearly computable. One must fix a thesaurus of n words that acts as a vector basis. Usually words not in the thesaurus or basis are expanded into their definition with words in the thesaurus. By counting occurrences of words from the thesaurus in the text (substituting words not in the thesaurus with their definition) and turning this into a n-dimensional vector reduced to be of euclidian norm 1, we obtain word meanings in the form of\nar X\niv :1\n60 5.\n04 12\n2v 1\n[ cs\n.C L\n] 1\n3 M\nay 2\n01 6\nn-dimensional vectors. This notion of semantics provides a useful measure of semantic similarity between words and texts; typical applications include exploring Big Data and finding relevant pages on the internet. This kind of semantics models what a word (or a text) speaks about. 2. The other notion of semantics, the one this paper is about, is of a logical nature. It models what is asserted, refuted, . . . assumed by the sentences. According to this view, computational semantics is the mapping of sentence(s) to logical formula(s). This is usually done compositionally, according to Frege\u2019s principle \u201cthe meaning of a compound expression is a function of the meaning of its components\u201d to which Montague added \u201cand of its syntactic structure\u201d. This paper focuses on this logical and compositional notion of semantics and its extension (by us and others) to lexical semantics; these extensions allow us to conclude from a sentence like \u201cI started a book\u201d that the speaker started reading (or, depending on the context, writing) a book.\nWe should comment that, in our view, semantics is a (computable) function from sentence(s) to logical formulae, since this viewpoint is not so common in linguistics.\n\u2013 Cognitive sciences also consider the language faculty as a computational device and insist on the computations involved in language analysis and production. Actually there are two different views of this cognitive and computational view: one view, promoted by authors such as Pinker (1994), claims that there is a specific cognitive function for language, a \u201clanguage module\u201d in the mind, while others, like Langacker (2008), think that our language faculty is just our general cognitive abilities applied to language. \u2013 In linguistics and above all in philosophy of language many people think that sentences cannot have any meaning without a context, such a context involving both linguistic and extra-linguistic information. Thus, according to this view, the input of our algorithm should include context. Our answer is firstly that linguistic context is partly taken into account since we are able to produce, in addition to formulae, discourse structures. Regarding the part of context that we cannot take into account, be it linguistic or not, our answer is that it is not part of semantics, but rather an aspect of pragmatics. And, as argued by Corblin (2013), if someone is given a few sentences on a sheet of paper without any further information, he starts imagining situations, may infer other statements from what he reads, . . . , and such thoughts are the semantics of the sentence. \u2013 The linguistic tradition initiated by Montague (1974) lacks some coherence regarding computability. On the one hand, Montague gives an algorithm for parsing sentences and for computing their meaning as a logical formula. On the other hand, he asserts that the meaning of a sentence is the interpretation of the formula in possible worlds, but these models are clearly uncomputable! Furthermore, according to him, each intermediate step, including the intensional/modal formulae should be forgotten, and the semantics is defined as the set of possible worlds in which the semantic formula is true: this cannot even be finitely described, except by these intermediate formulas; a fortiori it cannot be computed. Our view is different, for at least three reasons, from the weakest to the strongest:\n\u2022 Models for higher order logic, as in Montague, are not as simple as is sometimes assumed, and they do not quite match the formulas: completeness fails. This means that a model and even all models at once contains less information than the formula itself. \u2022 We do not want to be committed to any particular interpretation. Indeed, there are alternative relevant interpretations of formulas, as the following non exhaustive list shows: dialogical interpretations (that are the sets of proofs and/or refutations), game theoretic semantics and ludics (related to the former style of interpretation), set of consequences of the formula, structures inhabited by their normal proofs as in intuitionistic logic,... \u2022 Interpreting the formula(s) is no longer related to linguistics, although some interpretations might useful for some applications. Indeed, once you have a a formula, interpreting it in your favourite way is a purely logical question. Deciding whether it is true or not in a model, computing all its proofs or all its refutations, defining game strategies, computing its consequences or the corresponding structure has nothing to do with the particular natural language statement you started with."}, {"heading": "1 Computational semantics a\u0300 la Montague", "text": "We shall first present the general algorithm that maps sentences to logical formulae, returning to lexical semantics in Section 2. The first step is to compute a syntactic analysis that is rich and detailed enough to enable the computation of the semantics (in the form of logical formulae). The second step is to incorporate the lexical lambda terms and to reduce the obtained lambda term \u2014 this step possibly includes the choice of some lambda terms from the lexicon that fix the type mismatches."}, {"heading": "1.1 Categorial syntax", "text": "In order to express the process that maps a sentence to its semantic interpretation(s) in the form of logical formulae, we shall start with a categorial grammar. This is not strictly necessary: Montague (1974) used a context free grammar (augmented with a mechanism for quantifier scope), but if one reads between the lines, at some points he converts the phrase structure into a categorial derivation, so we shall, following Moot & Retore\u0301 (2012), directly use a categorial analysis. Although richer variants of categorial grammars are possible, and used in practice, we give here an example with Lambek grammars, and briefly comment on variants later.\nCategories are freely generated from a set of base categories, for example np (noun phrase), n (common noun), S (sentence), by two binary operators: \\ and /: A\\B and B/A are categories whenever A and B are categories. A category A\\B intuitively looks for a category A to its left in order to form a B. Similarly, a category B/A combines with an A to its right to form a B. The full natural deduction rules are shown in Figure 1.\nA lexicon provides, for each word w of the language, a finite set of categories lex(w). We say a sequence of words w1, . . . ,wn is of type C whenever \u2200i\u2203ci \u2208 lex(wi) c1, . . . ,cn ` C. Figure 2 shows an example lexicon (top) and a derivation of a sentence (bottom)."}, {"heading": "1.2 From syntactic derivation to typed linear lambda terms", "text": "Categorial derivations, being a proper subset of derivations in multiplicative intuitionistic linear logic, correspond to (simply typed) linear lambda terms. This makes the connection to Montague grammar particularly transparent.\nDenoting by e the set of entities (or individuals) and by t the type for propositions (these can be either true or false, hence the name t) one has the following mapping from syntactic categories to semantic/logical types.\n(Syntactic type)\u2217 = Semantic type S\u2217 = t a sentence is a proposition\nnp\u2217 = e a noun phrase is an entity n\u2217 = e\u2192 t a noun is a subset of the set of entities (maps entities\nto propositions) (A\\B)\u2217 = (B/A)\u2217 = A\u2217\u2192 B\u2217 extends easily to all syntactic categories\nUsing this translation of categories into types which forgets the non commutativity, the Lambek calculus proof of Figure 2 is translated to the linear intuitionistic proof shown in Figure 3; we have kept the order of the premisses unchanged to highlight the similarity with the previous proof. Such a proof can be viewed as a simply typed lambda term with the two base types e and t.\nAs observed by Church (1940), the simply typed lambda calculus with two types e and t is enough to express higher order logic, provided one introduces constants for the logical connectives and quantifiers, that is a constants \u201c\u2203\u201d and \u201c\u2200\u201d of type (e\u2192 t)\u2192 t, and constants \u201c\u2227\u201d, \u201c\u2228\u201d et \u201c\u21d2\u201d of type t\u2192 (t\u2192 t).\nIn addition to the syntactic lexicon, there is a semantic lexicon that maps any word to a simply typed lambda term with atomic types e and t and whose type is the translation of its syntactic formula. Figure 4 presents such a lexicon for our current example. For example, the word \u201cevery\u201d is assigned formula (S/(np\\S))/n. According to the translation function above, we know the corresponding semantic term must be of type (e\u2192 t)\u2192 ((e\u2192 t)\u2192 t), as it is in Figure 3. The term we assign in in the semantic lexicon is the following (both the type and the term are standard in a Montagovian setting).\n\u03bbPe\u2192t \u03bbQe\u2192t (\u2200(e\u2192t)\u2192t (\u03bbxe(\u21d2t\u2192(t\u2192t) (P x)(Q x))))\nUnlike the lambda terms computed for proof, the lexical entries in the semantic lexicon need not be linear: the lexical entry above is not a linear lambda term since the single abstraction binds two occurrences of x.\nSimilarly, the syntactic type of \u201ca\u201d, the formula ((S/np)\\S)/n has corresponding semantic type (e\u2192 t)\u2192 ((e\u2192 t)\u2192 t) (though syntactically different, a subject and an object generalized quantifier have the same semantic type), and the following lexical meaning recipe.\n\u03bbPe\u2192t \u03bbQe\u2192t (\u2203(e\u2192t)\u2192t (\u03bbxe(\u2227t\u2192(t\u2192t)(P x)(Q x)))) Finally, \u201ckid\u201d, \u201ccartoon\u201d and \u201cwatched\u201d are assigned the constants kide\u2192t ,\ncartoone\u2192t and watchede\u2192(e\u2192t) respectively.\nBecause the types of these lambda terms are the same as those of the words in the initial lambda term, we can take the linear lambda term associated with the sentence and substitute, for each word its corresponding lexical meaning, transforming the derivational semantics, in our case the following3\n(a(e\u2192t)\u2192((e\u2192t)\u2192t) cartoone\u2192t)(\u03bbye(every(e\u2192t)\u2192((e\u2192t)\u2192t) kide\u2192t)(watchede\u2192e\u2192t y))\ninto an (unreduced) representation of the meaning of the sentence.\n((\u03bbPe\u2192t \u03bbQe\u2192t(\u2203(e\u2192t)\u2192t (\u03bbxe(\u2227t\u2192(t\u2192t)(P x)(Q x)))))cartoone\u2192t)\n((\u03bbye(((\u03bbPe\u2192t \u03bbQe\u2192t (\u2200(e\u2192t)\u2192t (\u03bbxe(\u21d2t\u2192(t\u2192t) (P x)(Q x)))))kide\u2192t x)))(watchede\u2192e\u2192ty)) 3 There are exactly two (non-equivalent) proofs of this sentence. The second proof using the\nsame premisses corresponds to the second, more prominent reading of the sentence whose lambda term is: (every kid)(\u03bbxe.(a cartoon)(\u03bbye((watched y) x))\nThe above term reduces to\n(\u2203(e\u2192t)\u2192t \u03bbxe(\u2227t\u2192(t\u2192t)(cartoon x)(\u2200(e\u2192t)\u2192t (\u03bb ze(\u21d2t\u2192(t\u2192t) (kid z)((watched x)z))))))\nthat is4: \u2203x.cartoon(x)\u2227\u2200z.kid(z)\u21d2 watched(z,x) The full algorithm to compute the semantics of a sentence as a logical formula is shown in Figure 5."}, {"heading": "2 Adding sorts, coercions, and uniform operations", "text": "Montague (as Frege) only used a single type for entities: e. But it is much better to have many sorts in order to block the interpretation of some sentences:\n(1) * The table barked.\n(2) The dog barked.\n(3) ?The sergeant barked.\nAs dictionaries say \u201cbarked\u201d can be said from animals, usually dogs. The first one is correctly rejected: one gets barkdog\u2192t(the table)artifact and dog 6= artifact.\nHowever we need to enable the last example barkdog\u2192t(the sergeant)human and in this case we use coercions (Bassac et al. 2010, Retore\u0301 2014): the lexical entry for the verb \u201cbarked\u201d which only applies to the sort of \u201cdogs\u201d provides a coercion c : human\u2192 dog from \u201chuman\u201d to \u201cdog\u201d. The revised lexicon provides each word with the lambda term that we saw earlier (typed using some of the several sorts / base type) and some optional lambda terms that can be used if needed to solve type mismatches.\nSuch coercions are needed to understand sentences like: 4 We use the standard convention to translate a term ((py)x) into a predicate p(x,y).\nThe first two sentences will respectively use a coercions from book to physical object and a coercion from books to information. Any time an object has several related meanings, one can consider the conjunction of properties referring to those particular aspects. For these operations (and others acting uniformly on types) we exploit polymorphically typed lambda terms (system F). When the related meanings of a word are incompatible (this is usually the case) the corresponding coercions are declared to be incompatible in the lexicon (one is declared as rigid). This extended process is described in Figure 6. Some remarks on our use of system F:\n\u2013 We use it for the syntax of semantics (a.k.a. metalogic, glue logic) \u2013 The formulae of semantics are the usual ones (many sorted as in Tyn) \u2013 We have a single constant for operations that act uniformly on types, like quantifiers\nor conjunction over predicates that apply to different facets of a given word."}, {"heading": "3 Complexity of the syntax", "text": "As we remarked before, when computing the formal semantics of a sentence in the Montague tradition, we (at least implicitly) construct a categorial grammar proof. Therefore, we need to study the complexity of parsing/theorem proving in categorial\ngrammar first. The complexity generally studied in this context is the complexity of deciding about the existence of a proof (a parse) for a logical statement (a natural language sentence) as a function of the number of words in this sentence5.\nPerhaps surprisingly, the simple product-free version of the Lambek calculus we have used for our examples is already NP-complete (Savateev 2009). However, there is a notion of order, which measures the level of \u201cnesting\u201d of the implications as defined below.\norder(p) = 0 order(A/B) = order(B\\A) = max(order(A),(order(B)+1))\nAs an example, the order of formula (np\\S)/np is 1, whereas the order of formula S/(np\\S) is 2. For the Lambek calculus, the maximum order of the formulas in a grammar is a good indication of its complexity. Grammars used for linguistic purposes generally have formulas of order 3 or, at most, 4. We know that once we bound the order of formulas in the lexicon of our grammars to be less than a fixed n, parsing becomes polynomial for any choice of n (Pentus 2010)6.\nThe NP-completeness proof of Savateev (2009) uses a reduction from SAT, where a SAT problem with c clauses and v variables produces a Lambek grammar of order 3+4c, with (2c+1)(3v+1) atomic formulas.\nThe notion of order therefore provides a neat indicator of the complexity: the NPcompleteness proof requires formulas of order 7 and greater, whereas the formulas used for linguistic modelling are of order 4 or less.\nEven though the Lambek calculus is a nice and simple system, we know that the Lambek calculus generates only context-free languages (Pentus 1995), and there is good evidence that at least some constructions in natural language require a slightly larger class of languages (Shieber 1985). One influential proposal for such a larger class of languages are the mildly context-sensitive languages (Joshi 1985), characterised as follows.\n\u2013 contains the context-free languages, \u2013 limited cross-serial dependencies (i.e includes anbncn but maybe not anbncndnen) \u2013 semilinearity (a language is semilinear iff there exists a regular language to which\nit is equivalent up to permutation) \u2013 polynomial fixed recognition7\n5 For many algorithms, the complexity is a function of the number of atomic subformulas of the formulas in the sentence. Empirically estimation shows the number of atomic formulas is a bit over twice the number of words in a sentence. 6 For the algorithm of Pentus (2010), the order appears as an exponent in the worst-case complexity: for a grammar of order n there is a multiplicative factor of 25(n+1). So though polynomial, this algorithm is not necessarily efficient. 7 The last two items are sometimes stated as the weaker condition \u201cconstant growth\u201d instead of semilinearity and the stronger condition of polynomial parsing instead of polynomial fixed recognition. Since all other properties are properties of formal languages, we prefer the formal language theoretic notion of polynomial fixed recognition.\nThere are various extensions of the Lambek calculus which generate mildly contextsensitive languages while keeping the syntax-semantics interface essentially the same as for the Lambek calculus. Currently, little is known about upper bounds of the classes of formal languages generated by these extensions of the Lambek calculus. Though Moot (2002) shows that multimodal categorial grammars generate exactly the contextsensitive languages, Buszkowski (1997) underlines the difficulty of adapting the result of Pentus (1995) to extensions of the Lambek calculus8.\nBesides problems from the point of view of formal language theory, it should be noted that the goal we set out at the start of this paper was not just to generate the right string language but rather to generate the right string-meaning pairs. This poses additional problems. For example, a sentence with n quantified noun phrases has up to n! readings. Although the standard notion of complexity for categorial grammars is the complexity deciding whether or not a proof exists, formal semanticists, at least since Montague (1974), want their formalisms to generate all and only the correct readings for a sentence: we are not only interested in whether or not a proof exists but, since different natural deduction proofs correspond to different readings, also in what the different proofs of a sentence are9.\nWhen we look at the example below\n(10) Every representative of a company saw most samples.\nit has five possible readings (instead of 3! = 6), since the reading\n\u2200x.representative of(x,y)\u21d2 most(z,sample(z))\u21d2\u2203y.company(y) \u2227see(x,z)\nhas an unbound occurrence of y (the leftmost occurrence). The Lambek calculus analysis has trouble with all readings where \u201ca company\u201d has wide scope over at least one of the two other quantifiers. We can, of course, remedy this by adding new, more complex types to the quantifier \u201ca\u201d, but this would increase the order of the formulas and there is, in principle, no bound on the number of constructions where a medial quantifier has wide scope over a sentence. A simple counting argument shows that Lambek calculus grammars cannot generate the n! readings required for quantifier scope of an nquantifier sentence: the number of readings for a Lambek calculus proof is proportional to the Catalan numbers and this number is in o(n!)10; in other words, given a Lambek\n8 We can side-step the need for a Pentus-like proof by looking only at fragments of order 1, but these fragments are insufficient even for handling quantifier scope. 9 Of course, when our goal is to generate (subsets of) n! different proofs rather than a single proof (if one exists), then we are no longer in NP, though it is unknown whether an algorithm exists which produces a sort of shared representation for all such subsets such that 1) the algorithm outputs \u201cno\u201d when the sentence is ungrammatical 2) the algorithm has a fairly trivial algorithm (say of a low-degree polynomial at worst) for recovering all readings from the shared representation 3) the shared structure is polynomial in the size of the input. 10 We need to be careful here: the number of readings for a sentence with n quantifiers is \u0398(n!), whereas the maximum number of Lambek calculus proofs is O(cc2n0 Cc1c2n), for constants c0, c1, c2 which depend on the grammar (c0 is the maximum number of formulas for a single word, c1 is the maximum number of (negative) atomic subformulas for a single formula and c2 represent the minimum number of words needed to add a generalized quantifier to a sentence,\ncalculus grammar, the number of readings of a sentence with n quantifiers grows much faster than the number of Lambek calculus proofs for this sentence, hence the grammar fails to generate many of the required readings.\nSince the eighties, many variants and extensions of the Lambek calculus have been proposed, each with the goal of overcoming the limitations of the Lambek calculus. Extensions/variations of the Lambek calculus \u2014 which include multimodal categorial grammars (Moortgat 1997), the Displacement calculus (Morrill et al. 2011) and firstorder linear logic (Moot & Piazza 2001) \u2014 solve both the problems of formal language theory and the problems of the syntax-semantics interface. For example, there are several ways of implementing quantifiers yielding exactly the five desired readings for sentence 10 without appealing to extra-grammatical mechanisms. Carpenter (1994) gives many examples of the advantages of this logical approach to scope, notably its interaction with other semantic phenomena like negation and coordination.\nThough these modern calculi solve the problems with the Lambek calculus, they do so without excessively increasing the computational complexity of the formalism: multimodal categorial grammars are PSPACE complete (Moot 2002), whereas most other extensions are NP-complete, like the Lambek calculus.\nEven the most basic categorial grammar account of quantifier scope requires formulas of order 2, while, in contrast to the Lambek calculus, the only known polynomial fragments of these logics are of order 1. Hence the known polynomial fragments have very limited appeal for semantics.\nIs the NP-completeness of our logics in conflict with the condition of polynomial fixed recognition required of mildly context-sensitive formalisms? Not necessarily, since our goals are different: we are not only interested in the string language generated by our formalism but also in the string-meaning mappings. Though authors have worked on using mildly context-sensitive formalisms for semantics, they generally use one of the two following strategies for quantifier scope: 1) an external mechanism for computing quantifier scope (e.g. Cooper storage, (Cooper 1975)), or 2) an underspecification mechanism for representing quantifier scope (Fox & Lappin 2010).\nFor case 1 (Cooper 1975), a single syntactic structure is converted into up to n! semantic readings, whereas for case 2, though we represent all possible readings in a single structure, even deciding whether the given sentence has a semantic reading at all becomes NP-complete (Fox & Lappin 2010), hence we simply shift the NP-completeness from the syntax to the syntax-semantics interface11. Our current understanding therefore indicates that NP-complete is the best we can do when we want to generate the semantics for a sentence. We do not believe this to be a bad thing, since pragmatic and processing constraints rule out many of the complex readings and enumerating all readings of sentences like sentence 10 above (and more complicated examples) is a difficult task. There is a trade-off between the work done in the syntax and in the syntax-semantics interface, where the categorial grammar account incorporates more\ni.e. c2n is the number of words required to produce an n-quantifier sentence) and O(c c2n 0 Cc1c2n)\nis in o(n!). 11 In addition, Ebert (2005) argues that underspecification languages are not expressive enough\nto capture all possible readings of a sentence in a single structure. So underspecification does not solve the combinatorial problem but, at best, reduces it.\nthan the traditional mildly context-sensitive formalisms. It is rather easy to set up a categorial grammar parser in such a way that it produces underspecified representations in time proportional to n2 (Moot 2007). However, given that such an underspecified representation need not have any associated semantics, such a system would not actually qualify as a parser. We believe, following Carpenter (1994) and Jacobson (2002), that giving an integrated account of the various aspects of the syntax-semantics interface is the most promising path.\nOur grammatical formalisms are not merely theoretical tools, but also form the basis of several implementations (Morrill & Valent\u0131\u0301n 2015, Moot 2015), with a rather extensive coverage of various semantic phenomena and their interactions, including quantification, gapping, ellipsis, coordination, comparative subdeletion, etc."}, {"heading": "4 Complexity of the semantics", "text": "The complexity of the syntax discussed in the previous section only considered the complexity of computing unreduced lambda terms as the meaning of a sentence. Even in the standard, simply typed Montagovian framework, normalizing lambda terms is known to be of non-elementary complexity (Schwichtenberg 1982), essentially due to the possibility of recursive copying. In spite of this forbidding worst-time complexity, normalization does not seem to be a bottleneck in the computation of meaning for practical applications (Bos et al. 2004, Moot 2010).\nIs there a deeper reason for this? We believe that natural language semantics uses a restricted fragment of the lambda calculus, soft lambda calculus. This calculus restricts recursive copying and has been shown to characterize the complexity class P exactly (Lafont 2004, Baillot & Mogbil 2004). Hence, this would explain why even naive implementations of normalization perform well in practice.\nThe question of whether soft linear logic suffices for our semantic parser may appear hard to answer, however, it an obvious (although tedious) result. To show that all the semantic lambda terms can be typed in soft linear logic, we only need to verify that every lambda in the lexicon is soft. There is a finite number of words, with only a finite number of lambda terms per word. Furthermore, words from open classes (nouns, verbs, adjectifs, manner adverbs,... in which speakers may introduce new words... about 200.000 inflected word forms) are the most numerous and all have soft and often even linear lambda terms. Thus only closed class words (grammatical words such as pronouns, conjunctions, auxiliary verbs,... and some complex adverbs, such as \u201ctoo\u201d) may potentially need a non-soft semantic lambda term: there are less than 500 such words, so it is just a matter of patience to prove they all have soft lambda terms. Of course, finding deep reasons (cognitive, linguistic) for semantic lambda terms to be soft in any language would be much more difficult (and much more interesting!).\nWhen adding coercions, as in Section 2, the process becomes a bit more complicated. However, the system of Lafont (2004) includes second-order quantifiers hence reduction stays polynomial once coercions have been chosen. Their choice (as the choice of the syntactic category) increases complexity: when there is a type mismatch gA\u2192X uB one needs to chose one of the coercions of type B\u2192 A provided by the entries of the words in the analysed phrase, with the requirement that when a rigid coercion is used,\nall other coercions provided by the same word are blocked (hence rigid coercions, as opposed to flexible coercions decrease the number of choices for other type mismatches).\nFinally, having computed a set of formulas in higher-order logic corresponding to the meaning of a sentence, though of independent interest for formal semanticists, is only a step towards using these meaning representations for concrete applications. Typical applications such as question answering, automatic summarization, etc. require world knowledge and common sense reasoning but also a method for deciding about entailment: that is, given a set of sentences, can we conclude that another sentence is true. This question is of course undecidable, already in the first-order case. However, some recent research shows that even higher-order logic formulas of the type produced by our analysis can form the basis of effective reasoning mechanisms (Chatzikyriakidis & Luo 2014, Mineshima et al. 2015) and we leave it as an interesting open question to what extent such reasoning can be applied to natural language processing tasks."}, {"heading": "5 Conclusion", "text": "It is somewhat surprising that, in constrast to the well-developed theory of the algorithmic complexity of parsing, little is known about semantic analysis, even though computational semantics is an active field, as the recurring conferences with the same title as well as the number of natural language processing applications show. In this paper we simply presented remarks on the computability and on the complexity of this process. The good news is that semantics (at least defined as a set of logical formula) is computable. This was known, but only implicitly: Montague gave a set of instruction to compute the formula (and to interpret it in a model), but he never showed that, when computing such logical formula(s):\n\u2013 the process he defined stops with a normal lambda terms of type proposition (t), \u2013 eta-long normal lambda terms with constants being either logical connectives or\nconstants of a first (or higher order) logical language are in bijective correspondence with formulas of this logical language (this is more or less clear in the work of Church (1940) on simple type theory). \u2013 the complexity of the whole process has a known complexity class, in particular the beta-reduction steps which was only discovered years after his death (Schwichtenberg 1982).\nA point that we did not discuss is that we considered worst case complexity viewed as a function from the number of words in a sentence a logical formula. Both aspects of our point of view can be challenged: in practice, grammar size is at least as importance as sentence length and average case complexity may be more appropriate than worst case complexity. Though the high worst case complexity shows that computing the semantics of a sentence is not always efficient, we nevertheless believe, confirmed by actual practice, that statistical models of a syntactic or semantic domain improve efficiency considerably, by providing extra information (as a useful though faillible \u201coracle\u201d) for many of the difficult choices. Indeed, human communication and understanding are very effective in general, but, from time to time, we misunderstand eachother or\nneed to ask for clarifications. For computers, the situation is almost identical: most sentences are analysed quickly, while some require more time or even defeat the software. Even though it is quite difficult to obtain the actual probability distribution on sentencemeaning pairs, we can simply estimate such statistics empirically by randomly selecting manually annotated examples from a corpus. The other aspect, the sentence length, is, as opposed to what is commonly assumed in complexity theory, not a very safisfactory empirical measure of performance: indeed the average number of words per sentence is around 10 in spoken language and around 25 in written language. Sentences with more than 100 words are very rare12. Furthermore, lengthy sentences tend to have a simple structure, because otherwise they would quickly become incomprehensible (and hard to produce as well). Experience with parsing shows that in many cases, the grammar size is at least as important as sentence length for the empirical complexity of parsing algorithms (Joshi 1997, Sarkar 2000, Go\u0301mez-Rodr\u0131\u0301guez et al. 2006). Grammar size, though only a constant factor in the complexity, tends to be a big constant for realistic grammars: grammars with between 10.000 and 20.000 rules are common.\nWe believe that the complexity of computing the semantics and of reasoning with the semantic representations are some of the most important reasons that the Turing test is presently out of reach.\n12 To given an indication, the TLGbank contains more than 14.000 French sentences and has a median of 26 words per sentence, 99% of sentences having less than 80 words, with outliers at 190 and at 266 (the maximum sentence length in the corpus)."}], "references": [{"title": "Soft lambda-calculus: a language for polynomial time computation, in \u2018Foundations of software science and computation structures", "author": ["P. Baillot", "V. Mogbil"], "venue": null, "citeRegEx": "Baillot and Mogbil,? \\Q2004\\E", "shortCiteRegEx": "Baillot and Mogbil", "year": 2004}, {"title": "Towards a type-theoretical account of lexical semantics", "author": ["C. Bassac", "B. Mery", "C. Retor\u00e9"], "venue": "Journal of Logic, Language and Information", "citeRegEx": "Bassac et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bassac et al\\.", "year": 2010}, {"title": "Widecoverage semantic representation from a CCG parser, in \u2018Proceedings of COLING2004", "author": ["J. Bos", "S. Clark", "M. Steedman", "J.R. Curran", "J. Hockenmaier"], "venue": null, "citeRegEx": "Bos et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Mathematical linguistics and proof theory", "author": ["W. Buszkowski"], "venue": "eds, \u2018Handbook of Logic and Language\u2019, Elsevier,", "citeRegEx": "Buszkowski,? \\Q1997\\E", "shortCiteRegEx": "Buszkowski", "year": 1997}, {"title": "Quantification and scoping: A deductive account, in \u2018The Proceedings of the 13th West Coast Conference on Formal Linguistics", "author": ["B. Carpenter"], "venue": null, "citeRegEx": "Carpenter,? \\Q1994\\E", "shortCiteRegEx": "Carpenter", "year": 1994}, {"title": "Natural language inference in Coq", "author": ["S. Chatzikyriakidis", "Z. Luo"], "venue": "Journal of Logic, Language and Information", "citeRegEx": "Chatzikyriakidis and Luo,? \\Q2014\\E", "shortCiteRegEx": "Chatzikyriakidis and Luo", "year": 2014}, {"title": "A formulation of the simple theory of types", "author": ["A. Church"], "venue": "Journal of Symbolic Logic", "citeRegEx": "Church,? \\Q1940\\E", "shortCiteRegEx": "Church", "year": 1940}, {"title": "Montague\u2019s Semantic Theory and Transformational Grammar, PhD thesis, University of Massachusetts", "author": ["R. Cooper"], "venue": null, "citeRegEx": "Cooper,? \\Q1975\\E", "shortCiteRegEx": "Cooper", "year": 1975}, {"title": "Cours de s\u00e9mantique: Introduction, Armand Colin", "author": ["F. Corblin"], "venue": null, "citeRegEx": "Corblin,? \\Q2013\\E", "shortCiteRegEx": "Corblin", "year": 2013}, {"title": "Formal Investigations of Underspecified Representations, PhD thesis, King\u2019s College, University of London", "author": ["C. Ebert"], "venue": null, "citeRegEx": "Ebert,? \\Q2005\\E", "shortCiteRegEx": "Ebert", "year": 2005}, {"title": "Expressiveness and complexity in underspecified semantics", "author": ["C. Fox", "S. Lappin"], "venue": "Linguistic Analysis", "citeRegEx": "Fox and Lappin,? \\Q2010\\E", "shortCiteRegEx": "Fox and Lappin", "year": 2010}, {"title": "On the theoretical and practical complexity of TAG parsers, in \u2018Proceedings of Formal Grammar (FG 2006)", "author": ["C. G\u00f3mez-Rodr\u0131\u0301guez", "M.A. Alonso", "M. Vilares"], "venue": null, "citeRegEx": "G\u00f3mez.Rodr\u0131\u0301guez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "G\u00f3mez.Rodr\u0131\u0301guez et al\\.", "year": 2006}, {"title": "The (dis)organization of the grammar: 25 years", "author": ["P. Jacobson"], "venue": "Linguistics and Philosophy", "citeRegEx": "Jacobson,? \\Q2002\\E", "shortCiteRegEx": "Jacobson", "year": 2002}, {"title": "Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions?, in \u2018Natural Language Parsing", "author": ["A. Joshi"], "venue": null, "citeRegEx": "Joshi,? \\Q1985\\E", "shortCiteRegEx": "Joshi", "year": 1985}, {"title": "Survey of the State of the Art in Human Language Technology", "author": ["A. Joshi"], "venue": "Parsing techniques,", "citeRegEx": "Joshi,? \\Q1997\\E", "shortCiteRegEx": "Joshi", "year": 1997}, {"title": "Soft linear logic and polynomial time", "author": ["Y. Lafont"], "venue": "Theoretical Computer Science", "citeRegEx": "Lafont,? \\Q2004\\E", "shortCiteRegEx": "Lafont", "year": 2004}, {"title": "Cognitive Grammar \u2014 A Basic Introduction", "author": ["R. Langacker"], "venue": null, "citeRegEx": "Langacker,? \\Q2008\\E", "shortCiteRegEx": "Langacker", "year": 2008}, {"title": "Higher-order logical inference with compositional semantics, in \u2018Proceedings of EMNLP", "author": ["K. Mineshima", "P. Mart\u0131nez-G\u00f3mez", "Y. Miyao", "D. Bekki"], "venue": null, "citeRegEx": "Mineshima et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mineshima et al\\.", "year": 2015}, {"title": "The proper treatment of quantification in ordinary English, in R. Thomason, ed., \u2018Formal Philosophy", "author": ["R. Montague"], "venue": null, "citeRegEx": "Montague,? \\Q1974\\E", "shortCiteRegEx": "Montague", "year": 1974}, {"title": "Categorial type logics", "author": ["M. Moortgat"], "venue": "Elsevier/MIT Press,", "citeRegEx": "Moortgat,? \\Q1997\\E", "shortCiteRegEx": "Moortgat", "year": 1997}, {"title": "Proof Nets for Linguistic Analysis", "author": ["R. Moot"], "venue": "PhD thesis,", "citeRegEx": "Moot,? \\Q2002\\E", "shortCiteRegEx": "Moot", "year": 2002}, {"title": "Filtering axiom links for proof nets", "author": ["R. Moot"], "venue": null, "citeRegEx": "Moot,? \\Q2007\\E", "shortCiteRegEx": "Moot", "year": 2007}, {"title": "Wide-coverage French syntax and semantics using Grail, in \u2018Proceedings of Traitement Automatique des Langues Naturelles (TALN)", "author": ["R. Moot"], "venue": null, "citeRegEx": "Moot,? \\Q2010\\E", "shortCiteRegEx": "Moot", "year": 2010}, {"title": "Linear one: A theorem prover for first-order linear logic\u2019, https://github.com/RichardMoot/LinearOne", "author": ["R. Moot"], "venue": null, "citeRegEx": "Moot,? \\Q2015\\E", "shortCiteRegEx": "Moot", "year": 2015}, {"title": "Linguistic applications of first order multiplicative linear logic", "author": ["R. Moot", "M. Piazza"], "venue": "Journal of Logic, Language and Information", "citeRegEx": "Moot and Piazza,? \\Q2001\\E", "shortCiteRegEx": "Moot and Piazza", "year": 2001}, {"title": "The Logic of Categorial Grammars: A Deductive Account of Natural Language Syntax and Semantics, Springer", "author": ["R. Moot", "C. Retor\u00e9"], "venue": null, "citeRegEx": "Moot and Retor\u00e9,? \\Q2012\\E", "shortCiteRegEx": "Moot and Retor\u00e9", "year": 2012}, {"title": "Computational coverage of TLG: The Montague test, in \u2018Proceedings", "author": ["G. Morrill", "O. Valent\u0131\u0301n"], "venue": "CSSP 2015 Le onzie\u0300me Colloque de Syntaxe et Se\u0301mantique a\u0300 Paris\u2019,", "citeRegEx": "Morrill and Valent\u0131\u0301n,? \\Q2015\\E", "shortCiteRegEx": "Morrill and Valent\u0131\u0301n", "year": 2015}, {"title": "The displacement calculus", "author": ["G. Morrill", "O. Valent\u0131\u0301n", "M. Fadda"], "venue": "Journal of Logic, Language and Information", "citeRegEx": "Morrill et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Morrill et al\\.", "year": 2011}, {"title": "Lambek grammars are context free, in \u2018Proceedings of Logic in Computer Science", "author": ["M. Pentus"], "venue": null, "citeRegEx": "Pentus,? \\Q1995\\E", "shortCiteRegEx": "Pentus", "year": 1995}, {"title": "A polynomial-time algorithm for Lambek grammars of bounded order", "author": ["M. Pentus"], "venue": "Linguistic Analysis", "citeRegEx": "Pentus,? \\Q2010\\E", "shortCiteRegEx": "Pentus", "year": 2010}, {"title": "The Language Instinct", "author": ["S. Pinker"], "venue": "Penguin Science", "citeRegEx": "Pinker,? \\Q1994\\E", "shortCiteRegEx": "Pinker", "year": 1994}, {"title": "The Montagovian Generative Lexicon", "author": ["C. Retor\u00e9"], "venue": null, "citeRegEx": "Retor\u00e9,? \\Q2014\\E", "shortCiteRegEx": "Retor\u00e9", "year": 2014}, {"title": "Practical experiments in parsing using tree adjoining grammars, in \u2018Proceeding of TAG+ 5", "author": ["A. Sarkar"], "venue": null, "citeRegEx": "Sarkar,? \\Q2000\\E", "shortCiteRegEx": "Sarkar", "year": 2000}, {"title": "Product-free Lambek calculus is NP-complete, in \u2018Symposium on Logical Foundations of Computer Science (LFCS) 2009", "author": ["Y. Savateev"], "venue": null, "citeRegEx": "Savateev,? \\Q2009\\E", "shortCiteRegEx": "Savateev", "year": 2009}, {"title": "Complexity of normalization in the pure typed lambdacalculus, in \u2018The L", "author": ["H. Schwichtenberg"], "venue": "E. J. Brouwer Centenary Symposium\u2019,", "citeRegEx": "Schwichtenberg,? \\Q1982\\E", "shortCiteRegEx": "Schwichtenberg", "year": 1982}, {"title": "Evidence against the context-freeness of natural language", "author": ["S. Shieber"], "venue": "Linguistics & Philosophy", "citeRegEx": "Shieber,? \\Q1985\\E", "shortCiteRegEx": "Shieber", "year": 1985}, {"title": "Computing machinery and intelligence", "author": ["A. Turing"], "venue": "Mind", "citeRegEx": "Turing,? \\Q1950\\E", "shortCiteRegEx": "Turing", "year": 1950}], "referenceMentions": [{"referenceID": 36, "context": "In the well-known Turing test for artificial intelligence, a human interrogator needs to decide, via a question answering session with two terminals, which of his two interlocutors is a man and which is a machine (Turing 1950).", "startOffset": 213, "endOffset": 226}, {"referenceID": 27, "context": "Actually there are two different views of this cognitive and computational view: one view, promoted by authors such as Pinker (1994), claims that there is a specific cognitive function for language, a \u201clanguage module\u201d in the mind, while others, like Langacker (2008), think that our language faculty is just our general cognitive abilities applied to language.", "startOffset": 119, "endOffset": 133}, {"referenceID": 15, "context": "Actually there are two different views of this cognitive and computational view: one view, promoted by authors such as Pinker (1994), claims that there is a specific cognitive function for language, a \u201clanguage module\u201d in the mind, while others, like Langacker (2008), think that our language faculty is just our general cognitive abilities applied to language.", "startOffset": 251, "endOffset": 268}, {"referenceID": 8, "context": "And, as argued by Corblin (2013), if someone is given a few sentences on a sheet of paper without any further information, he starts imagining situations, may infer other statements from what he reads, .", "startOffset": 18, "endOffset": 33}, {"referenceID": 8, "context": "And, as argued by Corblin (2013), if someone is given a few sentences on a sheet of paper without any further information, he starts imagining situations, may infer other statements from what he reads, . . . , and such thoughts are the semantics of the sentence. \u2013 The linguistic tradition initiated by Montague (1974) lacks some coherence regarding computability.", "startOffset": 18, "endOffset": 319}, {"referenceID": 18, "context": "This is not strictly necessary: Montague (1974) used a context free grammar (augmented with a mechanism for quantifier scope), but if one reads between the lines, at some points he converts the phrase structure into a categorial derivation, so we shall, following Moot & Retor\u00e9 (2012), directly use a categorial analysis.", "startOffset": 32, "endOffset": 48}, {"referenceID": 18, "context": "This is not strictly necessary: Montague (1974) used a context free grammar (augmented with a mechanism for quantifier scope), but if one reads between the lines, at some points he converts the phrase structure into a categorial derivation, so we shall, following Moot & Retor\u00e9 (2012), directly use a categorial analysis.", "startOffset": 32, "endOffset": 285}, {"referenceID": 6, "context": "As observed by Church (1940), the simply typed lambda calculus with two types e and t is enough to express higher order logic, provided one introduces constants for the logical connectives and quantifiers, that is a constants \u201c\u2203\u201d and \u201c\u2200\u201d of type (e\u2192 t)\u2192 t, and constants \u201c\u2227\u201d, \u201c\u2228\u201d et \u201c\u21d2\u201d of type t\u2192 (t\u2192 t).", "startOffset": 15, "endOffset": 29}, {"referenceID": 33, "context": "Perhaps surprisingly, the simple product-free version of the Lambek calculus we have used for our examples is already NP-complete (Savateev 2009).", "startOffset": 130, "endOffset": 145}, {"referenceID": 29, "context": "We know that once we bound the order of formulas in the lexicon of our grammars to be less than a fixed n, parsing becomes polynomial for any choice of n (Pentus 2010)6.", "startOffset": 154, "endOffset": 167}, {"referenceID": 33, "context": "The NP-completeness proof of Savateev (2009) uses a reduction from SAT, where a SAT problem with c clauses and v variables produces a Lambek grammar of order 3+4c, with (2c+1)(3v+1) atomic formulas.", "startOffset": 29, "endOffset": 45}, {"referenceID": 28, "context": "Even though the Lambek calculus is a nice and simple system, we know that the Lambek calculus generates only context-free languages (Pentus 1995), and there is good evidence that at least some constructions in natural language require a slightly larger class of languages (Shieber 1985).", "startOffset": 132, "endOffset": 145}, {"referenceID": 35, "context": "Even though the Lambek calculus is a nice and simple system, we know that the Lambek calculus generates only context-free languages (Pentus 1995), and there is good evidence that at least some constructions in natural language require a slightly larger class of languages (Shieber 1985).", "startOffset": 272, "endOffset": 286}, {"referenceID": 13, "context": "One influential proposal for such a larger class of languages are the mildly context-sensitive languages (Joshi 1985), characterised as follows.", "startOffset": 105, "endOffset": 117}, {"referenceID": 28, "context": "6 For the algorithm of Pentus (2010), the order appears as an exponent in the worst-case complexity: for a grammar of order n there is a multiplicative factor of 25(n+1).", "startOffset": 23, "endOffset": 37}, {"referenceID": 19, "context": "Though Moot (2002) shows that multimodal categorial grammars generate exactly the contextsensitive languages, Buszkowski (1997) underlines the difficulty of adapting the result of Pentus (1995) to extensions of the Lambek calculus8.", "startOffset": 7, "endOffset": 19}, {"referenceID": 3, "context": "Though Moot (2002) shows that multimodal categorial grammars generate exactly the contextsensitive languages, Buszkowski (1997) underlines the difficulty of adapting the result of Pentus (1995) to extensions of the Lambek calculus8.", "startOffset": 110, "endOffset": 128}, {"referenceID": 3, "context": "Though Moot (2002) shows that multimodal categorial grammars generate exactly the contextsensitive languages, Buszkowski (1997) underlines the difficulty of adapting the result of Pentus (1995) to extensions of the Lambek calculus8.", "startOffset": 110, "endOffset": 194}, {"referenceID": 18, "context": "Although the standard notion of complexity for categorial grammars is the complexity deciding whether or not a proof exists, formal semanticists, at least since Montague (1974), want their formalisms to generate all and only the correct readings for a sentence: we are not only interested in whether or not a proof exists but, since different natural deduction proofs correspond to different readings, also in what the different proofs of a sentence are9.", "startOffset": 161, "endOffset": 177}, {"referenceID": 19, "context": "Extensions/variations of the Lambek calculus \u2014 which include multimodal categorial grammars (Moortgat 1997), the Displacement calculus (Morrill et al.", "startOffset": 92, "endOffset": 107}, {"referenceID": 27, "context": "Extensions/variations of the Lambek calculus \u2014 which include multimodal categorial grammars (Moortgat 1997), the Displacement calculus (Morrill et al. 2011) and firstorder linear logic (Moot & Piazza 2001) \u2014 solve both the problems of formal language theory and the problems of the syntax-semantics interface.", "startOffset": 135, "endOffset": 156}, {"referenceID": 4, "context": "Carpenter (1994) gives many examples of the advantages of this logical approach to scope, notably its interaction with other semantic phenomena like negation and coordination.", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "Though these modern calculi solve the problems with the Lambek calculus, they do so without excessively increasing the computational complexity of the formalism: multimodal categorial grammars are PSPACE complete (Moot 2002), whereas most other extensions are NP-complete, like the Lambek calculus.", "startOffset": 213, "endOffset": 224}, {"referenceID": 7, "context": "Cooper storage, (Cooper 1975)), or 2) an underspecification mechanism for representing quantifier scope (Fox & Lappin 2010).", "startOffset": 16, "endOffset": 29}, {"referenceID": 7, "context": "For case 1 (Cooper 1975), a single syntactic structure is converted into up to n! semantic readings, whereas for case 2, though we represent all possible readings in a single structure, even deciding whether the given sentence has a semantic reading at all becomes NP-complete (Fox & Lappin 2010), hence we simply shift the NP-completeness from the syntax to the syntax-semantics interface11.", "startOffset": 11, "endOffset": 24}, {"referenceID": 9, "context": "11 In addition, Ebert (2005) argues that underspecification languages are not expressive enough", "startOffset": 16, "endOffset": 29}, {"referenceID": 21, "context": "It is rather easy to set up a categorial grammar parser in such a way that it produces underspecified representations in time proportional to n2 (Moot 2007).", "startOffset": 145, "endOffset": 156}, {"referenceID": 4, "context": "We believe, following Carpenter (1994) and Jacobson (2002), that giving an integrated account of the various aspects of the syntax-semantics interface is the most promising path.", "startOffset": 22, "endOffset": 39}, {"referenceID": 4, "context": "We believe, following Carpenter (1994) and Jacobson (2002), that giving an integrated account of the various aspects of the syntax-semantics interface is the most promising path.", "startOffset": 22, "endOffset": 59}, {"referenceID": 34, "context": "Even in the standard, simply typed Montagovian framework, normalizing lambda terms is known to be of non-elementary complexity (Schwichtenberg 1982), essentially due to the possibility of recursive copying.", "startOffset": 127, "endOffset": 148}, {"referenceID": 15, "context": "However, the system of Lafont (2004) includes second-order quantifiers hence reduction stays polynomial once coercions have been chosen.", "startOffset": 23, "endOffset": 37}, {"referenceID": 6, "context": "\u2013 the process he defined stops with a normal lambda terms of type proposition (t), \u2013 eta-long normal lambda terms with constants being either logical connectives or constants of a first (or higher order) logical language are in bijective correspondence with formulas of this logical language (this is more or less clear in the work of Church (1940) on simple type theory).", "startOffset": 335, "endOffset": 349}, {"referenceID": 34, "context": "\u2013 the complexity of the whole process has a known complexity class, in particular the beta-reduction steps which was only discovered years after his death (Schwichtenberg 1982).", "startOffset": 155, "endOffset": 176}], "year": 2016, "abstractText": "This paper is a reflexion on the computability of natural language semantics. It does not contain a new model or new results in the formal semantics of natural language: it is rather a computational analysis of the logical models and algorithms currently used in natural language semantics, defined as the mapping of a statement to logical formulas \u2014 formulas, because a statement can be ambiguous. We argue that as long as possible world semantics is left out, one can compute the semantic representation(s) of a given statement, including aspects of lexical meaning. We also discuss the algorithmic complexity of this process.", "creator": "TeX"}}}