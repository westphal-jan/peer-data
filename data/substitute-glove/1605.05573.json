{"id": "1605.05573", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2016", "title": "Modelling Interaction of Sentence Pair with Coupled-LSTMs", "abstract": "Recently, there comes weakening interest 2003 analysing same differentiation of more sentences with deep biochemical networks. However, among also the regulations technique encode few thematic with additional encoders, where following a bail still encoded with feel do neither information from the frequently judgment. In this collected, see postpone takes deep architecture either model still also involves other sentence other a one coupled - LSTMs. Specifically, we introduce 16 reflects ways this instance while interdependences result included LSTMs, equivalence long local contextualized interactions of along sentences. We then 17-12 these inhibit and use making linear mashups meant eligible is generally informative numerous. Experiments on. very large caching undoubtedly the crestor particular you considering italianate and its rationale keep where - the - the - art numerical.", "histories": [["v1", "Wed, 18 May 2016 13:33:21 GMT  (344kb,D)", "https://arxiv.org/abs/1605.05573v1", null], ["v2", "Fri, 20 May 2016 01:28:43 GMT  (317kb,D)", "http://arxiv.org/abs/1605.05573v2", "Submitted to IJCAI 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pengfei liu", "xipeng qiu", "yaqian zhou", "jifan chen", "xuanjing huang"], "accepted": true, "id": "1605.05573"}, "pdf": {"name": "1605.05573.pdf", "metadata": {"source": "CRF", "title": "Modelling Interaction of Sentence Pair with Coupled-LSTMs", "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "emails": ["pfliu14@fudan.edu.cn", "xpqiu@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification [Kalchbrenner et al., 2014], question answering and machine translation [Sutskever et al., 2014] and so on. Among these tasks, a common problem is modelling the relevance/similarity of the sentence pair, which is also called text semantic matching.\nRecently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses [Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016].\nAccording to the phases of interaction between two sentences, previous models can be classified into three categories.\nWeak interaction Models Some early works focus on sentence level interactions, such as ARC-I[Hu et al., 2014], CNTN[Qiu and Huang, 2015] and so on. These models first encode two sequences with some basic (Neural Bagof-words, BOW) or advanced (RNN, CNN) components of neural networks separately, and then compute the matching\n\u2217Corresponding author.\nscore based on the distributed vectors of two sentences. In this paradigm, two sentences have no interaction until arriving final phase.\nSemi-interaction Models Some improved methods focus on utilizing multi-granularity representation (word, phrase and sentence level), such as MultiGranCNN [Yin and Schu\u0308tze, 2015] and Multi-Perspective CNN [He et al., 2015]. Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN [Yin et al., 2015], Attention LSTM[Rockta\u0308schel et al., 2015; Hermann et al., 2015]. These models can alleviate the weak interaction problem, but are still insufficient to model the contextualized interaction on the word as well as phrase level.\nStrong Interaction Models These models directly build an interaction space between two sentences and model the interaction at different positions. ARC-II [Hu et al., 2014] and MV-LSTM [Wan et al., 2016]. These models enable the model to easily capture the difference between semantic capacity of two sentences.\nIn this paper, we propose a new deep neural network architecture to model the strong interactions of two sentences. Different with modelling two sentences with separated LSTMs, we utilize two interdependent LSTMs, called coupled-LSTMs, to fully affect each other at different time steps. The output of coupled-LSTMs at each step depends on both sentences. Specifically, we propose two interdependent ways for the coupled-LSTMs: loosely coupled model (LCLSTMs) and tightly coupled model (TC-LSTMs). Similar to bidirectional LSTM for single sentence [Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005], there are four directions can be used in coupled-LSTMs. To utilize all the information of four directions of coupled-LSTMs, we aggregate them and adopt a dynamic pooling strategy to automatically select the most informative interaction signals. Finally, we feed them into a fully connected layer, followed by an output layer to compute the matching score.\nThe contributions of this paper can be summarized as follows.\n1. Different with the architectures of using similarity matrix, our proposed architecture directly model the strong interactions of two sentences with coupled-LSTMs, which can capture the useful local semantic relevances\nar X\niv :1\n60 5.\n05 57\n3v 2\n[ cs\n.C L\n] 2\n0 M\nay 2\n01 6\nof two sentences. Our architecture can also capture the multiple granular interactions by several stacked coupled-LSTMs layers.\n2. Compared to the previous works on text matching, we perform extensive empirical studies on two very large datasets. The massive scale of the datasets allows us to train a very deep neural networks. Experiment results demonstrate that our proposed architecture is more effective than state-of-the-art methods."}, {"heading": "2 Sentence Modelling with LSTM", "text": "Long short-term memory network (LSTM) [Hochreiter and Schmidhuber, 1997] is a type of recurrent neural network (RNN) [Elman, 1990], and specifically addresses the issue of learning long-term dependencies. LSTM maintains a memory cell that updates and exposes its content only when deemed necessary.\nWhile there are numerous LSTM variants, here we use the LSTM architecture used by [Jozefowicz et al., 2015], which is similar to the architecture of [Graves, 2013] but without peep-hole connections.\nWe define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units. The elements of the gating vectors it, ft and ot are in [0, 1].\nThe LSTM is precisely specified as follows.\n  c\u0303t ot it ft   =   tanh \u03c3 \u03c3 \u03c3  TA,b [ xt ht\u22121 ] , (1)\nct = c\u0303t it + ct\u22121 ft, (2) ht = ot tanh (ct) , (3)\nwhere xt is the input at the current time step; TA,b is an affine transformation which depends on parameters of the network A and b. \u03c3 denotes the logistic sigmoid function and denotes elementwise multiplication. Intuitively, the forget gate controls the amount of which each unit of the memory cell is erased, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state.\nThe update of each LSTM unit can be written precisely as follows\n(ht, ct) = LSTM(ht\u22121, ct\u22121,xt). (4)\nHere, the function LSTM(\u00b7, \u00b7, \u00b7) is a shorthand for Eq. (1-3)."}, {"heading": "3 Coupled-LSTMs for Strong Sentence Interaction", "text": "To deal with two sentences, one straightforward method is to model them with two separate LSTMs. However, this method is difficult to model local interactions of two sentences. An improved way is to introduce attention mechanism, which has\nbeen used in many tasks, such as machine translation [Bahdanau et al., 2014] and question answering [Hermann et al., 2015].\nInspired by the multi-dimensional recurrent neural network [Graves et al., 2007; Graves and Schmidhuber, 2009; Byeon et al., 2015] and grid LSTM [Kalchbrenner et al., 2015] in computer vision community, we propose two models to capture the interdependences between two parallel LSTMs, called coupled-LSTMs (C-LSTMs).\nTo facilitate our models, we firstly give some definitions. Given two sequences X = x1, x2, \u00b7 \u00b7 \u00b7 , xn and Y = y1, y2, \u00b7 \u00b7 \u00b7 , ym, we let xi \u2208 Rd denote the embedded representation of the word xi. The standard LSTM have one temporal dimension. When dealing with a sentence, LSTM regards the position as time step. At position i of sentence x1:n, the output hi reflects the meaning of subsequence x0:i = x0, \u00b7 \u00b7 \u00b7 , xi.\nTo model the interaction of two sentences as early as possible, we define hi,j to represent the interaction of the subsequences x0:i and y0:j .\nFigure 1(c) and 1(d) illustrate our two propose models. For intuitive comparison of weak interaction parallel LSTMs, we also give parallel LSTMs and attention LSTMs in Figure 1(a) and 1(b).\nWe describe our two proposed models as follows."}, {"heading": "3.1 Loosely Coupled-LSTMs (LC-LSTMs)", "text": "To model the local contextual interactions of two sentences, we enable two LSTMs to be interdependent at different positions. Inspired by Grid LSTM [Kalchbrenner et al., 2015] and word-by-word attention LSTMs [Rockta\u0308schel et al., 2015], we propose a loosely coupling model for two interdependent LSTMs.\nMore concretely, we refer to h(1)i,j as the encoding of subsequence x0:i in the first LSTM influenced by the output of the second LSTM on subsequence y0:j . Meanwhile, h (2) i,j is the\nencoding of subsequence y0:j in the second LSTM influenced by the output of the first LSTM on subsequence x0:i h (1) i,j and h (2) i,j are computed as\nh (1) i,j = LSTM 1(H (1) i\u22121, c (1) i\u22121,j ,xi), (5)\nh (2) i,j = LSTM 2(H (2) j\u22121, c (2) i,j\u22121,yj), (6)\nwhere H\n(1) i\u22121 = [h (1) i\u22121,j ,h (2) i\u22121,j ], (7)\nH (2) j\u22121 = [h (1) i,j\u22121,h (2) i,j\u22121]. (8)"}, {"heading": "3.2 Tightly Coupled-LSTMs (TC-LSTMs)", "text": "The hidden states of LC-LSTMs are the combination of the hidden states of two interdependent LSTMs, whose memory cells are separated. Inspired by the configuration of the multidimensional LSTM [Byeon et al., 2015], we further conflate both the hidden states and the memory cells of two LSTMs. We assume that hi,j directly model the interaction of the subsequences x0:i and y0:j , which depends on two previous interaction hi\u22121,j and hi,j\u22121, where i, j are the positions in sentence X and Y .\nWe define a tightly coupled-LSTMs units as follows.\n  c\u0303i,j oi,j ii,j f1i,j f2i,j   =   tanh \u03c3 \u03c3 \u03c3 \u03c3  TA,b   xi yj hi,j\u22121 hi\u22121,j   , (9)\nci,j = c\u0303i,j ii,j + [ci,j\u22121, ci\u22121,j ]T [ f1i,j f2i,j ] (10)\nhi,j = ot tanh (ci,j) (11) where the gating units ii,j and oi,j determine which memory units are affected by the inputs through c\u0303i,j , and which memory cells are written to the hidden units hi,j . TA,b is an affine transformation which depends on parameters of the network A and b. In contrast to the standard LSTM defined over time, each memory unit ci,j of a tightly coupled-LSTMs has two preceding states ci,j\u22121 and ci\u22121,j and two corresponding forget gates f1i,j and f 2 i,j ."}, {"heading": "3.3 Analysis of Two Proposed Models", "text": "Our two proposed coupled-LSTMs can be formulated as (hi,j , ci,j) = C-LSTMs(hi\u22121,j ,hi,j\u22121, ci\u22121,j , ci,j\u22121,xi,yj),\n(12) where C-LSTMs can be either TC-LSTMs or LC-LSTMs.\nThe input consisted of two type of information at step (i, j) in coupled-LSTMs: temporal dimension hi\u22121,j ,hi,j\u22121, ci\u22121,j , ci,j\u22121 and depth dimension xi,yj . The difference between TC-LSTMs and LC-LSTMs is the dependence of information from temporal and depth dimension.\nInteraction Between Temporal Dimensions The TCLSTMs model the interactions at position (i, j) by merging the internal memory ci\u22121,j ci,j\u22121 and hidden state hi\u22121,j hi,j\u22121 along row and column dimensions. In contrast with TC-LSTMs, LC-LSTMs firstly use two standard LSTMs in parallel, producing hidden states h1i,j and h 2 i,j along row and column dimensions respectively, which are then merged together flowing next step.\nInteraction Between Depth Dimension In TC-LSTMs, each hidden state hi,j at higher layer receives a fusion of information xi and yj , flowed from lower layer. However, in LC-LSTMs, the information xi and yj are accepted by two corresponding LSTMs at the higher layer separately.\nThe two architectures have their own characteristics, TC-LSTMs give more strong interactions among different dimensions while LCLSTMs ensures the two sequences interact closely without being conflated using two separated LSTMs.\nComparison of LC-LSTMs and word-by-word Attention LSTMs The main idea of attention LSTMs is that the representation of sentence X is obtained dynamically based on the alignment degree between the words in sentence X and Y, which is asymmetric unidirectional encoding. Nevertheless, in LC-LSTM, each hidden state of each step is obtained with the consideration of interaction between two sequences with symmetrical encoding fashion."}, {"heading": "4 End-to-End Architecture for Sentence Matching", "text": "In this section, we present an end-to-end deep architecture for matching two sentences, as shown in Figure 2."}, {"heading": "4.1 Embedding Layer", "text": "To model the sentences with neural model, we firstly need transform the one-hot representation of word into the distributed representation. All words of two sequences X = x1, x2, \u00b7 \u00b7 \u00b7 , xn and Y = y1, y2, \u00b7 \u00b7 \u00b7 , ym will be mapped into low dimensional vector representations, which are taken as input of the network."}, {"heading": "4.2 Stacked Coupled-LSTMs Layers", "text": "After the embedding layer, we use our proposed coupled-LSTMs to capture the strong interactions between two sentences. A basic block consists of five layers. We firstly use four directional coupledLSTMs to model the local interactions with different information flows. And then we sum the outputs of these LSTMs by aggregation layer. To increase the learning capabilities of the coupled-LSTMs, we stack the basic block on top of each other.\nFour Directional Coupled-LSTMs Layers The C-LSTMs is defined along a certain pre-defined direction, we can extend them to access to the surrounding context in all directions. Similar to bi-directional LSTM, there are four directions in coupled-LSTMs.\n(h1i,j , c 1 i,j) = C-LSTMs(hi\u22121,j ,hi,j\u22121, ci\u22121,j , ci,j\u22121,xi,yj),\n(h2i,j , c 2 i,j) = C-LSTMs(hi\u22121,j ,hi,j+1, ci\u22121,j , ci,j+1,xi,yj),\n(h3i,j , c 3 i,j) = C-LSTMs(hi+1,j ,hi,j+1, ci+1,j , ci,j+1,xi,yj),\n(h4i,j , c 4 i,j) = C-LSTMs(hi+1,j ,hi,j\u22121, ci+1,j , ci,j\u22121,xi,yj).\nAggregation Layer The aggregation layer sums the outputs of four directional coupledLSTMs into a vector.\nh\u0302i,j = 4\u2211 d=1 hdi,j , (13)\nwhere the superscript t of hi,j denotes the different directions.\nStacking C-LSTMs Blocks To increase the capabilities of network of learning multiple granularities of interactions, we stack several blocks (four C-LSTMs layers and one aggregation layer) to form deep architectures."}, {"heading": "4.3 Pooling Layer", "text": "The output of stacked coupled-LSTMs layers is a tensor H \u2208 Rn\u00d7m\u00d7d, where n and m are the lengths of sentences, and d is the number of hidden neurons. We apply dynamic pooling to automatically extract Rp\u00d7q subsampling matrix in each slice Hi \u2208 Rn\u00d7m, similar to [Socher et al., 2011].\nMore formally, for each slice matrix Hi, we partition the rows and columns of Hi into p \u00d7 q roughly equal grids. These grid are non-overlapping. Then we select the maximum value within each grid. Since each slice Hi consists of the hidden states of one neuron at different positions, the pooling operation can be regarded as the most informative interactions captured by the neuron.\nThus, we get a p\u00d7 q \u00d7 d tensor, which is further reshaped into a vector."}, {"heading": "4.4 Fully-Connected Layer", "text": "The vector obtained by pooling layer is fed into a full connection layer to obtain a final more abstractive representation."}, {"heading": "4.5 Output Layer", "text": "The output layer depends on the types of the tasks, we choose the corresponding form of output layer. There are two popular types of text matching tasks in NLP. One is ranking task, such as community question answering. Another is classification task, such as textual entailment.\n1. For ranking task, the output is a scalar matching score, which is obtained by a linear transformation after the last fullyconnected layer.\n2. For classification task, the outputs are the probabilities of the different classes, which is computed by a softmax function after the last fully-connected layer."}, {"heading": "5 Training", "text": "Our proposed architecture can deal with different sentence matching tasks. The loss functions varies with different tasks.\nMax-Margin Loss for Ranking Task Given a positive sentence pair (X,Y ) and its corresponding negative pair (X, Y\u0302 ). The matching score s(X,Y ) should be larger than s(X, Y\u0302 ).\nFor this task, we use the contrastive max-margin criterion [Bordes et al., 2013; Socher et al., 2013] to train our models on matching task.\nThe ranking-based loss is defined as\nL(X,Y, Y\u0302 ) = max(0, 1\u2212 s(X,Y ) + s(X, Y\u0302 )). (14)\nwhere s(X,Y ) is predicted matching score for (X,Y ).\nCross-entropy Loss for Classification Task Given a sentence pair (X,Y ) and its label l. The output l\u0302 of neural network is the probabilities of the different classes. The parameters of the network are trained to minimise the cross-entropy of the predicted and true label distributions.\nL(X,Y ; l, l\u0302) = \u2212 C\u2211\nj=1\nlj log(\u0302lj), (15)\nwhere l is one-hot representation of the ground-truth label l; l\u0302 is predicted probabilities of labels; C is the class number.\nTo minimize the objective, we use stochastic gradient descent with the diagonal variant of AdaGrad [Duchi et al., 2011]. To prevent exploding gradients, we perform gradient clipping by scaling the gradient when the norm exceeds a threshold [Graves, 2013]."}, {"heading": "6 Experiment", "text": "In this section, we investigate the empirical performances of our proposed model on two different text matching tasks: classification task (recognizing textual entailment) and ranking task (matching of question and answer)."}, {"heading": "6.1 Hyperparameters and Training", "text": "The word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, [Pennington et al., 2014]) and fine-tuned during training to improve the performance. The other parameters are initialized by randomly sampling from uniform distribution in [\u22120.1, 0.1].\nFor each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.05, 0.0005, 0.0001], l2 regularization [0.0, 5E\u22125, 1E\u22125, 1E\u22126] and the threshold value\nof gradient norm [5, 10, 100]. The final hyper-parameters are set as Table 1."}, {"heading": "6.2 Competitor Methods", "text": "\u2022 Neural bag-of-words (NBOW): Each sequence as the sum of\nthe embeddings of the words it contains, then they are concatenated and fed to a MLP.\n\u2022 Single LSTM: A single LSTM to encode the two sequences, which is used in [Rockta\u0308schel et al., 2015].\n\u2022 Parallel LSTMs: Two sequences are encoded by two LSTMs separately, then they are concatenated and fed to a MLP.\n\u2022 Attention LSTMs: An attentive LSTM to encode two sentences into a semantic space, which used in [Rockta\u0308schel et al., 2015].\n\u2022 Word-by-word Attention LSTMs: An improvement of attention LSTM by introducing word-by-word attention mechanism, which used in [Rockta\u0308schel et al., 2015]."}, {"heading": "6.3 Experiment-I: Recognizing Textual Entailment", "text": "Recognizing textual entailment (RTE) is a task to determine the semantic relationship between two sentences. We use the Stanford Natural Language Inference Corpus (SNLI) [Bowman et al., 2015]. This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human annotators. SNLI is two orders of magnitude larger than all other existing RTE corpora. Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper.\nResults Table 2 shows the evaluation results on SNLI. The 3rd column of the table gives the number of parameters of different models without the word embeddings.\nOur proposed two C-LSTMs models with four stacked blocks outperform all the competitor models, which indicates that our thinner and deeper network does work effectively.\nBesides, we can see both LC-LSTMs and TC-LSTMs benefit from multi-directional layer, while the latter obtains more gains than the former. We attribute this discrepancy between two models to their different mechanisms of controlling the information flow from depth dimension.\nCompared with attention LSTMs, our two models achieve comparable results to them using much fewer parameters (nearly 1/5).\nBy stacking C-LSTMs, the performance of them are improved significantly, and the four stacked TC-LSTMs achieve 85.1% accuracy on this dataset.\nMoreover, we can see TC-LSTMs achieve better performance than LC-LSTMs on this task, which need fine-grained reasoning over pairs of words as well as phrases.\nUnderstanding Behaviors of Neurons in C-LSTMs To get an intuitive understanding of how the C-LSTMs work on this problem, we examined the neuron activations in the last aggregation layer while evaluating the test set using TC-LSTMs. We find that some cells are bound to certain roles.\nLet hi,j,k denotes the activation of the k-th neuron at the position of (i, j), where i \u2208 {1, . . . , n} and j \u2208 {1, . . . ,m}. By visualizing the hidden state hi,j,k and analyzing the maximum activation, we can find that there exist multiple interpretable neurons. For example, when some contextualized local perspectives are semantically related at point (i, j) of the sentence pair, the activation value of hidden neuron hi,j,k tend to be maximum, meaning that the model could capture some reasoning patterns.\nFigure 3 illustrates this phenomenon. In Figure 3(a), a neuron shows its ability to monitor the local contextual interactions about color. The activation in the patch, including the word pair \u201c(red, green)\u201d, is much higher than others. This is informative pattern for the relation prediction of these two sentences, whose ground truth is contradiction. An interesting thing is there are two words describing color in the sentence \u201c A person in a red shirt and black pants hunched over.\u201d. Our model ignores the useless word \u201cblack\u201d, which indicates that this neuron selectively captures pattern by contextual understanding, not just word level interaction.\nIn Figure 3(b), another neuron shows that it can capture the local contextual interactions, such as \u201c(walking down the street, outside)\u201d. These patterns can be easily captured by pooling layer and provide a strong support for the final prediction.\nTable 3 illustrates multiple interpretable neurons and some representative word or phrase pairs which can activate these neurons. These cases show that our models can capture contextual interactions beyond word level.\nError Analysis Although our models C-LSTMs are more sensitive to the discrepancy of the semantic capacity between two sentences, some\nsemantic mistakes at the phrasal level still exist. For example, our models failed to capture the key informative pattern when predicting the entailment sentence pair \u201cA girl takes off her shoes and eats blue cotton candy/The girl is eating while barefoot.\u201d\nBesides, despite the large size of the training corpus, it\u2019s still very different to solve some cases, which depend on the combination of the world knowledge and context-sensitive inferences. For example, given an entailment pair \u201ca man grabs his crotch during a political demonstration/The man is making a crude gesture\u201d, all models predict \u201cneutral\u201d. This analysis suggests that some architectural improvements or external world knowledge are necessary to eliminate all errors instead of simply scaling up the basic model."}, {"heading": "6.4 Experiment-II: Matching Question and Answer", "text": "Matching question answering (MQA) is a typical task for semantic matching. Given a question, we need select a correct answer from some candidate answers.\nIn this paper, we use the dataset collected from Yahoo! Answers with the getByCategory function provided in Yahoo! Answers API, which produces 963, 072 questions and corresponding best answers. We then select the pairs in which the length of questions and answers are both in the interval [4, 30], thus obtaining 220, 000 question answer pairs to form the positive pairs.\nFor negative pairs, we first use each question\u2019s best answer as a query to retrieval top 1, 000 results from the whole answer set with Lucene, where 4 or 9 answers will be selected randomly to construct the negative pairs.\nThe whole dataset is divided into training, validation and testing data with proportion 20 : 1 : 1. Moreover, we give two test settings: selecting the best answer from 5 and 10 candidates respectively.\nResults Results of MQA are shown in the Table 4. For our models, due to stacking block more than three layers can not make significant improvements on this task, we just use three stacked C-LSTMs.\nBy analyzing the evaluation results of question-answer matching in table 4, we can see strong interaction models (attention LSTMs, our C-LSTMs) consistently outperform the weak interaction models (NBOW, parallel LSTMs) with a large margin, which suggests the importance of modelling strong interaction of two sentences.\nOur proposed two C-LSTMs surpass the competitor methods and C-LSTMs augmented with multi-directions layers and multiple stacked blocks fully utilize multiple levels of abstraction to directly boost the performance.\nAdditionally, LC-LSTMs is superior to TC-LSTMs. The reason may be that MQA is a relative simple task, which requires less reasoning abilities, compared with RTE task. Moreover, the parameters of LC-LSTMs are less than TC-LSTMs, which ensures the former can avoid suffering from overfitting on a relatively smaller corpus."}, {"heading": "7 Related Work", "text": "Our architecture for sentence pair encoding can be regarded as strong interaction models, which have been explored in previous models.\nAn intuitive paradigm is to compute similarities between all the words or phrases of the two sentences. Socher et al. [2011] firstly used this paradigm for paraphrase detection. The representations of words or phrases are learned based on recursive autoencoders. Wan et al. [2016] used LSTM to enhance the positional contextual interactions of the words or phrases between two sentences. The input of LSTM for one sentence does not involve another sentence.\nA major limitation of this paradigm is the interaction of two sentence is captured by a pre-defined similarity measure. Thus, it is not easy to increase the depth of the network. Compared with this paradigm, we can stack our C-LSTMs to model multiple-granularity interactions of two sentences.\nRockta\u0308schel et al. [2015] used two LSTMs equipped with attention mechanism to capture the iteration between two sentences. This architecture is asymmetrical for two sentences, where the obtained final representation is sensitive to the two sentences\u2019 order.\nCompared with the attentive LSTM, our proposed C-LSTMs are symmetrical and model the local contextual interaction of two sequences directly."}, {"heading": "8 Conclusion and Future Work", "text": "In this paper, we propose an end-to-end deep architecture to capture the strong interaction information of sentence pair. Experiments on two large scale text matching tasks demonstrate the efficacy of our proposed model and its superiority to competitor models. Besides, our visualization analysis revealed that multiple interpretable neurons in our proposed models can capture the contextual interactions of the words or phrases.\nIn future work, we would like to incorporate some gating strategies into the depth dimension of our proposed models, like highway or residual network, to enhance the interactions between depth and other dimensions thus training more deep and powerful neural networks."}], "references": [{"title": "ArXiv e-prints", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio. Neural machine translation by jointly learning to align", "translate"], "venue": "September", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "NIPS,", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "author": ["Wonmin Byeon", "Thomas M Breuel", "Federico Raue", "Marcus Liwicki. Scene labeling with lstm recurrent neural networks"], "venue": "pages 3547\u20133555,", "citeRegEx": "Byeon et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "The Journal of Machine Learning Research", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization"], "venue": "12:2121\u20132159,", "citeRegEx": "Duchi et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Cognitive science", "author": ["Jeffrey L Elman. Finding structure in time"], "venue": "14(2):179\u2013211,", "citeRegEx": "Elman. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Neural Networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber. Framewise phoneme classification with bidirectional lstm", "other neural network architectures"], "venue": "18(5):602\u2013610,", "citeRegEx": "Graves and Schmidhuber. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural networks"], "venue": "pages 545\u2013552,", "citeRegEx": "Graves and Schmidhuber. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Artificial Neural Networks\u2013ICANN 2007", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "J\u00fcrgen Schmidhuber. Multi-dimensional recurrent neural networks"], "venue": "pages 549\u2013558. Springer,", "citeRegEx": "Graves et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "author": ["Hua He", "Kevin Gimpel", "Jimmy Lin. Multiperspective sentence similarity modeling with convolutional neural networks"], "venue": "pages 1576\u20131586,", "citeRegEx": "He et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom. Teaching machines to read", "comprehend"], "venue": "pages 1684\u20131692,", "citeRegEx": "Hermann et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Hu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Jozefowicz et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of ACL,", "citeRegEx": "Kalchbrenner et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural tensor network architecture for community-based question answering", "author": ["Xipeng Qiu", "Xuanjing Huang"], "venue": "Proceedings of International Joint Conference on Artificial Intelligence,", "citeRegEx": "Qiu and Huang. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "Rockt\u00e4schel et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on", "author": ["Mike Schuster", "Kuldip K Paliwal. Bidirectional recurrent neural networks. Signal Processing"], "venue": "45(11):2673\u20132681,", "citeRegEx": "Schuster and Paliwal. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le. Sequence to sequence learning with neural networks"], "venue": "pages 3104\u20133112,", "citeRegEx": "Sutskever et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep architecture for semantic matching with multiple positional sentence representations", "author": ["Shengxian Wan", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Liang Pang", "Xueqi Cheng"], "venue": "AAAI,", "citeRegEx": "Wan et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze. Convolutional neural network for paraphrase identification"], "venue": "pages 901\u2013911,", "citeRegEx": "Yin and Sch\u00fctze. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1512.05193,", "citeRegEx": "Yin et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification [Kalchbrenner et al., 2014], question answering and machine translation [Sutskever et al.", "startOffset": 149, "endOffset": 176}, {"referenceID": 23, "context": ", 2014], question answering and machine translation [Sutskever et al., 2014] and so on.", "startOffset": 52, "endOffset": 76}, {"referenceID": 13, "context": "Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses [Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016].", "startOffset": 136, "endOffset": 192}, {"referenceID": 18, "context": "Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses [Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016].", "startOffset": 136, "endOffset": 192}, {"referenceID": 24, "context": "Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses [Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016].", "startOffset": 136, "endOffset": 192}, {"referenceID": 13, "context": "Weak interaction Models Some early works focus on sentence level interactions, such as ARC-I[Hu et al., 2014], CNTN[Qiu and Huang, 2015] and so on.", "startOffset": 92, "endOffset": 109}, {"referenceID": 18, "context": ", 2014], CNTN[Qiu and Huang, 2015] and so on.", "startOffset": 13, "endOffset": 34}, {"referenceID": 25, "context": "Semi-interaction Models Some improved methods focus on utilizing multi-granularity representation (word, phrase and sentence level), such as MultiGranCNN [Yin and Sch\u00fctze, 2015] and Multi-Perspective CNN [He et al.", "startOffset": 154, "endOffset": 177}, {"referenceID": 10, "context": "Semi-interaction Models Some improved methods focus on utilizing multi-granularity representation (word, phrase and sentence level), such as MultiGranCNN [Yin and Sch\u00fctze, 2015] and Multi-Perspective CNN [He et al., 2015].", "startOffset": 204, "endOffset": 221}, {"referenceID": 26, "context": "Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN [Yin et al., 2015], Attention LSTM[Rockt\u00e4schel et al.", "startOffset": 163, "endOffset": 181}, {"referenceID": 19, "context": ", 2015], Attention LSTM[Rockt\u00e4schel et al., 2015; Hermann et al., 2015].", "startOffset": 23, "endOffset": 71}, {"referenceID": 11, "context": ", 2015], Attention LSTM[Rockt\u00e4schel et al., 2015; Hermann et al., 2015].", "startOffset": 23, "endOffset": 71}, {"referenceID": 13, "context": "ARC-II [Hu et al., 2014] and MV-LSTM [Wan et al.", "startOffset": 7, "endOffset": 24}, {"referenceID": 24, "context": ", 2014] and MV-LSTM [Wan et al., 2016].", "startOffset": 20, "endOffset": 38}, {"referenceID": 20, "context": "Similar to bidirectional LSTM for single sentence [Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005], there are four directions can be used in coupled-LSTMs.", "startOffset": 50, "endOffset": 108}, {"referenceID": 6, "context": "Similar to bidirectional LSTM for single sentence [Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005], there are four directions can be used in coupled-LSTMs.", "startOffset": 50, "endOffset": 108}, {"referenceID": 12, "context": "Long short-term memory network (LSTM) [Hochreiter and Schmidhuber, 1997] is a type of recurrent neural network (RNN) [Elman, 1990], and specifically addresses the issue of learning long-term dependencies.", "startOffset": 38, "endOffset": 72}, {"referenceID": 5, "context": "Long short-term memory network (LSTM) [Hochreiter and Schmidhuber, 1997] is a type of recurrent neural network (RNN) [Elman, 1990], and specifically addresses the issue of learning long-term dependencies.", "startOffset": 117, "endOffset": 130}, {"referenceID": 14, "context": "While there are numerous LSTM variants, here we use the LSTM architecture used by [Jozefowicz et al., 2015], which is similar to the architecture of [Graves, 2013] but without peep-hole connections.", "startOffset": 82, "endOffset": 107}, {"referenceID": 9, "context": ", 2015], which is similar to the architecture of [Graves, 2013] but without peep-hole connections.", "startOffset": 49, "endOffset": 63}, {"referenceID": 0, "context": "been used in many tasks, such as machine translation [Bahdanau et al., 2014] and question answering [Hermann et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 11, "context": ", 2014] and question answering [Hermann et al., 2015].", "startOffset": 31, "endOffset": 53}, {"referenceID": 8, "context": "Inspired by the multi-dimensional recurrent neural network [Graves et al., 2007; Graves and Schmidhuber, 2009; Byeon et al., 2015] and grid LSTM [Kalchbrenner et al.", "startOffset": 59, "endOffset": 130}, {"referenceID": 7, "context": "Inspired by the multi-dimensional recurrent neural network [Graves et al., 2007; Graves and Schmidhuber, 2009; Byeon et al., 2015] and grid LSTM [Kalchbrenner et al.", "startOffset": 59, "endOffset": 130}, {"referenceID": 3, "context": "Inspired by the multi-dimensional recurrent neural network [Graves et al., 2007; Graves and Schmidhuber, 2009; Byeon et al., 2015] and grid LSTM [Kalchbrenner et al.", "startOffset": 59, "endOffset": 130}, {"referenceID": 16, "context": ", 2015] and grid LSTM [Kalchbrenner et al., 2015] in computer vision community, we propose two models to capture the interdependences between two parallel LSTMs, called coupled-LSTMs (C-LSTMs).", "startOffset": 22, "endOffset": 49}, {"referenceID": 16, "context": "Inspired by Grid LSTM [Kalchbrenner et al., 2015] and word-by-word attention LSTMs [Rockt\u00e4schel et al.", "startOffset": 22, "endOffset": 49}, {"referenceID": 19, "context": ", 2015] and word-by-word attention LSTMs [Rockt\u00e4schel et al., 2015], we propose a loosely coupling model for two interdependent LSTMs.", "startOffset": 41, "endOffset": 67}, {"referenceID": 3, "context": "Inspired by the configuration of the multidimensional LSTM [Byeon et al., 2015], we further conflate both the hidden states and the memory cells of two LSTMs.", "startOffset": 59, "endOffset": 79}, {"referenceID": 21, "context": "We apply dynamic pooling to automatically extract Rp\u00d7q subsampling matrix in each slice Hi \u2208 Rn\u00d7m, similar to [Socher et al., 2011].", "startOffset": 110, "endOffset": 131}, {"referenceID": 1, "context": "For this task, we use the contrastive max-margin criterion [Bordes et al., 2013; Socher et al., 2013] to train our models on matching task.", "startOffset": 59, "endOffset": 101}, {"referenceID": 22, "context": "For this task, we use the contrastive max-margin criterion [Bordes et al., 2013; Socher et al., 2013] to train our models on matching task.", "startOffset": 59, "endOffset": 101}, {"referenceID": 4, "context": "To minimize the objective, we use stochastic gradient descent with the diagonal variant of AdaGrad [Duchi et al., 2011].", "startOffset": 99, "endOffset": 119}, {"referenceID": 9, "context": "To prevent exploding gradients, we perform gradient clipping by scaling the gradient when the norm exceeds a threshold [Graves, 2013].", "startOffset": 119, "endOffset": 133}, {"referenceID": 17, "context": "The word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, [Pennington et al., 2014]) and fine-tuned during training to improve the performance.", "startOffset": 107, "endOffset": 132}, {"referenceID": 19, "context": "1 single LSTM [Rockt\u00e4schel et al., 2015] 100 111K 83.", "startOffset": 14, "endOffset": 40}, {"referenceID": 2, "context": "9 parallel LSTMs [Bowman et al., 2015] 100 221K 84.", "startOffset": 17, "endOffset": 38}, {"referenceID": 19, "context": "Attention LSTM [Rockt\u00e4schel et al., 2015] 100 252K 83.", "startOffset": 15, "endOffset": 41}, {"referenceID": 19, "context": "3 Attention(w-by-w) LSTM [Rockt\u00e4schel et al., 2015] 100 252K 83.", "startOffset": 25, "endOffset": 51}, {"referenceID": 19, "context": "\u2022 Single LSTM: A single LSTM to encode the two sequences, which is used in [Rockt\u00e4schel et al., 2015].", "startOffset": 75, "endOffset": 101}, {"referenceID": 19, "context": "\u2022 Attention LSTMs: An attentive LSTM to encode two sentences into a semantic space, which used in [Rockt\u00e4schel et al., 2015].", "startOffset": 98, "endOffset": 124}, {"referenceID": 19, "context": "\u2022 Word-by-word Attention LSTMs: An improvement of attention LSTM by introducing word-by-word attention mechanism, which used in [Rockt\u00e4schel et al., 2015].", "startOffset": 128, "endOffset": 154}, {"referenceID": 2, "context": "We use the Stanford Natural Language Inference Corpus (SNLI) [Bowman et al., 2015].", "startOffset": 61, "endOffset": 82}], "year": 2016, "abstractText": "Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state-ofthe-art methods.", "creator": "LaTeX with hyperref package"}}}