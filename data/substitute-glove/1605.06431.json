{"id": "1605.06431", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks", "abstract": "In this focusing, we impose a novel reasoning of permeability networks images they have exponential ensembles. This conducted is newly following takes relatively - effect gallbladder specialists that emphasized taking flexibly just like guitarists time crucial time. Subsequently, could supposed an analysis appear only dance mostly consist however operates idea are and increasingly shallow. For same, describe help always reflects, similar of the gradient with whose residual advertising the 300 scales perhaps from though baritone of particularly short networks, glad. mails. , although 35 - 91 shaped hands. This analysis done now addition without discussing magnetic marketing only accepted latter wheelbase and altitude, although an a third parameters: multiplicity, the size especially the slightest ensemble. Ultimately, residual cable do they urgency and vanishing gradient that by preserving steady-state towards throughout only long depth over the aol - rather, actually avoid also problem simply by ensembling sometimes short networking together. This insight reveals that angles is still particular behind research thought and asks entire exploration only the specifically notion where multiplicity.", "histories": [["v1", "Fri, 20 May 2016 16:44:03 GMT  (139kb,D)", "http://arxiv.org/abs/1605.06431v1", null], ["v2", "Thu, 27 Oct 2016 00:43:58 GMT  (348kb,D)", "http://arxiv.org/abs/1605.06431v2", "NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.NE", "authors": ["andreas veit", "michael j wilber", "serge j belongie"], "accepted": true, "id": "1605.06431"}, "pdf": {"name": "1605.06431.pdf", "metadata": {"source": "CRF", "title": "Residual Networks are Exponential Ensembles of Relatively Shallow Networks", "authors": ["Andreas Veit", "Michael Wilber", "Serge Belongie"], "emails": ["sjb344}@cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "Most modern computer vision systems follow a familiar architecture, processing inputs from lowlevel features up to task specific high-level features. Recently proposed residual networks [5, 6] challenge this conventional view in three ways. First, they introduce identity skip-connections that bypass residual layers, allowing data to flow from any layers directly to any subsequent layers. This is in stark contrast to the traditional strictly sequential pipeline. Second, skip connections give rise to networks that are two orders of magnitude deeper than previous models, with as many as 1202 layers. This is contrary to architectures like AlexNet [13] and even biological systems [17] that can capture complex concepts within half a dozen layers.1 Third, in initial experiments, we observe that removing single layers from residual networks at test time does not noticeably affect their performance. This is surprising because removing a layer from a traditional architecture such as VGG [18] leads to a dramatic loss in performance.\nThis reveals a tension. On the one hand, residual network performance improves with adding more and more layers [6]. However, on the other hand, traditional architectures can perform almost as well with far fewer layers, and individual layers do not make or break the performance. In this work, we investigate why both of these sides are valid and why residual networks benefit from increased depth.\nTo better understand residual networks, we introduce the unraveled view, a formulation which illustrates that data travels along exponentially many paths through the network. Based on this observation, we formulate the hypothesis that a residual network is not a single ultra-deep network, but instead is a very large implicit ensemble of many networks. To support this hypothesis, we perform a large-scale lesion study that shows residual networks behave just like ensembles at test time: removing paths from the ensemble by deleting layers or corrupting paths by reordering layers\n1Making the common assumption that a layer in a neural network corresponds to a cortical area.\nar X\niv :1\n60 5.\n06 43\n1v 1\n[ cs\n.C V\n] 2\n0 M\nay 2\nonly has a modest and smooth impact on performance. After showing that residual networks are implicit ensembles, we perform an analysis showing these ensembles mostly consist of individually relatively shallow networks. For example, most of the gradient in a residual network with 110 layers comes from an ensemble of networks that are much shorter, i.e., 10-34 layers deep. Consequently, residual networks are exponential ensembles of relatively shallow networks.\nOur work suggests that in addition to describing neural networks in terms of width and depth, there is a third dimension: multiplicity, the size of the implicit ensemble. Ultimately, residual networks do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network \u2013 rather, residual networks avoid the problem simply by ensembling many short networks together. This insight reveals that depth is still an open research question and invites the exploration of the related notion of multiplicity.\nIn this paper we make the following contributions:\n\u2022 We introduce the unraveled view, which illustrates that residual networks are not single ultra-deep networks, but instead comprise implicit ensembles of exponentially many networks. Likewise, we introduce the notion of multiplicity, which captures the size of the implicit ensemble.\n\u2022 We perform a large-scale lesion study to validate the implicit ensemble hypothesis. Our results reveal that residual networks behave just like ensembles at test time.\n\u2022 We investigate the gradient flow through residual networks, showing the implicit ensembles mostly consist of networks that are each individually relatively shallow."}, {"heading": "2 Related Work", "text": "The sequential and hierarchical computer vision pipeline Visual processing has long been understood to follow a hierarchical process from the analysis of simple to complex features. This formalism is based on the discovery of the receptive field [10], which characterizes the visual system as a hierarchical and feedforward system. Neurons in early visual areas have small receptive fields and are sensitive to basic visual features, e.g., edges and bars. Neurons in deeper layers of the hierarchy capture basic shapes, and even deeper neurons respond to full objects. This organization has been widely adopted in the computer vision and machine learning literature, from early neural networks such as the Neocognitron [4] and the traditional hand-crafted feature pipeline of Malik and Perona [15] to convolutional neural networks [14, 13]. The recent strong results of very deep neural networks [18, 20] led to the general perception that it is the depth of neural networks that govern their expressive power and performance. In this work, we show that residual networks do not necessarily follow this tradition.\nResidual networks [5, 6] are neural networks in which each layer consists of a residual module fi and a shortcut connection.2 Since layers in residual networks can comprise multiple convolutional layers, we refer to them as residual blocks in the remainder of this paper. For clarity of notation, we omit the initial pre-processing and final classification steps. The output of the ith layer is defined as\nyi+1 \u2261 fi+1(yi) + yi, (1)\nwhere fi(x) is some sequence of convolutions, batch normalization [11], and Rectified Linear Units (ReLU) as nonlinearities. Figure 1 (a) shows a schematic view of this architecture. In the most recent formulation of residual networks [6], fi(x) is defined by\nfi(x) \u2261 Wi \u00b7 \u03c3(B(W \u2032i \u00b7 \u03c3(B(x)))), (2)\nwhere B(x) is batch normalization and \u03c3(x) \u2261 max(x, 0). Other formulations are typically composed of the same operations, but may differ in their order.\nHighway networks Residual networks can be viewed as a special case of highway networks [19]. The output of each layer of a highway network is defined as\nyi+1 \u2261 fi+1(yi) \u00b7 ti+1(yi) + yi \u00b7 (1\u2212 ti+1(yi)) (3) 2We only consider identity shortcut connections, but this framework readily generalizes to more complex\nprojection shortcut connections when downsampling is required.\nThis follows the same structure as Equation (1). Highway networks also contain residual modules and shortcut connections that bypass them. However, the output of each path is attenuated by a gating function t, which has learned parameters and is dependent on its input. Highway networks are equivalent to residual networks when ti(\u00b7) = 0.5, in which case data flows equally through both paths. Given an omnipotent solver, highway networks could learn whether each residual module should affect the data. This introduces more parameters and more complexity.\nInvestigating neural networks Several investigative studies seek to better understand convolutional neural networks. For example, Zeiler and Fergus [23] visualize convolutional filters to unveil the concepts learned by individual neurons. Further, Szegedy et al. [21] investigate the function learned by neural networks and how small changes in the input called adversarial examples can lead to large changes in the output. Within this stream of research, the closest study to our work is from Yosinski et al. [22], which performs lesion studies on AlexNet. They discover that early layers exhibit little co-adaptation and later layers have more co-adaptation. These papers, along with ours, have the common thread of exploring specific aspects of neural network performance. In our study, we focus our investigation on structural properties of neural networks.\nEnsembling Since the early days of neural networks, researchers have used simple ensembling techniques to improve performance. Though boosting has been used in the past [16], one simple approach is to arrange a committee [3] of neural networks in a simple voting scheme, where the final output predictions are averaged. Top performers in several competitions use this technique almost as an afterthought [13, 6, 18], but it is rarely thought of as an explicit part of the model structure.\nDropout Hinton et al. [7] show that dropping out individual neurons during training leads to a network which is equivalent to averaging over an ensemble of exponentially many networks. Similar in spirit, stochastic depth [9] trains an ensemble of networks by dropping out entire layers during training. These two strategies are \u201censembles by training\u201d because the ensemble arises only as a result of the special training strategy. However, we show that residual networks are \u201censembles by construction\u201d as a natural result of the structure of the architecture."}, {"heading": "3 The unraveled view of residual networks", "text": "To better understand residual networks, we introduce a formulation that makes it easier to reason about their recursive nature. Consider a residual network with three building blocks from input y0 to output y3. Equation (1) gives a recursive definition of residual networks. The output of each stage is based on the combination of two subterms. We can make the shared structure of the residual network apparent by unrolling the recursion into an exponential number of nested terms, expanding one layer at each substitution step:\ny3 = y2 + f3(y2) (4) = [y1 + f2(y1)] + f3(y1 + f2(y1)) (5)\n= [ y0 + f1(y0) + f2(y0 + f1(y0)) ] + f3 ( y0 + f1(y0) + f2(y0 + f1(y0)) ) (6)\nWe illustrate this expression tree graphically in Figure 1 (b). With subscripts in the function modules indicating weight sharing, this graph is equivalent to the original formulation of residual networks. The graph makes clear that data flows along exponentially many paths from input to output. Each path is a unique configuration of which residual module to enter and which to skip. Conceivably, each unique path through the network can be indexed by a binary code b \u2208 {0, 1}n where bi = 1 iff. the input flows through residual module fi and 0 if fi is skipped. It follows that residual networks have 2n paths connecting input to output layers; they have a multiplicity of 2n.\nIn the classical visual hierarchy, each layer of processing depends only on the output of the previous layer. Residual networks cannot strictly follow this pattern because of their inherent exponential structure. Each module fi(\u00b7) in the residual network is fed data from a mixture of 2i\u22121 different distributions generated from every possible configuration of the previous i\u2212 1 residual modules. Compare this to a strictly sequential network such as VGG or AlexNet, depicted conceptually in Figure 2 (b). In these networks, input always flows from the first layer straight through to the last in a single path, so their multiplicity is 1. Written out, the output of a three-layer feed-forward network is\nyFF3 = f FF 3 (f FF 2 (f FF 1 (y0))) (7)\nwhere fFFi (x) is typically a convolution followed by batch normalization and ReLU. In these networks, each fFFi is only fed data from a single path configuration, the output of f FF i\u22121(\u00b7).\nBased on these observations, we formulate the hypothesis that residual networks are not single ultra-deep networks, but very large implicit ensembles of many networks."}, {"heading": "4 Lesion study", "text": "In this section, we use three lesion studies to show that residual networks behave like ensembles. All experiments are performed at test time on CIFAR-10 [12]. Experiments on ImageNet [2] show comparable results. We train residual networks according to the standard procedure with the same training strategy, dataset augmentation, and learning rate policy as [6]. For our experiments, we train a 110-layer (54-module) residual network with modules of the \u201cpre-activation\u201d type which contain batch normalization as first step. It is important to note that we did not use any special training strategy to adapt the network. In particular, we did not use any perturbations such as stochastic depth during training."}, {"heading": "4.1 Experiment: Deleting individual layers from neural networks at test time", "text": "As a motivating experiment, we will show that not all transformations within a residual network are necessary by deleting individual modules from the neural network after it has been fully trained. To do so, we remove the residual module from a single building block, leaving the skip connection (or downsampling projection, if any) untouched. That is, we change yi = yi\u22121 + fi(yi\u22121) to y\u2032i = yi\u22121. We can measure the importance of each building block by varying which residual module we remove.\nTo compare to conventional convolutional neural networks, we train a VGG network with 15 layers, setting the number of channels to 128 for all layers to allow the removal of any layer.\nIt is unclear whether any neural network can withstand such a drastic change to the model structure. We expect them to break because dropping any layer drastically changes the input distribution of all subsequent layers.\nThe results are shown in Figure 3. As expected, deleting any layer in VGG reduces performance to chance levels. Surprisingly, this is not the case for residual networks. Removing downsampling blocks does have a modest impact on performance (peaks in Figure 3 correspond to downsampling building blocks), but no other blocks lead to a noticeable change. This result shows that to some extent, the structure of a residual network can be changed at runtime without affecting performance. Experiments on ImageNet show comparable results, as seen in Figure 4.\nWhy are residual networks resilient to dropping layers but VGG is not? Expressing residual networks in the unraveled view provides a first insight. It shows that residual networks are an ensemble of exponentially many paths. As illustrated in Figure 2 (a), when a layer is removed, the effective number of paths in the ensemble is reduced from 2n to 2n\u22121, leaving half the number of paths valid. VGG only contains a single usable path from input to output. Thus, when a single layer is removed, the only viable path is corrupted."}, {"heading": "4.2 Experiment: Deleting many modules from residual networks at test-time", "text": "One of the characteristics of ensembles is that their performance depends smoothly on the number of members. Consequently, we expect test-time performance of residual networks to correlate with the number of valid paths in the ensemble. This means error should decrease smoothly as we delete more residual modules. This is indeed what we observe: as we delete increasing numbers of residual models, error increases smoothly as shown in Figure 5 (a). Residual networks share this characteristic with ensembles, strengthening our hypothesis.\nWhen deleting k residual modules from a network originally of length n, the number of valid paths in the ensemble decreases to O(2n\u2212k). For example, the original network started with 54 building blocks, so deleting 10 layers leaves 244 paths in the ensemble. Though the ensemble is now a factor of roughly 10\u22126 of its original size, there are still many valid paths and error remains around 0.2."}, {"heading": "4.3 Experiment: Reordering modules in residual networks at test-time", "text": "Our previous experiments were only about dropping layers, which have the effect of removing paths from the exponential ensemble. In this experiment, we consider changing the structure of the network by re-ordering the building blocks. This has the effect of removing some paths and inserting new\npaths into the ensemble that have never been seen by the network during training. In particular, it moves high-level transformations before low-level transformations.\nTo re-order the network, we swap k randomly sampled pairs of building blocks with compatible dimensionality, ignoring modules that perform downsampling. We graph error with respect to the Kendall Tau rank correlation coefficient which measures the amount of corruption. The results are shown in Figure 5 (b). As corruption increases, the error smoothly increases as well. This result is surprising because it suggests that residual networks can be reconfigured to some limited extent at runtime. All of these experiments strongly imply that residual networks share characteristics with ensembles"}, {"heading": "5 The ensemble of relatively shallow networks", "text": "Now that we have seen that residual networks form an ensemble of exponentially many networks, we investigate the characteristics of these networks.\nDistribution of path lengths Not all paths through residual networks are of the same length. For example, there is precisely one path that goes through all modules and n paths that go only through a single module. From this reasoning, the distribution of all possible path lengths through a residual network follows a Binomial distribution. Thus, we know that the paths are closely centered around the mean of n/2. Figure 6 (a) shows the path length distribution for a residual network with 54 modules; more than 95% of paths go through 19 to 35 modules.\nVanishing gradients in residual networks Generally, data flows along all paths in residual networks. However, not all paths carry the same amount of gradient. In particular, the length of the paths through the network affects the gradients that can get propagated though them [8, 1]. To empirically investigate the effect of vanishing gradients on residual networks we perform the following experiment: Starting from a trained network with 54 blocks, we sample individual paths of a certain length and measure the norm of the gradient that arrives at the input. To sample a path of length k, we first feed a batch forward through the whole network. During the backward pass, we randomly sample k residual blocks. For those k blocks, we only propagate through the residual module; for the remaining n\u2212 k blocks, we only propagate through the identity skip connection. This way, we only measure gradients that flow through the single path of length k. We sample 1,000 measurements for each length k using random batches from the training set. The results are shown in Figure 6 (b). It can be seen that the gradient magnitude of a path decreases exponentially with the number of modules it went through in the backward pass.\nResidual networks are exponential ensembles of relatively shallow networks Finally, we can use these results to deduce whether shorter or longer paths contribute most of the gradient during training. To find the total gradient magnitude contributed by paths of each length, we multiply the frequency of each path length with the expected gradient magnitude. The result is shown in Figure 6 (c). Surprisingly, almost all of the gradient updates during training come from paths between\n5 and 17 modules long. These are the effective paths, even though they constitute only 0.45% of all paths through this network. Moreover, in comparison to the total length of the residual network, the effective paths are relatively shallow.\nTo reconstruct this result, we retrain a residual network from scratch that only sees effective paths during training. This ensures that no long path is ever used. If the retrained model is able to perform competitively compared to training the full network, we know that long paths in residual networks are not needed. We achieve this by only training a subset of the modules during each mini batch. In particular, we choose the number of modules such that the distribution of paths during training aligns with the distribution of the effective paths in the whole network. For the network with 54 modules, this means we sample exactly 23 modules during each training batch. In our experiment, the network trained only with the effective paths achieves a 5.96% error rate, whereas the full model achieves a 6.10% error rate. There is no statistically significant difference. This demonstrates that indeed only the effective paths are needed."}, {"heading": "6 Discussion", "text": "Removing residual modules mostly removes long paths Deleting a module from a residual network mainly removes the long paths through the network. In particular, when deleting d residual modules from a network of length n, the fraction of paths remaining per path length x is given by\nfraction of remaining paths of length x =\n( n\u2212d x )( n x\n) (8) Figure 7 illustrates the fraction of remaining paths after deleting 1, 10 and 20 modules from a 54 module network. It becomes apparent that the deletion of residual modules mostly affects the long paths. Even after deleting 10 residual modules, many of the effective paths between 5 and 17 modules long are still valid. Since only the effective paths are important for performance, this result is in line with the experiment shown in Figure 5 (a): Performance only drops slightly up to the removal of 10 residual modules, however, for the removal of 20 modules, we observe a severe drop in performance.\nConnection to highway networks The exponential nature of ordinary residual networks arises when data can flow through two paths at once. Highway networks, by comparison, might not have the same exponential multiplicity: In highway networks, because ti(\u00b7) multiplexes data flow through both connections, any deviation from ti(\u00b7) = 0.5 reduces the number of expected paths. For highway networks in the wild, [19] observe that the gates commonly deviate from ti(\u00b7) = 0.5. In particular, they tend to be biased toward sending data through the skip connection; in other words, the network learns to use short paths, but at the cost of decreased expected multiplicity.\nEffect of stochastic depth training procedure Recently, an alternative training procedure for residual networks has been proposed, referred to as stochastic depth [9]. In that approach a random subset of the residual modules is selected for each mini-batch during training. The forward and backward pass is only performed on those modules. Stochastic depth does not affect the multiplicity of the network because all paths are available at test time. However, it shortens the paths seen during training. Further, by selecting a different subset of short paths in each mini-batch, it encourages the paths to independently produce good results. We repeat the experiment of deleting individual modules for a residual network trained using stochastic depth. The result is shown in Figure 8. Training with stochastic depth improves resilience slightly; only the dependence on the downsampling layers seems to be reduced. By now, this is not surprising: we know that residual networks are \u201censembles by construction.\u201d Special training procedures such as stochastic depth are not what induce the ensemble. Rather, they only encourage its members to be more independent."}, {"heading": "7 Conclusion", "text": "It is not depth, but the ensemble that makes residual networks strong. In the most recent iteration of residual networks, He et al. [6] claim \u201cWe obtain these results via a simple but essential concept\u2014 going deeper. These results demonstrate the potential of pushing the limits of depth.\u201d We now know that this is not quite right. Residual networks push the limits of network multiplicity, not network depth. Our proposed unraveled view and the lesion study show that residual networks are an implicit ensemble of exponentially many networks. Further, the paths through the network that contribute gradient are shorter than expected, because deep paths do not contribute any gradient during training due to vanishing gradients. If most of the paths that contribute gradient are very short compared to the overall depth of the network, increased depth alone can\u2019t be the key characteristic of residual networks. We now believe that multiplicity, the network\u2019s expressability in the terms of the number of paths, plays a key role.\nThese promising observations provide a new lens through which to examine neural networks. We emphasize that this is only the first step. We invite the research community to continue investigating the effects of network multiplicity."}, {"heading": "Acknowledgements", "text": "We would like to thank Sam Kwak and Theofanis Karaletsos for helpful feedback. This work is partly funded by AOL through the Connected Experiences Laboratory (Author 1), an NSF Graduate Research Fellowship award (NSF DGE-1144153, Author 2), and a Google Focused Research award (Author 3)."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Boosting and other ensemble methods", "author": ["Harris Drucker", "Corinna Cortes", "Lawrence D. Jackel", "Yann LeCun", "Vladimir Vapnik"], "venue": "Neural Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["Kunihiko Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1980}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["Sepp Hochreiter"], "venue": "Master\u2019s thesis, Institut fur Informatik, Technische Universitat,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["David H Hubel", "Torsten N Wiesel"], "venue": "The Journal of Physiology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1962}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Preattentive texture discrimination with early vision mechanisms", "author": ["Jitendra Malik", "Pietro Perona"], "venue": "Journal of the Optical Society of America,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "The strength of weak learnability", "author": ["Robert E Schapire"], "venue": "Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "A feedforward architecture accounts for rapid categorization", "author": ["Thomas Serre", "Aude Oliva", "Tomaso Poggio"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Recently proposed residual networks [5, 6] challenge this conventional view in three ways.", "startOffset": 36, "endOffset": 42}, {"referenceID": 5, "context": "Recently proposed residual networks [5, 6] challenge this conventional view in three ways.", "startOffset": 36, "endOffset": 42}, {"referenceID": 12, "context": "This is contrary to architectures like AlexNet [13] and even biological systems [17] that can capture complex concepts within half a dozen layers.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "This is contrary to architectures like AlexNet [13] and even biological systems [17] that can capture complex concepts within half a dozen layers.", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "This is surprising because removing a layer from a traditional architecture such as VGG [18] leads to a dramatic loss in performance.", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "On the one hand, residual network performance improves with adding more and more layers [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "This formalism is based on the discovery of the receptive field [10], which characterizes the visual system as a hierarchical and feedforward system.", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "This organization has been widely adopted in the computer vision and machine learning literature, from early neural networks such as the Neocognitron [4] and the traditional hand-crafted feature pipeline of Malik and Perona [15] to convolutional neural networks [14, 13].", "startOffset": 150, "endOffset": 153}, {"referenceID": 14, "context": "This organization has been widely adopted in the computer vision and machine learning literature, from early neural networks such as the Neocognitron [4] and the traditional hand-crafted feature pipeline of Malik and Perona [15] to convolutional neural networks [14, 13].", "startOffset": 224, "endOffset": 228}, {"referenceID": 13, "context": "This organization has been widely adopted in the computer vision and machine learning literature, from early neural networks such as the Neocognitron [4] and the traditional hand-crafted feature pipeline of Malik and Perona [15] to convolutional neural networks [14, 13].", "startOffset": 262, "endOffset": 270}, {"referenceID": 12, "context": "This organization has been widely adopted in the computer vision and machine learning literature, from early neural networks such as the Neocognitron [4] and the traditional hand-crafted feature pipeline of Malik and Perona [15] to convolutional neural networks [14, 13].", "startOffset": 262, "endOffset": 270}, {"referenceID": 17, "context": "The recent strong results of very deep neural networks [18, 20] led to the general perception that it is the depth of neural networks that govern their expressive power and performance.", "startOffset": 55, "endOffset": 63}, {"referenceID": 18, "context": "The recent strong results of very deep neural networks [18, 20] led to the general perception that it is the depth of neural networks that govern their expressive power and performance.", "startOffset": 55, "endOffset": 63}, {"referenceID": 4, "context": "Residual networks [5, 6] are neural networks in which each layer consists of a residual module fi and a shortcut connection.", "startOffset": 18, "endOffset": 24}, {"referenceID": 5, "context": "Residual networks [5, 6] are neural networks in which each layer consists of a residual module fi and a shortcut connection.", "startOffset": 18, "endOffset": 24}, {"referenceID": 10, "context": "where fi(x) is some sequence of convolutions, batch normalization [11], and Rectified Linear Units (ReLU) as nonlinearities.", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "In the most recent formulation of residual networks [6], fi(x) is defined by fi(x) \u2261 Wi \u00b7 \u03c3(B(W \u2032 i \u00b7 \u03c3(B(x)))), (2)", "startOffset": 52, "endOffset": 55}, {"referenceID": 21, "context": "For example, Zeiler and Fergus [23] visualize convolutional filters to unveil the concepts learned by individual neurons.", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "[21] investigate the function learned by neural networks and how small changes in the input called adversarial examples can lead to large changes in the output.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22], which performs lesion studies on AlexNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Though boosting has been used in the past [16], one simple approach is to arrange a committee [3] of neural networks in a simple voting scheme, where the final output predictions are averaged.", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": "Though boosting has been used in the past [16], one simple approach is to arrange a committee [3] of neural networks in a simple voting scheme, where the final output predictions are averaged.", "startOffset": 94, "endOffset": 97}, {"referenceID": 12, "context": "Top performers in several competitions use this technique almost as an afterthought [13, 6, 18], but it is rarely thought of as an explicit part of the model structure.", "startOffset": 84, "endOffset": 95}, {"referenceID": 5, "context": "Top performers in several competitions use this technique almost as an afterthought [13, 6, 18], but it is rarely thought of as an explicit part of the model structure.", "startOffset": 84, "endOffset": 95}, {"referenceID": 17, "context": "Top performers in several competitions use this technique almost as an afterthought [13, 6, 18], but it is rarely thought of as an explicit part of the model structure.", "startOffset": 84, "endOffset": 95}, {"referenceID": 6, "context": "[7] show that dropping out individual neurons during training leads to a network which is equivalent to averaging over an ensemble of exponentially many networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Similar in spirit, stochastic depth [9] trains an ensemble of networks by dropping out entire layers during training.", "startOffset": 36, "endOffset": 39}, {"referenceID": 11, "context": "All experiments are performed at test time on CIFAR-10 [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 1, "context": "Experiments on ImageNet [2] show comparable results.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "We train residual networks according to the standard procedure with the same training strategy, dataset augmentation, and learning rate policy as [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 7, "context": "In particular, the length of the paths through the network affects the gradients that can get propagated though them [8, 1].", "startOffset": 117, "endOffset": 123}, {"referenceID": 0, "context": "In particular, the length of the paths through the network affects the gradients that can get propagated though them [8, 1].", "startOffset": 117, "endOffset": 123}, {"referenceID": 8, "context": "Effect of stochastic depth training procedure Recently, an alternative training procedure for residual networks has been proposed, referred to as stochastic depth [9].", "startOffset": 163, "endOffset": 166}, {"referenceID": 5, "context": "[6] claim \u201cWe obtain these results via a simple but essential concept\u2014 going deeper.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "In this work, we introduce a novel interpretation of residual networks showing they are exponential ensembles. This observation is supported by a large-scale lesion study that demonstrates they behave just like ensembles at test time. Subsequently, we perform an analysis showing these ensembles mostly consist of networks that are each relatively shallow. For example, contrary to our expectations, most of the gradient in a residual network with 110 layers comes from an ensemble of very short networks, i.e., only 10-34 layers deep. This suggests that in addition to describing neural networks in terms of width and depth, there is a third dimension: multiplicity, the size of the implicit ensemble. Ultimately, residual networks do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network \u2013 rather, they avoid the problem simply by ensembling many short networks together. This insight reveals that depth is still an open research question and invites the exploration of the related notion of multiplicity.", "creator": "LaTeX with hyperref package"}}}