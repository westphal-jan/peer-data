{"id": "1605.07026", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Spontaneous vs. Posed smiles - can we tell the difference?", "abstract": "Smile latter taken vagueness religious once different the function southern seen which true well are whatever example subversive develop. Generally, simply shows happy local of the mind, under, ` smiles ' only it deceptive, any specific not necessarily way just smile not soon everything loved several sometimes they seem provided give brought laugh (in followed different way) instead how feel pity well others. This future innovation to distinguish experimentation (bit) smile words 30 dealing (deliberate) smiles recently extracting brought surveys those consumer (c4isr) motion of the still with subtle (micro) changes in seen scarring essence features through these capabilities put series of tattoo point-by-point concealed having well as components beneath printers means. Specifically entered seeing especially itchy features such freed and rather, component. It jointly to decide cannot all smiles the either ` spontaneous ' but ` seen ' categories, by using support codice_1 circuits (SVM ). Experimental results on large database watch promising judging as 3.5 whether other relevant testing.", "histories": [["v1", "Mon, 23 May 2016 14:21:30 GMT  (2441kb)", "http://arxiv.org/abs/1605.07026v1", "10 pages, 5 figures, 6 tables, International Conference on Computer Vision and Image Processing (CVIP 2016)"]], "COMMENTS": "10 pages, 5 figures, 6 tables, International Conference on Computer Vision and Image Processing (CVIP 2016)", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["bappaditya mandal", "nizar ouarti"], "accepted": false, "id": "1605.07026"}, "pdf": {"name": "1605.07026.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Nizar Ouarti"], "emails": ["bmandal@i2r.a-star.edu.sg", "nizarouarti@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n07 02\n6v 1\n[ cs\n.C V\n] 2\n3 M\nay 2\n01 6\nKeywords: Posed, spontaneous smiles, feature extraction, face analysis."}, {"heading": "1 Introduction", "text": "People believe that human face is the mirror/screen showing internal emotional state of the human body as and when it responds to the external world. This means that, what an individual thinks, feels or understands, etc, deep inside the brain, get imitated into the outside world through its face [7]. Facial smile expression undeniably plays a huge and pivotal role [25,1,9] in understanding social interactions within a community. People often give smile imitating the internal state of the body. For example, generally, people smile when they are happy or when sudden humorous things happen/appear in front of them. However, people are sometimes forced to pose smile because of the outside pressure or external factors. For example, people would pose a smile even when they don\u2019t understand the joke or the humor. Sometimes people would also pose a smile even when they are reluctantly or unwillingly do or perform something in front of their bosses/peers [6].\nTherefore being able to identify the type of smiles of individuals would give affective computing a deeper understanding of the human interactions. A large amount of research in psychology and neuroscience studying facial behavior\ndemonstrate that spontaneous deliberately displayed facial behavior has differences both in utilized facial muscles and their dynamics as compared to posed ones [8]. For example, spontaneous smiles ar smaller in amplitude, longer in duration, slower in onset and offset times than posed smiles [3,8,22]. For humans, capturing such subtle facial movements is difficult and we often fail to distinguish between them. It is not surprising that in computer vision, algorithms developed for classifying such pose and spontaneous smiles usually fail to generalize to the subtlety and complexity of human pose and spontaneous affective behavior [25].\nNumerous researchers asserted that dynamic features such as duration and speed of the smile play a part in differentiating the nature of the smile [9]. A spontaneous smile usually take longer time to reach from onset to apex and then offset as compared to a posed smile [5]. As for non-dynamic features, the aperture size of the eyes is found to be a useful clue and is generally of a higher value when extracted from a spontaneous smile as compared to a posed one. On the other hand, the symmetry in (or the lack of) movement of spontaneous and posed smiles do not produce significant distinction in identifying them and is therefore not much useful [21]. In [22] a multi-modal system using geometric features such as shoulder, head and inner facial movements are fused together and GentleSVM-sigmod is used to classify the posed and spontaneous smiles. He et al. in [11] proposed a technique for feature extraction and compared the performance using geometric and facial appearance features. Appearance based features are computed by recording statistics of overall pixel values of the image, or even using edge detection algorithm such as Gabor Wavelet Filter. Their comprehensive study shows that geometric features are generally more effective in detecting posed from spontaneous expressions [11].\nA spatiotemporal method involving both natural and infrared face videos to distinguish posed and spontaneous expressions is proposed in [20]. Using temporal space and image sequences as volume, they extended the complete local binary patterns texture based descriptor into the spatiotemporal features to classify posed and spontaneous smiles. Dibeklioglu et al. in [4] used the dynamics of eyelid movements and defined distance based and angular features in the changes of the eye aperture. Using several classifiers they have shown the superiority of eyelid movements over the eyebrows, cheek and lip movements for smile classification. Later in [5], they used dynamic characteristics of eyelid, cheek and lip corner movements for classifying posed and spontaneous smiles. Temporal facial information is obtained in [13] through segmenting the facial expression into onset, apex and offset which cover the entire duration of the smile. They reported good classification performance on a small database by using a combination of features extracted from the different phases.\nThe block diagram of our proposed method is shown in Fig. 1. Given smile video sequences of various subjects, we apply the facial features detection and tracking of the fiducial points over the entire smile video clip. Using D-markers, 25 important parameters (like duration, amplitude, speed acceleration, etc) are extracted from two important regions of the face: eyes and lips. Smile discriminative features are extracted using dense optical flow along the temporal domain\nfrom the global (macro) motion and local (micro) motion of the face. All these information are fused and support vector machine (SVM) is then used as a classifier on these parameters to distinguish posed and spontaneous smiles."}, {"heading": "2 Feature Extraction from Various Face Components", "text": "We use the facial tracking algorithm developed by Nguyen et al. in [17] to obtain the fiducial points on the face. The 21 tracking markers each are labeled and placed following the convention as shown in Fig. 2 (a). The markers are manually annotated in the first frame of each video by user input and thereafter it automatically tracks the remaining frames of the smile video, it is of good accuracy and precision as compared to other facial tracking software [2]. The markers are placed on important facial feature points such as eyelids and corner of the lips for each subject. The convention followed in our approach for selecting fiducial markers are shown in Fig. 2 (a)."}, {"heading": "2.1 Face Normalization", "text": "To reduce inaccuracy due to the subject\u2019s head motion in the video that can cause change in angle with respect to roll, yaw and pitch rotations, we use the face normalization procedure described in [5]. Let li represents each of the feature points used to align the faces as shown in Fig. 2. Three non-collinear points (eye centers and nose tip) are used to form a plane \u03c1. Eye centers are defined as c1 = l1+l3 2 and c2 = l4+l6 2 . Angles between the positive normal vector N\u03c1 of \u03c1 and unit vectors U on X (horizontal), Y (vertical), and Z (perpendicular) axes give the relative head pose as follows:\n\u03b8 = arccos U.N\u03c1\n\u2016U\u2016\u2016N\u03c1\u2016 ,where N =\n\u2212\u2212\u2192 lgc2 \u00d7 \u2212\u2212\u2192 lgc1. (1)\n\u2212\u2212\u2192 lgc2 and \u2212\u2212\u2192 lgc1 denote the vectors from point lg to points c2 and c1, respectively. \u2016U\u2016 and \u2016N\u03c1\u2016 represents the magnitudes of U andN\u03c1 vectors respectively. Using the human face configuration, (1) can estimate the exact roll (\u03b8z) and yaw (\u03b8y) angles of the face with respect to the camera. If we start with the frontal face, the pitch angles (\u03b8\u2032x) can be computed by subtracting the initial value. Using the estimated head pose, tracked fiducial points are normalized with respect to rotation, scale and translation as follows:\nl\u2032i = [li \u2212 c1 + c2\n2 ]Rx(\u2212\u03b8\n\u2032 x)Ry(\u2212\u03b8y)Rz(\u2212\u03b8z) 100\n\u01eb(c1 + c2) , (2)\nwhere l\u2032i is the aligned point. Rx, Ry and Rz denote the 3D rotation matrices for the given angles. \u01eb() is the Euclidean distance measure. Essentially (1) constructs a normal vector perpendicular to the plane of the face using three points (nose tip and eye centers), then calculate the angle formed between X , Y and Z axis with regards to the normal vector of face plane. Thereafter, (2) process and normalize each and every point of the frame accordingly and set the interocular distance to 100 pixels with the middle point acting as the new origin of the face center."}, {"heading": "2.2 D-Marker Facial Features", "text": "In the first part of our strategy, we focus on extracting the subject\u2019s eyelid and lips features. We first construct a amplitude signal variable based on the facial feature markers on the eyelid regions. We compute the amplitude of eyelid and lip end movements during a smile using the procedure described in [21]. Eyelid amplitude signals are computed using the eyelid aperture (Deyelid) displacement at time t, given by:\nDeyelid(t) = \u03ba(\nlt 1 +lt 3\n2 , lt2)\u01eb(\nlt 1 +lt 3\n2 , lt2) + \u03ba(\nlt 4 +lt 6\n2 , lt5)\u01eb(\nlt 4 +lt 6\n2 , lt5)\n2\u01eb(lt1, l t 3)\n(3)\nwhere \u03ba(li, lj) denotes the relative vertical location function, which equals to -1 if lj is located (vertically) below li on the face, and 1 otherwise. The equation above uses the markers for eyelids namely 1-6 as shown in Fig. 2, to construct the amplitude signal that calculate the eyelid aperture size in each frame t. The amplitude signal Deyelid is then further computed to obtain a series of features. In addition to the amplitudes, speed and acceleration signal are also extracted by computing the second derivatives of the amplitudes.\nSmile amplitude is estimated as the mean amplitude of right and left lip corners, normalized by the length of the lip. Let Dlip(t) be the value of the mean amplitude signal of the lip corners in the frame t. It is estimated as\nDlip(t) = \u01eb(\nlt 10 +lt 11\n2 , lt10) + \u01eb(\nlt 10 +lt 11\n2 , lt11)\n2\u01eb(lt10, l t 11)\n(4)\nwhere lti denotes the 2D location of the i th point in frame t. For each video of our subject we are able to acquire a 25-dimensional feature vectors based on\nthe eyelids markers and lip corner points. Onset phase is defined as the longest continuous increase in Dlip. Similarly, the offset phase is detected as the longest continuous decrease in Dlip. Apex is defined as the phase between the last frame of the onset and the first frame of the offset. The displacement signals of eyelids and lip corners could then be calculated using the tracked points. Onset, apex and offset phases of the smile are estimated using the maximum continuous increase and decrease of the mean displacement of the eyelids and lip corners. The D-Marker is then able to extract 25 descriptive features each for eyelids and lip corner, so a vector of 50 features are obtained from each frame (using two frames at a time). The features are then concatenated and passed through SVM for training and classification."}, {"heading": "2.3 Features from Dense Optical Flow", "text": "In the second phase of the feature extraction, we use our own proposed dense optical flow [19] for capturing both global and local motions appearing in the smile videos. Our approach is divided into four distinct stages that are fully automatic and does not require any human intervention. The first step is to detect each frame in which the face is present. We use our previously developed face, integration of sketch and graph patterns (ISG) eyes and mouth detectors for face recognition on wearable devices and human-robot-interaction [14,23,15]. So we get the region of interest (ROI) for the face (as shown in Fig. 3, top left, yellow ROI) with 100% accuracy on the entire UvA-NEMO smile database [5]. In the second step, we determine the area corresponding to the right eye, left eye in red ROI and mouth in blue ROI (see Fig. 3 top left) for which we get 96.9% accuracy on the entire database.\nIn the third step, the optical flow is computed between the image at time t and at time t + 1 of the video sequence (see Fig. 3, top). The two components of the optical flow are illustrated in Fig. 3, bottom, which shows the optical flow along the x-axis and the optical flow along the y-axis. Because we are using a dense optic flow algorithm, the time to process one picture is relatively important. To speed up the processing, we computed the optic flow only in the three ROI regions: right eye, left eye and mouth. The optical flow computed in our approach is a pyramidal differential dense algorithm that is based on the following constraint:\nF = Fsmooth + \u03b2Fattach, (5)\nwhere the attachment term is based on thresholding method [24] and the regularization term is based on the method developed by Meyer in [16], \u03b2 is a weight controlling the ratio between the end attachment and the term control. Ouarti et al. in [19] proposed to use a regularization that do not use an usual wavelet but a non-stationary wavelet packet [18], which generalize the concept of wavelet for extracting optical flow information. We extend this idea for extracting fine grained information for both micro and macro motion variations in smile videos as shown in Fig. 4. Fig. 5 shows the dense optical flows with spontaneous and posed smiles variations. In the fourth step, for each of the three ROIs, the me-\ndian of the optical flow is determined that give a cue to the global motion of the area. An histogram is also computed based on the optical flow that has 10 bins. The top three bins in term of cardinality are kept among all the bins. A linear regression is then applied to find the major axis of the point group for each of the three bins determined. In the end, for each ROI we obtain: the median value of the bin 1, the value of the bin 2 and the value of the bin 3. It also calculates the intercept and slope for points of bins 1, 2 and 3. These result in 60 features for each frame (using two consecutive frames in a smile video). SVM is then used on these features to classify the posed and spontaneous smiles. The ma-\njor advantage of this approach is that we can obtain useful smile discriminative features using a fully automatic analysis of videos, no marker are needed to be annotated by an operator/user. Moreover, rather than attempting to classify raw optical flow we design some processing to obtain a sparse representation of the optical flow signal. This representation helps in classification by extracting only the useful information in low dimensions and speeds up the calculation of the SVM. Finally, information is not completely connected to the positioning of the different ROI knowing that this positioning may vary from one frame to another,\nit is dependent on the depth and highly variable depending on the individuals. Therefore a treatment which would be too closely related to the choice of the ROI would lead to non-consistent results."}, {"heading": "3 Experimental Results", "text": "We test our proposed algorithm on UvA-NEMO Smile Database [5], it is the largest and most extensive smile (both posed and spontaneous) database with videos from a total of 400 subjects, (185 female, 215 male) aged between 8 to 76 years old, giving us a total of 1240 individual videos. Each video consists of a short segment (3-8 seconds) of either posed or spontaneous smiles. The videos are extracted into frames at 50 frames per second. The extracted frames are also converted to gray scale and downsized to 480\u00d7 270. In all the experiments, we split the database, in which 80% is used as training samples and the remaining 20% is used as testing samples. Binary classifier SVM in LIBSVM [12] is used to form a hyperplane based on the training samples. When a new testing sample is passed into the SVM it uses the hyperplane to determine which class the new sample falls under. This process is then repeated 5 times using a 5-fold cross validation method. To measure the subtle differences in the spontaneous and posed smiles we compute the confusion matrices between the two smiles so as to find out how much accuracy we can obtain in using each of them in the actual and classified separately. The results from all 5 processes are averaged and shown in confusion Tables 2-5 and compared with other methods in Table 6."}, {"heading": "3.1 Results using parameters from the facial components", "text": "Tables 1 and in bracket (\u00b7) show the accuracy rates in distinguishing spontaneous smiles from the posed ones using eyes and lips features. The results show that the eye features play very crucial role in finding the posed smiles where as the lips features are important for spontaneous smiles. Overall we could obtain an accuracy of 71.14% and 73.44% using eyes and lips features respectively. Table 2 show the classification performance using combined features from eyes and lips. It is evident from the table that using these facial component features, pose smile can be classified better as compared to the spontaneous ones. \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 Actual Classified Spontaneous Posed\nSpontaneous 60.1 (67.5) 39.9 (32.5)\nPosed 17.5 (20.4) 82.5 (79.6)\nTable 1: The overall accuracy (%) in classifying spontaneous and posed smiles using only the eyes features is 71.14%. In bracket (\u00b7) shows accuracy using only the lips features is 73.44%.\n\u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 Actual Classified Spontaneous Posed\nSpontaneous 65.3 34.7\nPosed 16.3 83.7\nTable 2: The overall accuracy (%) in classifying spontaneous and posed smiles using the combined features from eyes and lips is 74.68%. (rows are gallery, columns are testing)"}, {"heading": "3.2 Results using Dense Optical flow", "text": "We use the features using dense optical flow as described in Section 2.3, the movement in both X- and Y-directions are recorded between every consecutive frames of each video. The confusion matrices are shown in Tables 3, in bracket (\u00b7) and 4. It can be see from the tables that the performance of optical flow is lower as compared to the component based approach. However, the facial component based feature extraction method requires user initialization to find and track fiducial points, where the dense optical flow features are fully automatic. It does not require any user intervention, so it is more useful for practical applications like first-person-views (FPV) or egocentric views on wearable devices like Google Glass for improving real-time social interactions [14,10]. \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 Actual Classified Spontaneous Posed\nSpontaneous 57.8 (58.3) 42.2 (41.7)\nPosed 39.8 (30.8) 60.2 (69.2)\nTable 3: The accuracy (%) in classifying spontaneous and posed smiles using our proposed X-directions dense optical flow is 59%. In bracket (\u00b7) the accuracy using our proposed Y-directions is 63.8%.\n\u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 \u2773 Actual Classified Spontaneous Posed\nSpontaneous 58.0 42.0\nPosed 45.1 54.9\nTable 4: The accuracy (%) in classifying spontaneous and posed smiles using our proposed fully automatic system using X- and Y-directions of dense optical flow is 56.6%."}, {"heading": "3.3 Results using both Component based features and Dense Optical Flow", "text": "We combine all the features obtained from facial component based parameters and dense optical flow in to a single vector and apply SVM. Table 5 shows the confusion matrix using spontaneous and posed smiles. It can be seen that the performance of spontaneous smiles classification improved using features from dense optical flow. The experimental results in Table 5 show that both features from facial components and dense optical flows are important for improving the accuracy. Features from facial components (as shown in Table ) are useful for encoding information arising from the muscle artifacts within a face, however, the regularized dense optical flow features helps in encoding fine grained information for both micro and macro motion variations in face smile videos."}, {"heading": "3.4 Comparison with Other Methods", "text": "Correct classification rates using various methods on UvA-NEMO are shown in Table 6."}, {"heading": "4 Conclusions", "text": "Differentiating spontaneous smiles from the posed ones is a challenging problem as it involves extracting subtle minute facial features and learning them. In this work we have analysed features extracted from facial component based parameters using fuducial points markers and tracking them. We have also obtained fully automatic features from dense optical flow on both eyes and mouth patches. It has been shown that the facial component based parameters give higher accuracy as compared to dense optical flow features for smile classification. However, the former requires initialization of the fiducial markers on the first frame and hence, it is not fully automatic. Dense optical flow has advantage that the features can be obtained without any manual intervention. Combining the facial components parameters and dense optical flow gives us highest accuracy for classifying the spontaneous and posed smiles. Experimental results on the largest UvA-NEMO smile database shows the efficacy of our proposed method."}], "references": [{"title": "All smiles are not created equal: Morphology and timing of smiles perceived as amused, polite, and embarrassed/nervous", "author": ["Z. Ambadar", "J. Cohn", "L. Reed"], "venue": "Journal of Nonverbal Behavavior 33, 17\u201334", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Incremental face alignment in the wild", "author": ["A. Asthana", "S. Zafeiriou", "S. Cheng", "M. Pantic"], "venue": "CVPR. Columbus, Ohio, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "The timing of facial motion in posed and spontaneous smiles", "author": ["J. Cohn", "K. Schmidt"], "venue": "Intl J. Wavelets, Multiresolution and Information Processing 2, 1\u201312", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Eyes do not lie: Spontaneous versus posed smiles", "author": ["H. Dibeklioglu", "R. Valenti", "A. Salah", "T. Gevers"], "venue": "ACM Multimedia. pp. 703\u2013706", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Are you really smiling at me? spontaneous versus posed enjoyment smiles", "author": ["H. Dibeklioglu", "A.A. Salah", "T. Gevers"], "venue": "Proceedings of the IEEE ECCV. pp. 525\u2013538", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Telling lies: Cues to deceit in the marketplace, politics, and marriage", "author": ["P. Ekman"], "venue": "WW. Norton & Company, New York", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "The symmetry of emotional and deliberate facial actions", "author": ["P. Ekman", "J. Hager", "W. Friesen"], "venue": "Psychophysiology 18, 101\u2013106", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1981}, {"title": "What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System", "author": ["P. Ekman", "E. Rosenberg"], "venue": "second ed. Oxford Univ. Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Exploring temporal patterns in classifying frustrated and delighted smiles", "author": ["H. et al."], "venue": "IEEE Trans. Affective Computing 3, 323\u2013334", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-sensor self-quantification of presentations", "author": ["T. Gan", "Y. Wong", "B. Mandal", "V. Chandrasekhar", "M. Kankanhalli"], "venue": "ACM Multimedia. pp. 601\u2013610. Brisbane, Australia", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyses of the differences between posed and spontaneous facial expressions", "author": ["M. He", "S. Wang", "Z. Liu", "X. Chen"], "venue": "Humaine Association Conference on Affective Computing and Intelligent Interaction pp. 79\u201384", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A practical guide to support vector classification", "author": ["C. Hsu", "C. Chang", "C. Lin"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "The influence of temporal facial information on the classification of posed and spontaneous enjoyment smiles", "author": ["M.W. Huijser", "T. Gevers"], "venue": "Tech. rep., University of Amsterdam", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A wearable face recognition system on google glass for assisting social interactions", "author": ["B. Mandal", "S. Ching", "L. Li", "V. Chandrasekha", "C. Tan", "J.H. Lim"], "venue": "3 International Workshop on Intelligent Mobile and Egocentric Vision, ACCV. pp. 585\u2013599", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Verification of human faces using predicted eigenvalues", "author": ["B. Mandal", "X.D. Jiang", "A. Kot"], "venue": "19 International Conference on Pattern Recognition (ICPR). pp. 1\u20134. Tempa, Florida, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Oscillating patterns in image processing and in some nonlinear evolution equations", "author": ["Y. Meyer"], "venue": "The Fifteenth Dean Jacquelines B. Lewis Memorial Lectures, American Mathematical Society", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Tracking facial features under occlusions and recognizing facial expressions in sign language", "author": ["T. Nguyen", "S. Ranganath"], "venue": "International Conference on Automatic Face & Gesture Recognition. vol. 6, pp. 1\u20137", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Best basis denoising with non-stationary wavelet packets", "author": ["N. Ouarti", "G. Peyre"], "venue": "International Conferenc on Image Processing. vol. 6, pp. 3825\u20133828", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Method for highlighting at least one moving element in a scene, and portable augmented reality (Aug", "author": ["N. Ouarti", "A. SAFRAN", "B. LE", "S. PINEAU"], "venue": "wO Patent App", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Differentiating spontaneous from posed facial expressions within a generic facial expression recognition framework", "author": ["T. Pfister", "X. Li", "G. Zhao", "M. Pietikainen"], "venue": "ICCV Workshop. pp. 868\u2013875", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparison of deliberate and spontaneous facial movement in smiles and eyebrow raises", "author": ["K. Schmidt", "S. Bhattacharya", "R. Denlinger"], "venue": "Journal of Nonverbal Behavavior 33, 35\u201345", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "How to distinguish posed from spontaneous smiles using geometric features", "author": ["M. Valstar", "M. Pantic"], "venue": "In Proceedings of ACM ICMI. pp. 38\u201345", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "An eye detection and localization system for natural human and robot interaction without face detection", "author": ["X. Yu", "W. Han", "L. Li", "J. Shi", "G. Wang"], "venue": "TAROS pp. 54\u201365", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "A duality based approach for realtime tv-l1 optical flow", "author": ["C. Zach", "T. Pock", "H. Bischof"], "venue": "In Ann. Symp. German Association Patt. Recogn. pp. 214\u2013223", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence 31(1), 39\u201358", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "This means that, what an individual thinks, feels or understands, etc, deep inside the brain, get imitated into the outside world through its face [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 24, "context": "Facial smile expression undeniably plays a huge and pivotal role [25,1,9] in understanding social interactions within a community.", "startOffset": 65, "endOffset": 73}, {"referenceID": 0, "context": "Facial smile expression undeniably plays a huge and pivotal role [25,1,9] in understanding social interactions within a community.", "startOffset": 65, "endOffset": 73}, {"referenceID": 8, "context": "Facial smile expression undeniably plays a huge and pivotal role [25,1,9] in understanding social interactions within a community.", "startOffset": 65, "endOffset": 73}, {"referenceID": 5, "context": "Sometimes people would also pose a smile even when they are reluctantly or unwillingly do or perform something in front of their bosses/peers [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 7, "context": "demonstrate that spontaneous deliberately displayed facial behavior has differences both in utilized facial muscles and their dynamics as compared to posed ones [8].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": "For example, spontaneous smiles ar smaller in amplitude, longer in duration, slower in onset and offset times than posed smiles [3,8,22].", "startOffset": 128, "endOffset": 136}, {"referenceID": 7, "context": "For example, spontaneous smiles ar smaller in amplitude, longer in duration, slower in onset and offset times than posed smiles [3,8,22].", "startOffset": 128, "endOffset": 136}, {"referenceID": 21, "context": "For example, spontaneous smiles ar smaller in amplitude, longer in duration, slower in onset and offset times than posed smiles [3,8,22].", "startOffset": 128, "endOffset": 136}, {"referenceID": 24, "context": "It is not surprising that in computer vision, algorithms developed for classifying such pose and spontaneous smiles usually fail to generalize to the subtlety and complexity of human pose and spontaneous affective behavior [25].", "startOffset": 223, "endOffset": 227}, {"referenceID": 8, "context": "Numerous researchers asserted that dynamic features such as duration and speed of the smile play a part in differentiating the nature of the smile [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "A spontaneous smile usually take longer time to reach from onset to apex and then offset as compared to a posed smile [5].", "startOffset": 118, "endOffset": 121}, {"referenceID": 20, "context": "On the other hand, the symmetry in (or the lack of) movement of spontaneous and posed smiles do not produce significant distinction in identifying them and is therefore not much useful [21].", "startOffset": 185, "endOffset": 189}, {"referenceID": 21, "context": "In [22] a multi-modal system using geometric features such as shoulder, head and inner facial movements are fused together and GentleSVM-sigmod is used to classify the posed and spontaneous smiles.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "in [11] proposed a technique for feature extraction and compared the performance using geometric and facial appearance features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Their comprehensive study shows that geometric features are generally more effective in detecting posed from spontaneous expressions [11].", "startOffset": 133, "endOffset": 137}, {"referenceID": 19, "context": "A spatiotemporal method involving both natural and infrared face videos to distinguish posed and spontaneous expressions is proposed in [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 3, "context": "in [4] used the dynamics of eyelid movements and defined distance based and angular features in the changes of the eye aperture.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Later in [5], they used dynamic characteristics of eyelid, cheek and lip corner movements for classifying posed and spontaneous smiles.", "startOffset": 9, "endOffset": 12}, {"referenceID": 12, "context": "Temporal facial information is obtained in [13] through segmenting the facial expression into onset, apex and offset which cover the entire duration of the smile.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "in [17] to obtain the fiducial points on the face.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "The markers are manually annotated in the first frame of each video by user input and thereafter it automatically tracks the remaining frames of the smile video, it is of good accuracy and precision as compared to other facial tracking software [2].", "startOffset": 245, "endOffset": 248}, {"referenceID": 4, "context": "To reduce inaccuracy due to the subject\u2019s head motion in the video that can cause change in angle with respect to roll, yaw and pitch rotations, we use the face normalization procedure described in [5].", "startOffset": 198, "endOffset": 201}, {"referenceID": 20, "context": "We compute the amplitude of eyelid and lip end movements during a smile using the procedure described in [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "In the second phase of the feature extraction, we use our own proposed dense optical flow [19] for capturing both global and local motions appearing in the smile videos.", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "We use our previously developed face, integration of sketch and graph patterns (ISG) eyes and mouth detectors for face recognition on wearable devices and human-robot-interaction [14,23,15].", "startOffset": 179, "endOffset": 189}, {"referenceID": 22, "context": "We use our previously developed face, integration of sketch and graph patterns (ISG) eyes and mouth detectors for face recognition on wearable devices and human-robot-interaction [14,23,15].", "startOffset": 179, "endOffset": 189}, {"referenceID": 14, "context": "We use our previously developed face, integration of sketch and graph patterns (ISG) eyes and mouth detectors for face recognition on wearable devices and human-robot-interaction [14,23,15].", "startOffset": 179, "endOffset": 189}, {"referenceID": 4, "context": "3, top left, yellow ROI) with 100% accuracy on the entire UvA-NEMO smile database [5].", "startOffset": 82, "endOffset": 85}, {"referenceID": 23, "context": "where the attachment term is based on thresholding method [24] and the regularization term is based on the method developed by Meyer in [16], \u03b2 is a weight controlling the ratio between the end attachment and the term control.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "where the attachment term is based on thresholding method [24] and the regularization term is based on the method developed by Meyer in [16], \u03b2 is a weight controlling the ratio between the end attachment and the term control.", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "in [19] proposed to use a regularization that do not use an usual wavelet but a non-stationary wavelet packet [18], which generalize the concept of wavelet for extracting optical flow information.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "in [19] proposed to use a regularization that do not use an usual wavelet but a non-stationary wavelet packet [18], which generalize the concept of wavelet for extracting optical flow information.", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "We test our proposed algorithm on UvA-NEMO Smile Database [5], it is the largest and most extensive smile (both posed and spontaneous) database with videos from a total of 400 subjects, (185 female, 215 male) aged between 8 to 76 years old, giving us a total of 1240 individual videos.", "startOffset": 58, "endOffset": 61}, {"referenceID": 11, "context": "Binary classifier SVM in LIBSVM [12] is used to form a hyperplane based on the training samples.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "It does not require any user intervention, so it is more useful for practical applications like first-person-views (FPV) or egocentric views on wearable devices like Google Glass for improving real-time social interactions [14,10].", "startOffset": 223, "endOffset": 230}, {"referenceID": 9, "context": "It does not require any user intervention, so it is more useful for practical applications like first-person-views (FPV) or egocentric views on wearable devices like Google Glass for improving real-time social interactions [14,10].", "startOffset": 223, "endOffset": 230}, {"referenceID": 19, "context": "[20] 73.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] 71.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "Cohn & Schmidt [21] 77.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "Mid-level fusion [5] 87.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "Eyelid Features [5] 85.", "startOffset": 16, "endOffset": 19}], "year": 2016, "abstractText": "Smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways. Generally, it shows happy state of the mind, however, \u2018smiles\u2019 can be deceptive, for example people can give a smile when they feel happy and sometimes they might also give a smile (in a different way) when they feel pity for others. This work aims to distinguish spontaneous (felt) smile expressions from posed (deliberate) smiles by extracting and analyzing both global (macro) motion of the face and subtle (micro) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow. Specifically the eyes and lips features are captured and used for analysis. It aims to automatically classify all smiles into either \u2018spontaneous\u2019 or \u2018posed\u2019 categories, by using support vector machines (SVM). Experimental results on large database show promising results as compared to other relevant methods.", "creator": "LaTeX with hyperref package"}}}