{"id": "1605.07366", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Experiments in Linear Template Combination using Genetic Algorithms", "abstract": "Natural Language Generation systems multiple have took used - maintains (' everyone though say ') such capable (' how immediately because ' ). We present see testing taken building an reeducation choctaw - driven decoding operating objectives NLG operating. We difficult templates because called phases 's exactly containing deeper. Our nothing whose of sunday early detachment that templates are distinct locally (within through redaction span ). We posit although factories of a sentence once a highly restricted simplest called reasons interactively. This made even critical effort both creating the subsequent online visible using Genetic Algorithms give arrive in legally processes. We also a point implementation from might approach except outputs superblocks text.", "histories": [["v1", "Tue, 24 May 2016 10:28:43 GMT  (228kb)", "http://arxiv.org/abs/1605.07366v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nikhilesh bhatnagar", "radhika mamidi"], "accepted": false, "id": "1605.07366"}, "pdf": {"name": "1605.07366.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "NLG is the task of generating natural language from nonlinguistic inputs. Most NLG\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 systems can be classified into two broad camps  template based and statistical. Template\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 based systems are generally characterized by structure gapped text (slotfiller structure)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 which is predominantly manually created (Reiter 1995) which generally result in high quality\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 text but also limited linguistic coverage. Statistical systems such as (Langkilde 1998) on the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 other hand, use datadriven algorithms for text generation and have littletono reliance on\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 repetitive manual resources making them more adaptable and maintainable, albeit with lesser\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 text\u00a0quality\u00a0(Reiter\u00a01995).\u00a0\nIn this work, we consider generation on a sentence level and output gapped text. We\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 define templates as gapped text which can be filled to generate textual output. A (partial)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sentence is a linear sequence of such templates. Since there are a large number of choices\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (templates) at every step of sentence generation (sequence of templates), it naturally gives\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rise to a search space which contains all such sequences  grammatical and ungrammatical.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 The challenge is to navigate this large search space and arrive at reasonable grammatical\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sentences.\u00a0 \u00a0 RELEVANT\u00a0WORK\u00a0\nTemplate based systems have been considered to have a very shallow linguistic\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 representation and mapping of the nonlinguistic data to the generated text. General\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 consensus among the NLG community has been that template based systems are not as\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 flexible, maintainable and expressive (linguistic coverage) as full fledged NLG systems\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (Reiter\u00a01995).\u00a0 (Reiter 1997) mentions that template based systems and NLG systems are \u201cturing\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n1\u00a0\nequivalent\u201d meaning that at least in terms of expressiveness, there is no theoretical disparity\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 between the two. Early template based systems (Kukich 1983) used large phrasal units as\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 templates to generate text. However, later systems (van Deemter and Odijk, 1997, Theune,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 2001) in addition to working with smaller templates also exhibit templates with gaps (slots)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 which can be filled recursively which made the systems a lot more maintainable and robust to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output text requirement changes. NaturalOWL (Androustopoulos, 2007) is a template based\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 system which generates descriptions of artifacts in a museum based on the user expertise\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 using an OWL ontology as its data source. (Deemter 2005) gives convincing arguments that\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the line between templatebased systems and NLG is quite blurry. (Kondadadi, 2013) present\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 a hybrid NLG system which generates text by ranking tagged clusters of templates.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Combining that with the huge amount of text data available today, corpus based template\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 approaches\u00a0seem\u00a0feasible.\u00a0\nIn this work, we focus on template structure and combination. A template is gapped text\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 extracted from a corpus. Eg. \u201cin the NN\u201d, \u201cThe JJ NN\u201d, etc. We observe that a template is\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 grammatical locally, within its span. Thus, it doesn\u2019t need to be \u201cgenerated\u201d itself. It follows\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 that linear template combination (juxtaposition) can then be used to generate sentences. The\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 problem is then two fold: how to determine what templates to combine and constrain that\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 with a measure of grammaticality of the generated partial sentence. We make use of NGram\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 models to address both problems. Because the templates are grammatical locally, we can use\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the NGram probabilities at the template edges (junction) to discriminate whether two\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 templates can be combined or not. Also, we use an abstraction of the template  a \u201csyntactic\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 signature\u201d\u00a0to\u00a0compute\u00a0the\u00a0grammaticality\u00a0of\u00a0the\u00a0partial\u00a0sentence.\u00a0\nSince the search space for the partial sentences (template sequences) contains all the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 permutations of all lengths, it is also called a permutation space. Since we do not have a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 reliable \u201cbest\u201d sentence metric, we use GAs to explore the search space to get at a \u201cgood\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enough\u201d solution. We describe how the template combination and sentence grammaticality\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 are\u00a0used\u00a0in\u00a0the\u00a0genetic\u00a0algorithm\u00a0in\u00a0the\u00a0following\u00a0sections.\u00a0 \u00a0 APPROACH\u00a0\nTemplate\u00a0structure\u00a0and\u00a0extraction\u00a0\nIn this paper, we restrict ourselves to linear, subsentential templates. To reduce their\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 number, we consider chunks as templates. It also helps that chunks are a linguistically\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 contained structure. We chunk the UkWaC corpus (Ferraresi 2008) (first 10M sentences)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 using\u00a0CRFChunker\u00a0(XuanHieu\u00a0Phan,\u00a02006)\u00a0for\u00a0extracting\u00a0the\u00a0templates.\u00a0 \u00a0 Template\u00a0Factoring\u00a0\nIn this step, we introduce gaps in the templates. Instead of the gaps being blanks, we\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 specify some linguistic features for the gaps so that not all information is abstracted away.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 These features are called factors. For the baseline, we use the partofspeech as the factor. We\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 use\u00a0the\u00a0following\u00a0strategies\u00a0for\u00a0factoring\u00a0\u00a0\n\u25cf Absolute (count threshold): All tokens(words in a sequence) with counts below a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 threshold\u00a0are\u00a0replaced\u00a0with\u00a0their\u00a0factor\u00a0(POS).\u00a0 \u25cf Relative (rank threshold): All tokens(words in a sequence) with rank below a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 threshold\u00a0are\u00a0replaced\u00a0with\u00a0their\u00a0factor\u00a0(POS).\u00a0\nObviously,\u00a0more\u00a0aggressive\u00a0the\u00a0factoring,\u00a0more\u00a0abstract\u00a0are\u00a0the\u00a0templates.\u00a0\n2\u00a0\n\u00a0 GENETIC\u00a0ALGORITHM\u00a0\nGenetic\u00a0algorithms\u00a0are\u00a0a\u00a0class\u00a0of\u00a0optimization\u00a0algorithms\u00a0which\u00a0mimic\u00a0a\u00a0natural\u00a0process\u00a0 namely\u00a0natural\u00a0selection\u00a0(GA\u00a0primer\u00a0ref).\u00a0Following\u00a0is\u00a0a\u00a0general\u00a0working\u00a0of\u00a0a\u00a0GA:\u00a0\n1. Initialize\u00a0starting\u00a0population.\u00a0 2. While\u00a0stopping\u00a0criterion\u00a0is\u00a0false,\u00a0do\u00a03\u00a0\u00a06\u00a0 3. Selection:\u00a0Select\u00a0a\u00a0sample\u00a0from\u00a0the\u00a0population\u00a0for\u00a0reproduction.\u00a0 4. Crossover:\u00a0Derive\u00a0the\u00a0offspring\u00a0from\u00a0the\u00a0parent\u00a0sample.\u00a0 5. Mutation:\u00a0Mutate\u00a0the\u00a0offsprings\u00a0given\u00a0the\u00a0mutation\u00a0probability.\u00a0 6. Selection:\u00a0Select\u00a0fit\u00a0individuals\u00a0from\u00a0the\u00a0parent\u00a0and\u00a0offspring\u00a0populations\u00a0for\u00a0the\u00a0next\u00a0\ngeneration.\u00a0 These\u00a0components\u00a0are\u00a0described\u00a0below.\u00a0 \u00a0 Chromosome\u00a0\nEach partial solution in the space is an ordered list of templates. That is the chromosome.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 The collection of chromosomes is the total population. We used four factoring strategies:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 count < 100, count < 1000, rank > 1000 and rank > 100. We extracted 18M unique templates\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from unfactored text and 15M, 12M, 4.6M and 1M unique templates for factored text\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 respectively.\u00a0 \u00a0 Crossover\u00a0\nThe template combination problem is addressed in the crossover function. Crossover\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 determines how and with what probability two parent chromosomes produce an offspring.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 We use juxtaposition as the crossover process. So, if two templates \u201cin the NN\u201d and \u201cVBZ a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NN\u201d are combined, the resulting offspring is \u201cin the NN VBZ a NN\u201d. To determine the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 crossover probability, we train a trigram model on the corresponding factored text with\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modified KneserNey smoothing (Sundermeyer 2011) using SRILM (Stolcke 2002). The\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 crossover probability is the trigram probability of the token/factors at the junction of the two\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 partial\u00a0sentences.\u00a0\n\u00a0 Mutation\u00a0\nTo mutate a chromosome, we replace it with a single template chosen randomly from the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 total population. Note that the chromosome being replaced can have multiple templates in it.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 That\u00a0helps\u00a0curb\u00a0the\u00a0sentence\u00a0length\u00a0increase\u00a0rate.\u00a0The\u00a0mutation\u00a0probability\u00a0is\u00a00.05.\u00a0\n\u00a0 Fitness\u00a0function\u00a0\n\u00a0\u00a0\u00a0The\u00a0sentence\u00a0grammaticality\u00a0issue\u00a0is\u00a0handled\u00a0in\u00a0the\u00a0fitness\u00a0function.\u00a0The\u00a0fitness\u00a0function\u00a0 determines\u00a0how\u00a0\u201cgood\u201d\u00a0a\u00a0particular\u00a0solution\u00a0is.\u00a0It\u00a0is\u00a0critical\u00a0for\u00a0selecting\u00a0\u201cgood\u201d\u00a0candidates\u00a0 each\u00a0generation,\u00a0which\u00a0helps\u00a0converge\u00a0to\u00a0a\u00a0set\u00a0of\u00a0\u201cgood\u201d\u00a0solutions.\u00a0 \u00a0\u00a0\u00a0There\u00a0are\u00a0a\u00a0few\u00a0issues\u00a0for\u00a0the\u00a0fitness\u00a0function\u00a0to\u00a0address\u00a0\u00a0\n\u25cf Variable\u00a0length\u00a0\u00a0grammaticality\u00a0does\u00a0not\u00a0depend\u00a0on\u00a0length\u00a0 \u25cf Partial\u00a0sentences\u00a0\u00a0extra\u00a0incentive\u00a0to\u00a0fully\u00a0formed\u00a0sentences\u00a0\nSince the templates are locally grammatical, we simplify the sentential grammaticality to a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n3\u00a0\nsequence of templatelevel features  the syntactic signature. For the baseline, use use the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 chunk tags as the syntactic signature. We train a 5gram model over the sequence of chunk\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tags using modified Kneser Ney smoorthing. The fitness value of a partial sentence is the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 total probability of the chunk tag sequence of the templates constituting it. The crossover\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 probability is the total probability of the chunk tag sequence of the templates constituting it\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 normalized by the length of the chromosome and divided by difference of the target sentence\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 length.\u00a0 Pcrossover\u00a0=\u00a010\u00a0^\u00a0max(Ppartial\u00a0/\u00a0lengthpartial,\u00a0Ptotal\u00a0/\u00a0(lengthpartial\u00a0+\u00a02)\u00a0/\u00a0abs(L\u00a0\u00a0lengthpartial)\u00a0 lengthpartial\u00a0is\u00a0the\u00a0length\u00a0of\u00a0the\u00a0chromosome\u00a0 L\u00a0is\u00a0the\u00a0length\u00a0of\u00a0the\u00a0solution\u00a0chromosomes.\u00a0 Total\u00a0probability\u00a0is\u00a0computed\u00a0by\u00a0considering\u00a0begin\u00a0and\u00a0end\u00a0of\u00a0sentence\u00a0tags\u00a0as\u00a0well.\u00a0 We\u00a0take\u00a0the\u00a0max\u00a0so\u00a0that\u00a0we\u00a0can\u00a0distinguish\u00a0between\u00a0partial\u00a0or\u00a0a\u00a0full\u00a0(total)\u00a0sentence.\u00a0 \u00a0 \u00a0 Evolution\u00a0policy\u00a0\nWe use a basic tournament policy with nbest selection. To do that, a tournament is\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 conducted between a small sample of the population (tournament size say, 10) and the nbest\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (fittest) offspring are selected. This is done until enough offspring are created. We use 10 as\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the tournament size and select the 10 best offspring from each tournament. We maintained\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1M as the population size with a 5050 parentoffspring split and ran the search for 100\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 generations.\u00a0 \u00a0 RESULTS\u00a0\nUnfactored:\u00a0 1. the muslim salutation is so well drawn unanticipated changes hoping to wring\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 community\u00a0pool\u00a0timebank\u00a0a\u00a0bbc\u00a0rob\u00a0&\u00a0bev\u00a0 Rank\u00a0\u00a0100:\u00a0\n1. the\u00a0NN\u00a0UH\u00a01\u00a0EX\u00a0EX\u00a0 Rank\u00a0\u00a01000:\u00a0\n1. two RB related NNS tell the NNP NNS 1 john 5 , 20 NNP the same time NNS \ufffd CD\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 and\u00a0plus\u00a0\ufffd\u00a01\u00a0'\u00a0NNS\u00a0and\u00a0NNS\u00a0,\u00a0telephone\u00a0,\u00a0NN\u00a0young\u00a0NNP\u00a0school\u00a0NNS\u00a0 Count\u00a0\u00a01000:\u00a0 1. the granary holiday cottages NNP helps to sit a JJ cray NNP supercomputer\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\ninternationally\u00a0of\u00a0these\u00a0other\u00a0gentlemen\u00a0com\u00a0NNP\u00a0magazine\u00a0 Count\u00a0\u00a0100\u00a0\n1. sometimes western countries may also have been located her the personal\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 organisation skills had not been tested or used a window dialogue box should be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 properly\u00a0studied\u00a0such\u00a0a\u00a0safe\u00a0labour\u00a0seat\u00a0\n\u00a0 OBSERVATIONS\u00a0\n\u00a0\u00a0\u00a0\u00a0We\u00a0observe\u00a0that\u00a0the\u00a0fitness\u00a0values\u00a0fluctuate\u00a0around\u00a00.18\u00a0for\u00a0all\u00a0factoring\u00a0experiments\u00a0(that\u00a0 is\u00a0because\u00a0the\u00a0syntactic\u00a0signature\u00a0remains\u00a0the\u00a0same).\u00a0The\u00a0chromosome\u00a0length\u00a0did\u00a0stabilize\u00a0 around\u00a0generation\u00a040.\u00a0Clearly,\u00a0the\u00a0sentences\u00a0don\u2019t\u00a0end\u00a0with\u00a0punctuations.\u00a0Long\u00a0distance\u00a0\n4\u00a0\ndependencies\u00a0are\u00a0not\u00a0handled\u00a0well\u00a0by\u00a0NGram\u00a0models,\u00a0so\u00a0that\u00a0may\u00a0not\u00a0be\u00a0a\u00a0good\u00a0 grammaticality\u00a0measure.\u00a0\u00a0 \u00a0 CONCLUSIONS\u00a0AND\u00a0FUTURE\u00a0WORK\u00a0\nClearly, we can observe that the fitness function needs improement to better reflect\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 grammaticality.\u00a0For\u00a0future\u00a0work,\u00a0we\u00a0see\u00a0multiple\u00a0areas\u00a0of\u00a0improvement:\u00a0\n1. Length desensitization to N in the fitness function  percentile measures We currently\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 divide\u00a0logprob\u00a0by\u00a0length.\u00a0 2. Check\u00a0fitness\u00a0function\u00a0behavior\u00a0with\u00a0correct\u00a0sentences\u00a0to\u00a0help\u00a0tune\u00a0parameters.\u00a0 3. Make\u00a0selection\u00a0depend\u00a0on\u00a0average\u00a0length\u00a0of\u00a0the\u00a0sentences.\u00a0 4. Human\u00a0evaluation\u00a0and\u00a0BLEU\u00a0scores\u00a0can\u00a0be\u00a0incorporated\u00a0into\u00a0fitness\u00a0function\u00a0as\u00a0well.\u00a0 5. Create\u00a0better\u00a0syntactic\u00a0signatures\u00a0and\u00a0use\u00a0a\u00a0factored\u00a0language\u00a0model,\u00a0perhaps.\u00a0 6. Use semantic categorization factors in crossover probability to make the selection\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nmore\u00a0robust.\u00a0 7. Explore\u00a0using\u00a0skipgrams\u00a0for\u00a0grammaticality.\u00a0 8. Explore methods to distinguish partial sentences from full sentences in the fitness\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nfunction,\u00a0and\u00a0use\u00a0that\u00a0as\u00a0a\u00a0stopping\u00a0criterion\u00a0in\u00a0tournament\u00a0selection.\u00a0 \u00a0\nREFERENCES\u00a0\nKondadadi,\u00a0R.,\u00a0Howald,\u00a0B.,\u00a0&\u00a0Schilder,\u00a0F.\u00a0(2013,\u00a0August).\u00a0A\u00a0Statistical\u00a0NLG\u00a0Framework\u00a0for\u00a0 Aggregated\u00a0Planning\u00a0and\u00a0Realization.\u00a0In\u00a0ACL\u00a0(1)\u00a0(pp.\u00a014061415).\u00a0 \u00a0 Van\u00a0Deemter,\u00a0K.,\u00a0&\u00a0Odijk,\u00a0J.\u00a0(1997).\u00a0Context\u00a0modeling\u00a0and\u00a0the\u00a0generation\u00a0of\u00a0spoken\u00a0 discourse.\u00a0Speech\u00a0Communication,\u00a021(1),\u00a0101121.\u00a0 \u00a0 Galanis,\u00a0D.,\u00a0&\u00a0Androutsopoulos,\u00a0I.\u00a0(2007,\u00a0June).\u00a0Generating\u00a0multilingual\u00a0descriptions\u00a0from\u00a0 linguistically\u00a0annotated\u00a0OWL\u00a0ontologies:\u00a0the\u00a0NaturalOWL\u00a0system.\u00a0In\u00a0Proceedings\u00a0of\u00a0the\u00a0 Eleventh\u00a0European\u00a0Workshop\u00a0on\u00a0Natural\u00a0Language\u00a0Generation\u00a0(pp.\u00a0143146).\u00a0Association\u00a0 for\u00a0Computational\u00a0Linguistics.\u00a0 \u00a0 Langkilde,\u00a0I.,\u00a0&\u00a0Knight,\u00a0K.\u00a0(1998,\u00a0August).\u00a0The\u00a0practical\u00a0value\u00a0of\u00a0ngrams\u00a0in\u00a0generation.\u00a0In\u00a0 Proceedings\u00a0of\u00a0the\u00a0ninth\u00a0international\u00a0workshop\u00a0on\u00a0natural\u00a0language\u00a0generation\u00a0(pp.\u00a0 248255).\u00a0 \u00a0 Reiter,\u00a0E.,\u00a0&\u00a0Dale,\u00a0R.\u00a0(1997).\u00a0Building\u00a0applied\u00a0natural\u00a0language\u00a0generation\u00a0systems.\u00a0Natural\u00a0 Language\u00a0Engineering,\u00a03(01),\u00a05787.\u00a0 \u00a0 Kukich,\u00a0K.\u00a0(1983,\u00a0June).\u00a0Design\u00a0of\u00a0a\u00a0knowledgebased\u00a0report\u00a0generator.\u00a0In\u00a0Proceedings\u00a0of\u00a0 the\u00a021st\u00a0annual\u00a0meeting\u00a0on\u00a0Association\u00a0for\u00a0Computational\u00a0Linguistics\u00a0(pp.\u00a0145150).\u00a0 Association\u00a0for\u00a0Computational\u00a0Linguistics.\u00a0 \u00a0 XuanHieu\u00a0Phan,\u00a0CRFChunker:\u00a0CRF\u00a0English\u00a0Phrase\u00a0Chunker,\u00a0 http://crfchunker.sourceforge.net/,\u00a02006.\u00a0 \u00a0\n5\u00a0\nReiter,\u00a0E.\u00a0(1995).\u00a0NLG\u00a0vs.\u00a0templates.\u00a0arXiv\u00a0preprint\u00a0cmplg/9504013.\u00a0\nVan\u00a0Deemter,\u00a0K.,\u00a0Krahmer,\u00a0E.,\u00a0&\u00a0Theune,\u00a0M.\u00a0(2005).\u00a0Real\u00a0versus\u00a0templatebased\u00a0natural\u00a0 language\u00a0generation:\u00a0A\u00a0false\u00a0opposition?.\u00a0Computational\u00a0Linguistics,\u00a031(1),\u00a01524.\u00a0\nFerraresi,\u00a0A.,\u00a0Zanchetta,\u00a0E.,\u00a0Baroni,\u00a0M.,\u00a0&\u00a0Bernardini,\u00a0S.\u00a0(2008,\u00a0June).\u00a0Introducing\u00a0and\u00a0 evaluating\u00a0ukWaC,\u00a0a\u00a0very\u00a0large\u00a0webderived\u00a0corpus\u00a0of\u00a0English.\u00a0In\u00a0Proceedings\u00a0of\u00a0the\u00a04th\u00a0 Web\u00a0as\u00a0Corpus\u00a0Workshop\u00a0(WAC4)\u00a0Can\u00a0we\u00a0beat\u00a0Google\u00a0(pp.\u00a04754).\u00a0\nStolcke,\u00a0A.\u00a0(2002,\u00a0September).\u00a0SRILMan\u00a0extensible\u00a0language\u00a0modeling\u00a0toolkit.\u00a0In\u00a0 INTERSPEECH\u00a0(Vol.\u00a02002,\u00a0p.\u00a02002).\u00a0\nSundermeyer,\u00a0M.,\u00a0Schl\u00fcter,\u00a0R.,\u00a0&\u00a0Ney,\u00a0H.\u00a0(2011,\u00a0August).\u00a0On\u00a0the\u00a0Estimation\u00a0of\u00a0Discount\u00a0 Parameters\u00a0for\u00a0Language\u00a0Model\u00a0Smoothing.\u00a0In\u00a0INTERSPEECH\u00a0(pp.\u00a014331436).\u00a0\n\u00a0\n6\u00a0"}], "references": [{"title": "A Statistical NLG Framework for Aggregated Planning and Realization", "author": ["R. Kondadadi", "B. Howald", "Schilder", "August"], "venue": "(pp. 1406\u00ad1415)", "citeRegEx": "Kondadadi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kondadadi et al\\.", "year": 1997}, {"title": "Generating multilingual descriptions from linguistically annotated OWL ontologies: the NaturalOWL system", "author": ["D. Galanis", "Androutsopoulos", "June"], "venue": "In Proceedings of the Eleventh European Workshop on Natural Language Generation (pp. 143\u00ad146)", "citeRegEx": "Galanis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Galanis et al\\.", "year": 2007}, {"title": "The practical value of n\u00adgrams in generation", "author": ["I. Langkilde", "Knight", "August"], "venue": "In Proceedings of the ninth international workshop on natural language generation\u200b", "citeRegEx": "Langkilde et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Langkilde et al\\.", "year": 1998}, {"title": "Building applied natural language generation systems", "author": ["E. Reiter", "R. Dale"], "venue": "\u200bNatural Language Engineering\u200b,", "citeRegEx": "Reiter and Dale,? \\Q1997\\E", "shortCiteRegEx": "Reiter and Dale", "year": 1997}, {"title": "NLG vs", "author": ["E. Reiter"], "venue": "templates. arXiv preprint cmp\u00adlg/9504013.", "citeRegEx": "Reiter,? 1995", "shortCiteRegEx": "Reiter", "year": 1995}, {"title": "Real versus template\u00adbased natural language generation: A false opposition", "author": ["K. Van Deemter", "E. Krahmer", "M. Theune"], "venue": "\u200bComputational Linguistics\u200b,", "citeRegEx": "Deemter et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Deemter et al\\.", "year": 2005}, {"title": "Introducing and evaluating ukWaC, a very large web\u00adderived corpus of English. In \u200bProceedings of the 4th Web as Corpus Workshop (WAC\u00ad4) Can we beat Google\u200b", "author": ["A. Ferraresi", "E. Zanchetta", "M. Baroni", "Bernardini", "June"], "venue": null, "citeRegEx": "Ferraresi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ferraresi et al\\.", "year": 2008}, {"title": "September). SRILM\u00adan extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In INTERSPEECH\u200b (Vol. 2002,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "On the Estimation of Discount Parameters for Language Model Smoothing", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "Ney", "August"], "venue": "In \u200bINTERSPEECH\u200b (pp. 1433\u00ad1436)", "citeRegEx": "Sundermeyer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "Template based systems are generally characterized by structure gapped text (slot\u00adfiller structure) which is predominantly manually created (Reiter 1995) which generally result in high quality text but also limited linguistic coverage.", "startOffset": 140, "endOffset": 153}, {"referenceID": 4, "context": "Statistical systems such as (Langkilde 1998) on the other hand, use data\u00addriven algorithms for text generation and have little\u00adto\u00adno reliance on repetitive manual resources making them more adaptable and maintainable, albeit with lesser text quality (Reiter 1995).", "startOffset": 250, "endOffset": 263}, {"referenceID": 4, "context": "General consensus among the NLG community has been that template based systems are not as flexible, maintainable and expressive (linguistic coverage) as full fledged NLG systems (Reiter 1995).", "startOffset": 178, "endOffset": 191}, {"referenceID": 7, "context": "To determine the crossover probability, we train a trigram model on the corresponding factored text with modified Kneser\u00adNey smoothing (Sundermeyer 2011) using SRILM (Stolcke 2002).", "startOffset": 166, "endOffset": 180}], "year": 0, "abstractText": "Natural Language Generation systems typically have two parts \u00ad strategic (\u201cwhat to say\u201d) and tactical (\u201chow to say\u201d). We present our experiments in building an unsupervised corpus\u00addriven template based tactical NLG system. We consider templates as a sequence of words containing gaps. Our idea is based on the observation that templates are grammatical locally (within their textual span). We posit the construction of a sentence as a highly restricted sequence of such templates. This work is an attempt to explore the resulting search space using Genetic Algorithms to arrive at acceptable solutions. We present a baseline implementation of this approach which outputs gapped text.", "creator": null}}}