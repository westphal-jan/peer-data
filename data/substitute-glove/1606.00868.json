{"id": "1606.00868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Unified Framework for Quantification", "abstract": "Quantification is the printing learning mission according shortfall performance - data classes staggering now especially not perhaps either over be began training. Apart from of physiological yield been an quarter-finals estimation, quantification output reason also be actually to upgrading pseudoprime univariate, thereby increasing classification factual. We wrest major quantification concept previously a greatly multi - soaper loci facilitate, now specific mathematical interactive, estimate class proportions for ones loss functions. With all empirical consistent, what move providing integers - only conceptualization mechanisms which umbrella - class settings as it. We discerned adequately our unified framework took clandestinely have several full - class spectrophotometry similar the Stanford Sentiment Treebank though CIFAR - 15.", "histories": [["v1", "Thu, 2 Jun 2016 20:42:31 GMT  (530kb,D)", "http://arxiv.org/abs/1606.00868v1", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aykut firat"], "accepted": false, "id": "1606.00868"}, "pdf": {"name": "1606.00868.pdf", "metadata": {"source": "CRF", "title": "Unified Framework for Quantification", "authors": ["Aykut Firat"], "emails": ["aykut@crimsonhexagon.com"], "sections": [{"heading": "1 Introduction", "text": "In business applications such as opinion monitoring on social media, accurately predicting the proportion of each opinion class is more important than predicting each individual opinion accurately. Understanding the customer intent to watch a movie before and after the release of a trailer, for example, requires accurate quantification of the class proportions that may exhibit high variation day to day and be completely different than training-time proportions. Naively aggregating standard classifier probability outputs, in general, produces severely biased estimates of class proportions whenever test and training class distributions significantly differ, precisely when something interesting happens. In that\ncase, probability outputs are not optimal for classification either, because the assumed prior class probabilities, namely the training proportions, no longer match the actual priors corresponding to test class proportions.\nThe goal of quantification is to yield accurate estimates of the test class proportions regardless of the training class proportions. These estimates can then be used to adjust classifier probability outputs. The benefits are more pronounced when classes are less separable, and test and training distributions are more divergent. Obviously, a perfect classifier is also a perfect quantifier. Despite all the advances in machine learning, however, we do not often have access to perfect classifiers (Hand, 2006), especially when training data is scarce.\nOur main contribution here is unifying major quantification approaches mentioned in the literature under a single framework. In this unified view, all the major quantification approaches share a multivariate constrained regression model, and differ only in their choice of feature transformation and loss functions. This unification not only offers conceptual clarity and simplicity around the topic of quantification, but has practical benefits as well. By outlining the mathematical programming solutions for the constrained regression model for different loss functions, we extend existing binary-only quantification approaches to multi-class settings. Existing major quantification approaches will be explained next, before we detail the constrained regression framework. ar X\niv :1\n60 6.\n00 86\n8v 1\n[ cs\n.L G\n] 2\nJ un\n2 01"}, {"heading": "2 Related Work", "text": "Although its history goes back to at least 1966 (Gart and Buck, 1966), quantification, as a name, first occurred in a series of papers by Forman (Forman, 2005; Forman, 2007; Forman, 2008). Forman identified the importance of the problem at HP, as they needed to monitor the fast diverging prevalence of support issues over time by analyzing support log data.\nIn a binary setting, the classical quantification method is to first estimate the true positive rate (tpr) and false positive rate (fpr) using a classifier with cross-validation. Then by using an adjustment formula on naive proportion prediction p \u2032 , the adjusted class proportion p is estimated as:\np = max(min( p \u2032\u2212fpr\ntpr\u2212fpr , 1), 0)\nForman introduces variations of this idea by using different cut-off thresholds in calculating p \u2032 , tpr and fpr. The threshold determines the class decision boundary, and directly affects the class assignments; hence p \u2032 , tpr and fpr. The following methods based on threshold choices are described in his papers:\n1. The standard threshold of 0.5 is called adjusted count (AC)\n2. The threshold that maximizes the tpr \u2212 fpr is called Max\n3. The threshold where tpr = 1\u2212 fpr is called X\n4. The threshold where tpr = 0.5 is called T50\n5. Using every quantized threshold first and then taking the median of the estimates is called Median Sweep (MS)\nWhile these thresholds are empirically chosen, Friedman uses the optimum threshold that minimizes the variance of proportion estimates (Friedman, 2014). Friedman\u2019s optimum threshold is dynamic and for a given class corresponds to its training proportion.\nForman also introduces a mixture model, where he seeks to combine classifier score distributions per class obtained via cross-validation with test class\nproportions to obtain the test classifier score distribution. Test class proportions that minimize the difference (using a loss function called PP-Area that corresponds to minimizing L1-norm) between the predicted test CDF and actual CDF then becomes his solution. A similar idea is used by (Gonzalez-Castro et al., 2013) in their HDy method with basically two differences: they replace Forman\u2019s PP-Area metric with Hellinger distance, and use probability mass function (PMF) instead of the CDF to characterize the distributions. The method used by (Bella et al., 2010) can also be viewed as a mixture model. Instead of using a PMF or CDF, they use the mean value of the probability distributions in the mixture model. Friedman, on the other hand, utilizes a mixture model with PMF in setting up a diagnostic test for concept drift, i.e. whether the distribution of features per class is still the same in training and test, but does not utilize it in predicting the proportions.\nThe quantification approach by (Hopkins and King, 2010) is different from the above-mentioned methods in that it does not require a classifier to come up with the class proportion estimates. Yet this method can be seen as quite similar to the mixture model approach mentioned above with the exception that instead of classifier probabilities, feature distributions are used directly with a feature sampling strategy. Feature distribution is modeled as a univariate multinomial distribution with 2N possible outcomes (N=sampled feature size), each of which is a possible feature profile. The HDx method outlined in (Gonzalez-Castro et al., 2013) similarly works directly with features without a classifier.\nA completely different approach is pursued in (Esuli and Sebastiani, 2015) and (Barranquero et al., 2015). They replace the standard loss function used in training with one that targets proportion estimation directly. These methods do not use any post-adjustment and directly use the classifier output for quantification. A recent study, (Tasche, 2016), claims that \u201cquantification without adjustments\u201d methods work only in the degenerate case, i.e. when the class distributions in training and test are the same or nearly the same. We exclude these type of methods from our unification framework.\nAll of these approaches, with the exception of Friedman, and Hopkins & King, limit their focus to binary classification, and leave the multi-class case\nfor future work. Both Friedman, and Hopkins & King introduce standard constrained least squares as their solution methodology for multi-class settings. Friedman, in addition, demonstrates the infeasibility of \u201cone vs. rest\u201d approaches in extending quantification from binary to multi-class settings."}, {"heading": "3 Unifying Quantification Methods", "text": "We now show that existing threshold and mixturemodel based quantification approaches can be modeled as a constrained regression problem by identfying their feature transformation and loss functions. Because the framework is multi-variate, existing binary-only approaches are automatically extended to the multi-class setting under this modeling.\nWe assume that the distribution of features given a class is identical in test and training data. Any quantification task that does not largely satisfy this fundamental assumption is beyond the scope of this paper."}, {"heading": "3.1 Notation", "text": "We will be adopting a similar notation used in (Friedman, 2014):\nT = {yi, xi}NT1 is the labeled training data of size NT = \u2211K i=1Ni with K classes, where Ni is\nthe number of training examples for class i, and y \u2208 {c1, c2, ..., cK} is the class label variable, and x is the feature vector variable of size V ; yi and xi refer to actual values of these variables in the data.\nF = {xi}NF1 , is the future unlabeled data of size NF\nI(z) is an indicator function: 1 when z is true, 0 otherwise\n\u03c0Tk = 1\nNT\n\u2211 i\u2208T I(yi = ck), proportion of class k\nin training\n\u03c0\u0302Fk is the estimated proportion of class k in unlabeled future data,\u03c0\u0302F = (\u03c0\u0302F1, \u03c0\u0302F2, ..., \u03c0\u0302FK), a column vector\ny\u0302i = argmaxkK1 P\u0302Tk(xi) , where P\u0302Tk(x) is\nthe machine learning estimate of PTk(x) = Pr(y = ck|x)\nfl(x) is a feature transformation function to be defined later depending on the quantification approach, l = 1..L, L \u2265 K\nf\u0302Fl = 1\nNF\n\u2211 i\u2208F fl(xi)\nf\u0302lk = 1 Nk \u2211 i\u2208Tk fl(xi)\nX is a matrix of size L\u00d7K where Xlk = f\u0302lk\ny is a vector of length L where yl = f\u0302Fl\nis an error vector of length L\nbin(p) is one-hot encoding function that produces the vector output of size B (number of bins) with a value 1 for the bin p belongs, and zero elsewhere\ncumsum(v) is a function that outputs a vector z where zi = \u2211i j=1 v, v and z are vectors of size\nn, i = 1..n\nntuple(v) produces a sparse vector of size 2|v| with a value 1 for the cell v belongs, and zero elsewhere\nsample(x, n) takes a random subset of size n from x\nNaive estimate \u03c0\u0302naiveFk = 1\nNF\n\u2211 i\u2208F P\u0302Tk(xi)"}, {"heading": "3.2 Constrained Regression", "text": "The general constrained regression model for quantification is:\ny = X\u03c0\u0302F +\ns.t. \u03c0\u0302F >= 0 and \u2211 \u03c0\u0302F = 1\nExisting quantification approaches differ from each other simply by adopting either a different feature transformation function, fl(x), or a different loss function for the regression.\nHopkins & King use a feature transformation function that directly targets approximating the distribution of features via repeated random sampling. Roughly, they produce vectors, by first sampling n features, tabulating all existing permutations and\nnormalizing them to derive a distribution. Gonza\u0301lezCastro in their HDx method, on the other hand, derive a distribution for each feature separately. In these two methods maximum value of the subscript l refers to the number of repeated samples, and number of features respectively.\nThe remaining approaches use a feature transformation function that operates on the probability output, Pr(y = cl|x), (L = K) of a base classifier. Because the probability output is only affected by the same variables, the distribution of probability output given a class is also the same in test and training data by an extension of the fundamental assumption (see (Friedman, 2014) for an illustration).\nThe specific fl(x) choice for each quantification approach, derived simply from definitions, is shown in Table 1 along with abbreviations we will be using in the rest of the paper.\nNote that, we abuse the notation slightly, when fl(x) returns a vector output as in the approaches that use the bin function. In those cases the X and y should be flattened to change their row size from L to L \u00d7 B. We also consider a subset of Forman\u2019s methods that have clear interpretations in multi-class settings. The T50, X and Max methods, for example, are binary specific and need a new definition for multi-class; therefore they are excluded.\nIt is also important to note that approaches using a base classifier need to use cross-validation to estimate the fl(x). Repeating the cross-validation multiple times, and averaging the results increases the stability of the final estimate. As in Hopkins & King, the regression can be performed until a convergence criterion is reached. It is also possible to stack the data from many trials and solve one final regression problem."}, {"heading": "3.3 Solution for Different Loss Functions", "text": "The constrained regression problem in Section 3.2 can be solved using different loss functions. In the literature three different loss functions, namely least-squares, least-absolute deviation and Hellinger divergence, have been used, so we will outline the solution methodology for these three."}, {"heading": "3.3.1 Least-Squares", "text": "The least squares problem can be formulated as follows:\nmin (y \u2212X\u03c0\u0302F )T (y \u2212X\u03c0\u0302F )\ns.t. \u03c0\u0302F >= 0 and \u2211 \u03c0\u0302F = 1\nThe objective function can be expanded as follows:\n\u03c0\u0302TFX TX\u03c0\u0302F \u2212 2\u03c0\u0302TFXTy + yTy\nLet D = 2XTX and d = \u22122XTy and ignore the constant yTy\nthe objective function becomes 1 2 \u03c0\u0302 T FD\u03c0\u0302F + d T \u03c0\u0302F\nFurthermore, the constraints can easily be expressed in the following form: A\u03c0\u0302F \u2264 b\nThis reformulation fits the quadratic programming with linear constraints framework. Hence, the constrained least squares problem can be solved using quadratic programming. We used the quadprog package in R for our experiments."}, {"heading": "3.3.2 Least-Absolute-Deviation", "text": "The least absolute deviation problem can be formulated as follows:\nmin \u2211 |y \u2212X\u03c0\u0302F |\ns.t. \u03c0\u0302F >= 0 and \u2211 \u03c0\u0302F = 1\nThis problem can be transformed into a linear programming problem, and solved using standard techniques, by first defining artificial variables, vector u, and rewriting the objective function as:\nmin \u2211 u\ns.t. u \u2265 y \u2212X\u03c0\u0302F\nu \u2265 \u2212(y \u2212X\u03c0\u0302F )\u2211 \u03c0\u0302F \u2265 1\n\u2212( \u2211 \u03c0\u0302F ) \u2265 \u22121\n\u03c0\u0302F >= 0\nWe used the linprog package in R to formulate and solve this problem as a linear programming problem."}, {"heading": "3.3.3 Hellinger Divergence", "text": "The Hellinger divergence problem can be formulated as follows:\nmin 1\u2212 \u2211\ni\n\u221a yi(X\u03c0\u0302F )i\ns.t. \u03c0\u0302F >= 0 and \u2211 \u03c0\u0302F = 1\nA reduced gradient optimization approach to solve the dual of the above formulated problem is given in (Klafszky et al., 1989). Because the regression problem is quite small, we found out that directly solving this non-linear optimization problem with linear constraints was good enough for testing purposes. We used the R package NlcOptim to solve this problem."}, {"heading": "4 Experiments", "text": "We tested the extended versions of the major quantification algorithms using four different datasets: Stanford Sentiment Treebank, CIFAR-10 image data, marketing data presented in (Hastie et al., 2009) and 69 Twitter opinion analysis data sets we have created. Our goal is not to create a definite ranking of the existing methods, but to reveal an embodiment of our unified framework. At the same time, we empirically show that the multi-class extensions claimed in this paper work well. Although the rankings we get from the experiments provide\nhints on the viability of the methods, we caution the reader in over-generalizing our results to arrive at a definitive ranking of the quantification methods.\nOur R code is made publicly available, and can be used to replicate the experiments.1"}, {"heading": "4.1 Stanford Sentiment Treebank", "text": "We used the training and test split used by (Socher et al., 2013), of size 8544 and 3311 respectively. We worked with a binary document-term matrix with 3770 features, and 5 classes: very negative, negative, neutral, positive, very positive.\nInstead of using a sophisticated classifier such as Socher\u2019s RNTN (Socher et al., 2013), we used a logistic classifier as our base classifier for quantification. The accuracy of RNTN and logistic regression in this task was 44%, and 38% respectively. This was a practical choice for us, because performing cross-validations would take a lot longer if we used RNTN. It is also a test to see if we can get good results with average classifiers. We did use RNTN probability outputs, however, in calculating post-quantification classification adjustments.\nIn order to test how quantifiers perform in different settings, we considered all potential test-case possibilities with a step proportion size of 0.10. This amounts to (1/0.1+5\u22121 5\u22121 ) =1001 different test cases\n1https://github.com/aykutfirat/Quantification\nfor 5 classes with step size 0.1 (e.g. (1,0,0,0,0), (0.9,0.1,0,0,0)...(0.2,0.2,0.2,0.2,0.2)..., (0,0,0,0,1))"}, {"heading": "4.2 CIFAR-10 dataset", "text": "CIFAR-10 has 60000 colour images in 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). We used 55000 of these images as our training data in a bare-bones convolutional neural network (CNN) algorithm that reached about 80% accuracy. The state of the art CNN algorithms in this data set can reach more than 95% accuracy with data-augmentation and using ensembles. Our goal, however, was to experiment with an imperfect classifier, as quantification is not needed when we have a perfect classifier (for example in the case of the famous MNIST dataset).\nWe used the remaining 5000 images as our test data. We again created all possible test-case possibilities with a step proportion size of 0.10. For practical reasons, however, we randomly sampled 1001 from a total of 92378 cases, and conducted our tests on that sample.\nWe omitted HDx and VA methods from this experiment, as appropriately converting image pixel data into features is a hard task and would be very important for their ultimate performance."}, {"heading": "4.3 Marketing dataset", "text": "This dataset is an extract from a survey, and consists of 14 demographic attributes with a good mixture of categorical and continuous variables with a lot of missing data. We included this data for experiments as it was used previously by Friedman to test his quantifier. We adopted the same settings Friedman used, namely a classification problem with five occupation classes (Professional/Managerial, Factory Worker/Laborer/Driver, Homemaker, Student, Retired), 5043 rows and 75 binarized features.\nWe used a random two thirds of the data as training, and constructed all possible (1001) test combinations with a step size of 0.1 from the remaining test data."}, {"heading": "4.4 Twitter dataset", "text": "Twitter data set consists of 69 different test cases constructed from actual hand-coded tweets. Each test case is about a different topic, and number of classes range from 3 to 12. Data size of each test\nvary from 700 to 4000, and number of features range from 150 to 4200. We created two types of tests for this data set.\nIn the first set of tests, we follow the previous test settings by generating all possible proportions with a variable step size depending on the number of classes. Because the number of test cases can be too many, we sample 20 test proportions randomly for each of the 69 test topics for a total of 1380 tests.\nIn the second set of tests, we also generate 20 tests from each test topic. This time, however, we start with roughly the training data proportions and generate a random-walk like behavior by sampling from a Dirichlet distribution by using the previous proportion as an input. This test is intended to simulate the real-life behavior of class proportion changes, where the test set deviates from training gradually.\nIn all Twitter tests, we used a training size of 30 per class. The remaining data was partially used for test depending on the test proportions."}, {"heading": "4.5 Results", "text": "We measure the performance of methods using mean absolute deviation (MAD) of the predictions from the true proportions, and also by looking at the post-quantification accuracy. For post-quantification classification we use the post-quantification classification rule by (Saerens et al., 2002):\nP\u0302kF (x) = \u03c0\u0302kF \u03c0kT P\u0302kT (x)\u2211K l=1 \u03c0\u0302lF \u03c0lT P\u0302lT (x) .\nWe show the average MAD results in Table 2, and average post-quantification classification accuracy results in Table 3. Results are the averages of scores over all experiments per data set. Accuracy is simply the number of correct classifications divided by the total number of classifications. In both tables, \u201cNaive\u201d shows the results of naive quantification, a base-line for performance. In Table 3, \u201cTruth\u201d row shows the accuracy obtainable if we knew the test proportions precisely, an upper-bound for what is achievable.\nIn Figure 1, we show the best performing quantification methods for the Stanford, Marketing and CIFAR-10 data sets along with the actual test proportions, and naive quantification. 1001 experiments are stacked together in one graph for a finer look at the quantification performance. Each horizontal line corresponds to an experiment, and each color in that\nline is category whose length is its proportion. The punctuated nature of the graphs is due to the ordering of categories for easier comparability.\nIn Figure 2, the post-quantification accuracy densities are displayed for the best performing methods. The density graphs reveal that quantification benefits are higher when the gap between test and training distributions are higher, and the base-classifier does not have very high accuracy like the CIFAR-10 case."}, {"heading": "4.5.1 Post-Quantification Classification Performance", "text": "One important benefit of quantification is improved classification performance. In Figure 4, we show how different quantification approaches increase classification accuracy by fitting a regression curve for the classification accuracies as they vary with respect to MAD of test proportions from training proportions. Naive classifier performance gets worse as the MAD gets larger. The best methods show fast increasing classifier accuracy as test and training proportions diverge."}, {"heading": "4.5.2 Performance Sensitivity to Training Proportions", "text": "We show in Figure 3, how the performance of quantification methods vary as test proportions deviate from the training proportions. We fit a regression line for each quantification method by using MAD of test proportions from the training proportions as the independent variable, and MAD of the\nRegression Lines: MAD from Truth~MAD From Training Proportions\nPost\u2212Quantification Classification Accuracy(smoothed)\npredicted test proportions from the true test proportions as the dependent variable. The best quantifiers are expected to be insensitive to the training proportions. The Prob and MM variations, display remarkable insensitivity to training proportions, while the Hellinger divergence based methods have higher sensitivity."}, {"heading": "5 Conclusion", "text": "We presented a constrained regression framework for unifying major quantification approaches. Under this framework, quantification approaches primarily differ on their choice of a feature transformation function that is expected to remain near constant within classes in training and test data, and be sufficiently different across classes. Using mathematical programming, we also offered solution methodologies for least-squares, least-absolute deviation, and Hellinger divergence loss functions for the constrained regression problem. Furthermore, we extended the binary-only quantification approaches to multi-class settings, and presented the results of our experiments with four different data sets to verify that our multi-class extensions work as expected in practice. Our code is made available for replication\nand further experimentation."}], "references": [{"title": "Quantification-oriented learning based on reliable classifiers", "author": ["Jorge Dez", "Juan Jos del Coz"], "venue": "Pattern Recognition,", "citeRegEx": "Barranquero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barranquero et al\\.", "year": 2015}, {"title": "Quantification via Probability Estimators", "author": ["Bella et al.2010] A. Bella", "C. Ferri", "J. Hernandez-Orallo", "M.J. Ramirez-Quintana"], "venue": "In 2010 IEEE 10th International Conference on Data Mining (ICDM),", "citeRegEx": "Bella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bella et al\\.", "year": 2010}, {"title": "Optimizing Text Quantifiers for Multivariate Loss Functions", "author": ["Esuli", "Sebastiani2015] Andrea Esuli", "Fabrizio Sebastiani"], "venue": "ACM Trans. Knowl. Discov. Data,", "citeRegEx": "Esuli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Esuli et al\\.", "year": 2015}, {"title": "Counting positives accurately despite inaccurate classification", "author": ["George Forman"], "venue": "In Machine Learning: ECML", "citeRegEx": "Forman.,? \\Q2005\\E", "shortCiteRegEx": "Forman.", "year": 2005}, {"title": "Quantifying counts, costs, and trends accurately via machine learning", "author": ["George Forman"], "venue": "Technical report,", "citeRegEx": "Forman.,? \\Q2007\\E", "shortCiteRegEx": "Forman.", "year": 2007}, {"title": "Quantifying counts and costs via classification", "author": ["George Forman"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Forman.,? \\Q2008\\E", "shortCiteRegEx": "Forman.", "year": 2008}, {"title": "Class Counts In Future Unlabeled Samples (Detecting and dealing with concept drift), Presentation at MIT CSAIL Big Data Event, November", "author": ["Jerome Friedman"], "venue": null, "citeRegEx": "Friedman.,? \\Q2014\\E", "shortCiteRegEx": "Friedman.", "year": 2014}, {"title": "Comparison of a screening test and a reference test in epidemiologic studies. II. A probabilistic model for the comparison of diagnostic tests", "author": ["Gart", "Buck1966] J.J. Gart", "A.A. Buck"], "venue": "American Journal of Epidemiology,", "citeRegEx": "Gart et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Gart et al\\.", "year": 1966}, {"title": "Class distribution estimation based on the Hellinger distance", "author": ["Rocio Alaiz-Rodriguez", "Enrique Alegre"], "venue": "Information Sciences,", "citeRegEx": "Gonzalez.Castro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gonzalez.Castro et al\\.", "year": 2013}, {"title": "Classifier Technology and the Illusion of Progress", "author": ["David J. Hand"], "venue": "Statistical Science,", "citeRegEx": "Hand.,? \\Q2006\\E", "shortCiteRegEx": "Hand.", "year": 2006}, {"title": "The Elements of Statistical Learning", "author": ["Hastie et al.2009] Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "A Method of Automated Nonparametric Content Analysis for Social Science", "author": ["Hopkins", "King2010] Daniel Hopkins", "Gary King"], "venue": "American Journal of Political Science,", "citeRegEx": "Hopkins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2010}, {"title": "Linearly constrained estimation by mathematical programming", "author": ["Jnos Mayer", "Tams Terlaky"], "venue": "European Journal of Operational Research,", "citeRegEx": "Klafszky et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Klafszky et al\\.", "year": 1989}, {"title": "Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure", "author": ["Patrice Latinne", "Christine Decaestecker"], "venue": "Neural Computation,", "citeRegEx": "Saerens et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Saerens et al\\.", "year": 2002}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts Potts"], "venue": "In EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Does quantification without adjustments work? arXiv preprint arXiv:1602.08780", "author": ["Dirk Tasche"], "venue": null, "citeRegEx": "Tasche.,? \\Q2016\\E", "shortCiteRegEx": "Tasche.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Despite all the advances in machine learning, however, we do not often have access to perfect classifiers (Hand, 2006), especially when training data is scarce.", "startOffset": 106, "endOffset": 118}, {"referenceID": 3, "context": "Although its history goes back to at least 1966 (Gart and Buck, 1966), quantification, as a name, first occurred in a series of papers by Forman (Forman, 2005; Forman, 2007; Forman, 2008).", "startOffset": 145, "endOffset": 187}, {"referenceID": 4, "context": "Although its history goes back to at least 1966 (Gart and Buck, 1966), quantification, as a name, first occurred in a series of papers by Forman (Forman, 2005; Forman, 2007; Forman, 2008).", "startOffset": 145, "endOffset": 187}, {"referenceID": 5, "context": "Although its history goes back to at least 1966 (Gart and Buck, 1966), quantification, as a name, first occurred in a series of papers by Forman (Forman, 2005; Forman, 2007; Forman, 2008).", "startOffset": 145, "endOffset": 187}, {"referenceID": 6, "context": "While these thresholds are empirically chosen, Friedman uses the optimum threshold that minimizes the variance of proportion estimates (Friedman, 2014).", "startOffset": 135, "endOffset": 151}, {"referenceID": 8, "context": "A similar idea is used by (Gonzalez-Castro et al., 2013) in their HDy method with basically two differences: they replace Forman\u2019s PP-Area metric with Hellinger distance, and use probability mass function (PMF) instead of the CDF to characterize the distributions.", "startOffset": 26, "endOffset": 56}, {"referenceID": 1, "context": "The method used by (Bella et al., 2010) can also be viewed as a mixture model.", "startOffset": 19, "endOffset": 39}, {"referenceID": 8, "context": "The HDx method outlined in (Gonzalez-Castro et al., 2013) similarly works directly with features without a classifier.", "startOffset": 27, "endOffset": 57}, {"referenceID": 0, "context": "A completely different approach is pursued in (Esuli and Sebastiani, 2015) and (Barranquero et al., 2015).", "startOffset": 79, "endOffset": 105}, {"referenceID": 15, "context": "A recent study, (Tasche, 2016), claims that \u201cquantification without adjustments\u201d methods work only in the degenerate case, i.", "startOffset": 16, "endOffset": 30}, {"referenceID": 6, "context": "We will be adopting a similar notation used in (Friedman, 2014):", "startOffset": 47, "endOffset": 63}, {"referenceID": 6, "context": "Because the probability output is only affected by the same variables, the distribution of probability output given a class is also the same in test and training data by an extension of the fundamental assumption (see (Friedman, 2014) for an illustration).", "startOffset": 218, "endOffset": 234}, {"referenceID": 12, "context": "A reduced gradient optimization approach to solve the dual of the above formulated problem is given in (Klafszky et al., 1989).", "startOffset": 103, "endOffset": 126}, {"referenceID": 10, "context": "We tested the extended versions of the major quantification algorithms using four different datasets: Stanford Sentiment Treebank, CIFAR-10 image data, marketing data presented in (Hastie et al., 2009) and 69 Twitter opinion analysis data sets we have created.", "startOffset": 180, "endOffset": 201}, {"referenceID": 14, "context": "We used the training and test split used by (Socher et al., 2013), of size 8544 and 3311 respectively.", "startOffset": 44, "endOffset": 65}, {"referenceID": 14, "context": "Instead of using a sophisticated classifier such as Socher\u2019s RNTN (Socher et al., 2013), we used a logistic classifier as our base classifier for quantification.", "startOffset": 66, "endOffset": 87}, {"referenceID": 6, "context": "We included this data for experiments as it was used previously by Friedman to test his quantifier. We adopted the same settings Friedman used, namely a classification problem with five occupation classes (Professional/Managerial, Factory Worker/Laborer/Driver, Homemaker, Student, Retired), 5043 rows and 75 binarized features. We used a random two thirds of the data as training, and constructed all possible (1001) test combinations with a step size of 0.", "startOffset": 67, "endOffset": 418}, {"referenceID": 13, "context": "For post-quantification classification we use the post-quantification classification rule by (Saerens et al., 2002):", "startOffset": 93, "endOffset": 115}], "year": 2016, "abstractText": "Quantification is the machine learning task of estimating test-data class proportions that are not necessarily similar to those in training. Apart from its intrinsic value as an aggregate statistic, quantification output can also be used to optimize classifier probabilities, thereby increasing classification accuracy. We unify major quantification approaches under a constrained multi-variate regression framework, and use mathematical programming to estimate class proportions for different loss functions. With this modeling approach, we extend existing binary-only quantification approaches to multi-class settings as well. We empirically verify our unified framework by experimenting with several multi-class datasets including the Stanford Sentiment Treebank and CIFAR-10.", "creator": "LaTeX with hyperref package"}}}