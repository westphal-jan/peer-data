{"id": "1606.01161", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task Learning", "abstract": "Various treebanks but appeared released would dependency nontechnical. Despite make treebanks or belong hold this linguistics if none are annotation schemes, they commonly descriptors nature that be likely to benefit addition be. This paper presents an based fundamental for exploiting specifically multi - snippet treebanks they importantly judicious back deep multi - task learning. We might two kinds this treebanks more source: on interpreters universal treebanks different followed spanish-english understorey treebanks. Multiple treebanks because trainees establishing although incessantly followed provides - level orthogonal requires. Experiments on in sensex datasets in various language demonstrate it looking changing should actually effective typically a arbitrary source treebanks although achieve target parsing mercedes.", "histories": [["v1", "Fri, 3 Jun 2016 16:09:52 GMT  (757kb,D)", "http://arxiv.org/abs/1606.01161v1", "11 pages, 4 figures"]], "COMMENTS": "11 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiang guo", "wanxiang che", "haifeng wang", "ting liu"], "accepted": false, "id": "1606.01161"}, "pdf": {"name": "1606.01161.pdf", "metadata": {"source": "CRF", "title": "Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task Learning", "authors": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "emails": ["tliu}@ir.hit.edu.cn", "wanghaifeng@baidu.com"], "sections": [{"heading": null, "text": "Various treebanks have been released for dependency parsing. Despite that treebanks may belong to different languages or have different annotation schemes, they contain syntactic knowledge that is potential to benefit each other. This paper presents an universal framework for exploiting these multi-typed treebanks to improve parsing with deep multitask learning. We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. Multiple treebanks are trained jointly and interacted with multi-level parameter sharing. Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models."}, {"heading": "1 Introduction", "text": "As a long-standing central problem in natural language processing (NLP), dependency parsing has been dominated by data-driven approaches with supervised learning for decades. The foundation of data-driven parsing is the availability and scale of annotated training data (i.e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widelyused Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages.\nTo address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasisynchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically require manually designed rules or features, and in most cases, they are limited to the data resources that can be used. Furthermore, for the majority of world languages, such heterogeneous treebanks are not even available. In these cases, crosslingual treebanks may lend a helping hand.\nIn this paper, we aim at developing an universal framework that can exploit multi-typed source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual heterogeneous treebanks.\nCross-lingual supervision has proven highly beneficial for low-resource language parsing (Hwa et al., 2005; McDonald et al., 2011), implying that different languages have a great deal of common ground in grammars. But unfortunately, linguistic inconsistencies also exist in both typologies and lexical representations across languages. Figure 1(a) illustrates two sentences in German and English with universal dependency annotations. The typological differences (subject-verb-object order) results in the opposite directions of the dobj arcs, while the rest arcs remain consistent.\nSimilar problems also come with monolingual heterogeneous treebanks. Figure 1(b) shows an En-\nar X\niv :1\n60 6.\n01 16\n1v 1\n[ cs\n.C L\n] 3\nJ un\n2 01\nglish sentence annotated with respectively the universal dependencies which are content-head and the CONLL dependencies which instead take the functional heads. Despite the structural divergences, these treebanks express the syntax of the same language, thereby sharing a large amount of common knowledge that can be effectively transferred.\nThe present paper proposes a simple and effective framework that aims at making full use of the consistencies while avoids suffering from the inconsistencies across treebanks. Our framework effectively ties together the deep neural parsing models with multi-task learning, using multi-level parameter sharing to control the information flow across tasks. More specifically, learning with each treebank is maintained as an individual task, and their interactions are achieved through parameter sharing in different abstraction levels on the deep neural network, thus referred to as deep multi-task learning. We find that different parameter sharing strategies should be applied for different typed source treebanks adaptively, due to the different types of consistencies and inconsistencies (Figure 1).\nWe investigate the effect of multilingual treebanks as source using the Universal Dependency Treebanks (UDT) (McDonald et al., 2013). We show that our approach improves significantly over strong supervised baseline systems in six languages. We fur-\nther study the effect of monolingual heterogeneous treebanks as source using UDT and the CONLL-X shared task dataset (Buchholz and Marsi, 2006). We consider using UDT and CoNLL-X as source treebanks respectively, to investigate their mutual benefits. Experiment results show significant improvements under both settings. Moreover, indirect comparisons on the Chinese Penn Treebank 5.1 (CTB5) using the Chinese Dependency Treebank (CDT)1 as source treebank show the merits of our approach over previous work."}, {"heading": "2 Related Work", "text": "The present work is related to several strands of previous studies.\nMonolingual resources for parsing. Exploiting heterogeneous treebanks for parsing has been explored in various ways. Niu et al. (2009) automatically convert the dependency-structure CDT into the phrase-structure style of CTB5 using a trained constituency parser on CTB5, and then combined the converted treebanks for constituency parsing. Li et al. (2012) capture the annotation inconsistencies among different treebanks by designing several types of transformation patterns, based on which they introduce quasi-synchronous grammar features (Smith and Eisner, 2009) to augment the baseline parsing models. Johansson (2013) also adopts the idea of parameter sharing to incorporate multiple treebanks. They focused on parameter sharing at feature-level with discrete representations, which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach are capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multityped treebanks and thus more practically useful.\nAside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014).\nMultilingual resources for parsing. Crosslingual transfer has proven to be a promising way of\n1catalog.ldc.upenn.edu/LDC2012T05\ninducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; Ta\u0308ckstro\u0308m et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015).\nDuong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,2 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small treebank of \u223c 3K tokens. Their models may sacrifice accuracy on target languages with a large treebank. Ammar et al. (2016) instead train a single parser on a multilingual set of richresource treebanks, which is a more similar setting to ours. We refer to their approach as shallow multitask learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters.\nBilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning.\nMulti-task learning for NLP. There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010).\n2Duong et al. (2015b) used L2 regularizers to tie the lexical embeddings with a bilingual dictionary.\nMore recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models.\nTo the best of our knowledge, we present the first work that successfully integrate both monolingual and multilingual treebanks for parsing, with or without consistent annotation schemes."}, {"heading": "3 Approach", "text": "This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks."}, {"heading": "3.1 Transition-based Neural Parsing", "text": "Neural models for parsing have gained a lot of interests in recent years, particularly boosted by Chen and Manning (2014). The heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system, based on which the most likely transition action is determined. Typically, a state includes three primary components, a stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as continuous, low-dimensional vectors and use a cube activation function for implicit feature composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016). Here, we employ the LSTM-based architecture enhanced with character bidirectional LSTMs (Ballesteros et\nal., 2015) for the following major reasons:\n\u2022 Compared with Chen & Manning\u2019s architecture, it makes full use of the non-local features by modeling the full history information of a state with stack LSTMs.\n\u2022 By modeling words, stack, buffer and action sequence separately which indicate hierarchical abstractions of representations, we can control the information flow across tasks via parameter sharing with more flexibility (Section 3.2).\nBesides, we did not use the earlier ISBN parsing model (Titov and Henderson, 2007) due to its lack of scalability to large vocabulary. Figure 2 illustrates the transition-based parsing architecture using LSTMs. Bidirectional LSTMs are used for modeling the word representations (Figure 3), which we refer to as Char-BiLSTMs henceforth. CharBiLSTMs learn features for each word, and then the representation of each token can be calculated as:\nx = ReLU(V[\u00d0\u2192w;\u2190\u00d0w; t] + b) (1)\nwhere t is the POS tag embedding. The token embeddings are then fed into subsequent LSTM layers to obtain representations of the stack, buffer and action sequence respectively referred to as st,bt and\nat (The subscript t represents the time step). Note that the subtrees within the stack and buffer are modeled with a recursive neural network (RecNN) as described in Dyer et al. (2015). Next, a linear mapping (W) is applied to the concatenation of st,bt and at, and passed through a component-wise ReLU:\npt = ReLU(W[st;bt;at] + d) (2)\nFinally, the probability of next action z \u2208 A(S,B) is estimated using a softmax function:\np(z\u2223pt) = exp(g\u22bazpt + qz)\n\u03a3z\u2032\u2208A(S,B) exp(g\u22baz\u2032pt + qz\u2032) (3)\nwhere A(S,B) represents the set of valid actions given the current content in the stack and buffer.\nWe apply the non-projective transition system originally introduced by Nivre (2009) since most of the treebanks we consider in this study has a noticeable proportion of non-projective trees. In the SWAP-based system, both the stack and buffer may contain tree fragments, so RecNN is applied both in S and B to obtain representations of each position."}, {"heading": "3.2 Deep Multi-task Learning", "text": "Multi-task learning (MTL) is the procedure of inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation. A good overview, especially focusing on neural networks, can be found in Caruana (1997).\nWe illustrate our multi-task learning architecture in Figure 4. As discussed in previous sections, multiple treebanks, either multilingual or monolingual heterogeneous, contain knowledge that can be mutually beneficial. We consider the target treebank processing as the primary task, and the source treebank as a related task. The two tasks are interacted through multi-level parameter sharing (Section 3.2.1). Inspired by Ammar et al. (2016), we introduce a task-specific vector et (task embedding) which is first combined with st,bt,at to compute pt, and then further concatenated with pt to compute the probability distribution of transition actions. Therefore, Eqn 2, 3 become:\npt = ReLU(W[st;bt;at; et] + d) (4)\nRecNN\nStack LSTM\nBuffer LSTM\n\ud835\udc641\n\ud835\udc64\ud835\udc61\n\ud835\udc64\ud835\udc5b \ud835\udc64\ud835\udc61+1\nChar-BiLSTM\n\ud835\udc64\ud835\udc57\n\u2026\n\u2026\nStack LSTM\nBuffer LSTM\n\ud835\udc64\ud835\udc61 \ud835\udc641\n\ud835\udc64\ud835\udc61+1 \ud835\udc64\ud835\udc5a\n\u2026\n\u2026\n\u2026\u2026\n\ud835\udc52\ud835\udc612\ud835\udc52\ud835\udc611\nConfiguration (Task 1) Configuration (Task 2)\n\ud835\udc20\ud835\udfcf \ud835\udc20\ud835\udfd0\nA ctio n LS T M \ud835\udc4e\ud835\udc59 \ud835\udc4e1 \u2026\nA ctio n LS T M\n\ud835\udc4e\ud835\udc59\n\ud835\udc4e1\n\u2026\n\ud835\udc29\ud835\udc61 Parameters\nShared Task1specific Task2specific\nShared Parameters\nShared Parameters\nFigure 4: The architecture of deep multi-task learning.\np(z\u2223pt) = softmax(g\u22baz [pt; et] + qz) (5)\nEach task uses the same formalism for optimization, and the joint cross-entropy is used as the objective function. The key of multi-task learning is parameter sharing, without which the correlation between tasks will not be exploited. Conventional multi-task learning models typically share a small proportion of parameters across tasks. For example, Collobert and Weston (2008) only shares word embeddings, and Dong et al. (2015) shares the encoder of sequence-to-sequence models. In this work, we use more sophisticated parameter sharing strategies according to the linguistic similarities and differences between the tasks."}, {"heading": "3.2.1 Parameter Sharing", "text": "Deep neural networks automatically learn features for a specific task with hierarchical abstractions, which gives us the flexibility to control parameter sharing in different levels accordingly.\nIn this study, different parameter sharing strategies are applied according to the source and target treebanks being used. We consider two different scenarios: MTL with multilingual universal treebanks as source (MULTI-UNIV) and MTL with monolingual heterogeneous treebanks as source (MONOHETERO). Table 1 presents our parameter sharing strategies for each setting.\nMULTI-UNIV MONO-HETERO\nShared LSTM(S) LSTM(B) RecNN WA,WS ,WB Epos,Erel,Eact LSTM(S) LSTM(B) BiLSTM(chars) RecNN WA,WS ,WB Epos,Echar\nTaskspecific LSTM(A) BiLSTM(chars) g Echar, e t LSTM(A) g Erel,Eact, e t\nTable 1: Parameter sharing strategies for MULTI-UNIV and MONO-HETERO. LSTM(S) \u2013 stack LSTM; LSTM(B) \u2013 buffer LSTM; LSTM(A) \u2013 action LSTM; BiLSTM(chars) \u2013 Char-BiLSTM; RecNN \u2013 recursive NN modeling the subtrees; WA,WS ,WB \u2013 weights from A, S, B to the state (pt); g \u2013 weights from the state to output layer; E \u2013 embeddings.\nMULTI-UNIV. Multilingual universal treebanks are annotated with the same set of POS tags (Petrov et al., 2012), dependency relations, and thus share the same set of transition actions. However, the vocabularies (word, characters) are language-specific. Additionally, linguistic typologies such as the order of subject-verb-object and adjective-noun (Figure 1(a)) also varies across languages, which result in the divergence of inherent grammars of transition actions. Therefore, it makes sense to share the lookup tables (embeddings) of POS tags (Epos), relations (Erel) and actions (Eact), but separate the character embeddings (Echar) as well as the CharBiLSTM (BiLSTM(chars)), and also the LSTM modeling action sequence (LSTM(A))\nMONO-HETERO. Monolingual heterogeneous treebanks instead share the same lexical representations, but have different POS tags, structures and relations (Figure 1(b)) due to the different annotation schemes. Hence the transition actions set varies across treebanks. For simplicity reasons, we convert the language-specific POS tags in the heterogeneous treebanks into universal POS tags (Petrov et al., 2012). Consequently, Echar and BiLSTM(chars), Epos are shared across tasks, but Erel, Eact, LSTM(A) are separated.\nBesides, the LSTM parameters for modeling the stack and buffer (LSTM(S), LSTM(B)), the RecNN for modeling tree compositions, and the weights from S, B, A to the state pt (WA,WB,WS) are shared for both MULTI-UNIV and MONO-HETERO.\nAs standard in multi-task learning, the weights at the output layer (g) are task-specific in both settings."}, {"heading": "3.2.2 Learning", "text": "Training is achieved in a stochastic manner by looping over the tasks: 1. Randomly select a task. 2. Select a sentence from the task, and generate\ninstances for classification. 3. Update the corresponding parameters by back-\npropagation w.r.t. the instances. 4. Go to 1. We adopt the development data of the target tree-\nbank (primary task) for early-stopping."}, {"heading": "4 Experiments", "text": "We first describe the data and settings in our experiments, then the results and analysis."}, {"heading": "4.1 Data and Settings", "text": "We conduct experiments on UDT v2.03 and the CoNLL-X shared task data. For monolingual heterogeneous source, we also experiment on CTB5 using CDT as the source treebank, to compare with the previous work of Li et al. (2012). Statistics of the datasets are summarized in Table 2. We investigate the following experiment settings:\n\u2022 MULTILINGUAL (UNIV\u2192UNIV). In this setting, we study the integration of multilingual universal treebanks. Experiments are conducted using the UDT dataset. Specifically, we consider DE, ES, FR, PT, IT and SV treebanks as target treebanks, and the EN treebank as the common source treebank.\n\u2022 MONOLINGUAL (CONLL\u2194UNIV). Here we study the integration of monolingual heterogeneous treebanks. The CONLL-X corporas (DE, ES, PT, SV) and the UDT treebank of corresponding languages are used as source and target treebanks mutually.\n\u2022 MONOLINGUAL (CDT\u2192CTB5). We follow the same settings of Li et al. (2012), and consider two scenarios using automatic POS tags and gold-standard POS tags respectively.\n3github.com/ryanmcd/uni-dep-tb\nWe use the widely-adopted unlabeled attachment score (UAS) and labeled attachment score (LAS) for evaluation."}, {"heading": "4.2 Baseline Systems", "text": "We compare our approach with the following baseline systems.\n\u2022 Monolingual supervised training (SUP). Models are trained only on the target treebank, with the LSTM-based parser.\n\u2022 Cascaded training (CAS). This system has two stages. First, models are trained using the source treebank. Then the parameters are used to initialize the neural network for training target parsers. Similar approach was studied in Duong et al. (2015a) and Guo et al. (2016) for low-resource parsing.\nFor MULTILINGUAL (UNIV\u2192UNIV), we also compare with the shallow multi-task learning (SMTL) system, as described in Section 2, which is representative of the approach of Duong et al. (2015b) and Ammar et al. (2016). In SMTL all the parameters are shared except the character embeddings (Echar), and task embeddings (et) are not used. Unlike Duong et al. (2015b) and Ammar et al. (2016), we don\u2019t use external resources such as cross-lingual word clusters, embeddings and dictionaries which is beyond the scope of this work."}, {"heading": "4.3 Results", "text": "In this section, we present empirical evaluations under different settings."}, {"heading": "4.3.1 Multilingual Universal Source Treebanks", "text": "Table 3 shows the results under the MULTILINGUAL (UNIV\u2192UNIV) setting. CAS yields slightly better performance than SUP, especially for SV (+1.52% UAS and +2.04% LAS), indicating that pre-training with EN training data indeed provides a better initialization of the parameters for cascaded training. SMTL in turn outperforms CAS overall (comparable for IT), which implies that training two treebanks jointly helps even with an unique model.\nFurthermore, with appropriate parameter sharing, our deep multi-task learning approach (MTL) outperforms SUP overall and achieves the best performances in five out of six languages. An exception is Swedish. As we can see, both CAS and SMTL outperforms MTL by a significant margin for SV. The underlying reasons we suggest are two-fold.\n1. SV morphology is similar to EN with less inflections, encouraging the morphology-related parameters like BiLSTM(chars) to be shared.\n2. SV has a much smaller treebank compared with EN (1:9). Intuitively, SMTL and CAS work better in low resource setting.\nTo verify the first issue, we conduct tests on SMTL without sharing Char-BiLSTMs. As shown in Table 4, the performance of SMTL decreases significantly (-0.73 in UAS). This observation also indicates that MTL has the potential to reach higher performances through language-specific tuning of parameter sharing strategies.\nTo verify the second issue, we consider a low resource setup following Duong et al. (2015b), where the target language has a small treebank (3K tokens). We train our models on identical sampled dataset\nshared by Duong et al. (2015b) on DE, ES and FR. As we can find in Table 5, while all the models outperform SUP, both CAS and SMTL work better than MTL, which confirms our assumption. Although not the primary focus of this work, we find that SMTL and MTL can be significantly improved in low resource setting through weighted sampling of tasks during training. Specifically, in the training procedure (Section 3.2.2), we sample from the source language (EN) which has a much richer treebank with larger probability of 0.9, while sample from the target language with probability of 0.1. In this way, the two tasks are encouraged to converge at a similar rate. As shown in Table 5, both SMTL and MTL benefit from weighted task sampling."}, {"heading": "4.3.2 Monolingual Hetero. Source Treebanks", "text": "Table 6 shows the results of MONOLINGUAL (CONLL\u2194UNIV). Overall MTL systems outperforms the supervised baselines by significant margins in both conditions, showing the mutual benefits\nof UDT and CONLL-X treebanks.4\nIn addition, among the four languages here, the SV universal treebank is mainly converted from the Talbanken part of the Swedish bank (Nivre and Megyesi, 2007), thus has a large overlap with the CONLL-X Swedish treebank. In fact, we find a large proportion of the SV test data in UDT/CONLL-X appears in CONLL-X/UDT SV training data. Typically we expect fully unseen data for testing, so we further separate the SV testing data into two parts: IN-SRC and OUT-SRC including sentences that appear in the source treebank or not, respectively. Statistics are shown below.\nCONLL\u2192UNIV UNIV\u2192CONLL IN-SRC 875 352 OUT-SRC 344 37\n4An exception is PT in MONOLINGUAL (UNIV\u2192CONLL), in which both CAS and MTL get slightly degradation in performance. This may be due to the low quality of the PT universal treebank caused by the automatic construction process. We discussed and verified this with the author of UDT v2.0.\nThe SV\u2217 row in Table 6 presents the OUT-SRC results of SV, which shows consistent improvements.\nTo show the merit of our approach against previous approaches, we further conduct experiments on CTB5 using CDT as heterogeneous source treebank (Table 2). For CTB5, we follow (Li et al., 2012) and consider two scenarios which use automatic POS tags and gold-standard POS tags respectively. To compare with their results, we run SUP, CAS and MTL on CTB5. Table 7 presents the results. The indirect comparison indicates that our approach can achieve larger improvement than their method in both scenarios. Beside the empirical comparison, our method has the additional advantages in its scalability to multi-typed source treebanks without the painful human efforts of feature design."}, {"heading": "4.4 Remarks", "text": "Overall, our approach obtains substantial gains over supervised baselines with either multilingual universal treebanks or monolingual heterogeneous treebanks as source. With multilingual source treebanks, our model has the potential to improve even further via language-specific tuning. While not the primary focus of this study, in low resource setting, we show that more emphasize may be put on the source treebanks through weighted task sampling."}, {"heading": "5 Conclusion", "text": "This paper propose an universal framework based on deep multi-task learning that can integrate arbitrarytyped source treebanks to enhance the parsing models on target treebanks. We study two scenarios, respectively using multilingual universal source treebanks and monolingual heterogeneous source treebanks, and design effective parameter sharing strategies for each scenario.\nWe conduct extensive experiments on several\nbenchmark treebanks in various languages. Results demonstrate that our approach significantly improves over baseline systems under various experiment setting. Furthermore, our framework can flexibly incorporate richer treebanks and more related tasks, which we leave to future exploration."}, {"heading": "Acknowledgments", "text": "We thank Ryan McDonald for fruitful discussions, and thank Dr. Zhenghua Li for sharing the processed CTB and CDT dataset. This work was supported by the National Key Basic Research Program of China via grant 2014CB340503 and the National Natural Science Foundation of China (NSFC) via grant 61133012 and 61370164."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": null, "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of the 2015 Conference on EMNLP,", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "A transition-based system for joint part-ofspeech tagging and labeled non-projective dependency parsing", "author": ["Bohnet", "Nivre2012] Bernd Bohnet", "Joakim Nivre"], "venue": "In Proc. of the 2012 Joint Conference on EMNLP and CoNLL,", "citeRegEx": "Bohnet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bohnet et al\\.", "year": 2012}, {"title": "Conll-x shared task on multilingual dependency parsing", "author": ["Buchholz", "Marsi2006] Sabine Buchholz", "Erwin Marsi"], "venue": "In Proc. of the Tenth Conference on Computational Natural Language Learning (CoNLLX),", "citeRegEx": "Buchholz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Buchholz et al\\.", "year": 2006}, {"title": "Two languages are better than one (for syntactic parsing)", "author": ["Burkett", "Klein2008] David Burkett", "Dan Klein"], "venue": "In Proc. of the 2008 Conference on EMNLP,", "citeRegEx": "Burkett et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Burkett et al\\.", "year": 2008}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Bitext dependency parsing with bilingual subtree constraints", "author": ["Chen et al.2010] Wenliang Chen", "Jun\u2019ichi Kazama", "Kentaro Torisawa"], "venue": "In Proc. of the 48th ACL,", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proc. of the 25th ICML,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Multi-task learning for multiple language translation", "author": ["Dong et al.2015] Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume 1: Long Papers),", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "2015a. Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser", "author": ["Duong et al.2015a] Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume", "citeRegEx": "Duong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "2015b. A neural network model for low-resource universal dependency parsing", "author": ["Duong et al.2015b] Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook"], "venue": "In Proc. of the 2015 Conference on EMNLP,", "citeRegEx": "Duong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume 1: Long Papers),", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Crosslingual dependency parsing based on distributed representations", "author": ["Guo et al.2015] Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume 1: Long Papers),", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A representation learning framework for multi-source transfer parsing", "author": ["Guo et al.2016] Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI), February", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese", "author": ["Hatori et al.2012] Jun Hatori", "Takuya Matsuzaki", "Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In Proc. of the 50th ACL (Volume 1: Long Papers),", "citeRegEx": "Hatori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hatori et al\\.", "year": 2012}, {"title": "Incremental sigmoid belief networks for grammar learning", "author": ["Henderson", "Titov2010] James Henderson", "Ivan Titov"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Henderson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2010}, {"title": "Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model", "author": ["Paola Merlo", "Ivan Titov", "Gabriele Musillo"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Bilingually-constrained (monolingual) shift-reduce parsing", "author": ["Huang et al.2009] Liang Huang", "Wenbin Jiang", "Qun Liu"], "venue": "In Proc. of the 2009 Conference on EMNLP,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Hwa et al.2005] Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak"], "venue": "Natural language engineering,", "citeRegEx": "Hwa et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "Training parsers on incompatible treebanks", "author": ["Richard Johansson"], "venue": "In Proc. of NAACL: HLT,", "citeRegEx": "Johansson.,? \\Q2013\\E", "shortCiteRegEx": "Johansson.", "year": 2013}, {"title": "Joint models for chinese pos tagging and dependency parsing", "author": ["Li et al.2011] Zhenghua Li", "Min Zhang", "Wanxiang Che", "Ting Liu", "Wenliang Chen", "Haizhou Li"], "venue": "In Proc. of the 2011 Conference on EMNLP,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Exploiting multiple treebanks for parsing with quasi-synchronous grammars", "author": ["Li et al.2012] Zhenghua Li", "Ting Liu", "Wanxiang Che"], "venue": "In Proc. of the 50th ACL (Volume 1: Long Papers),", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Multi-task sequence to sequence learning. CoRR, abs/1511.06114", "author": ["Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Slav Petrov", "Keith Hall"], "venue": "In Proc. of the 2011 Conference on EMNLP,", "citeRegEx": "McDonald et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Exploiting heterogeneous treebanks", "author": ["Niu et al.2009] Zheng-Yu Niu", "Haifeng Wang", "Hua Wu"], "venue": null, "citeRegEx": "Niu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2009}, {"title": "Integrating graph-based and transitionbased dependency parsers", "author": ["Nivre", "McDonald2008] Joakim Nivre", "Ryan McDonald"], "venue": "In Proc. of ACL-08: HLT,", "citeRegEx": "Nivre et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2008}, {"title": "Bootstrapping a swedish treebank using cross-corpus harmonization and annotation projection", "author": ["Nivre", "Megyesi2007] Joakim Nivre", "Beata Megyesi"], "venue": "In Proc. of the 6th International Workshop on Treebanks and Linguistic Theories,", "citeRegEx": "Nivre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre"], "venue": "In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,", "citeRegEx": "Nivre.,? \\Q2009\\E", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2012] Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": "In Proc. of the Eighth International Conference on Language Resources and Evaluation", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Density-driven crosslingual transfer of dependency parsers", "author": ["Rasooli", "Michael Collins"], "venue": "In Proc. of the 2015 Conference on EMNLP,", "citeRegEx": "Rasooli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasooli et al\\.", "year": 2015}, {"title": "Parser adaptation and projection with quasisynchronous grammar features", "author": ["Smith", "Eisner2009] David A. Smith", "Jason Eisner"], "venue": "In Proc. of the 2009 Conference on EMNLP,", "citeRegEx": "Smith et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2009}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proc. of NAACL: HLT,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Rediscovering annotation projection for cross-lingual parser induction", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proc. of COLING", "citeRegEx": "Tiedemann.,? \\Q2014\\E", "shortCiteRegEx": "Tiedemann.", "year": 2014}, {"title": "Fast and robust multilingual dependency parsing with a generative latent variable model", "author": ["Titov", "Henderson2007] Ivan Titov", "James Henderson"], "venue": "In Proc. of the CoNLL Shared Task Session of EMNLPCoNLL", "citeRegEx": "Titov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2007}, {"title": "Stacking dependency parsers", "author": ["Dipanjan Das", "Noah A. Smith", "Eric P. Xing"], "venue": "In Proc. of the 2008 Conference on EMNLP,", "citeRegEx": "Martins et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2008}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume 1: Long Papers),", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Hierarchical low-rank tensors for multilingual transfer parsing", "author": ["Zhang", "Barzilay2015] Yuan Zhang", "Regina Barzilay"], "venue": "In Proc. of the 2015 Conference on EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing", "author": ["Zhang", "Clark2008] Yue Zhang", "Stephen Clark"], "venue": "In Proc. of the 2008 Conference on EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Jointly or separately: Which is better for parsing heterogeneous dependencies", "author": ["Zhang et al.2014] Meishan Zhang", "Wanxiang Che", "Yanqiu Shao", "Ting Liu"], "venue": "In Proc. of COLING", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "A neural probabilistic structured-prediction model for transition-based dependency parsing", "author": ["Zhou et al.2015] Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume 1: Long Papers),", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widelyused Penn Treebank (Marcus et al., 1993).", "startOffset": 171, "endOffset": 192}, {"referenceID": 25, "context": "To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasisynchronous grammar features (Li et al.", "startOffset": 160, "endOffset": 178}, {"referenceID": 21, "context": ", 2009), quasisynchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models.", "startOffset": 43, "endOffset": 60}, {"referenceID": 19, "context": ", 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models.", "startOffset": 42, "endOffset": 59}, {"referenceID": 18, "context": "Cross-lingual supervision has proven highly beneficial for low-resource language parsing (Hwa et al., 2005; McDonald et al., 2011), implying that different languages have a great deal of common ground in grammars.", "startOffset": 89, "endOffset": 130}, {"referenceID": 24, "context": "Cross-lingual supervision has proven highly beneficial for low-resource language parsing (Hwa et al., 2005; McDonald et al., 2011), implying that different languages have a great deal of common ground in grammars.", "startOffset": 89, "endOffset": 130}, {"referenceID": 22, "context": "Niu et al. (2009) automatically convert the dependency-structure CDT into the phrase-structure style of CTB5 using a trained constituency parser on CTB5, and then combined the converted treebanks for constituency parsing.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "Li et al. (2012) capture the annotation inconsistencies among different treebanks by designing several types of transformation patterns, based on which they introduce quasi-synchronous grammar features (Smith and Eisner, 2009) to augment the baseline parsing models.", "startOffset": 0, "endOffset": 17}, {"referenceID": 19, "context": "Johansson (2013) also adopts the idea of parameter sharing to incorporate multiple treebanks.", "startOffset": 0, "endOffset": 17}, {"referenceID": 39, "context": ", 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014).", "startOffset": 53, "endOffset": 96}, {"referenceID": 18, "context": "inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al.", "startOffset": 74, "endOffset": 136}, {"referenceID": 33, "context": "inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al.", "startOffset": 74, "endOffset": 136}, {"referenceID": 24, "context": ", 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015).", "startOffset": 70, "endOffset": 161}, {"referenceID": 32, "context": ", 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015).", "startOffset": 70, "endOffset": 161}, {"referenceID": 12, "context": ", 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015).", "startOffset": 70, "endOffset": 161}, {"referenceID": 6, "context": "Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning.", "startOffset": 67, "endOffset": 131}, {"referenceID": 17, "context": "Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning.", "startOffset": 67, "endOffset": 131}, {"referenceID": 14, "context": "There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012).", "startOffset": 124, "endOffset": 186}, {"referenceID": 20, "context": "There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012).", "startOffset": 124, "endOffset": 186}, {"referenceID": 14, "context": "There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings.", "startOffset": 125, "endOffset": 326}, {"referenceID": 14, "context": "There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings.", "startOffset": 125, "endOffset": 434}, {"referenceID": 14, "context": "There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010).", "startOffset": 125, "endOffset": 556}, {"referenceID": 8, "context": "Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.", "startOffset": 0, "endOffset": 169}, {"referenceID": 11, "context": "This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al.", "startOffset": 165, "endOffset": 184}, {"referenceID": 1, "context": ", 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015).", "startOffset": 57, "endOffset": 83}, {"referenceID": 1, "context": ", 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks.", "startOffset": 58, "endOffset": 152}, {"referenceID": 11, "context": "More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016).", "startOffset": 77, "endOffset": 155}, {"referenceID": 36, "context": "More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016).", "startOffset": 77, "endOffset": 155}, {"referenceID": 40, "context": "More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016).", "startOffset": 77, "endOffset": 155}, {"referenceID": 11, "context": "Note that the subtrees within the stack and buffer are modeled with a recursive neural network (RecNN) as described in Dyer et al. (2015). Next, a linear mapping (W) is applied to the concatenation of st,bt and at, and passed through a component-wise ReLU:", "startOffset": 119, "endOffset": 138}, {"referenceID": 28, "context": "We apply the non-projective transition system originally introduced by Nivre (2009) since most of the treebanks we consider in this study has a noticeable proportion of non-projective trees.", "startOffset": 71, "endOffset": 84}, {"referenceID": 8, "context": "For example, Collobert and Weston (2008) only shares word embeddings, and Dong et al. (2015) shares the encoder of sequence-to-sequence models.", "startOffset": 74, "endOffset": 93}, {"referenceID": 29, "context": "Multilingual universal treebanks are annotated with the same set of POS tags (Petrov et al., 2012), dependency relations, and thus share the same set of transition actions.", "startOffset": 77, "endOffset": 98}, {"referenceID": 29, "context": "For simplicity reasons, we convert the language-specific POS tags in the heterogeneous treebanks into universal POS tags (Petrov et al., 2012).", "startOffset": 121, "endOffset": 142}, {"referenceID": 20, "context": "For monolingual heterogeneous source, we also experiment on CTB5 using CDT as the source treebank, to compare with the previous work of Li et al. (2012). Statistics of the datasets are summarized in Table 2.", "startOffset": 136, "endOffset": 153}, {"referenceID": 20, "context": "We follow the same settings of Li et al. (2012), and consider two scenarios using automatic POS tags and gold-standard POS tags respectively.", "startOffset": 31, "endOffset": 48}, {"referenceID": 9, "context": "Similar approach was studied in Duong et al. (2015a) and Guo et al.", "startOffset": 32, "endOffset": 53}, {"referenceID": 9, "context": "Similar approach was studied in Duong et al. (2015a) and Guo et al. (2016) for low-resource parsing.", "startOffset": 32, "endOffset": 75}, {"referenceID": 9, "context": "For MULTILINGUAL (UNIV\u2192UNIV), we also compare with the shallow multi-task learning (SMTL) system, as described in Section 2, which is representative of the approach of Duong et al. (2015b) and Ammar et al.", "startOffset": 168, "endOffset": 189}, {"referenceID": 9, "context": "For MULTILINGUAL (UNIV\u2192UNIV), we also compare with the shallow multi-task learning (SMTL) system, as described in Section 2, which is representative of the approach of Duong et al. (2015b) and Ammar et al. (2016). In SMTL all the parameters are shared except the character embeddings (Echar), and task embeddings (e) are not used.", "startOffset": 168, "endOffset": 213}, {"referenceID": 9, "context": "For MULTILINGUAL (UNIV\u2192UNIV), we also compare with the shallow multi-task learning (SMTL) system, as described in Section 2, which is representative of the approach of Duong et al. (2015b) and Ammar et al. (2016). In SMTL all the parameters are shared except the character embeddings (Echar), and task embeddings (e) are not used. Unlike Duong et al. (2015b) and Ammar et al.", "startOffset": 168, "endOffset": 359}, {"referenceID": 9, "context": "For MULTILINGUAL (UNIV\u2192UNIV), we also compare with the shallow multi-task learning (SMTL) system, as described in Section 2, which is representative of the approach of Duong et al. (2015b) and Ammar et al. (2016). In SMTL all the parameters are shared except the character embeddings (Echar), and task embeddings (e) are not used. Unlike Duong et al. (2015b) and Ammar et al. (2016), we don\u2019t use external resources such as cross-lingual word clusters, embeddings and dictionaries which is beyond the scope of this work.", "startOffset": 168, "endOffset": 383}, {"referenceID": 9, "context": "To verify the second issue, we consider a low resource setup following Duong et al. (2015b), where the target language has a small treebank (3K tokens).", "startOffset": 71, "endOffset": 92}, {"referenceID": 9, "context": "shared by Duong et al. (2015b) on DE, ES and FR.", "startOffset": 10, "endOffset": 31}, {"referenceID": 21, "context": "sibling and grandparent structures, while LI12-O2SIB only use the sibling parts (Li et al., 2012).", "startOffset": 80, "endOffset": 97}, {"referenceID": 21, "context": "For CTB5, we follow (Li et al., 2012) and consider two scenarios which use automatic POS tags and gold-standard POS tags respectively.", "startOffset": 20, "endOffset": 37}], "year": 2016, "abstractText": "Various treebanks have been released for dependency parsing. Despite that treebanks may belong to different languages or have different annotation schemes, they contain syntactic knowledge that is potential to benefit each other. This paper presents an universal framework for exploiting these multi-typed treebanks to improve parsing with deep multitask learning. We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. Multiple treebanks are trained jointly and interacted with multi-level parameter sharing. Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models.", "creator": "LaTeX with hyperref package"}}}