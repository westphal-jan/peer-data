{"id": "1606.01305", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We adopt zoneout, own novel theory for regularizing RNNs. At over timestep, zoneout ptpmu guerrillas some presumably 600 to benefit our previous desire. Like dropout, zoneout uses matrix noise to commuters good machismo - dancer, recovery simplification. But late guaranteeing then, dropping pictures allied, gradient provides already carolina explaining each more longer suppressed in while, as part satellite-based decomposition useful customers. We sing however numerical evidence some also RNN regularizers, including impossible abroad today: zoneout gives perhaps quite improvements across difficulty, bonds state - of -, - created results in character - far language algorithms moving the Penn Treebank opcs and choice outcomes full i - continuing Penn Treebank include codice_14 sequential MNIST classification tasks.", "histories": [["v1", "Fri, 3 Jun 2016 23:31:47 GMT  (877kb,D)", "http://arxiv.org/abs/1606.01305v1", null], ["v2", "Mon, 13 Jun 2016 18:59:48 GMT  (877kb,D)", "http://arxiv.org/abs/1606.01305v2", null], ["v3", "Wed, 18 Jan 2017 03:12:03 GMT  (1270kb,D)", "http://arxiv.org/abs/1606.01305v3", null], ["v4", "Fri, 22 Sep 2017 20:43:09 GMT  (1270kb,D)", "http://arxiv.org/abs/1606.01305v4", "David Krueger and Tegan Maharaj contributed equally to this work"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["david krueger", "tegan maharaj", "j\\'anos kram\\'ar", "mohammad pezeshki", "nicolas ballas", "nan rosemary ke", "anirudh goyal", "yoshua bengio", "aaron courville", "chris pal"], "accepted": true, "id": "1606.01305"}, "pdf": {"name": "1606.01305.pdf", "metadata": {"source": "CRF", "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "authors": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville", "Chris Pal"], "emails": ["firstname.lastname@umontreal.ca.", "firstname.lastname@polymtl.ca.", "hlarochelle@twitter.com."], "sections": [{"heading": null, "text": "forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find encouraging results: zoneout gives significant performance improvements across tasks, yielding state-ofthe-art results in character-level language modeling on the Penn Treebank dataset and competitive results on word-level Penn Treebank and permuted sequential MNIST classification tasks."}, {"heading": "1 Introduction", "text": "We address the issue of regularization in recurrent neural networks (RNNs). Recurrent nets have recently been used to achieve state-of-the-art results in many machine learning domains including speech recognition [Miao et al., 2015], computer vision [Yao et al., 2015], language modeling [Kim et al., 2015] and machine translation [Bahdanau et al., 2014]. Solving machine learning tasks requires high capacity models, such as RNNs, that are flexible enough to capture complex relationships in the data. Regularizing neural networks often yields large performance gains when data is a limiting factor, as indicated by the frequent use of early stopping.\nThe use of RNNs is also motivated by their ability to sequentially construct fixed-length representations of arbitrary-length sequences by folding new observations into their hidden state using an input-dependent transition operator. The repeated application of the same transition operator, however, can make the dynamics of an RNN sensitive to minor perturbations in the hidden state.\nMotivated by the desire to improve robustness in the hidden dynamics of RNNs, and inspired by dropout [Hinton et al., 2012, Srivastava et al., 2014], we propose a novel regularizer, called zoneout, which stochastically modifies the transition operator during training. Instead of setting some units\u2019 activations to 0, as in dropout, zoneout replaces some units\u2019 activations with their activations from the previous time-step. As in dropout, we use the expectation of the random noise at test time.\nCompared with dropout, zoneout is appealing because it preserves information flow forwards and backwards through the network. Moreover, RNNs are frequently trained with output parameters which are shared across time-steps1, and this parameter sharing encourages recurrent units\u2019 activations to be semantically consistent\n1This is the case both when training with targets at every time-step, e.g. as in language modelling, or with a single target at the end of a variable-length sequence, e.g. as in sentiment analysis.\nar X\niv :1\n60 6.\n01 30\n5v 1\n[ cs\n.N E\n] 3\nacross time. Thus zoning out a unit in an RNN preserves not only its activation, but its meaning as well. We empirically evaluate zoneout on classification using the permuted sequential MNIST dataset, and on language modeling using the Penn Treebank dataset, where we demonstrate competitive performance. In particular, we show the benefit of our approach compared with the most successful application of dropout on recurrent connections [Semeniuta et al., 2016] and other proposed regularizers for RNNs."}, {"heading": "2 Related work", "text": "Concurrent with our work, Singh et al. [2016] propose zoneout for feedforward nets, calling the technique SkipForward. In their experiments on feedforward nets, zoneout is outperformed by stochastic depth, dropout, and their proposed swapout technique, which randomly drops either or both of the identity or residual connections, and gives the best performance on image recognition tasks. Contrarily, we find that zoneout outperforms stochastic depth and recurrent dropout in our experiments with RNNs."}, {"heading": "2.1 Relationship to dropout", "text": "Zoneout is named after the popular dropout [Srivastava et al., 2014] regularization technique. In zoneout, instead of dropping out (being set to 0), units zone out and are set to their previous value (ht = ht\u22121). Zoneout, like dropout, trains a pseudo-ensemble [Bachman et al., 2014], but uses a stochastic \u201cidentity-mask\u201d rather than a zero-mask. We conjecture that identity-masking is more appropriate for RNNs, since it makes it easier for the network to preserve information from previous time-steps going forwards, and it facilitates, rather than hinders, the flow of gradient information going backwards. Zoneout can also be seen as a selective application of dropout to only some of the nodes in a modified computational graph, as shown in Figure 1. This justifies our use of the expected identity-mask at test time."}, {"heading": "2.2 Dropout in RNNs", "text": "Initially successful applications of dropout in RNNs [Pham et al., 2013, Zaremba et al., 2014] only apply dropout to feed-forward connections (\u201cup the stack\u201d), and not recurrent connections (\u201cforward through time\u201d). Bayer et al. [2013] successfully apply fast dropout [Wang and Manning, 2013], a deterministic approximation of dropout, to RNNs. We now describe several recent works [Semeniuta et al., 2016, Moon et al., 2015, Gal, 2015] that propose variants of dropout for use in RNNs.\nSemeniuta et al. [2016] apply recurrent dropout to the updates to LSTM memory cells (or GRU states), i.e. they drop out the input/update gate in LSTM/GRU. Their motivation is to prevent the loss of long-term memories stored in the memory cells of an LSTM (or the states of a GRU). Their proposed method is similar to zoneout applied to these models. Zoneout differs, however, in that zoned-out units\u2019 states are preserved exactly. This difference is most salient when zoning out the hidden states (not the memory cells) of an LSTM, for which there is no analogue in recurrent dropout. Whereas saturated output gates or output nonlinearities would cause recurrent dropout to suffer from the vanishing gradient problem [Bengio et al., 1994], the identity-masked units in zoneout still propagate gradients effectively in this situation. Furthermore,\nwhile the recurrent dropout method is specific to LSTMs and GRUs, zoneout generalizes their intuition to any model that sequentially builds distributed representations of its input.\nAlso motivated by preventing memory loss, Moon et al. [2015] propose rnnDrop. This technique amounts to using the same dropout mask at every time-step, which the authors show results in improved performance on speech recognition in their experiments. Gal [2015] proposed the same technique and evaluated it on natural language processing tasks. His motivation, however, came from a variational inference perspective, i.e. he views dropout masks as applied to parameters rather than hidden units."}, {"heading": "2.3 Relationship to Stochastic Depth", "text": "Zoneout can also be viewed as a per-unit version of stochastic depth [Huang et al., 2016]. Recently introduced as a method to efficiently train very deep feed-forward neural networks, stochastic depth randomly drops entire layers. To our knowledge, this method has not been extended to a recurrent setting. This is equivalent to zoning out all of the units of a layer at the same a time in a feedforward network. In a typical RNN, there is a new input at each time-step, causing issues for a naive implementation of stochastic depth. Firstly, Huang et al. [2016] train deep residual networks (ResNets [He et al., 2015]) and preserve the identity connections when dropping a layer, so that the inputs from the previous layer are fed into the next layer in the stack. Including such residual connections in recurrent nets led to instability in our experiments, presumably due to the parameter sharing in RNNs. Zoning out an entire layer in an RNN also means the input at the corresponding time-step is completely ignored, whereas zoning out individual units allows the RNN to process each element of its input sequence."}, {"heading": "3 Zoneout and preliminaries", "text": "We now explain zoneout in full detail, and compare with other forms of dropout in RNNs. We start by reviewing recurrent neural networks (RNNs)."}, {"heading": "3.1 Recurrent Neural Networks", "text": "Recurrent neural networks process data x1, x2, . . . , xT sequentially, constructing a corresponding sequence of representations, h1, h2, . . . , hT . The transition operator, T , of a recurrent neural network converts the present hidden state and input into a new hidden state: ht = Tt(ht\u22121, xt). Thus each hidden state is trained (implicitly) to remember and emphasize all task-relevant aspects of the preceding inputs, and to incorporate new inputs into its representation of state.\nAnother way to see zoneout is as a modification of the transition operator, similar to dropout. For both methods, the masking coefficient, dt, is a Bernoulli random vector sampled independently at each time-step during training, and is replaced by its expectation at test time. Then, zoneout mixes the original transition operator T\u0303t with the identity operator, as opposed to the null operator used in dropout:\nZoneout: Tt = dt T\u0303t + (1\u2212 dt) 1 Dropout: Tt = dt T\u0303t + (1\u2212 dt) 0\nIn simple RNNs, the hidden state is computed by a standard \u201clinear layer\u201d, i.e., an affine transform followed by a nonlinearity \u03c6:\nht = \u03c6(Whht\u22121 +Wxxt + b),\nA naive application of dropout here is to simply apply a stochastic zero-mask to the updated ht; zoneout is similarly straightforward."}, {"heading": "3.2 Long short-term memory", "text": "In long short-term memory RNNs (LSTMs), the hidden state is divided into memory cells ct which are intended for internal long-term storage, and hidden state, ht which is used as a transient representation of state at time-step t. In an LSTM, ct and ht are computed via a set of four \u201cgates\u201d, including the forget gate, ft, which directly connects ct to the memories of the previous time-step ct\u22121, via an element-wise multiplication. Large values of the (ironically named) forget gate cause the cell to remember most (but not all) of its previous value. The other gates control the flow of information in (it, gt) and out (ot) of the cell. Each gate has its own weight matrices and bias vector; for example the forget gate has Wxf , Whf , and bf . For brevity, we will write these as Wx,Wh, b.\nAn LSTM is defined as follows:\nit, ft, ot = \u03c3(Wxxt +Whht\u22121 + b)\ngt = tanh(Wxgxt +Whght\u22121 + bg)\nct = ft ct\u22121 + it gt ht = ot tanh(ct)\nA naive application of dropout in LSTMs zero-masks either or both of the memory cells and hidden states. For example, here is an LSTM with dropout applied to the memory cells:\nit, ft, ot = \u03c3(Wxxt +Whht\u22121 + b)\ngt = tanh(Wxgxt +Whght\u22121 + bg)\nct = dt (ft ct\u22121 + it gt) ht = ot tanh(ct)\nAlternatives abound, however; masks can be applied to any subset of the gates, cells, and states. Recently, Semeniuta et al. [2016] proposed zero-masking the input gate (which they call recurrent dropout):\nit, ft, ot = \u03c3(Wxxt +Whht\u22121 + b)\ngt = tanh(Wxgxt +Whght\u22121 + bg)\nct = (ft ct\u22121 + dt it gt) ht = ot tanh(ct)\nWhen the input gate is masked like this, there is no additive contribution from the input or hidden state, and the value of the memory cell simply decays according to the forget gate.\n3.3 Zoneout in LSTMs\nIn zoneout, the values of the hidden state and memory cell may stick to their previous value (stochastically), or be updated as usual. This introduces stochastic identity connections between subsequent time steps:\nct = dt ct\u22121 + (1\u2212 dt) ( ft ct\u22121 + it gt ) ht = dt ht\u22121 + (1\u2212 dt) ( ot tanh ( ft ct\u22121 + it gt\n)) We also experiment with a variant of recurrent dropout that reuses the input dropout mask to zoneout\nthe corresponding output gates:\nit, ft, ot = \u03c3(Wxxt +Whht\u22121 + b)\ngt = tanh(Wxgxt +Whght\u22121 + bg)\nct = (ft ct\u22121 + dt it gt) ht = ((1\u2212 dt) ot + dt ot\u22121) tanh(ct)\nThe motivation for this variant is to prevent the network from being forced (by the output gate) to expose a memory cell which has not been updated, and hence may contain misleading information."}, {"heading": "4 Experiments and Discussion", "text": "We evaluate zoneout\u2019s performance on the following tasks:\n\u2022 Classification of hand-written digits on permuted sequential MNIST [Le et al., 2015].\n\u2022 Word-level language modeling on the Penn Treebank corpus [Marcus et al., 1993].\n\u2022 Character-level language modeling on the Penn Treebank corpus\nWe first investigate zoneout with a shared zoneout mask on cells and hiddens on the above tasks and compare its performance with other regularizers. In order to better study the dynamics of memory in LSTM and zoneout\u2019s effects on those dynamics, we also explore models with different zoneout probabilities on cells and hidden states."}, {"heading": "4.1 Sequential Permuted MNIST", "text": "In sequential MNIST, pixels of an image representing a number [0-9] are presented to a recurrent neural network one at a time, in lexographic order (left to right, top to bottom). The task is to classify the number shown in the image. In permuted sequential MNIST (pMNIST), the pixels are presented in a (fixed) random order.\nWe compare recurrent dropout and zoneout to a vanilla LSTM baseline on this task. All models are trained for 150 epochs using RMSProp [Tieleman and Hinton, 2012] with a decay rate of 0.5 for the moving average of gradient norms. The learning rate is set to 0.001 and the gradients are clipped to a maximum norm of 1.0 [Pascanu et al., 2013]. The recurrent dropout probability and zoneout probabilites are both set to 0.15. As shown in Table 1, zoneout gives a significant performance boost compared to the LSTM baseline and outperforms recurrent dropout, although recurrent batch normalization outperforms all three. Figure 3 shows the training and validation curves for this task."}, {"heading": "4.2 Penn Treebank Language Modeling Dataset", "text": "The Penn Treebank language model corpus contains 1 million words with predefined training, validation and test splits. The model is trained to predict the next word or character, and is evaluated based on the perplexity for word-level, or bits per character (BPC) for character-level, as is typical for these tasks2.\n2 These metrics are deterministic functions of the negative log-likelihood (NLL). Specifically, perplexity is the exponentiated NLL, and BPC is NLL divided by the natural logarithm of 2."}, {"heading": "4.2.1 Character-level", "text": "For the character-level task, we use an LSTM with one hidden layer with 1000 units. We train on nonoverlapping sequences of 100 in batches of 32. We optimize using Adam with a learning rate of 0.002 and gradient clipping with threshold 1.\nFigure 4a shows our exploration of different zoneout models, with different probabilities of zoneout for cells and hiddens. Results are reported in 2; we find that zoneout on cells with probability 0.5 and zoneout on states with probability 0.05 both outperform recurrent dropout, and combining these leads to our best-performing model (zc = 0.5, zh = 0.05).\nWe compare zoneout to recurrent dropout (for p \u2208 {.05, .2, .5, .7}), weight noise (\u03c3 = 0.075), and norm stabilizer (\u03b2 = 50) [Krueger and Memisevic, 2015]. Figure 4b shows the best performance achieved with each of these regularizers, as well as an unregularized LSTM baseline.\nZoneout with probability zc = 0.5 on cells and zh = 0.05 on hiddens achieves state-of-the-art BPC of 1.29. These results are shown in Table 2. By training on overlapping sequences, we are able to further improve this result to 1.27 BPC."}, {"heading": "4.2.2 Word-level", "text": "For the word-level task, we use similar settings to Semeniuta et al. [2016] and Mikolov et al. [2014]. Unlike these previous works, however, we slice the data into non-overlapping sequences, and we do not use the last hidden states from one batch to initialize the next batch\u2019s hidden states. This means we effectively train on\nless data, and so our results are not directly comparable to those reported in Semeniuta et al. [2016]. The model is an LSTM with a single hidden layer of 256 units and weights initialized uniformly between -0.05 and +0.05. We train on non-overlapping sequences of length 35, in batches of 32, and optimize using SGD with momentum of 0.99 and gradient clipping threshold of 10. We start with learning rate of 1, reducing it by a factor of 1.5 after every epoch without an improvement in validation performance after Semeniuta et al. [2016].\nWe perform a grid search over zoneout/dropout probabilities {0.5, 0.25, 0.05} for the following methods: zoneout on cells, zoneout on hiddens, zoneout on hiddens and cells (same p, different masks), recurrent dropout, recurrent dropout with output zoneout, and stochastic depth. We also compare with norm-stabilizer (\u03b2 = 50), weight noise (\u03c3 = 0.075), and an an unregularized vanilla LSTM. We report the best performance achieved with a given technique in Table 3. The top three performing models all used some form of zoneout."}, {"heading": "5 Conclusion", "text": "We have introduced zoneout, a novel way to regularize RNNs. Instead of dropping out hidden units, we zoneout hidden units, stochastically replacing their activations with the activations from one timestep earlier. We find zoneout brings significant performance improvements on the permuted sequential MNIST and Penn Treebank datasets, outperforming alternative regularizers, and achieving state of the art performance on character-level language modelling.\nWe conjecture that the improved generalization of zoneout arises from two main factors:\n\u2022 The stochasticity introduced by zoneout makes the network more robust to changes in the hidden state.\n\u2022 The identity connections of zoneout improve the flow of information through the network.\nFuture work could allow recurrent networks to adaptively set the probabilities of updating various units based on the input sequence. This could be used to learn a dynamic version of the hierarchical RNN proposed by Hihi and Bengio [1996], where the network would learn when to update different dimensions of its hidden state, which could be used to store information about different time-scales."}, {"heading": "Acknowledgments", "text": "We thank the developers of Theano [Theano Development Team, 2016], as well as Fuel and Blocks [van Merri\u00ebnboer et al., 2015]. We also acknowledge the computing resources provided by ComputeCanada and CalculQuebec. We are grateful to students at MILA, especially \u00c7a\u011flar G\u00fcl\u00e7ehre, Marcin Moczulski, Chiheb Trabelsi, and Christopher Beckham, for feedback and helpful discussions. We also thank IBM and Samsung for their support.\nThis research was developed with funding from the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government3."}], "references": [{"title": "Learning with pseudo-ensembles", "author": ["Philip Bachman", "Ouais Alsharif", "Doina Precup"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bachman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bachman et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "author": ["J. Bayer", "C. Osendorfer", "D. Korhammer", "N. Chen", "S. Urban", "P. van der Smagt"], "venue": null, "citeRegEx": "Bayer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bayer et al\\.", "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarvin Gal"], "venue": "ArXiv e-prints,", "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["Salah El Hihi", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hihi and Bengio.,? \\Q1996\\E", "shortCiteRegEx": "Hihi and Bengio.", "year": 1996}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Regularizing rnns by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1511.08400,", "citeRegEx": "Krueger and Memisevic.,? \\Q2015\\E", "shortCiteRegEx": "Krueger and Memisevic.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "arXiv preprint arXiv:1507.08240,", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Rnndrop: A novel dropout for rnns in asr", "author": ["Taesup Moon", "Heeyoul Choi", "Hoshik Lee", "Inchul Song"], "venue": "Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Moon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moon et al\\.", "year": 2015}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1312.6026,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Dropout improves Recurrent Neural Networks for Handwriting Recognition", "author": ["V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "ArXiv e-prints,", "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "Recurrent dropout without memory loss", "author": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth"], "venue": "arXiv preprint arXiv:1603.05118,", "citeRegEx": "Semeniuta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Semeniuta et al\\.", "year": 2016}, {"title": "Swapout: Learning an ensemble of deep architectures", "author": ["S. Singh", "D. Hoiem", "D. Forsyth"], "venue": "ArXiv e-prints,", "citeRegEx": "Singh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio"], "venue": "CoRR, abs/1506.00619,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Fast dropout training", "author": ["Sida Wang", "Christopher Manning"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Wang and Manning.,? \\Q2013\\E", "shortCiteRegEx": "Wang and Manning.", "year": 2013}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Recurrent nets have recently been used to achieve state-of-the-art results in many machine learning domains including speech recognition [Miao et al., 2015], computer vision [Yao et al.", "startOffset": 137, "endOffset": 156}, {"referenceID": 24, "context": ", 2015], computer vision [Yao et al., 2015], language modeling [Kim et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 9, "context": ", 2015], language modeling [Kim et al., 2015] and machine translation [Bahdanau et al.", "startOffset": 27, "endOffset": 45}, {"referenceID": 1, "context": ", 2015] and machine translation [Bahdanau et al., 2014].", "startOffset": 32, "endOffset": 55}, {"referenceID": 18, "context": "In particular, we show the benefit of our approach compared with the most successful application of dropout on recurrent connections [Semeniuta et al., 2016] and other proposed regularizers for RNNs.", "startOffset": 133, "endOffset": 157}, {"referenceID": 19, "context": "Concurrent with our work, Singh et al. [2016] propose zoneout for feedforward nets, calling the technique SkipForward.", "startOffset": 26, "endOffset": 46}, {"referenceID": 0, "context": "Zoneout, like dropout, trains a pseudo-ensemble [Bachman et al., 2014], but uses a stochastic \u201cidentity-mask\u201d rather than a zero-mask.", "startOffset": 48, "endOffset": 70}, {"referenceID": 23, "context": "[2013] successfully apply fast dropout [Wang and Manning, 2013], a deterministic approximation of dropout, to RNNs.", "startOffset": 39, "endOffset": 63}, {"referenceID": 3, "context": "Whereas saturated output gates or output nonlinearities would cause recurrent dropout to suffer from the vanishing gradient problem [Bengio et al., 1994], the identity-masked units in zoneout still propagate gradients effectively in this situation.", "startOffset": 132, "endOffset": 153}, {"referenceID": 2, "context": "Bayer et al. [2013] successfully apply fast dropout [Wang and Manning, 2013], a deterministic approximation of dropout, to RNNs.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Bayer et al. [2013] successfully apply fast dropout [Wang and Manning, 2013], a deterministic approximation of dropout, to RNNs. We now describe several recent works [Semeniuta et al., 2016, Moon et al., 2015, Gal, 2015] that propose variants of dropout for use in RNNs. Semeniuta et al. [2016] apply recurrent dropout to the updates to LSTM memory cells (or GRU states), i.", "startOffset": 0, "endOffset": 295}, {"referenceID": 14, "context": "Also motivated by preventing memory loss, Moon et al. [2015] propose rnnDrop.", "startOffset": 42, "endOffset": 61}, {"referenceID": 4, "context": "Gal [2015] proposed the same technique and evaluated it on natural language processing tasks.", "startOffset": 0, "endOffset": 11}, {"referenceID": 8, "context": "3 Relationship to Stochastic Depth Zoneout can also be viewed as a per-unit version of stochastic depth [Huang et al., 2016].", "startOffset": 104, "endOffset": 124}, {"referenceID": 5, "context": "[2016] train deep residual networks (ResNets [He et al., 2015]) and preserve the identity connections when dropping a layer, so that the inputs from the previous layer are fed into the next layer in the stack.", "startOffset": 45, "endOffset": 62}, {"referenceID": 7, "context": "3 Relationship to Stochastic Depth Zoneout can also be viewed as a per-unit version of stochastic depth [Huang et al., 2016]. Recently introduced as a method to efficiently train very deep feed-forward neural networks, stochastic depth randomly drops entire layers. To our knowledge, this method has not been extended to a recurrent setting. This is equivalent to zoning out all of the units of a layer at the same a time in a feedforward network. In a typical RNN, there is a new input at each time-step, causing issues for a naive implementation of stochastic depth. Firstly, Huang et al. [2016] train deep residual networks (ResNets [He et al.", "startOffset": 105, "endOffset": 598}, {"referenceID": 18, "context": "Recently, Semeniuta et al. [2016] proposed zero-masking the input gate (which they call recurrent dropout):", "startOffset": 10, "endOffset": 34}, {"referenceID": 18, "context": "Figure 2: (a) Zoneout, vs (b) the recurrent dropout strategy of [Semeniuta et al., 2016] in an LSTM.", "startOffset": 64, "endOffset": 88}, {"referenceID": 11, "context": "We evaluate zoneout\u2019s performance on the following tasks: \u2022 Classification of hand-written digits on permuted sequential MNIST [Le et al., 2015].", "startOffset": 127, "endOffset": 144}, {"referenceID": 12, "context": "\u2022 Word-level language modeling on the Penn Treebank corpus [Marcus et al., 1993].", "startOffset": 59, "endOffset": 80}, {"referenceID": 21, "context": "All models are trained for 150 epochs using RMSProp [Tieleman and Hinton, 2012] with a decay rate of 0.", "startOffset": 52, "endOffset": 79}, {"referenceID": 16, "context": "0 [Pascanu et al., 2013].", "startOffset": 2, "endOffset": 24}, {"referenceID": 10, "context": "075), and norm stabilizer (\u03b2 = 50) [Krueger and Memisevic, 2015].", "startOffset": 35, "endOffset": 64}, {"referenceID": 17, "context": "2 Word-level For the word-level task, we use similar settings to Semeniuta et al. [2016] and Mikolov et al.", "startOffset": 65, "endOffset": 89}, {"referenceID": 14, "context": "[2016] and Mikolov et al. [2014]. Unlike these previous works, however, we slice the data into non-overlapping sequences, and we do not use the last hidden states from one batch to initialize the next batch\u2019s hidden states.", "startOffset": 11, "endOffset": 33}, {"referenceID": 18, "context": "less data, and so our results are not directly comparable to those reported in Semeniuta et al. [2016]. The model is an LSTM with a single hidden layer of 256 units and weights initialized uniformly between -0.", "startOffset": 79, "endOffset": 103}, {"referenceID": 18, "context": "less data, and so our results are not directly comparable to those reported in Semeniuta et al. [2016]. The model is an LSTM with a single hidden layer of 256 units and weights initialized uniformly between -0.05 and +0.05. We train on non-overlapping sequences of length 35, in batches of 32, and optimize using SGD with momentum of 0.99 and gradient clipping threshold of 10. We start with learning rate of 1, reducing it by a factor of 1.5 after every epoch without an improvement in validation performance after Semeniuta et al. [2016]. We perform a grid search over zoneout/dropout probabilities {0.", "startOffset": 79, "endOffset": 540}, {"referenceID": 6, "context": "This could be used to learn a dynamic version of the hierarchical RNN proposed by Hihi and Bengio [1996], where the network would learn when to update different dimensions of its hidden state, which could be used to store information about different time-scales.", "startOffset": 82, "endOffset": 105}], "year": 2016, "abstractText": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find encouraging results: zoneout gives significant performance improvements across tasks, yielding state-ofthe-art results in character-level language modeling on the Penn Treebank dataset and competitive results on word-level Penn Treebank and permuted sequential MNIST classification tasks.", "creator": "LaTeX with hyperref package"}}}