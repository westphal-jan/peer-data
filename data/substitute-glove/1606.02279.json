{"id": "1606.02279", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Semi-supervised structured output prediction by local linear regression and sub-gradient descent", "abstract": "We enacting new non-fiction fourth - established essentially putting - return impact method developed held local linear correlations second beyond paper. The existing semi - empowered structured output underlying methods learn full global unitless for all the data points though small comparison set, which relates way differences of local redundancy major saw input only, and though contrast their put guidelines output prediction. To need most problem, we pave to learn was missing broadly compilers include local neurotrophic for southern of different data points organisations. Using the local formula_12 regression policy, been brought neighbourhoods according hand data point, we cuts to able taken groups linear taker by quantifiable while with complexity same the neurotrophic and present covered being of itself structured comparison loss. The unanticipated instance being solved. writethru - excitation movements algorithms. We activities lab followed two benchmark data sets, especially the changes series the improvements of the outlined uses.", "histories": [["v1", "Tue, 7 Jun 2016 19:52:02 GMT  (80kb)", "https://arxiv.org/abs/1606.02279v1", "arXiv admin note: substantial text overlap witharXiv:1604.03010"], ["v2", "Thu, 7 Jul 2016 14:34:24 GMT  (80kb)", "http://arxiv.org/abs/1606.02279v2", "arXiv admin note: substantial text overlap witharXiv:1604.03010"], ["v3", "Tue, 16 Aug 2016 13:10:04 GMT  (120kb)", "http://arxiv.org/abs/1606.02279v3", "arXiv admin note: substantial text overlap witharXiv:1604.03010"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1604.03010", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ru-ze liang", "wei xie", "weizhi li", "xin du", "jim jing-yan wang", "jingbin wang"], "accepted": false, "id": "1606.02279"}, "pdf": {"name": "1606.02279.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised structured output prediction by local linear regression and sub-gradient descent", "authors": ["Ru-Ze Liang", "Wei Xie", "Weizhi Li", "Jing-Yan Wang", "Jingbin Wang"], "emails": ["ruzeliang@outlook.com", "wei.xie@vanderbilt.edu", "weizhili2014@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n02 27\n9v 3\n[ cs\n.L G\n] 1\n6 A\nI. INTRODUCTION\nTraditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8]. However, in many machine learning applications, the forms of outputs of the prediction are structured. The structured outputs include vector, tree nodes, graph, sequence, etc. For example, in the part-of-speech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [9], [10]. In real-world application, many outputs are not available for the training data points, leading to the problem of semi-supervised structured output prediction. Many works have been done for the problem of semi-supervised structured output prediction. Altun et al. [11] proposed the problem of semi-supervised learning with structured outputs. Brefeld and Scheffer [12] proposed to solve the problem of semisupervised structured output prediction by learning in the space of input-output space, and using co-training method. Suzuki et al. [13] proposed a hybrid method to solve the\nproblem of semi-supervised structured output learning. Jiang et al. [14] proposed to regularize the structured outputs by the manifold constructed from the input space directly.\nAll the above semi-supervised structured output prediction methods learn one single predictor for the entire data set, ignoring the different local distributions of different neighborhoods of data points. However, from our observation, the local distributions of different neighborhoods of the data may be significantly different, and this might have a significant effect to the prediction of structured outputs. Thus using one single global predictor for all the different neighborhoods are not suitable. In this paper, we learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [15], [16], and we also propose to learn the missing structured outputs for a semi- supervised data set simultaneously. For each data point, the local distribution around this data point by its k nearest neighborhood was presented, and we try to model it by learning a local linear structured output predictor. In order to create a local linear structured output predictor, we minimize an upper bound of the structured losses of the data points in this neighborhood, as well as the squared \u21132 norm of the predictor parameter vector. Some data points are shared by different neighborhoods, playing the role of bridging different local distributions. To solve the problem, we develop an iterative algorithm by using the gradient descent method.\nThe rest parts of this paper are organized as follows. In section II, the proposed maximum top precision similarity learning method is introduced. In section III, the experiments in four benchmark data sets are shown. In section IV, the paper is concluded with future works."}, {"heading": "II. PROPOSED METHOD", "text": "Suppose we have a training set X = L \u222a U , where L = {(xi, yi)} l i=1 contains l labeled data points, U = {xi} n i=l+1 contains u = n\u2212 l unlabeled data points, and xi and yi \u2208 Y are the input vector and structured output of the i-th data point respectively. We propose to learn a local predictor for the neighborhood of each data points, and present the neighborhood of the i-th data point as the set of its k nearest neighbors, Ni. Given a candidate structured output, y, and an input feature vector, xj \u2208 Ni, we use a joint representation \u03a6(xj , y) \u2208 Rm to match them, and then we use a local linear predictor to predict the structured outputs for the data points in Ni,\ny\u2217j = argmax y\u2208Y w\u22a4i \u03a6(xj , y), \u2200 j : xj \u2208 Ni. (1)\nWe propose to learn the complete outputs simultaneously, {yi}|ni=1, for all the data points. We also assume that the outputs of the data points in L is equal to the true outputs, i.e., yi = yi, for i : (xi, yi) \u2208 L.\nIn each neighborhood, Ni, we learn wi and yj |j:xj\u2208Ni jointly by minimizing a structured loss function \u2206 and the squared \u21132 norm of wi simultaneously\nmin wi,yj|j:xj\u2208Ni\n1\nk\n\u2211\nj:xj\u2208Ni\n\u2206(yj , y \u2217 j ) +\nC 2 \u2016wi\u201622,\ns.t. yj = yj , j : (xj , yi) \u2208 L,\n(2)\nwhere C is a scale parameter. The upper bound of \u2206(yj , y\u2217j ) is given as\n\u2206(yj , y \u2217 j ) \u2264 w \u22a4 i\n( \u03a6(xj , z\u2217j )\u2212 \u03a6(xj , yj) ) +\u2206(yj , z \u2217 i,j), (3)\nwhere\nz\u2217i,j = argmax y\u2032 j \u2208Y\n[ w\u22a4i ( \u03a6(xj , y\u2032j)\u2212 \u03a6(xj , yj) ) +\u2206(yj , y \u2032 j) ] .\n(4) We approximate the upper bound of the structured loss based on the lower bound approximation method of the structure learning of the Bayesian network [17], [18]. Fan et al. [17] proposed to tighten the upper and lower bounds of the breadthfirst branch and bound algorithm for the learning of Bayesian network structures. The informed variable groupings are used to create the pattern databases to tighten the lower bound, while the anytime learning algorithm is used to tighten the upper bound. These strategies show good performance in the learning process of the Bayesian network structures. The work of [17] is a contribution of major significance to the bound approximation community, and our upper bound approximation method is also based on these strategies. Fan et al. [18] further proposed to improve the lower bound function of static k-cycle conflict heuristic for the learning of Bayesian network structures. This work is used to guild the search of the most promising search spaces. It uses a partition of the random variables of a data set, and their research methodologies are\nbased on the importance of the partition. A new partition method was proposed, and it uses the information extracted from the potentially optimal parent sets.\nThe minimization problem can be transferred to be the following problem,\nmin wi,yj|j:xj\u2208Ni\n1\nk\n\u2211\nj:xj\u2208Ni\n[ w\u22a4i ( \u03a6(xj , z\u2217i,j)\u2212 \u03a6(xj , yj) )\n+\u2206(yj, z \u2217 i,j)\n] + C\n2 \u2016wi\u201622\ns.t. yj = yj , j : (xj , yi) \u2208 L.\n(5)\nWe propose to combine them into one single problem over the entire data set,\nmin (wi,yi)|ni=1\nn \u2211\ni=1\n\n\n1\nk\n\u2211\nj:xj\u2208Ni\n[ w\u22a4i ( \u03a6(xj , z\u2217i,j)\u2212 \u03a6(xj , yj) )\n+\u2206(yj , z \u2217 i,j)\n] + C\n2 \u2016wi\u201622\n)\ns.t. yi = yi, i : (xi, yi) \u2208 L. (6)\nTo solve the problem in (6), we use an iterative algorithm. In each iteration, we first update wi|ni=1, and then update yi| n i=1 one by one.\n\u2022 Updating wi by sub-gradient descent algorithm If we only consider wi|ni=1, the problem in (6) is transferred to\nmin wi|ni=1\nn \u2211\ni=1\n\n\n1\nk\n\u2211\nj:xj\u2208Ni\n[ w\u22a4i ( \u03a6(xj , z\u2217i,j)\u2212 \u03a6(xj , yj) )]\n+ C\n2 \u2016wi\u201622 = g(wi)\n\n ,\n(7) We use the sub-gradient descent algorithm to update wi, and the sub-gradient function of g(wi) is\n\u2207g(wi) = 1\nk\n\u2211\nj:xj\u2208Ni\n[( \u03a6(xj , z\u2217i,j)\u2212 \u03a6(xj , yj) )] + Cwi,\n(8) and wi is updated as follows,\nwi \u2190 wi \u2212 \u03b7\u2207g(wi). (9)\n\u2022 Updating yi|ni=1 If we only consider yi| n i=1, the problem\nof (6) is transferred to\nmin yi|ni=1\nn \u2211\ni=1\n\n\n1\nk\n\u2211\nj:xj\u2208Ni\n[\n\u2206(yj , z \u2217 i,j)\u2212 w \u22a4 i \u03a6(xj , yj)\n]\n\n\ns.t. yi = yi, i : (xj , yi) \u2208 L. (10)\nWe solve the n outputs one by one, and the problem in (10) is reduced to be\nmin yi\n\u2211\ni\u2032:xi\u2208Ni\u2032\n{\n1\nk\n[\n\u2206(yi, z \u2217 i\u2032,i)\u2212 w \u22a4 i\u2032\u03a6(xi, yi)\n]\n}\ns.t. yi = yi, if (xi, yi) \u2208 L,\n(11)\nwith regard to only one output. We discuss the solution of this problem in two cases.\n\u2013 (xi, yi) \u2208 L:\nyi = yi. (12)\n\u2013 xi \u2208 U :\nyi = argmin y\u2208Y\n\u2211\ni\u2032:xi\u2208Ni\u2032\n(\n1\nk\n[\n\u2206(y, z\u2217i\u2032,i)\n\u2212w\u22a4i\u2032\u03a6(xi, y) ]\n)\n.\n(13)\nWe develop an iterative algorithm to learn the local structured output predictor and the outputs jointly, as given in Algorithm 1.\n\u2022 Algorithm 1. Iterative training algorithm of semisupervised learning of local structured output predictor. \u2022 Inputs: Training set, X . \u2022 Inputs: Maximum iteration number, T . \u2022 Initialize (wi, yi)|ni=1; \u2022 For t = 1, \u00b7 \u00b7 \u00b7 , T\n\u2013 Update the upper bound parameters \u2013 For i = 1, \u00b7 \u00b7 \u00b7 , n\n\u2217 For j : xj \u2208 Ni \u2217 Update z\u2217i,j according to (4)\n\u2013 Update the local predictor parameters \u2013 For i = 1, \u00b7 \u00b7 \u00b7 , n \u2013 Update wi according to (9) \u2013 Update the structured outputs \u2013 For i = 1, \u00b7 \u00b7 \u00b7 , n \u2013 Update yi according to (12) and (13)\n\u2022 Output: wi|ni=1."}, {"heading": "III. EXPERIMENTS", "text": ""}, {"heading": "A. Experiment setup", "text": "We use two benchmark data sets in our experiments.\n\u2022 SUN data set:The class labels of this image data set is organized as a tree structure. The tree has 15 different leaves [19]. For each class, we randomly select 200 images from the data set to conduct our experiments, thus there are 3,000 images in our data set in total. The structured output of a data point is a leave of the tree. We code it as a vector x. The joint representation is given as the tensor product of x and y.\n\u03a6(x, y) = x \u2297 y. (14)\nThe loss function between y and y\u2217, \u2206(y, y\u2217), is defined as the height of their first common ancestor. \u2022 Spanish news wire article sentence data set: This data set contains 300 sentences, and each sentence is used as a data point in our experiment [20]. The length of each sentence is 9, and the corresponding output of a sentence, y, is a sequence of labels of non-name and named entities. The joint representation \u03a6(x, y) of a sentence, x, and a sequence of labels, y, is defined as the histogram of state transition, and a set of emission features. The loss function to compare y and y\u2217 is defined as a 0-1 loss.\nWe use the 10-fold cross validation strategy to split the training and test subsets. The training set is also randomly split to be a labeled set and an unlabeled set. The average structured loss over the test set is used to evaluate the performance of the proposed algorithm."}, {"heading": "B. Experimental results", "text": "We compare the proposed algorithm to the algorithms proposed by Altun et al. [11], Brefeld and Scheffer [12], Suzuki et al. [13], and Jiang et al. [14]. Our algorithm is named as semi-supervised local structured output prediction algorithm (SSLSOP). The average losses of the compared algorithms over three different data sets are given in Table I. It is obvious that the proposed algorithm outperforms all competing algorithms significantly.\nWe are also interested in the running time of the proposed method, SSLSOP, and its competing methods. The running time of these methods over four benchmark data sets are given in Fig. 1, which shows that the proposed method, SSLSOP, consumes the second shortest running time over three data\nsets. The least time consuming algorithm is the one proposed by Altun et al. [11], however, its prediction results are not accurate."}, {"heading": "IV. CONCLUSION", "text": "In this paper, we investigate the problem of semi-supervised learning of structured output predictor. To handle the problem of diverse of the local distributions, we propose to learn local structured output predictors for neighborhoods of different data points. Moreover, we also propose to learn the missing outputs of the unlabeled data points. We build a new minimization problem to learn the local structured output predictors and the missing structured outputs simultaneously. This problem is modeled as the joint minimization of the local predictor complexity and the local structured output loss. The problem is optimized by gradient descent, and we design an iterative algorithm to learn the local predictors. The experiments are implemented over benchmark data sets including natural image classification data set and sentence part-of-speech tagging data set. In the future, we will study how to fit the proposed algorithm to big data sets, by using big data processing framework, such as Map-Reduce of Hadoop software. We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43]."}], "references": [{"title": "An effective image representation method using kernel classification", "author": ["H. Wang", "J. Wang"], "venue": "2014 IEEE 26th International Conference on Tools with Artificial Intelligence (ICTAI 2014), 2014, pp. 853\u2013858.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple kernel multivariate performance learning using cutting plane algorithm", "author": ["J. Wang", "H. Wang", "Y. Zhou", "N. McDonald"], "venue": "Systems, Man, and Cybernetics (SMC), 2015 IEEE International Conference on. IEEE, 2015, pp. 1870\u20131875.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised cross-modal factor analysis for multiple modal data classification", "author": ["J. Wang", "Y. Zhou", "K. Duan", "J.J.-Y. Wang", "H. Bensmail"], "venue": "Systems, Man, and Cybernetics (SMC), 2015 IEEE International Conference on. IEEE, 2015, pp. 1882\u20131888.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised learning of sparse context reconstruction coefficients for data representation and classification", "author": ["X. Liu", "J. Wang", "M. Yin", "B. Edwards", "P. Xu"], "venue": "Neural Computing and Applications, pp. 1\u20139, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-kernel learning for multivariate performance measures optimization", "author": ["F. Lin", "J. Wang", "N. Zhang", "J. Xiahou", "N. McDonald"], "venue": "Neural Computing and Applications, pp. 1\u201313, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Doubly regularized portfolio with risk minimization", "author": ["W. Shen", "J. Wang", "S. Ma"], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014, pp. 1286\u20131292.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Transaction costs-aware portfolio optimization via fast lowner-john ellipsoid approximation", "author": ["W. Shen", "J. Wang"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015, pp. 1854\u20131860.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Portfolio choices with orthogonal bandit learning", "author": ["W. Shen", "J. Wang", "Y.-G. Jiang", "H. Zha"], "venue": "Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015, pp. 974\u2013980.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "The relationship of dependency relations and parts of speech in hungarian", "author": ["V. Vincze"], "venue": "Journal of Quantitative Linguistics, vol. 22, no. 1, pp. 44\u201354, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating word embeddings and a revised corpus for part-of-speech tagging in portuguese", "author": ["E. Fonseca", "J. G Rosa", "S. Alusio"], "venue": "Journal of the Brazilian Computer Society, vol. 21, no. 1, p. 14, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximum margin semisupervised learning for structured variables", "author": ["Y. Altun", "M. Belkin", "D.A. Mcallester"], "venue": "Advances in neural information processing systems, 2005, pp. 33\u201340.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Semi-supervised learning for structured output variables", "author": ["U. Brefeld", "T. Scheffer"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 145\u2013152.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised structured output learning based on a hybrid generative and discriminative approach.", "author": ["J. Suzuki", "A. Fujino", "H. Isozaki"], "venue": "EMNLP-CoNLL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Manifold regularization in structured output space for semi-supervised structured output prediction", "author": ["F. Jiang", "L. Jia", "X. Sheng", "R. LeMieux"], "venue": "Neural Computing and Applications, pp. 1\u201310, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised distance metric learning based on local linear regression for data clustering", "author": ["H. Zhang", "J. Yu", "M. Wang", "Y. Liu"], "venue": "Neurocomputing, vol. 93, pp. 100\u2013105, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Local ridge regression for face recognition", "author": ["H. Xue", "Y. Zhu", "S. Chen"], "venue": "Neurocomputing, vol. 72, no. 4, pp. 1342\u20131346, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Tightening bounds for bayesian network structure learning", "author": ["X. Fan", "C. Yuan", "B. Malone"], "venue": "Proceedings of the 28th AAAI Conference on Artificial Intelligence, 2014, pp. 2439 \u2013 2445.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "An improved lower bound for bayesian network structure learning", "author": ["X. Fan", "C. Yuan"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015, pp. 2439 \u2013 2445.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba"], "venue": "Computer vision and pattern recognition (CVPR), 2010 IEEE conference on. IEEE, 2010, pp. 3485\u20133492.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Introduction to the conll-2002 shared task: Languageindependent named entity recognition", "author": ["T.K. Sang"], "venue": "Proceedings of the 6th conference on natural language learning, pp. 155\u2013158.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 0}, {"title": "Computational modeling of magnetic nanoparticle targeting to stent surface under high gradient field", "author": ["S. Wang", "Y. Zhou", "J. Tan", "J. Xu", "J. Yang", "Y. Liu"], "venue": "Computational mechanics, vol. 53, no. 3, pp. 403\u2013412, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Biomarker binding on an antibody-functionalized biosensor surface: the influence of surface properties, electric field, and coating density", "author": ["Y. Zhou", "W. Hu", "B. Peng", "Y. Liu"], "venue": "The Journal of Physical Chemistry C, vol. 118, no. 26, pp. 14 586\u201314 594, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Structure design of vascular stents", "author": ["Y. Liu", "J. Yang", "Y. Zhou", "J. Hu"], "venue": "Multiscale simulations and mechanics of biological materials, pp. 301\u2013 317, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling nanoparticle targeting to a vascular surface in shear flow through diffusive particle dynamics", "author": ["B. Peng", "Y. Liu", "Y. Zhou", "L. Yang", "G. Zhang", "Y. Liu"], "venue": "Nanoscale research letters, vol. 10, no. 1, pp. 1\u20139, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Mechanical response of cardiovascular stents under vascular dynamic bending", "author": ["J. Xu", "J. Yang", "N. Huang", "C. Uhl", "Y. Zhou", "Y. Liu"], "venue": "Biomedical engineering online, vol. 15, no. 1, p. 1, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Inferring clinical workflow efficiency via electronic medical record utilization", "author": ["Y. Chen", "W. Xie", "C.A. Gunter", "D. Liebovitz", "S. Mehrotra", "H. Zhang", "B. Malin"], "venue": "AMIA Annual Symposium Proceedings, vol. 2015. American Medical Informatics Association, 2015, p. 416.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Supporting regularized logistic regression privately and efficiently", "author": ["W. Li", "H. Liu", "P. Yang", "W. Xie"], "venue": "PloS one, vol. 11, no. 6, p. e0156479, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Securema: protecting participant privacy in genetic association meta-analysis", "author": ["W. Xie", "M. Kantarcioglu", "W.S. Bush", "D. Crawford", "J.C. Denny", "R. Heatherly", "B.A. Malin"], "venue": "Bioinformatics, p. btu561, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilayer joint gait-pose manifolds for human gait motion modeling", "author": ["M. Ding", "G. Fan"], "venue": "IEEE Transactions on Cybernetics, vol. 45, no. 11, pp. 2413\u20132424, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Articulated and generalized gaussian kernel correlation for human pose estimation", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Image Processing, vol. 25, no. 2, pp. 776\u2013789, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Image annotation with incomplete labelling by modelling image specific structured loss", "author": ["X. Xu", "A. Shimada", "H. Nagahara", "R.-i. Taniguchi", "L. He"], "venue": "IEEJ Transactions on Electrical and Electronic Engineering, vol. 11, no. 1, pp. 73\u201382, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Surgical wound debridement sequentially characterized in a porcine burn model with multispectral imaging", "author": ["D.R. King", "W. Li", "J.J. Squiers", "R. Mohan", "E. Sellke", "W. Mo", "X. Zhang", "W. Fan", "J.M. DiMaio", "J.E. Thatcher"], "venue": "Burns, vol. 41, no. 7, pp. 1478\u20131487, 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Outlier detection and removal improves accuracy of machine learning approach to multispectral burn diagnostic imaging", "author": ["W. Li", "W. Mo", "X. Zhang", "J.J. Squiers", "Y. Lu", "E.W. Sellke", "W. Fan", "J.M. DiMaio", "J.E. Thatcher"], "venue": "Journal of biomedical optics, vol. 20, no. 12, pp. 121 305\u2013 121 305, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Multispectral and photoplethysmography optical imaging techniques identify important tissue characteristics in an animal model of tangential burn excision", "author": ["J.E. Thatcher", "W. Li", "Y. Rodriguez-Vaqueiro", "J.J. Squiers", "W. Mo", "Y. Lu", "K.D. Plant", "E. Sellke", "D.R. King", "W. Fan"], "venue": "Journal of Burn Care & Research, vol. 37, no. 1, pp. 38\u201352, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Burn injury diagnostic imaging device\u2019s accuracy improved by outlier detection and removal", "author": ["W. Li", "W. Mo", "X. Zhang", "Y. Lu", "J.J. Squiers", "E.W. Sellke", "W. Fan", "J.M. DiMaio", "J.E. Thatcher"], "venue": "SPIE Defense+ Security. International Society for Optics and Photonics, 2015, pp. 947 206\u2013947 206.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Multispectral imaging burn wound tissue classification system: a comparison of test accuracies of several common machine learning algorithms", "author": ["J.J. Squiers", "W. Li", "D.R. King", "W. Mo", "X. Zhang", "Y. Lu", "E.W. Sellke", "W. Fan", "J.M. DiMaio", "J.E. Thatcher"], "venue": "2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Margin-based over-sampling method for learning from imbalanced datasets", "author": ["X. Fan", "K. Tang", "T. Weise"], "venue": "Proceedings of the 15th Pacific- Asia Conference on Knowledge Discovery and Data Mining (PAKDD- 2011). Springer Berlin Heidelberg, 2011, pp. 309\u2013320.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding optimal bayesian network structures with constraints learned from data", "author": ["X. Fan", "B. Malone", "C. Yuan"], "venue": "Proceed. of the 30th Conf. on Uncertainty in Artificial Intelligence (UAI-2014), 2014, pp. 200\u2013209.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Enhanced maximum auc linear classifier", "author": ["X. Fan", "K. Tang"], "venue": "Fuzzy Systems and Knowledge Discovery (FSKD), 2010 Seventh International Conference on, vol. 4. IEEE, 2010, pp. 1540\u20131544.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "A new variable sampling control scheme at fixed times for monitoring the process dispersion", "author": ["L. Shi", "C. Zou", "Z. Wang", "K.C. Kapur"], "venue": "Quality and Reliability Engineering International, vol. 25, no. 8, pp. 961\u2013972, 2009.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "A synthesis of feedback and feedforward control for process improvement under stationary and nonstationary time series disturbance models", "author": ["L. Shi", "K.C. Kapur"], "venue": "Quality and Reliability Engineering International, vol. 31, no. 3, pp. 343\u2013354, 2015.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Semisupervised coupled dictionary learning for cross-modal retrieval in internet images and texts", "author": ["X. Xu", "Y. Yang", "A. Shimada", "R.-i. Taniguchi", "L. He"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference. ACM, 2015, pp. 847\u2013850.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimizing top precision performance measure of content-based image retrieval by learning similarity function", "author": ["R.-Z. Liang", "L. Shi", "H. Wang", "J. Meng", "J.J.-Y. Wang", "Q. Sun", "Y. Gu"], "venue": "Pattern Recognition (ICPR), 2016 23st International Conference on. IEEE, 2016.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].", "startOffset": 136, "endOffset": 139}, {"referenceID": 7, "context": "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].", "startOffset": 141, "endOffset": 144}, {"referenceID": 8, "context": "For example, in the part-of-speech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [9], [10].", "startOffset": 240, "endOffset": 243}, {"referenceID": 9, "context": "For example, in the part-of-speech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [9], [10].", "startOffset": 245, "endOffset": 249}, {"referenceID": 10, "context": "[11] proposed the problem of semi-supervised learning with structured outputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Brefeld and Scheffer [12] proposed to solve the problem of semisupervised structured output prediction by learning in the space of input-output space, and using co-training method.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "[13] proposed a hybrid method to solve the problem of semi-supervised structured output learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed to regularize the structured outputs by the manifold constructed from the input space directly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In this paper, we learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [15], [16], and we also propose to learn the missing structured outputs for a semi- supervised data set simultaneously.", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "In this paper, we learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [15], [16], and we also propose to learn the missing structured outputs for a semi- supervised data set simultaneously.", "startOffset": 203, "endOffset": 207}, {"referenceID": 16, "context": "(4) We approximate the upper bound of the structured loss based on the lower bound approximation method of the structure learning of the Bayesian network [17], [18].", "startOffset": 154, "endOffset": 158}, {"referenceID": 17, "context": "(4) We approximate the upper bound of the structured loss based on the lower bound approximation method of the structure learning of the Bayesian network [17], [18].", "startOffset": 160, "endOffset": 164}, {"referenceID": 16, "context": "[17] proposed to tighten the upper and lower bounds of the breadthfirst branch and bound algorithm for the learning of Bayesian network structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The work of [17] is a contribution of major significance to the bound approximation community, and our upper bound approximation method is also based on these strategies.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "[18] further proposed to improve the lower bound function of static k-cycle conflict heuristic for the learning of Bayesian network structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The tree has 15 different leaves [19].", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "[14] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "504 Brefeld and Scheffer [12] 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": "[13] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] Altun et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] Brefeld and Scheffer [34]Suzuki et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[33] Brefeld and Scheffer [34]Suzuki et al.", "startOffset": 26, "endOffset": 30}, {"referenceID": 34, "context": "[35] 0 30 60 90 120 150", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] Altun et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] Brefeld and Scheffer [34]Suzuki et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[33] Brefeld and Scheffer [34]Suzuki et al.", "startOffset": 26, "endOffset": 30}, {"referenceID": 34, "context": "[35] 0 10 20 30 40", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "\u2022 Spanish news wire article sentence data set: This data set contains 300 sentences, and each sentence is used as a data point in our experiment [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "[11], Brefeld and Scheffer [12], Suzuki et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[11], Brefeld and Scheffer [12], Suzuki et al.", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "[13], and Jiang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], however, its prediction results are not accurate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 113, "endOffset": 117}, {"referenceID": 21, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 119, "endOffset": 123}, {"referenceID": 22, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 125, "endOffset": 129}, {"referenceID": 23, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 131, "endOffset": 135}, {"referenceID": 24, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 137, "endOffset": 141}, {"referenceID": 25, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 143, "endOffset": 147}, {"referenceID": 26, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 149, "endOffset": 153}, {"referenceID": 27, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 155, "endOffset": 159}, {"referenceID": 28, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 177, "endOffset": 181}, {"referenceID": 29, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 183, "endOffset": 187}, {"referenceID": 30, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 189, "endOffset": 193}, {"referenceID": 31, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 195, "endOffset": 199}, {"referenceID": 32, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 201, "endOffset": 205}, {"referenceID": 33, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 207, "endOffset": 211}, {"referenceID": 34, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 213, "endOffset": 217}, {"referenceID": 35, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 219, "endOffset": 223}, {"referenceID": 36, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 276, "endOffset": 280}, {"referenceID": 37, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 282, "endOffset": 286}, {"referenceID": 38, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 288, "endOffset": 292}, {"referenceID": 39, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 314, "endOffset": 318}, {"referenceID": 40, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 320, "endOffset": 324}, {"referenceID": 41, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 364, "endOffset": 368}, {"referenceID": 42, "context": "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].", "startOffset": 370, "endOffset": 374}], "year": 2016, "abstractText": "We propose a novel semi-supervised structured output prediction method based on local linear regression in this paper. The existing semi-supervise structured output prediction methods learn a global predictor for all the data points in a data set, which ignores the differences of local distributions of the data set, and the effects to the structured output prediction. To solve this problem, we propose to learn the missing structured outputs and local predictors for neighborhoods of different data points jointly. Using the local linear regression strategy, in the neighborhood of each data point, we propose to learn a local linear predictor by minimizing both the complexity of the predictor and the upper bound of the structured prediction loss. The minimization problem is solved by sub-gradient descent algorithms. We conduct experiments over two benchmark data sets, and the results show the advantages of the proposed method.", "creator": "LaTeX with hyperref package"}}}