{"id": "1606.02680", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "First Result on Arabic Neural Machine Translation", "abstract": "Neural machine scripts much likely given combined alternative them widely certain subtitle - also predictive laser phrase. We notice however. much means information with pathways machine translation from policy this European translations largely control translating stateless nature. In this paper, things apply neural machine translation one the bringing raised Arabic scripts (Ar & rb; - & 4x4; En) already compare not against a level language - based translation system. We behind extensive modest direct mainly aerodynamic in lamination Arabic script and watching that the epithet - private also peripheral context using perform svelte them each besides and put order iterative own Arabic penned widely a similar changing on both its its develop. We therefore observe they the constructs speed earliest diminished outperform another phrase - firm power well an few - large - domains test set, making alone attractive already still - world deployment.", "histories": [["v1", "Wed, 8 Jun 2016 18:36:09 GMT  (25kb,D)", "http://arxiv.org/abs/1606.02680v1", "EMNLP submission"]], "COMMENTS": "EMNLP submission", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amjad almahairi", "kyunghyun cho", "nizar habash", "aaron courville"], "accepted": false, "id": "1606.02680"}, "pdf": {"name": "1606.02680.pdf", "metadata": {"source": "CRF", "title": "First Result on Arabic Neural Machine Translation", "authors": ["Amjad Almahairi", "Kyunghyun Cho", "Nizar Habash", "Aaron Courville"], "emails": ["amjad.almahairi@umontreal.ca", "kyunghyun.cho@nyu.edu", "nizar.habash@nyu.edu", "aaron.courville@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has become a major alternative to the widely used statistical phrase-based translation system (Koehn et al., 2003), evidenced by the successful entries in WMT\u201915 and WMT\u201916.\nPrevious work on using neural networks for Arabic translation has mainly focused on using neural networks to induce an additional feature for phrasebased statistical machine translation systems (see, e.g., (Devlin et al., 2014; Setiawan et al., 2015)). This hybrid approach has resulted in impressive improvement over other systems without any neural\nnetwork, which raises a hope that a fully neural translation system may achieve a even higher translation quality. We however found no prior work on applying a fully neural translation system (i.e., neural machine translation) to Arabic translation.\nIn this paper, our aim is therefore to present the first result on the Arabic translation using neural machine translation. On both directions (Ar\u2192En and En\u2192Ar), we extensively compare a vanilla attention-based neural machine translation system (Bahdanau et al., 2015) against a vanilla phrase-based system (Moses, (Koehn et al., 2003)), while varying pre-/post-processing routines, including morphology-aware tokenization and orthographic normalization, which were found to be crucial in Arabic translation (Habash and Sadat, 2006; Badr et al., 2008; El Kholy and Habash, 2012).\nThe experiment reveals that neural machine translation performs comparably to the standard phrasebased system. We further observe that the tokenization and normalization routines, initially proposed for phrase-based systems, equally improve the translation quality of neural machine translation. Finally, on the En\u2192Ar task, we find the neural translation system to be more robust to the domain shift compared to the phrase-based system."}, {"heading": "2 Neural Machine Translation", "text": "A major workforce behind neural machine translation is an attention-based encoder-decoder model (Bahdanau et al., 2015; Cho et al., 2015). This attention-based encoder-decoder model consists of an encoder, decoder and attention mechanism. The encoder, which is often implemented as a bidirectional recurrent network, reads a source\nar X\niv :1\n60 6.\n02 68\n0v 1\n[ cs\n.C L\n] 8\nJ un\n2 01\nsentence X = (x1, . . . , xTx) and returns a set of context vectors C = (h1, . . . ,hTx).\nThe decoder is a recurrent language model. At each time t\u2032, it computes the new hidden state by\nzt\u2032 = \u03c6(zt\u2032\u22121, y\u0303t\u2032\u22121, ct\u2032),\nwhere \u03c6 is a recurrent activation function, and zt\u2032\u22121 and y\u0303t\u2032\u22121 are the previous hidden state and previously decoded target word respectively. ct\u2032 is a time-dependent context vector and is a weighted sum of the context vectors returned by the encoder: ct\u2032 = \u2211Tx t=1 \u03b1tht, where the attention weight \u03b1t is computed by the attention mechanism fatt: \u03b1t \u221d exp(fatt(zt\u2032\u22121, y\u0303t\u2032\u22121,ht)). In this paper, we use a feedforward network with a single tanh hidden layers to implement fatt.\nGiven a new decoder state zt\u2032 , the conditional distribution over the next target symbol is computed as\np(yt = w|y\u0303<t, X) \u221d exp(gw(zt\u2032)),\nwhere gw returns a score for the word w, and V is a target vocabulary.\nThe entire model, including the encoder, decoder and attention mechanism, is jointly tuned to maximize the conditional log-probability of a groundtruth translation given a source sentence using a training corpus of parallel sentence pairs. This learning process is efficiently done by stochastic gradient descent with backpropagation.\nSubword Symbols Sennrich et al. (2015), Chung et al. (2016) and Luong and Manning (2016) showed that the attention-based neural translation model can perform well when source and target sentences are represented as sequences of subword symbols such as characters or frequent character n-grams. This use of subword symbols elegantly addresses the issue of large target vocabulary in neural networks (Jean et al., 2014), and has become a de facto standard in neural machine translation. Therefore, in our experiments, we use character n-grams selected by byte pair encoding (Sennrich et al., 2015)."}, {"heading": "3 Processing of Arabic for Translation", "text": ""}, {"heading": "3.1 Characteristics of Arabic Language", "text": "Arabic exhibits a rich morphology. This makes Arabic challenging for natural language processing and\nmachine translation. For instance, a single Arabic token \u2018 \u00e9 J J. \u00bbQ \u00d6\u00cf\u00f0\u2019 (\u2018and to his vehicle\u2019 in English) is\nformed by prepending \u2018 \u00f0\u2019 (\u2018and\u2019) and \u2018 \u00cb\u2019 (\u2018to\u2019) to\nthe base lexeme \u2018 \u00e9 J. \u00bbQ\u00d3\u2019 (\u2018vehicle\u2019), appending \u2018 \u00e8\u2019\n(\u2018his\u2019) and replacing the feminine suffix \u2018 \u00e8 \u2019 (ta marbuta) of the base lexeme to \u2018 H \u2019. This feature of Arabic is challenging, as (1) it increases the number of out-of-vocabulary tokens, (2) it consequently worsens the issue of data sparsity 1, and (3) it complicates the word-level correspondence between Arabic and another language in translation. This is often worsened by the orthographic ambiguity found in Arabic scripts, such as the inconsistency in spelling certain letters.\nPrevious work has thus proposed morphologyaware tokenization and orthographic normalization as two crucial components for building a high quality phrase-based machine translation system (or its variants) for Arabic (Habash and Sadat, 2006; Badr et al., 2008; El Kholy and Habash, 2012). These techniques have been found very effective in alleviating the issue of data sparsity and improving the generalization to tokens not included in a training corpus (in their original forms.)"}, {"heading": "3.2 Morphology-Aware Tokenization", "text": "The goal of morphology-aware tokenization, or morpheme segmentation (Creutz and Lagus, 2005) is to split a word in its surface form into a sequence of linguistically sound sub-units. Contrary to simple string-based tokenization methods, morphologyaware tokenization relies on linguistic knowledge of a target language (Arabic in our case) and applies, for instance, various morphological or orthographic adjustments to the resulting sub-units.\nIn this paper, we investigate the tokenization scheme used in the Penn Arabic Treebank (ATB, (Maamouri et al., 2004)) which was found to work well with phrase-based translation system in (El Kholy and Habash, 2012). This tokenization separates all clitics other than definite articles.\nWhen translating to Arabic, the decoded sequence of tokenized symbols must be de-tokenized. This detokenization step is not trivial, as it needs to undo any adjustment (implicitly) made by the tokenization algorithm. In this work, we follow the approach\n1see Sec. 5.2.1 of (Cho, 2015) for detailed discussion.\nproposed in (Badr et al., 2008; Salameh et al., 2015). This approach builds a lookup table from a training corpus and uses it for mapping a tokenized form back to its original form. When the tokenized form is missing in the lookup table, we back off to a number of hand-crafted de-tokenization rules."}, {"heading": "3.3 Orthographic Normalization", "text": "Since the sources of major orthographic ambiguity are in the letters \u2018alif\u2019 and \u2018ya\u2019, we normalize these letters (and their inconsistent replacements.) Furthermore, we replace parentheses \u2018(\u2019 and \u2018)\u2019 with special tokens \u2018\u2013LRB\u2013\u2019 and \u2018\u2013RRB\u2013\u2019, and remove diacritics."}, {"heading": "4 Experimental Settings", "text": ""}, {"heading": "4.1 Data Preparation", "text": "Training Corpus We combine LDC2004T18, LDC2004T17 and LDC2007T08 to form a training parallel corpus. The combined corpus contains approximately 1.2M sentence pairs, with 33m tokens on the Arabic side. Most of the sentences are from news articles. We ignore sentence pairs which either side has more than 100 tokens.\nIn-Domain Evaluation Sets We use the evaluation sets from NIST 2004 (MT04) and 2005 (MT05) as development and test sets respectively. In Ar\u2192En, we use all four English reference translations to measure the translation quality. We use only the first English sentence out of four as a source during En\u2192Ar. Both of these sets are derived from news articles, just as the training corpus is.\nOut-of-Domain Evaluation Set In the case of En\u2192Ar, we evaluate both phrase-based and neural translation systems on MEDAR evaluation set (Hamon and Choukri, 2011). This set has four Arabic references per English sentence. It is derived from web pages discussing climate changes, significantly differing from the training corpus. This set was selected to highlight the robustness to domain mismatch between training and test sets.\nWe verify the domain mismatches of the evaluation sets relative to the training corpus by fitting a 5-gram language model on the training corpus and computing the likelihoods of the evaluation sets, on the Arabic side. As can be seen in Table 1, the domain of the MEDAR is significantly further away\nfrom the training corpus than the others are.\nNote on MT04 and MT05 We noticed that a significant portion of Arabic sentences in MT04 and MT05 are found verbatim in the training corpus (172 on MT04 and 26 on MT05). In order to accurately measure the generalization performance, we removed those duplicates from the evaluation sets."}, {"heading": "4.2 Machine Translation Systems", "text": "Phrase-based Machine Translation We use Moses (Koehn et al., 2007) to build a standard phrase-based statistical machine translation system. Word alignment was extracted by GIZA++ (Och and Ney, 2003), and we used phrases up to 8 words to build a phrase table. We use the following options for alignment symmetrization and reordering model: grow-diag-final-and and msd-bidirectionalfe. KenLM (Heafield et al., 2013) is used as a language model and trained on the target side of the training corpus.\nNeural machine translation We use a publicly available implementation of attention-based neural machine translation.2 For both directions\u2013En\u2192Ar and Ar\u2192En\u2013, the encoder is a bidirectional recurrent network with two layers of 512\u00d72 gated recurrent units (GRU, (Cho et al., 2014)), and the decoder a unidirectional recurrent network with 512 GRU\u2019s. Each model is trained for approximately seven days using Adadelta (Zeiler, 2012) until the cost on the development set stops improving. We regularize each model by applying dropout (Srivastava et al., 2014) to the output layer and penalizing the L2 norm of the parameters (coefficient 10\u22124). We use beam search with width set to 12 for decoding."}, {"heading": "4.3 Normalization and Tokenization", "text": "Arabic We test simple tokenization (Tok) based on the script from Moses, and orthographic normalization (Norm), and morphology-aware tokenization (ATB) using MADAMIRA (Pasha et al., 2014), . In the latter scenario, we reverse the tokenization before computing BLEU. Note that ATB includes\n2 https://github.com/nyu-dl/dl4mt-tutorial\nNorm, and both of them include simple tokenization performed by MADAMIRA.\nEnglish We test simple tokenization (Tok), lowercasing (Lower) for En\u2192Ar and truecasing (True, (Lita et al., 2003)) for Ar\u2192En.\nByte pair encoding As mentioned earlier in Sec. 2, we use byte pair encoding (BPE) for neural machine translation. We apply BPE to the alreadytokenized training corpus to extract a vocabulary of up to 20k subword symbols. We use the publicly available script released by Sennrich et al. (2015)."}, {"heading": "5 Result and Analysis", "text": "En\u2192Ar From Table 2, we observe that the translation quality improves as a better preprocessing routine is used. By using the normalization as well as morphology-aware tokenization (Tok+Norm+ATB), the phrase-based and neural systems each achieve as much as +4.46 and +4.98 BLEU over the baselines, on MT05. The improvement is even more apparent on the MEDAR whose domain deviates from the training corpus, confirming that proper preprocessing of Arabic script indeed helps in handling word tokens that are not present in a training corpus.\nWe notice that the tested tokenization strategies have nearly identical effect on both the phrasebased and neural translation systems. The translation quality of either system is mostly effective by the tokenization strategy employed for Arabic, and is largely insensitive to whether source sentences in English were lowercased. This reflects well the complexity of Arabic scripts, compared to English, discussed earlier in Sec. 3.1.\nAnother important observation is that the neural translation system significantly outperforms the phrase-based one on the out-of-domain evaluation set (MEDAR), while they perform comparably to each other in the case of the in-domain evaluation set (MT05). We conjecture that this is due to the neural translation system\u2019s superior generalization capability based on its use of continuous space representations. Ar\u2192En In the last column of Table 2, we observe a trend similar to that from En\u2192Ar. First, both phrase-based and neural machine translation benefit quite significantly from properly normalizing and tokenizing Arabic, while they are both less sensitive to truecasing English. The best translation quality using either model was achieved when all the tokenization methods were applied (Ar: Tok+Norm+ATB and En:Tok+True), improving upon the baseline by more than 2+ BLEU. Furthermore, we again observe that the phrase-based and neural translation systems perform comparably to each other."}, {"heading": "6 Conclusion", "text": "We have provided first results on Arabic neural MT, and performed extensive experiments comparing it with a standard phrase-based system. We have concluded that neural MT benefits from morphologybased tokenization and is robust to domain change."}], "references": [{"title": "Segmentation for english-to-arabic", "author": ["Badr et al.2008] Ibrahim Badr", "Rabih Zbib", "James Glass"], "venue": null, "citeRegEx": "Badr et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Badr et al\\.", "year": 2008}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Describing multimedia content using attention-based encoder-decoder networks. Multimedia", "author": ["Cho et al.2015] Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio"], "venue": "IEEE Transactions on,", "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Natural language understanding with distributed representation", "author": ["Kyunghyun Cho"], "venue": null, "citeRegEx": "Cho.,? \\Q2015\\E", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0", "author": ["Creutz", "Lagus2005] Mathias Creutz", "Krista Lagus"], "venue": "Helsinki University of Technology", "citeRegEx": "Creutz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2005}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Orthographic and morphological processing for english\u2013arabic statistical machine translation", "author": ["El Kholy", "Habash2012] Ahmed El Kholy", "Nizar Habash"], "venue": "Machine Translation,", "citeRegEx": "Kholy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kholy et al\\.", "year": 2012}, {"title": "Arabic preprocessing schemes for statistical machine translation", "author": ["Habash", "Sadat2006] Nizar Habash", "Fatiha Sadat"], "venue": null, "citeRegEx": "Habash et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2006}, {"title": "Evaluation methodology and results for english-to-arabic mt", "author": ["Hamon", "Choukri2011] Olivier Hamon", "Khalid Choukri"], "venue": "Proceedings of MT Summit XIII,", "citeRegEx": "Hamon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hamon et al\\.", "year": 2011}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Heafield et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "The penn arabic treebank: Building a large-scale annotated arabic corpus", "author": ["Ann Bies", "Tim Buckwalter", "Wigdan Mekki"], "venue": "In NEMLAR conference on Arabic language resources and tools,", "citeRegEx": "Maamouri et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Maamouri et al\\.", "year": 2004}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Madamira: A fast, comprehensive tool for morphological analysis", "author": ["Pasha et al.2014] Arfath Pasha", "Mohamed AlBadrashiny", "Mona T Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan Roth"], "venue": null, "citeRegEx": "Pasha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pasha et al\\.", "year": 2014}, {"title": "What matters most in morphologically segmented smt models. Syntax, Semantics and Structure in Statistical Translation, page 65", "author": ["Colin Cherry", "Grzegorz Kondrak"], "venue": null, "citeRegEx": "Salameh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salameh et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Statistical machine translation features with multitask tensor networks. arXiv:1506.00698", "author": ["Zhongqiang Huang", "Jacob Devlin", "Thomas Lamar", "Rabih Zbib", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Setiawan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Setiawan et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 23, "context": "Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has become a major alternative to the widely used statistical phrase-based translation system (Koehn et al.", "startOffset": 27, "endOffset": 101}, {"referenceID": 2, "context": "Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has become a major alternative to the widely used statistical phrase-based translation system (Koehn et al.", "startOffset": 27, "endOffset": 101}, {"referenceID": 13, "context": ", 2014) has become a major alternative to the widely used statistical phrase-based translation system (Koehn et al., 2003), evidenced by the successful entries in WMT\u201915 and WMT\u201916.", "startOffset": 102, "endOffset": 122}, {"referenceID": 7, "context": ", (Devlin et al., 2014; Setiawan et al., 2015)).", "startOffset": 2, "endOffset": 46}, {"referenceID": 21, "context": ", (Devlin et al., 2014; Setiawan et al., 2015)).", "startOffset": 2, "endOffset": 46}, {"referenceID": 1, "context": "vanilla attention-based neural machine translation system (Bahdanau et al., 2015) against a vanilla phrase-based system (Moses, (Koehn et al.", "startOffset": 58, "endOffset": 81}, {"referenceID": 13, "context": ", 2015) against a vanilla phrase-based system (Moses, (Koehn et al., 2003)), while varying pre-/post-processing routines, including morphology-aware tokenization and orthographic normalization, which were found to be crucial in Arabic translation (Habash and Sadat, 2006;", "startOffset": 54, "endOffset": 74}, {"referenceID": 1, "context": "A major workforce behind neural machine translation is an attention-based encoder-decoder model (Bahdanau et al., 2015; Cho et al., 2015).", "startOffset": 96, "endOffset": 137}, {"referenceID": 3, "context": "A major workforce behind neural machine translation is an attention-based encoder-decoder model (Bahdanau et al., 2015; Cho et al., 2015).", "startOffset": 96, "endOffset": 137}, {"referenceID": 20, "context": "Subword Symbols Sennrich et al. (2015), Chung", "startOffset": 16, "endOffset": 39}, {"referenceID": 20, "context": "Therefore, in our experiments, we use character n-grams selected by byte pair encoding (Sennrich et al., 2015).", "startOffset": 87, "endOffset": 110}, {"referenceID": 16, "context": "In this paper, we investigate the tokenization scheme used in the Penn Arabic Treebank (ATB, (Maamouri et al., 2004)) which was found to work well with phrase-based translation system in (El Kholy and Habash, 2012).", "startOffset": 93, "endOffset": 116}, {"referenceID": 4, "context": "1 of (Cho, 2015) for detailed discussion.", "startOffset": 5, "endOffset": 16}, {"referenceID": 0, "context": "proposed in (Badr et al., 2008; Salameh et al., 2015).", "startOffset": 12, "endOffset": 53}, {"referenceID": 19, "context": "proposed in (Badr et al., 2008; Salameh et al., 2015).", "startOffset": 12, "endOffset": 53}, {"referenceID": 14, "context": "Phrase-based Machine Translation We use Moses (Koehn et al., 2007) to build a standard", "startOffset": 46, "endOffset": 66}, {"referenceID": 11, "context": "KenLM (Heafield et al., 2013) is used as a", "startOffset": 6, "endOffset": 29}, {"referenceID": 2, "context": "2 For both directions\u2013En\u2192Ar and Ar\u2192En\u2013, the encoder is a bidirectional recurrent network with two layers of 512\u00d72 gated recurrent units (GRU, (Cho et al., 2014)), and the decoder a unidirectional recurrent network with 512 GRU\u2019s.", "startOffset": 142, "endOffset": 160}, {"referenceID": 24, "context": "Each model is trained for approximately seven days using Adadelta (Zeiler, 2012) until the cost on the development set stops improving.", "startOffset": 66, "endOffset": 80}, {"referenceID": 22, "context": "We regularize each model by applying dropout (Srivastava et al., 2014) to the output layer and penalizing the L2 norm of the parameters (coefficient 10\u22124).", "startOffset": 45, "endOffset": 70}, {"referenceID": 18, "context": "Arabic We test simple tokenization (Tok) based on the script from Moses, and orthographic normalization (Norm), and morphology-aware tokenization (ATB) using MADAMIRA (Pasha et al., 2014), .", "startOffset": 167, "endOffset": 187}, {"referenceID": 20, "context": "available script released by Sennrich et al. (2015).", "startOffset": 29, "endOffset": 52}], "year": 2016, "abstractText": "Neural machine translation has become a major alternative to widely used phrase-based statistical machine translation. We notice however that much of research on neural machine translation has focused on European languages despite its language agnostic nature. In this paper, we apply neural machine translation to the task of Arabic translation (Ar\u2194En) and compare it against a standard phrase-based translation system. We run extensive comparison using various configurations in preprocessing Arabic script and show that the phrase-based and neural translation systems perform comparably to each other and that proper preprocessing of Arabic script has a similar effect on both of the systems. We however observe that the neural machine translation significantly outperform the phrase-based system on an out-of-domain test set, making it attractive for real-world deployment.", "creator": "LaTeX with hyperref package"}}}