{"id": "1606.02710", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "A Modified Vortex Search Algorithm for Numerical Function Optimization", "abstract": "The Vortex Search (VS) workaround is one of full recently introduced calpain stochastic which had inspired saw been vortical flow entire three stirred juices. Although the VS optimization is appearance to be a else presumptive took but demands of certain optimization involved, make another has some pitfalls. In only VS algorithm, prime ways these continuous around end current players solution by simply a Gaussian locally one each iteration return. This proper conveys to both method any every also difference eventually some problems wide. Especially, for their hence those but a number of source minimum rebounds, take select a actually an actually generate republican strategies leads the calculation to often terrified another long including minimum point. Due to two computational step - size retention scheme although years the VS algorithm, part westernmost of over created schwarzenegger solve is increased evening each generalizes pass. Therefore, what form optimisation fact escape a local point as quickly as risk, because becomes enough more difficult the part algorithm whether searching although that distance over the eventually iterations. In any experts, called variant Vortex Search algorithm (MVS) fact suggested and fear above times anomaly such a existing VS exponential. In beginning MVS numerical, its gop solutions far generated hours with number of points room the iteration fumbled. Computational indication showed put which is return example come partial. strategy users willingness part for improvements VS algorithm whose strength and it MVS diophantine outpacing the expanding VS query, PSO2011 or ABC algorithms for from currencies algorithms probability both.", "histories": [["v1", "Wed, 8 Jun 2016 12:00:28 GMT  (492kb)", "http://arxiv.org/abs/1606.02710v1", "18 pages, 7 figures"]], "COMMENTS": "18 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["berat do\\u{g}an"], "accepted": false, "id": "1606.02710"}, "pdf": {"name": "1606.02710.pdf", "metadata": {"source": "CRF", "title": "A MODIFIED VORTEX SEARCH ALGORITHM FOR NUMERICAL FUNCTION OPTIMIZATION", "authors": ["Berat Do\u011fan"], "emails": [], "sections": [{"heading": null, "text": "DOI: 10.5121/ijaia.2016.7304 37\nThe Vortex Search (VS) algorithm is one of the recently proposed metaheuristic algorithms which was inspired from the vortical flow of the stirred fluids. Although the VS algorithm is shown to be a good candidate for the solution of certain optimization problems, it also has some drawbacks. In the VS algorithm, candidate solutions are generated around the current best solution by using a Gaussian distribution at each iteration pass. This provides simplicity to the algorithm but it also leads to some problems along. Especially, for the functions those have a number of local minimum points, to select a single point to generate candidate solutions leads the algorithm to being trapped into a local minimum point. Due to the adaptive step-size adjustment scheme used in the VS algorithm, the locality of the created candidate solutions is increased at each iteration pass. Therefore, if the algorithm cannot escape a local point as quickly as possible, it becomes much more difficult for the algorithm to escape from that point in the latter iterations. In this study, a modified Vortex Search algorithm (MVS) is proposed to overcome above mentioned drawback of the existing VS algorithm. In the MVS algorithm, the candidate solutions are generated around a number of points at each iteration pass. Computational results showed that with the help of this modification the global search ability of the existing VS algorithm is improved and the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABC algorithms for the benchmark numerical function set.\nKEYWORDS\nMetaheuristics, Numerical Function Optimization, Vortex Search Algorithm, Modified Vortex Search Algorithm."}, {"heading": "1. INTRODUCTION", "text": "In the past two decades, a number of metaheuristic algorithms have been proposed to solve complex real-world optimization problems. Most of these algorithms are nature inspired methods and therefore mimic natural metaphors such as, evolution of species (GA [1] and DE [2-3]), annealing process (SA [4-5]), ant behaviour (ACO [6]), swarm behaviour (PSO [7] and ABC [8- 9]) etc. These algorithms make few or no assumptions for the problem at hand and provide fast and robust solutions. Although, the solutions provided by metaheuristics may not be optimal solutions, they are highly preferred because of their simplicity and flexibility. Despite the high number of available metaheuristics, developing new metaheuristic algorithms is still an active research area. In [10-15], a number of recently proposed metaheuristics can be found. All of these metaheuristics have certain characteristics and thus each one may be more successful on a certain optimization problem when compared to the others. The Vortex Search (VS) algorithm [16] is one of these recently proposed metaheuristic algorithms which was inspired from the vortical flow of the stirred fluids. The search behaviour of the VS algorithm is modelled as a vortex pattern by using an adaptive step-size adjustment scheme. By this way, it is aimed to have a good balance between the explorative and exploitative behaviour of the search.\nThe proposed VS algorithm was tested over 50 benchmark mathematical functions and the obtained results compared to the single-solution based (Simulated Annealing, SA and Pattern Search, PS) and population-based (Particle Swarm Optimization, PSO2011 and Artificial Bee Colony, ABC) algorithms. A Wilcoxon-Signed Rank Test was performed to measure the pairwise statistical performances of the algorithms, the results of which indicated that the proposed VS algorithm outperforms the SA, PS and ABC algorithms while being competitive with the PSO2011 algorithm. Because of the simplicity of the proposed VS algorithm, a significant decrease in the computational time of the 50 benchmark numerical functions was also achieved when compared to the population-based algorithms. In some other studies [17-20], the VS algorithm has also been successfully used for the solution of some real-world optimization problems. Although the proposed VS algorithm is a good candidate for the solution of optimization problems, it also has some drawbacks. In the VS algorithm, candidate solutions are generated around the current best solution by using a Gaussian distribution at each iteration pass. This provides simplicity to the algorithm but it also leads to some problems along. Especially, for the functions those have a number of local minimum points, to select a single point to generate candidate solutions leads the algorithm to being trapped into a local minimum point. Due to the adaptive step-size adjustment scheme used in the VS algorithm, the locality of the created candidate solutions is increased at each iteration pass. Therefore, if the algorithm cannot escape a local point as quickly as possible, it becomes much more difficult for the algorithm to escape from that point in the latter iterations. In this study, a modified Vortex Search algorithm (MVS) is proposed to overcome above mentioned drawback of the existing VS algorithm. In the MVS algorithm, the candidate solutions are generated around different points at each iteration pass. These points are iteratively updated during the search process, details of which are given in the following section. The MVS algorithm is again tested by using the 50 benchmark mathematical functions that was used earlier in [16]. Because the SA and PS algorithms showed poor performances in [16], in this study these two algorithms are excluded and the results are compared to the results those obtained by the VS algorithm, PSO2011 and ABC algorithms. It is shown that, the MVS algorithm outperforms all of these algorithms and can successfully find the known global minimum points of the functions that the VS algorithm being trapped into the local minimum points earlier. The remaining part of this paper is organized as follows. In the following section, first a brief description of the VS algorithm is given. Then, the modification performed on the VS algorithm is detailed and the MVS algorithm is introduced. Section 3 covers the experimental results and discussion. Finally, Section 4 concludes the work."}, {"heading": "2. METHODOLOGY", "text": ""}, {"heading": "2.1. A Brief Description of the Vortex Search Algorithm", "text": "Let us consider a two-dimensional optimization problem. In a two dimensional space a vortex pattern can be modelled by a number of nested circles. Here, the outer (largest) circle of the vortex is first centered on the search space, where the initial center can be calculated using Eq. 1.\n2\nlowerlimitupperlimit \u00b50 + =\n(1)\nIn Eq.1, upperlimit and lowerlimit are 1\u00d7d vectors that define the bound constraints of the\nproblem in d dimensional space. Then, a number of neighbor solutions )(sCt , ( t represents\nthe iteration index and initially 0=t ) are randomly generated around the initial center 0\u00b5 in the d -dimensional space by using a Gaussian distribution. Here, { } nkssssC k ,...,2,1,...,,)( 210 == represents the solutions, and n represents the total number of candidate solutions. In Eq. 2, the general form of the multivariate Gaussian distribution is given.\n      \u2212\u03a3\u2212\u2212\n\u03a3\n=\u03a3 \u2212 )()(\n2\n1 exp\n)2(\n1 ),|( 1 \u00b5\u00b5\n\u03c0 \u00b5 xxxp T d\n(2)\nIn Eq.2, d represents the dimension, x is the 1\u00d7d vector of a random variable, \u00b5 is the 1\u00d7d\nvector of sample mean (center) and \u03a3 is the covariance matrix. If the diagonal elements (variances) of the values of \u03a3 are equal and if the off-diagonal elements (covariance) are zero (uncorrelated), then the resulting shape of the distribution will be spherical (which can be considered circular for a two-dimensional problem, as in our case). Thus, the value of \u03a3 can be computed by using equal variances with zero covariance by using Eq. 3.\n[ ] ddI \u00d7\u22c5=\u03a3 2\u03c3 (3)\nIn Eq. 3, 2\u03c3 represents the variance of the distribution and I represents the dd \u00d7 identity\nmatrix. The initial standard deviation ( 0\u03c3 ) of the distribution can be calculated by using Eq. 4.\n2\n)min()max( 0 lowerlimitupperlimit \u2212 =\u03c3\n(4)\nHere, 0\u03c3 can also be considered as the initial radius ( 0r ) of the outer circle for a two\ndimensional optimization problem. Because a weak locality is required in the initial phases, 0r is chosen to be a large value. Thus, a full coverage of the search space by the outer circle is provided in the initial step. This process provides a bird's-eye view for the problem at hand.\nIn the selection phase, a solution (which is the best one) )(0 ' sCs \u2208 is selected and memorized\nfrom )(0 sC to replace the current circle center 0\u00b5 . Prior to the selection phase, the candidate solutions must be ensured to be inside the search boundaries. For this purpose, the solutions that exceed the boundaries are shifted into the boundaries, as in Eq. 5.\n  \n \n\n>+\u2212\u22c5\n\u2264\u2264\n<+\u2212\u22c5\n=\nii k iii\nii k ii k\nii k iii\ni k\nupperlimit s,lowerlimit)lowerlimitt(upperlimirand\nupperlimit slowerlimit,s\nlowerlimit s,lowerlimit)lowerlimitt(upperlimirand\ns\n(5)\nIn Eq.5, nk ,...2,1= and di ,...,2,1= and rand is a uniformly distributed random number. Next,\nthe memorized best solution '\ns is assigned to be the center of the second circle (the inner one). In the generation phase of the second step, the effective radius ( 1r ) of this new circle is reduced, and then, a new set of solutions )(1 sC is generated around the new center. Note that in the second step, the locality of the generated neighbors increased with the decreased radius. In the selection phase of the second step, the new set of solutions )(1 sC is evaluated to select a solution\n)(1 ' sCs \u2208 . If the selected solution is better than the best solution found so far, then this solution is assigned to be the new best solution and it is memorized. Next, the center of the third circle is\nassigned to be the memorized best solution found so far. This process iterates until the termination condition is met. An illustrative sketch of the process is given in Figure 1. In this manner, once the algorithm is terminated, the resulting pattern appears as a vortex-like structure, where the center of the smallest circle is the optimum point found by the algorithm. A representative pattern is sketched in Figure 2 for a two-dimensional optimization problem for which the upper and lower limits are between the [-10,10] interval. A description of the VS algorithm is also provided in Figure 3. The radius decrement process given in Figure 3 can be considered as a type of adaptive step-size adjustment process which has critical importance on the performance of the VS algorithm. This process should be performed in such a way that allows the algorithm to behave in an explorative manner in the initial steps and in an exploitative manner in the latter steps. To achieve this type of process, the value of the radius must be tuned properly during the search process. In the VS algorithm, the inverse incomplete gamma function is used to decrease the value of the radius during each iteration pass.\nThe incomplete gamma function given in Eq. 6 most commonly arises in probability theory, particularly in those applications involving the chi-square distribution [21].\n\u222b >= \u2212\u2212\nx at\nadtteax\n0\n1 0),(\u03b3\n(6)\nIn Eq.6, 0>a is known as the shape parameter and 0\u2265x is a random variable. In conjunction with the incomplete gamma function, its complementary ),( ax\u0393 is usually also introduced (Eq. 7).\n\u222b \u221e \u2212\u2212 >=\u0393\nx\nat adtteax 0),( 1\n(7)\nThus, it follows that, )(),(),( aaxax \u0393=\u0393+\u03b3 (8)\nwhere )(a\u0393 is known as the gamma function. There exist many studies in the literature on\ndifferent proposed methods for the numerical calculation of the incomplete gamma function [22- 24]. MATLAB\u00ae also provides some tools for the calculation of the inverse incomplete gamma (gammaincinv) function. The inverse incomplete gamma function (gammaincinv), computes the inverse of the incomplete gamma function with respect to the integration limit x and represented as gammaincinv(x,a) in MATLAB\u00ae.\nIn Figure 4, the inverse incomplete gamma function is plotted for 1.0=x and [ ]1,0\u2208a . Here, for our case the parameter a of the inverse incomplete gamma function defines the resolution of\nthe search. By equally sampling a values within [ ]1,0 interval at a certain step size, the\nresolution of the search can be adjusted. For this purpose, at each iteration, a value of a is computed by using the Eq.9\nMaxItr\nt aat \u2212= 0\n(9)\nwhere 0a is selected as 10 =a to ensure a full coverage of the search space at the first iteration, t is the iteration index, and MaxItr represents the maximum number of iterations. Let us consider an optimization problem defined within the [-10,10] region. The initial radius 0r can be calculated with Eq. 10. Because 10 =a , the resulting function value is\n1),()1( 0 \u2248\u22c5 axvgammaincinx , which means 00 \u03c3\u2248r as indicated before.\n),()1( 000 axvgammaincinxr \u22c5\u22c5= \u03c3 (10)\nBy means of Eq.4, the initial radius value 0r can be calculated as 100 \u2248r . In Eq.11, a general formula is also given to obtain the value of the radius at each iteration pass.\n),()1(0 tt axvgammaincinxr \u22c5\u22c5= \u03c3 (11)\nHere, t represents the iteration index."}, {"heading": "2.2. The Modified Vortex Search Algorithm", "text": "The VS algorithm creates candidate solutions around a single point at each iteration pass. At the first iteration, this point is the initial center 0\u00b5 which is determined with the upper and lower limits of the problem at hand while in the latter iterations the center is shifted to the current best position found so far. As mentioned before, this mechanism leads the VS algorithm to being trapped into local minimum points for a number of functions. To overcome above mentioned drawback, in this study a modified VS algorithm (MVS) is proposed. In the MVS algorithm, candidate solutions are generated around multiple centers at\neach iterations pass. The search behavior of the MVS algorithm can be thought as a number of parallel vortices that have different centers at each iteration pass. Initially, the centers of these multiple vortices are selected as in the VS algorithm. Let us consider, the total number of centers (or vortices) to be represented by m . Let us say, )(\u00b5tM represents the matrix that stores the values of these m centers at each iteration pass and t represents the iteration index. Thus,\ninitially { } mlM l ,...,2,1,,...,,)( 020100 == \u00b5\u00b5\u00b5\u00b5 and initial positions of these centers are computed as in Eq. 12.\nml lowerlimitupperlimitl ,...,2,1,\n2 ... 0\n2 0 1 0 =\n+ ==== \u00b5\u00b5\u00b5\n(12)\nNext, a number of candidate solutions are generated with a Gaussian distribution around these initial centers by using the initial radius value 0r . In this case the total number of candidate solutions is again selected to be n . But note that, these n solutions are generated around m centers. Thus, one should select mn solutions around each center.\nLet us say, { } mnkssssCS k l t ,...,2,1,...,,)( 21 == represents the subset of solutions generated around the center ml ,...,2,1= for the iteration t . Then, the total solution set generated for the\niteration 0=t can be represented by { } mlCSCSCSsC l ,...,2,1,,...,,)( 020100 == . In the selection phase, for each subset of solutions, a solution (which is the best one) )(0 ' sCSs l l \u2208 is selected. Prior to the selection phase it must be ensured that the candidate subsets of solutions are inside the search boundaries. For this purpose, the solutions that exceed the boundaries are shifted into the boundaries, as in Eq. 5. Let us say, the best solution of each subset is stored in a matrix\n)( 'sPBest t at each iteration pass. Thus, for 0=t , { } mlssssPBest l ,...,2,1,,...,,)( ''2'1'0 == . Note that, the best solution of this matrix ( )( '0 sPBest ) is also the best solution of the total candidate\nsolution set )(0 sC for the current iteration, which is represented as bestItr .\nIn the VS algorithm, at each iterations pass, the center is always shifted to the best solution found so far, bests . However, in the MVS algorithm, there exist m centers which positions need to be updated for the next iteration. The most important difference between the VS and MVS algorithm arises from here. In the MVS algorithm, one of these centers is again shifted to the best solution found so far, bests . But, the remaining 1\u2212m centers are shifted to a new position determined by the best positions generated around the each center at the iteration t and the best\nposition found so far, bests as shown in Eq. 13.\n)( '' bestll l t ssrands +\u22c5+=\u00b5\n(13)\nIn Eq. 13, rand is a uniformly distributed random number, 1,...,2,1 \u2212= ml and\n)( '1 ' sPBests tl \u2212\u2208 . Thus, for 1=t , { } 1,...,2,1,,...,,)( 121111 \u2212== mlM l\u00b5\u00b5\u00b5\u00b5 is determined by using the )( '0 ' sPBestsl \u2208 positions and the best position found so far, bests . In Figure-5, an illustrative sketch of the center update process is given for a two-dimensional problem. In Figure 5, only one center is considered.\nIn the MVS algorithm, the radius decrement process is held totally in the same way as it is done in the VS algorithm. At each iteration pass, the radius is decreased by utilizing the inverse incomplete gamma function and thus, the locality of the generated solutions is increased. In Figure 6, a description of the MVS algorithm is provided."}, {"heading": "3. RESULTS", "text": "The proposed MVS algorithm is tested on 50 benchmark functions which were also used in [16] to measure the performance of the VS algorithm. By using the same functions, in this study, the\nperformance of the MVS algorithm is compared to the VS, PSO2011 and ABC algorithms. PSO2011 [25-26] is an extension of the standard PSO algorithm and the ABC algorithm is a well-known optimization algorithm which was inspired from the collective behaviours of honey bees. The functions used in the experiments are listed in Table 1. For the formulations of the functions listed in Table 1, please refer to the reference [16]."}, {"heading": "3.1. Algorithm Settings", "text": "The ABC and PSO2011 algorithms are selected to have a population size of 50, which is also the number of neighborhood solutions of the proposed VS algorithm. The acceleration coefficients ( 1c and 2c ) of the PSO2011 algorithm are both set to 1.8, and the inertia coefficient is set to 0.6, as in [27]. The limit value for the ABC algorithm is determined as limit = SN * D, where SN represents the number of food sources and D represents the dimension. VS algorithm does not have any additional parameters. Different from the VS algorithm, the MVS algorithm has the parameter m , which represents the total number of centers."}, {"heading": "3.2. Results", "text": "The proposed MVS algorithm is compared to the VS, PSO2011 and ABC algorithms by using the 50 benchmark functions given in Table 1. For each algorithm, 30 different runs are performed, and the mean and the best values are recorded. The maximum number of iterations is selected to be 500,000. For the MATLAB\u00ae codes of the PSO2011, ABC, VS and MVS algorithms please refer to [25], [28], [29] and [30]. For each algorithm, all of the functions are run in parallel using a 32 core Intel\u00ae CPU 32 GB RAM workstation. For the first set of experiments, results are given in Table 2. As shown in Table 2, for the MVS algorithm two different cases are considered. In the first case, the total number of candidate solutions is selected to be 50, which means 10 candidate solutions are generated around each center for 5=m . In this case, the MVS algorithm can avoid from the local minimum points for the functions F13, F16, F17, F22, F23, F41, and F43 which is not the case for the VS algorithm. However, poor sampling of the search space for this case (10 points around each center) leads the MVS algorithm to show a correspondingly poor performance on the improvement of the found near optimal solutions (exploitation) for some of the functions. This can be clearly seen in the pair-wise statistical comparison of the algorithms given in Table 3. The pair-wise statistical comparison of the algorithms is obtained by a Wilcoxon Signed-Rank Test. The null hypothesis H0 for this test is: \"There is no difference between the median of the solutions produced by algorithm A and the median of the solutions produced by algorithm B for the same benchmark problem\", i.e., median (A) = median (B). To determine whether algorithm A reached a statistically better solution than algorithm B, or if not, whether the alternative hypothesis is valid, the sizes of the ranks provided by the Wilcoxon Signed-Rank Test (i.e., T+ and T-, as defined in [10]) are examined. Because an arithmetic precision value that is higher than necessary makes it difficult to compare the local search abilities of the algorithms, during the statistical pair-wise comparison, resulting values below are considered as 0. In Table 3, \u2018+\u2019 indicates cases in which the null hypothesis is rejected and the MVS algorithm exhibited a statistically superior performance in the pair-wise Wilcoxon Signed-Rank Test at the 95% significance level ( 05.0=\u03b1 ); \u2018-\u2019 indicates cases in which the null hypothesis is rejected and the MVS algorithm exhibited an inferior performance; and \u2018=\u2019 indicates cases in which there is no statistical different between two algorithms. The last row of the Table 3 shows the total count of (+/=/-) for the three statistical significance cases in the pair-wise comparison.\nAs shown in Table 3, the VS algorithm outperforms the MVS algorithm for the functions F5, F10, F11, F38, and F42. But when the results given in Table 2 are compared for these functions, it can be clearly seen that this difference mainly arises because of the poor exploitation ability of the MVS algorithm with 50 candidate solutions. Therefore, another case in which the total number of candidate solutions is selected to be 250 is considered for the MVS algorithm. In this case, 50 candidate solutions are generated around each center for 5=m . As can be shown in Table 2, the MVS algorithm with 250 candidate solutions performs better than the MVS algorithm with 50 candidate solutions. Statistical pair-wise comparison of the algorithms for the second case is also given in Table 4.\nF42 Ackley Multimodal Non-Separable [-32, 32] 30 0 F43 Penalized Multimodal Non-Separable [-50, 50] 30 0 F44 Penalized2 Multimodal Non-Separable [-50, 50] 30 0 F45 Langerman2 Multimodal Non-Separable [0, 10] 2 -1.08 F46 Langerman5 Multimodal Non-Separable [0, 10] 5 -1.5 F47 Langerman10 Multimodal Non-Separable [0, 10] 10 NA F48 Fletcher Powell2 Multimodal Non-Separable [ \u03c0\u03c0 ,\u2212 ] 2 0\nF49 Fletcher Powell5 Multimodal Non-Separable [ \u03c0\u03c0 ,\u2212 ] 5 0\nF50 Fletcher Powell10 Multimodal Non-Separable [ \u03c0\u03c0 ,\u2212 ] 10 0\nBest 1.1432E-16 0 5.71959E-06 1.72679E-07 5.23427E-05 F14 0 Mean\nStdDev Best 3.79651E-05 0.000156663 0 0 0 0\n0 0 0\n1.094284383 0.870781136 0.107097937 8.51365E-16 0 6.93597E-16\nF15 0 Mean StdDev Best 0 0 0\n0 0 0\n0 0 0\n0 0 0\n0.000760232 0.000440926 0.00027179\nF16 0 Mean StdDev Best 1.20813E-07 2.94163E-07 1.14463E-12\n3.51659E-08 5.41004E-08 1.85577E-13\n0.367860114 1.130879848 9.42587E-05 0.930212233 1.714978077 0 0.003535257 0.003314818 7.08757E-05\nF17 0 Mean StdDev Best 0 0 0\n0 0 0\n0.666666667 7.68909E-16 0.666666667 0.666666667 4.38309E-16 0.666666667 1.91607E-15 2.55403E-16 1.1447E-15\nF31 -186.73 Mean StdDev Best -186.7309088 3.25344E-14 -186.7309088 -186.7309088 2.93854E-14 -186.7309088 -186.7309088 3.76909E-14 -186.7309088 -186.7309088 4.49449E-13 -186.7309088 -186.7309088 1.18015E-14 -186.7309088 F32 3 Mean StdDev Best 3 1.25607E-15 3 3 1.51835E-15 3 3 1.44961E-15 3 3 1.22871E-15 3 3 1.7916E-15 3 F33 0.00031 Mean StdDev Best 0.000307486 0 0.000307486 0.000307486 0 0.000307486 0.000307486 0 0.000307486 0.000307486 0 0.000307486 0.000319345 5.4385E-06 0.00030894 F34 -10.15 Mean StdDev Best -10.15319968 6.8481E-15 -10.15319968 -10.15319968 7.01294E-15 -10.15319968 -10.15319968 7.2269E-15 -10.15319968 -9.363375596 2.081063878 -10.15319968 -10.15319968 7.2269E-15 -10.15319968\nF48 0 Mean StdDev Best 0 0 0\n0 0 0\n0 0 0\n0 0 0\n0 0 0\nF49 0 Mean StdDev Best 0 0 0\n0 0 0\n0 0 0\n3.083487114 4.389694328 0 1.48707E-12 8.11041E-12 3.1715E-16\nF50 0 Mean StdDev Best 0 0 0\n0 0 0\n0 0 0\n580.0839029 1280.698395 0 1.111095363 0.598962098 0.182153237\nAs shown in Table 4, MVS algorithm with 250 candidate solutions can successfully improve the near optimal solutions and thus performs better than all of the other algorithms. In [31], authors stated that after a sufficient value for colony size, any increment in the value does not improve the performance of the ABC algorithm significantly. For the test problems carried out in [31] colony sizes of 10, 50 and 100 are used for the ABC algorithm. It is shown\nthat although from 10 to 50 the performance of the ABC algorithm significantly increased, there is not any significant difference between the performances achieved by 50 and 100 colony sizes. Similarly, for the PSO algorithm it is reported that, PSO with different population sizes has almost the similar performance which means the performance of PSO is not sensitive to the population size [32]. Based on the above considerations, in this study a comparison of the MVS algorithm to the ABC and PSO2011 algorithms with a different population size is not performed. For the VS algorithm it is expected to achieve better exploitation ability with an increased number of candidate solutions. But the problem with the VS algorithm is with its global search ability rather than the local search ability for some of the functions listed above. Therefore, a comparison of the MVS (m = 5, n = 50) to VS algorithm with 50 candidate solutions is thought to be enough to show the improvement achieved by the modification performed on the VS algorithm. In Figure 7, the average computational time of 30 runs for 500,000 iterations is also provided for the MVS (m = 5, n = 50), MVS (m = 5, n = 250), VS, PSO2011 and ABC algorithms. As shown in this figure, the required computational time to perform 500,000 iterations with the MVS algorithm is slightly increased when compared to the VS algorithm. However, even for the MVS (m = 5, n = 250) algorithm the required computational time to perform 500,000 iterations is still lower than the PSO2011 and ABC algorithms."}, {"heading": "4. CONCLUSIONS", "text": "This paper presents a modified VS algorithm in which the global search ability of the existing VS algorithm is improved. This is achieved by using multiple centers during the candidate solution generation phase of the algorithm at each iteration pass. In the VS algorithm, only one center is used for this purpose and this usually leads the algorithm to being trapped into a local minimum point for some of the benchmark functions. Although the complexity of the existing VS algorithm is a bit increased, there is not any significant difference between the computational time of the modified VS algorithm and the existing VS algorithm. Computational results showed that the MVS algorithm outperforms the existing VS algorithm, PSO2011 and ABC algorithms for the benchmark numerical function set. In the future studies, the MVS algorithm will be used for the solution of some real world optimization problems."}], "references": [{"title": "Adaptation in Natural and Artificial Systems, University of Michigan Press", "author": ["J.H. Holland"], "venue": "Ann Arbor, MI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1975}, {"title": "Differential evolution \u2013 a simple and efficient adaptive scheme for global optimization over continuous spaces", "author": ["R. Storn", "K. Price"], "venue": "Technical report,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Differential evolution \u2013 a simple and efficient heuristic for global optimization over continuous spaces", "author": ["R. Storn", "K. Price"], "venue": "Journal of Global Optimization", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Optimization by Simulated Annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt Jr.", "M.P. Vecchi"], "venue": "Science", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1983}, {"title": "Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm", "author": ["V. \u010cern\u00fd"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1985}, {"title": "Optimization, Learning and Natural Algorithms", "author": ["M. Dorigo"], "venue": "PhD thesis, Politecnico di Milano, Italy,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "An idea based on honeybee swarm for numerical optimization", "author": ["D. Karaboga"], "venue": "Technical Report TR06,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "powerful, A powerful and efficient algorithm for numerical function optimization: artificial bee colony (abc) algorithm", "author": ["D. Karaboga", "B.A. Basturk"], "venue": "Journal of Global Optimization", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Backtracking Search Optimization Algorithm for numerical optimization problems, Applied Mathematics and Computation, Volume 219", "author": ["P. Civicioglu"], "venue": "Issue 15,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "A new metaheuristic for optimization: Optics inspired optimization (OIO)", "author": ["A.H. Kashan"], "venue": "Computers & Operations Research, Volume", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Flower pollination algorithm for global optimization, Unconventional computation and natural computation", "author": ["X.S. Yang"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "ODMA: a novel swarm-evolutionary metaheuristic optimizer inspired by open source development model and communities", "author": ["H. Hajipour", "H.B. Khormuji", "H. Rostami"], "venue": "Soft Computing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A multi-start central force optimization for global optimization", "author": ["L. Yong", "T. Peng"], "venue": "Applied Soft Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Water wave optimization: A new nature-inspired metaheuristic", "author": ["Z. Yu-Jun"], "venue": "Computers & Operations Research, Volume", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "A new metaheuristic for numerical function optimization: Vortex Search algorithm, Information Sciences", "author": ["B. Do\u011fan", "T. \u00d6lmez"], "venue": "Volume 293,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Vortex search algorithm for the analog active filter component selection problem, AEU ", "author": ["B. Do\u011fan", "T. \u00d6lmez"], "venue": "International Journal of Electronics and Communications,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Analog filter group delay optimization using the Vortex Search algorithm", "author": ["B. Do\u011fan", "A. Yuksel"], "venue": "Signal Processing and Communications Applications Conference (SIU),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Modified Off-lattice AB Model for Protein Folding Problem Using the Vortex Search Algorithm", "author": ["B. Do\u011fan", "T. \u00d6lmez"], "venue": "International Journal of Machine Learning and Computing vol. 5,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Fuzzy clustering of ECG beats using a new metaheuristic approach, 2nd International Work-Conference on Bioinformatics and Biomedical Engineering (IWBBIO)", "author": ["B. Do\u011fan", "T. \u00d6lmez"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Special Functions of Mathematics for Engineers", "author": ["L.C. Andrews"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "A note on the recursive calculation of incomplete gamma functions", "author": ["W. Gautschi"], "venue": "ACM Trans. Math. Software,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Computing the incomplete Gamma function to arbitrary precision Computational Science and Its Applications ", "author": ["S. Winitzki"], "venue": "ICCSA 2003, of LNCS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Numerical calculation of incomplete gamma function by the trapezoidal rule, Numer. Math. (Numerische Mathematik", "author": ["G. Allasia", "Besenghi R"], "venue": "Journal of Artificial Intelligence and Applications (IJAIA),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1987}, {"title": "Standard Particle Swarm Optimization", "author": ["M. Clerc"], "venue": "Particle Swarm Central, Tech. Rep., 2012, http://clerc.maurice.free.fr/pso/SPSO_descriptions.pdf, accessed 25 February 2016", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "A comparative study of Artificial Bee Colony algorithm, Applied Mathematics and Computation, Volume 214", "author": ["D. Karaboga", "B. Akay"], "venue": "Issue 1,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "On the performance of artificial bee colony (abc) algorithm, Applied Soft Computing", "author": ["D. Karaboga", "B. Basturk"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Empirical study of particle swarm optimization", "author": ["Y. Shi", "Eberhart R.C"], "venue": "Evolutionary Computation,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "Most of these algorithms are nature inspired methods and therefore mimic natural metaphors such as, evolution of species (GA [1] and DE [2-3]), annealing process (SA [4-5]), ant behaviour (ACO [6]), swarm behaviour (PSO [7] and ABC [89]) etc.", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "Most of these algorithms are nature inspired methods and therefore mimic natural metaphors such as, evolution of species (GA [1] and DE [2-3]), annealing process (SA [4-5]), ant behaviour (ACO [6]), swarm behaviour (PSO [7] and ABC [89]) etc.", "startOffset": 136, "endOffset": 141}, {"referenceID": 2, "context": "Most of these algorithms are nature inspired methods and therefore mimic natural metaphors such as, evolution of species (GA [1] and DE [2-3]), annealing process (SA [4-5]), ant behaviour (ACO [6]), swarm behaviour (PSO [7] and ABC [89]) etc.", "startOffset": 136, "endOffset": 141}, {"referenceID": 3, "context": "Most of these algorithms are nature inspired methods and therefore mimic natural metaphors such as, evolution of species (GA [1] and DE [2-3]), annealing process (SA [4-5]), ant behaviour (ACO [6]), swarm behaviour (PSO [7] and ABC [89]) etc.", "startOffset": 166, "endOffset": 171}, {"referenceID": 4, "context": "Most of these algorithms are nature inspired methods and therefore mimic natural metaphors such as, evolution of species (GA [1] and DE [2-3]), annealing process (SA [4-5]), ant behaviour (ACO [6]), swarm behaviour (PSO [7] and ABC [89]) etc.", "startOffset": 166, "endOffset": 171}, {"referenceID": 5, "context": "Most of these algorithms are nature inspired methods and therefore mimic natural metaphors such as, evolution of species (GA [1] and DE [2-3]), annealing process (SA [4-5]), ant behaviour (ACO [6]), swarm behaviour (PSO [7] and ABC [89]) etc.", "startOffset": 193, "endOffset": 196}, {"referenceID": 8, "context": "In [10-15], a number of recently proposed metaheuristics can be found.", "startOffset": 3, "endOffset": 10}, {"referenceID": 9, "context": "In [10-15], a number of recently proposed metaheuristics can be found.", "startOffset": 3, "endOffset": 10}, {"referenceID": 10, "context": "In [10-15], a number of recently proposed metaheuristics can be found.", "startOffset": 3, "endOffset": 10}, {"referenceID": 11, "context": "In [10-15], a number of recently proposed metaheuristics can be found.", "startOffset": 3, "endOffset": 10}, {"referenceID": 12, "context": "In [10-15], a number of recently proposed metaheuristics can be found.", "startOffset": 3, "endOffset": 10}, {"referenceID": 13, "context": "In [10-15], a number of recently proposed metaheuristics can be found.", "startOffset": 3, "endOffset": 10}, {"referenceID": 14, "context": "The Vortex Search (VS) algorithm [16] is one of these recently proposed metaheuristic algorithms which was inspired from the vortical flow of the stirred fluids.", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "In some other studies [17-20], the VS algorithm has also been successfully used for the solution of some real-world optimization problems.", "startOffset": 22, "endOffset": 29}, {"referenceID": 16, "context": "In some other studies [17-20], the VS algorithm has also been successfully used for the solution of some real-world optimization problems.", "startOffset": 22, "endOffset": 29}, {"referenceID": 17, "context": "In some other studies [17-20], the VS algorithm has also been successfully used for the solution of some real-world optimization problems.", "startOffset": 22, "endOffset": 29}, {"referenceID": 18, "context": "In some other studies [17-20], the VS algorithm has also been successfully used for the solution of some real-world optimization problems.", "startOffset": 22, "endOffset": 29}, {"referenceID": 14, "context": "The MVS algorithm is again tested by using the 50 benchmark mathematical functions that was used earlier in [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "Because the SA and PS algorithms showed poor performances in [16], in this study these two algorithms are excluded and the results are compared to the results those obtained by the VS algorithm, PSO2011 and ABC algorithms.", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "A representative pattern is sketched in Figure 2 for a two-dimensional optimization problem for which the upper and lower limits are between the [-10,10] interval.", "startOffset": 145, "endOffset": 153}, {"referenceID": 19, "context": "6 most commonly arises in probability theory, particularly in those applications involving the chi-square distribution [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "Let us consider an optimization problem defined within the [-10,10] region.", "startOffset": 59, "endOffset": 67}, {"referenceID": 14, "context": "The proposed MVS algorithm is tested on 50 benchmark functions which were also used in [16] to measure the performance of the VS algorithm.", "startOffset": 87, "endOffset": 91}, {"referenceID": 23, "context": "PSO2011 [25-26] is an extension of the standard PSO algorithm and the ABC algorithm is a well-known optimization algorithm which was inspired from the collective behaviours of honey bees.", "startOffset": 8, "endOffset": 15}, {"referenceID": 14, "context": "For the formulations of the functions listed in Table 1, please refer to the reference [16].", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "as in [27].", "startOffset": 6, "endOffset": 10}, {"referenceID": 8, "context": ", T+ and T-, as defined in [10]) are examined.", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "12] 5 0 F2 Step Unimodal Separable [-100, 100] 30 0 F3 Sphere Unimodal Separable [-100, 100] 30 0 F4 SumSquares Unimodal Separable [-10, 10] 30 0 F5 Quartic Unimodal Separable [-1.", "startOffset": 131, "endOffset": 140}, {"referenceID": 8, "context": "5] 5 0 F7 Easom Unimodal Non-Separable [-100, 100] 2 -1 F8 Matyas Unimodal Non-Separable [-10, 10] 2 0 F9 Colville Unimodal Non-Separable [-10, 10] 4 0 F10 Trid6 Unimodal Non-Separable [-D,D] 6 -50 F11 Trid10 Unimodal Non-Separable [-D,D] 10 -210 F12 Zakharov Unimodal Non-Separable [-5,10] 10 0 F13 Powell Unimodal Non-Separable [-4,5] 24 0 F14 Schwefel 2.", "startOffset": 89, "endOffset": 98}, {"referenceID": 8, "context": "5] 5 0 F7 Easom Unimodal Non-Separable [-100, 100] 2 -1 F8 Matyas Unimodal Non-Separable [-10, 10] 2 0 F9 Colville Unimodal Non-Separable [-10, 10] 4 0 F10 Trid6 Unimodal Non-Separable [-D,D] 6 -50 F11 Trid10 Unimodal Non-Separable [-D,D] 10 -210 F12 Zakharov Unimodal Non-Separable [-5,10] 10 0 F13 Powell Unimodal Non-Separable [-4,5] 24 0 F14 Schwefel 2.", "startOffset": 138, "endOffset": 147}, {"referenceID": 8, "context": "5] 5 0 F7 Easom Unimodal Non-Separable [-100, 100] 2 -1 F8 Matyas Unimodal Non-Separable [-10, 10] 2 0 F9 Colville Unimodal Non-Separable [-10, 10] 4 0 F10 Trid6 Unimodal Non-Separable [-D,D] 6 -50 F11 Trid10 Unimodal Non-Separable [-D,D] 10 -210 F12 Zakharov Unimodal Non-Separable [-5,10] 10 0 F13 Powell Unimodal Non-Separable [-4,5] 24 0 F14 Schwefel 2.", "startOffset": 283, "endOffset": 290}, {"referenceID": 4, "context": "5] 5 0 F7 Easom Unimodal Non-Separable [-100, 100] 2 -1 F8 Matyas Unimodal Non-Separable [-10, 10] 2 0 F9 Colville Unimodal Non-Separable [-10, 10] 4 0 F10 Trid6 Unimodal Non-Separable [-D,D] 6 -50 F11 Trid10 Unimodal Non-Separable [-D,D] 10 -210 F12 Zakharov Unimodal Non-Separable [-5,10] 10 0 F13 Powell Unimodal Non-Separable [-4,5] 24 0 F14 Schwefel 2.", "startOffset": 330, "endOffset": 336}, {"referenceID": 8, "context": "22 Unimodal Non-Separable [-10, 10] 30 0", "startOffset": 26, "endOffset": 35}, {"referenceID": 8, "context": "2 Unimodal Non-Separable [-10, 10] 30 0 F16 Rosenbrock Unimodal Non-Separable [-30, 30] 30 0 F17 Dixon-Price Unimodal Non-Separable [-10, 10] 30 0 F18 Foxholes Multimodal Separable [-65.", "startOffset": 25, "endOffset": 34}, {"referenceID": 8, "context": "2 Unimodal Non-Separable [-10, 10] 30 0 F16 Rosenbrock Unimodal Non-Separable [-30, 30] 30 0 F17 Dixon-Price Unimodal Non-Separable [-10, 10] 30 0 F18 Foxholes Multimodal Separable [-65.", "startOffset": 132, "endOffset": 141}, {"referenceID": 8, "context": "998 F19 Branin Multimodal Separable [-5,10]x[0,15] 2 0.", "startOffset": 36, "endOffset": 43}, {"referenceID": 13, "context": "998 F19 Branin Multimodal Separable [-5,10]x[0,15] 2 0.", "startOffset": 44, "endOffset": 50}, {"referenceID": 8, "context": "F21 Booth Multimodal Separable [-10, 10] 2 0", "startOffset": 31, "endOffset": 40}, {"referenceID": 4, "context": "F28 Six Hump Camel Back Multimodal Non-Separable [-5, 5] 2 -1.", "startOffset": 49, "endOffset": 56}, {"referenceID": 8, "context": "F31 Shubert Multimodal Non-Separable [-10, 10] 2 -186.", "startOffset": 37, "endOffset": 46}, {"referenceID": 1, "context": "F32 GoldStein-Price Multimodal Non-Separable [-2, 2] 2 3", "startOffset": 45, "endOffset": 52}, {"referenceID": 4, "context": "F33 Kowalik Multimodal Non-Separable [-5, 5] 4 0.", "startOffset": 37, "endOffset": 44}, {"referenceID": 8, "context": "F34 Shekel5 Multimodal Non-Separable [0, 10] 4 -10.", "startOffset": 37, "endOffset": 44}, {"referenceID": 8, "context": "F35 Shekel7 Multimodal Non-Separable [0, 10] 4 -10.", "startOffset": 37, "endOffset": 44}, {"referenceID": 8, "context": "F36 Shekel10 Multimodal Non-Separable [0, 10] 4 -10.", "startOffset": 38, "endOffset": 45}, {"referenceID": 0, "context": "F39 Hartman3 Multimodal Non-Separable [0, 1] 3 -3.", "startOffset": 38, "endOffset": 44}, {"referenceID": 0, "context": "F40 Hartman6 Multimodal Non-Separable [0, 1] 6 -3.", "startOffset": 38, "endOffset": 44}, {"referenceID": 26, "context": "47 F42 Ackley Multimodal Non-Separable [-32, 32] 30 0", "startOffset": 39, "endOffset": 48}, {"referenceID": 8, "context": "F45 Langerman2 Multimodal Non-Separable [0, 10] 2 -1.", "startOffset": 40, "endOffset": 47}, {"referenceID": 8, "context": "F46 Langerman5 Multimodal Non-Separable [0, 10] 5 -1.", "startOffset": 40, "endOffset": 47}, {"referenceID": 8, "context": "F47 Langerman10 Multimodal Non-Separable [0, 10] 10 NA", "startOffset": 41, "endOffset": 48}, {"referenceID": 25, "context": "In [31], authors stated that after a sufficient value for colony size, any increment in the value does not improve the performance of the ABC algorithm significantly.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "For the test problems carried out in [31] colony sizes of 10, 50 and 100 are used for the ABC algorithm.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "Similarly, for the PSO algorithm it is reported that, PSO with different population sizes has almost the similar performance which means the performance of PSO is not sensitive to the population size [32].", "startOffset": 200, "endOffset": 204}], "year": 2016, "abstractText": "The Vortex Search (VS) algorithm is one of the recently proposed metaheuristic algorithms which was inspired from the vortical flow of the stirred fluids. Although the VS algorithm is shown to be a good candidate for the solution of certain optimization problems, it also has some drawbacks. In the VS algorithm, candidate solutions are generated around the current best solution by using a Gaussian distribution at each iteration pass. This provides simplicity to the algorithm but it also leads to some problems along. Especially, for the functions those have a number of local minimum points, to select a single point to generate candidate solutions leads the algorithm to being trapped into a local minimum point. Due to the adaptive step-size adjustment scheme used in the VS algorithm, the locality of the created candidate solutions is increased at each iteration pass. Therefore, if the algorithm cannot escape a local point as quickly as possible, it becomes much more difficult for the algorithm to escape from that point in the latter iterations. In this study, a modified Vortex Search algorithm (MVS) is proposed to overcome above mentioned drawback of the existing VS algorithm. In the MVS algorithm, the candidate solutions are generated around a number of points at each iteration pass. Computational results showed that with the help of this modification the global search ability of the existing VS algorithm is improved and the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABC algorithms for the benchmark numerical function set.", "creator": "PScript5.dll Version 5.2.2"}}}