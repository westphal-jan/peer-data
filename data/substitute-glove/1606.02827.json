{"id": "1606.02827", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Variational Information Maximization for Feature Selection", "abstract": "Feature selection most one of brought even fundamental suffering time instead teaches. An preservation entire entire work month communication - axioms story selection exists latter is network on maximizing mutual internet however permutations several images and category labels. Practical experimental not forced to rely on approximations due to the difficulty of estimating promote information. We willingness that approximations made that addition methods are as on unrealistic assumptions. We solutions a one easier such general for of societal publishing on darwinian distributions and other why as tractably creates typically boundary addition promote news. These bounds define followed mentions check - theoretic objectives this feature selection, also know unlikely even be linear into poplar front-end models by proper for means formalization short-term. Our experiments acknowledge all the extend e.g. viewed outperforms existing information - theoretic shows unusual rather.", "histories": [["v1", "Thu, 9 Jun 2016 05:19:23 GMT  (542kb,D)", "http://arxiv.org/abs/1606.02827v1", "15 pages, 9 figures"]], "COMMENTS": "15 pages, 9 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shuyang gao", "greg ver steeg", "aram galstyan"], "accepted": true, "id": "1606.02827"}, "pdf": {"name": "1606.02827.pdf", "metadata": {"source": "CRF", "title": "Variational Information Maximization for Feature Selection", "authors": ["Shuyang Gao", "Greg Ver Steeg", "Aram Galstyan"], "emails": ["gaos@usc.edu,", "gregv@isi.edu,", "galstyan@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "Feature selection is one of the fundamental problems in machine learning research [1, 2]. Many problems include a large number of features that are either irrelevant or redundant for the task at hand. In these cases, it is often advantageous to pick a smaller subset of features to avoid over-fitting, to speed up computation, or simply to improve the interpretability of the results.\nFeature selection approaches are usually categorized into three groups: wrapper, embedded and filter [3, 4, 5]. The first two methods, wrapper and embedded, are considered classifier-dependent, i.e., the selection of features somehow depends on the classifier being used. Filter methods, on the other hand, are classifier-independent and define a scoring function between features and labels in the selection process.\nBecause filter methods may be employed in conjunction with a wide variety of classifiers, it is important that the scoring function of these methods is as general as possible. Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.\nOwing to the difficulty of estimating mutual information in high dimensions, most existing MI-based feature selection methods are based on various low-order approximations for mutual information. While those approximations have been successful in certain applications, they are heuristic in nature and lack theoretical guarantees. In fact, as we demonstrate below (Sec. 2.2), a large family of approximate methods are based on two assumptions that are mutually inconsistent.\nTo address the above shortcomings, in this paper we introduce a novel feature selection method based on variational lower bound on mutual information; a similar bound was previously studied within the Infomax learning framework [13]. We show that instead of maximizing the mutual information, which is intractable in high dimensions (hence the introduction of many heuristics), we can maximize a lower bound on the MI with the proper choice of tractable variational distributions. We use this lower bound to define an objective function and derive a forward feature selection algorithm.\nar X\niv :1\n60 6.\n02 82\n7v 1\n[ st\nat .M\nL ]\n9 J\nun 2\n01 6\nWe provide a rigorous proof that the forward feature selection is optimal under tree graphical models by choosing an appropriate variational distribution. This is in contrast with previous informationtheoretic feature selection methods, which lack any performance guarantees. We also conduct empirical validation on various datasets and demonstrate that the proposed approach outperforms stateof-the-art information-theoretic feature selection methods.\nIn Sec. 2 we introduce general MI-based feature selection methods and discuss their limitations. Sec. 3 introduces the variational lower bound on mutual information and proposes two specific variational distributions. In Sec. 4, we report results from our experiments, and compare the proposed approach with existing methods."}, {"heading": "2 Information-Theoretic Feature Selection Background", "text": ""}, {"heading": "2.1 Mutual Information-Based Feature Selection", "text": "Consider a supervised learning scenario where x = {x1,x2, ...,xD} is a D-dimensional input feature vector, and y is the output label. In filter methods, the mutual information-based feature selection task is to select T features xS\u2217 = {xf1 ,xf2 , ...,xfT } such that the mutual information between xS\u2217 and y is maximized. Formally,\nS\u2217 = argmax S I (xS : y) s.t. |S| = T (1)\nwhere I(\u00b7) denotes the mutual information [6]. Forward Sequential Feature Selection Maximizing the objective function in Eq. 1 is generally NP-hard. Many MI-based feature selection methods adopt a greedy method, where features are selected incrementally, one feature at a time. Let St\u22121 = {xf1 ,xf2 , ...,xft\u22121} be the selected feature set after time step t \u2212 1. According to the greedy method, the next feature ft at step t is selected such that\nft = argmax i/\u2208St\u22121 I (xSt\u22121\u222ai : y) (2)\nwhere xSt\u22121\u222ai denotes x\u2019s projection into the feature space St\u22121 \u222a i. As shown in [5], the mutual information term in Eq. 2 can be decomposed as:\nI (xSt\u22121\u222ai : y) = I (xSt\u22121 : y) + I (xi : y|xSt\u22121) = I (xSt\u22121 : y) + I (xi : y)\u2212 I (xi : xSt\u22121) + I (xi : xSt\u22121 |y) = I (xSt\u22121 : y) + I (xi : y)\n\u2212 (H (xSt\u22121)\u2212H (xSt\u22121 |xi)) + (H (xSt\u22121 |y)\u2212H (xSt\u22121 |xi,y))\n(3)\nwhere H(\u00b7) denotes the entropy [6]. Omitting the terms that do not depend on xi in Eq. 3, we can rewrite Eq. 2 as follows:\nft = argmax i/\u2208St\u22121 I (xi : y) +H (xSt\u22121 |xi)\u2212H (xSt\u22121 |xi,y) (4)\nThe greedy learning algorithm has been analyzed in [14]."}, {"heading": "2.2 Limitations of Previous MI-Based Feature Selection Methods", "text": "Estimating high-dimensional information-theoretic quantities is a difficult task. Therefore most MI-based feature selection methods propose low-order approximation to H (xSt\u22121 |xi) and H (xSt\u22121 |xi,y) in Eq. 4. A general family of methods rely on the following approximations [5]:\nH (xSt\u22121 |xi) \u2248 t\u22121\u2211 k=1 H (xfk |xi)\nH (xSt\u22121 |xi,y) \u2248 t\u22121\u2211 k=1 H (xfk |xi,y)\n(5)\nThe approximations in Eq. 5 become exact under the following two assumptions [5]:\nAssumption 1. (Feature Independence Assumption) p (xSt\u22121 |xi) = t\u22121\u220f k=1 p (xfk |xi)\nAssumption 2. (Class-Conditioned Independence Assumption) p (xSt\u22121 |xi,y) = t\u22121\u220f k=1 p (xfk |xi,y) Assumption 1 and Assumption 2 mean that the selected features are independent and classconditionally independent, respectively, given the unselected feature xi under consideration.\nWe now demonstrate that the two assumptions cannot be valid simultaneously unless the data has a very specific (and unrealistic) structure. Indeed, consider the graphical models consistent with either assumption, as illustrated in Fig. 1. If Assumption 1 holds true, then xi is the only common cause of the previously selected features St\u22121 = {xf1 ,xf2 , ...,xft\u22121}, so that those features become independent when conditioned on xi. On the other hand, if Assumption 2 holds, then the features depend both on xi and class label y; therefore, generally speaking, distribution over those features does not factorize by solely conditioning on xi\u2014there will be remnant dependencies due to y. Thus, if Assumption 2 is true, then Assumption 1 cannot be true in general, unless the data is generated according to a very specific model shown in the rightmost model in Fig. 1. Note, however, that in this case, xi becomes the most important feature because I(xi : y) > I(xSt\u22121 : y); then we should have selected xi at the very first step, contradicting the feature selection process.\nAs we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc. Approaches based on global optimization of mutual information, such as quadratic programming feature selection (QPFS) [11] and state-of-the-art conditional mutual informationbased spectral method (SPECCMI) [12], are derived from the previous greedy methods and therefore also implicitly rely on those two assumptions.\nIn the next section we address these issues by introducing a novel information-theoretic framework for feature selection. Instead of estimating mutual information and making mutually inconsistent assumptions, our framework formulates a tractable variational lower bound on mutual information, which allows a more flexible and general class of assumptions via appropriate choices of variational distributions."}, {"heading": "3 Method", "text": ""}, {"heading": "3.1 Variational Mutual Information Lower Bound", "text": "Let p(x,y) be the joint distribution of input (x) and output (y) variables. Barber & Agkov [13] derived the following lower bound for mutual information I(x : y) by using the non-negativity of KL-divergence, i.e., \u2211 x p (x|y) log p(x|y) q(x|y) \u2265 0 gives:\nI (x : y) \u2265 H (x) + \u3008ln q (x|y)\u3009p(x,y) (6)\nwhere angled brackets represent averages and q(x|y) is an arbitrary variational distribution. This bound becomes exact if q(x|y) \u2261 p(x|y).\nIt is worthwhile to note that in the context of unsupervised representation learning, p(y|x) and q(x|y) can be viewed as an encoder and a decoder, respectively. In this case, y needs to be learned by maximizing the lower bound in Eq. 6 by iteratively adjusting the parameters of the encoder and decoder, such as [13, 17]."}, {"heading": "3.2 Variational Information Maximization for Feature Selection", "text": "Naturally, in terms of information-theoretic feature selection, we could also try to optimize the variational lower bound in Eq. 6 by choosing a subset of features S\u2217 in x, such that,\nS\u2217 = argmax S\n{ H (xS) + \u3008ln q (xS |y)\u3009p(xS ,y) } (7)\nHowever, the H(xS) term in RHS of Eq. 7 is still intractable when xS is very high-dimensional.\nNonetheless, by noticing that variable y is the class label, which is usually discrete, and henceH(y) is fixed and tractable, by symmetry we switch x and y in Eq. 6 and rewrite the lower bound as follows:\nI (x : y) \u2265 H (y) + \u3008ln q (y|x)\u3009p(x,y)\n= \u2329 ln ( q (y|x) p (y) )\u232a p(x,y)\n(8)\nThe equality in Eq. 8 is obtained by noticing that H(y) = \u3008\u2212 ln p (y)\u3009p(y).\nBy using Eq. 8, the lower bound optimal subset S\u2217 of x becomes:\nS\u2217 = argmax S\n{\u2329 ln ( q (y|xS) p (y) )\u232a p(xS ,y) } (9)"}, {"heading": "3.2.1 Choice of Variational Distribution", "text": "q(y|xS) in Eq. 9 can be any distribution as long as it is normalized. We need to choose q(y|xS) to be as general as possible while still keeping the term \u3008ln q (y|xS)\u3009p(xS ,y) tractable in Eq. 9.\nAs a result, we set q(y|xS) as\nq (y|xS) = q (xS ,y)\nq (xS) = q (xS |y) p (y)\u2211 y\u2032 q (xS |y\u2032) p (y\u2032) (10)\nWe can verify that Eq. 10 is normalized even if q(xS |y) is not normalized. If we further denote,\nq (xS) = \u2211 y\u2032 q (xS |y\u2032) p (y\u2032) (11)\nthen by combining Eqs. 9, 10, we get, I (xS : y) \u2265 \u2329 ln ( q (xS |y) q (xS) )\u232a p(xS ,y) \u2261 ILB (xS : y) (12)\nAuto-Regressive Decomposition. Now that q(y|xS) is defined, all we need to do is model q(xS |y) under Eq. 10, and q(xS) is easy to compute based on q(xS |y). Here we decompose q(xS |y) as an auto-regressive distribution assuming T features in S:\nq (xS |y) = q (xf1 |y) T\u220f\nt=2\nq (xft |xf<t ,y) (13)\nwhere xf<t denotes {xf1 ,xf2 , ...,xft\u22121}. The graphical model in Fig. 2 demonstrates this decom-\nposition. The main advantage of this model is that it is well-suited for the forward feature selection procedure where one feature is selected at a time (which we will explain in Sec. 3.2.3). And if q (xft |xf<t ,y) is tractable, then so is the whole distribution q(xS |y). Therefore, we would find tractable Q-Distributions over q (xft |xf<t ,y). Below we illustrate two such Q-distributions. Naive Bayes Q-distribution. An natural idea would be to assume xt is independent of other variables given y, i.e.,\nq (xft |xf<t ,y) = p (xft |y) (14)\nThen the variational distribution q(y|xS) can be written based on Eqs. 10 and 14 as follows:\nq (y|xS) = p (y)\n\u220f j\u2208S\np (xj |y)\u2211 y\u2032 p (y\u2032) \u220f j\u2208S p (xj |y\u2032) (15)\nAnd we also have the following theorem:\nTheorem 3.1 (Exact Naive Bayes). Under Eq. 15, the lower bound in Eq. 8 becomes exact if and only if data is generated by a Naive Bayes model, i.e., p (x,y) = p (y) \u220f i p (xi|y).\nThe proof for Theorem 3.1 becomes obvious by using the mutual information definition. Note that the most-cited MI-based feature selection method mRMR [10] also assumes conditional independence given the class label y as shown in [5, 18, 19], but they make additional stronger independence assumptions among only feature variables.\nPairwise Q-distribution. We now consider an alternative approach that is more general than the Naive Bayes distribution:\nq (xft |xf<t ,y) = ( t\u22121\u220f i=1 p (xft |xfi ,y) ) 1 t\u22121\n(16)\nIn Eq. 16, we assume q (xft |xf<t ,y) to be the geometric mean of conditional distributions q(xft |xfi ,y). This assumption is tractable as well as reasonable because if the data is generated by a Naive Bayes model, the lower bound in Eq. 8 also becomes exact using Eq. 16 due to p (xft |xfi ,y) \u2261 p (xft |y) in that case."}, {"heading": "3.2.2 Estimating Lower Bound From Data", "text": "Assuming either Naive Bayes Q-distribution or Pairwise Q-distribution, it is convenient to estimate q(xS |y) and q(xS) in Eq. 12 by using plug-in probability estimators for discrete data or one/twodimensional density estimator for continuous data. We also use the sample mean to approximate the expectation term in Eq. 12. Our final estimator for ILB (xS : y) is written as follows:\nI\u0302LB (xS : y) = 1\nN \u2211 x(k),y(k)\nln q\u0302 ( x (k) S |y(k) ) q\u0302 ( x (k) S\n) (17) where { x(k),y(k) } are samples from data, and q\u0302(\u00b7) denotes the estimate for q(\u00b7)."}, {"heading": "3.2.3 Variational Forward Feature Selection Under Auto-Regressive Decomposition", "text": "After defining q(y|xS) in Eq. 10 and auto-regressive decomposition of q(xS |y) in Eq. 14, we are able to do the forward feature selection previously described in Eq. 2, but replace the mutual information with its lower bound I\u0302LB . Recall that St\u22121 is the set of selected features after step t \u2212 1, then the feature ft will be selected at step t such that\nft = argmax i/\u2208St\u22121 I\u0302LB (xSt\u22121\u222ai : y) (18)\nwhere I\u0302LB (xSt\u22121\u222ai : y) can be obtained from I\u0302LB (xSt\u22121 : y) recursively by auto-regressive decomposition q (xSt\u22121\u222ai|y) = q (xSt\u22121 |y) q (xi|xSt\u22121 ,y) where q (xSt\u22121 |y) is stored at step t\u2212 1. This forward feature selection can be done under auto-regressive decomposition in Eqs. 10 and 13 for any Q-distribution. However, calculating q(xi|xSt ,y) may vary according to different Qdistributions. We can verify that it is easy to get q(xi|xSt ,y) recursively from q(xi|xSt\u22121 ,y) under Naive Bayes or Pairwise Q-distribution. We call our algorithm under these two Q-distributions VMInaive and VMIpairwise respectively. It is worthwhile noting that the lower bound does not always increase at each step. A decrease in lower bound at step t indicates that the Q-distribution would approximate the underlying distribution worse than it did at previous step t \u2212 1. In this case, the algorithm would re-maximize the lower bound from zero with only the remaining unselected features. We summarize the concrete implementation of our algorithms in supplementary Sec. A.\nTime Complexity. Although our algorithm needs to calculate the distributions at each step, we only need to calculate the probability value at each sample point. For both VMInaive and VMIpairwise, the total computational complexity is O(NDT ) assuming N as number of samples, D as total number of features, T as number of final selected features. The detailed time analysis is left for the supplementary Sec. A. As shown in Table 1, our methods VMInaive and VMIpairwise have the same time complexity as mRMR [10], while state-of-the-art global optimization method SPECCMI [12] is required to precompute the pairwise mutual information matrix, which gives an time complexity of O(ND2).\nOptimality Under Tree Graphical Models. Although our method VMInaive assumes a Naive Bayes model, we can prove that this method is still optimal if the data is generated according to tree graphical models. Indeed, both of our methods, VMInaive and VMIpairwise, will always prioritize the first layer features, as shown in Fig. 3. This optimality is summarized in Theorem B.1 in supplementary Sec. B."}, {"heading": "4 Experiments", "text": "We begin with the experiments on a synthetic model according to the tree structure illustrated in the left part of Fig. 3. The detailed data generating process is shown in supplementary section D. The root node Y is a binary variable, while other variables are continuous. We use VMInaive to optimize the lower bound ILB(x : y). 5000 samples are used to generate the synthethic data, and variational Q-distributions are estimated by kernel density estimator. We can see from the plot in the right part of Fig. 3 that our algorithm, VMInaive, selects x1, x2, x3 as the first three features, although x2 and x3 are only weakly correlated with y. If we continue to add deeper level features {x4, ...,x9}, the lower bound will decrease. For comparison, we also illustrate the mutual information between each single feature xi and y in Table 2. We can see from Table 2 that it would choose x1, x4 and x5 as the top three features by using the maximum relevance criteria [15]."}, {"heading": "4.1 Real-World Data", "text": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12]. We use 17 well-known datasets in previous feature selection studies [5, 12] (all data are discretized). The dataset summaries are illustrated in supplementary Sec. C. We use the average cross-validation error rate on the range of 10 to 100 features to compare different algorithms under the same setting as [12]. 10-fold cross-validation is employed for datasets with number of samples N \u2265 100 and leave-one-out cross-validation otherwise. The 3-Nearest-Neighbor classifier is used for Gisette and Madelon, following [5]. While for the remaining datasets, the classifier is chosen to be Linear SVM, following [11, 12].\nThe experimental results can be seen in Table 31. The entries with \u2217 and \u2217\u2217 indicate the best performance and the second best performance respectively (in terms of average error rate). We also use the paired t-test at 5% significant level to test the hypothesis that VMInaive or VMIpairwise performs significantly better than other methods, or vice visa. Overall, we find that both of our methods, VMInaive and VMIpairwise, strongly outperform other methods, indicating our variational feature selection framework is a promising addition to the current literature of information-theoretic feature selection.\n1we omit the results for MIM and CIFE due to space limitations, the complete results are shown in the supplementary Sec. C.\nWe also plot the average cross- validation error with respect to number of selected features. Fig. 4 shows the two most distinguishable data sets, Semeion and Gisette. We can see that both of our methods, VMINaive and VMIPairwise, have lower error rates in these two data sets."}, {"heading": "5 Related Work", "text": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few. Most of these methods are based on combinations of so-called relevant, redundant and complimentary information. Such combinations representing low-order approximations of mutual information are derived from two assumptions, and it has proved unrealistic to expect both assumptions to be true. Inspired by group testing [21], more scalable feature selection methods have been developed, but this method also requires the calculation of high-dimensional mutual information as a basic scoring function.\nEstimating mutual information from data requires an large number of observations\u2014especially when the dimensionality is high. The proposed variational lower bound can be viewed as a way of estimating mutual information between a high-dimensional continuous variable and a discrete variable. Only a few examples exist in literature [22] under this setting. We hope our method will shed light on new ways to estimate mutual information, similar to estimating divergences in [23]."}, {"heading": "6 Conclusion", "text": "Feature selection has been a significant endeavor over the past decade. Mutual information gives a general basis for quantifying the informativeness of features. Despite the clarity of mutual information, estimating it can be difficult. While a large number of information-theoretic methods exist, they are rather limited and rely on mutually inconsistent assumptions about underlying data distributions. We introduced a unifying variational mutual information lower bound to address these issues. We showed that by auto-regressive decomposition, feature selection can be done in a forward manner by progressively maximizing the lower bound. We also presented two concrete methods using Naive Bayes and Pairwise Q-distributions, which strongly outperform the existing methods. VMInaive only assumes a Naive Bayes model, but even this simple model outperforms the existing information-theoretic methods, indicating the effectiveness of our variational information maximization framework. We hope that our framework will inspire new mathematically rigorous algorithms for information-theoretic feature selection, such as optimizing the variational lower bound globally and developing more powerful variational approaches for capturing complex dependencies."}, {"heading": "A Detailed Algorithm for Variational Forward Feature Selection", "text": "We describe the detailed algorithm for our approach. We also provide open source code implementing VMInaive and VMIpairwise [24]. Concretely, let us suppose class label y is discrete and has L different values {y1, y2, ..., yL}; then we define the distribution q(xSt |y) vector Q(k)t of size L for each sample ( x(k),y(k) ) at step t:\nQ (k) t = [ q\u0302 ( x (k) St |y = y1 ) , ..., q\u0302 ( x (k) St |y = yL )]T (19)\nwhere x(k)St denotes the sample x (k) projects onto the xSt feature space.\nAlso, We further denote Y of size L\u00d7 1 as the distribution vector of y as follows:\nY = [p\u0302 (y = y1) , p\u0302 (y = y2) , ..., p\u0302 (y = yL)] T (20)\nThen we are able to rewrite q(xSt\u22121) and q(xSt\u22121 |y) in terms of Q (k) t\u22121, Y and substitute them into\nI\u0302LB(xSt\u22121 : y).\nTo illustrate, at step t\u2212 1 we have,\nI\u0302LB (xSt\u22121 : y) = 1\nN \u2211 x(k),y(k) log ( p ( x (k) St\u22121 |y = y (k) )) \u2212 1 N \u2211 k log ( Y TQ (k) t\u22121 ) (21)\nTo select a feature i at step t, let us define the conditional distribution vector C(k)i,t\u22121 for each feature i /\u2208 St\u22121 and each sample ( x(k),y(k) ) , i.e.,\nC (k) i,t\u22121 = [ q ( x (k) i |x (k) St\u22121 ,y = y1 ) , ..., q ( x (k) i |x (k) St\u22121 ,y = yL )]T (22)\nAt step t, we use C(k)i,t\u22121 and Q (k) t\u22121 previously stored and get,\nI\u0302LB (xSt\u22121\u222ai : y) = 1\nN \u2211 x(k),y(k) log ( p ( x (k) St\u22121 |y = y (k) ) p ( x (k) i |x (k) St\u22121 ,y = y (k) ))\n\u2212 1 N \u2211 k log ( Y T diag ( Q (k) t\u22121 ) C (k) i,t\u22121 ) (23) We summarize our detailed implementation in Algorithm 1.\nUpdatingQ(k)t and C (k) i,t in Algorithm 1 may vary according to differentQ-distributions. But we can verify that under Naive Bayes Q-distribution or Pairwise Q-distribution, Q(k)t and C (k) i,t can be obtained recursively from Q(k)t\u22121 and C (k) i,t\u22121 by noticing that q (xi|xSt ,y) = p (xi|y) for Naive Bayes\nQ-distribution and q (xi|xSt ,y) = ( p (xi|xft , y) q(xi|xSt\u22121 ,y) t\u22121 )t for Pairwise Q-distribution.\nLet us denote N as number of samples, D as total number of features, T as number of selected features and L as number of distinct values in class variable y. The computational complexity of Algorithm 1 involves calculating the lower bound for each feature i at every step which is O(NDL); updating C(k)i,t would cost O(NDL) for pairwise Q-distribution and O(1) for Naive Bayes Q-distribution; updating Q(k)t would cost O(NDL). We need to select T features, therefore the time complexity is O(NDT )2.\n2we ignore L here because the number of classes is usually much smaller.\nAlgorithm 1 Variational Forward Feature Selection (VMI) Data: ( x(1),y(1) ) , ( x(2),y(2) ) , ..., ( x(N),y(N) ) Input: T \u2190 {number of features to select} Output: F \u2190 {final selected feature set} F \u2190 {\u2205}; S0 \u2190 {\u2205}; t\u2190 1 Initialize Q(k)0 and C (k) i,0 for any feature i; calculate Y\nwhile |F | < T do I\u0302LB (xSt\u22121\u222ai : y)\u2190 {Eq. 23 for each i not in F} ft \u2190 argmax\ni/\u2208St\u22121 I\u0302LB (xi\u222aSt\u22121 : y) if I\u0302LB ( xSt\u22121\u222aft : y ) \u2264 I\u0302LB (xSt\u22121 : y) then\nClear S; Set t\u2190 1 else F \u2190 F \u222a ft St \u2190 St\u22121 \u222a ft Update Q(k)t and C (k) i,t\nt\u2190 t+ 1 end if\nend while"}, {"heading": "B Optimality under Tree Graphical Models", "text": "Theorem B.1 (Optimal Feature Selection). If data is generated according to tree graphical models, where the class label y is the root node, denote the child nodes set in the first layer as L1 = {x1,x2, ...,xL1}, as shown in Fig. B.1. Then there must exist a step T > 0 such that the following three conditions hold by using VMInaive or VMIpairwise:\nCondition I: The selected feature set ST \u2282 L1. Condition II: ILB(xSt : y) = I(xSt : y) for 1 \u2264 t \u2264 T . Condition III: ILB(xST : y) = I(x : y).\nProof. We prove this theorem by induction. For tree graphical model when selecting the first layer features, VMInaive and VMIpairwise are mathematically equal, therefore we only prove VMInaive case and VMIpairwise follows the same proof.\n1) At step t = 1, for each feature i, we have,\nILB (xi : y) =\n\u2329 ln ( q (xi|y) q (xi) )\u232a p(x,y)\n= \u2329 ln  p (xi|y)\u2211 y\u2032 p (y\u2032) p (xi|y\u2032) \u232a p(x,y)\n= \u2329 ln ( p (xi|y) p (xi) )\u232a p(x,y) = I (xi : y)\n(24)\nThus, we are choosing a feature that has the maximum mutual information with y at the very first step. Based on the data processing inequality, we have I(xi : y) \u2265 I(desc(xi) : y) for any xi in layer 1 where desc(xi) represents any descendant of xi. Thus, we always select features among the nodes of the first layer at step t = 1 without loss of generality. If node xj that is not in the first layer is selected at step t = 1, denote ances(xj) as xj\u2019s ancestor in layer 1, then I(xj : y) = I(ances(xj) : y) which means that the information is not lost from ances(xj)\u2192 xj . In this case, one can always switch ances(xj) with xj and let xj be in the first layer, which does not conflict with the model assumption.\nTherefore, condition I and II are satisfied in step t = 1.\n2) Assuming condition I and II are satisfied in step t, then we have the following argument in step t+ 1:\nWe discuss the candidate nodes in three classes, and argue that nodes in Remaining-Layer 1 Class are always being selected.\nRedundant Class For any descendant desc(St) of selected feature set St, we have, I ( xSt\u222adesc(St) : y ) = I (xSt : y) = ILB (xSt : y) (25)\nEq. 25 comes from the fact that the desc(St) carries no additional information about y other than St. The second equality is by induction.\nBased on Eq. 12 and 25, we have, ILB ( xSt\u222adesc(St) : y ) < I ( xSt\u222adesc(St) : y ) = I (xSt : y)\n(26)\nWe assume here that the LHS is strictly less than RHS in Eq. 26 without loss of generality. This is because if the equality holds, we have p (xSt |y) p (desc (St) |y) = p (xt, desc (St) |y) due to Theorem 3.1. In this case, we can always rearrange desc(St) to the first layer, which does not conflict with the model assumption.\nNote that by combining Eqs. 25 and 26, we can also get ILB ( xSt\u222adesc(St) : y ) < ILB (xSt : y) (27)\nEq. 27 means that adding a feature in Redundant Class will actually decrease the value of lower bound ILB .\nRemaining-Layer1 Class For any other unselected node j of the first layer, i.e., j \u2208 L1\\St, we have I (xSt : y) \u2264 I (xSt\u222aj : y) = ILB (xSt\u222aj : y) (28)\nThe inequality in Eq. 28 is obvious which comes from the data processing inequality [6]. And the equality in Eq. 28 comes directly from Theorem 3.1.\nDescendants-of-Remaining-Layer1 Class For any node desc(j) that is the descendant of j where j \u2208 L1\\St, we have,\nILB ( xSt\u222adesc(j) : y ) \u2264 I ( xSt\u222adesc(j) : y ) I ( xSt\u222adesc(j) : y ) \u2264 I (xSt\u222aj : y)\n(29)\nThe second inequality of Ineq. 29 also comes from data processing inequality.\nCombining Eqs. 26 and 28, we get, ILB ( xSt\u222adesc(St) : y ) < ILB (xSt\u222aj : y) (30)\nCombining Eqs. 28 and 29, we get, ILB ( xSt\u222adesc(j) : y ) \u2264 ILB (xSt\u222aj : y) (31)\nIneq. 30 essentially tells us the forward feature selection will always choose Remaining-Layer1 Class other than Redundant Class.\nIneq. 31 is saying we are choosing Remaining-Layer1 Class other than Descendants-of-RemainingLayer1 Class without loss of generality (for the equality concern, we can have the same argument in step t = 1).\nConsidering Ineqs. 30 and 31, in step t + 1, the algorithm chooses node j in Remaining-Layer1 Class, i.e., j \u2208 L1\\St. Therefore, condition I and II hold at step t+ 1.\nAt step t + 1, if ILB (xSt\u222aj : y) = ILB (xSt : y) for any j \u2208 L1\\St, that means I (xSt\u222aj : y) = I (xSt : y). Then we have,\nI (xSt : y) = I (xL1 : y) = I (x : y) (32)\nThe first equality in Eq. 32 holds because adding any j in L1\\St will not increase the mutual information. The second equality is due to the data processing inequality under tree graphical model assumption.\nTherefore, if ILB (xSt\u222aj : y) = ILB (xSt : y) for any j \u2208 L1\\St, we set T = t. Thus by combining condition II and Eq. 32, we have,\nILB (xST : y) = I (xST : y) = I (x : y) (33)\nThen condition III holds."}, {"heading": "C Datasets and Results", "text": "Table 4 summarizes the datasets used in the experiment. Table 5 shows the complete results.\nD ataset\nm R M R\nJM I\nM IM\nC M IM C IFE SP EC CM I\nV M I n a iv e V M I p a ir w ise\nL ung\n10.9\u00b1 (4.7) \u2217\u2217\n11.6\u00b1 (4.7)\n18.3\u00b1 (5.4)\n11.4\u00b1 (3.0)\n23.3\u00b1 (5.4)\n11.6\u00b1 (5.6)\n7.4\u00b1 (3.6) \u2217\n14.5\u00b1 (6.0)\nC olon\n19.7\u00b1 (2.6)\n17.3\u00b1 (3.0)\n22.0\u00b1 (4.3)\n18.4\u00b1 (2.6)\n23.5\u00b1 (4.3)\n16.1\u00b1 (2.0)\n11.2\u00b1 (2.7) \u2217\n11.9\u00b1 (1.7) \u2217\u2217\nL eukem ia 0.4\u00b1 (0.7) 1.4\u00b1 (1.2) 2.5\u00b1 (1.1) 1.1\u00b1 (2.0) 4.9\u00b1 (1.9) 1.8\u00b1 (1.3) 0.0\u00b1 (0.1) \u2217 0.2\u00b1 (0.5) \u2217\u2217 Lym phom a 5.6\u00b1 (2.8) 6.6\u00b1 (2.2) 13.0\u00b1 (6.4) 8.6\u00b1 (3.3) 35.6\u00b1 (4.3) 12.0\u00b1 (6.6) 3.7\u00b1 (1.9) \u2217 5.2\u00b1 (3.1) \u2217\u2217 Splice 13.6\u00b1 (0.4) \u2217 13.7\u00b1 (0.5) 13.6\u00b1 (0.5) \u2217\u2217 13.7\u00b1 (0.5) 14.7\u00b1 (0.3) 13.7\u00b1 (0.5) 13.7\u00b1 (0.5) 13.7\u00b1 (0.5) L andsat 19.5\u00b1 (1.2) 18.9\u00b1 (1.0) 22.0\u00b1 (3.8) 19.1\u00b1 (1.1) 19.7\u00b1 (1.7) 21.0\u00b1 (3.5) 18.8\u00b1 (0.8) \u2217 18.8\u00b1 (1.0) \u2217\u2217 W aveform 15.9\u00b1 (0.5) \u2217 15.9\u00b1 (0.5) \u2217 16.1\u00b1 (0.8) 16.0\u00b1 (0.7) 22.8\u00b1 (2.2) 15.9\u00b1 (0.6) \u2217\u2217 15.9\u00b1 (0.6) \u2217\u2217 15.9\u00b1 (0.5) \u2217 K rV sK p 5.1\u00b1 (0.7) 5.2\u00b1 (0.6) 5.3\u00b1 (0.6) 5.3\u00b1 (0.5) 5.0\u00b1 (0.7) \u2217 5.1\u00b1 (0.6) \u2217\u2217 5.3\u00b1 (0.5) 5.1\u00b1 (0.7) Ionosphere 12.8\u00b1 (0.9) 16.6\u00b1 (1.6) 13.3\u00b1 (0.9) 13.1\u00b1 (0.8) 16.1\u00b1 (1.6) 16.8\u00b1 (1.6) 12.7\u00b1 (1.9) \u2217\u2217 12.0\u00b1 (1.0) \u2217 Sem eion 23.4\u00b1 (6.5) 24.8\u00b1 (7.6) 26.7\u00b1 (9.7) 16.3\u00b1 (4.4) 28.6\u00b1 (5.8) 26.0\u00b1 (9.3) 14.0\u00b1 (4.0) \u2217 14.5\u00b1 (3.9) \u2217\u2217 M ultifeat. 4.0\u00b1 (1.6) 4.0\u00b1 (1.6) 4.9\u00b1 (2.3) 3.6\u00b1 (1.2) 7.2\u00b1 (3.0) 4.8\u00b1 (3.0) 3.0\u00b1 (1.1) \u2217 3.5\u00b1 (1.1) \u2217\u2217 O ptdigits 7.6\u00b1 (3.3) 7.6\u00b1 (3.2) 7.9\u00b1 (3.9) 7.5\u00b1 (3.4) \u2217\u2217 8.1\u00b1 (4.2) 9.2\u00b1 (6.0) 7.2\u00b1 (2.5) \u2217 7.6\u00b1 (3.6) M usk2 12.4\u00b1 (0.7) \u2217 12.8\u00b1 (0.7) 14.0\u00b1 (1.2) 13.0\u00b1 (1.0) 13.2\u00b1 (0.6) 15.1\u00b1 (1.8) 12.8\u00b1 (0.6) 12.6\u00b1 (0.5) \u2217\u2217 Spam base 6.9\u00b1 (0.7) 7.0\u00b1 (0.8) 7.3\u00b1 (0.9) 6.8\u00b1 (0.7) \u2217\u2217 10.3\u00b1 (1.8) 9.0\u00b1 (2.3) 6.6\u00b1 (0.3) \u2217 6.6\u00b1 (0.3) \u2217 Prom oter 21.5\u00b1 (2.8) 22.4\u00b1 (4.0) 21.7\u00b1 (3.1) 22.1\u00b1 (2.9) 27.4\u00b1 (3.2) 24.0\u00b1 (3.7) 21.2\u00b1 (3.9) \u2217\u2217 20.4\u00b1 (3.1) \u2217 G isette 5.5\u00b1 (0.9) 5.9\u00b1 (0.7) 7.2\u00b1 (1.2) 5.1\u00b1 (1.3) 6.5\u00b1 (0.8) 7.1\u00b1 (1.3) 4.8\u00b1 (0.9) \u2217\u2217 4.2\u00b1 (0.8) \u2217 M adelon 30.8\u00b1 (3.8) 15.3\u00b1 (2.6) \u2217\u2217 16.8\u00b1 (2.7) 17.4\u00b1 (2.6) 15.1\u00b1 (2.7) \u2217 15.9\u00b1 (2.5) 16.7\u00b1 (2.7) 16.6\u00b1 (2.9) #W 1 /T 1 /L 1 : 11/4/2 10/6/1 11/6/0 10/7/0 15/0/2 13/2/2 #W 2 /T 2 /L 2 : 9/6/2 9/6/2 15/2/0 13/3/1 15/1/1 12/3/2 Table 5:Average cross validation error rate com parison ofV M I againstother m ethods.T he lasttw o lines indicate w in(W )/tie(T )/loss(L )for V M I n a iv e and V M I p a ir w is e respectively."}, {"heading": "D Generating Synthetic Data", "text": "Here is a detailed generating process for synthetic tree graphical model data in the experiment.\nDraw y \u223c Bernoulli(0.5) Draw x1 \u223c Gaussian(\u03c3 = 1.0, \u00b5 = y) Draw x2 \u223c Gaussian(\u03c3 = 1.0, \u00b5 = y/1.5) Draw x3 \u223c Gaussian(\u03c3 = 1.0, \u00b5 = y/2.25) Draw x4 \u223c Gaussian(\u03c3 = 1.0, \u00b5 = x1) Draw x5 \u223c Gaussian(\u03c3 = 1.0, \u00b5 = x1) Draw x6 \u223c Gaussian(\u03c3 = 1.0, \u00b5 = x2) Draw x7 \u223c Gaussian(\u03c3 = 1.0, \u00b5 = x2) Draw x8 \u223c Gaussian(\u03c3 = 1.0, \u00b5 = x3) Draw x9 \u223c Gaussian(\u03c3 = 1.0, \u00b5 = x3)"}], "references": [{"title": "Feature selection for classification", "author": ["Manoranjan Dash", "Huan Liu"], "venue": "Intelligent data analysis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Feature selection for knowledge discovery and data mining, volume 454", "author": ["Huan Liu", "Hiroshi Motoda"], "venue": "Springer Science & Business Media,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H John"], "venue": "Artificial intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection", "author": ["Gavin Brown", "Adam Pocock", "Ming-Jie Zhao", "Mikel Luj\u00e1n"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["Roberto Battiti"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Data visualization and feature selection: New algorithms for nongaussian data", "author": ["Howard Hua Yang", "John E Moody"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Fast binary feature selection with conditional mutual information", "author": ["Fran\u00e7ois Fleuret"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Quadratic programming feature selection", "author": ["Irene Rodriguez-Lujan", "Ramon Huerta", "Charles Elkan", "Carlos Santa Cruz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Effective global approaches for mutual information based feature selection", "author": ["Xuan Vinh Nguyen", "Jeffrey Chan", "Simone Romano", "James Bailey"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "The im algorithm: a variational approach to information maximization", "author": ["David Barber", "Felix Agakov"], "venue": "In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection", "author": ["Abhimanyu Das", "David Kempe"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Feature selection and feature extraction for text categorization", "author": ["David D Lewis"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Conditional infomax learning: an integrated framework for feature extraction and fusion", "author": ["Dahua Lin", "Xiaoou Tang"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Variational information maximisation for intrinsically motivated reinforcement learning", "author": ["Shakir Mohamed", "Danilo Jimenez Rezende"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "On the feature selection criterion based on an approximation of multidimensional mutual information", "author": ["Kiran S Balagani", "Vir V Phoha"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Can high-order dependencies improve mutual information based feature selection", "author": ["Nguyen Xuan Vinh", "Shuo Zhou", "Jeffrey Chan", "James Bailey"], "venue": "Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Conditional mutual information-based feature selection analyzing for synergy and redundancy", "author": ["Hongrong Cheng", "Zhiguang Qin", "Chaosheng Feng", "Yong Wang", "Fagen Li"], "venue": "ETRI Journal,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Parallel feature selection inspired by group testing", "author": ["Yingbo Zhou", "Utkarsh Porwal", "Ce Zhang", "Hung Q Ngo", "Long Nguyen", "Christopher R\u00e9", "Venu Govindaraju"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Mutual information between discrete and continuous data sets", "author": ["Brian C Ross"], "venue": "PloS one,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["XuanLong Nguyen", "Martin J Wainwright", "Michael I Jordan"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Minimum redundancy feature selection from microarray gene expression data", "author": ["Chris Ding", "Hanchuan Peng"], "venue": "Journal of bioinformatics and computational biology,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Feature selection is one of the fundamental problems in machine learning research [1, 2].", "startOffset": 82, "endOffset": 88}, {"referenceID": 1, "context": "Feature selection is one of the fundamental problems in machine learning research [1, 2].", "startOffset": 82, "endOffset": 88}, {"referenceID": 2, "context": "Feature selection approaches are usually categorized into three groups: wrapper, embedded and filter [3, 4, 5].", "startOffset": 101, "endOffset": 110}, {"referenceID": 3, "context": "Feature selection approaches are usually categorized into three groups: wrapper, embedded and filter [3, 4, 5].", "startOffset": 101, "endOffset": 110}, {"referenceID": 4, "context": "Feature selection approaches are usually categorized into three groups: wrapper, embedded and filter [3, 4, 5].", "startOffset": 101, "endOffset": 110}, {"referenceID": 5, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 7, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 8, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 9, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 10, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 11, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 4, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 197, "endOffset": 200}, {"referenceID": 12, "context": "To address the above shortcomings, in this paper we introduce a novel feature selection method based on variational lower bound on mutual information; a similar bound was previously studied within the Infomax learning framework [13].", "startOffset": 228, "endOffset": 232}, {"referenceID": 5, "context": "where I(\u00b7) denotes the mutual information [6].", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "As shown in [5], the mutual information term in Eq.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "where H(\u00b7) denotes the entropy [6].", "startOffset": 31, "endOffset": 34}, {"referenceID": 13, "context": "The greedy learning algorithm has been analyzed in [14].", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "A general family of methods rely on the following approximations [5]:", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "5 become exact under the following two assumptions [5]:", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 132, "endOffset": 135}, {"referenceID": 14, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 185, "endOffset": 189}, {"referenceID": 7, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 222, "endOffset": 225}, {"referenceID": 8, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 278, "endOffset": 281}, {"referenceID": 9, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 327, "endOffset": 331}, {"referenceID": 15, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 379, "endOffset": 383}, {"referenceID": 10, "context": "Approaches based on global optimization of mutual information, such as quadratic programming feature selection (QPFS) [11] and state-of-the-art conditional mutual informationbased spectral method (SPECCMI) [12], are derived from the previous greedy methods and therefore also implicitly rely on those two assumptions.", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "Approaches based on global optimization of mutual information, such as quadratic programming feature selection (QPFS) [11] and state-of-the-art conditional mutual informationbased spectral method (SPECCMI) [12], are derived from the previous greedy methods and therefore also implicitly rely on those two assumptions.", "startOffset": 206, "endOffset": 210}, {"referenceID": 12, "context": "Barber & Agkov [13] derived the following lower bound for mutual information I(x : y) by using the non-negativity of KL-divergence, i.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "6 by iteratively adjusting the parameters of the encoder and decoder, such as [13, 17].", "startOffset": 78, "endOffset": 86}, {"referenceID": 16, "context": "6 by iteratively adjusting the parameters of the encoder and decoder, such as [13, 17].", "startOffset": 78, "endOffset": 86}, {"referenceID": 9, "context": "Note that the most-cited MI-based feature selection method mRMR [10] also assumes conditional independence given the class label y as shown in [5, 18, 19], but they make additional stronger independence assumptions among only feature variables.", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "Note that the most-cited MI-based feature selection method mRMR [10] also assumes conditional independence given the class label y as shown in [5, 18, 19], but they make additional stronger independence assumptions among only feature variables.", "startOffset": 143, "endOffset": 154}, {"referenceID": 17, "context": "Note that the most-cited MI-based feature selection method mRMR [10] also assumes conditional independence given the class label y as shown in [5, 18, 19], but they make additional stronger independence assumptions among only feature variables.", "startOffset": 143, "endOffset": 154}, {"referenceID": 18, "context": "Note that the most-cited MI-based feature selection method mRMR [10] also assumes conditional independence given the class label y as shown in [5, 18, 19], but they make additional stronger independence assumptions among only feature variables.", "startOffset": 143, "endOffset": 154}, {"referenceID": 9, "context": "As shown in Table 1, our methods VMInaive and VMIpairwise have the same time complexity as mRMR [10], while state-of-the-art global optimization method SPECCMI [12] is required to precompute the pairwise mutual information matrix, which gives an time complexity of O(ND).", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "As shown in Table 1, our methods VMInaive and VMIpairwise have the same time complexity as mRMR [10], while state-of-the-art global optimization method SPECCMI [12] is required to precompute the pairwise mutual information matrix, which gives an time complexity of O(ND).", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "We can see from Table 2 that it would choose x1, x4 and x5 as the top three features by using the maximum relevance criteria [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 134, "endOffset": 138}, {"referenceID": 7, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 144, "endOffset": 147}, {"referenceID": 14, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 153, "endOffset": 157}, {"referenceID": 8, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 164, "endOffset": 167}, {"referenceID": 15, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 174, "endOffset": 178}, {"referenceID": 11, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 192, "endOffset": 196}, {"referenceID": 4, "context": "We use 17 well-known datasets in previous feature selection studies [5, 12] (all data are discretized).", "startOffset": 68, "endOffset": 75}, {"referenceID": 11, "context": "We use 17 well-known datasets in previous feature selection studies [5, 12] (all data are discretized).", "startOffset": 68, "endOffset": 75}, {"referenceID": 11, "context": "We use the average cross-validation error rate on the range of 10 to 100 features to compare different algorithms under the same setting as [12].", "startOffset": 140, "endOffset": 144}, {"referenceID": 4, "context": "The 3-Nearest-Neighbor classifier is used for Gisette and Madelon, following [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 10, "context": "While for the remaining datasets, the classifier is chosen to be Linear SVM, following [11, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 11, "context": "While for the remaining datasets, the classifier is chosen to be Linear SVM, following [11, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 4, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 6, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 7, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 8, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 9, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 14, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 10, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 11, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 19, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 20, "context": "Inspired by group testing [21], more scalable feature selection methods have been developed, but this method also requires the calculation of high-dimensional mutual information as a basic scoring function.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "Only a few examples exist in literature [22] under this setting.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "We hope our method will shed light on new ways to estimate mutual information, similar to estimating divergences in [23].", "startOffset": 116, "endOffset": 120}], "year": 2016, "abstractText": "Feature selection is one of the most fundamental problems in machine learning. An extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels. Practical methods are forced to rely on approximations due to the difficulty of estimating mutual information. We demonstrate that approximations made by existing methods are based on unrealistic assumptions. We formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information. These bounds define a novel information-theoretic framework for feature selection, which we prove to be optimal under tree graphical models with proper choice of variational distributions. Our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches.", "creator": "LaTeX with hyperref package"}}}