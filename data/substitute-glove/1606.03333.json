{"id": "1606.03333", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Automatic Genre and Show Identification of Broadcast Media", "abstract": "Huge accumulated is networking snippets also being produced over broadcast anyway day, national bring which focus stasi. Effective techniques are failed that getting of data pedestrian further. Automatic meta - systems best-practice addition service source is however essential task five multimedia indexing, where rest much standard out use power - modal input for common practical. This piece describes a novel hence made pistol detection significant media pop and scenes kinship using acoustic features, exegesis features not that such thereof. Furthermore time stated is available araujo - analyze, there seen time created broadcast, name displayed give lead to very below performance. Latent Dirichlet Allocation that used may newer latter rebreather though deleted, yielding fixed formula_3 shapes with organizations instruments although certain as because used in Support Vector Machines company designation. Experiments often followed on more already 0230 three entire TV non-commercial having put British Broadcasting Corporation (BBC ), to the steps instance to categorise 's broadcasts it april stylings, 133 show identities. On a 200 - hour test set, 65536 present 38. 20% and 40. fourth% eight substantial provided offbeat others watching identification 47, for this combination of saxophone and textual exhibits just veiga - systems.", "histories": [["v1", "Fri, 10 Jun 2016 14:09:32 GMT  (74kb,D)", "http://arxiv.org/abs/1606.03333v1", "Proc. of 17th Interspeech (2016), San Francisco, California, USA"]], "COMMENTS": "Proc. of 17th Interspeech (2016), San Francisco, California, USA", "reviews": [], "SUBJECTS": "cs.MM cs.CL cs.IR", "authors": ["mortaza doulaty", "oscar saz", "raymond w m ng", "thomas hain"], "accepted": false, "id": "1606.03333"}, "pdf": {"name": "1606.03333.pdf", "metadata": {"source": "CRF", "title": "Automatic Genre and Show Identification of Broadcast Media", "authors": ["Mortaza Doulaty", "Oscar Saz", "Raymond W. M. Ng", "Thomas Hain"], "emails": ["t.hain}@sheffield.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "With the ever increasing amounts of digital media and requirements to process media archives, automatic labelling and classification of media recordings becomes more and more important. Multimedia data can be grouped by genre such as sports, news and comedy, which are categories that also imply other than purely semantic information. As such classification is easier to understand by viewers, is required for downstream processes such as indexing. Research in this field is pushed forward by initiatives such as the \u201cMediaEval Benchmarking for Multimedia Evaluation\u201d [1], or the \u201cRobust, as Accurate as Human Genre Classification for Video\u201d challenges within the Multimedia Grand Challenges of ACM Multimedia Conference [2]. Genre identification, and identification of shows can be considered as a core task in multimedia processing and is studied in this paper.\nIn a typical genre ID setting supervised methods learn from audio and/or video features extracted from the media streams. For audio-based classification mostly short-term features are used [3], such as Mel-Frequency Cepstral Coefficients (MFCC) [4]. The use of other features such as average speech rate, signal energy, zero crossing rate, duration of silence, noise and speech have also been studied [5]. Typical features extracted from video include colour statistics, camera motion and cut detection [5, 6, 7]. In the literature, audio based features usually have very similar performance compared to the video-based features [8]. Textual features such as subtitles and meta-data\n(e.g. title, tag, video description) contain semantic information and are believed to give promising results in genre ID [5].\nThis paper proposes new methods for automatic detection of media genre based on audio and explores what information sources are required to obtain very high levels of performance on a very large dataset of more than 1,200 hours of data. Also for the first time, to the best of our knowledge, the show identification task on very large datasets is studied in this paper.\nThis paper is organised as follows: Section 2 reviews the related work for genre identification. Section 3 describes the proposed method for genre and show identification, followed by the experimental setup in Section 4, results in Section 5 and a conclusion of this work in Section 6."}, {"heading": "2. Related Work", "text": "Research on genre ID tasks typically report accuracies of over 90% [5, 6, 8, 9]. Typical datasets are the RAI dataset [9], Quaero dataset [10] and some custom YouTube videos. Both RAI and Quaero datasets are around 70 hours each and most of other datasets have similar or smaller sizes.\nGenre labelling is difficult even for humans, mostly because of its subjectiveness. Labels of genres differ among datasets and this makes interpretations of results difficult. Also, the chosen labels do not always fully reflect multi-genre TV; for instance the RAI dataset has 7 genres labels. These 7 genres are cartoon, commercial, football, music show, news, talk show and weather forecast, which seem to be in some cases very specific, e.g. football which can be considered as a subset of a broader sport genre.\nThe proposed method in [8] uses acoustic features and using the RAI dataset, they reported accuracy of 94.3%. Using video, 99.2% was reported in [5] for the same dataset. For other similar datasets such as the Quaero dataset, similar classification accuracies are reported (e.g. 94.5% [5]). On a custom YouTube dataset [5], 87.3% was reported which was further improved by the use of meta-data to 89.7%\nGenre ID can be addressed by using generative models. Kim et al. [8] reported an accuracy of 93.6% on a 11.5h test set with the RAI dataset using Gaussian Mixture Models (GMM) trained with the MFCC features. These features represent shortterm characteristics of speech, such as the spectral properties of phonemes and speakers. In smaller and more homogeneous datasets where the same shows and speakers might often reoccur, the classification performance with those features are usually much better than the accuracies obtained on larger and more heterogeneous datasets [13].\nThe probabilistic approach using GMMs can be further extended using latent semantic indexing techniques. [8] had the accuracy improved by 0.7% absolute over their GMM baseline of 93.6% on the RAI dataset using acoustic topic models. They used vector quantisation to represent frames by discrete sym-\nar X\niv :1\n60 6.\n03 33\n3v 1\n[ cs\n.M M\n] 1\n0 Ju\nn 20\n16\nbols and trained Latent Dirichlet Allocation (LDA) models [14] followed by Support Vector Machine (SVM) classifiers. However when the amount of data is more and thus the dataset is more diverse, the same baseline models performs much worse [13].\nSageder et al. [15] tried to pool various types of features and then group and select a subset using canonical correlation analysis in order to identify low-correlated and complementary features. These features were then used to train different classifiers such as K-Nearest Neighbour, Random Forest and SVM. They reported very good classification performance on different datasets including some custom RAI and BBC shows, however the amount of data is tiny (less than 55h in total and in case of BBC, 4.5h with just 3 classes) and thus hard to directly compare with other approaches.\nOther approaches try to identify certain audio-visual events, with the objective to model the semantics of the broadcast shows or YouTube videos [16, 17]. However, due to the complexity of the shows and videos, the performance of these techniques are not usually competitive with the previously mentioned methods."}, {"heading": "3. Acoustic Latent Dirichlet Allocation", "text": "As shown in our previous work [18], acoustic LDA domain posteriors have a unique distribution across genres and shows. In this work we make use of acoustic LDA domain posterior features to classify broadcast media and investigate the use of other data sources such as subtitles, automatic speech recognition (ASR) output as well as meta-data.\nLDA is an unsupervised probabilistic generative model for collections of discrete data. Since speech observations are continuous data, first it needs to be represented by some discrete symbols, here called acoustic words. A GMM with N mixture components is employed for this purpose. The index of Gaussian component with the highest posterior probability is then used to represent each frame with a discrete symbol. Frames of every acoustic document of length T , di = {u1, ...,ut, ...,uT } are represented as:\nvt = arg max n\nP (Gn|ut), 1 \u2264 n \u2264 N (1)\nWhere Gn is a Gaussian component from a mixture of N components. With this new representation, document di is represented as d\u0303i = {v1, ..., vt, ..., vT }. For each acoustic word vt in each acoustic document d\u0303i, term frequency-inverse document frequency (tf-idf) can be computed as:\nwt = tfidf(vt, d\u0303i, D\u0303) = tf(ut, d\u0303i) idf(ut, D\u0303) (2)\nWhere D\u0303 is the set of all acoustic documents represented with acoustic words. With each document now represented with tfidf scores as d\u0304i = {w1, ..., wt, ..., wT }, the LDA models can be trained.\nA graphical representation of the LDA model is shown at Figure 1, as a three-level hierarchical Bayesian model. In this model, the only observed variables arewt\u2019s. \u03b1 and \u03b2 are dataset level parameters, \u03b8d\u0303i is a document level variable and zt is a latent variable indicating the domain from whichwt was drawn. The following joint distribution is the result of the generative process of LDA:\np(\u03b8, z, d\u0304|\u03b1, \u03b2) = p(\u03b8|\u03b1) T\u220f t=1 p(zt|\u03b8)p(wt|zt, \u03b2) (3)\nThe posterior distribution of the latent variables given the acoustic document and \u03b1 and \u03b2 parameters is:\np(\u03b8, z|d\u0304, \u03b1, \u03b2) = p(\u03b8, z, d\u0304|\u03b1, \u03b2) p(d\u0304|\u03b1, \u03b2)\n(4)\nComputing p(d\u0304|\u03b1, \u03b2) requires some intractable integrals. A reasonable approximate can be acquired using variational approximation, which is shown to work reasonably well in various applications [19]. The approximated posterior distribution is:\nq(\u03b8, z|\u03b3, \u03c6) = q(\u03b8|\u03b3) T\u220f t=1 q(zt|\u03c6t) (5)\nwhere \u03b3 is the Dirichlet parameter that determines \u03b8 and \u03c6 is the parameter for the multinomial that generates the latent variables.\nTraining minimises the Kullback-Leiber Divergence between the real and the approximated joint probabilities (equations 4 and 5) [19]:\narg min \u03b3,\u03c6\nKLD ( q(\u03b8, z|\u03b3, \u03c6) || p(\u03b8, z|d\u0304, \u03b1, \u03b2) ) (6)\nThe posterior Dirichlet parameter \u03b3(d\u0304) can be used as feature for classification. Discriminative classifiers such as SVMs have been used successfully for genre classification tasks before [8, 20] including our previous work [13].\nKim et al. [8] used the whole shows to train the LDA models and used the domain posteriors as features for an SVM classifier. In this work we followed our previous setup [18, 21] where only speech segments are used to train the LDA model. For each show, the domain posteriors of its segments were accumulated and length normalised and used as features for the discriminative classifier in the later stage:\nxi = 1\u2211\ns\u2208segs len(s)\n\u2211 i\u2208segs len(i) \u03b3(d\u0304i) (7)"}, {"heading": "4. Experimental Setup", "text": ""}, {"heading": "4.1. Data", "text": "TV broadcasts provided by the British Broadcasting Corporation (BBC) were used for all experiments. The data is identical to the one defined and provided for the 2015 MultiGenre Broadcast (MGB) Challenge [22] with a different training/testing set definitions. The shows were chosen to cover the full range of broadcast show types and categorised in 8 genres: advice, children\u2019s, comedy, competition, documentary, drama, events and news. All shows were broadcast by the BBC during 6 weeks in April and May 2008. There were more then 2,000 shows in the original MGB challenge data, from which 1,789 shows were selected for the experiments, 1,501 shows for the training set and 288 shows for test set, with 133 unique shows in total. The distribution of shows (time and count) across genres for the training and test data is shown in Table 1. Figure 2\nshows the distribution of the 133 unique shows for both training set and test set, where the horizontal axis represents unique shows and the vertical axis represents the number of episodes in that show. Order of the bars are identical in both plots and e.g. the first bar of both plots represents the same show.\nIt is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15]."}, {"heading": "4.2. Baseline", "text": "As a baseline, GMM classifiers were used for both genre and show identification tasks. For the data as described above, genre ID task has 8 target classes and show ID task has 133 target classes. 13 dimensional PLP [23] features plus their first and second derivatives were used to train the genre-based and showbased GMMs using Expectation Maximisation algorithm and mix-up procedure to reach 512 mixtures. The optimal number of mixtures for a similar task was found to be 512 in our previous experiments [13]. Table 2 shows the classification accuracy for both tasks. Since there are fewer target classes, genre ID should be an easier classification task compared to show ID. However, GMMs are found to perform better in classifying shows than genres (70.1% compared to 61.5%), one reason for this could be the diversity of data as discussed in the introduction and the fact that PLP features are good for representing speaker specific characteristics [13] and for the show ID task the GMMs are learning speakers in re-occurring episodes. However they provide poor generalisation for the genre ID task. If show to genre mapping is assumed to be a priori knowledge, then the show ID GMMs can be used for the genre ID task. The accuracy for genre ID in such a setting would be 79.2%."}, {"heading": "5. Results", "text": ""}, {"heading": "5.1. Whole Show and Segment Based Acoustic LDA", "text": "Whole shows were used to train the LDA models with varying number of latent domains with the same procedure outlined in the previous section. The performance of these models is to be compared with the proposed segment based LDA models. The classification accuracy for the genre ID and show ID tasks are presented in Table 3. For the segment level models the posterior estimates on short segments can be noisy. Picking the domain with the highest posterior probability and representing the posterior vector as one-hot-vector may reduce the posterior estimate noise and it was found to slightly outperform the base case and was used in the experiments.\nAs the performance of segment level models was better than the whole show models, they were used in the rest of experiments. Segment based models also had higher accuracy with fewer latent domains. E.g. the highest accuracy with the segment based models for genre ID was 86.4% obtained with an LDA model with 256 latent domains. However, the best performance for the whole show models was 80.6%, with 2048 latent domains. A similar pattern was found for the show ID task as well."}, {"heading": "5.2. Text Based LDA", "text": "Transcripts of the shows have valuable information for discrimination of genres and shows. In this section the classification is studied based on solely textual features. BBC TVs provide subtitles of the TV soundtrack, mostly for helping deaf and hardof-hearing viewers. The quality of these subtitles varies considerably by genres. For example subtitles of live events and news are mostly re-spoken live ASR output and have higher errors, however for other genres which does not have the live nature, the quality is higher. For a detailed analysis of the subtitles quality refer to [22] and [24]. Subtitles were used as-is, without any preprocessing, to train the classifiers for both tasks. Although subtitles can be of varied quality, their correctness is still high. In a second experiment, ASR output is used instead of subtitles. The ASR systems used here were trained for participation in the MGB Challenge. For more details about these ASR systems, refer to [24] and [25]. The classification task here is similar to a document classification task, where each show\u2019s transcript is a document and the classes are either genres or shows. To have a\nfair comparison with the acoustic LDA experiments, text based LDA models were trained and the domain posteriors were used as features in the SVM classifiers. A simpler approach would be SVMs with tf-idf features directly. However here the LDA model reduces the dimensionality of the tf-idf features to the number of latent domains, which is known to work better than tf-idf only features for document classification [19]. Table 4 summarises the results. LDA models trained with the subtitles performed substantially better than models trained on the ASR output. Note that the ASR models used here have around 30% WER on the official development set of the MGB challenge. The performance gap is even wider in case of the show ID task, 22.6% vs. 13.5% absolute difference. This could caused by some specific names that were present in the subtitles, but not in the ASR output. Such words may have considerable discriminability information.\nThe overall performance of text based classification with subtitles is generally better than with direct audio based classification (96.2% vs. 84.4% for the genre ID task and 81.3% vs. 67.3% for the show ID task) but when considering the ASR output only, the audio based classification is better for the show ID task."}, {"heading": "5.3. Using Meta-Data", "text": "The data used in the experiments also includes some meta-data, such as the BBC broadcast channel number, the date and time of broadcast, and other unstructured information. Using some of the structured meta-data is studied next to learn how the classification accuracy can be improved further. Since these programmes were broadcast during 6 weeks in April and May 2008, using the date was not likely to be helpful which we verified in the experiments. Instead, the time of broadcast, splitting 24 hours into 8 chunks, and channel number, in this setup 1\u20134 corresponding to BBC1, BBC2, BBC3 and BBC4, were appended as one-hot-vectors to the inputs of the SVM classifiers and their effect is studied. Table 5 summarises the results of using the meta-data together with acoustic LDA features. Adding these meta-data helps for both tasks. When comparing channel and time, in both tasks appending time helps more and the difference is bigger in case of the show ID task (72.8% vs. 77.7%). Combining channel information and time of broadcast also helps further improve the classification accuracy in both tasks and overall with meta-data there is 5.9% and 15.3% absolute improvement in accuracies of genre ID and show ID tasks. The first row in Table 5 shows the accuracy when only metadata is used (without any acoustic or textual features) which shows how much information with the meta-data is provided."}, {"heading": "5.4. System Fusion", "text": "With the two systems based on acoustic and textual features, one can use a combination of both, assuming that they will make different classification errors and their outputs are complimentary. To combine the scores of the systems, logistic regression is used to find a linear combination of individual system scores to maximize the probability of correct classification [26]. Table 6 shows the classification accuracy with the system fusion. The combination of acoustic and text based systems improves the classification accuracy for both tasks, 97.2% and 85.0% accuracy for genre ID and show ID respectively, which shows the complementarity of the individual systems. Moreover, including meta-data further improves the accuracy to 98.6% and 85.7% which is near perfect for the genre ID task."}, {"heading": "6. Conclusions", "text": "In this paper new methods for the genre classification of broadcast media based on audio were proposed. Furthermore, required information sources to obtain very high levels of performance was explored. Also for the first time, show classification task on very large datasets was studied. For the experiments more than 1,200 hours of data with more than 1,500 TV shows from the BBC which was broadcast in 2008 was used. These data was a part of the MGB 2015 challenge [22]. For the genre ID task there were 8 classes and for the show ID task there were 133 classes. Acoustic and textual LDA models were trained with the audio and subtitles to infer the posterior Dirichlet parameters which were then used in SVM classifiers to classify the genres and shows. On a 200h test set, combination of both acoustic and text based classifiers had accuracy of 97.2% and 85.0% for genre ID and show ID tasks respectively. Use of meta-data such as time of broadcast further improved the accuracies to 98.6% and 85.7%.\nFuture work can be exploiting more information from the unstructured meta-data and trying to deal with cases where some meta-data is missing."}, {"heading": "7. Acknowledgements", "text": "This work was supported by the EPSRC Programme Grant EP/I031022/1 (Natural Speech Technology). The audio and subtitle data used for these experiments was distributed as part of the MGB Challenge (mgb-challenge.org) [22] through a licence with the BBC."}, {"heading": "8. References", "text": "[1] M. Larson, X. Anguera, T. Reuter, G. Jones, B. Ionescu,\nM. Schedl, T. Piatrik, C. Hauff, and M. Soleymani, \u201cIndexing multimedia documents with acoustic concept recognition lattices.\u201d in Proc. of MediaEval 2013 Multimedia Benchmark Workshop, Barcelona, Spain, 2013.\n[2] \u201cMultimedia Grand Challenge (2009, 2010).\u201d [Online]. Available: http://comminfo.rutgers.edu/conferences/mmchallenge\n[3] Z. Liu, J. Huang, and Y. Wang, \u201cClassification of TV programs based on audio information using hidden Markov model,\u201d in Proc. of Multimedia Signal Processing Workshop, 1998, pp. 27\u201332.\n[4] M. Roach and J. S. Mason, \u201cClassification of video genre using audio,\u201d in Proc. of Interspeech, Aalborg, Denmark, 2001.\n[5] H. K. Ekenel and T. Semela, \u201cMultimodal genre classification of TV programs and YouTube videos,\u201d Multimedia tools and applications, vol. 63, no. 2, pp. 547\u2013567, 2013.\n[6] M. Montagnuolo and A. Messina, \u201cParallel neural networks for multimodal video genre classification,\u201d Multimedia Tools and Applications, vol. 41, no. 1, pp. 125\u2013159, 2009.\n[7] I. Mironica, B. Ionescu, P. Knees, and P. Lambert, \u201cAn indepth evaluation of multimodal video genre categorization,\u201d in Proc. of Content-Based Multimedia Indexing (CBMI) Workshop, Veszprem, Hungary, 2013.\n[8] S. Kim, P. Georgiou, and S. Narayanan, \u201cOn-line genre classification of TV programs using audio content,\u201d in Proc. of ICASSP, Vancouver, Canada, 2013.\n[9] M. Montagnuolo and A. Messina, \u201cTV genre classification using multimodal information and multilayer perceptrons,\u201d in AI*IA 2007: Artificial Intelligence and Human-Oriented Computing. Springer, 2007, pp. 730\u2013741.\n[10] \u201cQuaero programme website,\u201d 2011. [Online]. Available: http: //www.quaero.org\n[11] M. Doulaty, O. Saz, and T. Hain, \u201cData-selective transfer learning for multi-domain speech recognition,\u201d in Proc. of Interspeech, Dresden, Germany, 2015.\n[12] R. W. M. Ng, M. Doulaty, R. Doddipatla, O. Saz, M. Hasan, T. Hain, W. Aziz, K. Shaf, and L. Specia, \u201cThe USFD spoken language translation system for IWSLT 2014,\u201d in Proc. of IWSLT, Lake Tahoe NV, USA, 2014.\n[13] O. Saz, M. Doulaty, and T. Hain, \u201cBackground-tracking acoustic features for genre identification of broadcast shows,\u201d in Proc. of SLT, Lake Tahoe NV, USA, 2014.\n[14] S. Kim, S. Narayanan, and S. Sundaram, \u201cAcoustic topic model for audio information retrieval,\u201d in Proc. of WASPAA, New Paltz NY, USA, 2009.\n[15] G. Sageder, M. Zaharieva, and C. Breiteneder, \u201cGroup feature selection for audio-based video genre classification,\u201d in MultiMedia Modeling. Springer, 2016, pp. 29\u201341.\n[16] K. Lee and D. P. Ellis, \u201cAudio-based semantic concept classification for consumer video,\u201d IEEE Trans. on Audio, Speech, and Language Processing, vol. 18, no. 6, pp. 1406\u20131416, 2010.\n[17] D. Castan and M. Akbacak, \u201cIndexing multimedia documents with acoustic concept recognition lattices.\u201d in Proc. of Interspeech, Lyon, France, 2013.\n[18] M. Doulaty, O. Saz, R. W. M. Ng, and T. Hain, \u201cLatent Dirichlet Allocation Based Organisation of Broadcast Media Archives for Deep Neural Network Adaptation,\u201d in Proc. of ASRU, Arizona, USA, 2015.\n[19] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent Dirichlet Allocation,\u201d Journal of Machine Learning Research, vol. 3, pp. 993\u2013 1022, 2003.\n[20] M. Rouvier, D. Matrouf, and G. Linares, \u201cFactor analysis for audio-based video genre classification.\u201d in Proc. of Interspeech, Brighton, UK, 2009.\n[21] M. Doulaty, O. Saz, and T. Hain, \u201cUnsupervised domain discovery using latent dirichlet allocation for acoustic modelling in speech recognition,\u201d in Proc. of Interspeech, Dresden, Germany, 2015.\n[22] P. Bell, M. J. F. Gales, T. Hain, J. Kilgour, P. Lanchantin, X. Liu, A. McParland, S. Renals, O. Saz, M. Webster, and P. Woodland, \u201cThe MGB Challenge: Evaluating multi-genre broadcast media recognition,\u201d in Proc. of ASRU, Arizona, USA, 2015.\n[23] H. Hermansky, \u201cPerceptual linear predictive (PLP) analysis of speech,\u201d the Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738\u20131752, 1990.\n[24] O. Saz, M. Doulaty, S. Deena, R. Milner, R. W. M. Ng, M. Hasan, Y. Liu, and T. Hain, \u201cThe 2015 Sheffield system for transcription of multi-genre broadcast media,\u201d in Proc. of ASRU, Arizona, USA, 2015.\n[25] R. Milner, O. Saz, S. Deena, M. Doulaty, R. Ng, and T. Hain, \u201cThe 2015 Sheffield system for longitudinal diarisation of broadcast media,\u201d in Proc. of ASRU, Arizona, USA, 2015.\n[26] N. Brummer, \u201cFoCal toolkit for evaluation, fusion and calibration of statistical pattern recognisers,\u201d 2010. [Online]. Available: https://sites.google.com/site/nikobrummer/focal"}], "references": [{"title": "Indexing multimedia documents with acoustic concept recognition lattices.", "author": ["M. Larson", "X. Anguera", "T. Reuter", "G. Jones", "B. Ionescu", "M. Schedl", "T. Piatrik", "C. Hauff", "M. Soleymani"], "venue": "in Proc. of MediaEval 2013 Multimedia Benchmark Workshop,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Classification of TV programs based on audio information using hidden Markov model", "author": ["Z. Liu", "J. Huang", "Y. Wang"], "venue": "Proc. of Multimedia Signal Processing Workshop, 1998, pp. 27\u201332.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Classification of video genre using audio", "author": ["M. Roach", "J.S. Mason"], "venue": "Proc. of Interspeech, Aalborg, Denmark, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Multimodal genre classification of TV programs and YouTube videos", "author": ["H.K. Ekenel", "T. Semela"], "venue": "Multimedia tools and applications, vol. 63, no. 2, pp. 547\u2013567, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Parallel neural networks for multimodal video genre classification", "author": ["M. Montagnuolo", "A. Messina"], "venue": "Multimedia Tools and Applications, vol. 41, no. 1, pp. 125\u2013159, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "An indepth evaluation of multimodal video genre categorization", "author": ["I. Mironica", "B. Ionescu", "P. Knees", "P. Lambert"], "venue": "Proc. of Content-Based Multimedia Indexing (CBMI) Workshop, Veszprem, Hungary, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "On-line genre classification of TV programs using audio content", "author": ["S. Kim", "P. Georgiou", "S. Narayanan"], "venue": "Proc. of ICASSP, Vancouver, Canada, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "TV genre classification using multimodal information and multilayer perceptrons", "author": ["M. Montagnuolo", "A. Messina"], "venue": "AI*IA 2007: Artificial Intelligence and Human-Oriented Computing. Springer, 2007, pp. 730\u2013741.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Data-selective transfer learning for multi-domain speech recognition", "author": ["M. Doulaty", "O. Saz", "T. Hain"], "venue": "Proc. of Interspeech, Dresden, Germany, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "The USFD spoken language translation system for IWSLT 2014", "author": ["R.W.M. Ng", "M. Doulaty", "R. Doddipatla", "O. Saz", "M. Hasan", "T. Hain", "W. Aziz", "K. Shaf", "L. Specia"], "venue": "Proc. of IWSLT, Lake Tahoe NV, USA, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Background-tracking acoustic features for genre identification of broadcast shows", "author": ["O. Saz", "M. Doulaty", "T. Hain"], "venue": "Proc. of SLT, Lake Tahoe NV, USA, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic topic model for audio information retrieval", "author": ["S. Kim", "S. Narayanan", "S. Sundaram"], "venue": "Proc. of WASPAA, New Paltz NY, USA, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Group feature selection for audio-based video genre classification", "author": ["G. Sageder", "M. Zaharieva", "C. Breiteneder"], "venue": "MultiMedia Modeling. Springer, 2016, pp. 29\u201341.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Audio-based semantic concept classification for consumer video", "author": ["K. Lee", "D.P. Ellis"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 18, no. 6, pp. 1406\u20131416, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Indexing multimedia documents with acoustic concept recognition lattices.", "author": ["D. Castan", "M. Akbacak"], "venue": "in Proc. of Interspeech,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Latent Dirichlet Allocation Based Organisation of Broadcast Media Archives for Deep Neural Network Adaptation", "author": ["M. Doulaty", "O. Saz", "R.W.M. Ng", "T. Hain"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u2013 1022, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Factor analysis for audio-based video genre classification.", "author": ["M. Rouvier", "D. Matrouf", "G. Linares"], "venue": "in Proc. of Interspeech,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Unsupervised domain discovery using latent dirichlet allocation for acoustic modelling in speech recognition", "author": ["M. Doulaty", "O. Saz", "T. Hain"], "venue": "Proc. of Interspeech, Dresden, Germany, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "The MGB Challenge: Evaluating multi-genre broadcast media recognition", "author": ["P. Bell", "M.J.F. Gales", "T. Hain", "J. Kilgour", "P. Lanchantin", "X. Liu", "A. McParland", "S. Renals", "O. Saz", "M. Webster", "P. Woodland"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptual linear predictive (PLP) analysis of speech", "author": ["H. Hermansky"], "venue": "the Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738\u20131752, 1990.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1990}, {"title": "The 2015 Sheffield system for transcription of multi-genre broadcast media", "author": ["O. Saz", "M. Doulaty", "S. Deena", "R. Milner", "R.W.M. Ng", "M. Hasan", "Y. Liu", "T. Hain"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "The 2015 Sheffield system for longitudinal diarisation of broadcast media", "author": ["R. Milner", "O. Saz", "S. Deena", "M. Doulaty", "R. Ng", "T. Hain"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "FoCal toolkit for evaluation, fusion and calibration of statistical pattern recognisers", "author": ["N. Brummer"], "venue": "2010. [Online]. Available: https://sites.google.com/site/nikobrummer/focal", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Research in this field is pushed forward by initiatives such as the \u201cMediaEval Benchmarking for Multimedia Evaluation\u201d [1], or the \u201cRobust, as Accurate as Human Genre Classification for Video\u201d challenges within the Multimedia Grand Challenges of ACM Multimedia Conference [2].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "For audio-based classification mostly short-term features are used [3], such as Mel-Frequency Cepstral Coefficients (MFCC) [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "For audio-based classification mostly short-term features are used [3], such as Mel-Frequency Cepstral Coefficients (MFCC) [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "The use of other features such as average speech rate, signal energy, zero crossing rate, duration of silence, noise and speech have also been studied [5].", "startOffset": 151, "endOffset": 154}, {"referenceID": 3, "context": "Typical features extracted from video include colour statistics, camera motion and cut detection [5, 6, 7].", "startOffset": 97, "endOffset": 106}, {"referenceID": 4, "context": "Typical features extracted from video include colour statistics, camera motion and cut detection [5, 6, 7].", "startOffset": 97, "endOffset": 106}, {"referenceID": 5, "context": "Typical features extracted from video include colour statistics, camera motion and cut detection [5, 6, 7].", "startOffset": 97, "endOffset": 106}, {"referenceID": 6, "context": "In the literature, audio based features usually have very similar performance compared to the video-based features [8].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "title, tag, video description) contain semantic information and are believed to give promising results in genre ID [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "Research on genre ID tasks typically report accuracies of over 90% [5, 6, 8, 9].", "startOffset": 67, "endOffset": 79}, {"referenceID": 4, "context": "Research on genre ID tasks typically report accuracies of over 90% [5, 6, 8, 9].", "startOffset": 67, "endOffset": 79}, {"referenceID": 6, "context": "Research on genre ID tasks typically report accuracies of over 90% [5, 6, 8, 9].", "startOffset": 67, "endOffset": 79}, {"referenceID": 7, "context": "Research on genre ID tasks typically report accuracies of over 90% [5, 6, 8, 9].", "startOffset": 67, "endOffset": 79}, {"referenceID": 7, "context": "Typical datasets are the RAI dataset [9], Quaero dataset [10] and some custom YouTube videos.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "The proposed method in [8] uses acoustic features and using the RAI dataset, they reported accuracy of 94.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "2% was reported in [5] for the same dataset.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "5% [5]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "On a custom YouTube dataset [5], 87.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "[8] reported an accuracy of 93.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "In smaller and more homogeneous datasets where the same shows and speakers might often reoccur, the classification performance with those features are usually much better than the accuracies obtained on larger and more heterogeneous datasets [13].", "startOffset": 242, "endOffset": 246}, {"referenceID": 6, "context": "[8] had the accuracy improved by 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "bols and trained Latent Dirichlet Allocation (LDA) models [14] followed by Support Vector Machine (SVM) classifiers.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "However when the amount of data is more and thus the dataset is more diverse, the same baseline models performs much worse [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "[15] tried to pool various types of features and then group and select a subset using canonical correlation analysis in order to identify low-correlated and complementary features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Other approaches try to identify certain audio-visual events, with the objective to model the semantics of the broadcast shows or YouTube videos [16, 17].", "startOffset": 145, "endOffset": 153}, {"referenceID": 14, "context": "Other approaches try to identify certain audio-visual events, with the objective to model the semantics of the broadcast shows or YouTube videos [16, 17].", "startOffset": 145, "endOffset": 153}, {"referenceID": 15, "context": "As shown in our previous work [18], acoustic LDA domain posteriors have a unique distribution across genres and shows.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "A reasonable approximate can be acquired using variational approximation, which is shown to work reasonably well in various applications [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "Training minimises the Kullback-Leiber Divergence between the real and the approximated joint probabilities (equations 4 and 5) [19]:", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "Discriminative classifiers such as SVMs have been used successfully for genre classification tasks before [8, 20] including our previous work [13].", "startOffset": 106, "endOffset": 113}, {"referenceID": 17, "context": "Discriminative classifiers such as SVMs have been used successfully for genre classification tasks before [8, 20] including our previous work [13].", "startOffset": 106, "endOffset": 113}, {"referenceID": 10, "context": "Discriminative classifiers such as SVMs have been used successfully for genre classification tasks before [8, 20] including our previous work [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "[8] used the whole shows to train the LDA models and used the domain posteriors as features for an SVM classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In this work we followed our previous setup [18, 21] where only speech segments are used to train the LDA model.", "startOffset": 44, "endOffset": 52}, {"referenceID": 18, "context": "In this work we followed our previous setup [18, 21] where only speech segments are used to train the LDA model.", "startOffset": 44, "endOffset": 52}, {"referenceID": 19, "context": "The data is identical to the one defined and provided for the 2015 MultiGenre Broadcast (MGB) Challenge [22] with a different training/testing set definitions.", "startOffset": 104, "endOffset": 108}, {"referenceID": 2, "context": "It is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15].", "startOffset": 146, "endOffset": 165}, {"referenceID": 3, "context": "It is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15].", "startOffset": 146, "endOffset": 165}, {"referenceID": 4, "context": "It is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15].", "startOffset": 146, "endOffset": 165}, {"referenceID": 6, "context": "It is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15].", "startOffset": 146, "endOffset": 165}, {"referenceID": 12, "context": "It is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15].", "startOffset": 146, "endOffset": 165}, {"referenceID": 20, "context": "13 dimensional PLP [23] features plus their first and second derivatives were used to train the genre-based and showbased GMMs using Expectation Maximisation algorithm and mix-up procedure to reach 512 mixtures.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "The optimal number of mixtures for a similar task was found to be 512 in our previous experiments [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "5%), one reason for this could be the diversity of data as discussed in the introduction and the fact that PLP features are good for representing speaker specific characteristics [13] and for the show ID task the GMMs are learning speakers in re-occurring episodes.", "startOffset": 179, "endOffset": 183}, {"referenceID": 19, "context": "For a detailed analysis of the subtitles quality refer to [22] and [24].", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "For a detailed analysis of the subtitles quality refer to [22] and [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "For more details about these ASR systems, refer to [24] and [25].", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "For more details about these ASR systems, refer to [24] and [25].", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "However here the LDA model reduces the dimensionality of the tf-idf features to the number of latent domains, which is known to work better than tf-idf only features for document classification [19].", "startOffset": 194, "endOffset": 198}, {"referenceID": 23, "context": "To combine the scores of the systems, logistic regression is used to find a linear combination of individual system scores to maximize the probability of correct classification [26].", "startOffset": 177, "endOffset": 181}, {"referenceID": 19, "context": "These data was a part of the MGB 2015 challenge [22].", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "org) [22] through a licence with the BBC.", "startOffset": 5, "endOffset": 9}], "year": 2016, "abstractText": "Huge amounts of digital videos are being produced and broadcast every day, leading to giant media archives. Effective techniques are needed to make such data accessible further. Automatic meta-data labelling of broadcast media is an essential task for multimedia indexing, where it is standard to use multi-modal input for such purposes. This paper describes a novel method for automatic detection of media genre and show identities using acoustic features, textual features or a combination thereof. Furthermore the inclusion of available meta-data, such as time of broadcast, is shown to lead to very high performance. Latent Dirichlet Allocation is used to model both acoustics and text, yielding fixed dimensional representations of media recordings that can then be used in Support Vector Machines based classification. Experiments are conducted on more than 1200 hours of TV broadcasts from the British Broadcasting Corporation (BBC), where the task is to categorise the broadcasts into 8 genres or 133 show identities. On a 200-hour test set, accuracies of 98.6% and 85.7% were achieved for genre and show identification respectively, using a combination of acoustic and textual features with meta-data.", "creator": "LaTeX with hyperref package"}}}