{"id": "1606.04212", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Active Discriminative Text Representation Learning", "abstract": "We re a key local concept (AL) computation another text commonly based on convolutional computation networks (CNNs ). In AL, one selects called describe taken be discarded appear turned the need many teamwork originally performance then minimal effort. Neural suv eager from probably cubics created popular, tuning fact all the needs taking all. We assert we AL analysis same degeneration document models done focus on criteria instances should most affect brought generalization space (i. readers. , induce distortionary word representations ). This is however contrast not simple AL approaches (server. 51. , risks repeatable ), which specify stocks each commitments. We propose similar simple argument once 60-member instances processed heard remains embeddings were moreover without indeed charts with full title 3.6, minimizing recovery learn assimilationist, preparation - parameters embeddings. Empirical results featuring adding our method swankier baseline AL direction.", "histories": [["v1", "Tue, 14 Jun 2016 06:49:40 GMT  (75kb,D)", "http://arxiv.org/abs/1606.04212v1", null], ["v2", "Mon, 21 Nov 2016 19:31:07 GMT  (140kb,D)", "http://arxiv.org/abs/1606.04212v2", "This paper got accepted by AAAI 2017"], ["v3", "Sun, 27 Nov 2016 05:58:25 GMT  (124kb,D)", "http://arxiv.org/abs/1606.04212v3", "This paper got accepted by AAAI 2017"], ["v4", "Thu, 1 Dec 2016 18:53:32 GMT  (125kb,D)", "http://arxiv.org/abs/1606.04212v4", "This paper got accepted by AAAI 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ye zhang", "matthew lease", "byron c wallace"], "accepted": true, "id": "1606.04212"}, "pdf": {"name": "1606.04212.pdf", "metadata": {"source": "CRF", "title": "Active Discriminative Word Embedding Learning", "authors": ["Ye Zhang", "Byron Wallace"], "emails": ["yezhang@utexas.edu", "byron.wallace@utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "In active learning (AL), the machine learning algorithm being trained is allowed to select the examples to be manually annotated (and in turn learned from) (Settles, 2010). The idea is that by selecting training data cleverly, rather than at i.i.d. random, better models can be learned at lower cost. This approach is particularly attractive in scenarios in which labels are expensive but unlabeled data is plentiful.\nThere has been a wealth of work on AL approaches for traditional machine learning methods generally (Settles, 2010), and for text classification in particular (Tong and Koller, 2002; McCallumzy\nand Nigamy, 1998; Wallace et al., 2010). But practically no work has considered AL for text classification using modern neural models. We argue that the representation learning facet of these models motivates different AL approaches.\nHere we propose a method for AL using convolutional neural networks (CNNs), which have recently achieved strong performance across a diverse set of text classification tasks (Kim, 2014; Zhang and Wallace, 2015; Johnson and Zhang, 2014; Zhang et al., 2016). It is natural to consider how to make the best use of CNNs when annotation resources are scarce; that is the question we address here. Word embedding estimation and tuning (for a specific text classification task) may be viewed as feature learning. It is reasonable to optimize feature vectors before expending effort to tune the parameters of a model that accepts these as input (note that adjusting the former will render updates to the latter potentially useless). Thus the objective in AL should be to select instances that result in better features.\nMore specifically, we propose a novel, simple AL approach in which we select instances that contain words likely to most affect the embeddings. We achieve this by calculating the expected gradient length (EGL) with respect to the embeddings for each word comprising the respective unlabeled instances. We show that this approach allows us to rapidly learn discriminative embeddings. For example, in the task of sentiment classification, selecting examples in this way quickly pushes the embeddings of \u2018bad\u2019 and \u2018good\u2019 apart. We show that this in turn improves performance over several baseline AL approaches.\nar X\niv :1\n60 6.\n04 21\n2v 1\n[ cs\n.C L\n] 1\n4 Ju\nn 20"}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 CNNs for Text Classification", "text": "Here we provide a brief review of CNNs for text classification, using the model proposed by Kim (2014). We will denote the word embedding matrix by E \u2208 RV\u00d7d, where V is the vocabulary size, and d is the dimension of the embedding layer. A specific instance (piece of text to be categorized) is then represented by stacking the vectors corresponding to the words it contains (stored in E), preserving word order. This results in an instance matrix A \u2208 RS\u00d7d, where S is the text length.\nWe then apply convolution operations on this matrix, using multiple linear filters. Each filter matrix Wi \u2208 Rh\u00d7d performs a convolution operation on A, generating a feature map ci \u2208 RS\u2212h+1. We then apply 1-max pooing to each ci, obtaining a feature value oi for this filter. (We note that we use multiple filter heights and redundant filters of each height.) Finally, we concatenate all oi to compose a final feature vector o for each instance. This is run through a softmax layer to induce a probability distribution over the output space. Typically this model is trained by minimizing the cross-entropy loss via back-propagation (Rumelhart et al., 1988). We provide a schematic illustrating a toy realization of this model in Figure 1. For more details, we refer the reader to (Kim, 2014; Zhang and Wallace, 2015)."}, {"heading": "2.2 Active Learning", "text": "We consider the pool-based AL scenario, in which it is assumed that there is a small set of labeled data L and a large pool of available unlabeled data U . The task for the learner is to draw examples selectively from this pool to be labeled. These selections, or queries, are typically made in a greedy fashion; an informativeness measure is used to evaluate all\ninstances in the pool, and the instance maximizing this measure is selected.\nThe key to developing AL strategies is designing a good informativeness measure. Let x\u2217 be the most informative instance according to a query strategy \u03c6(xi;\u03b8), which is a function used to evaluate each instance xi in the unlabeled pool U conditioned on the current set of parameter estimates \u03b8. Then\nx\u2217 = argmaxxi\u2208U\u03c6(xi;\u03b8) (1)\nIn the case of CNNs, \u03b8 includes word embedding parameters E, convolution layer parameters C, and softmax layer parameters W.\nMany querying strategies have been proposed in the literature (Settles, 2010). In this work we consider three general strategies. Random sampling. This strategy is equivalent to standard (or \u2018passive\u2019) learning; here the training data is simply an i.i.d. sample from U . Uncertainty sampling. One of the simplest and perhaps the most commonly used query framework is uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2002; Zhu et al., 2008). In this approach, the learner requests labels for instances about which it is least certain, with respect to categorization. A general uncertainty sampling variant uses entropy (Shannon, 2001) as an uncertainty measure, defining \u03c6(xi;\u03b8) as:\n\u2212 \u2211 k P (yi = k|xi;\u03b8)logP (yi = k|xi;\u03b8) (2)\nwhere k ranges over all possible labels. Entropy-based uncertainty sampling has been observed to achieve strong empirical performance across many tasks (Settles, 2010). Expected Gradient Length (EGL). This general AL strategy aims to select instances expected to result in the greatest change to the current model parameter estimates when their labels are revealed (or provided) (Settles and Craven, 2008). If the true class for a given instance were known, the gradient could be directly calculated under this assignment. But in practice this is unknown, and so the expectation is taken by marginalizing over the gradients calculated under possible class assignments, scaled by current model estimates of the posterior probabilities of said assignments."}, {"heading": "3 AL with Embeddings", "text": "We now introduce our proposed AL strategy for text classification with embeddings. This is based on the EGL method described above. In past work on EGL \u2013 which involved linear models \u2013 the expected change to model parameters was evaluated over the entire set of parameters \u03b8. By contrast, we propose explicitly selecting examples that are likely to affect the feature-level parameters (i.e., the embeddings).\nIn gradient-based optimization, the training gradient back-propagated to a set of model parameters given label yi for instance xi may be viewed as a measure of change imparted by example i. Thus the learner should request the label for an instance expected to produce a large magnitude training gradient. If this gradient is taken w.r.t. all model parameters, then this is a straight-forward instantiation of EGL. Formally, let \u2207J(L;\u03b8) be the gradient of the objective function J w.r.t. the model parameters \u03b8, where J is the cost function. Further, let \u2207J(L \u222a \u3008xi, yi\u3009;\u03b8) be the new gradient that would be obtained by adding the training tuple \u3008xi, yi\u3009 to L. Because the true label yi will be unknown, we take an expectation over possible class assignments k. More precisely, we can calculate \u03c6(xi;\u03b8) as:\u2211\nk\nP (yi = k|xi;\u03b8)\u2016\u2207J(L \u222a \u3008xi, yi = k\u3009;\u03b8)\u2016\n(3) where \u2016r\u2016 denotes the Euclidean norm of r. Note that at query time \u2016\u2207J(L;\u03b8)\u2016 should be near zero, assuming J converged during the previous iteration. Thus, we can approximate \u2207J(L \u222a \u3008xi, yi = k\u3009;\u03b8) \u2248 \u2207J(\u3008xi, yi = k\u3009;\u03b8) for efficiency.\nThis approach selects instances that are likely to most perturb the model parameters. Thus far we have treated all parameters \u03b8 as being equally important. However, \u2018deep\u2019 neural architectures are notable for their hierarchical structure, which corresponds to a large set of features distributed across different layers in the architecture; this makes calculating the EGL computationally expensive. More importantly, it is arguably incoherent to jointly consider the expected change at different layers in the model. If we view lower levels in the model as learning features, it makes little sense to jointly maximize expected change in these features and the parameters of the final softmax layer that accepts these\nas input. Changes to the former will immediately change the implications of perturbing the latter.\nInstead, we want to select unlabeled instances that can most improve the features learned by the model. Intuitively, it is paramount that the model learn good (discriminative) representations; these will feed forward through the network, in turn improving classification. Thus for each text instance in U , we take the expected gradient with respect to only the embeddings of its constituent words, and we select the example that maximizes this max-expected embedding gradient as our measure of informativeness. Intuitively, we use a max-over-words approach because we aim to adjust particular word embeddings that are discriminative for the task at hand. Formally, we define our \u03c6(xi;\u03b8) as:\nmax j\u2208xi \u2211 k P (yi = k|xi;\u03b8)\u2016\u2207JE(j)(\u3008xi, yi = k\u3009;\u03b8)\u2016 (4) Where we denote by \u2207JE(j) the gradient of J w.r.t. only the embedding of word j (j ranges over the words in xi). Note that the gradient is only taken for each word in the instance xi; the gradients on embeddings corresponding to words not in instance are 0 and can thus be ignored, which is a computational boon. We refer to this strategy as EGL-word.\nEGL-word focuses on parameters associated with the lowest level in the model (Figure 1). We also consider the other extreme: taking the gradient w.r.t only the final softmax layer parameters W. In this case \u03c6(xi;\u03b8) becomes:\u2211\nk\nP (yi = k|xi;\u03b8)\u2016\u2207JW(\u3008xi, yi = k\u3009;\u03b8)\u2016 (5)\nwhere JW denotes the gradient w.r.t. the softmax layer. We refer to this strategy as EGL-sm."}, {"heading": "4 Experimental Setup and Results", "text": ""}, {"heading": "4.1 Datasets", "text": "CR Customer reviews of products categorized as \u2018positive\u2019 or \u2018negative\u2019 (Hu and Liu, 2004).1\nMR Movie reviews labeled as \u2018positive\u2019 or \u2018negative\u2019 (Pang and Lee, 2005). Subj Sentences deemed \u2018subjective\u2019 or \u2018objective\u2019 (Pang and Lee, 2004).2\n1http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html 2Both the MR and Subj datasets are available at:"}, {"heading": "4.2 Model Configuration", "text": "For CNNs, we used pre-trained word2vec-induced vectors3 to initialize E. We used three filter heights (3, 4, 5) and 50 filters for each. We did not tune these hyperparameters.\nWe performed 20 rounds of batch active learning. All learners began with the same 25 instances. In subsequent rounds, each learner selected 25 instances from U according to their respective querying strategies. These examples were added to L, and the models were retrained. Performance was evaluated by calculating accuracy on a held-out test set after each round. We repeated the entire AL process 10 times via 10-fold CV, replicating the experiment 5 times per fold to account for variation. Reported results are means of these averages.\nWe used Adadelta (Zeiler, 2012) for parameter estimation, and we tuned E during back-propagation, thus inducing discriminative embeddings."}, {"heading": "4.3 Results and Discussion", "text": "We show learning curves in Figure 2. Our proposed EGL-word active learning method outperforms all of the baselines. The method performs especially well on sentiment analysis tasks (MR and CR). We\nhttp://www.cs.cornell.edu/people/pabo/ movie-review-data/.\n3https://code.google.com/archive/p/word2vec/\nbelieve this is due to our model rapidly learning more discriminative representations of words with opposing polarities.\nTo further illustrate this point, we also provide plots displaying the Euclidean distances between selected pairs of word embeddings induced using different AL strategies (Figure 2). In the case of the customer review dataset, for example, we see that EGL-word quickly pushes the embeddings corresponding to \u2018good\u2019 and \u2018bad\u2019 apart. Similarly, on the movie review dataset, \u2018fun\u2019 and \u2018boring\u2019 are rapidly separated in embedding space. The subjectivity detection task is less clear-cut. Here we picked words \u2018amusing\u2019 and \u2018their\u2019, because \u2018amusing\u2019 strongly indicates subjectivity, while \u2018their\u2019 is plainly neutral. In the Figure 2, we observe that EGL-word separates these, as expected, though less rapidly than in the sentiment case."}, {"heading": "5 Conclusions", "text": "We have proposed a new active learning strategy for CNNs that is specifically designed to quickly induce discriminative word embeddings, and thus improve text classification. We have shown that this approach outperforms baseline AL strategies on three text classification datasets, and that it rapidly finds discriminative embeddings for words."}], "references": [{"title": "Mining and summarizing customer reviews", "author": ["Hu", "Liu2004] Minqing Hu", "Bing Liu"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1412.1058", "author": ["Johnson", "Zhang2014] Rie Johnson", "Tong Zhang"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "A sequential algorithm for training text classifiers", "author": ["Lewis", "Gale1994] David D Lewis", "William A Gale"], "venue": "In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Lewis et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 1994}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Lee2004] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Lee2005] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Learning representations by back-propagating errors", "author": ["Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "An analysis of active learning strategies for sequence labeling tasks", "author": ["Settles", "Craven2008] Burr Settles", "Mark Craven"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Settles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Settles et al\\.", "year": 2008}, {"title": "Active learning literature survey", "author": ["Burr Settles"], "venue": "University of Wisconsin,", "citeRegEx": "Settles.,? \\Q2010\\E", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "A mathematical theory of communication", "author": ["Claude Elwood Shannon"], "venue": "ACM SIGMOBILE Mobile Computing and Communications", "citeRegEx": "Shannon.,? \\Q2001\\E", "shortCiteRegEx": "Shannon.", "year": 2001}, {"title": "Support vector machine active learning with applications to text classification", "author": ["Tong", "Koller2002] Simon Tong", "Daphne Koller"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Tong et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tong et al\\.", "year": 2002}, {"title": "Active learning for biomedical citation screening", "author": ["Kevin Small", "Carla E Brodley", "Thomas A Trikalinos"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Wallace et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wallace et al\\.", "year": 2010}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Zhang", "Wallace2015] Ye Zhang", "Byron Wallace"], "venue": "arXiv preprint arXiv:1510.03820", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification", "author": ["Zhang et al.2016] Ye Zhang", "Stephen Roller", "Byron Wallace"], "venue": "arXiv preprint arXiv:1603.00968", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Active learning with sampling by uncertainty and density for word sense disambiguation and text classification", "author": ["Zhu et al.2008] Jingbo Zhu", "Huizhen Wang", "Tianshun Yao", "Benjamin K Tsou"], "venue": "In Proceedings of the 22nd International Conference", "citeRegEx": "Zhu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 8, "context": "In active learning (AL), the machine learning algorithm being trained is allowed to select the examples to be manually annotated (and in turn learned from) (Settles, 2010).", "startOffset": 156, "endOffset": 171}, {"referenceID": 8, "context": "There has been a wealth of work on AL approaches for traditional machine learning methods generally (Settles, 2010), and for text classification in particular (Tong and Koller, 2002; McCallumzy and Nigamy, 1998; Wallace et al.", "startOffset": 100, "endOffset": 115}, {"referenceID": 11, "context": "There has been a wealth of work on AL approaches for traditional machine learning methods generally (Settles, 2010), and for text classification in particular (Tong and Koller, 2002; McCallumzy and Nigamy, 1998; Wallace et al., 2010).", "startOffset": 159, "endOffset": 233}, {"referenceID": 2, "context": "Here we propose a method for AL using convolutional neural networks (CNNs), which have recently achieved strong performance across a diverse set of text classification tasks (Kim, 2014; Zhang and Wallace, 2015; Johnson and Zhang, 2014; Zhang et al., 2016).", "startOffset": 174, "endOffset": 255}, {"referenceID": 14, "context": "Here we propose a method for AL using convolutional neural networks (CNNs), which have recently achieved strong performance across a diverse set of text classification tasks (Kim, 2014; Zhang and Wallace, 2015; Johnson and Zhang, 2014; Zhang et al., 2016).", "startOffset": 174, "endOffset": 255}, {"referenceID": 2, "context": "Here we provide a brief review of CNNs for text classification, using the model proposed by Kim (2014). We will denote the word embedding matrix by E \u2208 RV\u00d7d, where V is the vocabulary size, and d is the dimension of the embedding layer.", "startOffset": 92, "endOffset": 103}, {"referenceID": 6, "context": "Typically this model is trained by minimizing the cross-entropy loss via back-propagation (Rumelhart et al., 1988).", "startOffset": 90, "endOffset": 114}, {"referenceID": 2, "context": "For more details, we refer the reader to (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 41, "endOffset": 77}, {"referenceID": 8, "context": "Many querying strategies have been proposed in the literature (Settles, 2010).", "startOffset": 62, "endOffset": 77}, {"referenceID": 15, "context": "One of the simplest and perhaps the most commonly used query framework is uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2002; Zhu et al., 2008).", "startOffset": 95, "endOffset": 158}, {"referenceID": 9, "context": "A general uncertainty sampling variant uses entropy (Shannon, 2001) as an uncertainty measure, defining \u03c6(xi;\u03b8) as:", "startOffset": 52, "endOffset": 67}, {"referenceID": 8, "context": "Entropy-based uncertainty sampling has been observed to achieve strong empirical performance across many tasks (Settles, 2010).", "startOffset": 111, "endOffset": 126}, {"referenceID": 12, "context": "We used Adadelta (Zeiler, 2012) for parameter estimation, and we tuned E during back-propagation, thus inducing discriminative embeddings.", "startOffset": 17, "endOffset": 31}], "year": 2017, "abstractText": "We propose a new active learning (AL) method for text classification based on convolutional neural networks (CNNs). In AL, one selects the instances to be manually labeled with the aim of maximizing model performance with minimal effort. Neural models capitalize on word embeddings as features, tuning these to the task at hand. We argue that AL strategies for neural text classification should focus on selecting instances that most affect the embedding space (i.e., induce discriminative word representations). This is in contrast to traditional AL approaches (e.g., uncertainty sampling), which specify higher level objectives. We propose a simple approach that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-specific embeddings. Empirical results show that our method outperforms baseline AL approaches.", "creator": "LaTeX with hyperref package"}}}