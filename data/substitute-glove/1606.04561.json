{"id": "1606.04561", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "A two-stage learning method for protein-protein interaction prediction", "abstract": "In coming page, a and method for PPI (proteinprotein phenomena) comparable though compromise. In PPI observations, taken reliable and sufficient number of instruction swabs because nor available, but a each number has adsorbent retrieved both entered get. In the which variations, the denoising auto suxamethonium are other and learning slowdown features. The receiving consistent presents number particular in immediately to near means glehn although called why same. The tests results objective the real-time of making policies method.", "histories": [["v1", "Tue, 14 Jun 2016 20:49:22 GMT  (390kb)", "http://arxiv.org/abs/1606.04561v1", null], ["v2", "Mon, 18 Jul 2016 16:04:09 GMT  (396kb)", "http://arxiv.org/abs/1606.04561v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CE", "authors": ["amir ahooye atashin", "parsa bagherzadeh", "kamaledin ghiasi-shirazi"], "accepted": false, "id": "1606.04561"}, "pdf": {"name": "1606.04561.pdf", "metadata": {"source": "CRF", "title": "A two-stage learning method for protein-protein interaction prediction", "authors": [], "emails": ["1amir.atashin@stu.um.ac.ir", "2parsa.bagherzadeh@stu.um.ac.ir", "3k.ghiasi@um.ac.ir"], "sections": [{"heading": null, "text": "Protein-protein interaction; Denoising auto encoder; Robust features; Unlabelled data;\nI. INTRODUCTION Protein-protein interaction (PPI) is one of the most important processes in biological systems. PPI refers to physical contacts established between two or more proteins. These interactions often perform biological functions. Many of important molecular processes in cell are performed by large molecular complexes which are consist of large number of interacting proteins. Because of their importance for understanding the biological functions, prediction of PPI's is the subject of research and has attracted great attention.\nThe prediction of PPI is performed either experimentally or computationally. Experimental methods are consisting of biochemical and genetic experiments for PPI prediction. Most of these methods are expensive and very time consuming. Yeast two-hybrid screening and Mass spectrometry are examples of these methods [2]. On the other hand, in the past years, investigation of PPI using computational methods has become the subject of research interest[3]. Computational methods often refer to deployment of machine learning techniques to obtain predictive model for PPI prediction. These models are built based on known pairs of protein which are labelled as interacting or non-interacting. In other words, the goal of computational methods is the integration of various source of evidence in a statistical framework.\nIn computational methods the goal is the learning of distribution ( | ) from the available instances. This problem is classification and a supervised learning task in which the distribution is learnt form the set of instances\n{( , ), \u2026 , ( , )} associated with predefined labels. It is assumed that the instance of this set are independent samples of the actual distribution ( | ) . Several examples of computational methods are naive logistic regression [4], random forest based method [5], Bayes classifier [6], decision tree [7], kernel based methods from [8], [9], and the strategies of summing likelihood ratio scores [10-12].\nDespite their advantages, the main drawback of computational methods is the availability of reliable labeled samples. Number of these samples are not sufficient for construction of a comprehensive model. On the other hand, a large number of unlabeled or partially labeled instances are available. It is indicated empirically that a feature vector which is more robust to noise can improve the performance of classifier. Obtaining such set of features can be an initial step for the main classification problem. These kind of features can be obtained from an expert, but the main drawback of this approach is its cost and time consumption.\nA less expensive and more time saving approach can be obtained using Denoising Auto Encoder (DAE) [13]. DAE is an unsupervised learning model which tries to achieve a useful representation of data. Using this method, a set of robust feature can be extracted automatically from data. The Denoising Auto Encoder (DAE) is an extension of a classical auto encoder and it was introduced as a building block for deep networks [1]. Since the DAE is an unsupervised learning model, it can benefit from the large number of unlabeled or partially labeled pairs of proteins and can be employed for solving the PPI prediction problem [14].\nIn this paper a new method for solving PPI prediction problem, based on denoising auto encoder is proposed. A Denoising Auto Encoder is employed for learning of robust features. The learnt features are used for training a multilayer feed forward neural network with a better performance [15].\nThe reminder of this paper is organized as follows: section II presents some preliminaries concerning auto encoder, denoising auto encoder and stacked auto encoder\n[16]. In section III the proposed method for PPI prediction is presented. Section IV reports the experimental results and finally section V gives the concluding remarks.\nII. PRELIMINARIES\nIn this section, the denoising auto encoder is reviewed. We begin with a short discussion on Auto encoders."}, {"heading": "A. Auto Encoder (AE)", "text": "An auto encoder can be described as two part, encoder and decoder. The Encoder is a function that takes an input \u2208 [0,1] and maps it to hidden representation \u2208 [0,1] through a deterministic mapping.\n(1) = ( ) = +\nWhere S is a nonlinear function such as sigmoid, and W\nis weight matrix with size of d x d\u2019.\nThe Decoder is a function that takes a representation y back to reconstruction z:\n(2) = ( ) = ( + )\nIn this paper we explore the tied weight case, that = . The autoencoder training can be done by finding parameters \u03b8 = , , that minimized the reconstruction error on a training set of data .\n(3) (\u03b8) = , ( ) \u2208\nThe reconstruction error can be measured in many ways. Typically choice are squared error ( , ) = \u2016 \u2212 \u2016 or the cross-entropy loss when the input is interpreted as either bit vectors or vectors of bit probabilities:\n(4) ( , ) = [ log + (1 \u2212 ) log(1 \u2212 )]"}, {"heading": "B. Denoising Auto Encoder (DAE)", "text": "The denoising auto encoder is a stochastic version of the auto encoder, where one simply corrupts the input before sending it to the autoencoder, which is trained to reconstruct the clean data (to denoise). This yields the following objective:\n(5) (\u03b8) = ~ ( | ) , ( ) \u2208\nWhere the expectation is over corrupted versions of examples x obtained from a corruption process ( | ).This objective is optimized by stochastic gradient descent. In this paper corruption is considered as additive isotropic Gaussian noise: = + , \u2208 (0, ) and a binary masking noise where a fraction of input components have their value set to 0."}, {"heading": "C. Stacked Denoising Auto Encoder (S-DAE)", "text": "Denoising autoencoders can be stacked to form a deep network by feeding the hidden representation of the denoising autoencoder found on the layer below as input to the current layer. The unsupervised pre-training of such an architecture is done one layer at a time. Each layer is trained as a denoising autoencoder by minimizing the error in reconstructing its input (which is the output code of the previous layer). Once the th layer is trained, we can train the + 1th layer because we can now compute the hidden representation from the previous layer.\nOnce all layers are pre-trained, the next step is to train network on supervised manner, this phase called finetuning. Here we consider fine-tuning where we want to minimize prediction error on a classification task.\nIII. THE PROPOSED METHOD In PPI prediction a small set of labelled data = {( , ), \u2026 , ( , )} where \u2208 \u211b is i-th data and with corresponding class label \u2208 {0,1}, and we have a large set of unlabeled data set = {( , \u2026 , )} are in hand. Thus, we are facing with a semi-supervised classification\nproblem. Semi-supervised learning is a class of supervised learning methods that not only uses the labelled instances for training but also make use of unlabeled data.\nNow we develop an algorithm for semi-supervised learning that employs the unlabeled data to improve the classification performance. DAE is used to construct a model that is able to provide a more robust set of features from data. The proposed model is consisting of stacking several layers of DAEs. Each of these DAEs are employed to build a robust feature vector form its input by learning original inputs from their corresponding corrupted versions.\nFirstly, each DAE is trained based on an unsupervised layer-wised greedy algorithm in which each layer is trained separately, one by one from bottom to up. Since DAE trained via unsupervised learning, it tries to find dependency between input variables. Since adding noise correspond to regularization [15] the obtained representation at final layer leads to a better generalization capability.\nAt final stage, we build a classifier based on the output signal of s-DAE model. Since the input of the classifier is a more robust version of the original input, it is expected to have a more generalization for test data.\nIV. EXPERIMENAL RESULT In this section the performance of proposed method is\nevaluated on PPI prediction problem."}, {"heading": "A. Data set", "text": "The proposed method is applied to HIV-1 and human protein-protein interaction prediction problem [17]. This data set is consisting of labeled and unlabeled pairs. Both parts are used in the pre-train phase. The pairs of proteins are described in 18 dimensional space. Interacting pairs of proteins are considered as positive raining samples. On the other hand, non-interacting pairs are considered as negative class. The interaction of these labeled samples is indicated by HIV experts computationally and the second part of data se is consisted of unlabeled pairs of proteins. For these unlabeled pairs, there is not enough evidence for\nindication of interaction so they are leaved unlabeled. The characteristics of the data se is summarized in table I."}, {"heading": "B. Model architecture", "text": "1) Unsupervised pre training: Two stacked de nosing auto encoder are used. These denoising auto encoders are choose to be contrastive. The first DAE tries to map the 18 dimensional input vector to 14 dimension. The second DAE maps the output of first DAE to 8 dimensions. This 8 dimensional feature vector is the robust and generalized version of 18 dimensional input feature vector. The stacked DAE model is trained layer wised. The parameters of Sacked DAE are summarized in Table II. A visual represenation of the filters learned by denoising\nautoencoders is shown in Figure 3. As it can be seen a meaningfull represenation of features is obtained.\n2) Supervised training: A three layer feed-forward network is constructed based on the obtained weighs of unsupervised phase. The first two layers are initialized by the sDAE weighs and the weighs of third layer are assigned randomly. Output of feed-forward network is a single sigmoid neuron, indicating the respective class of the input based on lowest difference of the output to either 1 or 0. The feed forward network is trained using back propagation algorithm by labeled samples.\nC. Numrical Resuls: In order to demonstrate the capabilites of the proposed method for PPI prediction, it is compared against other classical callsification methods such as SVM, kNN and MLP.The experiments were performod using k-fold approach with = 5. The accuracies of PPI prediction, obtained by each of these classifers are shown in Table IV.\nV. CONCLISIONS In this paper we proposed a new method for proteinprotein interaction prediction problem. Our two stage leaning method used a large number of unlabeled data to extract robust features from input data and in the second phase, we applied them to achieve better performance on the classification stage of our method. For future work we plan to see our method as learning framework which other generative models like Restricted Boltzmann Machine or\ndiscriminative models like support vector machine, can be replaced and tested in the first and second stage of our method."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Dr. Tasan et al. for providing the Human Protein to HIV-1 Virus Interaction Prediction data set and Dr. Qi et al. for providing the \u201cGold standard\u201d positive samples."}], "references": [{"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P Vincent"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Deciphering protein-protein interactions. Part I. Experimental techniques and databases", "author": ["B.A. Shoemaker", "A.R. Panchenko"], "venue": "PLOS Comput Biol,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Deciphering protein-protein interactions. Part II. Computational methods to predict protein and domain interaction partners", "author": ["B.A. Shoemaker", "A.R. Panchenko"], "venue": "PLoS Comput Biol,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Information assessment on predicting protein-protein interactions", "author": ["N Lin"], "venue": "BMC bioinformatics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Random forest similarity for protein-protein interaction prediction from multiple sources", "author": ["Y. Qi", "J. Klein-Seetharaman", "Z. Bar-Joseph"], "venue": "Pacific Symposium on Biocomputing", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "A Bayesian networks approach for predicting protein-protein interactions from genomic data", "author": ["R Jansen"], "venue": "Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Predicting co-complexed protein pairs using genomic and proteomic data integration", "author": ["Zhang", "L.V"], "venue": "BMC bioinformatics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Kernel methods for predicting protein\u2013protein interactions", "author": ["A. Ben-Hur", "W.S. Noble"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "A probabilistic functional network of yeast", "author": ["I Lee"], "venue": "genes. science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Probabilistic model of the human protein-protein interaction network", "author": ["Rhodes", "D.R"], "venue": "Nature biotechnology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P Vincent"], "venue": "Proceedings of the 25th international conference on Machine learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Training with noise is equivalent to Tikhonov regularization", "author": ["C.M. Bishop"], "venue": "Neural computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S Rifai"], "venue": "Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Semi-supervised multi-task learning for predicting interactions between HIV-1 and human proteins", "author": ["Y Qi"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Yeast two-hybrid screening and Mass spectrometry are examples of these methods [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "On the other hand, in the past years, investigation of PPI using computational methods has become the subject of research interest[3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "Several examples of computational methods are naive logistic regression [4], random forest based method [5], Bayes classifier [6], decision tree [7], kernel based methods from [8], [9], and the strategies of summing likelihood ratio scores [10-12].", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "Several examples of computational methods are naive logistic regression [4], random forest based method [5], Bayes classifier [6], decision tree [7], kernel based methods from [8], [9], and the strategies of summing likelihood ratio scores [10-12].", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": "Several examples of computational methods are naive logistic regression [4], random forest based method [5], Bayes classifier [6], decision tree [7], kernel based methods from [8], [9], and the strategies of summing likelihood ratio scores [10-12].", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "Several examples of computational methods are naive logistic regression [4], random forest based method [5], Bayes classifier [6], decision tree [7], kernel based methods from [8], [9], and the strategies of summing likelihood ratio scores [10-12].", "startOffset": 145, "endOffset": 148}, {"referenceID": 7, "context": "Several examples of computational methods are naive logistic regression [4], random forest based method [5], Bayes classifier [6], decision tree [7], kernel based methods from [8], [9], and the strategies of summing likelihood ratio scores [10-12].", "startOffset": 176, "endOffset": 179}, {"referenceID": 8, "context": "Several examples of computational methods are naive logistic regression [4], random forest based method [5], Bayes classifier [6], decision tree [7], kernel based methods from [8], [9], and the strategies of summing likelihood ratio scores [10-12].", "startOffset": 240, "endOffset": 247}, {"referenceID": 9, "context": "Several examples of computational methods are naive logistic regression [4], random forest based method [5], Bayes classifier [6], decision tree [7], kernel based methods from [8], [9], and the strategies of summing likelihood ratio scores [10-12].", "startOffset": 240, "endOffset": 247}, {"referenceID": 10, "context": "A less expensive and more time saving approach can be obtained using Denoising Auto Encoder (DAE) [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "The Denoising Auto Encoder (DAE) is an extension of a classical auto encoder and it was introduced as a building block for deep networks [1].", "startOffset": 137, "endOffset": 140}, {"referenceID": 11, "context": "Since the DAE is an unsupervised learning model, it can benefit from the large number of unlabeled or partially labeled pairs of proteins and can be employed for solving the PPI prediction problem [14].", "startOffset": 197, "endOffset": 201}, {"referenceID": 12, "context": "The learnt features are used for training a multilayer feed forward neural network with a better performance [15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "\u2208 [0,1] and maps it to hidden representation \u2208 [0,1] through a deterministic mapping.", "startOffset": 2, "endOffset": 7}, {"referenceID": 0, "context": "\u2208 [0,1] and maps it to hidden representation \u2208 [0,1] through a deterministic mapping.", "startOffset": 47, "endOffset": 52}, {"referenceID": 12, "context": "Since adding noise correspond to regularization [15] the obtained representation at final layer leads to a better generalization capability.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "The proposed method is applied to HIV-1 and human protein-protein interaction prediction problem [17].", "startOffset": 97, "endOffset": 101}], "year": 2016, "abstractText": "In this paper, a new method for PPI (proteinprotein interaction) prediction is proposed. In PPI prediction, a reliable and sufficient number of training samples is not available, but a large number of unlabeled samples is in hand. In the proposed method, the denoising auto encoders are employed for learning robust features. The obtained robust features are used in order to train a classifier with a better performance. The experimental results demonstrate the capabilities of the proposed method. Protein-protein interaction; Denoising auto encoder; Robust features; Unlabelled data;", "creator": null}}}