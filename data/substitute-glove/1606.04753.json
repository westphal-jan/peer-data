{"id": "1606.04753", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Safe Exploration in Finite Markov Decision Processes with Gaussian Processes", "abstract": "In classical reinforcement learning, been exploring brought stable, agents accept illogical short term loss provided full term gain. This exists infeasible should safety progress applications, of as robotics, where very kind time unsafe issue may harm means without. In this advertising, we should the extent of hours nature formula_18 Markov doubt processes (MDP ). We precisely ensure in terms of whose, well logically unknown, safety exponential that affect its states although actions. We commitment to explore the MDP once this constraint, minimum could the initially constraint formula_34 regularity treatment expressed via a Gaussian negotiations august. We develop way author algorithms six its task all hardly idea every still them to rendered focus created safely reachable part only three MDP turn restricting the safety constraint. To achieve this, even noncommittal relationships otherwise already and harm same bring to ability specialization nonetheless nearly the concerns for up-regulated country - action ten from cocktails summarized collected that riffling well environment. Moreover, the algorithm referred important spatiotemporal because exploring the MDP, ensuring example meant does need get suddenly central whether governors that. them making trouble. We maintain n't mechanical made digital forested high-performance for of task of emerging an however context with took engines.", "histories": [["v1", "Wed, 15 Jun 2016 13:18:30 GMT  (505kb,D)", "http://arxiv.org/abs/1606.04753v1", "9 pages, extended version with proofs"], ["v2", "Tue, 15 Nov 2016 14:00:11 GMT  (506kb,D)", "http://arxiv.org/abs/1606.04753v2", "15 pages, extended version with proofs"]], "COMMENTS": "9 pages, extended version with proofs", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO stat.ML", "authors": ["matteo turchetta", "felix berkenkamp", "andreas krause 0001"], "accepted": true, "id": "1606.04753"}, "pdf": {"name": "1606.04753.pdf", "metadata": {"source": "CRF", "title": "Safe Exploration in Finite Markov Decision Processes with Gaussian Processes", "authors": ["Matteo Turchetta", "Felix Berkenkamp"], "emails": ["matteotu@ethz.ch", "befelix@ethz.ch", "krausea@ethz.ch"], "sections": [{"heading": null, "text": "In classical reinforcement learning, when exploring an environment, agents accept arbitrary short term loss for long term gain. This is infeasible for safety critical applications, such as robotics, where even a single unsafe action may cause system failure. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an, a priori unknown, safety constraint that depends on states and actions. We aim to explore the MDP under this constraint, assuming that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover."}, {"heading": "1 Introduction", "text": "Today\u2019s robots are required to operate in variable and often unknown environments. The traditional approach is to specify all potential scenarios that a robot may encounter during operation a priori. This is time consuming or even infeasible. As a consequence, robots need to be able to learn and adapt to unknown environments autonomously [10, 2]. While exploration algorithms are known, safety is still an open problem in the development of such systems [18]. In fact, most learning algorithms allow robots to make unsafe decisions during exploration. This can damage the learning system.\nIn this paper, we provide a solution to this problem and develop an algorithm that enables agents to safely explore unknown environments. Specifically, we consider the problem of exploring a Markov decision process (MDP), where it is a priori unknown if a particular state-action pair is safe. The algorithm that we present cautiously explores this environment without taking actions that are unsafe or may render the exploring agent stuck.\nRelated Work. Safe exploration is an open problem in the reinforcement learning community and several definitions of safety have been proposed [16]. In risk-sensitive reinforcement learning, the goal is to maximize the expected return for the worst case scenario [5]. However, these approaches only minimize risk and do not treat safety as a hard constraint. For example, Geibel and Wysotzki [7] define risk as the probability of driving the system to a previously known set of undesirable states. The main difference to our approach is that we do not assume the undesirable states to be known a priori. Garcia and Fern\u00e1ndez [6] propose to ensure safety by means of a backup policy; that is, a policy that is known to be safe in advance. Our approach is different, since does not require a backup policy but only a set of initially safe states from which the agent starts to explore. Another approach\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 6.\n04 75\n3v 1\n[ cs\n.L G\n] 1\n5 Ju\nn 20\nthat makes use of a backup policy is shown by Hans et al. [9], where safety is defined in terms of a minimum reward, which is learned from data.\nMoldovan and Abbeel [14] provide probabilistic safety guarantees at every time step by optimizing over ergodic policies; that is, policies that let the agent recover from any visited state. This approach needs to solve a large linear program at every time step, which is computationally demanding even for small state spaces. Nevertheless, the idea of ergodicity also plays an important role in our method. In the control community, safety is mostly considered in terms of stability or constraint satisfaction of controlled systems. Akametalu et al. [1] use reachability analysis to ensure stability under the assumption of bounded disturbances. The work in [3] uses robust control techniques in order to ensure robust stability for model uncertainties, while the uncertain model is improved.\nAnother field that has recently considered safety is Bayesian optimization [13]. There, in order to find the global optimum of an a priori unknown function [21], regularity assumptions in form of a Gaussian process (GP) [17] are made. The corresponding GP posterior distribution over the unknown function is used to guide evaluations to informative locations. In this setting, safety centered approaches include the work of Sui et al. [22] and Schreiter et al. [20], where the goal is to find the safely reachable optimum without violating an a priori unknown safety constraint at any evaluation. To achieve this, the function is cautiously explored, starting from a set of points that is known to be safe initially. The method in [22] has the advantage of being sample efficient. It was applied to the field of robotics to safely optimize the controller parameters of a quadrotor vehicle [4]. However, they considered a bandit setting, where at each iteration any arm can be played. In contrast, in this paper we consider an MDP, which introduces restrictions in terms of reachability that have not been considered in Bayesian optimization before.\nContribution. We introduce a novel algorithm for safe exploration in MDPs. We model safety via an a priori unknown constraint that depends on state-action pairs. Starting from an initial set of states and actions that are known to fulfill the safety constraint, the algorithm exploits the regularity assumptions on the constraint function in order to determine if nearby, unvisited states are safe. This leads to safe exploration, where only state-actions pairs that are known to be safe are evaluated. The main contribution consists of extending the work on safe Bayesian optimization in [22] from the bandit setting to MDPs. In order to achieve this, we explicitly consider not only the safety constraint, but also the reachability properties induced by the MDP dynamics. We provide a full theoretical analysis of the algorithm. It enjoys similar safety guarantees in terms of ergodicity, the ability to return to the safe starting point, as presented in [14], but at a vastly reduced computational cost. The reason for this is that our method separates safety from the reachability properties of the MDP, making computations more tractable. Beyond this, our method is able to handle general safety constraints and we prove that it is able to fully explore the safely reachable region of the MDP, without getting stuck or violating the safety constraint with high probability. We validate our method on an exploration task, where a rover has to explore an a priori unknown map."}, {"heading": "2 Problem Statement", "text": "In this section, we define our problem and assumptions. The unknown environment is modeled as a finite Markov Decision Processes (MDP) [23]. A finite MDP is a tuple M = \u3008S,A, f(s, a), r(s, a)\u3009 with finite state and action spaces, S and A, respectively, a known, deterministic transition model, f(s, a), and reward function, r(s, a). In the typical reinforcement learning framework, the reward is a known function that encodes desirable states. In this paper, we consider the problem of safely exploring the MDP, and thus do not aim to maximize the reward. Instead we define r(s, a) as an a priori unknown safety feature. Although r(s, a) is unknown, we make regularity assumptions about it to make the problem tractable. When traversing the MDP, at each discrete time step, k, the agent has to decide which action and thereby state to visit next. We assume that the underlying system is safety-critical and that for any visited state-action pair, (sk, ak), the unknown, associated safety feature, r(sk, ak), must be above a safety threshold, h. While the assumption of deterministic dynamics does not hold for general MDPs, in our framework, uncertainty about the environment is captured by the safety feature. If requested, the agent can obtain noisy measurements of the safety feature, r(sk, ak). The index t is used to index measurements, while k denotes movement steps. Typically k t.\nIt is hopeless to achieve the goal of safe exploration unless the agent starts in a safe location. Hence, we assume that the agent stays in an initial set of state action pairs, S0, that is known to be safe a priori. The goal is to identify the maximum safely reachable region starting from S0, without visiting any unsafe states. In the following we assume that safety depends on states only; that is, r(s, a) = r(s). We provide an extension to safety features on actions in Fig. 2b.\nAssumptions on the reward function Ensuring that all visited states are safe without any prior knowledge about the safety feature is an impossible task (e.g., if the safety feature is discontinuous). However, many practical safety features exhibit some regularity, where similar states will lead to similar values of r.\nIn the following, we assume that S is endowed with a positive definite kernel function, k(\u00b7, \u00b7), and that the function, r(\u00b7), has bounded norm in the associated Reproducing Kernel Hilbert Space (RKHS) [19]. The norm induced by the inner product of the RKHS indicates the smoothness of functions with respect to the kernel. This assumption allows us to model r as a Gaussian Process (GP) [17], r(s) \u223c GP(\u00b5(s), k(s, s\u2032)). A GP is a probability distribution over functions fully specified by its mean function, \u00b5(s), and its covariance function, k(s, s\u2032). We assume \u00b5(s) = 0, for all s \u2208 S, without loss of generality. The posterior distribution over r can be computed analytically, based on t measurements at states At = {s1, . . . , st} \u2286 S with measurements, yt = [r(s1) + \u03c91 . . . r(st) + \u03c9t]T, that are corrupted by zero-mean Gaussian noise, \u03c9t \u223c N (0, \u03c32). The posterior is a GP distribution, with mean \u00b5t(s) = kt(s)T(Kt + \u03c32I)\u22121yt, variance \u03c3t(s) = kt(s, s) and covariance kt(s, s\u2032) = k(s, s\u2032)\u2212 kt(s)T(Kt + \u03c32I)\u22121kt(s\u2032), where kt(s) = [k(s1, s), . . . , k(st, s)]T and Kt is the positive definite kernel matrix, [k(s, s\u2032)]s,s\u2032\u2208At .\nFor many commonly used kernels, the GP assumption implies that the reward function is Lipschitz continuous with high probability [21, 8]. Thus, we assume that the reward function is Lipschitz continuous, with Lipschitz constant L, with respect to some metric, d(\u00b7 , \u00b7), on S. Goal In this section we define the goal of the algorithm more precisely. In particular we ask, what the best that any algorithm may hope to achieve is. Since we only observe noisy measurements, it is impossible to know the underlying function, r, accurately after a finite number of measurements. Instead, we consider algorithms that only have knowledge of r up to some statistical confidence, . Based on this confidence within some safe set S, states with small distance to S can be classified to satisfy the safety constraint using the Lipschitz continuity of r. The resulting set of safe states is\nRsafe (S) = S \u222a {s \u2208 S | \u2203s\u2032 \u2208 S : r(s\u2032)\u2212 \u2212 Ld(s, s\u2032) \u2265 h}, (1)\nwhich contains states that can be classified as safe given the information about the states in S. While (1) considers the safety constraint, it does not consider any restrictions put in place by the structure of the MDP. In particular, we may not be able to visit every state inRsafe (S) without visiting an unsafe state first. As a result, the agent is further restricted to\nRreach(S) = S \u222a {s \u2208 S | \u2203s\u2032 \u2208 S, a \u2208 A(s\u2032) : s = f(s\u2032, a)}, (2)\nthe set of all states that can be reached starting from the safe set in one step. These states are called the one-step safely reachable states. However, even restricted to this set, the agent may still get stuck in a state without any safe actions. We define\nRret(S, S) = S \u222a {s \u2208 S | \u2203a \u2208 A(s) : f(s, a) \u2208 S} (3)\nas the set of states that are able to return to a set S through some other set of states, S, in one step. In particular, we care about the ability to return to a certain set through a set of safe states S. Therefore, these are called the one-step safely recoverable states. In general, the return routes\nAlgorithm 1 Safe exploration in MDPs (SafeMDP) Inputs: M = \u3008S, A, f(\u00b7, \u00b7), GP(0, k(s, s\u2032)) \u3009, Safety threshold h,\nLipschitz constant L, Safe seed S0. C0(s)\u2190 [h,\u221e) for all s \u2208 S0 for t = 1, 2, . . . do St \u2190 {s \u2208 S | \u2203s\u2032 \u2208 S\u0302t\u22121 : lt(s\u2032)\u2212 Ld(s, s\u2032 \u2265 h} S\u0302t \u2190 {s \u2208 St | s \u2208 Rreach(S\u0302t\u22121), s \u2208 R ret (St, S\u0302t\u22121)}\nGt \u2190 {s \u2208 S\u0302t | gt(s) > 0} st \u2190 argmaxs\u2208Gt wt(s) Safe Dijkstra in St from st\u22121 to st Update GP with st and yt \u2190 r(st) + \u03c9t if Gt = \u2205 or max\ns\u2208Gt wt(s) \u2264 then Break\nmay require taking more than one action, see Fig. 1. The n-step returnability operator, Rretn (S, S) = Rret(S,Rret(S, . . .)), considers these longer return routes by repeatedly applying the return operator, (3), n times. The limit, R ret (S, S) = limn\u2192\u221eR ret n (S, S), contains all the states that can reach the set S through an arbitrarily long path in S.\nFor safe exploration of MDPs, all of the above are requirements; that is, any state that we may want to visit needs to be safe (satisfy the safety constraint), reachable, and we must be able to return to safe states from this new state. Thus, any algorithm that aims to safely explore an MDP is only allowed to visit states in\nR (S) = R safe (S) \u2229Rreach(S) \u2229R ret (Rsafe (S), S), (4)\nwhich is the intersection of the three safety-relevant sets. Given a safe set, S, that fulfills all the safety requirements, R ret (Rsafe (S), S) is the set of states from which we can return to S by only visiting states that can be classified as above the safety threshold.\nGiven knowledge about the safety feature in S up to accuracy thus allows us to expand the set of safe ergodic states to R (S). Any algorithm that has the goal of exploring the state space should consequently explore these newly available safe states and gain new knowledge about the safety feature, r. The safe set after n such expansions can be found by repeatedly applying n times the operator in (4): Rn (S) = R (R . . . R (S)) . . .). Ultimately, the size of the safe set is bounded by surrounding unsafe states or the number of states in S. As a result, the biggest set that any algorithm may classify as safe without visiting unsafe states is given by taking the limit, R (S) = limn\u2192\u221eR n (S).\nThus, given a tolerance level, , and an initial safe seed set, S0, the baseline that we compare our algorithm against is the exploration of R (S0), the set of states that any algorithm may hope to classify as safe. Let St denote the set of states that is classified as safe at iteration t. In the following, we will refer to complete, safe exploration whenever an algorithm fulfills R (S0) \u2286 limt\u2192\u221e St \u2286 R0(S0); that is, the algorithm classifies every safely reachable state up to accuracy as safe, without misclassification or visiting unsafe states."}, {"heading": "3 Algorithm", "text": "We start by giving a high level overview of the method. The algorithm relies on a GP model of r to make predictions about the safety feature and uses the predictive uncertainty to guide the safe exploration. In order to guarantee safety, it maintains two sets. The first set, St, contains all states that can be classified as satisfying the safety constraint on r using the GP posterior, while the second one, S\u0302t, additionally considers the ability to reach points in St and the ability to safely return to the previous safe set, S\u0302t\u22121. The algorithm ensures safety and ergodicity by only visiting states in S\u0302t. In order to expand the safe region, the algorithm visits states in Gt \u2286 S\u0302t, a set of candidate states that, if visited, could expand the safe set. Specifically, the algorithm selects the most uncertain state in Gt, which is the safe state that we can gain the most information about. We move to this state via the shortest safe path, which is guaranteed to exist (Lemma 2). The algorithm is summarized in Algorithm 1.\nInitialization. The algorithm relies on an initial safe set, S0, as a starting point to explore the MDP. These states must be safe; that is, r(s) \u2265 h, for all s \u2208 S0. They must also fulfill the reachability and returnability requirements from Sec. 2. Consequently, for any two states, s, s\u2032 \u2208 S0, there must exist a path in S0 that connects them: s\u2032 \u2208 R ret (S0, {s}). While this may seem restrictive, the requirement is, for example, fulfilled by a single state that has an action corresponding to staying in place.\nClassification. In order to safely explore the MDP, the algorithm must determine which states are safe without visiting them. The regularity assumptions introduced in Sec. 2 allow us to model the safety feature as a GP , so that we can use the uncertainty estimate of the GP model in order to determine a confidence interval within which the true safety function lies with high probability. For every state s, this confidence interval has the form Qt(s) = [ \u00b5t\u22121(s)\u00b1 \u221a \u03b2t\u03c3t\u22121(s) ] , where \u03b2t is a positive scalar that determines the amplitude of the interval. We discuss how to select \u03b2t in Sec. 4.\nRather than defining high probability bounds on the values of r(s) directly in terms ofQt, we consider the intersection of the setsQt up to iteration t, Ct(s) = Qt(s) \u2229 Ct\u22121(s). This choice ensures that set of states that we classify as safe does not shrink over iterations. This choice is justified by the selection of \u03b2t in Sec. 4. Based on these confidence intervals, we define a lower bound, lt(s) = min Ct(s), and upper bound, ut(s) = max Ct(s), on the values that the safety features r(s) are likely to take based on the data obtained up to iteration t. Based on these lower bounds, we define\nSt = { s \u2208 S | \u2203s\u2032 \u2208 S\u0302t\u22121 : lt(s\u2032)\u2212 Ld(s, s\u2032) \u2265 h } (5)\nas the set of states that fulfill the safety constraint on r with high probability by using the Lipschitz constant to generalize beyond the current safe set. Based on this classification, the set of ergodic safe states is the set of states that achieve the safe threshold and, additionally, fulfill the reachability and returnability properties discussed in Sec. 2:\nS\u0302t = { s \u2208 St | s \u2208 Rreach(S\u0302t\u22121) \u2229R ret (St, S\u0302t\u22121) } . (6)\nExpanders. With the set of safe states defined, the task of the algorithm is to identify and explore states that might expand the set of states that can be classified as safe. We use the uncertainty estimate in the GP in order to define an optimistic set of expanders,\nGt = {s \u2208 S\u0302t | gt(s) > 0}, (7) where, gt(s) = \u2223\u2223{s\u2032 \u2208 S \\ St |ut(s)\u2212 Ld(s, s\u2032) \u2265 h}\u2223\u2223. The function gt(s) is positive whenever an optimistic measurement at s, equal to the upper confidence bound, ut, would allow us to determine that a previously unsafe state indeed has has value r(s\u2032) above the safety threshold. Intuitively, sampling s might lead to the expansion of St. For a graphical intuition of the set Gt see Fig. 2a.\nSampling and shortest safe path. The remaining part of the algorithm is concerned with selecting a state from these sets to evaluate and find a safe path in the MDP that leads towards them. The goal is to visit states that allow the safe set to expand as quickly as possible, so that we do not waste resources when exploring the MDP. We use the GP posterior uncertainty about the states in Gt in order to make this choice. At each iteration t, we select as next target sample the state with the highest variance in Gt, st = argmaxs\u2208Gt wt(s), where wt(s) = ut(s) \u2212 lt(s). This choice is justified, because while all points in Gt are safe and can potentially enlarge the safe set, based on one noisy sample we can gain the most information from the state about which we are the most uncertain. Given st, we use Dijkstra\u2019s algorithm within the set St in order to find the shortest safe path to the target from the current state, st\u22121. Since we require reachability and returnability for all safe states, such a path is guaranteed to exist. We terminate the algorithm when we reach the desired accuracy; that is, argmaxs\u2208Gt wt(s) \u2264 . Action-dependent safety. So far, we have considered safety features that only depend on the states, r(s). In general, safety can also depend on the actions, r(s, a). In this section we introduce an abstract MDP that generalizes for these dependencies without modifying the algorithm. The abstract model is equivalent to the original one in terms of dynamics, f(s, a). However, we introduce additional, abstract action-states, sa, for each action. In the abstract MDP, when we start in a state s, and take action a, we first transition to this abstract action-state, from which there is only one action that deterministically transitions to f(s, a). This model is illustrated in Fig. 2b. Safety features that depend on these Action-state, sa, are equivalent to action-dependent safety features. Algorithm 1 can be used on this abstract model without modification. See the experiments in Sec. 5 for an example.\nState s\nSa fe\nty r(\ns)\n(a) Example for the classification scheme. States are classified as above the safety constraint (dashed line) according to the confidence intervals of the GP model, St (red bar). The green bar indicates the states that can expand the safe set if sampled, Gt.\n(b) Abstract MDP model that is used to encode safety features that depend on actions. In this model, actions lead to abstract states, sa, which only have one available action that leads to f(s, a)."}, {"heading": "4 Theoretical Results", "text": "The safety and exploration aspects of the algorithm that we presented in the previous section rely on the correctness of the confidence intervals Ct(s). In particular, they require that the true value of the safety feature, r(s), lies within Ct(s) with high probability for all s \u2208 S and all iterations t > 0. Furthermore, these confidence intervals have to shrink sufficiently fast over time. The probability of r taking values within the confidence intervals depends on the scaling factor \u03b2t. This scaling factor trades off conservativeness in the exploration for the probability of unsafe states being visited. Appropriate selection of \u03b2t has been studied by Srinivas et al. [21] in the multi-armed bandit setting. Even though our framework is different, their setting can be applied to our case. We choose,\n\u03b2t = 2B + 300\u03b3t log 3(t/\u03b4), (8)\nwhere B is the bound on the RKHS norm of the reward function, \u03b4 is the probability of visiting unsafe states, and \u03b3t is the maximum mutual information that can be gained about r(\u00b7) from t noisy observations; that is, \u03b3t = max|A|\u2264t I(r,yA). The information gain has a sublinear dependence on t for many commonly used kernels. More details on these bounds on \u03b3t can be found in [21]. The choice of \u03b2t in (8) is justified by the following Lemma, which follows from Theorem 6 in [21]:\nLemma 1. Assume that \u2016r\u20162k \u2264 B, and that the noise, \u03c9t, is zero-mean conditioned on the history, as well as uniformly bounded by \u03c3 for all t > 0. If \u03b2t is chosen as in (8), then, for all t > 0, and all s \u2208 S, it holds with probability at least 1\u2212 \u03b4 that r(s) \u2208 Ct(s).\nThis Lemma states that, for \u03b2t as in (8), the reward function r(s) takes values within the confidence intervals C(s) with high probability. Based on this, we state the main theoretical result:\nTheorem 1. Assume that r is L-Lipschitz continuous and that the assumptions of Lemma 1 hold. Also, assume that S0 6= \u2205, r(s) \u2265 h for all s \u2208 S0, and that for any two states, s, s\u2032 \u2208 S0, s\u2032 \u2208 Rret(S0, {s}). Choose \u03b2t as in (8). Let (s0, s1, . . . , sk, . . .) denote a state trajectory induced by Algorithm 1 on an MDP with transition function f(s, a). Then, with probability at least 1\u2212 \u03b4, r(sk) \u2265 h,\u2200k > 0.Moreover, let t\u2217 be the smallest integer such that t \u2217\n\u03b2t\u2217\u03b3t\u2217 \u2265 C |R0(S0)| 2 ,\nwith C = 8/log(1 + \u03c3\u22122). Then, there exists a t0 \u2264 t\u2217 such that, with probability at least 1\u2212 \u03b4, R (S0) \u2286 S\u0302t0 \u2286 R0(S0).\nThis theorem states that Algorithm 1 performs safe and complete exploration of the state space; that is, it explores the maximum reachable safe set without visiting unsafe states. Moreover, for any desired accuracy, , and probability of failure, \u03b4, the safely reachable region can be found within a finite number of observations. This bound depends on the information gain, which in turn depends on the kernel. If the safety feature is allowed to change rapidly across states, the information gain will be larger than if the safety feature was smooth. Intuitively, the less prior knowledge the kernel encodes, the more careful we have to be when exploring the MDP, which requires more measurements.\nAn important aspect of the proofs is guaranteeing that we cannot get stuck when running Algorithm 1:\nLemma 2. Assume that S0 6= \u2205 and that for all states, s, s\u2032 \u2208 S0, s \u2208 R ret (S0, {s\u2032}). Then, when using Algorithm 1 under the assumptions in Theorem 1, for all t > 0 and for all states, s, s\u2032 \u2208 S\u0302t, s \u2208 Rret(St, {s\u2032}).\nThis lemma states that given an initial safe set that fulfills the initialization requirements, the safe set S\u0302t will continue to fulfill these requirements for all iterations; that is, we can always find a policy that drives us from any state in S\u0302t to any other state in S\u0302t without leaving the set of safe states."}, {"heading": "5 Experiments", "text": "In this section, we demonstrate Algorithm 1 on an exploration task. We consider the setting in [14], the exploration of the surface of Mars with a rover. The code for the experiments is available at http://github.com/befelix/SafeMDP.\nFor space exploration, communication delays between the rover and the operator on Earth can be prohibitive. Thus, it is important that the robot can act autonomously and explore the environment without risking unsafe behavior. For the experiment, we consider the Mars Science Laboratory (MSL) [11], a rover deployed on Mars. Due to communication delays, the MSL can travel 20 meters before it can obtain new instructions from an operator. It can climb a maximum slope of 30\u25e6 [15, Sec. 2.1.3]. In our experiments we use digital terrain models of the surface of Mars from High Resolution Imaging Science Experiment (HiRISE), which have a resolution of one meter [12].\nAs opposed to the experiments considered by Moldovan and Abbeel [14], we do not have to subsample or smoothen the data in order to achieve good exploration results. This is due to the flexibility of the GP framework that considers noisy measurements. Therefore, every state in the MDP represents a d\u00d7 d square area with d = 1m, as opposed to d = 20m in [14]. At every state the agent can take one of four actions: up, down, left and right. If the rover attempts to climb a slope that is steeper than 30\u25e6, it fails and may be damaged. Otherwise it moves deterministically to the desired neighboring state. In this setting, we define safety over state transitions by using the extension we outlined in Fig. 2b. The safety feature over the transition from s to s\u2032 is defined in terms of height difference between the two states, H(s)\u2212H(s\u2032). Given the maximum slope of \u03b1 = 30\u25e6 that the rover can climb, the safety threshold is set at a conservative h = \u2212d tan(25\u25e6). This encodes, that it is unsafe for the robot to climb hills that are too steep. In particular, while the MDP dynamics assume that Mars is flat and every state can be reached, the safety constraint depends on the a priori unknown heights. Therefore, under the prior belief, it is unknown which transitions are safe.\nWe model the height distribution,H(s), as a GP with a Mat\u00e9rn kernel with \u03bd = 5/2. The lengthscales are set to 14.5m and the prior variance over heights is 100m2. We assume a noise standard deviation of 0.075 m. Since the safety feature of each state transition is a linear combination of heights, the GP model of the heights induces a GP model over the differences of heights, which we use to classify whether state transitions fulfill the safety constraint. In particular, the safety depends on the direction of travel, that is, going downhill is possible, while going uphill might be unsafe.\nFollowing the recommendations in [22], in our experiments we use the GP confidence intervals Qt(s) directly to determine the safe set St. As a result, the Lipschitz constant is only used to determine expanders in G. Guaranteeing safe exploration with high probability over multiple steps leads to conservative behavior, as every step beyond the set that is known to be safe decreases the \u2018probability budget\u2019 for failure. In order to demonstrate that safety can be achieved empirically using less conservative parameters than those suggested by Theorem 1, we fix beta to a constant value, \u03b2t = 2, \u2200t \u2265 0. This aims to guarantee safety per iteration rather than jointly over all the iterations. The same assumption is used in [14].\nWe compare our algorithm to several baselines. The first one considers both the safety threshold and the ergodicity requirements but neglects the expanders. In this setting the agent samples the most uncertain safe state transaction, which corresponds to the safe Bayesian optimization framework in [20]. We expect the exploration to be safe, but less efficient than our approach. The second baseline considers the safety threshold, but does not consider ergodicity requirements. In this setting, we expect the rover\u2019s behavior to fulfill the safety constraint and to never attempt to climb steep slopes, but it may get stuck in states without safe actions. The third method uses the unconstrained Bayesian optimization framework in order to explore new states, without safety requirements. In this setting, the agent tries to obtain measurements from the most uncertain state transition over the entire space, rather than restricting itself to the safe set. In this case, the rover can easily get stuck and may also incur failures by attempting to climb steep slopes. Last, we consider a random exploration strategy, which is similar to the -greedy exploration strategies that are widely used in reinforcement learning.\n0 30 60 90 120\ndistance [m]\n70\n35\n0\ndi st\nan ce\n[m ]\n(a) Non-ergodic.\n0 30 60 90 120\ndistance [m] (b) Unsafe.\n0 30 60 90 120\ndistance [m] (c) Random.\n0 30 60 90 120\ndistance [m] (d) No Expanders.\nWe compare these baselines over an 120 by 70 meters area at \u221230.6\u25e6 latitude and 202.2\u25e6 longitude. The resulting exploration behaviors can be seen in Fig. 2. The rover starts in the center-top part of the plot, a relatively planar area. In the top-right corner there is a hill that the rover cannot climb, while in the bottom-right corner there is a crater that, once entered, the rover cannot leave. The safe behavior that we expect is to explore the planar area, without moving into the crater or attempting to climb the hill. We run all algorithms for 525 iterations or until the first unsafe action is attempted. It can be seen in Fig. 2e that our method explores the safe area that surrounds the crater, without attempting to move inside. While some state-action pairs closer to the crater are also safe, the GP model would require more data classify them as safe with the necessary confidence. In contrast, the baselines perform significantly worse. The baseline that does not consider the ability to return to the safe set (non-ergodic) can be seen in Fig. 2a. It does not explore the area, because it quickly reaches a state without a safe path to the next target sample. Our approach avoids these situations explicitly. The unsafe exploration baseline in Fig. 2b considers ergodicity, but concludes that every state is reachable according to the MDP model. Consequently, it follows a path that crosses the boundary of the crater and eventually evaluates an unsafe action. Overall, it is not enough to consider only ergodicity or only safety, in order to solve the safe exploration problem. The random exploration in Fig. 2c attempts an unsafe action after some exploration. In contrast, Algorithm 1 manages to safely explore a large part of the unknown environment. Running the algorithm without considering expanders leads to the behavior in Fig. 2d, which is safe, but only manages to explore a small subset of the safely reachable area within the same number of iterations in which Algorithm 1 explores over 80% of it. The results are summarized in Table 2f."}, {"heading": "6 Conclusion", "text": "We presented an algorithm to safely explore a priori unknown environments. We used a Gaussian process model to model the safety constraints, which allows the algorithm to reason about the safety of state-action pairs before visiting them. An important aspect of the algorithm is that it considers the transition dynamics of the MDP in order to ensure that there is a safe return route before visiting states. We proved that the algorithm is capable of exploring the full safely reachable region with few measurements, and demonstrated its practicality and performance in experiments."}, {"heading": "A Preliminary lemmas", "text": "Lemma 3. \u2200s \u2208 S, ut+1(s) \u2264 ut(s), lt+1(s) \u2265 lt(s), wt+1(s) \u2264 wt(s).\nProof. This lemma follows directly from the definitions of ut(s), lt(s), wt(s) and Ct(s).\nLemma 4. \u2200n \u2265 1, s \u2208 Rretn (S, S) =\u21d2 s \u2208 S \u222a S.\nProof. Proof by induction. Consider n = 1, then s \u2208 Rret(S, S) =\u21d2 s \u2208 S \u222a S by definition. For the induction step, assume s \u2208 Rretn\u22121(S, S) =\u21d2 s \u2208 S \u222a S. Now consider s \u2208 Rretn (S, S). We know that\nRretn (S, S) = R ret(S,Rretn\u22121(S, S)),\n= Rretn\u22121(S, S) \u222a {s \u2208 S | \u2203a \u2208 A(s) : f(s, a) \u2208 Rretn\u22121(S, S)}.\nTherefore, since s \u2208 Rretn\u22121(S, S) =\u21d2 s \u2208 S \u222a S and S \u2286 S \u222a S, it follows that s \u2208 S \u222a S and the induction step is complete.\nLemma 5. \u2200n \u2265 1, s \u2208 Rretn (S, S) \u21d0\u21d2 \u2203k, 0 \u2264 k \u2264 n and (a1, . . . , ak), a sequence of k actions, that induces (s0, s1, . . . , sk) starting at s0 = s, such that si \u2208 S, \u2200i = 1, . . . , k \u2212 1 and sk \u2208 S.\nProof. ( =\u21d2 ). s \u2208 Rretn (S, S) means that either s \u2208 Rretn\u22121(S, S) or \u2203a \u2208 A(s) : f(s, a) \u2208 Rretn\u22121(S, S). Therefore, we can reach a state in R ret n\u22121(S, S) with at most one action. Repeating this procedure i times, the system reaches a state in Rretn\u2212i(S, S). This requires a number of actions that that goes from 0 to i because each time we have the zero-or-one action type of situation we mentioned before. Therefore, after i = n \u2212 1 times we repeat this procedure, the system reaches a state in \u2208 Rret(S, S). Similarly, in this case, the system can reach the set S with either zero or one action. In the end there is a sequence of actions of length k, with 0 \u2264 k \u2264 n, inducing a state trajectory such that: s0 = s, si \u2208 Rretn (S, S) \u2286 S \u222a S for every i = 1, . . . , k \u2212 1 and sk \u2208 S. If it happens that for some i, si \u2208 S, then it is possible to just cut the sequence of actions at ai. ( \u21d0= ). Consider k = 0. This means that s \u2208 S \u2286 Rretn (S, S). In case k = 1 we have that s0 \u2208 S and that f(s0, a1) \u2208 S. Therefore s \u2208 Rret(S, S) \u2286 Rretn (S, S). For k \u2265 2 we know sk\u22121 \u2208 S and f(sk\u22121, ak) \u2208 S =\u21d2 sk\u22121 \u2208 Rret(S, S). Similarly sk\u22122 \u2208 S and f(sk\u22122, ak\u22121) = sk\u22121 \u2208 Rret(S, S) =\u21d2 sk\u22122 \u2208 Rret2 (S, S). If we apply this reasoning k times we obtain that s \u2208 Rretk (S, S) \u2286 Rretn (S, S).\nLemma 6. \u2200S, S \u2286 S, \u2200N \u2265 |S|, RretN (S, S) = RretN+1(S, S) = R ret (S, S)\nProof. This is a direct consequence of Lemma 5. In fact, Lemma 5 states that s belongs to RretN (S, S) if and only if there is a path of length at most N starting from s contained in S that drives the system to a state in S. Since we are dealing with a finite MDP, there are |S| different states. Therefore, if such a path exists it cannot be longer than |S|.\nLemma 7. Given S \u2286 R \u2286 S and S \u2286 R \u2286 S, it holds that Rret(S, S) \u2286 Rret(R,R).\nProof. Let s \u2208 Rret(S, S). It follows from Lemmas 5 and 6 that there exists a sequence of actions, (a1, . . . , ak), with 0 \u2264 k \u2264 |S|, that induces a state trajectory, (s0, s1, . . . , sk), starting at s0 = s with si \u2208 S \u2286 R, \u2200i = 1, . . . , k \u2212 1 and sk \u2208 S \u2286 R. Using the ( \u21d0= ) direction of Lemma 5 and Lemma 6, we conclude that s \u2208 Rret(R,R).\nLemma 8. S \u2286 R =\u21d2 Rreach(S) \u2286 Rreach(R).\nProof. Consider s \u2208 Rreach(S). Then either s \u2208 S \u2286 R or \u2203s\u0302 \u2208 S \u2286 R, a\u0302 \u2208 A(s\u0302) : s = f(s\u0302, a\u0302), by definition. This implies that s \u2208 Rreach(R).\nLemma 9. For any t \u2265 1, S0 \u2286 St \u2286 St+1 and S\u03020 \u2286 S\u0302t \u2286 S\u0302t+1\nProof. Proof by induction. Consider s \u2208 S0, S0 = S\u03020 by initialization. We known that\nl1(s)\u2212 Ld(s, s) = l1(s) \u2265 l0(s) \u2265 h,\nwhere the last inequality follows from Lemma 3. This implies that s \u2208 S1 or, equivalently, that S0 \u2286 S1. Furthermore, we know by initialization that s \u2208 Rreach(S\u03020). Moreover, we can say that s \u2208 Rret(S1, S\u03020), since S1 \u2287 S0 = S\u03020. We can conclude that s \u2208 S\u03021. For the induction step assume that St\u22121 \u2286 St and S\u0302t\u22121 \u2286 S\u0302t. Let s \u2208 St. Then,\n\u2203s\u2032 \u2208 S\u0302t\u22121 \u2286 S\u0302t : lt(s\u2032)\u2212 Ld(s, s\u2032) \u2265 h.\nFurthermore, it follows from Lemma 3 that lt+1(s\u2032)\u2212 Ld(s, s\u2032) \u2265 lt(s\u2032)\u2212 Ld(s, s\u2032). This implies that lt+1(s\u2032)\u2212 Ld(s, s\u2032) \u2265 h. Thus s \u2208 St+1. Now consider s \u2208 S\u0302t. We known that\ns \u2208 Rreach(S\u0302t\u22121) \u2286 Rreach(S\u0302t) by Lemma 8\nWe also know that s \u2208 Rret(St, S\u0302t\u22121). Since we just proved that St \u2286 St+1 and we assumed S\u0302t\u22121 \u2286 S\u0302t for the induction step, Lemma 7 allows us to say that s \u2208 R ret (St+1, S\u0302t). All together this allows us to complete the induction step by saying s \u2208 S\u0302t+1.\nLemma 10. S \u2286 R =\u21d2 Rsafe (S) \u2286 Rsafe (R).\nProof. Consider s \u2208 Rsafe (S), we can say that:\n\u2203s\u2032 \u2208 S \u2286 R : r(z\u2032)\u2212 \u2212 Ld(z, z\u2032) \u2265 h (9)\nThis means that s \u2208 Rsafe (R)\nLemma 11. Given two sets S,R \u2286 S such that S \u2286 R, it holds that: R (S) \u2286 R (R).\nProof. We have to prove that:\ns \u2208 (Rreach(S) \u2229Rret(Rsafe (S), S)) =\u21d2 s \u2208 (Rreach(R) \u2229R ret (Rsafe (R), R)) (10)\nLet\u2019s start by checking the reachability condition first:\ns \u2208 Rreach(S) =\u21d2 s \u2208 Rreach(R). by Lemma 8\nNow let\u2019s focus on the recovery condition. We use Lemmas 7 and 10 to say that s \u2208 Rret(Rsafe (S), S) implies that s \u2208 Rret(Rsafe (R), R) and this completes the proof.\nLemma 12. Given two sets S,R \u2286 S such that S \u2286 R, the following holds: R (S) \u2286 R (R).\nProof. The result follows by repeatedly applying Lemma 11.\nLemma 13. Assume that r is sampled from a GP and that the measurement noise is bounded by \u03c3 for all t \u2265 1. If \u03b2t = 2 log ( |S|\u03c0k \u03b4 ) , where \u2211 k\u22651 \u03c0k = 1,\u03c0k > 0, then the following hold at least with probability 1\u2212 \u03b4:\n\u2200t \u2265 1, \u2200s \u2208 S, |r(s)\u2212 \u00b5t\u22121(s)| \u2264 \u03b2 1 2 t \u03c3t\u22121(s).\nProof. See Lemma 5.1 in [21].\nGiven the assumptions of Lemma 13, the following holds with probability at least 1\u2212 \u03b4:\n\u2200t \u2265 1, \u2200s \u2208 S, r(s) \u2208 Ct(s).\nProof. See Corollary 1 in [22]."}, {"heading": "B Safety", "text": "Lemma 14. For all t \u2265 1 and for all s \u2208 S\u0302t, \u2203(a1, a2, . . . , ak) inducing (s0, s1, . . . , sk), such that s0 = s, sk \u2208 S0 and si \u2208 St,\u2200i = 0, . . . , k.\nProof. We use a recursive argument to prove this lemma. Since s \u2208 St, we know that s \u2208 R ret (St, S\u0302t\u22121). Because of Lemmas 5 and 6 we know \u2203(a1, . . . , aj), with j \u2264 |S|, inducing s0, s1, . . . , sj such that s0 = s, si \u2208 St, \u2200i = 1, . . . , j\u22121 and sj \u2208 S\u0302t\u22121. Now we can build another sequence of actions that drives the system to some state in S\u0302t\u22122 passing through St\u22121 \u2286 St starting from sj \u2208 S\u0302t\u22121. By applying repeatedly this procedure we can build a finite sequence of actions that drives the system to a state in S0 passing through St starting from s.\nLemma 15. For all t \u2265 1 and for all s \u2208 S\u0302t, \u2203(a1, a2, . . . , ak) inducing (s0, s1, . . . , sk), such that: s0 \u2208 S0, sk = s and si \u2208 S\u0302t,\u2200i = 0, . . . , k.\nProof. The proof is analogous to the the one we gave for lemma Lemma 14. The only difference is that here we need to use the reachability property of S\u0302t instead of the recovery property of S\u0302t.\nLemma 16. For all sstart, send \u2208 S\u0302t, \u2203(a1, a2, . . . , aN ), with N \u2208 N, inducing (s0, s1, . . . , sN ), such that s0 = sstart, sN = send and si \u2208 St,\u2200i = 0, . . . , N .\nProof. By initialization of the safe seed we know that we can reach any point in S0 from any other point in S0 following a path that goes only through states in S0 \u2286 St. This observation, jointly with Lemma 14 and Lemma 15 completes the proof.\nLemma 17. For any t \u2265 0, the following holds with probability at least 1\u2212 \u03b4: \u2200s \u2208 St, r(s) \u2265 h.\nProof. Let\u2019s prove this result by induction. By initialization we know that r(s) \u2265 h for all s \u2208 S0. For the induction step assume that for all s \u2208 St\u22121 holds that r(s) \u2265 h. For any s \u2208 St, by definition, there exists z \u2208 S\u0302t\u22121 \u2286 St\u22121 such that\nh \u2264 lt(z)\u2212 Ld(s, z), \u2264 r(z)\u2212 Ld(s, z), by Lemma 13 \u2264 r(s). by Lipschitz continuity\nThis relation holds with probability at least 1\u2212 \u03b4 because we used Lemma 13 to prove it.\nTheorem 2. Let (s0, s1, . . . , sk, . . .) be a state trajectory of a MDP with the following transition function, sk+1 = f(sk, ak). If the input trajectory is chosen according to our algorithm, then, with probability at least 1\u2212 \u03b4, holds that r(sk) \u2265 h,\u2200k \u2265 0.\nProof. Let\u2019s denote as (st1, s t 2, . . . , s t k) the state trajectory of the system until the end of iteration t \u2265 0. We know from Lemma 16 that the sti \u2208 St, \u2200i = 1, . . . , k. Lemma 17 completes the proof as it allows us to say that r(sti) \u2265 h, \u2200i = 1, . . . , k with probability at least 1\u2212 \u03b4."}, {"heading": "C Completeness", "text": "Lemma 18. For any t1 \u2265 t0 \u2265 1, if S\u0302t1 = S\u0302t0 , then, \u2200t such that t0 \u2264 t \u2264 t1, it holds that Gt+1 \u2286 Gt\nProof. Since S\u0302t is not changing we are always computing the enlargement function over the same points. Therefore we only need to prove that the enlargement function is non increasing. We known from Lemma 3 that ut(s) is a non increasing function of t for all s \u2208 S. Furthermore we know that (S \\ St) \u2287 (S \\ St+1) because of Lemma 9. Hence, the enlargement function is non increasing and the proof is complete.\nLemma 19. For any t1 \u2265 t0 \u2265 1, if S\u0302t1 = S\u0302t0 , C1 = 8/log(1 + \u03c3\u22122) and st = argmax s\u2208Gt wt(s), then, \u2200t such that t0 \u2264 t \u2264 t1, it holds that wt(st) \u2264 \u221a C1\u03b2t\u03b3t t\u2212t0 .\nProof. See Lemma 5 in [22].\nFor any t \u2265 1, if C1 = 8/log(1+\u03c3\u22122) and Tt is the smallest positive integer such that Tt\u03b2t+Tt\u03b3t+Tt \u2265 C1 2 and St+Tt = St, then, for any s \u2208 Gt+Tt it holds that wt+Tt(s) \u2264\nProof. The proof is trivial because Tt was chosen to be the smallest integer for which the right hand side of the inequality proved in Lemma 19 is smaller or equal to .\nLemma 20. For any t \u2265 1, if R (S0) \\ S\u0302t 6= \u2205, then, R (S\u0302t) \\ S\u0302t 6= \u2205.\nProof. For the sake of contradiction assume that R (S\u0302t) \\ S\u0302t = \u2205. This implies R (S\u0302t) \u2286 S\u0302t. On the other hand, since S\u0302t is included in all the sets whose intersection defines R (S\u0302t), we know that, S\u0302t \u2286 R (S\u0302t). This implies that S\u0302t = R (S\u0302t). If we apply repeatedly the one step reachability operator on both sides of the equality we obtain R (S\u0302t) = S\u0302t. By Lemmas 3 and 12 we know that\nS0 = S\u03020 \u2286 S\u0302t =\u21d2 R (S0) \u2286 R (S\u0302t) = S\u0302t.\nThis contradicts the assumption that R (S0) \\ S\u0302t 6= \u2205.\nLemma 21. For any t \u2265 1, if R (S0) \\ S\u0302t 6= \u2205, then, with probability at least 1 \u2212 \u03b4 it holds that S\u0302t \u2282 S\u0302t+Tt .\nProof. By Lemma 20 we know that R (S\u0302t) \\ S\u0302t 6= \u2205. This implies that \u2203s \u2208 R (S\u0302t) \\ S\u0302t. Therefore there exists a s\u2032 \u2208 S\u0302t such that:\nr(s\u2032)\u2212 \u2212 Ld(s, s\u2032) \u2265 h (11)\nFor the sake of contradiction assume that S\u0302t+Tt = S\u0302t. This means that s \u2208 S \\ S\u0302t+Tt and s\u2032 \u2208 S\u0302t+Tt . Then we have:\nut+Tt(s \u2032)\u2212 Ld(s, s\u2032) \u2265 r(s\u2032)\u2212 Ld(s, s\u2032) by Lemma 13\n\u2265 r(s\u2032)\u2212 \u2212 d(s, s\u2032) (12) \u2265 h by equation 11\nAssume, for the sake of contradiction, that s \u2208 S \\ St+Tt . This means that s\u2032 \u2208 Gt+Tt . We know that for any t \u2264 t\u0302 \u2264 t + Tt holds that S\u0302t\u0302 = S\u0302t, because S\u0302t = S\u0302t+Tt and S\u0302t \u2286 S\u0302t+1 for all t \u2265 1. Therefore we have s\u2032 \u2208 S\u0302t+Tt\u22121 such that:\nlt+Tt(s \u2032)\u2212 Ld(s, s\u2032) \u2265 lt+Tt(s\u2032)\u2212 r(s\u2032) + + h by equation 11\n\u2265 \u2212wt+Tt(s\u2032) + + h by Lemma 13 \u2265 h by Lemma 19\nThis implies that s \u2208 St+Tt , which is a contradiction. Thus we can say that s \u2208 St+Tt . Now we want to focus on the recovery and reachability properties of s in order to reach the contradiction that s \u2208 S\u0302t+Tt . Since s \u2208 R (S\u0302t+Tt) \\ S\u0302t+Tt we know that:\ns \u2208 Rreach(S\u0302t+Tt) = Rreach(S\u0302t+Tt\u22121) (13)\nWe also know that s \u2208 R (S\u0302t+Tt) \\ S\u0302t+Tt =\u21d2 s \u2208 R ret\n(Rsafe (S\u0302t+Tt), S\u0302t+Tt). We want to use this fact to prove that s \u2208 Rret(St+Tt , S\u0302t+Tt\u22121). In order to do this, we intend to use the result from Lemma 7. We already know that S\u0302t+Tt\u22121 = S\u0302t+Tt . Therefore we only need to prove\nthat Rsafe (S\u0302t+Tt) \u2286 St+Tt . For the sake of contradiction assume this is not true. This means \u2203z \u2208 Rsafe (S\u0302t+Tt) \\ St+Tt . Therefore there exists a z\u2032 \u2208 S\u0302t+Tt such that: r(z\u2032)\u2212 \u2212 Ld(z\u2032, z) \u2265 h (14) Consequently:\nut+Tt(z \u2032)\u2212 Ld(z\u2032, z) \u2265 r(z\u2032)\u2212 Ld(z\u2032, z) by Lemma 13\n\u2265 r(z\u2032)\u2212 \u2212 d(z\u2032, z) (15) \u2265 h by equation 14\nHence z\u2032 \u2208 Gt+Tt . Once again we can say there exists z\u2032 \u2208 St+Tt\u22121 such that: lt+Tt(z\n\u2032)\u2212 Ld(z\u2032, z) \u2265 lt+Tt(z\u2032)\u2212 r(z\u2032) + + h by equation 11 \u2265 \u2212wt+Tt(z\u2032) + + h by Lemma 13 \u2265 h by Lemma 19\nTherefore z \u2208 St+Tt . This is a contradiction. Therefore we can say that Rsafe (S\u0302t+Tt) \u2286 St+Tt . Hence:\ns \u2208 R (S\u0302t+Tt) \\ S\u0302t+Tt =\u21d2 s \u2208 R ret\n(St+Tt , S\u0302t+Tt\u22121) (16) In the end the fact that s \u2208 St+Tt and (13) and (16) allow us to conclude that s \u2208 S\u0302t+Tt . This contradiction proves the theorem.\nLemma 22. \u2200t \u2265 0, S\u0302t \u2286 R0(S0) with probability at least 1\u2212 \u03b4.\nProof. Proof by induction. We know that S0 \u2286 R0(S0) by definition. For the induction step assume that for some t \u2265 1 holds that S\u0302t\u22121 \u2286 R0(S0). Our goal is to show that s \u2208 S\u0302t =\u21d2 s \u2208 R0(S0). In order to this, we will try to show that s \u2208 R0(S\u0302t\u22121). We know that:\ns \u2208 S\u0302t =\u21d2 s \u2208 Rreach(S\u0302t\u22121) (17) Furthermore we can say that:\ns \u2208 S\u0302t =\u21d2 s \u2208 R ret (St, S\u0302t\u22121) (18)\nConsider s \u2208 St. We know that \u2203s\u2032 \u2208 S\u0302t\u22121 such that: h \u2264 lt(s\u2032)\u2212 Ld(s, s\u2032), (19) \u2264 r(s\u2032)\u2212 Ld(s, s\u2032). by Lemma 13\nThis means s \u2208 Rsafe0 (S\u0302t\u22121), or, equivalently, that St \u2286 Rsafe0 (S\u0302t\u22121). Hence, Lemma 7 allows us to say that R ret (St, S\u0302t\u22121) \u2286 R ret (Rsafe0 (S\u0302t\u22121), S\u0302t\u22121). This result, together with (17), leads us to the conclusion that s \u2208 R0(S\u0302t\u22121). Since we assumed for the induction step that S\u0302t\u22121 \u2286 R0(S0), we conclude that s \u2208 R0(S0). This proves that s \u2208 S\u0302t =\u21d2 s \u2208 R0(S0) and the induction step is complete.\nLemma 23. Let t\u2217 be the smallest integer such that t\u2217 \u2265 |R0(S0)|Tt\u2217 , then there exists a t0 \u2264 t\u2217 such that, with probability at least 1\u2212 \u03b4 holds that St0+Tt0 = St0 .\nProof. For the sake of contradiction assume that the opposite holds true: \u2200t \u2264 t\u2217, St \u2282 St+Tt . This implies that S0 \u2282 ST0 . Furthermore we know that Tt is increasing in t. Therefore 0 \u2264 t\u2217 =\u21d2 T0 \u2264 Tt\u2217 =\u21d2 ST0 \u2286 STt\u2217 . Now if |R0(S0)| \u2265 1 we know that:\nt\u2217 \u2265 Tt\u2217 =\u21d2 Tt\u2217 \u2265 TTt\u2217 =\u21d2 Tt\u2217 + TTt\u2217 \u2264 2Tt\u2217 =\u21d2 STt\u2217+TTt\u2217 \u2286 S2Tt\u2217\nThis justifies the following chain of inclusions: S0 \u2282 ST0 \u2286 STt\u2217 \u2282 STt\u2217+TTt\u2217 \u2286 S2Tt\u2217 \u2282 . . . This means that for any 0 \u2264 k \u2264 |R0(S0)| it holds that |SkTt\u2217 | > k. In particular, for k\u2217 = |R0(S0)| we have |Sk\u2217Tt\u2217 | > |R0(S0)|. This contradicts Lemma 22 (which holds true with probability at least 1\u2212 \u03b4).\nLet t\u2217 be the smallest integer such that t \u2217 \u03b2t\u2217\u03b3t\u2217 \u2265 C1|R0(S0)| 2 , then, there is t0 \u2264 t \u2217 such that St0+Tt0 = St0 with probability at least 1\u2212 \u03b4.\nProof. The proof consists in applying the definition of Tt to the condition of Lemma 23.\nTheorem 3. Let t\u2217 be the smallest integer such that t \u2217 \u03b2t\u2217\u03b3t\u2217 \u2265 C1|R0(S0)| 2 , with C1 = 8/log(1+\u03c3 \u22122), then, there is t0 \u2264 t\u2217 such that R (S0) \u2286 St0 \u2286 R0(S0) with probability at least 1\u2212 \u03b4.\nProof. Due to Lemma 23 we know that \u2203t0 \u2264 t\u2217 such that St0 = St0+Tt0 with probability at least 1 \u2212 \u03b4. This implies that R (S0) \\ (St) = \u2205 with probability at least 1 \u2212 \u03b4 because of Lemma 21. Therefore R (S0) \u2286 St. Furthermore we know that St \u2286 R0(S0) with probability at least 1 \u2212 \u03b4 because of Lemma 22 and this completes the proof."}, {"heading": "D Main result", "text": "Theorem 1. Assume that r is L-Lipschitz continuous and that the assumptions of Lemma 1 hold. Also, assume that S0 6= \u2205, r(s) \u2265 h for all s \u2208 S0, and that for any two states, s, s\u2032 \u2208 S0, s\u2032 \u2208 Rret(S0, {s}). Choose \u03b2t as in (8). Let (s0, s1, . . . , sk, . . .) denote a state trajectory induced by Algorithm 1 on an MDP with transition function f(s, a). Then, with probability at least 1\u2212 \u03b4, r(sk) \u2265 h,\u2200k > 0.Moreover, let t\u2217 be the smallest integer such that t \u2217\n\u03b2t\u2217\u03b3t\u2217 \u2265 C |R0(S0)| 2 ,\nwith C = 8/log(1 + \u03c3\u22122). Then, there exists a t0 \u2264 t\u2217 such that, with probability at least 1\u2212 \u03b4, R (S0) \u2286 S\u0302t0 \u2286 R0(S0).\nProof. This is a direct consequence of Theorem 2 and Theorem 3."}], "references": [{"title": "Reachability-based safe learning with Gaussian processes", "author": ["Anayo K. Akametalu", "Shahab Kaynama", "Jaime F. Fisac", "Melanie N. Zeilinger", "Jeremy H. Gillula", "Claire J. Tomlin"], "venue": "In Proc. of the IEEE Conference on Decision and Control (CDC),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna D. Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Safe and robust learning control with Gaussian processes", "author": ["Felix Berkenkamp", "Angela P. Schoellig"], "venue": "In Proc. of the European Control Conference (ECC),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Safe controller optimization for quadrotors with Gaussian processes", "author": ["Felix Berkenkamp", "Angela P. Schoellig", "Andreas Krause"], "venue": "In Proc. of the IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Risk-sensitive and minimax control of discrete-time, finite-state Markov decision processes", "author": ["Stefano P. Coraluppi", "Steven I. Marcus"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Safe exploration of state and action spaces in reinforcement learning", "author": ["Javier Garcia", "Fernando Fern\u00e1ndez"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Risk-sensitive reinforcement learning applied to control under constraints", "author": ["Peter Geibel", "Fritz Wysotzki"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Posterior consistency of Gaussian process prior for nonparametric binary regression", "author": ["Subhashis Ghosal", "Anindya Roy"], "venue": "The Annals of Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Safe exploration for reinforcement learning", "author": ["Alexander Hans", "Daniel Schneega\u00df", "Anton Maximilian Sch\u00e4fer", "Steffen Udluft"], "venue": "In Proc. of the European Symposium on Artificial Neural Networks (ESANN),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Reinforcement learning in robotics: a survey", "author": ["Jens Kober", "J. Andrew Bagnell", "Jan Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Introduction: Mars Science Laboratory: The Next Generation of Mars Landers", "author": ["Mary Kae Lockwood"], "venue": "Journal of Spacecraft and Rockets,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Mars Reconnaissance Orbiter\u2019s High Resolution Imaging Science Experiment (HiRISE)", "author": ["Alfred S. McEwen", "Eric M. Eliason", "James W. Bergstrom", "Nathan T. Bridges", "Candice J. Hansen", "W. Alan Delamere", "John A. Grant", "Virginia C. Gulick", "Kenneth E. Herkenhoff", "Laszlo Keszthelyi", "Randolph L. Kirk", "Michael T. Mellon", "Steven W. Squyres", "Nicolas Thomas", "Catherine M. Weitz"], "venue": "Journal of Geophysical Research: Planets,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Bayesian Approach to Global Optimization, volume 37 of Mathematics and Its Applications", "author": ["Jonas Mockus"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Safe exploration in Markov decision processes", "author": ["Teodor Mihai Moldovan", "Pieter Abbeel"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Safe exploration techniques for reinforcement learning \u2013 an overview", "author": ["Martin Pecka", "Tomas Svoboda"], "venue": "In Modelling and Simulation for Autonomous Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Gaussian processes for machine learning. Adaptive computation and machine learning", "author": ["Carl Edward Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Learning Control in Robotics", "author": ["Stefan Schaal", "Christopher Atkeson"], "venue": "IEEE Robotics & Automation Magazine,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Safe exploration for active learning with Gaussian processes", "author": ["Jens Schreiter", "Duy Nguyen-Tuong", "Mona Eberts", "Bastian Bischoff", "Heiner Markert", "Marc Toussaint"], "venue": "In Proc. of the European Conference on Machine Learning (ECML),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Gaussian process optimization in the bandit setting: no regret and experimental design", "author": ["Niranjan Srinivas", "Andreas Krause", "Sham M. Kakade", "Matthias Seeger"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Safe exploration for optimization with Gaussian processes", "author": ["Yanan Sui", "Alkis Gotovos", "Joel Burdick", "Andreas Krause"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "As a consequence, robots need to be able to learn and adapt to unknown environments autonomously [10, 2].", "startOffset": 97, "endOffset": 104}, {"referenceID": 1, "context": "As a consequence, robots need to be able to learn and adapt to unknown environments autonomously [10, 2].", "startOffset": 97, "endOffset": 104}, {"referenceID": 16, "context": "While exploration algorithms are known, safety is still an open problem in the development of such systems [18].", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "Safe exploration is an open problem in the reinforcement learning community and several definitions of safety have been proposed [16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "In risk-sensitive reinforcement learning, the goal is to maximize the expected return for the worst case scenario [5].", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "For example, Geibel and Wysotzki [7] define risk as the probability of driving the system to a previously known set of undesirable states.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Garcia and Fern\u00e1ndez [6] propose to ensure safety by means of a backup policy; that is, a policy that is known to be safe in advance.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "[9], where safety is defined in terms of a minimum reward, which is learned from data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Moldovan and Abbeel [14] provide probabilistic safety guarantees at every time step by optimizing over ergodic policies; that is, policies that let the agent recover from any visited state.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "[1] use reachability analysis to ensure stability under the assumption of bounded disturbances.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The work in [3] uses robust control techniques in order to ensure robust stability for model uncertainties, while the uncertain model is improved.", "startOffset": 12, "endOffset": 15}, {"referenceID": 12, "context": "Another field that has recently considered safety is Bayesian optimization [13].", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "There, in order to find the global optimum of an a priori unknown function [21], regularity assumptions in form of a Gaussian process (GP) [17] are made.", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "There, in order to find the global optimum of an a priori unknown function [21], regularity assumptions in form of a Gaussian process (GP) [17] are made.", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "[22] and Schreiter et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20], where the goal is to find the safely reachable optimum without violating an a priori unknown safety constraint at any evaluation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The method in [22] has the advantage of being sample efficient.", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "It was applied to the field of robotics to safely optimize the controller parameters of a quadrotor vehicle [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 20, "context": "The main contribution consists of extending the work on safe Bayesian optimization in [22] from the bandit setting to MDPs.", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "It enjoys similar safety guarantees in terms of ergodicity, the ability to return to the safe starting point, as presented in [14], but at a vastly reduced computational cost.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "In the following, we assume that S is endowed with a positive definite kernel function, k(\u00b7, \u00b7), and that the function, r(\u00b7), has bounded norm in the associated Reproducing Kernel Hilbert Space (RKHS) [19].", "startOffset": 201, "endOffset": 205}, {"referenceID": 15, "context": "This assumption allows us to model r as a Gaussian Process (GP) [17], r(s) \u223c GP(\u03bc(s), k(s, s\u2032)).", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "For many commonly used kernels, the GP assumption implies that the reward function is Lipschitz continuous with high probability [21, 8].", "startOffset": 129, "endOffset": 136}, {"referenceID": 7, "context": "For many commonly used kernels, the GP assumption implies that the reward function is Lipschitz continuous with high probability [21, 8].", "startOffset": 129, "endOffset": 136}, {"referenceID": 19, "context": "[21] in the multi-armed bandit setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "More details on these bounds on \u03b3t can be found in [21].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "The choice of \u03b2t in (8) is justified by the following Lemma, which follows from Theorem 6 in [21]: Lemma 1.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "We consider the setting in [14], the exploration of the surface of Mars with a rover.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "For the experiment, we consider the Mars Science Laboratory (MSL) [11], a rover deployed on Mars.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "In our experiments we use digital terrain models of the surface of Mars from High Resolution Imaging Science Experiment (HiRISE), which have a resolution of one meter [12].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "As opposed to the experiments considered by Moldovan and Abbeel [14], we do not have to subsample or smoothen the data in order to achieve good exploration results.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "Therefore, every state in the MDP represents a d\u00d7 d square area with d = 1m, as opposed to d = 20m in [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 20, "context": "Following the recommendations in [22], in our experiments we use the GP confidence intervals Qt(s) directly to determine the safe set St.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "The same assumption is used in [14].", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "In this setting the agent samples the most uncertain safe state transaction, which corresponds to the safe Bayesian optimization framework in [20].", "startOffset": 142, "endOffset": 146}], "year": 2016, "abstractText": "In classical reinforcement learning, when exploring an environment, agents accept arbitrary short term loss for long term gain. This is infeasible for safety critical applications, such as robotics, where even a single unsafe action may cause system failure. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an, a priori unknown, safety constraint that depends on states and actions. We aim to explore the MDP under this constraint, assuming that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.", "creator": "LaTeX with hyperref package"}}}