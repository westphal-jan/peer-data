{"id": "1606.04801", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "A Powerful Generative Model Using Random Weights for the Deep Image Representation", "abstract": "To what extent exists the success although deep constructs increased only the providing? Could we do deep visualization using undertrained, counts weight commercial? To address kind states, we explore new and led ics models even double popular finger visualization tasks machine skilled, instances ratio ostium neural networks. First thing flatten interpretations has feature spaces from retrieving shows from white rhythm availability. The reconstruction fairly yet statistically consumer than any known the be required studying on well skilled venture back entire and earliest. Next realize synthesize textures also scaling randomness example e.g. now multiple layers and our results various almost indistinguishable although held model natural coating and took optically textures in well the instructors network. Third, by true-to-life 's content of however metaphor which entire style entire mostly creations, happen could cinematic images with area disorientation quality, normally competitive coming the upon learning including Gatys est al. on pretrained connected. To sort intellectual this an an first demonstration included sense phonetic using untrained sense insertion networks. Our work provides a part addition storytelling tool to researcher the representation created beneath communication architecture and sheds \u2014 on post mou day deep techniques.", "histories": [["v1", "Wed, 15 Jun 2016 14:56:42 GMT  (5212kb,D)", "https://arxiv.org/abs/1606.04801v1", "10 pages, 10 figures, submited to NIPS 2016 conference. Computer Vision and Pattern Recognition, Neurons and Cognition, Neural and Evolutionary Computing"], ["v2", "Thu, 16 Jun 2016 06:53:55 GMT  (5212kb,D)", "http://arxiv.org/abs/1606.04801v2", "10 pages, 10 figures, submited to NIPS 2016 conference. Computer Vision and Pattern Recognition, Neurons and Cognition, Neural and Evolutionary Computing"]], "COMMENTS": "10 pages, 10 figures, submited to NIPS 2016 conference. Computer Vision and Pattern Recognition, Neurons and Cognition, Neural and Evolutionary Computing", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["kun he", "yan wang", "john e hopcroft"], "accepted": true, "id": "1606.04801"}, "pdf": {"name": "1606.04801.pdf", "metadata": {"source": "CRF", "title": "A Powerful Generative Model Using Random Weights for the Deep Image Representation", "authors": ["Kun He", "Yan Wang"], "emails": ["brooklet60@hust.edu.cn,", "kh555@cs.cornell.edu", "yanwang@hust.edu.cn", "jeh@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4]. With advances in training, there is a growing trend towards understanding the inner working of these deep networks. By training on a very large image data set, DNNs develop a representation of images that makes object information increasingly explicit at various levels of the hierarchical architecture. Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11]. \u2217The three authors contribute equally. \u2020Corresponding author.\nar X\niv :1\n60 6.\n04 80\n1v 2\n[ cs\nInversion techniques have been developed to create synthetic images with feature representations similar to the representations of an original image in one or several layers of the network. Feature representations are a function \u03a6 of the source image x0. An approximate inverse \u03a6\u22121 is used to construct a new image x from the code \u03a6(x0) by reducing some statistical discrepancy between \u03a6(x) and \u03a6(x0). Mahendran et al. [7] use the pretrained CNN AlexNet [2] and define a squared Euclidean loss on the activations to capture the representation differences and reconstruct the image. Gatys et al. [8, 12] define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3]. Gatys et al. [13] then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups [14, 15]. Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features [5, 9] or the class prediction scores [6]. Nguyen et al. further try multifaceted visualization to separate and visualize different features that a neuron learns [16].\nFeature inversion and neural activation maximization both start from a white noise image and calculate the gradient via backpropagation to morph the white noise image and output a natural image. In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc. The method of visualizing the feature representation on the intermediate layers sheds light on the information represented at each layer of the pretrained CNN.\nA third set of researchers trains a separate feed-forward CNN with upconvolutional layers using representations or correlations of the feature maps produced in the original network as the input and the source image as the target to learn the inversion of the original network. The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15]. Instead of designing a natural prior, some researchers incorporate adversarial training[20] to improve the realism of the generated images[18]. Their trained upconvolutional network could give similar qualitative results as the inversion technique does and is two or three orders of magnitude faster, as the previous inversion technique needs a forward and backward pass through the pretrained network. This technique is slightly different from the previous two in that it does not focus on understanding the original CNN but on the visualization task.\nIt is well recognized that deep visualization techniques conduct a direct analysis of the visual information contained in image representations, and help us understand the representation encoded at the intermediate layers of the well trained DNNs. In this paper, we raise a fundamental issue that other researchers rarely address: Could we do deep visualization using untrained, random weight DNNs? This would allow us to separate the contribution of training from the contribution of the network structure. It might even give us a method to evaluate deep network architectures without spending days and significant computing resources in training networks so that we could compare them. Though Gray et al. demonstrated that the VGG architecture with random weights failed in generating textures and resulted in white noise images in an experiment indicating the trained filters might be crucial for texture generation [8], we conjecture the success of deep visualization mainly originates from the intrinsic nonlinearity and complexity of the deep network hierarchical structure rather than from the training, and that the architecture itself may cause the inversion invariant to the original image. Gatys et al.\u2019s unsuccessful attempt on the texture synthesis using the VGG architecture with random weights may be due to their inappropriate scale of the weighting factors.\nTo verify our hypothesis, we try three popular inversion tasks for visualization using the CNN architecture with random weights. Our results strongly suggest that this is true. Applying inversion techniques on the untrained VGG with random weights, we reconstruct high perceptual quality images. The results are qualitatively better than the reconstructed images produced on the pretrained VGG with the same architecture. Then, we try to synthesize natural textures using the random weight VGG. With automatic normalization to scale the squared correlation loss for different activation layers, we succeed in generating similar textures as the prior work of Gatys et al. [8] on well-trained VGG. Furthermore, we continue the experiments on style transfer, combining the content of an image and the style of an artwork, and create artistic imagery using random weight CNN.\nTo our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the perception and representation of deep network architecture, and shed light on new understandings on deep visualization. Our work will inspire more possibilities of using the generative power of CNNs with random weights, which do not need long training time on multi-GPUs. Furthermore, it is very hard to prove why trained deep neural networks work so well. Based on the networks with random weights, we might be able to prove some properties of the deep networks. Our work using random weights shows a possible way to start developing a theory of deep learning since with well-trained weights, theorems might be impossible."}, {"heading": "2 Convolutional neural network", "text": "VGG-19 [3] is a convolutional neural network trained on the 1.3 million-image ILSVRC 2012 ImageNet dataset [1] using the Caffe-framework [21]. The VGG architecture has 16 convolutional and 5 pooling layers, followed by 3 fully connected layers as illustrated in Figure 1. The filters are of size 3 \u00d7 3 \u00d7Nl where Nl is the number of feature maps (or channels). Pooling is applied between the convolutional layers with 2 \u00d7 2 spatial down-sampling on the feature maps. A pre-processing step subtracts the mean RGB value of each pixel of the training set.\nGatys et al. re-train the VGG-19 network using average pooling instead of maximum pooling, which they suggest could improve the gradient flow and obtain slightly better results [8]. They only consider the convolutional and pooling layers for texture synthesis, and they rescale the weights such that the mean activation of each filter over the images and positions is 1. Their trained network is denoted as VGG in the following discussion.\nWe adopt the same architecture, replacing the weights with purely random values from a Gaussian distribution N(0, \u03c3). The standard deviation, \u03c3, is set to a small number like 0.015 in the experiments. The VGG-based random weight network created as described in the method section is used as our reference network, denoted as ranVGG in the following discussion."}, {"heading": "3 Methods", "text": "In order to better understand the deep representation in the CNN architecture, we focus on three tasks: inverting the image representation, synthesizing texture, and creating artistic style images. Our methods are similar in spirit to existing methods [7, 8, 13]. The main difference is that we use random weights instead of trained weights, and we apply weighting factors determined by a pre-process to normalize the different impact scales of different activation layers on the input layer. Another change is that we apply a greedy approach to build a \u201cstacked\u201d random weight network using the inversion technique to stabilize the visualization quality.\nInverting deep representations. Given a representation function F l : RH\u00d7W\u00d7C \u2192 RNl\u00d7Ml for the lth layer of a deep network and F l(x0) for an input image x0, we want to reconstruct an image x that minimizes the L2 loss among the representations of x0 and x.\nx\u2217 = argmin x\u2208RH\u00d7W\u00d7C Lcontent(x, x0, l) = argmin x\u2208RH\u00d7W\u00d7C \u03c9l 2NlMl \u2016F l(x)\u2212 F l(x0)\u201622 (1)\nHere H and W denote the size of the image, C = 3 the color channels, and \u03c9l the weighting factor. We regard the feature map matrix F l as the representation function of the lth layer which\nhas Nl \u00d7Ml dimensions where Nl is the number of distinct feature maps, each of size Ml when vectorised. F lik denotes the activation of the i th filter at position k.\nThe representations are a chain of non-linear filter banks even if untrained random weights are applied to the network. We initialize the pre image with white noise, and apply the L BFGS gradient descent using standard error backpropagation to morph the input pre image to the target.\nxt+1 = xt \u2212 ( \u2202L(x, x0, l)\n\u2202F l \u2202F l \u2202x ) \u2223\u2223\u2223\u2223 xt\n(2)\n\u2202L(x, x0, l) \u2202F li,k \u2223\u2223\u2223\u2223 xt = 1 N lM l (F l(xt)\u2212 F l(x0))i,k (3)\nThe weighting factor \u03c9l is applied to normalize the gradient impact on the morphing image x. We use a pre-processing procedure to determine the value of \u03c9l. For the current layer l, we approximately calculate the maximum possible gradient by Equation (4), and back propagate the gradient to the input layer. Then we regard the reciprocal of the absolute mean gradient over all pixels and RGB channels as the value of \u03c9l such that the gradient impact of different layers is approximately of the same scale. This normalization doesn\u2019t affect the reconstruction from the activations of a single layer, but is added for the combination of content and style for the style transfer task.\n1\n\u03c9l =\n1\nWHC \u2223\u2223\u2223\u2223 W\u2211 i=1 H\u2211 j=1 C\u2211 k=1 \u2202L(x0, x\u2032, l) \u2202xi,j,k \u2223\u2223\u2223\u2223 F l(x\u2032)=0\n(4)\nTo stabilize the reconstruction quality, we apply a greedy approach to build a \u201cstacked\u201d random weight network ranVGG based on the VGG-19 architecture. Select one single image as the reference image and starting from the first convolutional layer, we build the stacked random weight VGG by sampling, selecting and fixing the weights of each layer in forward order. For the current layer l, fix the weights of the previous l \u2212 1 layers and sample several sets of random weights connecting the lth layer. Then reconstruct the target image using the rectified representation of layer l, and choose weights yielding the smallest loss. Experiments in the next section show our success on the reconstruction by using the untrained, random weight CNN, ranVGG.\nTexture synthesis. Can we synthesize natural textures based on the feature space of an untrained deep network? To address this issue, we refer to the method proposed by Gatys et al.[8] and use the correlations between feature responses on each layer as the texture representation. The inner product between pairwise feature maps i and j within each layer l,Glij = \u2211 k F l ikF l jk, defines a gram matrix Gl = F l(F l)T . We seek a texture image x that minimizes the L2 loss among the correlations of the representations of several candidate layers for x and a groundtruth image x0.\nx\u2217 = argmin x\u2208RH\u00d7W\u00d7C Ltexture = argmin x\u2208RH\u00d7W\u00d7C \u2211 l\u2208L \u00b5lE(x, x0, l), (5)\nwhere the contribution of layer l to the total loss is defined as\nE(x, x0, l) = 1\n4N2l M 2 l\n\u2016Gl(F l(x))\u2212Gl(F l(x0))\u201622. (6)\nThe derivative of E(x, x0, l) with respect to the activations F l in layer l is [8]: \u2202E(x, x0, l)\n\u2202F li,k =\n1\nN2l M 2 l\n{(F l(x))T [Gl(F l(x))\u2212Gl(F l(x0))]}i,k (7)\nThe weighting factor \u00b5l is defined similarly to \u03c9l, but here we use the loss contribution E(x, x0, l) of layer l to get its gradient impact on the input layer.\n1\n\u00b5l =\n1\nWHC \u2223\u2223\u2223\u2223 W\u2211 i H\u2211 j C\u2211 k \u2202E(x0, x \u2032, l) \u2202xi,j,k \u2223\u2223\u2223\u2223 F l(x\u2032)=0\n(8)\nWe then perform the L BFGS gradient descent using standard error backpropagation to morph the input image to a synthesized texture image using the untrained ranVGG.\nStyle transfer. Can we use the untrained deep network to create artistic images? Referring to the prior work of Gatys et al.[13] from the feature responses of VGG trained on ImageNet, we use an untrained VGG and succeed in separating and recombining content and style of arbitrary images. The objective requires terms for content and style respectively with suitable combination factors. For content we use the method of reconstruction on medium layer representations, and for style we use the method of synthesising texture on some lower through higher layer representation correlations.\nLet xc be the content image and xs the style image. We combine the content of the former and the style of the latter by optimizing the following objective:\nx\u2217 = argmin x\u2208RH\u00d7W\u00d7C \u03b1Lcontent(x, xc) + \u03b2Ltexture(x, xs) + \u03b3R(x) (9)\nHere \u03b1 and \u03b2 are the contributing factors for content and style respectively. We apply a regularizer R(x), total variation(TV) [7] defined as the squared sum on the adjacent pixel\u2019s difference of x, to encourage the spatial smoothness in the output image."}, {"heading": "4 Experiments", "text": "This section evaluates the representation inversion, texture synthesis and style transfer results obtained by our model using the untrained network ranVGG. The input image is required to be of size 256\u00d7 256 if we want to invert the representation of the fully connected layers. Else, the input could be of arbitrary size.\nInverting deep representations. We select several source images from the ILSVRC 2012 challenge [1] validation data as examples for the inversion task, and choose a monkey image as the reference image to build the stacked ranVGG3. The reconstruction of the monkey from each layer of the ranVGG is as shown in Figure 2. All convolutional and pooling layers retain photographically accurate information about the image, and the representations reveal the invariance even for the first two fully connected layers.\nAs compared with the inverting technique proposed by Mahendran et al. [7], we only consider the Euclidean loss over the activations and ignore the regularizer they used to capture the natural image prior. Our ranVGG contains 19 layers of random weights (16 convolutional layers and 3 fully connected layers), plus 5 pooling layers. Mahendran et al. use a reference network AlexNet [2] which contains 8 layers of trained weights (5 convolutional layers and 3 fully connected layers), plus 3 pooling layers. Figure 3 shows that we reach higher perceptive reconstructions4. The reason may lie in the fact that the VGG architecture uses filters with a small receptive field of 3 \u00d7 3 and we adopt average pooling. Though shallower than VGG, their reference network, AlexNet, adopts\n3Note that using other image as the reference image also returns similar results. 4All images are best viewed in color/screen.\nlarger filters and uses maximum pooling, which makes it harder to get images well inverted and easily leads to spikes. That\u2019s why they used regularizers to polish the reconstructed image. Figure 4 shows more examples (house, falmingo, girl) of reconstructions obtained by our method on ranVGG and on the well trained VGG.\nFigure 5 shows the convergence of the loss (average Euclidean distance) along the gradient descent iterations for two example images, monkey and house. The reconstruction converges much quicker on ranVGG and yields higher perceptual quality results. Note that the reconstruction on VGG remains the same even if we double the iteration limits to 4000 iterations.\nFigure 6 shows the variations on one example image (the girl image at Figure 4). As compared with the VGG with purely random weights, ranVGG, the VGG with stacked random weights, exhibits lower variations and lower reconstruction distances. As compared with the trained VGG, both stacked ranVGG and VGG with purely random weights exhibit lower reconstruction distance with lower variations. ranVGG demonstrates a more stable and high performance for the inversion task.\nTexture synthesis. Figure 8 shows the textures synthesized by our model on ranVGG for several natural texture images (fifth row) selected from a texture website5 and an artwork named Night Starry by Vincent van Gohn 1989. Each row of images was generated using an increasing number of convolutional layers to constrain the gradient descent. conv1 1 for the first row, conv1 1 and\n5http://www.textures.com/\nconv2 1 for the second row, etc (the labels at each row indicate the top-most layer included). The joint matching of conv1 1, conv2 1, and con3 1 (third row) already exhibits high quality texture representations. Adding one more layer of conv4 1 (forth row) could slightly improve the natural textures. By comparison, results of Gatys et al.[8] on the trained VGG using four convolutional layers up to conv4 1 are as shown at the bottom row.\nOur experiments show that with suitable weighted factors, calculated automatically by our method, ranVGG could synthesize complex natural textures that are almost indistinguishable with the original texture and the synthesized texture on the trained VGG. Trained VGG generates slightly better textures on neatly arranged original textures (cargo at the second column).\nStyle transfer. We select conv2 2 as the content layer, and use the combination of conv1 1, conv2 1, ..., conv5 1 as the style. We set the ratio of \u03b1 : \u03b2 : \u03b3 = 100 : 1 : 1000 in the experiments. We first compare our style transfer results with the prior work of Gatys et al.[13] on several well-known artworks for the style: Night Starry by Vincent van Gohn 1989, Der Schrei by Edward Munch 1893, Picasso by Pablo Picasso 1907, Woman with a Hat by Henri Matisse 1905, Meadow with Poplars by Claude Monet 1875. As shown in Figure 9, the second row, by recasting the content of a university image in the style of the five artworks, we obtain different artistic images based on the untrained ranVGG (second row). Our results are comparable to their work [13] on the pretrained VGG (third row), and are in the same order of magnitude. They have slightly smoother lines and textures which may attributed to the training. We further try the content and style combination on some Chinese paintings and scenery photographs, as shown in Figure 10, and create high perceptual artistic Chinese paintings that well combine the style of the painting and the content of the sceneries."}, {"heading": "5 Discussion", "text": "Our work offers a testable hypothesis about the representation of image appearance based only on the network structure. The success on the untrained, random weight networks on deep visualization raises several fundamental questions in the area of deep learning. Researchers have developed many visualization techniques to understand the representation of well trained deep networks. However,\nif we could do the same or similar visualization using an untrained network, then the understanding is not for the training but for the network architecture. What is the difference of a trained network and a random weight network with the same architecture, and how could we explore the difference? What else could one do using the generative power of untrained, random weight networks? Explore other visualization tasks in computer vision developed on the well-trained network, such as image morphing [22], would be a promising aspect."}, {"heading": "Acknowledgments", "text": "This research work was supported by US Army Research Office(W911NF-14-1-0477), National Science Foundation of China(61472147) and National Science Foundation of Hubei Province(2015CFB566)."}], "references": [{"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "University de Montreal Technical Report", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Understanding deep image representations by inverting them", "author": ["Aravindh Mahendran", "Andrea Vedaldi"], "venue": "In CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Texture synthesis using convolutional neural networks", "author": ["Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "venue": "In Deep Learning Workshop, International Conference on Machine Learning (ICML),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Inverting visual representations with convolutional networks", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": "arXiv preprint arXiv:1506.02753,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "arXiv preprint arXiv:1505.07376,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Exploring the neural algorithm of artistic style", "author": ["Yaroslav Nikulin", "Roman Novak"], "venue": "arXiv preprint arXiv:1602.07188,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Perceptual losses for real-time style transfer and superresolution", "author": ["Justin Johnson", "Alexandre Alahi", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1603.08155,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks", "author": ["Anh Mai Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "arXiv preprint arXiv:1602.03616,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Understanding intra-class knowledge inside CNN", "author": ["Donglai Wei", "Bolei Zhou", "Antonio Torralba", "William T. Freeman"], "venue": "arXiv preprint arXiv:1507.02379,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": "arXiv preprint arXiv:1602.02644,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Texture networks: Feedforward synthesis of textures and stylized images", "author": ["Dmitry Ulyanov", "Vadim Lebedev", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1603.03417,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Deep manifold traversal: Changing labels with convolutional features", "author": ["Jacob R. Gardner", "Paul Upchurch", "Matt J. Kusner", "Yixuan Li", "Kilian Q. Weinberger", "John E. Hopcroft"], "venue": "arXiv preprint arXiv:1511.06421,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].", "startOffset": 187, "endOffset": 199}, {"referenceID": 1, "context": "In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].", "startOffset": 187, "endOffset": 199}, {"referenceID": 2, "context": "In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].", "startOffset": 187, "endOffset": 199}, {"referenceID": 3, "context": "In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].", "startOffset": 187, "endOffset": 199}, {"referenceID": 4, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 5, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 6, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 7, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 8, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 9, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 10, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 6, "context": "[7] use the pretrained CNN AlexNet [2] and define a squared Euclidean loss on the activations to capture the representation differences and reconstruct the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[7] use the pretrained CNN AlexNet [2] and define a squared Euclidean loss on the activations to capture the representation differences and reconstruct the image.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "[8, 12] define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3].", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "[8, 12] define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3].", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "[8, 12] define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 12, "context": "[13] then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups [14, 15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13] then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups [14, 15].", "startOffset": 278, "endOffset": 286}, {"referenceID": 14, "context": "[13] then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups [14, 15].", "startOffset": 278, "endOffset": 286}, {"referenceID": 4, "context": "Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features [5, 9] or the class prediction scores [6].", "startOffset": 173, "endOffset": 179}, {"referenceID": 8, "context": "Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features [5, 9] or the class prediction scores [6].", "startOffset": 173, "endOffset": 179}, {"referenceID": 5, "context": "Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features [5, 9] or the class prediction scores [6].", "startOffset": 211, "endOffset": 214}, {"referenceID": 15, "context": "further try multifaceted visualization to separate and visualize different features that a neuron learns [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 5, "context": "In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc.", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc.", "startOffset": 148, "endOffset": 151}, {"referenceID": 6, "context": "In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc.", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc.", "startOffset": 178, "endOffset": 181}, {"referenceID": 16, "context": "In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc.", "startOffset": 208, "endOffset": 212}, {"referenceID": 9, "context": "The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15].", "startOffset": 133, "endOffset": 141}, {"referenceID": 17, "context": "The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15].", "startOffset": 133, "endOffset": 141}, {"referenceID": 18, "context": "The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15].", "startOffset": 187, "endOffset": 191}, {"referenceID": 19, "context": "Instead of designing a natural prior, some researchers incorporate adversarial training[20] to improve the realism of the generated images[18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "Instead of designing a natural prior, some researchers incorporate adversarial training[20] to improve the realism of the generated images[18].", "startOffset": 138, "endOffset": 142}, {"referenceID": 7, "context": "demonstrated that the VGG architecture with random weights failed in generating textures and resulted in white noise images in an experiment indicating the trained filters might be crucial for texture generation [8], we conjecture the success of deep visualization mainly originates from the intrinsic nonlinearity and complexity of the deep network hierarchical structure rather than from the training, and that the architecture itself may cause the inversion invariant to the original image.", "startOffset": 212, "endOffset": 215}, {"referenceID": 7, "context": "[8] on well-trained VGG.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "VGG-19 [3] is a convolutional neural network trained on the 1.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "3 million-image ILSVRC 2012 ImageNet dataset [1] using the Caffe-framework [21].", "startOffset": 45, "endOffset": 48}, {"referenceID": 20, "context": "3 million-image ILSVRC 2012 ImageNet dataset [1] using the Caffe-framework [21].", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "re-train the VGG-19 network using average pooling instead of maximum pooling, which they suggest could improve the gradient flow and obtain slightly better results [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 6, "context": "Our methods are similar in spirit to existing methods [7, 8, 13].", "startOffset": 54, "endOffset": 64}, {"referenceID": 7, "context": "Our methods are similar in spirit to existing methods [7, 8, 13].", "startOffset": 54, "endOffset": 64}, {"referenceID": 12, "context": "Our methods are similar in spirit to existing methods [7, 8, 13].", "startOffset": 54, "endOffset": 64}, {"referenceID": 7, "context": "[8] and use the correlations between feature responses on each layer as the texture representation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The derivative of E(x, x0, l) with respect to the activations F l in layer l is [8]: \u2202E(x, x0, l) \u2202F l i,k = 1 N2 l M 2 l {(F (x)) [G(F (x))\u2212G(F (x0))]}i,k (7)", "startOffset": 80, "endOffset": 83}, {"referenceID": 12, "context": "[13] from the feature responses of VGG trained on ImageNet, we use an untrained VGG and succeed in separating and recombining content and style of arbitrary images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "We apply a regularizer R(x), total variation(TV) [7] defined as the squared sum on the adjacent pixel\u2019s difference of x, to encourage the spatial smoothness in the output image.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "We select several source images from the ILSVRC 2012 challenge [1] validation data as examples for the inversion task, and choose a monkey image as the reference image to build the stacked ranVGG3.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "[7], we only consider the Euclidean loss over the activations and ignore the regularizer they used to capture the natural image prior.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "use a reference network AlexNet [2] which contains 8 layers of trained weights (5 convolutional layers and 3 fully connected layers), plus 3 pooling layers.", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "Figure 3: Reconstructions from layers of ranVGG (top) and the pretrained VGG (middle) and [7] (bottom).", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "[8] on the trained VGG using four convolutional layers up to conv4 1 are as shown at the bottom row.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] on several well-known artworks for the style: Night Starry by Vincent van Gohn 1989, Der Schrei by Edward Munch 1893, Picasso by Pablo Picasso 1907, Woman with a Hat by Henri Matisse 1905, Meadow with Poplars by Claude Monet 1875.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Our results are comparable to their work [13] on the pretrained VGG (third row), and are in the same order of magnitude.", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "What is the difference of a trained network and a random weight network with the same architecture, and how could we explore the difference? What else could one do using the generative power of untrained, random weight networks? Explore other visualization tasks in computer vision developed on the well-trained network, such as image morphing [22], would be a promising aspect.", "startOffset": 344, "endOffset": 348}, {"referenceID": 0, "context": "[1 3] on V G G Night Starry Der Schrei Photograph Picasso Woman with a Hat Meadow with Poplars", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[1 3] on V G G Night Starry Der Schrei Photograph Picasso Woman with a Hat Meadow with Poplars", "startOffset": 0, "endOffset": 5}, {"referenceID": 7, "context": "[8] on the pretrained VGG (bottom row).", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization.", "creator": "LaTeX with hyperref package"}}}