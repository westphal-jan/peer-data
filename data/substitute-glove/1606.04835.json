{"id": "1606.04835", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Learning Word Sense Embeddings from Word Sense Definitions", "abstract": "Word dipeptide play after significant life in that art NLP automated. However, already applied given embedding well mathematical learn as representation per is which known unavoidable they polysemous words now resen nonsense. To agenda this problem, we accept another multi - starting word passion embedding reinstallation method which interface a representations dualistic made them one embedding 1.6 idea sense. We use read way non-standard and relations between word torment acceptable saw though lexical distinguishes in same are way from enable software. Experimental round on any connotations coordinating coming saying our approach remarkablely stronger both quality of embeddings.", "histories": [["v1", "Wed, 15 Jun 2016 16:14:09 GMT  (14kb)", "http://arxiv.org/abs/1606.04835v1", "Submitted to COLING 2016"], ["v2", "Mon, 20 Jun 2016 14:59:47 GMT  (21kb)", "http://arxiv.org/abs/1606.04835v2", "Submitted to COLING 2016"], ["v3", "Mon, 18 Jul 2016 13:03:12 GMT  (34kb)", "http://arxiv.org/abs/1606.04835v3", "Submitted to COLING 2016"], ["v4", "Mon, 24 Oct 2016 00:56:54 GMT  (41kb)", "http://arxiv.org/abs/1606.04835v4", "To appear at NLPCC-ICCPOL 2016"]], "COMMENTS": "Submitted to COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qi li", "tianshi li", "baobao chang"], "accepted": false, "id": "1606.04835"}, "pdf": {"name": "1606.04835.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Qi Li", "Tianshi Li", "Baobao Chang"], "emails": ["qi.li@pku.edu.cn", "417@hotmail.com", "chbb@pku.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n04 83\n5v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\n16"}, {"heading": "1 Introduction", "text": "Learning distributed word representations (i.e. word embeddings) which contain syntax and semantic information from large scale unannotated corpus is a fundamental part for most state-of-the-art natural language processing (NLP) neural network models. Owing to the development of internet and computation efficiency, gigantic unannotated corpus can be obtained and processed to learn word embeddings with high quality. Most effective unsupervised models use context information to learn word embeddings. The word embeddings capture syntactic and semantic properties which can be exposed directly in tasks such as analogical reasoning (Mikolov et al., 2013), word similarity (Huang et al., 2012) etc. The most prevalent word embedding learning models are Skip-gram (Mikolov et al., 2013) and Glove (Pennington et al., 2014) and the variants of them.\nBasic Skip-gram (Mikolov et al., 2013) and Glove (Pennington et al., 2014) model output one vector for a word. However, as can be seen from Table 1, polysemous words or homonymous words may have largely different meanings and each of them should have inherently different representations. So using one embedding for a polysemous word or a homonymous word regardless of the specific word sense is inappropriate. To address this shortage, researchers propose to learn one representation per word sense. To this end, the first problem to be addressed is to determine the number of senses for a word. Some models use statistic-based methods to determine the number of senses for a word (e.g. Chinese Restaurant Processes (CRP))(Li and Jurafsky, 2015) or use fixed number of senses for all words (Suster et al., 2016). The number of senses for words determined in this way is imprecise which may lead to inferior results. Given the number of word senses, the second problem is how to train embeddings for word senses. Many work applies word sense induction model on the unannotated corpus to determine the sense of a word in a context and then train the embeddings of word senses on it (Chen et al., 2014). The error caused by word sense induction model will have a bad impact on the quality of the output embeddings and this process is time-consuming. Some other researchers try to use sense relations defined in a lexical ontology to learn word senses(Speer and Chin, 2016; Faruqui et al., 2015) and they most create a Markov Network (MN). These methods use lexical ontology which is the same as our method but they tend to ignore the definitions of word senses.\nDefinitions of word senses show the essential information of them. Word or sense embeddings also contain information of them in the mathematical form. It is reasonable to get the word sense embeddings from the definitions. Relations of words also provide semantic information of words and thus can also be of help to produce better word sense embeddings. Based on these intuitions, we propose a multi-phase\nword sense embedding learning model address the problems above. We initialize word and sense embeddings as the corresponding word embeddings learnt with Skip-gram, Glove or any unsupervised word embedding learning model on large scale unannotated corpus. Then we use word sense definitions in the lexical ontology to determine the number of senses for words and then to train the sense embeddings. Additionally, we use word sense relations defined in the lexical ontology in a better way than others to retrofit word sense embeddings. Experimental results show that both the definitions of word senses and word sense relations help to produce high quality sense embeddings.\nThe rest of this paper is organized as follows: Section 2 gives the details of our model. Section 3 reports the experimental results of our approach. Section 4 introduces the related work. Conclusions are given in section 5."}, {"heading": "2 Methodology", "text": "A large scale unannotated corpus and a lexical ontology are needed for our model. They are complementary to each other. The corpus presents abundant word co-occurences, but it doesn\u2019t explicitly provide word sense information and semantic relations between words. As a consequence, some words with totally opposite meanings may turn out to be similar in representation (e.g., \u2019good\u2019 and \u2019bad\u2019) if they are trained only with the corpus. A lexical ontology provides definitions for every word senses and their relations, so we can utilize those information to further distinguish those words with similar syntactic function but different exact meanings. Widely used lexical ontologies include WordNet (Miller, 1992), FrameNet(Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013) etc. We choose to use WordNet in this paper but the method we propose can be applied to other lexical ontologies. Our model utilizes the corpus and the lexical ontology in different training phases to initialize and retrofit sense embeddings. The phases can be summarized as follows:\n\u2022 Initialize word embeddings and sense embeddings with Skip-gram or Glove model.\n\u2022 Train a RNN-based definition understanding model to map the definitions to sense embeddings of monosemes.\n\u2022 Jointly learn sense embeddings and the parameters of RNN-based definition undestanding model.\n\u2022 Train mapping matrices for each relation defined on the lexical ontology.\n\u2022 Jointly train all parameters and sense embeddings."}, {"heading": "2.1 Embedding Initialization", "text": "Firstly, we initialize our word embedding with Skip-gram or Glove on the unannotated corpus. We get the number of word senses with the WordNet (Miller, 1992) and we initialize the embeddings of all the senses to be their word embeddings."}, {"heading": "2.2 Training RNN for Definition Understanding", "text": "Intuitively, since the definition of a word sense is an exact description of it, the representation should be able to get from the definition. Since recurrent Neural Network (RNN) is one of the most successful\nneural network for natural language understanding, we use RNN to compute the representation of a word sense from the definition. The output at the last unit hn of the RNN model is assumed to contain the whole information of a sentence. The specific RNN model can be a standard RNN model, Gated Recurrent Unit (GRU) (Chung et al., 2014) or Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997). Comparing with standard RNN, LSTM and GRU can hold long-term information, so they can alleviate the gradient vanishing and information-forgetting problem associated with the standard RNN for long sequences(Hochreiter and Schmidhuber, 1997; Chung et al., 2014). The differences between them can be shown in the computation unit.\nThe computation unit of standard RNN is:\nht = g(Whht\u22121 +Wxxt + b) (1)\n(2)\nwhere g is the non-linear activation function (e.g., simoid, tanh, Relu). The computation unit of GRU is:\nzt = \u03c3(Wzht\u22121 + Uzxt) (3)\nrt = \u03c3(Wrht\u22121 + Urxt) (4)\nh\u0303t = tanh(Wh(rt \u2299 ht\u22121) + Uhxt) (5)\nht = (1\u2212 z)\u2299 h\u0303t + z \u2299 ht\u22121 (6)\nwhere \u03c3 is sigmoid function and \u2299 denotes element-wise product. The computation unit of LSTM is:\nit = \u03c3(Wi[ht\u22121;xt] + bi) (7)\nft = \u03c3(Wf [ht\u22121;xt] + bf ) (8)\nC\u0303t = tanh(WC [ht\u22121;xt] + bC) (9)\nCt = ft \u2299 Ct\u22121 + it \u2299 C\u0303t (10)\not = \u03c3(Wo[ht\u22121;xt] + bo) (11)\nht = ot \u2299 tanh(Ct) (12)\nWe tried all there RNN models and compare their results. Given the models to understand definitions, the first problem is how to train the RNN model. According to our statistics, nearly half of the words we considered are monoseme and their sense embedding should be the same as their word embeddings. So we use these embeddings to train our RNN model in this phase. The input to the RNN model is a word sense definition which can be seen as a word sequence: {x1, x2, ..., xn}. The output of RNN hn is then mapped into the embedding space with a transformation matrix:\n\u02dcews = Whn + b (13)\nwhere hn is the output of RNN at the last word of the definition and e\u0303w is the sense embedding computed by the model.\nThe objective function of this training phase is\nJ1 = \u2212 \u2211\nw\u2208Vmono\ncos(ews, \u02dcews) (14)\nwhere Vmono is the monosemes set and ews is the initialized sense embedding (the same as word embedding for monoseme). All the embeddings are fixed in this phase."}, {"heading": "2.3 Sense Embedding Learning", "text": "Different from the first phase, in this phase, we train all the sense embeddings and the models parameters together. The sense embeddings are updated during the whole process. The objective function is:\nJ2 = \u2212 \u2211\nw\u2208V\n\u2211\ns\u2208Sw\ncos(ews, \u02dcews) (15)\nwhere V is the word set including all the words and Sw is the sense set of word w."}, {"heading": "2.4 Training Relation Mapping Matrices", "text": "Up to this phase, sense embeddings are learnt from their definitions, but the relations of word senses are still not utilized. Word senses are connected in lexical ontology with relations such as hypernymy, hyponymy and antonymy. These relations between word senses should be concerned with their embeddings. Therefore, for each relation defined in the ontology we define a mapping matrix. We train the matrix to map word senses to the senses with the respective relations. The objective function is:\nJ3 = \u2211\nr\u2208Vr\n\u2211\n(ws1,ws2)=r\n||Wrews1 \u2212 ews2 || (16)\nwhere Vr is the set of all relations, (ws1, ws2) is the relation of word sense 1 and word sense 2 and Wr is the mapping matrix for relation r. We only update all the Wr matrices in this phase."}, {"heading": "2.5 Jointly Training All Parameters and Sense Embeddings", "text": "All the parameters have been mentioned and trained respectively so far. We train them all together in the last phase. The objective function is:\nJ4 = \u03b1J2 + (1\u2212 \u03b1)J3 (17)\nwhere J2 is from equation 15, J3 is from equation 16 and \u03b1 is a hyper-parameter to control the weights of the two training part. We set \u03b1 to be 0.7. We update all sense embeddings and parameters. We optimize this objective function by training J2 and J3 in turn during each epoch.\nThose two phases we keep the sense embeddings and update only model parameters can be seen as initialization of model parameters. If we jointly train them with sense embeddings just after we randomly initialize the parameters, these random information would have negative effects on sense embeddings."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Setup", "text": "Since quality of trained word embeddings normally increases with scale of the corpus, we try two publicly released word vectors trained on the largest corpuses as initialization of our word embedding. One is the 300 dimentional word vectors trained with Glove on a corpus of 840B 1, the other is pre-trained 300dimensional vectors trained with word2vec2 (Skip-gram) on part of Google News dataset (about 100 billion words). We also take these two word embeddings as our baselines."}, {"heading": "3.2 Word Similarity Evaluation", "text": "Since the output of our method is the sense embedding, we need the context to determine the sense when we apply the trained sense embeddings. So evaluation on word similarity datasets which gives word pairs without context does not allow us to do the word sense induction and thus cannot reveal the effectiveness of our method. Stanford\u2019s Contextual Word Similarities (SCWS) (Huang et al., 2012) is a data set which gives the contexts of the target words. So we use the context to determine the word sense compute cosine similarity of the two sense embedding. The evaluation metrics is the Spearman\u2019s rank\n1http://nlp.stanford.edu/data/glove.840B.300d.zip 2https://code.google.com/archive/p/word2vec/\ncorrelation coefficient between the average human rating and the cosine similarity scores given by our method. The result is shown in Table 2.\nAs can be seen that the word embeddings of Skip-gram shows better performance than Glove on this task although the embeddings trained with Glove uses a much larger corpus. Our approach improves the quality of embeddings for both Skip-gram and Glove embeddings. Among the models to understanding definitions, GRU shows the best performance. It is still to be explored why LSTM and standard RNN show lower performance. As we continue to apply relation-based retrofitting on the embeddings retrofitted using definitions, the quality of sense embeddings get further improvement. As a whole, most improvements come from the understanding of word sense definitions from WordNet.\nWe also compare with some other systems on this task which are shown in the first our lines of Table 2. Since the corpuses used by these methods are in different scale and the lexical ontology they use are different, it is not quite appropriate to compare the performances between them."}, {"heading": "4 Related Work", "text": "While traditional methods use pointwise mutual information (PMI) matrices to learn word embeddings, prevalent work (e.g., Skip-gram and Glove) uses context information to learn word embeddings. Levy et al.(2015) compare the differences and relations of Skip-gram, Glove and PMI-based methods and conclude that it is the system design choices and hyper-parameter optimizations lead to performance gain rather than the embedding algorithms.\nSince multi-sense embedding are suggested to be better than single embedding for a word, many approaches have been proposed. Researchers tend to extend Skip-gram and Glove models to learn sense embedding with word sense induction (WSI) as a preliminary step. Most work uses only a corpus to determine the number of word senses for WSI which would cause error inevitably. Huang et al. (2012) determine a sense of a word by clustering the contexts and then apply it to neural language model with global context. Li et al. (2014) use Chinese Restaurant Processes to determine the sense of word and learning embedding jointly. Chen et al.(2014) use WordNet as its lexical ontology to determine word sense numbers and use the average word embedding for words in definition above a similarity threshold as the initialization of sense embeddings. And then they do WSI on corpus to train sense\nembeddings with Skip-gram. Their way to use the definition of word senses is too simple and their method inevitably consumes much more time on WSI and training sense embedding on corpus than our method. Besides, they doesn\u2019t use relations defined in WordNet to retrofit embeddings. As far as we know, no system use both sense definition and relation to retrofit sense embeddings. Moreover, the way we use relation and definiton is different from all the systems proposed. RNN-based model is much more preferable to understand sentence than just taking average of its words\u2019 embeddings for the latter ignores the word order and can\u2019t model the semantic compositionality of sentence. Other proposed relation-based retrofitting way is to get high similarity for related word embeddings in an ontology(Faruqui et al., 2015; Bollegala et al., 2016). But related words should be related in a principled way according to the relation. So our mapping matrices for relations are more appropriate intuitively."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a multi-phase word sense embedding retrofitting model. Our proposed method utilizes a lexical ontology to retrofit the embeddings learnt from a large scale unannotated corpus in a different way from existing systems. Experimental results on SCWS show that our proposed method successfully retrofit the embeddings learnt with Skip-gram and Glove."}], "references": [{"title": "The berkeley framenet project", "author": ["Charles J. Fillmore", "John B. Lowe"], "venue": null, "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Joint word representation learning using a corpus and a semantic lexicon. CoRR, abs/1511.06438", "author": ["Mohammed Alsuhaibani", "Takanori Maehara", "Ken ichi Kawarabayashi"], "venue": null, "citeRegEx": "Bollegala et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bollegala et al\\.", "year": 2016}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555", "author": ["Chung et al.2014] Junyoung Chung", "aglar G\u00fclehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy", "Noah A. Smith"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Ppdb: The paraphrase database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Improving word representations via global context and multiple word prototypes. In Annual Meeting of the Association for Computational Linguistics (ACL)", "author": ["Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Li", "Jurafsky2015] Jiwei Li", "Daniel Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller"], "venue": "Commun. ACM,", "citeRegEx": "Miller.,? \\Q1992\\E", "shortCiteRegEx": "Miller.", "year": 1992}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "An ensemble method to produce high-quality word embeddings. CoRR, abs/1604.01692", "author": ["Speer", "Chin2016] Robert Speer", "Joshua Chin"], "venue": null, "citeRegEx": "Speer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Speer et al\\.", "year": 2016}, {"title": "Bilingual learning of multi-sense embeddings with discrete autoencoders. CoRR, abs/1603.09128", "author": ["Suster et al.2016] Simon Suster", "Ivan Titov", "Gertjan van Noord"], "venue": null, "citeRegEx": "Suster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Suster et al\\.", "year": 2016}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Tian et al.2014] Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu"], "venue": "In COLING", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "The word embeddings capture syntactic and semantic properties which can be exposed directly in tasks such as analogical reasoning (Mikolov et al., 2013), word similarity (Huang et al.", "startOffset": 130, "endOffset": 152}, {"referenceID": 7, "context": ", 2013), word similarity (Huang et al., 2012) etc.", "startOffset": 25, "endOffset": 45}, {"referenceID": 10, "context": "The most prevalent word embedding learning models are Skip-gram (Mikolov et al., 2013) and Glove (Pennington et al.", "startOffset": 64, "endOffset": 86}, {"referenceID": 12, "context": ", 2013) and Glove (Pennington et al., 2014) and the variants of them.", "startOffset": 18, "endOffset": 43}, {"referenceID": 10, "context": "Basic Skip-gram (Mikolov et al., 2013) and Glove (Pennington et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 12, "context": ", 2013) and Glove (Pennington et al., 2014) model output one vector for a word.", "startOffset": 18, "endOffset": 43}, {"referenceID": 14, "context": "Chinese Restaurant Processes (CRP))(Li and Jurafsky, 2015) or use fixed number of senses for all words (Suster et al., 2016).", "startOffset": 103, "endOffset": 124}, {"referenceID": 2, "context": "Many work applies word sense induction model on the unannotated corpus to determine the sense of a word in a context and then train the embeddings of word senses on it (Chen et al., 2014).", "startOffset": 168, "endOffset": 187}, {"referenceID": 4, "context": "Some other researchers try to use sense relations defined in a lexical ontology to learn word senses(Speer and Chin, 2016; Faruqui et al., 2015) and they most create a Markov Network (MN).", "startOffset": 100, "endOffset": 144}, {"referenceID": 11, "context": "Widely used lexical ontologies include WordNet (Miller, 1992), FrameNet(Baker et al.", "startOffset": 47, "endOffset": 61}, {"referenceID": 0, "context": "Widely used lexical ontologies include WordNet (Miller, 1992), FrameNet(Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al.", "startOffset": 71, "endOffset": 91}, {"referenceID": 5, "context": ", 1998) and the Paraphrase Database (Ganitkevitch et al., 2013) etc.", "startOffset": 36, "endOffset": 63}, {"referenceID": 11, "context": "We get the number of word senses with the WordNet (Miller, 1992) and we initialize the embeddings of all the senses to be their word embeddings.", "startOffset": 50, "endOffset": 64}, {"referenceID": 3, "context": "The specific RNN model can be a standard RNN model, Gated Recurrent Unit (GRU) (Chung et al., 2014) or Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 79, "endOffset": 99}, {"referenceID": 3, "context": "Comparing with standard RNN, LSTM and GRU can hold long-term information, so they can alleviate the gradient vanishing and information-forgetting problem associated with the standard RNN for long sequences(Hochreiter and Schmidhuber, 1997; Chung et al., 2014).", "startOffset": 205, "endOffset": 259}, {"referenceID": 7, "context": "Stanford\u2019s Contextual Word Similarities (SCWS) (Huang et al., 2012) is a data set which gives the contexts of the target words.", "startOffset": 47, "endOffset": 67}, {"referenceID": 6, "context": "System Spearman Huang et al. (2012) (0.", "startOffset": 16, "endOffset": 36}, {"referenceID": 6, "context": "System Spearman Huang et al. (2012) (0.99B) 65.7 Tian et al. (2014) (0.", "startOffset": 16, "endOffset": 68}, {"referenceID": 2, "context": "4 Chen et al. (2014) (1B) 68.", "startOffset": 2, "endOffset": 21}, {"referenceID": 2, "context": "4 Chen et al. (2014) (1B) 68.9 Li et al. (2014) (1.", "startOffset": 2, "endOffset": 48}, {"referenceID": 2, "context": "4 Chen et al. (2014) (1B) 68.9 Li et al. (2014) (1.1B) 67.0 Li et al. (2014) (120B) 69.", "startOffset": 2, "endOffset": 77}, {"referenceID": 6, "context": "Levy et al.(2015) compare the differences and relations of Skip-gram, Glove and PMI-based methods and conclude that it is the system design choices and hyper-parameter optimizations lead to performance gain rather than the embedding algorithms.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Huang et al. (2012) determine a sense of a word by clustering the contexts and then apply it to neural language model with global context.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Huang et al. (2012) determine a sense of a word by clustering the contexts and then apply it to neural language model with global context. Li et al. (2014) use Chinese Restaurant Processes to determine the sense of word and learning embedding jointly.", "startOffset": 0, "endOffset": 156}, {"referenceID": 2, "context": "Chen et al.(2014) use WordNet as its lexical ontology to determine word sense numbers and use the average word embedding for words in definition above a similarity threshold as the initialization of sense embeddings.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Other proposed relation-based retrofitting way is to get high similarity for related word embeddings in an ontology(Faruqui et al., 2015; Bollegala et al., 2016).", "startOffset": 115, "endOffset": 161}, {"referenceID": 1, "context": "Other proposed relation-based retrofitting way is to get high similarity for related word embeddings in an ontology(Faruqui et al., 2015; Bollegala et al., 2016).", "startOffset": 115, "endOffset": 161}], "year": 2017, "abstractText": "Word embeddings play a significant role in many modern NLP systems. However, most used word embedding learning methods learn one representation per word which is problematic for polysemous words and homonymous words. To address this problem, we propose a multi-phase word sense embedding retrofitting method which utilizes a lexical ontology to learn one embedding per word sense. We use word sense definitions and relations between word senses defined in a lexical ontology in a different way from existing systems. Experimental results on word similarity task show that our approach remarkablely improves the quality of embeddings.", "creator": "LaTeX with hyperref package"}}}