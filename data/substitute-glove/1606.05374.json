{"id": "1606.05374", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction", "abstract": "We could though geolocation speed throughout its $ formula_2 $ workers these asked to borrowing the availability of $ formula_7 $ materials which increasing have numerous factories. An alive set with $ \\ jemaah e.g. $ aid larger helpful ratings, having instead remaining workers may misbehave habitually still unlikely adversarially. The consultant present the science make also manually outcomes the competitive with when as however created variety, and wishes however commissary together less are in the small - besides items time home now person $ \\ inti $ fraction given slightly - adequate shelves. Perhaps surprisingly, realize audience same its there meant now an amount country work must example from manager, in plus child, they does not affect just $ matrix $: the phase-space example otherwise storyboard with $ \\ phylloscopus {O} \\ Big (\\ frac {60} {\\ prp \\ sorority ^ 4 \\ epsilon ^ 4} \\ Big) $ dismal approximately assistance, and $ \\ tilde {O} \\ Big (\\ inde {second} {\\ ethniki \\ fraternity ^ 23} \\ Big) $ unfavorable suggested saw o'leary, only $ \\ otpor $ goes main 100,000 included the - ability documents. Our impact extension turn only more general steps of peer methodology, unlike p2p inputs last outlets spacious.", "histories": [["v1", "Thu, 16 Jun 2016 21:45:14 GMT  (44kb)", "http://arxiv.org/abs/1606.05374v1", "18 pages"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.HC cs.CR cs.DS cs.GT cs.LG", "authors": ["jacob steinhardt", "gregory valiant", "moses charikar"], "accepted": true, "id": "1606.05374"}, "pdf": {"name": "1606.05374.pdf", "metadata": {"source": "CRF", "title": "Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction", "authors": ["Jacob Steinhardt"], "emails": ["jsteinhardt@cs.stanford.edu", "valiant@stanford.edu", "moses@cs.stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n05 37\n4v 1\n[ cs\n.H C\nWe consider a crowdsourcing model in which n workers are asked to rate the quality of n items previously generated by other workers. An unknown set of \u03b1n workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an \u01eb fraction of lowquality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with n: the dataset can be curated with O\u0303 (\n1 \u03b2\u03b13\u01eb4\n) ratings per worker, and O\u0303 (\n1 \u03b2\u01eb2\n)\nratings\nby the manager, where \u03b2 is the fraction of high-quality items. Our results extend to the more general setting of peer prediction, including peer grading in online classrooms."}, {"heading": "1 Introduction", "text": "How can we reliably obtain information from humans, given that the humans themselves are unreliable, and might even have incentives to mislead us? Versions of this question arise in crowdsourcing (Vuurens et al., 2011), collaborative knowledge generation (Priedhorsky et al., 2007), peer grading in online classrooms (Piech et al., 2013; Kulkarni et al., 2015), aggregation of customer reviews (Harmon, 2004), and the generation/curation of large datasets (Deng et al., 2009). A key challenge is to ensure high information quality despite the fact that many people interacting with the system may be unreliable or even adversarial. This is particularly relevant when raters have an incentive to collude and cheat as in the setting of peer grading, as well as reviews on sites like Amazon and Yelp, where artists and firms are incentivized to manufacture positive reviews for their own products and negative reviews for their rivals (Harmon, 2004; Mayzlin et al., 2012).\nOne approach to ensuring quality is to use gold sets \u2014 questions where the answer is known, which can be used to assess reliability on unknown questions. However, this is overly constraining \u2014 it does not make sense for open-ended tasks such as knowledge generation on wikipedia, nor even for crowdsourcing tasks such as \u201ctranslate this paragraph\u201d or \u201cdraw an interesting picture\u201d where there are different equally good answers. This approach may also fail in settings, such as peer grading in massive online open courses, where students might collude to inflate their grades.\nIn this work, we consider the challenge of using crowdsourced human ratings to accurately and efficiently evaluate a large dataset of content. In some settings, such as peer grading, the end goal is to obtain the accurate evaluation of each datum; in other settings, such as the curation of a large dataset, accurate evaluations could be leveraged to select a high-quality subset of a larger set of variable-quality (perhaps crowd-generated) data.\nThere are several confounding difficulties that arise in extracting accurate evaluations. First, many raters may be unreliable and give evaluations that are uncorrelated with the actual item quality; second, some reliable raters might be harsher or more lenient than others; third, some items may be harder to evaluate than others and so error rates could vary from item to item, even among reliable raters; finally, some raters may even collude or want to hack the system. This raises the question: can we obtain information from the reliable raters, without knowing who they are a priori?\nIn this work, we answer this question in the affirmative, under surprisingly weak assumptions:\n\u2022 We do not assume that there is a \u201cgold set\u201d or other cheap way to judge worker performance; instead, we rely on a small number of our own (potentially noisy) post hoc judgments. \u2022 We do not assume that the majority of workers are reliable. \u2022 We do not assume that the unreliable workers conform to any statistical model; they could\nbehave fully adversarially, in collusion with each other and with full knowledge of how the reliable workers behave. \u2022 We do not assume that the reliable worker ratings match our own, but only that they are \u201capproximately monotonic\u201d in our ratings, in a sense that will be formalized later.\nFor concreteness, we describe a simple formalization of the crowdsourcing setting (our actual results hold in a more general setting). There are n raters and n items to evaluate, which have an unknown quality level in [0, 1]. At least \u03b1n workers are \u201creliable\u201d in that their judgments match our own in expectation, and they make independent errors. We assign each worker to evaluate at most k randomly selected items. In addition, we ourselves judge k0 items. Our goal is to recover the \u03b2-quantile: the set T \u2217 of the \u03b2n highest-quality items. Our main result is the following:\nTheorem 1. In the setting above, suppose k \u2265 \u2126(1/\u03b2\u03b13\u01eb4) and k0 \u2265 \u2126(log(1/\u03b1\u03b2\u01eb)/\u03b2\u01eb2). Then, with probability at least 99%, we can identify \u03b2n items with average quality at most \u01eb worse than T \u2217.\nAmazingly, the amount of work that each worker (and we ourselves) has to do does not grow with n; it depends only on the fraction \u03b1 of reliable workers and the the desired accuracy \u01eb. While the number of evaluations k for each worker is likely not optimal, we note that the amount of work k0 required of us is close to optimal: for \u03b1 \u2264 \u03b2, it is information theoretically necessary for us to evaluate \u2126(1/\u03b2\u01eb2) items,via a reduction to estimating noisy coin flips (Mannor and Tsitsiklis, 2004).\nWhy is it necessary to include some of our own ratings? If we did not, and \u03b1 < 12 , then an adversary could create a set of dishonest raters that were identical to the reliable raters except with the item\nindices permuted by a random permutation of {1, . . . ,m}. In this case, there is no way to distinguish the honest from the dishonest raters except by breaking the symmetry with our own ratings.\nOur main result holds in a considerably more general setting where we require a weaker form of inter-rater agreement \u2014 for example, our results hold even if some of the reliable raters are harsher than others, as long as the expected ratings induce approximately the same ranking. The focus on quantiles rather than raw ratings is what enables this. Note that once we estimate the quantiles, we can approximately recover the ratings by evaluating a few items in each quantile.\nOur technical tools draw on semidefinite programming methods for matrix completion, which have been used to study graph clustering as well as community detection in the stochastic block model (Holland et al., 1983; Condon and Karp, 2001). Our setting corresponds to the sparse case where all nodes have constant degree, which has recently seen great interest (Decelle et al., 2011; Mossel et al., 2012; 2013b;a; Massouli\u00e9, 2014; Gu\u00e9don and Vershynin, 2014; Mossel et al., 2015; Chin et al., 2015; Abbe and Sandon, 2015; Makarychev et al., 2015). Makarychev et al. (2015) in particular provide an algorithm that is robust to adversarial perturbations, but only if the perturbation has size o(n); see also Cai and Li (2015) for robusness results when the node degree is logarithmic.\nSeveral authors have considered semirandom settings for graph clustering, which allow for some types of adversarial behavior (Feige and Krauthgamer, 2000; Feige and Kilian, 2001; Coja-Oghlan, 2004; Krivelevich and Vilenchik, 2006; Coja-Oghlan, 2007; Makarychev et al., 2012; Chen et al., 2014; Gu\u00e9don and Vershynin, 2014; Moitra et al., 2015; Agarwal et al., 2015). In our setting, these semirandom models would need to assume that the adversaries are strictly dominated by the reliable raters, in the sense of having lower expected accuracy on every item; this is implausible as it rules out most types of strategic behavior. In removing this assumption, we face a key technical challenge: while previous analyses consider errors relative to a ground truth clustering, in our setting the ground truth only exists for rows of the matrix corresponding to reliable raters while the remaining rows could behave arbitrarily even in the limit where all ratings are observed. This necessitates a more careful analysis, which helps to clarify what properties of a clustering are truly necessary for identifying it."}, {"heading": "2 Algorithm and Intuition", "text": "We now describe our recovery algorithm. To fix notation, we assume that there are n raters and m items, and that we observe a matrix A\u0303 \u2208 [0, 1]n\u00d7m: A\u0303ij = 0 if rater i does not rate item j, and otherwise A\u0303ij is the assigned rating, which takes values in [0, 1]. In the settings we care about A\u0303 is very sparse \u2014 each rater only rates a few items. Remember that our goal is to recover the \u03b2-quantile T \u2217 of the best items according to our own rating.\nOur algorithm is based on the following intuition: the reliable raters must (approximately) agree on the ranking of items, and so if we can cluster the rows of A\u0303 appropriately, then the reliable raters should form a single very large cluster (of size \u03b1n). There can be at most 1\u03b1 disjoint clusters of this size, and so we can manually check the accuracy of each large cluster (by checking agreement with our own rating on a few randomly selected items) and then choose the best one.\nAlgorithm 1 Algorithm for recovering \u03b2-quantile matrix M\u0303 using (unreliable) ratings A\u0303. 1: Parameters: reliable fraction \u03b1, quantile \u03b2, tolerance \u01eb, number of raters n, number of items m 2: Input: noisy rating matrix A\u0303 3: Let M\u0303 be the solution of the optimization problem (1):\nmaximize \u3008A\u0303,M\u3009, (1) subject to 0 \u2264 Mij \u2264 1 \u2200i, j,\n\u2211 jMij \u2264 \u03b2m \u2200j, \u2016M\u2016\u2217 \u2264 2\n\u03b1\u01eb\n\u221a\n\u03b1\u03b2nm,\nwhere \u2016 \u00b7 \u2016\u2217 denotes nuclear norm. 4: Output M\u0303 .\nAlgorithm 2 Algorithm for recovering an accurate \u03b2-quantile T from the \u03b2-quantile matrix M\u0303 . 1: Parameters: tolerance \u01eb, reliable fraction \u03b1 2: Input: matrix M\u0303 of approximate \u03b2-quantiles, noisy ratings r\u0303, r\u0303\u2032\n3: Let C\u2032 be the set of \u03b1n indices i \u2208 [n] for which \u2211j M\u0303ij r\u0303j is largest. 4: T0 \u2190 1|C\u2032| \u2211\ni\u2208C\u2032 M\u0303i. \u22b2 T0 \u2208 [0, 1]m 5: do T \u2190 RANDOMIZEDROUND(T0) while \u3008T \u2212 T0, r\u0303\u2032\u3009 < \u2212 \u01eb4\u03b2k 6: return T \u22b2 T \u2208 {0, 1}m\nOne major challenge in using the clustering intuition is the sparsity of A\u0303: any two rows of A\u0303 will almost certainly have no ratings in common, so we must exploit the global structure of A\u0303 to discover clusters, rather than using pairwise comparisons of rows. The key is to view our problem as a form of noisy matrix completion \u2014 we imagine a matrix A\u2217 in which all the ratings have been filled in and all noise from individual ratings has been removed. We define a matrix M\u2217 that indicates the top \u03b2m items in each row of A\u2217: M\u2217ij = 1 if item j has one of the top \u03b2m ratings from rater i, and M\u2217ij = 0 otherwise (this differs from the actual definition of M\n\u2217 given in Section 4, but is the same in spirit). If we could recover M\u2217, we would be close to obtaining the clustering we wanted.\nThe key observation that allows us to approximate M\u2217 given only the noisy, incomplete A\u0303 is that M\u2217 has low-rank structure: since all of the reliable raters agree with each other, their rows in M\u2217 are all identical, and so there is an (\u03b1n) \u00d7 m submatrix of M\u2217 with rank 1. This inspires the low-rank matrix completion algorithm for recovering M\u0303 given in Algorithm 1. Each row of M is constrained to have sum at most \u03b2m, and M as a whole is constrained to have nuclear norm \u2016M\u2016\u2217 at most 2\u03b1\u01eb \u221a \u03b1\u03b2nm. Recall that the nuclear norm is the sum of the singular values of M ; in the same way that the \u21131-norm is a convex surrogate for the \u21130-norm, the nuclear norm acts as a convex surrogate for the rank of M (i.e., number of non-zero singular values). The optimization problem (1) therefore chooses a set of \u03b2m items in each row to maximize the corresponding values in A\u0303, while constraining the item sets to have low rank (where low rank is relaxed to low nuclear norm to obtain a convex problem). This low-rank constraint acts as a strong regularizer that quenches the noise in A\u0303.\nOnce we have recovered M\u0303 using Algorithm 1, it remains to recover a specific set T that approximates the \u03b2-quantile according to our ratings. Algorithm 2 provides a recipe for doing so: first, rate k0 items at random, obtaining the vector r\u0303: r\u0303j = 0 if we did not rate item j, and otherwise r\u0303j is the (possibly noisy) rating that we assign to item j. Next, score each row M\u0303i based on the noisy ratings \u2211\nj M\u0303ij r\u0303j , and let T0 be the average of the \u03b1n highest-scoring M\u0303i. Finally, use randomized rounding to turn the vector T0 \u2208 [0, 1]m into a discrete vector T \u2208 {0, 1}m, and treat T as the indicator function of a set approximating the \u03b2-quantile (see Section 5 for details of the rounding algorithm).\nIn summary, given a noisy rating matrix A\u0303, we will first run Algorithm 1 to recover a \u03b2-quantile matrix M\u0303 for each rater, and then run Algorithm 2 to recover our personal \u03b2-quantile from M\u0303 .\nPossible attacks by adversaries. In our algorithm, the adversaries can influence M\u0303i for reliable raters i via the nuclear norm constraint (note that the other constraints are separable across rows). This makes sense because the nuclear norm is what causes us to pool global structure across raters (and thus potentially pool bad information). In order to limit this influence, the constraint on the nuclear norm is weaker than is typical by a factor of 2\u01eb ; it is not clear to us whether this is actually necessary or due to a loose analysis. (Note that M\u2217C\u2013M\n\u2217 restricted to the reliable rows\u2013has nuclear norm \u221a \u03b1\u03b2nm, since it is the \u03b1n\u00d7\u03b2m all-1s matrix padded by zeros; the constraint on \u2016M\u2016\u2217 must be at least 1\u03b1 times as large as this since the adversaries could produce 1 \u03b1 permuted copies of M \u2217 C .) The constraint \u2211\nj Mij \u2264 \u03b2m is not typical in the literature. For instance, (Chen et al., 2014) place no constraint on the sum of each row in M (instead of recovering the \u03b2-quantile, they normalize A\u0303 to lie in [\u22121, 1]m\u00d7m and recover the items with a positive rating). Our row normalization constraint prevents an attack in which a spammer rates a random subset of items as high as possible and rates the remaining items as low as possible. If the actual set of high-quality items has density much smaller than 50%, then the spammer gains undue influence relative to honest raters that only rate\nAlgorithm 3 Algorithm for obtaining (unreliable) ratings matrix A\u0303 and noisy ratings r\u0303, r\u0303\u2032. 1: Input: number of raters n, number of items m, and ratings per rater k. 2: Initially assign each rater to each item independently with probability k/m. 3: For every rater assigned more than 2k items, un-assign items until there are 2k remaining. 4: For every item assigned to more than 2k raters, un-assign raters until there are 2k remaining. 5: Have the raters submit ratings of their assigned items, and let A\u0303 denote the resulting matrix of\nratings with missing entries fill in with zeros. 6: Generate each of r\u0303 and r\u0303\u2032 by rating items with probability k0m (fill in missing entries with zeros) 7: Output A\u0303, r\u0303, and r\u0303\u2032\ne.g. 10% of the items highly. Normalizing M to have a fixed row sum prevents this; see Section B for details."}, {"heading": "3 Assumptions and Approach", "text": "We now state our assumptions more formally, state the general form of our results, and outline the key ingredients of the proof. In our setting, we can query a rater i \u2208 [m] and item j \u2208 [m] to obtain a rating A\u0303ij \u2208 [0, 1]. Let r\u2217 \u2208 [0, 1]m denote the vector of true ratings of the items. We can also query an item j (by rating it ourself) to obtain a noisy rating r\u0303j such that E[r\u0303j ] = r\u2217j .\nLet C \u2286 [n] be the set of reliable raters, where |C| \u2265 \u03b1n. Our main assumption is that the reliable raters make independent errors:\nAssumption 1 (Independence). When we query a pair (i, j), and i \u2208 C, we obtain an output A\u0303ij whose value is independent of all of the other queries so far. Similarly, when we query an item j, we obtain an output r\u0303j that is independent of all of the other queries so far.\nNote that Assumption 1 allows the unreliable ratings to depend on all previous ratings and also allows arbitrary collusion among the unreliable raters. In our algorithm, we will generate our own ratings after querying everyone else, which ensures that at least r\u0303 is independent of the adversaries.\nWe need a way to formalize the idea that the reliable raters agree with us. To this end, for i \u2208 C let A\u2217ij be the expected rating that rater i assigns to item j. We want A \u2217 to be roughly increasing in r\u2217:\nDefinition 1 (Monotonic raters). We say that the reliable raters are (L, \u01eb)-monotonic if\nr\u2217j \u2212 r\u2217j\u2032 \u2264 L \u00b7 (A\u2217ij \u2212A\u2217ij\u2032 ) + \u01eb (2)\nwhenever r\u2217j \u2265 r\u2217j\u2032 , and for all i \u2208 C and all j, j\u2032 \u2208 [m].\nThe (L, \u01eb)-monotonicity property says that if we think that one item is substantially better than another item, the reliable raters should think so as well. As an example, suppose that our own ratings are binary (r\u2217j \u2208 {0, 1}) and that each rating A\u0303i,j matches r\u2217j with probability 35 . Then A\u2217i,j = 2 5 + 1 5r \u2217 j , and hence the ratings are (5, 0)-monotonic. In general, the monotonicity property is fairly mild \u2014 if the reliable ratings are not (L, \u01eb)-monotonic, it is not clear that they should even be called reliable!\nAlgorithm for collecting ratings. Under the model given in Assumption 1, our algorithm for collecting ratings is given in Algorithm 3. Given integers k and k0, Algorithm 3 assigns each rater at most 2k ratings, and assigns us 2k0 ratings in expectation. The output is a noisy rating matrix A\u0303 \u2208 [0, 1]n\u00d7m as well as noisy rating vectors r\u0303, r\u0303\u2032 \u2208 [0, 1]m (we need to create two independent rating vectors for technical reasons; in practice we can use a single vector). Our main result states that we can use A\u0303 and r\u0303 to estimate the \u03b2-quantile T \u2217; throughout we will assume that m is at least n.\nTheorem 2. Let m \u2265 n. Suppose that Assumption 1 holds, that the reliable raters are (L, \u01eb0)monotonic, and that we run Algorithm 3 to obtain noisy ratings, with k \u2265 \u2126 (\nlog3(1/\u03b4) \u03b2\u03b13\u01eb4 m n\n)\nand\nk0 \u2265 \u2126 ( log(1/\u03b1\u03b2\u01eb\u03b4) \u03b2\u01eb2 ) . Then, with probability 1\u2212 \u03b4, Algorithms 1 and 2 recover a set T satisfying\n1 \u03b2m\n\n\n\u2211\nj\u2208T\u2217\nr\u2217j \u2212 \u2211\nj\u2208T\nr\u2217j\n\n \u2264 (L+ 1) \u00b7 \u01eb+ \u01eb0. (3)\nNote that the amount of work for the raters scales as mn . Some dependence on m n is necessary, since we need to make sure that every item gets rated at least once.\nThe proof of Theorem 2 can be split into two parts: analyzing Algorithm 1 (Section 4), and analyzing Algorithm 2 (Section 5). At a high level, analyzing Algorithm 1 involves showing that the nuclear norm constraint in (1) imparts sufficient noise robustness while not allowing the adversary too much influence over the reliable rows of M\u0303 . Analyzing Algorithm 2 is far more straightforward, and requires only standard concentration inequalities and a standard randomized rounding idea (though the latter is perhaps not well-known, so we will explain it briefly in Section 5)."}, {"heading": "4 Recovering M\u0303 (Algorithm 1)", "text": "The goal of this section is to show that solving the optimization problem (1) recovers a matrix M\u0303 that approximates the \u03b2-quantile of r\u2217 in the following sense:\nProposition 1. Under the conditions of Theorem 2, Algorithm 1 outputs a matrix M\u0303 satisfying\n1 |C| 1 \u03b2m \u2211\ni\u2208C\n\u2211\nj\u2208[m]\n(T \u2217j \u2212 M\u0303i,j)A\u2217ij \u2264 \u01eb, (4)\nwhere T \u2217j = 1 if j lies in the \u03b2-quantile of r \u2217, and is 0 otherwise.\nProposition 1 says that the row M\u0303i is good according to rater i\u2019s ratings A\u2217i . Note that (L, \u01eb0)monotonicity then implies that M\u0303i is also good according to r\u2217. In particular (see A.2 for details)\n1 |C| 1 \u03b2m \u2211\ni\u2208C\n\u2211\nj\u2208[m]\n(T \u2217j \u2212 M\u0303ij)r\u2217j \u2264 L \u00b7 1 |C| 1 \u03b2m \u2211\ni\u2208C\n\u2211\nj\u2208[m]\n(T \u2217j \u2212 M\u0303ij)A\u2217ij + \u01eb0 \u2264 L \u00b7 \u01eb + \u01eb0. (5)\nProving Proposition 1 involves two major steps: showing (a) that the nuclear norm constraint in (1) imparts noise-robustness, and (b) that the constraint does not allow the adversaries to influence M\u0303C too much. (For a matrix X we let XC denote the rows indexed by C and XC the remaining rows.)\nIn a bit more detail, if we let M\u2217 denote a denoised version of M\u0303 , and B denote a denoised version of A\u0303, we first show (Lemma 1) that \u3008B, M\u0303 \u2212 M\u2217\u3009 \u2265 \u2212\u01eb\u2032 for some \u01eb\u2032 determined below. This is established via the matrix concentration inequalities in Le and Vershynin (2015). Lemma 1 already suffices for standard approaches (e.g., Gu\u00e9don and Vershynin, 2014), but in our case we must grapple with the issue that the rows of B could be arbitrary outside of C, and hence closeness according to B may not imply actual closeness between M\u0303 and M\u2217. Our main technical contribution, Lemma 2, shows that \u3008BC , M\u0303C \u2212 M\u2217C \u3009 \u2265 \u3008B, M\u0303 \u2212 M\u2217\u3009 \u2212 \u01eb\u2032; that is, closeness according to B implies closeness according to BC . We can then restrict attention to the reliable raters, and obtain Proposition 1.\nPart 1: noise-robustness. Let B be the matrix satisfying BC = kmA \u2217 C , BC = A\u0303C , which denoises A\u0303 on C. The scaling km is chosen so that E[A\u0303C ] \u2248 BC . Also define R \u2208 Rn\u00d7m by Rij = T \u2217j . Ideally, we would like to have MC = RC , i.e., M matches T \u2217 on all the rows of C. In light of this, we will let M\u2217 be the solution to the following \u201ccorrected\u201d program, which we don\u2019t have access to (since it involves knowledge of A\u2217 and C), but which will be useful for analysis purposes:\nmaximize \u3008B,M\u3009, (6) subject to MC = RC , 0 \u2264 Mij \u2264 1 \u2200i, j,\n\u2211 jMij \u2264 \u03b2m \u2200i, \u2016M\u2016\u2217 \u2264 2\n\u03b1\u01eb\n\u221a\n\u03b1\u03b2nm\nImportantly, (6) enforces M\u2217ij = T \u2217 j for all i \u2208 C. Lemma 1 shows that M\u0303 is \u201cclose\u201d to M\u2217:\nLemma 1. Let m \u2265 n. Suppose that Assumption 1 holds and that k = \u2126 ( log3(1/\u03b4) \u03b2\u03b13\u01eb4 m n ) . Then, the solution M\u0303 to (1) performs nearly as well as M\u2217 under B; specifically, with probability 1\u2212 \u03b4, \u3008B, M\u0303\u3009 \u2265 \u3008B,M\u2217\u3009 \u2212 \u01eb\u03b1\u03b2kn. (7)\nNote that M\u0303 is not necessarily feasible for (6), because of the constraintMC = RC ; Lemma 1 merely asserts that M\u0303 approximates M\u2217 in objective value. The proof of Lemma 1, given in Section A.3, primarily involves establishing a uniform deviation result; if we let P denote the feasible set for (1), then we wish to show that |\u3008A\u0303 \u2212 B,M\u3009| \u2264 12\u01eb\u03b1\u03b2kn for all M \u2208 P . This would imply that the objectives of (1) and (6) are essentially identical, and so optimizing one also optimizes the other.\nUsing the inequality |\u3008A\u0303 \u2212 B,M\u3009| \u2264 \u2016A\u0303 \u2212 B\u2016op\u2016M\u2016\u2217, where \u2016 \u00b7 \u2016op denotes operator norm, it suffices to establish a matrix concentration inequality bounding \u2016A\u0303 \u2212 B\u2016op. This bound follows from the general matrix concentration result of Le and Vershynin (2015), stated in Section A.1.\nPart 2: bounding the influence of adversaries. We next show that the nuclear norm constraint does not give the adversaries too much influence over the de-noised program (6); this is the most novel aspect of our argument.\nSuppose that the constraint on \u2016M\u2016\u2217 were not present in (6). Then the adversaries would have no influence on M\u2217C , because all the remaining constraints in (6) are separable across rows. How can we quantify the effect of this nuclear norm constraint? We exploit Lagrangian duality, which allows us to replace constraints with appropriate modifications to the objective function.\nTo gain some intuition, consider Figure 2. The key is that the Lagrange multiplier ZC can bound the amount that \u3008B,M\u3009 can increase due to changing M outside of C. If we formalize this and analyze Z in detail, we obtain the following result: Lemma 2. Let m \u2265 n. Suppose that k = \u2126 (\nlog3(1/\u03b4) \u03b1\u03b2\u01eb2 m n\n)\n. Then with probability at least 1 \u2212 \u03b4 there exists a matrix Z with rank(Z) = 1, \u2016Z\u2016F \u2264 \u01ebk \u221a\n\u03b1\u03b2n/m such that \u3008BC \u2212 ZC ,M\u2217C \u2212MC\u3009 \u2264 \u3008B,M\u2217 \u2212M\u3009 for all M \u2208 P . (8)\nBy localizing \u3008B,M\u2217 \u2212M\u3009 to C via (8), Lemma 2 bounds the effect that the adversaries can have on M\u0303C . It is therefore the key technical tool powering our results, and is proved in Section A.4. Proposition 1 is proved from Lemmas 1 and 2 in Section A.5."}, {"heading": "5 Recovering T (Algorithm 2)", "text": "In this section we show that if M\u0303 satisfies the conclusion of Proposition 1, then Algorithm 2 recovers a set T that approximates T \u2217 well. Formally, we show the following:\nProposition 2. Suppose Assumption 1 holds and k0 \u2265 \u2126 ( log(1/\u03b1\u03b2\u01eb\u03b4) \u03b2\u01eb2 )\n. With probability 1 \u2212 \u03b4, Algorithm 2 outputs a set T satisfying\n1\n\u03b2m\n\u2211\nj\u2208T\nr\u2217j \u2265\n\n\n1\n\u03b2m\n1\n|C| \u2211\ni\u2208C\n\u2211\nj\u2208[m]\nM\u0303ijr \u2217 j\n\n\u2212 \u01eb. (9)\nThe validity of this procedure hinges on two results. First, establish a concentration bound showing that \u2211\nj M\u0303ij r\u0303j is close to k0 m\n\u2211\nj M\u0303ijr \u2217 j for all i \u2208 C, which implies that the \u03b1n best rows of M\u0303\naccording to r\u0303 also look good according to r\u2217. This yields the following lemma:\nLemma 3. Let C\u2032 be the \u03b1n best rows according to r\u0303, as in Algorithm 2. Suppose that r\u0303 satisfies Assumption 1 and that k0 \u2265 \u2126 ( log(1/\u03b1\u03b4) \u03b2\u01eb2 ) . Then, with probability 1\u2212 \u03b4, we have\n1\n\u03b1n\n\u2211\ni\u2208C\u2032\n(\n\u2211\nj\u2208[m]\nM\u0303ijr \u2217 j\n) \u2265 1|C| \u2211\ni\u2208C\n(\n\u2211\nj\u2208[m]\nM\u0303ijr \u2217 j\n) \u2212 \u01eb 4 \u03b2m. (10)\nSee Section A.6 for a proof. The idea is to establish a uniform bound showing that \u2211\ni\u2208S\n\u2211\nj\u2208[m] M\u0303ij(r\u0303j \u2212 k0m r\u2217j ) is small for any set of \u03b1n rows S, and hence that greedily taking the \u03b1n best rows according to r\u0303 is almost as good as taking the \u03b1n best rows according to r\u2217. We improve over a na\u00efve union bound by exploiting power mean inequalities on cumulant functions.\nHaving recovered a set C\u2032 of good rows, define their average T0 \u2208 [0, 1]m as T0 def= 1|C\u2032| \u2211\ni\u2208C\u2032 M\u0303i. We need to turn T0 into a binary vector so that Algorithm 2 can output a set; we do so via randomized rounding, obtaining a vector T \u2208 {0, 1}m such that E[T0] = T . Our rounding procedure is given in Algorithm 4; the following lemma, proved in A.7, asserts its correctness:\nLemma 4. The output T of Algorithm 4 satisfies E[T ] = T0, \u2016T \u20160 \u2264 \u03b2m.\nAlgorithm 4 Randomized rounding algorithm. 1: procedure RANDOMIZEDROUND(T0) \u22b2 T0 \u2208 [0, 1]m satisfies \u2016T0\u20161 \u2264 \u03b2m 2: Let s be the vector of partial sums of T0 \u22b2 i.e., sj = (T0)1 + \u00b7 \u00b7 \u00b7+ (T0)j 3: Sample u \u223c Uniform([0, 1]). 4: T \u2190 [0, . . . , 0] \u2208 Rm 5: for z = 0 to \u03b2m\u2212 1 do 6: Find j such that u+ z \u2208 [sj\u22121, sj). \u22b2 if no such j exists, skip the next step 7: Tj \u2190 1 8: end for 9: return T 10: end procedure\nThe remainder of the proof involves lower-bounding the probability that T is accepted in each stage of the while loop in Algorithm 2. We refer the reader to Section A.8 for details."}, {"heading": "6 Open Directions and Related Work", "text": "Future Directions On the theoretical side, perhaps the most immediate open question is whether it is possible to improve the dependence of k (the amount of work required per worker) on the parameters \u03b1, \u03b2, and \u01eb. It is tempting to hope that when m = n a tight result would have k = \u0398 (\nlog(1/\u03b1) min(\u03b1,\u03b2)\u01eb2\n)\n,\nin loose analogy to recent results for the stochastic block model (Banks and Moore, 2016).\nOur results also leave some open questions for variations on our setting. One concerns the regime where m \u226a n: in this case, can we get by with much less work per rater? Another question concerns adaptivity: if the choice of queries is based on previous worker ratings, can we reduce the amount of work? We would be quite interested in answers to either question.\nRelated work. Our setting is closely related to the problem of peer prediction (Miller et al., 2005), in which we wish to obtain truthful information from a population of raters by exploiting inter-rater\nagreement. While several mechanisms have been proposed for these tasks, they typically assume that rater accuracy is observable online (Resnick and Sami, 2007), that raters are rational agents maximizing a payoff function (Dasgupta and Ghosh, 2013; Kamble et al., 2015; Shnayder et al., 2016), that the workers follow a simple statistical model (Karger et al., 2014; Zhang et al., 2014; Zhou et al., 2015), or some combination of the above (Shah and Zhou, 2015; Shah et al., 2015).\nThe work most close to ours is Christiano (2014; 2016), which studies online collaborative prediction in the presence of adversaries; roughly, when raters interact with an item they predict its quality and afterwards observe the actual quality; the goal is to minimize the number of incorrect predictions among the honest raters. This differs from our setting in that (i) the raters are trying to learn the item qualities as part of the task, and (ii) there is no requirement to induce a final global estimate of the high-quality items, which is necessary for estimating quantiles. It seems possible however that there are theoretical ties between this setting and ours, which would be interesting to explore."}, {"heading": "A Deferred Proofs", "text": ""}, {"heading": "A.1 Matrix Concentration Bound of Le and Vershynin (2015)", "text": "For ease of reference, here we state the matrix concentration bound from Le and Vershynin (2015), which we make use of in the proofs below. Theorem 3 (Theorem 2.1 in Le and Vershynin (2015)). Given an s\u00d7s matrix P with entries Pi,j \u2208 [0, 1], and a random matrix A with the properties that 1) each entry of A is chosen independently, 2) E[Ai,j ] = Pi,j , and 3) Ai,j \u2208 [0, 1], then for any r \u2265 1, the following holds with probability at least 1\u2212 s\u2212r: let d = s \u00b7maxi,j Pi,j , and modify any subset of at most 10s/d rows and/or columns of A by arbitrarily decreasing the value of nonzero elements of those rows or columns to form the matrix A\u2032 with entries in [0, 1], then\n||A\u2032 \u2212 P ||op \u2264 Cr3/2 (\u221a d+ \u221a d\u2032 ) ,\nwhere d\u2032 is the maximum \u21132 norm of any row or column of A\u2032, and C is an absolute constant.\nNote: The proof of this theorem in Le and Vershynin (2015) shows that the statement continues to hold in the slightly more general setting where the entries of A are chosen independently according to random variables with bounded variance and sub-Gaussian tails, rather than just random variables restricted to the interval [0, 1]."}, {"heading": "A.2 Details of Lipschitz Bound (Equation 5)", "text": "The proof essentially consists of matching up each value r\u2217j , for j \u2208 T \u2217, with a set of values r\u2217j\u2032 , j\u2032 \u2265 j, where the corresponding M\u0303i,j\u2032 sum to 1; we can then invoke the condition (2). Unfortunately, expressing this idea formally is a bit notationally cumbersome.\nBefore we start, we observe that the Lipschitz condition (2) implies that, if r\u2217j \u2265 r\u2217j\u2032 , then r\u2217j \u2212 r\u2217j\u2032 \u2264 L \u00b7 (\nA\u2217i,j \u2212A\u2217i,j\u2032 ) + \u01eb0. It is this form of (2) that we will make use of below.\nNow, let Ij = I[j \u2208 T \u2217], and without loss of generality suppose that the indices j are such that r\u22171 \u2265 r\u22172 \u2265 \u00b7 \u00b7 \u00b7 \u2265 r\u2217m. For a vector v \u2208 [0, 1]m, define\nh(\u03c4, v) def = inf{j |\nj \u2211\nj\u2032=1\nvj\u2032 \u2265 \u03c4}, (11)\nwhere h(\u03c4, v) = \u221e if no such j exists. We observe that for any vector v \u2208 [0, 1]m, we have \u2211\nj\u2208[m]\nvjr \u2217 j =\n\u222b \u221e\n0\nr\u2217h(\u03c4 ;v)d\u03c4, (12)\nwhere we define r\u2217\u221e = 0 (note that the integrand is therefore 0 for any \u03c4 \u2265 \u2016v\u20161). Hence, we have \u2211\nj\u2208T\u2217\nr\u2217j \u2212 \u2211\nj\u2208[m]\nM\u0303i,jr \u2217 j =\n\u2211\nj\u2208[m]\nIjr \u2217 j \u2212\n\u2211\nj\u2208[m]\nM\u0303i,jr \u2217 j (13)\n=\n\u222b \u03b2m\n0\nr\u2217h(\u03c4,I) \u2212 r\u2217h(\u03c4,M\u0303i)d\u03c4 (14)\n(i) \u2264 \u222b \u03b2m\n0\n[ L \u00b7 ( A\u2217h(\u03c4,I) \u2212A\u2217h(\u03c4,M\u0303i) ) + \u01eb0 ] d\u03c4 (15)\n= L \u00b7\n\n\n\u2211\nj\u2208[m]\nIjA \u2217 j \u2212\n\u2211\nj\u2208[m]\nM\u0303i,jA \u2217 j\n\n+ \u03b2m\u01eb0 (16)\n= L \u00b7\n\n\n\u2211\nj\u2208T\u2217\nA\u2217j \u2212 \u2211\nj\u2208[m]\nM\u0303i,jA \u2217 j\n\n + \u03b2m\u01eb0, (17)\nwhich implies (5). The key step is (i), which uses the fact that h(\u03c4, I) \u2264 h(\u03c4, M\u0303i) (because I is maximally concentrated on the left-most indices of [m]), and hence r\u2217h(\u03c4,I) \u2265 r\u2217h(\u03c4,M\u0303i)."}, {"heading": "A.3 Stability Under Noise (Proof of Lemma 1)", "text": "By H\u00f6lder\u2019s inequality, we have that |\u3008A\u0303\u2212B,M\u3009| \u2264 \u2016A\u0303\u2212B\u2016op\u2016M\u2016\u2217. We now leverage Theorem 3 to bound \u2016A\u0303 \u2212 B\u2016op. To apply the theorem, first note that from the construction of A\u0303 given in Algorithm 3, A\u0303 can be constructed by first having the raters rate each item independently with probability k/m to form matrix A\u0303o and then removing ratings from the \u201cheavy\u201d rows (i.e. rows with more than 2k ratings), and \u201cheavy\u201d columns (i.e. columns with more than 2k) ratings. By standard Chernoff bounds, the probability that a given row or column will need to be \u201cpruned\u201d is at most e\u2212k/3 \u2264 2/k, and hence from the independence of the rows, the probability that more than 5n/k rows are \u201cheavy\u201d is at most e\u22122n/3k. The probability that there are more than 5n/k heavy columns is identically bounded.\nNote that the expectation of the portion of A\u0303o corresponding to the reliable raters is exactly the corresponding portion of matrix B, and with probability at least 1\u2212 2e\u22122n/3k, at most 10n/k rows and/or columns of A\u0303o are pruned to form A\u0303. Consider padding matrices A\u0303 and B with zeros, to form the n \u00d7 n matrices A\u0303\u2032 and B\u2032. With probability 1 \u2212 2e\u22122n/3k the conditions of Theorem 3 now apply to A\u0303\u2032 and B\u2032, with the parameters d = nkm \u2264 k, and d\u2032 = 2k. Hence for any r \u2265 1, with probability at least 1\u2212 2e\u22122n/3k \u2212 n\u2212r\n\u2016A\u0303\u2212B\u2016op = \u2016A\u0303\u2032 \u2212 B\u2032\u2016op \u2264 Cr3/2 \u221a k,\nfor some absolute constant C. By assumption, \u2016M\u0303\u2016\u2217 \u2264 2\u03b1\u01eb \u221a \u03b1\u03b2nm and \u2016M\u2217\u2016\u2217 \u2264 2\u03b1\u01eb \u221a \u03b1\u03b2nm. Hence setting r = log(1/\u03b4), and k \u2265 C\u2032 log3(1\u03b4 ) m/n \u01eb4\u03b13\u03b2 for some absolute constant C\n\u2032, we have that with probability at least 1 \u2212 \u03b4, we have\n|\u3008A\u0303\u2212B, M\u0303\u3009| \u2264 1 2 \u01eb\u03b1\u03b2kn,\nand |\u3008A\u0303\u2212B,M\u2217\u3009| is bounded identically. To conclude, we have the following:\n\u3008B, M\u0303\u3009 \u2265 \u3008A\u0303, M\u0303\u3009 \u2212 1 2 \u01eb\u03b1\u03b2kn (18)\n\u2265 \u3008A\u0303,M\u2217\u3009 \u2212 1 2 \u01eb\u03b1\u03b2kn (since M\u0303 is optimal for A\u0303) (19)\n\u2265 \u3008B,M\u2217\u3009 \u2212 \u01eb\u03b1\u03b2kn, (20) which completes the proof."}, {"heading": "A.4 Bounding the Effect of Adversaries (Proof of Lemma 2)", "text": "In this section we prove Lemma 2. Let P0 be the superset of P where we have removed the nuclear norm constraint. By Lagrangian duality we know that there is some \u00b5 such that maximizing \u3008B,M\u3009 over P \u2229 {MC = RC} is equivalent to maximizing f\u00b5(M) def= \u3008B,M\u3009 + \u00b5 ( 2 \u01eb\u03b1 \u221a \u03b1\u03b2nm\u2212 \u2016M\u2016\u2217 ) over P0 \u2229 {MC = RC}. We start by bounding \u00b5. We claim that \u00b5 \u2264 \u01ebk \u221a\n\u03b1\u03b2n/m. To show this, we will first show that \u3008B,M\u3009 cannot get too large. Let E be the set of (i, j) for which ratings are observed, and define the matrix B\u2032 as (B\u2032)ij = km + I[(i, j) \u2208 E ] (Bij \u2212 1); note that (B \u2212 B\u2032)ij = I[(i, j) \u2208 E ]\u2212 km . For any M \u2208 P0, we have\n\u3008B,M\u3009 \u2264 \u3008B\u2032,M\u3009+ \u3008B \u2212B\u2032,M\u3009 (21) \u2264 \u03b2kn+ \u2016B \u2212B\u2032\u2016op\u2016M\u2016\u2217 (22) (i)\n\u2264 \u03b2kn+ log(1/\u03b4)3/22 \u221a 2k\u2016M\u2016\u2217 (23)\n(ii) \u2264 k ( \u03b2n+ \u01eb \u221a \u03b1\u03b2n/m\n2 \u2016M\u2016\u2217\n)\n. (24)\nIn (i) we used the matrix concentration inequality of Theorem 3, in a similar manner as was used in our proof of Lemma 1. Specifically, we consider padding B and B\u2032 with zeros so as to make both into n \u00d7 n matrices. Provided the total number of raters and items whose initial assignments are removed in the second and third steps of the rater/item assignment procedure (Algorithm 3) is bounded by 10n/k, which occurs with probability at least 1 \u2212 \u03b4/2 given our choice of k, then Theorem 3 applies with r = log(1/\u03b4), and d and d\u2032 bounded by 2k, yielding an operator norm bound of r3/2( \u221a k+ \u221a 2k) \u2264 log(1/\u03b4)3/22 \u221a 2k, that holds with probability 1\u2212 n\u2212r > 1\u2212 \u03b4/2. In (ii) we plug in our assumption k = \u2126 ( log(1/\u03b4)3\n\u03b1\u03b2\u01eb2 m n\n)\n.\nNow, suppose that we take \u00b50 = \u01eb \u221a \u03b1\u03b2n/mk and optimize \u3008B,M\u3009 \u2212 \u00b50\u2016M\u2016\u2217 over P0 \u2229 {MC = RC}. By the above inequalities, we have \u3008B,M\u3009 \u2212 \u00b50\u2016M\u2016\u2217 \u2264 \u03b2kn \u2212 \u01eb \u221a \u03b1\u03b2n/mk\n2 \u2016M\u2016\u2217, and so any M with \u2016M\u2016\u2217 > 2\u01eb\u03b1 \u221a \u03b1\u03b2nm cannot possibly be optimal, since the solution M = 0 would be better. Hence, \u00b50 is a large enough Lagrange multiplier to ensure that M \u2208 P , and so \u00b5 \u2264 \u00b50 = \u01ebk \u221a \u03b1\u03b2n/m, as claimed.\nWe next characterize the subgradient of f\u00b5 at M = M\u2217. Define the projection matrix P as\nPi,i\u2032 =\n{\n1 |C| : i, i \u2032 \u2208 C \u03b4i,i\u2032 : else . (25)\nThus PM = M if and only if all rows in C are equal to each other. In particular, PM = M whenever MC = RC . Now, since M\u2217 is the maximum of f\u00b5(M) over all M \u2208 P0 \u2229 {MC = RC}, there must be some G \u2208 \u2202f\u00b5(M\u2217) such that \u3008G,M \u2212 M\u2217\u3009 \u2264 0 for all M \u2208 P0 \u2229 {MC = RC}. The following lemma says that without loss of generality we can assume that PG = G:\nLemma 5. Suppose that G \u2208 \u2202f(M\u2217) satisfies \u3008G,M \u2212M\u2217\u3009 \u2264 0 for all M \u2208 P0 \u2229 {MC = RC}. Then, PG satisfies the same property, and lies in \u2202f(M\u2217) as well.\nWe can further note (by differentiating f\u00b5) that G = B \u2212 \u00b5Z0, where \u2016Z0\u2016op \u2264 11. Then PG = PB \u2212 \u00b5PZ0 = B \u2212 \u00b5PZ0. Let r(M) denote the matrix where MC is replaced with RC (so r(M) \u2208 P0 \u2229 {RC = MC} whenever M \u2208 P0). The rest of the proof is basically algebra; for any M \u2208 P , we have\n\u3008B,M \u2212M\u2217\u3009 (i)\n\u2264 f\u00b5(M)\u2212 f\u00b5(M\u2217) (26) (ii) \u2264 \u3008B \u2212 \u00b5PZ0,M \u2212M\u2217\u3009 (27) = \u3008B \u2212 \u00b5PZ0,M \u2212 r(M)\u3009 + \u3008B \u2212 \u00b5PZ0, r(M) \u2212M\u2217\u3009 (28) (iii)\n\u2264 \u3008B \u2212 \u00b5PZ0,M \u2212 r(M)\u3009 (29) (iv) = \u3008BC \u2212 \u00b5(PZ0)C ,MC \u2212 r(M)C\u3009 (30) = \u3008BC \u2212 \u00b5(PZ0)C ,MC \u2212M\u2217C \u3009, (31)\nwhere (i) is by complementary slackness (either \u00b5 = 0 or \u2016M\u2217\u2016\u2217 = 2\u03b1\u01eb \u221a \u03b1\u03b2nm); (ii) is concavity of f\u00b5, and the fact that B \u2212 \u00b5PZ0 is a subgradient; (iii) is the property from Lemma 5 (\u3008B \u2212 \u00b5PZ0, r(M)\u2212M\u2217\u3009 \u2264 0 since r(M) \u2208 P0); and (iv) is because M and r(M) only differ on C. To finish, we will take Z = \u00b5(PZ0)C . We note that \u2016Z\u2016op = \u2016\u00b5(PZ0)C\u2016op \u2264 \u00b5\u2016PZ0\u2016op \u2264 \u00b5\u2016Z0\u2016op \u2264 \u00b5. Moreover, Z has rank 1 and so \u2016Z\u2016F = \u2016Z\u2016op \u2264 \u00b5 \u2264 \u01ebk \u221a\n\u03b1\u03b2n/m, as was to be shown.\nProof of Lemma 5. First, since PM = M for all M \u2208 P0 \u2229 {MC = RC}, and PM\u2217 = M\u2217, we have \u3008PG,M \u2212M\u2217\u3009 = \u3008G,P (M \u2212M\u2217)\u3009 = \u3008G,M \u2212M\u2217\u3009 \u2264 0. We thus only need to show that\n1This is due to the more general result that, for any norm \u2016 \u00b7 \u2016, the subgradient of \u2016 \u00b7 \u2016 at any point has dual norm at most 1.\nPG is still a subgradient of f\u00b5. Indeed, we have (for arbitrary M )\n\u3008PG,M \u2212M\u2217\u3009 = \u3008G,PM \u2212M\u2217\u3009 (32) (i)\n\u2265 f\u00b5(PM)\u2212 f\u00b5(M\u2217) (33) = \u3008B,PM\u3009 \u2212 \u00b5\u2016PM\u2016\u2217 \u2212 f\u00b5(M\u2217) (34) = \u3008B,M\u3009 \u2212 \u00b5\u2016PM\u2016\u2217 \u2212 f\u00b5(M\u2217) (35) (ii) \u2265 \u3008B,M\u3009 \u2212 \u00b5\u2016M\u2016\u2217 \u2212 f\u00b5(M\u2217) (36) = f\u00b5(M)\u2212 f\u00b5(M\u2217), (37)\nwhere (i) is because G \u2208 \u2202f\u00b5(M\u2217), and (ii) is because projecting decreases the nuclear norm. Since the inequality \u3008PG,M \u2212M\u2217\u3009 \u2265 f\u00b5(M)\u2212 f\u00b5(M\u2217) is the defining property for PG to lie in \u2202f\u00b5(M \u2217), the proof is complete."}, {"heading": "A.5 Proof of Proposition 1", "text": "In this section, we will prove Proposition 1 from Lemmas 1 and 2. We start by plugging in M\u0303 for M in Lemma 2. This yields \u3008BC \u2212 ZC ,M\u2217C \u2212 M\u0303C\u3009 \u2264 \u3008B,M\u2217 \u2212 M\u0303\u3009 \u2264 \u01eb\u03b1\u03b2kn by Lemma 1. On the other hand, we have\n|\u3008ZC ,M\u2217C \u2212 M\u0303C\u3009| \u2264 \u2016ZC\u2016F\u2016M\u2217C \u2212 M\u0303C\u2016F (38)\n\u2264 \u01eb \u221a \u03b1\u03b2nk/m\n\u221a\n\u2016M\u2217C \u2212 M\u0303C\u20161\u2016M\u2217C \u2212 M\u0303C\u2016\u221e (39)\n\u2264 \u01eb \u221a \u03b1\u03b2nk/m \u221a 2\u03b1\u03b2mn = \u221a 2\u01eb\u03b1\u03b2kn. (40)\nPutting these together, we obtain \u3008BC ,M\u2217C \u2212 M\u0303C\u3009 \u2264 (1 + \u221a 2)\u01eb\u03b1\u03b2kn. Expanding \u3008BC ,M\u2217C \u2212 M\u0303C\u3009 as km \u2211 i\u2208C ( \u2211 j\u2208[m](Rij \u2212 M\u0303ij)A\u2217ij ) , we obtain\n1 |C| 1 \u03b2m \u2211\ni\u2208C\n\u2211\nj\u2208[m]\n(T \u2217j \u2212 M\u0303ij)A\u2217ij \u2264 (1 + \u221a 2)\u01eb. (41)\nScaling \u01eb by a factor of 1 + \u221a 2 yields the desired result."}, {"heading": "A.6 Concentration Bounds for r\u0303 (Proof of Lemma 3)", "text": "We start by stating a lemma which will be useful both here and later:\nLemma 6. Let M \u2208 [0, 1]n\u00d7m be a matrix of random variables such that \u2016Mi\u201622 \u2264 \u03b2m for all rows i \u2208 [n]. Define the deviation Di def= \u2211m j=1 M\u0303ij(r\u0303j \u2212 k0m r\u2217j ). Then, for k0 \u2265 3 log(2n/v\u03b4) min(\u01eb,\u01eb2) , with probability 1\u2212 \u03b4, we have \u2223 \u2223\n\u2223 1 |V |\n\u2211\ni\u2208V Di\n\u2223 \u2223 \u2223 \u2264 \u01eb\u03b2k0 for all sets V \u2286 [n] with |V | \u2265 v.\nGiven Lemma 6, the rest of the proof is fairly straightforward. Noting that \u01eb \u2264 1 and applying this conclusion for v = \u03b1n, and k0 \u2265 3\u00b78 2 log(2/\u03b1\u03b4) \u01eb2 , we see that\n1\n\u03b1n\n\u2211\ni\u2208C\u2032\n\u2211\nj\u2208[m]\nM\u0303ijr \u2217 ij \u2265\n1\n\u03b1n\nm\nk0\n\u2211\ni\u2208C\u2032\n\u2211\nj\u2208[m]\nM\u0303ij r\u0303ij \u2212 \u01eb\n8 \u03b2m (42)\n\u2265 1|C| m\nk0\n\u2211\ni\u2208C\n\u2211\nj\u2208[m]\nM\u0303ij r\u0303ij \u2212 \u01eb\n8 \u03b2m (43)\n\u2265 1|C| \u2211\ni\u2208C\n\u2211\nj\u2208[m]\nM\u0303ijr \u2217 ij \u2212\n\u01eb 4 \u03b2m, (44)\nas was to be shown.\nProof of Lemma 6. Define the cumulant function ci(\u03bb) def = log(Er\u0303[exp(\u03bbDi)]). We have\nci(\u03bb) = log(Er\u0303[exp(\u03bb \u2211\nj\nM\u0303ij(r\u0303j \u2212 (k0/m)r\u2217j ))]) (45)\n= \u2211\nj\nlog(Er\u0303[exp(\u03bbM\u0303ij(r\u0303j \u2212 (k0/m)r\u2217j ))]) (46)\n(i) \u2264 \u2211\nj\n(e\u03bb \u2212 \u03bb\u2212 1)M\u03032ij Var[r\u0303j ] (47)\n\u2264 (e\u03bb \u2212 \u03bb\u2212 1) \u2211\nj\nM\u03032ij k0 m\n(48)\n\u2264 (e\u03bb \u2212 \u03bb\u2212 1)\u03b2k0, (49) where (i) is Bennet\u2019s inequality.\nWe also consider the cumulant function for the maximum average deviation over possible sets V :\nCv(\u03bb) def = log\n(\nEr\u0303\n[\nmax |V |\u2265v exp\n(\n\u03bb\n|V | \u2211\ni\u2208V\nDi\n)])\n. (50)\nTo bound Cv(\u03bb), we use the power mean inequality\nmax |V |\u2265v exp\n(\n\u03bb\n|V | \u2211\ni\u2208V\nDi\n)\n\u2264 max |V |\u2265v\n1\n|V | \u2211\ni\u2208V\nexp (\u03bbDi) (51)\n\u2264 max |V |\u2265v\n1\n|V |\nn \u2211\ni=1\nexp (\u03bbDi) (52)\n\u2264 1 v\nn \u2211\ni=1\nexp (\u03bbDi) . (53)\nTherefore,\nCv(\u03bb) = log\n(\nEr\u0303\n[\nmax |V |\u2265v exp\n(\n\u03bb\n|V | \u2211\ni\u2208V\nDi\n)])\n(54)\n\u2264 log (\nEr\u0303\n[\n1\nv\nn \u2211\ni=1\nexp (\u03bbDi)\n])\n(55)\n\u2264 log (n\nv exp\n( (e\u03bb \u2212 \u03bb\u2212 1)\u03b2k0 )\n)\n(56)\n= log(n/v) + (e\u03bb \u2212 \u03bb\u2212 1)\u03b2k0. (57) By applying a standard Chernoff bound argument to Cv(\u03bb), we obtain\nP\n[\nmax |V |\u2265v\n\u2223 \u2223 \u2223 \u2223 \u2223 1 |V | \u2211\ni\u2208V\nDi\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2265 \u01eb\u03b2k0 ] \u2264 2n v exp ( \u2212\u03b2k0 3 min(\u01eb, \u01eb2) ) . (58)\nIn particular, for k0 \u2265 3 log(2n/v\u03b4)\u03b2min(\u01eb,\u01eb2) , we have with probability 1 \u2212 \u03b4 that \u2223 \u2223 \u2223 1 |V | \u2211 i\u2208V Di \u2223 \u2223\n\u2223 \u2264 \u01eb\u03b2k0 for all sets V \u2286 [n] with |V | \u2265 v, as was to be shown."}, {"heading": "A.7 Correctness of Randomized Rounding (Proof of Lemma 4)", "text": "Our goal is to show that the output of Algorithm 4 satisfies E[T ] = T0. First, observe that since (T0)j \u2264 1 for all j, each interval [sj\u22121, sj) has length at most 1, and so the for loop over z never picks the same index j twice. Moreover, the probability that j is included in T0 is exactly sj\u2212sj\u22121 = (T0)j . The result follows by linearity of expectation."}, {"heading": "A.8 Correctness of Algorithm 2 (Proof of Proposition 2)", "text": "First, we claim that with probability 1 \u2212 \u03b4, we will invoke RandomizedRound at most 4 log(1/\u03b4)\u01eb\u03b2 times. To see this, note that E[\u3008T, r\u0303\u2032\u3009] = \u3008T0, r\u0303\u2032\u3009, and \u3008T, r\u0303\u2032\u3009 \u2208 [0, k0] almost surely. By Markov\u2019s inequality, the probability that \u3008T, r\u0303\u2032\u3009 < \u3008T0, r\u0303\u2032\u3009 \u2212 \u01eb4\u03b2k0 is at most k0\u2212\u3008T0,r\u0303 \u2032\u3009 k0\u2212\u3008T0,r\u0303\u2032\u3009+(\u01eb/4)\u03b2k0 . We can assume that \u3008T0, r\u0303\u2032\u3009 \u2265 (\u01eb/4)\u03b2k0 (since otherwise we accept T with probability 1), in which case the preceding expression is bounded by k0\u2212(\u01eb/4)\u03b2k0k0 = 1 \u2212 \u01eb 4\u03b2. Therefore, the probability of accepting T in any given iteration of the while loop is at least \u01eb4\u03b2, and so the probability of accepting at least once in 4 log(1/\u03b4)\u01eb\u03b2 iterations is indeed at least 1\u2212 \u03b4.\nNext, for k0 \u2265 \u2126 ( log(2/\u01eb\u03b2\u03b4) \u03b2\u01eb2 )\n, we can make the probability that |\u3008T, r\u0303\u2212 k0m r\u2217\u3009| \u2265 \u01eb4\u03b2k0 be at most \u03b4\u01eb\u03b2 4 log(1/\u03b4)+1 (this follows from a standard Chernoff argument which we omit; Lemma 6 contains a superset of the necessary ideas). Therefore, by union bounding over the 4 log(1/\u03b4)\u01eb\u03b2 possible T as well as T0, with probability 1\u22122\u03b4 we have |\u3008T, r\u0303\u2212 k0m r\u2217\u3009| \u2264 \u01eb4\u03b2k0 for whichever T we end up accepting, as well as for T = T0.\nConsequently, we have\n\u3008T, r\u2217\u3009 \u2265 m k0 \u3008T, r\u0303\u2032\u3009 \u2212 \u01eb 4 \u03b2m (59)\n\u2265 m k0 \u3008T0, r\u0303\u2032\u3009 \u2212 2\u01eb 4 \u03b2m (60) \u2265 \u3008T0, r\u2217\u3009 \u2212 3\u01eb\n4 \u03b2m (61)\n\u2265 \u3008 1|C| \u2211\ni\u2208C\nM\u0303i, r \u2217\u3009 \u2212 \u01eb\u03b2m, (62)\nwhere the final step is Lemma 3. By scaling down the failure probability \u03b4 by a constant (to account for the probability of failure at each step of the above argument), Proposition 2 follows."}, {"heading": "A.9 Proof of Theorem 2", "text": "By Proposition 1, for k = \u2126 (\n1 \u03b2\u03b13\u01eb4 max\n( 1, mn )\n)\n, we can recover a matrix M\u0303 sat-\nisfying 1|C| 1 \u03b2m\n\u2211\ni\u2208C\n\u2211\nj\u2208[m](M \u2217 ij \u2212 T \u2217j )A\u2217ij \u2264 \u01eb, and hence by (5) M\u0303 also satisfies\n1 |C| 1 \u03b2m\n\u2211\ni\u2208C\n\u2211\nj\u2208[m](M \u2217 ij \u2212 T \u2217j )r\u2217j \u2264 L \u00b7 \u01eb+ \u01eb0.\nBy Proposition 2, we then recover a set T satisfying\n1\n\u03b2m\n\u2211\nj\u2208T\nr\u2217j \u2265\n\n\n1\n\u03b2m\n1\n|C| \u2211\ni\u2208C\n\u2211\nj\u2208[m]\nM\u0303ijr \u2217 j\n\n\u2212 \u01eb (63)\n\u2265\n\n\n1\n\u03b2m\n1\n|C| \u2211\ni\u2208C\n\u2211\nj\u2208[m]\nT \u2217j r \u2217 j\n\n \u2212 [(L+ 1) \u00b7 \u01eb+ \u01eb0] (64)\n=\n\n\n1\n\u03b2m\n\u2211\nj\u2208T\u2217\nr\u2217j\n\n\u2212 [(L + 1) \u00b7 \u01eb + \u01eb0], (65)\nas was to be shown."}, {"heading": "B Examples of Adversarial Behavior", "text": "In this section, in order to provide some intuition we show two possible attacks that adversaries could employ to make it hard for us to recover the good items. The first attack creates a symmetric\nsituation, whereby there are 1\u03b1 indistinguishable sets of potentially good items, and we are therefore forced to consider each set before we can find out which one is actually good. The second attack demonstrates the necessity of constraining each row to have a fixed sum, by showing that adversaries that are allowed to create very dense rows can have disproportionate influence on nuclear normbased recovery algorithms"}, {"heading": "B.1 Necessity of Nuclear Norm Scaling", "text": "Suppose for simplicity that \u03b1 = \u03b2 and n = m. Let J be the \u03b1n\u00d7 \u03b1n all-ones matrix, and suppose that the full rating matrix A has a block structure:\nA\u2217 =\n\n \n\nJ (1\u2212 \u01eb)J \u00b7 \u00b7 \u00b7 (1\u2212 \u01eb)J (1\u2212 \u01eb)J J \u00b7 \u00b7 \u00b7 (1\u2212 \u01eb)J\n... ...\n. . . ...\n(1\u2212 \u01eb)J (1\u2212 \u01eb)J \u00b7 \u00b7 \u00b7 J\n\n \n\n(66)\nIn other words, both the items and raters are partitioned into 1\u03b1 blocks, each of size \u03b1n. A rater assigns a rating of 1 to everything in their corresponding block, and a rating of 1 \u2212 \u01eb to everything outside of their block. Thus, there are 1\u03b1 completely symmetric blocks, only one of which corresponds to the good raters. Since we do not know which of these blocks is actually good, we need to include them all in our solution M\u2217. Therefore, M\u2217 should be\nM\u2217 =\n\n \n J 0 \u00b7 \u00b7 \u00b7 0 0 J \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 J\n\n \n\n(67)\nNote however that in this case, \u2016M\u2217\u2016\u2217 = n, while \u221a \u03b1\u03b2nm = \u221a \u03b12n2 = \u03b1n. We therefore need\nthe nuclear norm constraint in (1) to be at least 1\u03b1 times larger than \u221a \u03b1\u03b2nm in order to capture the solution M\u2217 above.\nIt is not obvious to us that the additional 2\u01eb factor appearing in (1) is actually necessary, but it was needed in our analysis in order to bound the impact of adversaries."}, {"heading": "B.2 Necessity of Row Normalization", "text": "Suppose that we did not include the row-normalization constraint \u2211 j M\u0303ij \u2264 \u03b2m in (1). For instance, this might happen if, instead of seeking all items of quality above a given quantile, we sought all items with quality above a given threshold (say, whose quality was great than 12 ). In this case we might pose the optimization problem\nmaximize \u3008A\u0303\u2212 12Jn,m,M\u3009, (68) subject to 0 \u2264 Mij \u2264 1 \u2200i, j,\n\u2016M\u2016\u2217 \u2264 2\n\u03b1\u01eb\n\u221a\n\u03b1\u03b2nm,\nwhere Jn,m is the n \u00d7 m all-ones matrix. There are several reasons not to do this (for instance, focusing on quality thresholds rather than quantile thresholds loses the robustness to monotonic transformations that our method enjoys). In this section, we will focus on the particular issue that (68) is less robust to adversaries than (1).\nConcretely, we will suppose that the adversaries are split into 13\u03b2 ( 1 \u03b1 \u2212 1 )\nblocks of size 3\u03b1\u03b2n, each of which rates a random subset of m2 items positively and the rest negatively. So for instance the\nmatrix A\u2217 might look like (with \u03b1 = 25 , \u03b2 = 1 6 , n = 10,m = 12):\nA\u2217 =\n\n                     \n\n\n                     \n\ngo od\n1 1 0 0 0 0 0 0 0 0 0 0\n1 1 0 0 0 0 0 0 0 0 0 0\n1 1 0 0 0 0 0 0 0 0 0 0\n1 1 0 0 0 0 0 0 0 0 0 0\nba d 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1\nba d 2 0 1 1 0 1 1 0 1 0 0 1 0\n0 1 1 0 1 1 0 1 0 0 1 0 ba d 3 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1\n(69)\nThe nuclear norm of each individual bad block is \u221a\n3 2\u03b1\u03b2nm, and because the blocks are chosen\nindependently of each other, the nuclear norm will be approximately additive across blocks. In addition, including a given bad block increases \u3008A\u0303 \u2212 12J,M\u3009 by 34\u03b1\u03b2nm. In contrast, including the good block increases the nuclear norm by \u221a \u03b1\u03b2nm and only increases the objective by 12\u03b1\u03b2nm. The bad blocks therefore all give more \u201cbang for the buck\u201d in terms of how much they increase the objective vs. how much much they increase the nuclear norm, so we will add them before the good block.\nTo accomodate all these bad blocks, we need to allow \u2016M\u2016\u2217 to be at least roughly 13\u03b2 ( 1 \u03b1 \u2212 1 ) \u00d7 \u221a\n3 2\u03b1\u03b2nm = \u2126\n(\n1 \u03b1\u03b2\n\u221a \u03b1\u03b2nm )\n, which is adds an extra factor of 1\u03b2 relative to when we constrain\nthe column sum. The issue can be seen in the above construction in (69): if we do not normalize the rows, then the rows controlled by adversaries can exert disproportionate influence (up to a factor of 1 \u03b2 ) by creating columns that are much denser than those of the reliable raters."}], "references": [{"title": "Community detection in general stochastic block models: fundamental limits and efficient recovery", "author": ["E. Abbe", "C. Sandon"], "venue": null, "citeRegEx": "Abbe and Sandon.,? \\Q2015\\E", "shortCiteRegEx": "Abbe and Sandon.", "year": 2015}, {"title": "Multisection in the stochastic block model using semidefinite programming", "author": ["N. Agarwal", "A.S. Bandeira", "K. Koiliaris", "A. Kolla"], "venue": null, "citeRegEx": "Agarwal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2015}, {"title": "Information-theoretic thresholds for community detection in sparse networks", "author": ["J. Banks", "C. Moore"], "venue": null, "citeRegEx": "Banks and Moore.,? \\Q2016\\E", "shortCiteRegEx": "Banks and Moore.", "year": 2016}, {"title": "Robust and computationally feasible community detection in the presence of arbitrary outlier nodes", "author": ["T.T. Cai", "X. Li"], "venue": "The Annals of Statistics,", "citeRegEx": "Cai and Li.,? \\Q2015\\E", "shortCiteRegEx": "Cai and Li.", "year": 2015}, {"title": "Improved graph clustering", "author": ["Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Stochastic block model and community detection in the sparse graphs: A spectral algorithm with optimal rate of recovery", "author": ["P. Chin", "A. Rao", "V. Vu"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Chin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chin et al\\.", "year": 2015}, {"title": "Provably manipulation-resistant reputation systems", "author": ["P. Christiano"], "venue": "arXiv,", "citeRegEx": "Christiano.,? \\Q2014\\E", "shortCiteRegEx": "Christiano.", "year": 2014}, {"title": "Robust collaborative online learning. arXiv, 2016", "author": ["P. Christiano"], "venue": null, "citeRegEx": "Christiano.,? \\Q2016\\E", "shortCiteRegEx": "Christiano.", "year": 2016}, {"title": "Coloring semirandom graphs optimally. Automata, Languages and Programming, pages 71\u2013100", "author": ["A. Coja-Oghlan"], "venue": null, "citeRegEx": "Coja.Oghlan.,? \\Q2004\\E", "shortCiteRegEx": "Coja.Oghlan.", "year": 2004}, {"title": "Solving NP-hard semirandom graph problems in polynomial expected time", "author": ["A. Coja-Oghlan"], "venue": "Journal of Algorithms,", "citeRegEx": "Coja.Oghlan.,? \\Q2007\\E", "shortCiteRegEx": "Coja.Oghlan.", "year": 2007}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R.M. Karp"], "venue": "Random Structures and Algorithms,", "citeRegEx": "Condon and Karp.,? \\Q2001\\E", "shortCiteRegEx": "Condon and Karp.", "year": 2001}, {"title": "Crowdsourced judgement elicitation with endogenous proficiency", "author": ["A. Dasgupta", "A. Ghosh"], "venue": "In World Wide Web (WWW),", "citeRegEx": "Dasgupta and Ghosh.,? \\Q2013\\E", "shortCiteRegEx": "Dasgupta and Ghosh.", "year": 2013}, {"title": "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications", "author": ["A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborov\u00e1"], "venue": "Physical Review E,", "citeRegEx": "Decelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Decelle et al\\.", "year": 2011}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Heuristics for semirandom graph problems", "author": ["U. Feige", "J. Kilian"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Feige and Kilian.,? \\Q2001\\E", "shortCiteRegEx": "Feige and Kilian.", "year": 2001}, {"title": "Finding and certifying a large hidden clique in a semirandom graph", "author": ["U. Feige", "R. Krauthgamer"], "venue": "Random Structures and Algorithms,", "citeRegEx": "Feige and Krauthgamer.,? \\Q2000\\E", "shortCiteRegEx": "Feige and Krauthgamer.", "year": 2000}, {"title": "Community detection in sparse networks via Grothendieck\u2019s", "author": ["O. Gu\u00e9don", "R. Vershynin"], "venue": "inequality. arXiv,", "citeRegEx": "Gu\u00e9don and Vershynin.,? \\Q2014\\E", "shortCiteRegEx": "Gu\u00e9don and Vershynin.", "year": 2014}, {"title": "Amazon glitch unmasks war of reviewers", "author": ["A. Harmon"], "venue": "New York Times,", "citeRegEx": "Harmon.,? \\Q2004\\E", "shortCiteRegEx": "Harmon.", "year": 2004}, {"title": "Stochastic blockmodels: Some first steps", "author": ["P.W. Holland", "K.B. Laskey", "S. Leinhardt"], "venue": "Social Networks,", "citeRegEx": "Holland et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Holland et al\\.", "year": 1983}, {"title": "Truth serums for massively crowdsourced evaluation", "author": ["V. Kamble", "N. Shah", "D. Marn", "A. Parekh", "K. Ramachandran"], "venue": "tasks. arXiv,", "citeRegEx": "Kamble et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kamble et al\\.", "year": 2015}, {"title": "Budget-optimal task allocation for reliable crowdsourcing systems", "author": ["D.R. Karger", "S. Oh", "D. Shah"], "venue": "Operations Research,", "citeRegEx": "Karger et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2014}, {"title": "Semirandom models as benchmarks for coloring algorithms", "author": ["M. Krivelevich", "D. Vilenchik"], "venue": "In Meeting on Analytic Algorithmics and Combinatorics,", "citeRegEx": "Krivelevich and Vilenchik.,? \\Q2006\\E", "shortCiteRegEx": "Krivelevich and Vilenchik.", "year": 2006}, {"title": "Concentration and regularization of random graphs", "author": ["K. Makarychev", "Y. Makarychev", "A. Vijayaraghavan"], "venue": "self assessment in massive online classes. Design Thinking Research, pages 131\u2013168,", "citeRegEx": "Makarychev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Makarychev et al\\.", "year": 2015}, {"title": "Stochastic block models and reconstruction", "author": ["E. Mossel", "J. Neeman", "A. Sly"], "venue": "arXiv,", "citeRegEx": "2015", "shortCiteRegEx": "2015", "year": 2012}, {"title": "A proof of the block model threshold conjecture", "author": ["E. Mossel", "J. Neeman", "A. Sly"], "venue": null, "citeRegEx": "Mossel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mossel et al\\.", "year": 2013}, {"title": "The influence limiter: provably manipulation-resistant recommender systems", "author": ["P. Resnick", "R. Sami"], "venue": "International ACM Conference on Supporting Group Work,", "citeRegEx": "Resnick and Sami.,? \\Q2007\\E", "shortCiteRegEx": "Resnick and Sami.", "year": 2007}], "referenceMentions": [{"referenceID": 17, "context": ", 2015), aggregation of customer reviews (Harmon, 2004), and the generation/curation of large datasets (Deng et al.", "startOffset": 41, "endOffset": 55}, {"referenceID": 13, "context": ", 2015), aggregation of customer reviews (Harmon, 2004), and the generation/curation of large datasets (Deng et al., 2009).", "startOffset": 103, "endOffset": 122}, {"referenceID": 17, "context": "This is particularly relevant when raters have an incentive to collude and cheat as in the setting of peer grading, as well as reviews on sites like Amazon and Yelp, where artists and firms are incentivized to manufacture positive reviews for their own products and negative reviews for their rivals (Harmon, 2004; Mayzlin et al., 2012).", "startOffset": 300, "endOffset": 336}, {"referenceID": 18, "context": "Our technical tools draw on semidefinite programming methods for matrix completion, which have been used to study graph clustering as well as community detection in the stochastic block model (Holland et al., 1983; Condon and Karp, 2001).", "startOffset": 192, "endOffset": 237}, {"referenceID": 10, "context": "Our technical tools draw on semidefinite programming methods for matrix completion, which have been used to study graph clustering as well as community detection in the stochastic block model (Holland et al., 1983; Condon and Karp, 2001).", "startOffset": 192, "endOffset": 237}, {"referenceID": 12, "context": "Our setting corresponds to the sparse case where all nodes have constant degree, which has recently seen great interest (Decelle et al., 2011; Mossel et al., 2012; 2013b;a; Massouli\u00e9, 2014; Gu\u00e9don and Vershynin, 2014; Mossel et al., 2015; Chin et al., 2015; Abbe and Sandon, 2015; Makarychev et al., 2015).", "startOffset": 120, "endOffset": 305}, {"referenceID": 16, "context": "Our setting corresponds to the sparse case where all nodes have constant degree, which has recently seen great interest (Decelle et al., 2011; Mossel et al., 2012; 2013b;a; Massouli\u00e9, 2014; Gu\u00e9don and Vershynin, 2014; Mossel et al., 2015; Chin et al., 2015; Abbe and Sandon, 2015; Makarychev et al., 2015).", "startOffset": 120, "endOffset": 305}, {"referenceID": 5, "context": "Our setting corresponds to the sparse case where all nodes have constant degree, which has recently seen great interest (Decelle et al., 2011; Mossel et al., 2012; 2013b;a; Massouli\u00e9, 2014; Gu\u00e9don and Vershynin, 2014; Mossel et al., 2015; Chin et al., 2015; Abbe and Sandon, 2015; Makarychev et al., 2015).", "startOffset": 120, "endOffset": 305}, {"referenceID": 0, "context": "Our setting corresponds to the sparse case where all nodes have constant degree, which has recently seen great interest (Decelle et al., 2011; Mossel et al., 2012; 2013b;a; Massouli\u00e9, 2014; Gu\u00e9don and Vershynin, 2014; Mossel et al., 2015; Chin et al., 2015; Abbe and Sandon, 2015; Makarychev et al., 2015).", "startOffset": 120, "endOffset": 305}, {"referenceID": 22, "context": "Our setting corresponds to the sparse case where all nodes have constant degree, which has recently seen great interest (Decelle et al., 2011; Mossel et al., 2012; 2013b;a; Massouli\u00e9, 2014; Gu\u00e9don and Vershynin, 2014; Mossel et al., 2015; Chin et al., 2015; Abbe and Sandon, 2015; Makarychev et al., 2015).", "startOffset": 120, "endOffset": 305}, {"referenceID": 15, "context": "Several authors have considered semirandom settings for graph clustering, which allow for some types of adversarial behavior (Feige and Krauthgamer, 2000; Feige and Kilian, 2001; Coja-Oghlan, 2004; Krivelevich and Vilenchik, 2006; Coja-Oghlan, 2007; Makarychev et al., 2012; Chen et al., 2014; Gu\u00e9don and Vershynin, 2014; Moitra et al., 2015; Agarwal et al., 2015).", "startOffset": 125, "endOffset": 364}, {"referenceID": 14, "context": "Several authors have considered semirandom settings for graph clustering, which allow for some types of adversarial behavior (Feige and Krauthgamer, 2000; Feige and Kilian, 2001; Coja-Oghlan, 2004; Krivelevich and Vilenchik, 2006; Coja-Oghlan, 2007; Makarychev et al., 2012; Chen et al., 2014; Gu\u00e9don and Vershynin, 2014; Moitra et al., 2015; Agarwal et al., 2015).", "startOffset": 125, "endOffset": 364}, {"referenceID": 8, "context": "Several authors have considered semirandom settings for graph clustering, which allow for some types of adversarial behavior (Feige and Krauthgamer, 2000; Feige and Kilian, 2001; Coja-Oghlan, 2004; Krivelevich and Vilenchik, 2006; Coja-Oghlan, 2007; Makarychev et al., 2012; Chen et al., 2014; Gu\u00e9don and Vershynin, 2014; Moitra et al., 2015; Agarwal et al., 2015).", "startOffset": 125, "endOffset": 364}, {"referenceID": 21, "context": "Several authors have considered semirandom settings for graph clustering, which allow for some types of adversarial behavior (Feige and Krauthgamer, 2000; Feige and Kilian, 2001; Coja-Oghlan, 2004; Krivelevich and Vilenchik, 2006; Coja-Oghlan, 2007; Makarychev et al., 2012; Chen et al., 2014; Gu\u00e9don and Vershynin, 2014; Moitra et al., 2015; Agarwal et al., 2015).", "startOffset": 125, "endOffset": 364}, {"referenceID": 9, "context": "Several authors have considered semirandom settings for graph clustering, which allow for some types of adversarial behavior (Feige and Krauthgamer, 2000; Feige and Kilian, 2001; Coja-Oghlan, 2004; Krivelevich and Vilenchik, 2006; Coja-Oghlan, 2007; Makarychev et al., 2012; Chen et al., 2014; Gu\u00e9don and Vershynin, 2014; Moitra et al., 2015; Agarwal et al., 2015).", "startOffset": 125, "endOffset": 364}, {"referenceID": 4, "context": "Several authors have considered semirandom settings for graph clustering, which allow for some types of adversarial behavior (Feige and Krauthgamer, 2000; Feige and Kilian, 2001; Coja-Oghlan, 2004; Krivelevich and Vilenchik, 2006; Coja-Oghlan, 2007; Makarychev et al., 2012; Chen et al., 2014; Gu\u00e9don and Vershynin, 2014; Moitra et al., 2015; Agarwal et al., 2015).", "startOffset": 125, "endOffset": 364}, {"referenceID": 16, "context": "Several authors have considered semirandom settings for graph clustering, which allow for some types of adversarial behavior (Feige and Krauthgamer, 2000; Feige and Kilian, 2001; Coja-Oghlan, 2004; Krivelevich and Vilenchik, 2006; Coja-Oghlan, 2007; Makarychev et al., 2012; Chen et al., 2014; Gu\u00e9don and Vershynin, 2014; Moitra et al., 2015; Agarwal et al., 2015).", "startOffset": 125, "endOffset": 364}, {"referenceID": 1, "context": "Several authors have considered semirandom settings for graph clustering, which allow for some types of adversarial behavior (Feige and Krauthgamer, 2000; Feige and Kilian, 2001; Coja-Oghlan, 2004; Krivelevich and Vilenchik, 2006; Coja-Oghlan, 2007; Makarychev et al., 2012; Chen et al., 2014; Gu\u00e9don and Vershynin, 2014; Moitra et al., 2015; Agarwal et al., 2015).", "startOffset": 125, "endOffset": 364}, {"referenceID": 0, "context": ", 2015; Abbe and Sandon, 2015; Makarychev et al., 2015). Makarychev et al. (2015) in particular provide an algorithm that is robust to adversarial perturbations, but only if the perturbation has size o(n); see also Cai and Li (2015) for robusness results when the node degree is logarithmic.", "startOffset": 8, "endOffset": 82}, {"referenceID": 0, "context": ", 2015; Abbe and Sandon, 2015; Makarychev et al., 2015). Makarychev et al. (2015) in particular provide an algorithm that is robust to adversarial perturbations, but only if the perturbation has size o(n); see also Cai and Li (2015) for robusness results when the node degree is logarithmic.", "startOffset": 8, "endOffset": 233}, {"referenceID": 4, "context": "For instance, (Chen et al., 2014) place no constraint on the sum of each row in M (instead of recovering the \u03b2-quantile, they normalize \u00c3 to lie in [\u22121, 1] and recover the items with a positive rating).", "startOffset": 14, "endOffset": 33}, {"referenceID": 2, "context": ", in loose analogy to recent results for the stochastic block model (Banks and Moore, 2016).", "startOffset": 68, "endOffset": 91}, {"referenceID": 25, "context": "While several mechanisms have been proposed for these tasks, they typically assume that rater accuracy is observable online (Resnick and Sami, 2007), that raters are rational agents maximizing a payoff function (Dasgupta and Ghosh, 2013; Kamble et al.", "startOffset": 124, "endOffset": 148}, {"referenceID": 11, "context": "While several mechanisms have been proposed for these tasks, they typically assume that rater accuracy is observable online (Resnick and Sami, 2007), that raters are rational agents maximizing a payoff function (Dasgupta and Ghosh, 2013; Kamble et al., 2015; Shnayder et al., 2016), that the workers follow a simple statistical model (Karger et al.", "startOffset": 211, "endOffset": 281}, {"referenceID": 19, "context": "While several mechanisms have been proposed for these tasks, they typically assume that rater accuracy is observable online (Resnick and Sami, 2007), that raters are rational agents maximizing a payoff function (Dasgupta and Ghosh, 2013; Kamble et al., 2015; Shnayder et al., 2016), that the workers follow a simple statistical model (Karger et al.", "startOffset": 211, "endOffset": 281}, {"referenceID": 20, "context": ", 2016), that the workers follow a simple statistical model (Karger et al., 2014; Zhang et al., 2014; Zhou et al., 2015), or some combination of the above (Shah and Zhou, 2015; Shah et al.", "startOffset": 60, "endOffset": 120}, {"referenceID": 23, "context": "1 Matrix Concentration Bound of Le and Vershynin (2015) For ease of reference, here we state the matrix concentration bound from Le and Vershynin (2015), which we make use of in the proofs below.", "startOffset": 50, "endOffset": 153}, {"referenceID": 23, "context": "1 Matrix Concentration Bound of Le and Vershynin (2015) For ease of reference, here we state the matrix concentration bound from Le and Vershynin (2015), which we make use of in the proofs below. Theorem 3 (Theorem 2.1 in Le and Vershynin (2015)).", "startOffset": 50, "endOffset": 246}], "year": 2016, "abstractText": "We consider a crowdsourcing model in which n workers are asked to rate the quality of n items previously generated by other workers. An unknown set of \u03b1n workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an \u01eb fraction of lowquality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with n: the dataset can be curated with \u00d5 (", "creator": "LaTeX with hyperref package"}}}