{"id": "1606.05491", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings", "abstract": "We more a material language generator similar on the sequence - if - sequence making still can could employed to unlike natural language chords one taken from deep syntax dependency vine from required understanding role, and we unlike example because generally compare those - means use with forming counts process however surface urgency successive to no delegation, one - step innovative. We were should though where addition setups successfully also hard 've training devices. The adding basically offers n't performance, doubled california - it - the - exhibit behind regards to r - grams - corp. showed while providing more relevant playback.", "histories": [["v1", "Fri, 17 Jun 2016 11:51:25 GMT  (116kb,D)", "http://arxiv.org/abs/1606.05491v1", "Accepted as a short paper for ACL 2016"]], "COMMENTS": "Accepted as a short paper for ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ond\\v{r}ej du\\v{s}ek", "filip jur\\v{c}\\'i\\v{c}ek"], "accepted": true, "id": "1606.05491"}, "pdf": {"name": "1606.05491.pdf", "metadata": {"source": "CRF", "title": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings", "authors": ["Ond\u0159ej Du\u0161ek"], "emails": ["odusek@ufal.mff.cuni.cz", "jurcicek@ufal.mff.cuni.cz"], "sections": [{"heading": null, "text": "We were able to train both setups successfully using very little training data. The joint setup offers better performance, surpassing state-of-the-art with regards to ngram-based scores while providing more relevant outputs."}, {"heading": "1 Introduction", "text": "In spoken dialogue systems (SDS), the task of natural language generation (NLG) is to convert a meaning representation (MR) produced by the dialogue manager into one or more sentences in a natural language. It is traditionally divided into two subtasks: sentence planning, which decides on the overall sentence structure, and surface realization, determining the exact word forms and linearizing the structure into a string (Reiter and Dale, 2000). While some generators keep this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).\nWe present a new, conceptually simple NLG system for SDS that is able to operate in both modes: it either produces natural language strings or generates deep syntax dependency trees, which are subsequently processed by an external surface realizer (Dus\u030cek et al., 2015). This allows us to show a direct comparison of two-step generation,\nwhere sentence planning and surface realization are separated, with a joint, one-step approach.\nOur generator is based on the sequence-tosequence (seq2seq) generation technique (Cho et al., 2014; Sutskever et al., 2014), combined with beam search and an n-best list reranker to suppress irrelevant information in the outputs. Unlike most previous NLG systems for SDS (e.g., (Stent et al., 2004; Raux et al., 2005; Mairesse et al., 2010)), it is trainable from unaligned pairs of MR and sentences alone. We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (Wen et al., 2015b; Mei et al., 2015), and we find that our generator learns successfully to produce both strings and deep syntax trees on the BAGEL restaurant information dataset (Mairesse et al., 2010). It is able to surpass n-gram-based scores achieved previously by Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2015), offering a simpler setup and more relevant outputs.\nWe introduce the generation setting in Section 2 and describe our generator architecture in Section 3. Section 4 details our experiments, Section 5 analyzes the results. We summarize related work in Section 6 and offer conclusions in Section 7."}, {"heading": "2 Generator Setting", "text": "The input to our generator are dialogue acts (DA) (Young et al., 2010) representing an action, such as inform or request, along with one or more attributes (slots) and their values. Our generator operates in two modes, producing either deep syntax trees (Dus\u030cek et al., 2012) or natural language strings (see Fig. 1). The first mode corresponds to the sentence planning NLG stage as it decides the syntactic shape of the output sentence; the resulting deep syntax tree involves content words (lemmas) and their syntactic form (formemes, purple in Fig. 1). The trees are linearized to strings using a\nar X\niv :1\n60 6.\n05 49\n1v 1\n[ cs\n.C L\n] 1\n7 Ju\nn 20\n16\nsurface realizer from the TectoMT translation system (Dus\u030cek et al., 2015). The second generator mode joins sentence planning and surface realization into one step, producing natural language sentences directly.\nBoth modes offer their advantages: The twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times (Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek, 2015), and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline (Konstas and Lapata, 2013)."}, {"heading": "3 The Seq2seq Generation Model", "text": "Our generator is based on the seq2seq approach (Cho et al., 2014; Sutskever et al., 2014), a type of an encoder-decoder RNN architecture operating on variable-length sequences of tokens. We address the necessary conversion of input DA and output trees/sentences into sequences in Section 3.1 and then describe the main seq2seq component in Section 3.2. It is supplemented by a reranker, as explained in Section 3.3."}, {"heading": "3.1 Sequence Representation of DA, Trees, and Sentences", "text": "We represent DA, deep syntax trees, and sentences as sequences of tokens to enable their usage in the sequence-based RNN components of our generator (see Sections 3.2 and 3.3). Each token is represented by its embedding \u2013 a vector of floatingpoint numbers (Bengio et al., 2003).\nTo form a sequence representation of a DA, we create a triple of the structure \u201cDA type, slot, value\u201d for each slot in the DA and concatenate\nthe triples (see Fig. 3). The deep syntax tree output from the seq2seq generator is represented in a bracketed notation similar to the one used by Vinyals et al. (2015, see Fig. 2). The inputs to the reranker are always a sequence of tokens; structure is disregarded in trees, resulting in a list of lemma-formeme pairs (see Fig. 2)."}, {"heading": "3.2 Seq2seq Generator", "text": "Our seq2seq generator with attention (Bahdanau et al., 2015, see Fig. 3)1 starts with the encoder stage, which uses an RNN to encode an input sequence x = {x1, . . . , xn} into a sequence of encoder outputs and hidden states h = {h1, . . . , hn}, where ht = lstm(xt, ht\u22121), a non-linear function represented by the long-short-term memory (LSTM) cell (Graves, 2013).\nThe decoder stage then uses the hidden states to generate a sequence y = {y1, . . . , ym} with a second LSTM-based RNN. The probability of each output token is defined as:\np(yt|y1, . . . , yt\u22121,x) = softmax((st \u25e6 ct)WY )\nHere, st is the decoder state where s0 = hn and st = lstm((yt\u22121 \u25e6 ct)WS , st\u22121), i.e., the decoder is initialized by the last hidden state and uses the previous output token at each step. WY and WS are learned linear projection matrices and \u201c\u25e6\u201d denotes concatenation. ct is the context vector \u2013 a weighted sum of the encoder hidden states ct = \u2211n i=1 \u03b1tihi, where \u03b1ti corresponds to an alignment model, represented by a feed-forward network with a single tanh hidden layer.\nOn top of this basic seq2seq model, we implemented a simple beam search for decoding (Sutskever et al., 2014; Bahdanau et al., 2015). It proceeds left-to-right and keeps track of log probabilities of top n possible output sequences, expanding them one token at a time."}, {"heading": "3.3 Reranker", "text": "To ensure that the output trees/strings correspond semantically to the input DA, we implemented a classifier to rerank the n-best beam search outputs and penalize those missing required information and/or adding irrelevant one. Similarly to Wen et al. (2015a), the classifier provides a binary decision for an output tree/string on the presence of all dialogue act types and slot-value combinations seen in the training data, producing a 1-hot vector.\n1We use the implementation in the TensorFlow framework (Abadi et al., 2015).\nThe input DA is converted to a similar 1-hot vector and the reranking penalty of the sentence is the Hamming distance between the two vectors (see Fig. 4). Weighted penalties for all sentences are subtracted from their n-best list log probabilities.\nWe employ a similar architecture for the classifier as in our seq2seq generator encoder (see Section 3.2), with an RNN encoder operating on the output trees/strings and a single logistic layer for classification over the last encoder hidden state. Given an output sequence representing a string or a tree y = {y1, . . . , yn} (cf. Section 3.1), the encoder again produces a sequence of hidden states h = {h1, . . . , hn} where ht = lstm(yt, ht\u22121). The output binary vector o is computed as: oi = sigmoid((hn \u00b7WR + b)i) Here, WR is a learned projection matrix and b is a corresponding bias term."}, {"heading": "4 Experiments", "text": "We perform our experiments on the BAGEL data set of Mairesse et al. (2010), which contains 202 DA from the restaurant information domain with two natural language paraphrases each, describing restaurant locations, price ranges, food types etc. Some properties such as restaurant names or phone numbers are delexicalized (replaced with \u201cX\u201d symbols) to avoid data sparsity.2Unlike Mairesse et al. (2010), we do not use\n2We adopt the delexicalization scenario used by Mairesse et al. (2010) and Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2015).\nmanually annotated alignment of slots and values in the input DA to target words and phrases and let the generator learn it from data, which simplifies training data preparation but makes our task harder. We lowercase the data and treat plural -s as separate tokens for generating into strings, and we apply automatic analysis from the Treex NLP toolkit (Popel and Z\u030cabokrtsky\u0301, 2010) to obtain deep syntax trees for training tree-based generator setups.3 Same as Mairesse et al. (2010), we apply 10-fold cross-validation, with 181 training DA and 21 testing DA. In addition, we reserve 10 DA from the training set for validation.4\nTo train our seq2seq generator, we use the Adam optimizer (Kingma and Ba, 2015) to minimize unweighted sequence cross-entropy.5 We perform 10 runs with different random initialization of the network and up to 1,000 passes over the training data,6 validating after each pass and selecting the parameters that yield the highest BLEU score on the validation set. Neither beam search nor the reranker are used for validation.\nWe use the Adam optimizer minimizing crossentropy to train the reranker as well.7 We perform a single run of up to 100 passes over the data, and we also validate after each pass and select the parameters giving minimal Hamming distance on both validation and training set.8\n3The input vocabulary size is around 45 (DA types, slots, and values added up) and output vocabulary sizes are around 170 for string generation and 180 for tree generation (45 formemes and 135 lemmas).\n4We treat the two paraphrases for the same DA as separate instances in the training set but use them together as two references to measure BLEU and NIST scores (Papineni et al., 2002; Doddington, 2002) on the validation and test sets.\n5Based on a few preliminary experiments, the learning rate is set to 0.001, embedding size 50, LSTM cell size 128, and batch size 20. Reranking penalty for decoding is 100.\n6Training is terminated early if the top 10 so far achieved validation BLEU scores do not change for 100 passes.\n7We use the same settings as with the seq2seq generator. 8The validation set is given 10 times more importance."}, {"heading": "5 Results", "text": "The results of our experiments and a comparison to previous works on this dataset are shown in Table 1. We include BLEU and NIST scores and the number of semantic errors (incorrect, missing, and repeated information), which we assessed manually on a sample of 42 output sentences (outputs of two randomly selected cross-validation runs).\nThe outputs of direct string generation show that the models learn to produce fluent sentences in the domain style;9 incoherent sentences are rare, but semantic errors are very frequent in the greedy search. Most errors involve confusion of semantically close items, e.g., Italian instead of French or riverside area instead of city centre (see Table 2); items occurring more frequently are preferred regardless of their relevance. The beam search brings a BLEU improvement but keeps most semantic errors in place. The reranker is able to reduce the number of semantic errors while increasing automatic scores considerably. Using a larger beam increases the effect of the reranker as expected, resulting in slightly improved outputs.\nModels generating deep syntax trees are also able to learn the domain style, and they have virtually no problems producing valid trees.10 The surface realizer works almost flawlessly on this lim-\n9The average sentence length is around 13 tokens. 10The generated sequences are longer, but have a very rigid structure, i.e., less uncertainty per generation step. The average output length is around 36 tokens in the generated sequence or 9 tree nodes; surface realizer outputs have a similar length as the sentences produced in direct string generation.\nited domain (Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek, 2015), leaving the seq2seq generator as the major error source. The syntax-generating models tend to make different kinds of errors than the string-based models: Some outputs are valid trees but not entirely syntactically fluent; missing, incorrect, or repeated information is more frequent than a confusion of semantically similar items (see Table 2). Semantic error rates of greedy and beam-search decoding are lower than for string-based models, partly because confusion of two similar items counts as two errors. The beam search brings an increase in BLEU but also in the number of semantic errors. The reranker is able to reduce the number of errors and improve automatic scores slightly. A larger beam leads to a small BLEU decrease even though the sentences contain less errors; here, NIST reflects the situation more accurately.\nA comparison of the two approaches goes in favor of the joint setup: Without the reranker, models generating trees produce less semantic errors and gain higher BLEU/NIST scores. However, with the reranker, the string-based model is able to reduce the number of semantic errors while producing outputs significantly better in terms of BLEU/NIST.11 In addition, the joint setup does not need an external surface realizer. The best results of both setups surpass the best results on this dataset using training data without manual alignments (Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek, 2015) in both automatic metrics12 and the number of semantic errors."}, {"heading": "6 Related Work", "text": "While most recent NLG systems attempt to learn generation from data, the choice of a particular approach \u2013 pipeline or joint \u2013 is often arbitrary and depends on system architecture or particular generation domain. Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek, 2015). Generators taking the joint approach employ various methods, e.g., factored language models (Mairesse et al., 2010), inverted parsing (Wong and Mooney, 2007; Konstas and Lapata, 2013), or a pipeline of discriminative classifiers (Angeli et al., 2010). Unlike most previous\n11The difference is statistically significant at 99% level according to pairwise bootstrap resampling test (Koehn, 2004).\n12The BLEU/NIST differences are statistically significant.\nNLG systems, our generator is trainable from unaligned pairs of MR and sentences alone.\nRecent RNN-based generators are most similar to our work. Wen et al. (2015a) combined two RNN with a convolutional network reranker; Wen et al. (2015b) later replaced basic sigmoid cells with an LSTM. Mei et al. (2015) present the only seq2seq-based NLG system known to us. We extend the previous works by generating deep syntax trees as well as strings and directly comparing pipeline and joint generation. In addition, we experiment with an order-of-magnitude smaller dataset than other RNN-based systems."}, {"heading": "7 Conclusions and Future Work", "text": "We have presented a direct comparison of two-step generation via deep syntax trees with a direct generation into strings, both using the same NLG system based on the seq2seq approach. While both approaches offer decent performance, their outputs are quite different. The results show the direct approach as more favorable, with significantly higher n-gram based scores and a similar number of semantic errors in the output.\nWe also showed that our generator can learn to produce meaningful utterances using a much smaller amount of training data than what is typically used for RNN-based approaches. The resulting models had virtually no problems with produc-\ning fluent, coherent sentences or with generating valid structure of bracketed deep syntax trees. Our generator was able to surpass the best BLEU/NIST scores on the same dataset previously achieved by a perceptron-based generator of Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2015) while reducing the amount of irrelevant information on the output.\nOur generator is released on GitHub at the following URL:\nhttps://github.com/UFAL-DSG/tgen\nWe intend to apply it to other datasets for a broader comparison, and we plan further improvements, such as enhancing the reranker or including a bidirectional encoder (Bahdanau et al., 2015; Mei et al., 2015; Jean et al., 2015) and sequence level training (Ranzato et al., 2015)."}, {"heading": "Acknowledgments", "text": "This work was funded by the Ministry of Education, Youth and Sports of the Czech Republic under the grant agreement LK11221 and core research funding, SVV project 260 333, and GAUK grant 2058214 of Charles University in Prague. It used language resources stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2015071). We thank our colleagues and the anonymous reviewers for helpful comments."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["G. Angeli", "P. Liang", "D. Klein."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502\u2013512.", "citeRegEx": "Angeli et al\\.,? 2010", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137\u2013 1155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio."], "venue": "Proceedings of the 2014 Conference", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Conditional Random Fields for Responsive Surface Realisation using Global Features", "author": ["N. Dethlefs", "H. Hastie", "H. Cuay\u00e1huitl", "O. Lemon."], "venue": "Proceedings of ACL, Sofia.", "citeRegEx": "Dethlefs et al\\.,? 2013", "shortCiteRegEx": "Dethlefs et al\\.", "year": 2013}, {"title": "Automatic Evaluation of Machine Translation Quality Using N-gram Cooccurrence Statistics", "author": ["G. Doddington."], "venue": "Proceedings of the Second International Conference on Human Language Technology Research, pages 138\u2013145, San Fran-", "citeRegEx": "Doddington.,? 2002", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "Training a Natural Language Generator From Unaligned Data", "author": ["O. Du\u0161ek", "F. Jur\u010d\u0131\u0301\u010dek"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "citeRegEx": "Du\u0161ek and Jur\u010d\u0131\u0301\u010dek.,? \\Q2015\\E", "shortCiteRegEx": "Du\u0161ek and Jur\u010d\u0131\u0301\u010dek.", "year": 2015}, {"title": "Formemes in English-Czech Deep Syntactic MT", "author": ["Ond\u0159ej Du\u0161ek", "Zden\u011bk \u017dabokrtsk\u00fd", "Martin Popel", "Martin Majli\u0161", "Michal Nov\u00e1k", "David Mare\u010dek."], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 267\u2013274,", "citeRegEx": "Du\u0161ek et al\\.,? 2012", "shortCiteRegEx": "Du\u0161ek et al\\.", "year": 2012}, {"title": "New Language Pairs in TectoMT", "author": ["O. Du\u0161ek", "L. Gomes", "M. Nov\u00e1k", "M. Popel", "R. Rosa."], "venue": "Proceedings of the 10th Workshop on Machine Translation, pages 98\u2013104, Lisbon, Portugal. Association for Computational Linguistics.", "citeRegEx": "Du\u0161ek et al\\.,? 2015", "shortCiteRegEx": "Du\u0161ek et al\\.", "year": 2015}, {"title": "Generating Sequences With Recurrent Neural Networks", "author": ["A. Graves."], "venue": "arXiv:1308.0850 [cs], August.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba."], "venue": "International Conference on Learning Representations. arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["P. Koehn."], "venue": "Proceedings of EMNLP, pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "A Global Model for Concept-to-Text Generation", "author": ["I. Konstas", "M. Lapata."], "venue": "Journal of Artificial Intelligence Research, 48:305\u2013346.", "citeRegEx": "Konstas and Lapata.,? 2013", "shortCiteRegEx": "Konstas and Lapata.", "year": 2013}, {"title": "Phrase-based statistical language generation using graphical models and active learning", "author": ["F. Mairesse", "M. Ga\u0161i\u0107", "F. Jur\u010d\u0131\u0301\u010dek", "S. Keizer", "B. Thomson", "K. Yu", "S. Young"], "venue": "In Proceedings of the 48th Annual Meeting of the Association", "citeRegEx": "Mairesse et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairesse et al\\.", "year": 2010}, {"title": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment", "author": ["H. Mei", "M. Bansal", "M.R. Walter."], "venue": "arXiv:1509.00838 [cs], September.", "citeRegEx": "Mei et al\\.,? 2015", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "Empirically-based control of natural language generation", "author": ["D.S. Paiva", "R. Evans."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL \u201905, pages 58\u201365, Stroudsburg, PA, USA. Association for Computa-", "citeRegEx": "Paiva and Evans.,? 2005", "shortCiteRegEx": "Paiva and Evans.", "year": 2005}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu."], "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "TectoMT: modular NLP framework", "author": ["M. Popel", "Z. \u017dabokrtsk\u00fd."], "venue": "Proceedings of IceTAL, 7th International Conference on Natural Language Processing,, pages 293\u2013304, Reykjav\u0131\u0301k.", "citeRegEx": "Popel and \u017dabokrtsk\u00fd.,? 2010", "shortCiteRegEx": "Popel and \u017dabokrtsk\u00fd.", "year": 2010}, {"title": "Sequence Level Training with Recurrent Neural Networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba."], "venue": "arXiv:1511.06732 [cs], November.", "citeRegEx": "Ranzato et al\\.,? 2015", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Let\u2019s go public! taking a spoken dialog system to the real world", "author": ["A. Raux", "B. Langner", "D. Bohus", "A.W. Black", "M. Eskenazi."], "venue": "in Proc. of Interspeech 2005. Citeseer.", "citeRegEx": "Raux et al\\.,? 2005", "shortCiteRegEx": "Raux et al\\.", "year": 2005}, {"title": "Building Natural Language Generation Systems", "author": ["E. Reiter", "R. Dale."], "venue": "Cambridge University Press, studies in natural language processing edition.", "citeRegEx": "Reiter and Dale.,? 2000", "shortCiteRegEx": "Reiter and Dale.", "year": 2000}, {"title": "Optimising information presentation for spoken dialogue systems", "author": ["V. Rieser", "O. Lemon", "X. Liu."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009\u20131018.", "citeRegEx": "Rieser et al\\.,? 2010", "shortCiteRegEx": "Rieser et al\\.", "year": 2010}, {"title": "Trainable sentence planning for complex information presentation in spoken dialog systems", "author": ["A. Stent", "R. Prasad", "M. Walker."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 79\u201386.", "citeRegEx": "Stent et al\\.,? 2004", "shortCiteRegEx": "Stent et al\\.", "year": 2004}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112. arXiv:1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a Foreign Language", "author": ["O. Vinyals", "\u0141. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, R. Garnett, and R. Garnett, editors, Advances in Neural Information Processing Systems", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "SPoT: a trainable sentence planner", "author": ["M.A. Walker", "O. Rambow", "M. Rogati."], "venue": "Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, pages 1\u20138, Stroudsburg,", "citeRegEx": "Walker et al\\.,? 2001", "shortCiteRegEx": "Walker et al\\.", "year": 2001}, {"title": "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking", "author": ["T.-H. Wen", "M. Gasic", "D. Kim", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young."], "venue": "Proceedings of the 16th Annual Meeting", "citeRegEx": "Wen et al\\.,? 2015a", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems", "author": ["T.-H. Wen", "M. Gasic", "N. Mrk\u0161i\u0107", "P.-H. Su", "D. Vandyke", "S. Young."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Wen et al\\.,? 2015b", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Generation by inverting a semantic parser that uses statistical machine translation", "author": ["Y.W. Wong", "R.J. Mooney."], "venue": "Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Compu-", "citeRegEx": "Wong and Mooney.,? 2007", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management", "author": ["S. Young", "M. Ga\u0161i\u0107", "S. Keizer", "F. Mairesse", "J. Schatzmann", "B. Thomson", "K. Yu."], "venue": "Computer Speech & Language, 24(2):150\u2013", "citeRegEx": "Young et al\\.,? 2010", "shortCiteRegEx": "Young et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 21, "context": "It is traditionally divided into two subtasks: sentence planning, which decides on the overall sentence structure, and surface realization, determining the exact word forms and linearizing the structure into a string (Reiter and Dale, 2000).", "startOffset": 217, "endOffset": 240}, {"referenceID": 26, "context": "While some generators keep this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).", "startOffset": 69, "endOffset": 134}, {"referenceID": 22, "context": "While some generators keep this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).", "startOffset": 69, "endOffset": 134}, {"referenceID": 4, "context": "While some generators keep this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).", "startOffset": 69, "endOffset": 134}, {"referenceID": 29, "context": ", 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).", "startOffset": 51, "endOffset": 100}, {"referenceID": 13, "context": ", 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).", "startOffset": 51, "endOffset": 100}, {"referenceID": 8, "context": "We present a new, conceptually simple NLG system for SDS that is able to operate in both modes: it either produces natural language strings or generates deep syntax dependency trees, which are subsequently processed by an external surface realizer (Du\u0161ek et al., 2015).", "startOffset": 248, "endOffset": 268}, {"referenceID": 23, "context": ", (Stent et al., 2004; Raux et al., 2005; Mairesse et al., 2010)), it", "startOffset": 2, "endOffset": 64}, {"referenceID": 20, "context": ", (Stent et al., 2004; Raux et al., 2005; Mairesse et al., 2010)), it", "startOffset": 2, "endOffset": 64}, {"referenceID": 14, "context": ", (Stent et al., 2004; Raux et al., 2005; Mairesse et al., 2010)), it", "startOffset": 2, "endOffset": 64}, {"referenceID": 28, "context": "We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (Wen et al., 2015b; Mei et al., 2015), and we find that our genera-", "startOffset": 110, "endOffset": 147}, {"referenceID": 15, "context": "We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (Wen et al., 2015b; Mei et al., 2015), and we find that our genera-", "startOffset": 110, "endOffset": 147}, {"referenceID": 14, "context": "tor learns successfully to produce both strings and deep syntax trees on the BAGEL restaurant information dataset (Mairesse et al., 2010).", "startOffset": 114, "endOffset": 137}, {"referenceID": 6, "context": "It is able to surpass n-gram-based scores achieved previously by Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015), offering a simpler", "startOffset": 65, "endOffset": 92}, {"referenceID": 30, "context": "The input to our generator are dialogue acts (DA) (Young et al., 2010) representing an action, such as inform or request, along with one or more attributes (slots) and their values.", "startOffset": 50, "endOffset": 70}, {"referenceID": 7, "context": "Our generator operates in two modes, producing either deep syntax trees (Du\u0161ek et al., 2012) or natural language strings (see Fig.", "startOffset": 72, "endOffset": 92}, {"referenceID": 8, "context": "surface realizer from the TectoMT translation system (Du\u0161ek et al., 2015).", "startOffset": 53, "endOffset": 73}, {"referenceID": 6, "context": "away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015), and the joint mode does not need to model", "startOffset": 161, "endOffset": 188}, {"referenceID": 13, "context": "structure explicitly and avoids accumulating errors along the pipeline (Konstas and Lapata, 2013).", "startOffset": 71, "endOffset": 97}, {"referenceID": 3, "context": "Our generator is based on the seq2seq approach (Cho et al., 2014; Sutskever et al., 2014), a type of an encoder-decoder RNN architecture operating on variable-length sequences of tokens.", "startOffset": 47, "endOffset": 89}, {"referenceID": 24, "context": "Our generator is based on the seq2seq approach (Cho et al., 2014; Sutskever et al., 2014), a type of an encoder-decoder RNN architecture operating on variable-length sequences of tokens.", "startOffset": 47, "endOffset": 89}, {"referenceID": 2, "context": "Each token is represented by its embedding \u2013 a vector of floatingpoint numbers (Bengio et al., 2003).", "startOffset": 79, "endOffset": 100}, {"referenceID": 9, "context": "cell (Graves, 2013).", "startOffset": 5, "endOffset": 19}, {"referenceID": 24, "context": "On top of this basic seq2seq model, we implemented a simple beam search for decoding (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 85, "endOffset": 132}, {"referenceID": 1, "context": "On top of this basic seq2seq model, we implemented a simple beam search for decoding (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 85, "endOffset": 132}, {"referenceID": 14, "context": "We perform our experiments on the BAGEL data set of Mairesse et al. (2010), which contains 202 DA from the restaurant information domain with two natural language paraphrases each, de-", "startOffset": 52, "endOffset": 75}, {"referenceID": 14, "context": "2Unlike Mairesse et al. (2010), we do not use", "startOffset": 8, "endOffset": 31}, {"referenceID": 13, "context": "We adopt the delexicalization scenario used by Mairesse et al. (2010) and Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015).", "startOffset": 47, "endOffset": 70}, {"referenceID": 6, "context": "(2010) and Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015). manually annotated alignment of slots and values in the input DA to target words and phrases and let the generator learn it from data, which simplifies training data preparation but makes our task", "startOffset": 11, "endOffset": 38}, {"referenceID": 18, "context": "We lowercase the data and treat plural -s as separate tokens for generating into strings, and we apply automatic analysis from the Treex NLP toolkit (Popel and \u017dabokrtsk\u00fd, 2010) to obtain deep syntax trees for training tree-based generator setups.", "startOffset": 149, "endOffset": 177}, {"referenceID": 14, "context": "3 Same as Mairesse et al. (2010), we apply 10-fold cross-validation, with 181 training DA and 21 testing DA.", "startOffset": 10, "endOffset": 33}, {"referenceID": 11, "context": "To train our seq2seq generator, we use the Adam optimizer (Kingma and Ba, 2015) to minimize unweighted sequence cross-entropy.", "startOffset": 58, "endOffset": 79}, {"referenceID": 17, "context": "We treat the two paraphrases for the same DA as separate instances in the training set but use them together as two references to measure BLEU and NIST scores (Papineni et al., 2002; Doddington, 2002) on the validation and test sets.", "startOffset": 159, "endOffset": 200}, {"referenceID": 5, "context": "We treat the two paraphrases for the same DA as separate instances in the training set but use them together as two references to measure BLEU and NIST scores (Papineni et al., 2002; Doddington, 2002) on the validation and test sets.", "startOffset": 159, "endOffset": 200}, {"referenceID": 13, "context": "Setup BLEU NIST ERR Mairesse et al. (2010)\u2217 \u223c67 - 0 Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015) 59.", "startOffset": 20, "endOffset": 43}, {"referenceID": 6, "context": "(2010)\u2217 \u223c67 - 0 Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015) 59.", "startOffset": 16, "endOffset": 43}, {"referenceID": 14, "context": "\u2217Mairesse et al. (2010) use manual alignments in their work, so their result is not directly comparable to ours.", "startOffset": 1, "endOffset": 24}, {"referenceID": 6, "context": "ited domain (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015), leaving the seq2seq generator as the major error source.", "startOffset": 12, "endOffset": 39}, {"referenceID": 6, "context": "sults of both setups surpass the best results on this dataset using training data without manual alignments (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015) in both automatic metrics12 and the number of semantic errors.", "startOffset": 108, "endOffset": 135}, {"referenceID": 26, "context": "Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015).", "startOffset": 111, "endOffset": 175}, {"referenceID": 23, "context": "Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015).", "startOffset": 111, "endOffset": 175}, {"referenceID": 16, "context": "Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015).", "startOffset": 111, "endOffset": 175}, {"referenceID": 6, "context": ", 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015).", "startOffset": 68, "endOffset": 95}, {"referenceID": 14, "context": ", factored language models (Mairesse et al., 2010), inverted parsing (Wong and Mooney, 2007; Konstas and Lapata, 2013), or a pipeline of discriminative clas-", "startOffset": 27, "endOffset": 50}, {"referenceID": 29, "context": ", 2010), inverted parsing (Wong and Mooney, 2007; Konstas and Lapata, 2013), or a pipeline of discriminative clas-", "startOffset": 26, "endOffset": 75}, {"referenceID": 13, "context": ", 2010), inverted parsing (Wong and Mooney, 2007; Konstas and Lapata, 2013), or a pipeline of discriminative clas-", "startOffset": 26, "endOffset": 75}, {"referenceID": 0, "context": "sifiers (Angeli et al., 2010).", "startOffset": 8, "endOffset": 29}, {"referenceID": 12, "context": "The difference is statistically significant at 99% level according to pairwise bootstrap resampling test (Koehn, 2004).", "startOffset": 105, "endOffset": 118}, {"referenceID": 27, "context": "Wen et al. (2015a) combined two RNN with a convolutional network reranker; Wen et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 27, "context": "Wen et al. (2015a) combined two RNN with a convolutional network reranker; Wen et al. (2015b) later replaced basic sigmoid", "startOffset": 0, "endOffset": 94}, {"referenceID": 15, "context": "Mei et al. (2015) present the only seq2seq-based NLG system known to us.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "scores on the same dataset previously achieved by a perceptron-based generator of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015) while reducing the amount of irrelevant information on the output.", "startOffset": 82, "endOffset": 109}, {"referenceID": 19, "context": ", 2015) and sequence level training (Ranzato et al., 2015).", "startOffset": 36, "endOffset": 58}], "year": 2016, "abstractText": "We present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax dependency trees from input dialogue acts, and we use it to directly compare two-step generation with separate sentence planning and surface realization stages to a joint, one-step approach. We were able to train both setups successfully using very little training data. The joint setup offers better performance, surpassing state-of-the-art with regards to ngram-based scores while providing more relevant outputs.", "creator": "LaTeX with hyperref package"}}}