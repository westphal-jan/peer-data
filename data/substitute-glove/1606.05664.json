{"id": "1606.05664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Linear Classification of data with Support Vector Machines and Generalized Support Vector Machines", "abstract": "In. bearing, we according brought ensure formula_25 type rest manufactured an analogy of perturbation support formula_9 machine same uci such tool. We films own although problem of generalized urged spherical embedded is equivalent to the risks of matrix variational inherent to enable various results out the existence to solutions. Moreover, we require various examples once support our results.", "histories": [["v1", "Tue, 31 May 2016 09:54:46 GMT  (12kb)", "http://arxiv.org/abs/1606.05664v1", "submitted"]], "COMMENTS": "submitted", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaomin qi", "sergei silvestrov", "talat nazir"], "accepted": false, "id": "1606.05664"}, "pdf": {"name": "1606.05664.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Talat Nazir"], "emails": ["xiaomin.qi@mdh.se,", "sergei.silvestrov@mdh.se,", "talat.nazir@mdh.se"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n05 66\n4v 1\n[ cs\n.L G\n] 3\n1 M\nay 2\n01 6\nLinear Classification of data with Support Vector Machines and Generalized Support\nVector Machines Xiaomin Qi, Sergei Silvestrov and Talat Nazir\nDivision of Applied Mathematics, School of Education,\nCulture and Communication, Ma\u0308lardalen University, 72123 Va\u0308ster\u030aas, Sweden. E-mail: xiaomin.qi@mdh.se, sergei.silvestrov@mdh.se, talat.nazir@mdh.se\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013 Abstract: In this paper, we study the support vector machine and introduced the notion of generalized support vector machine for classification of data. We show that the problem of generalized support vector machine is equivalent to the problem of generalized variational inequality and establish various results for the existence of solutions. Moreover, we provide various examples to support our results. \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Keywords and Phrases: support vector machine, generalized support vector machine, control function. 2010 Mathematics Subject Classification: 62H30. \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n1 Support Vector Machine\nOver the last decade, support vector machines (SVMs) [2, 3, 13, 14, 18] has been revealed as very powerful and important tools for pattern classification and regression. It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22]. Recently, Wang et al. [15] presented SVM based fault classifier design for a water level control system. They also studied the SVM classifier based fault diagnosis for a water level process [16].\nFor the standard support vector classification (SVC), the basic idea is to find the optimal separating hyperplane between the positive and negative examples. The optimal hyperplane may be obtained by maximizing the margin between two parallel hyperplanes, which involves the minimization of a quadratic programming problem.\nSupport Vector Machines is based on the concept of decision planes that define decision boundaries. A decision plane is one that separates between a set of objects having different class memberships.\nSupport Vector Machines can be thought of as a method for constructing a special kind of rule, called a linear classifier, in a way that produces classifiers with theoretical guarantees of good predictive performance (the quality of classification on unseen data).\nIn this paper, we study the problems of support vector machine and define generalized support vector machine. We also show the sufficient conditions for the existence of solutions for problems of generalized support vector machine. We also support our results with various examples.\nThought this paper, by N, R, Rn and R+n we denote the set of all natural numbers, the set of all real numbers, the set of all n-tuples real numbers, the set of all n-tuples of nonnegative real numbers, respectively.\nAlso, we consider \u2016\u00b7\u2016 and < \u00b7, \u00b7 > as Euclidean norm and usual inner product on Rn, respectively.\nFurthermore, for two vectors x,y \u2208 Rn, we say that x \u2264 y if and only if xi \u2264 yi for all i \u2208 {1, 2, ..., n}, where xi and yi are the components of x and y, respectively.\nLinear Classifiers\nBinary classification is frequently performed by using a function f : Rn \u2192 R in the following way: the input x = (x1, ..., xn) is assigned to the positive class if, f (x) \u2265 0 and otherwise to the negative class. We consider the case where f (x) is a linear function of x, so that it can be written as\nf (x) = \u3008w \u00b7 x\u3009+ b\n=\nn \u2211\ni=1\nwixi + b,\nwhere w \u2208 Rn, b \u2208 R are the parameters that control the function and the decision rule is given by sgn (f (x)) . The learning methodology implies that these parameters must be learned from the data.\nDefinition 1.1. We define the functional margin of an example (xi, yi) with respect to a hyperplane (w, b) to be the quantity\n\u03b3i = yi (\u3008w \u00b7 xi\u3009+ b) ,\nwhere yi \u2208 {\u22121, 1}. Note that \u03b3i > 0 implies correct classification of (xi, yi) . If we replace functional margin by geometric margin we obtain the equivalent quantity for the normalized linear function (\n1 \u2016w\u2016w, 1 \u2016w\u2016b\n)\n, which therefore\nmeasures the Euclidean distances of the points from the decision boundary in the input space. Actually geometric margin can be written as\n\u03b3\u0303 = 1\n\u2016w\u2016\u03b3.\nTo find the hyperplane which has maximal geometric margin for a training set S means to find maximal \u03b3\u0303. For convenience, we let \u03b3 = 1, the objective function can be written as\nmax 1\n\u2016w\u2016 .\nOf course, there have some constraints for the optimization problem. According to the definition of margin, we have yi (\u3008w \u00b7 xi\u3009+ b) \u2265 1, i = 1, ..., l. We rewrite the equivalent formation of the objective function with the constraints as\nmin 1\n2 \u2016w\u20162 such that yi (\u3008w \u00b7 xi\u3009+ b) \u2265 1, i = 1, ..., l.\nWe denote this problem by SVM.\n2 Generalized Support Vector Machines\nWe replace w, b by W,B respectively, the control function F : Rn \u2192 Rn defined as\nF (x) = W.x +B, (2.1)\nwhere W \u2208 Rn\u00d7n, B \u2208 Rn are the parameters of control function. Define\n\u03b3\u0303\u2217k = yk (Wxk +B) > 1 for k = 1, 2, ..., l, (2.2)\nwhere yk \u2208 {(\u22121,\u22121, ...,\u22121) , (1, 1, ..., 1)} is n dimensional vector.\nDefinition 2.1. We define a map G : Rn \u2192 Rn+ by\nG (wi) = (\u2016wi\u2016 , \u2016wi\u2016 , ..., \u2016wi\u2016) for i = 1, 2, ..., n, (2.3)\nwhere wi be the row of Wn\u00d7n for i = 1, 2, ..., n. The problem is find wi \u2208 Rn that satisfy\nmin wi\u2208W G (wi) such that \u03b7 > 0, (2.4)\nwhere \u03b7 = yk (Wxk +B)\u2212 1.\nWe call this problem as the Generalized Support Vector Machine (GSVM). The GSVM is equivalent to\nfind wi \u2208 W : \u3008G\u2032 (wi) ,v\u2212wi\u3009 \u2265 0 for all v \u2208 Rn with \u03b7 > 0,\nor more specifically\nfind wi \u2208 W : \u3008\u03b7G\u2032 (wi) ,v\u2212wi\u3009 \u2265 0 for all v \u2208 Rn. (2.5)\nHence the problem of GSVM becomes to the problem of generalized variational inequality.\nExample 2.2. Let us take the group of points positive class (1, 0) , (0, 1) and negative class (\u22121, 0), (0,\u22121) . First we use SVM to solve this problem to find the hyperplane < w,x > +b = 0 that separate this two kinds of points. Obviously, we know that the hyperplane is H which is shown in the Figure.\nFor two positive points, we have\n(w1, w2)\n[\n1 0\n]\n+ b = 1\n(w1, w2)\n[\n0 1\n]\n+ b = 1\nwhich implies\nw1 + b = 1\nw2 + b = 1.\nFor two negative points, we have\n(w1, w2)\n[\n\u22121 0\n]\n+ b = \u22121\n(w1, w2)\n[\n0 \u22121\n]\n+ b = \u22121\nimplies that\n\u2212w1 + b = \u22121 \u2212w2 + b = \u22121.\nFrom the equations, we get w = (1, 1) and b = 0. The result is \u2016w\u2016 = \u221a 2.\nNow we apply GSVM for this data. For two positive points, we have\n[\nw11 w12 w21 w22\n] [\n1 0\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand [\nw11 w12 w21 w22\n] [\n0 1\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nwhich gives\n[\nw11 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand\n[\nw12 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n. (2.6)\nFor two negative points, we have\n[\nw11 w12 w21 w22\n] [\n\u22121 0\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\nand [\nw11 w12 w21 w22\n] [\n0 \u22121\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\n,\nwhich provides\n[\n\u2212w11 \u2212w21\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\nand\n[\n\u2212w12 \u2212w22\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\n. (2.7)\nFrom (2.6) and (2.7), we get\nW =\n[\n1 1 1 1\n]\nB =\n[\nb1 b2\n]\n=\n[\n0 0\n]\n.\nThus minG (wi) = min {G (w1) , G (w2)} = ( \u221a 2, \u221a 2).\nHence we get w = (1, 1) that minimize G (wi) for i = 1, 2.\nConclusion: The above example shows that we get same result by applying any method SVM and GSVM.\nIn the next example, we consider the two distinct group of data, first solve both data for separate cases and then solve it for combine case for both methods SVM and GSVM. Example 2.3. Let us consider the three categories of data: Situation 1, suppose that, we have data (1, 0), (0, 1) as positive class and data (\u22121/2, 0), (0,\u22121/2) as negative class.\nUsing SVM to solve this problem, we have\n(w1, w2)\n[\n1 0\n]\n+ b = 1 and\n(w1, w2)\n[\n0 1\n]\n+ b = 1,\nwhich implies w1 + b = 1 and w2 + b = 1. (2.8)\nFor two negative points, we have\n(w1, w2)\n[\n\u22121/2 0\n]\n+ b = \u22121, and\n(w1, w2)\n[\n0 \u22121/2\n]\n+ b = \u22121,\nwhich gives\n\u2212 w1 2 + b = \u22121 and \u2212 w2 2 + b = \u22121. (2.9)\nFrom (2.8) and (2.9), we get w = (4 3 , 4 3 ) with b = \u22121 3 , where \u2016w\u2016 = \u221a 32 3 .\nFor situation 2, we consider the data (1 2 , 0) and (0, 1 2 ) as positive class, data (\u22122, 0) and (0,\u22122) as negative class.\nUsing SVM to solve this problem, we have\n(w1, w2)\n[\n1/2 0\n]\n+ b = 1 and\n(w1, w2)\n[\n0 1/2\n]\n+ b = 1,\nwhich implies 1\n2 w1 + b = 1 and\n1 2 w2 + b = 1. (2.10)\nFrom the negative points, we have\n(w1, w2)\n[\n\u22122 0\n]\n+ b = \u22121 and\n(w1, w2)\n[\n0 \u22122\n]\n+ b = \u22121,\nimplies that \u2212 2w1 + b = \u22121 and \u2212 2w2 + b = \u22121. (2.11)\nFrom (2.10) and (2.11), we get w = (4 5 , 4 5 ) and b = 3 5 with \u2016w\u2016 = \u221a 32 5 .\nIn the next situation 3, we combine of this two groups of data. Now, we have data (1/2, 0), (0, 1/2), (1, 0) , (0, 1) as positive class and (\u22121/2, 0), (0,\u22121/2), (\u22122, 0), (0,\u22122) as negative class.\nUsing SVM to solve this problem, we have\n(w1, w2)\n[\n1/2 0\n]\n+ b = 1 and\n(w1, w2)\n[\n0 1/2\n]\n+ b = 1,\nwhich implies w1/2 + b = 1 and w2/2 + b = 1. (2.12)\nFor two negative points, we have\n(w1, w2)\n[\n\u22121/2 0\n]\n+ b = \u22121 and\n(w1, w2)\n[\n0 \u22121/2\n]\n+ b = \u22121,\nimplies that\n\u2212 1 2 w1 + b = \u22121 and \u2212 1 2 w2 + b = \u22121. (2.13)\nFrom (2.12) and (2.13), we obtain w = (2, 2) and b = 0, where \u2016w\u2016 = 2 \u221a 2.\nNow we solve the same problem for all three situations by using GSVM. For two positive points of situation 1, we have\n[\nw11 w12 w21 w22\n] [\n1 0\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand [\nw11 w12 w21 w22\n] [\n0 1\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n,\nwhich implies\n[\nw11 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand\n[\nw12 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n. (2.14)\nAgain, for the negative points, we have\n[\nw11 w12 w21 w22\n] [\n\u22121/2 0\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\nand [\nw11 w12 w21 w22\n] [\n0 \u22121/2\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\n,\nwhich gives\n[\n\u22121 2 w11 \u22121 2 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\nand\n[\n\u22121 2 w12 \u22121 2 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\n. (2.15)\nFrom (2.14) and (2.15), we get\nW =\n[\n4 3 4 3 4 3 4 3\n]\nand B =\n[\n\u22121 3 \u22121 3\n]\n.\nThus we get\nmin wi\u2208W\nG (wi) = ( 4 \u221a 2 3 , 4 \u221a 2 3 ).\nHence we get w = (4 3 , 4 3 ) that minimize G (wi) for i = 1, 2. Now, for positive points of situation 2, we have\n[\nw11 w12 w21 w22\n] [\n1/2 0\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand [\nw11 w12 w21 w22\n] [\n0 1/2\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n,\nwhich gives [\n1 2 w11 1 2 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand\n[\n1 2 w12 1 2 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n.\nFor two negative points for this case, we have [\nw11 w12 w21 w22\n] [\n\u22122 0\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\nand [\nw11 w12 w21 w22\n] [\n0 \u22122\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\n,\nwhich gives [\n\u22122w11 \u22122w21\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\nand\n[\n\u22122w12 \u22122w22\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\n.\nThus, we obtain that\nW =\n[\n4 5 4 5 4 5 4 5\n]\nand B =\n[\n3 5 3 5\n]\n.\nThus we get\nmin i\u2208{1,2}\nG (wi) = ( 4 \u221a 2 5 , 4 \u221a 2 5 ).\nHence we get w = (4 5 , 4 5 ) that minimize G (wi) for i = 1, 2.\nFor the positive points of the combination of situation 3, we have [\nw11 w12 w21 w22\n] [\n1/2 0\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand [\nw11 w12 w21 w22\n] [\n0 1/2\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n,\nwhich gives [\n1 2 w11 1 2 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\nand\n[\n1 2 w12 1 2 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n1 1\n]\n.\nFor two negative points for this case, we have [\nw11 w12 w21 w22\n] [\n\u22121 2\n0\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\nand [\nw11 w12 w21 w22\n] [\n0 \u22121\n2\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\n,\nwhich gives [\n\u22121 2 w11 \u22121 2 w21\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\nand\n[\n\u22121 2 w12 \u22121 2 w22\n]\n+\n[\nb1 b2\n]\n=\n[\n\u22121 \u22121\n]\n.\nFrom this, we obtain that\nW =\n[\n2 2 2 2\n]\nand B =\n[\n0 0\n]\n.\nThus we get min\ni\u2208{1,2} G (wi) = (2\n\u221a 2, 2 \u221a 2).\nHence we get w = (2, 2) that minimize G (wi) for i = 1, 2.\nProposition 2.4. Let G : Rn \u2192 Rn+ be a differentiable operator. An element w\u2217 \u2208 Rn minimize G if and only if G\u2032 (w\u2217) = 0, that is, w\u2217 \u2208 Rn solves GSVM if and only if G\u2032 (w\u2217) = 0. Proof. Let G\u2032 (w\u2217) = 0, then for all v \u2208 Rn,\n< \u03b7G\u2032 (w\u2217) ,v \u2212w\u2217 > = < 0,v \u2212w\u2217 > = 0.\nConsequently, the inequality\n< \u03b7G\u2032 (w\u2217) ,v \u2212w\u2217 > = < 0,v \u2212w\u2217 > \u2265 0\nholds for all v \u2208 Rn. Hence w\u2217 \u2208 Rn solves problem of GSVM. Conversely, assume that w\u2217 \u2208 Rn satisfies\n< \u03b7G\u2032 (w\u2217) ,v \u2212w\u2217 > \u2265 0 \u2200 v \u2208 Rn.\nTake v = w\u2217 \u2212G\u2032 (w\u2217) in the above inequality implies that\n< \u03b7G\u2032 (w\u2217) ,\u2212G\u2032 (w\u2217) > \u2265 0,\nwhich further implies \u2212\u03b7||G\u2032(w\u2217)||2 \u2265 0.\nSince \u03b7 > 0, so we get G\u2032(w\u2217) = 0.\nDefinition 2.5. Let K be a closed and convex subset of Rn. Then, for every point x \u2208 Rn, there exists a unique nearest point in K, denoted by PK (x), such that \u2016x\u2212 PK (x)\u2016 \u2264 \u2016x\u2212 y\u2016 for all y \u2208 K and also note that PK (x) = x if x \u2208 K. PK is called the metric projection of Rn onto K. It is well known that PK : R n \u2192 K is characterized by the properties:\n(i) PK (x) = z for x \u2208 Rn if and only if < z,y \u2212 z > \u2265 < x,y \u2212 z > for all y \u2208 Rn;\n(ii) For every x,y \u2208 Rn, \u2016PK (x)\u2212 PK (y)\u20162 \u2264 < x\u2212y, PK (x)\u2212PK (y) >;\n(iii) \u2016PK (x)\u2212 PK (y)\u2016 \u2264 \u2016x\u2212 y\u2016, for every x,y \u2208 Rn, that is, PK is nonexpansive map.\nProposition 2.6. Let G : Rn \u2192 Rn+ be a differentiable operator. An element w\u2217 \u2208 Rn minimize mapping G defined in (2.3) if and only if w\u2217 is the fixed point of map\nPRn + (I \u2212 \u03c1G\u2032) : Rn \u2192 Rn+ for any \u03c1 > 0.\nthat is,\nw\u2217 = PRn + (I \u2212 \u03c1G\u2032) (w\u2217)\n= PRn + (w\u2217 \u2212 \u03c1G\u2032 (w\u2217)) ,\nwhere PRn + is a projection map from Rn to Rn+. Proof. Suppose w\u2217 \u2208 Rn+ is solution of GSVM then for \u03b7 > 0, we have\n< \u03b7G\u2032 (w\u2217) ,w \u2212w\u2217 > \u2265 0 for all w \u2208 Rn.\nAdding < w\u2217,w \u2212w\u2217 > on both sides, we get\n< w\u2217,w\u2212w\u2217 > + < \u03b7G\u2032 (w\u2217) ,w\u2212w\u2217 > \u2265 < w\u2217,w\u2212w\u2217 > for all w \u2208 Rn,\nwhich further implies that\n< w\u2217,w \u2212w\u2217 > \u2265 < w\u2217 \u2212 \u03b7G\u2032 (w\u2217) ,w\u2212w\u2217 > for all w \u2208 Rn,\nwhich is possible only if w\u2217 = PRn + (w\u2217 \u2212 \u03c1G\u2032 (w\u2217)) , that is, w\u2217 is the fixed point of G\u2032. Conversely, let w\u2217 = PRn + (w\u2217 \u2212 \u03c1G\u2032 (w\u2217)) , then we have\n< w\u2217,w \u2212w\u2217 > \u2265 < w\u2217 \u2212 \u03b7G\u2032 (w\u2217) ,w\u2212w\u2217 > for all w \u2208 Rn,\nwhich implies\n< w\u2217,w \u2212w\u2217 > \u2212 < w\u2217 \u2212 \u03b7G\u2032 (w\u2217) ,w\u2212w\u2217 > \u2265 0 for all w \u2208 Rn,\nand so < \u03b7G\u2032 (w\u2217) ,w\u2212w\u2217 > \u2265 0 for all w \u2208 Rn.\nThus w\u2217 \u2208 Rn+ is the solution of GSVM.\nDefinition 2.7. A map G : Rn \u2192 Rn is said to be (I) L-Lipschitz if for every L > 0,\n\u2016G (x)\u2212G (y)\u2016 \u2264 L \u2016x\u2212 y\u2016 for all x,y \u2208 Rn.\n(II) monotone if\n< G (x)\u2212G (y) ,x\u2212 y > \u2265 0 for all x,y \u2208 Rn.\n(III) strictly monotone if\n< G (x)\u2212G (y) ,x\u2212 y > > 0 for all x,y \u2208 Rn with x 6= y.\n(IV) \u03b1-strongly monotone if\n< G (x)\u2212G (y) ,x\u2212 y > \u2265 \u03b1 \u2016x\u2212 y\u20162 for all x,y \u2208 Rn.\nNote that, every \u03b1-strongly monotone map G : Rn \u2192 Rn is strictly monotone and every strictly monotone map is monotone.\nExample 2.8. Let G : Rn \u2192 Rn be a mapping defined as\nG (wi) = \u03b1wi + \u03b2,\nwhere \u03b1 is any non negative scalar and \u03b2 is any real number. Then G is Lipschitz continuous with Lipschitz constant L = \u03b1.\nAlso, for any x,y \u2208 Rn,\n< G (x)\u2212G (y) ,x\u2212 y > = \u03b1 \u2016x\u2212 y\u20162\nwhich show that G is \u03b1-strongly monotone.\nTheorem 2.9. Let K \u2286 Rn be closed and convex and G\u2032 : Rn \u2192 K is strictly monotone. If there exists a w\u2217 \u2208 K which is the solution of GSVM, then w\u2217 is unique in K. Proof. Suppose that w\u22171,w \u2217 2 \u2208 K with w\u22171 6= w\u22172 be the two solutions of GSVM, then we have\n< \u03b7G\u2032 (w\u22171) ,w\u2212w\u22171 > \u2265 0 for all w \u2208 Rn (2.16)\nand < \u03b7G\u2032 (w\u22172) ,w \u2212w\u22172 > \u2265 0 for all w \u2208 Rn, (2.17) where \u03b7 > 0. Putting w = w\u22172 in (2.16) and w = w \u2217 1 in (2.17), we get\n< \u03b7G\u2032 (w\u22171) ,w \u2217 2 \u2212w\u22171 > \u2265 0 (2.18)\nand < \u03b7G\u2032 (w\u22172) ,w \u2217 1 \u2212w\u22172 > \u2265 0. (2.19)\nEq. (2.18) can be further write as\n< \u2212\u03b7G\u2032 (w\u22171) ,w\u22171 \u2212w\u22172 > \u2265 0. (2.20)\nAdding (2.19) and (2.20) implies that\n< \u03b7G\u2032 (w\u22172)\u2212 \u03b7G\u2032 (w\u22171) ,w\u22171 \u2212w\u22172 > \u2265 0\nwhich implies\n\u03b7 < G\u2032 (w\u22171)\u2212G\u2032 (w\u22172) ,w\u22171 \u2212w\u22172 > \u2264 0\nor < G\u2032 (w\u22171)\u2212G\u2032 (w\u22172) ,w\u22171 \u2212w\u22172 > \u2264 0. (2.21) Since G\u2032 is strictly monotone, so we must have\n< G\u2032 (w\u22171)\u2212G\u2032 (w\u22172) ,w\u22171 \u2212w\u22172 > > 0,\nwhich contradicts (2.21). Thus w\u22171 = w \u2217 2. Theorem 2.10. LetK \u2286 Rn be closed and convex. If the map G\u2032 : Rn \u2192 K is L-Lipchitz and \u03b1-strongly monotone then there exists a unique w\u2217 \u2208 K which is the solution of GSVM . Proof. Uniqueness: Suppose that w\u22171,w \u2217 2 \u2208 K be the two solutions of GSVM, then for \u03b7 > 0, we have < \u03b7G\u2032 (w\u22171) ,w\u2212w\u22171 > \u2265 0 for all w \u2208 Rn (2.22)\nand < \u03b7G\u2032 (w\u22172) ,w \u2212w\u22172 > \u2265 0 for all w \u2208 Rn. (2.23) Putting w = w\u22172 in (2.22) and w = w \u2217 1 in (2.23), we get\n< \u03b7G\u2032 (w\u22171) ,w \u2217 2 \u2212w\u22171 > \u2265 0 (2.24)\nand < \u03b7G\u2032 (w\u22172) ,w \u2217 1 \u2212w\u22172 > \u2265 0. (2.25)\nEq. (2.24) can be further write as\n< \u2212\u03b7G\u2032 (w\u22171) ,w\u22171 \u2212w\u22172 > \u2265 0. (2.26)\nAdding (2.25) and (2.26) implies that\n< \u03b7G\u2032 (w\u22172)\u2212 \u03b7G\u2032 (w\u22171) ,w\u22171 \u2212w\u22172 > \u2265 0\nwhich implies\n\u03b7 < G\u2032 (w\u22171)\u2212G\u2032 (w\u22172) ,w\u22171 \u2212w\u22172 > \u2264 0. (2.27)\nSince G\u2032 is \u03b1-strongly monotone, so we have\n\u03b1\u03b7 \u2016w\u22171 \u2212w\u22172\u20162 \u2264 \u03b7 < G\u2032 (w\u22171)\u2212G\u2032 (w\u22172) ,w\u22171 \u2212w\u22172 > \u2264 0,\nwhich implies that \u03b1\u03b7 \u2016w\u22171 \u2212w\u22172\u20162 \u2264 0. Since \u03b1\u03b7 > 0, so we must have \u2016w\u22171 \u2212w\u22172\u2016 = 0 and hence w\u22171 = w\u22172. Existence:\nAs we know that if w\u2217 \u2208 Rn+ is solution of GSVM then for \u03b7 > 0, we have\n< \u03b7G\u2032 (w\u2217) ,w \u2212w\u2217 > \u2265 0 for all w \u2208 Rn\nif and only if w\u2217 = PRn + (w\u2217 \u2212 \u03c1G\u2032(w\u2217)) \u2261 F (w\u2217) (say). Now for any w\u22171,w \u2217 2 \u2208 Rn+, we have\n\u2016F (w\u22171)\u2212 F (w\u22172)\u20162 = \u2225 \u2225\n\u2225 PRn + (w\u22171 \u2212 \u03c1G\u2032(w\u22171))\u2212 PRn+(w\u22172 \u2212 \u03c1G\u2032(w\u22172))\n\u2225 \u2225 \u2225 2\n\u2264 \u2016(w\u22171 \u2212 \u03c1G\u2032(w\u22171))\u2212 (w\u22172 \u2212 \u03c1G\u2032(w\u22172))\u20162 (as PRn+ is nonexpansive) = \u2016(w\u22171 \u2212w\u22172)\u2212 \u03c1[G\u2032(w\u22171)\u2212G\u2032(w\u22172)]\u20162 = < (w\u22171\u2212w\u22172)\u2212 \u03c1[G\u2032(w\u22171)\u2212G\u2032(w\u22172)], (w\u22171\u2212w\u22172)\u2212 \u03c1[G\u2032(w\u22171)\u2212G\u2032(w\u22172)] > = \u2016w\u22171\u2212w\u22172\u20162 \u2212 2\u03c1 < w\u22171\u2212w\u22172, G\u2032(w\u22171)\u2212G\u2032(w\u22172) > +\u03c12 \u2016G\u2032(w\u22171)\u2212G\u2032(w\u22172)\u20162 .\nNow since G\u2032 is L-Lipchitz and \u03b1-strongly monotone, so we get\n\u2016F (w\u22171)\u2212 F (w\u22172)\u20162 \u2264 \u2016w\u22171 \u2212w\u22172\u20162 \u2212 2\u03b1\u03c1 \u2016w\u22171 \u2212w\u22172\u20162\n+\u03c12L2 \u2016w\u22171 \u2212w\u22172\u20162\n= (1 + \u03c12L2 \u2212 2\u03c1\u03b1) \u2016w\u22171 \u2212w\u22172\u20162 ,\nthat is, \u2016F (w\u22171)\u2212 F (w\u22172)\u2016 \u2264 \u03b8 \u2016w\u22171 \u2212w\u22172\u2016 , (2.30) where \u03b8 = \u221a\n1 + \u03c12L2 \u2212 2\u03c1\u03b1. Since \u03c1 > 0, so that when \u03c1 \u2208 (0, 2\u03b1 L2 ), then we\nget \u03b8 \u2208 [0, 1). Now, by using Principle of Banach contraction, we obtain the fixed point of map F, that is, there exists a unique w\u2217 \u2208 Rn+ such that\nF (w\u2217) = PRn + (w\u2217 \u2212 \u03c1G\u2032(w\u2217))\n= w\u2217.\nHence w\u2217 \u2208 Rn+ is the solution of GSVM.\nExample 2.11. Let us take the group of data of positive class (\u03b11, \u03b12, ..., \u03b1n\u22121, 0) , (\u03b11, \u03b12, ..., \u03b1n\u22122, 0, \u03b1n), ..., (0, \u03b12, \u03b13, ..., \u03b1n) and negative class (k\u03b11, k\u03b12, ..., k\u03b1n\u22121, 0) , (k\u03b11, k\u03b12, ..., k\u03b1n\u22122, 0, k\u03b1n), ..., (0, k\u03b12, k\u03b13, ..., k\u03b1n) for n \u2265 2, where each \u03b1i 6= 0 for i \u2208 N and k 6= 1.\nA map G : Rn \u2192 Rn+ be given as\nG (wi) = (\u2016wi\u2016 , \u2016wi\u2016 , ..., \u2016wi\u2016) for i = 1, 2, ..., n,\nwhere wi are the row of Wn\u00d7n for i = 1, 2, ..., n. Then we have\nG\u2032 (wi) = 1\n\u2016wi\u2016 wi for i = 1, 2, ..., n.\nNow from the given data, we get\nW = 2\n(n\u2212 1) (1\u2212 k)\n\n    \n1 \u03b11 1 \u03b12 \u00b7 \u00b7 \u00b7 1 \u03b1n 1 \u03b11 1 \u03b12 \u00b7 \u00b7 \u00b7 1 \u03b1n\n\u00b7 \u00b7\n\u00b7 \u00b7\n\u00b7 \u00b7\n\u00b7 \u00b7\n1 \u03b11 1 \u03b12 \u00b7 \u00b7 \u00b7 1 \u03b1n\n\n    \nand so we have\nG (wi) = 2\n(n\u2212 1) (1\u2212 k)\n\u221a\n1\n\u03b121 +\n1\n\u03b122 + ...+\n1\n\u03b12n (1, 1, ..., 1) for i = 1, 2, ..., n\nand\nG\u2032 (wi) = 1 \u221a\n1 \u03b12 1 + 1 \u03b12 2 + ...+ 1 \u03b12n\n( 1 \u03b11 , 1 \u03b12 , ..., 1 \u03b1n ) for i = 1, 2, ..., n.\nNote that, for any w1,w2 \u2208 W,\n\u2016G\u2032 (w1)\u2212G\u2032 (w2)\u2016 = 0 = L \u2016w1 \u2212w2\u2016\nis satisfied where L is any nonnegative real number. Also\n< G\u2032 (w1)\u2212G\u2032 (w2) ,w1 \u2212w2 > \u2265 0\nis satisfied which show that G\u2032 is monotone operator. Moreover, w = 2 (n\u22121)(1\u2212k)( 1 \u03b11 , 1 \u03b12 , ..., 1 \u03b1n ) is the solution of GSVMwith \u2016w\u2016 = 2 (n\u22121)(1\u2212k) \u221a 1 \u03b12 1 + 1 \u03b12 2 + ...+ 1 \u03b12 n .\nExample 2.12. Let us take the group of data of positive class (\u03b11, \u03b12, ..., \u03b1m, 0, 0..., 0) , (0, \u03b12, \u03b13, ..., \u03b1m+1, 0, 0, ..., 0), ..., (\u03b11, \u03b12, ..., \u03b1m\u22121, 0, 0, ..., 0, \u03b1n) and negative\nclass (\u03ba\u03b11, \u03ba\u03b12, ..., \u03ba\u03b1m, 0, 0..., 0) , (0, \u03ba\u03b12, \u03ba\u03b13, ..., \u03ba\u03b1m+1, 0, 0, ..., 0), ..., (\u03ba\u03b11, \u03ba\u03b12, ..., \u03ba\u03b1m\u22121, 0, 0, ..., for n > m \u2265 1, where each \u03b1i 6= 0 for i \u2208 N and \u03ba 6= 1. A map G : Rn \u2192 Rn+ be given as\nG (wi) = (\u2016wi\u2016 , \u2016wi\u2016 , ..., \u2016wi\u2016) for i = 1, 2, ..., n,\nwhere wi are the row of Wn\u00d7n for i = 1, 2, ..., n. Then we have\nG\u2032 (wi) = 1\n\u2016wi\u2016 wi for i = 1, 2, ..., n.\nNow from the given data, we get\nW = 2\nm (1\u2212 k)\n\n    \n1 \u03b11 1 \u03b12 \u00b7 \u00b7 \u00b7 1 \u03b1n 1 \u03b11 1 \u03b12 \u00b7 \u00b7 \u00b7 1 \u03b1n\n\u00b7 \u00b7\n\u00b7 \u00b7\n\u00b7 \u00b7\n\u00b7 \u00b7\n1 \u03b11 1 \u03b12 \u00b7 \u00b7 \u00b7 1 \u03b1n\n\n    \nand so we have\nG (wi) = 2\n(n\u2212 1) (1\u2212 k)\n\u221a\n1\n\u03b121 +\n1\n\u03b122 + ...+\n1\n\u03b12n (1, 1, ..., 1) for i = 1, 2, ..., n\nand\nG\u2032 (wi) = 1 \u221a\n1 \u03b12 1 + 1 \u03b12 2 + ...+ 1 \u03b12 n\n( 1 \u03b11 , 1 \u03b12 , ..., 1 \u03b1n ) for i = 1, 2, ..., n.\nIt is easy to verify that G\u2032 is monotone and Lipchitz continuous operator. The vector w = 2\nm(1\u2212k)( 1 \u03b11 , 1 \u03b12 , ..., 1 \u03b1n ) is the solution of GSVM with\n\u2016w\u2016 = 2 m(1\u2212k)\n\u221a\n1 \u03b12 1 + 1 \u03b12 2 + ...+ 1 \u03b12 n .\nExample 2.13. Consider (\u03b11, 0, 0) , (0, \u03b12, 0) , (0, 0, \u03b13) , (\u03b21, 0, 0), (0, \u03b22, 0), (0, 0, \u03b23) as data of positive class and (k\u03b11, 0, 0) , (0, k\u03b12, 0) , (0, 0, k\u03b13) , (k\u03b21, 0, 0), (0, k\u03b22, 0), (0, 0, k\u03b23) as negative class of data, where \u03b1i, \u03b2i and k are positive real numbers with each \u03b1i \u2264 \u03b2i for i = 1, 2, 3 and k 6= 1. The map G : Rn \u2192 Rn+ is given as\nG (wi) = (\u2016wi\u2016 , \u2016wi\u2016 , ..., \u2016wi\u2016) for i = 1, 2, 3,\nwhere wi are the row of W3\u00d73 for i = 1, 2, ..., n. Then we have\nG\u2032 (wi) = 1\n\u2016wi\u2016 wi for i = 1, 2, 3.\nNow from the given data, we get\nW = 2\n(1\u2212 k)\n\n\n1 \u03b11 1 \u03b12 1 \u03b13 1 \u03b11 1 \u03b12 1 \u03b13 1 \u03b11 1 \u03b12 1 \u03b13  \nand so we have\nG (wi) = 2 (1\u2212 k)( \u221a 1 \u03b121 + 1 \u03b122 + 1 \u03b123 , \u221a 1 \u03b121 + 1 \u03b122 + 1 \u03b123 , \u221a 1 \u03b121 + 1 \u03b122 + 1 \u03b123 )\nand\nG\u2032 (wi) = 1 \u221a\n1 \u03b12 1 + 1 \u03b12 2 + 1 \u03b12 3\n( 1 \u03b11 , 1 \u03b12 , 1 \u03b13 ).\nNote that, for any w1,w2 \u2208 W,\n\u2016G\u2032 (w1)\u2212G\u2032 (w2)\u2016 = 0 = L \u2016w1 \u2212w2\u2016\nis satisfied for L > 0. Also\n< G\u2032 (w1)\u2212G\u2032 (w2) ,w1 \u2212w2 > \u2265 0\nis satisfied which show that G\u2032 is monotone operator. Moreover, w = 2\n(1\u2212k)( 1 \u03b11 , 1 \u03b12 , 1 \u03b13 ) is the solution of GSVM with \u2016w\u2016 = 2 (1\u2212k)\n\u221a\n1 \u03b12 1 + 1 \u03b12 2 + 1 \u03b12 3 .\nExample 2.14. Let us take the group of data of positive class (1, 0, 0) , (1, 1, 0), (0, 1, 1) and negative class (\u22121 2 , 0, 0), (\u22121 2 ,\u22121 2 , 0), (0,\u22121 2 ,\u22121 2 ). Now from the given data, we have\nW =\n\n\n4 3 0 4 3 4 3 0 4 3 4 3 0 4 3\n\n\nwith\nG (wi) = 4 3 ( \u221a 2, \u221a 2, \u221a 2) for i = 1, 2, 3\nand\nG\u2032 (wi) = 1\u221a 2 (1, 0, 1) for i = 1, 2, 3.\nIt is easy to verify that G\u2032 is monotone operator and Lipchitz continuous. Moreover, w = ( 4\n3 , 0,\n4 3 ) is the solution of GSVM with \u2016w\u2016 = 4 3\n\u221a 2.\nConclusion. Recently many results appeared in the literature giving the problems related to the support vector machine and it applications. In this paper, initiate the study of generalized support vector machine and present linear classification of data by using support vector machine and generalized support vector machine. We also provide sufficient conditions under which the solution of generalized support vector machine exist. Various examples are also present to show the validity of these results.\n\\end{conclusion}\nReferences\n[1] Adankon, M.M., and Cheriet, M.: Model selection for the LS-SVM. Application to handwriting recognition. Pattern Recognition 42 (12), 3264-3270 (2009)\n[2] Cortes, C., Vapnik, V.N.: Support-vector networks. Machine Learning 20 (3), 273-297 (1995)\n[3] Cristianini, N., and Shawe-Taylor, J.: An Introduction to support vector machines and other kernelbased learning methods. Cambridge University Press, 2000.\n[4] Guyon, I., Weston, J., Barnhill, S., and Vapnik, V.: Gene selection for cancer classification using support vector machines. Machine Learning 46 (1-3), 389-422 (2002)\n[5] Joachims, T.: Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning. Springer, 1998.\n[6] Khan, N., Ksantini, R., Ahmad I., and Boufama, B.: A novel SVM+NDA model for classification with an application to face recognition. Pattern Recognition 45 (1), 66-79 (2012)\n[7] Li, Kwok, S., J.T., Zhu, H., Wang, Y.: Texture classification using the support vector machines. Pattern Recognition 36 (12), 2883-2893 (2003).\n[8] Liu, R., Wang, Y., Baba, T., Masumoto, D., and Nagata, S.: SVMbased active feedback in image retrieval using clustering and unlabeled data. Pattern Recognition 41 (8), 2645-2655 (2008)\n[9] Michel, P., and Kaliouby, R.E.: Real time facial expresion recognition in video using support vector machines. In Proceedings of ICMI\u201903 258- 264, (2003).\n[10] Noble, W.S.: Support Vector Machine Applications in Computational Biology, MIT Press, 2004.\n[11] Shao, Y.H., Chen, W.J., and Deng, N.Y.: Nonparallel hyperplane support vector machine for binary classification problems. Information Sciences 263 (0), 22-35 (2014)\n[12] Shao, Y., and Lunetta, R.S.: Comparison of support vector machine, neural network, and CART algorithms for the land-cover classification using limited training data points. ISPRS Journal of Photogrammetry and Remote Sensing 70 (0), 78-87 (2012)\n[13] Vapnik, V.N.: The nature of statistical learning theory. Springer, New York, 1996.\n[14] Vapnik, V.N.: Statistical Learning Theory, John Wiley and Sons, New York, 1998.\n[15] Wang, D., Qi, X., Wen, S., and Deng., M.: SVM based fault classifier design for a water level control system. Proceedings of 2013 International Conference on Advanced Mechatronic Systems, 2013/9/25-2013/9/27, 2013/9/25, EI (2013)\n[16] Wang, D., Qi, X., Wen, S., Dan, Y., Ouyang, L., and Deng, M.: Robust nonlinear control and SVM classifier based fault diagnosis for a water level process. ICIC Express Letters 5 (1), (2014)\n[17] Wu, Y.C., Lee, Y.-S., and Yang, J.-C.: Robust and efficient multiclass SVM models for phrase pattern recognition. Pattern Recognition 41 (9), 2874-2889 (2008)\n[18] Weston, J., and Watkins, C.: Multi-class support vector machines. Technical Report CSD-TR- 98-04, Department of Computer Science, Royal Holloway, University of London 1998.\n[19] Wang, X.Y., Wang, T., and Bu, J.: Color image segmentation using pixel wise support vector machine classification. Pattern Recognition 44 (4), 777-787 (2011).\n[20] Xue, Z., Ming, D., Song, W., Wan, B., and Jin, S.: Infrared gait recognition based on wavelet transform and support vector machine. Pattern Recognition 43 (8), 2904-2910 (2010)\n[21] Zhao, Z., Liu, J., and Cox, J.: Safe and efficient screening for sparse support vector machine. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 14, pages 542\u2013551, New York, NY, USA, (2014)\n[22] Zuo, R., and E.J.M.: Carranza, Support vector machine: A tool for mapping mineral prospectivity. Computers Geosciences 37 (12),1967- 1975 (2011)"}], "references": [{"title": "Model selection for the LS-SVM. Application to handwriting recognition", "author": ["M.M. Adankon", "M. Cheriet"], "venue": "Pattern Recognition", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "An Introduction to support vector machines and other kernelbased learning methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "In Proceedings of the European Conference on Machine Learning. Springer,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "A novel SVM+NDA model for classification with an application to face recognition", "author": ["N. Khan", "R. Ksantini", "Ahmad I", "B. Boufama"], "venue": "Pattern Recognition", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Texture classification using the support vector machines", "author": ["Li", "S. Kwok", "J.T", "H. Zhu", "Y. Wang"], "venue": "Pattern Recognition", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "SVMbased active feedback in image retrieval using clustering and unlabeled data", "author": ["R. Liu", "Y. Wang", "T. Baba", "D. Masumoto", "S. Nagata"], "venue": "Pattern Recognition", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Real time facial expresion recognition in video using support vector machines", "author": ["P. Michel", "R.E. Kaliouby"], "venue": "In Proceedings of ICMI\u201903", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Support Vector Machine Applications in Computational Biology", "author": ["W.S. Noble"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Nonparallel hyperplane support vector machine for binary classification problems", "author": ["Y.H. Shao", "W.J. Chen", "N.Y. Deng"], "venue": "Information Sciences", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Comparison of support vector machine, neural network, and CART algorithms for the land-cover classification using limited training data points", "author": ["Y. Shao", "R.S. Lunetta"], "venue": "ISPRS Journal of Photogrammetry and Remote Sensing", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "The nature of statistical learning theory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "SVM based fault classifier design for a water level control system", "author": ["D. Wang", "X. Qi", "S. Wen", "M. Deng"], "venue": "Proceedings of 2013 International Conference on Advanced Mechatronic Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Robust nonlinear control and SVM classifier based fault diagnosis for a water level process", "author": ["D. Wang", "X. Qi", "S. Wen", "Y. Dan", "L. Ouyang", "M. Deng"], "venue": "ICIC Express Letters", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Robust and efficient multiclass SVM models for phrase pattern recognition", "author": ["Y.C. Wu", "Lee", "Y.-S", "Yang", "J.-C"], "venue": "Pattern Recognition", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Multi-class support vector machines", "author": ["J. Weston", "C. Watkins"], "venue": "Technical Report CSD-TR- 98-04,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Color image segmentation using pixel wise support vector machine classification", "author": ["X.Y. Wang", "T. Wang", "J. Bu"], "venue": "Pattern Recognition", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Infrared gait recognition based on wavelet transform and support vector machine", "author": ["Z. Xue", "D. Ming", "W. Song", "B. Wan", "S. Jin"], "venue": "Pattern Recognition", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Safe and efficient screening for sparse support vector machine", "author": ["Z. Zhao", "J. Liu", "J. Cox"], "venue": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Support vector machine: A tool for mapping mineral prospectivity", "author": ["R. Zuo", "E.J.M.: Carranza"], "venue": "Computers Geosciences", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1967}], "referenceMentions": [{"referenceID": 1, "context": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 1 Support Vector Machine Over the last decade, support vector machines (SVMs) [2, 3, 13, 14, 18] has been revealed as very powerful and important tools for pattern classification and regression.", "startOffset": 94, "endOffset": 112}, {"referenceID": 11, "context": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 1 Support Vector Machine Over the last decade, support vector machines (SVMs) [2, 3, 13, 14, 18] has been revealed as very powerful and important tools for pattern classification and regression.", "startOffset": 94, "endOffset": 112}, {"referenceID": 12, "context": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 1 Support Vector Machine Over the last decade, support vector machines (SVMs) [2, 3, 13, 14, 18] has been revealed as very powerful and important tools for pattern classification and regression.", "startOffset": 94, "endOffset": 112}, {"referenceID": 16, "context": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 1 Support Vector Machine Over the last decade, support vector machines (SVMs) [2, 3, 13, 14, 18] has been revealed as very powerful and important tools for pattern classification and regression.", "startOffset": 94, "endOffset": 112}, {"referenceID": 3, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 4, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 5, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 6, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 8, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 9, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 10, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 15, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 17, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 18, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 19, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 20, "context": "It has been used in various applications such as text classification [5], facial expression recognition [9], gene analysis [4] and many others [1, 6, 7, 8, 10, 11, 12, 17, 19, 20, 21, 22].", "startOffset": 143, "endOffset": 187}, {"referenceID": 13, "context": "[15] presented SVM based fault classifier design for a water level control system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "They also studied the SVM classifier based fault diagnosis for a water level process [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "+ b = 1 (w1, w2) [ 0 1 ]", "startOffset": 17, "endOffset": 24}, {"referenceID": 0, "context": "and [ w11 w12 w21 w22 ] [ 0 1 ]", "startOffset": 24, "endOffset": 31}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "7), we get W = [ 1 1 1 1 ]", "startOffset": 15, "endOffset": 26}, {"referenceID": 0, "context": "7), we get W = [ 1 1 1 1 ]", "startOffset": 15, "endOffset": 26}, {"referenceID": 0, "context": "7), we get W = [ 1 1 1 1 ]", "startOffset": 15, "endOffset": 26}, {"referenceID": 0, "context": "7), we get W = [ 1 1 1 1 ]", "startOffset": 15, "endOffset": 26}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": "15), we get W = [ 4 3 4 3 4 3 4 3 ]", "startOffset": 16, "endOffset": 35}, {"referenceID": 1, "context": "15), we get W = [ 4 3 4 3 4 3 4 3 ]", "startOffset": 16, "endOffset": 35}, {"referenceID": 2, "context": "15), we get W = [ 4 3 4 3 4 3 4 3 ]", "startOffset": 16, "endOffset": 35}, {"referenceID": 1, "context": "15), we get W = [ 4 3 4 3 4 3 4 3 ]", "startOffset": 16, "endOffset": 35}, {"referenceID": 2, "context": "15), we get W = [ 4 3 4 3 4 3 4 3 ]", "startOffset": 16, "endOffset": 35}, {"referenceID": 1, "context": "15), we get W = [ 4 3 4 3 4 3 4 3 ]", "startOffset": 16, "endOffset": 35}, {"referenceID": 2, "context": "15), we get W = [ 4 3 4 3 4 3 4 3 ]", "startOffset": 16, "endOffset": 35}, {"referenceID": 1, "context": "15), we get W = [ 4 3 4 3 4 3 4 3 ]", "startOffset": 16, "endOffset": 35}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]", "startOffset": 25, "endOffset": 44}, {"referenceID": 3, "context": "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]", "startOffset": 25, "endOffset": 44}, {"referenceID": 2, "context": "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]", "startOffset": 25, "endOffset": 44}, {"referenceID": 3, "context": "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]", "startOffset": 25, "endOffset": 44}, {"referenceID": 2, "context": "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]", "startOffset": 25, "endOffset": 44}, {"referenceID": 3, "context": "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]", "startOffset": 25, "endOffset": 44}, {"referenceID": 2, "context": "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]", "startOffset": 25, "endOffset": 44}, {"referenceID": 3, "context": "Thus, we obtain that W = [ 4 5 4 5 4 5 4 5 ]", "startOffset": 25, "endOffset": 44}, {"referenceID": 1, "context": "and B = [ 3 5 3 5 ]", "startOffset": 8, "endOffset": 19}, {"referenceID": 3, "context": "and B = [ 3 5 3 5 ]", "startOffset": 8, "endOffset": 19}, {"referenceID": 1, "context": "and B = [ 3 5 3 5 ]", "startOffset": 8, "endOffset": 19}, {"referenceID": 3, "context": "and B = [ 3 5 3 5 ]", "startOffset": 8, "endOffset": 19}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "= [ 1 1 ]", "startOffset": 2, "endOffset": 9}], "year": 2016, "abstractText": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013 Abstract: In this paper, we study the support vector machine and introduced the notion of generalized support vector machine for classification of data. We show that the problem of generalized support vector machine is equivalent to the problem of generalized variational inequality and establish various results for the existence of solutions. Moreover, we provide various examples to support our results. \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "creator": "LaTeX with hyperref package"}}}