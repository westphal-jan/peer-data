{"id": "1606.05854", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2016", "title": "Full-Time Supervision based Bidirectional RNN for Factoid Question Answering", "abstract": "Recently, vitrea recurrent neural computer (BRNN) an been figures made it mind answering (QA) involve of promising ever. However, mainly plans BRNN models extract the relevant of testimony while detail following means allows an pooling operation to generate as relation make biggest simple derive assuming. Hence, specifically construct vehicle don ' - put guidelines (loss or trait measurement) at this time unlikely, which return lose almost simple accounts. In not picture, no recommending a film BRNN differs called one - time requirements leading BRNN (FTS - BRNN ), separate can down federal at someone because step. Experiments without the tiggers QA step yet this opportunity FTS - BRNN wo substantially other baselines giving guarantee set carolina - instance - to - famed calculate.", "histories": [["v1", "Sun, 19 Jun 2016 11:03:47 GMT  (726kb,D)", "https://arxiv.org/abs/1606.05854v1", "9 pages"], ["v2", "Tue, 21 Jun 2016 01:47:06 GMT  (726kb,D)", "http://arxiv.org/abs/1606.05854v2", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dong xu", "wu-jun li"], "accepted": false, "id": "1606.05854"}, "pdf": {"name": "1606.05854.pdf", "metadata": {"source": "CRF", "title": "Full-Time Supervision based Bidirectional RNN for Factoid Question Answering", "authors": ["Dong Xu", "Wu-Jun Li"], "emails": ["121220312@smail.nju.edu.cn,", "liwujun@nju.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Question answering (QA) has become an important research topic in natural language processing (NLP) with wide applications. Factoid QA (e.g., quiz bowl) is a special QA task, which has also attracted much attention recently [2, 7]. Traditional QA methods can be divided into three main categories. The first category is based on surface pattern matching which uses manually defined rules [12] or parse dependency trees [16, 4]. This category of methods need a lot of human work and usually need to be modified when the data is changed. The second category is based on similarity measures which usually uses bag-of-words (BOW) or term-frequency inverse-document-frequency (TF-IDF) as features and then calculates the inner-product or cosine similarity between the feature vectors of questions and answers. Although this category of methods are simple, they cannot achieve satisfactory performance by discarding the syntactic and semantic information. The third category treats each answer as a class label and represents each question by BOW or TF-IDF features, based on which the QA task is treated as a classification task. This category of methods are mainly for factoid QA tasks [2].\nThe traditional QA methods mentioned above are usually called shallow methods. Recently, researchers propose to adopt deep methods, especially deep neural network (DNN), for QA tasks [5, 7, 14, 15]. Different from traditional shallow methods which use manually constructed high-dimensional feature vectors to represent questions and answers, deep methods try to learn low-dimensional distributed representation for questions and answers [5, 7, 14, 15]. Furthermore, many works [7, 14] have shown that deep methods can achieve better performance than traditional shallow methods for QA. Hence, researchers have put more and more attention to the deep QA methods. The existing deep QA methods can be divided into two main categories. The first category is based on recursive neural network [7], and the second category is based on recurrent neural network (RNN) 1 [5, 14, 15].\n1In some literatures, the recursive neural network is also abbreviated as RNN. In this paper, we directly use the full name of recursive neural network, and RNN only represents the recurrent neural network.\nar X\niv :1\n60 6.\n05 85\n4v 2\n[ cs\n.C L\n] 2\n1 Ju\nn 20\nQANTA [7] is one of the representative deep QA methods based on recursive neural network. It is mainly developed for factoid QA. The inputs of QANTA are dependency parse trees of sentences in a question and its corresponding answer. After learning the feature representations for the questions, QANTA trains a logistic regression (LR) classifier on these feature representations and then uses it to classify the questions into their corresponding answers. Experiments show that QANTA outperforms traditional shallow methods. The disadvantage of recursive neural network based methods, such as QANTA, is that they need to build parse trees before the deep representation learning procedure .\nTraditional RNN is unidirectional. In many NLP applications, especially those with long sequences, researchers find that bidirectional RNN (BRNN) can outperform unidirectional RNN. Hence, BRNN has been widely used for QA tasks [14, 5, 15] with promising performance. QA-LSTM [14] first uses a gated BRNN, called bidirectional long-short term memory (BLSTM), to extract the feature of questions (answers) and concatenates the hidden states at time step t of both directions in BLSTM to generate the output at time step t. Then a pooling operation is performed on these outputs to generate the representations for questions (answers). Loss or similarity is calculated after this pooling operation. [5] uses an attention mechanism on the hidden states to generate the representations for questions. [15] concatenates the question and answer as one sequence, then uses an output layer to compute similarity directly. A pooling operation is also applied to the outputs during training. Different from recursive neural network based methods like QANTA which need some manual effort for building the parse trees, BRNN based deep QA methods can be used to train feature representation automatically in an end-to-end way. Furthermore, some work [9] has shown that BRNN can outperform recursive neural network based methods in many NLP tasks including QA.\nAlthough BRNN has achieved promising performance for QA, most existing BRNN models, such as QA-LSTM, extract the information of questions and answers by directly using a pooling operation to generate the representation for loss or similarity calculation. Hence, these existing models don\u2019t put supervision (loss or similarity calculation) at every time step, which will lose some useful information. In this paper, we propose a novel BRNN model called full-time supervision based BRNN (FTS-BRNN), for QA. The contributions of FTS-BRNN are briefly outlined as follows:\n\u2022 FTS-BRNN can put supervision at every time step to make full use of all information in all time steps in BRNN.\n\u2022 Different from existing BRNN methods which use LSTM as the hidden units, FTS-BRNN uses the gated recurrent unit (GRU) [3] as the hidden units.\n\u2022 Experiments on the factoid QA task show that FTS-BRNN can outperform other baselines to achieve the state-of-the-art accuracy in real applications.\nThe remaining content of this paper is organized as follows: Section 2 introduces the background of this paper, including a short overview of factoid QA, BRNN and GRU; Section 3 describes the model details of FTS-BRNN; Section 4 presents the experimental results, and we draw our conclusion in Section 5."}, {"heading": "2 Background", "text": "Although our model can also be generalized to handle general QA tasks, this paper focuses on a special QA task, called factoid QA. In this section, we introduce the background of this paper, including a short overview of factoid QA, BRNN and GRU."}, {"heading": "2.1 Factoid Question Answering", "text": "Factoid QA is a special QA task in which the answers are syntactic or semantic entities, such as organization or person names. Quiz bowl is a representative kind of factoid QA. Here, we use quiz bowl as an example to introduce factoid QA [7]. Quiz bowl can be seen as a kind of text classification task [2] or QA task [7]. Players are asked for an answer according to the given description (question). Each question consists of four to six sentences in which every single sentence contains useful clues to answer the question. The answer to a question is an entity represented by a phrase or a single word. Table 1 shows an example of quiz bowl. In real world competition, players can answer at any time. In our experiments, we slightly change the quiz bowl rule that players can only answer the question after getting all of the question information, which can better reflect the information extraction ability of the models and make the trained model be suitable for more general QA tasks."}, {"heading": "2.2 Bidirectional Recurrent Neural Network (BRNN)", "text": "RNN is proposed for processing sequential data containing several time steps, which has been widely used in NLP tasks, including neural machine translation (NMT) [13, 1] and QA and so on. Traditional RNN is unidirectional. BRNN consists of two unidirectional RNNs in opposite directions, a forward RNN and a backward RNN. So the hidden states h(t) of BRNN at time step t consist of the hidden states of the forward RNN f (t) \u2208 Rd and the hidden states of the backward RNN b(t) \u2208 Rd. There are several ways to combine these two hidden states. In [1, 14], a concatenating operation h(t) = [f (t), b(t)] is adopted. In [9], an output activation function o(t) = f(WL \u00b7 [f (t), b(t)]) is used, which can preserve the vector dimensionality with WL \u2208 Rd\u00d72d where d is the dimensionality of f (t) and b(t). The way used in [15] is similar to that in [9], which uses o(t) =W \u00b7 f (t) + U \u00b7 b(t) + bias without activation functions. BRNN has achieved better performance than unidirectional RNN in real applications, especially with long sequences."}, {"heading": "2.3 Gated Recurrent Unit (GRU)", "text": "In real applications, the gated RNN or gated BRNN is always adopted. Typical gated RNN includes long short-term memory (LSTM) [6] and gated recurrent unit (GRU). GRU is first proposed in [3] for NMT task. There are three gates in a single LSTM unit: input gate i, forget gate f and output gate o. But GRU only uses two gates, reset gate r and update gate z, to achieve similar functionality as that in LSTM.\nIn this paper, we choose GRU rather than LSTM because we find that in our model GRU is better than LSTM. Here, we give a brief introduction of GRU.\nIn GRU, reset gate r and update gate z are defined as follows:\nr = \u03c3(Wrx+ Urh (t\u22121) + br),\nz = \u03c3(Wzx+ Uzh (t\u22121) + bz),\nwhere Wr,Wz, Ur, Uz \u2208 Rd\u00d7d are weight matrices, br, bz are the bias vectors, x is input and h(t\u22121) is the hidden states at the previous time step, \u03c3 is the sigmoid function. Then the hidden states at time step t are computed by\nh(t) = z h(t\u22121) + (1\u2212 z) h\u0303(t), (1) where h\u0303(t) = \u03c6(Whx+ Uh(r h(t\u22121)) + bh), with Wh, Uh \u2208 Rd\u00d7d being the weight matrices and bh being the bias vector."}, {"heading": "3 Full-Time Supervision based BRNN (FTS-BRNN)", "text": "In this section, we present the details of our full-time supervision based BRNN (FTS-BRNN) which puts supervision for all time steps in BRNN.\nFTS-BRNN has two variants: the first one adopts BRNN for questions and RNN for answers, and the second one adopts the same BRNN for both questions and answers. In this paper, FTS-BRNN refers to the first variant, and FTS-BRNN-s refers to the second variant which uses the same BRNN for both questions and answers. BRNN is typically better than RNN for long sequences, but for short sequences BRNN is not necessarily better than RNN. Hence, if the answers are short, such as the case of factoid QA, we prefer to choose FTS-BRNN which adopts RNN for answers. This will also be verified in our experiments."}, {"heading": "3.1 FTS-BRNN", "text": "The architecture of FTS-BRNN is shown in Figure 1. FTS-BRNN uses a BRNN for questions and a RNN for answers. The unit to represent the hidden states in each time step is a GRU. Hence, the BRNN in FTS-BRNN is actually a bidirectional GRU, containing a forward GRU and a backward GRU. Furthermore, different from some existing BRNN methods like QA-LSTM [14] which directly concatenate the hidden states of the forward RNN and backward RNN as output, FTS-BRNN adds an output layer on BRNN. The output of the time step t is computed as follows:\no(t) =Wo \u00b7 f (t) + Uo \u00b7 b(t) + bias, where f (t) \u2208 Rd is the hidden states of the time step t in the forward GRU, b(t) \u2208 Rd is the hidden states of the time step t in the backward GRU, bias \u2208 Rd is a bias vector, Wo, Uo \u2208 Rd\u00d7d are weight matrices. Here, all the hidden states of both forward GRU and backward GRU are computed by (1).\nThe model of FTS-BRNN can be formulated as follows:\nQfx = BRNNf (x),\nQbx = BRNNb(x),\nQox =Wo \u00b7Qfx + Uo \u00b7Qbx + bias, Ahx = RNN(Ax),\nAex = A h x(Ta),\nwhere x is a question, Qfx = [f (1) x ; f (2) x ; ...; f (Tq) x ] \u2208 Rd\u00d7Tq is the forward hidden states at all the Tq time steps, Qbx = [b (1) x ; b (2) x ; ...; b (Tq) x ] \u2208 Rd\u00d7Tq , Qox = [o (1) x ; o (2) x ; ...; o (Tq) x ] \u2208 Rd\u00d7Tq are the backward hidden states and outputs respectively, Tq and Ta are respectively the length of the question and answer, Ax is the answer of x, Ahx \u2208 Rd\u00d7Ta are the hidden states of the answer at all the Ta time steps, Aex \u2208 Rd is the hidden states of the last time step Ta of the RNN. During the training procedure, FTS-BRNN minimizes the full-time margin loss:\nLoss = \u2211 x \u2211 t max(0, 1\u2212 o(t)x \u00b7Aex + o(t)x \u00b7Aewrong) (2)\nwhereAex is the corresponding answer of the question x andA e wrong is a wrong answer for question x. We use all the wrong answers in our experiments rather than just randomly sample a subset to calculate\nthe loss. This margin loss aims to make the inner product between the question representation and the corresponding answer representation bigger than those of the wrong answers as much as possible.\nMost existing methods, such as QA-LSTM [14], use pooling to generate the question representation. Based on the pooling result, the loss of these methods has the following formulations:\nLoss = \u2211 x max(0, 1\u2212 opx \u00b7Aex + opx \u00b7Aewrong), (3)\nwhere opx is the question representation after pooling. For example, o p x =\n\u2211 t o (t) x\nTq is the result of\naverage pooling. We can also use other pooling operations to generate opx, and we can also use other loss functions besides the margin loss.\nBy comparing the loss in (2) to that in (3), we can find that FTS-BRNN puts supervision for all the time steps. This full-time supervision strategy in FTS-BRNN can make better use of the information. Since each output o(t)x has all the information of input in BRNN, full-time supervision treats these outputs as representations of questions independently. By minimizing this full-time supervised loss, every o(t)x tries to make the inner product between o (t) x and Aex bigger than the inner product between o (t) x and Aewrong.\nAfter training, the distributed representation of a question is computed by an average pooling operation for out-of-sample prediction (test):\nQpk =\n\u2211 t o (t) k\nTq (4)\nwhere k is a question for test (prediction), Qpk is the representation for question k after average pooling.\nThen, the answer which gives the biggest inner product with Qpk will be chosen:\ny = argmax i\n(Qpk \u00b7A e i ). (5)\nSince the output o(t)x at each time step contributes to the loss in FTS-BRNN during training, the prediction function in (5) plays a role like ensemble by using the average pooling operation on the test questions.\nAfter we have learned the representation for all questions, we can also treat each answer as a class label, and then train a logistic regression (LR) classifier on question representations to predict the answer:\ny = LR(Qpk). (6)\nIn [9], the authors also use similar loss as that in (2). However, the motivation of [9] is to perform fair comparison with QANTA because QANTA also uses loss (supervision) at each node (step) in the recursive neural networks. Hence, the authors of [9] do not explicitly claim that full-time supervision is the key in BRNN for QA tasks because they do not perform any empirical comparison between full-time supervision and pooling-based supervision. In this paper, we perform detailed empirical comparison between full-time supervision and pooling-based supervision, and find that full-time supervision is much better than pooling-based supervision. Hence, our work is the first to explicitly claim that full-time supervision is the key in BRNN for QA. Furthermore, LSTM is adopted in [9], but our FTS-BRNN adopts GRU. From our experiments which will be presented below, we find that FTS-BRNN with GRU is much better than FTS-BRNN with LSTM."}, {"heading": "3.2 FTS-BRNN-s", "text": "Here, we introduce a variant of FTS-BRNN, which is called FTS-BRNN-s. Here, the \u2018s\u2019 means that we use the same BRNN for both questions and answers. The architecture of FTS-BRNN-s is shown in Figure 2. The question processing in FTS-BRNN-s is the same as that in FTS-BRNN. The only difference lies in the processing of answers. Different from Aex of FTS-BRNN, in FTS-BRNN-s we\nget Aox as follows:\nAfx = BRNNf (Ax),\nAbx = BRNNb(Ax),\nAox =Wo \u00b7Afx + Uo \u00b7Abx + bias. Then the full-time margin loss is changed to:\nLoss = \u2211 x \u2211 t max(0, 1\u2212 o(t)x \u00b7Aox(t) + o(t)x \u00b7Aowrong(t)),\nwhere Aox(t) denotes the output at the time step t for the answer of question x, A o wrong(t) denotes the output at the time step t for a wrong answer of question x.\nAt test (prediction) time, the distributed representation of the answer k is computed by an average pooling operation:\nApk =\n\u2211 tA o k(t)\nT ,\nwhere T = Ta = Tq is the defined length of sequence."}, {"heading": "4 Experiments", "text": "We evaluate our method on the factoid QA task. The experiments are performed on an NVIDIA K80 GPU server."}, {"heading": "4.1 Dataset", "text": "We use the factoid QA datasets from [7] for evaluation2. The whole dataset contains two subsets: Literature and History. Similar to [7], we first filter out all questions that do not belong to history or literature, and then only the answers that occur at least six times will be used. The statistics of these two subsets are summarized in Table 2.\nFor all questions belonging to the same answer, we sample 20% as test set, 20% as validation set, and the remaining 60% as training set. So we get 2524 training questions, 840 validation questions and 840 test questions for Literature. For History, we have 1535 training questions, 511 validation questions and 511 test questions.\n2We download the datasets from https://cs.umd.edu/~miyyer/qblearn/ which are provided by the authors of [7]. The publically available datasets for download are slightly smaller than those used in [7]."}, {"heading": "4.2 Baselines", "text": "We compare our method with several state-of-the-art baselines:\n\u2022 BOW [7]: BOW treats each answer as a class label, and adopts the LR classifier for classification based on the bag-of-words (BOW) features.\n\u2022 BOW-DT [7]: BOW-DT is similar to BOW method by using LR on BOW features. Different from BOW, the feature set is augmented with dependency relation indicators.\n\u2022 QANTA [7]: QANTA is a recursive neural network based method proposed in [7]. During the training procedure, QANTA adopts dependency parse trees to learn sentence representation and then trains a LR classifier based on this representation. During prediction (test), the answer is chosen by the LR classifier.\n\u2022 QA-LSTM [14]: QA-LSTM is a LSTM based BRNN method [14] which computes the loss after using a pooling operation and uses concatenating operation to generate the hidden states h(t) for each time step.\nBecause the question is a paragraph consisting of several sentences, we concatenate these sentences one by one as a single sentence3. As that in QANTA [7], we use the 100-dimensional pre-trained word embedding provided by GloVe [11] to represent the input words for all deep methods. Furthermore, we set the dimensionality of hidden states and embedding d = 100 to keep in line with QANTA. We choose rmsprop and momentum as our training algorithm. Learning rate is 0.002 or 0.001 for different methods to achieve the best performance, and the momentum is 0.8. Dropout is performed on the inputs of questions with rate 0.7. All methods are converged around 50-100 epochs. The weight matrices and initial states of RNN are initialized by a uniform distribution [\u2212a, a], where a = \u221a 6\nInputSize+OutputSize is related to the size of input and output."}, {"heading": "4.3 Results", "text": "We first perform experiments to verify the effectiveness of the full-time supervision strategy and the output layer in FTS-BRNN, and then verify the advantage of GRU against LSTM. Finally, we compare our method with other state-of-the-art baselines to show the promising performance of FTS-BRNN. All the results are based on the metric of prediction accuracy. \"InnerP\" represents the results with inner product for prediction as shown in (5), and \"LR\" represents the results with LR for prediction as shown in (6)."}, {"heading": "4.3.1 Effect of Full-Time Supervision and Output Layer", "text": "Because FTS-BRNN always needs the output layer to make the dimensionality of answers and questions be equal, we demonstrate the effect of full-time supervision and output layer based on FTS-BRNN-s. Table 3 shows the results with different configurations for FTS-BRNN and FTS-BRNN-s. Here, \"Pooling-loss\" is the loss used in [14] and (3) while \"FTS-loss\" is our loss function in (2) for full-time supervision. \"has-output\" represents the model with an output layer o(t) =Wo \u00b7 f (t) +Uo \u00b7 b(t) + bias while \"no-output\" represents the model without an output layer by directly using concatenating operation to get the hidden states h(t) = [f (t), b(t)]. The only difference between \"FTS-BRNN-s with pooling loss\" and \"FTS-BRNN-s\" is that \"FTS-BRNN-s with pooling loss\" adopts the loss in (3) and \u201cFTS-BRNN-s\" adopts the loss in (2). We can find that the FTSBRNN-s with full-time supervision can dramatically outperform the counterpart with pooling loss, which successfully verifies the effectiveness of full-time supervision. Furthermore, we can also find that the output layer in FTS-BRNN and FTS-BRNN-s is also very important. In addition, FTS-BRNN is slightly better than FTS-BRNN-s for this factoid QA task."}, {"heading": "4.3.2 Effect of GRU", "text": "The accuracy comparison between GRU and LSTM is shown in Table 4, where \"FTS-BRNN\" is the method proposed in this paper with GRU for BRNN and \"FTS-BRNN with LSTM\" denotes a variant\n3We also try the hierarchical RNN [8, 10] which aims to deal with paragraphs or documents. But it does not bring us better performance.\nby substituting GRU with LSTM. We can find that GRU can outperfom LSTM in our FTS-BRNN model. Hence, our FTS-BRNN adopts GRU for BRNN."}, {"heading": "4.3.3 Comparison to Baselines", "text": "Table 5 reports the accuracy comparison between our method and other state-of-the-art baselines introduced in Section 4.2. Because the public scripts of BOW, BOW-DT and QANTA don\u2019t use inner product to choose answers, we don\u2019t report inner product results of these three methods.\nFrom Table 5, we can find that the results of LR are better than those of inner product. All the deep methods, including QANTA, QA-LSTM, FTS-BRNN-s and FTS-BRNN, can outperform traditional non-deep methods. Furthermore, all the RNN-based methods, including QA-LSTM, FTS-BRNN-s and FTS-BRNN, can outperform recursive neural network based method (QANTA). In addition, our FTS-BRNN and FTS-BRNN-s can outperform all the other state-of-the-art baselines to achieve the best performance."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed a full-time supervision based bidirectional RNN method, called FTS-BRNN, for QA tasks. This is the first work to perform detailed empirical comparison between full-time supervision and pooling-based supervision and explicitly claim that full-time supervision is the key in BRNN for QA. Furthermore, we also find that GRU is better than LSTM for BRNN based QA. Experiments on factoid QA task show that our FTS-BRNN method can outperform other state-of-the-art baselines in real applications. In our future work, we will apply our method to other QA tasks, especially those with long answers."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Besting the quiz master: Crowdsourcing incremental classification", "author": ["Jordan Boyd-Graber", "Brianna Satinoff", "He He", "Hal Daume III"], "venue": "games. EMNLP-CoNLL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Learning phrase representations using rnn encoder decoder for statistical machine", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Question answering passage retrieval using dependency relations", "author": ["Hang Cui", "Renxu Sun", "Keya Li", "Min-Yen Kan", "Tat-Seng Chua"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermanny", "Tomas Kociskyz", "Edward Grefenstettey", "Lasse Espeholty", "Will Kayy", "Mustafa Suleymany", "Phil Blunsomyz"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daume III"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky", "Eduard Hovy"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "A rule-based question answering system for reading comprehension tests", "author": ["Ellen Riloff", "Michael Thelen"], "venue": "IIH-MSP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Lstm-based deep learning models for nonfactoid answer selection", "author": ["Ming Tan", "Cicero dos Santos", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Di Wang", "Eric Nyberg"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["Mengqiu Wang", "Noah A. Smith", "Teruko Mitamura"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": ", quiz bowl) is a special QA task, which has also attracted much attention recently [2, 7].", "startOffset": 84, "endOffset": 90}, {"referenceID": 6, "context": ", quiz bowl) is a special QA task, which has also attracted much attention recently [2, 7].", "startOffset": 84, "endOffset": 90}, {"referenceID": 11, "context": "The first category is based on surface pattern matching which uses manually defined rules [12] or parse dependency trees [16, 4].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "The first category is based on surface pattern matching which uses manually defined rules [12] or parse dependency trees [16, 4].", "startOffset": 121, "endOffset": 128}, {"referenceID": 3, "context": "The first category is based on surface pattern matching which uses manually defined rules [12] or parse dependency trees [16, 4].", "startOffset": 121, "endOffset": 128}, {"referenceID": 1, "context": "This category of methods are mainly for factoid QA tasks [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "Recently, researchers propose to adopt deep methods, especially deep neural network (DNN), for QA tasks [5, 7, 14, 15].", "startOffset": 104, "endOffset": 118}, {"referenceID": 6, "context": "Recently, researchers propose to adopt deep methods, especially deep neural network (DNN), for QA tasks [5, 7, 14, 15].", "startOffset": 104, "endOffset": 118}, {"referenceID": 13, "context": "Recently, researchers propose to adopt deep methods, especially deep neural network (DNN), for QA tasks [5, 7, 14, 15].", "startOffset": 104, "endOffset": 118}, {"referenceID": 14, "context": "Recently, researchers propose to adopt deep methods, especially deep neural network (DNN), for QA tasks [5, 7, 14, 15].", "startOffset": 104, "endOffset": 118}, {"referenceID": 4, "context": "Different from traditional shallow methods which use manually constructed high-dimensional feature vectors to represent questions and answers, deep methods try to learn low-dimensional distributed representation for questions and answers [5, 7, 14, 15].", "startOffset": 238, "endOffset": 252}, {"referenceID": 6, "context": "Different from traditional shallow methods which use manually constructed high-dimensional feature vectors to represent questions and answers, deep methods try to learn low-dimensional distributed representation for questions and answers [5, 7, 14, 15].", "startOffset": 238, "endOffset": 252}, {"referenceID": 13, "context": "Different from traditional shallow methods which use manually constructed high-dimensional feature vectors to represent questions and answers, deep methods try to learn low-dimensional distributed representation for questions and answers [5, 7, 14, 15].", "startOffset": 238, "endOffset": 252}, {"referenceID": 14, "context": "Different from traditional shallow methods which use manually constructed high-dimensional feature vectors to represent questions and answers, deep methods try to learn low-dimensional distributed representation for questions and answers [5, 7, 14, 15].", "startOffset": 238, "endOffset": 252}, {"referenceID": 6, "context": "Furthermore, many works [7, 14] have shown that deep methods can achieve better performance than traditional shallow methods for QA.", "startOffset": 24, "endOffset": 31}, {"referenceID": 13, "context": "Furthermore, many works [7, 14] have shown that deep methods can achieve better performance than traditional shallow methods for QA.", "startOffset": 24, "endOffset": 31}, {"referenceID": 6, "context": "The first category is based on recursive neural network [7], and the second category is based on recurrent neural network (RNN) 1 [5, 14, 15].", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "The first category is based on recursive neural network [7], and the second category is based on recurrent neural network (RNN) 1 [5, 14, 15].", "startOffset": 130, "endOffset": 141}, {"referenceID": 13, "context": "The first category is based on recursive neural network [7], and the second category is based on recurrent neural network (RNN) 1 [5, 14, 15].", "startOffset": 130, "endOffset": 141}, {"referenceID": 14, "context": "The first category is based on recursive neural network [7], and the second category is based on recurrent neural network (RNN) 1 [5, 14, 15].", "startOffset": 130, "endOffset": 141}, {"referenceID": 6, "context": "QANTA [7] is one of the representative deep QA methods based on recursive neural network.", "startOffset": 6, "endOffset": 9}, {"referenceID": 13, "context": "Hence, BRNN has been widely used for QA tasks [14, 5, 15] with promising performance.", "startOffset": 46, "endOffset": 57}, {"referenceID": 4, "context": "Hence, BRNN has been widely used for QA tasks [14, 5, 15] with promising performance.", "startOffset": 46, "endOffset": 57}, {"referenceID": 14, "context": "Hence, BRNN has been widely used for QA tasks [14, 5, 15] with promising performance.", "startOffset": 46, "endOffset": 57}, {"referenceID": 13, "context": "QA-LSTM [14] first uses a gated BRNN, called bidirectional long-short term memory (BLSTM), to extract the feature of questions (answers) and concatenates the hidden states at time step t of both directions in BLSTM to generate the output at time step t.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "[5] uses an attention mechanism on the hidden states to generate the representations for questions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] concatenates the question and answer as one sequence, then uses an output layer to compute similarity directly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Furthermore, some work [9] has shown that BRNN can outperform recursive neural network based methods in many NLP tasks including QA.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "\u2022 Different from existing BRNN methods which use LSTM as the hidden units, FTS-BRNN uses the gated recurrent unit (GRU) [3] as the hidden units.", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "Here, we use quiz bowl as an example to introduce factoid QA [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "Quiz bowl can be seen as a kind of text classification task [2] or QA task [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Quiz bowl can be seen as a kind of text classification task [2] or QA task [7].", "startOffset": 75, "endOffset": 78}, {"referenceID": 12, "context": "RNN is proposed for processing sequential data containing several time steps, which has been widely used in NLP tasks, including neural machine translation (NMT) [13, 1] and QA and so on.", "startOffset": 162, "endOffset": 169}, {"referenceID": 0, "context": "RNN is proposed for processing sequential data containing several time steps, which has been widely used in NLP tasks, including neural machine translation (NMT) [13, 1] and QA and so on.", "startOffset": 162, "endOffset": 169}, {"referenceID": 0, "context": "In [1, 14], a concatenating operation h = [f , b] is adopted.", "startOffset": 3, "endOffset": 10}, {"referenceID": 13, "context": "In [1, 14], a concatenating operation h = [f , b] is adopted.", "startOffset": 3, "endOffset": 10}, {"referenceID": 8, "context": "In [9], an output activation function o = f(WL \u00b7 [f , b]) is used, which can preserve the vector dimensionality with WL \u2208 Rd\u00d72d where d is the dimensionality of f (t) and b.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "The way used in [15] is similar to that in [9], which uses o =W \u00b7 f (t) + U \u00b7 b + bias without activation functions.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "The way used in [15] is similar to that in [9], which uses o =W \u00b7 f (t) + U \u00b7 b + bias without activation functions.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Typical gated RNN includes long short-term memory (LSTM) [6] and gated recurrent unit (GRU).", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "GRU is first proposed in [3] for NMT task.", "startOffset": 25, "endOffset": 28}, {"referenceID": 13, "context": "Furthermore, different from some existing BRNN methods like QA-LSTM [14] which directly concatenate the hidden states of the forward RNN and backward RNN as output, FTS-BRNN adds an output layer on BRNN.", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Most existing methods, such as QA-LSTM [14], use pooling to generate the question representation.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "In [9], the authors also use similar loss as that in (2).", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "However, the motivation of [9] is to perform fair comparison with QANTA because QANTA also uses loss (supervision) at each node (step) in the recursive neural networks.", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "Hence, the authors of [9] do not explicitly claim that full-time supervision is the key in BRNN for QA tasks because they do not perform any empirical comparison between full-time supervision and pooling-based supervision.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "Furthermore, LSTM is adopted in [9], but our FTS-BRNN adopts GRU.", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "1 Dataset We use the factoid QA datasets from [7] for evaluation2.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "Similar to [7], we first filter out all questions that do not belong to history or literature, and then only the answers that occur at least six times will be used.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "edu/~miyyer/qblearn/ which are provided by the authors of [7].", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "The publically available datasets for download are slightly smaller than those used in [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "\u2022 BOW [7]: BOW treats each answer as a class label, and adopts the LR classifier for classification based on the bag-of-words (BOW) features.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "\u2022 BOW-DT [7]: BOW-DT is similar to BOW method by using LR on BOW features.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "\u2022 QANTA [7]: QANTA is a recursive neural network based method proposed in [7].", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "\u2022 QANTA [7]: QANTA is a recursive neural network based method proposed in [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 13, "context": "\u2022 QA-LSTM [14]: QA-LSTM is a LSTM based BRNN method [14] which computes the loss after using a pooling operation and uses concatenating operation to generate the hidden states h for each time step.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "\u2022 QA-LSTM [14]: QA-LSTM is a LSTM based BRNN method [14] which computes the loss after using a pooling operation and uses concatenating operation to generate the hidden states h for each time step.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "As that in QANTA [7], we use the 100-dimensional pre-trained word embedding provided by GloVe [11] to represent the input words for all deep methods.", "startOffset": 17, "endOffset": 20}, {"referenceID": 10, "context": "As that in QANTA [7], we use the 100-dimensional pre-trained word embedding provided by GloVe [11] to represent the input words for all deep methods.", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "Here, \"Pooling-loss\" is the loss used in [14] and (3) while \"FTS-loss\" is our loss function in (2) for full-time supervision.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "We also try the hierarchical RNN [8, 10] which aims to deal with paragraphs or documents.", "startOffset": 33, "endOffset": 40}, {"referenceID": 9, "context": "We also try the hierarchical RNN [8, 10] which aims to deal with paragraphs or documents.", "startOffset": 33, "endOffset": 40}], "year": 2016, "abstractText": "Recently, bidirectional recurrent neural network (BRNN) has been widely used for question answering (QA) tasks with promising performance. However, most existing BRNN models extract the information of questions and answers by directly using a pooling operation to generate the representation for loss or similarity calculation. Hence, these existing models don\u2019t put supervision (loss or similarity calculation) at every time step, which will lose some useful information. In this paper, we propose a novel BRNN model called full-time supervision based BRNN (FTS-BRNN), which can put supervision at every time step. Experiments on the factoid QA task show that our FTS-BRNN can outperform other baselines to achieve the state-of-the-art accuracy.", "creator": "LaTeX with hyperref package"}}}