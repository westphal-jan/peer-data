{"id": "1606.06083", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Product Classification in E-Commerce using Distributional Semantics", "abstract": "Product classification an, task while automatically predicting small subspecies path for a product in example predefined gmos hierarchy given for template product perspective than title. For robust comparable classification 've require for suitable determining free a document (was textual concise include new product) feature vector besides components both fast coding for prediction. To clinton from 100 possibility, still propose just includes distributional dynamical representation they document formula_20 the. We also develop perfect new they - combined tenor difficult innovative (with respect even only taxonomy grass) a path - wise, quantile - wise only nature - wise classifiers for registering reduction on made final premium variable. Our lab doing a effectiveness of on deviations expression and the folk challenging on embedded place one a leading e - affairs new and emphasis what showing saturday as provide meta-data contrast to also possibilities.", "histories": [["v1", "Mon, 20 Jun 2016 12:26:21 GMT  (2011kb,D)", "http://arxiv.org/abs/1606.06083v1", null], ["v2", "Mon, 25 Jul 2016 10:38:52 GMT  (2026kb,D)", "http://arxiv.org/abs/1606.06083v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR", "authors": ["vivek gupta", "harish karnick", "ashendra bansal", "pradhuman jhala"], "accepted": false, "id": "1606.06083"}, "pdf": {"name": "1606.06083.pdf", "metadata": {"source": "CRF", "title": "Product Classification in E-commerce using Distributional Semantics", "authors": ["Vivek Gupta", "Harish Karnick", "Pradhuman Jhala"], "emails": ["vgupta@cse.iitk.ac.in", "hk@cse.iitk.ac.in", "ashendra.bansal@flipkart.com", "pradhuman.jhala@flipkart.com"], "sections": [{"heading": null, "text": "Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction. To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilizing (with respect to the taxonomy tree) a path-wise, node-wise and depth-wise classifiers for error reduction in the final product classification. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve better results on various evaluation metrics compared to earlier approaches."}, {"heading": "1 Introduction", "text": "Existing e-commerce platforms have evolved into large B2C and/or C2C marketplaces having large inventories with millions of products. Products in ecommerce are generally organized into a hierarchical taxonomy of multilevel hierarchical categories. Product classification is an important task in catalog formation and plays a vital role in customer oriented services like search and recommendation and seller\n\u2217Large part of the work was done when the first author was a Semester Extern with the Ecommerce Company.\noriented services like seller utilities on a seller platform. Product classification is a hierarchical classification problem and presents the following challenges: a) a large number of categories have data that is extremely sparse with a skewed long tailed distribution, b) A hierarchical taxonomy imposes constraints on activation of labels. If a child label is active then it is necessary for a parent label to be active, c) For practical use the prediction should happen in real time - ideally within few milli-seconds.\nTraditionally, documents have been represented as a weighted bag-of-words (BoW) or tf-idf feature vector, which contains weighted information about the presence or absence of words in a document by using a fixed length vector.\nWords that define the semantic content of a document are expected to be given higher weight. While tf-idf and BoW representations perform well for simple multi-class classification tasks, they generally do not do as well for more complex tasks because the BoW representation ignores word ordering and polysemy, is extremely sparse and high dimensional and does not encode word meaning. Such disadvantages have motivated continuous, low-dimensional, non-sparse distributional representations. A word is encoded as a vector in a low dimension vector space typicallyR100 toR300. The vector encodes local context and therefore is sensitive to local word order and captures word meaning to some extent. It relies on the \u2018Distributional Hypothesis\u2019(Harris, 1954) i.e. Similar words occur in similar contexts. Similarity between two words can be calculated via cosine distance between their vector representations. ar X iv :1\n60 6.\n06 08\n3v 1\n[ cs\n.A I]\n2 0\nJu n\n20 16\nRecently, Paragraph Vectors (Le and Mikolov, 2014) (Le and Mikolov, 2014) were proposed, which uses global context along with the local context to represent documents. But paragraph vectors suffer from the following problems: a) current techniques embed paragraph vectors in the same space (dimension) as word vectors despite a paragraph consisting of words belonging to multiple topics (senses), b) current techniques also ignore importance and distinctiveness of words across documents. They assume all words contribute equally both quantitatively (weight) and qualitatively (meaning).\nIn this work, we describe a new compositional technique for formation of document vectors from semantically enriched word vectors to address the above problems. Further, to capture importance, weight and distinctiveness of words across documents we use a graded weights approach, inspired by the work of Mukerjee et al. (Pranjal Singh, 2015), for our compositional model. We also propose a new two-level approach for product classification which uses an ensemble of classifiers for label paths, node labels and depth-wise labels (with respect to the taxonomy) to decrease classification error . Our new ensemble technique efficiently exploits the catalog hierarchy and achieves improved results in top K taxonomy path prediction. We show the effectiveness of the new representation and classification approach for product classification of two ecommerce data-sets containing book and non-book descriptions."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Distributional Semantic Word Representation", "text": "The distributional word embedding method was first introduced by Bengio et al. as the Neural Probabilistic Language Model (Bengio et al., 2003). The Neural Probabilistic Language model learns distributional word vectors along with a probability function. The model has prohibitive training complexity due to a large non-linear softmax layer. Later, Mikolov et al. (Mikolov et al., 2013a) proposed a simple log-linear model which considerably reduces training time - Word2Vec Continuous Bag-of-Words (CBoW) model and Skip-Gram with Negative Sampling model.\nThe training objective in CBoW model is to predict the central (middle) word given nearby words in a local window (context words). In Skip grams, the input is the target word and one tries to predict words in the context. See Figure 1 for CBoW (Left) and Skip Gram (Right) Architecture\nLater Glove (Jeffrey Pennington, 2014) a logbilinear model with a weighted least-squares objective was proposed which uses the statistical ratio of global word-word co-occurrence in the corpus for training word vectors.\nThe word vectors learned using the skip-gram model are known to encode many linear linguistic regularities and patterns (Levy and Goldberg, 2014b). The quality of word vectors is tested using the Semantic-Syntactic word relationship test.\nWhile the above methods look very different they implicitly factorize a shifted positive point-wise mutual information matrix (PPMI) with properly tuned hyper parameters as shown by Levy and Goldberg (Levy and Goldberg, 2014c).\nSome variants incorporate ordering information in context words to capture syntactic information by replacing summation of context word vectors with concatenation during training (Wang Ling, 2015) of CBoW and SGNS models. Various modifications have been incorporated to handle the problem of polysemic words(Huang et al., 2012) by clustering context words during training."}, {"heading": "2.2 Distributional Paragraph Representation", "text": "Most models for learning distributed representations for long text such as at the phrase-level, sentencelevel or document level that try to capture semantic composition do not go beyond simple weighted average of word vectors. This approach is analogous to a bag-of-words approach and neglects word order\nwhile representing documents. Socher et al. (Socher et al., 2013) propose a recursive tensor neural network where the dependency parse-tree of the sentence is used to compose word vectors in a bottomup approach to represent sentences or phrases. This approach considers syntactic dependencies but cannot go beyond sentences as it depends on parsing.\nMikolov proposed a distributional paragraph Vector framework called Paragraph vectors which are trained similar to word vectors. He proposed two types of models called Distributed Memory Model Paragraph Vectors (PV-DM) (Le and Mikolov, 2014) and Distributed BoWs paragraph vectors (PVDBoW) (Le and Mikolov, 2014).\nIn PV-DM the model is trained to predict the center word using context words in a small window and the paragraph vector (Le and Mikolov, 2014). Here context words to be predicted are represented by wt\u2212k,....,wt+k and the document vector is represented by Di. In PV-DBoW the paragraph vector is trained to predict context words directly. See Figure 2 for PV-DM(L) and PV-DBoW(R) Architecture.\nThe Paragraph vector presumably represents global semantic meaning of a paragraph and also incorporates properties of word vectors i.e. meanings of the words used. A paragraph vector exhibits a close resemblance to an n-gram model with a large n. This property is crucial because the n-gram model preserves a lot of information in a sentence (and the paragraph) and is sensitive to word order. This model mostly performs better than the BoW models which usually create a very high-dimensional representation leading to poorer generalization."}, {"heading": "2.3 Problem with Paragraph Vectors", "text": "Paragraph vectors obtained from PV-DM and PVDBoW are shared across context words generated from the same paragraph but not across paragraphs. On the other hand a word is shared across paragraphs. Paragraph vectors are also represented in the same space (dimension) as word vectors though a paragraph can contain words belonging to multiple topics (senses). The formulation for paragraph vectors ignores the importance and distinctiveness of a word across documents i.e. assumes all words contribute equally both quantitatively (weight wise) and qualitatively (meaning). Quantitatively, only binary weights i.e. 0 weight for stop-words and non-zero weight for others are used. Intuitively, a paragraph vector should be embedded in a larger and enriched space."}, {"heading": "2.4 Hierarchical Product Categorization", "text": "Most methods for hierarchical classification follow a gates-and-experts method which have a two level classifier. The high-level classifier serves as a \u201cgate\u201d to a lower level classifier called the \u201cexpert\u201d (Shen et al., 2011). The basic idea is to decompose the problem into two models, the first model is simple and does coarse-grained classification while the second model is more complex and does more finegrained classification. The coarse-grained classification deals with a huge number of examples while the fine-grained distinction is learned within a subtree under every top level category with better feature generation and classification algorithms and deals with fewer categories.\nKumar et al. (Kumar et al., 2002), proposed an approach that learnt a tree structure over the set of classes. They used a clustering algorithm based on Fishers discriminant that clustered training examples into mutually exclusive groups inducing a partitioning on the classes. As a result the prediction by this method is faster but the training process is slow as it involves solving many clustering problems.\nLater, Xue et al. (Xue et al., 2008) suggested an interesting two stage strategy called \u201cdeep classification\u201d. The first stage (search) groups documents in the training set that are similar to a given document. In the second stage (classification) a classifier is trained on these classes and used to classify\nthe document. In this approach a specific classifier is trained for each document making the algorithm computationally inefficient.\nFor large scale classification Bengio et al. (Bengio et al., 2010) use the confusion matrix for estimating class similarity instead of clustering data samples. Two classes are assumed to be similar if they are often confused by a classifier. Spectral clustering, where the edges of the similarity graph are weighted by class confusion probabilities, is used to group similar classes together.\nShen and Ruvini (Shen et al., 2012) (Shen et al., 2011) extend the previous approach by using a mixture of simple and complex classifiers for separating confused classes rather then spectral clustering methods which has faster training times. They approximate the similarity of two classes by the probability that the classifier incorrectly predicts one of the categories when the correct label is the other category. Graph algorithms are used to generate connected groups from estimated confusion probabilities. They represent the relationship among classes using an undirected graph G = (V,E), where the set of vertices V is the set of all classes and E is the set of all edges. Two vertices\u2019s are connected by an edge if the confusion probability Conf(c1, c2) is greater than a given threshold \u03b1 .(Shen et al., 2012).\nOther simple approaches like flat classification and top down classification are intractable due to the large number of classes and give poor results due to error propagation as described in (Shen et al., 2012)."}, {"heading": "3 Graded Weighted Bag of Word Vectors", "text": "We propose a new method to form a composite document vector using word vectors i.e. distributional meaning and tf-idf and call it a Graded Weighted Bag of Words Vector (gwBoWV). gwBoWV is inspired from the computer vision literature where we use a Bag of Visual words to form feature vectors. gwBoWV is calculated as follows:\n1. Each document is represented in a lower dimensional spaceD = k \u2217d, where k represents number of semantic topics and d is the dimension of the word-vectors.\n2. Each document is also weighted with idf values of the semantic topic present in the text calcu-\nlated using idf values of words present in the document.\nIdf values from the training corpus are directly used for the test corpus for weighting. Word vectors are first separated into a pre-defined number of semantic clusters using a suitable clustering algorithm (e.g. k-means). For each document and cluster we add word-vectors of words from that document and cluster to form a cluster vector. We finally concatenate cluster vectors from various clusters for each document to compose the document vector. Algorithm 1 describes this in more detail.\nAlgorithm 1: Graded Weighted Bag of Word Vectors\nData: Documents Di, n = 1 . . . N Result: Document vectors ~gwBoWVDn , n = 1\n. . . N 1 Train SGNS model to obtain word vector\nrepresentation (wvn) using all document Dn, n = 1..N ;\n2 Calculate idf values for all words: idf(wj), j = 1..|V |, |V | is vocabulary size; 3 Use K-means algorithm for clustering all words in V using their word-vectors into K clusters; 4 for i \u2208 (1..N) do 5 Initialize cluster vector ~cvk = ~0, k = 1..K; 6 initialize cluster frequency icfk = 0, k = 1..K; 7 while not at end of document Di do 8 read current word wj and obtain wordvec ~wvj ; 9 obtain cluster index k = idx( ~wvj) for\nwordvec ~wvj ; 10 update cluster vector ~cvk + = ~wvj ; 11 update cluster frequency icfk + = idf(wj); 12 end 13 obtain ~BoVWDi = \u2295K i=k ~cvk, here \u2295 represents concatenation; 14 obtain ~gwBoWVDi = \u2295K i=k ~cvk\u2295K i=k icfk, here \u2295; 15 end\nSince semantically different vectors are in separate clusters we avoid averaging of semantically dif-\nferent words during Bag of Words Vector formation. Incorporation of idf values captures the weight of each cluster vector which tries to model the importance and distinctiveness of words across documents."}, {"heading": "4 Ensemble of Multitype Predictors", "text": "We propose a two level ensemble technique to combine multiple classifiers predicting product paths, node labels and depth-wise labels respectively. We construct an ensemble of multi-type features for categorization inspired by the recent work of Zornitsa et. el. from Yahoo Labs (Kozareva, 2015). Below are the details of each classifier used at level one:\n\u2022 Path-Wise Prediction Classifier: We take each possible path in the catalog taxonomy tree, from leaf node to root node, as a possible class label and train a classifier (PP ) using these labels.\n\u2022 Node-Wise Prediction Classifier: We take each possible node in the catalog taxonomy tree as a possible prediction class and train a classifier (NP ) using these class labels.\n\u2022 Depth-Wise Node Prediction Classifiers: We train multiple classifiers (DNPi) one for each depth level of the taxonomy tree. Each possible node in the catalog taxonomy tree at that depth is a possible class label. All data samples which have a potential node at depth k, in addition 10% samples of data points which have no node at depth k (sample of data point whose path ended before depth k) are used for training.\nWe use the output probabilities of these classifiers at level one (PP , NP , DNPi) as a feature vector and train a classifer (level two) after some dimensionality reduction.\nThe increase in training time can be reduced by training all level one classifiers in parallel. The algorithm for training the ensemble is described in Algorithm 2. The testing algorithm is similar to training and described in in supplementary section 3.\nAlgorithm 2: Training Two Level Boosting Approach\nData: Catalog Taxonomy Tree (T) of depth K and training data D = (d, pd) where d is the product description and pd is the taxonomy path label. Result: Set of level one Classifiers C = {PP,NP,DNP1, . . . , DNPK} and level two classifier FPP .\n1 Obtain ~gwBoWVd features for each product description d ; 2 Train Path-Wise Prediction Classifier (PP ) with possible classes as product taxonomy paths (pd); 3 Train Node-Wise Prediction Classifier (NP ) with possible classes as nodes in taxonomy path i.e. (nd). Here each description will have multiple node labels. 4 for k \u2208 (1 . . .K) do 5 Train Depth-Wise Node Classifier for depth\nk (DNPK) with labels as nodes at depth k i.e. (nk)\n6 end 7 Obtain output probabilities ~PX over all classes\nfor each level one classifier X i.e. ~PPP , ~PNP and ~PDNPk , k = 1..K.;\n8 Obtain feature vector ~FVd for each description as:\n~FVd = ~gwBoWVd \u2295 ~PPP \u2295 ~PNP K\u2295 i=k ~PDNPk\n(1) Here \u2295 is the concatenation operation ;\n9 Reduce feature dimension ( ~RFVd) using suitable supervised feature selection technique based on mutual information criteria;\n10 Train Final Path-Wise Prediction Classifier ( ~FPPd) using RFVd as feature vector and possible class labels as product taxonomy paths (pd)"}, {"heading": "5 Dataset", "text": "We use seller product descriptions and title samples from a leading e-commerce site for experimentation1. The data set had two product taxonomies:nonbook and book. Non-book data is more discriminative with average description + title length of around 10 to 15 words, whereas book descriptions have an average length greater than 200 words. To give more importance to the title we generally weight it three times the description value. The distribution of items over leaf categories (verticals) exhibits high skewness, sparseness and heavy tailed nature and suffer from sparseness as shown in Figure 3. We use random forest and k nearest neighbor as base classifiers as they are less affected by data skewness\nWe have removed data samples with multiple paths to simplify the problem to single path prediction instead of multi-path prediction. Overall, we have 0.16 million training and 0.11 million testing samples for book data and 0.5 million training and 0.25 million testing non-book data. Since the taxonomy evolved over time all category nodes are not\n1This data is proprietary to the e-commerce Company.\nsemantically mutually exclusive. Some ambiguous leaf categories are even meta categories. We handle this by giving a unique id to every node in the category tree of book-data. Furthermore, there are also category paths with different categories at the top and similar categories at the leaf nodes i.e. reduplication of the same path with synonymous labels.\nThe quality of the descriptions and titles also varies a lot. There are titles and descriptions that do not contain enough information to decide an unique appropriate category.\nThere were labels like Others and General at various depths in the taxonomy tree which carry no specific semantic meaning. Also, descriptions with the special label \u2018wrong procurement\u2019 are removed manually for consistency."}, {"heading": "6 Results", "text": "The classification system is evaluated using the usual precision metric defined as fraction of products from test data for which the classifier predicts correct taxonomy paths. Since there are multiple similar paths in the data set predicting a single path is not appropriate. One solution is to predict more than one path or better a ranked list of of 3 to 6 paths with predicted label coverage matching labels in the true path. The ranking is obtained using the confidence score of the predictor. We also calculate the confidence score of the correct prediction path by using the k (3 to 6) confidence scores of the individual predicted paths. For the purpose of measuring accuracy when more than one path is predicted, the classifier result is counted as correct when the correct class (i.e. path assigned by seller) is one of the returned class (paths). Thus we calculated Top 1, Top 3 and Top 6 prediction accuracy when 1, 3 and 6 paths are predicted respectively."}, {"heading": "6.1 Non-Book Data Result", "text": "We also compare our results with document vectors formed by averaging word-vectors of words in the document i.e. Average Word Vectors (AWV), Distributed Bag of Words version of Paragraph Vector by Mikolov (PV-DBoW), Frequency Histogram of word distribution in Word-Clusters i.e. Bag of Cluster Vector (BoCV). We keep the classifier (random forest with 20 trees) common for all document vec-\ntor representations. We compare performance with respect to number of clusters, word-vector dimension, document vector dimension and vocabulary dimension (tf-idf) for various models.\nFigure 4 shows results for a random forest (20 trees) on various classifiers trained by various methods on 0.2 million training and 0.2 million testing samples with 3589 classes. It compares our approach gwBoWV with PV-DBoW and PV-DM models with varying word vector dimension and number of clusters. Clearly gwBoWV performs much better than other methods especially PV-DBoW and PVDM.\nWe use four evaluation metrics to measure performance for the top k predictions as described below:\n1. Path Precision @ k (PP) : PP@k = (pi/(p1 + p2 + . . .+ pk)), where pi is actual path prediction probability and p1, . . . , pk are top k predicted path probabilities.\n2. Count Precision @ k (CP) : CP@k = 1 if actual path is predicted in top k predicted paths else PP@k = 0.\n3. Label Precision @ k (LP) : LP@k = (\n#(True Labels \u222a { \u2229 Predicted Labels })/#True Labels)\nin top k predicted paths.\n4. Tag Commonality @ k (TC) : TC@k = ( #{ \u2229\nPredicted Labels })/#{ \u222a Predicted Labels } )\nin top k predicted paths.\nTable 2 shows the results on all evaluation metrics with varying word-vec dimension and clusters. Table 3 shows results of top 6 paths prediction for tfidf baseline with varying dimension."}, {"heading": "6.2 Book Data Result", "text": "Book data is harder to classify. There are more cases of improper paths and labels in the taxonomy and hence we had to do a lot of pre-processing. Around 51% of the books did not have labels at all and 15% books were given extremely ambiguous labels like \u2018general\u2019 and \u2018others\u2019. To maintain consistency we prune the above 66% data samples and work with the remaining 44% i.e. 0.37 million samples.\nTo handle improper labels and ambiguity in the taxonomy we use multiple classifiers one predicting path (or leaf) label, another predicting node labels and multiple classifiers, one at each depth level of the taxonomy tree, that predict node labels at that level.\nIn depth-wise node classification we also introduce the \u2018none\u2019 label to denote missing labels at a particular level i.e. for paths that end at earlier levels. However we only take a random strata sample for this \u2018none\u2019 label. Table 4 shows results for depth wise prediction at various depth."}, {"heading": "6.3 Ensemble Classification", "text": "We use the ensemble of multi-type predictors as describe in Section 4 for final classification. For dimensionality reduction we use feature selection methods based on mutual information criteria (ANOVA F-value i.e. analysis of variance). We obtain improved results for all four evaluation metrics. The list below says how the first column in Table 5 should be interpreted.\n\u2022 tf-idf (A-2-C): term frequency and inverse document frequency feature with #A top 1, 2 gram words and #C random forest trees\n\u2022 path-1 (A-C): path prediction model without ensemble, trained with gwBoWV with #A (cluster*wordvec dimension) using C trees\n\u2022 depth (A+B-C): trained with gwBoWV with A features, B represents size of out-probability vectors (#total nodes) for all depths using depth classifier of level 1 using #C trees.\n\u2022 node (A+B): trained with gwBoWV with A features, B represents size of output probability vector(#total nodes) by level 1 node classifier using #C tree.\n\u2022 comb-2(A): level two combined ensemble classifier with A reduced features (original features 21706).\nTo predict the path using depth-wise predictions we use an ensemble approach where we train another classifier using an MIC based reduced dimension vector. The original vector is a concatenation of\noutput probabilities of level one classifiers at each depth. The reduced vectors together with the path label are used to train a level two classifier. We obtain the best results on a reduced dimension of 2600 (original dimension = 7975)."}, {"heading": "7 Conclusions", "text": "We presented a novel compositional technique based on a distributional representation i.e. embedded word vectors to form appropriate document vectors. Further, to capture importance, weight and distinctiveness of words across documents we used a graded weighting approach for composition based on recent work by Mukerjee et. el. (Pranjal Singh, 2015). Our document vectors are embedded in a vector space different from the word embedding vector space. This document vector space is higher dimensional and tries to encode the intuition that a document has more topics or senses than a word.\nWe also developed a new technique which uses an ensemble of multiple classifiers that predicts label paths, node labels and depth-wise labels to decrease classification error.We tested our methods on data sets from a leading e-commerce platform and show good improvements in performance."}, {"heading": "8 Future Work", "text": "Much more work is possible to improve the quality of learned representations and extend the model to incorporate additional data and relations for joint relation modeling and prediction. Better performance may be achieved by Jointly learn vector representation and clustering using non parametric clustering methods for making semantic clusters."}, {"heading": "9 Supplementary Material", "text": ""}, {"heading": "9.1 Label Embedding Based Approach", "text": "Apart from tree based approaches there are label based embedding approches for Product Classification. Wei and Kwok (Bi and Kwok, 2011) suggested a label based embedding approach which exploits label dependency in tree-structured hierarchies for hierarchical classification. Kernel Dependency Estimation (KDE) is used to first project or embed the label vector (multi-label) in fewer orthogonal dimensions. An advantage of this approach is that all m learners in the projected space can learn from the full training data. In contrast n tree based methods training data reduces as we reach leaf nodes.\nTo preserve dependencies during prediction the authors suggest a greedy approach. The problem can be efficiently solved using a greedy algorithm called Condensing Sort and Select Algorithm. However, the algorithm is computationally intensive."}, {"heading": "9.1.1 Dependency Based Word Vectors", "text": "SGNS and CBoW both use linear bag of words context for training word vectors (Mikolov et al., 2013b). Levy and Goldberg (Levy and Goldberg, 2014a) suggested use of arbitrary functional context instead like syntactic dependencies generated from a parse of the sentence. Each word w and its modifiers m1, . . . ,mk are extracted from a sentence parse. Contexts in the form (m1, lbl1, . . . ,mk, lblk ) are generated for every sentence. Here lbl is the dependency relationship type between word and the modifier and lbl\u22121 is used to denote the inverse relationship. Figure 5 shows dependency based context for words in a given sentence.\nThe dependency based word vectors use the same training methods as SGNS. Compared to similarly learned linear context based vectors learned it is found that the dependency based vectors perform better on functional similarity. However, for the task of topical similarity estimation the linear context based word vectors encode better distributional semantic content.\nAlgorithm 3: Testing Two Level Boosting Approach\nData: Catalog Taxonomy Tree (T) of depth K and testing data D = (d,pd) where d is product description pd is taxonomy of paths. Set of level one Classifiers C = {PP ,NP ,DNP1 . . . DNPK} and final level two classifier FPP Result: top m prediction path Pdi for training description d, here i = 1 . . . m\n1 Obtain ~gwBoWVd features for each product description d in test data; 2 Get Prediction Probabilities from all level one classifiers to obtain level two feature vector ( ~FVd) using Equation 1; 3 Obtain ( ~RFVd) reduced feature vector; 4 Output top m paths from final prediction using\noutput probabilities from level two classifier FFP for description d."}, {"heading": "9.2 Example of gwBoWV Approach", "text": "1. Assume there are four clusters C = [C1, C2, C3, C4], here Ci represents the ith cluster\n2. Let Dn = [w1, . . . , w9, w10] be a document consisting of words w1, w2, . . . , w10 in order, whose document vectors need to be composed using word vectors ~wv1, . . . , ~wv10 respectively. Let us assume the following word-cluster assignment for document Dn as given in Table 6\n3. Obtain cluster Ci\u2019s contribution in document Dn by summation of word vectors for words coming from document Dn and cluster Ci:\n\u2022 ~cv1 = ~wv4+ ~wv3 + ~wv10+ ~wv5 \u2022 ~cv2 = ~wv9\n\u2022 ~cv3 = ~wv1 + ~wv6+ ~wv2 \u2022 ~cv4 = ~wv8+ ~wv7\nSimilarly, calculate idf values for each cluster Ci for document Dn:\n\u2022 icf1 = idf(w4)+idf(w3) +idf(w10)+idf(w5) \u2022 icf2 = idf(w9) \u2022 icf3 = idf(w1) + idf(w6)+idf(w2) \u2022 icf4 = idf(w8)+idf(w7)\n4. Concatenate cluster vectors to form Bag of Words Vector of dimension, #cluster \u00d7 #wordvec:\n~BoWV (Dn) = ~cv1 \u2295 ~cv2 \u2295 ~cv3 \u2295 ~cv4 (2)\n5. Concatenate word-cluster idf values to form graded weighted Bag of Word Vector of dimension #cluster \u00d7#wordvec+#cluster:\n~gwBoWV (Dn) = ~cv1 \u2295 ~cv2 \u2295 ~cv3 \u2295 ~cf4\u2295 icf1 \u2295 icf2 \u2295 icf3 \u2295 icf4.\n(3)"}, {"heading": "9.3 Quality of WordVec Clusters", "text": "Below are examples of words contained in some clusters and their possible cluster topic meaning for the book data. Each cluster is formed using clustering of word-vectors where the word belongs to particular topics. We number clusters according to the distance of the centroid from the origin to avoid confusion.\n1. Cluster #0 basically talks about crime and punishment related terms like accused, arrest, assault, attempted, beaten, attorney,brutal,confessions, convicted cops, corrupt, custody, dealer, gang, investigative, gangster, guns, hated, jails, judge, mob, undercover, trail, police, prison, lawyer, torture, witness etc\n2. Cluster #10 talks about scientific experiments related terms like yield, valid, variance, alternatives, analyses, calculating, comparing, assumptions, criteria, determining, descriptive, evaluation, formulation, experiments, measures model, parameters, inference, hypothesis etc\nSimilarly, Cluster #13 is talking about dating and marriage, Cluster #11 about tools and tutorials and Cluster #15 about persons. Other clusters also represent single or multiple topics similiar to each other. Similarity of words within a cluster represents efficient distributional semantic representation of wordvectors trained by the SGNS model."}, {"heading": "9.4 Two Level Classification Approach", "text": "We also experimented with a modified approach of two level classification given by Shen and Ruvini (Shen et al., 2011) (Shen et al., 2012) as describe in Section 2.4. However, instead of randomly giving direction and then finding a dense graph using Strongly Connected Components, we decided the edge direction from misclassification and used various methods like weakly connected component, bi connected component and articulation points to find Highly Connected Component. We followed this approach to improve sensitivity and cover missing edges as discussed in section 2.4. The value of the confusion probability and direction of edges is decided by the value of (i, j) element in the confusion matrix (CM)."}, {"heading": "9.5 Confused Category Group Discovery", "text": "Figure 6 shows Hard Disk, Hard Drive, Hard Disk Case and Hard Drive Enclosure are misclassified as each other and form a latent group in Computer and Computer accessories extracted by finding bi-connected components in the misclassification graph.\nFigure 7 shows the final latent groups discovered (color bubble) in Non-Book Data using graded weighted Bag of Word Vector methods and random forest classifier without class balance on raw data with varying thresholds on #mis classification for dropping edges based on edge weight.\nAlgorithm 4: Modified Connected Component Grouping Data: Set of Categories C = {c1, c2, c3...cn}\nand threshold \u03b1 Result: Set of dense sub-graphs\nCG = {cg1, cg2, cg3, cg4 . . . cgm} representing highly connected groups\n1 Train a weak classifier H on all possible categories ; 2 Compute pairwise confusion probabilities between classes using values from the confusion matrix (CM).\nConf(ci, cj) = { CM(ci, cj), if CM(ci, cj) \u2265 \u03b1 0 otherwise (4) here, Conf(ci, cj) may not be equal to Conf(cj , ci) due to non symmetric nature of Confusion Matrix CM .;\n3 Construct confusion graph G = (E, V ) with vertices (V ) as confused categories and edges (Eij) from i\u2192 j with weight = Conf(ci, cj).; 4 Apply Bi-Connected Component, Strongly Connected Component or Weakly Connected Component finding graph algorithm on G to obtain set of dense sub-graphs CG = {cg1, cg2, cg3, cg4 . . . cgm}."}, {"heading": "9.6 Data Visualization Tree Maps", "text": "We visualize the taxonomy for some main categories using Tree-Maps. Figures 9 - 10 show tree maps at various depths for the book taxonomy. It is evident from these maps that the tree exhibits high skewness and heavy tailed nature."}, {"heading": "9.7 More Results for Ensemble Approach", "text": "We use kNN and random forest for initial classification instead of SVM because of better stability to class imbalance and better performance due to generation of good set of meta features. Also SVM doesn\u2019t perform well with huge number of classes. Table 7 confirm the same empirically\nWe observed improvement in classification accuracy by using Shen and Lee approach of two level classifier for discovering latent groups and running fine classifiers on them. Table 9 shows improvement in accuracy by a level two classifier.\nTo prove that books were more confusing compared to non-book, we did a small experiment. We sampled all computer and computer accessories and Computer related books and binary labeled them and compared this with a direct classifier without binary labelling. The results are in table 10.\nResults from various approaches on top 3 taxonomy prediction are in Table 13. 12 shows results of node level two classifier on various reduced dimension vectors (using ANOVA) - original vectors were concatenated output probabilities of node prediction probabilities using gwBoWV. 12 show results of level one classifier on various reduced dimension vectors (using ANOVA) where original vectors were gwBoWV. Ensemble and gwBoWV perform better than other approaches."}, {"heading": "9.8 Classification Example from Book Data", "text": "Description : ignorance is bliss or so hopes antoine the lead character in martin pages stinging satire"}, {"heading": "1000 40.30 74.45 86.38 22.47", "text": ""}, {"heading": "2000 40.88 74.92 86.87 22.56", "text": ""}, {"heading": "3000 40.99 75.24 87.06 22.60", "text": ""}, {"heading": "4000 41.11 75.24 87.07 22.53", "text": ""}, {"heading": "1000 46.26 72.46 84.84 24.85", "text": ""}, {"heading": "2000 47.05 .72.26 84.52 25.05", "text": ""}, {"heading": "2500 .47.70 72.77 84.58 24.81", "text": ""}, {"heading": "3000 .44.45 73.84 85.83 23.74", "text": "how i became stupida modern day candide with a darwin award like sensibility a twenty five year old aramaic scholar antoine has had it with being brilliant and deeply self aware in todays culture so tortured is he by the depth of his perception and understanding of himself and the world around him that he vows to renounce his intelligence by any means necessary in order to become stupid enough to be a happy functioning member of society what follows is a dark and hilarious odyssey as antoine tries everything from alcoholism to stock trading in order to lighten the burden of his brain on his soul. how i became stupid. how i became stupid. how i became stupid Actual Class : books-tree\u2192 literature and fiction\nPredictions, Probability\nbooks-tree \u2192 literature and fiction \u2192 literary collections\u2192 essays 0.1 books-tree \u2192 reference \u2192 bibliographies and indexes 0.1 books-tree \u2192 hobbies and interests \u2192 travel \u2192 other books\u2192 reference 0.1 books-tree \u2192 children \u2192 children literature \u2192 fairy tales and bedtime stories 0.1 books-tree\u2192 dummy 0.2\nDescription : harpercollins continues with its commitment to reissue maurice sendaks most beloved works in hardcover by making available again this 1964 reprinting of an original fairytale by frank r stockton as illustrated by the incomparable maurice sendak in the ancient country of orn there lived an old man who was called the beeman because his whole time was spent in the company of bees one day a junior sorcerer stopped at the hut of the beeman the junior sorcerer told the beeman that he has been transformed if you will find out what you have been transformed from i will see that you are made all right again said the sorcerer could it have been a giant or a powerful prince or some gorgeous being whom the magicians or the fairies wish to punish the beeman sets out to discover his original form. the beeman of orn. the beeman of orn. the beeman of orn. Actual Class : books-tree\u2192 children\u2192 knowledge and learning \u2192 animals books \u2192 reptiles and amphibians\nPredictions, Probability\nbooks-tree \u2192 children \u2192 knowledge and learning \u2192 animals books \u2192 reptiles and amphibians , 0.28 books-tree\u2192 children\u2192 fun and humor, 0.72\nDescription : a new york times science reporter makes a startling new case that religion has an evolutionary basis for the last 50000 years and probably much longer people have practiced religion yet little attention has been given to the question of whether this universal human behavior might have been implanted in human nature in this original and thought provoking work nicholas wade traces how religion grew to be so essential to early societies in their struggle for survival how an instinct for faith became hardwired into human nature and how it provided an impetus for law and government the faith instinct offers an objective and non polemical exploration of humanitys quest for spiritual transcendence. the faith instinct how religion evolved and why it endures. the faith instinct how religion evolved and why it endures. the faith instinct how religion evolved and why it endures Actual Class : books-tree\u2192 academic texts\u2192 hu-\nmanities Predictions, Probability books-tree\u2192 academic texts\u2192 humanities 0.067 books-tree \u2192 religion and spirituality \u2192 new age and occult\u2192 witchcraft and wicca 0.1 books-tree\u2192 health and fitness\u2192 diet and nutrition \u2192 diets 0.1 books-tree\u2192 dummy 0.4\nDescription : behavioral economist and new york times bestselling author of predictably irrational dan ariely returns to offer a much needed take on the irrational decisions that influence our dating lives our workplace experiences and our general behaviour up close and personal in the upside of irrationality behavioral economist dan ariely will explore the many ways in which our behaviour often leads us astray in terms of our romantic relationships our experiences in the workplace and our temptations to cheat blending everyday experience with groundbreaking research ariely explains how expectations emotions social norms and other invisible seemingly illogical forces skew our reasoning abilities among the topics dan explores are what we think will make us happy and what really makes us happy why learning more about people make us like them less how we fall in love with our ideas what motivates us to cheat dan will emphasize the important role that irrationality plays in our daytoday decision making not just in our financial marketplace but in the most hidden aspects of our livesabout the author an ariely is the new york times bestselling author of predictably irrational over the years he has won numerous scientific awards and his work has been featured in leading scholarly journals in psychology economics neuroscience and in a variety of popular media outlets including the new york times the wall street journal the washington post the new yorker scientific american and science. the upside of irrationality. the upside of irrationality. the upside of irrationality Actual Class : books-tree\u2192 business, investing and management\u2192 business\u2192 economics\nPredictions, Probability books-tree\u2192 business, investing and management \u2192 business \u2192 economics 0.15 books-tree\u2192 philosophy\u2192 logic 0.175 books-tree\u2192 self-help\u2192 personal growth 0.21\nbooks-tree\u2192 academic texts\u2192 mathematics 0.465"}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Label embedding trees for large multi-class tasks", "author": ["Bengio et al.2010] Samy Bengio", "Jason Weston", "David Grangier"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2010}, {"title": "Multi-label classification on tree- and dag-structured hierarchies", "author": ["Bi", "Kwok2011] Wei Bi", "James T. Kwok"], "venue": null, "citeRegEx": "Bi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bi et al\\.", "year": 2011}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington and Socher.,? \\Q2014\\E", "shortCiteRegEx": "Pennington and Socher.", "year": 2014}, {"title": "Everyone likes shopping! multi-class product categorization for e-commerce", "author": ["Zornitsa Kozareva"], "venue": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL,", "citeRegEx": "Kozareva.,? \\Q2015\\E", "shortCiteRegEx": "Kozareva.", "year": 2015}, {"title": "Hierarchical fusion of multiple classifiers for hyperspectral data analysis", "author": ["Kumar et al.2002] Shailesh Kumar", "Joydeep Ghosh", "M. Melba Crawford"], "venue": "Pattern Analysis and Applications,", "citeRegEx": "Kumar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2002}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Dependencybased word embeddings", "author": ["Levy", "Goldberg2014a] Omer Levy", "Yoav Goldberg"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014b] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014c] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Words are not equal: Graded weighting model for building composite document vectors", "author": [], "venue": "In Proceedings of the twelfth International Conference on Natural Language Processing (ICON-2015). BSP Books", "citeRegEx": "Singh.,? \\Q2015\\E", "shortCiteRegEx": "Singh.", "year": 2015}, {"title": "Item categorization in the e-commerce domain", "author": ["Shen et al.2011] Dan Shen", "Jean David Ruvini", "Manas Somaiya", "Neel Sundaresan"], "venue": "In Proceedings of the 20th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Shen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2011}, {"title": "Large-scale item categorization for e-commerce", "author": ["Shen et al.2012] Dan Shen", "Jean-David Ruvini", "Badrul Sarwar"], "venue": "In Proceedings of the 21st ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Shen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "Proceedings of the conference", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Two/too simple adaptations of wordvec for syntax problems. In Proceedings of the 50th Annual Meeting of the North American Association for Computational Linguistics", "author": [], "venue": null, "citeRegEx": "Ling.,? \\Q2015\\E", "shortCiteRegEx": "Ling.", "year": 2015}, {"title": "Deep classification in large-scale text hierarchies", "author": ["Xue et al.2008] Gui-Rong Xue", "Dikan Xing", "Qiang Yang", "Yong Yu"], "venue": "In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Xue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "as the Neural Probabilistic Language Model (Bengio et al., 2003).", "startOffset": 43, "endOffset": 64}, {"referenceID": 3, "context": "Various modifications have been incorporated to handle the problem of polysemic words(Huang et al., 2012) by clustering context words during training.", "startOffset": 85, "endOffset": 105}, {"referenceID": 16, "context": "(Socher et al., 2013) propose a recursive tensor neural network where the dependency parse-tree of the sentence is used to compose word vectors in a bottomup approach to represent sentences or phrases.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "The high-level classifier serves as a \u201cgate\u201d to a lower level classifier called the \u201cexpert\u201d (Shen et al., 2011).", "startOffset": 93, "endOffset": 112}, {"referenceID": 6, "context": "(Kumar et al., 2002), proposed an approach that learnt a tree structure over the set of classes.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "(Xue et al., 2008) suggested an interesting two stage strategy called \u201cdeep classification\u201d.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "(Bengio et al., 2010) use the confusion matrix for estimating class similarity instead of clustering data samples.", "startOffset": 0, "endOffset": 21}, {"referenceID": 15, "context": "Shen and Ruvini (Shen et al., 2012) (Shen et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 14, "context": ", 2012) (Shen et al., 2011) extend the previous approach by using a mixture of simple and complex classifiers for separating confused classes rather then spectral clustering methods which has faster training times.", "startOffset": 8, "endOffset": 27}, {"referenceID": 15, "context": "(Shen et al., 2012).", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "Other simple approaches like flat classification and top down classification are intractable due to the large number of classes and give poor results due to error propagation as described in (Shen et al., 2012).", "startOffset": 191, "endOffset": 210}, {"referenceID": 5, "context": "from Yahoo Labs (Kozareva, 2015).", "startOffset": 16, "endOffset": 32}], "year": 2017, "abstractText": "Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction. To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilizing (with respect to the taxonomy tree) a path-wise, node-wise and depth-wise classifiers for error reduction in the final product classification. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve better results on various evaluation metrics compared to earlier approaches.", "creator": "LaTeX with hyperref package"}}}