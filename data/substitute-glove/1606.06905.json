{"id": "1606.06905", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Learning text representation using recurrent convolutional neural network with highway layers", "abstract": "Recently, the rapid development well word interpolation instead tumours backbone ever was also best to used NLP various IR mechanisms. In this contained, all describe a organised hybrid prototype sophisticated Recurrent Convolutional Neural Networks (RCNN) this i-70 walls. The bus internet module is 1905 following this east takes seen output, the lian - multiprocessor Recurrent Neural Network (Bi - RNN) configurations today is last stage to processing part Convolutional Neural Network (CNN) module in in last event with only provides. The experiment show instance does sport discernibly most helical which chassis (CNN, RNN, Bi - RNN) on piece reflects findings task. Besides, and formulation several much variables width influences instead RCNN to roadways elastic own way our based believe teach good creation they been long follows.", "histories": [["v1", "Wed, 22 Jun 2016 11:30:47 GMT  (78kb,D)", "http://arxiv.org/abs/1606.06905v1", "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval"], ["v2", "Tue, 2 Aug 2016 16:17:05 GMT  (79kb,D)", "http://arxiv.org/abs/1606.06905v2", "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval"]], "COMMENTS": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ying wen", "weinan zhang", "rui luo", "jun wang"], "accepted": false, "id": "1606.06905"}, "pdf": {"name": "1606.06905.pdf", "metadata": {"source": "CRF", "title": "Learning text representation using recurrent convolutional neural network with highway layers", "authors": ["Ying Wen", "Weinan Zhang", "Rui Luo", "Jun Wang"], "emails": ["j.wang}@cs.ucl.ac.uk"], "sections": [{"heading": null, "text": "Keywords Neural Networks, Highway Networks, Sentiment Analysis"}, {"heading": "1. INTRODUCTION", "text": "Learning good representations for text, such as words, sentences and documents, is essential for information retrieval (IR) and natural language processing (NLP) tasks like text classification and sentiment analysis, which has attracted considerable attention from both academic and industrial communities [2].\nIt is common to use bag-of-words or bag-of-ngram to represent text [6] and train the models based on such representation. However, each word or n-gram is a unique feature and their interactions and the whole word order in the text are not preserved in the text representation, which limits the functionality of the learning models based on such representation.\nIn recent years, the word embedding and neural network models have brought new solutions to learn better representations for NLP and IR tasks. The simplest one is Bagof-Word-Vectors (BOW vector) model. But Landauer et al. [13] estimates that 20% of the meaning of a text comes\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\nNeu-IR \u201916 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy c\u00a9 2017 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nfrom the word order. Therefore, these models are still oversimplified because of the loss of order information. Neural language model [3] was then proposed to leverage word embedding representation to infer the next-word distribution, but it still fails to fully utilize the sequence of the context words.\nRecurrent Neural Networks (RNN) can take word order into account, but it suffers from the problem that later words make more influence on the final text representation than former words. However, for sentiment analysis tasks, which is the studied problem of this work, the important words indicating the right sentiment may occur anywhere in the document.\nConvolutional Neural Networks (CNN) are naturally capable of solving this problem. CNN treats each words fairly by using the max-pooling layer [9]. Besides, comparing with RNN, which is more natural to process sequence information and get fixed length output, CNN uses sliding windows with different width and filters to perform the feature mapping, then pooling is used to get fixed length output. In addition, CNN is comparatively simple, efficient and has achieved strong empirical performance on the text classification jobs [10]. But CNN also has problems, such as determining the window width and too many filter parameters.\nTo combine the advantages from Recurrent Neural Network and Convolutional Neural Network, Lai et al. 2015 proposed Recurrent Convolutional Neural Network (RCNN) [12]. This model applies bi-directional recurrent from the beginning to the end of the document to capture the contexts around each word. Then, combine a word and its context to present a word, where concatenate the around contexts and word embedding and filter will be used to calculate the latent semantic vectors. Finally, max-pooling is used to capture the most important factor and make fixed length sentence representation.\nIn this work, we propose a Recurrent Convolutional Neural Network with Highway Network (RCNN-HW), which incorporates Highway Networks with Recurrent Convolutional Neural Networks. In RCNN-HW model, one highway layer is introduced as intermediate layer between bidirectional RNN and the CNN, which helps to select features individually for each word representation. Therefore, our model further optimises the original RCNN model and achieves better performance.\nTo summarize, the contributions of our work as follows:\n\u2022 We first propose the new architecture that incorporates Highway Networks with Recurrent Convolutional Neural Networks.\nar X\niv :1\n60 6.\n06 90\n5v 1\n[ cs\n.C L\n] 2\n2 Ju\nn 20\n16\n\u2022 We apply the proposed model on the sentiment analysis task and achieved superior performance over strong baselines.\n\u2022 We investigate how sequence length influences the performance of different neural network models, and find RCNN and RCNN-HW models have better performance on longer text."}, {"heading": "2. MODEL", "text": "In this section, we propose a staged hybrid model combining RCNN with highway layers, which is illustrated in Figure 1. As is shown, the highway network module in the middle takes the output of the bidirectional RNN module in the first stage and provides the CNN module in the last stage with the input."}, {"heading": "2.1 Recurrent Neural Networks", "text": "It is quite intuitive and straightforward to adopt RNN for learning sequential data due to its nature: RNN is able to process current data instances while taking into account preserved historical information as if it has memory. Among those approaches for enhancement of the long-term memory, Long Short-Term Memory (LSTM) [7] and its variants are\nof particular interest due to the great success in practice. Thus we use as the building block of our network a newly proposed variant, Gated Recurrent Unit (GRU) [4], for its simplicity and competitive performance. The expressions of GRU is given as follows:\nrt = \u03c3(Wrxt + Urht\u22121 + br)\nzt = \u03c3(Wzxt + Uzht\u22121 + bz)\nh\u0303t = tanh(Whxt + Uh(rt ht\u22121) + bh)\nht = zt ht\u22121 + (1\u2212 zt) h\u0303t\n(1)\nwhere represents element-wise multiplication and W,U, b are input weights, recurrent weights and biases, respectively.\nWe exploit the struture of bidirectional RNN in order to extract both forward-passing and backward-passing information: the context information from the left and right hand side of a particular word. Hence a new context-containing representation x\u0303t of a word is obtained by simply concatenating the output of the bidirectional RNN (h\u2190t ,h \u2192 t ) with the word embedding xt:\nx\u0303t = [h \u2190 t \u2016xt\u2016h\u2192t ] (2)"}, {"heading": "2.2 Highway Networks", "text": "A one-layer highway network is introduced as the intermediate layer between the bidirectional RNN and the CNN to select features individually for each word representation. The highway layer is expressed as follows:\nyt = \u03c4 g(WH x\u0303t + bH) + (1\u2212 \u03c4 ) x\u0303t (3)\nwhere g is nonlinear function and \u03c4 = \u03c3(W\u03c4xt + b\u03c4 ) represents \u201ctransform gate\u201d. Note that the design of highway connection is quite similar with that of GRU\u2019s \u201cupdate gate\u201d z, which is essentially a variant of \u201cleaky integration\u201d [1]. It allows part of the input information to be carried unchanged to the output while the rest to go through some (nonlinear) transformations. The experiments indicate that this kind of structure will help to extract substantial information."}, {"heading": "2.3 Convolutional Neural Networks", "text": "In recent years, CNN has achieved great success in CV and has been proved to be effective in various NLP and IR tasks. A CNN architecture is generally composed by a stack of distinct layers mapping input to output via some piecewise differentiable function. In order to extract new features from the input (phrases/sentences in our case), a convolutional layer essentially applies a set of learnable filters to the input, which have small receptive fields: they are in fact a set of masks with different window sizes h, i.e. they select h consecutive words for the neurons in the layer. For instance, feature ci is extracted from a window of word representations with size h: yi:i+h\u22121 according to the following expression:\nci = f(wconv \u00b7 yi:i+h\u22121 + bconv) (4)\nwhere f is a nonlinear function which in our case is the \u201cRectified Linear Unit\u201d (ReLU), wconv, bconv are the weight and bias.\nThe filter f will be applied to the word representations of the whole text with length n via a sliding window of size h to establish the feature map:\nc = [c1, c2, ..., cn\u2212h+1] (5)\nNext to the convolution layer, a max-pooling layer is applied to the feature map, which extracts the most significant feature c\u0302 = max{c}.\nThe proposed model utilises multiple filters with window size h = 1 to extract features for establishing the representation of the whole text. Note that although it is a common practice to exploit different window sizes, we fix the size to 1 since the bidirectional RNN and highway layer in the earlier layers have already obtained the context information around each word and thus we are free from applying filters in different window sizes."}, {"heading": "3. EXPERIMENT", "text": "To demonstrate the effectiveness of the proposed model, we perform the experiment on IMDB dataset [14] which contains 25,000 movie reviews for training. Each review is labelled by human with 1 represents positive sentiment and 0 represents negative sentiment. The review in the dataset has 267.9 words in average and standard deviation of review length is 198.8. To test the performance of learned text presenation of models, we run sentiment prediction on this dataset and report the accuracy evaluation metric. Besides, we are particularly interested in the relationship between input sequence length and performance of the model, and set up the experiment to test the performance of the model over different input sequence lengths.\nWe compare our model RCNN with highway layers (RCNNHW) with the following baseline neural network models for document level sentiment prediction:\n\u2022 Sum-of-Word-Vector (COW): simply sums up all the word embeddings as text representation.\n\u2022 LSTM: takes the average of the LSTM\u2019s hidden states of all words is used as text representation [7].\n\u2022 Bi-LSTM: similar to LSTM but exploits bidirectional LSTM [16] on the sequence to get text representation.\n\u2022 CNN: performs 1-demensional convolution followed by 1-demensional max-pooling with multiple filters on the sequence [9, 10].\n\u2022 CNN+LSTM: combines CNN and RNN (LSTM) by using CNN\u2019s output as LSTM\u2019s input.\n\u2022 RCNN: uses bidirectional RNN\u2019s output as CNN\u2019s input [12].\nWe use neural network models above to learn the text representation and adopt softmax output to make prediction. For the neural network models, hyperparameter tuning has great influence on its performance. For all the models we have done grid search with reasonable hyperparameters and tried different stochastic gradient descent training methods (e.g. adam, RMSprop, adadelta) [11]. For our model, RCNN with highway layers, we choose \u2018RMSprop\u2019 as training method with batch size of 32, the hidden layer dimension of 32, and the filter number of 256. Besides, we also tested the the performance of different neural network models with multiple input sequence lengthes, the RCNN based models get better performance with longer sequence length, in our experiment, we choose the best performance of the model over different sequence lengthes. Note, we did not apply pre-training methods like using pre-trained word2vec [15],\nand regularizer such as Dropout [17] which may bring further improvement [10]."}, {"heading": "4. RESULTS AND DISCUSSION", "text": "The experiment results can be found in Table 1. We compare our RCNN with highway layers with orginal RCNN model and find that the performance with highway layers are always better than those without. To quantitatively investigate the effect of highway network, we set up an experiment with RCNN based models. We train the RCNN model without highway layers, with one layer highway, two layers highway and one layer MLP respectively with the same parameters, then compare their accuracy. The results are shown in Table 2. We can find that having one to two highway layers is important, but more highway layers do not improve the performance. Besides, one MLP layer does not gain more improvement than higway layers. We think the reason why the highway layer works is because the highway layer helps select the features of word representation.\nBesides, we also compare with CNN, RNN and COW models. Empirical experiments show that CNN models usually yield better performance than RNNs which include LSTM and Bi-LSTM. It may be caused by RNN based models, especially the LSTMs, are hard to be trained, these models are really sensitive to the hyperparameters and latter words make more influence on the final text representation than former words in RNN models. CNN models usually perform remarkably well on many NLP and IR tasks [8, 9, 10, 18]. CNN based models use a fixed window of words as contextual information and the performance of a CNN is influenced by the window size. A small window may result in a loss of some long-distance patterns, whereas large windows will lead to data sparsity [5, 12]. However, RCNNs outperform the CNN and the CNN-LSTM model, we know that convolution structure can capture local context information, and recurrent can capture global information. We consider that in CNN-LSTM model, it does not make sense to use convolution layer and max-pooling layer before recurrent layer. Such architecture means using local information features extracted from CNN as RNN\u2019s input, but local information features do not have sequence relationship. As\nfor why RCNN based models outperform CNN, we think our context-containing representation of a word obtained by simply concatenating the output of the bidirectional RNN with the word embedding which has encoded context information would be a better input to the CNN comparing with original word embedding.\nWe further investigate how the input sequence length influences the performance of different neural network models. The average text length in our experiment distaste is 267 and the standard deviation of the text length is around 200. Therefore, we consider the input sequence length from 100 to 500 with step 100 to train the neural network models. The prediction accuracy for different neural network models with various input sequence length are shown in Figure 2. In the figure, we can observe that when the sequence length is less than 200, all models get bad performance since the short input length results in information loss. As the input sequence length increases, models receive longer input sequence which may bring more useful information as well as more noise information. From the figure, we can find that RCNN based models, especially our RCNN-HW model, obtain much higher improvement from longer input sequences. Meanwhile, the other models get lower improvement and even get worse performance than shorter input sequence length. We think this is because RCNN based models can preserve longer contextual information thanks to their recurrent structure and introduce less noise for their convolution structure with max-pooling. And our RCNN-HW can further reduce the noise with a highway layer to perform feature selection, which brings the best performance in longer input sequence lengthes."}, {"heading": "5. CONCLUSION", "text": "We have introduced a recurrent convolutional neural network with highway layers to learn text representation for sentiment analysis. The experiment demonstrates that our model RCNN-HW not only outperforms CNN and RNN models, but also works better than the RCNN without highway layer. Besides, since our model yield better performance\non longer text, it would be interesting to see if the model introduced in this paper works well in learning the text representation for the other NLP and IR tasks with relatively long documents."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "The authors would like to thank the anonymous reviewers for their helpful feedback and suggestions."}, {"heading": "7. REFERENCES", "text": "[1] Y. Bengio, N. Boulanger-Lewandowski, and\nR. Pascanu. Advances in optimizing recurrent networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8624\u20138628. IEEE, 2013.\n[2] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798\u20131828, 2013.\n[3] Y. Bengio, H. Schwenk, J.-S. Sene\u0301cal, F. Morin, and J.-L. Gauvain. Neural probabilistic language models. In Innovations in Machine Learning, pages 137\u2013186. Springer, 2006.\n[4] K. Cho, B. Van Merrie\u0308nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\n[5] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n[6] W. B. Croft, D. Metzler, and T. Strohman. Search engines: Information retrieval in practice, volume 283. Addison-Wesley Reading, 2010.\n[7] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.\n[8] B. Hu, Z. Lu, H. Li, and Q. Chen. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems, pages 2042\u20132050, 2014.\n[9] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A convolutional neural network for modelling sentences. In ACL, 2014.\n[10] Y. Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.\n[11] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[12] S. Lai, L. Xu, K. Liu, and J. Zhao. Recurrent convolutional neural networks for text classification. In AAAI, pages 2267\u20132273, 2015.\n[13] T. K. Landauer. On the computational basis of learning and cognition: Arguments from lsa. Psychology of learning and motivation, 41:43\u201384, 2002.\n[14] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 142\u2013150. Association for Computational Linguistics, 2011.\n[15] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111\u20133119. Curran Associates, Inc., 2013.\n[16] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. Signal Processing, IEEE Transactions on, 45(11):2673\u20132681, 1997.\n[17] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.\n[18] Y. Zhang and B. Wallace. A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification. arXiv preprint arXiv:1510.03820, 2015."}], "references": [{"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Search engines: Information retrieval in practice, volume 283", "author": ["W.B. Croft", "D. Metzler", "T. Strohman"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In ACL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["S. Lai", "L. Xu", "K. Liu", "J. Zhao"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "On the computational basis of learning and cognition: Arguments from lsa", "author": ["T.K. Landauer"], "venue": "Psychology of learning and motivation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1929}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Y. Zhang", "B. Wallace"], "venue": "arXiv preprint arXiv:1510.03820,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Learning good representations for text, such as words, sentences and documents, is essential for information retrieval (IR) and natural language processing (NLP) tasks like text classification and sentiment analysis, which has attracted considerable attention from both academic and industrial communities [2].", "startOffset": 306, "endOffset": 309}, {"referenceID": 5, "context": "It is common to use bag-of-words or bag-of-ngram to represent text [6] and train the models based on such representation.", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "[13] estimates that 20% of the meaning of a text comes", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Neural language model [3] was then proposed to leverage word embedding representation to infer the next-word distribution, but it still fails to fully utilize the sequence of the context words.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "CNN treats each words fairly by using the max-pooling layer [9].", "startOffset": 60, "endOffset": 63}, {"referenceID": 9, "context": "In addition, CNN is comparatively simple, efficient and has achieved strong empirical performance on the text classification jobs [10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "2015 proposed Recurrent Convolutional Neural Network (RCNN) [12].", "startOffset": 60, "endOffset": 64}, {"referenceID": 6, "context": "Among those approaches for enhancement of the long-term memory, Long Short-Term Memory (LSTM) [7] and its variants are of particular interest due to the great success in practice.", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Thus we use as the building block of our network a newly proposed variant, Gated Recurrent Unit (GRU) [4], for its simplicity and competitive performance.", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "Note that the design of highway connection is quite similar with that of GRU\u2019s \u201cupdate gate\u201d z, which is essentially a variant of \u201cleaky integration\u201d [1].", "startOffset": 150, "endOffset": 153}, {"referenceID": 13, "context": "To demonstrate the effectiveness of the proposed model, we perform the experiment on IMDB dataset [14] which contains 25,000 movie reviews for training.", "startOffset": 98, "endOffset": 102}, {"referenceID": 6, "context": "\u2022 LSTM: takes the average of the LSTM\u2019s hidden states of all words is used as text representation [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 15, "context": "\u2022 Bi-LSTM: similar to LSTM but exploits bidirectional LSTM [16] on the sequence to get text representation.", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "\u2022 CNN: performs 1-demensional convolution followed by 1-demensional max-pooling with multiple filters on the sequence [9, 10].", "startOffset": 118, "endOffset": 125}, {"referenceID": 9, "context": "\u2022 CNN: performs 1-demensional convolution followed by 1-demensional max-pooling with multiple filters on the sequence [9, 10].", "startOffset": 118, "endOffset": 125}, {"referenceID": 11, "context": "\u2022 RCNN: uses bidirectional RNN\u2019s output as CNN\u2019s input [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "adam, RMSprop, adadelta) [11].", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Note, we did not apply pre-training methods like using pre-trained word2vec [15], Table 1: Accuracy for different neural models on IMDB dataset Model Accuracy", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "and regularizer such as Dropout [17] which may bring further improvement [10].", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "and regularizer such as Dropout [17] which may bring further improvement [10].", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "CNN models usually perform remarkably well on many NLP and IR tasks [8, 9, 10, 18].", "startOffset": 68, "endOffset": 82}, {"referenceID": 8, "context": "CNN models usually perform remarkably well on many NLP and IR tasks [8, 9, 10, 18].", "startOffset": 68, "endOffset": 82}, {"referenceID": 9, "context": "CNN models usually perform remarkably well on many NLP and IR tasks [8, 9, 10, 18].", "startOffset": 68, "endOffset": 82}, {"referenceID": 17, "context": "CNN models usually perform remarkably well on many NLP and IR tasks [8, 9, 10, 18].", "startOffset": 68, "endOffset": 82}, {"referenceID": 4, "context": "A small window may result in a loss of some long-distance patterns, whereas large windows will lead to data sparsity [5, 12].", "startOffset": 117, "endOffset": 124}, {"referenceID": 11, "context": "A small window may result in a loss of some long-distance patterns, whereas large windows will lead to data sparsity [5, 12].", "startOffset": 117, "endOffset": 124}], "year": 2017, "abstractText": "Recently, the rapid development of word embedding and neural networks has brought new inspiration to various NLP and IR tasks. In this paper, we describe a staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN) with highway layers. The highway network module is incorporated in the middle takes the output of the bidirectional Recurrent Neural Network (Bi-RNN) module in the first stage and provides the Convolutional Neural Network (CNN) module in the last stage with the input. The experiment shows that our model outperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment analysis task. Besides, the analysis of how sequence length influences the RCNN with highway layers shows that our model could learn good representation for the long text.", "creator": "LaTeX with hyperref package"}}}