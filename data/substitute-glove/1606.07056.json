{"id": "1606.07056", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Emulating Human Conversations using Convolutional Neural Network-based IR", "abstract": "Conversational agents (\" crosswords \") they next also be that used in banter user-friendly. To collection a allowing being considered making one emulating protect - are interactions, with conversational solids making whenever keep as means leather a chat - like continuous with a friend exists making. In this packaging, we calls a prototype clear 1,600 Information Retrieval took processing co-axial putting appropriate complexity neural network - based features in came ranker be because important - sort responses by mounting calls their takes user. In conversations, practices for context most work for when retrieval modern; even show still take context - sensitive shift unlike little Convolutional Deep Structured Semantic Model (cDSSM) although character 5,186 significantly outperforms several expensive heddles from terms of is relevance present cues retrieved.", "histories": [["v1", "Wed, 22 Jun 2016 19:55:24 GMT  (574kb,D)", "http://arxiv.org/abs/1606.07056v1", "5 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy"]], "COMMENTS": "5 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR", "authors": ["abhay prakash", "chris brockett", "puneet agrawal"], "accepted": false, "id": "1606.07056"}, "pdf": {"name": "1606.07056.pdf", "metadata": {"source": "CRF", "title": "Emulating Human Conversations using Convolutional Neural Network-based IR", "authors": ["Abhay Prakash", "Chris Brockett", "Puneet Agrawal"], "emails": ["abprak@microsoft.com", "chrisbkt@microsoft.com", "punagr@microsoft.com"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors H.3.3 [Information Storage And Retrieval]: Information Search and Retrieval; I.2.7 [Artificial Intelligence]: Natural Language Processing\nKeywords Chat bot; Deep learning; Structured Semantics; Conversational agent; Convolutional Networks; Twitter data"}, {"heading": "1. INTRODUCTION", "text": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19]. However, as businesses move towards building conversational interfaces (for example, Facebook M, Cortana, Siri, and Orat.ai) where humans and systems work collaboratively to achieve their goals, it becomes important to model the context surrounding the conversation, since the system is not only responding to a question, but needs to answer in the context of the history of the exchange.\nContext is crucial, because conversational language does not always make for good query terms. Figure 1 presents an\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Neu-IR\u201916 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy\nc\u00a9 2016 Copyright is held by the owner/author(s).\nexample of an interaction between a conversational agent (\u201cbot\u201d) and user that resembles a dialog between friends. The bot responds, on the basis of the previous history of the conversation, mine is 25 June. It will be noted, however, that the user\u2019s messages in this conversation are casually colloquial and, taken in isolation, vaguely worded (cool. yours on? ). Such queries are not easily addressable in traditional QA systems, which might need to perform complex anaphora resolution to determine that the query yours on? relates to the phrase My bday month.\nIn this paper, we present a deep neural network-based approach to implementing a conversational agent that will engage with users in a friendly, engaging, conversational fashion, drawing on a database of Twitter conversations. We model the task of providing a response as an Information Retrieval problem, i.e., for a given query issued by the user, the bot must retrieve the best candidate from a conversational corpus to output as response. Below, we use the following notation: we will refer to user message as M, agent response as R, and the previous history of conversation as C (context). We use features derived from a Convolutional Deep Structured Semantic Model (cDSSM) [12] to predict the output R given (M, C) on a corpus of Twitter conversations and demonstrate that use of cDSSM-based contextual features can boost the 1-best precision of retrieved responses over several alternatives."}, {"heading": "2. RELATED WORK", "text": "A growing body of research has emerged in chat response generation, either using machine translation techniques [8] or deep neural networks, including sequence-to-sequence modeling [10][13][16]. These models, however, can be cumbersome to train, and may suffer a tendency towards blandness of output [5].\nThe system we explore here, by contrast, uses deep learning methods to retrieve the best response from a database of candidates. These candidates come from real human conversations and hence may be more vivid and less consensusdriven. In this respect, the system is close in spirit to the work of [15] on interactive storytelling where for a given mes-\nar X\niv :1\n60 6.\n07 05\n6v 1\n[ cs\n.A I]\n2 2\nJu n\n20 16\nsage (user\u2019s sentence) the system retrieves the best matching message (a sentence in story corpus) and returns with the corresponding response (following sentence in story corpus), using TF-IDF-based scoring.\nThe present work also bears a familial resemblance to [4] and the IR baseline in [13], where Twitter conversation pairs (each consisting of a tweet and its response) are used as data. Like our system, for a given query, those authors retrieve the best matching tweets to form a candidate set by taking their corresponding replies and rank them to display an appropriate response. [4] does not take into account context and does not deploy deep learning. [13] describe a neural language generation model (DCGM-II + CMM) that incorporates context to rank retrieval candidates. We use cDSSM, which is computationally efficient and relies on implicit semantic structure to derive features used in the ranker (described in Section 3.4).\nDeep learning techniques have recently been applied in direct community question-answering in [7][9][19], and also in [17] who applied cDSSM. These approaches seek to return the answer from a knowledge base in triplet format. Similarly, [9] used a convolutional neural network approach to rank short text pairs when answering questions. Other recent neural retrieval models include a multilayer perceptron classifier [14] and three-layered neural networks [2][6] in the NTCIR-12 short text conversation tasks. In these models, however, there is no dependency on the history of conversation to determine the user\u2019s intent."}, {"heading": "3. OUR APPROACH", "text": "We model the task of providing appropriate chat responses as an Information Retrieval problem, where for a given user message M and context C, the system retrieves and ranks the candidates by relevance and outputs one of the highest scoring responses. Offline, we create an index of paired tweets and their responses and index these using Lucene1. At runtime, the best response is chosen in a three step process. First, we use TF-IDF-based fetch to generate a candidate set appropriate to M and C. Then we extract features using a convolutional deep structured semantic network. Finally, a ranker is trained on 3-turn twitter conversations using these features to select response R from the candidate set (described in sections 3.4 below). Our setting is similar to any traditional IR system, and our main focus is on extracting relevant features and improving the candidate set selection. The runtime process is depicted in Figure 2.\n1http://lucene.apache.org/"}, {"heading": "3.1 Data for Index and Ranker", "text": "3.1.1 Data preparation for M-R pair Index We constructed a dataset of 17.62 million tweet conversa-\ntional pairs (tweets and their responses), extracted from the Twitter Firehose, covering the four year period from 2012 through 2015. To favor responses reflecting a culturally local persona, we limited the geographical region to a specific time zone. This permitted us to expose more culturallyappropriate responses, for example, the query what do you like for dinner triggers the response bhindi masala (an Indian curry made with okra) for Indian users and kaeng lueang (\u201cyellow curry\u201d) for users in Thailand.\nIn order to protect privacy and prevent personal information from surfacing in our chat agent\u2019s responses, we removed from this dataset, any conversational pairs where the response contained any individual\u2019s name, URL or hashtag. Further, we sought to minimize the risk of offending users by removing any pairs in which either M or R contained adult, politically sensitive, or ethnic-religious content, or other potentially offensive or contentious material, such as inappropriate references to violence, crime and illegal substances. We also filtered data to avoid scenarios where an adversarial query would result in retrieval of a candidate with high confidence from index like Does <celebrity> hate <religion>? \u2013> Yes, he does, where <celebrity> and <religion> stand in for specific entities2. After filtering, our corpus comprises 9.56 million usable M-R pairs.\n3.1.2 Training Data for the Ranker To train the ranker, we obtained a 2.58 million sample\nof 3-turn tweet conversations from the Twitter Firehose, in the same manner as described in Section 3.1.1 above. Each conversation comprises an exchange between two Twitter users, alternating over 3 turns."}, {"heading": "3.2 Retrieving Candidates", "text": "The goal of the retrieval step is to fetch, with high recall, relevant documents present in the index for the given M and C. As noted above, we have built an index of messages and responses. Given M, this index is used to retrieve best matching messages and their corresponding responses that constitute the candidate set. From a retrieval perspective, however, many messages in conversational scenarios are not self-sufficient as queries to return a good candidate set, but are dependent on expressions in the previous context e.g. why?, go ahead, and please. For shorter queries such as these, we fetch documents after appending context to query. This solution, though simple, works pragmatically and does not form the main focus of this paper."}, {"heading": "3.3 Convolutional DSSM", "text": "To extract features for our ranker, we trained a Convolutional Deep Structured Semantic Model (cDSSM) [12]. cDSSM finds semantic relevance between text query and document, after being trained on click-through data to maximize conditional likelihood of the clicked documents for the given query. We adapted this formulation to our purpose by utilizing M-R pairs from Twitter (described in 3.1.1 above) as training data, where the user message is treated as query\n2Queries involving such potentially offensive material must be detected and handled in a separate component that is beyond the scope of this paper.\nand response as clicked document. The cDSSM model thus learns the semantic relevance between a message and its response. Note that semantic relevance indicates, for a given M-R pair, how relevant R is for M, and is distinct from semantic similarity. We chose cDSSM over DSSM3 [3] in order to incorporate structural (word position) features in both message and response. To prepare the character trigram set [3], we took the most frequent 5k character trigrams found in the Twitter dataset.\nFigure 3 shows two models, a Message model (M-Model) and a Response model (R-Model), that have been obtained after training on M-R pairs. When a text X is forward propagated through either of the models, it is vectorized in the space defined by that model. We use the notation MX for the vector when X is vectorized using the M-model and RX when it is vectorized using the R-Model. We define two scoring functions using the two models as follows:\nSemantic Relevance Score(X,Y): We denote this score as SemRel(X, Y). This is the confidence of semantic relevance of Y as a response to X, calculated as follows:\nSemRel(X,Y ) = cosine(MX,RY) = MX TRY \u2016MX\u2016\u2016RY\u2016\nSemantic Similarity Score(X,Y): This score is denoted as SemSim(X, Y). It indicates semantic similarity of two texts X and Y in the space defined by the model used. To compute semantic similarity of two responses X and Y, both X and Y are vectorized using the R-Model, using the following formula:\nSemSim(X,Y ) = cosine(RX,RY) = RX TRY \u2016RX\u2016\u2016RY\u2016\nThe semantic similarity of two messages could also be computed using the M-model in an analogous manner, though we did not use this approach here. We use the above two scoring functions to generate features as described in Section 3.4 below.\n3Initial experimentation with DSSM failed to yield useful results."}, {"heading": "3.4 Learning to Rank in Multi-Turn Chat", "text": "Since we have modeled the problem as ranking task, we trained an implementation of the MART gradient boosting algorithm [1] to order the responses in candidate set. Note that as cDSSM is trained on only M-R pairs and thus learns to rank candidates only for a given message, we use it to derive features for the ranker to pick the best contextual response in the ongoing conversation.\nData format for ranker training: We used 3-turn Twitter conversations as described in Section 3.1.2. From each conversation, we prepared three training samples (1 positive and 2 negative), where Turn 1 and 2 were taken as C and M respectively. For positive samples, the original actual response in Turn 3 was used. For the two negative samples, random responses were selected (without replacement) from entire set of Tweets.\nFeatures for ranker training: We used the following features to train the ranker, which incorporated context while ranking the candidates: i) SemRel(M,R) \u2013 this represents the relevance of candidate response R for a given M without considering context. This is calculated from the cDSSM model as described in Section 3.3. ii) Context Message Match (CMM) \u2013 These are the exact matches between C, M and R (borrowed from [13]). These features incorporate lexical similarity of R with M and C. As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R. These matches are helpful for examples where for message i love you, i love you too! is a relevant response. iii) SemSim(C,R) \u2013 This feature captures semantic similarity between C and R, calculated using cDSSM. It helps pick a response that is semantically closer to the context and hence capture the mood or intent of conversation. We discuss it further in Section 4.3 with an example. iv) SemRel(C,M) \u2013 This feature captures change of context in the ongoing discussion. The score shows the relevance of M being the response for the C, where C was the previous message. The lower the value of this feature, the more likely it is that the user has changed topic and contextual features need not to be assigned higher importance.\nWe refer to the last two features (iii) and (iv) jointly as Context Capturing Features (CCF). At training time, our best system with all defined features scored NDCG@1 = 84.83, NDCG@2 = 92.56 and NDCG@3 = 94.02 on our development evaluation set."}, {"heading": "4. EVALUATION", "text": ""}, {"heading": "4.1 Setup", "text": "Since we are primarily interested in the contextual appropriateness of interactions from a human viewpoint, we evaluated two versions of our system, SemRel(M,R) +CMM and SemRel(M,R)+CMM+CCF, using human relevance judgments. To this end, we held out 1000 3-turn conversations from our Twitter dataset. We took first 2 turns of these 1000 conversations and retrieved the third turn using our system. The retrieved response was shown to human judges and labelled for relevance, using a crowdsourcing platform. In each case, we employed 5 human judges, who were shown"}, {"heading": "95% confidence intervals. The best-performing baseline and experimental systems are shown in bold.", "text": "the previous 2 turns by way of context along with the extracted response, and then asked to make a binary decision [0,1] whether or not the response was relevant to the conversational context. Other characteristics that might be appropriate in a human-agent interaction, such as interestingness and friendliness were not evaluated at this time. Also, since this is a retrieval-based system, we did not evaluate for grammatical correctness.\nWe compared two models against 3 baselines: IRstatus(as in [8]), IRstatus+CMM10feat., and DCGM-II+CMM10feat. (as proposed in [13]). For the purpose of our task, we believe the last is a strong baseline for a retrieval based system, since [13] also incorporated context when ranking candidates. When presenting the outputs to the judges, we randomly interpolated the system outputs to prevent the introduction of bias into the judgments."}, {"heading": "4.2 Results", "text": "We report the results of human judgments in two ways. First, we compute scores for each of the 1000 responses on a 5 point scale: we convert the binary decisions into a score between 0 and 5 by summing the judgments, then average over those scores. The results are shown in Table 2. Two-tailed t-tests indicate that the difference in means between the top-scoring baseline (DCGM-II+CMM) and the two SemRel(M,R) systems is statistically significant (p < 0.0001). The SemRel(M,R)+CMM+CCF appears to be slightly better overall; CCF provides a small but useful boost.\nTo assess the precision of retrieval, we also used a more conservative measure. Responses that achieved a supermajority of votes (\u2265 4) were assigned a score of 1, otherwise they were assigned 0. This we will term Precision@1, in other words, precision at rank 1. The Precision@1 results are presented in Table 3. It is evident from the confidence intervals that using features from deep learning models (i.e., cDSSM) to capture deep structured similarity yields significant improvement over other methods.\nThe gain in precision between SemRel(M,R)+CMM and the DCGM-II+CMM baseline is particularly significant (p < 0.0001) as measured by the Wilcoxon signed-rank test and McNemar\u2019s test for dichotomous variables. The addition of CCF features (SemRel(M,R)+CMM+CCF) appears to play a useful role in capturing context and in fine tuning ranking according to context."}, {"heading": "4.3 Qualitative Analysis", "text": "Table 1 shows examples of relevant and irrelevant results returned by the SemRel(M,R)+CMM+CCF model. The examples are taken from our 1000 conversation evaluation set as described in Section 4. The top example in the relevant results section is an instance where we observe same response with or without CCF features. We also note that there are no n-gram matches in C and R. This implies that SemRel(M,R) was alone sufficient to retrieve a relevant response, because M itself is descriptive and not heavily dependent on C. The example also suggests that our model does not overweight contextual features.\nThe second example in the relevant results, shows that the SemSim(C,R) feature helps retrieve a relevant response that is consistent with C. Candidate responses for M (he is? how so?) included who he is ? and aw tell me about it!! he is so hot!, which are relevant to M but irrelevant when C is taken into account.\nThe irrelevant result shown in Table 1 illustrates a known limitation of our model. Here, an irrelevant response was retrieved because you? was the focus of the message: candidates should have been ranked taking into account you? as M and i am from indonesia. as C. It would appear that none of the candidates received a high score for SemRel(M,R), while the CMM feature for M-R lexical match was assigned high importance.\nFigure 4 provides a short sample of chat by a human using our system, illustrating the fluency and human-like nature of exchanges."}, {"heading": "5. CONCLUSIONS", "text": "We have presented initial results of experiments in the use of convolutional Deep Structured Semantic neural networks (cDSSM) to emulate human conversations using IR techniques. Our approach is not only computationally performant but produces results that are significantly more relevant than baseline IR techniques. In these experiments, we employed a simple feature set and focused primarily on learning from cDSSM to output the best responses in context. In the future, we hope to expand the candidate set by introducing better context understanding and by taking into account user sentiment in order to further improve the quality of the human-agent interaction. Our results suggest that cDSSM is a viable approach to emulate interesting conversational exchanges, especially in cases where conversational data may be relatively limited, for example, regional markets and \u201csmaller\u201d or minority languages."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "We thank Bill Dolan, Michel Galley, Jianfeng Gao, Manoj K Chinnkotla, Chakrapani Ravi Kiran, Puneet Garg and Radhakrishnan Srikanth for helpful discussions and comments. We would also like to thank the two anonymous reviewers for their comments."}, {"heading": "7. REFERENCES", "text": "[1] C. J. Burges. From ranknet to lambdarank to\nlambdamart: An overview. Learning, 11:23\u2013581, 2010.\n[2] H. Denawa, T. Sano, Y. Kadotami, S. Kato, and T. Sakai. SLSTC at the NTCIR-12 STC task. In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies, 2016.\n[3] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of CIKM, pages 2333\u20132338. ACM, 2013.\n[4] S. Jafarpour, C. J. Burges, and A. Ritter. Filter, rank, and transfer the knowledge: Learning to chat. Advances in Ranking, 10, 2010.\n[5] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. A diversity-promoting objective function for neural\nconversation models. In Proceedings of HLT-NAACL, forthcoming 2016.\n[6] Y. Liu, C. Sun, L. Lin, and X. Wang. ITNLP: Pattern-based short text conversation system at NTCIR-12. In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies, 2016. [7] X. Qiu and X. Huang. Convolutional neural tensor network architecture for community-based question answering. In Proceedings of IJCAI, pages 1305\u20131311, 2015.\n[8] A. Ritter, C. Cherry, and W. B. Dolan. Data-driven response generation in social media. In Proceedings of EMNLP, pages 583\u2013593. ACL, 2011.\n[9] A. Severyn and A. Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of SIGIR, pages 373\u2013382. ACM, 2015.\n[10] L. Shang, Z. Lu, and H. Li. Neural responding machine for short-text conversation. In Proceedings of ACL, pages 1577\u20131586, 2015.\n[11] L. Shang, T. Sakai, Z. Liu, H. Li, R. Higashinaka, and Y. Miyao. Overview of the NTCIR-12 short text conversation task. In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies, 2016.\n[12] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. Learning semantic representations using convolutional neural networks for web search. In Proceedings of WWW, pages 373\u2013374, 2014.\n[13] A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell, J.-Y. Nie, J. Gao, and B. Dolan. A neural network approach to context-sensitive generation of conversational responses. In NAACL-HLT, pages 196\u2013205, 2015.\n[14] H. Sugiyama. Utterance selection based on sentence similarities and dialogue breakdown detection on NTCIR-12 STCV task. In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies, 2016.\n[15] R. Swanson and A. S. Gordon. Say anything: A massively collaborative open domain story writing companion. In Interactive Storytelling, pages 32\u201340. Springer, 2008.\n[16] O. Vinyals and Q. Le. A neural conversational model. arXiv preprint arXiv:1506.05869, 2015.\n[17] W.-t. Yih, X. He, and C. Meek. Semantic parsing for single-relation question answering. In ACL, pages 643\u2013648, 2014.\n[18] P. Yin, Z. Lu, H. Li, and B. Kao. Neural enquirer: Learning to query tables with natural language, 2015.\n[19] L. Yu, K. M. Hermann, P. Blunsom, and S. Pulman. Deep learning for answer sentence selection. NIPS deep learning workshop, 2014."}], "references": [{"title": "From ranknet to lambdarank to lambdamart: An overview", "author": ["C.J. Burges"], "venue": "Learning, 11:23\u2013581,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "SLSTC at the NTCIR-12 STC task", "author": ["H. Denawa", "T. Sano", "Y. Kadotami", "S. Kato", "T. Sakai"], "venue": "In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "In Proceedings of CIKM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Filter, rank, and transfer the knowledge: Learning to chat", "author": ["S. Jafarpour", "C.J. Burges", "A. Ritter"], "venue": "Advances in Ranking,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "A diversity-promoting objective function for neural  conversation models", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "B. Dolan"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "ITNLP: Pattern-based short text conversation system at NTCIR-12", "author": ["Y. Liu", "C. Sun", "L. Lin", "X. Wang"], "venue": "In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Convolutional neural tensor network architecture for community-based question answering", "author": ["X. Qiu", "X. Huang"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Data-driven response generation in social media", "author": ["A. Ritter", "C. Cherry", "W.B. Dolan"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "In Proceedings of ACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Overview of the NTCIR-12 short text conversation task", "author": ["L. Shang", "T. Sakai", "Z. Liu", "H. Li", "R. Higashinaka", "Y. Miyao"], "venue": "In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": "In Proceedings of WWW,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan"], "venue": "In NAACL-HLT,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Utterance selection based on sentence similarities and dialogue breakdown detection on NTCIR-12", "author": ["H. Sugiyama"], "venue": "STCV task. In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Say anything: A massively collaborative open domain story writing companion", "author": ["R. Swanson", "A.S. Gordon"], "venue": "In Interactive Storytelling,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Semantic parsing for single-relation question answering", "author": ["W.-t. Yih", "X. He", "C. Meek"], "venue": "In ACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Neural enquirer: Learning to query tables with natural language", "author": ["P. Yin", "Z. Lu", "H. Li", "B. Kao"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["L. Yu", "K.M. Hermann", "P. Blunsom", "S. Pulman"], "venue": "NIPS deep learning workshop,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 10, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 156, "endOffset": 160}, {"referenceID": 18, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "We use features derived from a Convolutional Deep Structured Semantic Model (cDSSM) [12] to predict the output R given (M, C) on a corpus of Twitter conversations and demonstrate that use of cDSSM-based contextual features can boost the 1-best precision of retrieved responses over several alternatives.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "A growing body of research has emerged in chat response generation, either using machine translation techniques [8] or deep neural networks, including sequence-to-sequence modeling [10][13][16].", "startOffset": 112, "endOffset": 115}, {"referenceID": 9, "context": "A growing body of research has emerged in chat response generation, either using machine translation techniques [8] or deep neural networks, including sequence-to-sequence modeling [10][13][16].", "startOffset": 181, "endOffset": 185}, {"referenceID": 12, "context": "A growing body of research has emerged in chat response generation, either using machine translation techniques [8] or deep neural networks, including sequence-to-sequence modeling [10][13][16].", "startOffset": 185, "endOffset": 189}, {"referenceID": 15, "context": "A growing body of research has emerged in chat response generation, either using machine translation techniques [8] or deep neural networks, including sequence-to-sequence modeling [10][13][16].", "startOffset": 189, "endOffset": 193}, {"referenceID": 4, "context": "These models, however, can be cumbersome to train, and may suffer a tendency towards blandness of output [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 14, "context": "In this respect, the system is close in spirit to the work of [15] on interactive storytelling where for a given mesar X iv :1 60 6.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "The present work also bears a familial resemblance to [4] and the IR baseline in [13], where Twitter conversation pairs (each consisting of a tweet and its response) are used as data.", "startOffset": 54, "endOffset": 57}, {"referenceID": 12, "context": "The present work also bears a familial resemblance to [4] and the IR baseline in [13], where Twitter conversation pairs (each consisting of a tweet and its response) are used as data.", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "[4] does not take into account context and does not deploy deep learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] describe a neural language generation model (DCGM-II + CMM) that incorporates context to rank retrieval candidates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Deep learning techniques have recently been applied in direct community question-answering in [7][9][19], and also in [17] who applied cDSSM.", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "Deep learning techniques have recently been applied in direct community question-answering in [7][9][19], and also in [17] who applied cDSSM.", "startOffset": 97, "endOffset": 100}, {"referenceID": 18, "context": "Deep learning techniques have recently been applied in direct community question-answering in [7][9][19], and also in [17] who applied cDSSM.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "Deep learning techniques have recently been applied in direct community question-answering in [7][9][19], and also in [17] who applied cDSSM.", "startOffset": 118, "endOffset": 122}, {"referenceID": 8, "context": "Similarly, [9] used a convolutional neural network approach to rank short text pairs when answering questions.", "startOffset": 11, "endOffset": 14}, {"referenceID": 13, "context": "Other recent neural retrieval models include a multilayer perceptron classifier [14] and three-layered neural networks [2][6] in the NTCIR-12 short text conversation tasks.", "startOffset": 80, "endOffset": 84}, {"referenceID": 1, "context": "Other recent neural retrieval models include a multilayer perceptron classifier [14] and three-layered neural networks [2][6] in the NTCIR-12 short text conversation tasks.", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "Other recent neural retrieval models include a multilayer perceptron classifier [14] and three-layered neural networks [2][6] in the NTCIR-12 short text conversation tasks.", "startOffset": 122, "endOffset": 125}, {"referenceID": 11, "context": "To extract features for our ranker, we trained a Convolutional Deep Structured Semantic Model (cDSSM) [12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "We chose cDSSM over DSSM [3] in order to incorporate structural (word position) features in both message and response.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "To prepare the character trigram set [3], we took the most frequent 5k character trigrams found in the Twitter dataset.", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "Since we have modeled the problem as ranking task, we trained an implementation of the MART gradient boosting algorithm [1] to order the responses in candidate set.", "startOffset": 120, "endOffset": 123}, {"referenceID": 12, "context": "ii) Context Message Match (CMM) \u2013 These are the exact matches between C, M and R (borrowed from [13]).", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R.", "startOffset": 49, "endOffset": 54}, {"referenceID": 1, "context": "As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R.", "startOffset": 49, "endOffset": 54}, {"referenceID": 2, "context": "As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R.", "startOffset": 49, "endOffset": 54}, {"referenceID": 3, "context": "As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R.", "startOffset": 49, "endOffset": 54}, {"referenceID": 0, "context": "the previous 2 turns by way of context along with the extracted response, and then asked to make a binary decision [0,1] whether or not the response was relevant to the conversational context.", "startOffset": 115, "endOffset": 120}, {"referenceID": 7, "context": "We compared two models against 3 baselines: IRstatus(as in [8]), IRstatus+CMM10feat.", "startOffset": 59, "endOffset": 62}, {"referenceID": 12, "context": "(as proposed in [13]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "For the purpose of our task, we believe the last is a strong baseline for a retrieval based system, since [13] also incorporated context when ranking candidates.", "startOffset": 106, "endOffset": 110}], "year": 2016, "abstractText": "Conversational agents (\u201cbots\u201d) are beginning to be widely used in conversational interfaces. To design a system that is capable of emulating human-like interactions, a conversational layer that can serve as a fabric for chat-like interaction with the agent is needed. In this paper, we introduce a model that employs Information Retrieval by utilizing convolutional deep structured semantic neural network-based features in the ranker to present human-like responses in ongoing conversation with a user. In conversations, accounting for context is critical to the retrieval model; we show that our context-sensitive approach using a Convolutional Deep Structured Semantic Model (cDSSM) with character trigrams significantly outperforms several conventional baselines in terms of the relevance of responses retrieved.", "creator": "LaTeX with hyperref package"}}}