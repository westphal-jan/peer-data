{"id": "1606.07262", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "On the Theoretical Capacity of Evolution Strategies to Statistically Learn the Landscape Hessian", "abstract": "We researcher the theoretical producing move religiously learn local vegetation associated addition Evolution Strategies (ESs ). Specifically, we security on real-valued formula_12 when destroyed by ESs existing brought next upcoming operator alone. We model periodic generation both votes solutions gone quadratic basins for accessible, with aether selection of on decision vectors if catastrophic still aspects function desire. Our minute not continue rigorously scene that continuous latter player present distance was reason one reveal enough information more itself information expansive, call. 2. , where leaving practically utilising by derandomized ES hybrid. We first appeared why under statistically - houses covariance matrix over example went commission formula_19 gained the it eigenvectors with held Hessian gaussian about both optimum. We never services an symplectic approximation. still euler matrix addition to less - elitist multi - murder $ (11, \\ ulama) $ - tool, but hold four puts with indigenous size $ \\ lambda $. Finally, need today numerically exonerating continue results.", "histories": [["v1", "Thu, 23 Jun 2016 10:38:49 GMT  (447kb)", "http://arxiv.org/abs/1606.07262v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["ofer m shir", "jonathan roslund", "amir yehudayoff"], "accepted": false, "id": "1606.07262"}, "pdf": {"name": "1606.07262.pdf", "metadata": {"source": "CRF", "title": "On the Theoretical Capacity of Evolution Strategies to Statistically Learn the Landscape Hessian", "authors": ["Ofer M. Shir", "Jonathan Roslund", "Amir Yehudayoff"], "emails": ["ofersh@telhai.ac.il", "jroslund@lkb.upmc.fr", "amir.yehudayoff@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n07 26\n2v 1\n[ cs\n.N E\n] 2\n3 Ju\nKeywords: Theory of evolution strategies, statistical learning, covariance matrix adaptation, landscape Hessian, limit distributions of order statistics, extreme value distributions"}, {"heading": "1 Introduction", "text": "ESs [2], popular heuristics that excel in global optimization of continuous search landscapes, utilize a Gaussian-based update (variation) step with an evolving covariance matrix. Since the development of ESs, it has been assumed that this learned covariance matrix, which defines the variation operation, approximates the inverse Hessian of the search landscape. It was supported by the rationale that locating the global optimum by an ES can be accommodated using mutation steps that fit the actual landscape, or in other words, that the optimal covariance distribution can offer mutation steps whose equidensity probability contours match the level sets of the landscape (maximizing the progress rate at the same time) [9].\nIt has been additionally argued that reducing a general problem to an isotropic quadratic problem may be achieved by sampling search-points based upon a covariance matrix that is the inverse Hessian (in equivalence to replacing the Euclidean distance measure with the Mahalanobis metric). Since ESs operate well on such isotropically quadratic problems, a successful ES run suggests that learning the covariance was accomplished. Nevertheless, it has never been formally proven that ESs\u2019 machinery can indeed learn the inverse of the Hessian. Rudolph [9] showed that ESs are capable of facilitating such learning and derived practical bounds on the population size toward the end of a successful learning period. That study paved the way toward accumulation of past search information by ESs by means of covariance matrices or any other forms of statistically learned algebraic structures. Especially, accumulation of selected individuals is practically utilized by derandomized ES variants [8], and it has led to the formulation of a successful family of search heuristics [1].\nThe goal of the current study is to investigate the statistical learning potential of an accumulated set of selected individuals (the so-called winners of each generation) when the ES operates in the vicinity of a landscape optimum. We argue and prove that accumulation of such winning individuals carries the potential to reveal valuable search landscape information. In particular, we consider the statistically-constructed covariance matrix over winning decision vectors and prove that it commutes with the Hessian matrix about the optimum (i.e., the two matrices share the same eigenvectors, and therefore their level sets are positioned along the same axes). This result indicates that in learning a covariance, an ES deduces the sensitive directions for an effective optimization. This carries the potential of a great benefit to an ES that learns a covariance matrix and may deduce the sensitive directions for effective optimization. Furthermore, we provide an analytic approximation of the covariance matrix, which holds for a large population size and when the Hessian is well-behaved (formal details appear below).\nThe remainder of this paper is organized as follows. The problem is formally stated in Section 2, where the assumed model is described in detail. In Section 3 we formulate the covariance matrix, derive the necessary density function, and then prove that the covariance and the Hessian commute. Section 4 provides an analytical covariance approximation for the problem of a (1, \u03bb)-ES. A simulation study encompassing various landscape scenarios for (1, \u03bb) selection is presented in Section 5, constituting a numerical corroboration for the theoretical outcomes in Sections 3 and 4. Finally, the results are discussed in Section 6."}, {"heading": "2 Statistical Landscape Learning", "text": "We outline the research question that we target:\nWhat is the relation between the statistically-learned covariance matrix and the landscape Hessian if a single winner is selected in each iteration assuming generated samples that follow an isotropic Gaussian (no adaptation)?\nWe focus on the a posteriori statistical construction of the covariance matrix of the decision variables upon reaching the proximity of the optimum, when subject to ES operation.\nIn what follows, we formulate the problem, assume a model and present our notation."}, {"heading": "2.1 Problem Statement and Assumed Model", "text": "Let J : Rn \u2192 R denote the objective function subject to minimization. We assume that J is minimized at the location ~x\u2217, which is assumed for simplicity to be the origin. The objective function may be Taylor-expanded about the optimum. We model the n-dimensional basin of attraction about ~x\u2217 by means of a quadratic approximation. We assume that this expansion is precise\nJ (~x\u2212 ~x\u2217) = J (~x) = ~xT \u00b7 H \u00b7 ~x, (1)\nwith H being the landscape Hessian about the optimum. The canonical non-elitist single-parent ES search process operates in the following manner: The ES generates \u03bb search-points ~x1, . . . , ~x\u03bb in each iteration, based upon Gaussian sampling with respect to the given search-point. We are especially concerned with the canonical variation operator of ES, which adds a normally distributed mutation ~z \u223c N ( ~0, I )\n. That is, ~x1, . . . , ~x\u03bb are independent and\neach is N ( ~0, I ) . Following the evaluation of those \u03bb search points with respect to J (~x), the best\n(minimal) individual is selected and recorded as\n~y = argmin{J(~xi)}. (2)\nFinally, let \u03c9 denote the winning objective function value,\n\u03c9 = J(~y) = min {J1, J2, . . . , J\u03bb} , (3)\nwhere Ji = J(~xi). We mention the difference between the optimization phase, which aims to arrive at the optimum and is not discussed here, to the statistical learning of the basin \u2013 which lies in the focus of this study. The sampling procedure is summarized as Algorithm 1, wherein statCovariance refers to a routine for statistically constructing a covariance matrix from raw observations.\n1 t \u2190 0 2 S \u2190 \u2205 3 repeat 4 for k \u2190 1 to \u03bb do 5 ~x\n(t+1) k \u2190 ~x\u2217 + ~zk, ~zk \u223c N\n( ~0, I )\n6 J (t+1) k \u2190 evaluate\n(\n~x (t+1) k\n)\n7 end 8 mt+1 \u2190 argmin ( { J (t+1) \u0131 }\u03bb\n\u0131=1\n)\n9 S \u2190 S \u222a {\n~x (t+1) mt+1\n}\n10 t \u2190 t+ 1 11 until t \u2265 Niter\noutput: Cstat =statCovariance(S) Algorithm 1: Statistical sampling by (1, \u03bb)-selection"}, {"heading": "2.2 Probability Functions of the ES Step", "text": "The length of a mutation vector, \u221a ~zT~z, obeys the so-called \u03c7-distribution with n degrees of freedom. Upon assuming a quadratic basin of attraction, \u03c8 = J(~z) is a random variable which obeys a generalized \u03c72-distribution. We consider two cases:\n1. The basic, simplified case of an isotropic basin, that is, its Hessian matrix constitutes the identity: H = I. In this case, the distribution of \u03c8 is the standard \u03c72-distribution, possessing the following cumulative distribution function (CDF ) accounting for the search-space dimensionality n:\nF\u03c72 (\u03c8) = 1\n2n/2\u0393 (n/2)\n\u222b \u03c8\n0 t n 2 \u22121 exp\n(\n\u2212 t 2\n)\ndt (4)\nwith \u0393(t) being the Gamma function, defined by:\n\u0393(t) =\n\u222b \u221e\n0 xt\u22121 exp(\u2212x)dx.\nThe probability density function (PDF ) is given by:\nf\u03c72 (\u03c8) = 1\n2n/2\u0393 (n/2) \u03c8n/2\u22121 exp\n(\n\u2212\u03c8 2\n)\n. (5)\n2. The generalized case of a globally minimal quadratic basin, where the Hessian matrix is positive definite with the following eigendecomposition form,\nH = UDU\u22121 D = diag [\u22061, . . . ,\u2206n] , with {\u2206i}ni=1 being the eigenvalues. The random variable \u03c8 = J(~z) now obeys a generalized \u03c72-distribution, whose exact distribution function is described as follows [5]:\nFH\u03c72(\u03c8) =\n\u222b \u221e\n0\n2\n\u03c0 sin t\u03c82 t cos\n\n\u2212t\u03c8 + 1 2\nn \u2211\nj=1\ntan\u22121 2\u2206jt\n\n\n\u00d7 n \u220f\nj=1\n( 1 + \u22062j t 2 )\u2212 1 4 dt,\n(6)\nwith an unknown closed form. At the same time, this CDF is known to follow an approximation [5],\nFT \u03c72 (\u03c8) = \u03a5\u03b7\n\u0393 (\u03b7)\n\u222b \u03c8\n0 t\u03b7\u22121 exp (\u2212\u03a5t) dt, (7)\nwith \u03a5 and \u03b7 accounting for matching the first two moments of ~zTH~z (and the subscript T marks the transformed distribution):\n\u03a5 = 1\n2\n\u2211n i=1 \u2206i \u2211n i=1\u2206 2 i , \u03b7 = 1 2 ( \u2211n i=1\u2206i) 2 \u2211n i=1 \u2206 2 i . (8)\nThe density function of this approximation reads:\nfT \u03c72 (\u03c8) = \u03a5\u03b7\n\u0393 (\u03b7) \u03c8\u03b7\u22121 exp (\u2212\u03a5\u03c8) . (9)\nThe accuracy of this approximation depends upon the standard deviation of the eigenvalues [5], which is clearly related to the so-called condition number. We assume that standard deviation to be moderate, and we thus adopt this approximation herein. For the isotropic case, it can be easily verified that Eq. 9 reduces to Eq. 5.\nWe conclude this section, by summarizing the relevant notation: The random vector ~z is a normal Gaussian mutation and \u03c8 = J(~z). The random vectors ~x1, . . . , ~x\u03bb are \u03bb independent copies of ~z, and Ji = J(~xi). The winner is ~y, and \u03c9 = J(~y). The matrix H is the Hessian about the optimum ~x\u2217, and C is the covariance matrix of ~y."}, {"heading": "3 Covariance Matrix Formulation", "text": "In the current section we formulate the covariance matrix by means of its defining density functions and then prove it commutes with the landscape Hessian.\nBy construction, the origin is set at the parent search-point, which is located at the optimum. Analytically, the covariance elements are thus reduced to the following expectation values:\nCij = \u222b xixjPDF~y (~x) d~x , (10)\nwhere PDF~y (~x) is an n-dimensional density function characterizing the winning decision variables about the optimum. In essence, the current study aims at understanding this expression in Eq. 10. To this end, revealing the nature of PDF~y is necessary for the interpretation of the covariance matrix. Importantly, the selection mechanism of the heuristic is blind to the location of the candidate solutions in the search space, and its sole criterion is the ranked function values. Specifically, in the decision-space perspective, the density function of a winning vector of decision variables ~y is related to the density of the winning function value \u03c9 via the following relation:\nPDF~y (~x) = PDF\u03c9 (J (~x)) \u00b7 PDF~z (~x)\nPDF\u03c8 (J (~x)) (11)\nwith PDF~z denoting the density function for generating an individual, and PDF\u03c8 denoting the density function of the objective function values (Eqs. 5 and 9). A brief justification follows. The density functions satisfy the conditional probability relation:\nPDF~y (~x) = PDF\u03c9 (J (~x)) \u00b7 PDF~y|\u03c9 (~x | J (~x)) . (12)\nNow consider the distribution of [~y;\u03c9] on Rn+1. The density of ~y conditioned on the value of J (~y) is that of a normal Gaussian subject to this conditioning, since we may sample [~y;\u03c9] by the following construction: First sample {J1, . . . , J\u03bb} according to PDF\u03c8 independently. Then sample {~x1, . . . , ~x\u03bb} conditioned on the values of J1, . . . , J\u03bb independently. Finally, J may be set to J\u2113 = \u03c9 that is minimal\nand ~y set to the respective ~x\u2113. In other words, following selection, a winning value of J is chosen to be \u03c9, and the corresponding ~x becomes the winning vector ~y. Importantly, the winning vector ~y conditioned upon the winning value \u03c9 is generated in the same manner as a normally-distributed ~z conditioned upon \u03c8. As a result, the conditional probability for the generation of ~y conditioned upon \u03c9 is the same as that for the creation of ~z conditioned upon \u03c8, i.e., PDF~y|\u03c9 = PDF~z|\u03c8. This density therefore reads:\nPDF~y|\u03c9 (~x | J (~x)) = PDF~z|\u03c8 (~x | J (~x)) = PDF~z (~x)\nPDF\u03c8 (J (~x)) . (13)\nTheorem 1. The covariance matrix and the Hessian are commuting matrices when the objective function follows the quadratic approximation.\nProof. Given the density function in Eq. 11, the objective function is assumed to satisfy J (~x) = ~xT \u00b7 H \u00b7 ~x, and the covariance matrix reads:\nCij = \u222b xixjPDF\u03c9 ( ~xT \u00b7 H \u00b7 ~x ) \u00b7 PDF~z (~x) PDF\u03c8 (~xT \u00b7 H \u00b7 ~x) d~x. (14)\nConsider the orthogonal matrix U , which diagonalizes H into D and possesses a determinant of value 1:\nU\u22121HU = D \u2261 diag [\u22061,\u22062, . . . ,\u2206n] ~\u03d1 = U\u22121~x d~\u03d1 = d~x .\nWe target the integral Iij = ( U\u22121CU ) ij and apply a change of variables into ~\u03d1 (after changing order of summations):\nIij = 1 \u221a\n(2\u03c0)n\n\u222b +\u221e\n\u2212\u221e\n\u222b +\u221e\n\u2212\u221e \u00b7 \u00b7 \u00b7\n\u222b +\u221e\n\u2212\u221e \u03d1i\u03d1j exp\n(\n\u22121 2 ~\u03d1T ~\u03d1\n)\n\u00d7\n\u00d7 PDF\u03c9\n( ~\u03d1T \u00b7 D \u00b7 ~\u03d1 )\nPDF\u03c8\n( ~\u03d1T \u00b7 D \u00b7 ~\u03d1 )d\u03d11d\u03d12 \u00b7 \u00b7 \u00b7 d\u03d1n.\n(15)\nIij vanishes for any i 6= j due to symmetry considerations: the overall integrand is an odd function, because all the terms are even functions, except for \u03d1j, \u03d1i when they differ. Therefore, the integration over the entire domain yields zero. Hence, I is the diagonalized form of C, with U holding the eigenvectors. C is thus diagonalized by the same eigenvectors as H, and therefore, by definition, they are commuting matrices, as claimed."}, {"heading": "4 Analytic Approximation", "text": "In this section we provide an approximation for PDF\u03c9 (J (~x)) and consequently for PDF~y (~x) in order to explicitly calculate the covariance matrix using Eq. 10.\nA non-elitist multi-child selection is considered here, where in each iteration a single individual is deterministically selected out of \u03bb generated offspring. In particular, consider a random sample from an absolutely continuous population with density PDF\u03c8 (\u03c8) and distribution CDF\u03c8 (\u03c8). In order to formulate the density of those winners, it is convenient to first characterize the distribution function of the winning event amongst \u03bb candidates1:\nCDF\u03c9 (\u03c8) = Pr {\u03c9 \u2264 \u03c8} = 1\u2212 (1\u2212 CDF\u03c8 (\u03c8))\u03bb . (17) 1Gupta [7] showed that when the dimension n is even, the distribution of the winners for the \u03c72 distribution (isotropic\nbasin case) possesses a simple form:\nCDF (n=2m) \u03c9 (\u03c8) = 1\u2212 exp\n(\n\u2212\u03bb \u03c8\n2\n)\n\n\nn 2 \u22121 \u2211\n=0\n\u03c8 !\n\n\n\u03bb\n(16)\n.\nThe density function is obtained upon differentiating Eq. 17:\nPDF\u03c9 (\u03c8) = \u03bb \u00b7 (1\u2212 CDF\u03c8 (\u03c8))\u03bb\u22121 \u00b7 PDF\u03c8 (\u03c8) . (18)\nUpon substituting the explicit forms into CDF\u03c8 and PDF\u03c8 (using either Eqs. (4,5) for the standard \u03c7 2 or Eqs. (7,9) for the generalized \u03c72), the desired density function PDF\u03c9 (J (~x)) is obtained, however not in a closed form.\nGupta [7] derived explicit order statistic results from the Gamma distribution, to which the \u03c72 distribution belongs, including the distribution function as well as moments of the kth order statistic. Such results could reveal a closed form for PDF\u03c9 (J (~x)), which seems cumbersome and far too complex to address when targeting Eq. 10. Next, we will seek an approximation for PDF\u03c9 (J (~x)), which will enable us to realize the relation of Eq. 11 when large values of \u03bb are assumed."}, {"heading": "4.1 Limit Distributions of Order Statistics", "text": "We treat the derived winners\u2019 distribution for large sample sizes, i.e., when the population size \u03bb tends to infinity. We denote the CDF in Eq. 17 with a subscript \u03bb, L\u03bb (\u03c8) = 1\u2212 (1\u2212 CDF\u03c8 (\u03c8))\u03bb, and consider the limit when \u03bb tends to infinity:\nlim \u03bb\u2212\u2192\u221e\nL\u03bb (\u03c8) = {\n0 if CDF\u03c8 (\u03c8) = 0 1 if CDF\u03c8 (\u03c8) > 0\nAccording to the Fisher-Tippett theorem [6], also known as the extremal types theorem, the vonMises family of distributions for minima (or the minimal generalized extreme value distributions (GEVDmin)) are the only non-degenerate family of distributions satisfying this limit. They are characterized as a unified family of distributions by the following CDF:\nL\u03ba (\u03c8;\u03ba1, \u03ba2, \u03ba3) = 1\u2212 exp { \u2212 [ 1 + \u03ba3 ( \u03c8 \u2212 \u03ba1 \u03ba2 )]1/\u03ba3 } . (19)\nFurthermore, since the minimum distribution moves toward the origin as \u03bb increases, normalizing constants are needed to avoid degeneracy and to obtain:\nlim \u03bb\u2212\u2192\u221e L\u03bb (a\u2217\u03bb\u03c8 + b\u2217\u03bb) = lim\n\u03bb\u2212\u2192\u221e 1\u2212 (1\u2212 CDF\u03c8 (a\u2217\u03bb\u03c8 + b\u2217\u03bb))\u03bb = L (\u03c8) \u2200\u03c8\n(20)\nThe location parameter, \u03ba1, and the scale parameter, \u03ba2, are obviously interlinked to the aforementioned normalizing constants. The shape parameter, \u03ba3, determines the identity of the characteristic CDF, namely either Weibull, Gumbell, or Freche\u0301t. This parameter is evaluated by means of the following limit, whose existence is a necessary and sufficient condition for a continuous distribution function CDF\u03c8 (\u03c8) to belong to the domain of attraction for minima of L\u03ba (\u03c8) :\n\u03ba3 = lim \u03b5\u2212\u21920\n\u2212 log2 CDF\n\u22121 \u03c8 (\u03b5)\u2212 CDF\u22121\u03c8 (2\u03b5)\nCDF \u22121 \u03c8 (2\u03b5)\u2212 CDF\u22121\u03c8 (4\u03b5)\n, (21)\nwhere CDF\u22121\u03c8 refers to the inverse CDF (the quantile function of CDF\u03c8 (\u03c8); see Theorem 9.6 in [3] [pp. 204-205]):\n\u2022 If \u03ba3 > 0, CDF\u03c8 (\u03c8) belongs to the Weibull minimal domain of attraction,\n\u2022 if \u03ba3 = 0, CDF\u03c8 (\u03c8) belongs to the Gumbel minimal domain of attraction, and\n\u2022 if \u03ba3 < 0, CDF\u03c8 (\u03c8) belongs to the Freche\u0301t minimal domain of attraction.\nNote that Rudolph had already taken a related mathematical approach, which he termed asymptotic theory of extreme order statistics, to characterize convergence properties of ESs on a class of convex objective functions [10]. Also, GEVD is introduced to the broad perspective of Stochastic Global Optimization in [11], a book which also constitutes a proper mathematical reference for this topic, yet in a slightly different light.\nLemma 1. For the standard \u03c72 distribution (i.e., isotropic basin), the limit for Eq. 21 exists and reads \u03ba3 = 2/n.\nProof. The limit needs to be evaluated about \u03b5 \u2212\u2192 0. By inserting an asymptotic expansion of the Gamma function\u2019s integrand, the overall CDF F\u03c72 (Eq. 4) may be written and approximated using Stirling\u2019s formula as\nF\u03c72 (\u03b5) = 1\n2n/2\u0393 (n/2) \u03b5\nn 2\n\u221e \u2211\nk=0\n(\u22121)k ( \u03b5 2\n)k\n(\nn 2 + k\n)\nk!\n\u2248 2 n \u00b7 2n/2\u0393 (n/2)\u03b5 n 2 \u2248 (\u03b5 4 e n\n)n 2\n,\ntaking only the zeroth-order term in the sum into consideration. The quantile (inverse) function has the form:\nF\u22121 \u03c72 (\u03b5) \u2248 4n e \u00b7 \u03b5 2n . (22)\nTargeting the limit in Eq. 21 yields\nF\u22121 \u03c72 ((\u03b5) \u2212 F\u22121 \u03c72 (2\u03b5) F\u22121 \u03c72 (2\u03b5) \u2212 F\u22121 \u03c72 (4\u03b5) \u2248\n\u03b5 2 n\n(\n2 2 n \u2212 1\n)\n\u03b5 2 n\n(\n4 2 n \u2212 2 2n\n) = 1\n2 2 n\n, (23)\nwhich allows to conclude with:\n\u03ba3 = lim \u03b5\u2212\u21920\n\u2212 log2 F\u22121 \u03c72 (\u03b5)\u2212 F\u22121 \u03c72 (2\u03b5)\nF\u22121 \u03c72 (2\u03b5)\u2212 F\u22121 \u03c72\n(4\u03b5) =\n2 n . (24)\nLemma 2. For the generalized \u03c72 distribution (i.e., non-isotropic basin), the limit for Eq. 21 exists and also reads \u03ba3 = 2/n.\nProof. In the limit \u03b5 \u2212\u2192 0, the generalized distribution (Eq. 7) has a similar CDF, with an approximated quantile function\nF\u22121 T \u03c72 (\u03b5) \u2248 4n e \u00b7 \u03b5 2n , (25)\nand thus Eq. 24 holds as is.\nLemma 3. For the standard and generalized \u03c72 distributions, the normalizing constants\na\u2217\u03bb = F \u22121 \u03c72\n(\n1\n\u03bb\n)\n, b\u2217\u03bb = inf { \u03c8 \u2223 \u2223F\u03c72 (\u03c8) > 0 } = 0\nensure that the limit distribution of Eq. 20 is not degenerate.\nProof. Given the constants\na\u2217\u03bb = F \u22121 \u03c72\n(\n1\n\u03bb\n)\n\u2248 4n e\n(\n1\n\u03bb\n)2/n\n, b\u2217\u03bb = 0,\nthe limit becomes (using F\u03c72(\u03b5) = FT \u03c72(\u03b5) = ( \u03b5 4 e n\n)n 2 )\nlim \u03bb\u2212\u2192\u221e\n1\u2212 { 1\u2212 CDF\u03c8 [ \u03c8 4n\ne\n(\n1\n\u03bb\n)2/n ]}\u03bb\n=\nlim \u03bb\u2212\u2192\u221e\n1\u2212 [ 1\u2212 r (n) ( \u03c8n/2\n\u03bb\n)]\u03bb\n=\n1\u2212 exp [ \u2212r (n) (\u03c8)n/2 ]\n, (26)\nwith r (n) = ( e 4n )n/2 . Hence, the limit distribution exists and is not degenerate, as claimed.\nCorollary 1. Since the shape parameter \u03ba3 is always positive, the extreme minima of the \u03c7 2-distributions belong to the Weibull domain of attraction. The normalized extreme minima, (\u03c9 \u2212 b\u2217\u03bb)/a\u2217\u03bb, may be represented by a random variable \u03c8\u0303, which then reduces Eq. 19 to the following transformed CDF (importantly, the so-called tail index reads 1/\u03ba3 = n/2):\nCDF\u03c9 (\u03c8) \u03bb\u2192\u221e\u2212\u2212\u2212\u2192 W\n( \u03c8\u0303 ) = 1\u2212 exp ( \u2212\u03c8\u0303n/2 )\n(27)\nSee [3] and [4] for an overview on the family of generalized extreme value distributions and on the limit distributions of order statistics. In particular, see table 9.1 in [3][p. 200] for the relationship between the parameters of the GEVD and the Weibull distribution, which allows the reduction of Eq. 19 to Eq. 27. Also, for the exact determination of the tail index value, when assuming certain conditions on the sampling distribution, see Theorem 2.3 in [11].\nCorollary 2. Under the GEVD approximation for treating large populations, \u03bb \u2192 \u221e, upon normalizing the variable to \u03c8\u0303 = (\u03c9 \u2212 b\u2217\u03bb) /a\u2217\u03bb and using the tail index result 1/\u03ba3 = n2 , the CDF and PDF forms for the single winning event read:\nCDF GEVD \u03c9\n( \u03c8\u0303 ) = 1\u2212 exp ( \u2212\u03c8\u0303 n2 )\nPDF GEVD \u03c9\n( \u03c8\u0303 ) = n\n2 \u03c8\u0303\nn 2 \u22121 exp\n( \u2212\u03c8\u0303 n2 ) (28)"}, {"heading": "4.2 Covariance Derivation", "text": "By setting the Weibull form as the characteristic density PDF\u03c9, we may rewrite Eq. 10 by utilizing Eq. 11 as follows with the normalized J\u0303(~x) \u2261 (J(~x)\u2212 b\u2217\u03bb) /a\u2217\u03bb:\nCij = \u222b +\u221e\n\u2212\u221e \u00b7 \u00b7 \u00b7\n\u222b +\u221e\n\u2212\u221e xixj\nn 2 J\u0303(~x) n 2 \u22121 exp [ \u2212J\u0303(~x)n2 ] \u00d7\n\u00d7 1\u221a (2\u03c0)n exp ( \u221212~xT~x )\n\u03a5\u03b7 \u0393(\u03b7)J(~x) \u03b7\u22121 exp (\u2212\u03a5J(~x))\ndx1dx2 \u00b7 \u00b7 \u00b7 dxn. (29)\nJ is assumed here to satisfy J (~x) = ~xT \u00b7 H \u00b7 ~x, and must be normalized only for the PDF\u03c9 term by means of a\u2217\u03bb alone since b \u2217 \u03bb = 0:\nCij = \u222b +\u221e\n\u2212\u221e \u00b7 \u00b7 \u00b7\n\u222b +\u221e\n\u2212\u221e NCxixj\n( ~xTH~x ) n 2 \u2212\u03b7 \u00d7\n\u00d7 exp [ \u03a5~xTH~x\u2212 ( ~xTH~x a\u2217\u03bb ) n 2 \u2212 1 2 ~xT~x ] dx1dx2 \u00b7 \u00b7 \u00b7 dxn. (30)\nwith a normalizing constant NC = n\u0393(\u03b7)\n2\u03a5\u03b7(a\u2217\u03bb) n 2 \u22121\n\u221a (2\u03c0)n .\nFor the isotropic case, H = h0I, the integration is straightforward (\u03b7 = n2 , \u03a5 = 12h0 ) \u2013 the attained covariance is proportional to the inverse Hessian, multiplied by an explicit factor:\nC(H=h0I) = \u0393( n 2 ) \u00b7 \u0393\n( 1 + 2n ) \u00b7 c (n) \u00b7 a\u2217\u03bb 2\u03c0n/2 \u00b7 H\u22121 (31)\nwherein\nc (n) =\n{\n\u03c0m m! n = 2m 2m+1\u03c0m\n1\u00b73\u00b75\u00b7\u00b7\u00b7(2m+1) n = 2m+ 1 (32)\nFor the general case of any positive-definite Hessian H, the integral in Eq. 30 has an unknown closed form. We were able, nevertheless, to numerically corroborate it for n = 3. We note that this form of the covariance also commutes with the Hessian, in line with the theorem discussed above."}, {"heading": "5 Simulation Study", "text": "We implemented our model into a numerical procedure in order to compare the statistical measures to the analytical calculations in practice, adhering to Algorithm 1, Cstat, statistically sampled as described therein. Numerical validation is provided here for two aspects of our theoretical work: Theorem 1 and the analytic approximation for the covariance matrix.\n5.1 Cstat versus H We generated a large set of random positive-definite matrices at various dimensions {n} with a spectrum of condition numbers. For each trial , the numerical procedure generated a random symmetric matrix A, diagonalized it into a set of orthonormal eigenvectors U, drew n random positive numbers in a diagonal matrix D, and set H = UAU\u22121 . We then applied Algorithm 1 by considering {H} as landscape Hessians.\nThe resultant covariance matrices, { Cstat }\n, were diagonalized and compared to the Hessian matrices and their eigendecomposition \u2013 which always matched. In practice, it was evident that the two matrices always commute (applying the commutator operator yields a zero matrix to a practical precision considering the max norm),\n\u2200 \u2016HCstat \u2212 Cstat H\u2016max < 10\u22121, (33)\nas claimed. However, the covariance matrices were not the inverse forms of the Hessian matrices.\n5.2 Analytic Approximation for Cstat\nHere, we corroborated the analytic approximation for the covariance matrix. To this end, we considered four quadratic basins of attraction at various search-space dimensions: (H-1) n = 3, H1 = [\u221a 2/2 0.25 0.1; 0.25 1 0; 0.1 0 \u221a 2 ] (H-2) n = 10, H2 = diag [1.0, 1.5, . . . , 5.5] (H-3) n = 30, H3 = diag [ ~I10, 2 \u00b7 ~I10, 3 \u00b7 ~I10 ] (H-4) n = 100, H4 = 2.0 \u00b7 I100\u00d7100"}, {"heading": "5.2.1 Validating the Approximated \u03c72 Density fT \u03c72", "text": "Figure 1 depicts the approximated density functions of the generalized \u03c72 distribution, fT \u03c72 (Eq. 9) for the four Hessian forms (H-1)-(H-4), which evidently constitute sound approximations.\n5.2.2 Validating the Winners\u2019 Densities PDF\u03c9 and PDF GEVD \u03c9\nFigures 2 and 3 provide validation for the winners\u2019 density, which was exactly described by PDF\u03c9 in Eq. 18, and was later approximated by PDFGEVD\u03c9 in Eq. 28 for large \u03bb. Interestingly, PDF\u03c9, which is realized here by the approximated generalized \u03c72 distribution FT \u03c72 , exhibits decreased accuracy on\nH1, H2 and H3. Evidently, it is highly sensitive to the approximation error of FT \u03c72 , which is amplified by the exponent \u03bb. At the same time, PDFGEVD\u03c9 exhibits decreased accuracy on H2, H3 and H4, due to its sensitivity to the population size \u03bb. Indeed, improvements for this approximation were evident when \u03bb was increased (see additional settings on Figure 3)."}, {"heading": "5.2.3 Validating the Approximated Integral", "text": "Finally, we compared Cstat to the obtained analytical approximations. For the isotropic case, the result of Eq. 31 has been successfully corroborated for a range of search-space dimensions n. For instance, Cstat for the 100-dimensional case (H-4) was constructed with \u03bb = 5000 and over 500, 000 iterations to obtain a diagonal with an expected value 0.5617 \u00b1 0.0012. Eq. 31 obtained a value of 0.5680.\nFor the general case, we considered the 3-dimensional case (H-1). Cstat was constructed with \u03bb = 20 and over 10, 000 iterations, to be presented side-by-side with the numerical integration of Eq. 30 in Table 1. Additionally, their explicit eigenvectors are provided therein.\nH1 (n = 3) ( \u03bb = 20, Niter = 10 5 )\n0 0.5 1 1.5 2 2.5 0\n0.5\n1\n1.5\n2\n2.5\nRaw Sampling\n0 1 2 3 4 5 6 7 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nNormalized Sampling\nH3 (n = 30) ( \u03bb = 1000, Niter = 2 \u00b7 105 )\nH4 (n = 100) ( \u03bb = 5000, Niter = 5 \u00b7 105 )\nH2 (n = 10) ( \u03bb = 1000, Niter = 10 5 )\n0 2 4 6 8 10 12 14 16 0\n0.05\n0.1\n0.15\n0.2\n0.25\nRaw Sampling\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\nNormalized Sampling\nH2 (n = 10) [LEFT]: ( \u03bb = 10000, Niter = 10 6 ) [RIGHT]: ( \u03bb = 20000, Niter = 5 \u00b7 106 )"}, {"heading": "6 Conclusion", "text": "Our analytical work modeled passive ES learning, that is, no step-size adaptation nor covariance matrix adaptation were utilized when constructing a covariance matrix from winning decision vectors. We proved that the statistically-constructed covariance commutes with the landscape Hessian about the optimum when a quadratic basin is assumed. The implication of this result is the enhanced capacity of ESs to identify sensitive optimization directions by extracting this information from the learned covariance matrix. We then derived an analytical approximation for the covariance matrix, based on two presumptions \u2013 (i) the generalized \u03c72 density function was approximated, assuming moderate standard deviation of the eigenvalues, and (ii) the winners\u2019 distribution was shown to follow the Weibull distribution with a calculated tail index when the population size \u03bb is large, adhering to the limit distributions of order statistics. Our results were then numerically validated at multiple levels, where the accuracy of the approximations was discussed."}], "references": [{"title": "Contemporary Evolution Strategies", "author": ["T. B\u00e4ck", "C. Foussette", "P. Krause"], "venue": "Natural Computing Series. Springer-Verlag Berlin Heidelberg,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Evolution Strategies a Comprehensive Introduction", "author": ["H.-G. Beyer", "H.-P. Schwefel"], "venue": "Natural Computing: An International Journal, 1(1):3\u201352,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Extreme Value and Related Models with Applications in Engineering and Science", "author": ["E. Castillo", "A.S. Hadi", "N. Balakrishnan", "J.M. Sarabia"], "venue": "John Wiley and Sons,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Modelling Extremal Events for Insurance and Finance", "author": ["P. Embrechts", "C. Kl\u00fcppelberg", "T. Mikosch"], "venue": "Springer-Verlag,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "The distribution and properties of a weighted sum of chi squares", "author": ["A.H. Feiveson", "F.C. Delaney"], "venue": "Technical report, National Aeronautics and Space Administration,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1968}, {"title": "Limiting forms of the frequency distribution of the largest or smallest member of a sample", "author": ["R. Fisher", "L. Tippett"], "venue": "Proc. Cambridge Philos. Soc., 24:180\u2013190,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1928}, {"title": "Order Statistics from the Gamma Distribution", "author": ["S.S. Gupta"], "venue": "Technometrics, 2, May", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1960}, {"title": "Completely Derandomized Self-Adaptation in Evolution Strategies", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evolutionary Computation, 9(2):159\u2013195,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "On Correlated Mutations in Evolution Strategies", "author": ["G. Rudolph"], "venue": "Parallel Problem Solving from Nature - PPSN II, pages 105\u2013114, Amsterdam,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Convergence rates of evolutionary algorithms for a class of convex objective functions", "author": ["G. Rudolph"], "venue": "Control and Cybernetics, 26(3),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Stochastic Global Optimization", "author": ["A. Zhigljavsky", "A. \u017dilinskas"], "venue": "Springer Optimization and Its Applications. Springer US,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction ESs [2], popular heuristics that excel in global optimization of continuous search landscapes, utilize a Gaussian-based update (variation) step with an evolving covariance matrix.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "It was supported by the rationale that locating the global optimum by an ES can be accommodated using mutation steps that fit the actual landscape, or in other words, that the optimal covariance distribution can offer mutation steps whose equidensity probability contours match the level sets of the landscape (maximizing the progress rate at the same time) [9].", "startOffset": 358, "endOffset": 361}, {"referenceID": 8, "context": "Rudolph [9] showed that ESs are capable of facilitating such learning and derived practical bounds on the population size toward the end of a successful learning period.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "Especially, accumulation of selected individuals is practically utilized by derandomized ES variants [8], and it has led to the formulation of a successful family of search heuristics [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "Especially, accumulation of selected individuals is practically utilized by derandomized ES variants [8], and it has led to the formulation of a successful family of search heuristics [1].", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "The random variable \u03c8 = J(~z) now obeys a generalized \u03c72-distribution, whose exact distribution function is described as follows [5]: FH\u03c72(\u03c8) = \u222b \u221e 0 2 \u03c0 sin t\u03c8 2 t cos \uf8eb", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "At the same time, this CDF is known to follow an approximation [5], FT \u03c72 (\u03c8) = \u03a5\u03b7 \u0393 (\u03b7) \u222b \u03c8 0 t exp (\u2212\u03a5t) dt, (7) with \u03a5 and \u03b7 accounting for matching the first two moments of ~zTH~z (and the subscript T marks the transformed distribution): \u03a5 = 1 2 \u2211n i=1 \u2206i \u2211n i=1\u2206 2 i , \u03b7 = 1 2 ( \u2211n i=1\u2206i) 2 \u2211n i=1 \u2206 2 i .", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "(9) The accuracy of this approximation depends upon the standard deviation of the eigenvalues [5], which is clearly related to the so-called condition number.", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "(17) Gupta [7] showed that when the dimension n is even, the distribution of the winners for the \u03c7 distribution (isotropic basin case) possesses a simple form:", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "Gupta [7] derived explicit order statistic results from the Gamma distribution, to which the \u03c72 distribution belongs, including the distribution function as well as moments of the kth order statistic.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "0 if CDF\u03c8 (\u03c8) = 0 1 if CDF\u03c8 (\u03c8) > 0 According to the Fisher-Tippett theorem [6], also known as the extremal types theorem, the vonMises family of distributions for minima (or the minimal generalized extreme value distributions (GEVDmin)) are the only non-degenerate family of distributions satisfying this limit.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "6 in [3] [pp.", "startOffset": 5, "endOffset": 8}, {"referenceID": 9, "context": "Note that Rudolph had already taken a related mathematical approach, which he termed asymptotic theory of extreme order statistics, to characterize convergence properties of ESs on a class of convex objective functions [10].", "startOffset": 219, "endOffset": 223}, {"referenceID": 10, "context": "Also, GEVD is introduced to the broad perspective of Stochastic Global Optimization in [11], a book which also constitutes a proper mathematical reference for this topic, yet in a slightly different light.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "(27) See [3] and [4] for an overview on the family of generalized extreme value distributions and on the limit distributions of order statistics.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "(27) See [3] and [4] for an overview on the family of generalized extreme value distributions and on the limit distributions of order statistics.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "1 in [3][p.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "3 in [11].", "startOffset": 5, "endOffset": 9}], "year": 2016, "abstractText": "We study the theoretical capacity to statistically learn local landscape information by Evolution Strategies (ESs). Specifically, we investigate the covariance matrix when constructed by ESs operating with the selection operator alone. We model continuous generation of candidate solutions about quadratic basins of attraction, with deterministic selection of the decision vectors that minimize the objective function values. Our goal is to rigorously show that accumulation of winning individuals carries the potential to reveal valuable information about the search landscape, e.g., as already practically utilized by derandomized ES variants. We first show that the statistically-constructed covariance matrix over such winning decision vectors shares the same eigenvectors with the Hessian matrix about the optimum. We then provide an analytic approximation of this covariance matrix for a non-elitist multi-child (1, \u03bb)-strategy, which holds for a large population size \u03bb. Finally, we also numerically corroborate our results.", "creator": "LaTeX with hyperref package"}}}