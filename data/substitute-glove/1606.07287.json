{"id": "1606.07287", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Picture It In Your Mind: Generating High Level Visual Representations From Textual Descriptions", "abstract": "In given paper all tackle within problem for icon access when their query gives set time anecdotal described in with displaying however hardware even looking for. We choose from implement the actual web process as took linguistic access although hand visual picture mars, by concept to surely into diagnoses handwriting small well imagery representation. Searching in the display feature provide on the without which any update it as manuscripts newer want not require giving reprocess second, whereas huge, image photographs coming which rest sharing referred vocalist. We propose Text2Vis, a helical network that shortfall a presents representation, early all visual different base of as fc6 - fc7 dense important ImageNet, from yet into descriptive passages. Text2Vis traceback were rout functions, can though inverse loss - selection mechanism. A visual - focused loss latter focusing at opportunities the mere books - to - digital contemporary mapping, while a book - critical upset yet aimed start theory now current - equivalent semantic esoteric expressed and background and countering first overfit on non - relevant artistry technology an the visual loss. We press preliminary level without two MS - COCO dataset.", "histories": [["v1", "Thu, 23 Jun 2016 12:25:09 GMT  (2000kb,D)", "http://arxiv.org/abs/1606.07287v1", "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy"]], "COMMENTS": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.CV cs.NE", "authors": ["fabio carrara", "rea esuli", "tiziano fagni", "fabrizio falchi", "alejandro moreo fern\\'andez"], "accepted": false, "id": "1606.07287"}, "pdf": {"name": "1606.07287.pdf", "metadata": {"source": "CRF", "title": "Picture It In Your Mind: Generating High Level Visual Representations From Textual Descriptions", "authors": ["Fabio Carrara", "Andrea Esuli", "Tiziano Fagni", "Fabrizio Falchi", "Alejandro Moreo"], "emails": ["fabio.carrara@isti.cnr.it", "andrea.esuli@isti.cnr.it", "tiziano.fagni@isti.cnr.it", "fabrizio.falchi@isti.cnr.it", "alejandro.moreo@isti.cnr.it"], "sections": [{"heading": "Keywords", "text": "image retrieval; cross-media retrieval; text representation"}, {"heading": "1. INTRODUCTION", "text": "Using a textual query to retrieve images is a very common cross-media search task, as text is the most efficient media to describe the kind of image the user is searching for. The actual retrieval process can be implemented in a number of ways, depending on how the shared search space between text and images is defined. The search space can be based\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\nNeu-IR \u201916 SIGIR Workshop on Neural Information Retrieval July 21, 2016, Pisa, Italy c\u00a9 2016 Copyright held by the owner/author(s).\non textual features, visual features, or a joint space in which textual and visual features are projected into.\nUsing textual features is the most common solution, specially at the Web scale. Each image is associated with a set of textual features extracted from its context of use (e.g., the text surrounding the image in the Web page, description fields in metadata), and eventually enriched by means of classifiers that assign textual labels related to the presence or certain relevant entities or abstract properties in the image. The textual search space model can exploit the actual visual content of the image only when classifiers for the concepts of interest are available, thus requiring a relevant number of classifiers; this also requires to reprocess the entire image collection whenever a new classifier is made available.\nOn the other side, the visual and joint search spaces represent each image through visual features extracted from its actual content. The method we propose in this paper adopts a visual space search model. A textual query is converted into a visual representation in a visual space, where the search is performed by similarity. An advantage of this model is that any improvement in the text representation model, and its conversion to visual features, has immediate benefits on the image retrieval process, without requiring to reprocess the whole image collection.\nA joint space model requires instead a reprocessing of all images whenever the textual model is updated, since the projection of images into the joint space is influenced also by the textual model part. It also requires managing and storing the additional joint space representations that are used only for the cross-media search.\nIn this paper we present the preliminary results on learning Text2Vis, a neural network model that converts textual descriptions into visual representations in the same space of those extracted from deep Convolutional Neural Networks (CNN) such as ImageNet [15]. Text2Vis achieves its goal by using a stochastic loss choice on two separate loss functions (as detailed in Section 3), one for textual representations autoencoding, and one for visual representations generation. Preliminary results show that the produced visual representations capture the high level concepts expressed in the textual description.\nar X\niv :1\n60 6.\n07 28\n7v 1\n[ cs\n.I R\n] 2\n3 Ju"}, {"heading": "2. RELATED WORK", "text": "Deep Learning and Deep Convolutional Neural Networks (DCNNs) in particular, have recently shown impressive performance on a number of multimedia information retrieval tasks [15, 23, 10]. Deep Learning methods learn representations of data with multiple levels of abstraction. As a result, the activation of the hidden layers has been used in the context of transfer learning and content-based image retrieval [5, 22] as high-level representations of the visual content. Somewhat similarly, distributional semantic models, such as those produced by Word2Vec [18], or GloVe [21], have been found useful in modeling semantic similarities among words by establishing a correlation between word meaning and position in a vector space.\nIn order to perform cross-media retrieval, the two feature spaces (text and images in our case) should become comparable, typically by learning how to properly map the different sources. This problem has been attempted in different manners so far, which could be roughly grouped into three main variants, depending on whether the mapping is performed into a common space, the textual space, or the visual space.\nMapping into a common space: The idea of comparing texts and images in a shared space has been investigated by means of Cross-modal Factor Analysis and (Kernel) Canonical Correlation Analysis in [4]. In a similar vein, Corr-AE was proposed for cross-modal retrieval, allowing the search to be performed in both directions, i.e., from text-to-image and viceversa [8]. The idea is to train two autoencoders, one for the image domain and another for the textual domain, imposing restrictions between the two. As will be seen, the architecture we are presenting here bears resemblance to one of the architectures investigated in [8], the so-called Correspondence full-modal autoencoder (which is inspired by the multimodal deep learning method [19]). Contrarily to the multimodal architectures though, we apply a stochastic criterion to jointly optimize for the two modals, thus refraining from combining them into a parametric single loss.\nMapping into the textual space: The BoWDNN method trains a deep neural network (DNN) to map images directly into a bag-of-words (BoW) space, where the cosine similarity between BoWs representations is used to generate the ranking [1]. Somehow similarly, a dedicated area of related research is focused on generating captions describing the salient information of an image (see, e.g., [13, 7]).\nTwo other important examples along these lines are DeViSE [9] and ConSE [20]. Both methods build upon the higher layers of the convolutional neural network of [15]; the main difference lies on the way both methods treat the last layer of the net. Whereas DeViSE replaces this last layer with a linear mapping (thus fine-tuning the whole network) ConSE, on the other side, directly takes the outputs of the last layer and learns a projection to the textual embedding space.\nMapping into the visual space: Our proposal Text2Vis belongs to this group where, to the best of our knowledge, the only example up to now was a method dubbed Word2VisualVec [6], which was reported just very recently. There are some fundamental points where their method and ours differ, though. On the one hand, their Word2VisualVec takes combinations of Word2Vec-like vectors as a starting point, thus reducing the dimensionality of the input space; we directly start from\nthe bag-of-words vector encoding of the textual space, as we did not observed any improvement in pre-training the textual part. On the other, they build a deep network on top of the textual representation. As shall be seen, our Text2Vis is much shallower, as we found the net to be capable of mapping textual vectors into the visual space quite efficiently, provided that the model is properly regularized; an issue on which we focused our attention."}, {"heading": "3. GENERATING VISUAL REPRESENTATIONS OF TEXT", "text": "In this section we describe the architecture of our Text2Vis network. Our idea is to map textual descriptions to highlevel visual representations. As the visual space we used the fc6 and fc7 layers of the Hybrid network [24] (i.e., an AlexNet [15] trained on both ImageNet1 and Places2 datasets). We tested two vectorial representations for the textual descriptions: Text2Vis1 uses simple bag-of-words vectors that mark with a value of one the positions that are relative to words that appear in the textual description and leave to zero all the others; Text2VisN adds a bit text structure info by considering also N-grams for a selection of part-of-speech patterns3. Text2VisN is a first approach at modeling text structure into the input vectorial representation, which differentiates the task of search from detailed/complex textual description we aim at from the traditional keyword search.\nWe have also investigated the use of pre-trained word embeddings, representing the textual description as the average of the embeddings of the words composing the description (see Equation 1 in [6]), but we have not observed any improvement. Generating the word embeddings is an additional cost, and the fitness of the embeddings for the task depends on the type of documents they are learned from. For example, an 11% improvement in MAP is reported in [2] from learning embedding from Flickr tags compared to learning them from Wikipedia pages. The direct use of bagof-words vectors in Text2Vis removes the variable of selecting an appropriate document collection to learn the embedding and its learning cost.\nAs described in the following, Text2Vis actually learns a description embedding space that is able to reconstruct both the original description and the visual description. To reach this, we started with a simple regressor model (Figure 1, left) trained to directly predict the visual representation of the image associated with the textual input. We observed a strong tendency to overfit (Figure 1, right), thus degrading the applicability of the method to unseen images.\nWe explained this overfitting with the fact that a visual representation keeps track of every element that appears in the image, regardless of their semantic relevance within the image, while a (short) textual description is more likely focused on the visually relevant information, disregarding the secondary content of the image, as shown in Figure 4.6. As the learning iterations proceed, the simple regressor model starts capturing secondary elements of the images that are not relevant for the main represented concept, but are somewhat characteristic in the training data.\n1http://image-net.org 2http://places.csail.mit.edu/index.html 3We considered the part-of-speech patterns: \u2018NOUNVERB\u2019, \u2018NOUN-VERB-VERB\u2019, \u2018ADJ-NOUN\u2019, \u2018VERBPRT\u2019, \u2018VERB-VERB\u2019, \u2018NUM-NOUN\u2019, and \u2018NOUN-NOUN\u2019.\nOur Text2Vis proposal to contrast such overfitting is to add a text-to-text autoencoding branch to the hidden layer (Figure 2, left), forcing the model to satisfy two losses: one visual (text-to-visual regression) and one linguistic (text-totext autoencoder). The linguistic loss works at higher level of abstraction than the visual one, acting as a regularization constraint on the model, and preventing, as confirmed by our experiments, overfitting on the visual loss (Figure 2, right). As detailed in the next section, we implemented the use of the two losses with a stochastic process, in which at each iteration one of the two is selected for optimization.\n3.1 Text2Vis Text2Vis consists of two overlapped feedforward neural nets with a shared hidden layer. The shared hidden layer causes a regularization effect during the combined optimization; i.e., the hidden state is constrained to be a good representation to accomplish with two different goals. The feedforward computation is described by the following equations:\nz = ReLU(W1tin + b1) (1)\nt\u2032 = ReLU(W2z + b2) (2) v\u2032 = ReLU(W3z + b3) (3)\nwhere tin represents the bag-of-words encoding for the textual descriptor given as input to the net, z is the hidden representation, v\u2032 and t\u2032 are the visual and textual predictions, respectively, obtained from the hidden representation z, \u0398 = {Wi, bi}i\u2208{1,2,3} are the model parameters to be learned, and ReLU is the activation function, defined by ReLU(x) = max{0, x}.\nBoth predictions v\u2032 and t\u2032 are then confronted with the expected outputs (i) the visual representation v corresponding to the fc6 or fc7 layers of [15], and (ii) a textual descriptor tout that is semantically equivalent to tin. We used the mean squared error (MSE) as the loss function in both cases:\nL(x, y; \u0398\u2032) = MSE(x, y) = 1 n n\u2211 i=1 (xi \u2212 yi)2 (4)\nThe model is thus multi-objective, and many alternative strategies could be followed at this point in order to set the \u0398 parameters so that both criteria are jointly minimized. We rather propose a much simpler, yet effective, way for carrying out the optimization search, that consists of considering both branches of the net as independent, and randomly de-\nciding in each iteration which of them is to be used for the gradient descend optimization.\nLet thus define \u0398t = {Wi, bi}i\u2208{1,2} and \u0398v = {Wi, bi}i\u2208{1,3} as the model parameters of each independent branch. The optimization problem has two objectives (Equations 5 and 6), and at each iteration, a random choice decides which of them is to be optimized. We call this heuristic the Stochastic Loss (SL) optimization.\n\u0398t\u0302 = argmin\u0398tLt(tout, t \u2032; \u0398t) (5)\n\u0398v\u0302 = argmin\u0398vLv(v, v \u2032; \u0398v) (6)\nNote that the net is fed with a triple \u3008v, tin, tout\u3009 at each iteration. When tout = tin the text-to-text branch is an autoencoder. It is also possible to have tin 6= tout, with the two pieces of text been semantically equivalent (e.g., tin =\u201ca woman cutting a pizza with a knife\u201d, tout =\u201ca woman holds a knife to cut pizza\u201d) then the text-to-text branch might be reminiscent of the Skip-gram- and CBOW - like architectures. The text-to-image branch is, in any case, a regressor. The SL causes the model to be co-regularized. Notwithstanding, since our final goal is to project the textual descriptor into the visual space, the text-to-text branch might be though as a regularization to the visual reconstruction (and, more specifically, to its internal encoding) which responds to constrains of linguistic nature."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1 Datasets", "text": "We used the Microsoft COCO dataset (MsCOCO4 [17]). MsCOCO was originally proposed for image recognition, segmentation, and caption generation. Although other datasets for image retrieval exist (e.g., the one proposed in [11]), they are more oriented to keyword-based queries. We believe MsCOCO to be more fit to the scenario we want to explore, since the captions associated to the images are expressed in natural language, thus semantically richer than a short list of keywords composing a query.\nMsCOCO contains 82.783 training images (Train2014 ), 40.504 validation images (Val2014 ), and about 40K and 80K test images corresponding to two different competitions [3] (Test2014 and Test2015 ). Because MsCOCO was proposed for caption generation, the captions are only accessible in\n4Publicly available at http://mscoco.org/\nthe Train2014 and Val2014 sets, while they are not yet released for Test2014 and Test2015. We have thus taken the Train2014 set for training, and split the Val2014 into two disjoint sets of 20K images each for validation and test.\nEach image in MsCOCO has 5 different captions associated. Let \u3008I, C\u3009 be any labeled instance in MsCOCO, where I is an image and C = {c1..c5} is a set of captions describing the content of I. Given a \u3008I, C\u3009 pair, we define a labeled instance in our model as \u3008v, tin, tout\u3009, where v \u2208 R4096 is the visual representation of the image I taken from the fc6 layer (or fc7, in separate experiments) of the Hybrid network [24]; tin and tout are two textual descriptors from C representing the input and output descriptors for the model, respectively. During training, tin and tout are uniformly chosen at random from C (thus tin and tout are not imposed to be different). Note that the number of training instances one could extract from a given \u3008I, C\u3009 amounts to 25, which increases the variability of the training set along the different epochs."}, {"heading": "4.2 Training", "text": "We solve the optimization problems of Equations 5 and 6, using the Adam method [14] for stochastic optimization, with default parameters (learning rate \u03b1 = 0.001, \u03b21 = 0.9, \u03b22 = 0.999, and = 1e\n\u22120.8). Note that there are two independent instances of the Adam optimizer, one associated to Lt (Equation 5) and other for Lv (Equation 6). In this preliminary study we decided to set for the SL an equal selection probability to both Lt and Lv; different distributions will be investigated in future research.\nWe set the size of the training batch to 100 examples. We set the maximum number of iterations to 300.000, but apply an early stop when the model starts overfitting (as reflected in the validation error). The training set is shuffled each time a complete pass over all images is completed.\nAll the \u0398 parameters have been initialized at random according to a truncated normal distribution centered in zero with standard deviation of 1\u221a\nn , where n is the number of\ncolumns. The biases have all been initialized to 0. The vocabulary size is 10,358 for Text2Vis1 after removing terms appearing in less than 5 captions. For Text2VisN we considered the 23,968 uni-grams and N-grams appearing at least in 10 captions. Since the number of units in the hidden and output layers are 1024 and 4096, respectively, the total number of parameters of the models amount to 25.4M in Text2Vis1 and 53.3M in Text2VisN .\nA Tensorflow implementation of Text2Vis is available at https://github.com/AlexMoreo/tensorflow-Tex2Vis."}, {"heading": "4.3 Evaluation Measures", "text": "Image retrieval is performed by similarity search in the visual space, using Euclidean distance on the l2-normalized visual vectors to generate a ranking of images, sorted by closeness. We measure the retrieval effectiveness of the visual representations produced from textual descriptions by our Text2Vis network by means of the Discounted Cumulative Gain (DCG [12]), defined as:\nDCGp = p\u2211 i=1 2reli \u2212 1 log2(i+ 1)\n(7)\nwhere reli quantifies the relevance of the retrieved element at rank position i with respect to the query, and p is the rank at which the metric is computed; we set p = 25 in our experiments, as was done in related research [11, 6].\nBecause the rel values are not provided in the MsCOCO, we estimate them by using theROUGEL [16] metric. ROUGEL is one of the evaluation measures for the MsCOCO caption generation competition5 [3]. We compute reli = ROUGEL(tin, Ci), where tin is the query caption, and Ci are the 5 captions associated to the retrieved image at rank i. This captionto-caption relevance model is thus aimed at measuring how much the concepts expressed in the query appear as relevant parts of the retrieved images."}, {"heading": "4.4 Results", "text": "We compared the performance of Text2Vis1 and Text2VisN models against: RRank, a lower bound baseline that produces a random ranking of images, for any query; VisSim, a direct similarity method that computes the Euclidean distances using the original fc6, or fc7, features for the image that is associated to query caption in MsCOCO; and VisReg, the text-to-image regressor described in Figure 1.\nTable 4.4 reports the averaged DCG scores obtained by the compared methods. These results show a significant improvement of our proposal with respect to the compared methods. When using fc6 as the visual space, Text2Vis1 obtains a 8.51% relative improvement with respect to VisSim and 1.40% over VisReg. The improvements of Text2VisN are respectively of 8.08% and 0.94%. When using fc7 as the\n5https://github.com/tylin/coco-caption\nvisual space it is Text2VisN that obtains, yet by a small margin, the best result. The relative improvements of Text2Vis1 over emphVisSim and VisReg are respectively of 8.48% and 0.97%, and for Text2VisN respectively of 8.60% and 1.09%.\nIn addition to the averaged performance, we also investigated how often the ranking produced by Text2Vis is more relevant (according to DCG) than those produced by VisSim and VisReg. Figure 3 indicates that in 69.2% of the cases, the ranking of Text2Vis1 was found more relevant than VisSim (see Figure 3). The same happens in 58.1% of the cases when comparing Text2Vis to VisReg."}, {"heading": "4.5 Why Stochastic Loss?", "text": "Text2Vis uses two independent optimizers to optimize the visual (Lv) and the textual (Lt) losses, based on a stochas-\ntic choice at each iteration (SL, section 3.1). Previous approaches to multimodal learning relied instead on a unique aggregated loss (typically of the form L = Lv +\u03bbLt) that is minimized by a single optimizer [8, 19]. We compared the two approaches on the case of equal relevance of the two losses (\u03bb = 1, uniform distribution for SL). SL better optimizes the two losses (Figure 4), and is less prone to overfit.\nWe deem that SL allows to model in a more natural way the relative relevance of the various losses that are combined, i.e., by selecting the losses in proportion to the assigned relevance, whereas the numeric aggregation is affected by the relative values of losses and the differences in their variation during the optimization (e.g., a loss that has a large improvement may compensate for another loss getting worse). SL is also computationally lighter than the aggregated loss, as SL updates only a part of the model on each iteration."}, {"heading": "4.6 Visual comparison", "text": "Figure 5 show a few samples6 that highlight the differences in results from the three compared methods. In all the cases results from the VisSim method are dominated by the main visual features of the images: a face for the first query, the content of the screen for the second query, an outdoor image with a light lower part, plants, people and a bit of sky in the third one. The two text based methods obtains results that\n6More results at https://github.com/AlexMoreo/ tensorflow-Tex2Vis"}, {"heading": "VisSim", "text": ""}, {"heading": "VisReg", "text": ""}, {"heading": "Text2Vis", "text": ""}, {"heading": "VisSim", "text": ""}, {"heading": "VisReg", "text": ""}, {"heading": "Text2Vis", "text": ""}, {"heading": "VisSim", "text": ""}, {"heading": "VisReg", "text": ""}, {"heading": "Text2Vis", "text": "more often contain the key elements of the description. For the first query, Text2Vis retrieves four relevant images out of five, one more that VisReg. For the other two queries the results are pretty similar, with Text2Vis placing in second position an image that is a perfect match for the query, while VisReg places it in fifth position."}, {"heading": "5. CONCLUSIONS", "text": "The preliminary experiments indicate our method produces more relevant rankings than those produced by similarity search directly on the visual features of a query image. This is an indication that our text-to-image mapping produces better prototypical representations of the desired scene than the representation of a sample image itself. A simple explanation of this result is that textual descriptions\nstrictly emphasize the relevant aspects of the scene the user has in mind, whereas the visual features, directly extracted from the query image, are keeping track of all the information that is contained in that image, causing the similarity search to be potentially confused by secondary elements of the scene. The Text2Vis model also improved, yet by a smaller margin, over the VisReg model , showing that an auto-enconding branch in the network is useful to avoid overfitting on visual features. We also found that combing losses in a stochastic fashion, rather than numerically, improves both the effectiveness and efficiency of the system. In the future we plan to compare Text2Vis against the recently proposed Word2VisualVec [6] model. We also intend to improve the modeling word order information in Text2Vis, likely by adding a recurrent component to the network architecture."}, {"heading": "6. REFERENCES", "text": "[1] Y. Bai, W. Yu, T. Xiao, C. Xu, K. Yang, W.-Y. Ma,\nand T. Zhao. Bag-of-words based deep neural network for image retrieval. In Proceedings of the ACM International Conference on Multimedia, pages 229\u2013232. ACM, 2014.\n[2] S. Cappallo, T. Mensink, and C. G. Snoek. Image2emoji: Zero-shot emoji prediction for visual media. In Proceedings of the 23rd ACM International Conference on Multimedia, MM \u201915, pages 1311\u20131314, New York, NY, USA, 2015. ACM.\n[3] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dolla\u0301r, and C. L. Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\n[4] J. Costa Pereira, E. Coviello, G. Doyle, N. Rasiwasia, G. R. Lanckriet, R. Levy, and N. Vasconcelos. On the role of correlation and abstraction in cross-modal multimedia retrieval. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 36(3):521\u2013535, 2014.\n[5] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013.\n[6] J. Dong, X. Li, and C. G. M. Snoek. Word2VisualVec: Cross-Media Retrieval by Visual Feature Prediction. ArXiv e-prints, Apr. 2016.\n[7] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dolla\u0301r, J. Gao, X. He, M. Mitchell, J. C. Platt, et al. From captions to visual concepts and back. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1473\u20131482, 2015.\n[8] F. Feng, X. Wang, and R. Li. Cross-modal retrieval with correspondence autoencoder. In Proceedings of the ACM International Conference on Multimedia, pages 7\u201316. ACM, 2014.\n[9] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al. Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems, pages 2121\u20132129, 2013.\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.\n[11] X.-S. Hua, L. Yang, J. Wang, J. Wang, M. Ye, K. Wang, Y. Rui, and J. Li. Clickage: Towards bridging semantic and intent gaps via mining click logs of search engines. In Proceedings of the 21st ACM international conference on Multimedia, pages 243\u2013252. ACM, 2013.\n[12] K. Ja\u0308rvelin and J. Keka\u0308la\u0308inen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems (TOIS), 20(4):422\u2013446, 2002.\n[13] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137, 2015.\n[14] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.\n[16] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In S. S. Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguistics.\n[17] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla\u0301r, and C. L. Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer, 2014.\n[18] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013.\n[19] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 689\u2013696, 2011.\n[20] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens, A. Frome, G. S. Corrado, and J. Dean. Zero-shot learning by convex combination of semantic embeddings. arXiv preprint arXiv:1312.5650, 2013.\n[21] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pages 1532\u20131543, 2014.\n[22] A. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 806\u2013813, 2014.\n[23] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[24] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In Advances in neural information processing systems, pages 487\u2013495, 2014."}], "references": [{"title": "Bag-of-words based deep neural network for image retrieval", "author": ["Y. Bai", "W. Yu", "T. Xiao", "C. Xu", "K. Yang", "W.-Y. Ma", "T. Zhao"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 229\u2013232. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Image2emoji: Zero-shot emoji prediction for visual media", "author": ["S. Cappallo", "T. Mensink", "C.G. Snoek"], "venue": "Proceedings of the 23rd ACM International Conference on Multimedia, MM \u201915, pages 1311\u20131314, New York, NY, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "On the role of correlation and abstraction in cross-modal multimedia retrieval", "author": ["J. Costa Pereira", "E. Coviello", "G. Doyle", "N. Rasiwasia", "G.R. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 36(3):521\u2013535", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Word2VisualVec: Cross-Media Retrieval by Visual Feature Prediction", "author": ["J. Dong", "X. Li", "C.G.M. Snoek"], "venue": "ArXiv e-prints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "et al", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "From captions to visual concepts and back. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1473\u20131482", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-modal retrieval with correspondence autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 7\u201316. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems, pages 2121\u20132129", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Clickage: Towards bridging semantic and intent gaps via mining click logs of search engines", "author": ["X.-S. Hua", "L. Yang", "J. Wang", "J. Wang", "M. Ye", "K. Wang", "Y. Rui", "J. Li"], "venue": "Proceedings of the 21st ACM international conference on Multimedia, pages 243\u2013252. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS), 20(4):422\u2013446", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), pages 689\u2013696", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G.S. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1312.5650", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 806\u2013813", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in neural information processing systems, pages 487\u2013495", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "In this paper we present the preliminary results on learning Text2Vis, a neural network model that converts textual descriptions into visual representations in the same space of those extracted from deep Convolutional Neural Networks (CNN) such as ImageNet [15].", "startOffset": 257, "endOffset": 261}, {"referenceID": 14, "context": "Deep Learning and Deep Convolutional Neural Networks (DCNNs) in particular, have recently shown impressive performance on a number of multimedia information retrieval tasks [15, 23, 10].", "startOffset": 173, "endOffset": 185}, {"referenceID": 22, "context": "Deep Learning and Deep Convolutional Neural Networks (DCNNs) in particular, have recently shown impressive performance on a number of multimedia information retrieval tasks [15, 23, 10].", "startOffset": 173, "endOffset": 185}, {"referenceID": 9, "context": "Deep Learning and Deep Convolutional Neural Networks (DCNNs) in particular, have recently shown impressive performance on a number of multimedia information retrieval tasks [15, 23, 10].", "startOffset": 173, "endOffset": 185}, {"referenceID": 4, "context": "As a result, the activation of the hidden layers has been used in the context of transfer learning and content-based image retrieval [5, 22] as high-level representations of the visual content.", "startOffset": 133, "endOffset": 140}, {"referenceID": 21, "context": "As a result, the activation of the hidden layers has been used in the context of transfer learning and content-based image retrieval [5, 22] as high-level representations of the visual content.", "startOffset": 133, "endOffset": 140}, {"referenceID": 17, "context": "Somewhat similarly, distributional semantic models, such as those produced by Word2Vec [18], or GloVe [21], have been found useful in modeling semantic similarities among words by establishing a correlation between word meaning and position in a vector space.", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "Somewhat similarly, distributional semantic models, such as those produced by Word2Vec [18], or GloVe [21], have been found useful in modeling semantic similarities among words by establishing a correlation between word meaning and position in a vector space.", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "Mapping into a common space: The idea of comparing texts and images in a shared space has been investigated by means of Cross-modal Factor Analysis and (Kernel) Canonical Correlation Analysis in [4].", "startOffset": 195, "endOffset": 198}, {"referenceID": 7, "context": ", from text-to-image and viceversa [8].", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "As will be seen, the architecture we are presenting here bears resemblance to one of the architectures investigated in [8], the so-called Correspondence full-modal autoencoder (which is inspired by the multimodal deep learning method [19]).", "startOffset": 119, "endOffset": 122}, {"referenceID": 18, "context": "As will be seen, the architecture we are presenting here bears resemblance to one of the architectures investigated in [8], the so-called Correspondence full-modal autoencoder (which is inspired by the multimodal deep learning method [19]).", "startOffset": 234, "endOffset": 238}, {"referenceID": 0, "context": "Mapping into the textual space: The BoWDNN method trains a deep neural network (DNN) to map images directly into a bag-of-words (BoW) space, where the cosine similarity between BoWs representations is used to generate the ranking [1].", "startOffset": 230, "endOffset": 233}, {"referenceID": 12, "context": ", [13, 7]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 6, "context": ", [13, 7]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 8, "context": "Two other important examples along these lines are DeViSE [9] and ConSE [20].", "startOffset": 58, "endOffset": 61}, {"referenceID": 19, "context": "Two other important examples along these lines are DeViSE [9] and ConSE [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "Both methods build upon the higher layers of the convolutional neural network of [15]; the main difference lies on the way both methods treat the last layer of the net.", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "Mapping into the visual space: Our proposal Text2Vis belongs to this group where, to the best of our knowledge, the only example up to now was a method dubbed Word2VisualVec [6], which was reported just very recently.", "startOffset": 174, "endOffset": 177}, {"referenceID": 23, "context": "As the visual space we used the fc6 and fc7 layers of the Hybrid network [24] (i.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": ", an AlexNet [15] trained on both ImageNet and Places datasets).", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "We have also investigated the use of pre-trained word embeddings, representing the textual description as the average of the embeddings of the words composing the description (see Equation 1 in [6]), but we have not observed any improvement.", "startOffset": 194, "endOffset": 197}, {"referenceID": 1, "context": "For example, an 11% improvement in MAP is reported in [2] from learning embedding from Flickr tags compared to learning them from Wikipedia pages.", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "Both predictions v\u2032 and t\u2032 are then confronted with the expected outputs (i) the visual representation v corresponding to the fc6 or fc7 layers of [15], and (ii) a textual descriptor tout that is semantically equivalent to tin.", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "1 Datasets We used the Microsoft COCO dataset (MsCOCO [17]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": ", the one proposed in [11]), they are more oriented to keyword-based queries.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "504 validation images (Val2014 ), and about 40K and 80K test images corresponding to two different competitions [3] (Test2014 and Test2015 ).", "startOffset": 112, "endOffset": 115}, {"referenceID": 23, "context": "Given a \u3008I, C\u3009 pair, we define a labeled instance in our model as \u3008v, tin, tout\u3009, where v \u2208 R is the visual representation of the image I taken from the fc6 layer (or fc7, in separate experiments) of the Hybrid network [24]; tin and tout are two textual descriptors from C representing the input and output descriptors for the model, respectively.", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "We solve the optimization problems of Equations 5 and 6, using the Adam method [14] for stochastic optimization, with default parameters (learning rate \u03b1 = 0.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "We measure the retrieval effectiveness of the visual representations produced from textual descriptions by our Text2Vis network by means of the Discounted Cumulative Gain (DCG [12]), defined as:", "startOffset": 176, "endOffset": 180}, {"referenceID": 10, "context": "where reli quantifies the relevance of the retrieved element at rank position i with respect to the query, and p is the rank at which the metric is computed; we set p = 25 in our experiments, as was done in related research [11, 6].", "startOffset": 224, "endOffset": 231}, {"referenceID": 5, "context": "where reli quantifies the relevance of the retrieved element at rank position i with respect to the query, and p is the rank at which the metric is computed; we set p = 25 in our experiments, as was done in related research [11, 6].", "startOffset": 224, "endOffset": 231}, {"referenceID": 15, "context": "Because the rel values are not provided in the MsCOCO, we estimate them by using theROUGEL [16] metric.", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "ROUGEL is one of the evaluation measures for the MsCOCO caption generation competition [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "Previous approaches to multimodal learning relied instead on a unique aggregated loss (typically of the form L = Lv +\u03bbLt) that is minimized by a single optimizer [8, 19].", "startOffset": 162, "endOffset": 169}, {"referenceID": 18, "context": "Previous approaches to multimodal learning relied instead on a unique aggregated loss (typically of the form L = Lv +\u03bbLt) that is minimized by a single optimizer [8, 19].", "startOffset": 162, "endOffset": 169}], "year": 2016, "abstractText": "In this paper we tackle the problem of image search when the query is a short textual description of the image the user is looking for. We choose to implement the actual search process as a similarity search in a visual feature space, by learning to translate a textual query into a visual representation. Searching in the visual feature space has the advantage that any update to the translation model does not require to reprocess the, typically huge, image collection on which the search is performed. We propose Text2Vis, a neural network that generates a visual representation, in the visual feature space of the fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis optimizes two loss functions, using a stochastic loss-selection method. A visual-focused loss is aimed at learning the actual text-to-visual feature mapping, while a text-focused loss is aimed at modeling the higherlevel semantic concepts expressed in language and countering the overfit on non-relevant visual components of the visual loss. We report preliminary results on the MS-COCO dataset.", "creator": "LaTeX with hyperref package"}}}