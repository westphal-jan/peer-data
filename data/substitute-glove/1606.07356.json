{"id": "1606.07356", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Analyzing the Behavior of Visual Question Answering Models", "abstract": "Recently, any number of broken - teach based models still since approving give held command of Visual Question Answering (VQA ). The talent brought most redesigned this clustered around 52 - 85% . In would picture let propose systematic methods to carefully full behavior but these conventional as brought next make towards participation and strengths and exposes, rest identifying saw being consultations shifts one change. We decipher in doing throughout models which two. classes of VQA engine - - all - attention although otherwise - attention and seeing put resemblances and relation in way merely of among modified.", "histories": [["v1", "Thu, 23 Jun 2016 16:05:16 GMT  (8531kb,D)", "http://arxiv.org/abs/1606.07356v1", "13 pages, 20 figures; Under review at EMNLP 2016"], ["v2", "Tue, 27 Sep 2016 19:56:22 GMT  (5647kb,D)", "http://arxiv.org/abs/1606.07356v2", "13 pages, 20 figures; To appear in EMNLP 2016"]], "COMMENTS": "13 pages, 20 figures; Under review at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["aishwarya agrawal", "dhruv batra", "devi parikh"], "accepted": true, "id": "1606.07356"}, "pdf": {"name": "1606.07356.pdf", "metadata": {"source": "CRF", "title": "Analyzing the Behavior of Visual Question Answering Models", "authors": ["Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh"], "emails": ["parikh}@vt.edu"], "sections": [{"heading": null, "text": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze the best performing models from two major classes of VQA models \u2013 with-attention and without-attention and show the similarities and differences in the behavior of these models.\nOur behavior analysis reveals that despite recent progress, today\u2019s VQA models are \u201cmyopic\u201d (tend to fail on sufficiently novel instances), often \u201cjump to conclusions\u201d (converge on a predicted answer after \u2018listening\u2019 to just half the question), and are \u201cstubborn\u201d (do not change their answers across images)."}, {"heading": "1 Introduction", "text": "Visual Question Answering (VQA) is a recentlyintroduced (Antol et al., 2015; Geman et al., 2014; Malinowski and Fritz, 2014) problem where \u2013 given an image and a natural language question (e.g., \u201cWhat kind of store is this?\u201d, \u201cHow many people are waiting in the queue?\u201d), the task is to automatically produce an accurate natural language answer (\u201cbakery\u201d, \u201c5\u201d). A flurry of recent deep-learning based models have been proposed for VQA (Antol et al., 2015; Chen et al., 2015; Yang et al., 2015; Xu\nand Saenko, 2015; Jiang et al., 2015; Andreas et al., 2015; Wang et al., 2015; Kafle and Kanan, 2016; Lu et al., 2016; Andreas et al., 2016; Shih et al., 2015; Kim et al., 2016; Fukui et al., 2016; Noh and Han, 2016; Ilievski et al., 2016; Wu et al., 2015; Xiong et al., 2016; Zhou et al., 2015; Saito et al., 2016). Curiously, the performance of most methods is clustered around 60-70% (compared to human performance of 83% on open-ended task and 91% on multiplechoice task) with a mere 5% gap between the top-9 entries on the VQA challenge.1 It seems clear that as a first step to understand these models, to meaningfully compare strengths and weaknesses of different models, to develop insights into their failure modes, and to identify the most fruitful directions for progress, it is crucial to develop techniques to understand the behavior of VQA models.\nIn this paper, we develop novel techniques for characterizing the behavior of VQA models. As concrete instantiations, we analyze two VQA models ((Lu et al., 2015),(Lu et al., 2016)), one from each of the two major classes of VQA models \u2013 with-attention and without-attention."}, {"heading": "2 Related Work", "text": "Our work is inspired by previous works that diagnose the failure modes of models for different tasks. (Karpathy et al., 2015) constructed a series of oracles to measure the performance of a character level language model. (Hoiem et al., 2012) provided analysis tools to facilitate detailed and meaningful investigation of object detector performance. This paper\n1http://www.visualqa.org/challenge.html\nar X\niv :1\n60 6.\n07 35\n6v 1\n[ cs\n.C L\n] 2\n3 Ju\nn 20\naims to perform behavior analyses as a first step towards diagnosing errors for VQA.\n(Yang et al., 2015) categorize the errors made by their VQA model into four categories \u2013 model focuses attention on incorrect regions, model focuses attention on appropriate regions but predicts incorrect answers, predicted answers are different from labels but might be acceptable, labels are wrong. While these are coarse but useful failure modes, we are interested in understanding the behavior of VQA models along specific dimensions \u2013 whether they generalize to novel instances, whether they listen to the entire question, whether they look at the image."}, {"heading": "3 Behavior Analyses", "text": "We analyze the behavior of VQA models along the following three dimensions \u2013\nGeneralization to novel instances: We investigate whether the test instances that are incorrectly answered are the ones that are \u201cnovel\u201d i.e., not similar to training instances. The novelty of the test instances may be in two ways \u2013 1) the test questionimage (QI) pair is \u201cnovel\u201d, i.e., too different from training QI pairs; and 2) the test QI pair is \u201cfamiliar\u201d, but the answer required at test time is \u201cnovel\u201d, i.e., answers seen during training are different from what needs to be produced for the test QI pairs.\nComplete question understanding: To investigate whether a VQA model is understanding the input question or not, we analyze whether the model \u2018listens\u2019 to only first few words of the question or the entire question, whether it \u2018listens\u2019 to only question (wh) words and nouns or all the words in the question.\nComplete image understanding: The absence of a large gap in performance of language-alone and language + vision VQA models (Antol et al., 2015) provides evidence that current VQA models seem to be heavily reliant on the language model, perhaps not really understanding the image. In order to analyze this behavior, we investigate whether the predictions of the model change across images for a given question.\nWe present our behavioral analyses on the VQA dataset (Antol et al., 2015). VQA is a large-scale dataset containing \u223c 0.25M images, \u223c\n0.76Mquestions, and \u223c 10Manswers, with openended and multiple choice modalities for answering the visual questions. All the experimental results are reported on the VQA validation set using the following models trained on the VQA train set for the open-ended task \u2013\nCNN + LSTM based model without-attention (CNN+LSTM): We use the best performing model of (Antol et al., 2015) (code provided by (Lu et al., 2015)), which achieves an accuracy of 54.13% on the VQA validation set. It is a two channel model \u2013 one channel processes the image (using Convolutional Neural Network (CNN) to extract image features) and the other channel processes the question (using Long Short-Term Memory (LSTM) recurrent neural network to obtain question embedding). The image and question features obtained from the two channels are combined and passed through a fully connected (FC) layer to obtain a softmax distribution over the space of answers.\nCNN + LSTM based model with-attention (ATT): We use the top-entry on the VQA challenge leaderboard (as of the time of this submission) (Lu et al., 2016), which achieves an accuracy of 57.02% on the VQA validation set.2 This model jointly reasons about image and question attention, in a hierarchical fashion. The attended image and question features obtained from different levels of the hierarchy are combined and passed through a FC layer to obtain a softmax distribution over the space of answers."}, {"heading": "3.1 Generalization to novel instances", "text": "Do VQA models make mistakes because test instances are too different from training ones? To analyze the first type of novelty (the test QI pair is novel), we measure the correlation between test accuracy and distance of test QI pairs from training QI pairs. For each test QI pair we find its k nearest neighbors (k-NNs) and compute the average distance between the test QI pair and its k-NNs. The kNNs are computed in the space of combined image + question embedding (just before passing through FC layer), using euclidean distance metric, for both the CNN+LSTM and ATT models.\n2Code available at https://github.com/ jiasenlu/HieCoAttenVQA\nThe correlation between accuracy and average distance is significant (-0.41 at k = 503 for the CNN+LSTM model and -0.42 at k = 154 for the ATT model). A high negative correlation value tells us that the model is less likely to predict correct answers for test QI pairs which are not very similar to training QI pairs, which suggests that the model is not very good at generalizing to novel test QI pairs.\nWe also found that 67.5% of mistakes made by VQA model can be successfully predicted by checking distance of test QI pair from its training nearest neighbors for the CNN+LSTM model (66.7% for the ATT model). Thus, this analysis not only exposes a reason for mistakes made by VQA models, but also allows us to build human-like models that can predict their own oncoming failures, and potentially refuse to answer questions that are \u2018too different\u2019 from ones seen in past.\nTo analyze the second type of novelty (the answer required at test time is not familiar), we compute the correlation between test accuracy and the average distance of the test ground truth (GT) answer with GT answers of its k-NN training QI pairs. The k-NNs are computed in the space of average Word2Vec (Mikolov et al., 2013) vectors of answers. This correlation turns out to be quite high, -0.62 for both CNN+LSTM and ATT models. A high negative value of correlation value tells us that the model tends to regurgitate answers seen during training.\nThese distance features are also good at predicting failures \u2013 74.19% of failures can be predicted by checking distance of test GT answer with GT answers of its k-NN training QI pairs for CNN+LSTM model (75.41% for the ATT model). Note that unlike the previous analysis, this analysis only explains failures but cannot be used to predict failures (since it uses GT labels). See Fig. 1 for qualitative examples.\nFrom Fig. 1 we can see that the test instance in row1 is semantically quite different from its neighbors, explaining the mistake. The second row shows an example where the model has seen the same question in the training set but, since it has not seen \u201cgreen cone\u201d in its nearest neighbor training set, it\n3k = 50 leads to highest correlation 4k = 15 leads to highest correlation\nis not able to answer the test sample correctly. This shows that current models lack compositionality \u2013 the ability to combine the concepts of \u201ccone\u201d and \u201cgreen\u201d (both of which have been seen in training set) to predict \u201cgreen cone\u201d as the answer to the test sample. This compositionality is desirable and central to intelligence."}, {"heading": "3.2 Complete question understanding", "text": "We feed partial questions of increasing lengths (from 0-100% of question from left to right). We then compute what percentage of responses do not change when more and more words are fed.\nFig. 2 shows the test accuracy and percentage of questions for which responses remain same (compared to entire question) as a function of partial question length. We can see that for 40% of the questions, the CNN+LSTM model seems to have converged on a predicted answer after \u2018listening\u2019 to just half the question. This shows that the model is listening to first few words of the question more\nthan the words towards the end. Also, the model has 68% of the final accuracy (54%) when making predictions based on half the original question. The ATT model seems to have converged on a predicted answer after listening to just half the question more often (49% of the times), achieving 74% of the final accuracy (57%). See Fig. 3 for qualitative examples.\nWe also analyze the change in responses of the model\u2019s predictions (see Fig. 4), when words of a particular part-of-the-speech (POS) tag are dropped from the question. The experimental results indicate that wh-words effect the model\u2019s decisions the most (most of the responses get changed on dropping these words from the question), and that pronouns effect the model\u2019s decisions the least."}, {"heading": "3.3 Complete image understanding", "text": "Does a VQA model really \u2018look\u2019 at the image? In order to analyze this, we compute the percentage of times (say X) the response does not change (i.e., answer for all images is \u201c2\u201d) across images for a given question (say \u201cHow many zebras?\u201d) and plot histogram of X across questions (see Fig. 5). We do this analysis only for those questions in VQA validation set which have been asked for atleast 25 images, resulting in total 263 questions. The cumu-\nlative plot indicates that for 56% of questions, the CNN+LSTM model outputs the same answer for at least half the images. This is fairly high, suggesting that the model is picking the same answer no matter what the image is. Promisingly, the ATT model (that does not work with a holistic entire-image representation and purportedly pays attention to specific spatial regions in an image) produces the same response for at least half of the images for fewer questions (42%).\nInterestingly, the average accuracy (see the VQA accuracy plots in Fig. 5) for such questions (for which the models produce same response for at least half the images) is 56% for the CNN+LSTM model (60% for the ATT model) which is more than the respective average accuracy on the entire VQA validation set (54.13% for the CNN+LSTM model and 57.02% for the ATT model). Thus, producing the same response across images seems to be statistically favorable. Fig. 6 shows examples where the CNN+LSTM model predicts the same response across images for a given question. The first row shows examples where the model makes errors on several images by predicting the same answer across all images. The last row shows examples where the model is always correct even if it predicts the same answer across images. This is so because questions such as \u201cWhat covers the ground?\u201d are asked for an image in the VQA dataset only when ground is covered with snow (because subjects were looking at the image while asking questions about it)."}, {"heading": "4 Conclusion", "text": "We develop novel techniques for characterizing the behavior of VQA models, as a first step towards understanding these models, meaningfully comparing the strengths and weaknesses of different models, developing insights into their failure modes, and identifying the most fruitful directions for progress. Our behavior analysis reveals that despite recent progress, today\u2019s VQA models often \u201cjump to conclusions\u201d (converge on a predicted answer after \u2018listening\u2019 to just half the question), are \u201cstubborn\u201d (do not change their answers across images) with attention based models being less \u201cstubborn\u201d than nonattention based models, and are \u201cmyopic\u201d (tend to fail on sufficiently novel instances)."}, {"heading": "Appendix Overview", "text": "In the appendix, we provide:\nI - Behavioral analysis for question-only and image-only VQA models (Appendix I).\nII - Scatter plot of average distance of test instances from nearest neighbor training instances w.r.t. VQA accuracy (Appendix II).\nIII - Additional qualitative examples for \u201cgeneralization to novel test instances\u201d (Appendix III).\nIV - The analyses on \u201ccomplete question understanding\u201d for different question types (Appendix IV).\nV - Additional qualitative examples for \u201ccomplete question understanding\u201d (Appendix V).\nVI - The analyses on \u201ccomplete image understanding\u201d for different question types (Appendix VI).\nVII - Additional qualitative examples for \u201ccomplete image understanding\u201d (Appendix VII).\nAppendix I: Behavioral analysis for question-only and image-only VQA models\nWe evaluated the performance of both CNN+LSTM and ATT models by just feeding in the question (and mean image embedding) and by just feeding in the image (and mean question embedding). We computed the percentage of responses that change on feeding the question as well, compared to only feeding in the image and the percentage of responses that change on feeding the image as well, compared to only feeding in the question. We found that that the responses changed much more (about 40% more) on addition of the question than they did on addition of the image. So this suggests that the VQA models are heavily driven by question rather than the image.\nAppendix II: Scatter plot of average distance of test instances from nearest neighbor training instances w.r.t. VQA accuracy\nFig. 7 shows the variation of accuracy of test point w.r.t their average distance from k-NN training points for the CNN+LSTM model. Each point in the plot represents average statistics (accuracy and average distance) for a random subset of 25 test points. We can see that for the test points with low accuracy, the average distance is higher compared to test points with high accuracy. The correlation between accuracy and average distance is significant (-0.41 at best k = 50)\nAppendix III: Additional qualitative examples for \u201cgeneralization to novel test instances\u201d\nFig. 8 shows test QI pairs for which the CNN+LSTM model produces the correct response and their nearest neighbor QI pairs from training set. It can be seen that the nearest neighbor QI pairs from the training set are similar to the test QI pair. In addition, the GT labels in the training set are similar to the test GT label.\nFig. 9 shows test QI pairs for which the CNN+LSTM model produces incorrect response and their nearest neighbor QI pairs from training set. Some of the mistakes are probably because the test QI pair does not have similar QI pairs in the training set (rows 2, 4 and 5) while other mistakes are probably because the GT labels in the training set are not similar to the GT test label (rows 1 and 3).\nAppendix IV: Analyses on \u201ccomplete question understanding\u201d for different question types\nWe show the breakdown of our analyses from the main paper \u2013 (i) whether the model \u2018listens\u2019 to the entire question; and (ii) which POS tags matter the most \u2013 over the three major categories of questions \u2013 \u201cyes/no\u201d, \u201cnumber\u201d and \u201cother\u201d as categorized in (Antol et al., 2015). \u201cyes/no\u201d are questions whose answers are either \u201cyes\u201d or \u201cno\u201d, \u201cnumber\u201d are questions whose answers are numbers (e.g., \u201cQ: How many zebras are there?\u201d, \u201cA: 2\u201d), \u201cother\u201d are rest of the questions.\nFor \u201cyes/no\u201d questions, the ATT model seems particularly \u2018jumpy\u2019 \u2013 converging on a predicted answer listening to only the first few words of the question (see Fig. 10). Surprisingly, the accuracy is also as much as the final accuracy (after listening to entire question) when making predictions based on first few words of the question. On the contrast, the CNN+LSTM model converges on a predicted answer later, after listening to atleast 35% of the question, achieving as much as the final accuracy after convergence. For \u201cnumber\u201d and \u201cother\u201d questions, both ATT and CNN+LSTM model show similar trends (see Fig. 11 for \u201cnumber\u201d and Fig. 12 for \u201cother\u201d).\nIt is interesting to note that VQA models are most sensitive to adjectives for \u201cyes/no\u201d questions (compared to wh-words for all questions) (see Fig. 13). This is probably because often the \u201cyes/no\u201d questions are about attributes of objects (e.g., \u201cIs the cup empty?\u201d). For \u201cnumber\u201d questions, the CNN+LSTM model is most sensitive to adjectives whereas the ATT model is most sensitive to wh-\nwords (see Fig. 14). For \u201cother\u201d questions, both the models are most sensitive to \u201cnouns\u201d (see Fig. 15).\nAppendix V: Additional qualitative examples for \u201ccomplete question understanding\u201d\nFig. 16 shows examples where the CNN+LSTM model converges on a predicted answer without listening to the entire question. On doing so, the model gets the answer correct for some QI pairs (first three rows) and incorrect for others (last two rows).\nAppendix VI: Analyses on \u201ccomplete image understanding\u201d for different question types\nFig. 17, Fig. 18 and Fig. 19 show the breakdown of percentage of questions for which the model produces same answer across images for \u201cyes/no\u201d, \u201cnumber\u201d and \u201cother\u201d respectively. The ATT model seems to be more \u201cstubborn\u201d (does not change its answers across images) for \u201cyes/no\u201d questions compared to the CNN+LSTM model, and less \u201cstubborn\u201d for \u201cnumber\u201d questions compared to the CNN+LSTM model.\nAppendix VII: Additional qualitative examples for \u201ccomplete image understanding\u201d\nFig. 20 shows examples where the CNN+LSTM model produces the same answer for atleast half the images for a given question and the accuracy achieved by the model for such QI pairs."}], "references": [{"title": "Deep compositional question answering with neural module", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "networks. CoRR,", "citeRegEx": "Andreas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2015}, {"title": "Learning to compose neural networks for question answering", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "CoRR, abs/1601.01705", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In International Conference on Computer Vision (ICCV)", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "ABC-CNN: an attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia"], "venue": "CoRR, abs/1511.05960", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Fukui et al.2016] Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": "CoRR, abs/1606.01847", "citeRegEx": "Fukui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "A Visual Turing Test for Computer Vision Systems", "author": ["Geman et al.2014] Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes"], "venue": "In PNAS", "citeRegEx": "Geman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2014}, {"title": "Diagnosing error in object detectors", "author": ["Hoiem et al.2012] Derek Hoiem", "Yodsawalai Chodpathumwan", "Qieyun Dai"], "venue": "In Proceedings of the 12th European Conference on Computer Vision - Volume Part III,", "citeRegEx": "Hoiem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoiem et al\\.", "year": 2012}, {"title": "A focused dynamic attention model for visual question answering", "author": ["Shuicheng Yan", "Jiashi Feng"], "venue": "CoRR, abs/1604.01485", "citeRegEx": "Ilievski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ilievski et al\\.", "year": 2016}, {"title": "Compositional memory for visual question answering", "author": ["Jiang et al.2015] Aiwen Jiang", "Fang Wang", "Fatih Porikli", "Yi Li"], "venue": "CoRR, abs/1511.05676", "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "Answer-type prediction for visual question answering", "author": ["Kafle", "Kanan2016] Kushal Kafle", "Christopher Kanan"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Kafle et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kafle et al\\.", "year": 2016}, {"title": "Visualizing and understanding recurrent networks", "author": ["Justin Johnson", "Fei-Fei Li"], "venue": "CoRR, abs/1506.02078", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Multimodal residual learning for visual", "author": ["Kim et al.2016] Jin-Hwa Kim", "Sang-Woo Lee", "DongHyun Kwak", "Min-Oh Heo", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": "qa. CoRR,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Deeper lstm and normalized cnn visual question answering model", "author": ["Lu et al.2015] Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh"], "venue": "https://github.com/VT-vision-lab/ VQA_LSTM_CNN", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Hierarchical question-image coattention", "author": ["Lu et al.2016] Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "CoRR, abs/1606.00061v1", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input", "author": ["Malinowski", "Fritz2014] Mateusz Malinowski", "Mario Fritz"], "venue": "In NIPS", "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Training recurrent answering units with joint loss minimization for vqa", "author": ["Noh", "Han2016] Hyeonwoo Noh", "Bohyung Han"], "venue": "CoRR, abs/1606.03647", "citeRegEx": "Noh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2016}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["Saito et al.2016] Kuniaki Saito", "Andrew Shin", "Yoshitaka Ushiku", "Tatsuya Harada"], "venue": "CoRR, abs/1606.06108", "citeRegEx": "Saito et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Saito et al\\.", "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Shih et al.2015] Kevin J. Shih", "Saurabh Singh", "Derek Hoiem"], "venue": "CoRR, abs/1511.07394", "citeRegEx": "Shih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shih et al\\.", "year": 2015}, {"title": "Explicit knowledge-based reasoning for visual question answering", "author": ["Wang et al.2015] Peng Wang", "Qi Wu", "Chunhua Shen", "Anton van den Hengel", "Anthony R. Dick"], "venue": "CoRR, abs/1511.02570", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Wu et al.2015] Qi Wu", "Peng Wang", "Chunhua Shen", "Anton van den Hengel", "Anthony R. Dick"], "venue": "CoRR, abs/1511.06973", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Xiong et al.2016] Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "CoRR, abs/1603.01417", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Ask, attend and answer: Exploring questionguided spatial attention for visual question answering", "author": ["Xu", "Saenko2015] Huijuan Xu", "Kate Saenko"], "venue": "CoRR, abs/1511.05234", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Yang et al.2015] Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola"], "venue": "CoRR, abs/1511.02274", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Simple baseline for visual question answering", "author": ["Zhou et al.2015] Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": "CoRR, abs/1512.02167", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Visual Question Answering (VQA) is a recentlyintroduced (Antol et al., 2015; Geman et al., 2014; Malinowski and Fritz, 2014) problem where \u2013 given an image and a natural language question (e.", "startOffset": 56, "endOffset": 124}, {"referenceID": 5, "context": "Visual Question Answering (VQA) is a recentlyintroduced (Antol et al., 2015; Geman et al., 2014; Malinowski and Fritz, 2014) problem where \u2013 given an image and a natural language question (e.", "startOffset": 56, "endOffset": 124}, {"referenceID": 12, "context": "els ((Lu et al., 2015),(Lu et al.", "startOffset": 5, "endOffset": 22}, {"referenceID": 13, "context": ", 2015),(Lu et al., 2016)), one from each of the two major classes of VQA models \u2013 with-attention and without-attention.", "startOffset": 8, "endOffset": 25}, {"referenceID": 10, "context": "(Karpathy et al., 2015) constructed a series of ora-", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "(Hoiem et al., 2012) provided analysis tools to facilitate detailed and meaningful investigation of object detector performance.", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "(Yang et al., 2015) categorize the errors made by their VQA model into four categories \u2013 model focuses attention on incorrect regions, model focuses attention on appropriate regions but predicts incorrect answers, predicted answers are different from labels but might be acceptable, labels are wrong.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Complete image understanding: The absence of a large gap in performance of language-alone and language + vision VQA models (Antol et al., 2015)", "startOffset": 123, "endOffset": 143}, {"referenceID": 2, "context": "We present our behavioral analyses on the VQA dataset (Antol et al., 2015).", "startOffset": 54, "endOffset": 74}, {"referenceID": 2, "context": "CNN + LSTM based model without-attention (CNN+LSTM): We use the best performing model of (Antol et al., 2015) (code provided by (Lu et al.", "startOffset": 89, "endOffset": 109}, {"referenceID": 12, "context": ", 2015) (code provided by (Lu et al., 2015)), which achieves an accuracy of 54.", "startOffset": 26, "endOffset": 43}, {"referenceID": 13, "context": "leaderboard (as of the time of this submission) (Lu et al., 2016), which achieves an accuracy of 57.", "startOffset": 48, "endOffset": 65}, {"referenceID": 15, "context": "The k-NNs are computed in the space of average Word2Vec (Mikolov et al., 2013) vectors of answers.", "startOffset": 56, "endOffset": 78}, {"referenceID": 2, "context": "most \u2013 over the three major categories of questions \u2013 \u201cyes/no\u201d, \u201cnumber\u201d and \u201cother\u201d as categorized in (Antol et al., 2015).", "startOffset": 103, "endOffset": 123}], "year": 2017, "abstractText": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze the best performing models from two major classes of VQA models \u2013 with-attention and without-attention and show the similarities and differences in the behavior of these models. Our behavior analysis reveals that despite recent progress, today\u2019s VQA models are \u201cmyopic\u201d (tend to fail on sufficiently novel instances), often \u201cjump to conclusions\u201d (converge on a predicted answer after \u2018listening\u2019 to just half the question), and are \u201cstubborn\u201d (do not change their answers across images).", "creator": "LaTeX with hyperref package"}}}