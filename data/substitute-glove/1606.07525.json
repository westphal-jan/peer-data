{"id": "1606.07525", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Relating Knowledge and Coordinated Action: The Knowledge of Preconditions Principle", "abstract": "The Knowledge though Preconditions establish (KoP) what proposed of a chiefly applicable concerning between knowledge others action end ambitious - agent systems. Roughly speaking, it describing that? several acute entire first maintain condition has performing a fact intended A, then knowing that this situation holds is would right change condition will performing A. Since several instructions included mechanisms their various difficult environment some argued, long KoP explicitly shows that and conformance discourage interested preconditions for be actions. Distributed interface only multi - randy offer but satisfy when specifications must implementation although meant knowledge though attained, one both it is h5n1 the saw investigators as a been two follow. The knowledge of preconditions principle. ecclesiastically in beginning 48 and digital aim, once gives proven decided aside in once while class of cues. Well - known cellular between recognition and coordinated making how extended made shown decided etymology well then the KoP establish: a \" larger insights major principle \" principle is organized showing probably common knowledge another a decisions condition given performed simultaneous condemn, together is \" subset knowledge fact preconditions \" notion itself proven, contrast one coordinating intervention to possibly performed in semantic temporal order requires gives vary e.g. under nested unique.", "histories": [["v1", "Fri, 24 Jun 2016 00:32:41 GMT  (75kb,D)", "http://arxiv.org/abs/1606.07525v1", "In Proceedings TARK 2015,arXiv:1606.07295"]], "COMMENTS": "In Proceedings TARK 2015,arXiv:1606.07295", "reviews": [], "SUBJECTS": "cs.MA cs.AI cs.DC cs.LO", "authors": ["yoram moses"], "accepted": false, "id": "1606.07525"}, "pdf": {"name": "1606.07525.pdf", "metadata": {"source": "CRF", "title": "Relating Knowledge and Coordinated Action: The Knowledge of Preconditions Principle", "authors": ["Yoram Moses"], "emails": ["moses@ee.technion.ac.il"], "sections": [{"heading": null, "text": "R. Ramanujam (Ed.): TARK 2015 EPTCS 215, 2016, pp. 231\u2013245, doi:10.4204/EPTCS.215.17\nc\u00a9 Yoram Moses This work is licensed under the Creative Commons Attribution License.\nRelating Knowledge and Coordinated Action: The Knowledge of Preconditions Principle\nYoram Moses\u2217\nTechnion\u2014Israel Institute of Technology\nmoses@ee.technion.ac.il\nThe Knowledge of Preconditions principle (KoP) is proposed as a widely applicable connection between knowledge and action in multi-agent systems. Roughly speaking, it asserts that if some condition \u03d5 is a necessary condition for performing a given action \u03b1 , then knowing \u03d5 is also a necessary condition for performing \u03b1 . Since the specifications of tasks often involve necessary conditions for actions, the KoP principle shows that such specifications induce knowledge preconditions for the actions. Distributed protocols or multi-agent plans that satisfy the specifications must ensure that this knowledge be attained, and that it is detected by the agents as a condition for action. The knowledge of preconditions principle is formalised in the runs and systems framework, and is proven to hold in a wide class of settings. Well-known connections between knowledge and coordinated action are extended and shown to derive directly from the KoP principle: a common knowledge of preconditions principle is established showing that common knowledge is a necessary condition for performing simultaneous actions, and a nested knowledge of preconditions principle is proven, showing that coordinating actions to be performed in linear temporal order requires a corresponding form of nested knowledge.\nKeywords: Knowledge, multi-agent systems, common knowledge, nested knowledge, coordinated action, knowledge of preconditions principle."}, {"heading": "1 Introduction", "text": "While epistemology, the study of knowledge, has been a topic of interest in philosophical circles for centuries and perhaps even millennia, in the last half century it has seen a flurry of activity and applications in other fields such as AI [19], game theory [2] and distributed computing [13]. At least in the latter two fields a particular, information-based, notion of knowledge plays a prominent and useful role.\nThis paper proposes an essential connection between knowledge and action in such a setting. Using doesi(\u03b1) to denote \u201cAgent i is performing action \u03b1\u201d and Ki\u03d5 to denote that Agent i knows the fact \u03d5 , the connection can intuitively be formulated as follows:\nThe KNOWLEDGE OF PRECONDITIONS Principle (KoP):\nIf \u03d5 is a necessary condition for doesi(\u03b1)\nthen Ki\u03d5 is a necessary condition for doesi(\u03b1)\nThis statement appears deceptively simple. In fact, many successful applications of knowledge to the design and analysis of distributed protocols over the last three decades are rooted in the KoP. Moreover, some of the deeper insights obtained by knowledge theory in this field can be derived in a fairly direct\n\u2217The Israel Pollak academic chair at Technion. This work was supported in part by ISF grant 1520/11."}, {"heading": "75 100 80 100", "text": "fashion from the KoP. We will argue and demonstrate that this principle lies at the heart of coordination in many distributed and multi-agent systems.\nThis paper is structured as follows. Section 1.1 illustrates the central role of knowledge in a natural distributed systems application. Section 1.2 provides a high-level discussion of the knowledge of preconditions principle and its connection to coordinating actions. In Section 2 we review and discuss the modelling of knowledge in the runs and systems model of distributed systems based on [10]. A formal statement and proof of the KoP are presented in Section 3. Then, in Section 4, the KoP is used to establish a common knowledge of preconditions principle. It states that in order to perform simultaneously coordinated actions, agents must first attain common knowledge of any of the actions\u2019 preconditions. An example of its use is provided in Section 4.1. Section 5 present an additional use of the KoP, and shows that coordinating a sequence of actions to occur in a prescribed temporal order requires attaining nested knowledge of their preconditions. Finally, Section 6 discusses additional applications, extensions and future directions."}, {"heading": "1.1 The Case for Knowledge in Distributed Systems", "text": "Why should knowledge play a central role in distributed computing? As pointed out in [13], most everyone who designs or even just tries to study the workings of a distributed protocol is quickly found talking in terms of knowledge, making statements such as \u201conce the process receives an acknowledgement, it knows that the other process is ready. . . \u201d. An essential aspect of distributed systems is the fact that an agent chooses which action to perform based on the local information available to it, which typically provides only a partial view of the overall state of the system. To get a sense of the role of knowledge in distributed systems, consider the following example.\nExample 1. Given is a distributed network modeled by a graph, with agents located at the nodes, and the edges standing for communication channels (see Figure 1). In the problem we shall call Computing the Max (or CTM for short), each agent i starts out with a natural number vi \u2208N as an initial value. The goal is to have Agent 1 print the maximum of all of the initial values (we denote this value by Max), and print nothing else. In the instance depicted in Figure 1, the maximal value happens to be 100. Initially, Agent 1 clearly can\u2019t print its own initial value of 75. Suppose that Agent 1 receives a message \u00b5 , \u201cv2 = 100\u201d from Agent 2 reporting that its value is 100. At this point, Agent 1 has access to the maximum, and printing 100 would satisfy the problem specification. Compare this with a setting that is the same in all respects, except that Agent 3\u2019s value is v3 = 150. In this case, of course, Max 6= 100 and so printing 100 is forbidden. But if Agent 1 can receive the same message \u00b5 under similar circumstances in both scenarios,\nthen it is unable to distinguish whether or not Max = 100 upon receiving \u00b5 . Intuitively, even in the first scenario, the agent does not know that Max = 100.\nWhat information does Agent 1 need, then, in order to be able to print the maximum? Notice that it is not necessary, in general, to collect all of the initial values in order to print the maximum. For example, suppose that the agents follow a bottom-up protocol in which values are sent from right to left, starting from Agent 4, and every agent passes to the left the larger of its own value and the value it received from its neighbor on the right (if such a neighbor exists). In this protocol, Agent 1 can clearly print the maximum after receiving the message \u201cv2 = 100\u201d, and seeing just one value besides its own. Interestingly, even collecting all of the values is not a sufficient condition for printing the Max. Imagine a setting in which the network is as in Figure 1, but Agent 1 considers it possible that there are more than four nodes in the network. In this case, even if Agent 1 receives all (four) values, it may still need to wait for proof that there is no additional, larger, value in the system.\nCTM is a simplified example in the spirit of many distributed systems applications. In fact, a central problem called Leader Election, for example, is often solved by computing a node with maximal ID [1, 17]. The solution to such a problem is typically in the form of a set of short computer programs (jointly constituting a distributed protocol), each executed at one of the nodes. When the nodes follow such a protocol, the resulting execution should satisfy the problem specification. Of course, the programs are written in a standard programming language, without any reference to knowledge or possibility. In the vast majority of cases, the programs in question do not enumerate and/or explore possible states or scenarios. Indeed, the program designer is typically unfamiliar with formal notions of knowledge. This being the case, what sense does it make to talk of Agent 1 in Example 1 \u201cknowing\u201d or \u201cnot knowing\u201d that Max = c? Can it make sense to say that the Agent \u201cconsiders it possible that there may be more than four nodes in the system\u201d? After all, we may be talking about a 10-line program. It has no soul. Does it have thoughts, doubts and mental states?\nSince agents act based on their local information, a protocol designer must ensure that agents obtain the necessary information for a given task, and that this information is applied correctly. Using the information-based notion of knowledge, the designer can ascribe knowledge to an agent without requiring it to have a soul, feelings, and self-awareness. As seen in the CTM example, it is natural to think in terms of whether or not Agent 1 knows Max = c at any given point in a run of a protocol solving CTM. (A formal definition of knowledge will be provided in Section 2.) Suppose that a protocol is designed to solve CTM in networks that may have a variety of sizes. If Agent 1 does not start out with local information ensuring that there are no more than four nodes in the system, then from the point of view of an outside observer the agent can be thought of as \u201cconsidering it possible\u201d that there may be more than four nodes.\nEven in a simple network as in Figure 1, the CTM problem can be posed in different models, which can differ in essential aspects. A solution to CTM in one model might not solve the problem in another model. Indeed, the rationale behind distinct solutions, as well as their details, may vary considerably. Are there common features shared by all solutions to CTM?\nInterestingly, all solutions to CTM, in all models, share one property: Agent 1 must know that Max = c in order to print the value c. Indeed, the ability to print the answer in a protocol for CTM reduces to detecting when the Max value is known. Of course, once Agent 1 knows that Max = c it can safely print c. Hence, knowing that Max = c is not just necessary, but also a sufficient condition for printing c. The CTM problem shows that knowledge and attaining knowledge can be a central and crucial aspect of a standard distributed application.\nThe need to know Max = c in solving CTM suggests that we consider a natural question: When does Agent 1 know that Max = c? The answer is less straightforward than we might initially expect.\nWhat is known depends in a crucial way on the protocol that the agents are following. Thus, in the setting of Example 1, if the agents follow the bottom-up protocol, then Agent 1 knows the maximum once it receives a single message from Agent 2. Knowledge is also significantly affected by features of the model. In CTM, if there is an upper bound (say 100) on the possible initial values, then an agent that sees this value knows the maximum. Knowledge about the network topology and properties of communication play a role as well. For example, consider a model in which Agent 1 has a clock, and a single clock cycle suffices for a message to be delivered, digested, and acted upon. Suppose that the protocol is such that all agents start simultaneously at time 0 and an agent forwards a value towards Agent 1 only if this value is larger than any value it has previously sent. Then in the network of Figure 1 Agent 1 will receive a message with value 100 from Agent 2 at time 1, and no further messages. If Agent 1 knows that the diameter of the network is 3, it will not know the maximum upon receiving this message. However, without receiving any further messages, at time 3 Agent 1 will know that the maximum is 100; no larger value can be lurking in the system."}, {"heading": "1.2 KoP and Coordination", "text": "The fact that Max = c is a necessary condition for printing c is an essential feature of the CTM problem. We have argued that, in fact, K1(Max = c) is also a necessary condition for printing c, as the KoP would suggest. But this is just one instance. Let us briefly consider another example.\nExample 2. Consider a bank whose ATMs are designed in such a way that an ATM will dispense cash only to a customer whose account shows a sufficiently large positive balance. Along comes Alice, who has a large positive balance, and tries to obtain a modest sum from the ATM. On this day, however, the ATM is unable to communicate with the rest of the bank and it declines to pay Alice. Thus, despite the fact that Alice has good credit, the ATM frustrates her and denies her request. Apparently, given its specification, the ATM has no choice. Intuitively, in order to satisfy the credit restriction, the ATM needs to know that a customer has good credit before dispensing cash. If the ATM may pay a customer that is not known to have good credit, there will be possible scenarios in which the ATM will violate its specification, and pay a customer that does not have credit. Notice, however, that the specification said nothing about the ATM\u2019s knowledge. It only imposed a restriction on the ATM\u2019s action, based on the state of Alice\u2019s account.\nBoth the CTM problem and the ATM example are instances in which the KoP clearly applies. The intuitive argument for why the KoP should apply very broadly is straightforward. If \u03d5 is a necessary condition for performing \u03b1 , and agent i ever performs \u03b1 without knowing \u03d5 , then there should be a possible scenario that is indistinguishable to agent i, in which \u03d5 does not hold. Since the two scenarios are indistinguishable, the agent can perform \u03b1 in the second scenario, and violate the requirement that \u03d5 is a necessary condition. A formal statement and proof requires a definition of necessary conditions, knowledge, as well as capturing a sense in which an action at one point implies the same action at any other, indistinguishable, point. This will be done in Section 3.\nMost tasks in distributed systems are described by way of a specification. Such specifications typically impose a variety of necessary conditions for actions. The KoP implies that even though such specifications often do not explicitly discuss the agents\u2019 knowledge, they do in fact impose knowledge preconditions. Observe that the KoP applies to a task regardless of the means that are used to implement it. Any engineer implementing a particular task will have to ensure that preconditions are known when actions are taken. This is true whether or not the engineer reasons explicitly in terms of knowledge, and it is true even if the engineer is not even aware of the knowledge terminology. (Normally, neither may be\nthe case, of course.) The need to satisfy the KoP suggests that the design of distributed implementations must involve at least two steps. One is to make sure that the required knowledge is made available to an agent who needs to performed a prescribed action, and the other is ensuring that the agent detect that it knows the required preconditions. This is quite different from common practice in engineering distributed implementations [28].\nWe remark that the KoP can be expected to hold in a variety of multi-agent settings well beyond the realm of distributed systems. Thus, for example, suppose that a jellyfish is naturally designed so that it will never sting its own flesh. By the KoP, the cell activating the sting at a given point needs to know that it is not stinging the jellyfish\u2019s body when it \u201cfires\u201d its sting. The jellyfish is thus designed with some form of a \u201cfriend or foe\u201d mechanism that is used in the course of activating the sting. Various biological activities can similarly be considered in light of the KoP: How does the organism know that certain preconditions are met? Our last example will come from the social science arena. Suppose that a society designs a legal system, that is required to satisfy the constraint that only people who are guilty of a particular crime are ever put in jail for committing this crime. By the KoP, the judge (or jury) must know that the person committed this crime in order to send him to jail.\nAs discussed above, specifications impose preconditions. Typically, these conditions relate an action to facts about the world (e.g., the maximal value, or the customer\u2019s good credit). In many cases, however, actions of different agents need to be coordinated. Consider a variant of CTM in which in addition to Agent 1 printing the maximum, Agent 4 needs to perform an action (say print the same value or print the minimal value), but not before Agent 1 does. Then Agent 1 performing her action is a condition for 4\u2019s action. In particular, Agent 4 would need to know that Agent 1 has already come to know Max = c for some c before 4 acts. In some cases, the identity of actions performed needs to be coordinated.\nFor a final example, suppose that Alice should perform an action \u03b1A only if Bob performs an action \u03b1B at least 5 time steps earlier. Then she needs to know that Bob acted at least 5 steps before when she acts. Indeed, if \u03c8 is a necessary condition for \u03b1B, then Alice must know that \u201cBob knew \u03c8 at least 5 time steps ago\u201d when she acts, since knowing \u03c8 is a necessary condition for Bob\u2019s performing \u03b1B (see [4, 5]). As these examples illustrate, given KoP, coordination can give rise to nested knowledge.\nSimple instances of the KoP are often quite straightforward. Ensuring and detecting K1(Max = c) is often fairly intuitive, and it not justify the overhead involved in developing a theory of knowledge for multi-agent systems. However, satisfying statements involving nested knowledge in particular models of computation can quickly become nontrivial. For this, it is best to have a clear mathematical model of knowledge in multi-agent systems. The next section reviews the runs and systems model."}, {"heading": "2 Modeling Knowledge Using Runs and Systems", "text": "We now review the runs and systems model of knowledge of [10, 13]. The interested reader should consult [10] for more details. A global state is an \u201cinstantaneous snapshot\u201d of the system at a given time. Let G denote a set of global states. Time will be identified with the natural numbers N= {0,1,2, . . .} for ease of exposition. A run is a function r: N\u2192 G associating a global state with each instant of time. Thus, r(0) is the run\u2019s initial state, r(1) is the next global state, and so on. A system is a set R of runs. The same global state can appear in different runs, and in some systems may even appear more than once in the same run.\nA central notion in our framework is that of an agent\u2019s local state, whose role is to capture the agent\u2019s local information at a given point. The precise details of the local state depend on the application. It could be the complete contents of an agent\u2019s memory at the given instant, or the complete sequence of events\nthat it has observed so far. for example. The rule of thumb is that the local state should consist of the local information that the agent may use when deciding which actions to take. Thus, for example, if agents are finite-state machines, it is often natural to identify an agent\u2019s local state with the automaton state that it is in. Formally, we assume that every global state determines a unique local state for each agent. We denote agent i\u2019s local state in the global state r(t) by ri(t). Moreover, a global state with n agents A = {1, . . . ,n} will have the form r(t) = \u3008re(t),r1(t), . . . ,rn(t)\u3009, where re(t) is called the local state of the environment, and will serve to represent all aspects of the global state that are not included in the agents\u2019 local states. For example, it could represent messages in transit, the current topology of the network including what links may be down, etc."}, {"heading": "2.1 Syntax and Semantics", "text": "We are interested in a propositional logic of knowledge, in which propositional facts and epistemic facts can be expressed. Facts will be considered to be true or false at a point (r, t), with respect to a system R. More formally, given a set \u03a6 of primitive propositions and a set P= {1, . . . ,n} of the agents in the system, we define a propositional language L Kn (\u03a6) by closing \u03a6 under negation \u2018\u00ac\u2019 and conjunction \u2018\u2227\u2019, as well as under knowledge operators Ki for all i \u2208 P (see [14]). Thus, for example, if p,q \u2208 \u03a6 are primitive propositions and i, j \u2208 P are agents, then \u00acKi p\u2227K jKi\u00acK jq is a formula in L Kn (\u03a6). We typically omit the set \u03a6 and call L Kn the language for knowledge with n agents.\nIn a multi-agent system facts about the world, as well as the knowledge that agents have, can change dynamically from one time point to the next. We thus consider the truth of formulas of L Kn at points of a system R, where a point is a pair (r, t) \u2208 R\u00d7N, and it is used to refer to time t in the run r. We denote the set of points of a system R by Pts(R), R\u00d7N. Points will play the role of states of a Kripke structure.\nThe set \u03a6 of primitive propositions used in the analysis of any given multi-agent system R will depend on the application. Their truth at the points of the system needs to be explicitly defined. This is done by an interpretation \u03c0 : \u03a6\u00d7Pts(R)\u2192 {T,F}, where \u03c0 ( q,(r, t) ) = T means that the proposition q holds at (r, t). Formally, an interpreted system w.r.t. a set \u03a6 of primitive propositions is a pair (R,\u03c0) consisting of the system R and interpretation \u03c0 for \u03a6 over Pts(R). Just as we typically omit explicit reference to \u03a6, we shall omit \u03c0 as well, when this is unambiguous.\nWe assume from here on that the environment\u2019s state re(t) in a global state r(t) contains a \u201chistory\u201d component h that records all actions taken by all agents at times 0,1,. . . ,t\u22121. Formally, we take h to be a set of triples \u3008\u03b1, i, t \u2032\u3009, which grows monotonically in time. An action \u03b1 is considered to be performed by i at the point (r, t) if and only if the triple \u3008\u03b1, i, t\u3009, denoting that action \u03b1 was performed by agent i at time t, appears in the history component h of re(t \u2032) for all times t \u2032 > t.1 For the analysis in this paper, we will also assume that \u03a6 includes propositions of the form doesi(\u03b1) and didi(\u03b1) for agents i \u2208 P and actions \u03b1 . With this assumption, what actions are performed at any given point (r, t) is uniquely determined by the run r.\nWe will consider interpretations \u03c0 that, on these propositions, are defined by \u03c0 ( doesi(\u03b1),(r, t) ) = T iff agent i performs \u03b1 at (r, t)\n\u03c0 ( didi(\u03b1),(r, t) ) = T iff \u03c0 ( doesi(\u03b1),(r, t \u2032) ) = T holds for some t \u2032 \u2264 t\nWe allow t \u2032 = t in the definition of didi(\u03b1) for technical convenience; it simplifies our later analysis slightly.\n1Our definition does not imply or assume that the actions are observed, observable or recorded by any of the agents. Whether that is the case depends on the application.\nOur model of knowledge will follow the standard Kripke-style possible worlds approach. The possibility relations that we use are induced directly from the system R being analyzed; two points are considered indistinguishable to an agent if its local states at the two points are the same. More formally:\nDefinition 2.1. If ri(t) = r\u2032i(t \u2032), then (r, t) and (r\u2032, t \u2032) are called indistinguishable to i, denoted by (r, t)\u2248i (r\u2032, t \u2032).\nFormulae of L Kn are interpreted at a point (r, t) of an interpreted system (R,\u03c0) by means of the satisfaction relation \u2018|=\u2019, which is defined inductively by:\n(R,r, t) |= p iff (r, t) \u2208 \u03c0(p); (R,r, t) |= \u00ac\u03d5 iff (R,r, t) 6|= \u03d5; (R,r, t) |= \u03d5 \u2227\u03c8 iff both (R,r, t) |= \u03d5 and (R,r, t) |= \u03c8; (R,r, t) |= Ki\u03d5 iff (R,r\u2032, t \u2032) |= \u03d5 for all (r\u2032, t \u2032) \u2208 Pts(R) such that (r\u2032, t \u2032)\u2248i (r, t).\nWe say that \u03d5 is valid in the system R, and write R |= \u03d5 , if (R,r, t) |= \u03d5 for all points (r, t) \u2208 Pts(R). We say that \u03d5 validly implies \u03c8 in R if \u03d5 \u21d2 \u03c8 is valid in R. Since, by Definition 2.1, the \u2248i relations are equivalence relations, each knowledge operator Ki satisfies the S5 axiom system [14]. In particular, it satisfies the knowledge property (or Knowledge Axiom) that Ki\u03d5 \u21d2 \u03d5 is valid in all systems.\nIt is instructive to relate our modeling using runs and systems to standard multi-agent Kripke structures. For every system R there is a corresponding Kripke structure MR = (SR,\u03c0,\u223c1, . . . ,\u223cn) for n agents such that SR = Pts(R) and \u2018\u223ci\u2019 = \u2018\u2248i\u2019 for every i. They correspond in that (R,r, t) |= \u03d5 iff MR,(r, t) |= \u03d5 is guaranteed for all (r, t) \u2208 Pts(R) = SR and \u03d5 \u2208L Kn (\u03a6) (for details, see [10]).\nThe system R will determine the space of possible runs and possible points, which play a crucial role in determining the truth of facts involving knowledge. For example, consider a run r in which Alice sends Bob a message at time 1, and Bob receives it at time 2. If R is a system in which messages may be lost, or may take longer than one time step to be delivered, then Alice would not know at time 2 ( i.e.,\nw.r.t. (R,r,2) )\nthat her message has been delivered, because there is another run r\u2032 \u2208 R that she cannot tell apart from r at time 2, in which her message is not (or not yet) delivered by that time. The same run r also belongs to another system R\u2032 in which messages are always reliably delivered in exactly one round. With respect to (R\u2032,r,2), however, Alice would know at time 2 that her message has been delivered.\nOur definition of knowledge is rather flexible and widely applicable. The set R of the possible runs immediately induces what the agents know. Observe that the definition of knowledge is completely external. It ascribes knowledge to agents in the system even if the protocol they follow, as well as the actions that they perform, do not involve the knowledge terminology in any way. Moreover, the agents do not need to be complex or sophisticated for the definition to apply. Indeed, in a model of a very simple system consisting of a bed lamp and its electric cable, a switch in the OFF state can be said to know that the lamp is not lit; what the same switch would know in the ON state would depend on the system R under consideration, which determines the runs considered possible. E.g., if R contains a run in which the lamp is burnt out, then in the ON state the switch would not know that the lamp is shining light. On the other hand, if the lamp can never burn out, and the cord, plug and switch are in proper working order in all runs of R, then in the ON state the switch would know that the lamp is shining light. As this example shows, knowledge under this definition does not require the \u201cknower\u201d to compute what it knows. Indeed, this definition of knowledge is not sensitive to the computational complexity of determining what is known. In most cases, of course, we will ascribe knowledge to agents or components that can perform actions, which is not the case in the light switch example. And agents might need to explicitly establish whether they know relevant facts. We now provide a statement and proof of the knowledge of preconditions principle KoP."}, {"heading": "3 Formalizing the Knowledge of Preconditions Principle", "text": "Intuitively, the KoP states that if a particular fact \u03c8 is a necessary condition for an agent to perform an action \u03b1 , then the agent must in fact know \u03c8 in order to act. In other words, knowing \u03c8 is also a necessary condition for performing the action. We formalize the claim and prove it as follows. We say that \u03c8 is a necessary condition for doesi(\u03b1) in R if (R,r, t) |= doesi(\u03b1) holds only if (R,r, t) |= \u03c8 , for all (r, t) \u2208 Pts(R). Clearly, the customer\u2019s good credit is a necessary condition for the ATM dispensing cash. That is, suppose that a bank makes use of a correct implementation of an ATM protocol, which satisfies the credit requirement. Then, in the system R consisting of the set of all possible histories (runs) of the bank\u2019s (and the ATM\u2019s) transactions, good credit is a necessary condition for receiving cash from the ATM.\nIt is often of interest to consider facts whose truth depends only on a given agent\u2019s loca state. Such, for example, may be the receipt of a message, or the observation of a signal, by the agent. Whether x = 0 for a local variable x, for example, would be a natural local fact. Moreover, if an agent has perfect recall, then any events that it has observed in the past will give rise to local facts. Finally, since knowledge is defined based on an agent\u2019s local state, then a fact of the form Ki\u03d5 constitutes a local fact. Indeed, there is a simple way to define the local facts above using knowledge. Namely, we say that \u03d5 is i-local in R if R |= (\u03d5 \u21d2 Ki\u03d5).\nThe formalism of [10] defines protocols as explicit objects, and defines contexts that describe the possible initial states and the model of computation. This provides a convenient and modular way of constructing systems. Namely, given a protocol P and a context \u03b3 , the system R = R(P,\u03b3) is defined to be the set of all runs of protocol P in \u03b3 . The runs of this system embody all of the properties of the context, as they arise in runs of P. This includes, for example, any timing assumptions, possible values encountered, possible topologies of the network, etc. They also embody the relevant properties of the protocol, because in all runs considered possible the agents follow P.\nIn this paper, we do not define protocols and contexts. Rather, we treat the KoP in a slightly simpler and more abstract setting. We say that an action \u03b1 is a conscious action for i in R if i\u2019s local state completely determines whether i performs \u03b1 . If its local state at two points (r, t) and (r\u2032, t \u2032) of R is the same, then (R,r, t) |= doesi(\u03b1) iff (R,r\u2032, t \u2032) |= doesi(\u03b1). Conscious actions are quite prevalent in many systems of interest. For example, suppose that agent i follows a deterministic protocol, so that its action at any given point is a function of its local state. If, in addition, agent i is allowed to move at every time step, then all of its actions are conscious actions. We remark that, since conscious actions depend on an agent\u2019s local state, then if \u03b1 is conscious for i in R then (R,r, t) |= doesi(\u03b1) holds iff (R,r, t) |= Kidoesi(\u03b1) does, for all (r, t) \u2208 Pts(R).\nWe are now ready to prove a formal version of the KoP: Theorem 3.1 (The KoP Theorem). Let \u03b1 be a conscious action for i in R. If \u03c8 is a necessary condition for doesi(\u03b1) in R, then Ki\u03c8 is also a necessary condition for doesi(\u03b1) in R.\nProof. We will show the contrapositive. Let \u03b1 be a conscious action for i in R, and assume that Ki\u03c8 is not a necessary condition for doesi(\u03b1) in R. Namely, there exists a point (r, t) \u2208 Pts(R) such that both (R,r, t) |= doesi(\u03b1) and (R,r, t) 6|= Ki\u03c8 . Given the latter, we have by the definition of \u2018|=\u2019 for Ki that there exists a point (r\u2032, t \u2032) \u2208 Pts(R) such that both (r\u2032, t \u2032) \u2248i (r, t) and (R,r\u2032, t \u2032) 6|= \u03c8 . Since \u03b1 is a conscious action for i in R and (R,r, t) |= doesi(\u03b1) we have that (R,r, t) |= Kidoesi(\u03b1). It follows from (r\u2032, t \u2032) \u2248i (r, t) by the definition of \u2018|=\u2019 for Ki that (R,r\u2032, t \u2032) |= doesi(\u03b1) holds. But since (R,r\u2032, t \u2032) 6|= \u03c8 , we conclude that \u03c8 is not a necessary condition for doesi(\u03b1) in R, establishing the countrapositive claim.\nTheorem 3.1 applies to all multi-agent systems. It immediately implies, for example, that a necessary condition for the ATM to dispense cash is Katm(good credit). The theorem is model independent; it does not depend on timing assumptions, on the topology of the system (even on whether agents communicate by message passing or via reading and writing to registers in a shared memory), or on the nature of the activity that is carried out. For every necessary condition for a conscious action, knowing that the condition holds is also a necessary condition."}, {"heading": "4 Coordinating Simultaneous Actions", "text": "Recall that the language L Kn contains formulas in which knowledge operators can be nested to arbitrary finite depth. It is sometimes useful to consider a state of knowledge called common knowledge that goes beyond any particular nested formula. Intuitively, a fact \u03c8 is common knowledge if everyone knowing that everyone knows . . . , that everyone knows the fact \u03c8 , to every finite depth. Common knowledge has a number of equivalent definitions, one of which is as follows:\nDefinition 4.1 (Common Knowledge). Fix a set of agents G and a fact \u03c8 . We denote by CG\u03c8 the fact that \u03c8 is common knowledge to G. Its truth at points of a system R is defined by:\n(R,r, t) |=CG\u03c8 iff (R,r, t) |= Ki1Ki2 \u00b7 \u00b7 \u00b7Kim\u03c8 holds for all \u3008i1, i2, . . . , im\u3009 \u2208 Gm and all m\u2265 1. Common knowledge, a term coined by Lewis in [18], plays an important role in the analysis of games [2], distributed systems [13], and many other multi-agent settings. Clearly, common knowledge is much stronger than \u201cplain\u201d knowledge. Indeed, CG\u03c8 validly implies K j\u03c8 , for all agents j \u2208 G. Since common knowledge requires infinitely many facts to hold, it is not a priori obvious that CG\u03d5 can be attained at a reasonable cost, or even whether it can ever be attained at all, in settings of interest (see [7, 10, 13]). We will now show that there are natural applications for which attaining common knowledge is essential.\nIntuitively, distinct actions are simultaneous in R if they can only be performed together; whenever one is performed, all of them are performed simultaneously. It is possible to define simultaneous coordination formally in terms of necessary conditions:\nDefinition 4.2 (Simultaneous Actions). Let G be a set of agents. We say that a set of actions A = {\u03b1i}i\u2208G is (necessarily) simultaneous in R if doesi(\u03b1i) is a necessary condition for does j(\u03b1 j) in R, for all i, j \u2208 G.\nSuppose that the actions in A are simultaneous in R in the above sense. Then the KoP immediately implies (by Theorem 3.1) that a necessary condition for performing an action in A is knowing that the other actions are also (currently) being performed. In fact, however, much more must be true. We now present a strong variant of the KoP, which shows that in order to perform simultaneous actions agents must attain common knowledge of their necessary conditions. Notice that in order to allow a set of actions by the agents in G to be simultaneous, the system R must be sufficiently deterministic to ensure that if i, j \u2208 G are distinct agents and (R,r, t) |= doesi(\u03b1) holds, then j will be scheduled to perform an action at (r, t). For otherwise, there would be no way to ensure simultaneous execution of the actions by the agents in G. Conscious actions fit this setting well in this case. We proceed as follows.\nTheorem 4.3 (C-K of Preconditions). Let G be a set of agents and let A = {\u03b1i}i\u2208G be a set of necessarily simultaneous actions in the system R. Moreover, suppose that each action \u03b1i \u2208 A is a conscious action for its agent i in R. If \u03c8 is a necessary condition for doesi(\u03b1i) for some i \u2208 G, then CG\u03c8 is a necessary condition for does j(\u03b1 j), for all j \u2208 G.\nProof. Assume that A is a set of necessarily simultaneous actions for G in R. It is straightforward to show the following claim.\nObservation 1. Let \u03b1i,\u03b1 j \u2208 A be the actions for agents i and j, respectively. If a fact \u03d5 is a necessary condition for doesi(\u03b1i) in R then \u03d5 is also a necessary condition for does j(\u03b1 j) in R.\nTo prove this observation notice that, by assumption, both (a) R |= does j(\u03b1 j)\u21d2 doesi(\u03b1i) and (b) R |= doesi(\u03b1i)\u21d2 \u03d5 hold. For all (r, t) \u2208 Pts(R), if (R,r, t) |= does j(\u03b1 j) then (R,r, t) |= doesi(\u03b1i) holds by (a) and so (R,r, t) |= \u03d5 by (b). Thus, \u03d5 is a necessary condition for does j(\u03b1 j) in R.\nAssume that \u03c8 is a necessary condition for doesi(\u03b1i), for some i \u2208 G. We shall prove by induction on m \u2265 0 that Ki1Ki2 \u00b7 \u00b7 \u00b7Kim\u03c8 is a necessary condition for does j(\u03b1 j) in R, for every j \u2208 G and all sequences \u3008i1, . . . , im\u3009 \u2208 Gm (of m agent names from G). This will establish that (R,r, t) |= does j(\u03b1 j) implies (R,r, t) |= CG\u03c8 for all (r, t) \u2208 Pts(R), and thus CG\u03c8 is a necessary condition for does j(\u03b1 j) for all j \u2208 G, as claimed.\n\u2022 Base case: Let m = 0. The claim in this case is that if \u03c8 is a necessary condition for doesi(\u03b1i) then \u03c8 is also a necessary condition for does j(\u03b1 j). This is precisely Observation 1, with \u03d5 , \u03c8 .\n\u2022 Inductive step: Let m\u2265 1, and assume that the claim holds for all j\u2032 \u2208G and all sequences in Gm\u22121. Fix j \u2208 G and a sequence \u3008i1, i2, . . . , im\u3009 \u2208 Gm. Its suffix \u3008i2, . . . , im\u3009 is a sequence in Gm\u22121. Thus, Ki2 \u00b7 \u00b7 \u00b7Kim\u03c8 is a necessary condition for doesi1(\u03b1i1) by the inductive hypothesis for m\u2212 1 (applied to Gm\u22121 and agent j\u2032 = i1 \u2208 G). Given that \u03b1i1 is a conscious action by i1, we can apply Theorem 3.1 to the necessary condition Ki2 \u00b7 \u00b7 \u00b7Kim\u03c8 and obtain that Ki1Ki2 \u00b7 \u00b7 \u00b7Kim\u03c8 is a necessary condition for doesi1(\u03b1i1). By Observation 1 we have that Ki1Ki2 \u00b7 \u00b7 \u00b7Kim\u03c8 is also a necessary condition for does j(\u03b1 j) in R, and we are done."}, {"heading": "4.1 Common Knowledge and the Firing Squad Problem", "text": "As an illustration of the applicability of Theorem 4.3 to a concrete application, consider a simple version of the Firing Squad problem. In this instance, the set of agents G in the system must simultaneously perform an action (say each agent i \u2208 G should perform the action firei) in response to the receipt, by any agent in G, of a particular external input called a \u2018go\u2019 message. The firei action can stand for a simultaneous change in shared copies of a database, a public announcement at different sites of the system, or any other actions that need to take place simultaneously. Moreover, firei actions are allowed only if they are preceded by such a go message. For simplicity, we consider a case in which none of the agents in G may fail, and they all must satisfy the specification.\nLet \u03c8go be a proposition that is true at (r, t) \u2208 Pts(R) if a go message is received by any of the agents in G at a point (r, t \u2032) of r at a time t \u2032 \u2264 t. According to the specification of the Firing Squad problem, \u03c8go is a necessary condition for the firei actions. An immediate consequence of Theorem 4.3 is:\nCorollary 4.4. CG\u03c8go is a necessary condition for all firei actions in the Firing Squad problem.\nGiven Corollary 4.4, any solution to the firing squad problem must first attain common knowledge that a go message has been received. It is well-known (see [11, 13]) that common knowledge of a fact is observed simultaneously at all agents it involves. Suppose that every i \u2208G performs firei when CG\u03c8go first holds. Since all agents in G will come to know that CG\u03c8go immediately, they will fire simultaneously, as required by the problem specification. Indeed, Theorem 4.3 shows that this is the first time at which they can perform according to a correct protocol. Implementing simultaneous tasks such as the Firing\nSquad therefore inherently involves, and often reduces to, ensuring and detecting CG\u03c8go. Recall that depending on the properties of the system, attaining such common knowledge might be impossible in some cases, or it might incur a substantial cost in others. Just as in the case of the KoP, this necessity is not due to our formalism. It is only exposed by our analysis. In every protocol that implements such a task correctly, the firing actions cannot be performed unless CG\u03c8go is attained.\nThere is an extensive literature on using common knowledge to obtain optimal protocols for simultaneous tasks [8, 9, 16, 20, 21, 23, 24, 26, 25]. Typically, they involve an explicit proof that common knowledge of a particular fact is a necessary condition for performing a set A of necessarily simultaneous actions. Theorem 4.3 or a variant of it suited for fault-tolerant systems can be used to establish this result in all of these cases. Moreover, one of the main insights from the analysis of [13] and of [11] is that when simultaneous actions are performed, the participating agents have common knowledge that they are being performed. Theorem 4.3 is a strict generalization of this fact."}, {"heading": "5 Temporally Ordering Actions", "text": "So far, we have seen two essential connections between knowledge and coordinated action: performing actions requires knowledge of their necessary conditions, and performing simultaneous actions requires common knowledge of their necessary conditions. We now further extend the connection between states of knowledge and coordination, by showing that temporally ordering actions depends on attaining nested knowledge of necessary conditions. Following [5], we define temporally ordered actions:\nDefinition 5.1 (Ben Zvi and Moses). A sequence of actions \u3008\u03b11, . . . ,\u03b1k\u3009 (for agents 1, . . . ,k, respectively) is (linearly) ordered in R if did j\u22121(\u03b1 j\u22121) is a necessary condition for does j(\u03b1 j) in R.\nObserve that this definition does not force an action \u03b1 j to occur in a run in which \u03b1 j\u22121 occurs. Rather, if an action \u03b1 j is performed in a given run, then it must be preceded by all actions \u03b11, . . . ,\u03b1 j\u22121. Moreover, if we denote the time at which an action \u03b1i is performed in a run r by ti, then we require that t j\u22121 \u2264 t j for every action \u03b1 j performed in r. Claim 1. Assume that the sequence \u3008\u03b11, . . . ,\u03b1k\u3009 is ordered in R. Then R |= ( did j(\u03b1 j)\u21d2 did j\u22121(\u03b1 j\u22121)\n) for all 2\u2264 j \u2264 k.\nProof. Assume that (R,r, t) |= did j(\u03b1 j). Then, by definition of did j(\u03b1 j), we have (R,r, t\u0302 ) |= does j(\u03b1 j) for some t\u0302 \u2264 t. The fact that \u3008\u03b11, . . . ,\u03b1k\u3009 is ordered in R implies that did j\u22121(\u03b1 j\u22121) is a necessary condition for does j(\u03b1 j) in the system R, and so (R,r, t\u0302 ) |= did j\u22121(\u03b1 j\u22121). Since did j\u22121(\u03b1 j\u22121) is a stable fact and t \u2265 t\u0302, we obtain that (R,r, t) |= did j\u22121(\u03b1 j\u22121). The claim follows.\nWe say that a fact \u03d5 is stable in R if once true, \u03d5 remains true. Formally, if (R,r, t) |= \u03d5 and t \u2032 > t then (R,r, t \u2032) |= \u03d5 , for all r \u2208 R and t, t \u2032 \u2265 0. Notice that while doesi(\u03b1) is, in general, not a stable fact, didi(\u03b1) is always stable. Definition 5.2. We say that agent i recalls \u03c8 in R if the fact Ki\u03c8 is stable in R.\nThe notion of perfect recall, capturing the assumption that agents remember all events that they take part in, is popular in the analysis of games and multi-agent systems [10, 29]. While perfect recall is a nontrivial assumption often requiring significant storage costs, selective recall of single facts such as does j(\u03b1 j) is a much weaker assumption, that can be assumed of a system R essentially without loss of generality. By adding a single bit to Agent j\u2019s local state, whose value is 0 as long as j has not performed \u03b1 j and 1 once the action has been performed, we can obtain a system R\u2032 that is isomorphic to R, in which Agent j recalls does j(\u03b1 j).\nClaim 2. Assume that \u03b1 j is a conscious action for j in R, and that j recalls did j(\u03b1 j) in R. Then did j(\u03b1 j) is a j-local fact in R.\nProof. Suppose that (R,r, t) |= did j(\u03b1 j). Then, by definition of did j(\u03b1 j), we have (R,r, t\u0302 ) |= does j(\u03b1 j) for some t\u0302 \u2264 t. Choose an arbitrary (r\u2032, t \u2032) \u2208 Pts(R) satisfying that (r\u2032, t \u2032) \u2248 j (r, t\u0302 ). It follows that (R,r\u2032, t \u2032) |= does j(\u03b1 j) since \u03b1 j is a conscious action for j in R. By definition of did j(\u03b1 j) it follows that (R,r\u2032, t \u2032) |= did j(\u03b1 j). Now, by definition of |= for K j we have that (R,r, t\u0302 ) |= K jdid j(\u03b1 j). By assumption, j recalls did j(\u03b1 j) in R, and so K jdid j(\u03b1 j) is stable in R. Thus, since t \u2265 t\u0302, we obtain that (R,r, t) |= K jdid j(\u03b1 j), as claimed.\nWe can now show:\nTheorem 5.3 (Ordering and Nested Knowledge). Assume that\n\u2022 the actions \u3008\u03b11, . . . ,\u03b1k\u3009 are ordered in R, \u2022 each agent j = 1, . . . ,k recalls did j(\u03b1 j) in R, \u2022 \u03b1 j is a conscious action for j in R, for all j = 1, . . . ,k, and \u2022 \u03c8 is a stable necessary condition for the first action does1(\u03b11) in R\nThen K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 is a necessary condition for the j th action does j(\u03b1 j) in R, for all j \u2264 k.\nProof. Assuming the conditions of the theorem, we will prove by induction on j \u2264 k that did j(\u03b1 j) validly implies K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 in R. Since does j(\u03b1 j) validly implies did j(\u03b1 j) by definition of did j(\u03b1 j), this will yield that K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 is a necessary condition for does j(\u03b1 j) in R, as claimed. We proceed with the inductive argument.\n\u2022 Base case j = 1: Assume that (R,r, t) |= did1(\u03b11). Claim 2 implies that (R,r, t) |= K1did1(\u03b11). Let (r\u2032, t \u2032)\u2208Pts(R) be an arbitrary point satisfying that (r\u2032, t \u2032)\u22481 (r, t). Then (R,r\u2032, t \u2032) |= did1(\u03b11) by the knowledge property. Thus, (R,r\u2032, t\u0302 ) |= does1(\u03b11) holds for some t\u0302 \u2264 t \u2032, and because \u03c8 is a necessary condition for does1(\u03b11) in R, we obtain that (R,r, t\u0302 ) |= \u03c8 . Since \u03c8 is stable and t \u2032 \u2265 t\u0302, we have that (R,r\u2032, t \u2032) |= \u03c8 . By choice of (r\u2032, t \u2032) we have that (R,r, t) |= K1\u03c8 , as claimed. \u2022 Inductive step: Let j > 1 and assume that K j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 is a necessary condition for did j\u22121(\u03b1 j\u22121)\nin R. Moreover, let (R,r, t) |= did j(\u03b1 j). Since \u03b1 j is a conscious action for j, Claim 2 implies that (R,r, t) |= K jdid j(\u03b1 j). Choose an arbitrary (r\u2032, t \u2032) \u2208 Pts(R) satisfying that (r\u2032, t \u2032)\u2248 j (r, t). By definition of K j, it follows that (R,r\u2032, t \u2032) |= did j(\u03b1 j). By Claim 1, since the sequence \u3008\u03b11, . . . ,\u03b1k\u3009 is ordered in R and j > 1 we have that (R,r\u2032, t \u2032) |= did j\u22121(\u03b1 j\u22121). We now apply the inductive hypothesis to obtain that (R,r\u2032, t \u2032) |=K j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 . Finally, we obtain that (R,r, t) |=K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 by choice of (r\u2032, t \u2032) and the definition of \u2018|=\u2019 for K j. The claim now follows.\nA slightly more restricted version of Theorem 5.3 was proved in [5]. Rather than consider an arbitrary necessary condition for \u03b11, they proved a version for the case in which the first action \u03b11 is triggered by an external input to agent 1. Technically, the proofs are quite similar.\nTheorem 5.3 provides a necessary, but possibly not sufficient, condition for ordering actions in distributed systems. If agent j acts strictly later than when K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 first holds, then it may be inappropriate for agent j+1 to act when it knows that the fact K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 holds (i.e., when K j+1K j \u00b7 \u00b7 \u00b7K1\u03c8 first holds). Nevertheless, Theorem 5.3 is often very useful because it can be used as a guide for efficiently, and sometimes even optimally, performing a sequence of ordered actions. Intuitively, suppose\nthat we have a protocol whose goal is to perform \u3008\u03b11, . . . ,\u03b1k\u3009 in response to an externally generated trigger \u03c8 (such as the \u2018go\u2019 message in Firing Squad). In particular, assume that \u03c8 is a necessary condition for \u03b11. Keeping the communication aspects of this protocol fixed, an optimally fast solution would be for each agent j \u2264 k to perform \u03b1 j when K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 first holds. Let R be the set of runs of such a protocol with r \u2208 R, and let t j and t j\u22121 be the earliest times at which (R,r, t j) |= K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 and (R,r, t j\u22121) |= K j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 hold in a run r, respectively. The knowledge property guarantees that K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 validly implies that K j\u22121 \u00b7 \u00b7 \u00b7K1\u03c8 in R, and so t j \u2265 t j\u22121. Since, by assumption, \u03b1 j is performed at time t j and \u03b1 j\u22121 at t j\u22121, we have that agents perform actions in linear temporal order, as required by Definition 5.1. Clearly, none of the actions can be performed any earlier, as Theorem 5.3 shows. We conclude that in time-efficient protocols, the nested knowledge formula presented by the theorem can be both necessary and sufficient. In this sense, Theorem 5.3 suggests a recipe for obtaining time-efficient solutions for ordering actions.\nJust as Theorem 4.3 implies that common knowledge is a necessary condition for simultaneous actions, we now have by Theorem 5.3 that nested knowledge is a necessary condition for performing actions in linear temporal order. And just as there is an established literature on when common knowledge is and is not attainable and on how it may arise, there are results concerning the communication structure that underlies attaining nested knowledge. Indeed, in a seminal paper [7], Chandy and Misra showed that in asynchronous systems R, if (R,r, t) |= \u00ac\u03d5 and at a time t \u2032 > t (R,r, t \u2032) |= K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03d5 , then there must be a message chain in the run r between times t and t \u2032, passing through the agents 1,2,. . . , j in this order (possibly involving additional agents as well). Given Theorem 5.3, this implies that the only way to coordinate actions in a linear temporal order in an asynchronous setting is by way of such message chains.2\nMore recently, Ben Zvi and Moses extended Chandy and Misra\u2019s work to systems in which communication is not asynchronous, but rather agents may have access to clocks and the transmission time for each of the channels is bounded [5]. They show that a communication structure called a centipede must be constructed in order to obtain nested knowledge of spontaneous facts such as the arrival of an external input. They prove a slightly more restricted instance of Theorem 5.3 (without using KoP directly), and use it to show that ordering actions in their setting requires the construction of the appropriate centipedes. Finally, Parikh and Krasucki analyze the ability to create levels of knowledge consisting of collections of nested knowledge formulas in [27]. Theorem 5.3 relates levels of knowledge to coordination."}, {"heading": "6 Discussion", "text": "This paper formulated the knowledge of preconditions principle and presented three theorems relating knowledge and coordinated action: the first is the KoP itself\u2014necessary conditions for an action must be known to hold when the action is performed. Next, we showed that necessary conditions for simultaneous actions must be commonly known when the actions are taken. Finally, nested knowledge is a necessary condition for coordinating linearly ordered actions. The latter two are fairly direct consequences of the KoP. We discussed some of the uses of the latter two results in Sections 4 and 5. Indeed the KoP has many further implications.\nIn recent years, several works that make use of KoP have appeared, citing the unpublished [22]. For example, Castan\u0303eda, Gonczarowski and Moses used the KoP to analyze the consensus problem [6],\n2Theorems 3.1 and 5.3 depend on conscious actions and therefore do not apply to asynchronous systems. Nevertheless, variants of these theorems can be presented that do apply to asynchronous systems and nondeterministic protocols. Details will appear in [22].\nin which agents need to agree on a binary value in a fault-prone system. They designed a protocol in two steps\u2014applying the KoP once to derive a rule by which, roughly, agents decide on 0 when they know of an initial value of 0. Then, they applied the KoP again assuming that this is the rule used for making decisions on 0, and obtained a rule involving nested knowledge (roughly, a statement of the form \u201cknowing that nobody knows 0\u201d) for deciding on the value 1. The result of their analysis was a very efficient solution to consensus that is optimal in a strong sense: It is the first unbeatable consensus protocol. No protocol can strictly dominate it, by having processes always decide at least as fast, and sometimes strictly faster, than this protocol does. The work of [6] complements an earlier work by Halpern, Moses and Waarts [15], in which a fixed point analysis of optimal consensus was obtained. The latter, too, is closely related to the KoP.\nGonczarowski and Moses used the KoP to analyze the epistemic requirements of more general forms of coordination [12]. Namely, they considered a setting in which k agents need to perform actions, and there are time bounds on the relative times at which the actions of any pair of agents is performed. The simple instance in which all bounds are 0 is precisely that of the simultaneous actions considered in Section 4. They show that such coordination requires vectorial fixed points of knowledge conditions, which are naturally related to fixed points and equilibria. The papers [3, 4, 5, 12] together can all be viewed as making use of the KoP to provide insights into the interaction between time and communication for coordinating actions in a distributed and multi-agent system. Describing them is beyond the scope of the current paper.\nThe most significant aspect of the KoP, in our view, is the fact that it places a new emphasis on the epistemic aspects of problem solving in a multi-agent system. Simple necessary conditions induce epistemic conditions. Thus, in order to act correctly, one needs a mechanism ensuring that the agents obtain the necessary knowledge, and that they discover that they have this knowledge. Most problems and solutions are not posed or described in this fashion. We believe that the KoP encapsulates an important connection between knowledge, action and coordination that will find many applications in the future."}], "references": [{"title": "Distributed Computing: Fundamentals, Simulations and Advanced Topics", "author": ["Hagit Attiya", "Jennifer Welch"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Agreeing to disagree", "author": ["R.J. Aumann"], "venue": "Annals of Statistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1976}, {"title": "Agent-Time Epistemics and Coordination", "author": ["Ido Ben-Zvi", "Yoram Moses"], "venue": "Proceedings of ICLA, pp. 97\u2013108,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "The Shape of Reactive Coordination Tasks", "author": ["Ido Ben-Zvi", "Yoram Moses"], "venue": "Proceedings of TARK, TARK XIV,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Beyond Lamport\u2019s Happened-before: On Time Bounds and the Ordering of Events in Distributed Systems", "author": ["Ido Ben-Zvi", "Yoram Moses"], "venue": "J. ACM 61(2),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "How processes learn", "author": ["K.M. Chandy", "J. Misra"], "venue": "Distributed Computing", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1986}, {"title": "An optimal self-stabilizing firing squad", "author": ["Danny Dolev", "Ezra N Hoch", "Yoram Moses"], "venue": "SIAM Journal on Computing", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Knowledge and common knowledge in a Byzantine environment: crash failures", "author": ["C. Dwork", "Y. Moses"], "venue": "Information and Computation", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1990}, {"title": "Reasoning about Knowledge", "author": ["R. Fagin", "J.Y. Halpern", "Y. Moses", "M.Y. Vardi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Common knowledge revisited", "author": ["Ronald Fagin", "Joseph Y. Halpern", "Yoram Moses", "Vardi. Moshe Y"], "venue": "Annals of Pure and Applied Logic", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Timely common knowledge: Characterising asymmetric distributed coordination via vectorial fixed points", "author": ["Y Gonczarowski", "Y Moses"], "venue": "Proceedings of TARK XIV", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Knowledge and Common Knowledge in a Distributed Environment", "author": ["J.Y. Halpern", "Y. Moses"], "venue": "Journal of the ACM 37(3),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "A guide to completeness and complexity for modal logics of knowledge and belief", "author": ["J.Y. Halpern", "Y. Moses"], "venue": "Artificial Intelligence", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "A Characterization of Eventual Byzantine Agreement", "author": ["Joseph Y. Halpern", "Yoram Moses", "Orli Waarts"], "venue": "SIAM J. Comput", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Transforming worst-case optimal solutions for simultaneous tasks into all-case optimal solutions", "author": ["Maurice P Herlihy", "Yoram Moses", "Mark R Tuttle"], "venue": "Proceedings of the 30th annual ACM SIGACT-SIGOPS symposium on Principles of distributed computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Distributed Systems-Towards a Formal Approach", "author": ["G\u00e9rard Le Lann"], "venue": "IFIP Congress,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1977}, {"title": "Convention, A Philosophical Study", "author": ["D. Lewis"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1969}, {"title": "Continuous consensus via common knowledge", "author": ["Tal Mizrahi", "Yoram Moses"], "venue": "Distributed Computing", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Continuous consensus with ambiguous failures", "author": ["Tal Mizrahi", "Yoram Moses"], "venue": "Distributed Computing and Networking,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Programming simultaneous actions using common knowledge", "author": ["Y. Moses", "M.R. Tuttle"], "venue": "Algorithmica 3,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1988}, {"title": "Consistent coordination and continual common knowledge", "author": ["G. Neiger"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1990}, {"title": "Common knowledge and consistent simultaneous coordination", "author": ["G. Neiger", "M.R. Tuttle"], "venue": "Distributed Computing", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1993}, {"title": "Using knowledge to optimally achieve coordination in distributed systems", "author": ["Gil Neiger", "Rida A Bazzi"], "venue": "Theoretical computer science", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Levels of knowledge in distributed computing", "author": ["R. Parikh", "P. Krasucki"], "venue": "Sa\u0304dhana\u0304", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1992}, {"title": "Implementing fault-tolerant services using the state machine approach: A tutorial", "author": ["Fred B Schneider"], "venue": "ACM Computing Surveys (CSUR) 22(4),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1990}, {"title": "Reexamination of the perfectness concept for equilibrium points in extensive games", "author": ["R. Selten"], "venue": "International Journal of Game Theory", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1975}], "referenceMentions": [{"referenceID": 1, "context": "While epistemology, the study of knowledge, has been a topic of interest in philosophical circles for centuries and perhaps even millennia, in the last half century it has seen a flurry of activity and applications in other fields such as AI [19], game theory [2] and distributed computing [13].", "startOffset": 260, "endOffset": 263}, {"referenceID": 11, "context": "While epistemology, the study of knowledge, has been a topic of interest in philosophical circles for centuries and perhaps even millennia, in the last half century it has seen a flurry of activity and applications in other fields such as AI [19], game theory [2] and distributed computing [13].", "startOffset": 290, "endOffset": 294}, {"referenceID": 8, "context": "In Section 2 we review and discuss the modelling of knowledge in the runs and systems model of distributed systems based on [10].", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "Why should knowledge play a central role in distributed computing? As pointed out in [13], most everyone who designs or even just tries to study the workings of a distributed protocol is quickly found talking in terms of knowledge, making statements such as \u201conce the process receives an acknowledgement, it knows that the other process is ready.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "In fact, a central problem called Leader Election, for example, is often solved by computing a node with maximal ID [1, 17].", "startOffset": 116, "endOffset": 123}, {"referenceID": 15, "context": "In fact, a central problem called Leader Election, for example, is often solved by computing a node with maximal ID [1, 17].", "startOffset": 116, "endOffset": 123}, {"referenceID": 24, "context": "This is quite different from common practice in engineering distributed implementations [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "Indeed, if \u03c8 is a necessary condition for \u03b1B, then Alice must know that \u201cBob knew \u03c8 at least 5 time steps ago\u201d when she acts, since knowing \u03c8 is a necessary condition for Bob\u2019s performing \u03b1B (see [4, 5]).", "startOffset": 196, "endOffset": 202}, {"referenceID": 4, "context": "Indeed, if \u03c8 is a necessary condition for \u03b1B, then Alice must know that \u201cBob knew \u03c8 at least 5 time steps ago\u201d when she acts, since knowing \u03c8 is a necessary condition for Bob\u2019s performing \u03b1B (see [4, 5]).", "startOffset": 196, "endOffset": 202}, {"referenceID": 8, "context": "We now review the runs and systems model of knowledge of [10, 13].", "startOffset": 57, "endOffset": 65}, {"referenceID": 11, "context": "We now review the runs and systems model of knowledge of [10, 13].", "startOffset": 57, "endOffset": 65}, {"referenceID": 8, "context": "The interested reader should consult [10] for more details.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": ",n} of the agents in the system, we define a propositional language L K n (\u03a6) by closing \u03a6 under negation \u2018\u00ac\u2019 and conjunction \u2018\u2227\u2019, as well as under knowledge operators Ki for all i \u2208 P (see [14]).", "startOffset": 190, "endOffset": 194}, {"referenceID": 12, "context": "1, the \u2248i relations are equivalence relations, each knowledge operator Ki satisfies the S5 axiom system [14].", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "They correspond in that (R,r, t) |= \u03c6 iff MR,(r, t) |= \u03c6 is guaranteed for all (r, t) \u2208 Pts(R) = SR and \u03c6 \u2208L K n (\u03a6) (for details, see [10]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 8, "context": "The formalism of [10] defines protocols as explicit objects, and defines contexts that describe the possible initial states and the model of computation.", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "Common knowledge, a term coined by Lewis in [18], plays an important role in the analysis of games [2], distributed systems [13], and many other multi-agent settings.", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "Common knowledge, a term coined by Lewis in [18], plays an important role in the analysis of games [2], distributed systems [13], and many other multi-agent settings.", "startOffset": 99, "endOffset": 102}, {"referenceID": 11, "context": "Common knowledge, a term coined by Lewis in [18], plays an important role in the analysis of games [2], distributed systems [13], and many other multi-agent settings.", "startOffset": 124, "endOffset": 128}, {"referenceID": 5, "context": "Since common knowledge requires infinitely many facts to hold, it is not a priori obvious that CG\u03c6 can be attained at a reasonable cost, or even whether it can ever be attained at all, in settings of interest (see [7, 10, 13]).", "startOffset": 214, "endOffset": 225}, {"referenceID": 8, "context": "Since common knowledge requires infinitely many facts to hold, it is not a priori obvious that CG\u03c6 can be attained at a reasonable cost, or even whether it can ever be attained at all, in settings of interest (see [7, 10, 13]).", "startOffset": 214, "endOffset": 225}, {"referenceID": 11, "context": "Since common knowledge requires infinitely many facts to hold, it is not a priori obvious that CG\u03c6 can be attained at a reasonable cost, or even whether it can ever be attained at all, in settings of interest (see [7, 10, 13]).", "startOffset": 214, "endOffset": 225}, {"referenceID": 9, "context": "It is well-known (see [11, 13]) that common knowledge of a fact is observed simultaneously at all agents it involves.", "startOffset": 22, "endOffset": 30}, {"referenceID": 11, "context": "It is well-known (see [11, 13]) that common knowledge of a fact is observed simultaneously at all agents it involves.", "startOffset": 22, "endOffset": 30}, {"referenceID": 6, "context": "There is an extensive literature on using common knowledge to obtain optimal protocols for simultaneous tasks [8, 9, 16, 20, 21, 23, 24, 26, 25].", "startOffset": 110, "endOffset": 144}, {"referenceID": 7, "context": "There is an extensive literature on using common knowledge to obtain optimal protocols for simultaneous tasks [8, 9, 16, 20, 21, 23, 24, 26, 25].", "startOffset": 110, "endOffset": 144}, {"referenceID": 14, "context": "There is an extensive literature on using common knowledge to obtain optimal protocols for simultaneous tasks [8, 9, 16, 20, 21, 23, 24, 26, 25].", "startOffset": 110, "endOffset": 144}, {"referenceID": 17, "context": "There is an extensive literature on using common knowledge to obtain optimal protocols for simultaneous tasks [8, 9, 16, 20, 21, 23, 24, 26, 25].", "startOffset": 110, "endOffset": 144}, {"referenceID": 18, "context": "There is an extensive literature on using common knowledge to obtain optimal protocols for simultaneous tasks [8, 9, 16, 20, 21, 23, 24, 26, 25].", "startOffset": 110, "endOffset": 144}, {"referenceID": 19, "context": "There is an extensive literature on using common knowledge to obtain optimal protocols for simultaneous tasks [8, 9, 16, 20, 21, 23, 24, 26, 25].", "startOffset": 110, "endOffset": 144}, {"referenceID": 20, "context": "There is an extensive literature on using common knowledge to obtain optimal protocols for simultaneous tasks [8, 9, 16, 20, 21, 23, 24, 26, 25].", "startOffset": 110, "endOffset": 144}, {"referenceID": 22, "context": "There is an extensive literature on using common knowledge to obtain optimal protocols for simultaneous tasks [8, 9, 16, 20, 21, 23, 24, 26, 25].", "startOffset": 110, "endOffset": 144}, {"referenceID": 21, "context": "There is an extensive literature on using common knowledge to obtain optimal protocols for simultaneous tasks [8, 9, 16, 20, 21, 23, 24, 26, 25].", "startOffset": 110, "endOffset": 144}, {"referenceID": 11, "context": "Moreover, one of the main insights from the analysis of [13] and of [11] is that when simultaneous actions are performed, the participating agents have common knowledge that they are being performed.", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "Moreover, one of the main insights from the analysis of [13] and of [11] is that when simultaneous actions are performed, the participating agents have common knowledge that they are being performed.", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "Following [5], we define temporally ordered actions:", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "The notion of perfect recall, capturing the assumption that agents remember all events that they take part in, is popular in the analysis of games and multi-agent systems [10, 29].", "startOffset": 171, "endOffset": 179}, {"referenceID": 25, "context": "The notion of perfect recall, capturing the assumption that agents remember all events that they take part in, is popular in the analysis of games and multi-agent systems [10, 29].", "startOffset": 171, "endOffset": 179}, {"referenceID": 4, "context": "3 was proved in [5].", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "Indeed, in a seminal paper [7], Chandy and Misra showed that in asynchronous systems R, if (R,r, t) |= \u00ac\u03c6 and at a time t \u2032 > t (R,r, t \u2032) |= K jK j\u22121 \u00b7 \u00b7 \u00b7K1\u03c6 , then there must be a message chain in the run r between times t and t \u2032, passing through the agents 1,2,.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "More recently, Ben Zvi and Moses extended Chandy and Misra\u2019s work to systems in which communication is not asynchronous, but rather agents may have access to clocks and the transmission time for each of the channels is bounded [5].", "startOffset": 227, "endOffset": 230}, {"referenceID": 23, "context": "Finally, Parikh and Krasucki analyze the ability to create levels of knowledge consisting of collections of nested knowledge formulas in [27].", "startOffset": 137, "endOffset": 141}, {"referenceID": 13, "context": "The work of [6] complements an earlier work by Halpern, Moses and Waarts [15], in which a fixed point analysis of optimal consensus was obtained.", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "Gonczarowski and Moses used the KoP to analyze the epistemic requirements of more general forms of coordination [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "The papers [3, 4, 5, 12] together can all be viewed as making use of the KoP to provide insights into the interaction between time and communication for coordinating actions in a distributed and multi-agent system.", "startOffset": 11, "endOffset": 24}, {"referenceID": 3, "context": "The papers [3, 4, 5, 12] together can all be viewed as making use of the KoP to provide insights into the interaction between time and communication for coordinating actions in a distributed and multi-agent system.", "startOffset": 11, "endOffset": 24}, {"referenceID": 4, "context": "The papers [3, 4, 5, 12] together can all be viewed as making use of the KoP to provide insights into the interaction between time and communication for coordinating actions in a distributed and multi-agent system.", "startOffset": 11, "endOffset": 24}, {"referenceID": 10, "context": "The papers [3, 4, 5, 12] together can all be viewed as making use of the KoP to provide insights into the interaction between time and communication for coordinating actions in a distributed and multi-agent system.", "startOffset": 11, "endOffset": 24}], "year": 2016, "abstractText": "The Knowledge of Preconditions principle (KoP) is proposed as a widely applicable connection between knowledge and action in multi-agent systems. Roughly speaking, it asserts that if some condition \u03c6 is a necessary condition for performing a given action \u03b1 , then knowing \u03c6 is also a necessary condition for performing \u03b1 . Since the specifications of tasks often involve necessary conditions for actions, the KoP principle shows that such specifications induce knowledge preconditions for the actions. Distributed protocols or multi-agent plans that satisfy the specifications must ensure that this knowledge be attained, and that it is detected by the agents as a condition for action. The knowledge of preconditions principle is formalised in the runs and systems framework, and is proven to hold in a wide class of settings. Well-known connections between knowledge and coordinated action are extended and shown to derive directly from the KoP principle: a common knowledge of preconditions principle is established showing that common knowledge is a necessary condition for performing simultaneous actions, and a nested knowledge of preconditions principle is proven, showing that coordinating actions to be performed in linear temporal order requires a corresponding form of nested knowledge.", "creator": "LaTeX with hyperref package"}}}