{"id": "1606.09371", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Recurrent neural network models for disease name recognition using domain invariant features", "abstract": "Hand - crafted shows component with cross-cultural and domain - knowledge debut task role as determines the performance. leprosy \u201c recognition providers. Such methods besides further further included that scope of both features generally 2001 been written, as ability keep cover the manifestations means word dependencies within takes pardon. In this better, be opportunities however funding such dependencies into reconsider way unitary - invariant coordination for through asthma 's recognition decisions. In example, something implemented various place - to - end recurrent neural communications (RNN) introduced without the undertake on diagnosis this regarding same taken commonly into held possible - definitions varied. We is systems convolution morphology network (CNN) in hopkinton of RNN not do character - currently embedded screen and ordinary it with follow - disk features in our similar. We instance how wheel with however state - made - the - \u201d analysis for the 14 tasks two NCBI epidemic optimized. Our initial six their disease mention given ensure numbers that state - from - rest - classical full ca also accepted without too last comedy study. Further the proposed typically obtained improved performance as within classification exercise of disease names.", "histories": [["v1", "Thu, 30 Jun 2016 07:15:56 GMT  (91kb,D)", "http://arxiv.org/abs/1606.09371v1", "This work has been accepted in ACL-2016 as long paper"]], "COMMENTS": "This work has been accepted in ACL-2016 as long paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sunil kumar sahu", "ashish anand"], "accepted": true, "id": "1606.09371"}, "pdf": {"name": "1606.09371.pdf", "metadata": {"source": "CRF", "title": "Recurrent neural network models for disease name recognition using domain invariant features", "authors": ["Sunil Kumar Sahu", "Ashish Anand"], "emails": ["anand.ashish}@iitg.ernet.in"], "sections": [{"heading": null, "text": "Recurrent neural network models for disease name recognition using domain invariant features\nSunil Kumar Sahu and Ashish Anand Department of Computer Science and Engineering\nIndian Institute of Technology Guwahati Assam, India - 781039\n{sunil.sahu, anand.ashish}@iitg.ernet.in\n\u2014 Abstract\nHand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task. In particular, we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined categories. We also utilize convolution neural network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model. We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering. Further the proposed models obtained improved performance on the classification task of disease names."}, {"heading": "1 Introduction", "text": "Automatic recognition of disease names in biomedical and clinical texts is of utmost importance for development of more sophisticated NLP systems such as information extraction, question answering, text summarization and so on (Rosario and Hearst, 2004). Complicate and inconsistent terminologies, ambiguities caused by use of ab-\nbreviations and acronyms, new disease names, multiple names (possibly of varying number of words) for the same disease, complicated syntactic structure referring to multiple related names or entities are some of the major reasons for making automatic identification of the task difficult and challenging (Leaman et al., 2009). State-ofthe-art disease name recognition systems (Mahbub Chowdhury and Lavelli, 2010; Dog\u0306an and Lu, 2012; Dogan et al., 2014) depends on user defined features which in turn try to capture context keeping in mind above mentioned challenges. Feature engineering not only requires linguistic as well as domain insight but also is time consuming and is corpus dependent.\nRecently window based neural network approach of (Collobert and Weston, 2008; Collobert et al., 2011) got lot of attention in different sequence tagging tasks in NLP. It gave state-of-art results in many sequence labeling problems without using many hand designed or manually engineered features. One major drawback of this approach is its inability to capture features from outside window. Consider a sentence \u201cGiven that the skin of these adult mice also exhibits signs of de novo hair-follicle morphogenesis, we wondered whether human pilomatricomas might originate from hair matrix cells and whether they might possess beta-catenin-stabilizing mutations\u201d (taken verbatim from PMID: 10192393), words such as signs and originate appearing both sides of the word \u201cpilomatricomas\u201d, play important role in deciding it is a disease. Any model relying on features defined based on words occurring within a fixed window of neighboring words will fail to capture information of influential words occurring outside this window.\nOur motivation can be summarized in the following question: can we identify disease name and categorize them without relying on feature en-\nar X\niv :1\n60 6.\n09 37\n1v 1\n[ cs\n.C L\n] 3\n0 Ju\nn 20\n16\ngineering, domain-knowledge or task specific resources? In other words, we can say this work is motivated towards mitigating the two issues: first, feature engineering relying on linguistic and domain-specific knowledge; and second, bring flexibility in capturing influential words affecting model decisions irrespective of their occurrence anywhere within the sentence. For the first, we used character-based embedding (likely to capture orthographic and morphological features) as well as word embedding (likely to capture lexicosemantic features) as features of the neural network models.\nFor the second issue, we explore various recurrent neural network (RNN) architectures for their ability to capture long distance contexts. We experiment with bidirectional RNN (Bi-RNN), bidirectional long short term memory network (BiLSTM) and bidirectional gated recurrent unit (BiGRU). In each of these models we used sentence level log likelihood approach at the top layer of the neural architecture. The main contributions of the work can be summarized as follows\n\u2022 Domain invariant features with various RNN architectures for the disease name recognition and classification tasks,\n\u2022 Comparative study on the use of character based embedded features, word embedding features and combined features in the RNN models.\n\u2022 Failure analysis to check where exactly our models are failed in the considered tasks.\nAlthough there are some related works (discussed in sec 6), this is the first study, to the best of our knowledge, which comprehensively uses various RNN architectures without resorting to feature engineering for disease name recognition and classification tasks.\nOur results show near state-of-the-art performance can be achieved on the disease name recognition task. More significantly, the proposed models obtain significantly improved performance on the disease name classification task."}, {"heading": "2 Methods", "text": "We first give overview of the complete model used for the two tasks. Next we explained embedded features used in different neural network models. We provide short description of different RNN\nmodels in the section 2.3. Training and inference strategies are explained in the section 2.4."}, {"heading": "2.1 Model Architectures", "text": "Similar to any named entity recognition task, we formulate the disease mention recognition task as a token level sequence tagging problem. Each word has to be labeled with one of the defined tags. We choose BIO model of tagging, where B stands for beginning, I for intermediate and O for outsider or other. This way we have two possible tags for all entities of interest, i.e., for all disease mentions, and one tag for other entities.\nGeneric neural architecture is shown in the figure 1. In the very first step, each word is mapped to its embedded features.\nWe call this layer as embedding layer. This layer acts as input to hidden layers of RNN model. We study the three different RNN models, and have described them briefly in the section 2.3. Output of the hidden layers is then fed to the output layer to compute the scores for all tags of interest (Collobert et al., 2011; Huang et al., 2015). In output layer we are using sentence level log likelihood, to make inference. Table 1 briefly describes all notations used in the paper."}, {"heading": "2.2 Features", "text": ""}, {"heading": "Distributed Word Representation (WE)", "text": "Distributed word representation or word embedding or simply word vector (Bengio et al., 2003; Collobert and Weston, 2008) is the technique of learning vector representation of a word in a given\ncorpus. Word vectors are present in columns of matrix Mwe. We can get this vector by taking product of matrix Mwe and one hot vector of vi.\nw(i) =Mwe h(i) (1)\nHere h(i) is the one hot vector representation of ith word in V. We use pre-trained 50 dimensional word vectors learned using skipgram method on a biomedical corpus (Mikolov et al., 2013b; Mikolov et al., 2013a; TH et al., 2015)."}, {"heading": "Character Level Word Embedding (CE)", "text": "Word embedding preserve syntactic and semantic information well but fails to seize morphological\nand shape information. However, for the disease entity recognition task, such information can play an important role. For instance, letter -o- in the word gastroenteritis is used to combine various body parts gastro for stomach, enter for intestines, and itis indicates inflammation. Hence taken together it implies inflammation of stomach and intestines, where -itis play significant role in determining it is actually a disease name.\nCharacter level word embedding was first introduced by (dos Santos and Zadrozny, 2014) with the motivation to capture word shape and morphological features in word embedding. Character level word embedding also automatically mitigate the problem of out of vocabulary words as we can embed any word by its characters through character level embedding. In this case, a vector is initialized for every character in the corpus. Then we learn vector representation for any word by applying CNN on each vector of character sequence of that word as shown in figure 2. These character vectors will get update while training RNN in supervised manner only. Since number of characters in the dataset is not high we assume that every character vectors will get sufficient updation while training RNN itself.\nLet {p1, c1, c2...cM , p2} is sequence of characters for a word with padding at beginning and ending of word and let {al, a1, a2...aM , ar} is its sequence of character vector, which we obtain by multiplying M cw with one hot vector of corresponding character. To obtain character level word embedding we need to feed this in convolution neural network (CNN) with max pooling layer (dos Santos and Zadrozny, 2014). Let W c \u2208\nRdceX(dchrXk) is a filter and bc bias of CNN, then\n[y(i)]j = max 1<m<M [W cq(m) + bc]j (2)\nHere k is window size, q(m) is obtained by concatenating the vector of (k\u2212 1)/2 character left to (k\u22121)/2 character right of cm. Same filter will be used for all window of characters and max pooling operation is performed on results of all. We learn 100 dimensional character embedding for all characters in a given dataset (avoiding case sensitivity) and 25 dimensional character level word embedding from character sequence of words."}, {"heading": "2.3 Recurrent Neural Network Models", "text": "Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers (Graves et al., 2009; Graves, 2013). We experiment with three different variants of RNN, which are briefly described in subsequent subsections."}, {"heading": "Bi-directional Recurrent Neural Network", "text": "In Bi-RNN, context of the word is captured through past and future words. This is achieved by having two hidden components in the intermediate layer, as schematically shown in the fig 1. One component process the information in forward direction (left to right) and other in reverse direction. Subsequently outputs of these components then concatenated and fed to the output layer to get score for all tags of the considered word. Let x(t) is a feature vector of tth word in sentence (concatenation of corresponding embedding features wti and yti) and h(t\u22121)l is the computation of last hidden state at (t \u2212 1)th word, then computation of hidden and output layer values would be:\nh (t) l = tanh(U lx(t) +W lh (t\u22121) l )\nz(t) = V (h (t) l : h (t) r ) (3)\nHere U l \u2208 RnH\u00d7nI andW l \u2208 RnH\u00d7nH , where nI is input vector of length dwe + dce, nH is hidden layer size and V \u2208 RnO\u00d7(nH+nH) is the output layer parameter. h(t)l and h (t) r correspond to left and right hidden layer components respectively and h(t)r is calculated similarly to h (t) l by reversing the words in the sentence. At the beginning h (0) l and h (0) r are initialized randomly."}, {"heading": "Bi-directional Long Short Term Memory Network", "text": "Traditional RNN models suffer from both vanishing and exploding gradient (Pascanu et al., 2012; Bengio et al., 2013). Such models are likely to fail where we need longer contexts to do the job. These issues were the main motivation behind the LSTM model (Hochreiter and Schmidhuber, 1997). LSTM layer is just another way to compute a hidden state which introduces a new structure called a memory cell (ct) and three gates called as input (it), output (ot) and forget (ft) gates.\nThese gates are composed of sigmoid activation function and responsible for regulating information in memory cell. The input gate by allowing incoming signal to alter the state of the memory cell, regulates proportion of history information memory cell will keep. On the other hand, the output gate regulates what proportion of stored information in the memory cell will influence other neurons. Finally, the forget gate can modulate the memory cells and allowing the cell to remember or forget its previous state. Computation of memory cell (c(t)) is done through previous memory cell and candidate hidden state (g(t)) which we compute through current input and the previous hidden state. The final output of hidden state would be calculated based on memory cell and forget gate.\nIn our experiment we used model discussed in (Graves, 2013; Huang et al., 2015). Let x(t) is feature vector for tth word in a sentence and h(t\u22121)l is previous hidden state then computation of hidden (h(t)l ) and output layer (z (t)) of LSTM would be.\ni (t) l = \u03c3(U (i) l x (t) +W (i) l h (t\u22121) l + b i l) f (t) l = \u03c3(U (f) l x (t) +W (f) l h (t\u22121) l + b f l ) o (t) l = \u03c3(U (o) l x (t) +W (o) l h (t\u22121) l + b o l ) g (t) l = tanh(U (g) l x (t) +W (g) l h (t\u22121) l + b g l ) c (t) l = c (t\u22121) l \u2217 fl + gl \u2217 il h (t) l = tanh(c (t) l ) \u2217 ol\nWhere \u03c3 is sigmoid activation function, \u2217 is a element wise product, U (i)l , U (f) l , U (o) l , U (g) l \u2208 RnH\u00d7nI andW (i)l ,W (o) l ,W (f) l ,W (g) l \u2208 R\nnH\u00d7nH , where nI is input size (dwe + dce) and nH is hidden layer size. We compute h(t)r in similar manner as h(t)l by reversing the all words of sentence. Let V \u2208 RnO\u00d7(nH+nH) (nO size of output layer) is\nthe parameter of output layer of LSTM then computation of output layer will be:\nz(t) = V (h (t) l : h (t) r ) (4)"}, {"heading": "Bi-directional Gated Recurrent Unit Network", "text": "A gated recurrent unit (GRU) was proposed by (Cho et al., 2014) to make each recurrent unit to adaptively capture dependencies of different time scales. Similar to the LSTM unit, the GRU has gating units reset r and update z gates that modulate the flow of information inside the unit, however, without having a separate memory cells. The resulting model is simpler than standard LSTM models.\nWe follow (Chung et al., 2014) model of GRU to transform the extracted word embedding and character embedding features to score for all tags. Let x(t) embedding feature for tth word in sentence and h(t\u22121)l is computation of hidden state for (t\u22121)th word then computation of GRU would be:\nz (t) l = \u03c3(U (z) l x (t) +W (z) l h (t\u22121) l + b (z) l ) r (t) l = \u03c3(U (r) l x (t) +W (r) l h (t\u22121) l + b (r) l ) h\u0303 (t) l = tanh(U (h) l x (t) +W (h) l h (t\u22121) l \u2217 rl + b (h) l ) h (t) l = z (t) l \u2217 h\u0303l + (1\u2212 z (t) l ) \u2217 h (t\u22121) l\nz(t) = V (h (t) l : h (t) r ) (5)\nWhere \u2217 is pair wise multiplication, U (z)l , U (r) l , U (h) l , U (h) l \u2208 R nH\u00d7nI and W (z)l ,W (r) l W (h) l \u2208 RnH\u00d7nH are parameters of GRU. V \u2208 RnO\u00d7(nH+nH) is output layer parameter. Computation of h(t)r is done in similar manner as h (t) l by reversing the words of sentence."}, {"heading": "2.4 Training and Inference", "text": "Equations 3, 4 and 5 are the scores of all possible tags for tth word sentence. We follow sentencelevel log-likelihood (SLL) (Collobert et al., 2011) approach equivalent to linear-chain CRF to infer the scores of a particular tag sequence for the given word sequence. Let [w]|s|1 is sentence and [t] |s| 1 is the tag sequence for which we want to find the joint score, then score for the whole sentence with the particular tag sequence would be:\ns([w] |s| 1 , [t] |s| 1 ) = \u2211 1\u2264i\u2264|s| (W transti\u22121,ti + z (i) ti ), (6)\nwhere W trans is transition score matrix and W transi,j is indicating the transition score moving from tag ti to tj ; tj is tag for the jth word; z (i) ti\nis the output score from the neural network model for the tag ti of ith word. To train our model we used cross entropy loss function and adagrad (Duchi et al., 2010) approach to optimize the loss function. Entire neural network parameters, word embedding, character embedding and W trans (transition score matrix used in the SLL) was updated during training. Entire code has been implemented using theano (Bastien et al., 2012) library in python language."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Dataset", "text": "We used NCBI dataset (Dog\u0306an and Lu, 2012), the most comprehensive publicly available dataset annotated with disease mentions, in this work. NCBI dataset has been manually annotated by a group of medical practitioners for identifying diseases and their types in biomedical articles. All disease mentions were categorized into four different categories, namely, specific disease, disease class, composite disease and modifier. A word is annotated as specific disease, if it indicates a particular disease. Disease class category indicates a word describing a family of many specific diseases, such as autoimmune disorder. A string signifying two or more different disease mentions is annotated with composite mention. Modifier category indicates disease mention has been used as modifiers for other concepts. This dataset is a extension of the AZDC dataset (Leaman et al., 2009) which was annotated with disease mentions only and not with their categories. Statistics of the dataset is mentioned in the Table 2.\nIn our evaluation we used this dataset in two settings, A: disease mention recognition, where all\ndisease types are flattened into a single category and, the B: disease class recognition, where we need to decide exact categories of disease mentions. It is noteworthy to mention that the Task B is more challenging as it requires model to capture semantic contexts to put disease mentions into appropriate categories."}, {"heading": "4 Results and Discussion", "text": "Evaluation of different models using CE\nWe first evaluate the performance of different RNNs using only character embedding features. We compare the results of RNN models with window based neural network (Collobert et al., 2011) using sentence level log likelihood approach (NN + CE). For the window based neural network, we considered window size 5 (two words from both left and right, and one central word) and same settings of character embedding were used as features. The same set of parameters are used in all experiments unless we mention specifically otherwise. We used exact matching scheme to evaluate performance of all models.\nTable 3 shows the results obtained by different RNN models with only character level word embedding features. For the task A (Disease name recognition) Bi-LSTM and NN models gave competitive performance on the test set, while Bi-RNN and Bi-GRU did not perform so well. On the other hand for the task B, there is 2.08% \u2212 3.8% improved performance (F1-score) shown by RNN models over the NN model again on the test set. Bi-LSTM model obtained F1-score of 59.78% while NN model gave 57.56%. As discussed earlier, task B is difficult than task A as disease category is more likely to be influenced by the words falling outside the context window considered in\nwindow based methods. This could be reason for RNN models to perform well over the NN model. This hypothesis will be stronger if we observe similar pattern in our other experiments.\nEvaluation of different models with WE and WE+CE\nNext we investigated the results obtained by the various models using only 50 dim word embedding features. The first part of table 4 shows the results obtained by different RNNs and the window based neural network (NN). In this case RNN models are giving better results than the NN model for both the tasks. In particular performance of Bi-LSTM models are best than others in both the tasks. We observe that for the task A, RNN models obtained 1.2% to 3% improvement in F1-score than the baseline NN performance. Similarly 2.55% to 4% improvement in F1-score are observed for the task B, with Bi-LSTM model obtaining more than 4% improvement.\nIn second part of this table we compare the results obtained by various models using the features set obtained by combining the two feature sets. If we look at performance of individual model using three different set of features, model using only word embedding features seems to give consistently best performance. Among all models, BiLSTM using word embedding features obtained best F1-scores of 79.13% and 63.16% for the tasks A and B respectively.\nImportance of tuning pre-trained word vectors\nWe further empirically evaluate the importance of updating of word vectors while training. For this, we performed another set of experiments, where pre-trained word vectors are not updated while\ntraining. Results obtained on the validation dataset of the Task A are shown in the Table 5. One can observe that performance of all models have deteriorated. Next, instead of using pre-trained word vectors, we initialized each word with zero vector but kept updating them while training. Although performance (Table 6) deteriorated (compare to Table 4) but not as much as in table 5. This observation highlights the importance of tuning word vectors for a specific task during training."}, {"heading": "Comparison with State-of-art", "text": "At the end we are comparing our results with stateof-the art results reported in (Dog\u0306an and Lu, 2012) on this dataset using BANNER (Leaman and Gonzalez, 2008) in table 7. BANNER is a CRF based bio entity recognition model, which uses general linguistic, orthographic, syntactic dependency fea-\ntures. Although the result reported in (Dog\u0306an and Lu, 2012) (F1-score = 81.8) is better than that of our RNN models but it should be noted that competitive result (F1-score = 79.13%) is obtained by the proposed Bi-LSTM model which does not depend on any feature engineering or domainspecific resources and is using only word embedding features trained in unsupervised manner on a huge corpus.\nFor the task B, we did not find any paper except (Li, 2012). Li (2012) used linear soft margin support vector (SVM) machine with a number of hand designed features including dictionary based features. The best performing proposed model shows more than 37% improvement in F1-score (benchmark: 46% vs Bi-LSTM+WE: 63.16%)."}, {"heading": "5 Failure Analysis", "text": "To see where exactly our models failed to recognize diseases, we analyzed the results carefully.\nWe found that significant proportion of errors are coming due to use of acronyms of diseases and use of disease form which is rarely appearing in our corpus. Examples of few such cases are \u201cCD\u201d, \u201cHNPCC\u201d,\u201cSCA1\u201d. We observe that this error is occurring because we do not have exact word embedding for these words. Most of the acronyms in the disease corpus were mapped to rare-word embedding1. Another major proportion of errors in our results were due to difficulty in recognizing nested forms of disease names. For example, in all of the following cases: \u201chereditary forms of \u2019ovarian cancer\u2019\u201d , \u201cinherited \u2018breast cancer\u2019\u201d, \u201cmale and female \u2018breast cancer\u2019\u201d, part of phrase such as ovarian cancer in hereditary forms of ovarian cancer, breast cancer in inherited breast cancer and male and female breast cancer are disease names and our models are detecting this very well. However, according to annotation scheme if any disease is part of nested disease name, annotators considered whole phrase as a single disease. So even our model is able to detect part of the disease accurately but due to the exact matching scheme, this will be false positive for us."}, {"heading": "6 Related Research", "text": "In biomedical domain, named entity recognition has attracted much attention for identification of entities such as genes and proteins (Settles, 2005; Leaman and Gonzalez, 2008; Leaman et al., 2009) but not as much for disease name recognition. Notable works, such as of Chowdhury and Lavelli (2010), are mainly conditional random field (CRF) based models using lots of manually designed template features. These include linguistic, orthographic, contextual and dictionary based features. However, they have evaluated their model on the AZDC dataset which is small compared to\n1we obtained pre-trained word-embedding features from (TH et al., 2015) and in their pre-processing strategy, all words of frequency less than 50 were mapped to rare-word.\nthe NCBI dataset, which we have considered in this study. Nikfarjam et al. (2015) have proposed a CRF based sequence tagging model, where cluster id of embedded word as an extra feature with manually engineered features is used for adverse drug reaction recognition in tweets.\nRecently deep neural network models with minimal dependency on feature engineering have been used in few studies in NLP including NER tasks (Collobert et al., 2011; Collobert and Weston, 2008). dos Santos et al. (2015) used deep neural network based model such as window based network to recognize named entity in Portuguese and Spanish texts. In this work, they exploit the power of CNN to get morphological and shape features of words in character level word embedding, and used it as feature with concatenation of word embedding. Their results indicate that CNN are able to preserve morphological and shape features through character level word embedding. Our models are quite similar to this model but we used different variety of RNN in place of window based neural network.\nLabeau et al. (2015) used Bi-RNN with character level word embedding only as a feature for PoS tagging in German text. Their results also show that with only character level word embedding we can get state-of-art results in PoS tagging in German text. Our model used word embedding as well as character level word embedding together as features and also we have tried more sophisticated RNN models such as LSTM and GRU in bi-directional structure. More recent work of Huang et al. (2015) used LSTM and CRF in variety of combination such as only LSTM, LSTM with CRF and Bi-LSTM with CRF for PoS tagging, chunking and NER tasks in general texts. Their results shows that Bi-LSTM with CRF gave best results in all these tasks. These two works have used either Bi-RNN with character embedding features or Bi-LSTM with word embedding\nfeatures in general or news wire texts, whereas in this work we compare the performance of three different types of RNNs: Bi-RNN, Bi-GRU and Bi-LSTM with both word embedding and character embedding features in biomedical text for disease name recognition."}, {"heading": "7 Conclusions", "text": "In this work, we used three different variants of bidirectional RNN models with word embedding features for the first time for disease name and class recognition tasks. Bidirectional RNN models are used to capture both forward and backward long term dependencies among words within a sentence. We have shown that these models are able to obtain quite competitive results compared to the benchmark result on the disease name recognition task. Further our results have shown a significantly improved results on the relatively harder task of disease classification which has not been studied much. All our results were obtained without putting any effort on feature engineering or requiring domain-specific knowledge. Our results also indicate that RNN based models perform better than window based neural network model for the two tasks. This could be due to the implicit ability of RNN models to capture variable range dependencies of words compared to explicit dependency on context window size of window based neural network models."}, {"heading": "Acknowledgments", "text": "We acknowledge the use of computing resources made available from the Board of Research in Nuclear Science (BRNS), Dept of Atomic Energy (DAE) Govt. of India sponsered project (No.2013/13/8-BRNS/10026) by Dr Aryabartta Sahu at Department of Computer Science and Engineering, IIT Guwahati."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Advances in optimizing recurrent networks", "author": ["Bengio et al.2013] Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259", "author": ["Cho et al.2014] KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555", "author": ["Chung et al.2014] Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "NCBI disease corpus: A resource for disease name recognition and concept normalization", "author": ["Robert Leaman", "Zhiyong Lu"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "Dogan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dogan et al\\.", "year": 2014}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Bianca Zadrozny"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro"], "venue": "Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "An improved corpus of disease mentions in pubmed citations", "author": ["Do\u011fan", "Lu2012] Rezarta Islamaj Do\u011fan", "Zhiyong Lu"], "venue": "In Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,", "citeRegEx": "Do\u011fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Do\u011fan et al\\.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2010] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Technical Report UCB/EECS-2010-24, EECS Department,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves et al.2009] Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Non-lexical neural architecture for fine-grained pos tagging", "author": ["Kevin Lser", "Alexandre Allauzen"], "venue": "In Llus Mrquez,", "citeRegEx": "Labeau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Labeau et al\\.", "year": 2015}, {"title": "Banner: An executable survey of advances in biomedical named entity recognition", "author": ["Leaman", "Gonzalez2008] Robert Leaman", "Graciela Gonzalez"], "venue": null, "citeRegEx": "Leaman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Leaman et al\\.", "year": 2008}, {"title": "Enabling recognition of diseases in biomedical text with machine learning: corpus and benchmark", "author": ["Leaman et al.2009] Robert Leaman", "Christopher Miller", "G Gonzalez"], "venue": "Proceedings of the 2009 Symposium on Languages", "citeRegEx": "Leaman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leaman et al\\.", "year": 2009}, {"title": "Disease mention recognition using soft-margin", "author": ["Gang Li"], "venue": "svm. Training,", "citeRegEx": "Li.,? \\Q2012\\E", "shortCiteRegEx": "Li.", "year": 2012}, {"title": "Disease mention recognition with specific features", "author": ["Mahbub Chowdhury", "Alberto Lavelli"], "venue": "In Proceedings of the 2010 Workshop on Biomedical Natural Language Processing,", "citeRegEx": "Chowdhury et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chowdhury et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding", "author": ["Abeed Sarker", "Karen OConnor", "Rachel Ginn", "Graciela Gonzalez"], "venue": null, "citeRegEx": "Nikfarjam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nikfarjam et al\\.", "year": 2015}, {"title": "Understanding the exploding gradient problem. CoRR, abs/1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Classifying semantic relations in bioscience texts", "author": ["Rosario", "Hearst2004] Barbara Rosario", "Marti A. Hearst"], "venue": "In Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Rosario et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosario et al\\.", "year": 2004}, {"title": "ABNER: An open source tool for automatically tagging genes, proteins, and other entity names in text", "author": ["B. Settles"], "venue": null, "citeRegEx": "Settles.,? \\Q2005\\E", "shortCiteRegEx": "Settles.", "year": 2005}, {"title": "Evaluating distributed word representations for capturing semantics of biomedical concepts", "author": ["TH et al.2015] MUNEEB TH", "Sunil Sahu", "Ashish Anand"], "venue": "In Proceedings of BioNLP", "citeRegEx": "TH et al\\.,? \\Q2015\\E", "shortCiteRegEx": "TH et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Complicate and inconsistent terminologies, ambiguities caused by use of abbreviations and acronyms, new disease names, multiple names (possibly of varying number of words) for the same disease, complicated syntactic structure referring to multiple related names or entities are some of the major reasons for making automatic identification of the task difficult and challenging (Leaman et al., 2009).", "startOffset": 378, "endOffset": 399}, {"referenceID": 7, "context": "State-ofthe-art disease name recognition systems (Mahbub Chowdhury and Lavelli, 2010; Do\u011fan and Lu, 2012; Dogan et al., 2014) depends on user defined features which in turn try to capture context keeping in mind above mentioned challenges.", "startOffset": 49, "endOffset": 125}, {"referenceID": 6, "context": "Recently window based neural network approach of (Collobert and Weston, 2008; Collobert et al., 2011) got lot of attention in different sequence tagging tasks in NLP.", "startOffset": 49, "endOffset": 101}, {"referenceID": 6, "context": "Output of the hidden layers is then fed to the output layer to compute the scores for all tags of interest (Collobert et al., 2011; Huang et al., 2015).", "startOffset": 107, "endOffset": 151}, {"referenceID": 15, "context": "Output of the hidden layers is then fed to the output layer to compute the scores for all tags of interest (Collobert et al., 2011; Huang et al., 2015).", "startOffset": 107, "endOffset": 151}, {"referenceID": 1, "context": "Distributed word representation or word embedding or simply word vector (Bengio et al., 2003; Collobert and Weston, 2008) is the technique of learning vector representation of a word in a given", "startOffset": 72, "endOffset": 121}, {"referenceID": 27, "context": "We use pre-trained 50 dimensional word vectors learned using skipgram method on a biomedical corpus (Mikolov et al., 2013b; Mikolov et al., 2013a; TH et al., 2015).", "startOffset": 100, "endOffset": 163}, {"referenceID": 12, "context": "Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers (Graves et al., 2009; Graves, 2013).", "startOffset": 164, "endOffset": 199}, {"referenceID": 13, "context": "Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers (Graves et al., 2009; Graves, 2013).", "startOffset": 164, "endOffset": 199}, {"referenceID": 24, "context": "Traditional RNN models suffer from both vanishing and exploding gradient (Pascanu et al., 2012; Bengio et al., 2013).", "startOffset": 73, "endOffset": 116}, {"referenceID": 2, "context": "Traditional RNN models suffer from both vanishing and exploding gradient (Pascanu et al., 2012; Bengio et al., 2013).", "startOffset": 73, "endOffset": 116}, {"referenceID": 13, "context": "In our experiment we used model discussed in (Graves, 2013; Huang et al., 2015).", "startOffset": 45, "endOffset": 79}, {"referenceID": 15, "context": "In our experiment we used model discussed in (Graves, 2013; Huang et al., 2015).", "startOffset": 45, "endOffset": 79}, {"referenceID": 3, "context": "A gated recurrent unit (GRU) was proposed by (Cho et al., 2014) to make each recurrent unit to adaptively capture dependencies of different time scales.", "startOffset": 45, "endOffset": 63}, {"referenceID": 4, "context": "We follow (Chung et al., 2014) model of GRU to transform the extracted word embedding and character embedding features to score for all tags.", "startOffset": 10, "endOffset": 30}, {"referenceID": 6, "context": "We follow sentencelevel log-likelihood (SLL) (Collobert et al., 2011) approach equivalent to linear-chain CRF to infer the scores of a particular tag sequence for the given word sequence.", "startOffset": 45, "endOffset": 69}, {"referenceID": 11, "context": "To train our model we used cross entropy loss function and adagrad (Duchi et al., 2010) approach to optimize the loss function.", "startOffset": 67, "endOffset": 87}, {"referenceID": 0, "context": "Entire code has been implemented using theano (Bastien et al., 2012) library in python language.", "startOffset": 46, "endOffset": 68}, {"referenceID": 18, "context": "This dataset is a extension of the AZDC dataset (Leaman et al., 2009) which was annotated with disease mentions only and not with their categories.", "startOffset": 48, "endOffset": 69}, {"referenceID": 6, "context": "We compare the results of RNN models with window based neural network (Collobert et al., 2011) using sentence level log likelihood approach (NN + CE).", "startOffset": 70, "endOffset": 94}, {"referenceID": 19, "context": "For the task B, we did not find any paper except (Li, 2012).", "startOffset": 49, "endOffset": 59}, {"referenceID": 19, "context": "For the task B, we did not find any paper except (Li, 2012). Li (2012) used linear soft margin support vector (SVM) machine with a number of hand designed features including dictionary based features.", "startOffset": 50, "endOffset": 71}, {"referenceID": 19, "context": "16 SM-SVM(Li, 2012) - - - 66.", "startOffset": 9, "endOffset": 19}, {"referenceID": 26, "context": "In biomedical domain, named entity recognition has attracted much attention for identification of entities such as genes and proteins (Settles, 2005; Leaman and Gonzalez, 2008; Leaman et al., 2009) but not as much for disease name recognition.", "startOffset": 134, "endOffset": 197}, {"referenceID": 18, "context": "In biomedical domain, named entity recognition has attracted much attention for identification of entities such as genes and proteins (Settles, 2005; Leaman and Gonzalez, 2008; Leaman et al., 2009) but not as much for disease name recognition.", "startOffset": 134, "endOffset": 197}, {"referenceID": 17, "context": "In biomedical domain, named entity recognition has attracted much attention for identification of entities such as genes and proteins (Settles, 2005; Leaman and Gonzalez, 2008; Leaman et al., 2009) but not as much for disease name recognition. Notable works, such as of Chowdhury and Lavelli (2010), are mainly conditional random field (CRF) based models using lots of manually designed template features.", "startOffset": 177, "endOffset": 299}, {"referenceID": 27, "context": "we obtained pre-trained word-embedding features from (TH et al., 2015) and in their pre-processing strategy, all words of frequency less than 50 were mapped to rare-word.", "startOffset": 53, "endOffset": 70}, {"referenceID": 23, "context": "Nikfarjam et al. (2015) have proposed a CRF based sequence tagging model, where cluster id of embedded word as an extra feature with manually engineered features is used for adverse drug reaction recognition in tweets.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "Recently deep neural network models with minimal dependency on feature engineering have been used in few studies in NLP including NER tasks (Collobert et al., 2011; Collobert and Weston, 2008).", "startOffset": 140, "endOffset": 192}, {"referenceID": 5, "context": "Recently deep neural network models with minimal dependency on feature engineering have been used in few studies in NLP including NER tasks (Collobert et al., 2011; Collobert and Weston, 2008). dos Santos et al. (2015) used deep neural network based model such as window based network to recognize named entity in Portuguese and Spanish texts.", "startOffset": 141, "endOffset": 219}, {"referenceID": 15, "context": "More recent work of Huang et al. (2015) used LSTM and CRF in variety of combination such as only LSTM, LSTM with CRF and Bi-LSTM with CRF for PoS tagging, chunking and NER tasks in general texts.", "startOffset": 20, "endOffset": 40}], "year": 2016, "abstractText": "Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task. In particular, we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined categories. We also utilize convolution neural network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model. We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering. Further the proposed models obtained improved performance on the classification task of disease names.", "creator": "LaTeX with hyperref package"}}}