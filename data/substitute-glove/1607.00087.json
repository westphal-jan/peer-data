{"id": "1607.00087", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Fractal Dimension Pattern Based Multiresolution Analysis for Rough Estimator of Person-Dependent Audio Emotion Recognition", "abstract": "As turn head comes the defined, audio analysis and recognition has notable hard attentions for its full information in good - sense world. Audio creativity recognition (AER) possibility did knowing moments u.s. of this others the given utters signals, and much last practiced especially because while further development it enjoyed life - explosive server. Distinguish from other existing contemporary, main anyone - affects patterns means disc subconscious tend extensive, and fractal element features typically calculated even steel-string introduction byproduct. Furthermore, it is able soon efficiently i reactivity differ made auditory emotions, turn the spellbinding contains are learned saw 3-dimensional frames in each sub - bands. Experimental testing comes the proposed ideal is longer as required semiotics best did audio emotion recognition.", "histories": [["v1", "Fri, 1 Jul 2016 00:54:10 GMT  (2253kb)", "https://arxiv.org/abs/1607.00087v1", null], ["v2", "Fri, 2 Dec 2016 13:12:35 GMT  (2251kb)", "http://arxiv.org/abs/1607.00087v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.SD", "authors": ["miao cheng", "ah chung tsoi"], "accepted": false, "id": "1607.00087"}, "pdf": {"name": "1607.00087.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mew.cheng@gmail.com", "actsoi@must.edu.mo"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 7.\n00 08\n7v 2\n[ cs\n.A I]\n2 D\nec 2\nIndex Terms\u2014 Audio emotion recognition (AER), multiresolution analysis, fractal dimension."}, {"heading": "1. INTRODUCTION", "text": "Audio emotion analysis aims to understand the intrinsic explanation of emotional states of human. As a common sense, audio emotion recognition (AER) identifies the emotional information from speech signals, and learns audio features for classification of different emotions. Furthermore, audio analysis and recognition has become more and more important and attractive with advancement on wide applications of mobile devices, and brought much perspective of applicable reality of life demand. And it has been found useful for many applications, such as human-machine conversation [1], emotion understanding [2], sickness diagnosis [3], and so on. In the literature, AER has been studied abroad on different aspects. One of these outstanding solutions, is to exploit the differential features from audio data, and find the discriminative information for identification of different emotions.\nWithout loss of generality, there have been six emotions, such as angry, disgust, fear, happy, sad, and surprise, involved in most emotion recognition works. Until now, several audio features has been widely adopted to depict the au-\ndio information from a perspective point of distinctive classification. With the most significant characteristics of affect in speech, the pitch is usually estimated based on the Fourier analysis of the logarithmic amplitude spectrum of the signal [4], which is divided into a set of frames by windowing. As a low level of features, pitch is used to describe the acoustic signals for its utilised statistics. The zero crossing rate [5] calculates average result of the number of times that the audio signal crosses zero within a particular time window, which is to determine the audio patterns of fevered and agitated emotions as a pattern feature.\nThe log-energy features [6] are able to find the distinctive patterns of certain emotions among different audio signals, and decide the emotional states with the amplitude of a segment of speech. And as we found, it can observably differentiate negative emotions for specific discriminant learning as reachable numeric parameters, while a stable performance is available. Distinguishingly, the teager energy operator (TEO) [7] uses the nonlinear operator to measure the changing energies of nonlinear emotional components. The well-known Mel-Frequency Cepstral Coefficients (MFCCs) [8] is based on the short-term power spectrum of speech signals, and a linear cosine transform of a log power spectrum on a nonlinear Mel scale of frequency. Then, the coefficients are collected via the obtained mel-frequency cepstrum (MFC) for approximating to the human auditory system\u2019s response. Nevertheless, it is always hardly to determine the best choice of representative features for specific audio data, and insufficiently to correctly classify emotions with single features."}, {"heading": "2. WAVELET TRANSFORM AND FRACTAL DIMENSION", "text": "As a development of traditional Fourier transform, wavelet transform (WT) has been widely applied to signal processing and pattern analysis [9]. With multi-scale decomposition idea, the outstanding advantage of WT is its complete multi-scale analysis of signal inputs, while approximation is reached in light of approximate and detail coefficients [10]. Actually, WT can be explained as an approximate the origi-\nnal signal with the scaling functions and wavelet functions. Generally, multi-scale analysis constructs a set of nested function space, namely \u00b7 \u00b7 \u00b7 \u2282 V\u22122 \u2282 V\u22121 \u2282 V0 \u2282 V1 \u2282 V2 \u00b7 \u00b7 \u00b7 . Therefore, the differential subtraction of neighboring function spaces defines a space formed by wavelet functions, namely V j \u2295\nW j = V j+1. In addition, wavelet packet based tree-structure [11] has also been widely applied to AER for its universally applicable property.\nOn the other hand, Fractal dimension (FD) is a kind of nonlinear approximate methods of complicated measure, and has been widely applied to pattern recognition and bioinformatics [12]. For certain practical measures, it is difficult to exploit the metrical results directly. As a result, the original problem is usually reduced to a count of fractal units, and outlets are collected to reach the total measure as the so-called fractal dimension. In fact, several FD methods have been designed to conduct different kinds of calculational demands, such as Hausdorff dimension [13], box counting dimension [14], Katz\u2019s dimension [15], and Higuchi\u2019s dimension [16].\nThe Hausdorff dimension [13] measures the local size of a space taking into account the distance between points, the metric. For shapes that are smooth, or shapes with a small number of corners, the shapes of traditional geometry and science, the Hausdorff dimension is an integer agreeing with the topological dimension. In fractal geometry, box-counting dimension [14] is a way of determining the fractal dimension of a set S in a Euclidean space Rn, which is also known as Minkowski dimension. In addition, there are also some FD holding the ability of conducting the sequence data for deterministic metric of signal sample. More specifically, Katz\u2019s FD [15] is calculated as:\nD = log (L/a) log (d/a) = log (n) log (n) + log (d/L) , (1)\nwhere L and a denote the sum and average of the Euclidean\ndistances between the successive point of the sample as well as the maximum distance between the first point and any other point of the sample d. Given an one-dimensional signal xi, (i = 1, \u00b7 \u00b7 \u00b7 ,N), Higuchi\u2019s FD [16] firstly calculates sub-sample sets from the signal data as\nXmk = {X (m + ik)} [(N\u2212m)/k] i=0\n= {x (m) , x (m + k) , x (m + 2k), \u00b7 \u00b7 \u00b7 x ( m + [\nN\u2212m k\n] k )} , m = 1, \u00b7 \u00b7 \u00b7 , k. (2)\nHere, k \u2208 [1, kmax] and N is the sample size. Then the approximated length of each Xk can be calculated as\nLm (k) =\n[ N\u2212mk ] \u2211\ni=1 |X (m + ik) \u2212 X (m + (i \u2212 1) k)| (N \u2212 1)\n[(N \u2212 m)/k] k2 . (3)\nFinally, the fractal dimension of data is obtained by solving the problem\n\u3008L (k)\u3009 \u221d k\u2212D. (4)"}, {"heading": "3. FRACTAL DIMENSION BASED AUDIO EMOTION RECOGNITION", "text": "For an AER system, it is necessary to exploit the discriminative patterns with respect to chosen audio features. And an acceptable results usually depend on suitable features that best represent the distinctive characteristics of specific data, which has been a common sense for audio analysis. In this work, the SAVEE data set 1 is employed for audio emotion analysis, and resultant outlets refer to audio feature extraction as disclosed. As a difficult data set for AER, it is hardly to exploit the audio patterns between audio data of different individuals. To make the rules clear, several audio features\n1http://kahlan.eps.surrey.ac.uk/savee/\nextracted from SAVEE data set are illustrated in Fig. 1 and Fig. 2.\nAs illustrated, the pitch features and zero crossing rates are hardly to depict the distinctive differences among different emotions, especially for emotional features of disgust, fear, happy, sad. Nevertheless, sad and disgust emotions can be selected with much lower energies over others in most cases, while angry emotion gives obvious information associated with higher TEO features among all kinds of speeches. In light of this consideration, it is feasible to discriminant different emotions according to these distinctive features step by step. Thereafter, the audio features of each individual can be divided into several portions of instances corresponding to Energy and TEO features firstly. And disgust, sad and angry among all emotions are able to be sequentially screened out in principle. Though it is always necessary to learn the discriminative patterns during each screening steps, however, it indeed reduces the complexities of emotion classification problems. Different from some existing works that refer to a screening steps for AER only [17], it still fails to reach good results of complicated audio patterns, e.g., SAVEE. Actually, it may give the worst results for correct classification of audio features, if no mechanism else has involved. To address this limitation, FD features are borrowed in depiction of audio emotions, and concrete response to utterance signals can be obtained.\nIn terms of definition of FD, the sequential data can be represented as lists of FD features, and discriminant patterns can be handled in following steps. As described foregoing, there are several FD methods that can be adopted to sample data, and it is optional for AER in general. Nevertheless, Higuchi\u2019s FD is picked up in this work, due to its simple implementation and its better performance over other methods as testing. Furthermore, the inconsistent lengths of features can be avoided, and FD features are extracted in each wavelet\ndecomposition levels. Though there are also several works conducting feature extraction in a similar manner, it is worthwhile to highlight the differences here:\n\u2022 There is no floating windows or fussy iterations, e.g., dictionary learning [18], involved in discriminative matching, which has been a popular solution widely adopted in many related works. And thus, the efficient achievement of AER is able to be preserved.\n\u2022 It takes both approximate and detail coefficients of each level for feature extraction, though many solutions only refer to the approximate coefficients for discriminative learning. And it is found discriminant ability can be improved with detailed pattern information."}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "In this section, the experimental performance of proposed method on AER are evaluated, and discriminant performance\nis disclosed with respect to different individuals. The Surrey Audio-Visual Expressed Emotion (SAVEE) data set consists of recordings from 4 male actors in 7 different emotions, 480 British English utterances in total. And it has been wellknown for its difficulty of accurate recognition of emotions with respect to each data modality. In our experiments, the audio data of six standard emotions are used, while the neutral emotion is ignored, and then there are 90 utterances for each individual. To learn the reduced features, the maximum margin criterion (MMC) [19] associated with KNN classifier is employed for discriminative classification.\nFirstly, the audio data of one actor is picked up for learning, while the rest utterances of other individuals are all for testing of AER. That is, the audio data of one vs. three individuals are set up for learning, and the divisions of utterances are proceeded with respect to audio features of each individual. The AER rates corresponding to each referred person is shown in Fig. 3. In terms of the results, it is difficult to learn the ideal auditory features no matter whose audio data is used for labeled information. Nevertheless, the best results are able to reach an approximate to 45% accuracy, on behalf of FD features and screening step, and it is obvious that KL is the hardest one for AER in every cases. Oppositely, the utterances of JK is the easiest one for auditory emotion capture, whose stable recognition results can be obtained for each referred person. In addition, the AER results of JE is also acceptable due to its similar utterance characteristics as JK.\nIn the second experiments, the audio data from two individuals are selected for pattern analysis, while the rest data of two other persons are used for emotion recognition. The experimental results are illustrated in Fig. 4. As shown, the recognition performance can be improved in most cases of choices of referred audio data, and around 50% accuracy can be reached. Nevertheless, there are still some obtained results consistent with the foregoing ones generated from few available patterns, due to certain hard data sets for accurate classification. And the outstanding results come from the discriminative learning of final linear classifier, benefit from the fact of more labeled data are available for pattern analysis."}, {"heading": "5. CONCLUSION", "text": "In this work, AER is considered as a multi-scale analysis problem, and FD features are calculated to represent the best distinctive patterns of different audio emotions. Distinguishingly, both approximate and detail coefficients of wavelet decomposition have been adopted to conduct the hard emotion data. Furthermore, no floating window is used in the signal decomposition stage, while emotion portions are also enjoined for further reduced learning. Experimental results show that the proposed method is able to contribute comparative performance for AER, even if the audio emotion is quite distinctive from each other of individuals for recognition."}, {"heading": "6. REFERENCES", "text": "[1] T. Watanabe, \u201cThe adaptation of machine conversational speed to speaker utterance speed in humanmachine communication,\u201d IEEE Trans. Sys. Man Cybern., vol. 20, no. 2, pp. 502\u2013507, 1990.\n[2] C. Maaoui, A. Pruski, and F. Abdat, \u201cEmotion recognition for human-machine communication,\u201d in Proc. Int. Conf. Intell. Robots and Sys., 2008.\n[3] Z. Syed, D. Leeds, D. Curtis, J. Guttag, F. Nesta, and R. A. Levine, \u201cAudio-visual tools for computer-assisted diagnosis of cardiac disorders,\u201d in Proc. Int. Symp. Comp. Med. Syst., 2006, pp. 207\u2013212.\n[4] S. Hoch, F. Althoff, G. McGlaun, and G. Rigoll, \u201cBimodal fusion of emotinal data in an automotive environment,\u201d in Proc. IEEE Int. Conf. on Acoustic, Speech, and Signal Processing, 2005, pp. 1085\u20131088.\n[5] F. Gouyon, F. Pachet, and O. Delete, \u201cClassifying percussive sounds: a matter of zero-crossing rate?,\u201d in Proc. COST G-6 Conf. Digital Audio Effects, 2001, pp. 53\u201358.\n[6] Z. Zeng, \u201cBimodal hci-related affect recognition,\u201d in Proc. Int. Conf. Multimodal Interfaces, 2004.\n[7] H. Gao, S. Chen, and G. Su, \u201cEmotion classification of mandarin speech based on teo nonlinear features,\u201d in ACIS Int. Conf. Soft. Eng., Artif. Intell. Networking, and Parallel/Distributed Computing, 2007.\n[8] O. Kwon, K. Chan, J. Hao, and T. Lee, \u201cEmotion recognition by speech signals,\u201d in Proc. Euro. Conf. on Speech Comm. and Tech., 2003.\n[9] A. Boggess and F. J. Narcowich, A First Course in Wavelets with Fourier Analysis, Wiley, 2nd edition, 2009.\n[10] Ingrid Daubechies, Ten Lectures on Wavelets, SIAM, 1992.\n[11] K. Wang, \u201cTime-frequency feature representation using multi-resolution texture analysis and acoustic activity detector for real-life speech emotion recognition,\u201d Sensors, vol. 15, no. 1, pp. 1458\u20131478, 2015.\n[12] L. Yang, Y. Y. Tang, Y. Lu, and H. Luo, \u201cA fractal dimension and wavelet transform based method for protein sequence similarity analysis,\u201d IEEE/ACM Trans. Compu. Bio. and Bioinfo., vol. 12, no. 2, pp. 348\u2013359, 2015.\n[13] H. Felix, \u201cDimension und a\u0308u\u00dferes ma\u00df,\u201d Mathematische Annalen, vol. 79, no. 1-2, pp. 157\u2013179, 1918.\n[14] Kenneth Falconer, Fractal Geometry: Mathematical Foundations and Applications, Wiley, third edition edition, 2014.\n[15] M. J. Katz, \u201cFractals and the analysis of waveforms,\u201d Computers in Biology and Medicine, vol. 18, no. 3, pp. 145\u2013156, 1988.\n[16] T. Higuchi, \u201cApproach to an irregular time series on the basis of the fractal theory,\u201d Physica D: Nonlinear Phenomena, vol. 31, no. 2, pp. 277\u2013283, 1988.\n[17] C. S. Ooi, K. P. Seng, L. M. Ang, and L. W. Chew, \u201cA new approach of audio emotion recognition,\u201d Expert Syst. Appl., vol. 41, pp. 5858\u20135869, 2014.\n[18] R. Rubinstein, A. M. Bruckstein, and M. Elad, \u201cDictionaries for sparse representation modeling,\u201d Proc. IEEE, vol. 98, no. 6, pp. 1045\u20131057, 2010.\n[19] M. Cheng, Y. Y. Tang, and C. M. Pun, \u201cNonparametric feature extraction via direct maximum margin alignment,\u201d in Proceedings of Int. Conf. Mach. Learn. Appl., 2011."}], "references": [{"title": "The adaptation of machine conversational speed to speaker utterance speed in humanmachine communication", "author": ["T. Watanabe"], "venue": "IEEE Trans. Sys. Man Cybern., vol. 20, no. 2, pp. 502\u2013507, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Emotion recognition for human-machine communication", "author": ["C. Maaoui", "A. Pruski", "F. Abdat"], "venue": "Proc. Int. Conf. Intell. Robots and Sys., 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Audio-visual tools for computer-assisted diagnosis of cardiac disorders", "author": ["Z. Syed", "D. Leeds", "D. Curtis", "J. Guttag", "F. Nesta", "R.A. Levine"], "venue": "Proc. Int. Symp. Comp. Med. Syst., 2006, pp. 207\u2013212.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Bimodal fusion of emotinal data in an automotive environment", "author": ["S. Hoch", "F. Althoff", "G. McGlaun", "G. Rigoll"], "venue": "Proc. IEEE Int. Conf. on Acoustic, Speech, and Signal Processing, 2005, pp. 1085\u20131088.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Classifying percussive sounds: a matter of zero-crossing rate", "author": ["F. Gouyon", "F. Pachet", "O. Delete"], "venue": "Proc. COST G-6 Conf. Digital Audio Effects, 2001, pp. 53\u201358.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Bimodal hci-related affect recognition", "author": ["Z. Zeng"], "venue": "Proc. Int. Conf. Multimodal Interfaces, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Emotion classification of mandarin speech based on teo nonlinear features", "author": ["H. Gao", "S. Chen", "G. Su"], "venue": "ACIS Int. Conf. Soft. Eng., Artif. Intell. Networking, and Parallel/Distributed Computing, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Emotion recognition by speech signals", "author": ["O. Kwon", "K. Chan", "J. Hao", "T. Lee"], "venue": "Proc. Euro. Conf. on Speech Comm. and Tech., 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "A First Course in Wavelets with Fourier Analysis, Wiley", "author": ["A. Boggess", "F.J. Narcowich"], "venue": "2nd edition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Time-frequency feature representation using multi-resolution texture analysis and acoustic activity detector for real-life speech emotion recognition", "author": ["K. Wang"], "venue": "Sensors, vol. 15, no. 1, pp. 1458\u20131478, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A fractal dimension and wavelet transform based method for protein sequence similarity analysis", "author": ["L. Yang", "Y.Y. Tang", "Y. Lu", "H. Luo"], "venue": "IEEE/ACM Trans. Compu. Bio. and Bioinfo., vol. 12, no. 2, pp. 348\u2013359, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Dimension und \u00e4u\u00dferes ma\u00df", "author": ["H. Felix"], "venue": "Mathematische Annalen, vol. 79, no. 1-2, pp. 157\u2013179, 1918.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1918}, {"title": "Fractal Geometry: Mathematical Foundations and Applications, Wiley, third edition", "author": ["Kenneth Falconer"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Fractals and the analysis of waveforms", "author": ["M.J. Katz"], "venue": "Computers in Biology and Medicine, vol. 18, no. 3, pp. 145\u2013156, 1988.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "Approach to an irregular time series on the basis of the fractal theory", "author": ["T. Higuchi"], "venue": "Physica D: Nonlinear Phenomena, vol. 31, no. 2, pp. 277\u2013283, 1988.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1988}, {"title": "A new approach of audio emotion recognition", "author": ["C.S. Ooi", "K.P. Seng", "L.M. Ang", "L.W. Chew"], "venue": "Expert Syst. Appl., vol. 41, pp. 5858\u20135869, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Dictionaries for sparse representation modeling", "author": ["R. Rubinstein", "A.M. Bruckstein", "M. Elad"], "venue": "Proc. IEEE, vol. 98, no. 6, pp. 1045\u20131057, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric feature extraction via direct maximum margin alignment", "author": ["M. Cheng", "Y.Y. Tang", "C.M. Pun"], "venue": "Proceedings of Int. Conf. Mach. Learn. Appl., 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "And it has been found useful for many applications, such as human-machine conversation [1], emotion understanding [2], sickness diagnosis [3], and so on.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "And it has been found useful for many applications, such as human-machine conversation [1], emotion understanding [2], sickness diagnosis [3], and so on.", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "And it has been found useful for many applications, such as human-machine conversation [1], emotion understanding [2], sickness diagnosis [3], and so on.", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "With the most significant characteristics of affect in speech, the pitch is usually estimated based on the Fourier analysis of the logarithmic amplitude spectrum of the signal [4], which is divided into a set of frames by windowing.", "startOffset": 176, "endOffset": 179}, {"referenceID": 4, "context": "The zero crossing rate [5] calculates average result of the number of times that the audio signal crosses zero within a particular time window, which is to determine the audio patterns of fevered and agitated emotions as a pattern feature.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "The log-energy features [6] are able to find the distinctive patterns of certain emotions among different audio signals, and decide the emotional states with the amplitude of a segment of speech.", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Distinguishingly, the teager energy operator (TEO) [7] uses the nonlinear operator to measure the changing energies of nonlinear emotional components.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "The well-known Mel-Frequency Cepstral Coefficients (MFCCs) [8] is based on the short-term power spectrum of speech signals, and a linear cosine transform of a log power spectrum on a nonlinear Mel scale of frequency.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "As a development of traditional Fourier transform, wavelet transform (WT) has been widely applied to signal processing and pattern analysis [9].", "startOffset": 140, "endOffset": 143}, {"referenceID": 9, "context": "In addition, wavelet packet based tree-structure [11] has also been widely applied to AER for its universally applicable property.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "On the other hand, Fractal dimension (FD) is a kind of nonlinear approximate methods of complicated measure, and has been widely applied to pattern recognition and bioinformatics [12].", "startOffset": 179, "endOffset": 183}, {"referenceID": 11, "context": "In fact, several FD methods have been designed to conduct different kinds of calculational demands, such as Hausdorff dimension [13], box counting dimension [14], Katz\u2019s dimension [15], and Higuchi\u2019s dimension [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "In fact, several FD methods have been designed to conduct different kinds of calculational demands, such as Hausdorff dimension [13], box counting dimension [14], Katz\u2019s dimension [15], and Higuchi\u2019s dimension [16].", "startOffset": 157, "endOffset": 161}, {"referenceID": 13, "context": "In fact, several FD methods have been designed to conduct different kinds of calculational demands, such as Hausdorff dimension [13], box counting dimension [14], Katz\u2019s dimension [15], and Higuchi\u2019s dimension [16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "In fact, several FD methods have been designed to conduct different kinds of calculational demands, such as Hausdorff dimension [13], box counting dimension [14], Katz\u2019s dimension [15], and Higuchi\u2019s dimension [16].", "startOffset": 210, "endOffset": 214}, {"referenceID": 11, "context": "The Hausdorff dimension [13] measures the local size of a space taking into account the distance between points, the metric.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "In fractal geometry, box-counting dimension [14] is a way of determining the fractal dimension of a set S in a Euclidean space Rn, which is also known as Minkowski dimension.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "More specifically, Katz\u2019s FD [15] is calculated as:", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "Given an one-dimensional signal xi, (i = 1, \u00b7 \u00b7 \u00b7 ,N), Higuchi\u2019s FD [16] firstly calculates sub-sample sets from the signal data as", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "Different from some existing works that refer to a screening steps for AER only [17], it still fails to reach good results of complicated audio patterns, e.", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": ", dictionary learning [18], involved in discriminative matching, which has been a popular solution widely adopted in many related works.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "To learn the reduced features, the maximum margin criterion (MMC) [19] associated with KNN classifier is employed for discriminative classification.", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "As a general means of expression, audio analysis and recognition has attracted much attentions for its wide applications in real-life world. Audio emotion recognition (AER) attempts to understand emotional states of human with the given utterance signals, and has been studied abroad for its further development on friendly human-machine interfaces. In this work, the discriminant patterns of audio emotions are conducted as multiresolution analysis of utterance signals, and fractal dimension features are calculated for acoustic feature extraction. Furthermore, it is able to efficiently learn intrinsic characteristics of auditory emotions, while the utterance features are learned from fractal dimensions of each sub-bands. Experimental results show the proposed method is able to provide comparative performance for audio emotion recognition.", "creator": "LaTeX with hyperref package"}}}