{"id": "1607.00424", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Learning Relational Dependency Networks for Relation Extraction", "abstract": "We considering came task especially KBP slot coals - - liquefying relation mail with newswire documents giving solely reserve construction. We years certainly siberia, several 15,000 Relational Dependency Networks (RDNs) as they linguistic changing for aspect deposits. Additionally, obviously necessity imagine notably components such instead weak supervision, word2vec elements, efforts learning well for allows main cases done, can be 1893 year this relational structure. We evaluate time different modified 2004 time unchanged KBP 2015 task took tv that RDNs power simplified a diverse seven of page when roles competitively left following administrative - include - as - theater relation extraction.", "histories": [["v1", "Fri, 1 Jul 2016 22:11:38 GMT  (231kb,D)", "http://arxiv.org/abs/1607.00424v1", "In Proceedings of Sixth International Workshop on Statistical Relational AI at the 25th International Joint Conference on Artificial Intelligence (IJCAI)"]], "COMMENTS": "In Proceedings of Sixth International Workshop on Statistical Relational AI at the 25th International Joint Conference on Artificial Intelligence (IJCAI)", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["dileep viswanathan", "ameet soni", "jude shavlik", "sriraam natarajan"], "accepted": false, "id": "1607.00424"}, "pdf": {"name": "1607.00424.pdf", "metadata": {"source": "CRF", "title": "Learning Relational Dependency Networks for Relation Extraction", "authors": ["Dileep Viswanathan", "Ameet Soni", "Jude Shavlik", "Sriraam Natarajan"], "emails": ["diviswan@indiana.edu", "soni@cs.swarthmore.edu", "shavlik@cs.wisc.edu", "natarasr@indiana.edu"], "sections": [{"heading": "Introduction", "text": "The problem of knowledge base population (KBP) \u2013 constructing a knowledge base (KB) of facts gleaned from a large corpus of unstructured data \u2013 poses several challenges for the NLP community. Commonly, this relation extraction task is decomposed into two subtasks \u2013 entity linking, in which entities are linked to already identified identities within the document or to entities in the existing KB, and slot filling, which identifies certain attributes about a target entity.\nWe present our work-in-progress for KBP slot filling based on our probabilistic logic formalisms and present the different components of the system. Specifically, we employ Relational Dependency Networks (Neville and Jensen 2007), a formalism that has been successfully used for joint learning and inference from stochastic, noisy, relational data. We consider our RDN system against the current stateof-the-art for KBP to demonstrate the effectiveness of our probabilistic relational framework.\nAdditionally, we show how RDNs can effectively incorporate many popular approaches in relation extraction such as joint learning, weak supervision, word2vec features, and human advice, among others. We provide a comprehensive comparison of settings such as joint learning vs learning of individual relations, use of weak supervision vs gold standard labels, using expert advice vs only learning from data, etc. These questions are extremely interesting from a general machine learning perspective, but also critical to the NLP community. As we show empirically, some of the re-\nsults such as human advice being useful in many relations and joint learning being beneficial in the cases where the relations are correlated among themselves are on the expected lines. However, some surprising observations include the fact that weak supervision is not as useful as expected and word2vec features are not as predictive as the other domain-specific features.\nWe first present the proposed pipeline with all the different components of the learning system. Next we present the set of 14 relations that we learn on before presenting the experimental results. We finally discuss the results of these comparisons before concluding by presenting directions for future research."}, {"heading": "Proposed Pipeline", "text": "We present the different aspects of our pipeline, depicted in Figure 1. We will first describe our approach to generating features and training examples from the KBP corpus, before describing the core of our framework \u2013 the RDN Boost algorithm."}, {"heading": "Feature Generation", "text": "Given a training corpus of raw text documents, our learning algorithm first converts these documents into a set of facts (i.e., features) that are encoded in first order logic\nar X\niv :1\n60 7.\n00 42\n4v 1\n[ cs\n.A I]\n1 J\nul 2\n01 6\n(FOL). Raw text is processed using the Stanford CoreNLP Toolkit1 (Manning et al. 2014) to extract parts-of-speech, word lemmas, etc. as well as generate parse trees, dependency graphs and named-entity recognition information. The full set of extracted features is listed in Table 1. These are then converted into features in prolog (i.e., FOL) format and are given as input to the system.\nIn addition to the structured features from the output of Stanford toolkit, we also use deeper features based on word2vec (Mikolov et al. 2013) as input to our learning system. Standard NLP features tend to treat words as individual objects, ignoring links between words that occur with similar meanings or, importantly, similar contexts (e.g., city-country pairs such as Paris \u2013 France and Rome \u2013 Italy occur in similar contexts). word2vec provide a continuous-space vector embedding of words that, in practice, capture many of these relationships (Mikolov et al. 2013; Mikolov, Yih, and Zweig 2013). We use word vectors from Stanford2 and Google3 along with a few specific words that, experts believe, are related to the relations learned. For example, we include words such as \u201cfather\u201d and \u201cmother\u201d (inspired by the parent relation) or \u201cdevout\u201d,\u201cconvert\u201d, and \u201cfollow\u201d (religion relation). We generated features from word vectors by finding words with high similarity in the embedded space. That is, we used word vectors by considering relations of the following\n1http://stanfordnlp.github.io/CoreNLP/ 2http://nlp.stanford.edu/projects/glove/ 3https://code.google.com/p/word2vec/\nform: similarWords(wordA,wordB,maxSim), where maxSim is the cosine similarity score between the words. Only the top cosine similarity scores for a word are utilized."}, {"heading": "Weak Supervision", "text": "One difficulty with the KBP task is that very few documents come labeled as gold standard labels, and further annotation is prohibitively expensive beyond a few hundred documents. This is problematic for discriminative learning algorithms, like the RDN learning algorithm, which excel when given a large supervised training corpus. To overcome this obstacle, we employ weak supervision \u2013 the use of external knowledge (e.g., a database) to heuristically label examples. Following our work in Soni et al. (2016), we employ two approaches for generating weakly supervised examples \u2013 distant supervision and knowledge-based weak supervision.\nDistant supervision entails the use of external knowledge (e.g., a database) to heuristically label examples. Following standard procedure, we use three data sources \u2013 Never Ending Language Learner (NELL) (Carlson et al. 2010), Wikipedia Infoboxes and Freebase. For a given target relation, we identify relevant database(s), where the entries in the database form entity pairs (e.g., an entry of (Barack Obama,Malia Obama) for a parent database) that will serve as a seed for positive training examples. These pairs must then be mapped to mentions in our corpus \u2013 that is, we must find sentences in our corpus that contain both entities together (Zhang et al. 2012). This process is done heuristically and is fraught with potential errors and noise (Riedel, Yao, and McCallum 2010).\nAn alternative approach, knowledge-based weak supervision is based on previous work (Natarajan et al. 2014; Soni et al. 2016) with the following insight: labels are typically created by \u201cdomain experts\u201d who annotate the labels carefully, and who typically employ some inherent rules in their mind to create examples. For example, when identifying family relationship, we may have an inductive bias towards believing two persons in a sentence with the same last name are related, or that the words \u201cson\u201d or \u201cdaughter\u201d are strong indicators of a parent relation. We call this world knowledge as it describes the domain (or the world) of the target relation.\nTo this effect, we encode the domain expert\u2019s knowledge in the form of first-order logic rules with accompanying weights to indicate the expert\u2019s confidence. We use the probabilistic logic formalism Markov Logic Networks (Domingos and Lowd 2009) to perform inference on unlabeled text (e.g., the TAC KBP corpus). Potential entity pairs from the corpus are queried to the MLN, yielding (weaklysupervised) positive examples. We choose MLNs as they permit domain experts to easily write rules while providing a probabilistic framework that can handle noise, uncertainty, and preferences while simultaneously ranking positive examples.\nWe use the Tuffy system (Niu et al. 2011) to perform inference. The inference algorithm implemented inside Tuffy appears to be robust and scales well to millions of docu-\nments4. For the KBP task, some rules that we used are shown in Table 2. For example, the first rule identifies any number following a person\u2019s name and separated by a comma is likely to be the person\u2019s age (e.g., \u201cSharon, 42\u201d). The third and fourth rule provide examples of rules that utilize more textual features; these rules state the appearance of the lemma \u201cmother\u201d or \u201cfather\u201d between two persons is indicative of a parent relationship (e.g.,\u201cMalia\u2019s father, Barack, introduced her...\u201d)."}, {"heading": "Learning Relational Dependency Networks", "text": "Previous research (Meza-Ruiz and Riedel 2009) has demonstrated that joint inferences of the relations are more effective than considering each relation individually. Consequently, we have considered a formalism that has been successfully used for joint learning and inference from stochastic, noisy, relational data called Relational Dependency Networks (RDNs) (Neville and Jensen 2007; Natarajan et al. 2010). RDNs extend dependency networks (DN) (Heckerman et al. 2001) to the relational setting. The key idea in a DN is to approximate the joint distribution over a set of random variables as a product of their marginal distributions, i.e., P (y1, ..., yn|X) \u2248 \u220f i P (yi|X). It has been shown that employing Gibbs sampling in the presence of a large amount of data allows this approximation to be particularly effective. Note that, one does not have to explicitly check for acyclicity making these DNs particularly easy to be learned.\nIn an RDN, typically, each distribution is represented by a relational probability tree (RPT) (Neville et al. 2003). However, following previous work (Natarajan et al. 2010), we replace the RPT of each distribution with a set of relational regression trees (Blockeel and Raedt 1998) built in a sequential manner i.e., replace a single tree with a set of gradient boosted trees. This approach has been shown to have stateof-the-art results in learning RDNs and we adapted boosting to learn for relation extraction. Since this method requires negative examples, we created negative examples by considering all possible combinations of entities that are not present in positive example set and sampled twice as many negatives as positive examples.\n4As the structure and weights are pre-defined by the expert, learning is not needed for our MLN"}, {"heading": "Incorporating Human Advice", "text": "While most relational learning methods restrict the human to merely annotating the data, we go beyond and request the human for advice. The intuition is that we as humans read certain patterns and use them to deduce the nature of the relation between two entities present in the text. The goal of our work is to capture such mental patterns of the humans as advice to the learning algorithm. We modified the work of Odom et al. (2015a; 2015b) to learn RDNs in the presence of advice. The key idea is to explicitly represent advice in calculating gradients. This allows the system to trade-off between data and advice throughout the learning phase, rather than only consider advice in initial iterations. Advice, in particular, become influential in the presence of noisy or less amout of data.\nA few sample advice rules in English (these are converted to first-order logic format and given as input to our algorithm) are presented in Table 3. Note that some of the rules are \u201csoft\u201d rules in that they are not true in many situations. Odom et al. (2015b) weigh the effect of the rules against the data and hence allow for partially correct rules."}, {"heading": "Experiments and Results", "text": "We now present our experimental evaluation. We considered 14 specific relations from two categories, person and organization from the TAC KBP competition. The relations considered are listed in the left column of Table 4. We utilize documents from KBP 2014 for training while utilizing documents from the 2015 corpus for testing.\nAll results presented are obtained from 5 different runs of the train and test sets to provide more robust estimates of accuracy. We consider three standard metrics \u2013 area under the ROC curve, F-1 score and the recall at a certain precision. We chose the precision as 0.66 since the fraction of positive examples to negatives is 1:2 (we sub-sampled the negative examples for the different training sets). Negative examples are re-sampled for each training run. It must be mentioned that not all relations had the same number of hand-annotated (gold standard) examples because the 781 documents that we annotated had different number of instances for these relations. The train/test gold-standard sizes are provided in the table, including weakly supervised examples, if available. Lastly, to control for other factors, the default setting for our experiments is individual learning, standard features, with gold standard examples only (i.e., no weak supervision, word2vec, advice, or advice).\nSince our system had different components, we aimed to answer the following questions: Q1: Do weakly supervised examples help construct better models? Q2: Does joint learning help in some relations? Q3: Are word2vec features more predictive than standard features presented in Table 1? Q4: Does advice improve performance compared to just learn-\ning from data?\nQ5: How does our system, that includes all the components, perform against a robust baseline (Relation Factory (Roth et al. 2014))?"}, {"heading": "Weak Supervision", "text": "To answer Q1, we generated positive training examples using the weak supervision techniques specified earlier. Specifically, we evaluated 10 relations as show in Table 5. Based on experiments from (Soni et al. 2016), we utilized our knowledge-based weak supervision approach to provide positive examples in all but two of our relations. A range of 4 to 8 rules are derived for each relation. Examples for the organization relations countryHQ and foundedBy were generated using standard distant supervision techniques \u2013 Freebase databases were mapped to foundedBy while Wikipedia Infoboxes provides entity pairs for countryHQ. Lastly, only 150 weakly supervised examples were utilized in our experiments (all gold standard examples were utilized). Performing larger runs is part of work in progress.\nThe results are presented in Table 5. We compared our standard pipeline (individually learned relations with only standard features) learned on gold standard examples only versus our system learned with weak and gold examples combined. Surprisingly, weak supervision does not seem to help learn better models for inferring relations in most cases. Only two relations \u2013 origin, otherFamily \u2013 see substantial improvements in AUC ROC, while F1 shows improvements for age and, otherFamily, and dateFounded. We hypothesize that generating more examples will help (some relations produced thousands of examples), but nonetheless find the lack of improved models from even a modest number of examples a surprising result. Alternatively, the number of gold standard examples provided may be sufficient to learn RDN models. Thus Q1 is answered equivocally, but in the negative."}, {"heading": "Joint learning", "text": "To address our next question, we assessed our pipeline when learning relations independently (i.e., individually) versus\nlearning relations jointly within the RDN, displayed in Table 6. Recall and F1 are omitted for conciseness \u2013 the conclusions are the same across all metrics. Joint learning appears to help in about half of the relations (8/14). Particularly, in person category, joint learning with gold standard outperforms their individual learning counterparts. This is due to the fact that some relations such as parents, spouse, siblings etc. are inter-related and learning them jointly indeed improves performance. Hence Q2 can be answered affirmatively for half the relations.\nword2vec Table 7 shows the results of experiments comparing the RDN framework with and without word2vec features. word2vec appears to largely have no impact, boosting results in just 4 relations. We hypothesize that this may be due to a limitation in the depth of trees learned. Learning more and/or deeper trees may improve use of word2vec features, and additional work can be done to generate deep features from word vectors. Q3 is answered cautiously in the negative, although future work could lead to improvements."}, {"heading": "Advice", "text": "Table 8 shows the results of experiments that test the use of advice within the joint learning setting. The use of advice improves or matches the performance of using only joint learning. The key impact of advice can be mostly seen in the improvement of recall in several relations. This clearly shows that using human advice patterns allows us to extract more relations effectively making up for noisy or less number of training examples. This is in-line with previously published machine learning literature (Towell and Shavlik 1994; Fung, Mangasarian, and Shavlik 2002; Kunapuli et al. 2013; Odom et al. 2015b) in that humans can be more than mere labelers by providing useful advice to learning algorithms that can improve their performance. Thus Q4 can be answered affirmatively."}, {"heading": "RDN Boost vs Relation Factory", "text": "Relation factory (RF) (Roth et al. 2014) is an efficient, open source system for performing relation extraction based on distantly supervised classifiers. It was the top system in the TAC KBP 2013 competition (Surdeanu 2013) and thus serves as a suitable baseline for our method. RF is very conservative in its responses, making it very difficult to adjust the precision levels. To be most generous to RF, we present recall for all returned results (i.e., score > 0). The AUC ROC, recall, and F1 scores of our system against RF are presented in Table 9.\nOur system performs comparably, and often better than the state-of-the-art Relation Factory system. In particular, our method outperforms Relation Factory in AUC ROC across all relations. Recall provides a more mixed picture with both approaches showing some improvements \u2013 RDN outperforms in 6 relations while Relation Factory does so in\n8. Note that in the instances where RDN provides superior recall, it does so with dramatic improvements (RF often returns 0 positives in these relations). F1 also shows RDN\u2019s superior performance, outperforming RF in most relations. Thus, the conclusion for Q5 is that our RDN framework performas comparably, if not better, across all metrics against the state-of-the-art."}, {"heading": "Conclusion", "text": "We presented our fully relational system utilizing Relational Dependency Networks for the Knowledge Base Population task. We demonstrated RDN\u2019s ability to effectively learn the relation extraction task, performing comparably (and often better) than the state-of-art Relation Factory system. Furthermore, we demonstrated the ability of RDNs to incorporate various concepts in a relational framework, including word2vec, human advice, joint learning, and weak supervision. Some surprising results are that weak supervision and word2vec did not significantly improve performance. However, advice is extremely useful thus validating the long-standing results inside the Artificial Intelligence community for the relation extraction task as well. Possible future directions include considering a larger number of relations, deeper features and finally, comparisons with more systems. We believe further work on developing word2vec features and utilizing more weak supervision examples may reveal further insights into how to effectively utilize such features in RDNs."}], "references": [{"title": "Top-down induction of first-order logical decision trees. Artificial intelligence 101(1):285\u2013297", "author": ["Blockeel", "H. Raedt 1998] Blockeel", "L.D. Raedt"], "venue": null, "citeRegEx": "Blockeel et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blockeel et al\\.", "year": 1998}, {"title": "Toward an architecture for never-ending language learning", "author": ["Carlson"], "venue": null, "citeRegEx": "Carlson,? \\Q2010\\E", "shortCiteRegEx": "Carlson", "year": 2010}, {"title": "Markov Logic: An Interface Layer for AI", "author": ["Domingos", "P. Lowd 2009] Domingos", "D. Lowd"], "venue": null, "citeRegEx": "Domingos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Domingos et al\\.", "year": 2009}, {"title": "Knowledge-Based support vector machine classifiers", "author": ["Mangasarian Fung", "G. Shavlik 2002] Fung", "O. Mangasarian", "J. Shavlik"], "venue": null, "citeRegEx": "Fung et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Fung et al\\.", "year": 2002}, {"title": "Dependency networks for inference, collaborative filtering, and data visualization", "author": ["Heckerman"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Heckerman,? \\Q2001\\E", "shortCiteRegEx": "Heckerman", "year": 2001}, {"title": "Guiding an autonomous agent to better behaviors through human advice", "author": ["Kunapuli"], "venue": "In ICDM", "citeRegEx": "Kunapuli,? \\Q2013\\E", "shortCiteRegEx": "Kunapuli", "year": 2013}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Manning"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "Manning,? \\Q2014\\E", "shortCiteRegEx": "Manning", "year": 2014}, {"title": "Jointly identifying predicates, arguments and senses using markov logic", "author": ["Meza-Ruiz", "I. Riedel 2009] Meza-Ruiz", "S. Riedel"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "Meza.Ruiz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meza.Ruiz et al\\.", "year": 2009}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov"], "venue": "Proceedings of Workshop at ICLR", "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Yih Mikolov", "T. Zweig 2013] Mikolov", "W. Yih", "G. Zweig"], "venue": "Proceedings of NAACL HLT", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Boosting relational dependency networks", "author": ["Natarajan"], "venue": "In Proceedings of the International Conference on Inductive Logic Programming (ILP)", "citeRegEx": "Natarajan,? \\Q2010\\E", "shortCiteRegEx": "Natarajan", "year": 2010}, {"title": "Effectively creating weakly labeled training examples via approximate domain knowledge", "author": ["K. Kersting", "C. Re", "J. Shavlik"], "venue": "International Conference on Inductive Logic Programming.", "citeRegEx": "Kersting et al\\.,? 2014", "shortCiteRegEx": "Kersting et al\\.", "year": 2014}, {"title": "Relational dependency networks. In Introduction to Statistical Relational Learning", "author": ["Neville", "J. Jensen 2007] Neville", "D. Jensen"], "venue": null, "citeRegEx": "Neville et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Neville et al\\.", "year": 2007}, {"title": "Learning relational probability trees", "author": ["Neville"], "venue": "Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "Neville,? \\Q2003\\E", "shortCiteRegEx": "Neville", "year": 2003}, {"title": "Tuffy: Scaling up statistical inference in Markov logic networks using an RDBMS", "author": ["Niu"], "venue": "Proceedings of Very Large Data Bases (PVLDB)", "citeRegEx": "Niu,? \\Q2011\\E", "shortCiteRegEx": "Niu", "year": 2011}, {"title": "Extracting adverse drug events from text using human advice", "author": ["Odom"], "venue": "In Artificial Intelligence in Medicine (AIME)", "citeRegEx": "Odom,? \\Q2015\\E", "shortCiteRegEx": "Odom", "year": 2015}, {"title": "2015b. Knowledge-based probabilistic logic learning", "author": ["Odom"], "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Odom,? \\Q2015\\E", "shortCiteRegEx": "Odom", "year": 2015}, {"title": "Modeling relations and their mentions without labeled text. In Proceedings of the 2010 European conference on Machine learning and knowledge discovery in databases (ECML KDD)", "author": ["Yao Riedel", "S. McCallum 2010] Riedel", "L. Yao", "A. McCallum"], "venue": null, "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Relationfactory: A fast, modular and effective system for knowledge base population", "author": ["Roth"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Roth,? \\Q2014\\E", "shortCiteRegEx": "Roth", "year": 2014}, {"title": "A comparison of weak supervision methods for knowledge base construction", "author": ["Soni"], "venue": "In 5th Workshop on Automated Knowledge Base Construction (AKBC) at NAACL", "citeRegEx": "Soni,? \\Q2016\\E", "shortCiteRegEx": "Soni", "year": 2016}, {"title": "Knowledge-based artificial neural networks. Artif", "author": ["Towell", "G. Shavlik 1994] Towell", "J. Shavlik"], "venue": null, "citeRegEx": "Towell et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Towell et al\\.", "year": 1994}, {"title": "Big data versus the crowd: Looking for relationships in all the right places", "author": ["Zhang"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1,", "citeRegEx": "Zhang,? \\Q2012\\E", "shortCiteRegEx": "Zhang", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "In addition to the structured features from the output of Stanford toolkit, we also use deeper features based on word2vec (Mikolov et al. 2013) as input to our learning system.", "startOffset": 122, "endOffset": 143}, {"referenceID": 9, "context": "word2vec provide a continuous-space vector embedding of words that, in practice, capture many of these relationships (Mikolov et al. 2013; Mikolov, Yih, and Zweig 2013).", "startOffset": 117, "endOffset": 168}, {"referenceID": 19, "context": "Following our work in Soni et al. (2016), we employ two approaches for generating weakly supervised examples \u2013 distant supervision and knowledge-based weak supervision.", "startOffset": 22, "endOffset": 41}, {"referenceID": 15, "context": "Odom et al. (2015b) weigh the effect of the rules against the data and hence allow for partially correct rules.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "We consider the task of KBP slot filling \u2013 extracting relation information from newswire documents for knowledge base construction. We present our pipeline, which employs Relational Dependency Networks (RDNs) to learn linguistic patterns for relation extraction. Additionally, we demonstrate how several components such as weak supervision, word2vec features, joint learning and the use of human advice, can be incorporated in this relational framework. We evaluate the different components in the benchmark KBP 2015 task and show that RDNs effectively model a diverse set of features and perform competitively with current state-of-the-art relation extraction.", "creator": "LaTeX with hyperref package"}}}