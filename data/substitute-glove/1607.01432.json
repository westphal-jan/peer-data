{"id": "1607.01432", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2016", "title": "Global Neural CCG Parsing with Optimality Guarantees", "abstract": "We introduce an first continuing recursive actuation iterative model with tresca guarantees during synchronizing. To support global features, just find and core colleges it only search instead in the tower brought all possible subtrees. Although this mars form multiplies large of the sentence length, anything saw one way possibly to imagine his utilize A * ansatz. We much-needed provides parsing wheel, other had refreshingly bounds began the outside gave, with same boost model why has loose bounds instead first putting soon example non - local evolution. The aim model is trained with a which therefore yet encourages the parser over efforts right along fraction mainly as taken apart. The methods instance studies be CCG comprehension, improves state - large - present - aesthetics conclusive by 1-3. november F1. The linearization finds several optimal parse own 79. (% especially weeks - out prosecutors, innovative on rate with 153 subtrees.", "histories": [["v1", "Tue, 5 Jul 2016 22:25:10 GMT  (36kb,D)", "https://arxiv.org/abs/1607.01432v1", null], ["v2", "Sat, 24 Sep 2016 01:41:43 GMT  (33kb,D)", "http://arxiv.org/abs/1607.01432v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kenton lee", "mike lewis", "luke zettlemoyer"], "accepted": true, "id": "1607.01432"}, "pdf": {"name": "1607.01432.pdf", "metadata": {"source": "CRF", "title": "Global Neural CCG Parsing with Optimality Guarantees", "authors": ["Kenton Lee", "Mike Lewis", "Luke Zettlemoyer"], "emails": ["kentonl@cs.washington.edu", "mlewis@cs.washington.edu", "lsz@cs.washington.edu"], "sections": [{"heading": null, "text": "We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a novel objective that encourages the parser to search both efficiently and accurately. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees."}, {"heading": "1 Introduction", "text": "Recursive neural models perform well for many structured prediction problems, in part due to their ability to learn representations that depend globally on all parts of the output structures. However, global models of this sort are incompatible with existing exact inference algorithms, since they do not decompose over substructures in a way that allows effective dynamic programming. Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015; Dyer et al., 2015) or reranking (Socher et al., 2013). We introduce the first global recursive neural parsing approach\nwith optimality guarantees for decoding and use it to build a state-of-the-art CCG parser.\nTo enable learning of global representations, we modify the parser to search directly in the space of all possible parse trees with no dynamic programming. Optimality guarantees come from A\u2217 search, which provides a certificate of optimality if run to completion with a heuristic that is a bound on the future cost. Generalizing A\u2217 to global models is challenging; these models also break the locality assumptions used to efficiently compute existing A\u2217 heuristics (Klein and Manning, 2003; Lewis and Steedman, 2014). Rather than directly replacing local models, we show that they can simply be augmented by adding a score from a global model that is constrained to be non-positive and has a trivial upper bound of zero. The global model, in effect, only needs to model the remaining non-local phenomena. In our experiments, we use a recent factored A\u2217 CCG parser (Lewis et al., 2016) for the local model, and we train a Tree-LSTM (Tai et al., 2015) to model global structure.\nFinding a model that achieves these A\u2217 guarantees in practice is a challenging learning problem. Traditional structured prediction objectives focus on ensuring that the gold parse has the highest score (Collins, 2002; Huang et al., 2012). This condition is insufficient in our case, since it does not guarantee that the search will terminate in subexponential time. We instead introduce a new objective that optimizes efficiency as well as accuracy. Our loss function is defined over states of the A\u2217 search agenda, and it penalizes the model whenever the top agenda item is not a part of the gold parse.\nar X\niv :1\n60 7.\n01 43\n2v 2\n[ cs\n.C L\n] 2\n4 Se\np 20\nMinimizing this loss encourages the model to return the correct parse as quickly as possible.\nThe combination of global representations and optimal decoding enables our parser to achieve state-of-the-art accuracy for Combinatory Categorial Grammar (CCG) parsing. Despite being intractable in the worst case, the parser in practice is highly efficient. It finds optimal parses for 99.9% of held out sentences while exploring just 190 subtrees on average\u2014allowing it to outperform beam search in both speed and accuracy."}, {"heading": "2 Overview", "text": "Parsing as hypergraph search Many parsing algorithms can be viewed as a search problem, where parses are specified by paths through a hypergraph.\nA node y in this hypergraph is a labeled span, representing structures within a parse tree, as shown in Figure 1. Each hyperedge e in the hypergraph represents a rule production in a parse. The head node\nof the hyperedge HEAD(e) is the parent of the rule production, and the tail nodes of the hyperedge are the children of the rule production. For example, consider the hyperedge in Figure 1b whose head is like bananas. This hyperedge represents a forward application rule applied to its tails, like and bananas.\nTo define a path in the hypergraph, we first include a special start node \u2205 that represents an empty parse. \u2205 has outgoing hyperedges that reach every leaf node, representing assignments of labels to words (supertag assignments in Figure 1). We then define a path to be a set of hyperedges E starting at \u2205 and ending at a single destination node. A path therefore specifies the derivation of the parse constructed from the labeled spans at each node. For example, in Figure 1, the set of bolded hyperedges form a path deriving a complete parse.\nEach hyperedge e is weighted by a score s(e) from a parsing model. The score of a path E is the\nsum of its hyperedge scores: g(E) = \u2211 e\u2208E s(e)\nViterbi decoding is equivalent to finding the highest scoring path that forms a complete parse.\nSearch on parse forests Traditionally, the hypergraph represents a packed parse chart. In this work, our hypergraph instead represents a forest of parses. Figure 1 contrasts the two representations.\nIn the parse chart, labels on the nodes represent local properties of a parse, such as the category of a span in Figure 1a. As a result, multiple parses that contain the same property include the same node in their path, (e.g. the node spanning the phrase Fruit flies with category NP). The number of nodes in this hypergraph is polynomial in the sentence length, permitting exhaustive exploration (e.g. CKY parsing). However, the model scores can only depend on local properties of a parse. We refer to these models as locally factored models.\nIn contrast, nodes in the parse forest are labeled with entire subtrees, as shown in Figure 1b. For example, there are two nodes spanning the phrase Fruit flies with the same category NP but different internal substructures. While the parse forest requires an exponential number of nodes in the hypergraph, the model scores can depend on entire subtrees.\nA\u2217 parsing A\u2217 parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016). We present a special case of A\u2217 parsing that is conceptually simpler, since the parse forest constrains each node to be reachable via a unique path. During exploration, we maintain the unique (and therefore highest scoring) path to a hyperedge e, which we define as PATH(e).\nSimilar to the standard A\u2217 search algorithm, we maintain an agenda A of hyperedges to explore and a forest F of explored nodes that initially contains only the start node \u2205.\nEach hyperedge e in the agenda is sorted by the sum of its inside score g(PATH(e)) and an admissible heuristic h(e). A heuristic h(e) is admissible if it is an upper bound of the sum of hyperedge scores leading to any complete parse reachable from e (the Viterbi outside score). The efficiency of the search\nimproves when this bound is tighter. At every step, the parser removes the top of the agenda, emax = argmaxe\u2208A(g(PATH(e)) + h(e)). emax is expanded by combining HEAD(emax) with previously explored parses from F to form new hyperedges. These new hyperedges are inserted into A, and HEAD(emax) is added it to F . We repeat these steps until the first complete parse y\u2217 is explored. The bounds provided by h(e) guarantee that the path to y\u2217 has the highest possible score. Figure 1b shows an example of the agenda and the explored forest at the end of perfectly efficient search, where only the optimal path is explored.\nApproach The enormous search space described above presents a challenge for an A\u2217 parser, since computing a tight and admissible heuristic is difficult when the model does not decompose locally.\nOur key insight in addressing this challenge is that existing locally factored models with an informative A\u2217 heuristic can be augmented with a global score (Section 3). By constraining the global score to be non-positive, the A\u2217 heuristic from the locally factored model is still admissible.\nWhile the heuristic from the local model offers some estimate of the future cost, the efficiency of the parser requires learning a well-calibrated global score, since the heuristic becomes looser as the global score provides stronger penalties (Section 5).\nAs we explore the search graph, we incrementally construct a neural network, which computes representations of the parses and allows backpropagation of errors from bad search steps (Section 4).\nIn the following sections, we present our approach in detail, assuming an existing locally factored model slocal(e) for which we can efficiently compute an admissible A\u2217 heuristic h(e).\nIn the experiments, we apply our model to CCG parsing, using the locally factored model and A\u2217 heuristic from Lewis et al. (2016)."}, {"heading": "3 Model", "text": "Our model scores a hyperedge e by combining the score from the local model with a global score that conditions on the entire parse at the head node:\ns(e) = slocal(e) + sglobal(e)\nIn sglobal(e), we first compute a hidden representation encoding the parse structure of y = HEAD(e). We use a variant of the Tree-LSTM (Tai et al., 2015) connected to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) at the leaves. The combination of linear and tree LSTMs allows the hidden representation of partial parses to condition on both the partial structure and the full sentence. Figure 2 depicts the neural network that computes the hidden representation for a parse.\nFormally, given a sentence \u3008w1, w2, . . . , wn\u3009, we compute hidden states ht and cell states ct in the forward LSTM for 1 < t \u2264 n:\nit =\u03c3(Wi[ct\u22121, ht\u22121, xt] + bi)\not =\u03c3(Wo[c\u0303t, ht\u22121, xt] + bo)\nc\u0303t =tanh(Wc[ht\u22121, xt] + bc) ct =it \u25e6 c\u0303t + (1\u2212 it) \u25e6 ct\u22121 ht =ot \u25e6 tanh(ct)\nwhere \u03c3 is the logistic sigmoid, \u25e6 is the componentwise product, and xt denotes a learned word embedding for wt. We also construct a backward LSTM, which produces the analogous hidden and cell states starting at the end of the sentence, which we denote as c\u2032t and h \u2032 t respectively. The start and end latent states, c\u22121, h\u22121, c\u2032n+1, and h \u2032 n+1, are learned embeddings. This variant of the LSTM includes peephole connections and couples the input and forget gates.\nThe bidirectional LSTM over the words serves as a base case when we recursively compute a hidden representation for the parse y using the treestructured generalization of the LSTM:\niy = \u03c3(W R i [cl, hl, cr, hr, xy] + b R i ) fy = \u03c3(W R f [cl, hl, cr, hr, xy] + b R f ) oy = \u03c3(W R o [c\u0303y, hl, hr, xy] + b R o ) clr = fy \u25e6 cl + (1\u2212 fy) \u25e6 cr c\u0303y = tanh(W R c [hl, hr, xy] + b R c ) cy = iy \u25e6 c\u0303y + (1\u2212 iy) \u25e6 clr hy = oy \u25e6 tanh(cy)\nwhere the weights and biases are parametrized by the rule R that produces y from its children, and xy denotes a learned embedding for the category at the root of y. For example, in CCG, the rule would correspond to the CCG combinator, and the label would\ncorrespond to the CCG category. We assume that nodes are binary, unary, or leaves. Their left and right latent states, cl, hl, cr, and hr are defined as follows:\n\u2022 In a binary node, cl and hl are the cell and hidden states of the left child, and cr and hr are the cell and hidden states of the right child. \u2022 In a unary node, cl and hl are learned embed-\ndings, and cr and hr are the cell and hidden states of the singleton child. \u2022 In a leaf node, let w denote the index of the\ncorresponding word. Then cl and hl are cw and hw from the forward LSTM, and cr and hr are c\u2032w and h \u2032 w from the backward LSTM.\nThe cell state of the recursive unit is a linear combination of the intermediate cell state c\u0303y, the left cell state cl, and the right cell state cr. To preserve the normalizing property of coupled gates, we perform coupling in a hierarchical manner: the input gate iy decides the weights for c\u0303y, and the forget gate fy shares the remaining weights between cl and cr.\nGiven the hidden representation hy at the root, we score the global component as follows:\nsglobal(e) = log(\u03c3(W \u00b7 hy))\nThis definition of the global score ensures that it is non-positive\u2014an important property for inference."}, {"heading": "4 Inference", "text": "Using the hyperedge scoring model s(e) described in Section 3, we can find the highest scoring path that derives a complete parse tree by using the A\u2217 parsing algorithm described in Section 2.\nFruit flies like bananas NP/NP NP (S\\NP )/NP NP > >\nNP S\\NP <\nS\nFruit flies like bananas NP/NP NP (S\\NP )/NP NP > >\nNP S\\NP <\nS\nAdmissible A\u2217 heuristic Since our full model adds non-positive global scores to the existing local scores, path scores under the full model cannot be greater than path scores under the local model. Upper bounds for path scores under the local model also hold for path scores under the full model, and we simply reuse the A\u2217 heuristic from the local model to guide the full model during parsing without sacrificing optimality guarantees.\nIncremental neural network construction The recursive hidden representations used in sglobal(e) can be computed in constant time during parsing. When scoring a new hyperedge, its children must have been previously scored. Instead of computing the full recursion, we reuse the existing latent states of the children and compute sglobal(e) with an incremental forward pass over a single recursive unit in the neural network. By maintain the latent states of each parse, we incrementally build a single DAGstructured LSTM mirroring the explored subset of the hypergraph. This not only enables quick forward passes during decoding, but also allows backpropagation through the search space after decoding, which is crucial for efficient learning (see Section 5).\nLazy global scoring The global score is expensive to compute. We introduce an optimization to avoid computing it when provably unnecessary. We split each hyperedge e into two successive hyperedges, elocal and eglobal, as shown in Figure 3. The score for e, previously s(e) = slocal(e) + sglobal(e), is\nalso split between the two new hyperedges:\ns(elocal) = slocal(elocal)\ns(eglobal) = sglobal(eglobal)\nIntuitively, this transformation requires A\u2217 to verify that the local score is good enough before computing the global score, which requires an incremental forward pass over a recursive unit in the neural network. In the example, this involves first summing the supertag scores of Fruit and flies and inserting the result back into the agenda. The score for applying the forward application rule to the recursive representations is only computed if that item appears again at the head of the agenda. In practice, the lazy global scoring reduces the number of recursive units by over 91%, providing a 2.4X speed up."}, {"heading": "5 Learning", "text": "During training (Algorithm 1), we assume access to sentences labeled with gold parse trees y\u0302 and gold derivations E\u0302. The gold derivation E\u0302 is a path from \u2205 to y\u0302 in the parse forest.\nA\u2217 search with our global model is not guaranteed to terminate in sub-exponential time. This creates challenges for learning\u2014for example, it is not possible in practice to use the standard structured perceptron update (Collins, 2002), because the search procedure rarely terminates early in training. Other common loss functions assume inexact search (Huang et al., 2012), and do not optimize efficiency.\nInstead, we optimize a new objective that is tightly coupled with the search procedure. During parsing, we would like hyperedges from the gold derivation to appear at the top of the agenda A. When this condition does not hold, A\u2217 is searching inefficiently, and we refer to this as a violation of the agenda, which we formally define as:\nv(E\u0302,A) = max e\u2208A (g(PATH(e)) + h(e))\n\u2212 max e\u2208A\u2229E\u0302 (g(PATH(e)) + h(e))\nwhere g(PATH(e)) is the score of the unique path to e, and h(e) is the A\u2217 heuristic. If all violations are zero, we find the gold parse without exploring any incorrect partial parses\u2014maximizing both accuracy and efficiency. Figure 1b shows such a case\u2014if any other nodes were explored, they would be violations.\nIn existing work on violation-based updates, comparisons are only made between derivations with the same number of steps (Huang et al., 2012; Clark et al., 2015)\u2014whereas our definition allows subtrees of arbitrary spans to compete with each other, because hyperedges are not explored in a fixed order. Our violations also differ from Huang et al.\u2019s in that we optimize efficiency as well as accuracy.\nWe define loss functions over these violations, which are minimized to encourage correct and efficient search. During training, we parse each sentence until either the gold parse is found or we reach computation limits. We record V , the list of nonzero violations of the agenda A observed:\nV = \u3008v(E\u0302,A) | v(E\u0302,A) > 0\u3009\nWe can optimize several loss functions over V , as defined in Table 1. The greedy and max-violation updates are roughly analogous to the violationfixing updates proposed by Huang et al. (2012), but adapted to exact agenda-based parsing. We also introduce a new all-violations update, which minimizes the sum of all observed violations. The allviolations update encourages correct parses to be explored early (similar to the greedy update) while being robust to parses with multiple deviations from the gold parse (similar to the max-violation update).\nThe violation losses are optimized with subgradient descent and backpropagation. For our experiments, slocal(e) and h(e) are kept constant. Only the parameters \u03b8 of sglobal(e) are updated. Therefore, a subgradient of a violation v(E\u0302,A) can be computed by summing subgradients of the global scores.\n\u2202v(E\u0302,A) \u2202\u03b8\n= \u2211\ne\u2208PATH(emax)\n\u2202sglobal(e)\n\u2202\u03b8 \u2212 \u2211 e\u2208PATH(e\u0302max) \u2202sglobal(e) \u2202\u03b8\nwhere emax denotes the hyperedge at the top of the agenda A and e\u0302max denotes the hyperedge in the gold derivation E\u0302 that is closest to the top of A.\nAlgorithm 1 Violation-based learning algorithm Definitions D is the training data containing input sentences x and gold derivations E\u0302. e variables denote scored hyperedges. TAG(x) returns a set of scored pre-terminals for every word. ADD(F , y) adds partial parse y to forest F . RULES(F , y) returns the set of scored hyperedges that can be created by combining y with entries in F . SIZE OK(F ,A) returns whether the sizes of the forest and agenda are within predefined limits. 1: function VIOLATIONS(E\u0302, x, \u03b8) 2: V \u2190 \u2205 . Initialize list of violations V 3: F \u2190 \u2205 . Initialize forest F 4: A\u2190 \u2205 . Initialize agenda A 5: for e \u2208 TAG(x) do 6: PUSH(A, e) 7: while |A \u2229 E\u0302| > 0 and SIZE OK(F ,A) do 8: if v(E\u0302,A) > 0 then 9: APPEND(V, v(E\u0302,A)) . Record violation 10: emax\u2190 EXTRACT MAX(A) . Pop agenda 11: ADD(F , HEAD(emax)) . Explore hyperedge 12: for e \u2208 RULES(F , HEAD(emax), \u03b8) do 13: PUSH(A, e) . Expand hyperedge 14: return V 15: 16: function LEARN(D) 17: for i = 1 to T do 18: for x, E\u0302 \u2208 D do 19: V \u2190 VIOLATIONS(E\u0302, x, \u03b8) 20: L\u2190 LOSS(V) 21: \u03b8\u2190 OPTIMIZE(L, \u03b8) 22: return \u03b8"}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Data", "text": "We trained our parser on Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), using Section 00 for development and Section 23 for test. To recover a single gold derivation for each sentence to use during training, we find the right-most branching parse that satisfies the gold dependencies."}, {"heading": "6.2 Experimental Setup", "text": "For the local model, we use the supertag-factored model of Lewis et al. (2016). Here, slocal(e) corresponds to a supertag score if a HEAD(e) is a leaf and zero otherwise. The outside score heuristic is computed by summing the maximum supertag score for every word outside of each span. In the reported results, we back off to the supertag-factored model after the forest size exceeds 500,000, the agenda size exceeds 2 million, or we build more than 200,000 recursive units in the neural network.\nOur full system is trained with all-violations updates. During training, we lower the forest size limit to 2000 to reduce training times. The model is trained for 30 epochs using ADAM (Kingma and Ba, 2014), and we use early stopping based on development F1. The LSTM cells and hidden states have 64 dimensions. We initialize word representations with pre-trained 50-dimensional embeddings from Turian et al. (2010). Embeddings for categories have 16 dimensions and are randomly initialized. We also apply dropout with a probability of 0.4 at the word embedding layer during training. Since the structure of the neural network is dynamically determined, we do not use mini-batches. The neural networks are implemented using the CNN library,1 and the CCG parser is implemented using the EasySRL library.2 The code is available online.3"}, {"heading": "6.3 Baselines", "text": "We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al., 2015), which is the C&C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser; Vaswani et al. (2016) who combine a bidirectional LSTM supertagger with a beam search parser using global features (Clark et al., 2015); and supertag-factored (Lewis et al., 2016), which uses deterministic A\u2217 decoding and an LSTM supertagging model."}, {"heading": "6.4 Parsing Results", "text": "Table 2 shows parsing results on the test set. Our global features let us improve over the supertagfactored model by 0.6 F1. Vaswani et al. (2016) also\n1https://github.com/clab/cnn 2https://github.com/mikelewis0/EasySRL 3https://github.com/kentonl/neuralccg\nuse global features, but our optimal decoding leads to an improvement of 0.4 F1.\nAlthough we observed an overall improvement in parsing performance, the supertag accuracy was not significantly different after applying the parser.\nOn the test data, the parser finds the optimal parse for 99.9% sentences before reaching our computational limits. On average, we parse 27.1 sentences per second,4 while exploring only 190.2 subtrees."}, {"heading": "6.5 Model Ablations", "text": "We ablate various parts of the model to determine how they contribute to the accuracy and efficiency of the parser, as shown in Table 3. For each model, the comparisons include the average number of parses explored and the percentage of sentences for which an optimal parse can be found without backing off.\nStructure ablation We first ablate the global score, sglobal(y), from our model, thus relying entirely on the local supertag-factors that do not explicitly model the parse structure. This ablation allows dynamic programming and is equivalent to the backoff model (supertag-factored in Table 3). Surprisingly, even in the exponentially larger search space, the global model explores fewer nodes than the supertag-factored model\u2014showing that the global model efficiently prune large parts of the search space. This effect is even larger when not using dynamic programming in the supertag-factored model.\nGlobal structure ablation To examine the importance of global features, we ablate the recursive hidden representation (span-factored in Table 3). The model in this ablation decomposes over labels for\n4We use a single 3.5GHz CPU core.\nspans, as in Durrett and Klein (2015). In this model, the recursive unit uses, instead of latent states from its children, the latent states of the backward LSTM at the start of the span and the latent states of the forward LSTM at the end of the span. Therefore, this model encodes the lexical information available in the full model but does not encode the parse structure beyond the local rule production. While the dynamic program allows this model to find the optimal parse with fewer explorations, the lack of global features significantly hurts its parsing accuracy.\nLexical ablation We also show lexical ablations instead of structural ablations. We remove the bidirectional LSTM at the leaves, thus delexicalizing the global model. This ablation degrades both accuracy and efficiency, showing that the model uses lexical information to discriminate between parses.\nTo understand the importance of contextual information, we also perform a partial lexical ablation by using word embeddings at the leaves instead of the bidirectional LSTM, thus propagating only lexical information from within the span of each parse. The degradation in F1 is about half of the degradation from the full lexical ablation, suggesting that a significant portion of the lexical cues comes from the context of a parse. Figure 4 illustrates the importance of context with an incorrect partial parse that appears syntactically plausible in isolation. These bottom-up garden paths are typically problematic for parsers, since their incompatibility with the remaining sentence is difficult to recognize until later stages of decoding. However, our global model learns to heavily penalize these garden paths by using the context provided by the bidirectional LSTM\nand avoid paths that lead to dead ends or bad regions of the search space."}, {"heading": "6.6 Update Comparisons", "text": "Table 4 compares the different violation-based learning objectives, as discussed in Section 5. Our novel all-violation updates outperform the alternatives. We attribute this improvement to the robustness over poor search spaces, which the greedy update lacks, and the incentive to explore good parses early, which the max-violation update lacks. Learning curves in Figure 5 show that the all-violations update also converges more quickly."}, {"heading": "6.7 Decoder Comparisons", "text": "Lastly, to show that our parser is both more accurate and efficient than other decoding methods, we decode our full model using best-first search, reranking, and beam search. Table 5 shows the F1 scores with and without the backoff model, the portion of the sentences that each decoder is able to parse, and the time spent decoding relative to the A\u2217 parser.\nIn the best-first search comparison, we do not include the informative A\u2217 heuristic, and the parser completes very few parses before reaching computational limits\u2014showing the importance of heuristics in large search spaces. In the reranking comparison,\nwe obtain n-best lists from the backoff model and rerank each result with the full model. In the beam search comparison, we use the approach from Clark et al. (2015) which greedily finds the top-n parses for each span in a bottom-up manner. Results indicate that both approximate methods are less accurate and slower than A\u2217."}, {"heading": "7 Related Work", "text": "Many structured prediction problems are based around dynamic programs, which are incompatible with recursive neural networks because of their realvalued latent variables. Some recent models have neural factors (Durrett and Klein, 2015), but these cannot condition on global parse structure, making them less expressive. Our search explores fewer nodes than dynamic programs, despite an exponentially larger search space, by allowing the recursive neural network to guide the search.\nPrevious work on structured prediction with recursive or recurrent neural models has used beam search\u2013e.g. in shift reduce parsing (Dyer et al., 2015), string-to-tree transduction (Vinyals et al., 2015), or reranking (Socher et al., 2013)\u2013at the cost of potentially recovering suboptimal solutions. For our model, beam search is both less efficient and less accurate than optimal A\u2217 decoding. In the non-neural setting, Zhang et al. (2014) showed that global features with greedy inference can improve dependency parsing. The CCG beam search parser of Clark et al. (2015), most related to this work, also uses global features. By using neural representations and exact search, we improve over their results.\nA\u2217 parsing has been previously proposed for lo-\ncally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014). We generalize these methods to enable global features. Vaswani and Sagae (2016) apply best-first search to an unlabeled shift-reduce parser. Their use of error states is related to our global model that penalizes local scores. We demonstrated that best-first search is infeasible in our setting, due to the larger search space.\nA close integration of learning and decoding has been shown to be beneficial for structured prediction. SEARN (Daume\u0301 III et al., 2009) and DAGGER (Ross et al., 2011) learn greedy policies to predict structure by sampling classification examples over actions from single states. We similarly generate classification examples over hyperedges in the agenda, but actions from multiple states compete against each other. Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daume\u0301 III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear."}, {"heading": "8 Conclusion", "text": "We have shown for the first time that a parsing model with global features can be decoded with optimality guarantees. This enables the use of powerful recursive neural networks for parsing without resorting to approximate decoding methods. Experiments show that this approach is effective for CCG parsing, resulting in a new state-of-the-art parser. In future work, we will apply our approach to other structured prediction tasks, where neural networks\u2014and greedy beam search\u2014have become ubiquitous."}, {"heading": "Acknowledgements", "text": "We thank Luheng He, Julian Michael, and Mark Yatskar for valuable discussion, and the anonymous reviewers for feedback and comments.\nThis work was supported by the NSF (IIS1252835, IIS-1562364), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google."}], "references": [{"title": "Globally Normalized Transition-Based Neural Networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of the 54th Annual Meeting of the Association for", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Efficient CCG parsing: A* versus Adaptive Supertagging", "author": ["Michael Auli", "Adam Lopez."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1.", "citeRegEx": "Auli and Lopez.,? 2011", "shortCiteRegEx": "Auli and Lopez.", "year": 2011}, {"title": "Widecoverage Efficient Statistical Parsing with CCG and Log-Linear Models", "author": ["Stephen Clark", "James R Curran."], "venue": "Computational Linguistics, 33(4).", "citeRegEx": "Clark and Curran.,? 2007", "shortCiteRegEx": "Clark and Curran.", "year": 2007}, {"title": "The Java Version of the C&C Parser: Version 0.95", "author": ["Stephen Clark", "Darren Foong", "Luana Bulat", "Wenduan Xu"], "venue": "Technical report,", "citeRegEx": "Clark et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2015}, {"title": "Incremental Parsing with the Perceptron Algorithm", "author": ["Michael Collins", "Brian Roark."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 111. Association for Computational Linguistics.", "citeRegEx": "Collins and Roark.,? 2004", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["Michael Collins."], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Association for", "citeRegEx": "Collins.,? 2002", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Learning as search optimization: Approximate Large Margin Methods for Structured Prediction", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 169\u2013176. ACM.", "citeRegEx": "III and Marcu.,? 2005", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Search-based structured prediction", "author": ["Hal Daum\u00e9 III", "John Langford", "Daniel Marcu."], "venue": "Machine learning, 75(3):297\u2013325.", "citeRegEx": "III et al\\.,? 2009", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Neural CRF Parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "Transitionbased Dependency Parsing with Stack Long ShortTerm Memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proc. ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Long Short-term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "CCGbank: a Corpus of CCG derivations and Dependency Structures Extracted from the Penn Treebank", "author": ["Julia Hockenmaier", "Mark Steedman."], "venue": "Computational Linguistics, 33(3).", "citeRegEx": "Hockenmaier and Steedman.,? 2007", "shortCiteRegEx": "Hockenmaier and Steedman.", "year": 2007}, {"title": "Structured Perceptron with Inexact Search", "author": ["Liang Huang", "Suphan Fayong", "Yang Guo."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A* Parsing: Fast Exact Viterbi Parse Selection", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "A* CCG Parsing with a Supertag-factored Model", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "Joint A* CCG Parsing and Semantic Role Labelling", "author": ["Mike Lewis", "Luheng He", "Luke Zettlemoyer."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Lewis et al\\.,? 2015", "shortCiteRegEx": "Lewis et al\\.", "year": 2015}, {"title": "LSTM CCG Parsing", "author": ["Mike Lewis", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Lewis et al\\.,? 2016", "shortCiteRegEx": "Lewis et al\\.", "year": 2016}, {"title": "K-best A* Parsing", "author": ["Adam Pauls", "Dan Klein."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 958\u2013966. As-", "citeRegEx": "Pauls and Klein.,? 2009", "shortCiteRegEx": "Pauls and Klein.", "year": 2009}, {"title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning", "author": ["St\u00e9phane Ross", "Geoffrey J. Gordon", "Drew Bagnell."], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS", "citeRegEx": "Ross et al\\.,? 2011", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Parsing with Compositional Vector Grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the ACL conference.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved Semantic Representations from Tree-structured Long Short-term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Word representations: A Simple and General Method for Semi-supervised Learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Efficient Structured Inference for Transition-Based Parsing", "author": ["Ashish Vaswani", "Kenji Sagae"], "venue": null, "citeRegEx": "Vaswani and Sagae.,? \\Q2016\\E", "shortCiteRegEx": "Vaswani and Sagae.", "year": 2016}, {"title": "Supertagging With LSTMs", "author": ["Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa."], "venue": "Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Vaswani et al\\.,? 2016", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Sequenceto-Sequence Learning as Beam-Search Optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "CCG Supertagging with a Recurrent Neural Network", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "Volume 2: Short Papers, page 250.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "LSTM Shift-Reduce CCG Parsing", "author": ["Wenduan Xu."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Xu.,? 2016", "shortCiteRegEx": "Xu.", "year": 2016}, {"title": "Greed is Good if Randomized: New Inference for Dependency Parsing", "author": ["Yuan Zhang", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015; Dyer et al., 2015) or reranking (Socher et al.", "startOffset": 81, "endOffset": 122}, {"referenceID": 9, "context": "Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015; Dyer et al., 2015) or reranking (Socher et al.", "startOffset": 81, "endOffset": 122}, {"referenceID": 20, "context": ", 2015) or reranking (Socher et al., 2013).", "startOffset": 21, "endOffset": 42}, {"referenceID": 14, "context": "Generalizing A\u2217 to global models is challenging; these models also break the locality assumptions used to efficiently compute existing A\u2217 heuristics (Klein and Manning, 2003; Lewis and Steedman, 2014).", "startOffset": 149, "endOffset": 200}, {"referenceID": 15, "context": "Generalizing A\u2217 to global models is challenging; these models also break the locality assumptions used to efficiently compute existing A\u2217 heuristics (Klein and Manning, 2003; Lewis and Steedman, 2014).", "startOffset": 149, "endOffset": 200}, {"referenceID": 17, "context": "In our experiments, we use a recent factored A\u2217 CCG parser (Lewis et al., 2016) for the local model, and we train a Tree-LSTM (Tai et al.", "startOffset": 59, "endOffset": 79}, {"referenceID": 21, "context": ", 2016) for the local model, and we train a Tree-LSTM (Tai et al., 2015) to model global structure.", "startOffset": 54, "endOffset": 72}, {"referenceID": 5, "context": "Traditional structured prediction objectives focus on ensuring that the gold parse has the highest score (Collins, 2002; Huang et al., 2012).", "startOffset": 105, "endOffset": 140}, {"referenceID": 12, "context": "Traditional structured prediction objectives focus on ensuring that the gold parse has the highest score (Collins, 2002; Huang et al., 2012).", "startOffset": 105, "endOffset": 140}, {"referenceID": 14, "context": "A\u2217 parsing A\u2217 parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016).", "startOffset": 79, "endOffset": 170}, {"referenceID": 15, "context": "A\u2217 parsing A\u2217 parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016).", "startOffset": 79, "endOffset": 170}, {"referenceID": 16, "context": "A\u2217 parsing A\u2217 parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016).", "startOffset": 79, "endOffset": 170}, {"referenceID": 17, "context": "A\u2217 parsing A\u2217 parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016).", "startOffset": 79, "endOffset": 170}, {"referenceID": 16, "context": "In the experiments, we apply our model to CCG parsing, using the locally factored model and A\u2217 heuristic from Lewis et al. (2016).", "startOffset": 110, "endOffset": 130}, {"referenceID": 21, "context": "We use a variant of the Tree-LSTM (Tai et al., 2015) connected to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) at the leaves.", "startOffset": 34, "endOffset": 52}, {"referenceID": 10, "context": ", 2015) connected to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) at the leaves.", "startOffset": 42, "endOffset": 76}, {"referenceID": 5, "context": "This creates challenges for learning\u2014for example, it is not possible in practice to use the standard structured perceptron update (Collins, 2002), because the search procedure rarely terminates early in training.", "startOffset": 130, "endOffset": 145}, {"referenceID": 12, "context": "Other common loss functions assume inexact search (Huang et al., 2012), and do not optimize efficiency.", "startOffset": 50, "endOffset": 70}, {"referenceID": 12, "context": "In existing work on violation-based updates, comparisons are only made between derivations with the same number of steps (Huang et al., 2012; Clark et al., 2015)\u2014whereas our definition allows subtrees of arbitrary spans to compete with each other, because hyperedges are not explored in a fixed order.", "startOffset": 121, "endOffset": 161}, {"referenceID": 3, "context": "In existing work on violation-based updates, comparisons are only made between derivations with the same number of steps (Huang et al., 2012; Clark et al., 2015)\u2014whereas our definition allows subtrees of arbitrary spans to compete with each other, because hyperedges are not explored in a fixed order.", "startOffset": 121, "endOffset": 161}, {"referenceID": 12, "context": "The greedy and max-violation updates are roughly analogous to the violationfixing updates proposed by Huang et al. (2012), but adapted to exact agenda-based parsing.", "startOffset": 102, "endOffset": 122}, {"referenceID": 11, "context": "We trained our parser on Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), using Section 00 for development and Section 23 for test.", "startOffset": 51, "endOffset": 83}, {"referenceID": 16, "context": "For the local model, we use the supertag-factored model of Lewis et al. (2016). Here, slocal(e) corresponds to a supertag score if a HEAD(e) is a leaf and zero otherwise.", "startOffset": 59, "endOffset": 79}, {"referenceID": 27, "context": "0 Xu (2016) 87.", "startOffset": 2, "endOffset": 12}, {"referenceID": 24, "context": "8 Vaswani et al. (2016) 87.", "startOffset": 2, "endOffset": 24}, {"referenceID": 13, "context": "The model is trained for 30 epochs using ADAM (Kingma and Ba, 2014), and we use early stopping based on development F1.", "startOffset": 46, "endOffset": 67}, {"referenceID": 13, "context": "The model is trained for 30 epochs using ADAM (Kingma and Ba, 2014), and we use early stopping based on development F1. The LSTM cells and hidden states have 64 dimensions. We initialize word representations with pre-trained 50-dimensional embeddings from Turian et al. (2010). Embeddings for categories have 16 dimensions and are randomly initialized.", "startOffset": 47, "endOffset": 277}, {"referenceID": 2, "context": "We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al.", "startOffset": 70, "endOffset": 94}, {"referenceID": 27, "context": "We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al., 2015), which is the C&C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser; Vaswani et al.", "startOffset": 106, "endOffset": 123}, {"referenceID": 3, "context": "(2016) who combine a bidirectional LSTM supertagger with a beam search parser using global features (Clark et al., 2015); and supertag-factored (Lewis et al.", "startOffset": 100, "endOffset": 120}, {"referenceID": 17, "context": ", 2015); and supertag-factored (Lewis et al., 2016), which uses deterministic A\u2217 decoding and an LSTM supertagging model.", "startOffset": 31, "endOffset": 51}, {"referenceID": 2, "context": "We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al., 2015), which is the C&C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser; Vaswani et al.", "startOffset": 71, "endOffset": 184}, {"referenceID": 2, "context": "We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al., 2015), which is the C&C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser; Vaswani et al. (2016) who combine a bidirectional LSTM supertagger with a beam search parser using global features (Clark et al.", "startOffset": 71, "endOffset": 235}, {"referenceID": 24, "context": "Vaswani et al. (2016) also", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "spans, as in Durrett and Klein (2015). In this model, the recursive unit uses, instead of latent states from its children, the latent states of the backward LSTM at the start of the span and the latent states of the forward LSTM at the end of the span.", "startOffset": 13, "endOffset": 38}, {"referenceID": 3, "context": "In the beam search comparison, we use the approach from Clark et al. (2015) which greedily finds the top-n parses for each span in a bottom-up manner.", "startOffset": 56, "endOffset": 76}, {"referenceID": 8, "context": "Some recent models have neural factors (Durrett and Klein, 2015), but these cannot condition on global parse structure, making them less expressive.", "startOffset": 39, "endOffset": 64}, {"referenceID": 9, "context": "in shift reduce parsing (Dyer et al., 2015), string-to-tree transduction (Vinyals et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 25, "context": ", 2015), string-to-tree transduction (Vinyals et al., 2015), or reranking (Socher et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 20, "context": ", 2015), or reranking (Socher et al., 2013)\u2013at the cost of potentially recovering suboptimal solutions.", "startOffset": 22, "endOffset": 43}, {"referenceID": 14, "context": "A\u2217 parsing has been previously proposed for locally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014).", "startOffset": 68, "endOffset": 164}, {"referenceID": 18, "context": "A\u2217 parsing has been previously proposed for locally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014).", "startOffset": 68, "endOffset": 164}, {"referenceID": 1, "context": "A\u2217 parsing has been previously proposed for locally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014).", "startOffset": 68, "endOffset": 164}, {"referenceID": 15, "context": "A\u2217 parsing has been previously proposed for locally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014).", "startOffset": 68, "endOffset": 164}, {"referenceID": 6, "context": "Some recent models have neural factors (Durrett and Klein, 2015), but these cannot condition on global parse structure, making them less expressive. Our search explores fewer nodes than dynamic programs, despite an exponentially larger search space, by allowing the recursive neural network to guide the search. Previous work on structured prediction with recursive or recurrent neural models has used beam search\u2013e.g. in shift reduce parsing (Dyer et al., 2015), string-to-tree transduction (Vinyals et al., 2015), or reranking (Socher et al., 2013)\u2013at the cost of potentially recovering suboptimal solutions. For our model, beam search is both less efficient and less accurate than optimal A\u2217 decoding. In the non-neural setting, Zhang et al. (2014) showed that global features with greedy inference can improve dependency parsing.", "startOffset": 40, "endOffset": 752}, {"referenceID": 2, "context": "The CCG beam search parser of Clark et al. (2015), most related to this work, also uses global features.", "startOffset": 30, "endOffset": 50}, {"referenceID": 1, "context": "A\u2217 parsing has been previously proposed for locally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014). We generalize these methods to enable global features. Vaswani and Sagae (2016) apply best-first search to an unlabeled shift-reduce parser.", "startOffset": 117, "endOffset": 246}, {"referenceID": 19, "context": ", 2009) and DAGGER (Ross et al., 2011) learn greedy policies to predict structure by sampling classification examples over actions from single states.", "startOffset": 19, "endOffset": 38}, {"referenceID": 4, "context": "Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum\u00e9 III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear.", "startOffset": 121, "endOffset": 237}, {"referenceID": 12, "context": "Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum\u00e9 III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear.", "startOffset": 121, "endOffset": 237}, {"referenceID": 0, "context": "Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum\u00e9 III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear.", "startOffset": 121, "endOffset": 237}, {"referenceID": 26, "context": "Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum\u00e9 III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear.", "startOffset": 121, "endOffset": 237}], "year": 2016, "abstractText": "We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a novel objective that encourages the parser to search both efficiently and accurately. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.", "creator": "TeX"}}}