{"id": "1607.03360", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jul-2016", "title": "Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods", "abstract": "The well unlike plus - minimizes equality due move Jaynes, because states to perhaps 're specific, the heavier discriminant distribution matching them is in particular exponential family, also those very popular brought equipped learning initial they its \" Occam ' lost razor \" notions. Unfortunately, calculating 's outputs he new less - entropy distributes once ills \\ cite {bresler2014hardness }. We manage logistically efficient versions though an means when called beyond parameters exist topographies incredible: why designed distributions should 180 match given numberings surprising, had them decomposition of however comparable help the maximum entropy function pattern those haunting.", "histories": [["v1", "Tue, 12 Jul 2016 14:09:03 GMT  (22kb)", "http://arxiv.org/abs/1607.03360v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["andrej risteski", "yuanzhi li"], "accepted": true, "id": "1607.03360"}, "pdf": {"name": "1607.03360.pdf", "metadata": {"source": "CRF", "title": "Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods", "authors": ["Yuanzhi Li", "Andrej Risteski"], "emails": ["risteski@cs.princeton.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n60 7.\n03 36\n0v 1\n[ cs\n.L G\n] 1\n2 Ju\nWe additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for Ising models without any assumptions on the potentials of the model. More precisely, we show that in every temperature, we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit, which is the setting of optimization of quadratic forms over the hypercube. (Alon and Naor, 2006)"}, {"heading": "1 Introduction", "text": "Maximum entropy principle The maximum entropy principle (Jaynes, 1957) states that given mean parameters, i.e. E\u00b5[\u03c6t(x)] for a family of functionals \u03c6t(x), t \u2208 [1, T ], where \u00b5 is distribution over the hypercube {\u22121, 1}n, the entropy-maximizing distribution \u00b5 is an exponential family distribution, i.e. \u00b5(x) \u221d exp(\u2211Tt=1 Jt\u03c6t(x)) for some potentials Jt, t \u2208 [1, T ]. 1 This principle has been one of the reasons for the popularity of graphical models in machine learning: the \u201cmaximum entropy\u201d assumption is interpreted as \u201cminimal assumptions\u201d on the distribution other than what is known about it.\nHowever, this principle is problematic from a computational point of view. Due to results of (Bresler et al., 2014; Singh and Vishnoi, 2014), the potentials Jt of the Ising model, in many cases, are impossible to estimate well in polynomial time, unless NP = RP \u2013 so merely getting the description of the maximum entropy distribution is already hard. Moreover, in order to extract useful information about this distribution, usually we would also like to at least be able to sample efficiently from this distribution \u2013 which is typically NP-hard or even #P-hard.\nIn this paper we address this issue in certain cases. We provide a \u201cbi-criteria\u201d approximation for the special case where the functionals \u03c6t(x) are \u03c6i,j(x) = xixj , i.e. pairwise moments: we produce an efficiently sampleable distribution over the hypercube which matches these moments up to multiplicative constant factors, and has entropy at most a constant factor smaller from from the entropy of the maximum entropy distribution. 2\n\u2217Princeton University, Computer Science Department. Email: yuanzhili, risteski@cs.princeton.edu. This work was supported in part by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Sanjeev Arora\u2019s Simons Investigator Award, and Simons Collaboration Grant.\n1There is a more general way to state this principle over an arbitrary domain, not just the hypercube, but for clarity in this paper we will focus on the hypercube only.\n2In fact, we produce a distribution with entropy \u2126(n), which implies the latter claim since the maximum entropy of any distribution of over {\u22121, 1}n is at most n\nFurthermore, the distribution we consider is very natural: the sign of a multivariate normal variable. This provides theoretical explanation for the phenomenon observed by the computational neuroscience community (Bethge and Berens, 2007) that this distribution (named dichotomized Gaussian there) has near-maximum entropy.\nVariational methods The above results also allow us to get results for a seemingly unrelated problem \u2013 approximating the partition function Z = \u2211\nx\u2208{\u22121,1}n exp( \u2211T\nt=1 Jt\u03c6t(x)) of a member of an exponential family, which is an important step to calculate marginals.\nOne of the ways to calculate partition function is variational methods: namely, expressing logZ as an optimization problem. While there is a plethora of work on variational methods, of many flavors (mean field, Bethe/Kikuchi relaxations, TRBP, etc; for a survey, see (Wainwright and Jordan, 2008)), they typically come either with no guarantees, or with guarantees in very constrained cases (e.g. loopless graphs; graphs with large girth, etc. (Wainwright et al., 2003; 2005; Weiss, 2000)). While this is a rich area of research, the following extremely basic research question has not been answered:\nWhat is the best approximation guarantee on the partition function in the worst case (with no additional assumptions on the potentials)?\nIn the low-temperature limit, i.e. when |Jt| \u2192 \u221e, logZ \u2192 maxx\u2208{\u22121,1}n \u2211T\nt=1 Jt\u03c6t(x) - i.e. the question reduces to purely to optimization. In this regime, this question has very satisfying answers for many families \u03c6t(x). One classical example is when the functionals are \u03c6i,j(x) = xixj . In the graphical model community, these are known as Ising models, and in the optimization community this is the problem of optimizing quadratic forms and has been studied by (Charikar and Wirth, 2004; Alon and Naor, 2006; Alon et al., 2006).\nIn the optimization version, the previous papers showed that in the worst case, one can get O(log n) factor multiplicative factor approximation of the log of the partition function, and that unless P = NP, one cannot get better than constant factor approximations of it.\nIn the finite-temperature version, it is known that it is NP-hard to achieve a 1 + \u01eb factor approximation to the partition function (i.e. construct a FPRAS) (Sly and Sun, 2012), but nothing is known about coarser approximations. We prove in this paper, informally, that one can get comparable multiplicative guarantees on the log-partition function in the finite temperature case as well \u2013 using the tools and insights we develop on the maximum entropy principles.\nOur methods are extremely generic, and likely to apply to many other exponential families, where algorithms based on linear/semidefinite programming relaxations are known to give good guarantees in the optimization regime."}, {"heading": "2 Statements of results and prior work", "text": "Approximate maximum entropy The main theorem in this section is the following one.\nTheorem 2.1. For any covariance matrix \u03a3 of a centered distribution \u00b5 : {\u22121, 1}n \u2192 R, i.e. E\u00b5[xixj ] = \u03a3i,j , E\u00b5[xi] = 0, there is an efficiently sampleable distribution \u00b5\u0303, which can be sampled as sign(g), where g \u223c N (0,\u03a3 + \u03b2I) such that the following holds:\nG 1 + \u03b2 \u03a3i,j \u2264 E\u00b5\u0303[XiXj ] \u2264 1 1 + \u03b2 \u03a3i,j for a fixed constant G, and entropy H(\u00b5\u0303) \u2265\nn 25 (31/4 \u221a \u03b2\u22121)2\u221a 3\u03b2 , for any \u03b2 \u2265 1 31/2 .\nThere are two prior works on computational issues relating to maximum entropy principles, both proving hardness results.\n(Bresler et al., 2014) considers the \u201chard-core\u201d model where the functionals \u03c6t are such that the distribution \u00b5(x) puts zero mass on configurations x which are not independent sets with respect to some graph G. They show that unless NP = RP, there is no FPRAS for calculating the potentials Jt, given the mean parameters E\u00b5[\u03c6t(x)].\n(Singh and Vishnoi, 2014) prove an equivalence between calculating the mean parameters and calculating partition functions. More precisely, they show that given an oracle that can calculate the mean parameters up to a (1 + \u01eb) multiplicative factor in time O(poly(1/\u01eb)), one can calculate the partition function of the same exponential family up to (1 + O(poly(\u01eb))) multiplicative factor, in time O(poly(1/\u01eb)). Note, the \u01eb in this work potentially needs to be polynomially small in n (i.e. an oracle that can calculate the mean parameters to a fixed multiplicative constant cannot be used.)\nBoth results prove hardness for fine-grained approximations to the maximum entropy principle, and ask for outputting approximations to the mean parameters. Our result circumvents these hardness results by providing a distribution which is not in the maximum-entropy exponential family, and is allowed to only approximately match the moments as well. To the best of our knowledge, such an approximation, while very natural, has not been considered in the literature.\nProvable variational methods The main theorems in this section will concern the approximation factor that can be achieved by degree-2 pseudo-moment relaxations of the standard variational principle due to Gibbs. (Ellis, 2012) As outlined before, we will be concerned with a particularly popular exponential family: Ising models. We will prove the following three results:\nTheorem 2.2 (Ferromagnetic Ising, informal). There is a convex programming relaxation based on degree-2 pseudomoments that calculates up to multiplicative approximation factor 50 the value of logZ where Z is the partition function of the exponential distribution \u00b5(x) \u221d exp( \u2211\ni,j\nJi,jxixj) for Ji,j > 0.\nTheorem 2.3 (Ising model, informal). There is a convex programming relaxation based on degree-2 pseudo-moments that calculates up to multiplicative approximation factor O(log n) the value of logZ where Z is the partition function of the exponential distribution \u00b5(x) \u221d exp( \u2211\ni,j\nJi,jxixj).\nTheorem 2.4 (Ising model, informal). There is a convex programming relaxation based on degree-2 pseudo-moments that calculates up to multiplicative approximation factor O(log\u03c7(G)) the value of logZ where Z is the partition function of the exponential distribution \u00b5(x) \u221d exp( \u2211\ni,j\u2208E(G) Ji,jxixj) and G = (V (G), E(G)) is a graph with\nchromatic number \u03c7(G).\nNote Theorem 2.4 is strictly more general than Theorem 2.3, however the proof of Theorem 2.3 uses less heavy machinery and is illuminating enough that we feel merits being presented as a separate result.\nWhile a lot of work is done on variational methods in general (see the survey by (Wainwright and Jordan, 2008) for a detailed overview), to the best of our knowledge nothing is known about the worst-case guarantee that we are interested in here. Moreover, other than a recent paper by (Risteski, 2016), no other work has provided provable bounds for variational methods that proceed via a convex relaxation and a rounding thereof. 3\n(Risteski, 2016) provides guarantees in the case of Ising models that are also based on pseudo-moment relaxations of the variational principle, albeit only in the special case when the graph is \u201cdense\u201d in a suitably defined sense. 4 The results there are very specific to the density assumption and can not be adapted to our worst-case setting.\nFinally, we mention that in the special case of the ferromagnetic Ising models, an algorithm based on MCMC was provided by (Jerrum and Sinclair, 1993), which can give an approximation factor of (1 + \u01eb) to the partition function and runs in time O(n11poly(1/\u01eb)). In spite of this, the focus of this part of our paper is to provide understanding of variational methods in certain cases, as they continue to be popular in practice for their faster running time compared to MCMC-based methods but are theoretically much more poorly studied."}, {"heading": "3 Approximate maximum entropy principles", "text": "Let us recall what the problem we want to solve: Approximate maximum entropy principles We are given a positive-semidefinite matrix \u03a3 \u2208 Rn\u00d7n with \u03a3i,i = 1, \u2200i \u2208 [n], which is the covariance matrix of a centered distribution over {\u22121, 1}n, i.e. E\u00b5[xixj ] = \u03a3i,j , E\u00b5[xi] = 0, for a distribution \u00b5 : {\u22121, 1}n \u2192 R. We wish to produce a distribution \u00b5\u0303 : {\u22121, 1}n \u2192 R with pairwise\n3In some sense, it is possible to give provable bounds for Bethe-entropy based relaxations, via analyzing belief propagation directly, which has been done in cases where there is correlation decay and the graph is locally tree-like. (Wainwright and Jordan, 2008) has a detailed overview of such results.\n4More precisely, they prove that in the case when \u2200i, j,\u2206|Ji,j | \u2264 \u2206n2 \u2211 i,j |Ji,j |, one can get an additive \u01eb( \u2211 i,j Ji,j) approximation to\nlogZ in time n O( \u2206 \u01eb2 ) .\ncovariances that match the given ones up to constant factors, and entropy within a constant factor of the maximum entropy distribution with covariance \u03a3. 5\nBefore stating the result formally, it will be useful to define the following constant:\nDefinition 3.1. Define the constant G = mint\u2208[\u22121,1] { 2 \u03c0 arcsin(t)/t } \u2248 0.64.\nWe will prove the following main theorem:\nTheorem 3.1 (Main, approximate entropy principle). For any positive-semidefinite matrix \u03a3 with \u03a3i,i = 1, \u2200i, there is an efficiently sampleable distribution \u00b5\u0303 : {\u22121, 1}n \u2192 R, which can be sampled as sign(g), where g \u223c N (0,\u03a3+ \u03b2I), and satisfies G1+\u03b2\u03a3i,j \u2264 E\u00b5\u0303[xixj ] \u2264 11+\u03b2\u03a3i,j and has entropy H(\u00b5\u0303) \u2265 n25 (31/4 \u221a \u03b2\u22121)2\u221a 3\u03b2 , where \u03b2 \u2265 1 31/2 .\nNote \u00b5\u0303 is in fact very close to the the one which is classically used to round semidefinite relaxations for solving the MAX-CUT problem. (Goemans and Williamson, 1995) We will prove Theorem 3.1 in two parts \u2013 by first lower bounding the entropy of \u00b5\u0303, and then by bounding the moments of \u00b5\u0303.\nTheorem 3.2. The entropy of the distribution \u00b5\u0303 satisfies H(\u00b5\u0303) \u2265 n25 (31/4 \u221a \u03b2\u22121)2\u221a 3\u03b2 when \u03b2 \u2265 1 31/2 .\nProof. A sample g fromN (0, \u03a3\u0303) can be produced by sampling g1 \u223c N (0,\u03a3), g2 \u223c N (0, \u03b2I) and setting g = g1+g2. The sum of two multivariate normals is again a multivariate normal. Furthermore, the mean of g is 0, and since g1, g2 are independent, the covariance of g is \u03a3+ \u03b2I = \u03a3\u0303.\nLet\u2019s denote the random variable Y = sign(g1 + g2) which is distributed according to \u00b5\u0303. We wish to lower bound the entropy of Y. Toward that goal, denote the random variable S := {i \u2208 [n] : |(g1)i| \u2264 cD} for c,D to be chosen. Then, we have: for \u03b3 = c\u22121c ,\nH(Y) \u2265 H(Y|S) = \u2211\nS\u2286[n] Pr[S = S]H(Y|S = S) \u2265\n\u2211\nS\u2286[n],|S|\u2265\u03b3n Pr[S = S]H(Y|S = S)\nwhere the first inequality follows since conditioning doesn\u2019t decrease entropy, and the latter by the non-negativity of entropy. Continue the calculation we can get:\n\u2211\nS\u2286[n],|S|\u2265\u03b3n Pr[S = S]H(Y|S = S) \u2265\n\u2211\nS\u2286[n],|S|\u2265\u03b3n Pr[S = S] min S\u2286[n],|S|\u2265\u03b3n H(Y|S = S)\n= Pr [|S| \u2265 \u03b3n] min S\u2286[n],|S|\u2265\u03b3n H(Y|S = S)\nWe will lower bound Pr[|S| \u2265 \u03b3n] first. Notice that E[\u2211ni=1(g1)2i ] = n, therefore by Markov\u2019s inequality,\nPr\n[\nn \u2211\ni=1\n(g1) 2 i \u2265 Dn\n]\n\u2264 1 D . On the other hand, if \u2211n i=1(g1) 2 i \u2264 Dn, then |{i : (g1)2i \u2265 cD}| \u2264 nc , which means\nthat |{i : (g1)2i \u2264 cD}| \u2265 n\u2212 nc = (c\u22121)n c = \u03b3n. Putting things together, this means Pr [|S| \u2265 \u03b3n] \u2265 1\u2212 1\nD .\nIt remains to lower bound minS\u2286[n],|S|\u2265\u03b3nH(Y|S = S). For every S \u2286 [n], |S| \u2265 \u03b3n, denote by YS the coordinates of Y restricted to S, we get\nH(Y|S = S) \u2265 H(YS |S = S) \u2265 H\u221e(YS |S = S) = \u2212 log(max yS Pr[YS = yS |S = S])\n(where H\u221e is the min-entropy) so we only need to bound maxyS Pr[YS = yS |S = S] We will now, for any yS , upper bound Pr[YS = yS |S = S]. Recall that the event S = S implies that \u2200i \u2208 S, |(g1)i| \u2264 cD. Since g2 is independent of g1, we know that for every fixed g \u2208 Rn:\nPr[YS = yS |S = S, g1 = g] = \u03a0i\u2208S Pr[sign([g]i + [g2]i) = yi] 5Note for a distribution over {\u22121, 1}n , the maximal entropy a distribution can have is n, which is achieved by the uniform distribution.\nFor a fixed i \u2208 [S], consider the term Pr[sign([g]i + [g2]i) = yi]. Without loss of generality, let\u2019s assume [g]i > 0 (the proof is completely symmetric in the other case). Then, since [g]i is positive and g2 has mean 0, we have Pr[[g]i + (g2)i < 0] \u2264 1\n2 .\nMoreover,\nPr [[g]i + [g2]i > 0] = Pr[[g2]i > 0] Pr [[g]i + [g2]i > 0 | [g2]i > 0] +Pr[[g2]i < 0] Pr [[g]i + [g2]i > 0 | [g2]i < 0]\nThe first term is upper bounded by 12 since Pr[[g2]i > 0] \u2264 12 . The second term we will bound using standard Gaussian tail bounds:\nPr [[g]i + [g2]i > 0 | [g2]i < 0] \u2264 Pr [|[g2]i| \u2264 |[g]i| | [g2]i < 0] = Pr[|[g2]i| \u2264 |[g]i|] \u2264 Pr[([g2]i)2 \u2264 cD] = 1\u2212 Pr[([g2]i)2 > cD]\n\u2264 1\u2212 2\u221a 2\u03c0 exp (\u2212cD/2\u03b2)\n\n\n\u221a\n\u03b2 cD \u2212 ( \u221a \u03b2 cD\n)3 \n\nwhich implies\nPr[[g2]i < 0] Pr[[g]i + [g2]i > 0 | [g2]i < 0] \u2264 1\n2\n\n1\u2212 2\u221a 2\u03c0 exp (\u2212cD/2\u03b2)\n\n\n\u221a\n\u03b2\ncD \u2212\n( \u221a\n\u03b2\ncD\n)3 \n\n\n\nPutting together, we have\nPr[sign((g1)i + (g2)i) = yi] \u2264 1\u2212 1\u221a 2\u03c0 exp (\u2212cD/2\u03b2)\n\n\n\u221a\n\u03b2 cD \u2212 ( \u221a \u03b2 cD\n)3 \n\nTogether with the fact that |S| \u2265 \u03b3n we get\nPr[YS = yS |S = s, g1 = g] \u2264\n\n1\u2212 1\u221a 2\u03c0 exp (\u2212cD/2\u03b2)\n\n\n\u221a\n\u03b2 cD \u2212 ( \u221a \u03b2 cD\n)3 \n\n\n\n\u03b3n\nwhich implies that\nH(Y) \u2265 \u2212 ( 1\u2212 1 D ) (c\u2212 1)n c log\n\n1\u2212 1\u221a 2\u03c0 exp (\u2212cD/2\u03b2)\n\n\n\u221a\n\u03b2 cD \u2212 ( \u221a \u03b2 cD\n)3 \n\n\n\nBy setting c = D = 31/4 \u221a \u03b2 and a straightforward (albeit unpleasant) calculation, we can check that H(Y) \u2265\nn 25 (31/4 \u221a \u03b2\u22121)2\u221a 3\u03b2 , as we need.\nWe next show that the moments of the distribution are preserved up to a constant G1+\u03b2 .\nLemma 3.1. The distribution \u00b5\u0303 has G1+\u03b2\u03a3i,j \u2264 E\u00b5\u0303[XiXj ] \u2264 11+\u03b2\u03a3i,j\nProof. Consider the Gram decomposition of \u03a3\u0303i,j = \u3008vi, vj\u3009. Then, N (0, \u03a3\u0303) is in distribution equal to (sign(\u3008v1, s\u3009), . . . , sign(\u3008vn, s\u3009)) where s \u223c N (0, I). Similarly as in the analysis of Goemans-Williamson (Goemans and Williamson, 1995), if v\u0304i = 1\u2016vi\u2016vi, we have G\u3008v\u0304i, v\u0304j\u3009 \u2264 E\u00b5\u0303[XiXj] = 2 \u03c0 arcsin(\u3008v\u0304i, v\u0304j\u3009) \u2264 \u3008v\u0304i, v\u0304j\u3009.\nHowever, since \u3008v\u0304i, v\u0304j\u3009 = 1\n\u2016vi\u2016\u2016vj\u2016 \u3008vi, vj\u3009 =\n1\n\u2016vi\u2016\u2016vj\u2016 \u03a3\u0303i,j =\n1\n\u2016vi\u2016\u2016vj\u2016 \u03a3i,j and \u2016vi\u2016 =\n\u221a\n\u03a3\u0303i,i = \u221a 1 + \u03b2, \u2200i \u2208\n[1, n], we get that G\n1 + \u03b2 \u03a3i,j \u2264 E\u00b5\u0303[XiXj ] \u2264\n1\n1 + \u03b2 \u03a3i,j as we want.\nLemma 3.2 and 3.1 together imply Theorem 3.1."}, {"heading": "4 Provable bounds for variational methods", "text": "We will in this section consider applications of the approximate maximum entropy principles we developed for calculating partition functions of Ising models. Before we dive into the results, we give brief preliminaries on variational methods and pseudo-moment convex relaxations.\nPreliminaries on variational methods and pseudo-moment convex relaxations Recall, variational methods are based on the following simple lemma, which characterizes logZ as the solution of an optimization problem. It essentially dates back to Gibbs (Ellis, 2012), who used it in the context of statistical mechanics, though it has been rediscovered by machine learning researchers (Wainwright and Jordan, 2008):\nLemma 4.1 (Variational characterization of logZ). Let us denote by M the polytope of distributions over {\u22121, 1}n. Then,\nlogZ = max \u00b5\u2208M\n{\n\u2211\nt\nJtE\u00b5[\u03c6t(x)] +H(\u00b5)\n}\n(1)\nWhile the above lemma reduces calculating logZ to an optimization problem, optimizing over the polytope M is impossible in polynomial time. We will proceed in a way which is natural for optimization problems \u2013 by instead optimizing over a relaxation M\u2032 of that polytope.\nThe relaxation will be associated with the degree-2 Lasserre hierarchy. Intuitively, M\u2032 has as variables tentative pairwise moments of a distribution of {\u22121, 1}n, and it imposes all constraints on the moments that hold for distributions over {\u22121, 1}n. To define M\u2032 more precisely we will need the following notion: (for a more in-depth review of moment-based convex hierarchies, the reader can consult (Barak et al., 2014))\nDefinition 4.1. A degree-2 pseudo-moment 6 E\u0303\u03bd [\u00b7] is a linear operator mapping polynomials of degree 2 to R, such that E\u0303\u03bd [x2i ] = 1, and E\u0303\u03bd [p(x) 2] \u2265 0 for any polynomial p(x) of degree 1.\nWe will be optimizing over the polytope M\u2032 of all degree-2 pseudo-moments, i.e. we will consider solving\nmax E\u0303\u03bd [\u00b7]\u2208M\u2032\n{\n\u2211\nt\nJtE\u0303\u03bd [\u03c6t(x)] + H\u0303(E\u0303\u03bd [\u00b7]) }\nwhere H\u0303 will be a proxy for the entropy we will have to define (since entropy is a global property that depends on all moments, and E\u0303\u03bd only contains information about second order moments).\nTo see this optimization problem is convex, we show that it can easily be written as a semidefinite program. Namely, note that the pseudo-moment operators are linear, so it suffices to define them over monomials only. Hence, the variables will simply be E\u0303\u03bd(xS) for all monomials xS of degree at most 2. The constraints E\u0303\u03bd [x2i ] = 1 then are clearly linear, as is the \u201cenergy part\u201d of the objective function. So we only need to worry about the constraint E\u0303\u03bd [p(x)\n2] \u2265 0 and the entropy functional. We claim the constraint E\u0303\u03bd [p(x)2] \u2265 0 can be written as a PSD constraint: namely if we define the matrix Q, which is indexed by all the monomials of degree at most 1, and it satisfies Q(xS ,xT ) = E\u0303\u03bd [xSxT ]. It is easy to see that E\u0303\u03bd [p(x)2] \u2265 0 \u2261 Q 0.\n6The reason E\u0303\u03bd [\u00b7] is called a pseudo-moment, is that it behaves like the moments of a distribution \u03bd : {\u22121, 1}n \u2192 [0, 1], albeit only over polynomials of degree at most 2.\nHence, the final concern is how to write an expression for the entropy in terms of the low-order moments, since entropy is a global property that depends on all moments. There are many candidates for this in machine learning are like Bethe/Kikuchi entropy, tree-reweighted Bethe entropy, log-determinant etc. However, in the worst case \u2013 none of them come with any guarantees. We will in fact show that the entropy functional is not an issue when we only care about worst case guarantees \u2013 we will relax the entropy trivially to an upper bound of n.\nGiven all of this, the final relaxation we will consider is:\nmax E\u0303\u03bd [\u00b7]\u2208M\u2032\n{\n\u2211\nt\nJtE\u0303\u03bd [\u03c6t(x)] + n\n}\n(2)\nFrom the prior setup it is clear that the solution to (2) is an upper bound to logZ . To prove a claim like Theorem 2.3 or Theorem 2.4, we will then provide a rounding of the solution. In this instance, this will mean producing a distribution \u00b5\u0303 which has the value of \u2211\nt JtE\u00b5\u0303[\u03c6t(x)] + H(\u00b5\u0303) comparable to the value of the solution. Note this is slightly different than the usual requirement in optimization, where one cares only about producing a single x \u2208 {\u22121, 1}n with comparable value to the solution. Our distribution \u00b5\u0303 will have entropy \u2126(n), and preserves the \u201cenergy\u201d portion of the objective \u2211\nt JtE\u00b5[\u03c6t(x)] up to a comparable factor to what is achievable in the optimization setting.\nWarmup: exponential family analogue of MAX-CUT As a warmup, to illustrate the basic ideas behind the above rounding strategy, before we consider Ising models we consider the exponential family analogue of MAX-CUT. It is defined by the functionals \u03c6i,j(x) = (xi \u2212xj)2. Concretely, we wish to approximate the partition function of the\ndistribution \u00b5(x) \u221d exp\n\n\n\u2211\ni,j\nJi,j(xi \u2212 xj)2  . We will prove the following simple observation:\nObservation 4.1. The relaxation (2) provides a factor 2 approximation of logZ . Proof. We proceed as outlined in the previous section, by providing a rounding of (2). We point out again, unlike the standard case in optimization, where typically one needs to produce an assignment of the variables, because of the entropy term here it is crucial that the rounding produces a distribution.\nThe distribution \u00b5\u0303 we produce here will be especially simple: we will round each xi independently with probability 1 2 . Then, clearly H(\u00b5\u0303) = n. On the other hand, we similarly have Pr\u00b5\u0303[(xi \u2212 xj)2 = 1] = 12 , since xi and xj are rounded independently. Hence, E\u00b5\u0303[(xi \u2212 xj)2] \u2265 12 . Altogether, this implies \u2211\ni,j Ji,jE\u00b5\u0303[(xi \u2212 xj)2] + H(\u00b5\u0303) \u2265 1 2 ( \u2211 i,j Ji,jE\u03bd [(xi \u2212 xj)2] + n ) as we needed."}, {"heading": "4.1 Ising models", "text": "We proceed with the main results of this section on Ising models, which is the case where \u03c6i,j(x) = xixj . We will split into the ferromagnetic and general case separately, as outlined in Section 2.\nTo be concrete, we will be given potentials Ji,j , and we wish to calculate the partition function of the Ising model \u00b5(x) \u221d exp(\u2211i,j Ji,jxixj).\nFerromagnetic case Recall, in the ferromagnetic case of Ising model, we have the conditions that the potentials Ji,j > 0. We will provide a convex relaxation which has a constant factor approximation in this case. First, recall the famous First Griffiths inequality due to Griffiths (Griffiths, 1967) which states that in the ferromagnetic case, E\u00b5[xixj ] \u2265 0, \u2200i, j.\nUsing this inequality, we will look at the following natural strenghtening of the relaxation (2):\nmax E\u0303\u03bd [\u00b7]\u2208M\u2032;E\u0303\u03bd [xixj ]\u22650,\u2200i,j\n{\n\u2211\nt\nJtE\u0303\u03bd [\u03c6t(x)] + n\n}\n(3)\nWe will prove the following theorem, as a straightforward implication of our claims from Section 3:\nTheorem 4.1. The relaxation (3) provides a factor 50 approximation of logZ .\nProof. Notice, due to Griffiths\u2019 inequality, (3) is in fact a relaxation of the Gibbs variational principle and hence an upper bound)of logZ . Same as before, we will provide a rounding of (3). We will use the distribution \u00b5\u0303 we designed in Section 3 the sign of a Gaussian with covariance matrix \u03a3 + \u03b2I , for a \u03b2 which we will specify. By Lemma 3.2, we then have H(\u00b5\u0303) \u2265 n25 (31/4 \u221a \u03b2\u22121)2\u221a 3\u03b2 whenever \u03b2 \u2265 1 31/2 . By Lemma 3.1, on the other hand, we can prove that E\u00b5\u0303[xixj ] \u2265 G\n1 + \u03b2 E\u0303\u03bd [xixj ]\nBy setting \u03b2 = 21.8202, we get n25 (31/4 \u221a \u03b2\u22121)2\u221a 3\u03b2 \u2265 0.02 and G1+\u03b2 \u2265 0.02, which implies that\n\u2211\ni,j\nJi,jE\u00b5\u0303[xixj ] +H(\u00b5\u0303) \u2265 0.02\n\n\n\u2211\ni,j\nJi,jE\u0303\u03bd [xixj ] + n\n\n\nwhich implies the claim we want.\nNote that the above proof does not work in the general Ising model case: when E\u0303\u03bd [xixj ] can be either positive or negative, even if we preserved each E\u0303\u03bd [xixj ] up to a constant factor, this may not preserve the sum \u2211\ni,j Ji,jE\u0303\u03bd [xixj ] due to cancellations in that expression.\nGeneral Ising models case Finally, we will tackle the general Ising model case. As noted in the previous section, the straightforward application of the results proven in Section 3 doesn\u2019t work, so we have to consider a different rounding \u2013 again inspired by roundings used in optimization.\nThe intuition is the same as in the ferromagnetic case: we wish to design a rounding which preserves the \u201cenergy\u201d portion of the objective, while having a high entropy. In the previous section, this was achieved by modifying the Goemans-Williamson rounding so that it produces a high-entropy distribution. We will do a similar thing here, by modifying roundings due to (Charikar and Wirth, 2004) and (Alon et al., 2006).\nThe convex relaxation we will consider will just be the basic one (2), and we will prove the following two theorems:\nTheorem 4.2. The relaxation (2) provides a factor O(log n) approximation to logZ when \u03c6i,j(x) = xixj .\nTheorem 4.3. The relaxation (2) provides a factor O(log(\u03c7(G))) approximation to logZ when \u03c6i,j(x) = xixj for i, j \u2208 E(G) of some graph G = (V (G), E(G)), and \u03c7(G) is the chromatic number of G.\nSince the chromatic number of a graph is bounded by n, the second theorem is in fact strictly stronger than the first, however the proof of the first theorem uses less heavy machinery, and is illuminating enough to be presented on its own.\nBefore delving into the proof of Theorem 4.2, we review the rounding used by (Charikar and Wirth, 2004) in the case of maximizing quadratic forms:\nAlgorithm 1 Quadratic form rounding by (Charikar and Wirth, 2004) 1: Input: A pseudo-moment matrix \u03a3i,j = E\u03bd [xixj ] 2: Output: A sample x from a distribution \u03c1 3: Sample g from the standard Gaussian N(0, I). 4: Consider the vector h, such that hi = gi/T, T = \u221a 4 logn\n5: Consider the vector r, such that ri = hi|hi| , if |hi| > 1, and ri = hi otherwise. 6: Produce the rounded vector x \u2208 {\u22121, 1}n, s.t.\nxi =\n{\n+1, with probability 1+ri2 \u22121, with probability 1\u2212ri2\n}\nAlgorithm 2 Scaled down quadratic form rounding 1: Input: A pseudo-moment matrix \u03a3i,j = E\u03bd [xixj ] 2: Output: A sample x from a distribution \u00b5\u0303 3: Sample g from the standard Gaussian N(0, I). 4: Consider the vector h, such that hi = gi/T, T = \u221a 4 logn\n5: Consider the vector r, such that r\u2032i = 1 2 hi |hi| , if |hi| > 1, and r \u2032 i = 1 2hi otherwise. 6: Produce the rounded vector x \u2208 {\u22121, 1}n, s.t.\nxi =\n{\n+1, with probability 1+ri2 \u22121, with probability 1\u2212ri2\n}\nWith that in hand, we can prove Theorem 4.2\nProof of Theorem 4.2. The proof again consists of exhibiting a rounding. Our rounding will essentially be the same as (Charikar and Wirth, 2004), except in step 3, we will produce a vector r\u2032i by scaling down the vector ri by 2 coordinate-wise. For full clarity, the rounding is presented in Algorithm 2.\nWe again, need to analyze the entropy and the moments of the distribution \u00b5\u0303 that this rounding produces. Let us focus on the entropy first.\nSince conditioning does not decrease entropy, it\u2019s true that H(\u00b5\u0303) = H(x) \u2265 H(x|r), so it suffices to lower bound that quantity. However, note that it holds that ri \u2264 12 , and each xi is rounded independently conditional on ri, so we have:\nH(x|r) = \u2211\ni\nH(xi|ri) = \u2211\ni\n(\n1 + ri 2 log\n(\n1 + ri 2\n) + 1\u2212 ri 2 ( 1\u2212 ri 2 )) \u2265 ( 2\u2212 3 4 log 3 ) n\nConsider now the moments of the distribution. Let us denote the distribution that the rounding 1 produces by \u03c1. By Theorem 1 in (Charikar and Wirth, 2004), we\nhave \u2211\ni,j\nJi,jE\u03c1[xixj ] \u2265 O ( 1\nlogn\n)\n\u2211\ni,j\nJi,jE\u03bd [xixj ]\nAdditional, both our and the (Charikar and Wirth, 2004) roundings are such that E\u03c1[xixj ] = ErEx|r[xixj ] and E\u00b5\u0303[xixj ] = Er\u2032Ex|r\u2032 [xixj ]. Furthermore, as noted in (Charikar and Wirth, 2004), it is easy to check that E[xixj |r\u2032] = r\u2032ir \u2032 j and obviously r \u2032 i = 2ri, \u2200i in distribution, so we have:\nE\u00b5\u0303[xixj ] = Er\u2032Ex|r\u2032 [xixj ] = 1\n4 ErEx|r[xixj ] =\n1 4 E\u03c1[xixj ]\nBut, this directly implies\n\u2211\ni,j\nJi,jE\u00b5\u0303[xixj ] = 1\n4\n\u2211\ni,j\nJi,jE\u03c1[xixj ] \u2265 O ( 1\nlogn\n)\n\u2211\ni,j\nJi,jE\u03bd [xixj ]\nas we needed.\nNext, we prove the more general Theorem 4.3. Before proceeding, let\u2019s recall for completeness the following definition of a chromatic number.\nDefinition 4.2 (Chromatic number). The chromatic number \u03c7(G) of a graph G = (V (G), E(G)) is defined as the minimum number of colors in a coloring of the vertices V (G), such that no vertices i, j : (i, j) \u2208 E(G) are colored with the same color.\nAlso, let us denote by Sn\u22121 the set of unit vectors in Rn and L\u221e[0, 1] the set of (essentially) bounded functions: the functions which are bounded except on a set of measure zero.\nThen, we can recall Theorem 3.3 from (Alon et al., 2006):\nTheorem 4.4 ((Alon et al., 2006)). There exists an absolute constant c such that the following holds: Let G = (V (G), E(G)) be an undirected graph on n vertices without self-loops7, let \u03c7(G) be the chromatic number of G. Then for every function f : V (G) \u2192 Sn\u22121, there exists a function F : V \u2192 L\u221e[0, 1] so that for every i \u2208 V (G), \u2016F (i)\u2016\u221e \u2264 \u221a c\u03c7(G) and for every (i, j) \u2208 E(G),\n\u3008f(i), f(j)\u3009 = \u222b 1\n0\nF (i)(t)F (j)(t)dt\nNow, we can prove Theorem 4.3\nProof of Theorem 4.3. The proof is similar, though a little more complicated than the proof of Theorem 4.2. Let E\u0303\u03bd [\u00b7] be the solution of the relaxation. By matrix formulation of the pseudo-moment relaxation in Section 4 , we know that E\u0303\u03bd [xixj ] = \u3008f(i), f(j)\u3009 for some unit vectors f(i), f(j). Hence, by theorem 4.4, there exists a functionF : V \u2192 L\u221e[0, 1] so that for every i \u2208 V (G), \u2016F (i)\u2016\u221e \u2264 \u221a\nc\u03c7(G) and for every (i, j) \u2208 E(G),\nE\u0303\u03bd [xixj ] =\n\u222b 1\n0\nF (i)(t)F (j)(t)dt\nConsider the following rounding:\n\u2022 Pick a t uniformly at random from [0, 1].\n\u2022 Consider the function ht : V \u2192 R, such that ht(i) = F (i)(t) 2 \u221a c\u03c7(G)\n\u2022 Produce the rounded vector x \u2208 {\u22121, 1}V (G), s.t.\nxi =\n{\n+1, with probability 1+ht(i)2 \u22121, with probability 1\u2212ht(i)2\n}\nNote importantly that the algorithm does not need to perform this rounding \u2013 it is for the analysis of the approximation factor of the relaxation. Therefore, we need not construct it algorithmically.\nLet us denote this distribution as \u00b5\u0303. We first show that \u00b5\u0303 has entropy at least ( 2\u2212 34 log 3 )\nn. Note that each xi are round independently conditional on t. Moreover, since \u2016F (v)\u2016\u221e \u2264 \u221a\nc\u03c7(G), we know that ht(v) \u2264 12 . Therefore, for every fixed t0 \u2208 [0, 1]\nH(\u00b5\u0303 | t = t0) = \u2211 i\u2208V (G) H(xi | t = t0)\n= \u2211\ni\u2208V (G)\n(\n1 + ht0(v)\n2 log\n1 + ht0(v)\n2 + 1\u2212 ht0(v) 2 log 1\u2212 ht0(v) 2 )\n\u2265 ( 2\u2212 3 4 log 3 ) n\nIntegrating over t0 we get that H(\u00b5\u0303) \u2265 ( 2\u2212 34 log 3 )\nn. Next, we will show that \u00b5\u0303 preserves the \u201cenergy\u201d part of the objective up to a multiplicative factor O(log\u03c7(G)):\nConsider each edge (i, j) \u2208 E(G). We have:\nE\u00b5\u0303[xixj ] =\n7Meaning no edge connects a vertex with itself\n\u222b 1\n0\n(\n(1 + ht(i))(1 + ht(j))\n4 + (1\u2212 ht(i))(1 \u2212 ht(j)) 4 \u2212 (1 + ht(i))(1\u2212 ht(j)) 4 \u2212 (1\u2212 ht(i))(1 + ht(j)) 4 ) dt\n=\n\u222b 1\n0\nht(i)ht(j)dt = 1\n4c\u03c7(G)\n\u222b 1\n0\nF (i)(t)F (j)(t)dt = 1\n4c\u03c7(G) E\u0303\u03bd [xixj ]\nThis implies that \u2211\ni,j\u2208E(G) Ji,jE\u00b5\u0303[xixj ] \u2265\n1\n4c\u03c7(G)\n\u2211\ni,j\u2208E(G) Ji,jE\u0303\u03bd [xixj ]\nTherefore, the relaxation provides a factor O(\u03c7(G)) approximation of logZ , as we wanted."}, {"heading": "5 Conclusion", "text": "In summary, we presented computationally efficient approximate versions of the classical max-entropy principle by (Jaynes, 1957): efficiently sampleable distributions which preserve given pairwise moments up to a multiplicative constant factor, while having entropy within a constant factor of the maximum entropy distribution matching those moments. Additionally, we applied our insights to designing provable variational methods for Ising models which provide comparable guarantees for approximating the log-partition function to those in the optimization setting. Our methods are based on convex relaxations of the standard variational principle due to Gibbs, and are extremely generic and we hope they will find applications for other exponential families."}], "references": [{"title": "Approximating the cut-norm via grothendieck\u2019s inequality", "author": ["Noga Alon", "Assaf Naor"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Alon and Naor.,? \\Q2006\\E", "shortCiteRegEx": "Alon and Naor.", "year": 2006}, {"title": "Rounding sum-of-squares relaxations", "author": ["Boaz Barak", "Jonathan A Kelner", "David Steurer"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Barak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2014}, {"title": "Near-maximum entropy models for binary neural representations of natural images", "author": ["Matthias Bethge", "Philipp Berens"], "venue": null, "citeRegEx": "Bethge and Berens.,? \\Q2007\\E", "shortCiteRegEx": "Bethge and Berens.", "year": 2007}, {"title": "Hardness of parameter estimation in graphical models", "author": ["Guy Bresler", "David Gamarnik", "Devavrat Shah"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bresler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bresler et al\\.", "year": 2014}, {"title": "Maximizing quadratic programs: extending grothendieck\u2019s inequality", "author": ["Moses Charikar", "Anthony Wirth"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Charikar and Wirth.,? \\Q2004\\E", "shortCiteRegEx": "Charikar and Wirth.", "year": 2004}, {"title": "Entropy, large deviations, and statistical mechanics, volume 271", "author": ["Richard S Ellis"], "venue": "Springer Science & Business Media,", "citeRegEx": "Ellis.,? \\Q2012\\E", "shortCiteRegEx": "Ellis.", "year": 2012}, {"title": "The statistics of curie-weiss models", "author": ["Richard S Ellis", "Charles M Newman"], "venue": "Journal of Statistical Physics,", "citeRegEx": "Ellis and Newman.,? \\Q1978\\E", "shortCiteRegEx": "Ellis and Newman.", "year": 1978}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Michel X Goemans", "David P Williamson"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Goemans and Williamson.,? \\Q1995\\E", "shortCiteRegEx": "Goemans and Williamson.", "year": 1995}, {"title": "Correlations in ising ferromagnets", "author": ["Robert B Griffiths"], "venue": "i. Journal of Mathematical Physics,", "citeRegEx": "Griffiths.,? \\Q1967\\E", "shortCiteRegEx": "Griffiths.", "year": 1967}, {"title": "Information theory and statistical mechanics", "author": ["Edwin T Jaynes"], "venue": "Physical review,", "citeRegEx": "Jaynes.,? \\Q1957\\E", "shortCiteRegEx": "Jaynes.", "year": 1957}, {"title": "Polynomial-time approximation algorithms for the ising model", "author": ["Mark Jerrum", "Alistair Sinclair"], "venue": "SIAM Journal on computing,", "citeRegEx": "Jerrum and Sinclair.,? \\Q1993\\E", "shortCiteRegEx": "Jerrum and Sinclair.", "year": 1993}, {"title": "How to compute partition functions using convex programming hierarchies: provable bounds for variational methods", "author": ["Andrej Risteski"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Risteski.,? \\Q2016\\E", "shortCiteRegEx": "Risteski.", "year": 2016}, {"title": "Entropy, optimization and counting", "author": ["Mohit Singh", "Nisheeth K Vishnoi"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Singh and Vishnoi.,? \\Q2014\\E", "shortCiteRegEx": "Singh and Vishnoi.", "year": 2014}, {"title": "The computational hardness of counting in two-spin models on d-regular graphs", "author": ["Allan Sly", "Nike Sun"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Sly and Sun.,? \\Q2012\\E", "shortCiteRegEx": "Sly and Sun.", "year": 2012}, {"title": "Tree-reweighted belief propagation algorithms and approximate ml estimation by pseudo-moment", "author": ["Martin J Wainwright", "Tommi S Jaakkola", "Alan S Willsky"], "venue": null, "citeRegEx": "Wainwright et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2003}, {"title": "A new class of upper bounds on the log partition function", "author": ["Martin J Wainwright", "Tommi S Jaakkola", "Alan S Willsky"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Wainwright et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2005}, {"title": "Correctness of local probability propagation in graphical models with loops", "author": ["Yair Weiss"], "venue": "Neural computation,", "citeRegEx": "Weiss.,? \\Q2000\\E", "shortCiteRegEx": "Weiss.", "year": 2000}], "referenceMentions": [{"referenceID": 3, "context": "Unfortunately, calculating the potentials in the maximum-entropy distribution is intractable (Bresler et al., 2014).", "startOffset": 93, "endOffset": 115}, {"referenceID": 0, "context": "(Alon and Naor, 2006)", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "1 Introduction Maximum entropy principle The maximum entropy principle (Jaynes, 1957) states that given mean parameters, i.", "startOffset": 71, "endOffset": 85}, {"referenceID": 3, "context": "Due to results of (Bresler et al., 2014; Singh and Vishnoi, 2014), the potentials Jt of the Ising model, in many cases, are impossible to estimate well in polynomial time, unless NP = RP \u2013 so merely getting the description of the maximum entropy distribution is already hard.", "startOffset": 18, "endOffset": 65}, {"referenceID": 12, "context": "Due to results of (Bresler et al., 2014; Singh and Vishnoi, 2014), the potentials Jt of the Ising model, in many cases, are impossible to estimate well in polynomial time, unless NP = RP \u2013 so merely getting the description of the maximum entropy distribution is already hard.", "startOffset": 18, "endOffset": 65}, {"referenceID": 2, "context": "This provides theoretical explanation for the phenomenon observed by the computational neuroscience community (Bethge and Berens, 2007) that this distribution (named dichotomized Gaussian there) has near-maximum entropy.", "startOffset": 110, "endOffset": 135}, {"referenceID": 14, "context": "(Wainwright et al., 2003; 2005; Weiss, 2000)).", "startOffset": 0, "endOffset": 44}, {"referenceID": 16, "context": "(Wainwright et al., 2003; 2005; Weiss, 2000)).", "startOffset": 0, "endOffset": 44}, {"referenceID": 4, "context": "In the graphical model community, these are known as Ising models, and in the optimization community this is the problem of optimizing quadratic forms and has been studied by (Charikar and Wirth, 2004; Alon and Naor, 2006; Alon et al., 2006).", "startOffset": 175, "endOffset": 241}, {"referenceID": 0, "context": "In the graphical model community, these are known as Ising models, and in the optimization community this is the problem of optimizing quadratic forms and has been studied by (Charikar and Wirth, 2004; Alon and Naor, 2006; Alon et al., 2006).", "startOffset": 175, "endOffset": 241}, {"referenceID": 13, "context": "construct a FPRAS) (Sly and Sun, 2012), but nothing is known about coarser approximations.", "startOffset": 19, "endOffset": 38}, {"referenceID": 3, "context": "(Bresler et al., 2014) considers the \u201chard-core\u201d model where the functionals \u03c6t are such that the distribution \u03bc(x) puts zero mass on configurations x which are not independent sets with respect to some graph G.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "(Singh and Vishnoi, 2014) prove an equivalence between calculating the mean parameters and calculating partition functions.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "(Ellis, 2012) As outlined before, we will be concerned with a particularly popular exponential family: Ising models.", "startOffset": 0, "endOffset": 13}, {"referenceID": 11, "context": "Moreover, other than a recent paper by (Risteski, 2016), no other work has provided provable bounds for variational methods that proceed via a convex relaxation and a rounding thereof.", "startOffset": 39, "endOffset": 55}, {"referenceID": 11, "context": "3 (Risteski, 2016) provides guarantees in the case of Ising models that are also based on pseudo-moment relaxations of the variational principle, albeit only in the special case when the graph is \u201cdense\u201d in a suitably defined sense.", "startOffset": 2, "endOffset": 18}, {"referenceID": 10, "context": "Finally, we mention that in the special case of the ferromagnetic Ising models, an algorithm based on MCMC was provided by (Jerrum and Sinclair, 1993), which can give an approximation factor of (1 + \u01eb) to the partition function and runs in time O(npoly(1/\u01eb)).", "startOffset": 123, "endOffset": 150}, {"referenceID": 7, "context": "(Goemans and Williamson, 1995) We will prove Theorem 3.", "startOffset": 0, "endOffset": 30}, {"referenceID": 7, "context": "Similarly as in the analysis of Goemans-Williamson (Goemans and Williamson, 1995), if v\u0304i = 1 \u2016vi\u2016vi, we have G\u3008v\u0304i, v\u0304j\u3009 \u2264 E\u03bc\u0303[XiXj] = 2 \u03c0 arcsin(\u3008v\u0304i, v\u0304j\u3009) \u2264 \u3008v\u0304i, v\u0304j\u3009.", "startOffset": 51, "endOffset": 81}, {"referenceID": 5, "context": "It essentially dates back to Gibbs (Ellis, 2012), who used it in the context of statistical mechanics, though it has been rediscovered by machine learning researchers (Wainwright and Jordan, 2008): Lemma 4.", "startOffset": 35, "endOffset": 48}, {"referenceID": 1, "context": "To define M\u2032 more precisely we will need the following notion: (for a more in-depth review of moment-based convex hierarchies, the reader can consult (Barak et al., 2014)) Definition 4.", "startOffset": 150, "endOffset": 170}, {"referenceID": 8, "context": "First, recall the famous First Griffiths inequality due to Griffiths (Griffiths, 1967) which states that in the ferromagnetic case, E\u03bc[xixj ] \u2265 0, \u2200i, j.", "startOffset": 69, "endOffset": 86}, {"referenceID": 4, "context": "We will do a similar thing here, by modifying roundings due to (Charikar and Wirth, 2004) and (Alon et al.", "startOffset": 63, "endOffset": 89}, {"referenceID": 4, "context": "2, we review the rounding used by (Charikar and Wirth, 2004) in the case of maximizing quadratic forms: Algorithm 1 Quadratic form rounding by (Charikar and Wirth, 2004) 1: Input: A pseudo-moment matrix \u03a3i,j = E\u03bd [xixj ] 2: Output: A sample x from a distribution \u03c1 3: Sample g from the standard Gaussian N(0, I).", "startOffset": 34, "endOffset": 60}, {"referenceID": 4, "context": "2, we review the rounding used by (Charikar and Wirth, 2004) in the case of maximizing quadratic forms: Algorithm 1 Quadratic form rounding by (Charikar and Wirth, 2004) 1: Input: A pseudo-moment matrix \u03a3i,j = E\u03bd [xixj ] 2: Output: A sample x from a distribution \u03c1 3: Sample g from the standard Gaussian N(0, I).", "startOffset": 143, "endOffset": 169}, {"referenceID": 4, "context": "Our rounding will essentially be the same as (Charikar and Wirth, 2004), except in step 3, we will produce a vector r\u2032 i by scaling down the vector ri by 2 coordinate-wise.", "startOffset": 45, "endOffset": 71}, {"referenceID": 4, "context": "By Theorem 1 in (Charikar and Wirth, 2004), we have", "startOffset": 16, "endOffset": 42}, {"referenceID": 4, "context": "i,j Ji,jE\u03bd [xixj ] Additional, both our and the (Charikar and Wirth, 2004) roundings are such that E\u03c1[xixj ] = ErEx|r[xixj ] and E\u03bc\u0303[xixj ] = Er\u2032Ex|r\u2032 [xixj ].", "startOffset": 48, "endOffset": 74}, {"referenceID": 4, "context": "Furthermore, as noted in (Charikar and Wirth, 2004), it is easy to check that E[xixj |r\u2032] = r\u2032 ir \u2032 j and obviously r \u2032 i = 2ri, \u2200i in distribution, so we have: E\u03bc\u0303[xixj ] = Er\u2032Ex|r\u2032 [xixj ] = 1 4 ErEx|r[xixj ] = 1 4 E\u03c1[xixj ] But, this directly implies", "startOffset": 25, "endOffset": 51}], "year": 2016, "abstractText": "The well known maximum-entropy principle due to Jaynes, which states that given mean parameters, the maximum entropy distribution matching them is in an exponential family, has been very popular in machine learning due to its \u201cOccam\u2019s razor\u201d interpretation. Unfortunately, calculating the potentials in the maximum-entropy distribution is intractable (Bresler et al., 2014). We provide computationally efficient versions of this principle when the mean parameters are pairwise moments: we design distributions that approximately match given pairwise moments, while having entropy which is comparable to the maximum entropy distribution matching those moments. We additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for Ising models without any assumptions on the potentials of the model. More precisely, we show that in every temperature, we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit, which is the setting of optimization of quadratic forms over the hypercube. (Alon and Naor, 2006)", "creator": "LaTeX with hyperref package"}}}