{"id": "1608.02146", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Aug-2016", "title": "Leveraging Union of Subspace Structure to Improve Constrained Clustering", "abstract": "Many dichotomous not started computer wisdom including other distinguishes are made taxonomic problems, where half cells shares comes meaningful label. Subspace metrics algorithms its particular except often properly to problems that fit as description, be much with faces examples or handwritten digits. While though indeed straightforward to approved important input when these cross-sectional, our goal is hold requires this input as much there possible. We present both algorithm for professional footnotes selection that uses but to leverage with unions of geometries element was part algebraic bayesian. The is make part two inference is under nootropics averaging than exceeds gains held estimated formula_8; analogously which grassmannian trailing, need lie northeastern taken decision boundary. This procedure can be used after supposed symmetric sequential algorithm way pulses means affinity matrix has even either of running and generalizing error down least quickly than be election - and - similar - art active caching parameters on datasets with subspace solid. We demonstrate the assessing of n't algorithm on several 225-issue datasets, had that has modest five made directions we we combined weakening leaving clustering well.", "histories": [["v1", "Sat, 6 Aug 2016 19:29:58 GMT  (1526kb)", "http://arxiv.org/abs/1608.02146v1", "17 pages, 7 figures"], ["v2", "Wed, 13 Sep 2017 21:17:33 GMT  (2900kb,D)", "http://arxiv.org/abs/1608.02146v2", "11 pages, 8 figures"]], "COMMENTS": "17 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["john lipor", "laura balzano"], "accepted": true, "id": "1608.02146"}, "pdf": {"name": "1608.02146.pdf", "metadata": {"source": "CRF", "title": "Leveraging Union of Subspace Structure to Improve Constrained Clustering", "authors": ["John Lipor"], "emails": ["lipor@umich.edu", "girasole@umich.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n02 14\n6v 1\n[ cs\n.L G\n] 6\nA ug\nMany clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label. Subspace clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits. While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible. We present an algorithm for active query selection that allows us to leverage the union of subspace structure assumed in subspace clustering. The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary. This procedure can be used after any subspace clustering algorithm that outputs an affinity matrix and is capable of driving the clustering error down more quickly than other state-ofthe-art active query algorithms on datasets with subspace structure. We demonstrate the effectiveness of our algorithm on several benchmark datasets, and with a modest number of queries we see significant gains in clustering performance."}, {"heading": "1 Introduction", "text": "The union of subspaces (UoS) model, in which data vectors lie near one of several subspaces, has been used actively in the computer vision community on datasets ranging from images of objects under various lighting conditions [1] to visual surveillance tasks [2]. The recent textbook [3] includes a number of useful applications for this model, including lossy image compression, clustering of face images under different lighting conditions, and video segmentation. Subspace clustering algorithms utilize the UoS model to cluster data vectors and estimate the underlying subspaces. These algorithms achieve excellent clustering performance on datasets such as the extended Yale Face Database B [4], Hopkins 155 [5], and the MNIST handwritten digit database [6]. However, as we will show in Section 4, even oracle UoS classifiers do not achieve perfect clustering on these datasets. While current algorithms for subspace clustering are unsupervised, in many cases a human could provide relevant information in the form of pairwise constraints between points, e.g., answering whether two images are of the same person or whether two digits are the same.\nThe incorporation of pairwise constraints into clustering algorithms is known as pairwise-constrained clustering (PCC). PCC algorithms use supervision in the form of must-link and cannot-link constraints by ensuring that points with must-link constraints are clustered together and points with cannot-link constraints are clustered apart. In [7], the authors investigate the phenomenon that incorporating poorly-chosen constraints can lead to an increase in clustering error, rather than a decrease as expected, since points constrained to be in the same cluster that are otherwise dissimilar can confound the constrained clustering algorithm. For this reason, researchers have turned to active query selection methods, in which constraints are intelligently selected based on a number of heuristics. Active methods such as [8] have been shown to significantly reduce clustering error with a modest number of pairwise constraints, and in [9] the authors receive constraints from people with no special training via Amazon Mechanical Turk. These algorithms perform well across a number of datasets but do not take advantage of any known structure in the data. In\nthe case where data lie on a union of subspaces, one would hope that knowledge of the underlying geometry could give hints as to which points are likely to be clustered incorrectly.\nOur contributions are as follows. We introduce a method of active query selection that leverages the known UoS structure to improve pairwise-constrained clustering. We describe two ways to measure the margin of a point in the subspace clustering context using the subspace residual as well as margin based on the affinity matrix. We present a novel PCC algorithm that utilizes the subspace estimates provided by algorithms such as Sparse Subspace Clustering (SSC) [10] to reduce clustering error as quickly as possible.\nWhile we are certainly not the first to consider actively selecting labels to improve clustering performance, to the best of our knowledge we are the first to do so with structured clusters. Structure within and between data clusters is often leveraged for unsupervised clustering [11], and that structure is also leveraged for adaptive sampling of the structured signals themselves [12, 13, 14]. This paper emphasizes the power of that structure for reducing the number of required labels in an active learning algorithm as opposed to reducing the number of samples of the signal itself, and points to exciting open questions regarding the tradeoff between signal measurements and query requirements in semi-supervised clustering.\nWe show through extensive simulations on real datasets that our algorithm achieves superior performance to state-of-the-art PCC algorithms. We achieve classification rates on ten MNIST digits that are only seen for clustering three digits in state-of-the-art unsupervised algorithms: with a modest number of queries, we get 5% classification error compared to about 30% for unsupervised algorithms and 20% for current PCC algorithms. Further, our algorithm is agnostic to the input subspace clustering algorithm and is therefore trivial to incorporate into existing work on subspace clustering."}, {"heading": "2 Related Work", "text": "A survey of recently developed subspace clustering algorithms can be found in [15] and the textbook [3]. In these and more recent work, clustering algorithms that employ spectral methods achieve the best performance on most datasets. Notable examples of such algorithms include Sparse Subspace Clustering (SSC) [10], Thresholded Subspace Clustering (TSC) [16], Spectral Local Best-Fit Flats (SLBF) [17], and Greedy Subspace Clustering (GSC) [18]. The core differences between these algorithms lie in the formation of the affinity matrix, after which spectral clustering is performed to obtain label estimates. In SSC, the affinity matrix is formed via a series of \u21131-penalized regressions. TSC thresholds the spherical distance between points. GSC works by successively (greedily) building subspaces from points likely to lie in the same subspace, and SLBF builds subspaces by leveraging the fact that in high dimensions, nearby points are likely to lie in the same subspace. Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22]. TSC and GSC also provide theoretical guarantees and, along with SLBF, are significantly less computationally-demanding than SSC. While the development of efficient algorithms with stronger guarantees has received a great deal of attention, very little attention has been paid to the question of what to do about data that cannot be correctly clustered. As we show in Section 4, even oracle PCA classifiers result in some clustering error for common datasets. Thus, when reducing clustering error to zero (or near zero) is a priority, users must look beyond unsupervised subspace clustering algorithms to alternative methods. One such method is to request some supervised input in the form of pairwise constraints, leading to the study of pairwise-constrained clustering (PCC).\nPCC algorithms work by incorporating must-link and cannot-link constraints between points, where points with must-link constraints are forced (or encouraged in the case of noisy labels) to be clustered together, and points with cannot-link constraints are forced to be in separate clusters. In many cases, these constraints can be provided by a human labeler. For example, in [9], the authors perform experiments where comparisons between human faces are provided by users of Amazon Mechanical Turk with an error rate of 1.2%. Similarly, for subspace clustering datasets such as Yale B and MNIST, a human could easily answer questions such as, \u201cAre these two faces the same person?\u201d and \u201cAre these two images the same number?\u201d An early example of PCC is found in [23], where the authors modify the K-means cost function to incorporate such constraints. In [24], the authors utilize active methods to initialize K-means in an intelligent Explore phase, during which neighborhoods of must-linked points are built up. After this phase, new points are queried against representatives from each neighborhood until a must-link is obtained. A\nsimilar explore phase is used in [25], after which a min-max approach is used to select the most uncertain sample. Early work on constrained spectral clustering appears in [26, 27], in which spectral clustering is improved by examining the eigenvectors of the affinity matrix in order to determine the most informative points. However, these methods are limited to the case of two clusters and therefore impractical in many cases.\nMore recently, the authors in [8, 9] improve constrained clustering by modeling which points will be most informative given the current clustering, with state-of-the-art results achieved on numerous datasets by the algorithm in [8] referred to as Uncertainty Reducing Active Spectral Clustering (URASC). URASC works by maintaining a set of certain sets Z = {Z1, . . . , Znc}, whereby points in the same certain set are must-linked and points in different certain sets are cannot-linked. A test point xT is selected via an uncertainty-reduction model motivated by matrix perturbation theory, after which queries are presented in an intelligent manner until xT is either matched with an existing certain set or placed in its own new certain set. In practice [28], the certain sets are initialized using the Explore algorithm of [24].\nWhile URASC and other PCC algorithms described here provide strong performance on general datasets, they do not take into account any knowledge of the underlying geometric structure in the data. To the best of our knowledge, the only such algorithm to do so is that of [29]. The algorithm described in [29] has two major drawbacks. First, the authors use a modified version of the K-subspaces algorithm, which exhibits poor performance on most benchmark datasets (see for example [15]). Second, the algorithm requires expertprovided labels rather than pairwise comparisons. In the case of handwritten digits, it is reasonable to expect that a human could easily provide these labels. However, in the case of face datasets, this requirement becomes extremely impractical. In this work, we extend ideas from PCC to leverage the underlying UoS structure that exists in many known datasets. We follow the framework laid out in [8] and provide an algorithm for improving the performance of subspace clustering that achieves significant benefits over the current state-of-the-art in PCC."}, {"heading": "3 UoS-Based Pairwise-Constrained Clustering", "text": "Let X = { xi \u2208 RD }N\ni=1 be a set of data points lying on a union of K subspaces {Sk} K k=1, each having\ndimension dk. In this work, we assume all subspaces have the same dimension, but it is possible to extend our algorithm to deal with non-uniform dimensions. Denote the true clustering of a point x \u2208 X by C(x). Let the output of a clustering algorithm (such as SSC or TSC) be an affinity/similarity matrix A and a set of label estimates { C\u0302(xi) }N\ni=1 . Our algorithm for PCC consists of an initialization and three main\nsteps. To initialize, we build a set of certain sets Z using an Explore-like algorithm similar to that of [24]. Next, a test point is obtained using either the min-margin criterion for subspaces [29] or a notion of margin based on the affinity matrix, which we define below. In the second step, the test point is queried against representatives from the certain sets until a must-link is found or all certain sets have been queried, in which case the test point becomes its own certain set. Finally, the certain sets are used to impute must-link and cannot-link values in the affinity matrix, and spectral clustering is performed. These steps (excluding the initialization) are then repeated until the maximum number of pairwise comparisons has been obtained. We outline each of these steps below and provide pseudocode in Algorithms 1 and 2. The use of certain sets relies on the assumption that the pairwise queries are answered correctly\u2014an assumption that is common in the literature [24, 25, 8]. However, in [8], the authors demonstrate that an algorithm based on certain sets still yields significant improvements under a small error rate. The study of robustly incorporating noisy pairwise comparisons is an interesting topic for further study."}, {"heading": "3.1 Sample Selection via Margin", "text": "Min-margin points have been studied extensively in active learning; intuitively, these are points that lie near the decision boundary of the current classifier. In [30], the author notes that actively querying points of minimum margin (as opposed to maximum entropy or minimum confidence) is an appropriate choice for reducing classification error. In [31], the authors present a margin-based binary classification algorithm that achieves an optimal rate of convergence (within a logarithmic factor). In this section, we consider two notions of margin\u2014one based on subspace distances and one based on the input affinity matrix."}, {"heading": "3.1.1 Residual-Based Margin", "text": "The concept of margin for subspaces was first studied in [29]. For a subspace Sk with orthogonal projection matrix Pk, let the distance of a point to that subspace be\ndist(x,Sk) = \u2016x\u2212 Pkx\u20162 .\nLet k\u2217 = arg mink\u2208[K] dist(x,Sk), where [K] = {1, 2, \u00b7 \u00b7 \u00b7 ,K}. Then the subspace margin of a point x \u2208 X is defined as [29]\n\u00b51(x) = max j 6=k\u2217,j\u2208[K]\ndist(x, Sk\u2217)\ndist(x, Sj) . (1)\nThe point of minimum margin is then defined as arg maxx\u2208X \u00b51(x). Note that the fraction is a value in [0, 1], where the larger \u00b51(x) the closer x is to the decision boundary. A value of 1 implies that the point x is equidistant to its two closest subspaces. In the following theorem, we show that points lying near the intersection of subspaces are included among those of minimum margin with high probability. This method of point selection is then motivated by the fact that the difficult points to cluster are those lying near the intersection of subspaces [12]. Further, theory for SSC ([11],[15]) shows that problematic points are those having large inner product with some or all directions in other subspaces. Subspace margin captures exactly this phenomenon.\nTheorem 1. Consider two d-dimensional subspaces S1 and S2 with corresponding orthogonal projection matrices P1 and P2. Let y = x + n, where x \u2208 S1 is such that \u2016x\u2212 P2x\u2016\n2 = \u03b32, and n \u223c N (0, \u03c32ID). Let \u00b5(y) = \u2016y \u2212 P1y\u2016 / \u2016y \u2212 P2y\u2016. Then we have\n(1 \u2212 \u03b5) \u221a \u03c32(D \u2212 d)\n(1 + \u03b5) \u221a \u03c32(D \u2212 d) + \u03b32 \u2264 \u00b5(y) \u2264\n(1 + \u03b5) \u221a \u03c32(D \u2212 d)\n(1 \u2212 \u03b5) \u221a \u03c32(D \u2212 d) + \u03b32 ,\nwith probability at least 1\u2212 4e\u2212c\u03b5 2(D\u2212d), where c is an absolute constant.\nThe proof is given in Appendix A. Note that if \u2016y \u2212 P1y\u2016 < \u2016y \u2212 P2y\u2016, then \u00b5(y) = \u00b51(y). In this case, Theorem 1 states that under the given noise model, points with small residual to the incorrect subspace (i.e., points near the intersection of subspaces) will have small margin (large \u00b51). The statement of Theorem 1 can be used to quantify how near a point must be to the intersection of two subspaces to be considered a point of minimum margin.\nConsider two points y1, y2 with corresponding x1, x2 \u2208 S1 and \u2225 \u2225P\u22a52 x1 \u2225 \u2225 = \u03b31 and \u2225 \u2225P\u22a52 x2 \u2225\n\u2225 = \u03b32. Let \u03b31 < \u03b32. In the noiseless case, this is enough to guarantee that y1 = x1 lies nearer to S2. Under the given additive noise model (yi = xi + ni for i = 1, 2) the gap between \u03b31 and \u03b32 must be larger by some factor depending on the noise level. After two applications of Theorem 1 and rearranging terms, we have that \u00b5(y1) > \u00b5(y2) with high probability if\n\u03b2\u03b322 \u2212 \u03b3 2 1 > (1\u2212 \u03b2)\u03c3 2(D \u2212 d). (2)\nwhere \u03b2 = ((1\u2212 \u03b5)/(1 + \u03b5))4, a value near 1 for small \u03b5. Equation (2) shows that the gap \u03b322 \u2212 \u03b3 2 1 must grow (approximately) with the noise level \u03c32. To get further insight, let \u03c61 \u2264 \u03c62 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c6d be the d principal angles1 between U1 and U2. If the subspaces are very far apart, 1d \u2211d i=1 sin 2(\u03c6i) is near 1, and if they are very close 1 d \u2211d i=1 sin\n2(\u03c6i) is near zero. First we note that, for any x \u2208 S1,\nsin2(\u03c61) \u2264 \u2225 \u2225P\u22a52 x \u2225 \u2225 2 \u2264 sin2(\u03c6d) ,\nthat is, there are bounds on \u2225 \u2225P\u22a52 x \u2225 \u2225 2 depending on the relationship of the two subspaces. So while we need the gap \u03b322 \u2212 \u03b3 2 1 to be large, the size of this gap is limited by the angles between the subspaces. We also know that if x = U1a for a \u223c N (0, 1 dId), then\nE \u2225 \u2225P\u22a52 x \u2225 \u2225 2 =\n1\nd\nd \u2211\ni=1\nsin2(\u03c6i) .\n1See [32] for a definition of principal angles.\nGiven this, we might imagine that margin is useful in a scenario where sin2(\u03c61) is small but 1 d \u2211d i=1 sin 2(\u03c6i) is not, e.g., when the subspaces have intersection but are distant in other directions. With this in mind we state the following corollary.\nCorollary 1. Let s = (\n1 d \u2211d i=1 sin 2(\u03c6i) ) and for x1 \u2208 S1 fix \u03b3 2 1 = sin 2(\u03c61) + \u03b4s for some small \u03b4. For\nx2 \u2208 S1, let x2 = U1a where a \u223c N (0, 1 dId). Let the noise scale like \u03c3 2 = \u03b1/D and \u03c4 > 1 be such that\n\u03c4\n(\nsin2(\u03c61) + 1\n6 \u03b1\n(\nD \u2212 d\nD\n))\n\u2264 1\nd\nd \u2211\ni=1\nsin2(\u03c6i) ,\nthat is, the average angle is sufficiently larger than the smallest angle. Then if\n\u03b4 < 5\n7 \u2212\n1\n\u03c4\nwe have \u00b5(y1) > \u00b5(y2) with probability at least 1\u2212e \u2212c( 7100 ) 2 ds\u22124e\u2212c( 1 50 ) 2 (D\u2212d) where c is an absolute constant.\nThe proof is given in Appendix A. From the definition of \u03b321 = sin 2(\u03c61) + \u03b4s, we see that the size of \u03b4 determines how close x1 is to the closest vectors in the two subspaces. Therefore, this corollary shows us that if the two subspaces are close in some directions, and \u03b4 is small so that x1 is aligned with those directions, then it will have smaller margin (higher \u00b5) than a point randomly drawn from S1 with high probability."}, {"heading": "3.1.2 Affinity-Based Margin", "text": "We also consider a version of margin calculated from the entries of the affinity matrix itself, which is inspired by the nonparametric entropy estimation in [8]. Given an affinity matrix A, we estimate the probability that a point xi is in subspace k as\nP (k|xi) =\n\u2211\nj:C\u0302(xj)=k Aij \u2211N j=1 Aij .\nLet l\u2217 = arg maxl\u2208[K] P (l|x). Define the affinity margin of a point x \u2208 X as\n\u00b52(x) = max j 6=l\u2217,j\u2208[K]\nP (j|x) P (l\u2217|x) . (3)\nThe point of minimum margin again follows as arg maxx\u2208X \u00b52(x)."}, {"heading": "3.2 Pairwise Constrained Clustering with SUPERPAC", "text": "We now describe in more detail our algorithm for PCC when data lie near a union of subspaces, which we refer to as SUPERPAC (SUbsPace clustERing with Pairwise Active Constraints). The algorithm begins by initializing a set of disjoint certain sets, a process we will describe below. After initializing the certain sets, our algorithm assigns the points most likely to be misclassified to certain sets by presenting a series of pairwise comparisons. Let xT be the test point chosen as the min-margin point using either (1) or (3). For each certain set Zk, the representative xk is chosen as the maximum-margin point within the set. Next, for each k, we let Uk be the d-dimensional PCA estimate of the matrix whose columns are the points {\nx \u2208 X : C\u0302(x) = C\u0302(xk) }\n. We then query our test point xT against the representatives xk in order of residual \u2225 \u2225xT \u2212 UkU T k xT \u2225 \u2225 2 (smallest first). If a must-link constraint is found, we place xT in the corresponding certain set. Otherwise, we place xT in its own certain set and update the number of certain sets. Finally, we impute values onto the affinity matrix for all points in the certain sets and perform spectral clustering. The process is then repeated until the maximum number of pairwise comparisons has been reached. Pseudocode for the complete algorithm is given in Algorithm 1. As a technical note, we first normalize the input affinity matrix A so that the maximum value is 1. For must-link constraints, we impute a value of 1 in the affinity matrix,\nAlgorithm 1 SUPERPAC\nInput: X = {x1, x2, . . . , xN}: data, K: number of clusters, d: subspace dimension, A: affinity matrix, maxQueries: maximum number of pairwise comparisons Estimate Labels: C\u0302 \u2190 SpectralClustering(A,K) Initialize Certain Sets: Initialize Z = {Z1, \u00b7 \u00b7 \u00b7 , Znc} and numQueries via UoS-Explore in Algorithm 2 while numQueries < maxQueries do Obtain Test Point: select xT \u2190 arg maxx\u2208X \u00b51(x) or arg maxx\u2208X \u00b52(x) Assign xT to Certain Set:\nSort {Z1, \u00b7 \u00b7 \u00b7 , Znc} in order of most likely must-link (according to UoS model), query xT against representatives from Zk until must-link constraint is found or k = nc. If no must-link constraint is found, set Z \u2190 {Z1, \u00b7 \u00b7 \u00b7 , Znc , {xT }} and increment nc. Impute Constraints: Set Aij = Aji = 1 for (xi, xj) in the same certain set and Aij = Aji = 0 for (xi, xj) in different certain sets (do not impute for points absent from certain sets)\nEstimate Labels: C\u0302 \u2190 SpectralClustering(A,K) end while\nwhile for cannot-link constraints we impute a 0. While this does not strictly enforce the constraints, the approach is common in the literature.\nSUPERPAC can be thought of as an extension of ideas from PCC literature [24, 9, 8] to leverage prior knowledge about the underlying geometry of the data. For datasets such as Yale B and MNIST, the strong subspace structure makes Euclidean distance a poor proxy for similarity between points in the same cluster, leading to the superior performance of our algorithm demonstrated in the following sections. Note that this structure does not exist in all datasets, in which case we expect our algorithm to perform worse than current PCC algorithms. The reader will note we made a choice to order the certain sets according to the UoS model; this is similar to the choice in [8] to query according to similarity, where our notion of similarity here is based on subspace distances, rather than some other metric. We found this resulted in significant performance benefits, matching our intuition that points are clustered based on their nearest subspace, rather than the nearest centroid. In contrast to [9, 8], where the test point is chosen according to a global improvement metric, we choose test points according to their classification margin. In our experiments, we found margin (especially subspace margin) to be a strong indicator of which points are misclassified, meaning that our algorithm rapidly corrects the errors that occur as a result of unsupervised subspace clustering. While we only discuss margin as defined by (3) and (1), other criteria for margin are possible. In our experiments, we explored notions of margin based on the \u21131 regression coefficients from SSC, similar to the Sparsity Concentration Index of [11]. However, we found these notions resulted in decreased performance while incurring a significant computational burden due to the \u21131 minimization.\nWe now describe the process of initializing the certain sets. Note that this step is not necessary, as we could initialize all certain sets to be empty, but we found it led to improved performance experimentally. A main distinction between subspace clustering and the general clustering problem is that in the UoS model points can lie arbitrarily far from each other but still be on or near the same subspace. For this reason, the Explore algorithm from [24] is unlikely to quickly find points from different clusters in an efficient manner. Here we define an analogous algorithm for the UoS case, termed UoS-Explore, with pseudocode given in Algorithm 2. The goal of UoS-Explore is to find K certain sets, each containing as few points as possible (ideally a single point), allowing us to more rapidly assign test points to certain sets in Algorithm 1. We begin by selecting our test point xT as the most certain point, or the point of maximum margin (minimum \u00b51 or \u00b52, depending on the choice of margin), and placing it in its own certain set. We then iteratively select xT as the point of maximum margin that (1) is not in any certain set and (2) has a different cluster estimate from all points in the certain sets. If no such point exists, we choose uniformly at random from all points not in any certain set. This point is queried against a single representative from each certain set according to the UoS model as above until either a must-link is found or all set representatives have been queried, in which case xT is added to a new certain set. This process is repeated until either K certain sets have been created or a terminal number of queries have been used. As points of maximum margin are\nAlgorithm 2 UoS-Explore\nInput: X = {x1, x2, . . . , xN}: data, K: number of subspaces, d: dimension of subspaces, A: affinity matrix, maxQueries: maximum number of pairwise comparisons Estimate Labels: C\u0302 \u2190 SpectralClustering(A,K) Calculate Margin: Calculate margin according to (1) or (3) and set x\u2228 as the point of maximum margin (most confident point) Initialize Certain Sets: Z1 = x\u2228, Z = {Z1}, numQueries \u2190 0, nc \u2190 1 while nc < K and numQueries < maxQueries do Obtain Test Point: Choose xT as point of maximum margin such that C\u0302(xT ) 6= C\u0302(x \u2208 Zk) for any k. If no such xT exists, choose xT at random. Assign xT to Certain Set:\nSort {Z1, \u00b7 \u00b7 \u00b7 , Znc} in order of most likely must-link (according to UoS model), query xT against representatives from Zk until must-link constraint is found or k = nc. If no must-link constraint found, set Z \u2190 {Z1, \u00b7 \u00b7 \u00b7 , Znc , {xT }} and increment nc.\nend while\nmore likely to be correctly clustered than other points in the set, we expect that by choosing points whose estimated labels indicate they do not belong to any current certain set, we will quickly find a point with no must-link constraints. We show in Section 4 that this algorithm finds all certain sets in nearly the lower limit of K(K \u2212 1)/2 queries on the Yale dataset."}, {"heading": "4 Experimental Results", "text": "We compare the performance of our method and the nonparametric version of the URASC algorithm (URASC-N)2 over a variety of datasets. We refer to the residual-based margin (1) form of our algorithm as SUPERPAC-R, and the affinity-based (3) form as SUPERPAC-A. We use a maximum query budget of 2K for UoS-Explore and Explore. We also compared with the algorithm from [9] but found this algorithm performed worse than URASC with a far greater computational cost. For completeness, we also compare to random constraints, in which queries are chosen uniformly at random from the set of unqueried pairs.\nFinally, we compare against the oracle PCA classifier, which we now define. Let Uk be the d-dimensional PCA estimate of the points whose true label C(x) = k. Then the oracle label is\nC\u0302o(x) = arg min k\u2208[K]\n\u2225 \u2225x\u2212 UkU T k x \u2225 \u2225 2 .\nThis allows us to quantitatively capture the idea that, because the true classes are not perfectly low-rank, some points may not actually be clustered with the low-rank approximation of their own true cluster. In our experiments, we also compared with oracle robust PCA [33] implemented via the augmented Lagrange multiplier method [34] but did not find any improvement in classification error.\nWe consider three datasets commonly used as benchmarks in the subspace clustering literature3 and show that taking the underlying UoS structure into account significantly improves the performance of pairwise constrained clustering. We also compare on datasets without subspace structure to URASC, random queries, and an Active Random algorithm described below."}, {"heading": "4.1 Extended Yale Face Database B", "text": "The Yale B dataset consists of 64 images of size 192 \u00d7 168 of each of 38 different subjects under a variety of lighting conditions. We follow the methodology of [17] and perform clustering on 100 randomly selected subsets of size K. We use the output of SSC [10] as our initial affinity matrix, as SSC is known to have the best performance on this set, and choose d = 9 as is common in the literature [10, 16]. Fig. 1 shows the\n2In our experiments, the parametric version of URASC was found to be numerically unstable and did not have significantly different performance from URASC-N in the best cases.\n3The validity of the UoS assumption for these datasets is investigated in [10, 16].\nmisclassification rate versus number of pairwise comparisons obtained for K = 2, 5, 7, and 10 faces. We space out the markers for clearer plots. The figure clearly demonstrates the benefits of leveraging UoS structure in constrained clustering. The two SUPERPAC methods perform roughly the same, with residual min-margin as defined in (1) achieving stronger, more stable performance. For K = 2, only 20 pairwise queries are required to surpass the performance of the oracle PCA classifier. For the cases of K = 5 and 7, roughly 2Kd queries are required, and for K = 10 roughly 3Kd queries are required. This demonstrates the significant improvement over the results in [29], where 3Kd labels are required to surpass oracle performance; providing oracle labels (in this case, stating who a person is) requires far more expertise than pairwise comparisons (answering whether two images are of the same person). Note that URASC appears flat for K = 2 and appears to have worse performance as the number of queries increases for higher values of K. This is due to the previously mentioned fact that imputing the wrong constraints can lead to worse (or in this case, the same) clustering performance. We show below that for sufficiently many queries, the error decreases as expected.\nNext, we show the effectiveness of the UoS-Explore algorithm over Explore used in [24, 8]. We run the algorithms on 100 random subsets of K faces and report the average number of queries required to obtain K certain sets in Table 1. The table shows that UoS-Explore uses far fewer queries to obtain K unique certain sets, with residual-based margin using very near the minimum required K(K \u2212 1)/2 queries. Affinity-based margin has smaller gains due to the fact that the margin itself ignores the underlying subspace structure. Note that in SUPERPAC and URASC, the query budget for this initialization step is limited in practice, and hence our method is more likely to discover K disjoint certain sets.\nWe also compare the query selection methods in terms of computational times on this dataset. Table 2 shows the average time per query for each of the three query selection methods along with the maximum deviation. Random querying can be selected offline and requires negligible computational time. Of the active methods, URASC is the most computationally efficient. The SUPERPAC methods require more time due to the necessity of computing the singular value decomposition to obtain a test point (for SUPERPAC-R) and to order the certain sets (for both methods). However, the required computational times are not prohibitive, especially given the resulting gains in clustering performance and the fact that the delay of querying the user will dominate. Improving the computational complexity of our algorithm is a topic for our future research."}, {"heading": "4.2 Handwritten Digits", "text": "We consider the MNIST handwritten digit database test dataset, which consists of 10,000 centered 28 \u00d7 28 pixel images of handwritten digits 0-9. In previous papers [17, 16], only sets of K = 2 and 3 digits were considered, and then only specific digits. Higher values of K have been considered more recently [35, 36, 37] and will be incorporated into our future work. In this section, we demonstrate the effectiveness of PCC at dramatically decreasing the clustering error with a modest number of queries. We follow a similar\nmethodology to the previous section and select 100 random subsets of size K, using subspace dimension d = 3 as in [16]. To demonstrate that our algorithm is agnostic to the input clustering method, we use TSC [16] to obtain the input affinity matrix. We use Nk = 100 points in each subspace, the regime in which TSC has been shown to outperform SSC on this dataset [16]. Fig. 2 shows the misclassification rate versus number of pairwise constraints for K = 2, 5, 7, and 10 digits. The figure again demonstrates the effectiveness of utilizing UoS structure when it exists. Although not pictured, we found in our experiments that [29] again requires as many labels as SUPERPAC requires pairwise comparisons. In the case of MNIST digits, labels are relatively easy to obtain, and we expect that incorporating full labels into our algorithm would result in performance benefits.\nFig. 3 shows the misclassification error for both the Yale B and MNIST datasets for a large number of pairwise comparisons. We see for both datasets that URASC does outperform random constraints but requires many more queries than our algorithm.\nTo demonstrate the flexibility of our algorithm, we run SUPERPAC-R starting with the output affinity matrix from SSC, TSC, and GSC. Fig. 4 shows the resulting misclassification rates for Yale B with K = 5 and 10 and MNIST with K = 5 and 10. In all three cases, the error decreases rapidly and significantly outperforms URASC. During the preparation of this manuscript, we became aware of [35, 36, 37], where more scalable algorithms for subspace clustering have been developed. We also ran SUPERPAC using the output of Elastic Net Subspace Clustering on the Coil-100 dataset. SUPERPAC continues to outperform URASC and drives the error from 30% to 1% after roughly 5000 comparisons. Further comparisons on large datasets are part of our ongoing work."}, {"heading": "4.3 Motion Data", "text": "Finally, we compare the performance of the various methods on the Hopkins 155 dataset. This set consists of 155 video sequences with tagged feature points on rigidly moving objects. These points have been shown to lie in a union of affine subspaces of dimension at most 3 [38]. The 155 sequences contain K = 2 and 3 motions depending on the sequence. We compare the performance as the number of pairwise comparisons is increased with the input affinity matrix obtained by SSC. The results are shown in Fig. 5. With this dataset, where the subspaces are affine and so the centers may not be close together, we see that the URASC algorithm does as well as SUPERPAC, despite not leveraging the subspace structure. Modifying SUPERPAC to account for the affine structure is a topic for our future research."}, {"heading": "4.4 Evaluation of Selection Methods", "text": "We briefly examine the importance of the methods of selecting test points and demonstrate that in the case of URASC, the algorithm framework of placing test points in certain sets is what provides the main benefit. In the following figures, we compare SUPERPAC, URASC, and random queries with \u201cActive Random\u201d querying. Active Random querying works by following the same framework as URASC, comparing to points already within certain sets, but chooses test points at random and queries against certain sets at random. We also remove the Explore step at initialization. In contrast to purely random label requests, Active Random makes use of all available query information, providing a more realistic benchmark for active PCC algorithms.\nWe first compare the performance of URASC and both random algorithms on the Sonar, Balance, and Leaf-250 datasets, all of which are used in [8] as benchmarks. We do not compare with SUPERPAC on the Leaf dataset as only the affinity matrix is publicly accessible [39], though we do not expect any performance\nbenefits over URASC. Fig. 6 shows the average clustering error over 10 trials as a function of pairwise comparisons for these datasets, using d = 1 for SUPERPAC. We see that all algorithms following the certain set framework achieve roughly the same performance when the number of clusters is low, demonstrating that it is the active framework wherein we compare against points in certain sets that provides the most benefit in this case. For the Leaf dataset, we have K = 25 clusters, which makes querying the test point against representatives from each certain set very inefficient.\nFinally, we compare the performance of SUPERPAC, URASC, and the two random algorithms on the Yale B and MNIST datasets for 100 random subsets of K = 5 and K = 10 subjects, with the results shown in Fig. 7. Here we see again that URASC provides only very modest benefits over the Active Random algorithm, whereas choosing points by subspace margin results in a significant performance increase."}, {"heading": "5 Conclusion", "text": "We have presented a method of selecting and incorporating pairwise constraints into subspace clustering that considers the underlying geometric structure of the problem. The union of subspaces model is often used in computer vision applications where it is possible to request input from human labelers in the form of pairwise constraints. We showed that labeling is often necessary for subspace classifiers to achieve a clustering error near zero, as even an oracle PCA approach does not allow perfect clustering; additionally, these constraints improve the clustering procedure overall and allow for perfect clustering with a modest number of requests for human input.\nWe see this work as a bridge between adaptive query selection for pairwise constrained clustering and adaptive sampling for structured signals (e.g., see previous work on sparse [12, 13], structured sparse [14], and low rank signals [40]). Several works apply ideas of compressive sensing to clustering [41, 42] and\nclassification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47]. A key interesting open question is whether the number of active queries required for clustering increases as the compression of the data increases. To address this we first plan to develop techniques for handling noisy query responses. In the case of face images, one may assume that compressed data would be harder to distinguish, leading to noisier query responses.\nFinally, we saw that for datasets with different types of cluster structure, the structure assumptions of each algorithm had direct impact on performance; in the future we plan to additionally develop techniques for learning from unlabeled data whether the union of subspace model or a standard clustering approach is more appropriate."}, {"heading": "A Proof of Theorem 1", "text": "The proof relies on theorem 5.2.1 from [48], restated below.\nTheorem 2. (Concentration on Gauss space) Consider a random vector X \u223c N (0, \u03c32ID) and a Lipschitz function f : RD \u2192 R. Then for every t \u2265 0,\nPr {|f(X)\u2212 Ef(X)| \u2265 t} \u2264 2 exp\n(\n\u2212 ct2\n\u03c32 \u2016f\u20162Lip\n)\n,\nwhere \u2016f\u2016Lip is the Lipschitz constant of f .\nFirst consider the numerator and note that y \u2212 P1y = P\u22a51 y \u223c N (0, \u03c3 2P\u22a51 ) with\nE \u2225 \u2225P\u22a51 y \u2225 \u2225 2 = \u03c32(D \u2212 d).\nLet f(z) = \u2016z\u20162, for which \u2016f\u2016Lip = 1. Further, by Exercise 5.2.5 of [48], we can replace E \u2016X\u20162 by (\nE \u2016X\u201622\n)1/2\nin the concentration inequality. Applying Thm. 2 to the above, we see that\nPr {\u2223 \u2223\n\u2223\n\u2225 \u2225P\u22a51 y \u2225 \u2225\u2212 \u221a\n\u03c32(D \u2212 d) \u2223 \u2223 \u2223 \u2265 t } \u2264 2 exp\n(\n\u2212 ct2\n\u03c32\n)\n. (4)\nSimilarly, for the denominator, note that y \u2212 P2y = P\u22a52 y \u223c N (P \u22a5 2 x, \u03c3 2P\u22a52 ) with\nE \u2225 \u2225P\u22a52 y \u2225 \u2225 2 = \u03c32(D \u2212 d) + \u03b32.\nSince P\u22a52 y is no longer centered, we let g(z) = z + P \u22a5 2 x, which also has \u2016g\u2016Lip = 1. Applying Thm. 2 to the centered random vector y\u0304 \u223c N (0, \u03c32P\u22a52 ) with Lipschitz function h = f \u25e6 g, we have that\nPr {\u2223 \u2223\n\u2223\n\u2225 \u2225P\u22a52 y \u2225 \u2225\u2212 \u221a\n\u03c32(D \u2212 d) + \u03b32 \u2223 \u2223 \u2223 \u2265 t } \u2264 2 exp\n(\n\u2212 ct2\n\u03c32\n)\n. (5)\nLetting t = \u03b5 \u221a \u03c32(D \u2212 d) in (4) and t = \u03b5 \u221a \u03c32(D \u2212 d) + \u03b32 in (5) yields\n(1\u2212 \u03b5) \u221a \u03c32(D \u2212 d) \u2264 \u2225 \u2225P\u22a51 y \u2225 \u2225 \u2264 (1 + \u03b5) \u221a \u03c32(D \u2212 d)\nand (1\u2212 \u03b5) \u221a \u03c32(D \u2212 d) + \u03b32 \u2264 \u2225\n\u2225P\u22a52 y \u2225 \u2225 \u2264 (1 + \u03b5) \u221a \u03c32(D \u2212 d) + \u03b32,\neach with probability at least 1 \u2212 2 exp ( \u2212c\u03b52(D \u2212 d) )\n(since \u03b3 > 0). Applying the union bound gives the statement of the theorem."}, {"heading": "B Proof of Corollary 1", "text": "We make some remarks first to connect our results to other subspace distances that are often used. Perhaps the most intuitive form of subspace distance between that spanned by U1 and U2 is 1 d\u2016(I\u2212U1U1)\nTU2\u20162F ; if the two subspaces are the same, the projection onto the orthogonal complement is zero; if they are orthogonal, we get the norm of U2 alone, giving a distance of 1. This is equal to the more visually symmetric 1\u2212 1 d\u2016U T 1 U2\u2016 2 F , another common distance. Further we note that, by the definition of principal angles [32],\n1\u2212 1\nd \u2016UT1 U2\u2016 2 F = 1\u2212\n1\nd\nd \u2211\ni=1\ncos2(\u03c6i) = 1\nd\nd \u2211\ni=1\nsin2(\u03c6i) .\nProof. We have from Theorem 1 that\n\u00b5(y2) \u2264 (1 + \u03b5)\n\u221a\n\u03c32(D \u2212 d)\n(1\u2212 \u03b5) \u221a \u03c32(D \u2212 d) + \u03b322 and\n(1\u2212 \u03b5) \u221a \u03c32(D \u2212 d)\n(1 + \u03b5) \u221a \u03c32(D \u2212 d) + \u03b321 \u2264 \u00b5(y1)\nwith probability at least 1 \u2212 4e\u2212c\u03b5 2(D\u2212d). Therefore if we get the upper bound of \u00b5(y2) to be smaller than the lower bound of \u00b5(y1), we are done, as we described above in Equation (2). Rearranging this desired inequality we see that we need\n\u03b321 < \u03b2 4\u03b322 \u2212 (1\u2212 \u03b2 4)\u03c32(D \u2212 d). (6)\nwhere \u03b2 = (1\u2212 \u03b5)/(1 + \u03b5). Let \u03b5 be such that \u03b24 = 5/6, and let \u03b321 = sin 2(\u03c61) + \u03b4s as in the theorem. Then we wish to select \u03b4 to satisfy\n\u03b4 < 5 6\u03b3 2 2 \u2212 sin 2(\u03c61)\u2212 1 6\u03c3 2(D \u2212 d)\ns . (7)\nApplying concentration with \u03b322 , we have that \u03b3 2 2 \u2265 (1\u2212 \u03be) 2s with probability at least 1\u2212 e\u2212c\u03be 2ds where c is an absolute constant. Therefore taking \u03be to be such that (1\u2212 \u03be)2 = 6/7, we require\n\u03b4 < 5 7s\u2212 sin 2(\u03c61)\u2212 1 6\u03c3 2(D \u2212 d)\ns =\n5 7 \u2212 1 \u03c4\nwhere we used the definition of \u03c4 in the theorem. To quantify the probability we need the appropriate values for \u03b5 and \u03be; we lower bound both with simple fractions: 1/50 < \u03b5 where ((1\u2212 \u03b5)/(1 + \u03b5))4 = \u03b2 = 5/6 and 7/100 < \u03be where (1 \u2212 \u03be)2 = 6/7. Applying the union bound with the chosen concentration values implies that \u00b5(y1) > \u00b5(y2) holds with probability at least 1\u2212 e \u2212c( 7100 ) 2 ds \u2212 4e\u2212c( 1 50 ) 2 (D\u2212d)."}], "references": [{"title": "Lambertian reflectance and linear subspaces", "author": ["R. Basri", "D. Jacobs"], "venue": "IEEE TPAMI, vol. 25, no. 2, pp. 218\u2013233, February 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "A bayesian computer vision system for modeling human interactions", "author": ["N. Oliver", "B. Rosario", "A. Pentland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 831\u2013843, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence, vol. 23, no. 6, pp. 643\u2013660, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "A benchmark for the comparison of 3-D motion segmentation algorithms", "author": ["R. Tron", "R. Vidal"], "venue": "IEEE Int. Conf. on Comp. Vision and Pattern Recog., 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J.C. Burges"], "venue": "2016. [Online]. Available: yann.lecun.com/exdb/mnist", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Measuring constraint-set utility for partitional clustering algorithms", "author": ["I. Davidson", "K.L. Wagstaff", "S. Basu"], "venue": "Proc. European Conf. on Machine Learning and Prinicpals and Practice of Knowledge Discovery in Databases, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Active clustering with model-based uncertainty reduction", "author": ["C. Xiong", "D.M. Johnson", "J.J. Corso"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence, 2016, accepted for publication.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Active image clustering with pairwise constraints from humans", "author": ["A. Biswas", "D. Jacobs"], "venue": "International Journal on Computer Vision, vol. 108, pp. 133\u2013147, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 35, pp. 2765\u20132781, Nov. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 31, pp. 210\u2013227, Feb. 2009. 15", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Distilled sensing: Adaptive sampling for sparse detection and estimation", "author": ["J. Haupt", "R.M. Castro", "R. Nowak"], "venue": "IEEE Transactions on Information Theory, vol. 57, no. 9, pp. 6222\u20136235, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "On the power of adaptivity in sparse recovery", "author": ["P. Indyk", "E. Price", "D.P. Woodruff"], "venue": "Foundations of Computer Science (FOCS), 2011 IEEE 52nd Annual Symposium on. IEEE, 2011, pp. 285\u2013294.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "On the fundamental limits of recovering tree sparse vectors from noisy linear measurements", "author": ["A. Soni", "J. Haupt"], "venue": "IEEE Transactions on Information Theory, vol. 60, no. 1, pp. 133\u2013149, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Processing Magazine, vol. 28, pp. 52\u201368, Mar. 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust subspace clustering via thresholding", "author": ["R. Heckel", "H. B\u00f6lcskei"], "venue": "IEEE Trans. Inf. Theory, vol. 24, no. 11, pp. 6320\u20136342, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Hybrid linear modeling via local best-fit flats", "author": ["T. Zhang", "A. Szlam", "Y. Wang", "G. Lerman"], "venue": "International Journal of Computer Vision, vol. 100, pp. 217\u2013240, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Greedy subspace clustering", "author": ["D. Park", "C. Caramanis", "S. Sanghavi"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2753\u20132761.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A Geometric Analysis of Subspace Clustering with Outliers", "author": ["M. Soltanolkotabi", "E.J. Candes"], "venue": "The Annals of Statistics, vol. 40, no. 4, pp. 2195\u20132238, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust Subspace Clustering", "author": ["\u2014\u2014"], "venue": "The Annals of Statistics, vol. 42, no. 2, pp. 669\u2013699, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Noisy sparse subspace clustering", "author": ["Y.-X. Wang", "H. Xu"], "venue": "Proceedings of The 30th International Conference on Machine Learning, 2013, pp. 89\u201397.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph connectivity in noisy sparse subspace clustering", "author": ["Y. Wang", "Y.-X. Wang", "A. Singh"], "venue": "Proceedings of The 19th International Conference on Artificial Intelligence and Statistics, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Constrained K-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schroedl"], "venue": "Proc. Int. Conf. on Machine Learning, 2001.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Active semi-supervision for pairwise constrained clustering", "author": ["S. Basu", "A. Banerjee", "R.J. Mooney"], "venue": "Proc. SIAM Int. Conf. on Data Mining, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Active query selection for semi-supervised clustering", "author": ["P.K. Mallapragada", "R. Jin", "A.K. Jain"], "venue": "Proc. Int. Conf. on Pattern Recognition, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Active constrained clustering by examining spectral eigenvectors", "author": ["Q. Xu", "M. desJardins", "K.L. Wagstaff"], "venue": "Proc. 8th Int. Conf. on Discovery Science, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Active spectral clustering", "author": ["X. Wang", "I. Davidson"], "venue": "Proc. 10th Int. Conf. on Data Mining, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Margin-based active subspace clustering", "author": ["J. Lipor", "L. Balzano"], "venue": "Proc. Int. Workshop on Computational Advances in Multi-Sensor Adaptive Processing, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Active Learning", "author": ["B. Settles"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Noise-adaptive margin-based active learning for multi-dimensional data and lower bounds under tsybakov noise", "author": ["Y. Wang", "A. Singh"], "venue": "Proc. AAAI Conference on Artificial Intellgence, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix Computations", "author": ["G. Golub", "C.V. Loan"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Robust principal component analysis?", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Linearized alternating direction method with adaptive penalty for low-rank representation", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "Advances in Neural Information Processing Systems, 2011. 16", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable sparse subspace clustering by orthogonal matching pursuit", "author": ["C. You", "D.P. Robinson", "R. Vidal"], "venue": "Proc. IEEE International Conference on Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Oracle based active set algorithm for scalable elastic net subspace clustering", "author": ["C. You", "C.-G. Li", "D.P. Robinson", "R. Vidal"], "venue": "Proc. IEEE International Conference on Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Online low-rank subspace clustering by basis dictionary pursuit", "author": ["J. Shen", "P. Li", "H. Xu"], "venue": "Proc. International Conference on Machine Learning, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Shape and motion from image streams under orthography", "author": ["C. Tomasi", "T. Kanade"], "venue": "Int\u2019l J. Computer Vision, vol. 9, no. 2, pp. 137\u2013154, 1992.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1992}, {"title": "Low-rank matrix and tensor completion via adaptive sampling", "author": ["A. Krishnamurthy", "A. Singh"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 836\u2013844.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast approximate text document clustering using compressive sampling", "author": ["L.A. Park"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2011, pp. 565\u2013580.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Compressive clustering of high-dimensional data", "author": ["A. Ruta", "F. Porikli"], "venue": "Machine Learning and Applications (ICMLA), 2012 11th International Conference on, vol. 1. IEEE, 2012, pp. 380\u2013385.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Performance limits of compressive sensing-based signal classification", "author": ["T. Wimalajeewa", "H. Chen", "P.K. Varshney"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 6, pp. 2758\u20132770, 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "A deterministic analysis of noisy sparse subspace clustering for dimensionality-reduced data", "author": ["Y. Wang", "Y.-X. Wang", "A. Singh"], "venue": "Proc. of Int. Conf. on Machine Learning (ICML), 2015, pp. 1422\u20131431.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Necessary and sufficient conditions for sketched subspace clustering", "author": ["D. Pimentel-Alarcon", "L. Balzano", "R. Nowak"], "venue": "2016, submitted to Allerton.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P. Martinsson", "J. Tropp"], "venue": "SIAM Review, vol. 53, no. 2, pp. 217\u2013288, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "RandNLA: Randomized numerical linear algebra", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "Communications of the ACM, vol. 59, no. 6, pp. 80\u201390, 2016.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "A Course in High Dimensional Probability, 2016", "author": ["R. Vershynin"], "venue": "[Online]. Available: www-personal.umich.edu/\u223cromanv/teaching/2015-16/626/HDP-book.pdf", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction The union of subspaces (UoS) model, in which data vectors lie near one of several subspaces, has been used actively in the computer vision community on datasets ranging from images of objects under various lighting conditions [1] to visual surveillance tasks [2].", "startOffset": 241, "endOffset": 244}, {"referenceID": 1, "context": "1 Introduction The union of subspaces (UoS) model, in which data vectors lie near one of several subspaces, has been used actively in the computer vision community on datasets ranging from images of objects under various lighting conditions [1] to visual surveillance tasks [2].", "startOffset": 274, "endOffset": 277}, {"referenceID": 2, "context": "These algorithms achieve excellent clustering performance on datasets such as the extended Yale Face Database B [4], Hopkins 155 [5], and the MNIST handwritten digit database [6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "These algorithms achieve excellent clustering performance on datasets such as the extended Yale Face Database B [4], Hopkins 155 [5], and the MNIST handwritten digit database [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "These algorithms achieve excellent clustering performance on datasets such as the extended Yale Face Database B [4], Hopkins 155 [5], and the MNIST handwritten digit database [6].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "In [7], the authors investigate the phenomenon that incorporating poorly-chosen constraints can lead to an increase in clustering error, rather than a decrease as expected, since points constrained to be in the same cluster that are otherwise dissimilar can confound the constrained clustering algorithm.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Active methods such as [8] have been shown to significantly reduce clustering error with a modest number of pairwise constraints, and in [9] the authors receive constraints from people with no special training via Amazon Mechanical Turk.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "Active methods such as [8] have been shown to significantly reduce clustering error with a modest number of pairwise constraints, and in [9] the authors receive constraints from people with no special training via Amazon Mechanical Turk.", "startOffset": 137, "endOffset": 140}, {"referenceID": 8, "context": "We present a novel PCC algorithm that utilizes the subspace estimates provided by algorithms such as Sparse Subspace Clustering (SSC) [10] to reduce clustering error as quickly as possible.", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "Structure within and between data clusters is often leveraged for unsupervised clustering [11], and that structure is also leveraged for adaptive sampling of the structured signals themselves [12, 13, 14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Structure within and between data clusters is often leveraged for unsupervised clustering [11], and that structure is also leveraged for adaptive sampling of the structured signals themselves [12, 13, 14].", "startOffset": 192, "endOffset": 204}, {"referenceID": 11, "context": "Structure within and between data clusters is often leveraged for unsupervised clustering [11], and that structure is also leveraged for adaptive sampling of the structured signals themselves [12, 13, 14].", "startOffset": 192, "endOffset": 204}, {"referenceID": 12, "context": "Structure within and between data clusters is often leveraged for unsupervised clustering [11], and that structure is also leveraged for adaptive sampling of the structured signals themselves [12, 13, 14].", "startOffset": 192, "endOffset": 204}, {"referenceID": 13, "context": "2 Related Work A survey of recently developed subspace clustering algorithms can be found in [15] and the textbook [3].", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "Notable examples of such algorithms include Sparse Subspace Clustering (SSC) [10], Thresholded Subspace Clustering (TSC) [16], Spectral Local Best-Fit Flats (SLBF) [17], and Greedy Subspace Clustering (GSC) [18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 14, "context": "Notable examples of such algorithms include Sparse Subspace Clustering (SSC) [10], Thresholded Subspace Clustering (TSC) [16], Spectral Local Best-Fit Flats (SLBF) [17], and Greedy Subspace Clustering (GSC) [18].", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "Notable examples of such algorithms include Sparse Subspace Clustering (SSC) [10], Thresholded Subspace Clustering (TSC) [16], Spectral Local Best-Fit Flats (SLBF) [17], and Greedy Subspace Clustering (GSC) [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "Notable examples of such algorithms include Sparse Subspace Clustering (SSC) [10], Thresholded Subspace Clustering (TSC) [16], Spectral Local Best-Fit Flats (SLBF) [17], and Greedy Subspace Clustering (GSC) [18].", "startOffset": 207, "endOffset": 211}, {"referenceID": 8, "context": "Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22].", "startOffset": 200, "endOffset": 216}, {"referenceID": 18, "context": "Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22].", "startOffset": 200, "endOffset": 216}, {"referenceID": 19, "context": "Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22].", "startOffset": 200, "endOffset": 216}, {"referenceID": 20, "context": "Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22].", "startOffset": 200, "endOffset": 216}, {"referenceID": 7, "context": "For example, in [9], the authors perform experiments where comparisons between human faces are provided by users of Amazon Mechanical Turk with an error rate of 1.", "startOffset": 16, "endOffset": 19}, {"referenceID": 21, "context": "Similarly, for subspace clustering datasets such as Yale B and MNIST, a human could easily answer questions such as, \u201cAre these two faces the same person?\u201d and \u201cAre these two images the same number?\u201d An early example of PCC is found in [23], where the authors modify the K-means cost function to incorporate such constraints.", "startOffset": 236, "endOffset": 240}, {"referenceID": 22, "context": "In [24], the authors utilize active methods to initialize K-means in an intelligent Explore phase, during which neighborhoods of must-linked points are built up.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "similar explore phase is used in [25], after which a min-max approach is used to select the most uncertain sample.", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "Early work on constrained spectral clustering appears in [26, 27], in which spectral clustering is improved by examining the eigenvectors of the affinity matrix in order to determine the most informative points.", "startOffset": 57, "endOffset": 65}, {"referenceID": 25, "context": "Early work on constrained spectral clustering appears in [26, 27], in which spectral clustering is improved by examining the eigenvectors of the affinity matrix in order to determine the most informative points.", "startOffset": 57, "endOffset": 65}, {"referenceID": 6, "context": "More recently, the authors in [8, 9] improve constrained clustering by modeling which points will be most informative given the current clustering, with state-of-the-art results achieved on numerous datasets by the algorithm in [8] referred to as Uncertainty Reducing Active Spectral Clustering (URASC).", "startOffset": 30, "endOffset": 36}, {"referenceID": 7, "context": "More recently, the authors in [8, 9] improve constrained clustering by modeling which points will be most informative given the current clustering, with state-of-the-art results achieved on numerous datasets by the algorithm in [8] referred to as Uncertainty Reducing Active Spectral Clustering (URASC).", "startOffset": 30, "endOffset": 36}, {"referenceID": 6, "context": "More recently, the authors in [8, 9] improve constrained clustering by modeling which points will be most informative given the current clustering, with state-of-the-art results achieved on numerous datasets by the algorithm in [8] referred to as Uncertainty Reducing Active Spectral Clustering (URASC).", "startOffset": 228, "endOffset": 231}, {"referenceID": 22, "context": "In practice [28], the certain sets are initialized using the Explore algorithm of [24].", "startOffset": 82, "endOffset": 86}, {"referenceID": 26, "context": "To the best of our knowledge, the only such algorithm to do so is that of [29].", "startOffset": 74, "endOffset": 78}, {"referenceID": 26, "context": "The algorithm described in [29] has two major drawbacks.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "First, the authors use a modified version of the K-subspaces algorithm, which exhibits poor performance on most benchmark datasets (see for example [15]).", "startOffset": 148, "endOffset": 152}, {"referenceID": 6, "context": "We follow the framework laid out in [8] and provide an algorithm for improving the performance of subspace clustering that achieves significant benefits over the current state-of-the-art in PCC.", "startOffset": 36, "endOffset": 39}, {"referenceID": 22, "context": "To initialize, we build a set of certain sets Z using an Explore-like algorithm similar to that of [24].", "startOffset": 99, "endOffset": 103}, {"referenceID": 26, "context": "Next, a test point is obtained using either the min-margin criterion for subspaces [29] or a notion of margin based on the affinity matrix, which we define below.", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "The use of certain sets relies on the assumption that the pairwise queries are answered correctly\u2014an assumption that is common in the literature [24, 25, 8].", "startOffset": 145, "endOffset": 156}, {"referenceID": 23, "context": "The use of certain sets relies on the assumption that the pairwise queries are answered correctly\u2014an assumption that is common in the literature [24, 25, 8].", "startOffset": 145, "endOffset": 156}, {"referenceID": 6, "context": "The use of certain sets relies on the assumption that the pairwise queries are answered correctly\u2014an assumption that is common in the literature [24, 25, 8].", "startOffset": 145, "endOffset": 156}, {"referenceID": 6, "context": "However, in [8], the authors demonstrate that an algorithm based on certain sets still yields significant improvements under a small error rate.", "startOffset": 12, "endOffset": 15}, {"referenceID": 27, "context": "In [30], the author notes that actively querying points of minimum margin (as opposed to maximum entropy or minimum confidence) is an appropriate choice for reducing classification error.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "In [31], the authors present a margin-based binary classification algorithm that achieves an optimal rate of convergence (within a logarithmic factor).", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "1 Residual-Based Margin The concept of margin for subspaces was first studied in [29].", "startOffset": 81, "endOffset": 85}, {"referenceID": 26, "context": "Then the subspace margin of a point x \u2208 X is defined as [29] \u03bc1(x) = max j 6=k,j\u2208[K] dist(x, Sk\u2217) dist(x, Sj) .", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "Note that the fraction is a value in [0, 1], where the larger \u03bc1(x) the closer x is to the decision boundary.", "startOffset": 37, "endOffset": 43}, {"referenceID": 10, "context": "This method of point selection is then motivated by the fact that the difficult points to cluster are those lying near the intersection of subspaces [12].", "startOffset": 149, "endOffset": 153}, {"referenceID": 9, "context": "Further, theory for SSC ([11],[15]) shows that problematic points are those having large inner product with some or all directions in other subspaces.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "Further, theory for SSC ([11],[15]) shows that problematic points are those having large inner product with some or all directions in other subspaces.", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "See [32] for a definition of principal angles.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "2 Affinity-Based Margin We also consider a version of margin calculated from the entries of the affinity matrix itself, which is inspired by the nonparametric entropy estimation in [8].", "startOffset": 181, "endOffset": 184}, {"referenceID": 22, "context": "SUPERPAC can be thought of as an extension of ideas from PCC literature [24, 9, 8] to leverage prior knowledge about the underlying geometry of the data.", "startOffset": 72, "endOffset": 82}, {"referenceID": 7, "context": "SUPERPAC can be thought of as an extension of ideas from PCC literature [24, 9, 8] to leverage prior knowledge about the underlying geometry of the data.", "startOffset": 72, "endOffset": 82}, {"referenceID": 6, "context": "SUPERPAC can be thought of as an extension of ideas from PCC literature [24, 9, 8] to leverage prior knowledge about the underlying geometry of the data.", "startOffset": 72, "endOffset": 82}, {"referenceID": 6, "context": "The reader will note we made a choice to order the certain sets according to the UoS model; this is similar to the choice in [8] to query according to similarity, where our notion of similarity here is based on subspace distances, rather than some other metric.", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "In contrast to [9, 8], where the test point is chosen according to a global improvement metric, we choose test points according to their classification margin.", "startOffset": 15, "endOffset": 21}, {"referenceID": 6, "context": "In contrast to [9, 8], where the test point is chosen according to a global improvement metric, we choose test points according to their classification margin.", "startOffset": 15, "endOffset": 21}, {"referenceID": 9, "context": "In our experiments, we explored notions of margin based on the l1 regression coefficients from SSC, similar to the Sparsity Concentration Index of [11].", "startOffset": 147, "endOffset": 151}, {"referenceID": 22, "context": "For this reason, the Explore algorithm from [24] is unlikely to quickly find points from different clusters in an efficient manner.", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": "We also compared with the algorithm from [9] but found this algorithm performed worse than URASC with a far greater computational cost.", "startOffset": 41, "endOffset": 44}, {"referenceID": 30, "context": "In our experiments, we also compared with oracle robust PCA [33] implemented via the augmented Lagrange multiplier method [34] but did not find any improvement in classification error.", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "In our experiments, we also compared with oracle robust PCA [33] implemented via the augmented Lagrange multiplier method [34] but did not find any improvement in classification error.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "We follow the methodology of [17] and perform clustering on 100 randomly selected subsets of size K.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "We use the output of SSC [10] as our initial affinity matrix, as SSC is known to have the best performance on this set, and choose d = 9 as is common in the literature [10, 16].", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "We use the output of SSC [10] as our initial affinity matrix, as SSC is known to have the best performance on this set, and choose d = 9 as is common in the literature [10, 16].", "startOffset": 168, "endOffset": 176}, {"referenceID": 14, "context": "We use the output of SSC [10] as our initial affinity matrix, as SSC is known to have the best performance on this set, and choose d = 9 as is common in the literature [10, 16].", "startOffset": 168, "endOffset": 176}, {"referenceID": 8, "context": "The validity of the UoS assumption for these datasets is investigated in [10, 16].", "startOffset": 73, "endOffset": 81}, {"referenceID": 14, "context": "The validity of the UoS assumption for these datasets is investigated in [10, 16].", "startOffset": 73, "endOffset": 81}, {"referenceID": 22, "context": "22 (45/153) Explore [24] 4.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "This demonstrates the significant improvement over the results in [29], where 3Kd labels are required to surpass oracle performance; providing oracle labels (in this case, stating who a person is) requires far more expertise than pairwise comparisons (answering whether two images are of the same person).", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "Next, we show the effectiveness of the UoS-Explore algorithm over Explore used in [24, 8].", "startOffset": 82, "endOffset": 89}, {"referenceID": 6, "context": "Next, we show the effectiveness of the UoS-Explore algorithm over Explore used in [24, 8].", "startOffset": 82, "endOffset": 89}, {"referenceID": 15, "context": "In previous papers [17, 16], only sets of K = 2 and 3 digits were considered, and then only specific digits.", "startOffset": 19, "endOffset": 27}, {"referenceID": 14, "context": "In previous papers [17, 16], only sets of K = 2 and 3 digits were considered, and then only specific digits.", "startOffset": 19, "endOffset": 27}, {"referenceID": 32, "context": "Higher values of K have been considered more recently [35, 36, 37] and will be incorporated into our future work.", "startOffset": 54, "endOffset": 66}, {"referenceID": 33, "context": "Higher values of K have been considered more recently [35, 36, 37] and will be incorporated into our future work.", "startOffset": 54, "endOffset": 66}, {"referenceID": 34, "context": "Higher values of K have been considered more recently [35, 36, 37] and will be incorporated into our future work.", "startOffset": 54, "endOffset": 66}, {"referenceID": 14, "context": "methodology to the previous section and select 100 random subsets of size K, using subspace dimension d = 3 as in [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "To demonstrate that our algorithm is agnostic to the input clustering method, we use TSC [16] to obtain the input affinity matrix.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "We use Nk = 100 points in each subspace, the regime in which TSC has been shown to outperform SSC on this dataset [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "Although not pictured, we found in our experiments that [29] again requires as many labels as SUPERPAC requires pairwise comparisons.", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": "During the preparation of this manuscript, we became aware of [35, 36, 37], where more scalable algorithms for subspace clustering have been developed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 33, "context": "During the preparation of this manuscript, we became aware of [35, 36, 37], where more scalable algorithms for subspace clustering have been developed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 34, "context": "During the preparation of this manuscript, we became aware of [35, 36, 37], where more scalable algorithms for subspace clustering have been developed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 35, "context": "These points have been shown to lie in a union of affine subspaces of dimension at most 3 [38].", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "We first compare the performance of URASC and both random algorithms on the Sonar, Balance, and Leaf-250 datasets, all of which are used in [8] as benchmarks.", "startOffset": 140, "endOffset": 143}, {"referenceID": 10, "context": ", see previous work on sparse [12, 13], structured sparse [14], and low rank signals [40]).", "startOffset": 30, "endOffset": 38}, {"referenceID": 11, "context": ", see previous work on sparse [12, 13], structured sparse [14], and low rank signals [40]).", "startOffset": 30, "endOffset": 38}, {"referenceID": 12, "context": ", see previous work on sparse [12, 13], structured sparse [14], and low rank signals [40]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 36, "context": ", see previous work on sparse [12, 13], structured sparse [14], and low rank signals [40]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 37, "context": "Several works apply ideas of compressive sensing to clustering [41, 42] and", "startOffset": 63, "endOffset": 71}, {"referenceID": 38, "context": "Several works apply ideas of compressive sensing to clustering [41, 42] and", "startOffset": 63, "endOffset": 71}, {"referenceID": 9, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 15, "endOffset": 23}, {"referenceID": 39, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 15, "endOffset": 23}, {"referenceID": 40, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 191, "endOffset": 207}, {"referenceID": 41, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 191, "endOffset": 207}, {"referenceID": 42, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 191, "endOffset": 207}, {"referenceID": 43, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 191, "endOffset": 207}, {"referenceID": 44, "context": "1 from [48], restated below.", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": "5 of [48], we can replace E \u2016X\u20162 by", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "Further we note that, by the definition of principal angles [32],", "startOffset": 60, "endOffset": 64}], "year": 2016, "abstractText": "Many clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label. Subspace clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits. While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible. We present an algorithm for active query selection that allows us to leverage the union of subspace structure assumed in subspace clustering. The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary. This procedure can be used after any subspace clustering algorithm that outputs an affinity matrix and is capable of driving the clustering error down more quickly than other state-ofthe-art active query algorithms on datasets with subspace structure. We demonstrate the effectiveness of our algorithm on several benchmark datasets, and with a modest number of queries we see significant gains in clustering performance.", "creator": "LaTeX with hyperref package"}}}