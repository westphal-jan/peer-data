{"id": "1608.04929", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Reinforcement Learning algorithms for regret minimization in structured Markov Decision Processes", "abstract": "A few goal in followed Reinforcement Learning (RL) implemented probably if choose a element of dealt or takes changes making maximize the collect accounts or minimize two regret resulted where rather infinitesimal here galactic. For several RL conditions which launching specialists made depending control, full maximize support of once underlying Markov Decision Process (MDP) is differs by. particular structure. The expected state example held art calculations do without utilize so unlike underlying followed two optimal regarding with minimizing remark. In this he, we develop also RL algorithms probably advantages the building of the predictive strategy to minimize regret. Numerical observations on MDPs up transparent optimal stance audiences we thinking arithmetic only better good, much either on timetable, have a small run - but and benefits about number for random combined generations.", "histories": [["v1", "Wed, 17 Aug 2016 11:35:32 GMT  (55kb,D)", "http://arxiv.org/abs/1608.04929v1", "An extended abstract appears in AAMAS 2016"]], "COMMENTS": "An extended abstract appears in AAMAS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["k j prabuchandran", "tejas bodas", "theja tulabandhula"], "accepted": false, "id": "1608.04929"}, "pdf": {"name": "1608.04929.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning algorithms for regret minimization in structured Markov Decision Processes", "authors": ["Prabuchandran K J", "Tejas Bodas"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Optimal sequential decision problems are common in artificial intelligence (chess, Go, Backgammon), operation research (inventory control management, queueing admission control) and various other scientific disciplines. These problems are modeled using the framework of MDPs where a goal is to find a policy that maximizes the long run average reward collected. In an important subclass of these problems, one can characterize the structure of the optimal policy that maximizes this long run average reward. For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure. For such class of problems, one can develop efficient algorithms to obtain the optimal policy by restricting the search over such structured policies.\nIn the MDP problems outlined above, the modeling parameters such as the transition probability and the reward/cost matrices are assumed to be known. However in many practical scenarios, these parameters are unknown. This is the realm of RL where the objective is to learn the optimal policy online based on the simulation sample. RL algorithms such as Q-learning [10] and SARSA [11] achieve this objective. However these algorithms learn the optimal policy only asymptotically making them unsuitable for many practical problems. This has led to interest in developing algorithms that have finite time guarantees. In these algorithms the aim is to bound the number of samples required for finding an -optimal policy with high\nar X\niv :1\n60 8.\n04 92\n9v 1\n[ cs\n.L G\n] 1\n7 A\nprobability. EXP3 [12] and Rmax [13] algorithms provide such PAC guarantees while learning the optimal behaviour. More recently the focus in RL problems is to maximize the cumulative reward collected (or equivalently minimize the total regret). Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms. These algorithms emphasize exploration over unexplored policies, but also ensure that the best policy discovered so far is used frequently.\nIn this paper, our aim is to minimize the cumulative regret in a finite time horizon for the class of RL problems where the structure of the optimal policy is known. We propose algorithms that utilize this structural information to restrict the policy search. Note that none of the existing algorithms exploit the known structure of the optimal policy when minimizing the cumulative regret. Further, in case of UCRL [15] and PSRL [16], certain explored policies chosen by the algorithm need not satisfy the structural property of the optimal policy.\nAs is the case with UCRL and PSRL, our regret minimization algorithms are also inspired from the regret minimization algorithms of the multi-arm bandit (MAB) problem ([19], [20]). We provide three algorithms, pUCB, pThompson and warmPSRL, that treat \u2018structured policies\u2019 as arms of a corresponding MAB problem. As the names suggest, our algorithms are conceptually inspired by the UCB [19] and Thompson sampling [20] algorithms for the MAB problem. While the UCRL algorithm is also related to the UCB algorithm, our pUCB algorithm is very different from UCRL. The UCRL algorithm maintains upper confidence estimates on the entries of the transition probability and the reward matrix. On the other hand, pUCB maintains such confidence bounds on the estimate for the average reward of a policy. The state of the art PSRL algorithm is based on Thompson sampling [20] and maintains a belief on the transition probability and the reward matrix. In contrast, our pThompson algorithm maintains a belief on the average rewards of policies. This makes our algorithms model-free in contrast to UCRL and PSRL.\nOur pUCB and pThompson algorithms are episodic in nature, i.e., a chosen policy is implemented for a number of rounds before being changed. Now, to be able to estimate the average reward of a policy correctly we require unbiased estimates for the same. To ensure this, we use the notion of recurrence times and implement the chosen policy for an episode where the starting state and the end state are the same. This is a nontrivial technicality that some of the previous algorithms such as the RLPA [18] overlooked by assuming episodes of arbitrary random duration.\nOur idea of treating policies as arms and converting a RL problem into a corresponding MAB problem cannot be used in general for any RL problem. For an MDP with M actions and N states, there are potentially MN policies. Treating all such policies as arms and then applying either a UCB-like or a Thompson-like algorithm may not be productive in minimizing regret. However as we show with our numerical experiments, this is not the case when the structure of the optimal policy is utilized. Our pUCB and pThompson algorithms turn out to be meaningful for the class of sequential decision problems with a structure on the optimal policy.\nThe rest of the paper is organized as follows. In the next section, we explain the requisite preliminaries for describing our algorithms. In Section 3, we describe our pUCB, pThompson and warmPSRL algorithms. In Section 4, we provide two MDP settings where the optimal policy has a known structure. We perform numerical experiments in both settings to analyze the performance of our algorithms. We conclude in Section 5 with a brief summary and some future direction."}, {"heading": "2 Preliminaries and Motivation", "text": "A finite MDP consists of state space S = {1, 2, . . . , N}, action space A = {1, 2, . . . ,M}, a probability transition matrix P = [[pi,j(a)]] where i, j \u2208 S and a \u2208 A. Here pi,j(a) denotes the probability of a transition to state j when action a is chosen at state i. The reward is characterized by the function R(i, a, j) \u2200i, j \u2208 S, a \u2208 A that specifies the reward for choosing action a in state i and transitioning to j. We assume the reward function is bounded by 1. A stationary deterministic policy \u03c0 specifies the action (\u03c0(i)) that needs to be chosen in the state i. Let \u03a0 denote the set of all deterministic stationary policies. Associated with each policy \u03c0,\nwe define the long-run average reward \u03c1(\u03c0) as \u03c1(\u03c0) = lim T\u2192\u221e\nE [\u2211T t=1R(st, \u03c0(st), st+1) ]\nT , where\nwe have suppressed the dependence on the starting state in the notation. The classical goal in the MDP framework is to find a policy \u03c0\u2217 that maximizes the long-term average reward, i.e.,\n\u03c0\u2217 = arg max \u03c0\u2208\u03a0 \u03c1(\u03c0).\nThis problem is well behaved if \u03c1(\u03c0) is the same for all starting states. A sufficient condition for this to happen (and also assumed in this paper) is that the MDP satisfies the unichain condition [21]. That is, for every policy \u03c0, the resulting Markov chain has a single ergodic class.\nIn the RL setting that is of interest to us, the transition probabilities and reward values are not known a priori, thus making it harder to compute \u03c0\u2217. In many modern RL settings, an alternative goal involves finding a sequence of policy {\u03c0t} that minimizes the expected regret in a given horizon T . Here, the expected regret Rsstart(T ) when started from state sstart is defined as:\nRsstart(T ) = \u03c1(\u03c0\u2217)T \u2212 T\u2211 t=1 E[R(st, \u03c0t(st))],\nwhere \u03c0t represents the policy chosen at time t. In this paper we are concerned with minimizing regret when there exists some structure in the optimal policy \u03c0\u2217. Such structure exists in a wide collection of MDPs including instances in inventory management, optimal control (for instance, linear-quadratic regulators), queueing systems, dynamic resource allocation problems and others. We discuss two such MDP settings in Section 4. Now using the structure of the optimal policy, one can create a smaller subset of policies that satisfies this structure. The size of this set can be polynomial in the number of states as opposed to exponential (this is problem dependent). We can search for a policy that minimizes regret using this set as compared to searching in the complete policy space. Our algorithms are designed to take advantage of the structure present in the optimal policy of the unknown MDP. To motivate these, we give an interpretation of the PSRL algorithm that naturally leads to our algorithms that use the structural information.\nAn Interpretation of PSRL\nCorresponding to the true underlying MDP MTrue there are MN stationary deterministic policies. The regret can be minimized if we knew the average reward optimal policy \u03c0\u2217 of MTrue. In this case, we would just follow this policy till the end of horizon. However, we do not know the transition probabilities P True and rewards RTrue of MTrue and thus have to minimize the cumulative regret using the samples. This leads us to reinterpret our regret\nminimization problem in the famous multi-arm bandit problem setting with MN arms. In this interpretation, each arm corresponds to a policy \u03c0 of MTrue. Associated with each arm, we thus have a transition probability matrix P True,\u03c0, a reward vector RTrue,\u03c0 and an average reward \u03c1(\u03c0). Let d\u03c0 correspond to the stationary distribution vector of the Markov chain induced by this policy. The average reward is then \u03c1(\u03c0) = \u3008d\u03c0, RTrue,\u03c0\u3009 (where \u3008\u00b7, \u00b7\u3009 represents inner-product).\nAgain, as in the initial description of PSRL, we act in episodes. Let us assume a belief on the transition probability matrix P True,\u03c0 of the Markov chain under policy \u03c0 and reward vector RTrue,\u03c0 for each policy \u03c0. At the beginning of each episode, let us sample an instance (P \u03c0, R\u03c0) for each policy \u03c0 from the corresponding belief distribution. We can then compute the average reward value for each policy (which may not be equal to \u03c1(\u03c0) since we are not using P True,\u03c0 and RTrue,\u03c0). We then choose the policy \u03c0chosen which has the highest computed average reward value to execute in the episode (notice how this is like Thompson sampling in the bandit setting where one picks the arm that has the highest sample value). Using the samples collected during the episode, we can update the belief distribution on P True,\u03c0 and RTrue,\u03c0. We repeat this process at the beginning of each episode till the end of the horizon.\nNote that computing average reward estimates for each of the MN policies and choosing the highest is computationally expensive. This is happening because we are sampling P \u03c0 and R\u03c0 separately for each of the policy. To alleviate this, the PSRL algorithm can be seen as sampling an MDP (P,R) and then deriving {(P \u03c0, R\u03c0)} for all policies simultaneously. It then determines the policy with the highest estimated average reward using policy iteration (known to converge in a few iterations in practice) without estimating average rewards for each of the MN policies.\nPolicies as arms\nGiven this policies-as-arms reinterpretation of PSRL, we can go a step further. We can directly maintain a belief on the average reward \u03c1(\u03c0) for each policy. This is precisely what we do in our proposed algorithms in Section 3. Note that this has the advantage that we can reuse ideas from the UCB algorithm and the Thompson sampling algorithm, which are the state-of-the-art algorithms in the multi-armed bandit setting, to our problem. Another particularly attractive feature is that we can work directly with policies rather than the underlying transition probabilities. If we now use the knowledge of the structure of the optimal policy, then we can prune the exponential (MN ) number of policies to a number that is polynomial in M and N making the algorithm computationally and statistically attractive.\nOne of the key innovations that we provide in this paper is the way the beliefs on the average rewards \u03c1(\u03c0) will be updated. We propose two algorithms, pUCB that is related to the UCB algorithm, and pThompson that is related to the Thompson sampling algorithm. In the vanilla multi-armed bandit setting, the rewards obtained after each episode (which is the same as each round) are independent and identically distributed. On the other hand, the situation is not so clear in our case. How do we decide the length of the episode? Does the episode length influence the bias on the estimates of \u03c1(\u03c0)? Does the update procedure also influence the bias? We answer these questions using the Renewal-Reward Theorem.\nLet S1, S2, ... be a sequence of positive i.i.d random variables with finite means. The random process {XT }T\u22650 with random variable XT = sup{n : \u2211n i=1 Si \u2264 T} is called a renewal process. Let W1,W2, ... be a sequence of i.i.d. random variables with finite means. Wi can be interpreted as the reward received during period Si. The process defined by YT = \u2211XT i=1Wi is called a renewal-reward process.\nTheorem 1. The Renewal-Reward theorem ([22], Chapter 3, Theorem 3.6.1) states that\nlim T\u2192\u221e E[YT ] T = E[W1] E[S1] .\nThe above theorem states that the expected long run average reward is equal to the expectation of the reward in an episode divided by the expectation of the duration of that episode assuming the random variables related to every episode are independent and identically distributed. If we can get unbiased estimates of these two quantities, then we can get an unbiased estimate of the expected long run average reward. Note that an obvious choice such as estimating E[W1S1 ] will not lead to correct estimates of the long run average reward.\nUnder a given policy \u03c0, by fixing the return times to a fixed state s0 as renewal times, the MDP becomes a renewal reward process. Now we can estimate \u03c1(\u03c0) by estimating the reward collected in an episode and the duration of the episode. Note that the reward collected in an episode is a sample for W1 and the duration of the episode is the sample for S1 . In the algorithms that we propose, we can indeed ensure that the episode lengths are random and that each episode gives an i.i.d. sample (see Section 3, Algorithms 1 and 2). Note that if we deterministically set episode lengths, then we are introducing bias."}, {"heading": "3 Algorithms", "text": "In our algorithms, we consider policies that has same structure as of the optimal policy. Let K denote the number of such policies. These K policies are known in advance."}, {"heading": "3.1 Algorithm 1: pUCB", "text": "In pUCB algorithm, the set of K policies along with a starting state sstart, the number of rounds T , parameters \u03c4 and {\u03b2(t)}Tt=1 are provided as input. At the start of the algorithm a random policy is decided to be followed in the episode. After an episode starts, we keep track of the total reward collected (see Line 12 in Algorithm 1 and Line 11 in Algorithm 2) and the number of time steps elapsed t\u2032 before one of the termination conditions is satisfied. The termination conditions are as follows: we end the episode if either the time steps in the episode is equal to \u03c4 or we have reached the starting state sstart. Note that a key difference from MAB algorithms is that we act in episodes instead of time steps.\nWe maintain an estimate of the long-run average reward obtained under each policy \u03c0k as \u03c1\u0302(k). At the end of an episode, we update \u03c1\u0302(k) using r and t\u2032 in a careful way as shown in line numbers 15-17 in Algorithm 1. This is justified using the renewal-reward theorem as discussed in Section 2. In the next episode, we follow the policy that has the highest value, of\nthe sum of the average reward estimate \u03c1\u0302(k) and the confidence bonus \u03b2(t) \u221a\n2 log t n(k) . Here n(k)\nis used to track the count of the number of times policy k has been picked by round t. The algorithm is detailed in Algorithm 1.\nThe sequence {\u03b2(t)}Tt=1 is an input to the algorithm that determines the explorationexploitation trade-off as a function of time. If \u03b2(t) is set to a constant (typically value 1), then the decision rule is the same as UCB1 of [19]. Note that appropriately choosing this sequence can further decrease regret. Parameter \u03c4 should be set to \u221e to ensure our estimates \u02c6\u03c1(k) remain unbiased. When \u03c4 =\u221e, we can only switch between policies at the end of recurrent cycles. Mean recurrence times may potentially be large and are dependent on the unknown transition probabilities and the current policy being used. If they are indeed large, then \u03c4 can\nAlgorithm 1: pUCB algorithm (pUCB)\n1: Input: T,\u03a0 = {\u03c0k : k = 1, . . . ,K}, {\u03b2(t)}Tt=1, sstart, \u03c4 2: Output: CRT (Cumulative reward in T rounds) 3: for k = 1 to K do 4: \u03c1\u0302(k) = 1, n(k) = 0 5: Rarm(k) = 0 (Running sum of rewards for policy \u03c0k) 6: Tarm(k) = 0 (Running sum of number of rounds for \u03c0k) 7: end for 8: s = sstart, t\n\u2032 = 0, r = 0 9: k \u223c rand(1,K)\n10: for t = 1 to T do 11: if (t 6= 1 and s = sstart) or t\u2032 \u2265 \u03c4 then 12: Rarm(k) = Rarm(k) + r 13: Tarm(k) = Tarm(k) + t \u2032 14: \u03c1\u0302(k) = Rarm(k)Tarm(k)\n15: c(k) = \u03b2(t) \u221a\n2 log t n(k)\n16: n(k) = n(k) + 1\n17: k = arg maxj=1,...,K\n{ \u03c1\u0302(j) + c(j) } 18: t\u2032 = 0, r = 0 19: end if 20: snext \u223c P True(\u00b7 | s, \u03c0k(s)), r = RTrue(s, a, snext) 21: t\u2032 = t\u2032 + 1, s = snext 22: end for 23: CRT = \u2211K j=1Rarm(j)\nlet us switch between policies at the expense of getting biased estimates of \u03c1(\u03c0). On the other hand, if they are small relative to \u03c4 , then setting \u03c4 to a finite value does not affect estimation quality."}, {"heading": "3.2 Algorithm 2: pThompson", "text": "Algorithm 2 illustrates our second algorithm pThompson. Its structure, inputs and output are similar to the pUCB algorithm described above and we will focus on the differences here. In terms of inputs, pThompson does not have the sequence {\u03b2(t)}Tt=1 as one of its inputs.\nThe initialization for pThompson is similar to that of pUCB except that pThompson maintains a different set of internal estimates. In particular, for each policy \u03c0k, it maintains two estimates S(k) and F (k). These two estimates parameterize a Beta distribution that encodes our beliefs on the average cost reward of policy \u03c0k. During each episode, we keep track of the total reward collected r and the number of rounds t\u2032 elapsed before either of the termination conditions are met. The cumulative reward for the episode r is added to the running estimate S(k) of the current policy k and an update of the form t \u2212 r is done to F (k). This update step is critical and ensure that the mean of the beta distribution is an unbiased estimate of average reward \u03c1(k). This is different from the update step in Thompson sampling. Note that our updates also rely on conjugacy properties [20] (similar to Beta-Binomial conjugacy used in Thompson sampling). Then for new policy selection, we draw a realization for each of the\nAlgorithm 2: pThompson algorithm (pThompson)\n1: Input: T,\u03a0 = {\u03c0k : k = 1, . . . ,K}, sstart, \u03c4 2: Output: CRT (Cumulative reward in T rounds) 3: for k = 1 to K do 4: CRT = 0, S(k) = 0, F (K) = 0 5: end for 6: s = sstart, t\n\u2032 = 0, r = 0 7: k \u223c rand(1,K) 8: for t = 1 to T do 9: if (t 6= 1 and s = sstart) or t\u2032 \u2265 \u03c4 then\n10: CRT = CRT + r 11: S(k) = S(k) + r 12: F (k) = F (k) + t\u2032 \u2212 r 13: for k = 1 to K do 14: \u03b8(k) \u223c Beta(S(k), F (k)) 15: end for 16: k = arg maxj=1,...,K { \u03b8(j)\n} 17: t\u2032 = 0, r = 0 18: end if 19: snext \u223c P True(\u00b7 | s, \u03c0k(s)), r = RTrue(s, a, snext) 20: t\u2032 = t\u2032 + 1, s = snext 21: end for\nK Beta distributions and pick that policy whose realization value is the highest. It is important to note that although we mention the use of structural information, such information is only used to define the inputs to the algorithms proposed. Our algorithms work for many different classes of MDPs (including MDPs where there is no structure leading to exponential number of policies, although this may render our algorithms non-competitive). Structural properties are application/MDP specific, and we do not outline how to derive structural properties here. Obtaining structural properties is highly non-trivial and mathematical and the process of extracting such structural information and using it is not automatic. See references to several applications in Section 1 for more details on deriving structural properties, which typically reduce the policy search space dramatically. Indeed, a complete characterization of the class of MDPs based on structural properties of the optimal policy can be a separate research thread in itself. In this sense, pUCB and pThompson are structure agnostic RL regret minimization algorithms, which when coupled with structure restricted policy spaces can compete with the state-of-the-art.\nComparison with UCRL and PSRL\nThe UCRL and PSRL algorithms maintain O(M2N) estimates internally. This is significantly higher compared to our algorithms, that typically maintain O(M) estimates, as seen in the settings of Section 4. Further, PSRL as described in [16] needs to reset to the starting state after the end of the each episode (we take this into account carefully in our experiments). Another important aspect of our algorithms is that they do not incur high sampling costs that are inherently necessary for PSRL. In PSRL, we have to sample O(M2N) transition probability values and reward values from a belief that we maintain over them. Without using\nconjugacy, belief updates also become expensive to compute."}, {"heading": "3.3 Algorithm 3: warmPSRL", "text": "For both algorithms, pUCB and the pThompson, we can simultaneously estimate, at any round t \u2208 {0, T}, the unknown transition probabilities and reward values. For estimating transition probabilities, we keep track of the counts of state transitions for every action. To estimate rewards, we take empirical averages of rewards observed for each state-action pair. This is beneficial in settings where the policies-as-arms based algorithms such as ours can be used in conjunction with algorithms such as PSRL to further improve on the cumulative rewards collected. These estimates can be used to warmstart PSRL. This leads to the algorithm warmPSRL that we have listed in Algorithm 3. In this algorithm, we provide an additional input Tswitch that is chosen depending on problem instance. For the initial Tswitch rounds, we run modified versions of pUCB or pThompson (pUCB-Extended and pThompsonExtended respectively) or any other bandit algorithm, where we empirically estimate transition probabilities and rewards in parallel. For T \u2212 Tswitch, we run PSRL algorithm with the estimates computed by our algorithms as the initialization values. Finally, note that the warmPSRL is a combination of model free and model based methods.\nAlgorithm 3: PSRL warmstarted with pUCB or pThompson\n1: Input: T,\u03a0 = {\u03c0k : k = 1, . . . ,K}, sstart, \u03c4, Tswitch, {\u03b2(t)}Tswitcht=1 , Alg = pUCB-Extended or pThompson-Extended. 2: Output: CRT (Cumulative reward in T rounds) 3: Execution 4: (CR1, P\u0302 , R\u0302) = Alg(Tswitch,\u03a0, {\u03b2(t)}, sstart, \u03c4) 5: CR2 = PSRL(T \u2212 Tswitch, \u03c4, P\u0302 , R\u0302, sstart) 6: CRT = CR1 + CR2\nBounds on regret\nHere we provide a brief sketch of the regret analysis for pUCB and pThompson algorithms. For the following analysis, we will assume \u03c4 =\u221e. By the unichain assumption on MDP, Markov chains under all policies have a single recurrent class. Further, irreducibility implies that the mean recurrence time starting from any state will be finite. Let us consider the recurrent times to state sstart under all policies. Let \u03b7max = max\n\u03c0k, k\u2208{1,2,...,K} E\u03c0k [s0 \u2192 s0], where\nE\u03c0k [s0 \u2192 s0] denotes the mean recurrent time to state sstart under policy \u03c0k. Note that \u03b7max is an upperbound on mean recurrent times under all policies. In a horizon length T , on an average, we can have at most T\u03b7max pulls of the policies. Now we have a multi-arm bandit setting with horizon length T\u03b7max instead of the original horizon length T in the MDP setting. From this reduction, it is clear one can use the regret bounds of UCB as well as the Thompson sampling algorithm with the total number of pulls being T\u03b7max . This leads to a cumulative regret of O(log( T\u03b7max )), i.e., logarithmic pulls of sub-optimal arms. A rigorous analysis of this result involves analyzing the average number of pulls of the arms N\u03c0k(t) by time t and utilizing the renewal reward theorem to establish the convergence of average of i.i.d bounded rewards to the average reward of policy \u03c0k. Note that we can use Hoeffding\u2019s inequality for Markov chains to get the rate of convergence. The proofs of pUCB\nand pThompson algorithms then will respectively follow along the lines of UCB and Thompson sampling algorithms giving us the needed regret bounds."}, {"heading": "4 Numerical Results", "text": "Below we describe two MDP settings where there is structure in the corresponding optimal policies. Knowing this information simplifies the search for the policy that minimizes regret."}, {"heading": "4.1 The slow server problem", "text": "Consider a service system comprising of K servers and a single queue where the arriving customers wait before obtaining service. The customers arrive according to a Poisson process with rate \u03bb and each customer is required to obtain service at one of the parallel servers. Let us assume that the service requirement of each arriving customer is unity and that the time taken by server k to serve a customer is a random variable which has an exponential distribution with rate \u00b5k for k = 1, . . . ,K. This service system reflects many practical scenarios (including call center operations, web server load balancing and others).\nThis problem is well studied [6, 23, 24] in the setting where K = 2, \u00b51 > \u00b52 and all parameters are known. In particular, it has been shown that in an optimal admission control policy, the faster server should always be occupied when there are customers waiting for service in the queue. Further, the optimal policy is of the threshold type, which means that the slower of the two servers must be utilized only when the number of customers in the queue exceeds a threshold. While the optimal policy is of the threshold type, no known methods seem to exist that characterize the threshold in terms of the parameters.\nFor our experiment, we chose \u03bb = 1231 , \u00b51 = 18 31 and \u00b52 = 1 31 . The buffer length of the queue was chosen to be 20 (thus leading to a number of states equal to 80). We ran 10 Monte Carlo simulations and the resulting regret achieved by the proposed algorithms is shown in Figure 1. We compare our algorithms with PSRL. UCRL was not used for comparison because incorporating state dependent action spaces into the algorithm is difficult. On the other hand, for PSRL, we were able to do this by suitably modifying the Dirichlet distribution [16]. We ran each simulation for 106 rounds. The starting state corresponds to the state where the queue is empty and both the servers are free. The parameter \u03c4 was set to \u221e for pUCB and pThompson. Further, \u03b2(t) was set to 1 for pUCB.\nAs shown in the plot, pUCB and pThompson perform much better than the state-of-the-art algorithm PSRL from the very outset. This clearly illustrates the advantage of using knowledge of the optimal policy structure."}, {"heading": "4.2 The machine replacement problem", "text": "The setting for this problem is adapted from [25] (Example 1.1.3, page 8) and has various applications in inventory management. Consider the problem of operating a machine efficiently. The machine can be in one of n possible states (S = {1, 2, . . . , n}). Let state 1 correspond to the machine being in perfect condition and each subsequent state corresponding to increasingly deteriorated condition of the machine.Let there be an average cost g(i) for operating the machine for one time period when it is in state i. Because of the increasing failure probability, we can assume that g(1) \u2264 g(2) \u2264 \u00b7 \u00b7 \u00b7 \u2264 g(n). We can take two actions in each state: continue operating the machine without maintenance (C) or perform maintenance (PM). Once maintenance has been performed, the machine is guaranteed to remain in state 1 for one\ntime period.The cost that we incur for maintenance is thus R + g(1) (R for repairing and g(1) because the machine is now functioning in state 1).\nLet P = [[pij(a)]], i, j \u2208 S, a \u2208 {C,PM} denote the transition probability matrix, with the following properties: (a) pi1(PM) = 1, (b) pij(PM) = 0, \u2200j 6= 1, (c) pij(C) = 0, \u2200j < i, and (d) pij(C) \u2264 p(i+1)j(C), \u2200j > i. Intuitively, when the machine is operated in state j, its well-being will deteriorate to another state i \u2265 j after the current time period.\nFor this application and many others based on it, the optimal policy is a threshold policy if our objective is to minimize the average cost of using the machine. That is, we should perform maintenance if and only if the state of the machine i \u2265 i\u2217, where i\u2217 is a certain threshold state. This threshold state can be identified if we know the precise transition probability values.\nWe chose the following experimental configuration. The number of states was chosen to be 100. We ran 10 Monte Carlo simulations and the resulting regret achieved by the proposed algorithms is shown in Figure 2. The true transition probability values were generated randomly (taking into account the constraints relating these values) and were kept fixed for each simulation run. The algorithms that we compare to are PSRL and UCRL. We ran each simulation for 106 rounds. The starting state corresponds to the state where the machine is in perfect condition. The parameter \u03c4 was set to \u221e for pUCB and pThompson. Further, \u03b2(t) was set to 1 for pUCB. In warmPSRL, we used pThompson for 105 rounds, estimated (P,R) and then switched to PSRL with the estimated (P,R) as the starting values for the remaining rounds. Appropriate best values were chosen for PSRL and UCRL parameters as well.\nAs we can see from the plot, PSRL, warmPSRL and pThompson are very close in terms of performance. In fact, pThompson is better than PSRL for the first 105 rounds. Around this point, PSRL has learned accurate enough estimates of transition probabilities and reward values that it seems to collect relatively slightly more reward per round. Thus, if fast initial \u2018learning\u2019 is needed or if lesser rate of regret is desired in fewer number of rounds, pThompson algorithm can be chosen since it outperforms state-of-the-art PSRL (as shown in the plot). The regret of warmPSRL is very close to that of PSRL overall and better in the initial rounds. Among the remaining two algorithms, pUCB performs better than UCRL although neither of\nthem are as good as pThompson, warmPSRL and PSRL. Finally, note that our algorithms ran much faster than PSRL and UCRL (also true in the preceding experiment, although running times are not reported for both)."}, {"heading": "5 Concluding Remarks", "text": "We have built on the well established, easy to use UCB and Thompson sampling algorithms to provide competitive regret minimizing RL algorithms (making novel modifications for getting unbiased estimated of the long run average reward \u03c1(k) and the number of rounds that each policy \u03c0k needs to be applied). Such direct use of structural information is not easily possible for the current state of the art algorithms (PSRL and UCRL). The algorithms presented are simple, competitive and many times better than state-of-the-art methods for cumulative regret minimization in RL settings. They have significant advantage in terms of computation and sampling costs."}], "references": [{"title": "Puterman, Markov Decision Processes: discrete stochastic dynamic programming", "author": ["L. M"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Reinforcement learning methods for continuous-time Markov decision problems", "author": ["S.J. Duff"], "venue": "Advances in Neural Information Processing Systems 7, vol. 7, p. 393, 1995.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Structured Threshold Policies for Dynamic Sensor Scheduling: A Partially Observed Markov Decision Process Approach", "author": ["V. Krishnamurthy", "D. Djonin"], "venue": "IEEE Transactions on Signal Processing, vol. 55, no. 10, pp. 4938\u20134957, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Structural properties of the optimal resource allocation policy for single-queue systems", "author": ["R. Yang", "S. Bhulai", "R. van der Mei"], "venue": "Annals of Operations Research, vol. 202, no. 1, pp. 211\u2013233, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal control of a queuing system with two heterogeneous servers", "author": ["W. Lin", "P. Kumar"], "venue": "IEEE Trans. on Automatic Control, vol. 29, pp. 696\u2013703, 1984.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1984}, {"title": "Assigning customers to two parallel servers with resequencing", "author": ["N.R. Gogate", "S.S. Panwar"], "venue": "Communications Letters, IEEE, vol. 3, no. 4, pp. 119\u2013121, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimal monotone forwarding policies in delay tolerant mobile ad-hoc networks", "author": ["E. Altman", "T. Ba\u015far", "F. De Pellegrini"], "venue": "Performance Evaluation, vol. 67, no. 4, pp. 299\u2013317, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "A discrete semi-Markov decision model to determine the optimal repair/replacement policy under general repairs", "author": ["C. Love", "Z.G. Zhang", "M. Zitron", "R. Guo"], "venue": "European journal of operational research, vol. 125, no. 2, pp. 398\u2013409, 2000.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 279\u2013292, 1992.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1992}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Machine Learning, vol. 49, no. 2-3, pp. 209\u2013232, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 213\u2013231, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Logarithmic online regret bounds for undiscounted reinforcement learning", "author": ["P. Ortner", "R. Auer"], "venue": "Advances in Neural Information Processing Systems, vol. 19, p. 49, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 1563\u20131600, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "more) efficient reinforcement learning via posterior sampling", "author": ["I. Osband", "D. Russo", "B. Van Roy"], "venue": "pp. 3003\u20133011, 2013. 12", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Thompson Sampling for Learning Parameterized Markov Decision Processes", "author": ["A. Gopalan", "S. Mannor"], "venue": "Proceedings of COnference on Learning Theory (COLT), vol. 40, 2015, pp. 1\u201338.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Regret bounds for reinforcement learning with policy advice", "author": ["M.G. Azar", "A. Lazaric", "E. Brunskill"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer, 2013, pp. 97\u2013112.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, vol. 47, no. 2-3, pp. 235\u2013256, 2002.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "Proceedings of the Twenty-Fifth Annual Conference on Learning Theory, 2012, pp. 39.1\u201339.26.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "NP-hardness of checking the unichain condition in average cost MDPs", "author": ["J.N. Tsitsiklis"], "venue": "Operations research letters, vol. 35, no. 3, pp. 319\u2013323, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic processes", "author": ["S.M. Ross"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "A note on optimal control of a queuing system with two heterogeneous servers,", "author": ["W. Lin", "P. Kumar"], "venue": "System and Control letters, vol. 4, pp. 131\u2013134, 1984.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1984}, {"title": "A simple proof of the optimality of a threshold policy in a two-server queueing system", "author": ["G. Koole"], "venue": "System and Control letters, vol. 26, pp. 301\u2013303, 1995.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Dynamic Programming and Optimal Control, 3rd ed", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 76, "endOffset": 82}, {"referenceID": 1, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 147, "endOffset": 162}, {"referenceID": 2, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 147, "endOffset": 162}, {"referenceID": 3, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 147, "endOffset": 162}, {"referenceID": 4, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 147, "endOffset": 162}, {"referenceID": 5, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 147, "endOffset": 162}, {"referenceID": 6, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 189, "endOffset": 192}, {"referenceID": 7, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 250, "endOffset": 253}, {"referenceID": 8, "context": "RL algorithms such as Q-learning [10] and SARSA [11] achieve this objective.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "RL algorithms such as Q-learning [10] and SARSA [11] achieve this objective.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "EXP3 [12] and Rmax [13] algorithms provide such PAC guarantees while learning the optimal behaviour.", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "EXP3 [12] and Rmax [13] algorithms provide such PAC guarantees while learning the optimal behaviour.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms.", "startOffset": 81, "endOffset": 89}, {"referenceID": 13, "context": "Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms.", "startOffset": 81, "endOffset": 89}, {"referenceID": 14, "context": "Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms.", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms.", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms.", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "Further, in case of UCRL [15] and PSRL [16], certain explored policies chosen by the algorithm need not satisfy the structural property of the optimal policy.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Further, in case of UCRL [15] and PSRL [16], certain explored policies chosen by the algorithm need not satisfy the structural property of the optimal policy.", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "As is the case with UCRL and PSRL, our regret minimization algorithms are also inspired from the regret minimization algorithms of the multi-arm bandit (MAB) problem ([19], [20]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 18, "context": "As is the case with UCRL and PSRL, our regret minimization algorithms are also inspired from the regret minimization algorithms of the multi-arm bandit (MAB) problem ([19], [20]).", "startOffset": 173, "endOffset": 177}, {"referenceID": 17, "context": "As the names suggest, our algorithms are conceptually inspired by the UCB [19] and Thompson sampling [20] algorithms for the MAB problem.", "startOffset": 74, "endOffset": 78}, {"referenceID": 18, "context": "As the names suggest, our algorithms are conceptually inspired by the UCB [19] and Thompson sampling [20] algorithms for the MAB problem.", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "The state of the art PSRL algorithm is based on Thompson sampling [20] and maintains a belief on the transition probability and the reward matrix.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "This is a nontrivial technicality that some of the previous algorithms such as the RLPA [18] overlooked by assuming episodes of arbitrary random duration.", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "A sufficient condition for this to happen (and also assumed in this paper) is that the MDP satisfies the unichain condition [21].", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "The Renewal-Reward theorem ([22], Chapter 3, Theorem 3.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "If \u03b2(t) is set to a constant (typically value 1), then the decision rule is the same as UCB1 of [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "Note that our updates also rely on conjugacy properties [20] (similar to Beta-Binomial conjugacy used in Thompson sampling).", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "Further, PSRL as described in [16] needs to reset to the starting state after the end of the each episode (we take this into account carefully in our experiments).", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "This problem is well studied [6, 23, 24] in the setting where K = 2, \u03bc1 > \u03bc2 and all parameters are known.", "startOffset": 29, "endOffset": 40}, {"referenceID": 21, "context": "This problem is well studied [6, 23, 24] in the setting where K = 2, \u03bc1 > \u03bc2 and all parameters are known.", "startOffset": 29, "endOffset": 40}, {"referenceID": 22, "context": "This problem is well studied [6, 23, 24] in the setting where K = 2, \u03bc1 > \u03bc2 and all parameters are known.", "startOffset": 29, "endOffset": 40}, {"referenceID": 14, "context": "On the other hand, for PSRL, we were able to do this by suitably modifying the Dirichlet distribution [16].", "startOffset": 102, "endOffset": 106}, {"referenceID": 23, "context": "The setting for this problem is adapted from [25] (Example 1.", "startOffset": 45, "endOffset": 49}], "year": 2016, "abstractText": "A recent goal in the Reinforcement Learning (RL) framework is to choose a sequence of actions or a policy to maximize the reward collected or minimize the regret incurred in a finite time horizon. For several RL problems in operation research and optimal control, the optimal policy of the underlying Markov Decision Process (MDP) is characterized by a known structure. The current state of the art algorithms do not utilize this known structure of the optimal policy while minimizing regret. In this work, we develop new RL algorithms that exploit the structure of the optimal policy to minimize regret. Numerical experiments on MDPs with structured optimal policies show that our algorithms have better performance, are easy to implement, have a smaller run-time and require less number of random number generations.", "creator": "LaTeX with hyperref package"}}}