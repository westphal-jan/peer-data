{"id": "1608.07249", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "Benchmarking State-of-the-Art Deep Learning Software Tools", "abstract": "Deep learning since. be as a primarily machine thinking types brought a variety significant efficient, brought but success disappointing in events open - regarding deep school hardware instruction coming hold authority. Training also deep digital is any example very finally - consuming required. To address of big calculation challenge in deep learning, many tools expertise hardware include such as multi - core CPUs and even - broader GPUs although relaxation the training still. However, different integrated art consist recording for running performances when training similar sizes mainly trouble system new different uses unix-like, although certainly without difficult it previous application coming enter response certain took of software and hardware. In this newspaper, thought helped to yet rather comparative comparative of present state - of - the - book GPU - spurring out learning software equipment, followed Caffe, CNTK, TensorFlow, , Torch. We weighted through running performance of these tool six 10 popular contain within neural lines back two CPU decks more set GPU elevators. Our emphasis true two - split. First, for putting learning finally content, mind benchmarking positive to states as a educational even drafting appropriate software practical those electronics launch. Second, make deep academic chip developers, what later - continuous analysis 8 gone possible needs around made further facilitates another maintenance performance.", "histories": [["v1", "Thu, 25 Aug 2016 18:48:16 GMT  (314kb,D)", "http://arxiv.org/abs/1608.07249v1", "6 pages"], ["v2", "Fri, 26 Aug 2016 06:25:05 GMT  (314kb,D)", "http://arxiv.org/abs/1608.07249v2", "6 pages"], ["v3", "Sat, 3 Sep 2016 16:40:32 GMT  (410kb,D)", "http://arxiv.org/abs/1608.07249v3", "Revision history: 1. Correct the CUDA version and re-test the experiments. 2. Revise minor difference of network configuration and delete some extra operations like dropout on AlexNet. 3. On RNN of CNTK, we remove an extra LSTM classification task which is not included in other tools and change the configuration file with \"SimpleNetworkBuilder\" to customized brain scripts"], ["v4", "Sun, 11 Sep 2016 06:13:13 GMT  (416kb,D)", "http://arxiv.org/abs/1608.07249v4", "Revision history: 1. Remedy the bug of ResNet-50 configuration in TensorFlow. 2. Change time measurement method of Caffe from \"caffe time\" to \"caffe train\". 3. Add an option of \"prefetch=true\" to configuration file of CNNs in CNTK"], ["v5", "Mon, 19 Sep 2016 07:09:07 GMT  (415kb,D)", "http://arxiv.org/abs/1608.07249v5", "Revision history: 1. Revise a bug of AlexNet configuration in TensorFlow. 2. Add an update operation in AlexNet with Torch"], ["v6", "Wed, 25 Jan 2017 09:27:52 GMT  (541kb,D)", "http://arxiv.org/abs/1608.07249v6", "Revision history: 1, Includes results of multiple GPUs. 2, Include MXNet into our evaluation. 3, Update all tools to the latest major versions. 4, Include results of real data sets: MNIST and Cifar10"], ["v7", "Fri, 17 Feb 2017 11:02:08 GMT  (336kb,D)", "http://arxiv.org/abs/1608.07249v7", "Revision history: 1. Revise ResNet-50 configuration in MXNet. 2. Add faster implementation of ResNet-56 in TensorFlow with multiple GPUs"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["shaohuai shi", "qiang wang", "pengfei xu", "xiaowen chu"], "accepted": false, "id": "1608.07249"}, "pdf": {"name": "1608.07249.pdf", "metadata": {"source": "CRF", "title": "Benchmarking State-of-the-Art Deep Learning Software Tools", "authors": ["Shaohuai Shi", "Qiang Wang", "Pengfei Xu", "Xiaowen Chu"], "emails": ["chxw}@comp.hkbu.edu.hk"], "sections": [{"heading": null, "text": "Index Terms\u2014Deep Learning; GPU; Feed-forward Neural Networks; Convolutional Neural Networks; Recurrent Neural Network"}, {"heading": "1. Introduction", "text": "In the past decade, deep learning has been successfully applied in diverse areas including computer vision, speech recognition, natural language processing, etc. The success of deep learning is attributed to its high representational ability of input data, by using various layers of artificial neurals [1]. GPUs have played a key role in the success of deep learning by significantly reducing the training time [2]. In order to increase the efficiency in developing deep learning methods, there are a number of open-source deep learning toolkits including Caffe from UC Berkeley [3], CNTK from Microsoft [4], TensorFlow from Google [5], and Torch from personal group [6], all these tools support multi-core CPUs and many-core GPUs. One of the main tasks of deep learning is to learn a number of weights in each layer of network, which can be implemented by vector or matrix operations. TensoFlow uses Eigen [7] as accelerated matrix operation library, while Caffe, CNTK, Torch\nemploy OpenBlas [8] or cuBLAS [9] to speed up matrix related calculations. All the mentioned tools import cuDNN [10], which is a GPU-accelerated deep learning library, for their neural network computing. However, because of the difference of optimization methods by vendors, these tools exhibit different running performance even when training the same neural network on the same hardware platform. Furthermore, the performance of a tool also changes a lot when training different types of networks, or using different types of hardware.\nGiven the diversity of deep learning tools and hardware platforms, it could be confused for users to choose an appropriate tool to carry out their deep learning tasks. In this paper, we benchmark three major types of deep neural networks (i.e., fully connected neural networks (FCNs) [11], convolutional neural networks (CNNs) [12][13][14], and recurrent neural networks (RNNs) [15]) on state-of-the-art GPU-accelerated tools (i.e., CNTK, TensoFlow, Caffe and Torch), and analyze their advantage and disadvantage on both CPUs and GPUs, in terms of running time performance.\nFor each type of networks, we benchmark the networks of both small size and large size.1 Our major findings are summarized as follows: (1) In general, all tools do not scale well on many-core CPUs. The performance using 16 CPU cores is only slightly better than using 4 CPU cores. (2) For FCNs and CNNs, all tools can achieve significant speedup by using contemporary GPUs. With GPUs, Caffe performs the best on FCNs while TensorFlow performs the best on CNNs. (3) For RNNs, Torch and TensorFlow can achieve much better performance than CNTK on GPU. But on the other hand CNTK performs much better than Torch and TensorFlow on CPU. (4) Among the three GPU platforms, GTX1080 always performs the best.\nThe rest of the paper is organized as follows. Section 2 presents the background and related work. Section 3 introduces our benchmarking methodology. Experimental results are presented in Section 4, followed by our discussion in Section 5. We conclude the paper in Section 6."}, {"heading": "2. Background and Related Work", "text": "With the fast development of deep learning framework, there exists numerous deep neural networks including fully connected neural networks (FCNs), convolutional neural\n1Our source code and experimental data can be downloaded from http: //www.comp.hkbu.edu.hk/\u223cchxw/dlbench.html\nar X\niv :1\n60 8.\n07 24\n9v 1\n[ cs\n.D C\n] 2\n5 A\nug 2\n01 6\nnetworks (CNNs), recurrent neural networks (RNNs), restricted boltzmann machine (RBM), etc. for different applications [16]. In this paper, we focus on analyzing three types of neural networks (i.e., FCNs, CNNs and RNNs). FCN is one of the deep neural networks that have the longest history dated back to 1980s when the backpropagation (BP) algorithm [17] was developed. And for CNN and RNN, they have been revealed strong power on the applications of image recognition and natural language processing respectively [12][13][14].\nTo reduce the total number of parameters in each layer, CNNs build a convolutional layer by using a set of kernels, and the parameters of each kernel are shared across entire field (e.g., a channel of color image). RNNs allow cyclical connections of the units in the network [18][13][14]. Furthermore, Long-short Term Memory(LSTM) [19][15] has been proposed to address vanished and exploding gradients on RNNs.\nWith the growing success of deep learning, there comes out many popular open source GPU-accelerated frameworks, among which Caffe, CNTK, TensorFlow and Torch are the most popular ones.\nCaffe is developed by Berkeley Vision and Learning Center (BVLC) and has become open source since 2014. The authors [3] claim that Caffe can process 40 million images per day with GPU-accelerated version on a single NVIDIA K40 or Titan GPU. After integrated with cuDNN, it achieves speedup about 1.3x on NVIDIA K40 card [10].\nCNTK is a unified computational network toolkit developed by Microsoft Research, which supports most popular networks. At December 2015, the official reported a performance result benchmarking on a fully connected 4-layer neural network compared to Caffe, TensorFlow, Theano and Torch, and the result shows that CNTK with multiple GPUs on single node or cross multiple machines achieve a much better speed (about 1.5x speedup) than the other compared toolkits. However, CNTK does not support concatenation operation in some CNNs (e.g., GoogLeNet [20]).\nTensorFlow is developed by Google which has integrated most common units in deep learning framework using data flow graphs. It supports many up-to-date networks such as CNNs, RNNs with different settings. TensorFlow is designed for remarkable flexibility, portability, and high efficiency of equipped hardware.\nTorch is a scientific computing framework which provides data structures for the most useful components in machine learning algorithms such as multi-dimensional tensors and mathematical operations over them.\nTo accelerate the training speed of deep neural networks, both CPUs SSE techniques and float points SIMD models are used to implement deep learning algorithms [21], which achieve 3x speedup over optimized floating-point baseline. Considering parallel algorithms for GPU, Jeffrey Dean et al. [22] proposed a large scaled distribute deep networks and developed two algorithms (i.e., Downpour SGD and Sandblaster L-BFGS) that can be easily running on computing clusters with thousands of machines including GPU machines. Another way to accelerate training speed is to\nreduce the number of learning parameters, Song Han et al. [23] use the method of pruning redundant connections to reduce parameters without losing network representational ability, which could reduce the number of parameters of AlexNet from 61 millions to 6.7 millions. Bahrampour et al. [24] did the similar work with us, but they only used a single architecture of GPU (i.e., NVIDIA Maxwell Titan X) and old version softwares (e.g., cuDNN v2, v3). We use three major architectures of GPU and benchmark on some new networks (e.g., ResNet-50) and softwares (e.g., cuDNN v4), and we also go insight into the source codes to analyze performances."}, {"heading": "3. Experimental Methods", "text": "For each type of network, we set up a small size of network and a large size of network to do comparison. One of the important character to measure training time of models is the time of an iteration which takes a batch size, called mini-batch, of data. After maximum number of iterations or converged of learning, the training progress is terminated. Therefore, we benchmark the networks by using a range of mini-batch size for each network on these tools. For each mini-batch size, we run numerous iterations and evaluate their average speed.\nThe software versions and related libraries are shown in Table 1.\nIn terms of hardware consideration, we run experiments on single CPU, multiple CPUs and single GPU; and multiple generations of GPUs are used to generate differential results\non GPU computing. The details of experimental setup are given as follows:\nNeural networks. Firstly, for fully connected network, a 5 layers neural network (FCN-5) and an 8 layers neural network (FCN-8) are constructed with 3 and 6 hidden layers respectively. Secondly, we choose AlexNet [12] and ResNet50 [25], both of which are CNNs, to do the testing. Lastly, because of the main computation complexity of RNNs is the length of input sequence, we select 2 LSTM [15] layers for testing, with input length of 32 (LSTM-32) and 64 (LSTM64) respectively. The networks configuration details can be found in Table 2.\nHardware. We use two types of multi-core CPUs, say desktop level (i.e., Intel i7-3820 CPU @ 3.60GHz) and server level (i.e., Intel Xeon CPU E5-2630 v3 @ 2.40GHz), to test the performance of tools with different number of threads; and three generations of GPU cards, NVIDIA GTX 980 with Maxwell architecture, GTX 1080 with Pascal architecture and Telsa K80 with Kepler architecture, are used to compare the performance on different GPU platforms. Notice that we only use one of the two GK210 chips of K80 GPU in this study. In order to avoid host memory dependency of neural network size, the two test machines are equipped with 64GB memory and 128GB memory respectively. The details of hardware configurations are shown in Table 3."}, {"heading": "4. Results", "text": "Since the official Caffe does not implement LSTM neuron operation, its benchmark on RNNs will not be included. For CNTK, batch normalization (an operation of ResNet50) is not supported on CPU version, therefore, result of ResNet-50 with CPU on CNTK will not be shown.\nTo make a total comparison of all toolkits, networks and hardware, we choose a proper mini-batch size for each type of network so that both CPUs and GPUs have their best performance. In our cases, we choose mini-batch sizes of 64, 16 and 128 for FCNs, CNNs and RNNs respectively, and the comparative results are shown in Table 4.\nCaffe and Torch have close results on FCNs by using CPU-only platform, and both of them cannot run normally when threads usage is set to be bigger than the number of CPU cores on desktop CPU. On the contrary, TensorFlow is slower, but its 8 threads version goes better than its smaller threads versions. TensorFlow performs the worst results on CPUs, and the best training speed of TensorFlow on FCN-8 with 8 CPU threads is at about 1.9 second, while Caffe only needs 0.48 second.\nOn the CPU parallel version, CNTK has a worst performance when training CNNs, but the speeds are doubled faster when the number of CPU cores are doubled (the test machine is with 4 CPU cores). Caffe, TensorFlow and Torch achieve average time of around 3 seconds per mini-batch when it is set to 1 CPU core, and the running times of these three tools are slightly shorter with the number of CPU cores being set larger. But the results of Torch with 4 and 8 CPUs are not filled on destktop CPU because the training time goes\nup to more than 400 seconds which are exceptional results. Caffe achieves a best time efficiency when the amount of CPU threads used are not bigger than the number of CPU cores, while TensorFlow is speedup even at 8 threads on 4 cores CPU. Table 4 indicates the results of ResNet-50 on CPUs, in which the supported three tools have a similar speedup trend when increasing the number of CPU core usage with AlexNet. In ResNet-50, with the single CPU core version, TensorFlow has a dramatically slow training time which is up to 3 times than that of Caffe.\nWe also compare the parallelization speedup ability of different toolkits on RNNs under different CPU thread setting. Torch and CNTK hardly benefit from multi-threads for performance improving whereas TensorFlow has nearly linear speed up with increasing thread number. However, Torch still achieves the best performance in our experiments in which Torch has nearly 12x speed up compared with TensorFlow under 4-thread setting.\nThe numbers in Table 4 show that GPU version has an obviously high performance than CPU versions, and the maximum speedup on GTX 1080 reaches to 112 times than the 4 cores CPU version, and approximate 100 times than 16 cores CPU by using TensorFlow. All the training speeds on GTX 1080 are faster than other cases on GTX 980 and K80. Fig. 1 shows more detailed comparison on GPUs with a range of mini-batch size.\nFully connected networks testing results on GPUs are displayed in Fig. 1(a) and Fig. 1(b). Caffe has a best time efficiency when the mini-batch size is smaller than 256 on all the GPU cards, and TensorFlow has a significantly poor performance on these kinds of networks when the minibatch size is small, while CNTK and Torch have a similar performance which is better than TensorFlow.\nThe benchmark results of AlexNet and ResNet-50, which are CNNs, on GPUs are shown in Fig. 1(c) and 1(d), respectively. In tools level analysis of these two figures, TensorFlow has a better performance than any others with all the mini-batch sizes, while Caffe displays the poorest performance on AlexNet. On different GPUs, GTX 1080 card aalways performs the best, and the average running time on GTX 1080 is around 1.5x shorter than on GTX 980 which is about 2 times faster than running on Tesla K80 card. However, when running much larger network (e.g., ResNet-50), with the mini-batch size getting larger, much more GPU memory is needed to do the computation, which results in crash of some tools on smaller memory GPUs. For example, with mini-batch size of 16, Caffe cannot run on GTX 980. The programs with CNTK are also crashed at not less than 32 mini-batch size on both GTX 980 and GTX 1080 cards. Torch can run all the configured mini-batch size except 64 on GTX 980 and GTX 1080 cards. All the benchmark results can be generated with TensorFlow on all GPU cards, and it acquires better performance on ResNet50 than the other three tools. It seems that TensoFlow has a much better strategy of GPU memory utilization on running convolutional neural networks.\nAs for RNN, we have different settings of input sequence lengths which are 32 and 64 and mini-batch size varying\nfrom 64 to 256. According to our results demonstrated in Figure 1(e) and Figure 1(f), TensorFlow achieves the best performance for all available settings, specifically near 8x speed up for LSTM-32 compared with CNTK under 128 mini-batch setting. Regarding performance scaling with different mini-batch size, most of the toolkits consume more time with bigger mini-batch size. However, we can also observe that Torch has similar performance under 256 and 512 mini-batch size when training LSTM-64. This also reveals the best choice of mini-batch size for different toolkits."}, {"heading": "5. Discussion", "text": "With GPU computing resources, all the deep learning tools mentioned achieve very high speedups compared to their CPU-only versions because of high parallelization on lots of CUDA cores. The theoretical performance of GTX 1080 is up to 8873 GFLOPS which is much higher than 4 or 16 cores CPUs, so that the maximum speedup can reach 112x in TensorFlow with GTX 1080 card. In overall, the performance of GTX 1080 is better than GTX 980, and much better than Tesla K80 (with single GK210 chip) corresponding to GPU cores computing performances (i.e., GTX 1080 at 8.8 TFLOPS >GTX 980 at 4.9 TFLOPS >Telsa K80 at 2.8 TFLOPS).\nConsidering parallelization on CPUs, the number of computation threads are recommended to be not larger than the number of physical CPU cores. Because if there are lots of calculation tasks in all CPUs, it is difficult for the system to specify idle CPU to do scheduling, which could easily lead to bad performance such as Caffe, CNTK and Torch at 8 threads on 4-core CPUs.\nOn FCNs, CNTK performs a little better than TensorFlow, but slightly worse than Caffe and Torch with one GPU. The results are matched with the official claims of\nCNTK [26]. It seems that CNTK has very outstanding performance in multiple GPUs scaling, but the scalability is not discussed in this paper. In general, training a network is a two-phase computations (i.e., feed forward and backward propagation). In feed forward phase, matrix multiplication is the main cost, and cuBLAS API: cublasSgemm is the optimazation solution to all the four tools. However, there is a trick calling parameter of cublasSgemm, which may result in different performance when doing matrix-matrix multiplication. With same matrix sizes, if we set the second parameter to CUBLAS OP T of cublasSgemm API, the kernel actually used by the API is different and it results in 3 times slower compared to CUBLAS OP N. CNTK and TensorFlow construct its data structure to call cublasSgemm use CUBLAS OP N, while Caffe and Torch use CUBLAS OP T. In the phase of backward propagation, it needs to use matrix multiplication to calculate the gradients and use element-wise matrix operation to update the parameters. The cublasSgemm API provides the full support to backward propagation because it adds a scaled (parameter beta as scalar) matrix after matrix multiplication. So if we merged gradients calculation and update operation into a single GPU kernel, the calculation effeciency could be much better. To optimize the effeciency of FCNs, it is better to use cublasSgemm API without transpose and use cublasSgemm to calculate gradients and do update operations at the same time.\nOn CNNs, all the toolkits use cuDNN library to do convolution operations. Though the same API call, the parameters may determine different kernels to use to do related computations. We found that on convolution operation, FFT is an optimized way compared to do convolution directly. After FFT of a matrix, convolution calculation can be considered as inner product operation which is much faster. TensorFlow and Torch acheive much better performance\nthan the other two tools, and FFT is one of the main factor that contributes to the efficiency. And CNTK also performs some parts of convolution by using FFT so that it performs slightly better than Caffe. Therefore, FFT algorithms could be better accelerated to improve the effeciency of CNNs training.\nOn RNNs with LSTM, Tensorflow and Torch achieve similar performance while CNTK runs nearly 4 times slower on GTX 1080. To launch training procedure of LSTM, Torch executes lots of Pointwise kernels of basic operations such as multiplication, addition, sigmoid, etc. on tensors designed by itself. Regarding the workload of kernels, Torch gives more than 50 blocks of which size is batch size. In this way Torch can somehow fully utilize the computation resources of GPU. We know that TensorFlow organizes all the computations as graph operations [5], which is also indicated by the kernels launched during the training procedure to some\nextend. There are a large number of TensorAssignOp kernels with different types of operations also including scalar sum, scalar product etc. on tensors. However, we found that the order of launched kernels during one training procedure is not consistent. The best performance achieved by TensorFlow probably results from the out-of-order executions of a set of the graph operations which are independent from each other. As for CNTK, some specific kernels for training LSTM designed by itself are also very different from those of Torch and TensorFlow. The most two time-consuming kernels are logSoftMaxRowWise and sgemm sm35 ldg nn. However, more than half of kernels are launched with only one block which could not fully utilize the resources of GPU. This may cause the worst performance among these three toolkits. So both parallelization of operations and full utilization of GPU resource should be better considered when we design algorithms to accelerate RNNs."}, {"heading": "6. Conclusion", "text": "This is an evaluation of GPU-accelerated tools on three types of popular deep learning methods utilizing different hardware platforms. According to benchmark results, when there exists GPUs, we found that Caffe performs better on fully connected networks, and TensorFlow is outstanding on both convolutional neural networks and recurrent neural networks. The GPU memory is one of the key component for running big size networks in many tools, such as Caffe, CNTK and Torch cannot run ResNet-50 with 32 mini-batch size or more on GTX 980 card which has 4GB memory, while TensorFlow has a better capability in managing GPU memory and it runs normally in all of configured cases. cuBLAS is a high-performance BLAS library, but the parameters of APIs are very important to achieve good results. FFT is a better choice to calculate the convolution operation in some circumstances.\nOn the CPU-only machines, Caffe has better results in CPU parallelization, and TensorFlow also makes better use of CPU resources. On the CPU parallel mechanism, it is better that threads allocated are equal to the number of CPU cores.\nGTX 1080, which has higher base clock at 1733 MHz and more CUDA cores, gets better results in most cases. However, there is larger memory (12 GB) on Tesla K80 card which allows applications run with larger size of networks and greater mini-batch size. In addition, each K80 card is equipped with 2 GPU ships, which may acquire better performance if the programs are parallelized, but it is not totally used in our benchmarking.\nLimitations: We do not test the scalability across multiple GPUs and multiple machines, in which way it might not enhance main features of some tools, such as CNTK is the one that supports parallelization across both multiple GPUs and machines while others are not."}], "references": [{"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Three classes of deep learning architectures and their applications: a tutorial survey", "author": ["L. Deng"], "venue": "APSIPA transactions on signal and information processing, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia, 2014, pp. 675\u2013678.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["D. Yu", "A. Eversole", "M. Seltzer", "K. Yao", "Z. Huang", "B. Guenter", "O. Kuchaiev", "Y. Zhang", "F. Seide", "H. Wang"], "venue": "Technical report, Tech. Rep. MSR, Microsoft Research, 2014, 2014. research. microsoft. com/apps/pubs, Tech. Rep., 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensorflow: Largescale machine learning on heterogeneous systems, 2015", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "Software available from tensorflow. org, vol. 1, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Torch7: A matlablike environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop, no. EPFL-CONF-192376, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1923}, {"title": "4.0 cublas library", "author": ["C. Toolkit"], "venue": "Nvidia Corporation, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "arXiv preprint arXiv:1410.0759, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation, vol. 1, no. 4, pp. 541\u2013551, 1989.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["D. Tang", "B. Qin", "T. Liu"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1422\u20131432.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, vol. 61, pp. 85\u2013117, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Theory of the backpropagation neural network", "author": ["R. Hecht-Nielsen"], "venue": "Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1989, pp. 593\u2013605.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1989}, {"title": "Towards end-to-end speech recognition with recurrent neural networks.", "author": ["A. Graves", "N. Jaitly"], "venue": "in ICML, vol", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2012, pp. 1223\u20131231.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1135\u20131143.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparative study of deep learning software frameworks", "author": ["S. Bahrampour", "N. Ramakrishnan", "L. Schott", "M. Shah"], "venue": "arXiv preprint arXiv:1511.06435, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft computational network toolkit offers most efficient distributed deep learning computational performance", "author": ["X. Huang"], "venue": "https: //goo.gl/9UUwVn, 2015, accessed: 2016-07-12.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The success of deep learning is attributed to its high representational ability of input data, by using various layers of artificial neurals [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 1, "context": "GPUs have played a key role in the success of deep learning by significantly reducing the training time [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "In order to increase the efficiency in developing deep learning methods, there are a number of open-source deep learning toolkits including Caffe from UC Berkeley [3], CNTK from Microsoft [4], TensorFlow from Google [5], and Torch from personal group [6], all these tools support multi-core CPUs and many-core GPUs.", "startOffset": 163, "endOffset": 166}, {"referenceID": 3, "context": "In order to increase the efficiency in developing deep learning methods, there are a number of open-source deep learning toolkits including Caffe from UC Berkeley [3], CNTK from Microsoft [4], TensorFlow from Google [5], and Torch from personal group [6], all these tools support multi-core CPUs and many-core GPUs.", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "In order to increase the efficiency in developing deep learning methods, there are a number of open-source deep learning toolkits including Caffe from UC Berkeley [3], CNTK from Microsoft [4], TensorFlow from Google [5], and Torch from personal group [6], all these tools support multi-core CPUs and many-core GPUs.", "startOffset": 216, "endOffset": 219}, {"referenceID": 5, "context": "In order to increase the efficiency in developing deep learning methods, there are a number of open-source deep learning toolkits including Caffe from UC Berkeley [3], CNTK from Microsoft [4], TensorFlow from Google [5], and Torch from personal group [6], all these tools support multi-core CPUs and many-core GPUs.", "startOffset": 251, "endOffset": 254}, {"referenceID": 6, "context": "TensoFlow uses Eigen [7] as accelerated matrix operation library, while Caffe, CNTK, Torch employ OpenBlas [8] or cuBLAS [9] to speed up matrix related calculations.", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "All the mentioned tools import cuDNN [10], which is a GPU-accelerated deep learning library, for their neural network computing.", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": ", fully connected neural networks (FCNs) [11], convolutional neural networks (CNNs) [12][13][14], and recurrent neural networks (RNNs) [15]) on state-of-the-art GPU-accelerated tools (i.", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": ", fully connected neural networks (FCNs) [11], convolutional neural networks (CNNs) [12][13][14], and recurrent neural networks (RNNs) [15]) on state-of-the-art GPU-accelerated tools (i.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": ", fully connected neural networks (FCNs) [11], convolutional neural networks (CNNs) [12][13][14], and recurrent neural networks (RNNs) [15]) on state-of-the-art GPU-accelerated tools (i.", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": ", fully connected neural networks (FCNs) [11], convolutional neural networks (CNNs) [12][13][14], and recurrent neural networks (RNNs) [15]) on state-of-the-art GPU-accelerated tools (i.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": ", fully connected neural networks (FCNs) [11], convolutional neural networks (CNNs) [12][13][14], and recurrent neural networks (RNNs) [15]) on state-of-the-art GPU-accelerated tools (i.", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "for different applications [16].", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "FCN is one of the deep neural networks that have the longest history dated back to 1980s when the backpropagation (BP) algorithm [17] was developed.", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "And for CNN and RNN, they have been revealed strong power on the applications of image recognition and natural language processing respectively [12][13][14].", "startOffset": 144, "endOffset": 148}, {"referenceID": 10, "context": "And for CNN and RNN, they have been revealed strong power on the applications of image recognition and natural language processing respectively [12][13][14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 11, "context": "And for CNN and RNN, they have been revealed strong power on the applications of image recognition and natural language processing respectively [12][13][14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 15, "context": "RNNs allow cyclical connections of the units in the network [18][13][14].", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "RNNs allow cyclical connections of the units in the network [18][13][14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "RNNs allow cyclical connections of the units in the network [18][13][14].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Furthermore, Long-short Term Memory(LSTM) [19][15] has been proposed to address vanished and exploding gradients on RNNs.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "Furthermore, Long-short Term Memory(LSTM) [19][15] has been proposed to address vanished and exploding gradients on RNNs.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "The authors [3] claim that Caffe can process 40 million images per day with GPU-accelerated version on a single NVIDIA K40 or Titan GPU.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "3x on NVIDIA K40 card [10].", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": ", GoogLeNet [20]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "To accelerate the training speed of deep neural networks, both CPUs SSE techniques and float points SIMD models are used to implement deep learning algorithms [21], which achieve 3x speedup over optimized floating-point baseline.", "startOffset": 159, "endOffset": 163}, {"referenceID": 19, "context": "[22] proposed a large scaled distribute deep networks and developed two algorithms (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] use the method of pruning redundant connections to reduce parameters without losing network representational ability, which could reduce the number of parameters of AlexNet from 61 millions to 6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] did the similar work with us, but they only used a single architecture of GPU (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Secondly, we choose AlexNet [12] and ResNet50 [25], both of which are CNNs, to do the testing.", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "Secondly, we choose AlexNet [12] and ResNet50 [25], both of which are CNNs, to do the testing.", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "Lastly, because of the main computation complexity of RNNs is the length of input sequence, we select 2 LSTM [15] layers for testing, with input length of 32 (LSTM-32) and 64 (LSTM64) respectively.", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "The results are matched with the official claims of CNTK [26].", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "We know that TensorFlow organizes all the computations as graph operations [5], which is also indicated by the kernels launched during the training procedure to some extend.", "startOffset": 75, "endOffset": 78}], "year": 2016, "abstractText": "Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training time. However, different tools exhibit different features and running performance when training different types of deep networks on different hardware platforms, which makes it difficult for end users to select an appropriate pair of software and hardware. In this paper, we aim to make a comparative study of the state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We benchmark the running performance of these tools with three popular types of neural networks on two CPU platforms and three GPU platforms. Our contribution is twofold. First, for deep learning end users, our benchmarking results can serve as a guide to selecting appropriate software tool and hardware platform. Second, for deep learning software developers, our in-depth analysis points out possible future directions to further optimize the training performance.", "creator": "LaTeX with hyperref package"}}}