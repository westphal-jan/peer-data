{"id": "1608.07253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "Learning Latent Vector Spaces for Product Search", "abstract": "We enable a novel impulses vector space experimental might jointly fiance the differentiation representations include words, click - noting products there a high-resolution this part two without the let there explaining annotations. The power of next modeling area brought its ability to directly custom the discriminative relation between imports having a particular same. We compare our reasoning then current generalised vector it variants (LSI, LDA and word2vec) this formulate it another a feature in goes learning intended 2nd moving. Our latent approximated entire features persistence now lacks career as entire flees not sells representations. Furthermore, seen mapping since nothing once buy such the representations of verses contribution some from the deficiencies anarcho-capitalism back came several combining representations time parameter equation. We require came brought - considerable analysis major the amazing than our same and mapping be structure of their learned modes.", "histories": [["v1", "Thu, 25 Aug 2016 18:57:50 GMT  (782kb,D)", "http://arxiv.org/abs/1608.07253v1", "CIKM2016, Proceedings of the 25th ACM International Conference on Information and Knowledge Management. 2016"]], "COMMENTS": "CIKM2016, Proceedings of the 25th ACM International Conference on Information and Knowledge Management. 2016", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["christophe van gysel", "maarten de rijke", "evangelos kanoulas"], "accepted": false, "id": "1608.07253"}, "pdf": {"name": "1608.07253.pdf", "metadata": {"source": "META", "title": "Learning Latent Vector Spaces for Product Search", "authors": ["Christophe Van Gysel", "Maarten de Rijke", "Evangelos Kanoulas"], "emails": ["cvangysel@uva.nl", "derijke@uva.nl", "e.kanoulas@uva.nl", "permissions@acm.org."], "sections": [{"heading": null, "text": "Keywords Entity retrieval; Latent space models; Representation learning"}, {"heading": "1. INTRODUCTION", "text": "Retail through online channels has become an integral part of consumers\u2019 lives [40]. In addition to using these online platforms that generate hundreds of billions of dollars in revenue [22], consumers increasingly participate in multichannel shopping where they research items online before purchasing them in brick-andmortar stores. Search engines are essential for consumers to be able to make sense of these large collections of products available online [30]. In the case of directed searching (in contrast to exploratory browsing), users formulate queries using characteristics of the product they are interested in (e.g., terms that describe the product\u2019s category) [51]. However, it is widely known that there exists a mismatch between queries and product representations where both use different terms to describe the same concepts [34]. Thus, there is an urgent need for better semantic matching methods.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\nCIKM\u201916 , October 24 - 28, 2016, Indianapolis, IN, USA c\u00a9 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4073-1/16/10. . . $15.00\nDOI: http://dx.doi.org/10.1145/2983323.2983702\nProduct search is a particular example of the more general entity finding task that is increasingly being studied. Other entity finding tasks considered recently include searching for people [6], books [23] and groups [35]. Products are retrievable entities where every product is associated with a description and one or more user reviews. Therefore, we use the terms \u201cproduct\u201d and \u201centity\u201d interchangeably in this paper. However, there are two important differences between product search and the entity finding task as defined by de Vries et al. [14]. First, in entity finding one retrieves entities of a particular type from large broad coverage multi-domain knowledge bases such as Wikipedia [3, 14]. In contrast, product search engines operate within a single domain which can greatly vary in size. Second, user queries in product search consist of free-form text [51], as opposed to the semi-structured queries with additional type or relational constraints being used in entity finding [5, 14].\nIn this paper we tackle the problem of discriminating between products based on the language (i.e., descriptions and reviews) they are associated with. Existing methods that are aimed at discriminating between entities based on textual data learn word representations using a language modeling objective or heuristically construct entity representations [16, 57]. Our approach directly learns two things: a unidirectional mapping between words and entities, as well as distributed representations of both words and entities. It does so in an unsupervised and automatic manner such that words that are strongly evidential for particular products are projected nearby those products. While engineering of representations is important in information retrieval [2, 10, 12, 16, 25, 61], unsupervised joint representation learning of words and entities has not received much attention. We fill this gap. Our focus on learning representations for an end-to-end task such as product search is in contrast to the large volume of recent literature on word representation learning [56] that has a strong focus on upstream components such as distributional semantics [42, 49], parsing [13, 56] and information extraction [13, 56]. In addition, our focus on unsupervised representation learning is in contrast to recent entity representation learning methods [10, 61] that heavily depend on precomputed entity relationships and cannot be applied in their absence.\nIn recent years, significant progress has been made concerning semantic representations of entities. We point out three key insights on which we build: (1) Distributed representations [27] learned by discriminative neural networks reduce the curse of dimensionality and improve generalization. Latent features encapsulated by the model are shared by different concepts and, consequently, knowledge about one concept influences knowledge about others. (2) Discriminative approaches outperform generative models if enough training data is available [7, 47] as discriminative models solve the classification problem directly instead of solving a more general problem first [58]. (3) Recently proposed unsu-\nar X\niv :1\n60 8.\n07 25\n3v 1\n[ cs\n.I R\n] 2\n5 A\nug 2\n01 6\npervised neural retrieval models [57] do not scale as they model a distribution over all retrievable entities; the approach is infeasible during training if the collection of retrievable entities is large.\nBuilding on these insights, we introduce Latent Semantic Entities (LSE), a method that learns separate representations of words and retrievable objects jointly for the case where mostly unstructured documents are associated with the objects (i.e., descriptions and user reviews for products) and without relying on predefined relationships between objects (e.g., knowledge graphs). LSE learns to discriminate between entities for a given word sequence by mapping the sequence into the entity representation space. Contrary to heuristically constructed entity representations [16], LSE learns the relationship between words and entities directly using gradient descent. Unlike [57], we avoid computing the full probability distribution over entities; we do so by using noise-contrastive estimation.\nOur research questions are as follows: (1) How do the parameters of LSE influence its efficacy? (2) How does LSE compare to latent vector models based on LDA, LSI and word2vec? (3) How does LSE compare to a smoothed language model that applies lexical term matching? (4) What is the benefit of incorporating LSE as a feature in a learning-to-rank setting?\nWe contribute: (1) A latent vector model, LSE, that jointly learns the representations of words, entities and the relationship between the former, together with an open-source implementation.1 (2) A study of the influence of LSE\u2019s parameters and how these influence its ability to discriminate between entities. (3) An in-depth comparative analysis of the entity retrieval effectiveness of latent vector models. (4) Insights in how LSE can improve retrieval performance in entity-oriented search engines. (5) An analysis of the differences in performance between latent vector models by examining entity representations and mappings from queries to entity space."}, {"heading": "2. RELATED WORK", "text": ""}, {"heading": "2.1 Product retrieval", "text": "Product search engines are an important source of traffic in the e-commerce market [30]. Specialized solutions are needed to maximize the utilization of these platforms. Nurmi et al. [48] note a discrepancy between buyers\u2019 shopping lists and how retail stores maintain information. They introduce a grocery retrieval system that retrieves products using shopping lists written in natural language. Product resolution [1] is an important task for e-commerce aggregation platforms, such as verticals of major web search engines and price comparison websites. Duan et al. [19] propose a probabilistic mixture model for the attribute-level analysis of product search logs. They focus on structured aspects of product entities, while in this work we learn representations from unstructured documents. Duan et al. [20] extend the language modeling approach to product databases by incorporating the ability to condition on specification (e.g., lightweight products only). They note that while languages such as SQL can be used effectively to query these databases, their use is difficult for non-experienced end users. Duan and Zhai [18] study the problem of learning query intent representation for structured product entities. They emphasize that existing methods focus only on the query space and overlook critical information from the entity space and the connection in between.\nWe agree that modeling the connection between query words and entities and propagating information from the entity representations back to words is essential. In contrast to their work, we consider the problem of learning representations for entities based on their associations with unstructured documents. 1https://github.com/cvangysel/SERT"}, {"heading": "2.2 Latent semantic information retrieval", "text": "The mismatch between queries and documents is a critical challenge in search [34]. Latent Semantic Models (LSMs) enable retrieval based on conceptual content, instead of exact word matches. LSMs have become popular through the introduction of Latent Semantic Indexing (LSI) [15], followed by probabilistic LSI (pLSI) [28]. Salakhutdinov and Hinton [52] use a deep auto-encoder for the unsupervised learning of latent semantic document bit patterns. Deep Structured Semantic Models [29, 54] employ click data to predict a document\u2019s relevance to a query. Methods based on neural networks have also been used for machine-learned ranking [11, 17, 36]. Van Gysel et al. [57] introduce an LSM for entity retrieval, with an emphasis on expert finding; they remark that training the parameters of their model becomes infeasible when the number of entities increases. In this work we mitigate this problem by considering only a random sample of entities as negative examples during training. This allows us to efficiently estimate model parameters in large product retrieval collections, which is not possibly using the approach of [57] due to its requirement to compute a normalization constant over all entities."}, {"heading": "2.3 Representation learning", "text": "Recently, there has been a growing interest in neural probabilistic language models (LMs) for the modeling of word sequences [8, 43, 44]. Distributed representations [27] of words learned by neural LMs, also known as word embeddings, incorporate syntactic and semantic information [42, 45, 49] as a side-effect of their ability to reduce the dimensionality. Feed-forward [13] and recurrent [42] neural networks perform well in various NLP tasks. Very recently, there has been an increased interest in multimodal neural language models [33], which are used for the task of automated image captioning, amongst others. Learning representations of entities is not new. Bordes et al. [10] leverage structured relations captured in Knowledge Bases (KB) for entity representation learning and evaluate their representations on the link prediction task. Our approach has a strong focus on modeling the language of all entities collaboratively, without the need for explicit entity relations during training. Zhao et al. [61] employ matrix factorization methods to construct low-dimensional continuous representations of entities, categories and words for determining similarity of Wikipedia entities. They employ a word pair similarity evaluation set and only evaluate on pairs referring to Wikipedia entities; they learn a single semantic space for widely-differing concepts (entities, categories and words) of different cardinalities and make extensive use of an underlying Knowledge Graph (KG) to initialize their parameters. In contrast, we model representations of words and entities jointly in separate spaces, in addition to a mapping from word to entity representations, in an unsupervised manner.\nWe tackle the task of learning latent continuous vector representations for e-commerce products for the purpose of product search. The focus of this work lies in the language modeling and representation learning challenge. We learn distributed representations [27] of words and entities and a mapping between the two. At retrieval time, we rank entities according to the similarity of their latent representations to the projected representation of a query. Our model LSE is compared against existing entity-oriented latent vector representations that have been created using LSI, LDA and word2vec. We provide an analysis of model parameters and give insight in the quality of the joint representation space."}, {"heading": "3. LATENT VECTOR SPACES FOR ENTITY RETRIEVAL", "text": "We first introduce a generalized formalism and notation for entityoriented latent vector space models. After that, in \u00a73.2, we introduce Latent Semantic Entities, a latent vector space model that jointly learns representations of words, entities and a mapping between the two directly, based on the idea that entities are characterized by the words they are associated with and vice versa. Product representations are constructed based on the n-grams the products are likely to generate based on their description and reviews, while word representations are based on the entities they are associated with and the context they appear in. We model the relation between word and product representations explicitly so that we can predict the product representation for a previously unseen word sequence."}, {"heading": "3.1 Background", "text": "We focus on a product retrieval setting in which a user wants to retrieve the most relevant products on an e-commerce platform. As in typical information retrieval scenarios, the user encodes their information need as a query q and submits it to a search engine. Product search queries describe characteristics of the product the user is searching for, such as a set of terms that describe the product\u2019s category [51].\nBelow, X denotes the set of entities that we consider. For every xi \u2208 X we assume to have a set of associated documents Dxi . The exact relation between the entity and its documents depends on the problem setting. In this paper, entities are products [18, 48] and documents associated with these products are descriptions and product reviews.\nLatent vector space models rely on a function f : V + \u2192 E that maps a sequence of words (e.g., a query q during retrieval) from a vocabulary V to a eE-dimensional continuous entity vector space E \u2282 ReE . Every entity xi \u2208 X has a corresponding vector representation ei \u2208 E. Let Sc : E \u00d7 E \u2192 R denote the cosine similarity between vectors in E. For a given query q, entities xi are ranked in decreasing order of the cosine similarity between ei and the query projected into the space of entities, f(q). Fig. 1 illustrates how entities are ranked according to a projected query. For\nLSI, f is defined as the multiplication of the term-frequency vector representation of q with the rank-reduced term-concept matrix and the inverse of the rank-reduced singular value matrix [15]. In the case of LDA, f becomes the distribution over topics conditioned on q [9]. This distribution is computed as the sum of the topic distributions conditioned on the individual words of q. In this paper, the embedding f is learned; see \u00a73.3 below.\nTraditional vector space models operate on documents instead of entities. Demartini et al. [16] extend document-oriented vector spaces to entities by representing an entity as a weighted sum of the representations of their associated documents:\nei = \u2211\ndj\u2208Dxi\nri,jf(dj) (1)\nwhere f(dj) is the vector representation of dj and ri,j denotes the relationship weight between document dj and entity xi. In this work we put ri,j = 1 whenever dj \u2208 Dxi for a particular xi \u2208 X and ri,j = 0 otherwise, as determining the relationship weight between entities and documents is a task in itself."}, {"heading": "3.2 Latent semantic entities", "text": "While Eq. 1 adapts document-oriented vector space models to entities, in this work we define f by explicitly learning (\u00a73.3) the mapping between word and entity representations and the representations themselves:\nf(s) = tanh ( W \u00b7 (Wv \u00b7 1\n|s| \u2211 wi\u2208s \u03b4i) + b\n) (2)\nfor a string s of constituent words w1, . . . , w|s| (an n-gram extracted from a document or a user-issued query), where Wv is the eV \u00d7 |V | projection matrix that maps the averaged one-hot representations (i.e., a |V |-dimensional vector with element i turned on and zero elsewhere) of word wi, \u03b4i, to its eV -dimensional distributed representation. This is equivalent to taking the embeddings of the words in s and averaging them. In addition, b is a eE-dimensional bias vector,W is the eE\u00d7eV matrix that maps averaged word embeddings to their corresponding position in entity space E and tanh is the element-wise smooth hyperbolic tangent with range (\u22121, 1). This transformation allows word embeddings and entity embeddings to be of a different dimensionality.\nIn other words, for a given string of words we take the representation of this string to be the average of the representations of the words it contains [42, 50]. This averaged word representation is then transformed using a linear map (W ) and afterwards translated using b. We then apply the hyperbolic tangent as non-linearity such that every component lies between \u22121 and 1. First of all, this regularizes the domain of the space and avoids numerical instability issues that occur when the magnitude of the vector components be-\ncomes too large. Secondly, by making the function non-linear we are able to model non-linear class boundaries in the optimization objective that we introduce in the next section. We use We to denote the |X|\u00d7eE matrix that holds the entity representations. Row i of We corresponds to the vector representation, ei, of entity xi. Fig. 2 depicts a schematic overview of the proposed model. The parameters Wv , W , b and We will be learned automatically using function approximation methods as explained below.\nThe model proposed in this section shares similarities with previous work on word embeddings and unsupervised neural retrieval models [42, 57]. However, its novelty lies in its ability to scale to large collections of entities and its underlying assumption that words and entities are embedded in spaces of different dimensionality: (1) The model of [42] has no notion of entity retrieval as it estimates a language model for the whole corpus. (2) Similar to [42], Eq. 2 aggregates words wi \u2208 s to create a single phrase representation of s. However, in [57], a distribution P (X | wi) is computed for everywi independently and aggregation occurs using the factor product. This is infeasible during model training when the collection of retrievable objects becomes too large, as is the case for product search. In the next section (\u00a73.3) we solve this problem by sampling. (3) In both [42, 57] two sets of representations of the same dimensionality are learned for different types of objects with potentially different latent structures (e.g., words, word contexts and experts). As mentioned earlier, Eq. 2 alleviates this problem by transforming one latent space to the other."}, {"heading": "3.3 Parameter estimation", "text": "For a particular document d \u2208 Dxi associated with entity xi, we generate n-grams wj,1, . . . , wj,n where n (window size) remains fixed during training. For every n-gram wj,1, . . . , wj,n, we compute its projected representation f(wj,1, . . . , wj,n) in E using f (Eq. 2). The objective, then, is to directly maximize the similarity between the vector representation of the entity ei and the projected n-gram f(wj,1, . . . , wj,n) with respect to Sc (\u00a73.1), while minimizing the similarity between f(wj,1, . . . , wj,n) and the representations of non-associated entities. This allows the model to learn relations between neighboring words in addition to the associated entity and every word.\nHowever, considering the full set of entities for the purpose of discriminative training can be costly when the number of entities |X| is large. Therefore, we apply a variant of Noise-Contrastive Estimation (NCE) [26, 41, 45, 46] where we sample negative instances from a noise distribution with replacement. We use the uniform distribution over entities as noise distribution. Define\nP (S | ei, f(wj,1, . . . , wj,n)) = \u03c3(ei \u00b7 f(wj,1, . . . , wj,n)) (3)\nas the similarity of two representations in latent entity space, where\n\u03c3(t) = 1\n1 + e\u2212t\ndenotes the sigmoid function and S is an indicator binary random variable that says whether xi is similar to f(wj,1, . . . , wj,n).\nWe then approximate the probability of an entity xi given an ngram by randomly sampling z contrastive examples:\nlog P\u0303 (xi | wj,1, . . . , wj,n) (4) = logP (S | ei, f(wj,1, . . . , wj,n))\n+ z\u2211 k=1,\nxk\u223cU(X)\nlog (1\u2212 P (S | ek, f(wj,1, . . . , wj,n)))\nwhere U(X) denotes the uniform distribution over entities X , the noise distribution used in NCE [26]. Eq. 4 avoids iterating over all entities during parameter estimation as we stochastically sample z entities uniformly as negative training examples.2\nDuring model construction we maximize the log-probability (4) using batched gradient descent. The loss function for a single batch of m instances ((wk,1, . . . , wk,n), xk) consisting of n-grams sampled from documents Dxk (see \u00a74.2) and associated entity xk is as follows:\nL(Wv,We,W , b)\n= \u2212 1 m m\u2211 k=1 log P\u0303 (xk | wk,1, . . . , wk,n)\n+ \u03bb\n2m (\u2211 i,j Wv 2 i,j + \u2211 i,j We 2 i,j + \u2211 i,j W 2i,j ) , (5)\nwhere \u03bb is a weight regularization parameter. Instances are shuffled before batches are created. The update rule for a particular parameter \u03b8 (Wv , We, W or b) given a single batch of size m is:\n\u03b8(t+1) = \u03b8(t) \u2212\u03b1(t) \u2202L \u2202\u03b8 (Wv (t),We (t),W (t), b(t)), (6)\nwhere \u03b1(t) and \u03b8(t) denote the per-parameter learning rate and parameter \u03b8 at time t, respectively. The learning rate \u03b1 consists of the same number of elements as there are parameters; in the case of a global learning rate, all elements of \u03b1 are equal to each other. The derivatives of the loss function (5) are given in the Appendix."}, {"heading": "4. EXPERIMENTAL SETUP", "text": ""}, {"heading": "4.1 Research questions", "text": "In this paper we investigate the problem of constructing a latent vector model of words and entities by directly modeling the discriminative relation between entities and word context. We seek to answer the following research questions: RQ1 How do the parameters of LSE influence its efficacy? In \u00a73 we introduced various hyper-parameters along with the definition of Latent Semantic Entities. We have the size of word representations eV and the dimensionality of the entity representations eE . During parameter estimation, the window size n influences the context width presented as evidence for a particular entity. What is the influence of these parameters on the effectiveness of LSE and can we identify relations among parameters? RQ2 How does LSE compare to latent vector models based on LDA, LSI and word2vec? Is there a single method that always performs best or does effectiveness differ per domain? Does an increase in the vector space dimensionality impact the effectiveness of these methods? RQ3 How does LSE compare to a smoothed language model that applies lexical term matching? How does LSE compare to language models on a per-topic basis? Are there particular topics that work especially well with either type of ranker? RQ4 What is the benefit of incorporating LSE as a feature in a\nlearning-to-rank setting?\n2We exploit the special nature of our evaluation scenario where we know the unique association between documents and entities. The setup can easily be adapted to the more general case where a document is associated with multiple entities by extracting the same word sequences from the document for every associated entity.\nWhat if we combine popularity-based, exact matching and latent vector space features in a linear learning-to-rank setting? Do we observe an increase in effectiveness if we combine these features?"}, {"heading": "4.2 Experimental design", "text": "To answer the research questions posed in \u00a74.1, we evaluate LSE in an entity retrieval setting organized around Amazon products (see \u00a74.3). We choose to experiment with samples of Amazon product data [38, 39] for the following reasons: (1) The collection contains heterogeneous types of evidential documents associated with every entity: descriptions as well as reviews. (2) Every department (e.g., Home & Kitchen) constitutes a separate, self-contained domain. (3) Within each department there is a hierarchical taxonomy that partitions the space of entities in a rich structure. We can use the labels associated with these partitions and the partitions themselves as ground truth during evaluation. (4) Every department consists of a large number of products categorized over a large number of categories. Importantly, this allows us to construct benchmarks with an increasing number of entities. (5) Every product has a variety of attributes that can be used as popularity-based features in a learning-to-rank setting.\nTo answer RQ1 we investigate the relation between the dimensionality of the entity representations eE and window size n. The latter, the window size n, controls the context width the model can learn from, while the former, the dimensionality of the entity representations eE , influences the number of parameters and expressive power of the model. We sweep exponentially over n (2i for 0 \u2264 i < 6) and eE (2i for 6 \u2264 i < 11). RQ2 is answered by comparing LSE with latent vector space model baselines (\u00a74.5) for an increasing entity space dimensionality eE (2i for 6 \u2264 i < 11). For RQ3, we compare the per-topic paired differences between LSE and a lexical language model. In addition, we investigate the correlation between lexical matches in relevant entity documents and ranker preference. We address RQ4 by evaluating LSE as a feature in a machine-learned ranking in addition to query-independent and lexical features.\nThe number of n-grams sampled per entity x \u2208 X from associated documents Dx in every epoch (i.e., iteration of the training data) is equal to \u2308 1 |X| \u2211 d\u2208D max (|d| \u2212 n+ 1, 0) \u2309 ,where the | \u00b7 | operator is used interchangeably for the size of set X and the number of tokens in documents d \u2208 D. This implicitly imposes a uniform prior over entities (i.e., stratified sampling where every entity is of equal importance). The word vocabulary V is created for each benchmark by ignoring punctuation, stop words and case; numbers are replaced by a numerical placeholder token. We prune V by only retaining the 216 most-frequent words so that each word can be encoded by a 16-bit unsigned integer. In terms of parameter initialization of the Latent Semantic Entities model, we sample the initial matrices Wv , W (Eq. 2) and We uniformly in the\nrange [ \u2212 \u221a\n6.0 m+n\n, \u221a\n6.0 m+n\n] for an m\u00d7 n matrix, as this initializa-\ntion scheme is known to improve model training convergence [24], and take the bias vector b to be null. The number of word features is set to eV = 300, similar to [41]. We take the number of negative examples z = 10 to be fixed. Mikolov et al. [41] note that a value of z between 10 and 20 is sufficient for large data sets [45].\nWe used Adam (\u03b1 = 0.001, \u03b21 = 0.9, \u03b22 = 0.999) [32] with batched gradient descent (m = 4096) and weight decay \u03bb = 0.01 during training on NVidia Titan X GPUs. Adam has been designed specifically for non-stationary, stochastic cost functions like the one we defined in Eq. 4. For every model, we iterate over the training data 15 times and choose the best epoch based on the validation sets (Table 1)."}, {"heading": "4.3 Product search benchmarks", "text": "We evaluate on four samples from different product domains3 (Amazon departments), each with of an increasing number of products: Home & Kitchen (8,192 products), Clothing, Shoes & Jewelry (16,384 products), Pet Supplies (32,768 products) and Sports & Outdoors (65,536 products); see Table 1. The documents associated with every product consist of the product description plus reviews provided by Amazon customers.\nRowley [51, p. 24] describes directed product search as users searching for \u201ca producer\u2019s name, a brand or a set of terms which describe the category of the product.\u201d Following this observation, the test topics ci are extracted from the categories each product belongs to. Category hierarchies of less than two levels are ignored, as the first level in the category hierarchy is often non-descriptive for the product (e.g., in Clothing, Shoes & Jewelry this is the gender for which the clothes are designated). Products belonging to a particular category hierarchy are considered as relevant for its extracted topic. Products can be relevant for multiple topics. Textual representations qci of the topics based on the categories are extracted as follows. For a single hierarchy of categories, we tokenize the titles of its sub-categories and remove stopwords and duplicate words. For example, a digital camera lense found in the Electronics department under the categorical topic Camera & Photo \u2192 Digital Camera Lenses will be relevant for the textual query \u201cphoto camera lenses digital.\u201d Thus, we only have two levels of relevance. We do not index the categories of the products as otherwise the query would match the category and retrieval would be trivial."}, {"heading": "4.4 Evaluation measures and significance", "text": "To measure retrieval effectiveness, we report Normalized Discounted Cumulative Gain (NDCG). For RQ4, we additionally report Precision@k (k = 5, 10). Unless mentioned otherwise, significance of observed differences is determined using a two-tailed paired Student\u2019s t-test [55] (\u2217\u2217\u2217 p < 0.01; \u2217\u2217 p < 0.05; \u2217 p < 0.1)."}, {"heading": "4.5 Methods used in comparisons", "text": "We compare Latent Semantic Entities to state-of-the-art latent vector space models for entity retrieval that are known to perform semantic matching [34]. We also conduct a contrastive analysis between LSE and smoothed language models with exact matching capabilities. Vector Space Models for entity finding. Demartini et al. [16] propose a formal model for finding entities using document vector space models (\u00a73.1). We compare the retrieval effectiveness of LSE with baseline latent vector space models created using (1) Latent Semantic Indexing (LSI) [15] with TF-IDF term weighting, (2) Latent Dirichlet Allocation (LDA) [9] with \u03b1 = \u03b2 = 0.1, where a document is represented by its topic distribution, and (3) word2vec [42] with CBOW and negative sampling, where a query/document is represented by the average of its word embeddings (same for queries in LSE). Similar to LSE, we train word2vec for 15 iterations and select the best-performing model using the validation sets (Table 1). Query-likelihood Language Model. For every entity a profile-based statistical language model is constructed using maximumlikelihood estimation [4, 37, 58], which is then smoothed by the language model of the entire corpus. The retrieval score of entity x for query q is defined as\nP\u0303 (q | x) = \u220f ti\u2208q P (ti | \u03b8x), (7)\n3A list of product identifiers, topics and relevance assessments can be found at https://github.com/cvangysel/SERT.\nwhereP (t | \u03b8x) is the probability of term t occurring in the smoothed language model of x (Jelinek-Mercer smoothing [60]). Given a query q, entities are ranked according to P\u0303 (q | x) in descending order. Machine-learned ranking. RankSVM models [31] in \u00a75.2 and 6 are trained using stochastic gradient descent using the implementation of Sculley [53]. We use default values for all parameters, unless stated otherwise. For the experiment investigating LSE as a feature in machine-learned ranking in \u00a75.2, we construct training examples by using the relevant entities as positive examples. Negative instances are generated by sampling from the non-relevant entities with replacement until the class distribution is uniform."}, {"heading": "5. RESULTS AND DISCUSSION", "text": "We start by giving a high-level overview of our experimental results (RQ1 and RQ2), followed by a comparison with lexical matching methods (RQ3) and the use of LSE as a ranking feature (RQ4) (see \u00a74.2 for an overview of the experimental design)."}, {"heading": "5.1 Overview of experimental results", "text": "RQ1: Fig. 3 depicts a heat map for every combination of window size and entity space dimensionality evaluated on the validation sets (Table 1). Fig. 3 shows that neither extreme values for the dimensionality of the entity representations nor the context width alone achieve the highest performance on the validation sets.\nInstead, a low-dimensional entity space (128- and 256-dimensional) combined with a medium-sized context window (4- and 8- grams) achieve the highest NDCG. In the two largest benchmarks (Fig. 3c, 3d) we see that for 16-grams, NDCG actually lowers as the dimensionality of the entity space increases. This is due to the model fitting the optimization objective (Eq. 5), which we use as an unsupervised surrogate of relevance, too well. That is, as the model is given more learning capacity (i.e., higher dimensional representations), it starts to learn more regularities of natural language which counteract retrieval performance.\nRQ2: Fig. 4 presents a comparison between LSE (window size n = 4) and vector space model baselines (\u00a74.5) for increasing entity representation dimensionality (2i for 6 \u2264 i < 11) on the test sets. LSE significantly outperforms (p < 0.01) all baseline methods in most cases (except for Fig. 4a where eE = 1024). For the smaller benchmarks (Fig. 4a, 4b), we see LSI as the main competitor of LSE. However, as the training corpora become larger (in Fig. 4c, 4d), word2vec outperforms LSI and becomes the main con-\ntester of LSE. On all benchmarks, LSE peaks when the entity representations are low-dimensional (128- or 256-dimensional) and afterwards (for a higher dimensionality) performance decreases. On the other hand, word2vec stagnates in terms of NDCG around representations of 512 dimensions and never achieves the same level as LSE did for one or two orders of magnitude (base 2) smaller representations. This is a beneficial trait of LSE, as high-dimensional vector spaces are undesirable due to their high computational cost during retrieval [59]."}, {"heading": "5.2 A feature for machine-learned ranking", "text": "We now investigate the use of LSE as a feature in a learning to rank setting [36]. Latent vector space models are known to provide a means of semantic matching as opposed to a purely lexical matching [34, 57]. To determine to which degree this is indeed the case, we first perform a topic-wise comparison between LSE and a lexical language model, the Query-likelihood Language Model (QLM) [60], as described in \u00a74.5. We optimize the parameters of LSE and QLM on the validation sets for every benchmark (Table 1). In the case of LSE, we select the model that performs best in Fig. 3. For QLM, we sweep over \u03bb linearly from 0.0 to 1.0 (inclusive) with increments of 0.05.\nRQ3: Fig. 5 shows the per-topic paired difference between LSE and QLM in terms of NDCG. Topics that benefit more from LSE have a positive value on the y-axis, while those that prefer QLM have a negative value. We can see that both methods perform similarly for many topics (where 4 = 0.0). For certain topics one\nmethod performs substantially better than the other, suggesting that the two are complementary. To further quantify this, we investigate the relation between specific topic terms and their occurrence in documents relevant to these topics. That is, we measure the correlation between the per-topic 4NDCG (as described above) and the average inverse document frequency (IDF) of exact/lexically matched terms in the profile-based language model. In Table 2 we observe that queries that contain specific tokens (i.e., with high inverse document frequency) and occur exactly in documents associated with relevant products, benefit more from QLM (lexical matches). Conversely, queries with less specific terms or without exact matches in the profiles of relevant products gain more from LSE (semantic matches).\nThis observation motivates the use of LSE as a ranking feature in addition to traditional language models. Specifically, we now evaluate the use of LSE as a feature in a linear RankSVM (\u00a74.5). Following Fang et al. [21], we consider query-independent (QI) popularity-based features in addition to features provided by LSE and QLM. This allows us to consider the effect of the querydependent features independent from their ability to model a popularity prior over entities. Table 3 lists the feature sets.\nRQ4: Table 4 shows the results for different combinations of feature sets used in a machine-learned ranker, RankSVM. The experiment was performed using 10-fold cross validation on the test sets (Table 1). The combination using all features outperforms smaller subsets of features, on all metrics. We conclude that Latent Semantic Entities adds a signal that is complementary to traditional (lexical) language models, which makes it applicable in a wide range of entity-oriented search engines that use ranker fusion techniques."}, {"heading": "6. ANALYSIS OF REPRESENTATIONS", "text": "Next, we analyze the entity representations ei of the vector space models independent of the textual representations by providing empirical lower-bounds on their maximal retrieval performance, followed by a comparison with their actual performance so as to measure the effectiveness of word-to-entity mapping f .\nFig. 3 and 4 show which levels of performance may be achieved by using the latent models to generate a ranking from textual queries (Eq. 2). But this is only one perspective. As entities are ranked according to their similarity with the projected query vector f(qc), the performance for retrieving entities w.r.t. the textual representation of a topic c depends on the structure of the entity space E, the ideal retrieval vector e\u2217c \u2208 E (i.e., the vector that optimizes retrieval performance), and the similarity between f(qc) and e\u2217c .\nHow can we determine the ideal vector e\u2217c? First, we define it to be the vector for which the cosine similarity with each of the entity embeddings results in a ranking where relevant entities are ranked higher than non-relevant or unjudged entities. We approximate e\u2217c by optimizing the pair-wise SVM objective [31, 53]. That is, for every topic c we construct a separate RankSVM model based on its ground-truth as follows. We only consider topics with at least two relevant entities, as topics with a single relevant entity have a trivial optimal retrieval vector (the entity representation of the single relevant entity). Using the notation of [31], the normalized entity representations are used as features, and hence the feature mapping \u03c6 is defined as\n\u03c6(c, xi) = ei \u2016ei\u20162 for all xi \u2208 X.\nThe target ranking r\u2217c is given by the entities relevant to topic c. Thus, the features for every entity become the entity\u2019s normalized representation and its label is positive if it is relevant for the topic and negative otherwise. The pair-wise objective then finds a weight vector such that the ranking generated by ordering according to the vector scalar product between the weight vector and the normalized entity representations correlates with the target ranking r\u2217c . Thus, our approximation of the ideal vector, e\u0303\u2217c , is given by the weight vector wc for every c.4\nWhat is the performance of this approximately ideal vector representation? And how far are our representations removed from it? Fig. 6 shows the absolute performance of e\u0303\u2217c (dashed curves) and f(q) (solid curves) in terms of NDCG. Comparing the (absolute) difference between every pair of dashed and solid curves for a sin-\n4Note that e\u0303\u2217c does not take into account the textual representations qc of topic c, but only the clustering of entities relevant to c and their relation to other entities.\ngle latent model gives an intuition of how much performance in terms of NDCG there is to gain by improving the projection function f for that method. The approximately ideal vectors e\u0303\u2217c discovered for LSE outperform all baselines significantly. Interestingly, for representations created using LDA, the optimal performance goes up while the actual performance stagnates. This indicates that a higher vector space dimensionality renders better representations using LDA, however, the projection function f is unable to keep up in the sense that projected query vectors are not similar to the representations of their relevant entities. The latent models with the best representations (LSE and LSI) also have the biggest gap between f(q) and e\u0303\u2217c in terms of achieved NDCG.\nWe interpret the outcomes of our analysis as follows. The entity space E has more degrees of freedom to cluster entities more appropriately as the dimensionality of E increases. Consequently, the query projection function f is expected to learn a more complex function. In addition, as the dimensionality of E increases, so does the modeling capacity of the projection function f in the case of LSE and LSI (i.e., the transformation matrices become larger) and therefore more parameters have to be learned. We conclude that our method can more effectively represent entities in a lowerdimensional space than LSI by making better use of the vector space capacity. This is highly desirable, as the asymptotic runtime complexity of many algorithms operating on vector spaces increases at least linearly [59] with the size of the vectors."}, {"heading": "7. CONCLUSIONS", "text": "We have introduced Latent Semantic Entities, an unsupervised latent vector space model for product search. It jointly learns a unidirectional mapping between, and latent vector representations of, words and products. We have also defined a formalism for latent vector space models where latent models are decomposed into a mapping from word sequences to the product vector space, representations of products in that space, and a similarity function. We have evaluated our model using Amazon product data, and compared it to state-of-the-art latent vector space models for product ranking (LSI, LDA and word2vec). LSE outperforms all baselines for lower-dimensional vector spaces.\nIn an analysis of the vector space models, we have compared the performance achieved with the ideal performance of the proposed product representations. We have shown that LSE constructs better product representations than any of the baselines. In addition, we have obtained important insights w.r.t. how much performance there is to gain by improving the individual components of latent vector space models. Future work can focus on improving the mapping from words to products by incorporating specialized features or increasing the mapping\u2019s complexity. In addition, semisupervised learning may help specialize the vector space and mapping function for particular retrieval settings.\nA comparison of LSE with a smoothed lexical language model unveils that the two methods make very different errors. Some directed product search queries require lexical matching, others benefit from the semantic matching capabilities of latent models. We have evaluated LSE as a feature in a machine-learned ranking setting and found that adding LSE to language models and popularitybased features significantly improves retrieval performance.\nAs to future work, in this paper we focus on the unsupervised setting where we have a description and a set of reviews associated with every product. Fig. 6 shows that there is a lot of performance to gain by improving the query projection function f . In a semi-supervised setting, the difference between e\u2217c and f(q) can be minimized according to pairs of queries and ideal rankings. As an additional step, query-relevance training data could be incorporated during estimation of the entity space E. Moreover, as mentioned in \u00a76, the query projection function f is expected to learn a more\ncomplicated mapping. Hence, it may be beneficial to consider incorporating additional hierarchical depth using multiple non-linear transformations in the construction of f . More generally, the obtained product representations can be beneficial for various entityoriented prediction tasks such as entity disambiguation or related entity finding. While we have focused on product retrieval in this work, the proposed model, insights and ideas can be applied in broader settings, such as entity finding and ad-hoc retrieval.\nAcknowledgments. The authors would like to thank Artem Grotov, Nikos Voskarides, Zhaochun Ren, Tom Kenter, Manos Tsagkias, Hosein Azarbonyad and the anonymous reviewers for their valuable comments and suggestions. This research was supported by Ahold, Amsterdam Data Science, Blendle, the Bloomberg Research Grant program, the Dutch national program COMMIT, Elsevier, the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the ESF Research Network Program ELIAS, the Google Faculty Research Award scheme, the Royal Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project, the Microsoft Research Ph.D. program, the Netherlands eScience Center under project number 027.012.105, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scientific Research (NWO) under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15, 652.002.- 001, 612.001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We introduce a novel latent vector space model that jointly learns<lb>the latent representations of words, e-commerce products and a<lb>mapping between the two without the need for explicit annotations.<lb>The power of the model lies in its ability to directly model the dis-<lb>criminative relation between products and a particular word. We<lb>compare our method to existing latent vector space models (LSI,<lb>LDA and word2vec) and evaluate it as a feature in a learning to<lb>rank setting. Our latent vector space model achieves its enhanced<lb>performance as it learns better product representations. Further-<lb>more, the mapping from words to products and the representations<lb>of words benefit directly from the errors propagated back from the<lb>product representations during parameter estimation. We provide<lb>an in-depth analysis of the performance of our model and analyze<lb>the structure of the learned representations.", "creator": "LaTeX with hyperref package"}}}