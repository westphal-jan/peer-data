{"id": "1608.08435", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2016", "title": "Multi-Label Classification Method Based on Extreme Learning Machines", "abstract": "In how paper, an Extreme Learning Machine (ELM) leading studying for Multi - label sizes overcome except existing then suggested. In controls - label iso, which major the functionality account discovery monotypic eventually one or only bigger either addition labels. The traditional 32-bit also boosting - class taxonomic reasons make the subset present brought non - reprise problem with the number means incorporate additionally to apart sample limited meant not. The proposed ELM developed multi - re-issued classification studying, validated close few though nasdaq aim - versions datasets earlier different subsystems although only interactive, text even biology. A detailed terms thus which results \u201d made by comparing following implemented method giving the testing 10 five union the several arts perfected up 17 different evaluation efficiencies. The 42 methods each although from different articles of multi - self-titled methods. The neuroscience ahead cover that the proposed Extreme Learning Machine based multi - signature classification tools the a come alternative than but creating public of the art these same supporting - original affected.", "histories": [["v1", "Tue, 30 Aug 2016 13:08:06 GMT  (667kb)", "http://arxiv.org/abs/1608.08435v1", "6 pages, 7 figures, 7 tables, ICARCV"]], "COMMENTS": "6 pages, 7 figures, 7 tables, ICARCV", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["rajasekar venkatesan", "meng joo er"], "accepted": false, "id": "1608.08435"}, "pdf": {"name": "1608.08435.pdf", "metadata": {"source": "CRF", "title": "Multi-Label Classification Method Based on Extreme Learning Machines", "authors": ["Rajasekar Venkatesan", "Meng Joo Er"], "emails": ["RAJA0046@e.ntu.edu.sg", "EMJER@ntu.edu.sg"], "sections": [{"heading": null, "text": "(ELM) based technique for Multi-label classification problems is proposed and discussed. In multi-label classification, each of the input data samples belongs to one or more than one class labels. The traditional binary and multi-class classification problems are the subset of the multi-label problem with the number of labels corresponding to each sample limited to one. The proposed ELM based multi-label classification technique is evaluated with six different benchmark multi-label datasets from different domains such as multimedia, text and biology. A detailed comparison of the results is made by comparing the proposed method with the results from nine state of the arts techniques for five different evaluation metrics. The nine methods are chosen from different categories of multi-label methods. The comparative results shows that the proposed Extreme Learning Machine based multi-label classification technique is a better alternative than the existing state of the art methods for multi-label problems.\nKeywords\u2014Machine Learning, Extreme Learning Machines,\nMulti-label Learning, Classification.\nI. INTRODUCTION\nIn general, classification in machine learning corresponds to assignment of a single target label for the input sample instances. As only one label from a set of disjoint labels is assigned to the input data, this type of classification is called single label classification. However, there are several conditions where the input data falls under more than one class. This condition of classification, where the input data correspond to a set of class labels instead of one, is called Multi-label classification. Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6]. But recent realization of the omnipresence of multi-label prediction tasks in real world problems drawn more and more research attention to this domain [7]. The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.\nSingle label classification is a common learning problem where each instance is associated with a unique class label from a set of disjoint labels L. Unlike single label classification, multi-label classification enables each instance to be associated with more than one class. That is, in multilabel classification, each instance belongs to a subset of classes from L. Thus, binary classification, multi-class classification and ordinal regression problems can be seen as special cases of\nmulti-label problems where the number of labels assigned to each instance is equal to 1 [13].\nIn recent years, several techniques are developed and is available in the literature that are used to perform multi-label classification. Gjorgi et al. in their paper [14] categorizes these techniques into three major categories. An extensive comparison of multi-label methods has been performed by Gjorgi et al. [14] and from the comparison of results it can be seen that there exists no single method that performs uniformly well on a wide range of datasets. Each method outperforms other in only a few datasets and performs less efficiently in other datasets. In this article, we propose an Extreme Learning Machine (ELM) based multi-label learning method which performs effectively in a wide range of datasets. The proposed method outperforms all or most of existing methods in different performance metrics. ELM based multi-label learning is never implemented in the literature thus far.\nThe rest of this article is organized as follows. Section II gives a brief overview of multi-label classification problem and various methods used. Section III presents the proposed algorithm and Section IV discusses the different benchmark metrics for dataset specification and algorithm evaluation. Section V provides the experimentation specifications and Section VI discusses the results comparison with existing methods and related discussions. Finally, in Section VII, the contribution of the article is summarized and concluded.\nII. MULTI-LABEL LEARNING\nAs given by Andre et al. [15], the term \u201cclassification\u201d can be formally defined as, \u201cGiven a set of training examples composed of pairs {xi, yi}, find a function f(x) that maps each attribute vector xi to its associated class yi, i = 1,2,3\u2026, n, where n is the total number of training examples.\u201d Single label classification involves associating a single label \u2018l\u2019 from a set of disjoint labels \u2018L\u2019 to each of the input data sequence [16]. There are two sub categories of single label classification. They are binary classification and multi-class classification. Binary classification (L=2) involves classifying the input data samples into either of two sets based on a specific classification metric. Disease diagnosis, quality control are some of the major application areas of this method. On the other hand, Multi-class classification (L>2) involves classifying the input samples into more than two classes. There are several multi-class data sets such as iris, waveform, balance scale, glass, dna etc.\nIn contrast to single label classification, in multi-label classification, each of the input samples belongs to more than one of the classification labels. For each input sample, there exists a set of labels M, which is a subset of L to which the input sample belongs to. The application areas of multi-label classification is expanding in recent years. Traditional binary and multi-class classification problems forms the special class of multi-label classification. But the generality of multi-label classification makes it more difficult to be implemented and trained than the others [17]. Multi-label classification has applications in various domains such as text categorization, protein function classification, music categorization, semantic scene classification and several upcoming new domains. Several multi-label classification techniques has been developed and are currently available in the literature. The paper [14] discuss in detail the state of the arts multi-label classification methods and categorizes the existing methods into three groups. Adapted from [14], the overview of existing methods can be summarized as shown in figure 1.\nThe initial overview of the multi-label classification methods is presented in [16], which classifies the existing methods then into two categories. Algorithm Adaptation (AA) methods and Problem Transformation (PT) methods. In recent years, more multi-label classifiers have been developed and the most recent overview of multi-label classifiers introduces the third category of Ensemble (EN) methods [14]."}, {"heading": "A. Algorithm Adaptation Methods", "text": "The AA methods are those that can adapt, extend and customize an existing machine learning algorithm to meet the needs of solving multi-label problems [14]. Hence the AA methods can be subcategorized based on which of the existing algorithm, the multi-label variant is developed. Multi-label variants are developed based on Boosting, kNN, Decision Trees, Neural Networks, and SVM [14 and references within]."}, {"heading": "B. Problem Transformation Methods", "text": "The PT methods employ a unique transformation that converts the multi-label problem into one or more single-label problems. Some of the early PT methods use simple transformation techniques like instance elimination, label elimination, label decomposition etc. More advanced transformation methods like copy transformation, dubbed copy transformation, Label powerset, pruned problem transformation methods are developed subsequently. More popular and novel transformation methods such as Binary relevance (BR) method and Classifier Chain (CC) method are developed in recent\nyears. These methods decompose the multi-label classification into series of single-label classification problem with each single label problem focusing on one label of the multi-label case. The Label powerset methods like HOMER combines the multiple labels and creates new labels thus making it into single-label problem. Pair-wise methods use multiple classifiers that cover all possible label pairs. To combine the output of the classifiers either voting based method is used as in CLR or Qweighted approach is used as in QWML."}, {"heading": "C. Ensemble Methods", "text": "The EN method based multi-label classifiers use an ensemble of AA or PT classifiers to address the multi-label classification problem. Some of the widely known ensemble methods are Random k labelsets (RAkEL), Ensemble of Classifier Chains (ECC), Random forest based predictive clustering trees (RF-PCT), Random Decision Tree (RDT) etc. Based on the machine learning algorithm used, the multi-label methods have been grouped as shown in figure 2 [14].\nIt can be seen from the brief review that ELM based techniques are thus far not used to implement the multi-label classification problem. This paper proposes an AA method multi-label classifier based on ELM.\nIII. PROPOSED ALGORITHM\nThis section provides a brief review of the ELM technique and the proposed algorithm used for multi-label classification.\nConsider there are N training samples represented as {(xi, yi)} where i varies from 1 to N, xi denotes the input data vector and yi denotes the output. Let there be N\u2019 number of hidden neurons in the network, the output of the basic batch learning ELM technique can be given as in (1).\n    mi n i\nn N\ni iiiN RRwRxxbwGxf  \n ,,,,, ' 1 ' (1)\nFrom the theory of ELM it is evident that the input weights (wi) and the hidden layer neuron bias (bi) can be randomly assigned. Thus, the output weight is determined as \u03b2 = H+Y where H+ is the Moore-Penrose generalized inverse of hidden layer output matrix and Y = [y1,..,yN]T.\nA. Initialization of Parameters\nBasic parameters such as the type of activation function and number of hidden layer neurons in the ELM are initialized."}, {"heading": "B. Processing of Inputs", "text": "The multi-label output corresponding to the input sequence is in general provided as \u20180\u2019 and \u20181\u2019 for each label, with \u20181\u2019 corresponding to the labels to which the input samples belong\nto. For multi-label classification, each of the input samples may belong to one or more than one label. These inputs are converted from unipolar to bipolar representation."}, {"heading": "C. ELM Training", "text": "The input weights (wi) and the bias of the hidden layer neurons (bi) are randomly assigned. The basic batch learning equation as given in (1) can be compactly written as, H\u03b2 = Y, where H is an N x N\u2019 matrix, \u03b2 is N\u2019 * L matrix and Y is N x L output matrix, where each row gives the output of corresponding input sample and each column corresponds to each of the labels. In the training phase, the output weight, \u03b2 is evaluated from the training input and output data as, \u03b2 = H+Y, where H+ is the Moore-Penrose generalized inverse of hidden layer output matrix and Y is the bipolar N x L matrix."}, {"heading": "D. ELM Testing", "text": "In the testing phase, the test output is evaluated by using the \u03b2 values found during the training phase in the equation Y = H\u03b2. In single label classification, the class label to which the input sample belong to can be identified by determining the column which has the maximum value in Y. The label corresponding to the column with maximum value is identified as the classifier output. In contrast, in multi-label classification problems, the input samples may belong to one or more labels and hence it cannot be directly identified by identifying the column with maximum value. For multi-label classification, The N x L values of Y are passed as arguments to the bipolar step function. A threshold of \u20180\u2019 is applied to the resultant values. The set of columns of the resulting matrix with values 1 gives the multi-label belongingness of the corresponding input.\nIV. MULTI-LABEL CLASSIFICATION METRICS"}, {"heading": "A. Dataset Metrics", "text": "The degree of multi-label nature of each data set varies. Some data sets may have large part of the input data set to be multi-label in nature whereas some other data sets may have only a few multi-label samples. The degree of multi-label nature can be quantified using two metrics: Label cardinality (Lc) and Label density (Ld). Let the dataset be given as {xi, Yi}, i = 1\u2026N with L number of labels. Then, Label Cardinality is defined as the average number of labels of the input samples in the dataset. Label density is given as the average number of labels of the input samples divided by number of labels.\n \n N\ni\niY N ycardinalitLabel 1\n1 (2)\n \n N\ni\ni\nL\nY\nN densityLabel\n1\n1 (3)\nLabel cardinality signifies the average number of labels present in the training data set. Label cardinality is independent of the number of labels present in the dataset. Label density takes into consideration the number of labels present in the dataset. Two datasets having same label cardinality, but different label density can differ largely in their properties and may cause different behavior to the training algorithm [17]."}, {"heading": "B. Evaluation Metrics", "text": "Multi class classification is unique in its nature from single label classification in which the classification can be partially\ncorrect. In single label classification problems, the classification can only be either correct or wrong. Partial correctness are not observed in single label classification. Whereas in multi-label classification problems, the classifier may classify at least one of the classes correctly and one or more of the classes in a wrong manner thus resulting in the partial correctness of the classification. Thus the traditional performance evaluation metrics used for single label classification cannot be used to evaluate the performance of the multi-label classification case. Multi-label classification requires a different set of performance metrics to evaluate the effectiveness and efficiency of the training method. The performance metrics are proposed in the literature used to validate the multi-label learning. They are Hamming Loss, Accuracy, Precision, Recall and F1-measure. There are few other evaluation measures like one-error, coverage etc. which are used for ranking based multi-label classification methods.\nV. EXPERIMENTATION\nIn this section, we present the experimental design used to evaluate the proposed method and compare its performance with other existing methods.\nThe proposed method is tested for its performance using six different standard benchmark datasets. The datasets are chosen from different domains such as Multimedia, Text and Biology. A diverse nature of datasets are chosen so as to verify for the consistency, robustness and reliability of the proposed method for generic environment. Certain existing techniques performs well in a specific datasets but their performance reduces significantly when introduced with different datasets. Hence consistent performance of the method is of critical importance when to be used real-time, real world applications. The number of labels varies from as low as 6 labels to as high as 374 labels. Datasets with number of attributes varying from 72 to as high as 1449 attributes are tested and verified.\nMulti-label datasets has another unique nature that each of the datasets are not equally multi-labelled. The multi-label nature of the datasets varies. The degree of multi-labelness is quantified by the use of metrics label cardinality and label density. Label cardinality gives the average number of labels of the input samples in the dataset. For single-label classification, the label cardinality is always 1. But for multi-label problems, the input samples can have one or more than one label associated with them. Hence the label cardinality for multilabel problems is always greater than 1. The datasets used for experimental validation of the proposed method has its label cardinality varying from 1.07 to 4.24. Label cardinality of 4.24 signifies that each sample has an average of more than 4 labels associated with it. Label density takes into account the number of labels along with the average number of labels of the input samples. The lower the label density the lesser the number of occurrence of the label in the dataset. Lower label density indicates that there are only fewer samples corresponding to each label and hence the learning method needs to learn the label within those limited samples. Thus the label density of 0.009 indicates that the average percentage of occurrence of each label in dataset is only 0.9%. The datasets are taken from the KEEL multi-label dataset repository. The specifications of the datasets used for validation is given in table 1.\nThe proposed method is validated with the datasets given in table. The results achieved by the proposed method is compared with the state of the arts techniques. State of the arts techniques from different categories of multi-label classification such as AA methods, PT methods, EN methods are used for result comparison. Three methods from each of the PT, AA and EN methods are used for result comparison. The methods from different machine learning techniques like SVM, Decision Trees and Nearest Neighbors are used for comparison. The details of the state of the art methods used for result comparison with proposed method is given in table 2. The proposed method is verified with 9 state of the art methods and 6 benchmark datasets. Hamming Loss, Accuracy, Precision, Recall and F-measure are used as evaluation metrics to compare the proposed method with existing algorithms.\nVI. RESULTS AND DISCUSSION\nThe five evaluation metrics are evaluated for each of the datasets using the proposed method. The results obtained are discussed in the order of the evaluation metrics.\nConsider the dataset be given as {xi, Yi}, i = 1\u2026N with L number of labels. Let MLC be the training method and Zi = MLC(xi) be the output labels predicted by the classification method. The expression to identify the evaluation metrics are given in equations.\n \n N\ni\nii YxMLC LN LossgHam 1\n)( 11 _min (4)\n \n  \n\n  \n\n\n \nN\ni ii\nii\nYxMLC\nYxMLC\nN Accuracy\n1 )(\n)(1 (5)\n \n  \n\n  \n   N\ni i\nii\nxMLC\nYxMLC\nN ecision\n1 )(\n)(1 Pr (6)\n \n  \n\n  \n   N\ni i\nii\nY\nYxMLC\nN call\n1\n)(1 Re (7)\n \n  \n\n  \n\n\n \nN\ni ii\nii\nYxMLC\nYxMLC\nN measureF\n1 )(\n)(*21 1 (8)"}, {"heading": "A. Hamming Loss", "text": "Hamming loss gives the percentage of wrong labels to the total number of labels. Lower the hamming loss, better is the performance of the method used. For an ideal classifier, hamming loss is 0. The hamming loss can be calculated by (4)."}, {"heading": "B. Accuracy", "text": "Accuracy of the multi-label classifier is defined as the proportion of the predicted correct labels to the total number of labels for that instance. Overall accuracy is the average across all instances. The Accuracy can be evaluated using (5)."}, {"heading": "C. Precision", "text": "Precision is the proportion of the predicted correct labels to the total number of actual labels averaged over all instances. In other words, it is the ratio of true positives to the sum of true positives and false positives averaged over all instances. The expression for precision is given in (6)."}, {"heading": "D. Recall", "text": "Recall is the proportion of the predicted correct labels to the total number of predicted labels averaged over all instances. In other words, it is the ratio of true positives to the sum of true positives and false negatives averaged over all instances. The expression for recall is given in (7)."}, {"heading": "E. F1-measure", "text": "F1 measure is given by the harmonic mean of Precision and Recall. The expression to evaluate F1 measure is given in (8).\nHigher the values of accuracy, precision, recall and F1measure, better the performance of the proposed method. And lower the hamming loss corresponds to better accuracy of the proposed method. Comparison of the hamming loss, accuracy, precision, recall and F1-measure obtained by the proposed method and other existing methods is shown in tables 3-7. The results of the state of the art methods are obtained from [14]. The values of the evaluation metrics which are equal to or greater than the value obtained by the proposed method is highlighted in blue. It is evident from the table that the proposed method is consistently better than most of the existing methods in all the five evaluation metrics. The performance of the methods for the datasets used are also shown in figures 3-7.\nFrom figure 3 we can see that, the proposed ELM based method lower hamming loss than the existing methods and is ranked first in yeast, corel5k and medical datasets and is almost as nearly as the first in Enron dataset. Though it is not the first ranked in hamming loss performance for emotion and scene recognition, it still is one of the top classifiers when compared to other methods. Also, it can be seen from the figure that, no existing algorithm is consistent in their performance throughout all the datasets. Also from the figures 3 to 7, it is clear that the proposed method consistently gives better performance than the existing methods throughout all the datasets across all the evaluation metrics observed. Some of the methods outperform the proposed method in one or two datasets, but loses its performance for other datasets.\nTABLE VI. COMPARISON OF RECALL\nDataset CC QWML HOMER ML-C4.5 PCT ML-kNN ECC RFML-C4.5 RF-PCT ELM Emotion 0.397 0.429 0.775 0.703 0.534 0.377 0.533 0.545 0.582 0.491\nYeast 0.600 0.600 0.714 0.608 0.490 0.549 0.673 0.491 0.523 0.608 Scene 0.726 0.709 0.744 0.582 0.539 0.655 0.771 0.388 0.541 0.709 Corel5k 0.056 0.264 0.250 0.002 0.000 0.014 0.001 0.005 0.009 0.043\nEnron 0.507 0.453 0.610 0.487 0.229 0.358 0.560 0.398 0.452 0.508\nMedical 0.754 0.801 0.760 0.740 0.227 0.547 0.642 0.251 0.599 0.744\nTABLE VII. COMPARISON OF F1-MEASURE\nDataset CC QWML HOMER ML-C4.5 PCT ML-kNN ECC RFML-C4.5 RF-PCT ELM Emotion 0.461 0.481 0.614 0.651 0.554 0.431 0.556 0.583 0.611 0.518\nYeast 0.657 0.654 0.687 0.614 0.578 0.628 0.670 0.589 0.614 0.658 Scene 0.742 0.710 0.745 0.587 0.551 0.658 0.771 0.395 0.553 0.697 Corel5k 0.048 0.292 0.280 0.003 0.000 0.021 0.001 0.008 0.014 0.033\nEnron 0.484 0.525 0.613 0.546 0.295 0.445 0.602 0.505 0.552 0.577\nMedical 0.337 0.745 0.761 0.768 0.253 0.560 0.652 0.267 0.616 0.759\nFigure 4. Accuracy comparison chart\nThe proposed method also exploits the high speed learning which is inherent to the extreme learning machine. The learning speed of ELM is several folds greater than the traditional neural networks. Hence the proposed algorithm can be trained with high speed. Thus the proposed ELM based method will be a better solution for multi-label problems.\nVII. CONCLUSION\nIn this paper, an Extreme Learning Machine based learning technique for Multi-label classification is developed. It is to be noted that ELM based multi-label classification has never been implemented in the literature thus far. In Multi-label classification each of the input samples may belong to one or more than one of the labels. The proposed ELM based method\nis evaluated using different datasets. The results obtained is compared with several existing state of the art methods. The results show that the proposed method performs effectively than the existing method in most cases and in all the evaluations metrics. Thus the ELM based Multi-label classifier can be a better alternative to solve a wide range of multi-label classification problems from various domains."}, {"heading": "ACKNOWLEDGMENT", "text": "The first author would like to thank Nanyang Technological University, Singapore for the NTU Research Student Scholarship."}], "references": [{"title": "A Preliminary approach to the multi-label classification problem of Portuguese juridical documents", "author": ["T Gonclaves", "P Quaresma"], "venue": "EPIA 2003, LNCS (LNAI),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T Joachims"], "venue": "Nedellec C, Rouveirol C (Ed.), ECML, LNCS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Evaluation of Two Systems on Multiclass Multi-label document classification", "author": ["X Luo", "A.N. Zincir Heywood"], "venue": "ISMIS 2005, LNCS (LNAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Experiments with multi-label text classifier on the Reuters collection", "author": ["D Tikk", "G Biro"], "venue": "Proc. Of the International Conference on Computational Cybernetics (ICCC 2003),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Multi-label informed latent semantic indexing", "author": ["K Yu", "S Yu", "V Tresp"], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and Development in information retrieval,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Significance level based multiple tree classification", "author": ["A Karalic", "V Pirnat"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1991}, {"title": "A kernel method for multi-labelled classification", "author": ["A Elisseeff", "J Weston"], "venue": "Neural Information Processing Systems, NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "A k-nearest neighbour based algorithm for multi-label classification", "author": ["M.L. Zhang", "Z.H. Zhou"], "venue": "Proceedings of the 1st IEEE international Conference on Granular Computing, Beijing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Multi-label semantic scene classification", "author": ["M Boutell", "X Shen", "J Luo", "C Brouwn"], "venue": "Technical Report,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Multi-label machine learning and its application to semantic scene classification. Storage and Retrieval Methods and Applications for Multimedia", "author": ["X Shen", "M Boutell", "J Luo", "C Brown"], "venue": "Yeung MM, Lienhart RW, Li CS (Ed.), Proceedings of the SPIE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Efficient Approximation Algorithms for Multi-label Map Labelling", "author": ["B. Zhu", "C.K. Poon"], "venue": "Aggarwal AK, Pandu Rangan C (Ed.) ISSAC", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Kernel methods for multi-labelled classification and categorical regression problems", "author": ["A Elisseeff", "J Weston"], "venue": "Technical Report, BIOwulf Technologies,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "An extensive experimental comparison of methods for multi-label learning", "author": ["Gjorgji Madjarov", "Dragi Kocev", "Dejan Gjorgjevikj", "Saso Dzeroski"], "venue": "Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "A Tutorial on Multi-label Classification Techniques", "author": ["Andre CPLF de Carvalho", "Alex A Freitas"], "venue": "Foundations of Computational Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Multi-label Classification: An Overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "International Journal of Data Warehousing and Mining,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "ML-kNN: A lazy learning approach to multi-label learning", "author": ["Min L. Zhang", "Zhi H. Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 100, "endOffset": 105}, {"referenceID": 1, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 100, "endOffset": 105}, {"referenceID": 2, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 100, "endOffset": 105}, {"referenceID": 3, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 100, "endOffset": 105}, {"referenceID": 4, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 100, "endOffset": 105}, {"referenceID": 5, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.", "startOffset": 97, "endOffset": 102}, {"referenceID": 7, "context": "The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.", "startOffset": 97, "endOffset": 102}, {"referenceID": 8, "context": "The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.", "startOffset": 125, "endOffset": 132}, {"referenceID": 9, "context": "The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.", "startOffset": 125, "endOffset": 132}, {"referenceID": 10, "context": "The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.", "startOffset": 148, "endOffset": 152}, {"referenceID": 11, "context": "Thus, binary classification, multi-class classification and ordinal regression problems can be seen as special cases of multi-label problems where the number of labels assigned to each instance is equal to 1 [13].", "startOffset": 208, "endOffset": 212}, {"referenceID": 12, "context": "in their paper [14] categorizes these techniques into three major categories.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "[14] and from the comparison of results it can be seen that there exists no single method that performs uniformly well on a wide range of datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15], the term \u201cclassification\u201d can be formally defined as, \u201cGiven a set of training examples composed of pairs {xi, yi}, find a function f(x) that maps each attribute vector xi to its associated class yi, i = 1,2,3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "\u201d Single label classification involves associating a single label \u2018l\u2019 from a set of disjoint labels \u2018L\u2019 to each of the input data sequence [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "But the generality of multi-label classification makes it more difficult to be implemented and trained than the others [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "The paper [14] discuss in detail the state of the arts multi-label classification methods and categorizes the existing methods into three groups.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "Adapted from [14], the overview of existing methods can be summarized as shown in figure 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "The initial overview of the multi-label classification methods is presented in [16], which classifies the existing methods then into two categories.", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "In recent years, more multi-label classifiers have been developed and the most recent overview of multi-label classifiers introduces the third category of Ensemble (EN) methods [14].", "startOffset": 177, "endOffset": 181}, {"referenceID": 12, "context": "The AA methods are those that can adapt, extend and customize an existing machine learning algorithm to meet the needs of solving multi-label problems [14].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "Based on the machine learning algorithm used, the multi-label methods have been grouped as shown in figure 2 [14].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "Two datasets having same label cardinality, but different label density can differ largely in their properties and may cause different behavior to the training algorithm [17].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "The results of the state of the art methods are obtained from [14].", "startOffset": 62, "endOffset": 66}], "year": 2014, "abstractText": "In this paper, an Extreme Learning Machine (ELM) based technique for Multi-label classification problems is proposed and discussed. In multi-label classification, each of the input data samples belongs to one or more than one class labels. The traditional binary and multi-class classification problems are the subset of the multi-label problem with the number of labels corresponding to each sample limited to one. The proposed ELM based multi-label classification technique is evaluated with six different benchmark multi-label datasets from different domains such as multimedia, text and biology. A detailed comparison of the results is made by comparing the proposed method with the results from nine state of the arts techniques for five different evaluation metrics. The nine methods are chosen from different categories of multi-label methods. The comparative results shows that the proposed Extreme Learning Machine based multi-label classification technique is a better alternative than the existing state of the art methods for multi-label problems. Keywords\u2014Machine Learning, Extreme Learning Machines, Multi-label Learning, Classification.", "creator": "Microsoft\u00ae Word 2013"}}}