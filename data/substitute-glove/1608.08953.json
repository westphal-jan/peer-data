{"id": "1608.08953", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Dynamic Allocation of Crowd Contributions for Sentiment Analysis during the 2016 U.S. Presidential Election", "abstract": "Opinions those brought 2016 U. S. Presidential Candidates other well regarding years huge than e-mailed if either promising already ascertain easier. Crowdsourcing still calculations both issues 070 effectively is also better, thus to large brescia - garis conflict when pathos is involved. Each vlog its typically analyzed first turned level number the forcing and overwhelmingly balloting. We day measure a crowdsourcing approach clearly instead form a linear allocation an then far however dozen. We important two dynamic - allocation methods: (1) The example known poor beleived should decca which faxed present magnification download other on the rate taking of discerning taken discontent of that although tweet. (2) The numbers 's mobbed workers it determined magazine, during an iterative crowd sourcing solve, included on parma - rater licensing since produce. We applied our way even third, 5,000 downloads sent about only followed U. S. contest candidates Clinton, Cruz, Sanders, including Trump, containing came February ioc. We provisions the two reform methods using immediately trees because allocate alone crowd resolve to diksha predicted to both ribald. We come to our fully outperforming of traditional element allocation plan. It contributions judgment labels back over crowd at but quite lower tax while maintaining amended indicate.", "histories": [["v1", "Wed, 31 Aug 2016 17:20:09 GMT  (406kb,D)", "http://arxiv.org/abs/1608.08953v1", "10 pages, 3 figures"], ["v2", "Thu, 9 Feb 2017 18:05:46 GMT  (250kb,D)", "http://arxiv.org/abs/1608.08953v2", "10 pages, 3 figures"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.HC cs.CL cs.SI", "authors": ["mehrnoosh sameki", "mattia gentil", "kate k mays", "lei guo", "margrit betke"], "accepted": false, "id": "1608.08953"}, "pdf": {"name": "1608.08953.pdf", "metadata": {"source": "CRF", "title": "Dynamic Allocation of Crowd Contributions for Sentiment Analysis during the 2016 U.S. Presidential Election", "authors": ["Mehrnoosh Sameki", "Mattia Gentil", "Kate K. Mays", "Lei Guo", "Margrit Betke"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "During the 2016 U.S. presidential primary election season, the political debate on Twitter about the four presidential candidates Hillary Clinton, Ted Cruz, Bernie Sanders, and Donald Trump was particularly lively and created a huge corpus of data. It has been argued that Twitter can be considered a valid indicator of political opinion (Tumasjan et al. 2010), and so various parties, including journalists, campaign managers, politicians, and social scientists, are interested in using automated natural language processing tools to mine this corpus.\nUnsupervised learning methods have been used previously to analyze a similar corpus, 77 millions tweets about the 2012 U.S. presidential election and create summary statistics such as \u201ctwitter users mentioned foreign affairs in connection with Obama more than with\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nRomney\u201d (Guo et al. 2016). Supervised learning methods also have been used, for example, to analyze filtered \u201csnippets\u201d of political blogs (Hsueh, Melville, and Sindhwani 2009). However, creating accurate learning methods to analyze positive or negative sentiments is challenging. Political opinions expressed on the internet often contain sarcasm and mockery (Guo et al. 2016; Hsueh, Melville, and Sindhwani 2009), which are difficult to discern by machine or human computation (Gonza\u0301lez-Iba\u0301n\u0303ez, Muresan, and Wacholder 2011; Young and Soroka 2012)\nCrowdsourcing has been proposed to collect training data for predictive models used to classify political sentiments (Hsueh, Melville, and Sindhwani 2009; Wang et al. 2012). Out of concern for the accuracy of human annotation, it is standard practice to collect multiple labels for the same data point and then use the label that obtained a majority vote (Karger, Oh, and Shah 2013). Typically an odd number of crowd workers, e.g., five or seven, is chosen to create this redundancy. Redundancy, however, cannot guarantee reliability, i.e., agreement among the raters with each other about the sentiment present in the text in question. For example, when five crowd workers analyzed the sentiments expressed in the political snippets dataset (Hsueh, Melville, and Sindhwani 2009), only a 47% agreement rate on the three labels \u201cpositive,\u201d \u201cnegative,\u201d or \u201cneutral sentiment\u201d could be achieved.\nHsueh et al., 2009, noted that \u201cnot all snippets [of political blogs] are equally easy to annotate.\u201d We made the same observation for our data \u2013 sarcastic twitter messages are more difficult to label, and we propose to allocate crowd resources according to the predicted difficulty level: The more difficult the sentiment analysis may be, the higher the number of workers becomes that our model assigns. In allocating fewer crowd workers to tasks that are predicted to be easy, we aim to balance the goals of labeling accuracy and efficiency.\nThe literature describes techniques for optimal tradeoffs between accuracy and redundancy in crowdsourcing (Karger, Oh, and Shah 2013; Tran-Thanh et al. 2013). In these works, the proposed crowdsourcing mechanism uses a fixed number of crowd workers per task, and the assignment is agnostic about the latent dif-\nar X\niv :1\n60 8.\n08 95\n3v 1\n[ cs\n.H C\n] 3\n1 A\nug 2\nficulty level of each task. If the difficulty of a task can be discerned, easy tasks could be routed to novice workers and difficult tasks to expert annotators (Kolobov, Mausam, and Weld 2013). Optimal task routing, however, is an NP-hard problem, and so online schemes for task-to-worker assignments have been proposed (Bragg et al. 2014; Rajpal, Goel, and Mausam 2015). Our work falls into this category of online crowdsourcing methodology. Our contributions are as follows:\n\u2022 We propose a decision-tree approach for dynamically determining the number of crowd workers for tasks that require redundant annotations.\n\u2022 We provide two versions of this approach: The offline version computes the number of workers needed based on the content of the data they are asked to analyze. The online version relies on iterative rounds of crowdsourcing and determines the number based on content and annotation results in previous rounds.\n\u2022 To illustrate and evaluate our approach, we conducted a crowdsourcing experiment with a dataset of 1,000 tweets that were sent during the 2016 primary election season. We collected 5,075 ratings of the sentiment towards presidential candidates Clinton, Cruz, Sanders, and Trump in these tweets and evaluated their accuracy with respect to a gold standard established by experts in political communication.\n\u2022 Comparisons with traditional crowdsourcing strategies show that the proposed offline and online selection methods intelligently detect ambiguities in sentiment analysis and recruit more workers to resolve those. We show that a large portion of the crowdsourcing budget can be saved at a small loss of accuracy."}, {"heading": "2 Method", "text": "We here describe our method to solve the problem of dynamically assigning crowd workers to analyze the sentiment of political tweets. Our approach consists of three main components. First, we designed a method to detect sarcasm in tweets (Section 2.1). This first step was important because sarcasm is one of the most confusing and misleading language features to classify even for a human annotator, especially when a single out-ofcontext tweet is being analyzed.\nWe then constructed a decision tree that assigns to each tweet a fixed number of crowd workers based on the presidential candidates mentioned in the tweet and other text properties, in particular, its sarcasm (Section 2.2). In designing such a tree, we were motivated by the following insight: For tweets which are expected to be clear and straight-forward to analyze, fewer annotators would be required than for tweets that are sarcastic and complicated. To build the tree, we estimated how troublesome it would be for a crowd worker to correctly understand what kind of sentiment is being expressed towards the candidates.\nThe third component of our approach moves from an offline to an online consideration of how many crowd workers to involve in the labeling process (Section 2.3). Based on the inter-rater agreements between labels obtained in a first phase of an iterative crowd sourcing process, for tweets which proved to be challenging to annotate, our method determines how many additional labels to acquire in one or more subsequent crowd sourcing phases.\nOur final methodological contribution is a description of the equivalency between two crowdsourcing schemes, the traditional 5-worker-per-task scheme and the dynamic scheme that assigns 3 workers per task in the first round and 2 additional workers in a second round if disagreement is encountered in the 1st round. This is a general result about offline versus online crowdsourcing schemes. It holds for any application and is therefore presented in Section 2.4, separate from the results of our sentiment analysis of political tweets."}, {"heading": "2.1 Sarcasm Detection", "text": "Our first step was trying to predict whether a given tweet was sarcastic or not. We used a Bayesian approach to estimate the likelihood of sarcasm based on training data provided by domain experts. Our training data contains the label \u201csarcasm present\u201d or \u201csarcasm not present\u201d for 800 tweets about the four presidential candidates Clinton, Cruz, Sanders, and Trump.\nWe looked for general features that are usually clues for the presence of sarcasm in a sentence (Gonza\u0301lezIba\u0301n\u0303ez, Muresan, and Wacholder 2011; Davidov, Tsur, and Rappoport 2010) and grouped them into 7 categories:\n1. Quotes: People often copy a candidate\u2019s words to make fun of them.\n2. Question marks, exclamation or suspension points.\n3. All capital letters: Tweeters sometimes highlight sarcasm by writing words or whole sentences with allcapital letters.\n4. Emoticons like \u2019:)\u2019, \u2019:(\u2019\n5. Words expressing a laugh, or other texting lingo, such as \u2019ahah,\u2019 \u2019lol,\u2019 \u2019rofl,\u2019 \u2019OMG,\u2019 \u2019eww,\u2019 etc.\n6. The words \u2019yet\u2019 and \u2019sudden.\u2019\n7. Comparisons: Many tweeters use comparisons to make fun of a candidate, using words such as \u2019like\u2019 and \u2019would\u2019.\nThe sarcasm detecting algorithm that we designed scans the tweet text for those features and returns the list of sarcastic clues. The clues are represented by a 7- component feature vector f that contains a Boolean value for each of the categories listed above \u2013 \u201c1\u201d indicates \u201cpresence\u201d of the feature, \u201c0\u201d otherwise.\nGiven a tweet t and its feature vector f , our method computes the probability that the tweet t contains sar-\ncasm by using Bayes rule:\nP (t is sarcastic|fn) = (1) P (fn| t is sarcastic) P (t is sarcastic)\nP (fn) = (2)\n# of sarcastic tweets with fn # of tweets with feature fn . (3)\nTo weigh the presence of the n-th feature in sarcastic tweets appropriately, our method computes a weight vector w by normalizing its n-th component by the probability that it is sarcastic, given any of the seven features is present:\nwn = P (t is sarcastic|fn)\u22117\nn=1 P (t is sarcastic| fn) . (4)\nOur sarcasm score for each tweet is then defined to be the dot product wTf (5) of the weight and feature vectors."}, {"heading": "2.2 Decision Tree", "text": "The decision tree we designed maps a tweet to a number of crowd workers that will be asked to label the tweet. To gain insight into the properties of a tweet that could cause a crowd worker to struggle in sentiment classification and warrant additional crowd work, we obtained gold standard data and conducted a formative crowdsourcing study.\nExpert Labels We used 1,000 tweets about the four presidential candidates Clinton, Cruz, Sanders, and Trump. For these tweets, we had gold standard labels about two categories, provided by experts in political communication. The first category was whether each of the four candidates was mentioned in the tweet. The second category described whether the tweet was in general \u201cpositive,\u201d \u201cneutral\u201d or \u201cnegative\u201d about each candidate mentioned in the tweet. If more than one candidate was mentioned in a tweet, the sentiment towards each candidate was labeled.\nFormative Crowdsouring Experiment We asked 5 crowd workers to analyze each tweet, calling our experiment the \u201cTrad 5 baseline\u201d (the details on the crowdsourcing methodology are given in Section 3). We asked the workers who among the four candidates Sanders, Trump, Clinton and Cruz was mentioned and to indicate the attitude that the tweeter expressed towards them on a three-point scale \u201cpositive,\u201d \u201cneutral,\u201d or \u201cnegative.\u201d\nDecision Tree Design We designed our decision tree (see Fig. 1) based on the properties we observed that influence the accuracy with which a worker interprets the sentiment of the tweet. The first branching of the tree accounts for whether one or more candidates are mentioned in the tweet text, the most relevant factor in its sentiment analysis. Tweets in which several candidates are mentioned are more difficult to classify because annotators can become confused by the different attitudes\nthat the writer expresses towards each of the candidates or by the presence of comparisons between them. We here provide three examples: Tweet 1\n@BecketAdams @JPTruss @GayPatriot except Cruz now realises Trump\u2019s power and is debating him. Rubio is still hiding from Trump on stage\nis \u201cpositive\u201d towards Trump and \u201cneutral\u201d towards Cruz, according to expert opinion. Four crowd workers agreed that the message was \u201cneutral\u201d towards both candidates, and one labeled it \u201cpositive\u201d towards Trump and \u201cneutral\u201d towards Cruz. Tweet 2\nBernie\u2019s Super PAC Hypocrisy: Twice as Much Outside Money Spent Supporting Sanders as Promoting Clinton https://t.co/RVAi7X4shS\nis \u201cpositive\u201d towards Clinton and \u201cnegative\u201d towards Sanders, according to expert opinion. All five crowd workers agreed but not on the correct labels \u2013 they selected a negative sentiment towards Sanders and a neutral for Clinton. Tweet 3\nHas Trump mentioned that he doesn\u2019t think Cruz is eligible to be President recently? That seemed like a go-to for him\nmisled annotators because both sarcasm is present and two candidates are mentioned. As a consequence, only 3 workers out of 5 agreed on a negative overall feeling towards both candidates.\nIt is important whether Clinton or Trump was mentioned in the Tweet. Opinions towards these candidates are usually more challenging to understand as tweeters have very disparate and unclear attitudes towards them.\nThe next layer of the decision tree accounts for the length of the tweet and the presence of a link. We consider a tweet short if it contains fewer than 10 proper words. Tweets that contain a webpage address are not always fully understandable by themselves as they refer to the content of the link or they are a response to another tweet, and therefore their context is not always clear.\nFinally, the terminating decision layer in the tree is based on the sarcastic score that was produced by the sarcasm predictor. The decision tree uses the sarcasm score as defined in Eq. 5 to determine the likelihood of sarcasm in the particular tweet.\nWe assigned a fixed number of crowd workers to each leaf of the tree, which specifies the number of annotations needed for a particular tweet. In this first model we grouped the tweets into 4 categories (very easy, easy, medium and hard) and assigned 2, 3, 5, or 7 workers to them respectively. We call the model \u201cStatic Decision Tree\u201d (SDT) due the fact that the number of crowd workers depends only on the content analysis of the tweet (and not dynamically on the workers\u2019 labels, as described below). With this tree, the number of crowd workers to be queried for each tweet can be computed\noffline \u2013 in advance of any crowdsourcing experiment (i.e., the numbers shown in Fig. 1 with a green-shaded background)."}, {"heading": "2.3 Dynamic Worker Assignment", "text": "We here propose an online scheme for determining the number of crowd workers to be queried for each tweet. This approach cannot be computed in advance to the crowdsourcing experiment but is an iterative method that relies on the results of the crowd work.\nOur idea is to request a low number of workers to provide the sentiment analysis of each tweet in a first round of crowdsourcing, and then perform one or more rounds of crowdsourcing for the tweets for which workers disagreed. In this way, the difficulty of the tweet is observed directly as a measure of disagreement in the first round of crowdsourcing, and we do not risk wasting effort on tweets that are trivial to classify. To evaluate our approach, we designed two instantiations of our idea involving two rounds of crowdsourcing:\nDynamic Decision Tree 1 (DDT1) The first dynamic tree assigns 2 workers to the \u2019very easy\u2019 and \u2019easy\u2019 difficulty classes, 3 for \u2019medium\u2019 and 5 for \u2019hard.\u2019 If the 2 workers disagree on classifying a \u2019very easy\u2019 or \u2019easy\u2019 tweet, we conduct a second round of crowdsourcing on that tweet so that we can get a majority vote. If some annotators disagree for a \u2019medium\u2019-class tweet, 2 more workers are involved. The number of workers for \u2019hard\u2019 tweets stays fixed.\nDynamic Decision Tree 1 (DDT2) Finally, we pushed the dynamic assignment design even further and set up a tree that starts with a very low numbers of annotators in order to minimize the number of crowdsourced tasks. This tree initially assigns 2 workers to the \u2019very easy\u2019 and \u2019easy\u2019 classes and requires 3 more annotators if the initial workers disagree. The tweets in the \u2019medium\u2019 and \u2019hard\u2019 categories were first only analyzed by 3 workers, and this number is increased by 2 workers if at least one disagreement is observed."}, {"heading": "2.4 Equivalency of Traditional Static versus Proposed Dynamic Worker Allocation", "text": "Past work showed that the probability p that a crowd workerw correctly performs a task t according to a gold standard label can be described as a function p(t, w) of the task difficulty and the worker skill (Ho and Vaughan 2012). For simplicity of our analysis, we omit the dependence on the worker.\nFor a generic task, we can compute the probability PM that the gold standard is successfully obtained by majority voting for a set of crowd sourcing baseline schemes as a function of p. For example, the probability PM that the traditional 3-worker-per-task crowdsourcing scheme yields the correct results is the probability that at least 2 out of 3 performed the task correctly,\nwhich is\nPM = 3\u2211 i=2 P (iworkers are correct) =\n3\u2211 i=2 ( 3 i ) pi(1\u2212 p)(3\u2212i) = p2[3(1\u2212 p) + p]. (6)\nSimilarly, with the traditional 5-worker-per-task crowdsourcing scheme, we attain PM = 5\u2211 i=3 P (iworkers are correct)= 5\u2211 i=3 ( 5 i ) pi(1\u2212p)(5\u2212i)\n= p3[10(1\u2212 p)2 + 5p(1\u2212 p) + p2]. (7)\nNext we simulate the dynamic assignment of workers with 3 initial workers, where 2 more workers are involved if disagreement is encountered. The probability that this model produces the correct result by majority voting is the sum of three probabilities: (1) the probability that the three initial workers agree on the correct result, (2) the probability that one initial worker performs the task incorrectly and at least one new worker correctly, and (3) the probability that only one initial worker performs the task correctly and both the new workers follow up correctly:(\n3\n3\n) p3 + [( 3\n2\n) p2(1\u2212 p) ] (1\u2212 (1\u2212 p)2)\n+\n[( 3\n1\n) p(1\u2212 p)2 ] p2 =\np3[1 + 3(1\u2212 p)(2\u2212 p) + 3(1\u2212 p)2)] = p3[10(1\u2212 p)2 + 5p(1\u2212 p) + p2]. (8)\nThe derivations in Eqs. 7 and 8 result in the same formula. We can therefore infer that a dynamic 3(+2) allocation method for workers achieves the same prediction accuracy as the traditional 5-worker crowdsourcing scheme. As we will describe in more details below, by running such a model on all tweets in our dataset we were able to obtain optimal results from crowdsourcing with only 4,058 tasks. This result is impressive because it proves that we can reach exactly the same accuracy level and save 18.84% of our budget only by running two \u201csmart\u201d rounds of crowdsourcing."}, {"heading": "3 Experimental Methodology", "text": "Our data consists of 1,000 tweets about the four presidential candidates Clinton, Cruz, Sanders, and Trump sent during the primary election season in February 2016. We selected these candidates because they were the two leading candidates in the polls at the time of data collection from each major U.S. political party (Republican and Democrat). The data were collected by using the Crimson Hexagon ForSight social media analytics platform (http://www.crimsonhexagon.com/platform).\nThe tweets were labeled by two domain experts with a background in political communication in a two-phase process. In the first phase, the experts determined the sentiment towards each candidate mentioned in each tweet independently. In the second phase, they came to a consensus on the tweets that they had initially disagreed on.\nFor our crowdsourcing experiments, we used the Amazon Mechanical Turk (AMT) Internet marketplace to recruit workers. We accepted all workers from the U.S. who had previously completed 100 HITs and maintained at least a 92% approval rating. We paid each worker $0.05 per completed task. We conducted two crowdsourcing studies, a formative and a summative study, involving 200 and 800 tweets respectively.\nFormative Study. We gave the following instruction before presenting every tweet:\nCarefully read through each tweet and decide the author\u2019s attitude toward each mentioned presidential candidate (support, neutral, or against).\nWe verified that short tweets (fewer than 10 proper words) were very difficult to tag. Tweets with links to an external page were also difficult to analyze. It is likely that the sentiment of the tweet heavily relies on the content of the referenced webpage. Workers may have tried to follow the link or may have selected a random sentiment instead of following the link. In our instructions for our summative study, we therefore specifically asked the crowd workers not to click on any external link for completing the task. We also adjusted the label for positive and negative sentiments towards a candidate.\nSummative Study. We updated the instructions as follows:\nRead through the tweet and answer the following questions. Do NOT click on any links. Read the tweet and decide whether the candidate was mentioned at all or not. Note that the reference of Twitter user names (e.g., @realDonaldTrump, @HillaryClinton) or hashtags (e.g.,#Trump2016, #HillaryClinton2016) is also counted as a mention. Express which sentiment was manifested by the writer towards them: positive, neutral, or negative.\nWe collected ratings from a traditional crowdsourcing scheme that involves 5 independent workers per tweet. We call this the \u201cTrad 5\u201d baseline. For 15 tweets that were deemed \u2019hard\u2019 to analyze by our decision tree and thus required the ratings from 7 workers, we needed to collect additional ratings. Instead of simply collecting two more, we asked for 5 additional ratings per tweet from which we could then draw additional samples randomly for analysis. This resulted in a total of 5,075 tasks.\nTo simulate a crowd sourcing experiment that employs a fixed number of three crowd workers per tweet (our traditional Trad 3 baseline), we randomly sample the results produced by 5 crowd workers. To simulate\nthe crowd sourcing experiments that use the decision tree we designed (SDT, DDT1, DDT2), we similarly use random samples from our Trad 5 baseline. To obtain the results of our decision trees, we averaged the collected metrics over 5 different model runs to attenuate potential noise generated by the randomness in selecting crowd workers.\nEvaluation Measures We use two metrics for evaluating our work. They are meaningful for understanding the trade-off between accuracy and budget concerns, which is the focus of our work.\n\u2022 Number of crowd worker tasks: This is the total number of Human Intelligence Tasks requested by our decision tree model. The number provides an indication of the budget needs of a crowd experiment. To find the monetary costs of crowdsourcing, we can multiply this number by the price per task (we used $0.05/task).\n\u2022 Accuracy of the labeling: The accuracy of the crowdsourced sentiment analysis can be determined by how much agreement exists between the majority crowdsourced opinion and the gold standard opinion provided by experts. Our main measure of accuracy is Cohen\u2019s Kappa score \u03ba for measuring inter-rater reliability (IRR). Cohen\u2019s Kappa score accounts for the possibility that raters are guessing and so an agreement is obtained by chance."}, {"heading": "4 Results", "text": "Sarcasm detection Our experiments showed that the clues we used for sarcasm detection are very diverse, and were used in different ways according to the topic of the tweet. We found that smileys were not used at all, while the most meaningful element for sarcasm detection was the presence of expressions like \u2019lol\u2019, \u2019hahaha,\u2019 for example, in the following tweet:"}, {"heading": "RT @rickygervais If Trump was a teacher he\u2019d be fired for publicly saying the things he says. Luckily he isn\u2019t a teacher. Just the next president. Hahaha.", "text": "The presence of sarcasm was indeed a factor which increased the difficulty of tweet classification: in our dataset, sarcastic tweets had a 71.2% percentage interrater agreement. This metric increased to 78.3% when dealing with non-sarcastic tweets.\nIt turned out that the presence of sarcasm was not as ubiquitous as we had expected, as only 73 messages out of 800 were estimated to be sarcastic by domain expert, and a surprising 68.5% of them concern Donald Trump (see Table 1). The last row of the table shows that even after weighing the sarcasm presence over the number of tweets that mentioned each candidate, Donald Trump still leads with 12% of his tweets that are sarcastic. Regarding the sentiment that is usually associated with sarcasm, the last column of the table proves that sarcasm is usually associated with a negative feeling towards a\ncandidate. In fact this language feature is usually employed to make fun of a candidate and criticize him for his statements or actions.\nDifferences Based on Specific Candidates As expected, we found that which presidential candidate was mentioned in a tweet had an impact on how difficult it was to discern the tweeter\u2019s opinion about the candidate. The sentiments that tweeters expressed towards Hillary Clinton and Donald Trump were often unclear or veiled by sarcasm. To illustrate this point qualitatively, we give an example tweet about Trump that confused the crowd workers:\nI was watching the Texas gop debate on snapchat lol and this is the only state where I\u2019ve seen people actually rally against trump YOUNG PPL.\nOne crowd worker labeled the tweet to show \u201ca positive attitude,\u201d 2 crowd workers labeled it as \u201cneutral\u201d and the remaining 2 agreed on a \u201cnegative\u201d sentiment towards the candidate. In this case, it is impossible to determine a result by majority vote, and a final label can be assigned by a reasonable random choice. We here chose randomly between \u201cneutral\u201d and \u201cnegative.\u201d\nTo illustrate the issue quantitatively, we here provide the inter-rater reliability values among 5 crowd workers of our formative study when classifying sentiments towards each candidate and report both the relative observed agreement among crowd workers and Cohen\u2019s Kappa score \u03ba:\nCandidate Agreement Kappa IRR Bernie Sanders: 83.05% \u03ba = 0.74 Ted Cruz: 87.78% \u03ba = 0.78 Hillary Clinton: 63.41% \u03ba = 0.41 Donald Trump 78.13% \u03ba = 0.66\nIt is evident from the above numbers that annotators disagreed much more often when Clinton or Trump were mentioned. For our summative study, we therefore designed an offline model that can account for this observation and involve more workers to label tweets from these two candidates.\nResults for Traditional Fixed-Allocation Model The first two models that we considered are a fixed\ncrowdsourcing round with the same amount of workers for every tweet. With a total of 3 annotators we requested 3,000 ratings and we achieved a 0.612 Kappa value (see Table 2). If we increase the number of crowd workers by 2 we require 5,000 tasks and we would get a 0.653 reliability measure. These results align with previous observations that the task of sentiment analysis is challenging even for human annotators (Young and Soroka 2012; Tumasjan et al. 2010) Despite the significantly higher costs of requesting 2,000 additional labels from crowd workers, a 40% increase, the average agreement between the majority of crowd contributions and expert labels improved by only 6.3 percent (or, equivalently, by a difference of Kappa values of 4.1 percent points).\nResults for the Proposed Static Decision Tree For the static decision tree (SDT), 3,907 labels were requested, on average, and an IRR score of 0.624 was obtained. The allocated numbers of workers based on the text analysis of the tweets and decision rules of the tree are shown in red in Figure 1. With this static decision tree, 22% of the budget would be saved with respect to the traditional 5-worker-per-task model (Trad 5). The loss in accuracy is 4.4 percent points.\nResults for the Proposed Dynamic Decision Trees The first dynamic tree (DDT1) showed a meaningful improvement as it involves only 3,206 tasks on average and has an IRR score of 0.630. This model costs 36% less than the fixed one with 5 workers and only 6.9% more than the model with 3 annotators but the gain in accuracy with respect to the latter is quite high (2.9%). This model would be preferable in low-budget situations.\nThe second dynamic tree (DDT2) is a bit more expensive as it requires 3,608 annotators by average but the Cohen\u2019s Kappa IRR rate improves to 0.643. Even\nthis classifier is much cheaper than the fixed 5-worker as it saves almost 28% of the budget and the accuracy is comparable (the difference between Kappas scores is only 1 percent point). We propose that this predictor is suitable if we are willing to spend a bit more in order to achieve a very good performance.\nBoth dynamic trees produce notably better results than the fixed decision tree in both cost and accuracy. This shows that the difficulty of a tweet can be inferred from the crowdsourcing outcomes themselves and that heuristic rules for determining it are extremely complex and hard to formulate. Correct results can be obtained by a second round of annotations, which needs to be set up accordingly, thus saving a meaningful amount of budget.\nCost Savings of Dynamic versus Static Worker Assignment The traditional 5-worker-per-task allocation model Trad 5 performs exactly the same as a dynamic model which assigns 3 annotators +2 more if there is disagreement, as described in Section 2.4. This result shows that our model allows the same accuracy but at a much lower cost. A visualization of the differences in accuracy and efficiency between traditional static crowdsourcing schemes and the proposed dynamic schemes is given in Figure 2.\nAnalysis of Crowd Work Properties We submitted 5,075 tasks to Mechanical Turk for an overall cost of $253.75. The number of MT workers who contributed labels to all the tweets was 218. An average of 23 annotations was submitted per worker.\nWe analyzed how much time workers spent in labeling a single tweet, which is illustrated in Figure 3. Annotators spent an average of 85.1 seconds for classifying a single message but some workers were very meticulous and used up to 10 minutes to complete a single task. For example one of the best annotators who worked for us labeled 217 tweets with an average of 212 seconds per task, which sums up to almost 13 hours spent on the platform. On the other hand, other annotators were very quick, for instance one worker contributed by labeling 42 tweets and spent on average less than 9 seconds per message.\nSample Results on Political Tweets Analysis of the annotations of our 1,000 tweet dataset provides some fascinating observations about political opinions. We can report the overall sentiment that people showed towards candidates, as rated by the crowd workers (Table 3) and by the experts in political communication (Table 4). We found that Trump is the \u201cmost popular\u201d candidate to tweet about, considering that more than half of the total tweets mentioned him, while the other candidates were evenly referred to on average. Furthermore it is clear that tweeters who discuss candidates for presidential elections often express negative feelings and complain about candidates, since there are about twice as many negative messages than positive ones in our entire dataset. The main difference between the\ncrowd worker and expert annotations was the tendency of the crowd worker to label fewer tweets as \u201cneutral.\u201d"}, {"heading": "5 Discussion and Conclusions", "text": "As crowdsourcing becomes more and more popular for large scale information retrieval, the cost of this human computation is becoming relevant. Example applications are real-time sentiment analysis to provide fast indications of changes in public opinion or collection of a sufficiently large training data for machine learning methods for big data analytics (Wang et al. 2012). Investigations, as ours, about how to balance the goals of efficiency and accuracy in crowdsourcing, are therefore particularly timely.\nFew works have explored dynamic approaches to crowdsourcing that rely on iterative rounds of crowdsourcing and determine the number of worker assignments based on content and annotation results in previ-\nous rounds (Bragg et al. 2014; Ho and Vaughan 2012; Kolobov, Mausam, and Weld 2013). Connections to active and reactive learning (Yan et al. 2011; Lin, Mausam, and Weld 2015) have been made. While prior work involves theoretical analysis and simulation studies, we here provide a concrete solution to the problem of analyzing the sentiment of political twitter messages using a dynamic worker allocation framework.\nWe proposed a dynamic two-round crowdsourcing scheme that we embedded into a decision tree classifier. Other types of classifiers may be used, and, in future work, we will explore additional learning methods.\nAnalysis of political tweets is challenging due to the short text and unknown context. Sentiment analysis is particularly difficult. Existing off-the-shelf text analysis systems can only provide a single sentiment label for a given text automatically. We found that they fail to distinguish the separate sentiments that were expressed\nwhen more than one presidential candidate was mentioned in a tweet. The presence of sarcasm exacerbated the problem. Our proposed solution is to design a classifier that early in the analysis makes a decision about the number of sentiments that must be revealed. Our new dataset may inspire other researchers to develop text analysis tools that address the difficult problem of multi-sentiment analysis and sarcasm detection.\nWe are committed to sharing our dataset and source code at http://anonymous.edu. Our corpus of 1,000 twitter messages is unique because it includes information about (1) the presence/absence of sarcasm and (2) a label about the specific sentiment for each candidate mentioned in the tweet (positive, neutral, negative), as determined by consensus of two domain experts.\nIt is notable that our study involved communication researchers in many aspects of the research, such as the development and refinement of crowdsourcing task instructions and the design of the Mechanical Turk interface. The intervention of domain experts greatly helped improve the validity and performance of our crowdsourcing method.\nLikewise, the proposed approach has the potential to make a significant contribution to communication research. Traditionally, communication researchers use manual content analysis, a method that usually relies on two or three human coders, to analyze text in different media outlets or that of public opinion (Riffe, Lacy, and Fico 2014). However, the traditional method is tedious, time consuming, and limited by the nature of human subjectivity. Arguably, the use of the dynamic online crowdsourcing framework introduced in\nthis study allows communication researchers to process larger datasets in a more efficient and reliable manner. Given the results of the study, future research should also consider cross-disciplinary collaboration to advance theories and methods for large-scale text analysis.\nAcknowledgments The authors would like to thank Boston University Rafik B. Hariri Institute for Computing and Computational Science and Engineering for support of the research.\nReferences [Bragg et al. 2014] Bragg, J.; Kolobov, A.; Mausam, M.; and Weld, D. S. 2014. Parallel task routing for crowdsourcing. In Second AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2014), 11\u201321.\n[Davidov, Tsur, and Rappoport 2010] Davidov, D.; Tsur, O.; and Rappoport, A. 2010. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL \u201910, 107\u2013116.\n[Gonza\u0301lez-Iba\u0301n\u0303ez, Muresan, and Wacholder 2011] Gonza\u0301lez-Iba\u0301n\u0303ez, R.; Muresan, S.; and Wacholder, N. 2011. Identifying sarcasm in twitter: A closer look. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 581\u2013586.\n[Guo et al. 2016] Guo, L.; Vargo, C. J.; Pan, Z.; Ding, W.; and Ishwar, P. 2016. Big social data analyt-\nics in journalism and mass communication: Comparing dictionary-based text analysis and unsupervised topic modeling. Journalism and Mass Communication Quarterly 1\u201328.\n[Ho and Vaughan 2012] Ho, C.-J., and Vaughan, J. W. 2012. Online task assignment in crowdsourcing markets. In AAAI\u201912 Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, 45\u201351.\n[Hsueh, Melville, and Sindhwani 2009] Hsueh, P.-Y.; Melville, P.; and Sindhwani, V. 2009. Data quality from crowdsourcing: A study of annotation selection criteria. In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing, 27\u201335.\n[Karger, Oh, and Shah 2013] Karger, D. R.; Oh, S.; and Shah, D. 2013. Efficient crowdsourcing for multiclass labeling. In Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems, Pittsburgh, PA, USA, 81\u201392.\n[Kolobov, Mausam, and Weld 2013] Kolobov, A.; Mausam; and Weld, D. S. 2013. Joint crowdsourcing of multiple tasks. In 1st AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2013), 36\u201337.\n[Lin, Mausam, and Weld 2015] Lin, C. H.; Mausam, M.; and Weld, D. S. 2015. Reactive learning: Actively trading off larger noisier training sets against smaller cleaner ones. In Proceedings of the 32nd International Conference on Machine Learning, Lille, France (ICML). 5 pages.\n[Rajpal, Goel, and Mausam 2015] Rajpal, S.; Goel, K.; and Mausam, M. 2015. POMDP-basedworker pool selection for crowdsourcing. In Proceedings of the 32nd International Conference on Machine Learning, Lille, France (ICML). 9 pages.\n[Riffe, Lacy, and Fico 2014] Riffe, D.; Lacy, S.; and Fico, F. 2014. Analyzing media messages: Using quantitative content analysis in research. Routledge, New York, N.Y.\n[Tran-Thanh et al. 2013] Tran-Thanh, L.; Venanzi, M.; Rogers, A.; and Jennings, N. R. 2013. Efficient budget allocation with accuracy guarantees for crowdsourcing classification tasks. In Proceedings of the 2013 International Conference on Autonomous Agents and Multiagent Systems, 901\u2013908.\n[Tumasjan et al. 2010] Tumasjan, A.; Sprenger, T. O.; Sandner, P. G.; and Welpe, I. M. 2010. Predicting elections with Twitter: What 140 characters reveal about political sentiment. In Fourth International AAAI Conference on Weblogs and Social Media, ICWSM 2010, 178\u2013185.\n[Wang et al. 2012] Wang, H.; Can, D.; Kazemzadeh, A.; Bar, F.; and Narayanan, S. 2012. A system for realtime twitter sentiment analysis of 2012 U.S. presidential election cycle. In Proceedings of the 50th Annual\nMeeting of the Association for Computational Linguistics, Jeju, Republic of Korea, 115\u2013120.\n[Yan et al. 2011] Yan, Y.; Rosales, R.; Fung, G.; and Dy, J. G. 2011. Active learning from crowds. In Proceedings of the 28th International Conference on Machine Learning, Bellvue, WA. 8 pages.\n[Young and Soroka 2012] Young, L., and Soroka, S. 2012. Affective news: The automated coding of sentiment in political texts. Political Communication 29(2):205\u2013231."}], "references": [{"title": "Parallel task routing for crowdsourcing", "author": ["Bragg"], "venue": "In Second AAAI Conference on Human Computation and Crowdsourcing (HCOMP", "citeRegEx": "Bragg,? \\Q2014\\E", "shortCiteRegEx": "Bragg", "year": 2014}, {"title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon", "author": ["Tsur Davidov", "D. Rappoport 2010] Davidov", "O. Tsur", "A. Rappoport"], "venue": "In Proceedings of the Fourteenth Conference on Computational Natural Language", "citeRegEx": "Davidov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Identifying sarcasm in twitter: A closer look", "author": ["Muresan Gonz\u00e1lez-Ib\u00e1\u00f1ez", "S. Muresan", "N. Wacholder"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1\u00f1ez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1\u00f1ez et al\\.", "year": 2011}, {"title": "Big social data", "author": ["Guo"], "venue": null, "citeRegEx": "Guo,? \\Q2016\\E", "shortCiteRegEx": "Guo", "year": 2016}, {"title": "Online task assignment in crowdsourcing markets", "author": ["Ho", "Vaughan 2012] Ho", "C.-J", "J.W. Vaughan"], "venue": "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ho et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2012}, {"title": "Data quality from crowdsourcing: A study of annotation selection criteria", "author": ["Melville Hsueh", "P.-Y. Sindhwani 2009] Hsueh", "P. Melville", "V. Sindhwani"], "venue": "In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language", "citeRegEx": "Hsueh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsueh et al\\.", "year": 2009}, {"title": "Efficient crowdsourcing for multiclass labeling", "author": ["Oh Karger", "D.R. Shah 2013] Karger", "S. Oh", "D. Shah"], "venue": "In Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems,", "citeRegEx": "Karger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2013}, {"title": "Joint crowdsourcing of multiple tasks", "author": ["Mausam Kolobov", "A. Weld 2013] Kolobov", "Mausam", "D.S. Weld"], "venue": "In 1st AAAI Conference on Human Computation and Crowdsourcing (HCOMP", "citeRegEx": "Kolobov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kolobov et al\\.", "year": 2013}, {"title": "Reactive learning: Actively trading off larger noisier training sets against smaller cleaner ones", "author": ["Mausam Lin", "C.H. Weld 2015] Lin", "M. Mausam", "D.S. Weld"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "POMDP-basedworker pool selection for crowdsourcing", "author": ["Goel Rajpal", "S. Mausam 2015] Rajpal", "K. Goel", "M. Mausam"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Rajpal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajpal et al\\.", "year": 2015}, {"title": "Analyzing media messages: Using quantitative content analysis in research", "author": ["Lacy Riffe", "D. Fico 2014] Riffe", "S. Lacy", "F. Fico"], "venue": null, "citeRegEx": "Riffe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Riffe et al\\.", "year": 2014}, {"title": "Efficient budget allocation with accuracy guarantees for crowdsourcing classification tasks", "author": ["Tran-Thanh"], "venue": "In Proceedings of the 2013 International Conference on Autonomous Agents and Multi-", "citeRegEx": "Tran.Thanh,? \\Q2013\\E", "shortCiteRegEx": "Tran.Thanh", "year": 2013}, {"title": "Predicting elections with Twitter: What 140 characters reveal about political sentiment", "author": ["Tumasjan"], "venue": "In Fourth International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "Tumasjan,? \\Q2010\\E", "shortCiteRegEx": "Tumasjan", "year": 2010}, {"title": "A system for realtime twitter sentiment analysis of 2012 U.S. presidential election cycle", "author": ["Wang"], "venue": "In Proceedings of the 50th Annual", "citeRegEx": "Wang,? \\Q2012\\E", "shortCiteRegEx": "Wang", "year": 2012}, {"title": "Active learning from crowds", "author": ["Yan"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Yan,? \\Q2011\\E", "shortCiteRegEx": "Yan", "year": 2011}, {"title": "Affective news: The automated coding of sentiment in political texts. Political Communication 29(2):205\u2013231", "author": ["Young", "L. Soroka 2012] Young", "S. Soroka"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Young et al\\.", "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "Opinions about the 2016 U.S. Presidential Candidates have been expressed in millions of tweets that are challenging to analyze automatically. Crowdsourcing the analysis of political tweets effectively is also difficult, due to large inter-rater disagreements when sarcasm is involved. Each tweet is typically analyzed by a fixed number of workers and majority voting. We here propose a crowdsourcing framework that instead uses a dynamic allocation of the number of workers. We explore two dynamic-allocation methods: (1) The number of workers queried to label a tweet is computed offline based on the predicted difficulty of discerning the sentiment of a particular tweet. (2) The number of crowd workers is determined online, during an iterative crowd sourcing process, based on inter-rater agreements between labels. We applied our approach to 1,000 twitter messages about the four U.S. presidential candidates Clinton, Cruz, Sanders, and Trump, collected during February 2016. We implemented the two proposed methods using decision trees that allocate more crowd efforts to tweets predicted to be sarcastic. We show that our framework outperforms the traditional static allocation scheme. It collects opinion labels from the crowd at a much lower cost while maintaining labeling accuracy.", "creator": "LaTeX with hyperref package"}}}