{"id": "1609.00799", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2016", "title": "Lexical-Morphological Modeling for Legal Text Analysis", "abstract": "In two definition of the Competition which Legal Information Extraction / Entailment (COLIEE ), we implemented a types comprising. process move for mind relevant documents even part authority believe having intend wednesday textual entailment believed keep ability a incomplete me. The plans method is is on of make latter number metadata turn morphological complexity, either renovate similar urdu models two entire giving also features for Machine Learning algorithms. We ensure a detailed testing as also amendments method performance example severe dealing, indicating concerned then it focused long state - for - as - art approaches when Legal Information Retrieval and Question Answering, close not needing partly conjunction monitor nor level on expert show understand. The discuss use thanks particular tournament in with competition, indicating a substantial normally of adequacy over the day-to-day that.", "histories": [["v1", "Sat, 3 Sep 2016 07:24:08 GMT  (143kb,D)", "http://arxiv.org/abs/1609.00799v1", "16 pages, 5 figures, Lecture notes in computer science: New Frontiers in Artificial Intelligence, 2016/03"]], "COMMENTS": "16 pages, 5 figures, Lecture notes in computer science: New Frontiers in Artificial Intelligence, 2016/03", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["danilo s carvalho", "minh-tien nguyen", "tran xuan chien", "minh le nguyen"], "accepted": false, "id": "1609.00799"}, "pdf": {"name": "1609.00799.pdf", "metadata": {"source": "CRF", "title": "Lexical-Morphological Modeling for Legal Text Analysis", "authors": ["Danilo S. Carvalho", "Minh-Tien Nguyen", "Chien-Xuan Tran", "Minh-Le Nguyen"], "emails": ["nguyenml}@jaist.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Answering legal questions has been a long-standing challenge in the Information Systems research landscape. This topic draws progressively more attention, as we experience an explosive growth in legal document availability on the World Wide Web and specialized systems. This growth is not accompanied by a matching increase in information analysis capabilities, which points to a severe under-utilization of available resources and to potential for information quality issues [1]. As a consequence, increasing pressure has been put into professionals of law, since having the relevant and correct information is a vital step in legal case solving and thus is closely tied to the matter of professional ethics and liability. This problem is often referred as the \u201cinformation crisis\u201d of law.\nThe ability to retrieve relevant and correct information given a legal query has improved over time, with the combination of expert Knowledge Engineering and Natural Language Processing (NLP) methods. However, the ability to answer questions in the legal domain is of special difficulty, due to the need of reasoning over different types of information, such as past decisions, laws and\n? Supported by CNPq \u2013 Brazil scholarship grant\nar X\niv :1\n60 9.\n00 79\n9v 1\n[ cs\n.I R\n] 3\nS ep\nfacts. Furthermore, concepts in legal text are often used in a way that differs from common language use, and differences in laws and procedures from each country prevent the creation of comprehensive and coherent international law corpora. Common legal ontologies are among the efforts to facilitate automatic legal reasoning, but have not seen strong development in the past years [2]. In this context, Textual Entailment Recognition plays a very important role, as a set of hypothesis presented in a question will certainly have answers in the previously cited types of information (decisions, laws, facts). The Recognition of Textual Entailment (RTE) challenge series 3, although not specific to the legal domain, is a recognized benchmark for methods that can be adapted to legal texts.\nTo effectively answer legal questions, one fundamental set of information that must be available is the law, presented as the collection of codes, sections, articles and paragraphs that should be unequivocally referenced when a hypothesis is raised as part of a legal inquiry. Therefore, adequate representation of law corpora is the basis of a functional system for legal question answering. The representation problem is often associated with ontologies and other annotated knowledge bases, but these methods are costly and more difficult to automate when compared to fully text-based approaches, such as bag-of-words, n-gram and topic models.\nIn this work, we propose a fully text-based method for legal text analysis, in the context of the Competition on Legal Information Extraction/Entailment (COLIEE), covering both the tasks of Information Extraction and Question Answering. The goal is the retrieval of relevant law articles to a given yes/no legal question and the use of the retrieved articles to correctly answer the question in a completely automated way. Our contributions in this paper are as follows: (i) a ranking and selection method for legal information retrieval based on a mixed size n-gram model, including an original scoring function for ranking; (ii) an improved adaptation of a Textual Entailment classification method, based on Machine Learning ensembles (Adaboost), including a similarity feature built upon Distributional Semantics (Word2Vec). Lexical and morphological analysis were done on the English translated Japanese Civil Code, comprising tokenization, POS-tagging, lemmatization, word clustering and a set of lexical statistics. A study on success and fail cases is also provided, with common baseline practices and related works used as means of performance comparison. The results of COLIEE are presented as a means of substantiating the experimental evaluation and also discussing the proposed method\u2019s perceived shortcomings and improvements.\nThe remaining of this work is structured as follows: Section 2 presents the related works and relevant results; Section 3 details the Legal Question Answering problem and the COLIEE competition shared task; Section 4 explains our approach to the competition problem; Section 5 presents the experimental setting, results and discussion; Finally, Section 6 offers some concluding remarks.\n3 www.aclweb.org/aclwiki/index.php?title=Recognizing Textual Entailment"}, {"heading": "2 Related Works", "text": "Liu, Chen and Ho [3] presented the three-phase prediction (TPP) method for retrieval of relevant statutes in Taiwan\u2019s criminal law, given general language queries. The method was a hierarchical ranking approach to law corpora, featuring a combination of several Information Retrieval techniques, as well as Machine Learning and feature selection ones. Results were evaluated in terms of recall, achieving from 0.52 to 0.91, from the top 3 to 10 retrieved results, respectively.\nInkpen et al. showed one of the first successful models for RTE using SVMs [5]. Later, Castillo proposed a new system for solving RTE using SVMs [6], in which training data includes RTE-3, annotated data set from RTE-4, and the development set of RTE-5. 32 features were used and the training model achieved the best F-measure of 0.69 in two-way and 0.67 in three-way classification task.\nNguyen et al. [7] conducted a study of RTE on a Vietnamese version of RTE-3 [8] translated from Giampiccolo et. al. [9]. The author used SVMs trained with 15 features divided in two groups: distance and statistical features, in which the first group captures the distance and the second one represents the word overlapping between two sentences. A voting system combining three classifiers built on three feature groups (distance, statistical, and combined features) was used to judge entailment relation. The method obtained 0.684 of F-measure in two-way task.\nIn legal text, Tran et al. addressed legal text QA by using inference [10]. The author used requisite-effectuation structures of legal sentences and similarity measures to find out correct answers without training data and achieved 60.8% accuracy on 51 articles on Japanese National Pension Law.\nKim et al. proposed a hybrid method containing simple rules and unsupervised learning using deep linguistic features to address RTE in civil law [11]. The author also constructed a knowledge base for negation and antonym words which would be used for classifying simple questions. To deal with difficult questions, the author used morphological, syntactic and lexical analysis to identify premises and conclusions. The accuracy was 68.36% with easy questions and 60.02 with difficult ones.\nThis work uses all features in [7], as they apply to the same purpose. Additional features were also included: Word2Vec similarity and term frequency \u2013 inverse document frequency (TF-IDF). Our approach differs from [6] in using Word2Vec[17] similarity instead of WordNet."}, {"heading": "3 Legal Question Answering", "text": "Legal Question Answering (LQA) consists in finding out and providing \u201ccorrect answers\u201d to a legal question given by users. An overview of LQA is shown in Fig. 1.\nLQA can be divided in three tasks: 1) retrieving relevant articles, i.e., the ones containing the answer; 2) finding correct evidence in the relevant articles that allows answering the question; and 3) answering the question. While the first task is a specific case of Information Retrieval (IR), the second can be considered as a form of Recognition of Textual Entailment (RTE), in which given a question,\nthe LQA system has to decide whether and how a relevant article can answer the question. The third one is the final result of the two previous tasks, combined with answer formatting.\nLegal text is considerably different to other types of text, e.g., news articles, due to their structural and semantic characteristics. Firstly, they have specific logical sentence structures e.g., requisite and effectuation [12]. Secondly, words and writing style are used in a strict form, because law documents require high correctness and should avoid ambiguity. Another aspect is that law documents are written in a high abstraction level [13]; therefore, they often require collection and linking of multiple concept references to enable understanding and answering of a question. The use of concept references leads to a situation in which there are few, or in some cases, even no overlapping words between a law question and its relevant articles.\nIn this work, LQA tasks are considered into the context of COLIEE, a competition on legal information extraction/entailment which was first held in 2014, in association with Workshop on Juris-informatics (JURISIN). COLIEE 2015 4 is the second competition, consisting of three phases:\n\u2013 Phase One: retrieving relevant articles from all Japanese Civil Code Articles given a set of YES/NO questions. \u2013 Phase Two: evaluating the entailment relationship between the question and retrieved articles. \u2013 Phase Three: combination of Phase One and Phase Two, the system will retrieve list of relevant articles given a query, and then decide the entailment relationship between retrieved articles and the provided question.\nThe Japanese Civil Code is composed by a collection of numbered articles, each one containing a set of declarations pertaining to a specific topic of the law, e.g., labor contracts, mortgages.\nInformation Retrieval Task: Relevance Analysis\nThe first phase consists on an explicit IR task, for which the goal is to retrieve the relevant articles that can be used to correctly answer a given yes/no question. The challenge in this task is to determine the relative relevance, i.e., Relevance Analysis (RA), of an article to the query presented in the question. Different\n4 webdocs.cs.ualberta.ca/m\u0303iyoung2/COLIEE2015/\narticles dealing with the same topic often have similar wording and it is common for questions not to refer to topic keywords or refer to alternative ones. Furthermore, the restricted size of the Japanese Civil Code means that obtaining reliable linguistic information from articles is difficult and most questions will present new language structures that can range from useful to necessary for answering.\nSimple Question Answering Task: Textual Entailment\nThe goal of Textual Entailment (TE) is to decide whether a legal query/question can be answered by a set of relevant articles retrieved with RA. This task can be accomplished by recognizing textual entailment (RTE), in which the query/question is treated as an hypothesis and relevant articles as evidence. Given a question Q and a set of relevant articles A, (A = {a1, ..., an}), if Q is answered by ai (1 6 i 6 n), then ai entails Q [9], [14]. A pair (Q, ai) is assigned label YES if a entailment relationship exists, i.e., ai can answer Q; otherwise, NO."}, {"heading": "4 Proposed Approach", "text": "In order to be able to perform both Relevance Analysis and Textual Entailment recognition independently in phases one and two, and jointly in phase three, IR and classifier methods were developed separately. First, both the legal corpus and training data are analyzed and combined into representation models. The models are then used to rank articles or classify answers according to the task. The representation model used for Relevance Analysis is a mixed size n-gram collection and the one used for textual entailment are feature vectors for Machine Learning. Figure 2 shows the overall view of the proposed method."}, {"heading": "4.1 Relevance Analysis", "text": "A detailed analysis of the Civil Code and training data revealed that lexical and syntactic overlapping may vary to a high degree between questions and articles, and also between articles concerning the same topic. However, certain morphological features, such as lemmas, retain a higher level of consistency among topics. For this reason, the adopted representation model was a mixed size ngram model, with n : [1, k], i.e., terms made by sequences up to k words, in which the terms are lemmatized. For simplicity, the Relevance Analysis method\nhereon described was named R2NC (Ranking Related N-gram Collections). A summarized view of the process is shown on Figure 3. The steps to build the model are detailed as follows:\n1. Collect the entire content for each article, including section title; 2. Check references between articles and annotate accordingly; 3. Tokenize and POS-tag; 4. Remove stopwords: determiners, conjunctions, prepositions and punctuation; 5. Lemmatize words; 6. Generate n-grams; 7. Expand the n-gram set, by including references n-grams; 8. Associate article number and references; 9. Store the model.\nExcept for step 4, each step is responsible for adding new information to the model. The information is obtained either from the text, e.g., section title, references, or from morphological analysis, e.g., POS-tags, lemmas. If an article have references, its n-gram set is expanded with the references\u2019 n-grams. This is done so that all the necessary information for interpretation of any single article is self-contained. Besides the n-grams, links between the articles are also stored. To include the training data information, the same process is repeated for the questions, and n-gram sets from the trained questions are used to expand the associated articles\u2019 n-gram models. Since COLIEE disallowed explicit expert knowledge input, an optional information source was added after the competition, as a way of including expert knowledge in the model when available, and possibly improve system performance. This source consists in a simple term dictionary, where legal terms are associated with other correlated ones. If a given question contains n-grams referred in the dictionary, its n-gram model is expanded with the associated entries. The dictionary was written manually and contains 26 entries that were considered important after analyzing the training data, e.g., \u201cfor a third party\u201d \u2192 \u201cto others\u201d, and extrapolating answers to user defined queries. Tokenization and lemmatization were done using NLTK 5 (v. 3.0.2) with the Punkt tokenizer and WordNetLemmatizer modules, respectively.\n5 www.nltk.org\nThose modules were used with their unchanged default models and settings, trained with the Punk corpus and WordNet, respectively. POS-tagging was done using Stanford Tagger6 (v. 3.5.2), using the unchanged english-left3words-distsim model, which is trained on the part-of-speech tagged WSJ section of the Penn Treebank corpus.\nTo determine the relative relevance of an article with regard to the content of a question, a ranking approach was adopted. First, the n-gram set of the question is obtained by applying steps 1-6, using the question content instead of article. Then, for each article in the Civil Code, a relevance score is calculated using the following formula:\nscore =\n\u2211 \u2200t idf(t)\nIq \u00d7 |q ng set|+ Iart \u00d7 |art ng set| , t \u2208 (q ng set \u2229 art ng set) (1)\nwhere q ng set is the set of n-grams for the question, art ng set is the set of n-grams for the article in the stored model, Iq is the relative significance of the question n-gram set size and Iart is the relative significance of the article n-gram set size. idf(t) is the Inverse Document Frequency for the term t over the articles collection\nidf(t) = log N\ndft (2)\nwhere N is the total number of articles and dft is the number of articles in which t appears.\nThe formula (1) is a variation of the traditional TF-IDF scoring method, disregarding term frequency and giving different weights for the two types of document being evaluated: articles and questions, according to their size. Iq and Iart are parameters to be adjusted according to the corpus characteristics. This formula was developed during the first stages of analysis on the Civil Code corpus, when experiments with a TF-IDF based classifier showed poor results for this task and further observation showed that TF did not contribute for article relevance in many cases. As TF is absent from the formula, document size becomes a more relevant feature and must be considered in the scoring. In the studied corpus, law articles are usually much larger than questions in number of words, hence the different weights to adjust normalization of the score regarding the respective sets.\nFrom this point, the articles are sorted by descending score and the 10 best are selected for filtering. The filtering step consists in fetching the best scoring article and verifying if its score exceeds a parameter threshold confidence thresh. If it does, all the articles in the list that are referred by the first and exceed a parameter threshold reference thresh are also fetched. The fetched articles compose the final list of relevant articles to the input question. Parameter adjustment is described in Section 5.\n6 nlp.stanford.edu/software/tagger.shtml"}, {"heading": "4.2 Textual Entailment", "text": "A textual entailment (TE) relation in law domain comprises two levels of information. The first level describes whether or not (YES/NO, respectively) the textual evidence addresses the hypothesis. The second level describes whether the evidence supports or opposes (YES/NO, respectively) the hypothesis. However, due to the time constraint of the competition, only the first level is explored. Therefore, semantic relations such as negation and antonym were not considered in the TE evaluation step.\nTo detect a TE relation on a pair (Q, a), a similarity-based approach [4] can be used, in which a can answer Q if the similarity is greater than a certain threshold. However, high level inference (see Section 3) and the identification of the threshold make these methods more challenging to apply. We, therefore, propose to apply classification for detecting the TE relation with two advantages: (1) use of a rich feature set to represent data characteristics and (2) avoiding to identify the threshold.\nThis work shares most of the goals presented in Nguyen et. al. [7], so all the features in that work were used. However, the corpus size in this case makes it difficult to effectively train Machine Learning algorithms. For this reason, \u201cstronger\u201d features were sought as a way of compensating such problem. An additional Word2Vec feature was added to capture the semantic similarity of a pair (Q, a), as observation of statistical data in Table 1 shows that the lexical overlapping may not be a strong enough feature for the classification on a (Q, a) pair (e.g., cannot capture the similarity of person and manager). By adding the Word2Vec feature, the model aims to cover the semantic aspect instead of only lexical similarity. Word2Vec was trained by JPN Law corpus: a collection of all Civil law articles of Japan\u2019s constitution7. It contains 642 cleaned and tokenized articles, with about 13.5 million words in total.\nFor the classification, the Weka toolset 8 implementation of AdaBoost [18] was used, with classifier = DecisionStump.\nThe features are shown in Table 2, in which distance features measure distance between a question Q and relevant article a and statistical features capture word overlapping of this pair. After extracting features, a pipeline model was proposed and is shown in Figure 4.\nIn Figure 4, the first step is to preprocess the data from the input files, in which sentences and words are segmented and stopwords9 are removed. Next, the training data is represented in a vector space model by features in Table 2. The retrieved data from relevance analysis is also denoted in the same mechanism. Finally, a classifier was trained on the training data and applied on retrieved\n7 www.japaneselawtranslation.go.jp 8 weka.wikispaces.com 9 https://sites.google.com/site/kevinbouge/stopwords-lists\ndata to judge the entailment relation. Note that features in Section 4.1 can be also used for this task."}, {"heading": "5 Experiments and Results", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "The dataset was obtained from the published data for the COLIEE shared task 10, consisting in a text file with the Japanese Civil Code and a set of XML files with training and testing data for phases one to three. The training set for the three tasks contains 267 pairs (question, relevant articles). Experiments where divided in phases one and two only, dealing with Information Retrieval and Textual Entailment methods respectively. Each experiment comprised: i) data analysis, ii) model and parameter adjustments and iii) test runs. 10 webdocs.cs.ualberta.ca/m\u0303iyoung2/COLIEE2015/"}, {"heading": "5.2 Parameter adjustment", "text": "For R2NC, parameters Iq, Iart, confidence thresh (shortened to ct here), reference thresh (rt) and also k, the maximum n-gram size, were adjusted empirically on the training data using the following simple procedure:\n\u2013 Starting with Iq = 0.8, ct = 0.5, rt = 0.5 and k = 1, 1. Increase or decrease a single parameter by step r = 0.1 until the F-\nmeasure cannot be increased for a leave-one-out test. 2. Repeat (1) starting from the last obtained value, with r = 0.01. 3. Repeat (1) and (2) for all parameters.\nFor k, step was fixed on r = 1. Iq and Iart respect the constraint Iq+Iart = 1. The parameters are changed in a specific order: 1. confidence thesh, 2. k, 3. reference thresh, 4. Iq. Iq and Iart respect the constraint Iq + Iart = 1 Performance metrics were recorded for the parameter adjustment during the experiments. Fig. 5 shows the performance progression on post-competition experiments for the parameters Iq, Iart , with the other ones locked into their best respective values. Performance for k 6= 3 is negatively affected in both directions (-,+), and no further investigation was conducted for a larger range of values.\nFinal parameter values used in the competition are k = 3, Iq = 0.965, Iart = 0.035, confidence thresh = 0.32 and reference thresh = 0.2.\nFor the RTE classifier, default parameters from the Weka toolset11 were used for all the experiments and were not changed. The parameter values are: iterations = 10, seed = 1, no re-sampling and weightthreshold = 100."}, {"heading": "5.3 Baselines", "text": "As for the second edition of COLIEE, there is still no definite baseline for the competition dataset. However, common baseline practices and related works\n11 weka.wikispaces.com\ncould be used for evaluating performance on each task. For phase one, a relationship can be drawn between R2NC and TPP [3]. For the TE task, the following baselines were used for comparison:\n\u2013 SVMs: uses Support Vector Machines (SVMs)12 [19] with Weka. The parameters are C = 1, \u03b3 = 0, kernel Type = radial basis function (RBF). \u2013 AdaBoost-SVMs: uses SVMs as weak learners instead of DecisionStump."}, {"heading": "5.4 Evaluation Method", "text": "Given the limited training data available, leave-one-out validation was used to evaluate the performance of the model in both tasks on the training dataset with three measures: precision (P), recall (R) and F-measure (F) as in Eq. (3), (4) and (5). In phase two, accuracy (A) measurement is also used as in Eq. (6).\nP = Cr\nRt (3) R =\nCr Rl (4) F = 2(P \u2217R) P +R\n(5) A = Cq\nQ (6)\nwhere Cr counts the correctly retrieved articles for all queries, Rt counts the retrieved articles for all queries, Rl counts the relevant articles for all queries, Cq counts the queries correctly confirmed as true or false and Q counts all the queries."}, {"heading": "5.5 Pre-competition Results", "text": "Pre-competition experiment results on the shared data are presented in Tables 3 and 4.\nThe results indicate that R2NC is expected to be competitive with state-ofthe-art approaches to relevance analysis in legal documents, such as TPP [3]. However, the proposed method is much simpler when compared to TPP and operates with considerably less training data: 266 documents for R2NC against 1518 documents for TPP. R2NC design also makes it difficult for the model to be overtrained beyond the parameter adjustment, since no training data is counted\n12 https://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nmore than one time and the method is single-shot, as opposed to convergencebased. Experiments were repeated with traditional TF-IDF scoring instead of R2NC formula, yielding 0.51 F-measure.\nResults of RTE in Tab. 4 indicate that AdaBoost with a set of appropriate features outperforms the baselines by 7.74% (SVMs) and 12.94% (AdaboostSVMs) on F-measure. Moreover, the precision and accuracy of this method also achieve considerable improvements when compared to the baselines. This suggests that the features are expected to be efficient for addressing TE in the legal domain. This conclusion is supported by the accuracy measurements.\nAnother interesting point is that Word2Vec similarity contributes to improve the performance of RTE. As stated in Section 3, legal documents usually require concept linking to understand and answer a question; therefore, semantic similarity from Word2Vec helps to improve the performance. The results also show the efficiency of the lexical features.\nThe performance of RTE in the law domain, however, is not comparable with the same task in common data i.e., news articles [6,7] due to the characteristics of law dataset, as shown in Section 3. The performance was not improved very much even when many features in both phase one and two were combined. This suggests that more sophisticated approaches e.g., semantic inference or semantic rules should be considered in feature construction. Finally, negation and antonym analysis should be considered to improve the quality of the entailment recognition, effectively exploring the second level of entailment information as described in Section 4.2."}, {"heading": "5.6 Feature Evaluation", "text": "Further evaluation of feature impact on TE model was conducted by leave-oneout test. The most effective features are shown in Table 5.\nTable 5 shows an indication of contribution from features to the model. Results show that all effective features contribute to the method. Note that both Damerau-Levenshtein and Euclidean are distance features whereas the longest common substring (lcs) is a statistical feature. The results support that in legal texts, there is not much word overlapping between a question and relevant articles. An interesting aspect is that Word2Vec similarity has a big positive impact to the model. This supports the conclusion on similarity stated in Section 5.5."}, {"heading": "5.7 Competition Results", "text": "The method presented in this paper achieved significant results in COLIEE, being ranked 2nd in phase one (IR) and 3rd in phase three (combined IR + TE). It was not well ranked in phase two (TE). The relevant competition results are presented in Table 6 as they were announced in JURISIN 2015."}, {"heading": "5.8 Post-competition Analysis and Improvements", "text": "Post competition analysis pointed us to possible sources of classification problems in phase 2 (TE) and also gave directions of improvement in both tasks.\nFor R2NC, the lack of an implicit semantic mapping was an important factor when compared to the top ranked approach. To compensate for that, a term dictionary was included as a new information source for expanding the question n-gram models as described in Section 4.1. By using linguistic observations, it was possible to create basic entries in the dictionary (non-expert knowledge), improving phase 1 F-measure on the shared data (Table 3) from 0.54 to 0.55.\nIn the case of phase 2, over-fitting on training data was deemed the main factor that reduced classification performance. Our system achieved over 61% accuracy (Table 4) when running on the shared data, but only 37.88% reported from the competition results. Phase three results show that accuracy improved when restricting information for the classifier and this is consistent with the over-fitting assumption. Another important point is that a question q and all sentences in an article a were used in building the vector space model. As a result, imbalance of length between the question and the article may have affected feature calculation. This can be addressed by developing a better text segmentation method. Finally, the over-fitting assumption can also be dealt by using other classification approaches e.g., Deep Neural Networks, together with over-fitting avoidance techniques e.g., pruning, dropout."}, {"heading": "5.9 Error Analysis and Discussion", "text": "An investigation was done on the ranked list obtained with R2NC in phase one (see Section 4.1). It revealed that relevant articles ranked 3rd and below had keywords that did not appear in the corresponding question in the corpus. This reinforces the view that the questions are highly directed, albeit in a conceptual level. Relevant articles that ranked lower than 15th (approx. 20%) were found to require a relatively high level of abstraction to obtain an interpretation that could link to the corresponding question. Table 7 shows an example of complex relevance relationship.\nTable 8 shows a case in which our system gives correct outputs (ID H18-2-4). In this example, there are several common words from which this approach can correctly judge the TE relation, e.g., reimbursement. In addition, several words can be inferred from the questions by using Word2Vec similarity e.g., person \u223c manager, fees \u223c costs or expenses. This supports our observation that TE can be addressed by using lexical features and word similarity. For example, in (ID H18-2-4), our system can still predict the TE relation correctly, even with little\nlexical overlap. This indicates the efficiency of this approach, and especially of the word similarity feature.\nOn the other hand, the pair H18-26-1 exemplifies a case in which the system predicted NO while TE relation was annotated YES even when the question and answer share more common words. This shows the limitation of this feature set in cases where the question and answer are short. In this case, after removing stop words, a few remaining words may not be enough to capture the TE relation. Moreover, the lack of important words e.g., building, connection or belong reveals a big challenge for our system to decide the TE relation. This suggests that a keyword enriching mechanism such as term expansion used in phase one could improve the results.\nIn order to facilitate the understanding of different error cases and give other people the opportunity to try the system developed for the competition, an online demo system13 has been made available. In this demo it is possible to input user defined questions or just verify the answers to questions in the COLIEE shared data.\n13 http://150.65.242.101:3001/"}, {"heading": "6 Conclusion", "text": "This paper explores the challenging issue of building a QA system in the legal domain. We propose a model including three stages: legal information retrieval, legal textual entailment and legal text answering. In the first stage, a mixed size n-gram model built from morphological analysis is used to rank and select relevant articles corresponding to a legal question; next, pairs of questions and retrieved articles are judged by a machine learning algorithm trained on lexical features and Distributional Semantic similarity, to decide whether the questions can be answered positively or negatively by the retrieved articles; and finally, correct answers would be provided for users in the final stage. The contributions of this work in IR and TE task are: 1) a simple, yet effective language model for law corpora coupled with a Relevance Analysis method (R2NC) capable of exploiting such model; 2) The use of TF-IDF and Word2Vec similarity features for applying Machine Learning algorithms to RTE. With a recall of 0.64 for the top 3 ranked articles, R2NC appears as competitive when compared to state-of-the-art similar work, in spite of being more simple and applicable with less training data. By combining lexical features and Word2Vec similarity, this approach for LQA also outperformed the baselines by 8.4% (SVM ) and 11.3% (Adaboost-SVMs) on F-measure. Results in the COLIEE competition for the IR task (0.508 F-measure, 2nd place) and the combined IR+TE task (0.582 accuracy, 3rd place) indicate a substantial adequacy to the tasks addressed. The competition also provided important shortcomings of the proposed approach, namely the lack of implicit semantic representation and classifier over-fitting. Those shall be addressed in future work.\nStill on future directions, information on a higher abstraction level, e.g., syntactic mappings, could be used to improve the language model for the IR task. In the TE task, since a sentence in a legal article is usually long, a sophisticated method of sentence partition e.g., requisite and effectuation should be considered. In feature extraction, features in IR should be combined with lexical features in TE and investigated to improve the quality of the judgment. Moreover, capturing contradictions in the TE relation by current statistical features is a big challenge. To solve this issue, semantic rules over negation and antonym detection should be defined and incorporated into the feature extraction. Finally, we would like to investigate and apply sentence similarity calculation by Sent2Vec to improve the performance of the TE."}, {"heading": "Acknowledgements", "text": "This work is supported partly by the grant of NII Research Cooperation and JAIST\u2019s Research grant."}], "references": [{"title": "Berring: \u201cThe heart of legal information: The crumbling infrastructure of legal research", "author": ["C. Robert"], "venue": "Legal information and the development of American law. St. Paul, MN: Thomson/West,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "The LKIF Core ontology of basic legal concepts", "author": ["Rinke Hoekstra", "Joost Breuker", "Marcello Di Bello", "Alexander Boer"], "venue": "Proc. of the Workshop on Legal Ontologies and Artificial Intelligence Techniques (LOAIT", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Predicting associated statutes for legal problems", "author": ["Yi-Hung Liu", "Yen-Liang Chen", "Wu-Liang Ho"], "venue": "Information Processing & Management", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Refining the judgment threshold to improve recognizing textual entailment using similarity.", "author": ["Quang-Thuy Ha", "Thi-Oanh Ha", "Thi-Dung Nguyen", "Thuy-Linh Nguyen Thi"], "venue": "Computational Collective Intelligence. Technologies and Applications", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Machine Learning Experiments for Textual Entailment", "author": ["Diana Inkpen", "Darren Kipp", "Vivi Nastase"], "venue": "Proceedings of the Second Challenge Workshop Recognising Textual Entailment : 17-20,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "An approach to Recognizing Textual Entailment and TE Search Task using SVM", "author": ["Julio Javier Castillo"], "venue": "Procesamiento del Lenguaje Natural,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Recognizing Textual Entailment in Vietnamese Text: An Experiment Study.", "author": ["Minh-Tien Nguyen", "Quang-Thuy Ha", "Thi-Dung Nguyen", "Tri-Thanh Nguyen", "Le-Minh Nguyen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Using Machine Translation for Recognizing Textual Entailment in Vietnamese Language.", "author": ["Quang Nhat Minh Pham", "Le Minh Nguyen", "Akira Shimazu"], "venue": "RIVF: 1-6,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "The third PASCAL recognising textual entailment challenge", "author": ["Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan"], "venue": "Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing. Association for Computational Linguistics:", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Answering Legal Questions by Mining Reference Information", "author": ["Oanh Thi Tran", "Bach Xuan Ngo", "Minh Le Nguyen", "Akira Shimazu"], "venue": "New Frontiers in Artificial Intelligence. Springer International Publishing:", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Answering Yes/No Questions in Legal Bar Exams", "author": ["Mi-Young Kim", "Ying Xu", "Randy Goebel", "Ken Satoh"], "venue": "New Frontiers in Artificial Intelligence. Springer International Publishing:", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "RRE Task: The Task of Recognition of Requisite Part and Effectuation Part in Law Sentences.", "author": ["Bach Xuan Ngo", "Minh Le Nguyen", "Akira Shimazu"], "venue": "J. IJCPOL", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Reference Resolution in Legal Texts.", "author": ["Oanh Thi Tran", "Bach Xuan Ngo", "Minh Le Nguyen", "Akira Shimazu"], "venue": "In Proc. of ICAIL: 101-110,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Recognizing textual entailment: Rational, evaluation and approaches - Erratum", "author": ["Ido Dagan", "Bill Dolan", "Bernardo Magnini", "Dan Roth"], "venue": "Natural Language Engineering", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Intrinsic and Extrinsic Approaches to Recognizing Textual Entailment", "author": ["Rui Wang"], "venue": "Saarland University,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "A decision-theoretic generalization of online learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of computer and system sciences", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Vapnik: \u201cSupport-Vector Networks", "author": ["Corinna Cortes", "Vladimir"], "venue": "Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "This growth is not accompanied by a matching increase in information analysis capabilities, which points to a severe under-utilization of available resources and to potential for information quality issues [1].", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "Common legal ontologies are among the efforts to facilitate automatic legal reasoning, but have not seen strong development in the past years [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "Liu, Chen and Ho [3] presented the three-phase prediction (TPP) method for retrieval of relevant statutes in Taiwan\u2019s criminal law, given general language queries.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "showed one of the first successful models for RTE using SVMs [5].", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "Later, Castillo proposed a new system for solving RTE using SVMs [6], in which training data includes RTE-3, annotated data set from RTE-4, and the development set of RTE-5.", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "[7] conducted a study of RTE on a Vietnamese version of RTE-3 [8] translated from Giampiccolo et.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7] conducted a study of RTE on a Vietnamese version of RTE-3 [8] translated from Giampiccolo et.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "addressed legal text QA by using inference [10].", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "proposed a hybrid method containing simple rules and unsupervised learning using deep linguistic features to address RTE in civil law [11].", "startOffset": 134, "endOffset": 138}, {"referenceID": 6, "context": "This work uses all features in [7], as they apply to the same purpose.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Our approach differs from [6] in using Word2Vec[17] similarity instead of WordNet.", "startOffset": 26, "endOffset": 29}, {"referenceID": 16, "context": "Our approach differs from [6] in using Word2Vec[17] similarity instead of WordNet.", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": ", requisite and effectuation [12].", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "Another aspect is that law documents are written in a high abstraction level [13]; therefore, they often require collection and linking of multiple concept references to enable understanding and answering of a question.", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": ", an}), if Q is answered by ai (1 6 i 6 n), then ai entails Q [9], [14].", "startOffset": 62, "endOffset": 65}, {"referenceID": 13, "context": ", an}), if Q is answered by ai (1 6 i 6 n), then ai entails Q [9], [14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "To detect a TE relation on a pair (Q, a), a similarity-based approach [4] can be used, in which a can answer Q if the similarity is greater than a certain threshold.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "[7], so all the features in that work were used.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "For the classification, the Weka toolset 8 implementation of AdaBoost [18] was used, with classifier = DecisionStump.", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "For phase one, a relationship can be drawn between R2NC and TPP [3].", "startOffset": 64, "endOffset": 67}, {"referenceID": 18, "context": "For the TE task, the following baselines were used for comparison: \u2013 SVMs: uses Support Vector Machines (SVMs) [19] with Weka.", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "The results indicate that R2NC is expected to be competitive with state-ofthe-art approaches to relevance analysis in legal documents, such as TPP [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 5, "context": ", news articles [6,7] due to the characteristics of law dataset, as shown in Section 3.", "startOffset": 16, "endOffset": 21}, {"referenceID": 6, "context": ", news articles [6,7] due to the characteristics of law dataset, as shown in Section 3.", "startOffset": 16, "endOffset": 21}], "year": 2016, "abstractText": "In the context of the Competition on Legal Information Extraction/Entailment (COLIEE), we propose a method comprising the necessary steps for finding relevant documents to a legal question and deciding on textual entailment evidence to provide a correct answer. The proposed method is based on the combination of several lexical and morphological characteristics, to build a language model and a set of features for Machine Learning algorithms. We provide a detailed study on the proposed method performance and failure cases, indicating that it is competitive with state-of-the-art approaches on Legal Information Retrieval and Question Answering, while not needing extensive training data nor depending on expert produced knowledge. The proposed method achieved significant results in the competition, indicating a substantial level of adequacy for the tasks addressed.", "creator": "LaTeX with hyperref package"}}}