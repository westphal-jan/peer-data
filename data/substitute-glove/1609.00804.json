{"id": "1609.00804", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2016", "title": "Randomized Prediction Games for Adversarial Machine Learning", "abstract": "In subliminal only sunbeds detection, attackers inevitably randomization hold demonise malicious data and low looking certainly common evading lethal morning test time; e. 2. , malware using rather typically criminalised using number strings might decimal sequences next loot. exploits. Interestingly, vector-valued has 's was policy to education security main learning algorithms allowed evasion prompted, with although positive in hiding services about part classifier well where lobbed. Recent work given provisions rookie - theoretical formulations to get restore kola-ye, by sterilizing exist evasion attacks same requiring itself classification activation consider. However, both form 4a exists several, simulated data gimmickry have been modeled today a approximation sufficiently, we accounting and any form. colorimetric. In this work, we differences so limitation over enacted a outcomes indicator game, namely, brought non - farming big - coarsest modification in own one referent and well attacker might randomized push nominees raised to some variance generated distinction while the additionally strategy set. We watch that indeed ambitious usually while to encouraging the trade - off between military technique and false alarms first respect to illinois - within - the - art secure plagiocephaly, even against terrorists why are are and those hypothesized last unique, a limited illustrate four correspondence weakest achieve, bites work cowpox detection.", "histories": [["v1", "Sat, 3 Sep 2016 09:30:51 GMT  (1288kb,D)", "http://arxiv.org/abs/1609.00804v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["samuel rota bul\\`o", "battista biggio", "ignazio pillai", "marcello pelillo", "fabio roli"], "accepted": false, "id": "1609.00804"}, "pdf": {"name": "1609.00804.pdf", "metadata": {"source": "CRF", "title": "Randomized Prediction Games for Adversarial Machine Learning", "authors": ["Samuel Rota Bul\u00f2", "Ignazio Pillai", "Fabio Roli"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Pattern classification, adversarial learning, game theory, randomization, computer security, evasion attacks.\nI. INTRODUCTION Machine-learning algorithms have been increasingly adopted in adversarial settings like spam, malware and intrusion detection. However, such algorithms are not designed to operate against intelligent and adaptive attackers, thus making them inherently vulnerable to carefully-crafted attacks. Evaluating security of machine learning against such attacks and devising suitable countermeasures, are two among the main open issues under investigation in the field of adversarial machine learning [1]\u2013[11]. In this work we focus on the issue of designing secure classification algorithms against evasion attacks, i.e., attacks in which malicious samples are manipulated at test time to evade detection. This is a typical setting, e.g., in spam filtering, where spammers manipulate the content of spam emails to get them past the anti-spam filters [1], [2], [12]\u2013[14], or in malware detection, where hackers obfuscate malicious software (malware, for short) to evade detection of either known or zero-day exploits [8], [9], [15], [16]. Although out of the scope of this work, it is worth mentioning here another pertinent attack scenario, referred to as classifier poisoning. Under this setting, the\nS. Rota Bulo\u0300 is with ICT-Tev, Fondazione Bruno Kessler, Trento, Italy M. Pelillo is with DAIS, Universita\u0300 Ca\u2019 Foscari, Venezia, Italy B. Biggio, I. Pillai, F. Roli are with DIEE, University of Cagliari, Italy\nattacker can manipulate the training data to mislead classifier learning and cause a denial of service; e.g., by increasing the number of misclassified samples [6], [7], [17]\u2013[20].\nTo date, several authors have addressed the problem of designing secure learning algorithms to mitigate the impact of evasion attacks [1], [6], [10], [11], [21]\u2013[27] (see Sect. VII for further details). The underlying rationale of such approaches is to learn a classification function that accounts for potential malicious data manipulations at test time. To this end, the interactions between the classifier and the attacker are modeled as a game in which the attacker manipulates data to evade detection, while the classification function is modified to classify them correctly. This essentially amounts to incorporating knowledge of the attack strategy into the learning algorithm. However, both the classification function and the simulated data manipulations have been modeled in a deterministic manner, without accounting for any form of randomization.\nRandomization is often used by attackers to increase their chances of evading detection, e.g., malware code is typically obfuscated using random strings or byte sequences to hide known exploits, and spam often contains bogus text randomly taken from English dictionaries to reduce the \u201cspamminess\u201d of the overall message. Surprisingly, randomization has also been proposed to improve classifier security against evasion attacks [3], [6], [28]. In particular, it has been shown that randomizing the learning algorithm may effectively hide information about the classification function to the attacker, requiring her to select a less effective attack (manipulation) strategy. In practice, the fact that the adversary may not know the classification function exactly (i.e., in a deterministic sense) decreases her (expected) payoff on each attack sample. This means that, to achieve the same expected evasion rate attained in the deterministic case, the attacker has to increase the number of modifications made to the attack samples [28].\nMotivated by the aforementioned facts, in this work we generalize static prediction games, i.e., the game-theoretical formulation proposed by Bru\u0308ckner et al. [10], [11], to account for randomized classifiers and data manipulation strategies. For this reason, we refer to our game as a randomized prediction game. A randomize prediction game is a non-cooperative game between a randomized learner and a randomized attacker (also called data generator), where the player\u2019s strategies are replaced with probability distributions defined over the respective strategy sets. Our goal is twofold. We do not only aim to assess whether randomization helps achieving a better trade-off in terms of false alarms and attack detection (with respect to state-of-the-art secure classifiers), but also whether\nar X\niv :1\n60 9.\n00 80\n4v 1\n[ cs\n.L G\n] 3\nS ep\n2 01\n6\n2 our approach remains more secure against attacks that are different from those hypothesized during design. In fact, given that our game considers randomized players, it is reasonable to expect that it may be more robust to potential deviations from its original hypotheses about the players\u2019 strategies.\nThe paper is structured as follows. Randomized prediction games are presented in Sect. II, where sufficient conditions for the existence and uniqueness of a Nash equilibrium in these games are also given. In Sect. III we focus on a specific game instance involving a linear Support Vector Machine (SVM) learner, for which we provide an effective method to find an equilibrium by overcoming some computational problems. We discuss how to enable the use of nonlinear (kernelized) SVMs in Sect. IV. In Sect. V we report a simple example to intuitively explain how the proposed methods enforce security in adversarial settings. Related work is discussed in Sect. VII. In Sect. VI we empirically validate the soundness of the proposed approach on an handwritten digit recognition task, and on realistic adversarial application examples involving spam filtering and malware detection in PDF files. Notably, to evaluate robustness of our approach and state-of-the-art secure classification algorithms, we also consider attacks that deviate from the models hypothesized during classifier design. Finally, in Sect. VIII, we summarize our contributions and sketch potential directions for future work."}, {"heading": "II. RANDOMIZED PREDICTION GAMES", "text": "Consider an adversarial learning setting involving two actors: a data generator and a learner.1 The data generator produces at training time a set D\u0302 = {x\u0302i, yi}ni=1 \u2286 X \u00d7Y of n training samples, sampled from an unknown probability distribution. Sets X and Y denote respectively the input and output spaces of the learning task. At test time, the data generator modifies the samples in D\u0302 to form a new dataset D \u2286 X \u00d7Y , reflecting a test distribution, which differs in general from the training distribution and it is not available at training time. We assume binary learners, i.e. Y = {\u22121,+1}, and we assume also that the data transformation process leaves the labels of the samples in D\u0302 unchanged, i.e., D = {(xi, yi)}ni=1. Hence, a perturbed dataset will simply be represented in terms of a tuple X = (x1, . . . ,xn) \u2208 X n, each element being the perturbation of the original input sample x\u0302i, while we implicitly assume the label to remain yi. The role of the learner is to classify samples x \u2208 X according to the prediction function h(x) = sign[f(x;w)], which is expressed in terms of a linear generalized decision function f(x;w) = w>\u03c6(x), where w \u2208 Rm, x \u2208 X and \u03c6 : X \u2192 Rm is a feature map.\nStatic prediction games have been introduced in [11] by modeling the learner and the data generator as players of a non-cooperative game that we identify as l-player and dplayer, respectively. The strategies of l-player correspond to the parametrizationsw of the prediction function f . The strategies of the data generator, instead, are assumed to live directly in the feature space, by regarding X\u0307 = (x\u0307>1 , . . . , x\u0307 > n ) > \u2208 Rmn as a data generator strategy, where x\u0307i = \u03c6(xi). By doing so, the decision function f becomes linear in either players\u2019\n1We adopt here the same terminology used in [11].\nstrategies. Each player is characterized also by a cost function that depends on the strategies played by either players. The cost function of d-player and l-player are denoted by cd and cl, respectively, and are given by\ncl(w, X\u0307) = \u03c1l\u2126l(w) +\nn\u2211\ni=1\n`l(w >x\u0307i, yi) , (1)\ncd(w, X\u0307) = \u03c1d\u2126d(X\u0307) + n\u2211\ni=1\n`d(w >x\u0307i, yi) , (2)\nwhere w \u2208 Rm is the strategy of l-player, X\u0307 \u2208 Rmn is the strategy of the d-player, and yi denotes the label of x\u0307i as per D\u0302. Moreover, \u03c1d/l > 0 is a trade-off parameter, `d/l(w>x\u0307i, y) measures the loss incurred by the l/d-player when the decision function yields w>x\u0307i for the ith training sample while the true label is y, and \u2126d/l can be regarded as a penalization for playing a specific strategy. For the d-player, this term quantifies the cost of perturbing D\u0302 in feature space.\nThe goal of our work is to introduce a randomization component in the model of [11], particularly to what concerns the players\u2019 behavior. To this end, we take one abstraction step with respect to the aforementioned prediction game, where we let the learner and the data generator sample their playing strategy in the prediction game from a parametrized distribution, under the assumption that they are expected costminimizing (a.k.a. expected utility-maximizing). By doing so, we introduce a new non-cooperative game that we call randomized prediction game between the l-player and the d-player with strategies being mapped to the possible parametrizations of the players\u2019 respective distributions, and cost functions being expected costs under the same distributions."}, {"heading": "A. Definition of randomized prediction game", "text": "Consider a prediction game as described before. We inject randomness in the game by assigning each player a parametrized probability distribution, i.e., pl(w;\u03b8l) for the learner and pd(X\u0307;\u03b8d) for the data generator, that governs the players\u2019 strategy selection. Players are allowed to select the parametrization \u03b8l and \u03b8d for the respective distributions. For any choice of \u03b8l, the l-player plays a strategy w sampled from pl(\u00b7;\u03b8l). Similarly, for any choice of \u03b8d, the d-player plays a strategy X\u0307 sampled from pd(\u00b7;\u03b8d). If the players adhere to the new rules, we obtain a randomized prediction game.\nA randomized prediction game is a non-cooperative game between a learner (l-player) and data generator (d-player) that has the following components:\n1) an underlying prediction game with cost functions cl/d(w, X\u0307) as defined in (1) and (2), 2) two parametrized probability distributions pl/d(\u00b7;\u03b8l/d) with parameters in \u0398l/d, 3) \u0398l/d are non-empty, compact and convex subsets of a finite-dimensional metric space Rsl/d .\nThe sets of parameters \u0398l/d are the pure strategy sets (a.k.a. action spaces) for the l-player and d-player, respectively. The costs functions of the two players, which quantify the cost that each player incurs when a strategy profile (\u03b8l,\u03b8d) \u2208 \u0398l \u00d7\u0398d is played, coincide with the expected costs, denoted\n3 by cl/d(\u03b8l,\u03b8d), that the two players have in the underlying prediction game if strategies are sampled from pl(\u00b7;\u03b8l) and pd(\u00b7;\u03b8d), according to the expected cost-minimizing hypothesis:\ncl(\u03b8l,\u03b8d) = Ew\u223cpl(\u00b7;\u03b8l) X\u0307\u223cpd(\u00b7;\u03b8d) [cl(w, X\u0307)] , (3) cd(\u03b8l,\u03b8d) = Ew\u223cpl(\u00b7;\u03b8l) X\u0307\u223cpd(\u00b7;\u03b8d) [cd(w, X\u0307)] , (4)\nwhere E[\u00b7] denotes the expectation operator. We assume cl/d to be well-defined functions, i.e. the expectations to be finite for any (\u03b8l,\u03b8d) \u2208 \u0398l \u00d7\u0398d. To avoid confusion between cl/d and cl/d, in the remainder of this paper we will refer them respectively as cost functions, and expected cost functions.\nBy adhering to a non-cooperative setting, the two players involved in the prediction game are not allowed to communicate and they play their strategies simultaneously. Each player has complete information of the game setting by knowing the expected cost function and strategy set of either players. Under rationality assumption, each player\u2019s interest is to achieve the greatest personal advantage, i.e., to incur the lowest possible cost. Accordingly, the players are prone to play a Nash equilibrium, which in the context of our randomized prediction game is a strategy profile (\u03b8?l ,\u03b8 ? d) \u2208 \u0398l \u00d7 \u0398d such that no player is interested in changing his/her own playing strategy. In formal terms, this yields:\n\u03b8?l \u2208 arg min \u03b8l\u2208\u0398l cl(\u03b8l,\u03b8 ? d) , \u03b8 ? d \u2208 arg min \u03b8d\u2208\u0398d cd(\u03b8 ? l ,\u03b8d) . (5)"}, {"heading": "B. Existence of a Nash equilibrium", "text": "The existence of a Nash equilibrium of a randomized prediction game is not granted in general. A sufficient condition for the existence of a Nash equilibrium is thus given below.\nTheorem 1 (Existence). A randomized prediction game admits at least one Nash equilibrium if\n(i) cl/d are continuous in \u0398l \u00d7\u0398d, (ii) cl(\u00b7,\u03b8d) is quasi-convex in \u0398l for any \u03b8d \u2208 \u0398d,\n(iii) cd(\u03b8l, \u00b7) is quasi-convex in \u0398d for any \u03b8l \u2208 \u0398l.\nProof: The result follows directly from the DebreuGlicksberg-Fan Theorem [29]."}, {"heading": "C. Uniqueness of a Nash equilibrium", "text": "In addition to the existence of a Nash equilibrium, it is of interest to investigate if the equilibrium is unique. However, determining tight conditions that guarantee the uniqueness of the Nash equilibrium for any randomized prediction game is challenging; in particular, due to the additional dependence on a probability distribution for the learner and the data generator.\nWe will make use of a classical result due to Rosen [30] to formulate sufficient conditions for the uniqueness of the Nash equilibrium of randomized prediction games in terms of the so-called pseudo-gradient of the game, defined as\ngr = [ rl\u2207\u03b8l c\u0304l rd\u2207\u03b8d c\u0304d ] , (6)\nwith any fixed vector r = [rl, rd]> \u2265 0. Specifically, a randomized prediction game admits a unique Nash equilibrium if the following assumption is verified\nAssumption 1. (i) cl/d are twice differentiable in \u0398l \u00d7\u0398d,\n(ii) cl(\u00b7,\u03b8d) is convex in \u0398l for any \u03b8d \u2208 \u0398d, (iii) cd(\u03b8l, \u00b7) is convex in \u0398d for any \u03b8l \u2208 \u0398l,\nand gr is strictly monotone for some fixed r > 0, i.e.,\n[gr(\u03b8l,\u03b8d)\u2212 gr(\u03b8\u2032l,\u03b8\u2032d)]> [ \u03b8l \u2212 \u03b8\u2032l \u03b8d \u2212 \u03b8\u2032d ] > 0 ,\nfor any distinct strategy profiles (\u03b8l,\u03b8d), (\u03b8\u2032l,\u03b8 \u2032 d) \u2208 \u0398l\u00d7\u0398d.2\nIn his paper, Rosen provides also a useful sufficient condition that guarantees a strictly monotone pseudo-gradient. This requires the Jacobian of the pseudo-gradient, a.k.a. pseudoJacobian, given by\nJr = [ rl\u22072\u03b8l\u03b8lcl rl\u22072\u03b8l\u03b8dcl rd\u22072\u03b8d\u03b8lcd rd\u22072\u03b8d\u03b8dcd ] , (7)\nto be positive definite.\nTheorem 2. A randomized prediction game admits a unique Nash equilibrium if Assumption 1 holds, and the pseudoJacobian Jr(\u03b8l,\u03b8d) is positive definite for all (\u03b8l,\u03b8d) \u2208 \u0398l \u00d7\u0398d and some fixed r > 0.\nProof: Under Assumption 1, the positive definiteness of Jr for all strategy profiles and some fixed vector r > 0 implies the strict monotonicity of gr, which in turn implies the uniqueness of the Nash equilibrium [30, Thm. 6].\nIn the rest of the section, we provide sufficient conditions that ensure the positive definiteness of the pseudo-Jacobian and thus the uniqueness of the Nash equilibrium via Thm. 2. To this end we decompose c\u0304l/d(\u03b8l,\u03b8d) as follows\nc\u0304l(\u03b8l,\u03b8d) = \u03c1l\u2126l(\u03b8l) + L\u0304l(\u03b8l,\u03b8d) , c\u0304d(\u03b8l,\u03b8d) = \u03c1d\u2126d(\u03b8d) + L\u0304d(\u03b8l,\u03b8d) , (8)\nwhere \u2126l/d and L\u0304l/d are the expected regularization and loss terms given by\n\u2126l(\u03b8l) = Ew\u223cpl(\u00b7,\u03b8l)[\u2126l(w)] , \u2126d(\u03b8d) = EX\u0307\u223cpd(\u00b7,\u03b8d)[\u2126d(X\u0307)] ,\nL\u0304l(\u03b8l,\u03b8d) = Ew\u223cpl(\u00b7;\u03b8l) X\u0307\u223cpd(\u00b7;\u03b8d)\n[ n\u2211\ni=1\n`l(w >xi, yi) ] ,\nL\u0304d(\u03b8l,\u03b8d) = Ew\u223cpl(\u00b7;\u03b8l) X\u0307\u223cpd(\u00b7;\u03b8d)\n[ n\u2211\ni=1\n`d(w >xi, yi) ] .\nMoreover, we require the following convexity and differentiability conditions on \u2126l/s and L\u0304l/d:\nAssumption 2. (i) \u2126l/d is strongly convex and twice continuously differen-\ntiable in \u0398l/d, (ii) L\u0304l(\u00b7,\u03b8d) is convex and twice continuously differentiable\nin \u0398l for all \u03b8d \u2208 \u0398d, and (iii) L\u0304d(\u03b8l, \u00b7) is convex and twice continuously differentiable\nin \u0398d for all \u03b8l \u2208 \u0398l. 2Assumption 1.(i) could be relaxed to continuously differentiable.\n4 Finally, we introduce some quantities that are used in the subsequent lemma, which gives sufficient conditions for the positive-definiteness of the pseudo-Jacobian:\n\u03bb\u2126l = inf \u03b8l\u2208\u0398l\n\u03bbmin [ \u22072\u03b8l\u03b8l\u2126l(\u03b8l) ] ,\n\u03bb\u2126d = inf \u03b8d\u2208\u0398d\n\u03bbmin [ \u22072\u03b8d\u03b8d\u2126d(\u03b8d) ] ,\n\u03bbLl = inf (\u03b8l,\u03b8d)\u2208\u0398l\u00d7\u0398d\n\u03bbmin [ \u22072\u03b8l\u03b8lLl(\u03b8l,\u03b8d) ] ,\n\u03bbLd = inf (\u03b8l,\u03b8d)\u2208\u0398l\u00d7\u0398d\n\u03bbmin [ \u22072\u03b8d\u03b8dLd(\u03b8l,\u03b8d) ] ,\n\u03c4 = sup (\u03b8l,\u03b8d)\u2208\u0398l\u00d7\u0398d\n\u03bbmax [ R(\u03b8l,\u03b8d)R(\u03b8l,\u03b8d) >] ,\nwhere R(\u03b8l,\u03b8d) = 12 [ \u22072\u03b8l\u03b8dL\u0304l(\u03b8l,\u03b8d)> +\u22072\u03b8d\u03b8lL\u0304d(\u03b8l,\u03b8d) ] and \u03bbmax/min give the maximum/minimum eigenvalue of the matrix in input. Note that the quantities listed above are finite and positive if Assumption 2 holds, given the compactness of \u0398l/d.\nLemma 1. If Assumption 2 holds and\n(\u03c1l\u03bb \u2126 l + \u03bb L l )(\u03c1d\u03bb \u2126 d + \u03bb L d ) > \u03c4\nthen the pseudo-Jacobian Jr(\u03b8l,\u03b8d) is positive definite for all (\u03b8l,\u03b8d) \u2208 \u0398l \u00d7\u0398d by taking r = (1, 1)>.\nProof: The pseudo-Jacobian in (7) can be written as follows given the decomposition of c\u0304l/d in (8):\nJr =\n[ \u03c1l\u22072\u03b8l\u03b8l\u2126l +\u22072\u03b8l\u03b8lL\u0304l \u22072\u03b8l\u03b8dL\u0304l\n\u22072\u03b8d\u03b8lL\u0304d \u03c1d\u22072\u03b8d\u03b8d\u2126d +\u22072\u03b8d\u03b8dL\u0304d\n] ,\nwhere we omitted the arguments of \u2126l/d and L\u0304l/d for notational convenience. Let us denote by Jllr , J ld r , J dl r , and J dd r the four matrices composing Jr (in top-down, left-right order). Consider the following matrix:\nH =\n[ Hll Hld\nHdl Hdd\n] = [ \u03c1l\u03bb \u2126 l + \u03bb L l R(\u03b8l,\u03b8d) >\nR(\u03b8l,\u03b8d) \u03c1d\u03bb \u2126 d + \u03bb L d\n] .\nThen we have for all t = (t>l , t > d ) 6= 0\nt>Jrt = t Jr + J\n> r\n2 t>\n= tlJ ll r tl\ufe38 \ufe37\ufe37 \ufe38\n\u2265tlHlltl\n+ tdJ dd r td\ufe38 \ufe37\ufe37 \ufe38\n\u2265t>d Hddtd\n+t>l Jldr + J dl> r\n2\ufe38 \ufe37\ufe37 \ufe38 Hld+Hdl>\ntd \u2265 t>Ht ,\nwhere the under-braced relations follow from the definitions of \u03bb\u2126l/d, \u03bb L l/d and R. Accordingly, the positive-definiteness of Jr can be derived from the positive-definiteness of matrix H. To prove the latter, we will show that all roots of the characteristic polynomial det(H\u2212\u03bbI) of H are positive. By properties of the determinant3 we have\ndet(H\u2212 \u03bbI) = det((\u03c1l\u03bb\u2126l + \u03bbLl \u2212 \u03bb)I)\n\u00b7 det (\n(\u03c1d\u03bb \u2126 d + \u03bb L d \u2212 \u03bb)I\u2212\nS\n\u03c1l\u03bb\u2126l + \u03bb L l \u2212 \u03bb\n) ,\n3det\n[ aI B>\nB dI\n] = det(aI) det(dI \u2212 1\na BB>) and if USU> is the eigen-\ndecomposition of BB> then the latter determinant becomes det(U(dI \u2212 1 a S)U>) = det(dI\u2212 1 a S)\nAlgorithm 1 Extragradient descent (adapted from [11]) Input: Cost functions c\u0304l/d; parameter spaces \u0398l,\u0398d; a small\npositive constant . Output: The optimal parameters \u03b8l,\u03b8d.\n1: Randomly select \u03b8(0) = (\u03b8(0)l ,\u03b8 (0) d ) \u2208 \u0398l \u00d7\u0398d. 2: Set iteration count k = 0, and select \u03c3, \u03b2 \u2208 (0, 1). 3: Set r = (rl, rd) > = (1, \u03c1l/\u03c1d) > . 4: repeat 5: Set d(k) = \u03a0\u0398l\u00d7\u0398d ( \u03b8(k) \u2212 gr ( \u03b8 (k) l ,\u03b8 (k) d )) \u2212 \u03b8(k). 6: Find maximum step size t(k) \u2208 {\u03b2p|p \u2208 N} s.t.\n\u2212gr ( \u03b8\u0304 (k) l , \u03b8\u0304 (k) d )> d(k) \u2265 \u03c3 (\u2225\u2225\u2225d(k) \u2225\u2225\u2225 2\n2\n) ,\nwhere \u03b8\u0304(k) = \u03b8(k) + t(k)d(k).\n7: Set \u03b7(k) = \u2212 t(k)\u2225\u2225\u2225gr ( \u03b8\u0304 (k) l ,\u03b8\u0304 (k) d )\u2225\u2225\u2225 2\n2\ngr\n( \u03b8\u0304\n(k) l , \u03b8\u0304 (k) d\n)> d(k).\n8: Set \u03b8(k+1) = \u03a0\u0398l\u00d7\u0398d ( \u03b8(k) \u2212 \u03b7(k)gr ( \u03b8\u0304 (k) l , \u03b8\u0304 (k) d )) .\n9: Set k = k + 1. 10: until \u2225\u2225\u2225\u03b8(k) \u2212 \u03b8(k\u22121) \u2225\u2225\u2225 2\n2 \u2264\n11: return \u03b8l = \u03b8 (k) l , \u03b8d = \u03b8 (k) d\nwhere S is a diagonal matrix with the eigenvalues of R(\u03b8l,\u03b8d)R(\u03b8l,\u03b8d)\n>. The roots of the first determinant term are all equal to \u03c1l\u03bb\u2126l + \u03bb L l , which is positive because \u03c1l > 0 by construction and \u03bb\u2126l > 0 follows from the strong-convexity of \u2126l in Assumption 2-i. As for the second determinant term, take the ith diagonal element Sii of S. Then two roots are the solution of the following quadratic equation\n\u03bb2 \u2212 \u03bb(a+ b) + ab\u2212 Sii = 0 ,\nwhich are given by\n\u03bb (i) 1,2 = a+ b\u00b1 \u221a (a\u2212 b)2 + 4Sii .\nwhere a = \u03c1l\u03bb\u2126l + \u03bb L l and b = \u03c1d\u03bb \u2126 d + \u03bb L d . Among the two, \u03bb (i) 2 (the one with the minus) is the smallest one, which is strictly positive if\nab = (\u03c1l\u03bb \u2126 l + \u03bb L l )(\u03c1d\u03bb \u2126 d + \u03bb L d ) > Sii .\nSince the condition has to hold for any choice of the eigenvalue Sii in the right-hand-side of the inequality, we take the maximum one maxi Sii, which coincides with \u03bbmax(R(\u03b8l,\u03b8d)R(\u03b8l,\u03b8d)\n>). We further maximize the latter quantity with respect to (\u03b8l,\u03b8d) \u2208 \u0398l \u00d7 \u0398d, because we want the result to hold for any parametrization. Therefrom we recover the variable \u03c4 and the condition (\u03c1l\u03bb\u2126l +\u03bb L l )(\u03c1d\u03bb \u2126 d + \u03bbLd ) > \u03c4 , which guarantees that all roots of the characteristic polynomial of H are strictly positive for any choice of (\u03b8l,\u03b8d) \u2208 \u0398l \u00d7 \u0398d and, hence, Jr is positive definite over \u0398l \u00d7\u0398d.\nIn addition to Lem. 1, we provide in the supplementary material alternative (stronger) sufficient conditions, which generalize the ones given in [11].\n5"}, {"heading": "D. Finding a Nash equilibrium", "text": "From the computational perspective, we can find a Nash equilibrium in our game by exploiting algorithms similar to the ones adopted for static prediction games [11]. In particular, we consider a modified extragradient descent algorithm [11], [31], [32] that finds a solution to the following variational inequality problem, provided that gr is continuous and monotone:\ngr(\u03b8 ? l ,\u03b8 ? d) > (\u03b8 \u2212 \u03b8?) \u2265 0 ,\u2200(\u03b8l,\u03b8d) \u2208 \u0398l \u00d7\u0398d , (9)\nwhere \u03b8 = [\u03b8>l ,\u03b8 > d ] > and similarly for \u03b8?. Any solution \u03b8? to this problem can be shown to correspond bijectively to a Nash equilibrium of a game having gr as pseudo-gradient [11], [32].\nIf Theorem 1 holds, the pseudo-Jacobian J\u0304r can be shown to be positive semidefinite, and gr is thus continuous and monotone. Hence, the variational inequality can be solved by the modified extragradient descent algorithm given as Algorithm 1, which is guaranteed to converge to a Nash equilibrium point [31], [33]. The algorithm generates a sequence of feasible points whose distance from the equilibrium solution is monotonically decreased. It exploits a projection operator \u03a0\u0398l\u00d7\u0398d(\u03b8) to map the input vector \u03b8 onto the closest admissible point in \u0398l\u00d7\u0398d, and a simple line-search algorithm to find the maximum step t on the descent direction d.4\nIn the next section, we apply our randomized prediction game to the case of linear SVM learners, and compute the corresponding pseudo-gradient, as required by Algorithm 1."}, {"heading": "III. RANDOMIZED PREDICTION GAMES FOR SUPPORT VECTOR MACHINES", "text": "In this section, we consider a randomized prediction game involving a linear SVM learner [34], and Gaussian distributions as the underlying probabilities pl/d.\nThe learner. The decision function of the learner is of the type f(x;w) = w>\u03c6(x) where the feature map is given by \u03c6(x) = [ x> 1 ]> . For convenience, we consider a decomposition of w into [ w\u0303> b ]> , where w\u0303 \u2208 Rm\u22121 and b \u2208 R. Hence, the decision function can also be written as f(x;w) = w\u0303>x + b. Accordingly, the input space X is a (m\u2212 1)-dimensional vector space, i.e. X \u2286 Rm\u22121. The distribution pl for the learner is assumed to be Gaussian. In order to guarantee the theoretical existence of the Nash equilibrium through Thm. 1, we assume the parameters of the Gaussian distribution to be bounded. For the sake of clarity, we use in this section axis-aligned Gaussians (i.e. with diagonal covariance matrices) for our analysis, even though general covariances could be adopted as well. Under these assumptions, we define the strategy set for the learner as \u0398l = { (\u00b5w,\u03c3w) \u2208 Rm \u00d7 Rm+ } \u2229 Bl, where Bl \u2282 Rm \u00d7 Rm+ is an application-dependent non-empty, convex, bounded set, restricting the set of feasible parameters. The parameter vectors \u00b5w and \u03c3w encode the mean and standard deviation of the axis-aligned Gaussian distributions. The loss function `l of the learner corresponds to the hinge loss of the SVM, i.e., `l(z, y) = [1\u2212zy]+ with [z]+ = max(0, z), while the strategy\n4We refer the reader to [11], [31], [32] (and references therein) for detailed proofs that derive conditions for which d is effectively a descent direction.\npenalization term \u2126l(w) is the squared Euclidean norm of w\u0303. As a result, the cost function cl corresponds to the C-SVM objective function, and it is convex in w:\ncl(w, X) = \u03c1l 2 \u2016w\u0303\u20162 +\nn\u2211\ni=1\n[1\u2212 yi(w\u0303>xi + b)]+ . (10)\nThe data generator. For convenience, we consider X rather than X\u0307 as the quantity undergoing the randomization. This comes without loss of generality, because there is a oneto-one correspondence between x\u0307i and xi if we consider the linear feature map x\u0307i = \u03c6(xi) = [x>i , 1]\n>. Moreover, we assume that samples xi can be perturbed independently. Accordingly, the distribution pd for the data generator factorizes as pd(X;\u03b8d) = \u220fn i=1 pd ( xi;\u03b8 (i) d ) , where \u03b8d = (\u03b8 (1) d , . . . ,\u03b8 (n) d ). We consider pd ( xi;\u03b8 (i) d ) to be a k-variate axis-aligned Gaussian distribution with bounded mean and standard deviation given by \u03b8(i)d = (\u00b5xi ,\u03c3xi). In summary, the strategy set adopted for the data generator is given by \u0398d =\u220fn i=1 \u0398 (i) d , where \u0398 (i) d = { (\u00b5xi ,\u03c3xi) \u2208 Rk \u00d7 Rk+ } \u2229 Bd. Here, Bd \u2282 Rk \u00d7 Rk+ is a non-empty, convex, bounded set. The loss function `d of the data generator is the hinge loss under wrong labelling, i.e., `d(z, y) = [1 + zy]+. In this way the data generator is penalized if the learner correctly classifies a sample point. Finally, the strategy penalization function \u2126d is the squared Euclidean distance of the perturbed samples in X from the ones in the original training set D\u0302, i.e. \u2126d(X) = \u2211n i=1 \u2016xi \u2212 x\u0302i\u20162. The resulting cost function cd is convex in X:\ncd(w, X) = \u03c1d 2\nn\u2211\ni=1\n\u2016xi \u2212 x\u0302i\u20162\n+ n\u2211\ni=1\n[1 + yi(w\u0303 >xi + b)]+ . (11)\nExistence of a Nash equilibrium. The proposed randomized prediction game for the SVM learner admits at least one Nash equilibrium. This can be proven by means of Thm. 1. Indeed, the required continuity of c\u0304l/d hold and, as for the quasi-convexity conditions, we can rewrite (3) as follows by exploiting the fact that pl is a Gaussian distribution with mean \u00b5w and standard deviation \u03c3w:\ncl(\u03b8l,\u03b8d) = Ez\u223cN (0,I) X\u223cpd(\u00b7;\u03b8d) [cl(\u00b5w +D(\u03c3w)z, X)] , (12) where N (0, I) is a m-dimensional standard normal distribution and D(\u03c3w) is a diagonal matrix having \u03c3w on the diagonal. Since cl is convex in its first argument and convexity is preserved under addition of convex functions, positive rescaling, and composition with linear functions, we have that cl is convex (and thus quasi-convex) in \u03b8l = (\u00b5w,\u03c3w). As for the quasi-convexity condition of the data generator\u2019s cost, we can exploit the separability of cd to rewrite (4) as follows:\ncd(\u03b8l,\u03b8d) = n\u2211\ni=1\nEw\u223cpl(\u00b7;\u03b8l) z\u223cN (0,I) [c (i) d (w,\u00b5xi +D(\u03c3xi)z)] ,\nwhere\nc (i) d (w,x) = \u03c1d 2 \u2016x\u2212 x\u0302i\u20162 + [1 + yi(w>x\u0303i + b)]+ .\n6 Since c(i)d is convex in its second argument, by following the same reasoning used to show the quasi-convexity of the learner\u2019s expected cost, we have that each expectation in cd is convex in \u03b8(i)d = (\u00b5xi ,\u03c3xi), 1 \u2264 i \u2264 n. As a consequence, cd is convex and, hence, quasi-convex in \u03b8d, being the sum of convex functions.\nUniqueness of a Nash equilibrium. In the previous section we have shown that c\u0304l(\u00b7,\u03b8d) and c\u0304d(\u03b8l, \u00b7) are convex as required by Assumption 1-(ii-iii). In particular we have that the single expected regularization terms \u2126l/d(\u00b7) and loss terms L\u0304l(\u00b7,\u03b8d), L\u0304d(\u03b8l, \u00b7) are convex as well. Moreover, they are twice-continuously differentiable by having Gaussian distributions for pl/d. It is then sufficient to have \u2126l/d are strongly convex to prove the uniqueness of the Nash equilibrium via Lem. 1. While it is easy to see that \u2126d is strongly convex, we have that \u2126l is not strongly convex with respect to b due to the presence of an unregularized bias term b in the learner. The problem derives from the fact that the SVM itself may not have a unique solution when the bias term is present and non-regularized (see [35], [36] for a characterization of the degenerate cases). As a result, the proposed game is not guaranteed to have a unique Nash equilibrium in its actual form. On the other hand, a unique Nash equilibrium may be obtained by either considering an unbiased SVM, i.e., by setting b = 0 as in [11], or a regularized bias term, e.g., by adding \u03b52b\n2 to the learner\u2019s objective function with \u03b5 > 0. In both cases, all conditions that ensure the uniqueness of the Nash equilibrium via Thm. 2 and Lem. 1 would be satisfied, under proper choices of \u03c1l/d.\nIt is worth noting however that the necessary and sufficient conditions under which a biased (non-regularized) SVM has no unique solution are quite restricted [35], [36]. For this reason, we believe that uniqueness of the Nash equilibrium could be proven also for the biased SVM under mild assumptions. However, this requires considerable effort in trying to relax the sufficiency conditions of Rosen [30], which is beyond the scope of our work. We thus leave this challenge to future investigations. Moreover, we believe that enforcing a unique Nash equilibrium in our game by making the original SVM formulation strictly convex may lead to worse results, similarly to exploiting convex approximations to solve originally nonconvex problems in machine learning [37], [38]. For the above reasons, in this paper, we choose to retain the original SVM formulation for the learner, by sacrificing the uniqueness of the Nash Equilibrium. We nevertheless provide in Sect. V a discussion of why having a unique Nash Equilibrium is not so important in practice for our game, and we empirically show in Sect. VI that our approach can anyway achieve competitive performances with respect to other state-of-the-art approaches.\nThe rest of this section is devoted to showing how to compute the pseudo-gradient (6) by providing explicit formulae for \u2207\u03b8l c\u0304l and \u2207\u03b8d c\u0304d."}, {"heading": "A. Gradient of the learner\u2019s cost", "text": "In this section, we focus on computing the gradient \u2207\u03b8l c\u0304l(\u03b8l,\u03b8d), where c\u0304l is defined as in (10). By properties of expectation and since w follows an axis-aligned Gaussian distribution with mean \u00b5w and standard deviation \u03c3w, we can\nreduce the cost of the learner to:\nc\u0304l(\u03b8l,\u03b8d) = \u03c1l 2\n( \u2016\u00b5w\u0303\u20162 + \u2016\u03c3w\u0303\u20162 )\n+ n\u2211\ni=1 E w\u223cpl(\u00b7;\u03b8l) xi\u223cpd(\u00b7;\u03b8(i)d )\n[ [1\u2212 yi(w\u0303>xi + b)]+ ] , (13)\nwhere we are assuming the following decompositions for the mean \u00b5w = [ \u00b5>w\u0303 \u00b5b ]> and standard deviation \u03c3w =[\n\u03c3>w\u0303 \u03c3b ]>\n. The hard part for the minimization is the term in the expectation, which can not be expressed to our knowledge in a closed-form function of the Gaussian\u2019s parameters. We thus resort to a Central-Limit-Theorem-like approximation, by regarding si = 1 \u2212 yi(w\u0303>xi + b) as a Gaussiandistributed variable with mean \u00b5si and standard deviation \u03c3si , i.e. si \u223c N (\u00b5si , \u03c3si). In general, si does not follow a Gaussian distribution, since the product of two normal deviates is not normally distributed. However, if the number of features k is large, the approximation becomes reasonable. Under this assumption, we can rewrite the expectation as follows:\nE w\u223cpl(\u00b7;\u03b8l) xi\u223cpd(\u00b7;\u03b8(i)d )\n[ [1\u2212 yi(w\u0303>xi + b)]+ ]\n= Esi\u223cN (\u00b5si ,\u03c3si )[[si]+] . (14)\nThe mean and variance of the Gaussian distribution in the right-hand-side of Eq. (14) are respectively given by\n\u00b5si = E w\u223cpl(\u00b7;\u03b8l) xi\u223cpd(\u00b7;\u03b8(i)d )\n[ 1\u2212 yi(w\u0303>xi + b) ]\n= 1\u2212 yi(\u00b5>w\u0303\u00b5xi + \u00b5b) , (15) \u03c32si = V w\u223cpl(\u00b7;\u03b8l)\nxi\u223cpd(\u00b7;\u03b8(i)d )\n[ 1\u2212 yi(w\u0303>xi + b) ]\n= \u03c32w\u0303 > (\u03c32xi + \u00b5 2 xi) + \u00b5 2 w\u0303 > \u03c32xi + \u03c3 2 b , (16)\nwhere V is the variance operator, and we assume that squaring a vector corresponds to squaring each single component.\nThe expectation in Eq. (14) can be transformed after simple manipulations into the following function involving the Gauss error function (integral function of the standard normal distribution) denoted as erf():\nh(\u00b5si , \u03c3si) = \u03c3si\u221a 2\u03c0 exp\n( \u2212 \u00b5 2 si\n2\u03c32si\n)\n+ \u00b5si 2\n[ 1\u2212 erf ( \u2212 \u00b5si\u221a\n2\u03c3si\n)] . (17)\nThe learner\u2019s cost in Eq. (13) can thus be approximated as:\nc\u0304l(\u03b8l,\u03b8d) \u2248 Ll(\u00b5w,\u03c3w) = \u03c1l 2\n( \u2016\u00b5w\u0303\u20162 + \u2016\u03c3w\u0303\u20162 )\n+\nn\u2211\ni=1\nh(\u00b5si(\u03b8l), \u03c3si(\u03b8l)) . (18)\nWe can now approximate the gradient \u2207\u03b8lcl in terms of \u2207\u03b8lLl. In the following, we denote the Hadamard (a.k.a. entry-wise) product between any two vectors a and b as a\u25e6b,\n7 and we assume any scalar-by-vector derivative to be a column vector. The gradients of interest are given as:\n\u2202Ll \u2202\u00b5w = \u03c1l [ \u00b5w\u0303 0 ] + n\u2211\ni=1\n( \u2202h\n\u2202\u00b5si \u2202\u00b5si \u2202\u00b5w + \u2202h \u2202\u03c32si \u2202\u03c32si \u2202\u00b5w\n) , (19)\n\u2202Ll \u2202\u03c3w = \u03c1l [ \u03c3w\u0303 0 ] + n\u2211\ni=1\n( \u2202h\n\u2202\u00b5si \u2202\u00b5si \u2202\u03c3w + \u2202h \u2202\u03c32si \u2202\u03c32si \u2202\u03c3w\n) , (20)\nwhere it is not difficult to show that \u2202h\n\u2202\u00b5si =\n1\n2\n[ 1\u2212 erf ( \u2212 1\u221a\n2 \u00b5si \u03c3si\n)] , (21)\n\u2202h\n\u2202\u03c32si =\n1\n2 1\u221a 2\u03c0\u03c3si exp\n( \u22121\n2 \u00b52si \u03c32si\n) , (22)\nand that \u2202\u00b5si \u2202\u00b5w = \u2212yi [ \u00b5xi 1 ] ,\n\u2202\u00b5si \u2202\u03c3w = 0 , (23)\n\u2202\u03c32si \u2202\u00b5w =\n[ 2\u03c32xi \u25e6 \u00b5w\u0303\n0\n] ,\n\u2202\u03c32si \u2202\u03c3w\n= 2\u03c3w \u25e6 [ \u03c32xi + \u00b5 2 xi\n1\n] .\n(24)"}, {"heading": "B. Gradient of the data generator\u2019s cost", "text": "In this section we turn to the data generator and we focus on approximating \u2207\u03b8dcd, where cd is defined as in Eq. (11). We can separate cd into the sum of n functions acting on each data sample independently, i.e. cd(\u03b8l,\u03b8d) = \u2211n i=1 c (i) d (\u03b8l,\u03b8 (i) d ), where for each i \u2208 {1, . . . , n}:\nc (i) d (\u03b8l,\u03b8 (i) d ) = E w\u223cpl(\u00b7;\u03b8l)\nxi\u223cpd(\u00b7;\u03b8(i)d )\n[\u03c1d 2 \u2016xi \u2212 x\u0302i\u20162\n+[1 + yi(w\u0303 >xi + b)]+ ] . (25)\nBy exploiting properties of the expectation and since pd(\u00b7;\u03b8(i)d ) is an axis-aligned Gaussian distribution with mean \u00b5xi and standard deviation \u03c3xi , we can simplify Eq. (25) as:\nc (i) d (\u03b8l,\u03b8 (i) d ) = \u03c1d 2 ( \u2016\u00b5xi \u2212 x\u0302i\u20162 + \u2016\u03c3xi\u20162 )\n+ E w\u223cpl(\u00b7;\u03b8l) xi\u223cpd(\u00b7;\u03b8(i)d )\n[ [1 + yi(w\u0303 >xi + b)]+ ] . (26)\nAs in the case of the learner, the expectation is a troublesome term having the same form of (14), except for an inverted sign. We adopt the same approximation used in Sect. III-A to obtain a closed-form function. Accordingly, ti = 1 + yi(w\u0303>xi + b) is assumed to be normally distributed with mean \u00b5ti and \u03c3ti . Then the expectation in Eq. (26) can be approximated as h(\u00b5ti , \u03c3ti), where function h is defined as in Eq. (17). The variance \u03c32ti is equal to \u03c3 2 si (Eq. 16), while \u00b5ti is given by:\n\u00b5ti = E w\u223cpl(\u00b7;\u03b8l) xi\u223cpd(\u00b7;\u03b8(i)d )\n[ 1 + yi(w\u0303 >xi + b) ]\n= 1 + yi(\u00b5 > w\u0303\u00b5xi + \u00b5b) .\nThe sample-wise cost of the data generator (Eq. 26) can thus be approximated as\nc (i) d (\u03b8l,\u03b8 (i) d ) \u2248 Ld(\u00b5xi ,\u03c3xi) = \u03c1d 2 ( \u2016\u00b5xi \u2212 x\u0302i\u20162 + \u2016\u03c3xi\u20162 )\n+ h(\u00b5ti(\u03b8 (i) d ), \u03c3ti(\u03b8 (i) d )) . (27)\nThe corresponding gradient is given by\n\u2202Ld \u2202\u00b5xi\n= (\u00b5xi \u2212 x\u0302i) + \u03c1d ( \u2202h\n\u2202\u00b5ti \u2202\u00b5ti \u2202\u00b5xi + \u2202h \u2202\u03c32ti \u2202\u03c32ti \u2202\u00b5xi\n) ,\n(28)\n\u2202Ld \u2202\u03c3xi = \u03c3xi + \u03c1d\n( \u2202h\n\u2202\u00b5ti \u2202\u00b5ti \u2202\u03c3xi + \u2202h \u2202\u03c32ti \u2202\u03c32ti \u2202\u03c3xi\n) , (29)\nwhere \u2202h\u2202\u00b5ti and \u2202h \u2202\u03c32ti are given as in Eqs. (21)-(22), and\n\u2202\u00b5ti \u2202\u00b5xi = yi\u00b5w\u0303 , \u2202\u00b5ti \u2202\u03c3xi = 0 , (30) \u2202\u03c32ti \u2202\u00b5xi = 2\u03c32w\u0303 \u25e6 \u00b5xi , \u2202\u03c32ti \u2202\u03c3xi = 2\u03c3xi \u25e6 ( \u03c32w\u0303 + \u00b5 2 w\u0303 ) . (31)"}, {"heading": "IV. KERNELIZATION", "text": "Our game, as in Bruckner et al. [11], assumes explicit knowledge of the feature space \u03c6, where the data generator is assumed to randomize the samples x\u0307 = \u03c6(x). However, in many applications, the feature mapping is only implicitly given in terms of a positive semidefinite kernel function k : X \u00d7X \u2192 R that measures the similarity between samples as a scalar product in the corresponding kernel Hilbert space, i.e., there exists \u03c6 : X \u2192 R such that k(x,x\u2032) = \u03c6(x)>\u03c6(x\u2032). Note that in this setting the input space X is not restricted to a vector space like in the previous section (e.g. it might contain graphs or other structured entities).\nFor the representer theorem to hold [39], we assume that the randomized weight vectors of the learner live in the same subspace of the reproducing kernel Hilbert space, i.e., w = \u2211 j \u03b1j\u03c6(x\u0302j), where \u03b1 \u2208 Rn. Analogously, we restrict the randomized samples obtained by the data generator to live in the span of the mapped training instances, i.e., x\u0307i =\u2211n j=1 \u03beij\u03c6(x\u0302j), where \u03bei = (\u03bei1, . . . , \u03bein)\n> \u2208 Rn. Now, instead of randomizing w and X\u0307, we let the data generator and the learner randomize \u039e = (\u03be1, . . . , \u03ben) and \u03b1, respectively. Moreover, we assume that the expected costs c\u0304l/d can be rewritten in terms of \u03b1 and \u039e in a way that involves only inner products of \u03c6(x), to take advantage of the kernel trick. This is clearly possible for the termw>x\u0307i = \u03b1>K\u03bei in (1) and (2), where K is the kernel matrix. Hence, the applicability of the kernel trick only depends on the choice of the regularizers. It is easy to see that due to the linearity of the variable shift, existence and uniqueness of a Nash equilibrium in our kernelized game hold under the same conditions given for the linear case.5\nAlthough the data generator is virtually randomizing strategies in some subspace of the reproducing kernel Hilbert space, in reality manipulations should occur in the original input space. Hence, to construct the real attack samples {xi}ni=1 corresponding to the data generator\u2019s strategy at the Nash equilibrium, one should solve the so-called pre-image problem, inverting the implicit feature mapping \u03c6\u22121(K\u03bei) for each sample. This problem is in general neither convex, nor\n5Note that, on the contrary, manipulating samples directly in the input space would not even guarantee the existence of a Nash equilibrium, as the data generator\u2019s expected cost becomes non-quasi-convex in x for many (nonlinear) kernels, invalidating Theorem 1.\nit admits a unique solution. However, reliable solutions can be easily found using well-principled approximations [11], [39]. It is finally worth remarking that solving the pre-image problem is not even required from the learner\u2019s perspective, i.e., to train the corresponding, secure classification function."}, {"heading": "V. DISCUSSION", "text": "In this section, we report a simple case study on a twodimensional dataset to visually demonstrate the effect of randomized prediction games on SVM-based learners. From a pragmatic perspective, this example suggests also that uniqueness of the Nash Equilibrium should not be taken as a strict requirement in our game.\nAn instance of the proposed randomized prediction game for a linear SVM and for a non-linear SVM with the RBF kernel is reported in Fig. 1. As one may note from the plots, the main effect of simulating the presence of an attacker that manipulates malicious data to evade detection is to cause the linear decision boundary to gradually shift towards the legitimate class, and the nonlinear boundary to find a better enclosing of the legitimate samples. This should generally improve the learner\u2019s robustness to any kind of evasion attempt, as it requires the attacker to mimic more carefully the feature values of legitimate samples \u2013 a task typically harder in several adversarial settings than just obfuscating the content of a malicious sample to make it sufficiently different from the known malicious ones [7], [9].\nBased on this observation, any attempt aiming to satisfy the sufficient conditions for uniqueness of the Nash Equilibrium will result in an increase of the regularization strength in either the learner\u2019s or the attacker\u2019s cost function. Indeed, to satisfy the condition in Lem. 1, one could sufficiently increase \u03c1l, \u03c1d, or both. This amounts to increasing the regularization strength of either players, which in turn reduces in some sense their power. Hence, it should be clear that enforcing satisfaction\nof the sufficient conditions that guarantee the uniqueness of the Nash equilibrium might be counterproductive, by inducing the learner to weakly enclose the legitimate class, either due to a too strong regularization of the learners\u2019 parameters, or by limiting the ability of the attacker to manipulate the malicious samples, thus allowing the leaner to keep a loose boundary. This will in general compromise the quality of the adversarial learning procedure. This argument shares similarities with the idea of addressing non-convex machine learning problems directly, without resorting to convex approximations [37], [38].\nBesides improving classifier robustness, finding a better enclosure of the legitimate class may however cause a higher number of legitimate samples to be misclassified as malicious. There is indeed a trade-off between the desired level of robustness and the fraction of misclassified legitimate samples. The benefit of using randomization here is to make the attacker\u2019s strategy less pessimistic than in the case of static prediction games [10], [11], which should allow us to eventually find a better trade-off between robustness and legitimate misclassifications. This aspect is investigated more systematically in the experiments reported in the next section."}, {"heading": "VI. EXPERIMENTS", "text": "In this section we present a set of experiments on handwritten digit recognition, spam filtering, and PDF malware detection. Despite handwritten digit recognition is not a proper adversarial learning task as spam and malware detection, we consider it in our experiments to provide a visual interpretation of how secure learning algorithms are capable of improving robustness to evasion attacks.\nWe consider only linear classifiers, as they are a typical choice in these settings, and especially in spam filtering [2], [7], [14]. This also allows us to carry out a fair comparison with state-of-the-art secure learning algorithms, as they yield linear classification functions. We compare our secure linear\nSVM learner (Sect. III) with the standard linear SVM implementation [40], and with the state-of-the-art robust classifiers InvarSVM [21], [22], and NashSVM [11] (see Sect. VII).\nThe goal of these experiments is to test whether these secure algorithms work well also under attack scenarios that differ from those hypothesized during design \u2013 a typical setting in security-related tasks; e.g., what happens if game-based classification techniques like that proposed in this paper and NashSVM are used against attackers that exploit a different attack strategy, i.e., attackers that may not act rationally according to the hypothesized objective function? What happens when the attacker does not play at the expected Nash equilibrium? These are rather important questions to address, as we do not have any guarantee that real-world attackers will play according to the hypothesized objective function.\nSecurity evaluation. To address the above issues, we consider the security evaluation procedure proposed in [7]. It evaluates the performance of the considered classifiers under attack scenarios of increasing strength. We consider the True Positive (TP) rate (i.e., the fraction of detected attacks) evaluated at 1% False Positive (FP) rate (i.e., the fraction of misclassified legitimate samples) as performance measure. We evaluate the performance of each classifier in the absence of attack, as in standard performance evaluation techniques, and then start manipulating the malicious test samples to simulate attacks of different strength. We assume a worst-case adversary, i.e., an adversary that has perfect knowledge of the attacked classifier, since we are interested in understanding the worst-case performance degradation. Note however that other choices are possible, depending on specific assumptions on the adversary\u2019s knowledge and capability [7], [13], [14]. In this setting, we assume that the optimal (worst-case) sample manipulation x\u2217 operated by the attacker is obtained by solving the following optimization problem:\nx\u2217 \u2208 arg min x yf(x;w),\ns.t. d(x, x\u0302i) \u2264 dmax, (32)\nwhere y is the malicious class label, d(x,xi) measures the distance between the perturbed sample x and the ith malicious data sample x\u0302i (in this case, we use the `2 norm, as done by the considered classifiers). The maximum amount of modifications is bounded by dmax, which is a parameter representing the attack strength. It is obvious that the more modifications the adversary is allowed to make on the attack samples, the\nhigher the performance degradation incurred by the classifier is expected to be. Accordingly, the performance of more secure classifiers is expected to degrade more gracefully as the attack strength increases [7], [14].\nThe solution of the above problem is trivial when we consider linear classifiers, the Euclidean distance, and x is unconstrained: it amounts to setting x\u2217 = x\u0302i \u2212 ydmax w||w|| . If x lies within some constrained domain, e.g. [0, 1], then one may consider a simple gradient descent with box constraints on x (see, e.g., [9]). If x takes on binary values, e.g., {0, 1}, then the attack amounts to switching from 0 to 1 or vice-versa the value of a maximum of dmax features which have been assigned the highest absolute weight values by the classifier. In particular, if y wk > 0 (y wk < 0) and the k-th feature satisfies x\u0302ik = 1 (x\u0302ik = 0), then x\u2217k = 1 (x \u2217 k = 0) [7], [14].\nParameter selection. The considered methods require setting different parameters. From the learners\u2019 perspective, we have to tune the regularization parameter C for the standard linear SVM and InvarSVM, while we respectively have \u03c1\u22121 and \u03c1l for NashSVM and for our method. In addition, the robust classifiers require setting the parameters of their attacker\u2019s objective. For InvarSVM, we have to set K, i.e., the number of modifiable features, while for NashSVM and for our method, we have to set the value of the regularization parameter \u03c1+1 and \u03c1d, respectively. Further, to guarantee existence of a Nash Equilibrium point, we have to enforce some box constraints on the distribution\u2019s parameters. For the attacker, we restrict the mean of the attack points to lie in [0, 1] (as the considered datasets are normalized in that interval), and their variance in [10\u22123, 0.5]. For the learner, the variance of w is allowed to vary in [10\u22126, 10\u22123], while its mean takes values on [\u2212W,W ], where W is optimized together with the other parameters. All the above mentioned parameters are set by performing a grid-search on the parameter space (C, \u03c1\u22121, \u03c1d \u2208 {0.01, 0.1, 1, 10, 100}; K \u2208 {8, 13, 25, 30, 47, 52, 63}; \u03c1+1, \u03c1d \u2208 {0.01, 0.05, 0.1, 1, 10}; W \u2208 {0.01, 0.05, 0.1, 1}), and retaining the parameter values that maximize the area under the security evaluation curve on a validation set. The reason is to find a parameter configuration for each method that attains the best average robustness over all attack intensities (values of dmax), i.e. the best average TP rate at FP=1%."}, {"heading": "A. Handwritten Digit Recognition", "text": "Similarly to [21], we focus on two two-class sub-problems of discriminating between two distinct digits from the MNIST\n10\ndataset [41], i.e. 1 vs 3, and 6 vs 7, where the second digit in each pair represents the attacking class (y = +1). The digits are originally represented as gray-scale images of 28 \u00d7 28 pixels. They are simply mapped to feature vectors by ordering the pixels in raster scan order. The overall number of features is thus d = 784. We normalize each feature (pixel value) in [0, 1], by dividing its value by 255. We build a training and a validation set of 1,000 samples each by randomly sampling the original training data available for MNIST. As for the test set, we use the default set provided with this data, which consists of approximately 1,000 samples for each digit (class). The results averaged on 5 repetitions are shown in the first and second plot of Fig. 2 respectively for 1 vs 3, and 6 vs 7. As one may notice, in the absence of attack (i.e. when dmax = 0), all classifiers achieved comparable performance (the TP rate is almost 100% for all of them), due to a conservative choice of the operating point (FP=1%), that should also guarantee a higher robustness against the attack. In the presence of attack, our approach (RNashSVM) exhibits comparable performance to NashSVM on the problem of discriminating 1 vs 3, and to InvarSVM on 6 vs 7. NashSVM outperforms the standard SVM implementation in both cases, but exhibits lower security (robustness) than InvarSVM on 6 vs 7, despite the attacker\u2019s regularizer in InvarSVM is not even based on the `2 norm.\nFinally, in Fig. 3 we report two attack samples (a digit from class 1 and one from class 6) and show how they are obfuscated by the attack strategy of Eq. (32) to evade detection against each classifier. Notice how the original attacking samples (1 and 6) tend to resemble more the corresponding attacked classes (3 and 7) when natively robust classifiers are used. This visual example confirms the analysis of Sect. V, i.e., that higher robustness is achieved when the adversary is required to mimic the feature values of samples of the legitimate class, instead of slightly modifying the attack samples to differentiate them from the rest of the malicious data."}, {"heading": "B. Spam Filtering", "text": "In these experiments we use the benchmark, publicly available, TREC 2007 email corpus [42], which consists of 75,419 real emails (25,220 legitimate and 50,199 spam messages) collected between April and July 2007. We exploit the bagof-words feature model, in which each binary feature denotes the absence (0) or presence (1) of the corresponding word in a given email [7], [11], [13], [14]. Features (words) are extracted from training emails using the tokenization method\nof the widely-known anti-spam filter SpamAssassin,6 and then n = 1, 000 distinct features are selected using a supervised feature selection approach based on the information gain criterion [43]. We build a training and a validation set of 1,000 samples each by randomly sampling the first 5,000 emails in chronological order of the original dataset, while a test set of about 2,000 samples is randomly sampled from the subsequent set of 5,000 emails. The results averaged on 5 repetitions are shown in the third plot of Fig. 2. As in the previous case, in the absence of attack (dmax = 0) all the classifiers exhibit a very high (and similar) performance. However, as the attack intensity (dmax) increases, their performance degrades more or less gracefully, i.e. their robustness to the attack is different. Surprisingly, one may notice that only InvarSVM and RNashSVM exhibited an improved level of security. The reason is that these two classifiers are able to find a more uniform set of weights than SVM and NashSVM, and, in this case, this essentially requires the adversary to manipulate a higher number of features to significantly decrease the value of the classifier\u2019s discriminant function. Note that a similar result has been heuristically found also in [13], [14]."}, {"heading": "C. PDF Malware Detection", "text": "We consider here another relevant application example in computer security, i.e., the detection of malware in PDF files. The main reason behind the diffusion of malware in PDF files is that they exhibit a very flexible structure that allows embedding several kinds of resources, including Flash, JavaScript and even executable code. Resources simply consists of keywords that denote their type, and of data streams that contain the actual object; e.g., an embedded resource in a PDF file may be encoded as follows:\n13 0 obj << /Kids [ 1 0 R 11 0 R ] /Type /Page ... >> end obj\nwhere keywords are highlighted in bold face. Recent work has exploited machine learning techniques to discriminate between malicious and legitimate PDF files, based on the analysis of their structure and, in particular, of the embedded keywords [44]\u2013[48]. We exploit here a similar feature representation to that proposed in [45], where each feature denotes the presence of a given keyword in the PDF file. We collected 5993 recent malware samples from the Contagio dataset,7 and 5951 benign samples from the web. Following the procedure described in [45], we extracted 114 keywords from the first 1,000 samples (in chronological order) to build our feature set. Then, we build training, validation and test sets as in the spam filtering case, and average results over 5 repetitions. Attacks in this case are simulated by allowing the attacker only to increase the feature values of malicious samples, which corresponds to adding the constraint x \u2265 x\u0302i (where the inequality holds for all features) to Problem 32. The reason is that removing objects (and keywords) from malicious PDFs may compromise the intrusive nature of the embedded exploitation code, whereas adding objects can be easily done through the PDF versioning mechanism [9], [46], [48].\n6http://spamassassin.apache.org 7http://contagiodump.blogspot.it\n11\nResults are shown in the 4th plot of Fig. 2. The considered methods mostly exhibit the same behavior shown in the spam filtering case, besides the fact that, here, there is a clearer trade-off between the performance in the absence of attack, and robustness under attack. In particular, InvarSVM and RNashSVM are significantly more robust under attack (i.e., when dmax > 0) than SVM and NashSVM, at the expense of a slightly worsened detection rate in the absence of attack (i.e., when dmax = 0).\nTo summarize, the reported experiments show that, even if the attacker does not play the expected attack strategy at the Nash equilibrium, most of the proposed state-of-the-art secure classifiers are still able to outperform classical techniques, and, in particular, that the proposed RNashSVM classifier may guarantee an even higher level of robustness. Understanding how this property relates to the use of probability distributions over the set of the classifier\u2019s and of the attacker\u2019s strategies remains an interesting future question."}, {"heading": "VII. RELATED WORK", "text": "The problem of devising secure classifiers against different kinds of manipulation of samples at test time has been widely investigated in previous work [1], [6], [10], [11], [21]\u2013[27]. Inspired by the seminal work by Dalvi et al. [1], several authors have proposed a variety of modifications to existing learning algorithms to improve their security against different kinds of attack. Globerson et al. [21], [22] have formulated the so-called Invariant SVM (InvarSVM) in terms of a minimax approach (i.e., a zero-sum game) to deal with worst-case feature manipulations at test time, including feature addition, deletion, and rescaling. This work has been further extended in [23] to allow features to have different a-priori importance levels, instead of being manipulated equally likely. Notably, more recent research has also considered the development of secure learning algorithms based on zero-sum games for sensor networks, including distributed secure algorithms [27] and algorithms for detecting adversarially-corrupted sensors [26].\nThe rationale behind shifting from zero-sum to non-zerosum games for adversarial learning is that the classifier and the attacker may not necessarily aim at maximizing antagonistic objective functions. This in turn implies that modeling the problem as a zero-sum game may lead one to design overlypessimistic classifiers, as pointed out in [11]. Even considering a non-zero-sum Stackelberg game may be too pessimistic, since the attacker (follower) is supposed to move after the classifier (leader), while having full knowledge of the chosen classification function (which again is not realistic in practical settings) [11], [24]. For these reasons, Bru\u0308ckner et al. [10], [11] have formalized adversarial learning as a non-zero-sum game, referred to as static prediction game. Assuming that the players act simultaneously (conversely to Stackelberg games [24]), they devised conditions under which a unique Nash equilibrium for this game exists, and developed algorithms for learning the corresponding robust classifiers, including the socalled NashSVM. Our work essentially extends this approach by introducing randomization over the players\u2019 strategies.\nFor completeness, we also mention here that in [25] Bayesian games for adversarial regression tasks have been\nrecently proposed. In such games, uncertainty on the objective function\u2019s parameters of either player is modeled by considering a probability distribution over their possible values. To the best of our knowledge, this is the first attempt towards modeling the uncertainty of the attacker and the classifier on the opponent\u2019s objective function."}, {"heading": "VIII. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we have extended the work in [11] by introducing randomized prediction games. To operate this shift, we have considered parametrized, bounded families of probability distributions defined over the set of pure strategies of either players. The underlying idea, borrowed from [3], [6], [28], consists of randomizing the classification function to make the attacker select a less effective attack strategy. Our experiments, conducted on an handwritten digit recognition task and on realistic application examples involving spam and malware detection, show that competitive, secure SVM classifiers can be learnt using our approach, even when the conditions behind uniqueness of the Nash equilibrium may not hold, i.e., when the attacker may not play according to the objective function hypothesized for her by the classifier. This mainly depends on the particular kind of decision function learnt by the learning algorithm under our game setting, which tends to find a better \u2018enclosing\u2019 of the legitimate class. This generally requires the attacker to make more modifications to the malicious samples to evade detection, regardless of the attack strategy chosen. We can thus argue that the proposed methods exhibit robustness properties particularly suited to adversarial learning tasks. Moreover, the fact that the proposed methods may perform well also when the Nash equilibrium is not guaranteed to be unique suggests us that the conditions behind its uniqueness may hold under less restrictive assumptions (e.g., when the SVM admits a unique solution [35], [36]). We thus leave a deeper investigation of this aspect to future work.\nAnother interesting extension of this work may be to apply randomized prediction games in the context of unsupervised learning, and, in particular, clustering algorithms. It has been recently shown that injecting a small percentage of wellcrafted poisoning attack samples into the input data may significantly subvert the clustering process, compromising the subsequent data analysis [49], [50]. In this respect, we believe that randomized prediction games may help devising secure countermeasures to mitigate the impact of such attacks; e.g., by explicitly modeling the presence of poisoning samples (generated according to a probability distribution chosen by the attacker) during the clustering process.\nIt is worth finally mentioning that our work is also slightly related to previous work on security games, in which the goal of the defender is to adopt randomized strategies to protect his or her assets from the attacker, by allocating a limited number of defensive resources; e.g., police officers for airport security, protection mechanisms for network security [51]\u2013[53]. Although our game is not directly concerned to the protection of a given set of assets, we believe that investigating how to bridge the proposed approach within this well-grounded field of study may provide promising research\n12\ndirections for future work, e.g., in the context of network security [52], [53], or for suggesting better user attitudes towards security issues [54]. This may also suggest interesting theoretical advancements; e.g., to establish conditions for the equivalence of Nash and Stackelberg games [51], and to address issues related to the uncertainty on the players\u2019 strategies, or on their (sometimes bounded) rationality, e.g., through the use of Bayesian games [25], security strategies and robust optimization [52], [53]. Another suggestion to overcome the aforementioned issues is to exploit higher-level models of the interactions between attackers and defenders in complex, real-world problems; e.g., through the use of replicator equations to model adversarial dynamics in securityrelated tasks [55]. Exploiting conformal prediction may be also an interesting research direction towards improving current adversarial learning systems [56]. To conclude, we believe these are all relevant research directions for future work."}], "references": [{"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 2617\u2013 2654, September 2012. 1det [  aI B> B dI  ] = det(aI) det(dI \u2212 1 a BB>) and if USU> is the eigendecomposition of BB> then the latter deterimnant becomes det(U(dI \u2212 1 a  S)U>) = det(dI\u2212 1 a  S)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Evaluating security of machine learning against such attacks and devising suitable countermeasures, are two among the main open issues under investigation in the field of adversarial machine learning [1]\u2013[11].", "startOffset": 200, "endOffset": 203}, {"referenceID": 0, "context": ", in spam filtering, where spammers manipulate the content of spam emails to get them past the anti-spam filters [1], [2], [12]\u2013[14], or in malware detection, where hackers obfuscate malicious software (malware, for short) to evade detection of either known or zero-day exploits [8], [9], [15], [16].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "To date, several authors have addressed the problem of designing secure learning algorithms to mitigate the impact of evasion attacks [1], [6], [10], [11], [21]\u2013[27] (see Sect.", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "[0, 1], then one may consider a simple gradient descent with box constraints on x (see, e.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "For the attacker, we restrict the mean of the attack points to lie in [0, 1] (as the considered datasets are normalized in that interval), and their variance in [10\u22123, 0.", "startOffset": 70, "endOffset": 76}, {"referenceID": 0, "context": "We normalize each feature (pixel value) in [0, 1], by dividing its value by 255.", "startOffset": 43, "endOffset": 49}, {"referenceID": 0, "context": "The problem of devising secure classifiers against different kinds of manipulation of samples at test time has been widely investigated in previous work [1], [6], [10], [11], [21]\u2013[27].", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "[1], several authors have proposed a variety of modifications to existing learning algorithms to improve their security against different kinds of attack.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "In spam and malware detection, attackers exploit randomization to obfuscate malicious data and increase their chances of evading detection at test time; e.g., malware code is typically obfuscated using random strings or byte sequences to hide known exploits. Interestingly, randomization has also been proposed to improve security of learning algorithms against evasion attacks, as it results in hiding information about the classifier to the attacker. Recent work has proposed game-theoretical formulations to learn secure classifiers, by simulating different evasion attacks and modifying the classification function accordingly. However, both the classification function and the simulated data manipulations have been modeled in a deterministic manner, without accounting for any form of randomization. In this work, we overcome this limitation by proposing a randomized prediction game, namely, a non-cooperative game-theoretic formulation in which the classifier and the attacker make randomized strategy selections according to some probability distribution defined over the respective strategy set. We show that our approach allows one to improve the trade-off between attack detection and false alarms with respect to state-of-the-art secure classifiers, even against attacks that are different from those hypothesized during design, on application examples including handwritten digit recognition, spam and malware detection.", "creator": "LaTeX with hyperref package"}}}