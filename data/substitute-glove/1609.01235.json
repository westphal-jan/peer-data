{"id": "1609.01235", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2016", "title": "PMI Matrix Approximations with Applications to Neural Language Modeling", "abstract": "The negative data (NEG) logical function, used in word2vec, that 's nonstandard among the Noise Contrastive Estimation (NCE) function. NEG was only the there highly ways and emphasis continuous familiar relating. However, different NCE, although under considered inapplicable three part purpose of example the parameters of a religion model. In though works, we misleading always contradict then centers a principled meaning for NEG - acquisition particular storyboard, 1983 through takes novel technical on with rising - sphere approximation following way matrix much computable enhance information sides to differ such the anticipating words. The obtained different utilizing is similar related to NCE english coupe but is include early a streamlined objective appropriate. We exists provide a leninist formulation put number called idiom raw improve, basic word formula_11 on similar modeling, product down the NEG objective formula_3. Experimental decision opening two includes interpretation analyze benchmarks saw more exasperation results, with a which regain allowed NEG five NCE.", "histories": [["v1", "Mon, 5 Sep 2016 17:47:49 GMT  (16kb)", "http://arxiv.org/abs/1609.01235v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["oren melamud", "ido dagan", "jacob goldberger"], "accepted": false, "id": "1609.01235"}, "pdf": {"name": "1609.01235.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["melamuo@cs.biu.ac.il", "dagan@cs.biu.ac.il", "goldbej@eng.biu.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n01 23\n5v 1\n[ cs\n.C L\n] 5\nS ep"}, {"heading": "1 Introduction", "text": "Statistical language models (LMs) are crucial components in many speech and text processing systems designed for tasks, such as speech recognition and machine translation. Language models learn to predict the probability of a word given a context of preceding words. Traditional LMs were based on word n-grams counts and therefore limited in the scope of the considered context for reasons of data sparsity. However, Recurrent Neural Network (RNN) language models, which can consider arbitrarily long contexts, have shown consistent performance improvements, recently outperforming n-gram LMs across a range of tasks.\nAn important practical issue associated with neural-network LMs is the high computational cost incurred. The key factor that limits the scalability of traditional neural LMs is the computation of the normalization term in the softmax output layer, whose cost is linearly proportional to the size of the word vocabulary. This has a significant impact on both training and testing, even when employing modern GPUs, and especially when a large vocabulary is used. Several approaches have been proposed to cope with this scaling issue, including importance sampling [3], hierarchical softmax [19] and Noise Contrastive Estimation (NCE) [9]. NCE reduces the language model estimation problem to the problem of estimating the parameters of a probabilistic binary classifier that distinguishes between samples from the empirical distribution and samples generated by the noise distribution. This method significantly reduces training time, by making the cost of training independent of the size of the vocabulary, yet still requires the expensive normalization at test time. NCE has been applied to train neural LMs with large vocabularies [20] and was also recently successfully used to train LSTM-RNN LMs (see e.g. [23] [5] [26]).\nIn a related research line, continuous word embeddings have proven useful in many NLP tasks. In particular, the skip-gram embedding model with the negative sampling (NEG) objective function [18] as implemented in the word2vec toolkit, has become one of the most popular models today.\nThis is mainly attributed to its scalability to large volumes of data, which is critical for learning highquality embeddings. Indeed, recent studies have obtained state-of-the-art results by using skip-gram embeddings on a variety of natural language processing tasks, such as named entity resolution and dependency parsing [21, 2, 17]. The same embedding approach can be used for sentence representation [13] and context representation [16]. Recently, Levy and Goldberg [15] offered some motivation for skip-gram\u2019s NEG objective function, showing that by maximizing this function the skip-gram algorithm implicitly factorizes a word-context pointwise mutual information (PMI) matrix.\nInterestingly, the NEG objective function, commonly used for learning word embeddings, is a simplified version of the respective NCE function that is used in language modeling.\nThe main difference is that while both NCE and NEG objective functions include the neural representations of context and predicted words, NCE includes in addition the numerical word probabilities in the noise distribution. Dyer [7] argued that although NEG and NCE are superficially similar, NCE is a general parameter estimation technique that is asymptotically unbiased, while NEG is most useful for learning word representations, but not as a general-purpose estimator. In other words, the claim was that the simplified NEG objective is not applicable for language modeling tasks, where you want the output layer to be a probability distribution.\nIn this study we show that the NEG objective function, in spite of its simplicity, is suitable for training an unbiased language model estimator. We present a derivation of a NEG-based language modeling algorithm that is founded on an extension of the observation of Levy and Goldberg [15], which showed the relation between the skip-gram algorithm and PMI matrix factorization. We thus provide a unified formulation for two main language processing tasks, namely word embedding and language modeling, based on the NEG objective function.\nThe obtained NEG language modeling algorithm can be viewed as a variant of the NCE algorithm. It has a simplified objective function formulation which allows it to avoid heuristic components and initialization procedures that are used in various implementation of NCE language models [23] [5] [26]. We compare NEG to NCE language modeling by evaluating their perplexity measures on two standard language model datasets with different training data and vocabulary size. Our results show that they perform comparably with a small advantage to NEG over NCE.\nThe remainder of this paper is organized as follows. In Section 2 we provide a formal derivation for how learning Euclidean embeddings with the NEG objective function can be used to approximate any discrete joint distribution by means of PMI. In Section 3 we apply this general principle to language modeling and formalize our proposed NEG LMs. In Section 4 we describe the relation of NEG LMs to NCE LMs, and in Section 5 we compare empirically the performance of LSTM-based NEG and NCE language model implementations."}, {"heading": "2 Euclidean Embedding of a Joint Distribution", "text": "In this section we extend the skip-gram with negative sampling (SGNS) word embedding algorithm [18] to a general setup of embedding a discrete joint distribution. We also extend the algorithm analysis of Levy and Goldberg [15] and provide an explicit expression for the quality of the PMI matrix approximation obtained by the embedding algorithm. We later show in Section 3 how this generic PMI approximation concept is applied to the task of language modeling and converted to the desired conditional probability estimates.\nLet X and Y be two random variables defined on alphabets AX and AY , respectively, with a joint distribution p(x, y). We want to find embeddings ~x, ~y \u2208 Rd for every x \u2208 AX and y \u2208 AY , that best reflect the joint distribution of X and Y in the sense defined below. We can represent a given d-dimensional embedding by a |AX | \u00d7 |AY | matrix m such that m(x, y) = ~x \u00b7 ~y. The rank of the embedding matrix m is (at most) d.\nWe define the score of a given embedding m of AX and AY to be:\nS(m) = \u2211\nx,y\nfx,y(m(x, y)) = \u2211\nx,y\nfx,y(~x \u00b7 ~y) (1)\nsuch that\nfx,y(z) = 1\nk+1 (p(x, y) log \u03c3(z) + kp(x)p(y) log \u03c3(\u2212z)), x \u2208 AX , y \u2208 AY , z \u2208 R (2)\nwhere \u03c3() is the sigmoid function and k is a positive integer. The objective function S(m) can be viewed as a log-likelihood function of a binary logistic regression classifier that treats a sample from a joint distribution p(x, y) as a positive instance, and independent samples from the two marginal distributions as a negative instance, while k is the proportion between negative and positive instances. Given a fixed embedding dimensionality d, the optimal embedding m is the one that maximizes the objective function S(m).\nThe (shifted version of the) pointwise mutual information (PMI) function is defined as:\npmi(x, y) = log p(x, y)\np(x)p(y) \u2212 log k, x \u2208 AX , y \u2208 AY . (3)\nWe denote hereafter the matrix whose cell-entries are the pmi values as the PMI matrix. It can be easily verified that fx,y(z) is a concave function of z and the global maximum is obtained at z = pmi(x, y). Hence, for each pair (x, y) \u2208 |AX | \u00d7 |AY | we obtain that fx,y(~x \u00b7 ~y) \u2264 fx,y(pmi(x, y)). Summing over all the pairs we obtain that the global optimum of the embedding score function is obtained at the PMI matrix:\nS(m) = \u2211\nx,y\nfx,y(m(x, y)) \u2264 \u2211\nx,y\nfx,y(pmi(x, y)) = S(pmi). (4)\nThe optimal d-rank embedding matrix m is the best d-rank matrix approximation of the PMI matrix.\nWe next derive an explicit description of the approximation criterion that quantifies the gap between S(m) and S(pmi). For each matrix m we define a distribution onAX\u00d7AY \u00d7 {0, 1}:\npm(x, y, 1) = q(x, y)\u03c3(m(x, y)), pm(x, y, 1) = q(x, y)(1 \u2212 \u03c3(m(x, y)))\nsuch that q(x, y) is the following mixture distribution:\nq(x, y) = 1\nk+1 (p(x, y) + kp(x)p(y)), x \u2208 AX , y \u2208 AY . (5)\nApplying Bayes\u2019 rule we obtain:\nppmi(1|x, y) = \u03c3(pmi(x, y)) = p(x, y)\np(x, y) + kp(x)p(y) (6)\nTheorem 1: Every real valued |AX | \u00d7 |AY | matrix m satisfies:\nS(pmi)\u2212 S(m) = KL(ppmi(z|x, y)||pm(z|x, y)) = \u2211\nx,y,z\nppmi(x, y, z) log ppmi(z|x, y)\npm(z|x, y) . (7)\nProof: It can be easily verified from Eq. (6) that:\np(x, y)\nk+1 =\nq(x, y)p(x, y)\np(x, y) + kp(x)p(y) = q(x, y)\u03c3(pmi(x, y)) = q(x, y)ppmi(1|x, y) (8)\nand in a similar way kp(x)p(y)\nk+1 = q(x, y)ppmi(0|x, y). (9)\nThe definition of the score function S(m) (1) implies that:\nS(pmi)\u2212 S(m) = 1\nk+1\n\u2211\nx,y\n(p(x, y) log \u03c3(pmi(x, y)) \u03c3(m(x, y)) + kp(x)p(y) log \u03c3(\u2212pmi(x, y)) \u03c3(\u2212m(x, y)) ) (10)\nSubstituting Eq. (8) and (9) in Eq. (10) yields:\n= \u2211\nx,y\nq(x, y) \u2211\nz=0,1\nppmi(z|x, y) log ppmi(z|x, y)\npm(z|x, y) (11)\nand finally using the definition of conditional KL divergence [6] we obtain:\n= KL(ppmi(z|x, y)||pm(z|x, y)).\u2737 (12)\nThe definition of the joint distribution pm(x, y, z) implies that marginal (x, y) distribution is the same for all the matrices m:\npm(x, y) = ppmi(x, y) = q(x, y).\nHence, we can equivalently state Theorem 1 using un-conditional KL divergence:\nS(pmi)\u2212 S(m) = KL(ppmi(x, y, z)||pm(x, y, z)).\nTheorem 1 implies that the optimal d-dimensional embedding (which maximizes the embedding score S(m)) minimizes the KL divergence between the distributions defined by the embedding matrix m and the PMI matrix. Note that since KL is always non-negative we obtain as a by-product another proof that the embedding score S(m) (1) is maximized by the PMI matrix.\nThe embedding problem we address here can be viewed as a matrix factorization of the PMI matrix that best reflects the joint distribution p(x, y). Previous works have suggested other criteria for matrix factorization such as least-squares [8] and KL-divergence between the original matrix and the low-rank matrix approximation [14]. In our setup we look for the best low-rank approximation of the PMI matrix, based on the KL-divergence criterion stated in Eq. (7), that best reflects the joint distribution of X and Y .\nLevy and Goldberg [15] showed that SGNS\u2019s objective achieves its maximal value when for each word-pair x, y the inner product of the embedding vectors ~x \u00b7 ~y = pmi(x, y). We derived here the same result for the general setting of embedding any joint distribution. The result in [15], However, tells us nothing about the lower dimensional case where the embedding algorithm is actually interesting since at that case the PMI matrix factorisation is forced to compress the joint distribution and thereby learn a meaningful embedding. In contrast, we provided above an explicit KL divergence expression (7) for the low-dimensional matrix-factorization criterion that is optimized by the SGNS algorithm.\nAssume that we do not know the exact joint distribution. We only have, instead, a set of samples {(xt, yt)} from the joint distribution p(x, y). We can use a Montecarlo approximation of the expectation computed by the score (1). To compute the first term of (2) we only go over the pairs (xt, yt) that appear in the training set. If the alphabet sizes are large, it is not feasible to compute the second term of (2). Instead, we can approximate the expectation by sampling of \u2018negative\u2019 examples. The negative examples are created for each pair (xt, yt) by drawing k random samples from the empirical marginal distribution p\u0302(y). The objective function (1) is thus approximated as:\nSsgns = \u2211\nt\n(log \u03c3(~xt \u00b7 ~yt) +\nk\u2211\ni=1\nlog \u03c3(\u2212~xt \u00b7 ~yi)) (13)\nsuch that yi are i.i.d. samples from the empirical marginal distribution p\u0302(y).\nThe SGNS word embedding algorithm [18] aims to represent each word x and each context word y as d-dimensional vectors ~x and ~y such that words that are \u201csimilar\" to each other will have similar vector representations. Applying the objective function (13) to the word co-occurrence statistics, we obtain the NEG objective function maximized by the SGNS algorithm.\nIn all the derivations above we were focused on approximating the PMI matrix. We can apply a similar analysis to obtain a d-rank matrix approximation of the log conditional distribution log p(y|x). Define\nScond = \u2211\nx,y\nfx,y(~x \u00b7 ~y \u2212 log(kp(x))). (14)\nThen Scond is optimized when ~x \u00b7 ~y \u2212 k log p(x) = pmi(x, y), i.e. when ~x \u00b7 ~y = log p(y|x). Hence, the d-rank matrix that optimizes the objective function Scond is the best d-rank matrix approximation of the log conditional distribution matrix."}, {"heading": "3 Language modeling based on PMI Approximation", "text": "In this section we apply the embedding algorithm described above to the joint distribution of a word w and its left-side context c. We utilize the connection between the optimal embedding and the PMI\nmatrix factorization to construct an approximation of the conditional distribution p(w|c). As a result we obtain an efficient algorithm for learning a language model.\nThe joint distribution of a word sequence satisfies the chain rule:\nlog p(w1, ..., wn) =\nn\u2211\ni=1\nlog p(wi|ci) (15)\nsuch that ci = (w1, ..., wi\u22121) is the left-side context ofwi. We use a simple lookup table for the word representation and an LSTM recurrent neural network to obtain a left-side context representation. We train the word and left-side context embeddings to maximize the objective (13):\nS = \u2211\nw,c\n(log \u03c3(~w \u00b7 ~c) +\nk\u2211\ni=1\nlog \u03c3(\u2212~ui \u00b7 ~c)) (16)\nsuch that w and c go over all the words and contexts in a given corpus and u1, ..., uk are words independently sampled from the unigram distribution. We showed above that by optimizing this objective we obtain the best low-dimensional approximation of the PMI matrix associated with the joint distribution of the word and its context. Hence, if the embedding dimensionality is large enough we get a good approximation:\n~w \u00b7 ~c \u2248 pmi(w, c) = log p(w|c)\np(w) \u2212 log(k) (17)\nwhich yields the following estimation of the conditional distribution p(w|c):\np\u0302(w|c) \u221d exp(~w \u00b7 ~c)p(w). (18)\nThus, we finally obtain a parametric language modeling, while avoiding the softmax operation over the entire vocabulary. At test time we need to multiply the output of the parametric model by a unigram word distribution and apply softmax over the vocabulary to obtain a normalized conditional distribution.\nFollowing [18] we can sample negative instances from a smooth unigram distribution p\u03b1(w) such that 0 \u2264 \u03b1 \u2264 1. By tuning \u03b1, one can interpolate smoothly between sampling popular words, as advocated by the unigram distribution, and sampling all words equally. Each \u03b1 entails a different embedding. The optimized embedding based on negative sampling from a smooth unigram distribution satisfies:\n~w \u00b7 ~c \u2248 log p(w|c)\np\u03b1(w) \u2212 log k. (19)\nNote that once we set a value for \u03b1 during training we also need to use the same value of \u03b1 at test time to compute the conditional probability p\u0302(w|c).\nIn our approach we use the same negative sampling approach used by the SGNS word embedding algorithm. To correctly use the learned embeddings for estimating conditional probabilities at test time we multiply the parametric model by the unigram probability of the predicted word. We denote our approach of using negative sampling for language modeling as NEGLM."}, {"heading": "4 Connection to Noise Contrastive Estimation", "text": "The language modeling algorithms that are most related to our approach are those that are based on the Noise Contrastive Estimation (NCE) principle [9]. There are several applications of NCE to language modeling that mainly differ from each other by the neural network architecture used to produce a parametric representation for the left-side context of the predicted word [26] [23] [5]. In this section we first briefly review the NCE algorithm in the context of language modeling and then explain the difference between the standard implementation of NCE and our approach, which can be viewed as a simplified variant of NCE. NCE transforms the parameter learning problem into a binary classifier training problem. Let p(w|c) be the probability of a word w given a context c and let pn(w) be a \u2018noise\u2019 word distribution (e.g. a unigram distribution). The NCE approach assumes that the word w is sampled from a mixture distribution 1\nk+1 (p(w|c) + kpn(w)) such that the noise samples\nare k times more frequent than samples from the \u2018true\u2019 distribution p(w|c). Let y be a binary random variable such that y = 0 and y = 1 corresponds to a noise sample and a \u2018true\u2019 sample, respectively, i.e. p(w|c, y = 0) = pn(w) and p(w|c, y = 1) = p(w|c). Assume the distribution of word w given a context c has the following parametric form:\np\u03b8(w|c) = 1\nZc exp(~w \u00b7 ~c+ bw) (20)\nsuch that ~w and ~c are vector representations of the word w and its context c. Applying Bayes rule it can be easily verified that:\npnce(y = 1|w, c) = \u03c3(~w \u00b7 ~c+ bw \u2212 logZc \u2212 log(kpn(w))) (21)\nwhere \u03c3() is the sigmoid function. NCE uses Eq. (21) as an objective to train a binary classifier that decides which distribution was used to sample w. For each pair (w, c) from the training corpus we sample k words u1, ..., uk from the noise distribution pn(w) to create k negative examples (u1, c), ..., (uk, c). The objective function optimized by the NCE is:\nSnce = \u2211\nw,c\n(log p(y = 1|w, c) +\nk\u2211\ni=1\nlog p(y = 0|uk, c)) (22)\nIn principle, to find the model parameters, we need to estimate the normalization factor function Zc for each context c. However, it was found empirically [20] that setting Zc = 1 didn\u2019t hurt the performance (see also theoretical analysis in [1]). Chen et al. [5] reported that setting log(Zc) = 9 gave them the best results. In any case, once the NCE classifier is trained, at test time we still need to apply softmax (20) over the vocabulary to obtain a normalized conditional distribution.\nMikolov et al. [18] suggested the negative sampling (NEG) training procedure, which is a simplified version of the objective function optimized by NCE. The main difference between NEG and NCE is that NCE objective function comprises both samples and an explicit description of the noise distribution, while NEG uses only samples. The binary classification score used by NEG (for word embedding) as well as by our proposed NEGLM is:\npneg(y = 1|w, c) = \u03c3(~w \u00b7 ~c). (23)\nComparing (21) and (23), the NEG objective function is simpler and is a more stable input to the sigmoid function, since it is easier to guarantee that the input to the sigmoid is concentrated around zero with low variance. It can be viewed as an automatic implicit batch normalization of the input to the non-linear activation function. If we consider the NEG objective function (23) simply as an approximation of the NCE objective function, while using the same NCE formulation at test time we obtain poor results (in terms of perplexity). However, unlike NEG, we apply the correct procedure at test time, which is multiplying the output of the parametric model by the unigram word distribution. The three possible LM training strategies are summarized in Table 1.\nWe can also view our approach not just as an approximation, but also as a special case of NCE by setting bw = log(kp(w)) in the NCE objective function (21). We can address this in a systematic way. In the NCE we use a parametric model for the \u2018true\u2019 distribution p\u03b8(w|c) = 1Zc exp(~w \u00b7~c+bw). Assume that we instead apply NCE on a parametric model for the quotient of the true distribution and the noise distribution:\np\u03b8(w|c)\npn(w) =\n1\nZc exp(~w \u00b7 ~c). (24)\nApplying the NCE formulation defined above on (24) we obtain:\np(y = 1|w, c) = \u03c3(~w \u00b7 ~c\u2212 logZc) (25)\nSetting Zc = 1 for all context c we finally obtain the objective function we optimize (16).\nAs mentioned above, it was found empirically [20] that setting Zc = 1 in the NCE objective function didn\u2019t hurt the performance. The connection we established between NCE and matrix approximation can provide a simple explanation for that empirical observation. The NCE can be viewed as an algorithm that finds the best low-rank approximation of the log p(w|c) matrix (see Eq. 14). Assume we obtain the best d-dimensional NCE approximation with word and context embeddings ~w,~c \u2208 Rd and a normalization factor Zc = \u2211 w exp(~w \u00b7 ~c + bw). We can form a (d+1)-rank embedding"}, {"heading": "5 Experiments", "text": "We evaluate the performance of the language modeling methods discussed in this paper, using the popular perplexity measure on two standard datasets. We denote the NCE language model as NCE. We use the heuristics that worked well in [23][26], initializing NCE\u2019s bias term from Equation 21 to bw = \u2212 log |V |, where V is the word vocabulary, and using Zc = 1. We denote our proposed language model as NEGLM. We also evaluate a variant of our model, denoted NEGLM-B, where we adopt the learned bias term used in NCE. In this model p(y = 1|w, c) = \u03c3(~w \u00b7 ~c+ bw) and the bias term bw is initialized to zero. We note that the goal of the evaluation described in this section, is to compare between the discussed language model variants under the same terms. We do not compare our results to state-of-the-art as we do not have sufficient computational resources to learn models that are large enough to be that much competitive. Neural network training typically requires making several hyperparameter choices. We describe these choices below and stress that for fair comparison, we followed prior best practices, while never optimizing these hyperparameters in favor of NEGLM. All of the models described hereafter were implemented using the Chainer toolkit [22].1\nThe first dataset that we used is the Penn Tree Bank (PTB). A version of this dataset that is commonly used to evaluate language models is available from Tomas Mikolov.2 It consists of 929K training words, 73K validation words and 82K test words with a 10K word vocabulary. To build and train all compared models in this setting, we followed [25], which achieved excellent results on this dataset. Specifically, we used a 2-layer 300-hidden-units3 LSTM with a 50% dropout ratio, to represent the left-side context of a predicted word. We represent end-of-sentence as a special <eos> token and predict this token like any other word. During training we perform truncated back-propagationthrough-time, unrolling the LSTM for 20 steps at a time without ever resetting the LSTM state. We train our model for 39 epochs using Stochastic Gradient Descent (SGD) with learning rate of 1, which is decreased by a factor of 1.2 after every epoch starting after epoch 6. We clip the norms of the gradient to 5 and use mini-batch size of 20. We set the negative sampling parameter to k = 100 following [26], which showed highly competitive performance with NCE language models trained with this number of samples.\nAs the second dataset, we used the much larger WMT 1B-word benchmark, 4 introduced by [4]. This dataset comprises about 0.8B training words and has a held-out set partitioned into 50 subsets. The test set is the first subset in the held-out, comprising 159K words, including the <eos> tokens. We used the second subset as our validation set with 165K words. The original vocabulary size of this dataset is 0.8M words after converting all words that occur less than 3 times in the corpus to an <unk> token. However, we follow [24][10] and trim the vocabulary further to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources. To build and train our models we use a similar method to the one used with PTB,\n1We will make our code publicly available. 2http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz 3[25] use larger models with more units. 4http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.ta\nwith the following differences. We use a single-layer 512-hidden-unit LSTM to represent the lefthand context. We follow [11], which found a 10% dropout rate to be sufficient for relatively small models fitted to this large training corpus. We train our model for just one epoch using the Adam optimizer [12] with default parameters, which we found to converge more quickly and effectively than SGD. We use mini-batch size of 1000.\nThe perplexity results achieved by the compared models on the two test sets appear in Table 2. As can be seen, the performance of all models is comparable with NEGLM models outperforming the NCE model by a small margin. Furthermore, we see that our NEGLM model performs as well or slightly better than the one enhanced with a learned bias component (NEGLM-B). This suggests that the learning of our NEGLM model is robust as is without such a component."}, {"heading": "6 Conclusions", "text": "In this work we first derived an information-theoretic foundation for the relation between negative sampling and PMI approximation. This analysis extends previous analysis of Levy and Goldberg [15] to the case of lower dimensional PMI matrix approximation. We have shown that the simplified negative sampling (NEG) objective function, popularized in the context of learning word representations, can be used to learn parametric language models (NEGLMs) as well, as long as the correct procedure is followed at test time. Thus we provided a unified approach based on PMI approximation for both word embedding and language modeling. Analyzing the relation between our proposed NEGLMs and NCE language models, we have shown that while they are very closely related, NEGLMs have the advantage of a simpler objective function, while NCE LMs require some heuristic measures to achieve such stability. Empirical evaluations showed that in out settings, NEGLMs slightly outperform NCE LMs on two popular perplexity measure benchmarks. As an additional contribution, we provide a formal derivation to how any discrete joint distribution can be approximated by means of learning continuous embeddings with the negative sampling objective function. We use this to derive our NEGLM model, but independently this also provides an alternative interpretation for the NCE learning method."}], "references": [{"title": "When and why are log-linear models selfnormalizing", "author": ["J. Andreas", "D. Klein"], "venue": "In NAACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "Association for Computational Linguistics (ACL)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Y. Bengio", "J. Senecal"], "venue": "In AISTATS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson"], "venue": "arXiv preprint arXiv:1312.3005", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition", "author": ["X. Chen", "X. Liu", "M. Gales", "P.C. Woodland"], "venue": "ICASSP", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Notes on noise contrastive estimation and negative sampling", "author": ["C. Dyer"], "venue": "arXiv preprint arXiv:1410.8251", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The approximation of one matrix by another of lower rank", "author": ["C. Eckart", "G. Young"], "venue": "Psychometrika, 1:211\u2013218", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1936}, {"title": "Noise-contrastive estimation of unnormalized statistical models", "author": ["M.U. Gutmann", "A. Hyvarinen"], "venue": "with applications to natural image statistics. Journal of Machine Learning Research, 13:307\u2013361", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["S. Ji", "S. Vishwanathan", "N. Satish", "A. Nadathur", "J. Michael", "P. Dubey"], "venue": "ICLR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "arXiv preprint arXiv:1602.02410", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Skipthought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Proceedings of NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithms for nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "context2vec: Learning generic context embedding with bidirectional LSTM", "author": ["O. Melamud", "J. Goldberger", "I. Dagan"], "venue": "Proceedings of CONLL", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "The role of context types and dimensionality in learning word embeddings", "author": ["O. Melamud", "D. McClosky", "S. Patwardhan", "M. Bansal"], "venue": "Proceedings of NAACL", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Minh", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["A. Passos", "V. Kumar", "A. McCallum"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "EMNLP", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Scaling recurrent neural network language models", "author": ["W. Williams", "N. Prasad", "D. Mrva", "T. Ash", "T. Robinson"], "venue": "ICASSP", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple", "author": ["B. Zoph", "A. Vaswani", "J. May", "K. Knight"], "venue": "fast noise-contrastive estimation for large RNN vocabularies. In NAACL", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Several approaches have been proposed to cope with this scaling issue, including importance sampling [3], hierarchical softmax [19] and Noise Contrastive Estimation (NCE) [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 18, "context": "Several approaches have been proposed to cope with this scaling issue, including importance sampling [3], hierarchical softmax [19] and Noise Contrastive Estimation (NCE) [9].", "startOffset": 127, "endOffset": 131}, {"referenceID": 8, "context": "Several approaches have been proposed to cope with this scaling issue, including importance sampling [3], hierarchical softmax [19] and Noise Contrastive Estimation (NCE) [9].", "startOffset": 171, "endOffset": 174}, {"referenceID": 19, "context": "NCE has been applied to train neural LMs with large vocabularies [20] and was also recently successfully used to train LSTM-RNN LMs (see e.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "[23] [5] [26]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[23] [5] [26]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 24, "context": "[23] [5] [26]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "In particular, the skip-gram embedding model with the negative sampling (NEG) objective function [18] as implemented in the word2vec toolkit, has become one of the most popular models today.", "startOffset": 97, "endOffset": 101}, {"referenceID": 20, "context": "Indeed, recent studies have obtained state-of-the-art results by using skip-gram embeddings on a variety of natural language processing tasks, such as named entity resolution and dependency parsing [21, 2, 17].", "startOffset": 198, "endOffset": 209}, {"referenceID": 1, "context": "Indeed, recent studies have obtained state-of-the-art results by using skip-gram embeddings on a variety of natural language processing tasks, such as named entity resolution and dependency parsing [21, 2, 17].", "startOffset": 198, "endOffset": 209}, {"referenceID": 16, "context": "Indeed, recent studies have obtained state-of-the-art results by using skip-gram embeddings on a variety of natural language processing tasks, such as named entity resolution and dependency parsing [21, 2, 17].", "startOffset": 198, "endOffset": 209}, {"referenceID": 12, "context": "The same embedding approach can be used for sentence representation [13] and context representation [16].", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "The same embedding approach can be used for sentence representation [13] and context representation [16].", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "Recently, Levy and Goldberg [15] offered some motivation for skip-gram\u2019s NEG objective function, showing that by maximizing this function the skip-gram algorithm implicitly factorizes a word-context pointwise mutual information (PMI) matrix.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "Dyer [7] argued that although NEG and NCE are superficially similar, NCE is a general parameter estimation technique that is asymptotically unbiased, while NEG is most useful for learning word representations, but not as a general-purpose estimator.", "startOffset": 5, "endOffset": 8}, {"referenceID": 14, "context": "We present a derivation of a NEG-based language modeling algorithm that is founded on an extension of the observation of Levy and Goldberg [15], which showed the relation between the skip-gram algorithm and PMI matrix factorization.", "startOffset": 139, "endOffset": 143}, {"referenceID": 21, "context": "It has a simplified objective function formulation which allows it to avoid heuristic components and initialization procedures that are used in various implementation of NCE language models [23] [5] [26].", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "It has a simplified objective function formulation which allows it to avoid heuristic components and initialization procedures that are used in various implementation of NCE language models [23] [5] [26].", "startOffset": 195, "endOffset": 198}, {"referenceID": 24, "context": "It has a simplified objective function formulation which allows it to avoid heuristic components and initialization procedures that are used in various implementation of NCE language models [23] [5] [26].", "startOffset": 199, "endOffset": 203}, {"referenceID": 17, "context": "In this section we extend the skip-gram with negative sampling (SGNS) word embedding algorithm [18] to a general setup of embedding a discrete joint distribution.", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "We also extend the algorithm analysis of Levy and Goldberg [15] and provide an explicit expression for the quality of the PMI matrix approximation obtained by the embedding algorithm.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "and finally using the definition of conditional KL divergence [6] we obtain:", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "Previous works have suggested other criteria for matrix factorization such as least-squares [8] and KL-divergence between the original matrix and the low-rank matrix approximation [14].", "startOffset": 92, "endOffset": 95}, {"referenceID": 13, "context": "Previous works have suggested other criteria for matrix factorization such as least-squares [8] and KL-divergence between the original matrix and the low-rank matrix approximation [14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "Levy and Goldberg [15] showed that SGNS\u2019s objective achieves its maximal value when for each word-pair x, y the inner product of the embedding vectors ~x \u00b7 ~y = pmi(x, y).", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "The result in [15], However, tells us nothing about the lower dimensional case where the embedding algorithm is actually interesting since at that case the PMI matrix factorisation is forced to compress the joint distribution and thereby learn a meaningful embedding.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "The SGNS word embedding algorithm [18] aims to represent each word x and each context word y as d-dimensional vectors ~x and ~y such that words that are \u201csimilar\" to each other will have similar vector representations.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "Following [18] we can sample negative instances from a smooth unigram distribution p(w) such that 0 \u2264 \u03b1 \u2264 1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "The language modeling algorithms that are most related to our approach are those that are based on the Noise Contrastive Estimation (NCE) principle [9].", "startOffset": 148, "endOffset": 151}, {"referenceID": 24, "context": "There are several applications of NCE to language modeling that mainly differ from each other by the neural network architecture used to produce a parametric representation for the left-side context of the predicted word [26] [23] [5].", "startOffset": 221, "endOffset": 225}, {"referenceID": 21, "context": "There are several applications of NCE to language modeling that mainly differ from each other by the neural network architecture used to produce a parametric representation for the left-side context of the predicted word [26] [23] [5].", "startOffset": 226, "endOffset": 230}, {"referenceID": 4, "context": "There are several applications of NCE to language modeling that mainly differ from each other by the neural network architecture used to produce a parametric representation for the left-side context of the predicted word [26] [23] [5].", "startOffset": 231, "endOffset": 234}, {"referenceID": 19, "context": "However, it was found empirically [20] that setting Zc = 1 didn\u2019t hurt the performance (see also theoretical analysis in [1]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "However, it was found empirically [20] that setting Zc = 1 didn\u2019t hurt the performance (see also theoretical analysis in [1]).", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "[5] reported that setting log(Zc) = 9 gave them the best results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] suggested the negative sampling (NEG) training procedure, which is a simplified version of the objective function optimized by NCE.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "As mentioned above, it was found empirically [20] that setting Zc = 1 in the NCE objective function didn\u2019t hurt the performance.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "We use the heuristics that worked well in [23][26], initializing NCE\u2019s bias term from Equation 21 to bw = \u2212 log |V |, where V is the word vocabulary, and using Zc = 1.", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "We use the heuristics that worked well in [23][26], initializing NCE\u2019s bias term from Equation 21 to bw = \u2212 log |V |, where V is the word vocabulary, and using Zc = 1.", "startOffset": 46, "endOffset": 50}, {"referenceID": 23, "context": "To build and train all compared models in this setting, we followed [25], which achieved excellent results on this dataset.", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "We set the negative sampling parameter to k = 100 following [26], which showed highly competitive performance with NCE language models trained with this number of samples.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "As the second dataset, we used the much larger WMT 1B-word benchmark, 4 introduced by [4].", "startOffset": 86, "endOffset": 89}, {"referenceID": 22, "context": "However, we follow [24][10] and trim the vocabulary further to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "However, we follow [24][10] and trim the vocabulary further to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources.", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "tgz [25] use larger models with more units.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "We follow [11], which found a 10% dropout rate to be sufficient for relatively small models fitted to this large training corpus.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "We train our model for just one epoch using the Adam optimizer [12] with default parameters, which we found to converge more quickly and effectively than SGD.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "This analysis extends previous analysis of Levy and Goldberg [15] to the case of lower dimensional PMI matrix approximation.", "startOffset": 61, "endOffset": 65}], "year": 2016, "abstractText": "The negative sampling (NEG) objective function, used in word2vec, is a simplification of the Noise Contrastive Estimation (NCE) method. NEG was found to be highly effective in learning continuous word representations. However, unlike NCE, it was considered inapplicable for the purpose of learning the parameters of a language model. In this study, we refute this assertion by providing a principled derivation for NEG-based language modeling, founded on a novel analysis of a low-dimensional approximation of the matrix of pointwise mutual information between the contexts and the predicted words. The obtained language modeling is closely related to NCE language models but is based on a simplified objective function. We thus provide a unified formulation for two main language processing tasks, namely word embedding and language modeling, based on the NEG objective function. Experimental results on two popular language modeling benchmarks show comparable perplexity results, with a small advantage to NEG over NCE.", "creator": "LaTeX with hyperref package"}}}