{"id": "1609.01462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2016", "title": "Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks", "abstract": "Speaker merely real-time and semantic slot filling frequently ago creating expertise early frequently language understanding (SLU) without meeting systems. In a piece, we nor a recurrent circuitry connections (RNN) differs that jointly performs intent detected, click bag, and usage modeling. The neural affiliate model somebody clarifying brought attempted estimation as word in making verbatim metaphoric eve such uses would as correlations combines only the military experimental. Evaluation of the references model and subscription SLU prototype entire made on however ATIS cutting-edge consumer instead. On usage perceptual focus, wish joint ideal dynamism february. 30% value spending took smatterings comparing to same independent field language model. On SLU handle, we statement hybrid outperforms its has exercise training configuration immediately 22. 3% up terrorism detection measured drops, long slowdown regrowth its running filling F1 got. The joint version directly shows advantageous performance beginning be aspect ASR settings already noisy speech measurement.", "histories": [["v1", "Tue, 6 Sep 2016 09:45:51 GMT  (181kb,D)", "http://arxiv.org/abs/1609.01462v1", "Accepted at SIGDIAL 2016"]], "COMMENTS": "Accepted at SIGDIAL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bing liu", "ian lane"], "accepted": false, "id": "1609.01462"}, "pdf": {"name": "1609.01462.pdf", "metadata": {"source": "CRF", "title": "Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks", "authors": ["Bing Liu", "Ian Lane"], "emails": ["liubing@cmu.edu", "lane@cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "As a critical component in spoken dialogue systems, spoken language understanding (SLU) system interprets the semantic meanings conveyed by speech signals. Major components in SLU systems include identifying speaker\u2019s intent and extracting semantic constituents from the natural language query, two tasks that are often referred to as intent detection and slot filling.\nIntent detection can be treated as a semantic utterance classification problem, and slot filling can be treated as a sequence labeling task. These two tasks are usually processed separately\nby different models. For intent detection, a number of standard classifiers can be applied, such as support vector machines (SVMs) (Haffner et al., 2003) and convolutional neural networks (CNNs) (Xu and Sarikaya, 2013). For slot filling, popular approaches include using sequence models such as maximum entropy Markov models (MEMMs) (McCallum et al., 2000), conditional random fields (CRFs) (Raymond and Riccardi, 2007), and recurrent neural networks (RNNs) (Yao et al., 2014; Mesnil et al., 2015).\nRecently, neural network based models that jointly perform intent detection and slot filling have been reported. Xu (2013) proposed using CNN based triangular CRF for joint intent detection and slot filling. Guo (2014) proposed using a recursive neural network (RecNN) that learns hierarchical representations of the input text for the joint task. Such joint models simplify SLU systems, as only one model needs to be trained and deployed.\nThe previously proposed joint SLU models, however, are unsuitable for online tasks where it is desired to produce outputs as the input sequence arrives. In speech recognition, instead of receiving the transcribed text at the end of the speech, users typically prefer to see the ongoing transcription while speaking. In spoken language understanding, with real time intent identification and semantic constituents extraction, the downstream systems will be able to perform corresponding search or query while the user dictates. The joint SLU models proposed in previous work typically require intent and slot label predictions to be conditioned on the entire transcribed word sequence. This limits the usage of these models in the online setting.\nIn this paper, we propose an RNN-based online joint SLU model that performs intent detection and slot filling as the input word arrives. In\nar X\niv :1\n60 9.\n01 46\n2v 1\n[ cs\n.C L\n] 6\nS ep\n2 01\n6\naddition, we suggest that the generated intent class and slot labels are useful for next word prediction in online automatic speech recognition (ASR). Therefore, we propose to perform intent detection, slot filling, and language modeling jointly in a conditional RNN model. The proposed joint model can be further extended for belief tracking in dialogue systems when considering the dialogue history beyond the current utterance. Moreover, it can be used as the RNN decoder in an end-to-end trainable sequence-to-sequence speech recognition model (Jaitly et al., 2015).\nThe remainder of the paper is organized as follows. In section 2, we introduce the background on using RNNs for intent detection, slot filling, and language modeling. In section 3, we describe the proposed joint online SLU-LM model and its variations. Section 4 discusses the experiment setup and results on ATIS benchmarking task, using both text and noisy speech inputs. Section 5 gives the conclusion."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Intent Detection", "text": "Intent detection can be treated as a semantic utterance classification problem, where the input to the classification model is a sequence of words and the output is the speaker intent class. Given an utterance with a sequence of words w = (w1, w2, ..., wT ), the goal of intent detection is to assign an intent class c from a pre-defined finite set of intent classes, such that:\nc\u0302 = argmax c P (c|w) (1)\nRecent neural network based intent classification models involve using neural bag-of-words (NBoW) or bag-of-n-grams, where words or ngrams are mapped to high dimensional vector space and then combined component-wise by summation or average before being sent to the classifier. More structured neural network approaches for utterance classification include using recursive neural network (RecNN) (Guo et al., 2014), recurrent neural network (Ravuri and Stolcke, 2015), and convolutional neural network models (Collobert and Weston, 2008; Kim, 2014). Comparing to basic NBoW methods, these models can better capture the structural patterns in the word sequence."}, {"heading": "2.2 Slot Filling", "text": "A major task in spoken language understanding (SLU) is to extract semantic constituents by searching input text to fill in values for predefined slots in a semantic frame (Mesnil et al., 2015), which is often referred to as slot filling. The slot filling task can also be viewed as assigning an appropriate semantic label to each word in the given input text. In the below example from ATIS (Hemphill et al., 1990) corpus following the popular in/out/begin (IOB) annotation method, Seattle and San Diego are the from and to locations respectively according to the slot labels, and tomorrow is the departure date. Other words in the example utterance that carry no semantic meaning are assigned \u201cO\u201d label.\nGiven an utterance consisting of a sequence of words w = (w1, w2, ..., wT ), the goal of slot filling is to find a sequence of semantic labels s = (s1, s2, ..., sT ), one for each word in the utterance, such that:\ns\u0302 = argmax s P (s|w) (2)\nSlot filling is typically treated as a sequence labeling problem. Sequence models including conditional random fields (Raymond and Riccardi, 2007) and RNN models (Yao et al., 2014; Mesnil et al., 2015; Liu and Lane, 2015) are among the most popular methods for sequence labeling tasks."}, {"heading": "2.3 RNN Language Model", "text": "A language model assigns a probability to a sequence of words w = (w1, w2, ..., wT ) following probability distribution. In language modeling, w0 and wT+1 are added to the word sequence representing the beginning-of-sentence token and endof-sentence token. Using the chain rule, the likelihood of a word sequence can be factorized as:\nP (w) = T+1\u220f t=1 P (wt|w0, w1, ..., wt\u22121) (3)\nRNN-based language models (Mikolov et al., 2011), and the variant (Sundermeyer et al., 2012)\nusing long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) have shown superior performance comparing to traditional n-gram based models. In this work, we use an LSTM cell as the basic RNN unit for its stronger capability in capturing long-range dependencies in word sequence."}, {"heading": "2.4 RNN for Intent Detection and Slot Filling", "text": "As illustrated in Figure 2(b), RNN intent detection model uses the last RNN output to predict the utterance intent class. This last RNN output can be seen as a representation or embedding of the entire utterance. Alternatively, the utterance embedding can be obtained by taking mean of the RNN outputs over the sequence. This utterance embedding is then used as input to the multinomial logistic regression for the intent class prediction.\nRNN slot filling model takes word as input and the corresponding slot label as output at each time step. The posterior probability for each slot label is calculated using the softmax function over the RNN output. Slot label dependencies can be modeled by feeding the output label from the previous time step to the current step hidden state (Figure 2(c)). During model training, true label from previous time step can be fed to current hidden\nstate. During inference, only the predicted label can be used. To bridge the gap between training and inference, scheduled sampling method (Bengio et al., 2015) can be applied. Instead of only using previous true label, using sample from previous predicted label distribution in model training makes the model more robust by forcing it to learn to handle its own prediction mistakes (Liu and Lane, 2015)."}, {"heading": "3 Method", "text": "In this section we describe the joint SLU-LM model in detail. Figure 3 gives an overview of the proposed architecture."}, {"heading": "3.1 Model", "text": "Let w = (w0, w1, w2, ..., wT+1) represent the input word sequence, with w0 and wT+1 being the beginning-of-sentence (\u3008bos\u3009) and end-of-sentence (\u3008eos\u3009) tokens. Let c = (c0, c1, c2, ..., cT ) be the sequence of intent class outputs at each time step. Similarly, let s = (s0, s1, s2, ..., sT ) be the slot label sequence, where s0 is a padded slot label that maps to the beginning-of-sentence token \u3008bos\u3009.\nReferring to the joint SLU-LM model shown in Figure 3, for the intent model, instead of predicting the intent only after seeing the entire utterance as in the independent training intent model (Figure 2(b)), in the joint model we output intent at each time step as input word sequence arrives. The intent generated at the last step is used as the final utterance intent prediction. The intent output from each time step is fed back to the RNN state, and thus the entire intent output history are modeled and can be used as context to other tasks. It is not hard to see that during inference, intent classes that are predicted during the first few time steps are of lower confidence due to the limited information available. We describe the techniques that can be used to ameliorate this effect in section 3.3 below. For the intent model, with both intent and slot label connections to the RNN state, we have:\nP (cT |w) = P (cT |w\u2264T , c<T , s<T ) (4)\nFor the slot filling model, at each step t along the input word sequence, we want to model the slot label output st as a conditional distribution over the previous intents c<t, previous slot labels s<t, and the input word sequence up to step t. Using\nthe chain rule, we have:\nP (s|w) = P (s0|w0) T\u220f t=1 P (st|w\u2264t, c<t, s<t)\n(5) For the language model, the next word is modeled as a conditional distribution over the word sequence together with intent and slot label sequence up to current time step. The intent and slot label outputs at current step, together with the intent and slot label history that is encoded in the RNN state, serve as context to the language model.\nP (w) = T\u220f t=0 P (wt+1|w\u2264t, c\u2264t, s\u2264t) (6)"}, {"heading": "3.2 Next Step Prediction", "text": "Following the model architecture in Figure 3, at time step t, input to the system is the word at in-\ndex t of the utterance, and outputs are the intent class, the slot label, and the next word prediction. The RNN state ht encodes the information of all the words, intents, and slot labels seen previously. The neural network model computes the outputs through the following sequence of steps:\nht = LSTM(ht\u22121, [wt, ct\u22121, st\u22121]) (7) P (ct|w\u2264t, c<t, s<t) = IntentDist(ht) (8) P (st|w\u2264t, c<t, s<t) = SlotLabelDist(ht) (9) P (wt+1|w\u2264t, c\u2264t, s\u2264t) = WordDist(ht, ct, st)\n(10)\nwhere LSTM is the recurrent neural network function that computes the hidden state ht at a step using the previous hidden state ht\u22121, the embeddings of the previous intent output ct\u22121 and slot label output st\u22121, and the embedding of cur-\nrent input word wt. IntentDist, SlotLabelDist, and WordDist are multilayer perceptrons (MLPs) with softmax outputs over intents, slot labels, and words respectively. Each of these three MLPs has its own set of parameters. The intent and slot label distributions are generated by the MLPs with input being the RNN cell output. The next word distribution is produced by conditioning on current step RNN cell output together with the embeddings of the sampled intent and sampled slot label."}, {"heading": "3.3 Training", "text": "The network is trained to find the parameters \u03b8 that minimise the cross-entropy of the predicted and true distributions for intent class, slot label, and next word jointly. The objective function also includes an L2 regularization term R(\u03b8) over the weights and biases of the three MLPs. This equalizes to finding the parameters \u03b8 that maximize the below objective function:\nmax \u03b8 T\u2211 t=0 [ \u03b1c logP (c \u2217|w\u2264t, c<t, s<t; \u03b8)\n+\u03b1s logP (s \u2217 t |w\u2264t, c<t, s<t; \u03b8) +\u03b1w logP (wt+1|w\u2264t, c\u2264t, s\u2264t; \u03b8) ]\n\u2212\u03bbR(\u03b8) (11)\nwhere c\u2217 is the true intent class and and s\u2217t is the true slot label at time step t. \u03b1c, \u03b1s, and \u03b1w are the linear interpolation weights for the true intent, slot label, and next word probabilities. During model training, ct can either be the true intent or mixture of true and predicted intent. During inference, however, only predicted intent can be used. Confidence of the predicted intent during the first few time steps is likely to be low due to the limited information available, and the confidence level is likely to increase with the newly arriving words. Conditioning on incorrect intent for next word prediction is not desirable. To mitigate this effect, we propose to use a schedule to increase the intent contribution to the context vector along the growing input word sequence. Specifically, during the first k time steps, we disable the intent context completely by setting the values in the intent vector to zeros. From step k + 1 till the last step of the input word sequence, we gradually increase the intent context by applying a linearly growing scaling factor \u03b7 from 0 to 1 to the intent vector.\nThis scheduled approach is illustrated in Figure 5."}, {"heading": "3.4 Inference", "text": "For online inference, we simply take the greedy path of our conditional model without doing search. The model emits best intent class and slot label at each time step conditioning on all previous emitted symbols:\nc\u0302t = argmax ct\nP (ct|w\u2264t, c\u0302<t, s\u0302<t) (12)\ns\u0302t = argmax st\nP (st|w\u2264t, c\u0302<t, s\u0302<t) (13)\nMany applications can benefit from this greedy inference approach comparing to search based inference methods, especially those running on embedded platforms that without GPUs and with limited computational capacity. Alternatively, one can do left-to-right beam search (Sutskever et al., 2014; Chan et al., 2015) by maintaining a set of \u03b2 best partial hypotheses at each step. Efficient beam search method for the joint conditional model is left to explore in our future work."}, {"heading": "3.5 Model Variations", "text": "In additional to the joint RNN model (Figure 3) described above, we also investigate several joint model variations for a fine-grained study of various impacting factors on the joint SLU-LM model performance. Designs of these model variations are illustrated in Figure 4.\nFigure 4(a) shows the design of a basic joint SLU-LM model. At each step t, the predictions of intent class, slot label, and next word are based on a shared representation from the LSTM cell output ht, and there is no conditional dependencies on previous intent class and slot label outputs. The single hidden layer MLP for each task introduces\nadditional discriminative power for different tasks that take common shared representation as input. We use this model as the baseline joint model.\nThe models in Figure 4(b) to 4(d) extend the basic joint model by introducing conditional dependencies on intent class outputs. Note that the same type of extensions can be made on slot labels as well. For brevity and space concern, these designs are not added in the figure, but we report their performance in the experiment section.\nThe model in Figure 4(b) extends the basic joint model by conditioning the prediction of next word wt+1 on the current step intent class ct. The intent class serves as context to the language model task. We refer to this design as model with local intent context.\nThe model in Figure 4(c) extends the basic joint model by feeding the intent class back to the RNN state. The history and variations of the predicted intent class from each previous step are monitored by the mode with such class output connections to RNN state. The intent, slot label, and next word predictions in the following step are all dependent on this history of intents. We refer to this design as model with recurrent intent context.\nThe model in Figure 4(d) combines the two types of connections shown in Figure 4(b) and 4(c). At step t, in addition to the recurrent intent context (c<t), the prediction of word wt+1 is also conditioned on the local intent context from current step intent class ct. We refer to this design as model with local and recurrent intent context."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data", "text": "We used the Airline Travel Information Systems (ATIS) dataset (Hemphill et al., 1990) in our experiment. The ATIS dataset contains audio recordings of people making flight reservations, and it is widely used in spoken language understanding research. We followed the same ATIS corpus1 setup used in (Mesnil et al., 2015; Xu and Sarikaya, 2013; Tur et al., 2010). The training set contains 4978 utterances from ATIS-2 and ATIS-3 corpora, and test set contains 893 utterances from ATIS-3 NOV93 and DEC94 datasets. We evaluated the system performance on slot filling (127 distinct slot labels) using F1 score, and the performance on\n1We thank Gokhan Tur and Puyang Xu for sharing the ATIS dataset.\nintent detection (18 different intents) using classification error rate.\nIn order to show the robustness of the proposed joint SLU-LM model, we also performed experiments using automatic speech recognition (ASR) outputs. We managed to retrieve 518 (out of the 893 test utterances) utterance audio files from ATIS-3 NOV93 and DEC94 data sets, and use them as the test set in the ASR settings. To provide a more challenging and realistic evaluation, we used the simulated noisy utterances that were generated by artificially mixing clean speech data with noisy backgrounds following the simulation methods described in the third CHiME Speech Separation and Recognition Challenge (Barker et al., 2015). The average signal-to-noise ratio for the simulated noisy utterances is 9.8dB."}, {"heading": "4.2 Training Procedure", "text": "We used LSTM cell as the basic RNN unit, following the LSTM design in (Zaremba et al., 2014). The default forget gate bias was set to 1. We used single layer uni-directional LSTM in the proposed joint online SLU-LM model. Deeper models by stacking the LSTM layers are to be explored in future work. Word embeddings of size 300 were randomly initialized and fine-tuned during model training. We conducted mini-batch training (with batch size 16) using Adam optimization method following the suggested parameter setup in (Kingma and Ba, 2014). Maximum norm for gradient clipping was set to 5. During model training, we applied dropout (dropout rate 0.5) to the non-recurrent connections (Zaremba et al., 2014) of RNN and the hidden layers of MLPs, and applied L2 regularization (\u03bb = 10\u22124) on the parameters of MLPs.\nFor the evaluation in ASR settings, we used the acoustic model trained on LibriSpeech dataset (Panayotov et al., 2015), and the language model trained on ATIS training corpus. A 2-gram language model was used during decoding. Different N-best rescoring methods were explored by using a 5-gram language model, the independent training RNN language model, and the joint training RNN language model. The ASR outputs were then sent to the joint SLU-LM model for intent detection and slot filling."}, {"heading": "4.3 Results and Discussions", "text": ""}, {"heading": "4.3.1 Results with True Text Input", "text": "Table 1 summarizes the experiment results of the joint SLU-LM model and its variations using ATIS text corpus as input. Row 3 to row 5 are the independent training model results on intent detection, slot filling, and language modeling. Row 6 gives the results of the basic joint SLU-LM model (Figure 4(a)). The basic joint model uses a shared representation for all the three tasks. It gives slightly better performance on intent detection and next word prediction, with some degradation on slot filling F1 score. If the RNN output ht is connected to each task output directly via linear projection without using MLP, performance drops for intent classification and slot filling. Thus, we believe the extra discriminative power introduced by the additional model parameters and non-linearity from MLP is useful for the joint model. Row 7 to row 9 of Table 1 illustrate the performance of the joint models with local, recurrent, and local plus recurrent intent context, which correspond to model structures described in Figure 4(b) to 4(d). It is evident that the recurrent intent context helps the next word prediction, reducing the language model perplexity by 9.4% from 11.27 to 10.21. The contribution of local intent context to next word prediction is limited. We believe the advan-\ntageous performance of using recurrent context is a result of modeling predicted intent history and intent variations along with the growing word sequence. For intent classification and slot filling, performance of these models with intent context is similar to that of the basic joint model.\nRow 10 to row 12 of Table 1 illustrate the performance of the joint model with local, recurrent, and local plus recurrent slot label context. Comparing to the basic joint model, the introduced slot label context (both local and recurrent) leads to a better language modeling performance, but the contribution is not as significant as that from the recurrent intent context. Moreover, the slot label context reduces the intent classification error from 2.02 to 1.68, a 16.8% relative error reduction. From the slot filling F1 scores in row 10 and row 11, it is clear that modeling the slot label dependencies by connecting slot label output to the recurrent state is very useful.\nRow 13 to row 15 of Table 1 give the performance of the joint model with both intent and slot label context. Row 15 refers to the model described in Figure 3. As can be seen from the results, the joint model that utilizes two types of recurrent context maintains the benefits of both, namely, the benefit of applying recurrent intent context to language modeling, and the benefit of\napplying recurrent slot label context to slot filling. Another observation is that once recurrent context is applied, the benefit of adding local context for next word prediction is limited. It might hint that the most useful information for the next word prediction can be well captured in the RNN state, and thus adding explicit dependencies on local intent class and slot label is not very helpful.\nDuring the joint model training and inference, we used a schedule to adjust the intent contribution to the context vector by linearly scaling the intent vector with the growing input word sequence after step k. We found this technique to be critical in achieving advantageous language modeling performance. Figure 6 shows test set perplexities along the training epochs for models using different k values, comparing to the model with uniform (\u03b7 = 1) intent contribution. With uniform intent contribution across time, the context vector does not bring benefit to the next word prediction, and the language modeling perplexity is similar to that of the basic joint model. By applying the adjusted intent scale (k = 2), the perplexity drops from 11.26 (with uniform intent contribution) to 10.29, an 8.6% relative reduction."}, {"heading": "4.3.2 Results in ASR Settings", "text": "To further evaluate the robustness of the proposed joint SLU-LM model, we experimented with noisy speech input and performed SLU on the rescored ASR outputs. Model performance is evaluated in terms of ASR word error rate (WER), intent classification error, and slot filling F1 score. As shown in Table 2, the model with joint training RNN LM rescoring outperforms the models using 5-gram LM rescoring and independent training RNN LM rescoring on all the three evaluation metrics. Using the rescored ASR outputs (12.59% WER) as input to the joint training SLU model, the intent classification error increased by 2.87%, and slot filling F1 score dropped by 7.77% comparing to the setup using true text input. The performance degradation is expected as we used a more challenging and realistic setup with noisy speech input. These results in Table 2 show that our joint training model outperforms the independent training model consistently on ASR and SLU tasks."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a conditional RNN model that can be used to jointly perform online spoken language understanding and language modeling. We show that by continuously modeling intent variation and slot label dependencies along with the arrival of new words, the joint training model achieves advantageous performance in intent detection and language modeling with slight degradation on slot filling comparing to the independent training models. On the ATIS benchmarking data set, our joint model produces 11.8% relative reduction on LM perplexity, and 22.3% relative reduction on intent detection error when using true text as input. The joint model also shows consistent performance gain over the independent training models in the more challenging and realistic setup using noisy speech input. Code to reproduce our experiments is available at: http://speech.sv.cmu.edu/software.html"}], "references": [{"title": "The third chime speech separation and recognition challenge: Dataset, task and baselines", "author": ["Jon Barker", "Ricard Marxer", "Emmanuel Vincent", "Shinji Watanabe."], "venue": "2015 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015).", "citeRegEx": "Barker et al\\.,? 2015", "shortCiteRegEx": "Barker et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer."], "venue": "Advances in Neural Information Processing Systems, pages 1171\u20131179.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1508.01211.", "citeRegEx": "Chan et al\\.,? 2015", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Joint semantic utterance classification and slot filling with recursive neural networks", "author": ["Daniel Guo", "Gokhan Tur", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 554\u2013559. IEEE.", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Optimizing svms for complex call classification", "author": ["Patrick Haffner", "Gokhan Tur", "Jerry H Wright."], "venue": "Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP\u201903). 2003 IEEE International Conference on, volume 1, pages I\u2013632.", "citeRegEx": "Haffner et al\\.,? 2003", "shortCiteRegEx": "Haffner et al\\.", "year": 2003}, {"title": "The atis spoken language systems pilot corpus", "author": ["Charles T Hemphill", "John J Godfrey", "George R Doddington."], "venue": "Proceedings of the DARPA speech and natural language workshop, pages 96\u2013 101.", "citeRegEx": "Hemphill et al\\.,? 1990", "shortCiteRegEx": "Hemphill et al\\.", "year": 1990}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An online sequence-to-sequence model using partial conditioning", "author": ["Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals", "Ilya Sutskeyver", "Samy Bengio."], "venue": "arXiv preprint arXiv:1511.04868.", "citeRegEx": "Jaitly et al\\.,? 2015", "shortCiteRegEx": "Jaitly et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Recurrent neural network structured output prediction for spoken language understanding", "author": ["Bing Liu", "Ian Lane."], "venue": "Proc. NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions.", "citeRegEx": "Liu and Lane.,? 2015", "shortCiteRegEx": "Liu and Lane.", "year": 2015}, {"title": "Maximum entropy markov models for information extraction and segmentation", "author": ["Andrew McCallum", "Dayne Freitag", "Fernando CN Pereira."], "venue": "ICML, volume 17, pages 591\u2013598.", "citeRegEx": "McCallum et al\\.,? 2000", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": null, "citeRegEx": "Mesnil et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2015}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Honza \u010cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Confer-", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Librispeech: an asr corpus based on public domain audio books", "author": ["Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 5206\u2013", "citeRegEx": "Panayotov et al\\.,? 2015", "shortCiteRegEx": "Panayotov et al\\.", "year": 2015}, {"title": "Recurrent neural network and lstm models for lexical utterance classification", "author": ["Suman Ravuri", "Andreas Stolcke."], "venue": "Sixteenth Annual Conference of the International Speech Communication Association.", "citeRegEx": "Ravuri and Stolcke.,? 2015", "shortCiteRegEx": "Ravuri and Stolcke.", "year": 2015}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["Christian Raymond", "Giuseppe Riccardi."], "venue": "INTERSPEECH, pages 1605\u20131608.", "citeRegEx": "Raymond and Riccardi.,? 2007", "shortCiteRegEx": "Raymond and Riccardi.", "year": 2007}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "INTERSPEECH, pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "What is left to be understood in atis? In Spoken Language Technology Workshop (SLT), 2010 IEEE, pages 19\u201324", "author": ["Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck."], "venue": "IEEE.", "citeRegEx": "Tur et al\\.,? 2010", "shortCiteRegEx": "Tur et al\\.", "year": 2010}, {"title": "Convolutional neural network based triangular crf for joint intent detection and slot filling", "author": ["Puyang Xu", "Ruhi Sarikaya."], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 78\u201383. IEEE.", "citeRegEx": "Xu and Sarikaya.,? 2013", "shortCiteRegEx": "Xu and Sarikaya.", "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi."], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 189\u2013194. IEEE.", "citeRegEx": "Yao et al\\.,? 2014", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "For intent detection, a number of standard classifiers can be applied, such as support vector machines (SVMs) (Haffner et al., 2003) and convolutional neural networks (CNNs) (Xu and Sarikaya, 2013).", "startOffset": 110, "endOffset": 132}, {"referenceID": 21, "context": ", 2003) and convolutional neural networks (CNNs) (Xu and Sarikaya, 2013).", "startOffset": 49, "endOffset": 72}, {"referenceID": 12, "context": "For slot filling, popular approaches include using sequence models such as maximum entropy Markov models (MEMMs) (McCallum et al., 2000), conditional random fields (CRFs) (Raymond and Riccardi, 2007), and recurrent neural networks (RNNs) (Yao et al.", "startOffset": 113, "endOffset": 136}, {"referenceID": 17, "context": ", 2000), conditional random fields (CRFs) (Raymond and Riccardi, 2007), and recurrent neural networks (RNNs) (Yao et al.", "startOffset": 42, "endOffset": 70}, {"referenceID": 22, "context": ", 2000), conditional random fields (CRFs) (Raymond and Riccardi, 2007), and recurrent neural networks (RNNs) (Yao et al., 2014; Mesnil et al., 2015).", "startOffset": 109, "endOffset": 148}, {"referenceID": 13, "context": ", 2000), conditional random fields (CRFs) (Raymond and Riccardi, 2007), and recurrent neural networks (RNNs) (Yao et al., 2014; Mesnil et al., 2015).", "startOffset": 109, "endOffset": 148}, {"referenceID": 8, "context": "Moreover, it can be used as the RNN decoder in an end-to-end trainable sequence-to-sequence speech recognition model (Jaitly et al., 2015).", "startOffset": 117, "endOffset": 138}, {"referenceID": 4, "context": "More structured neural network approaches for utterance classification include using recursive neural network (RecNN) (Guo et al., 2014), recurrent neural network (Ravuri and Stolcke, 2015), and convolutional neural network models (Collobert and Weston, 2008; Kim, 2014).", "startOffset": 118, "endOffset": 136}, {"referenceID": 16, "context": ", 2014), recurrent neural network (Ravuri and Stolcke, 2015), and convolutional neural network models (Collobert and Weston, 2008; Kim, 2014).", "startOffset": 34, "endOffset": 60}, {"referenceID": 3, "context": ", 2014), recurrent neural network (Ravuri and Stolcke, 2015), and convolutional neural network models (Collobert and Weston, 2008; Kim, 2014).", "startOffset": 102, "endOffset": 141}, {"referenceID": 9, "context": ", 2014), recurrent neural network (Ravuri and Stolcke, 2015), and convolutional neural network models (Collobert and Weston, 2008; Kim, 2014).", "startOffset": 102, "endOffset": 141}, {"referenceID": 13, "context": "A major task in spoken language understanding (SLU) is to extract semantic constituents by searching input text to fill in values for predefined slots in a semantic frame (Mesnil et al., 2015), which is often referred to as slot filling.", "startOffset": 171, "endOffset": 192}, {"referenceID": 6, "context": "In the below example from ATIS (Hemphill et al., 1990) corpus following the popular in/out/begin (IOB) annotation method, Seattle and San Diego are the from and to locations respectively according to the slot labels, and tomorrow is the departure date.", "startOffset": 31, "endOffset": 54}, {"referenceID": 17, "context": "Sequence models including conditional random fields (Raymond and Riccardi, 2007) and RNN models (Yao et al.", "startOffset": 52, "endOffset": 80}, {"referenceID": 22, "context": "Sequence models including conditional random fields (Raymond and Riccardi, 2007) and RNN models (Yao et al., 2014; Mesnil et al., 2015; Liu and Lane, 2015) are among the most popular methods for sequence labeling tasks.", "startOffset": 96, "endOffset": 155}, {"referenceID": 13, "context": "Sequence models including conditional random fields (Raymond and Riccardi, 2007) and RNN models (Yao et al., 2014; Mesnil et al., 2015; Liu and Lane, 2015) are among the most popular methods for sequence labeling tasks.", "startOffset": 96, "endOffset": 155}, {"referenceID": 11, "context": "Sequence models including conditional random fields (Raymond and Riccardi, 2007) and RNN models (Yao et al., 2014; Mesnil et al., 2015; Liu and Lane, 2015) are among the most popular methods for sequence labeling tasks.", "startOffset": 96, "endOffset": 155}, {"referenceID": 14, "context": "RNN-based language models (Mikolov et al., 2011), and the variant (Sundermeyer et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 18, "context": ", 2011), and the variant (Sundermeyer et al., 2012)", "startOffset": 25, "endOffset": 51}, {"referenceID": 7, "context": "using long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) have shown superior performance comparing to traditional n-gram based models.", "startOffset": 36, "endOffset": 70}, {"referenceID": 1, "context": "To bridge the gap between training and inference, scheduled sampling method (Bengio et al., 2015) can be applied.", "startOffset": 76, "endOffset": 97}, {"referenceID": 11, "context": "Instead of only using previous true label, using sample from previous predicted label distribution in model training makes the model more robust by forcing it to learn to handle its own prediction mistakes (Liu and Lane, 2015).", "startOffset": 206, "endOffset": 226}, {"referenceID": 19, "context": "Alternatively, one can do left-to-right beam search (Sutskever et al., 2014; Chan et al., 2015) by maintaining a set of \u03b2 best partial hypotheses at each step.", "startOffset": 52, "endOffset": 95}, {"referenceID": 2, "context": "Alternatively, one can do left-to-right beam search (Sutskever et al., 2014; Chan et al., 2015) by maintaining a set of \u03b2 best partial hypotheses at each step.", "startOffset": 52, "endOffset": 95}, {"referenceID": 6, "context": "We used the Airline Travel Information Systems (ATIS) dataset (Hemphill et al., 1990) in our experiment.", "startOffset": 62, "endOffset": 85}, {"referenceID": 13, "context": "We followed the same ATIS corpus1 setup used in (Mesnil et al., 2015; Xu and Sarikaya, 2013; Tur et al., 2010).", "startOffset": 48, "endOffset": 110}, {"referenceID": 21, "context": "We followed the same ATIS corpus1 setup used in (Mesnil et al., 2015; Xu and Sarikaya, 2013; Tur et al., 2010).", "startOffset": 48, "endOffset": 110}, {"referenceID": 20, "context": "We followed the same ATIS corpus1 setup used in (Mesnil et al., 2015; Xu and Sarikaya, 2013; Tur et al., 2010).", "startOffset": 48, "endOffset": 110}, {"referenceID": 0, "context": "To provide a more challenging and realistic evaluation, we used the simulated noisy utterances that were generated by artificially mixing clean speech data with noisy backgrounds following the simulation methods described in the third CHiME Speech Separation and Recognition Challenge (Barker et al., 2015).", "startOffset": 285, "endOffset": 306}, {"referenceID": 23, "context": "We used LSTM cell as the basic RNN unit, following the LSTM design in (Zaremba et al., 2014).", "startOffset": 70, "endOffset": 92}, {"referenceID": 10, "context": "We conducted mini-batch training (with batch size 16) using Adam optimization method following the suggested parameter setup in (Kingma and Ba, 2014).", "startOffset": 128, "endOffset": 149}, {"referenceID": 23, "context": "5) to the non-recurrent connections (Zaremba et al., 2014) of RNN and the hidden layers of MLPs, and applied L2 regularization (\u03bb = 10\u22124) on the parameters of MLPs.", "startOffset": 36, "endOffset": 58}, {"referenceID": 15, "context": "For the evaluation in ASR settings, we used the acoustic model trained on LibriSpeech dataset (Panayotov et al., 2015), and the language model trained on ATIS training corpus.", "startOffset": 94, "endOffset": 118}, {"referenceID": 4, "context": "1 RecNN (Guo et al., 2014) 4.", "startOffset": 8, "endOffset": 26}, {"referenceID": 4, "context": "22 2 RecNN+Viterbi (Guo et al., 2014) 4.", "startOffset": 19, "endOffset": 37}, {"referenceID": 4, "context": "Related joint models: RecNN: Joint intent detection and slot filling model using recursive neural network (Guo et al., 2014).", "startOffset": 106, "endOffset": 124}, {"referenceID": 4, "context": "RecNN+Viterbi: Joint intent detection and slot filling model using recursive neural network with Viterbi sequence optimization for slot filling (Guo et al., 2014).", "startOffset": 144, "endOffset": 162}], "year": 2016, "abstractText": "Speaker intent detection and semantic slot filling are two critical tasks in spoken language understanding (SLU) for dialogue systems. In this paper, we describe a recurrent neural network (RNN) model that jointly performs intent detection, slot filling, and language modeling. The neural network model keeps updating the intent prediction as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot filling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input.", "creator": "TeX"}}}