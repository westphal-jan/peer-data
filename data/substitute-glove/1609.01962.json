{"id": "1609.01962", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Using Gaussian Processes for Rumour Stance Classification in Social Media", "abstract": "Social media too all be rife with apologised one move reports are january transparently during breaking news. Interestingly, , 'll heavy multiple reactions that whose interests media rely in now lack, discussing their toward northwards controversy, ultimately enabling new fledgling of become disputed stemmed such being possible false. In this work, if set out that need very printing, assisted collineation have intended expanding - task provides to classify the adopt president in every such ludu part brought rumourous discussions as either supporting, denying or superiors the speculating. Using a classifier based on Gaussian Processes, besides exploring means evaluations on, datasets with even only analogous made variations allocation of orientations, because show one whatever coherent consistently outperforms promotion baseline potoos. Our pseudoprime is especially effective prior adjusted the flow well distinct larger of criticizes linked when a given rumour, which should set forth turned after combine characteristic for a misunderstood - tracking is that allow warn especially enjoy users since Twitter in professional today professions he as livid form same rebutted.", "histories": [["v1", "Wed, 7 Sep 2016 12:33:02 GMT  (57kb,D)", "http://arxiv.org/abs/1609.01962v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.SI", "authors": ["michal lukasik", "kalina bontcheva", "trevor cohn", "arkaitz zubiaga", "maria liakata", "rob procter"], "accepted": false, "id": "1609.01962"}, "pdf": {"name": "1609.01962.pdf", "metadata": {"source": "CRF", "title": "Using Gaussian Processes for Rumour Stance Classification in Social Media", "authors": ["MICHAL LUKASIK", "KALINA BONTCHEVA", "TREVOR COHN", "ARKAITZ ZUBIAGA", "MARIA LIAKATA"], "emails": [], "sections": [{"heading": null, "text": "Using Gaussian Processes for Rumour Stance Classification in Social Media\nMICHAL LUKASIK, University of Sheffield KALINA BONTCHEVA, University of Sheffield TREVOR COHN, University of Melbourne ARKAITZ ZUBIAGA, University of Warwick MARIA LIAKATA, University of Warwick ROB PROCTER, University of Warwick\nSocial media tend to be rife with rumours while new reports are released piecemeal during breaking news. Interestingly, one can mine multiple reactions expressed by social media users in those situations, exploring their stance towards rumours, ultimately enabling the flagging of highly disputed rumours as being potentially false. In this work, we set out to develop an automated, supervised classifier that uses multi-task learning to classify the stance expressed in each individual tweet in a rumourous conversation as either supporting, denying or questioning the rumour. Using a classifier based on Gaussian Processes, and exploring its effectiveness on two datasets with very different characteristics and varying distributions of stances, we show that our approach consistently outperforms competitive baseline classifiers. Our classifier is especially effective in estimating the distribution of different types of stance associated with a given rumour, which we set forth as a desired characteristic for a rumour-tracking system that will warn both ordinary users of Twitter and professional news practitioners when a rumour is being rebutted."}, {"heading": "1. INTRODUCTION", "text": "There is an increasing need to interpret and act upon rumours spreading quickly through social media during breaking news, where new reports are released piecemeal and often have an unverified status at the time of posting. Previous research has posited the damage that the diffusion of false rumours can cause in society, and that corrections issued by news organisations or state agencies such as the police may not necessarily achieve the desired effect sufficiently quickly [Lewandowsky et al. 2012; Procter et al. 2013a]. Being able to determine the accuracy of reports is therefore crucial in these scenarios. However, the veracity of rumours in circulation is usually hard to establish [Allport and Postman 1947], since as many views and testimonies as possible need to be assembled and examined in order to reach a final judgement. Examples of rumours that were later disproven, after being widely circulated, include a 2010 earthquake in Chile, where rumours of a volcano eruption and a tsunami warning in Valparaiso spawned on Twitter [Mendoza et al. 2010]. Another example is the England riots in 2011, where false rumours claimed that rioters were going to attack Birmingham\u2019s Children\u2019s Hospital and that animals had escaped from London Zoo [Procter et al. 2013b].\nPrevious work by ourselves and others has argued that looking at how users in social media orient to rumours is a crucial first step towards making an informed judgement on the veracity of a rumourous report [Zubiaga et al. 2016; Tolmie et al. 2015; Mendoza et al. 2010]. For example, in the case of the riots in England in August 2011, Procter et al. manually analysed the stance expressed by users in social media towards rumours [Procter et al. 2013b]. Each tweet discussing a rumour was manually categorised as supporting, denying or questioning it. It is obvious that manual methods have their disadvantages in that they do not scale well; the ability to perform stance categorisation of tweets in an automated way would be of great use in tracking rumours, flagging those that are largely denied or questioned as being more likely to be false.\nDetermining the stance of social media posts automatically has been attracting increasing interest in the scientific community in recent years, as this is a useful first step towards more in-depth rumour analysis:\n\u2014 It can help detect rumours and flag them as such more quickly [Zhao et al. 2015]. \u2014 It is useful for tracking public opinion about rumours and hence for monitoring their wider effect\non society.\nar X\niv :1\n60 9.\n01 96\n2v 1\n[ cs\n.C L\n] 7\nS ep\n2 01\n6\n\u2014 Aggregate stance information and dynamics over time can be leveraged for rumour veracity classification [Derczynski et al. 2015; Liu et al. 2015].\nWork on automatic rumour stance classification, however, is still in its infancy, with some methods ignoring temporal ordering and rumour identities (e.g. [Qazvinian et al. 2011]), while others being rule-based and thus with unclear generalisability to new rumours [Zhao et al. 2015].\nOur work advances the state-of-the-art in tweet-level stance classification through multi-task learning and Gaussian Processes. This article substantially extends our earlier short paper [Lukasik et al. 2015a], fistly by using a second dataset, which enables us to test the generalisability of our results. Secondly, a comparison against additional baseline classifiers and recent state-of-the-art approaches has been added to the experimental section. Lastly, we carried out a more thorough analysis of the results, including now per-class performance scores, which furthers our understanding of rumour stance classification.\nIn comparison to the state-of-the-art, our approach is novel in several crucial aspects:\n(1) We perform stance classification on unseen rumours, given a training set of already annotated rumours on different topics and from different time periods. (2) The temporal ordering of tweets on a given rumour is respected, both during training and stance classification. (3) Generalisability to new datasets is a core aspect of our methodology, which is built on the premise that patterns of stance should exhibit similar characteristics across different rumours.\nBased on the assumption of a common underlying linguistic signal in rumours on different topics, we build a transfer learning system based on Gaussian Processes, that can classify stance in newly emerging rumours. The paper reports results on two different rumour datasets and explores two different experimental settings \u2013 without any training data and with very limited training data. We refer to these as:\n\u2014 Leave One Out: all tweets pertaining to a target rumour are only used for testing, i.e. method performance on a completely unseen rumour is reported; \u2014 Leave Part Out: the first few tweets of a target rumour (as annotated by journalists) and added to the training set of the Gaussian Process classifier, together with tweets pertaining to older rumours. The rest of the tweets on the target rumour are used for evaluation.\nOur results demonstrate that Gaussian Process-based, multi-task learning leads to significantly improved performance over state-of-the-art methods and competitive baselines, as demonstrated on two very different datasets. The classifier relying on Gaussian Processes performs particularly well over the rest of the baseline classifiers in the Leave Part Out setting, proving that it does particularly well in determining the distribution of supporting, denying and questioning tweets associated with a rumour. Estimating the distribution of stances is the key aspect for which our classifier performs especially well compared to the baseline classifiers."}, {"heading": "2. RELATED WORK", "text": "This section provides a more in-depth motivation of the rumour stance detection task and an overview of the state-of-the-art methods and their limitations. First, however, let us start by introducing the formal definition of a rumour."}, {"heading": "2.1. Rumour Definition", "text": "There have been multiple attempts at defining rumours in the literature. Most of them are complementary to one another, with slight variations depending on the context of their analyses. The core concept that most researchers agree on matches the definition that major dictionaries provide, such as the Oxford English Dictionary1 defining a rumour as \u201ca currently circulating story or report of uncertain or doubtful truth\u201d. For instance, DiFonzo and Bordia [DiFonzo and Bordia 2007] defined rumours as \u201cunverified and instrumentally relevant information statements in circulation.\u201d\nResearchers have long looked at the properties of rumours to understand their diffusion patterns and to distinguish them from other kinds of information that people habitually share [Donovan 2007]. Allport and Postman [Allport and Postman 1947] claimed that rumours spread due to two factors: people want to find meaning in things and, when faced with ambiguity, people try to find meaning by telling stories. The latter factor also explains why rumours tend to change in time by becoming shorter, sharper and more coherent. This is the case, it is argued, because in this way rumours explain things more clearly. On the other hand, Rosnow [Rosnow 1991] claimed that there are four important factors for rumour transmission. Rumours must be outcome-relevant to the listener, must increase personal anxiety, be somewhat credible and be uncertain. Furthermore, Shibutani [Shibutani 1969] defined rumours to be a recurrent form of communication through which men [sic] caught together in an ambiguous situation attempt to construct a meaningful interpretation of it by pooling their intellectual resources. It might be regarded as a form of collective problem-solving.\nIn contrast with these three theories, Guerin and Miyazaki [Guerin and Miyazaki 2006] state that a rumour is a form of relationship-enhancing talk. Building on their previous work, they recall that many ways of talking serve the purpose of forming and maintaining social relationships. Rumours, they say, can be explained by such means.\nIn our work, we adhere to the widely accepted fact that rumours are unverified pieces of information. More specifically, following [Zubiaga et al. 2016], we regard a rumour in the context of breaking news, as a \u201ccirculating story of questionable veracity, which is apparently credible but hard to verify, and produces sufficient skepticism and/or anxiety so as to motivate finding out the actual truth\u201d."}, {"heading": "2.2. Descriptive Analysis of Rumours in Social Media", "text": "One particularly influential piece of work in the field of rumour analysis in social media is that by Mendoza et al. [Mendoza et al. 2010]. By manually analysing the data from the earthquake in Chile in 2010, the authors selected 7 confirmed truths and 7 false rumours, each consisting of close to 1000 tweets or more. The veracity value of the selected stories was corroborated by using reliable sources. Each tweet from each of the news items was manually classified into one of the following classes: affirmation, denial, questioning, unknown or unrelated. In this way, each tweet was classified according to the position it showed towards the topic it was about. The study showed that a much higher percentage of tweets about false rumours are shown to deny the respective rumours (approximately 50%). This is in contrast to rumours later proven to be true, where only 0.3% of tweets were denials. Based on this, authors claimed that rumours can be detected using aggregate analysis of the stance expressed in tweets.\nRecent research put together in a special issue on rumours and social media [Papadopoulos et al. 2016] also shows the increasing interest of the scientific community in the topic. [Webb et al. 2016] proposed an agenda for research that establishes an interdisciplinary methodology to explore in\n1http://www.oxforddictionaries.com/definition/english/rumour\nfull the propagation and regulation of unverified content on social media. [Middleton and Krivcovs 2016] described an approach for geoparsing social media posts in real-time, which can be of help to determine the veracity of rumours by tracking down the poster\u2019s location. The contribution of [Hamdi et al. 2016] to rumour resolution is to build an automated system that rates the level of trust of users in social media, hence enabling to get rid of users with low reputation. Complementary to these approaches, our objective is to determine the stance of tweets towards a rumour, which can then be aggregated to establish an overall veracity score for the rumour.\nAnother study that shows insightful conclusions with respect to stance towards rumours is that by Procter et al. [Procter et al. 2013b]. The authors conducted an analysis of a large dataset of tweets related to the riots in the UK, which took place in August 2011. The dataset collected in the riots study is one of the two used in our experiments, and we describe it in more detail in section 3.3. After grouping the tweets into topics, where each represents a rumour, they were manually categorised into different classes, namely: (1) media reports, which are tweets sent by mainstream media accounts or journalists connected to media, (2) pictures, being tweets uploading a link to images, (3) rumours, being tweets claiming or counter claiming something without giving any source, (4) reactions, consisting of tweets being responses of users to the riots phenomenon or specific event related to the riots.\nBesides categorisation of tweets by type, Procter et al. also manually categorised the accounts posting tweets into different types, such as mainstream media, only on-line media, activists, celebrities, bots, among others.\nWhat is interesting for the purposes of our work is that the authors observed the following fourstep pattern recurrently occurring across the collected rumours: (1) a rumour is initiated by someone claiming it may be true, (2) a rumour spreads together with its reformulations, (3) counter claims appear, (4) a consensus emerges about the credibility of the rumour.\nThis leads the authors to the conclusion that the process of \u2019inter-subjective sense making\u2019 by Twitter users plays a key role in exposing false rumours. This finding, together with subsequent work by Tolmie et al. into the conversational characteristics of microblogging [Tolmie et al. 2015] has motivated our research into automating stance classification as a methodology for accelerating this process."}, {"heading": "2.3. Rumour Stance Classification", "text": "Qazvinian et al. [Qazvinian et al. 2011] conducted early work on rumour stance classification. They introduced a system that analyzes a set of tweets associated with a given topic predefined by the user. Their system would then classify each of the tweets as supporting, denying or questioning a tweet. We have adopted this scheme in terms of the different types of stance in the work we report here. However, their work ended up merging denying and questioning tweets for each rumour into a single class, converting it into a 2-way classification problem of supporting vs denying-orquestioning. Instead, we keep those classes separate and, following Procter et al., we conduct a 3-way classification [Zubiaga et al. 2014].\nAnother important characteristic that differentiates Qazvinian et al.\u2019s work from ours is that they looked at support and denial on longstanding rumours, such as the fact that many people conjecture whether Barack Obama is a Muslim or not. By contrast, we look at rumours that emerge in the context of fast-paced, breaking news situations, where new information is released piecemeal, often with statements that employ hedging words such as \u201creportedly\u201d or \u201caccording to sources\u201d to make it clear that the information is not fully verified at the time of posting. This is a very different scenario from that in Qazvinian et al.\u2019s work as the emergence of rumourous reports can lead to sudden changes in vocabulary, leading to situations that might not have been observed in the training data.\nAnother aspect that we deal with differently in our work, aiming to make it more realistically applicable to a real world scenario, is that we apply the method to each rumour separately. Ultimately, our goal is to classify new, emerging rumours, which can differ from what the classifier has observed in the training set. Previous work ignored this separation of rumours, by pooling together tweets from all the rumours in their collections, both in training and test data. By contrast,\nwe consider the rumour stance classification problem as a form of transfer learning and seek to classify unseen rumours by training the classifier from previously labelled rumours. We argue that this makes a more realistic classification scenario towards implementing a real-world rumour-tracking system.\nFollowing a short gap, there has been a burst of renewed interest in this task since 2015. For example, Liu et al. [Liu et al. 2015] introduce rule-based methods for stance classification, which were shown to outperform the approach by [Qazvinian et al. 2011]. Similarly, [Zhao et al. 2015] use regular expressions instead of an automated method for rumour stance classification. Hamidian and Diab [Hamidian and Diab 2016] use Tweet Latent Vectors to assess the ability of performing 2-way classification of the stance of tweets as either supporting or denying a rumour. They study the extent to which a model trained on historical tweets can be used for classifying new tweets on the same rumour. This, however, limits the method\u2019s applicability to long-running rumours only.\nThe work closest to ours in terms of aims is Zeng et al. [Zeng et al. 2016], who explored the use of three different classifiers for automated rumour stance classification on unseen rumours. In their case, classifiers were set up on a 2-way classification problem dealing with tweets that support or deny rumours. In the present work, we extend this research by performing 3-way classification that also deals with tweets that question the rumours. Moreover, we adopt the three classifiers used in their work, namely Random Forest, Naive Bayes and Logistic Regression, as baselines in our work.\nLastly, researchers [Zhao et al. 2015; Ma et al. 2015] have focused on the related task of detecting rumours in social media. While a rumour detection system could well be the step that is applied prior to our stance classification system, here we assume that rumours have already been identified to focus on the subsequent step of determining stances."}, {"heading": "3. PROBLEM DEFINITION: TWEET LEVEL RUMOUR STANCE CLASSIFICATION", "text": ""}, {"heading": "3.1. Definition of the Task", "text": "Individual tweets may discuss the same rumour in different ways, where each user expresses their own stance towards the rumour. Within this scenario, we define the tweet level rumour stance classification task as that in which a classifier has to determine the stance of each tweet towards the rumour. More specifically, given the tweet ti as input, the classifier has to determine which of the set Y = {supporting, denying, questioning} applies to the tweet, y(ti) \u2208 Y .\nHere we define the task as a supervised classification problem, where the classifier is trained from a labelled set of tweets and is applied to tweets on a new, unseen set of rumours."}, {"heading": "3.2. Problem formulation", "text": "Let R be a set of rumours, each of which consists of tweets discussing it, \u2200r\u2208R Tr = {tr1, \u00b7 \u00b7 \u00b7 , trrn}. T = \u222ar\u2208RTr is the complete set of tweets from all rumours. Each tweet is classified as supporting, denying or questioning with respect to its rumour: y(ti) \u2208 {s, d, q}.\nWe formulate the problem in two different settings. First, we consider the Leave One Out (LOO) setting, which means that for each rumour r \u2208 R, we construct the test set equal to Tr and the training set equal to T \\ Tr. This is the most challenging scenario, where the test set contains an entirely unseen rumour.\nThe second setting is Leave Part Out (LPO). In this formulation, a very small number of initial tweets from the target rumour is added to the training set {tr1, \u00b7 \u00b7 \u00b7 , trrk}. This scenario becomes applicable typically soon after a rumour breaks out and journalists have started monitoring and analysing the related tweet stream. The experimental section investigates how the number of initial training tweets influences classification performance on a fixed test set, namely: {trrl , \u00b7 \u00b7 \u00b7 , t r rn}, l > k. The tweet-level stance classification problem here assumes that tweets from the training set are already labelled with the rumour discussed and the attitude expressed towards that. This information can be acquired either via manual annotation as part of expert analysis, as is the case with our\ndataset, or automatically, e.g. using pattern-based rumour detection [Zhao et al. 2015]. Our method is then used to classify the stance expressed in each new tweet from the test set."}, {"heading": "3.3. Datasets", "text": "We evaluate our work on two different datasets, which we describe below. We use two recent datasets from previous work for our study, both of which adapt to our needs. We do not use the dataset by [Qazvinian et al. 2011] given that it uses a different annotation scheme limited to two categories of stances.\nThe reason why we use the two datasets separately instead of combining them is that they have very different characteristics. Our experiments, instead, enable us to assess the ability of our classifier to deal with these different characteristics.\n3.3.1. England riots dataset. The first dataset consists of several rumours circulating on Twitter during the England riots in 2011 (see Table II). The dataset was collected by tracking a long set of keywords associated with the event. The dataset was analysed and annotated manually as supporting, questioning, or denying a rumour, by a team of social scientists studying the role of social media during the riots [Procter et al. 2013b].\nAs can be seen from the dataset overview in Table II, different rumours exhibit varying proportions of supporting, denying and questioning tweets, which was also observed in other studies of rumours [Mendoza et al. 2010; Qazvinian et al. 2011]. These variations in the number of instances for each class across rumours posits the challenge of properly modelling a rumour stance classifier. The classifier needs to be able to deal with a test set where the distribution of classes can be very different to that observed in the training set.\nThus, we perform 7-fold cross-validation in the experiments, each fold having six rumours in the training set, and the remaining rumour in the test set.\nThe seven rumours were as follows [Procter et al. 2013b]:\n\u2014 Rioters had attacked London Zoo and released the animals. \u2014 Rioters were gathering to attack Birmingham\u2019s Children\u2019s Hospital. \u2014 Rioters had set the London Eye on fire. \u2014 Police had beaten a sixteen year old girl. \u2014 The Army was being mobilised in London to deal with the rioters. \u2014 Rioters had broken into a McDonalds and set about cooking their own food. \u2014 A store belonging to the Miss Selfridge retail group had been set on fire in Manchester.\n3.3.2. PHEME dataset. Additionally, we use another rumour dataset associated with five different events, which was collected as part of the PHEME FP7 research project and described in detail in [Zubiaga et al. 2016, 2015]. Note that the authors released datasets for nine events, but here we remove non-English datasets, as well as small English datasets each of which includes only 1\nrumour, as opposed to the 40+ rumours in each of the datasets that we are using. We summarise the details of the five events we use from this dataset in Table III.\nIn contrast to the England riots dataset, the PHEME datasets were collected by tracking conversations initiated by rumourous tweets. This was done in two steps. First, we collected tweets that contained a set of keywords associated with a story unfolding in the news. We will be referring to the latter as an event. Next, we sampled the most retweeted tweets, on the basis that rumours by definition should be \u201ca circulation story which produces sufficient skepticism or anxiety\u201d. This allows us to filter potentially rumourous tweets and collect conversations initiated by those. Conversations were tracked by collecting replies to tweets and, therefore, unlike the England riots, this dataset also comprises replying tweets by definition. This is an important characteristic of the dataset, as one would expect that replies are generally shorter and potentially less descriptive than the source tweets that initiated the conversation. We take this difference into consideration when performing the analysis of our results.\nThis dataset includes tweets associated with the following five events:\n\u2014 Ferguson unrest: Citizens of Ferguson in Michigan, USA, protested after the fatal shooting of an 18-year-old African American, Michael Brown, by a white police officer on August 9, 2014. \u2014 Ottawa shooting: Shootings occurred on Ottawa\u2019s Parliament Hill in Canada, resulting in the death of a Canadian soldier on October 22, 2014. \u2014 Sydney siege: A gunman held as hostages ten customers and eight employees of a Lindt chocolate cafe\u0301 located at Martin Place in Sydney, Australia, on December 15, 2014. \u2014 Charlie Hebdo shooting: Two brothers forced their way into the offices of the French satirical weekly newspaper Charlie Hebdo in Paris, killing 11 people and wounding 11 more, on January 7, 2015. \u2014 Germanwings plane crash: A passenger plane from Barcelona to Du\u0308sseldorf crashed in the French Alps on March 24, 2015, killing all passengers and crew on board. The plane was ultimately found to have been deliberately crashed by the co-pilot of the plane.\nIn this case, we perform 5-fold cross-validation, having four events in the training set and the remaining event in the test set for each fold."}, {"heading": "4. EXPERIMENT SETTINGS", "text": "This section details the features and evaluation measures used in our experiments on tweet level stance classification."}, {"heading": "4.1. Classifiers", "text": "We begin by describing the classifiers we use for our experimentation, including Gaussian Processes, as well as a set of competitive baseline classifiers that we use for comparison.\n4.1.1. Gaussian Processes for Classification. Gaussian Processes are a Bayesian non-parametric machine learning framework that has been shown to work well for a range of NLP problems, often beating other state-of-the-art methods [Cohn and Specia 2013; Lampos et al. 2014; Beck et al. 2014; Preotiuc-Pietro et al. 2015].\nA Gaussian Process defines a prior over functions, which combined with the likelihood of data points gives rise to a posterior over functions explaining the data. The key concept is a kernel function, which specifies how outputs correlate as a function of the input. Thus, from a practitioner\u2019s point of view, a key step is to choose an appropriate kernel function capturing the similarities between inputs.\nWe use Gaussian Processes as this probabilistic kernelised framework avoids the need for expensive cross-validation for hyperparameter selection.2 Instead, the marginal likelihood of the data can be used for hyperparameter selection.\nThe central concept of Gaussian Process Classification (GPC; [Rasmussen and Williams 2005]) is a latent function f over inputs x: f(x) \u223c GP(m(x), k(x,x\u2032)), where m is the mean function, assumed to be 0 and k is the kernel function, specifying the degree to which the outputs covary as a function of the inputs. We use a linear kernel, k(x,x\u2032) = \u03c32x>x\u2032. The latent function is then mapped by the probit function \u03a6(f) into the range [0, 1], such that the resulting value can be interpreted as p(y = 1|x).\nThe GPC posterior is calculated as\np(f\u2217|X,y,x\u2217) = \u222b p(f\u2217|X,x\u2217, f)\np(y|f)p(f) p(y|X) df ,\nwhere p(y|f) = n\u220f\nj=1\n\u03a6(fj) yj (1\u2212\u03a6(fj))1\u2212yj is the Bernoulli likelihood of class y. After calculating\nthe above posterior from the training data, this is used in prediction, i.e., p(y\u2217=1|X,y,x\u2217)= \u222b \u03a6 (f\u2217) p (f\u2217|X,y,x\u2217) df\u2217 .\nThe above integrals are intractable and approximation techniques are required to solve them. There exist various methods to deal with calculating the posterior; here we use Expectation Propagation (EP; [Minka and Lafferty 2002]). In EP, the posterior is approximated by a fully factorised distribution, where each component is assumed to be an unnormalised Gaussian.\nIn order to conduct multi-class classification, we perform a one-vs-all classification for each label and then assign the one with the highest likelihood, amongst the three (supporting, denying, questioning). We choose this method due to interpretability of results, similar to recent work on occupational class classification [Preotiuc-Pietro et al. 2015].\nIntrinsic Coregionalisation Model. In the Leave-Part-Out (LPO) setting initial labelled tweets from the target rumour are observed as well, as opposed to the Leave-One-Out (LOO) setting. In the case of LPO, we propose to weigh the importance of tweets from the reference rumours depending on how similar their characteristics are to the tweets from the target rumour available for training. To handle this with GPC, we use a multiple output model based on the Intrinsic Coregionalisation Model (ICM; [A\u0301lvarez et al. 2012]). This model has already been applied successfully to NLP regression problems [Beck et al. 2014] and it can also be applied to classification ones. ICM parametrizes the kernel by a matrix which represents the extent of covariance between pairs of tasks. The complete kernel takes form of\nk((x, d), (x\u2032, d\u2032)) = kdata(x,x \u2032)Bd,d\u2032 ,\nwhere B is a square coregionalisation matrix, d and d\u2032 denote the tasks of the two inputs and kdata is a kernel for comparing inputs x and x\u2032 (here, linear). We parametrize the coregionalisation matrix B = \u03baI + vvT , where v specifies the correlation between tasks and the vector \u03ba controls the extent of task independence. Note that in case of LOO setting this model does not provide useful information, since no target rumour data is available to estimate similarity to other rumours.\n2There exist frequentist kernel methods, such as SVMs, which additionally require extensive heldout parameter tuning.\nHyperparameter selection. We tune hyperparameters v, \u03ba and \u03c32 by maximizing evidence of the model p(y|X), thus having no need for a validation set.\nMethods. We consider GPs in three different settings, varying in what data the model is trained on and what kernel it uses. The first setting (denoted GP) considers only target rumour data for training. The second (GPPooled) additionally considers tweets from reference rumours (i.e. other than the target rumour). The third setting is GPICM, where an ICM kernel is used to weight influence from tweets from reference rumours.\n4.1.2. Baselines. To assess and compare the efficiency of Gaussian Processes for rumour stance classification, we also experimented with five more baseline classifiers, all of which were implemented using the scikit Python package [Pedregosa et al. 2011]: (1) majority classifier, which is a naive classifier that labels all the instances in the test set with the most common class in the training set, (2) logistic regression (MaxEnt), (3) support vector machines (SVM), (4) naive bayes (NB) and (5) random forest (RF). The selection of these baselines is in line with the classifiers used in recent research on stance classification [Zeng et al. 2016], who found that random forests, followed by logistic regression, performed best."}, {"heading": "4.2. Features", "text": "We conducted a series of preprocessing steps in order to address data sparsity. All words were converted to lowercase; stopwords have been removed3; all emoticons were replaced by words4; and stemming was performed. In addition, multiple occurrences of a character were replaced with a double occurrence [Agarwal et al. 2011], to correct for misspellings and lengthenings, e.g., looool. All punctuation was also removed, except for ., ! and ?, which we hypothesize to be important for expressing emotion. Lastly, usernames were removed as they tend to be rumour-specific, i.e., very few users comment on more than one rumour.\nAfter preprocessing the text data, we use either the resulting bag of words (BOW) feature representation and replace all words with their Brown cluster ids (Brown). Brown clustering is a hard hierarchical clustering method [Liang 2005]. It clusters words based on maximizing the probability of the words under the bigram language model, where words are generated based on their clusters. In previous work it has been shown that Brown clusters yield better performance than directly using the BOW features [Lukasik et al. 2015a].\nIn our experiments, the clusters used were obtained using 1000 clusters acquired from a large scale Twitter corpus [Owoputi et al. 2013], from which we can learn Brown clusters aimed at representing a generalisable Twitter vocabulary. Retweets are removed from the training set to prevent bias [Llewellyn et al. 2014]. More details on the Brown clusters that we used as well as the words that are part of each cluster are available online5.\nDuring the experimentation process, we also tested additional features, including the use of the bag of words instead of the Brown clusters, as well as using word embeddings trained from the training sets [Mikolov et al. 2013]. However, results turned out to be substantially poorer than those we obtained with the Brown clusters. We conjecture that this was due to the little data available to train the word embeddings; further exploring use of word embeddings trained from larger training datasets is left future work. In order to focus on our main objective of proving the effectiveness of a multi-task learning approach, as well as for clarity purposes, since the number of approaches to show in the figures increases if we also consider the BOW features, we only show results for the classifiers relying on Brown clusters as features.\n3We removed stopwords using the English list from Python\u2019s NLTK package. 4We used the dictionary from: http://bit.ly/1rX1Hdk and extended it with: :o, : |, =/, :s, :S, :p. 5http://www.cs.cmu.edu/ ark/TweetNLP/cluster viewer.html"}, {"heading": "4.3. Evaluation Measures", "text": "Accuracy is often deemed a suitable evaluation measure to assess the performance of a classifier on a multi-class classification task. However, the classes are clearly imbalanced in our case, with varying tendencies towards one of the classes in each of the rumours. We argue that in these scenarios the sole evaluation based on accuracy is insufficient, and further measurement is needed to account for category imbalance. This is especially necessary in our case, as a classifier that always predicts the majority class in an imbalanced dataset will achieve high accuracy, even if the classifier is useless in practice. To tackle this, we use both micro-averaged and macro-averaged F1 scores. Note that the micro-averaged F1 score is equivalent to the well-known accuracy measure, while the macroaveraged F1 score complements it by measuring performance assigning the same weight to each category.\nBoth of the measures rely on precision (Equation 1) and recall (Equation 2) to compute the final F1 score.\nPrecisionk = tpk\ntpk + fpk (1)\nRecallk = tpk\ntpk + fnk (2)\nwhere tpk (true positives) refer to the number of instances correctly classified in class k, fpk is the number of instances incorrectly classified in class k, and fnk is the number of instances that actually belong to class k but were not classified as such.\nThe above equations can be used to compute precision and recall for a specific class. Precision and recall for all the classes in a problem with c classes are computed differently if they are microaveraged (see Equations 3 and 4) or macroaveraged (see Equations 5 and 6).\nPrecisionmicro = \u2211c\nk=1 tpk\u2211c k=1 tpk + \u2211c k=1 fpk\n(3)\nRecallmicro = \u2211c\nk=1 tpk\u2211c k=1 tpk + \u2211c k=1 fnk\n(4)\nPrecisionmacro = \u2211c\nk=1 Precisionk c\n(5)\nRecallmacro = \u2211c\nk=1 Recallk c\n(6)\nAfter computing microaveraged and macroveraged precision and recall, the final F1 score is computed in the same way, i.e., calculating the harmonic mean of the precision and recall in question (see Equation 7).\nF1 = 2\u00d7 Precision\u00d7 Recall\nPrecision + Recall (7)\nAfter computing the F1 score for each fold, we compute the micro-averaged score across folds."}, {"heading": "5. RESULTS", "text": "First, we look at the results on each dataset separately. Then we complement the analysis by aggregating the results from both datasets, which leads to further understanding the performance of our classifiers on rumour stance classification."}, {"heading": "5.1. Comparison of Classifiers", "text": "We show the results for the LOO and LPO settings in the same figure, distinguished by the training size displayed in the X axis. In all the cases, labelled tweets from the remainder of the rumours (rumours other than the test/targer rumour) are used for training, and hence the training size shown in the X axis is in addition to those. Note that the training size refers to the number of labelled instances that the classifier is making use of from the target rumour. Thus, a training size of 0 indicates the LOO setting, while training sizes from 10 to 50 pertain to the LPO setting.\nFigure 1 and Table IV show how micro-averaged and macro-averaged F1 scores for the England riots dataset change as the number of tweets from the target rumour used for training increases. We observe that, as initially expected, the performance of most of the methods improves as the number of labelled training instances from the target rumour increases. This increase is especially remarkable with the GP-ICM method, which gradually increases after having as few as 10 training instances. GP-ICM\u2019s performance keeps improving as the number of training instances approaches 506 Two aspects stand out from analysing GP-ICM\u2019s performance:\n\u2014 It performs poorly in terms of micro-averaged F1 when no labelled instances from the target rumour are used. However, it makes very effective use of the labelled training instances, overtaking the rest of the approaches and achieving the best results. This proves the ability of GP-ICM to make the most of the labelled instances from the target rumour, which the rest of the approaches struggle with. \u2014 Irrespective of the number of labelled instances, GP-ICM is robust when evaluated in terms of macro-averaged F1. This means that GP-ICM is managing to determine the distribution of classes effectively, assigning labels to instances in the test set in a way that is better distributed than the rest of the classifier.\nDespite the saliency of GP-ICM, we notice that two other baseline approaches, namely MaxEnt and RF, achieve competitive results that are above the rest of the baselines, but still perform worse than GP-ICM.\nThe results from the PHEME dataset are shown in Figure 2 and Table V. Overall, we can observe that results are lower in this case than they were for the riots dataset. The reason for this can be attributed to the following two observations: on the one hand, each fold pertaining to a different event in the PHEME dataset means that the classifier encounters a new event in the classification, where it will likely find new vocabulary, which may be more difficult to classify; on the other hand,\n6Note that 50 tweets represent, on average, less than 7% of the whole rumour, with the rest of the rumour yet to be observed.\nthe PHEME dataset is more prominently composed of tweets that are replying to others, which are likely shorter and less descriptive on their own and hence more difficult to get meaningful features from. Despite the additional difficulty in this dataset, we are interested in exploring if the same trend holds across classifiers, from which we can generalise the analysis to different types of classifiers.\nOne striking difference with respect to the results from the riots dataset is that, in this case, the classifiers, including GP-ICM, are not gaining as much from the inclusion of labelled instances from the target rumour. This is likely due to the heterogeneity of each of the events in the PHEME dataset. Here a diverse set of rumourous newsworthy pieces of information are discussed pertaining to the selected events as they unfold. By contrast, each rumour in the riots dataset is more homogeneous, as each rumour focuses on a specific story.\nInterestingly, when we compare the performance of different classifiers, we observe that GP-ICM again outperforms the rest of the approaches, both in terms of micro-averaged and macro-averaged F1 scores. While the micro-averaged F1 score does not increase as the number of training instances increases, we can see a slight improvement in terms of macro-averaged F1. This improvement suggests that GP-ICM does still take advantage of the labelled training instances to boost performance, in this case by better distributing the predicted labels.\nAgain, as we observed in the case of the riots dataset, two baselines stand out, MaxEnt and RF. They are very close to the performance of GP-ICM for the PHEME dataset, event outperforming it in a few occasions. In the following subsection we take a closer look at the differences among the three classifiers."}, {"heading": "5.2. Analysing the Performance of the Best-Performing Classifiers", "text": "We delve into the results of the best-performing classifiers, namely GP-ICM, MaxEnt and RF, looking at their per-class performance. This will help us understand when they perform well and where it is that GP-ICM stands out achieving the best results.\nTables VI and VII show per-class F1 measures for the aforementioned three best-performing classifiers for the England riots dataset and the PHEME dataset, respectively. They also show statistics of the mis-classifications that the classifiers made, in the form of percentage of deviations towards the other classes.\nLooking at the per-class performance analysis, we observe that the performance of GP-ICM varies when we look into Precision and Recall. Still, in all the dataset-class pairs, GP-ICM performs best in terms of either Precision or Recall, even though never in both. Moreover, it is generally the best in terms of F1, achieving the best Precision and Recall. The only exception is with MaxEnt classifying questioning tweets more accurately in terms of F1 for the England riots.\nWhen we look at the deviations, we see that all the classifiers suffer from the datasets being imbalanced towards supporting tweets. This results in all classifiers classifying numerous instances as supporting, while they are actually denying or questioning. This is a known problem in rumour diffusion, as previous studies have found that people barely deny or question rumours but generally tend to support them irrespective of their actual veracity value [Zubiaga et al. 2016]. While we have found that GP-ICM can tackle the imbalance issue quite effectively and better than other classifiers, this caveat posits the need for further research in dealing with the striking majority of supporting tweets in the context of rumours in social media."}, {"heading": "6. DISCUSSION", "text": "Experimentation with two different approaches based on Gaussian Processes (GP and GP-ICM) and comparison with respect to a set of competitive baselines over two rumour datasets enables us to gain generalisable insight on rumour stance classification on Twitter. This is reinforced by the fact that the two datasets are very different from each other. The first dataset, collected during the England riots in 2011, is a single event that we have split into folds, each fold belonging to a separate rumour within the event; hence, all the rumours are part of the same event. The second dataset, collected within the PHEME project, includes tweets for a set of five newsworthy events, where each event has been assigned a separate fold; therefore, the classifier needs to learn from four events and test on a new, unknown event, which has proven more challenging.\nResults are generally consistent across datasets, which enables us to generalise conclusions well. We observe that while GP itself does not suffice to achieve competitive results, GP-ICM does instead help boost the performance of the classifier substantially to even outperform the rest of the baselines in the majority of the cases.\nGP-ICM has proven to consistently perform well in both datasets, despite their very different characteristics, being competitive not only in terms of micro-averaged F1, but also in terms of macro-averaged F1. GP-ICM manages to balance the varying class distributions effectively, showing that its performance is above the rest of the baselines in accurately determining the distribution of\nclasses. This is very important in this task of rumour stance classification, owing to the fact that even if a classifier that is 100% accurate is unlikely, a classifier that accurately guesses the overall distribution of classes can be of great help. If a classifier makes a good estimation of the number of denials in an aggregated set of tweets, it can be useful to flag those potentially false rumours with high level of confidence.\nAnother factor that stands out from GP-ICM is its capacity to perform well when a few labelled instances of the target rumour are leveraged in the training phase. GP-ICM effectively exploits the knowledge garnered from the few instances from the target rumour, outperforming the rest of the baselines even when its performance was modest when no labelled instances were used from the target rumour.\nIn light of these results, we deem GP-ICM the most competitive approach to use when one can afford to get a few instances labelled from the target rumour. The labels from the target rumour can be obtained in practice in different ways: (1) having someone in-house (e.g. journalists monitoring breaking news stories) label a few instances prior to running the classifier, (2) making use of resources for human computation such as crowdsourcing platforms to outsource the labelling work, or (3) developing techniques that will attempt to classify the first few instances, incorporating in the training set those for which a classification with high level of confidence has been produced. The latter presents an ambitious avenue for future work that could help alleviate the labelling task.\nOn the other hand, in the absence of labelled data from the target rumour, which is the case of the LOO setting, the effectiveness of the GP-ICM classifier is not as prominent. For this scenario, other classifiers such as MaxEnt and Random Forests have proven more competitive and one could\nsee them as better options. However, we do believe that the remarkable difference that the reliance on the LPO setting produces is worth exploiting where possible."}, {"heading": "7. CONCLUSIONS", "text": "Social media is becoming an increasingly important tool for maintaining social resilience: individuals use it to express opinions and follow events as they unfold; news media organisations use it as a source to inform their coverage of these events; and government agencies, such as the emergency services, use it to gather intelligence to help in decision-making and in advising the public about how they should respond [Procter et al. 2013a]. While previous research has suggested that mechanisms for exposing false rumours are implicit in the ways in which people use social media [Procter et al. 2013b], it is nevertheless critically important to explore if there are ways in which computational tools can help to accelerate these mechanisms so that misinformation and disinformation can be targeted more rapidly, and the benefits of social media to society maintained [Derczynski et al. 2015].\nAs a first step to achieving this aim, this paper has investigated the problem of classifying the different types of stance expressed by individuals in tweets about rumours. First, we considered a setting where no training data from the target rumours is available (LOO). Without access to annotated examples of the target rumour the learning problem becomes very difficult. We showed that in the supervised domain adaptation setting (LPO), even annotating a small number of tweets helps to achieve better results. Moreover, we demonstrated the benefits of a multi-task learning approach, as well as that Brown cluster features are more useful for the task than simple bag of words.\nFindings from previous work, such as Castillo et al.; Procter et al., have suggested that the aggregate stance of individual users is correlated with actual rumour veracity. Hence, the next step in our own work will be to make use of the classifier for the stance expressed in the reactions of individual Twitter users in order to predict the actual veracity of the rumour in question. Another interesting direction for future work would be the addition of non-textual features to the classifier. For example, the rumour diffusion patterns [Lukasik et al. 2015b] may be a useful cue for stance classification."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is partially supported by the European Union under grant agreement No. 611233 PHEME. The work was implemented using the GPy toolkit [GPy authors 2015]. This research utilised Queen Mary\u2019s MidPlus computational facilities, supported by QMUL Research-IT and funded by EPSRC grant EP/K000128/1."}], "references": [{"title": "Sentiment Analysis of Twitter Data", "author": ["Apoorv Agarwal", "Boyi Xie", "Ilia Vovsha", "Owen Rambow", "Rebecca Passonneau."], "venue": "Proceedings of the Workshop on Languages in Social Media (LSM \u201911). 30\u201338.", "citeRegEx": "Agarwal et al\\.,? 2011", "shortCiteRegEx": "Agarwal et al\\.", "year": 2011}, {"title": "The psychology of rumor", "author": ["G.W. Allport", "L. Postman."], "venue": "Journal of Clinical Psychology (1947).", "citeRegEx": "Allport and Postman.,? 1947", "shortCiteRegEx": "Allport and Postman.", "year": 1947}, {"title": "Kernels for Vector-Valued Functions: A Review", "author": ["Mauricio A. \u00c1lvarez", "Lorenzo Rosasco", "Neil D. Lawrence."], "venue": "Found. Trends Mach. Learn. 4, 3 (2012), 195\u2013266.", "citeRegEx": "\u00c1lvarez et al\\.,? 2012", "shortCiteRegEx": "\u00c1lvarez et al\\.", "year": 2012}, {"title": "Joint Emotion Analysis via Multi-task Gaussian Processes", "author": ["Daniel Beck", "Trevor Cohn", "Lucia Specia."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP \u201914). 1798\u20131803.", "citeRegEx": "Beck et al\\.,? 2014", "shortCiteRegEx": "Beck et al\\.", "year": 2014}, {"title": "Predicting information credibility in time-sensitive social media", "author": ["Carlos Castillo", "Marcelo Mendoza", "Barbara Poblete."], "venue": "Internet Research 23, 5 (2013), 560\u2013588.", "citeRegEx": "Castillo et al\\.,? 2013", "shortCiteRegEx": "Castillo et al\\.", "year": 2013}, {"title": "Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation", "author": ["Trevor Cohn", "Lucia Specia."], "venue": "51st Annual Meeting of the Association for Computational Linguistics (ACL \u201913). 32\u201342.", "citeRegEx": "Cohn and Specia.,? 2013", "shortCiteRegEx": "Cohn and Specia.", "year": 2013}, {"title": "Rumor, gossip and urban legends", "author": ["Nicholas DiFonzo", "Prashant Bordia."], "venue": "Diogenes 54, 1 (2007), 19\u201335.", "citeRegEx": "DiFonzo and Bordia.,? 2007", "shortCiteRegEx": "DiFonzo and Bordia.", "year": 2007}, {"title": "How idle is idle talk? One hundred years of rumor research", "author": ["Pamela Donovan."], "venue": "Diogenes 54, 1 (2007), 59\u201382.", "citeRegEx": "Donovan.,? 2007", "shortCiteRegEx": "Donovan.", "year": 2007}, {"title": "GPy: A Gaussian process framework in Python", "author": ["The GPy authors."], "venue": "http://github.com/ SheffieldML/GPy. (2015).", "citeRegEx": "authors.,? 2015", "shortCiteRegEx": "authors.", "year": 2015}, {"title": "Analyzing Rumors, Gossip, and Urban Legends Through Their Conversational Properties", "author": ["Bernard Guerin", "Yoshihiko Miyazaki."], "venue": "The Psychological Record: Vol. 56: Iss. 1, Article 2. (2006).", "citeRegEx": "Guerin and Miyazaki.,? 2006", "shortCiteRegEx": "Guerin and Miyazaki.", "year": 2006}, {"title": "TISoN: Trust Inference in Trust-Oriented Social Networks", "author": ["Sana Hamdi", "Alda Lopes Gancarski", "Amel Bouzeghoub", "Sadok Ben Yahia."], "venue": "ACM Transactions on Information Systems (TOIS) 34, 3 (2016), 17.", "citeRegEx": "Hamdi et al\\.,? 2016", "shortCiteRegEx": "Hamdi et al\\.", "year": 2016}, {"title": "Rumor Identification and Belief Investigation on Twitter", "author": ["Sardar Hamidian", "Mona T Diab."], "venue": "Proceedings of NAACL-HLT. 3\u20138.", "citeRegEx": "Hamidian and Diab.,? 2016", "shortCiteRegEx": "Hamidian and Diab.", "year": 2016}, {"title": "Predicting and Characterising User Impact on Twitter", "author": ["Vasileios Lampos", "Nikolaos Aletras", "Daniel Preotiuc-Pietro", "Trevor Cohn."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL\u201914). 405\u2013413.", "citeRegEx": "Lampos et al\\.,? 2014", "shortCiteRegEx": "Lampos et al\\.", "year": 2014}, {"title": "Misinformation and its correction continued influence and successful debiasing", "author": ["Stephan Lewandowsky", "Ullrich KH Ecker", "Colleen M Seifert", "Norbert Schwarz", "John Cook."], "venue": "Psychological Science in the Public Interest 13, 3 (2012), 106\u2013131.", "citeRegEx": "Lewandowsky et al\\.,? 2012", "shortCiteRegEx": "Lewandowsky et al\\.", "year": 2012}, {"title": "Semi-Supervised Learning for Natural Language", "author": ["Percy Liang."], "venue": "Ph.D. Dissertation. Department of Electrical Engineering and Computer Science at the Massachusetts Institute of Technology.", "citeRegEx": "Liang.,? 2005", "shortCiteRegEx": "Liang.", "year": 2005}, {"title": "Real-time Rumor Debunking on Twitter", "author": ["Xiaomo Liu", "Armineh Nourbakhsh", "Quanzhi Li", "Rui Fang", "Sameena Shah."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM \u201915). ACM, New York, NY, USA, 1867\u20131870. DOI:http://dx.doi.org/10.1145/2806416.2806651", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Re-using an Argument Corpus to Aid in the Curation of Social Media Collections", "author": ["Clare Llewellyn", "Claire Grover", "Jon Oberlander", "Ewan Klein."], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (26-31) (LREC\u201914). 462\u2013468.", "citeRegEx": "Llewellyn et al\\.,? 2014", "shortCiteRegEx": "Llewellyn et al\\.", "year": 2014}, {"title": "Classifying Tweet Level Judgements of Rumours in Social Media", "author": ["Michal Lukasik", "Trevor Cohn", "Kalina Bontcheva."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. 2590\u2013 2595. http://aclweb.org/anthology/D/D15/D15-1311.pdf", "citeRegEx": "Lukasik et al\\.,? 2015a", "shortCiteRegEx": "Lukasik et al\\.", "year": 2015}, {"title": "Point Process Modelling of Rumour Dynamics in Social Media", "author": ["Michal Lukasik", "Trevor Cohn", "Kalina Bontcheva."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015. 518\u2013523. http://aclweb.org/anthology/P/P15/P15-2085.pdf", "citeRegEx": "Lukasik et al\\.,? 2015b", "shortCiteRegEx": "Lukasik et al\\.", "year": 2015}, {"title": "Detect Rumors Using Time Series of Social Context Information on Microblogging Websites", "author": ["Jing Ma", "Wei Gao", "Zhongyu Wei", "Yueming Lu", "Kam-Fai Wong."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM \u201915). ACM, New York, NY, USA, 1751\u20131754. DOI:http://dx.doi.org/10.1145/2806416.2806607", "citeRegEx": "Ma et al\\.,? 2015", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Twitter under crisis: Can we trust what we RT", "author": ["Marcelo Mendoza", "Barbara Poblete", "Carlos Castillo."], "venue": "1st Workshop on Social Media Analytics (SOMA\u201910). 71\u201379.", "citeRegEx": "Mendoza et al\\.,? 2010", "shortCiteRegEx": "Mendoza et al\\.", "year": 2010}, {"title": "Geoparsing and geosemantics for social media: spatio-temporal grounding of content propagating rumours to support trust and veracity analysis during breaking news", "author": ["Stuart E Middleton", "Vadims Krivcovs."], "venue": "ACM Transactions on Information Systems 34, 3 (2016), 1\u201327.", "citeRegEx": "Middleton and Krivcovs.,? 2016", "shortCiteRegEx": "Middleton and Krivcovs.", "year": 2016}, {"title": "Efficient estimation of word", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Expectation-propagation for the Generative Aspect Model", "author": ["Thomas Minka", "John Lafferty."], "venue": "Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI\u201902). 352\u2013359.", "citeRegEx": "Minka and Lafferty.,? 2002", "shortCiteRegEx": "Minka and Lafferty.", "year": 2002}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith."], "venue": "Proceedings of NAACL. 380\u2013390.", "citeRegEx": "Owoputi et al\\.,? 2013", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "Overview of the Special Issue on Trust and Veracity of Information in Social Media", "author": ["Symeon Papadopoulos", "Kalina Bontcheva", "Eva Jaho", "Mihai Lupu", "Carlos Castillo."], "venue": "ACM Transactions on Information Systems (TOIS) 34, 3 (2016), 14.", "citeRegEx": "Papadopoulos et al\\.,? 2016", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2016}, {"title": "An analysis of the user occupational class through Twitter content", "author": ["Daniel Preotiuc-Pietro", "Vasileios Lampos", "Nikolaos Aletras."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015. 1754\u2013 1764. http://aclweb.org/anthology/P/P15/P15-1169.pdf", "citeRegEx": "Preotiuc.Pietro et al\\.,? 2015", "shortCiteRegEx": "Preotiuc.Pietro et al\\.", "year": 2015}, {"title": "Cantijoch. 2013a. Reading the riots: What were the Police doing on Twitter", "author": ["Rob Procter", "Jeremy Crump", "Susanne Karstedt", "Alex Voss", "Marta"], "venue": "Policing and society 23,", "citeRegEx": "Procter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Procter et al\\.", "year": 2013}, {"title": "Reading the riots on Twitter: methodological innovation for the analysis of big data", "author": ["Rob Procter", "Farida Vis", "Alex Voss."], "venue": "International journal of social research methodology 16, 3 (2013), 197\u2013214.", "citeRegEx": "Procter et al\\.,? 2013b", "shortCiteRegEx": "Procter et al\\.", "year": 2013}, {"title": "Rumor Has It: Identifying Misinformation in Microblogs", "author": ["Vahed Qazvinian", "Emily Rosengren", "Dragomir R. Radev", "Qiaozhu Mei."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP \u201911). 1589\u20131599.", "citeRegEx": "Qazvinian et al\\.,? 2011", "shortCiteRegEx": "Qazvinian et al\\.", "year": 2011}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["Carl Edward Rasmussen", "Christopher K.I. Williams."], "venue": "The MIT Press.", "citeRegEx": "Rasmussen and Williams.,? 2005", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2005}, {"title": "The psychology of rumor", "author": ["Ralph L. Rosnow."], "venue": "American Psychologist, Vol 46(5) (1991), 484\u2013496.", "citeRegEx": "Rosnow.,? 1991", "shortCiteRegEx": "Rosnow.", "year": 1991}, {"title": "Improvised News: A Sociological Study of Rumor", "author": ["Tamotsu Shibutani."], "venue": "Social Research 36(1) (1969), 169\u2013171.", "citeRegEx": "Shibutani.,? 1969", "shortCiteRegEx": "Shibutani.", "year": 1969}, {"title": "Microblog Analysis as a Programme of Work", "author": ["Peter Tolmie", "Rob Procter", "Mark Rouncefield", "Maria Liakata", "Arkaitz Zubiaga."], "venue": "arXiv preprint arXiv:1511.03193 (2015).", "citeRegEx": "Tolmie et al\\.,? 2015", "shortCiteRegEx": "Tolmie et al\\.", "year": 2015}, {"title": "Digital Wildfires: propagation, verification, regulation, and responsible innovation", "author": ["Helena Webb", "Pete Burnap", "Rob Procter", "Omer Rana", "Bernd Carsten Stahl", "Matthew Williams", "William Housley", "Adam Edwards", "Marina Jirotka."], "venue": "ACM Transactions on Information Systems (TOIS) 34, 3 (2016), 15.", "citeRegEx": "Webb et al\\.,? 2016", "shortCiteRegEx": "Webb et al\\.", "year": 2016}, {"title": " Unconfirmed: Classifying Rumor Stance in Crisis-Related Social Media Messages", "author": ["Li Zeng", "Kate Starbird", "Emma S Spiro."], "venue": "Tenth International AAAI Conference on Web and Social Media.", "citeRegEx": "Zeng et al\\.,? 2016", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}, {"title": "Early Detection of Rumors in Social Media from Enquiry Posts", "author": ["Zhe Zhao", "Paul Resnick", "Qiaozhu Mei."], "venue": "International World Wide Web Conference Committee (IW3C2).", "citeRegEx": "Zhao et al\\.,? 2015", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Analysing How People Orient to and Spread Rumours in Social Media by Looking at Conversational Threads", "author": ["Arkaitz Zubiaga", "Maria Liakata", "Rob Procter", "Geraldine Wong Sak Hoi", "Peter Tolmie."], "venue": "PLoS ONE 11, 3 (03 2016), 1\u201329. DOI:http://dx.doi.org/10.1371/journal.pone.0150989", "citeRegEx": "Zubiaga et al\\.,? 2016", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2016}, {"title": "D2.1 Development of an Annotation Scheme for Social Media Rumours", "author": ["Arkaitz Zubiaga", "Peter Tolmie", "Maria Liakata", "Rob Procter"], "venue": "PHEME deliverable", "citeRegEx": "Zubiaga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2014}, {"title": "D2.4 Qualitative Analysis of Rumours, Sources, and Diffusers across Media and Languages", "author": ["Arkaitz Zubiaga", "Peter Tolmie", "Maria Liakata", "Rob Procter"], "venue": "Technical Report. University of Warwick", "citeRegEx": "Zubiaga et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "Previous research has posited the damage that the diffusion of false rumours can cause in society, and that corrections issued by news organisations or state agencies such as the police may not necessarily achieve the desired effect sufficiently quickly [Lewandowsky et al. 2012; Procter et al. 2013a].", "startOffset": 254, "endOffset": 301}, {"referenceID": 20, "context": "Examples of rumours that were later disproven, after being widely circulated, include a 2010 earthquake in Chile, where rumours of a volcano eruption and a tsunami warning in Valparaiso spawned on Twitter [Mendoza et al. 2010].", "startOffset": 205, "endOffset": 226}, {"referenceID": 28, "context": "Another example is the England riots in 2011, where false rumours claimed that rioters were going to attack Birmingham\u2019s Children\u2019s Hospital and that animals had escaped from London Zoo [Procter et al. 2013b].", "startOffset": 186, "endOffset": 208}, {"referenceID": 37, "context": "Previous work by ourselves and others has argued that looking at how users in social media orient to rumours is a crucial first step towards making an informed judgement on the veracity of a rumourous report [Zubiaga et al. 2016; Tolmie et al. 2015; Mendoza et al. 2010].", "startOffset": 208, "endOffset": 270}, {"referenceID": 33, "context": "Previous work by ourselves and others has argued that looking at how users in social media orient to rumours is a crucial first step towards making an informed judgement on the veracity of a rumourous report [Zubiaga et al. 2016; Tolmie et al. 2015; Mendoza et al. 2010].", "startOffset": 208, "endOffset": 270}, {"referenceID": 20, "context": "Previous work by ourselves and others has argued that looking at how users in social media orient to rumours is a crucial first step towards making an informed judgement on the veracity of a rumourous report [Zubiaga et al. 2016; Tolmie et al. 2015; Mendoza et al. 2010].", "startOffset": 208, "endOffset": 270}, {"referenceID": 28, "context": "manually analysed the stance expressed by users in social media towards rumours [Procter et al. 2013b].", "startOffset": 80, "endOffset": 102}, {"referenceID": 36, "context": "\u2014 It can help detect rumours and flag them as such more quickly [Zhao et al. 2015].", "startOffset": 64, "endOffset": 82}, {"referenceID": 15, "context": "\u2014 Aggregate stance information and dynamics over time can be leveraged for rumour veracity classification [Derczynski et al. 2015; Liu et al. 2015].", "startOffset": 106, "endOffset": 147}, {"referenceID": 29, "context": "[Qazvinian et al. 2011]), while others being rule-based and thus with unclear generalisability to new rumours [Zhao et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 36, "context": "2011]), while others being rule-based and thus with unclear generalisability to new rumours [Zhao et al. 2015].", "startOffset": 92, "endOffset": 110}, {"referenceID": 17, "context": "This article substantially extends our earlier short paper [Lukasik et al. 2015a], fistly by using a second dataset, which enables us to test the generalisability of our results.", "startOffset": 59, "endOffset": 81}, {"referenceID": 37, "context": "More specifically, following [Zubiaga et al. 2016], we regard a rumour in the context of breaking news, as a \u201ccirculating story of questionable veracity, which is apparently credible but hard to verify, and produces sufficient skepticism and/or anxiety so as to motivate finding out the actual truth\u201d.", "startOffset": 29, "endOffset": 50}, {"referenceID": 20, "context": "[Mendoza et al. 2010].", "startOffset": 0, "endOffset": 21}, {"referenceID": 25, "context": "Recent research put together in a special issue on rumours and social media [Papadopoulos et al. 2016] also shows the increasing interest of the scientific community in the topic.", "startOffset": 76, "endOffset": 102}, {"referenceID": 34, "context": "[Webb et al. 2016] proposed an agenda for research that establishes an interdisciplinary methodology to explore in", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "The contribution of [Hamdi et al. 2016] to rumour resolution is to build an automated system that rates the level of trust of users in social media, hence enabling to get rid of users with low reputation.", "startOffset": 20, "endOffset": 39}, {"referenceID": 28, "context": "[Procter et al. 2013b].", "startOffset": 0, "endOffset": 22}, {"referenceID": 33, "context": "into the conversational characteristics of microblogging [Tolmie et al. 2015] has motivated our research into automating stance classification as a methodology for accelerating this process.", "startOffset": 57, "endOffset": 77}, {"referenceID": 29, "context": "[Qazvinian et al. 2011] conducted early work on rumour stance classification.", "startOffset": 0, "endOffset": 23}, {"referenceID": 38, "context": ", we conduct a 3-way classification [Zubiaga et al. 2014].", "startOffset": 36, "endOffset": 57}, {"referenceID": 15, "context": "[Liu et al. 2015] introduce rule-based methods for stance classification, which were shown to outperform the approach by [Qazvinian et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 29, "context": "2015] introduce rule-based methods for stance classification, which were shown to outperform the approach by [Qazvinian et al. 2011].", "startOffset": 109, "endOffset": 132}, {"referenceID": 36, "context": "Similarly, [Zhao et al. 2015] use regular expressions instead of an automated method for rumour stance classification.", "startOffset": 11, "endOffset": 29}, {"referenceID": 35, "context": "[Zeng et al. 2016], who explored the use of three different classifiers for automated rumour stance classification on unseen rumours.", "startOffset": 0, "endOffset": 18}, {"referenceID": 36, "context": "Lastly, researchers [Zhao et al. 2015; Ma et al. 2015] have focused on the related task of detecting rumours in social media.", "startOffset": 20, "endOffset": 54}, {"referenceID": 19, "context": "Lastly, researchers [Zhao et al. 2015; Ma et al. 2015] have focused on the related task of detecting rumours in social media.", "startOffset": 20, "endOffset": 54}, {"referenceID": 36, "context": "using pattern-based rumour detection [Zhao et al. 2015].", "startOffset": 37, "endOffset": 55}, {"referenceID": 29, "context": "We do not use the dataset by [Qazvinian et al. 2011] given that it uses a different annotation scheme limited to two categories of stances.", "startOffset": 29, "endOffset": 52}, {"referenceID": 28, "context": "The dataset was analysed and annotated manually as supporting, questioning, or denying a rumour, by a team of social scientists studying the role of social media during the riots [Procter et al. 2013b].", "startOffset": 179, "endOffset": 201}, {"referenceID": 20, "context": "As can be seen from the dataset overview in Table II, different rumours exhibit varying proportions of supporting, denying and questioning tweets, which was also observed in other studies of rumours [Mendoza et al. 2010; Qazvinian et al. 2011].", "startOffset": 199, "endOffset": 243}, {"referenceID": 29, "context": "As can be seen from the dataset overview in Table II, different rumours exhibit varying proportions of supporting, denying and questioning tweets, which was also observed in other studies of rumours [Mendoza et al. 2010; Qazvinian et al. 2011].", "startOffset": 199, "endOffset": 243}, {"referenceID": 28, "context": "The seven rumours were as follows [Procter et al. 2013b]:", "startOffset": 34, "endOffset": 56}, {"referenceID": 12, "context": "Gaussian Processes are a Bayesian non-parametric machine learning framework that has been shown to work well for a range of NLP problems, often beating other state-of-the-art methods [Cohn and Specia 2013; Lampos et al. 2014; Beck et al. 2014; Preotiuc-Pietro et al. 2015].", "startOffset": 183, "endOffset": 272}, {"referenceID": 3, "context": "Gaussian Processes are a Bayesian non-parametric machine learning framework that has been shown to work well for a range of NLP problems, often beating other state-of-the-art methods [Cohn and Specia 2013; Lampos et al. 2014; Beck et al. 2014; Preotiuc-Pietro et al. 2015].", "startOffset": 183, "endOffset": 272}, {"referenceID": 26, "context": "Gaussian Processes are a Bayesian non-parametric machine learning framework that has been shown to work well for a range of NLP problems, often beating other state-of-the-art methods [Cohn and Specia 2013; Lampos et al. 2014; Beck et al. 2014; Preotiuc-Pietro et al. 2015].", "startOffset": 183, "endOffset": 272}, {"referenceID": 26, "context": "We choose this method due to interpretability of results, similar to recent work on occupational class classification [Preotiuc-Pietro et al. 2015].", "startOffset": 118, "endOffset": 147}, {"referenceID": 2, "context": "alisation Model (ICM; [\u00c1lvarez et al. 2012]).", "startOffset": 22, "endOffset": 43}, {"referenceID": 3, "context": "This model has already been applied successfully to NLP regression problems [Beck et al. 2014] and it can also be applied to classification ones.", "startOffset": 76, "endOffset": 94}, {"referenceID": 35, "context": "The selection of these baselines is in line with the classifiers used in recent research on stance classification [Zeng et al. 2016], who found that random forests, followed by logistic regression, performed best.", "startOffset": 114, "endOffset": 132}, {"referenceID": 0, "context": "In addition, multiple occurrences of a character were replaced with a double occurrence [Agarwal et al. 2011], to correct for misspellings and lengthenings, e.", "startOffset": 88, "endOffset": 109}, {"referenceID": 17, "context": "In previous work it has been shown that Brown clusters yield better performance than directly using the BOW features [Lukasik et al. 2015a].", "startOffset": 117, "endOffset": 139}, {"referenceID": 24, "context": "In our experiments, the clusters used were obtained using 1000 clusters acquired from a large scale Twitter corpus [Owoputi et al. 2013], from which we can learn Brown clusters aimed at representing a generalisable Twitter vocabulary.", "startOffset": 115, "endOffset": 136}, {"referenceID": 16, "context": "Retweets are removed from the training set to prevent bias [Llewellyn et al. 2014].", "startOffset": 59, "endOffset": 82}, {"referenceID": 22, "context": "During the experimentation process, we also tested additional features, including the use of the bag of words instead of the Brown clusters, as well as using word embeddings trained from the training sets [Mikolov et al. 2013].", "startOffset": 205, "endOffset": 226}, {"referenceID": 37, "context": "This is a known problem in rumour diffusion, as previous studies have found that people barely deny or question rumours but generally tend to support them irrespective of their actual veracity value [Zubiaga et al. 2016].", "startOffset": 199, "endOffset": 220}, {"referenceID": 28, "context": "While previous research has suggested that mechanisms for exposing false rumours are implicit in the ways in which people use social media [Procter et al. 2013b], it is nevertheless critically important to explore if there are ways in which computational tools can help to accelerate these mechanisms so that misinformation and disinformation can be targeted more rapidly, and the benefits of social media to society maintained [Derczynski et al.", "startOffset": 139, "endOffset": 161}, {"referenceID": 18, "context": "For example, the rumour diffusion patterns [Lukasik et al. 2015b] may be a useful cue for stance classification.", "startOffset": 43, "endOffset": 65}], "year": 2016, "abstractText": "Social media tend to be rife with rumours while new reports are released piecemeal during breaking news. Interestingly, one can mine multiple reactions expressed by social media users in those situations, exploring their stance towards rumours, ultimately enabling the flagging of highly disputed rumours as being potentially false. In this work, we set out to develop an automated, supervised classifier that uses multi-task learning to classify the stance expressed in each individual tweet in a rumourous conversation as either supporting, denying or questioning the rumour. Using a classifier based on Gaussian Processes, and exploring its effectiveness on two datasets with very different characteristics and varying distributions of stances, we show that our approach consistently outperforms competitive baseline classifiers. Our classifier is especially effective in estimating the distribution of different types of stance associated with a given rumour, which we set forth as a desired characteristic for a rumour-tracking system that will warn both ordinary users of Twitter and professional news practitioners when a rumour is being rebutted.", "creator": "LaTeX with hyperref package"}}}