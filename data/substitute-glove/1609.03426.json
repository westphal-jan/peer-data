{"id": "1609.03426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Multi-Label Learning with Provable Guarantee", "abstract": "Here we study three extent of likelihood labels an addition text corpora on then translate without be redesignated multiple formats. The whether consider look trivial when saw number of brand present several, and know one easily solved various called titles of another - vs - all distractors. However, as held example of supplements increases to several 350, new multiplication space becomes extremely heavy, that fact is clear longer prove to use when one - 8/6 - all measurement. Here 'll drafting followed model enterprise but the parameter of current need describing tensors moments, as part as the cross moments where although catalog also instead literally also promising - label prediction. Our different management guaranteed regions direction recently brought extracted formula_4. Further, our styling takes only three onto through then training dataset to poison the parameters, resulting new was significant adaptable regression any can passenger leaving GB ' key latter input forming of millions of documents way houses major thousands of pop intended old nominal nature of a perfect machine one 16GB RAM. Our defined achieves 5x - stancil order especially gear - from through with - result infographics. creating competitive performance entered comparison with limits dropped algorithms.", "histories": [["v1", "Mon, 12 Sep 2016 14:38:08 GMT  (218kb,D)", "http://arxiv.org/abs/1609.03426v1", null], ["v2", "Tue, 13 Sep 2016 23:26:50 GMT  (242kb,D)", "http://arxiv.org/abs/1609.03426v2", null], ["v3", "Sun, 18 Sep 2016 14:57:20 GMT  (169kb,D)", "http://arxiv.org/abs/1609.03426v3", null], ["v4", "Tue, 1 Nov 2016 16:21:54 GMT  (176kb,D)", "http://arxiv.org/abs/1609.03426v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sayantan dasgupta"], "accepted": false, "id": "1609.03426"}, "pdf": {"name": "1609.03426.pdf", "metadata": {"source": "CRF", "title": "Large-Scale Label Prediction for Sparse Data with Probable Guarantees", "authors": ["Sayantan Dasgupta"], "emails": [], "sections": [{"heading": null, "text": "1. Introduction\nMulti-label prediction is one of the most difficult problems in Large-Scale Machine Learning. Unlike the multiclass classification where a text is assigned only one label from a set of labels, here a text can have a variable number of labels. It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4]. A basic approach to the problem is to use 1-vs-all classification technique by training a single binary classifier for every label. However, if there are L labels and the data has D dimension, then these 1-vs-all models require O(DL) parameters. Also, it requires O(DL) steps to predict labels for a new data point. Most of the applications for this task use data with moderate to high dimension, such as text or image datasets, and 1-vs-all models for label prediction is feasible as long as L D. However, as the number of labels increases to a point when L \u223c D, it is no longer possible to use 1-vs-all classifier, since the number of parameters required increases to O(D2), and the model can no longer be stored in the memory.\nRecently there has been attempts to reduce the complexity of such models, by using a low rank mapping \u03a6 : RD \u2192 RL in between the data and the labels. If the rank of such mappings is limited to K D, then the model requires \u0398 ((L+D)K) parameters. Both WSABIE [1] and LEML [5] utilizes such mappings. WSABIE\ndefines weighted approximate pair-wise rank (WARP) loss on such mappings and optimizes the loss on the training dataset. LEML uses similar mapping but generalizes the loss function to squared-loss, sigmoid loss or hinge loss, which are typical to the cases of Linear Regression, Logistic Regression, and Linear SVM respectively.\nBoth of WSABIE and LEML uses low-rank discriminative models, where the low-rank mapping usually has the form Z = HW>, where W \u2208 RD\u00d7K and H \u2208 RL\u00d7K . Here we propose a generative solution for the same problem using latent variable based probabilistic modeling. Unlike the usual cases where such latent variable models are trained using EM, we use Method of Moments [6] to extract the parameters from the latent variable model. We show that our method can be globally convergent when the sample size is larger than a specific lower bound, and establish theoretical bounds for the extracted parameters. We also show the competitive performance of our method regarding classification measures as well as computation time.\n2. Latent Variable Model for Method of Moments\nWe use a generative model as shown in Figure 1. The underlying generative process of the model is described as follows.\n2.1. Generative Model\nLet us assume that there are N documents, the vocabulary size is D, and total number of labels are L. For any document d \u2208 {d1, d2 . . . dN} we first choose a latent state of h \u2208 {1, 2 . . .K} from the discrete distribution P [ h ] , then we choose an word v \u2208 {v1, v2 . . . vD} from the discrete distribution P [ v|h ] , and a label l \u2208 {l1, l2 . . . lL} from the\ndiscrete distribution P [ l|h ] . The generative process is as follows,\nh \u223c Discrete(P [ h ] )\nv \u223c Discrete(P [ v|h ] )\nl \u223c Discrete(P [ l|h ] ) (1)\nLet us denote the probability of the latent variable h assuming the state k \u2208 1 . . .K as,\n\u03c0k = P [ h = k ] (2)\nar X\niv :1\n60 9.\n03 42\n6v 1\n[ cs\n.L G\n] 1\n2 Se\np 20\n16\nLet us define \u00b5k \u2208 RD as the probability vector of all the words conditional to the latent state k \u2208 1 . . .K, i.e.\n\u00b5k = P [ v|h = k ] (3)\nand \u03b3k \u2208 RL as the probability vector of all the labels conditional to the latent state k \u2208 1 . . .K, i.e.\n\u03b3k = P [ l|h = k ] (4)\nLet the matrix O \u2208 RD\u00d7K denote the conditional probabilities for the words, i.e. Oi,k = P [ vi|h = k ] . Then O = [\u00b51|\u00b52| . . . |\u00b5K ]. Similarly, let Q \u2208 RL\u00d7K denote the conditional prob-\nabilities for the words, i.e. Qj,k = P [ lj |h = k ] . Then, Q = [\u03b31|\u03b32| . . . |\u03b3K ]. We assume that the matrix O and Q are of full rank, and their columns are fully identifiable. The aim of our algorithm is to estimate the matrices O, Q and the vector \u03c0.\nFollowing the generative model in equation 1, we can define the probability of individual word as,\nP [vj ] = K\u2211 k=1 P [vj |h]P [h = k] = K\u2211 k=1 \u03c0k[\u00b5k]j\n\u2200j = 1, 2, . . . D\nwhere [\u00b5k]j is the jth element of the vector \u00b5k, for j \u2208 [D].\nTherefore, the average probability of the words across the data can be defined as,\nM1 = P [v1, v2, . . . vD] > = K\u2211 k=1 \u03c0k [[\u00b5k]1, [\u00b5k]1 . . . [\u00b5k]D] >\n= K\u2211 k=1 \u03c0k\u00b5k (5)\nFrom [6], if we define M2 as the pairwise probability matrix, with [M2]i,j = P [ vi, vj ] , we can express it as,\nM2 = K\u2211 k=1 \u03c0k\u00b5k\u00b5k > = K\u2211 k=1 \u03c0k\u00b5k \u2297 \u00b5k (6)\nSimilarly, the tensor M3 defined as the third order probability moment, with [M3]i,j,\u03c4 = P [vi, vj , v\u03c4 ] \u2200i, j, \u03c4 \u2208 {1, 2 . . . D}, can be represented as,\nM3 = K\u2211 k=1 \u03c0k\u00b5k \u2297 \u00b5k \u2297 \u00b5k (7)\nFurther, if we define the cross moment between the labels and the words as M2L, with [M2L]\u03c4,i,j = P [l\u03c4 , vi, vj ], where i, j \u2208 {1, 2 . . . D} and \u03c4 \u2208 {1, 2 . . . L}, then\nM2L = K\u2211 k=1 \u03c0k\u03b3k \u2297 \u00b5k \u2297 \u00b5k (8)\n2.2. Parameter Extraction\nIn this section, we revisit the method to extract the matrices O and Q as well as the latent state probabilities \u03c0. The first step is to whiten the matrix M2, where we try to find a matrix low rank W such that W>M2W = I . This is a method similar to the whitening in ICA, with the covariance matrix being replaced with the co-occurrence probability matrix in our case.\nThe whitening is usually done through eigenvalue decomposition of M2. If the K maximum eigenvalues of M2 are {\u03bdk}Kk=1, and the corresponding eigenvectors are {\u03c9k}Kk=1, then the whitening matrix of rank K is computed as W = \u2126\u03a3\u22121/2, where \u2126 = [ \u03c91|\u03c92| . . . |\u03c9K ] , and \u03a3 = diag(\u03bd1, \u03bd2, . . . , \u03bdK). Upon whitening M2 takes the form\nW>M2W = W >( K\u2211\nk=1\n\u03c0k\u00b5k\u00b5 > k\n) W\n= K\u2211 k=1 (\u221a \u03c0kW >\u00b5k )(\u221a \u03c0kW >\u00b5k )> =\nK\u2211 k=1 \u00b5\u0303k\u00b5\u0303 > k = I (9)\nHence \u00b5\u0303k = \u221a \u03c0kW\n>\u00b5k are orthonormal vectors. Multiplying M3 along all three dimensions by W , we get\nM\u03033 = M3(W,W,W )\n= K\u2211 k=1 \u03c0k(W >\u00b5k)\u2297 (W>\u00b5k)\u2297 (W>\u00b5k)\n= K\u2211 k=1 1 \u221a \u03c0k \u00b5\u0303k \u2297 \u00b5\u0303k \u2297 \u00b5\u0303k (10)\nUpon canonical decomposition of M\u03033, if the eigenvalues and eigenvectors are {\u03bbk}Kk=1 and {uk}Kk=1 respectively, then \u03bbk = 1/\u221a\u03c0k. i.e., \u03c0k = \u03bb\u22122k , and,\nuk = \u00b5\u0303k = \u221a \u03c0kW >\u00b5k = 1\n\u03bbk W>\u00b5k (11)\nThe \u00b5ks can be recovered as \u00b5k = \u03bbkW \u2020uk, where W \u2020 is the pseudo-inverse of W>, i.e., W \u2020 = W ( W>W )\u22121 .\nThe matrix O can be constructed as O = [ \u00b51|\u00b52| . . . |\u00b5K ] . Since we normalize the columns of O as Ovk = Ovk\u2211 v Ovk\n. it is sufficient to compute \u00b5k = W \u2020uk, since \u03bbk will be cancelled during normalization.\nIt is possible to compute the \u03b3k for k = 1 . . .K through the factorization of second and third order moments of the labels. However, it is not possible to match the topics between \u00b51:K and \u03b31:K . Therefore, we use the cross moment M2L between the words and the labels. If we multiply the tensor M2L twice by W , we get\nM\u03032L = M2L(W,W )\n= K\u2211 k=1 \u03c0k\u03b3k \u2297 (W>\u00b5k)\u2297 (W>\u00b5k)\n= K\u2211 k=1 \u03b3k \u2297 ( \u221a \u03c0kW >\u00b5k)\u2297 ( \u221a \u03c0kW >\u00b5k)\n= K\u2211 k=1 \u03b3k \u2297 \u00b5\u0303k \u2297 \u00b5\u0303k (12)\nIf the kth eigenvalue of M\u03033 is uk, then\nu>kM2L(W,W )uk\n= \u00b5\u0303>kM2L(W,W )\u00b5\u0303k\n= \u00b5\u0303>k ( K\u2211 k=1 \u03b3k \u2297 \u00b5\u0303k \u2297 \u00b5\u0303k ) \u00b5\u0303k\n= \u03b3k\ni.e., \u03b3k can be retrieved as u>kM2L(W,W )uk, since \u00b5\u0303k is orthonormal. Thus, we can make sure that \u00b5k and \u03b3k will correspond to the same topic k for k = 1, 2 . . .K.\nTherefore, Q = [ \u03b31|\u03b32| . . . |\u03b3K ] = [ u>1 M2L(W,W )u1|u>2 M2L(W,W )u2| . . .\n. . . |u>KM2L(W,W )uK ]\n= [ u>1 |u2|> . . . |u>K ] M2L(W,W ) [u1|u2| . . . |uK ] = U>M2L(W,W )U\n= M2L(WU,WU) (13)\nwhere, U = [u1|u2| . . . |uK ] are all the K eigenvectors of the tensor M\u03033.\n2.3. Label Prediction\nOnce we have O and \u03c0, the probability of a document d given h can be expressed as,\nP [ d|h = k ] = \u220f v\u2208Wd P [ v|h = k ] (14)\nwhere Wd is the set of distinct words in the document d. Then the document personalization probabilities P [ h =\nk|d ] can be estimated using Bayes Rule.\nP [ h = k|d ] =\nP [ h = k ]\u220f v\u2208Wd P [ v|h = k ]\u2211K k=1 P [ h = k ]\u220f v\u2208Wd P [ v|h = k\n] = \u03c0k \u220f v\u2208Wd Ovk\u2211K\nk=1 \u03c0k \u220f v\u2208Wd Ovk\n(15)\nThen the probability of a label l for the document can be computed as,\nP [ l|d ] = K\u2211 k=1 P [ l|h = k ] P [ h = k \u2223\u2223d] =\nK\u2211 k=1 QlkP [ h = k \u2223\u2223d] (16) The labels are ranked by the probabilities P [ l|d ] , and the labels with highest ranks are assigned to the document. If the number of unique words in a test document is nd = |Wd|, then the prediction step has a complexity of \u0398 ((nd + L)K) to compute the probability for all L labels.\n3. Implementation Detail\nWe create an estimation of the sparse moments M2 by counting the pairwise occurrence of the items across the selections made by all the users in the dataset, and normalizing by the total number of occurrence in each case. For large datasets, this can be achieved in one pass through the dataset using frameworks like Hadoop. Alternately, if X \u2208 RN\u00d7D is the sparse matrix representing the data, then the sum of all the pairwise word count is \u2211N i=1 nnz(xi)\n2, where xi is the word vector of the ith document, and nnz(xi) is the number of distinct words or non-zero entries in that document.\nTherefore, M2 can be estimated as,\nM\u03022 = 1\u2211N\ni=1 nnz(xi) 2 X>X (17)\nAlso, M3 can be estimated as\nM\u03023 = 1\u2211N\ni=1 nnz(xi) 3 X \u2297X \u2297X (18)\nThe dimensions of M2 and M3 are D2 and D3 respectively, but in practice, these quantities are extremely sparse. M2 has a total number of elements O (\u2211N i=1 nnz(xi) 2 )\n, with the worst case occurring when no two documents has any word in common, and all the pairwise counts are 1. The whitening of M2 is carried out through extracting the K maximum eigenvalues and corresponding eigenvectors. This step is the bottleneck of the algorithm. We use the eigs function in Matlab for computing the eigenvalues, which uses Arnoldi\u2019s iterations, and has a complexity\nAlgorithm 1 Method of Moments for Parameter Extraction\nInput: Sparse Data X \u2208 RN\u00d7D, Label Y \u2208 RN\u00d7L and K \u2208 Z+ Output: P [ v|h ] , P [ l|h ]\nand \u03c0 1) Estimate\nM\u03022 = 1\u2211N\ni=1 nnz(xi) 2 X>X (pass #1)\n2) Compute maximum eigenvalues K of M\u03022 as {\u03bdk}Kk=1, and corresponding eigenvectors as {\u03c9k}Kk=1. Define \u2126 =[ \u03c91|\u03c92| . . . |\u03c9K ] , and \u03a3 = diag (\u03bd1, \u03bd2, . . . , \u03bdK) 3) Estimate the whitening matrix W\u0302 = \u2126\u03a3\u22121/2 so that W\u0302>M\u03022W\u0302 = IK\u00d7K 4) Estimate\n\u02c6\u0303M3 = 1\u2211N\ni=1 nnz(xi) 3 XW\u0302 \u2297XW\u0302 \u2297XW\u0302 (pass #2)\n5) Compute eigenvalues {\u03bbk}Kk=1 and eigenvectors {uk}Kk=1 of \u02c6\u0303M3. Assign U\u0302 = [u1|u2 . . . |uK ]. 6) Estimate the columns of O as \u00b5\u0302k = W\u0302 \u2020uk and \u03c0\u0302k = \u03bb\u22122k , \u2200k \u2208 1, 2 . . .K 7) Assign O\u0302 = [\u02c6\u0304\u00b51| \u02c6\u0304\u00b52| . . . | \u02c6\u0304\u00b5K ] & \u03c0\u0302 = [\u03c0\u03021, \u03c0\u03022 . . . \u03c0\u0302K ]> 8) Estimate\nQ\u0302 = 1\u2211N\ni=1 nnz(xi) 2nnz(yi)\nY \u2297XW\u0302U\u0302 \u2297XW\u0302U\u0302 (pass #3)\n9) Estimate P [ v|h = k ] = O\u0302vk\u2211\nv O\u0302vk ,\u2200k \u2208 1 . . .K, v \u2208 v1 . . . vD P [ l|h = k ] = Q\u0302lk\u2211\nl Q\u0302lk ,\u2200k \u2208 1 . . .K, l \u2208 l1 . . . lL\nO ( ( \u2211N\ni=1 nnz(xi) 2)K\n) , since the total number of non-\nzero entries in M2 is O (\u2211N i=1 nnz(xi) 2 )\n[7]. As for M3, we do not need to explicitly compute it. Since M\u03033 = M3(W,W,W ), we can estimate M\u03033 right away as,\n\u02c6\u0303M3 = 1\u2211N\ni=1 nnz(xi) 3 XW \u2297XW \u2297XW (19)\nComputing \u02c6\u0303M3 takes a second pass through the entire dataset, and has a complexity of O(NK3).\nSimilarly, if Y \u2208 RN\u00d7L represents the labels for N documents, M2L can be estimated as,\nM\u03022L = 1\u2211N\ni=1 nnz(xi) 2nnz(yi)\nY \u2297X \u2297X (20)\nwhere yi is the label vector of ith document, and nnz(yi) is the number of distinct labels in that document. We do not need to compute M2L either. Once we obtain the eigenvectors U of M\u03033, since Q = M2L(WU,WU) from Equation 13, we can estimate Q right away as,\nQ\u0302 = 1\u2211N\ni=1 nnz(xi) 2nnz(yi)\nY \u2297XWU \u2297XWU (21)\nThis step has a complexity of O(K2 \u2211D\ni=1 nnz(yi)). The entire algorithm is outlined as Algorithm 1. The overall complexity is O ( ( \u2211N\ni=1 nnz(xi) 2)K +K2 \u2211D i=1 nnz(yi) +NK 3 )\n. We used the Tensor Toolbox [8] for tensor decomposition.\nOnce the matrix O and \u03c0 are extracted, it requires one more pass through the entire dataset to compute P [l|h], resulting in a total of three passes to extract all parameters. The label prediction step has a complexity of \u0398 ((nd + L)K) for a document with distinct number of words nd.\nTheorem 1. Let us assume that we draw N i.i.d samples x1, x2 . . . xN with labels y1, y2 . . . yN using the generative process in Equation 1 with bounded support such that\n||x|| \u2264 1. Let us define \u03b51 = ( 1 + \u221a log(1/\u03b4) 2 ) , and\n\u03b52 =\n( 1 + \u221a log(2/\u03b4)\n2\n) for some \u03b4 \u2208 (0, 1). Then, if the\nnumber of samples N \u2265 max(n1, n2, n3), where\n\u2022 n1 = c2\n( logK + log log ( K c1 \u00b7 \u221a \u03c0max \u03c0min )) \u2022 n2 = \u2126 (( \u03b51\nd\u03032s\u03c3K(M2) )2) \u2022 n3 = \u2126 ( K2 (\n10 d\u03032s\u03c3K(M2)5/2\n+ 2 \u221a\n2 d\u03033s\u03c3K(M2)3/2\n)2 \u03b521 ) for some constants c1 and c2, and we run Algorithm 1 on these N samples, then the following bounds on the estimated parameters hold with probability at least 1\u2212 \u03b4,\n||\u00b5k \u2212 \u00b5\u0302k|| \u2264 ( 160 \u221a \u03c31(M2)\nd\u03032s\u03c3K(M2)5/2 +\n32 \u221a\n2\u03c31(M2) d\u03033s\u03c3K(M2)3/2\n+ 4 \u221a \u03c31(M2)\nd\u03032s\u03c3K(M2)\n) \u03b51\u221a N ,\n||\u03b3k \u2212 \u03b3\u0302k|| \u2264 1024\u03c3K(M2) ( 5 \u03c3K(M2)5/2 + \u221a 2 \u03c3K(M2)3/2 )2 \u03b521 (d\u03033s)2N\n+ 16\u03c3K(M2)3 \u03b521 (d\u03032s)2N + 4\u03c3K(M2) \u03b52 d\u0303ls \u221a N ,\nand |\u03c0k \u2212 \u03c0\u0302k| \u2264 (\n200 \u03c3K(M2)5/2\n+ 40 \u221a\n2 \u03c3K(M2)3/2\n) \u03b51\nd\u03033s \u221a N , where \u03c31(M2) . . . \u03c3K(M2) are the K largest eigenvalues of the pairwise probability matrix M2, d\u03032s = \u2211N i=1 nnz(xi)\n2, d\u03033s = \u2211N i=1 nnz(xi) 3 and d\u0303ls = \u2211N i=1 nnz(xi) 2nnz(yi) The proof is included in the appendix.\n4. Experimental Results\nWe used six datasets for our methods, as described in table 1. The datasets range from small datasets like Bibtex with 4880 training instances with 159 labels to large datasets like WikiLSHTC with around 1.7M training instances with 325K labels. Since LEML is shown to outperform WSABIE and other benchmark algorithms on various small and large-scale datasets in [5], we benchmark the performance of our method against LEML. Also both LEML and MoM has similar model complexity due to similar number (\u0398 ((L+D)K)) of parameters for the same latent dimensionality K. For LEML, we ran ten iterations for the smaller datasets (Bibtex and Delicious) and five iterations for the larger datasets, since the authors of LEML chose a similar number of iterations for their experiments in [5]. We measured AUC (of Receiver Operating Characteristics (ROC)) against K. AUC is a versatile measure, and is used to evaluate the performance of classification as well as prediction algorithms [9]. Also, it is shown that there exists a one-to-one relation between AUC and PrecisionRecall curve in [10], i.e., a classifier with higher AUC will also achieve better Precision and Recall. We carried out our experiments on Unix Platform on a single machine with Intel\ni5 Processor (2.4GHz) and 16GB memory, and no multithreading or any other performance enhancement method is used in the code. For AmazonCat and WikiLSHTC datasets, we ran LEML on an i2.4xlarge instance of Amazon EC2 with 122 GB of memory, since LEML needs significantly larger memory for these two datasets (Figure 2).\nWe computed AUC for every test documents and perform a macro-averaging across the documents, and repeat the experiments for K = {50, 75, 100, 125, 150} (Figure 2). Both LEML and Method of Moments perform very similarly, but the memory footprint (Figure 2) of MoM is significantly less than LEML. MoM takes longer to finish for the smaller datasets like Bibtex or Delicious since tensor factorization takes a lot more time compared to the LEML iterations on smaller datasets. However, for the larger datasets, each iteration of LEML becomes extremely costly, and MoM takes a fraction of the time taken by LEML. For WikiLSHTC dataset, LEML takes more than two days to finish, while MoM finished within a few hours. The runtime as well as speed-up is shown in Table 3 for K = 100. Due to the large discrepancy between the runtime of LEML and MoM for the larger datasets, we do not give a detailed plot of runtime vs. K.\n5. Conclusion\nHere we propose a method for multi-label prediction for large-scale datasets based on moment factorization. Our method (MoM) gives similar performance in comparison with state-of-art algorithms like LEML while taking a fraction of time and memory for the larger datasets. MoM takes\nonly three passes through the training dataset to extract all the parameters. Since MoM consists of only linear algebraic operations, it is embarrassingly parallel, and can easily be scaled up in any parallel eco-system using linear algebra libraries. In our implementation, we used Matlab\u2019s linear algebra library based on LAPACK/ARPACK, although we did not incorporate any parallelization.\nBoth LEML and MoM have error bound of O(1/ \u221a N) on training performance w.r.t. the number of training samples N . However, when we compute the AUC on test dataset, the AUC of LEML decreases with latent dimensionality(K) for some datasets, including the larger dataset of AmazonCat containing more than 1M training instance. This shows the possibility of over-fitting in LEML. MoM, on the other hand, is not an optimization algorithm, and the parameters are extracted from Moment Factorization\nrather than optimizing any target function. It is not susceptible to over-fitting, which is evident from its performance. On the other hand, MoM has the requirement N \u2265 \u2126(K2) on the number of documents in the training set, and it will not work if N < \u0398(K2). However, for smaller text corpora where N < \u0398(K2) hold, 1-vs-all classifiers are usually sufficient to predict the labels. We need dimensionality reduction techniques for large text corpora where 1-vs-all classifiers fail, and MoM provides a very competitive choice for such cases.\nReferences\n[1] J. Weston, S. Bengio, and N. Usunier, \u201cWsabie: Scaling up to large vocabulary image annotation,\u201d in IJCAI, vol. 11, 2011, pp. 2764\u2013 2770.\n[2] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, \u201cLarge-scale video classification with convolutional neural networks,\u201d in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2014, pp. 1725\u20131732.\n[3] T. N. Rubin, A. Chambers, P. Smyth, and M. Steyvers, \u201cStatistical topic models for multi-label document classification,\u201d Machine learning, vol. 88, no. 1-2, pp. 157\u2013208, 2012.\n[4] R. Agrawal, A. Gupta, Y. Prabhu, and M. Varma, \u201cMulti-label learning with millions of labels: Recommending advertiser bid phrases for web pages,\u201d in Proceedings of the 22nd international conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2013, pp. 13\u201324.\n[5] H.-F. Yu, P. Jain, P. Kar, and I. S. Dhillon, \u201cLarge-scale multi-label learning with missing labels,\u201d in ICML, vol. 31, 2014.\n[6] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky, \u201cTensor decompositions for learning latent variable models,\u201d Journal of Machine Learning Research, vol. 15, pp. 2773\u20132832, 2014. [Online]. Available: http://jmlr.org/papers/v15/anandkumar14b.html\n[7] J. Lee, V. Balakrishnan, C.-K. Koh, and D. Jiao, \u201cFrom o (k 2 n) to o (n): a fast complex-valued eigenvalue solver for large-scale onchip interconnect analysis,\u201d in Microwave Symposium Digest, 2009. MTT\u201909. IEEE MTT-S International. IEEE, 2009, pp. 181\u2013184.\n[8] B. W. Bader, T. G. Kolda et al., \u201cMatlab tensor toolbox version 2.6,\u201d Available online, February 2015. [Online]. Available: http://www.sandia.gov/\u223ctgkolda/TensorToolbox/\n[9] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme, \u201cBpr: Bayesian personalized ranking from implicit feedback,\u201d in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009, pp. 452\u2013461.\n[10] J. Davis and M. Goadrich, \u201cThe relationship between precision-recall and roc curves,\u201d in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 233\u2013240.\n[11] Y. Wang and J. Zhu, \u201cSpectral methods for supervised topic models,\u201d in Advances in Neural Information Processing Systems, 2014, pp. 1511\u20131519.\n[12] T. G. Kolda and J. R. Mayo, \u201cShifted power method for computing tensor eigenpairs,\u201d SIAM Journal on Matrix Analysis and Applications, vol. 32, no. 4, pp. 1095\u20131124, October 2011.\n[13] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain, \u201cSparse local embeddings for extreme multi-label classification,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 730\u2013738.\n[14] Y. Prabhu and M. Varma, \u201cFastxml: A fast, accurate and stable treeclassifier for extreme multi-label learning,\u201d in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 263\u2013272.\n[15] A. T. Chaganty and P. Liang, \u201cSpectral experts for estimating mixtures of linear regressions,\u201d arXiv preprint arXiv:1306.3729, 2013.\nAppendix A. Vector Norms\nLet the true pairwise probability matrix and the third order probability moment be M2 and M3. Let us assume that we select N i.i.d. samples x1, . . . xN from the population, and the estimates of pairwise matrix and third order moment are M\u03022 and M\u03023. Let \u03b5M2 = ||M2\u2212M\u03022||2. We use the second order operator norm of the matrices here. Let us assume \u03b5M2 \u2264 \u03c3K(M2)/2, where \u03c3K is the Kth largest eigenvalue of M2. We will derive the conditions which satisfies this later.\nIf \u03a3 = diag(\u03c31, \u03c32 . . . \u03c3K) are the top-K eigenvalues of M2, and U are the corresponding eigenvectors, then the whitening matrix W = U\u03a3\u22121/2. Also, W>M2KW = IK\u00d7K , where M2K is the K rank approximation of M2. Then,\n||W ||2 = \u221a max eig(W>W ) = \u221a max eig(\u03a3\u22121)\n= 1\u221a\n\u03c3K(M2)\nSimilarly, if W \u2020 = W (W>W )\u22121, then W \u2020 = W\u03a3 = U\u03a31/2. Therefore,\n||W \u2020||2 = \u221a max eig(\u03a3) = \u221a \u03c31(M2) (22)\nLet W\u0302 be the whitening matrix for M\u03022, i.e., W\u0302>M\u03022W\u0302 = IK\u00d7K . Then by Weyl\u2019s inequality, \u03c3k(M2)\u2212 \u03c3k(M\u03022) \u2264 ||M2 \u2212 M\u03022||,\u2200k = 1, 2 . . .K.\nTherefore,\n||W\u0302 ||22 = 1\n\u03c3K(M\u03022)\n\u2264 1 \u03c3K (M2)\u2212 ||M2 \u2212 M\u03022|| \u2264 2 \u03c3K (M2)\n(23)\nAlso, by Weyl\u2019s Theorem,\n||W\u0302 \u2020||22 = \u03c31(M\u03022) \u2264 \u03c31(M2) + \u03b5M2 \u2264 1.5\u03c31(M2) =\u21d2 ||W\u0302 \u2020||2 \u2264 \u221a 1.5\u03c31(M2) \u2264 1.5 \u221a \u03c31(M2) (24)\nLet D be the eigenvectors of W\u0302M2W\u0302 , and A be the corresponding eigenvalues. Then we can write, W\u0302M2W\u0302=ADA>. Then W = W\u0302AD\u22121/2A> whitens M2, i.e., W>M2W = I . Therefore,\n||I \u2212D||2 = ||I \u2212ADA>||2 = ||I \u2212 W\u0302M2W\u0302 ||2 = ||W\u0302M\u03022W\u0302 \u2212 W\u0302M2W\u0302 ||2 \u2264 ||W\u0302 ||22||M2 \u2212 M\u03022||\n\u2264 2 \u03c3K (M2) \u03b5M2 (25)\n\u03b5W = ||W \u2212 W\u0302 ||2 = ||W \u2212WAD1/2A>||2 = ||W ||2||I \u2212AD1/2A>||2 = ||W ||2||I \u2212D1/2||2 \u2264 ||W ||2||I \u2212D1/2||2||I +D1/2||2 = ||W ||2||I \u2212D||2\n\u2264 2 \u03c3K(M2)3/2 \u03b5M2\n\u03b5W \u2020 = ||W \u2020 \u2212 W\u0302 \u2020||2 = ||W\u0302 \u2020AD1/2A> \u2212 W\u0302 \u2020||2 = ||W\u0302 \u2020||2||I \u2212AD1/2A>||2 \u2264 ||W\u0302 \u2020||2||I \u2212D||2\n\u2264 2 \u221a \u03c31(M2)\n\u03c3K (M2) \u03b5M2 (26)\nAppendix B. Tensor Norm\nLet us define the second order operator norm of a tensor T \u2208 RD\u00d7D\u00d7D as,\n||T ||2 = sup u {|T (u, u, u)| : u \u2208 RD&||u|| = 1} (27)\nLemma 1. For a tensor T \u2208 RD\u00d7D\u00d7D, ||T ||2 \u2264 ||T ||F , where ||T ||F is the Frobenius norm defined as,\n||T ||F = \u221a\u2211 i,j,k (Ti,j,k)2 (28)\nProof. For any real matrix A, ||A||2 \u2264 ||A||F . Let us unfold the tensor T as the collection of D matrices, as, T = {T1, T2 . . . TD}. Then,\nT (u, u, u) = u>[T1u|T2u| . . . |TKu]u = \u3008[u>T1u, u>T2u, . . . u>TKu], u\u3009 (29)\nTherefore,\n||T ||2 = sup u {|T (u, u, u)| : u \u2208 RD&||u|| = 1} = sup u { \u2223\u2223\u3008[u>T1u, u>T2u, . . . , u>TKu], u\u3009\u2223\u2223 : u \u2208 RD\n&||u|| = 1} (30)\nUsing Cauchy-Schwarz inequality,\n||T ||2 \u2264 sup u { \u2223\u2223\u2223\u2223[u>T1u, u>T2u, . . . u>TKu]\u2223\u2223\u2223\u2223 ||u||\n: u \u2208 RD&||u|| = 1} = sup u { \u2223\u2223\u2223\u2223[u>T1u, u>T2u, . . . u>TKu]\u2223\u2223\u2223\u2223\n: u \u2208 RD&||u|| = 1} = \u2223\u2223\u2223\u2223[ ||T1||2 , ||T2|| , . . . ||TD|| ]\u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223[ ||T1||F , ||T2||F , . . . ||TD||F ]\u2223\u2223\u2223\u2223\n= \u221a( ||T1||2F + ||T2|| 2 F + \u00b7 \u00b7 \u00b7+ ||TD||F ) = ||T ||F (31)\nLet us define \u03b5M3 = ||M3\u2212M\u03023||2. Then from Appendix B in [15],\n\u03b5tw = ||M3(W,W,W )\u2212 M\u03023(W\u0302 , W\u0302 , W\u0302 )||2 \u2264 ||M3||2 ( ||W\u0302 ||22 + ||W\u0302 ||2||W ||2 + ||W ||22 ) \u03b5W\n+ ||W\u0302 ||3\u03b5M3\n\u2264 ||M3||2 (2 +\n\u221a 2 + 1)\n\u03c3K(M2) \u03b5W +\n2 \u221a 2\n\u03c3K(M2)3/2 \u03b5M3\n\u2264 ||M3||2 (3 +\n\u221a 2)\n\u03c3K(M2) \u00b7 2 \u03c3K(M2)3/2\n\u03b5M2 + 2 \u221a 2\n\u03c3K(M2)3/2 \u03b5M3\n\u2264 10||M3||2 \u03c3K(M2)5/2\n\u00b7 \u03b5M2 + 2 \u221a 2\n\u03c3K(M2)3/2 \u03b5M3 \u2264 ( 10\nd\u03032s\u03c3K(M2)5/2 +\n2 \u221a 2\nd\u03033s\u03c3K(M2)3/2 ) 2\u03b5\u221a N\n(32)\nPlease note that ||M3||2 \u2264 ||M3||F \u2264 1, because M3 is a tensor with individual elements as probabilities.\nLemma 2. (Robust Power Method from [6]) If T\u0302 = T + E \u2208 RK\u00d7K\u00d7K , where T is an symmetric tensor with orthogonal decomposition T = \u2211K k=1 \u03bbkuk \u2297 uk \u2297 uk with each \u03bbk > 0, and E has operator norm ||E||2 \u2264 . Let \u03bbmin = min K k=1{\u03bbk} and \u03bbmax = maxKk=1{\u03bbk}. Let there exist constants c1, c2 such that \u2264 c1 \u00b7 (\u03bbmin/K), and N \u2265 c2(logK + log log (\u03bbmax/ )). Then if Algorithm 1 in [6] is called for K times, with L = poly(K) log(1/\u03b7) restarts each time for some \u03b7 \u2208 (0, 1), then with probability at least 1 \u2212 \u03b7, there exists a permutation \u03a0 on [K], such that,\n||u\u03a0(k) \u2212 u\u0302k|| \u2264 8 \u03bb\u03a0(k) , |\u03bbk \u2212 \u03bb\u03a0(k)| \u2264 5 \u2200k \u2208 [K]\n(33)\nSince \u03bbk = 1 \u221a \u03c0k , \u2200k \u2208 [K] (34)\nTherefore, we need, N \u2265 c2 ( logK + log log ( K\u03bbmax c1\u03bbmin )) = c2 ( logK + log log ( K c1 \u00b7 \u221a \u03c0max \u03c0min )) (35)\nThis contributes in the first lower bound (n1) of N in Theorem 1.\nAppendix C. Tail Inequality\nLemma 3. If we draw N i.i.d. documents x1, x2 . . . xN through the generative process in Equation 1, with the labels as y1, y2 . . . yN , and the vectors probability mass function of the words v estimated from these N samples are p\u0302(v) whereas the true p.m.f is p(v) with v \u2208 {v1, v2 . . . vD} , then with probability at least 1\u2212 \u03b4 with \u03b4 \u2208 (0, 1),\n||p\u0302(v)\u2212 p(v)||F \u2264 2\nd\u03031s \u221a N\n( 1 + \u221a log(1/\u03b4)\n2 ) (36)\n||p\u0302(v, v)\u2212 p(v, v)||F \u2264 2\nd\u03032s \u221a N\n( 1 + \u221a log(1/\u03b4)\n2 ) (37)\n||p\u0302(v, v, v)\u2212 p(v, v, v)||F \u2264 2\nd\u03033s \u221a N\n( 1 + \u221a log(1/\u03b4)\n2 ) (38)\nwhere, d\u03031s = 1N \u2211N\ni=1 nnz(xi), d\u03032s = 1 N \u2211N i=1 nnz(xi) 2, d\u03033s = 1N \u2211N i=1 nnz(xi) 3, and nnz(xi) is the non-zero entries in row xi of the data X as described in section 3.\nProof. The generative process in Equation 1 results in N sample documents x1:N that are vectors of count data, with\u2211\nv[x]v = nd, where x is the sample corresponding to the document d, and nd is the sum of the counts of all distinct words in that vector. The operation \u2211 v denotes the sum\nacross the dimensions. From here, we can show that ||x|| =\u221a\u2211 v[x] 2 v \u2264 \u2211 v[x]v = nd, since [x]v \u2265 0,\u2200v \u2208 1, 2 . . . D. Therefore, the samples have bounded norm. Without loss of generality, if we assume ||x|| \u2264 1 \u2200x \u2208 X , then from Lemma 7 of supplementary material of [11], with probability at least 1\u2212 \u03b4 with \u03b4 \u2208 (0, 1),\n\u2223\u2223\u2223\u2223\u2223\u2223E\u0302[x]\u2212 E[x]\u2223\u2223\u2223\u2223\u2223\u2223 F \u2264 2\u221a\nN\n( 1 + \u221a log(1/\u03b4)\n2 ) (39)\u2223\u2223\u2223\u2223\u2223\u2223E\u0302[x\u2297 x]\u2212 E[x\u2297 x]\u2223\u2223\u2223\u2223\u2223\u2223\nF \u2264 2\u221a\nN\n( 1 + \u221a log(1/\u03b4)\n2 ) (40)\u2223\u2223\u2223\u2223\u2223\u2223E\u0302[x\u2297 x\u2297 x]\u2212 E[x\u2297 x\u2297 x]\u2223\u2223\u2223\u2223\u2223\u2223\nF \u2264 2\u221a\nN\n( 1 + \u221a log(1/\u03b4)\n2 ) (41)\nwhere E stands for true expectation, and E\u0302 stands for the expectation estimated from the N samples, i.e.,\nE\u0302[x] = 1\nN N\u2211 i=1 xi = 1 N X>1\nE\u0302[x\u2297 x] = 1 N N\u2211 i=1 xi \u2297 xi = 1 N X>X\nE\u0302[x\u2297 x\u2297 x] = 1 N N\u2211 i=1 xi \u2297 xi \u2297 xi = 1 N X \u2297X \u2297X\nNow, since each of our samples x1:N contains binary data, probability of the items can be estimated from the training data as p\u0302(v) = E\u0302[x]\u2211\nv E\u0302[x] , where\n\u2211 v E\u0302[x] is the sum\nof E\u0302[x] across the dimensions, i.e., all the items. Also, it can be shown that \u2211 v E\u0302[x] = d\u03031s. Therefore p\u0302(v) = E\u0302[x] d\u03031s .\nPlease note that \u2211 v E[x] \u2248 \u2211\nv E\u0302[x] = d\u03031s, and therefore, p\u0302(v)\u2212 p(v) = 1\nd\u03031s (E\u0302[x]\u2212E[x]), and using this in Equation\n39, we get the first inequality of the Lemma (Equation 36). Since d\u03032s = \u2211 v \u2211 v E\u0302[x \u2297 x] and d\u03033s =\u2211\nv \u2211 v \u2211 v E\u0302[x\u2297x\u2297x], the pairwise and triple-wise prob-\nability matrices can be estimated as,\np\u0302(v, v) = E\u0302[x\u2297 x]\u2211\nv \u2211 v E\u0302[x\u2297 x] = E\u0302[x\u2297 x] d\u03032s\np\u0302(v, v, v) = E\u0302[x\u2297 x\u2297 x]\u2211\nv \u2211 v \u2211 v E\u0302[x\u2297 x\u2297 x] = E\u0302[x\u2297 x\u2297 x] d\u03033s\nSince \u2211\nv \u2211 v E[x \u2297 x] \u2248 \u2211 v \u2211 v E\u0302[x \u2297 x] = d\u03032s, and\u2211\nv \u2211 v \u2211 v E[x\u2297x\u2297x] \u2248 \u2211 v \u2211 v \u2211 v E\u0302[x\u2297x\u2297x] = d\u03033s,\nwe can establish the following equations,\np\u0302(v, v)\u2212 p(v, v) = 1 d\u03032s\n( E\u0302[x\u2297 x]\u2212 E[x\u2297 x] ) p\u0302(v, v, v)\u2212 p(v, v, v) = 1\nd\u03033s\n( E\u0302[x\u2297 x\u2297 x]\u2212 E[x\u2297 x\u2297 x] ) Substituting these equations in Equation 40 and 41, we\ncomplete the proof.\nAlso, if yi represents the label vector associated with ith document, whereas xi represent the word vector,\nE\u0302[y \u2297 x\u2297 x]\u2212 E[y \u2297 x\u2297 x]\n= 1\nN N\u2211 i=1 yi \u2297 xi \u2297 xi \u2212 E[y \u2297 x\u2297 x]\n= 1\nN N\u2211 i=1 yi \u2297 xi \u2297 xi \u2212 1 N N\u2211 i=1 yi \u2297 E[x\u2297 x]\n+ 1\nN N\u2211 i=1 yi \u2297 E[x\u2297 x]\u2212 E[y \u2297 x\u2297 x]\n(42)\nTherefore,\u2223\u2223\u2223\u2223\u2223\u2223E\u0302[y \u2297 x\u2297 x]\u2212 E[y \u2297 x\u2297 x]\u2223\u2223\u2223\u2223\u2223\u2223 F\n\u2264 \u2223\u2223\u2223\u2223\u2223\u2223E\u0302[y]\u2223\u2223\u2223\u2223\u2223\u2223\nF \u2223\u2223\u2223\u2223\u2223\u2223E\u0302[x\u2297 x]\u2212 E[x\u2297 x]\u2223\u2223\u2223\u2223\u2223\u2223 F\n+ ||E[x\u2297 x]||F \u2223\u2223\u2223\u2223\u2223\u2223E\u0302[y]\u2212 E[y]\u2223\u2223\u2223\u2223\u2223\u2223\nF\nWithout loss of generality, we can assume ||y|| \u2264 1. Then, ||E\u0302[y]|| \u2264 1.\nFrom Equation 39 and 40,\nP [\u2223\u2223\u2223\u2223\u2223\u2223E\u0302[y]\u2212 E[y]\u2223\u2223\u2223\u2223\u2223\u2223 F \u2265 2\u221a\nN\n( 1 + \u221a log(1/\u03b4)\n2\n)] \u2264 \u03b4\n(43)\nP [\u2223\u2223\u2223\u2223\u2223\u2223E\u0302[x\u2297 x]\u2212 E[x\u2297 x]\u2223\u2223\u2223\u2223\u2223\u2223 F \u2265 2\u221a\nN\n( 1 + \u221a log(1/\u03b4)\n2 )] \u2264 \u03b4\nTherefore, using union-bound principle on the above two probability,\nP [ \u2223\u2223\u2223\u2223\u2223\u2223E\u0302[x]\u2223\u2223\u2223\u2223\u2223\u2223\nF \u2223\u2223\u2223\u2223\u2223\u2223E\u0302[x\u2297 x]\u2212 E[x\u2297 x]\u2223\u2223\u2223\u2223\u2223\u2223 F\n+ ||E[x\u2297 x]||F \u2223\u2223\u2223\u2223\u2223\u2223E\u0302[y]\u2212 E[y]\u2223\u2223\u2223\u2223\u2223\u2223\nF\n\u2265 4\u221a N\n( 1 + \u221a log(1/\u03b4)\n2\n)] \u2264 2\u03b4\nUsing Equation 42, and replacing \u03b4 by \u03b4/2,\nP [ \u2223\u2223\u2223\u2223\u2223\u2223E\u0302[y \u2297 x\u2297 x]\u2212 E[y \u2297 x\u2297 x]\u2223\u2223\u2223\u2223\u2223\u2223\nF\n\u2264 4\u221a N\n( 1 + \u221a log(2/\u03b4)\n2\n)] \u2265 1\u2212 \u03b4\np\u0302(l, v, v) = E\u0302[y \u2297 x\u2297 x]\u2211\nv \u2211 v \u2211 v E\u0302[y \u2297 x\u2297 x] = E\u0302[y \u2297 x\u2297 x]\nd\u0303ls (44) Also, since \u2211\nl \u2211 v \u2211 v E[y\u2297x\u2297x] \u2248 \u2211 l \u2211 v \u2211 v E\u0302[y\u2297\nx \u2297 x] = d\u0303ls, where d\u0303ls = 1N \u2211N i=1 nnz(yi)nnz(xi) 2, and nnz(yi) is the number of labels associated with the ith document. In a similar way to the proof of Lemma 3, we can prove that with probability at least 1\u2212 \u03b4,\n||p\u0302(l, v, v)\u2212 p(l, v, v)||F \u2264 4\nd\u0303ls \u221a N\n( 1 + \u221a log(2/\u03b4)\n2\n) (45)\nAssigning \u03b51 = ( 1 + \u221a log(1/\u03b4) 2 ) , and \u03b52 =(\n1 + \u221a\nlog(2/\u03b4) 2\n) , we get\n\u03b5M2 \u2264 ||p(v, v)\u2212 p\u0302(v, v)||F \u2264 2\u03b51\nd\u03032s \u221a N\n\u03b5M3 \u2264 ||p(v, v, v)\u2212 p\u0302(v, v, v)||F \u2264 2\u03b51\nd\u03033s \u221a N\n\u03b5M2L \u2264 ||p(l, v, v)\u2212 p\u0302(l, v, v)||F \u2264 4\u03b52\nd\u0303ls \u221a N\nsince operator norm is smaller than Frobenius norm. Also, to satisfy \u03b5M2 \u2264 \u03c3K(M2)/2, we need,\nN \u2265 \u2126 ( 1 d\u03032s\u03c3K(M2) ( 1 + \u221a log(1/\u03b4) 2 ))2 (46) Or, N \u2265 \u2126 (( \u03b51\nd\u03032s\u03c3K(M2)\n)2) . This contributes in the\nsecond lower bound (n2) of N in Theorem 1. Also, from Equation 32,\n\u03b5tw \u2264 (\n10\nd\u03032s\u03c3K(M2)5/2 +\n2 \u221a 2\nd\u03033s\u03c3K(M2)3/2 ) 2\u03b51\u221a N (47)\nFrom Lemma 2, \u2264 c1 \u00b7 (\u03bbmin/K), and we can assign as the upper bound of \u03b5tw. To satisfy this, we need,(\n10\nd\u03032s\u03c3K(M2)5/2 +\n2 \u221a 2\nd\u03033s\u03c3K(M2)3/2 ) 2\u03b51\u221a N \u2264 c1 \u03bbmin K\n, or,( 10\nd\u03032s\u03c3K(M2)5/2 +\n2 \u221a 2\nd\u03033s\u03c3K(M2)3/2 ) 2\u03b51\u221a N \u2264 c1\n1\nK \u221a \u03c0max\nSince \u03c0max \u2264 1, we need\nN \u2265 \u2126 ( K2 (\n10\nd\u03032s\u03c3K(M2)5/2 +\n2 \u221a 2\nd\u03033s\u03c3K(M2)3/2\n)2 \u03b521 ) This contributes to n3 in Theorem 1.\nAppendix D. Completing the Proof\nHere, we will derive the final bounds for the reconstruction error for the parameters. Since \u00b5k = W \u2020uk (Algorithm 1), with probability at least 1\u2212 \u03b4,\n||\u00b5k \u2212 \u00b5\u0302k|| = ||W \u2020uk \u2212 W\u0302 \u2020u\u0302k|| = ||W \u2020uk \u2212W \u2020u\u0302k +W \u2020u\u0302k \u2212 W\u0302 \u2020u\u0302k|| \u2264 ||W \u2020||2||uk \u2212 u\u0302k||+ ||W \u2020 \u2212 W\u0302 \u2020||2||u\u0302k||\n\u2264 ||W \u2020||2 8\n\u03bbk + \u03b5W \u2020\n\u2264 8 \u221a \u03c31(M2) +\n2 \u221a \u03c31(M2)\n\u03c3K (M2) \u03b5M2\n(48)\nSince 1\u03bbk = \u221a \u03c0k \u2264 1. Assigning as the upper bound\nof \u03b5tw in equation 32, with probability at least 1\u2212 \u03b4,\n||\u00b5k \u2212 \u00b5\u0302k|| \u2264 8 \u221a \u03c31(M2) ( 10\nd\u03032s\u03c3K(M2)5/2 +\n2 \u221a 2\nd\u03033s\u03c3K(M2)3/2 ) 2\u03b51\u221a N\n+ 2 \u221a \u03c31(M2)\n\u03c3K (M2)\n2\u03b51\nd\u03032s \u221a N\n\u2264\n( 160 \u221a \u03c31(M2)\nd\u03032s\u03c3K(M2)5/2 +\n32 \u221a\n2\u03c31(M2)\nd\u03033s\u03c3K(M2)3/2 +\n4 \u221a \u03c31(M2)\nd\u03032s\u03c3K (M2)\n) \u03b51\u221a N\n(49)\nSimilarly, since \u03c0k \u2264 1, with probability at least 1\u2212 \u03b4,\n|\u03c0k \u2212 \u03c0\u0302k| = \u2223\u2223\u2223\u2223\u2223 1\u03bb2k \u2212 1\u03bb\u03022k \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 (\u03bbk + \u03bb\u0302k)(\u03bbk \u2212 \u03bb\u0302k)\u03bb2k\u03bb\u03022k \u2223\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u221a\u03c0k\u03c0\u0302k (\u221a\u03c0k +\u221a\u03c0\u0302k) (\u03bbk \u2212 \u03bb\u0302k)\u2223\u2223\u2223\n\u2264 2|\u03bbk \u2212 \u03bb\u0302k| \u2264 10\nsince |\u03bbk \u2212 \u03bb\u0302k| \u2264 5 from Lemma 2. Therefore, with probability at least 1\u2212 \u03b4, we get\n|\u03c0k \u2212 \u03c0\u0302k| \u2264 (\n200\n\u03c3K(M2)5/2 +\n40 \u221a 2\n\u03c3K(M2)3/2\n) \u03b51\nd\u03033s \u221a N\nwhere \u03b51 = ( 1 + \u221a log(1/\u03b4) 2 ) .\nAlso, since \u03b3k = u>kM2L(W,W )uk, with probability at least 1\u2212 \u03b4,\n||\u03b3k \u2212 \u03b3\u0302k|| = \u2223\u2223\u2223\u2223\u2223\u2223u>kM2L(W,W )uk \u2212 u\u0302>k M\u03022L(W\u0302 , W\u0302 )u\u0302k\u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223u>kM2L(W,W )uk \u2212 u\u0302>kM2L(W,W )u\u0302k\u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223\u2223\u2223u\u0302>kM2L(W,W )u\u0302\u2212 u\u0302>k M\u03022L(W\u0302 , W\u0302 )u\u0302k\u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 ||uk \u2212 u\u0302k||2 ||M2L(W,W )||2 + ||u\u0302k||2 \u2223\u2223\u2223\u2223\u2223\u2223M2L(W,W )\u2212 M\u03022L(W\u0302 , W\u0302 )\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 ||uk \u2212 u\u0302k||2||W ||2 ||M2L||2 + ||M2L||2||W \u2212 W\u0302 || 2\n+ ||W\u0302 ||2||M2L \u2212 M\u03022L||2\n\u2264 64||M2L||2 \u03bb2k\u03c3K(M2) 2 + 4||M2L||2 \u03c3K(M2)3 \u03b52M2 + 2 \u03c3K(M2) \u03b5M2L \u2264 64||M2L||2 \u03bb2k\u03c3K(M2) 2 + 4||M2L||2 \u03c3K(M2)3 \u03b52M2 + 2 \u03c3K(M2) \u03b5M2L \u2264 64 \u03c3K(M2) 2 + 4 \u03c3K(M2)3 \u03b52M2 + 2 \u03c3K(M2) \u03b5M2L\nSince the elements of M2L are also a probability, ||M2L||2 \u2264 ||M2L||F \u2264 1, and 1/\u03bb2k = \u03c02k \u2264 1.\nAssigning as the upper bound of \u03b5tw in equation 32, with probability at least 1\u2212 \u03b4,\n||\u03b3k \u2212 \u03b3\u0302k||\n\u2264 1024 \u03c3K(M2)\n( 5\n\u03c3K(M2)5/2 +\n\u221a 2\n\u03c3K(M2)3/2\n)2 \u03b521\n(d\u03033s)2N\n+ 16 \u03c3K(M2)3 \u03b521 (d\u03032s)2N +\n4\n\u03c3K(M2)\n\u03b52\nd\u0303ls \u221a N\n(50) where \u03b51 = ( 1 + \u221a log(1/\u03b4) 2 ) and \u03b52 =(\n1 + \u221a\nlog(2/\u03b4) 2\n) . This completes the proof of Theorem 1."}], "references": [{"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "IJCAI, vol. 11, 2011, pp. 2764\u2013 2770.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2014, pp. 1725\u20131732.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical topic models for multi-label document classification", "author": ["T.N. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers"], "venue": "Machine learning, vol. 88, no. 1-2, pp. 157\u2013208, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["R. Agrawal", "A. Gupta", "Y. Prabhu", "M. Varma"], "venue": "Proceedings of the 22nd international conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2013, pp. 13\u201324.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale multi-label learning with missing labels", "author": ["H.-F. Yu", "P. Jain", "P. Kar", "I.S. Dhillon"], "venue": "ICML, vol. 31, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 2773\u20132832, 2014. [Online]. Available: http://jmlr.org/papers/v15/anandkumar14b.html", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "From o (k 2 n) to o (n): a fast complex-valued eigenvalue solver for large-scale onchip interconnect analysis", "author": ["J. Lee", "V. Balakrishnan", "C.-K. Koh", "D. Jiao"], "venue": "Microwave Symposium Digest, 2009. MTT\u201909. IEEE MTT-S International. IEEE, 2009, pp. 181\u2013184.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Matlab tensor toolbox version 2.6", "author": ["B.W. Bader", "T.G. Kolda"], "venue": "Available online, February 2015. [Online]. Available: http://www.sandia.gov/\u223ctgkolda/TensorToolbox/", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009, pp. 452\u2013461.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "The relationship between precision-recall and roc curves", "author": ["J. Davis", "M. Goadrich"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 233\u2013240.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral methods for supervised topic models", "author": ["Y. Wang", "J. Zhu"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1511\u20131519.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Shifted power method for computing tensor eigenpairs", "author": ["T.G. Kolda", "J.R. Mayo"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 32, no. 4, pp. 1095\u20131124, October 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["K. Bhatia", "H. Jain", "P. Kar", "M. Varma", "P. Jain"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 730\u2013738.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Fastxml: A fast, accurate and stable treeclassifier for extreme multi-label learning", "author": ["Y. Prabhu", "M. Varma"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 263\u2013272.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].", "startOffset": 159, "endOffset": 162}, {"referenceID": 0, "context": "Both WSABIE [1] and LEML [5] utilizes such mappings.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "Both WSABIE [1] and LEML [5] utilizes such mappings.", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "Unlike the usual cases where such latent variable models are trained using EM, we use Method of Moments [6] to extract the parameters from the latent variable model.", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": "From [6], if we define M2 as the pairwise probability matrix, with [M2]i,j = P [ vi, vj ] , we can express it as,", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "O ( ( \u2211N i=1 nnz(xi) )K ) , since the total number of nonzero entries in M2 is O (\u2211N i=1 nnz(xi) 2 ) [7].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "We used the Tensor Toolbox [8] for tensor decomposition.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "The scores of LEML and MoM are not directly comparable, since the score of LEML can be negative, whereas the score of MoM lies within [0, 1]", "startOffset": 134, "endOffset": 140}, {"referenceID": 4, "context": "Since LEML is shown to outperform WSABIE and other benchmark algorithms on various small and large-scale datasets in [5], we benchmark the performance of our method against LEML.", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "For LEML, we ran ten iterations for the smaller datasets (Bibtex and Delicious) and five iterations for the larger datasets, since the authors of LEML chose a similar number of iterations for their experiments in [5].", "startOffset": 213, "endOffset": 216}, {"referenceID": 8, "context": "AUC is a versatile measure, and is used to evaluate the performance of classification as well as prediction algorithms [9].", "startOffset": 119, "endOffset": 122}, {"referenceID": 9, "context": "Also, it is shown that there exists a one-to-one relation between AUC and PrecisionRecall curve in [10], i.", "startOffset": 99, "endOffset": 103}], "year": 2017, "abstractText": "Here we study the problem of predicting labels for large text corpora where each text can be assigned multiple labels. The problem might seem trivial when the number of labels is small, and can be easily solved using a series of one-vsall classifiers. However, as the number of labels increases to several thousand, the parameter space becomes extremely large, and it is no longer possible to use the one-vs-all technique. Here we propose a model based on the factorization of higher order word vector moments, as well as the cross moments between the labels and the words for multi-label prediction. Our model provides guaranteed converge bounds on the extracted parameters. Further, our model takes only three passes through the training dataset to extract the parameters, resulting in a highly scalable algorithm that can train on GB\u2019s of data consisting of millions of documents with hundreds of thousands of labels using a nominal resource of a single processor with 16GB RAM. Our model achieves 10x-15x order of speed-up on large-scale datasets while producing competitive performance in comparison with existing benchmark algorithms.", "creator": "LaTeX with hyperref package"}}}