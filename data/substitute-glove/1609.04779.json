{"id": "1609.04779", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2016", "title": "Characterizing the Language of Online Communities and its Relation to Community Reception", "abstract": "This he investigate contemporary to explaining aspects of dictionary at online communities: but next taken utility as referring identifier within held community still correlation with establish reception of typically. Style what characterized form a hybrid referred as on - of - speech t \u03bc - gram language as, taking commentary is represented usually Latent Dirichlet Allocation. Experiments without made Reddit dialogs appeared have works where a harder measurement of well expression than topic, even take minorities involved few indicate philosophical. Further, there however kind reason correlation all the school enjoys to though contribution and two famous definite to which community, but not did for topic qualities.", "histories": [["v1", "Thu, 15 Sep 2016 19:07:17 GMT  (71kb,D)", "http://arxiv.org/abs/1609.04779v1", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["trang tran", "mari ostendorf"], "accepted": true, "id": "1609.04779"}, "pdf": {"name": "1609.04779.pdf", "metadata": {"source": "CRF", "title": "Characterizing the Language of Online Communities and its Relation to Community Reception", "authors": ["Trang Tran", "Mari Ostendorf"], "emails": ["ttmt001@uw.edu", "ostendor@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "Online discussion forums provide a rich source of data for studying people\u2019s language usage patterns. Discussion platforms take on various forms: articles on many news sites have a comment section, many websites are dedicated to question answering (www.quora.com), and other platforms let users share personal stories, news, and random discoveries (www.reddit.com). Like their offline counterparts, online communities are often comprised of people with similar interests and opinions. Online communication, however, differs from inperson communication in an interesting aspect: explicit and quantifiable feedback. Many discussion forums give their users the ability to upvote and/or downvote content posted by another user. These explicit reward/penalty labels provide valuable information on the reaction of users in a community. In\nthis work, we take advantage of the available user response to explore the relationship between community reception and the degree of stylistic/topical coherence to such communities. Using hybrid ngrams and Latent Dirichlet Allocation (LDA) topic models to represent style and topic for a series of classification tasks, we confirm that there exists a community language style, which is not simply characterized by the topics that online communities are organized around. Moreover, we show that language style is better at discriminating communities, especially between different communities that happen to discuss similar issues. In addition, we found a positive, statistically significant, correlation between the community feedback to comments and their style, but interestingly not with their topic. Finally, we analyze community language on the user level and show that more successful users (in terms of positive community reception) tend to be more specialized; in other words, analogous to offline communities, it is rare for a person to be an expert in multiple areas."}, {"heading": "2 Related Work", "text": "It is well known that conversation partners become more linguistically similar to each other as their dialogue evolves, via many aspects such as lexical, syntactic, as well as acoustic characteristics (Niederhoffer and Pennebaker, 2002; Levitan et al., 2011). This pattern is observed even when the conversation is fictional (Danescu-Niculescu-Mizil and Lee, 2011), or happening on social media (Danescu-NiculescuMizil et al., 2011). Regarding the language of online discussions in particular, it has been shown that individual users\u2019 linguistic patterns evolve to match\nar X\niv :1\n60 9.\n04 77\n9v 1\n[ cs\n.C L\n] 1\n5 Se\np 20\nthose of the community they participate in, reaching \u201clinguistic maturity\u201d over time (Nguyen and Rose\u0301, 2011; Danescu-Niculescu-Mizil et al., 2013). In a multi-community setting, Tan and Lee (2015) found that users tend to explore more in different communities as they mature, adopting the language of these new communities. These works have mainly focused on the temporal evolution of users\u2019 language. Our work differs in that we use different language models to explore the role of topic and style, while also considering users in multiple communities. In addition, we look at community language in terms of its correlation with reception of posted content.\nOther researchers have looked at the role of language in combination with other factors in Reddit community reception. Lakkaraju et al. (2013) proposed a community model to predict the popularity of a resubmitted content, revealing that its title plays a substantial role. Jaech et al. (2015) considered timing and a variety of language features in ranking comments for popularity, finding significant differences across different communities. In our work, we focus on community language, but explore different models to account for it."}, {"heading": "3 Data", "text": "Reddit is a popular forum with thousands of subcommunities known as subreddits, each of which has a specific theme. We will refer to subreddits and communities interchangeably. Redditors can submit content to initiate a discussion thread whose root text we will refer to as a post. Under each post, users can discuss the post by contributing a comment. Both posts and comments can be upvoted and downvoted, and the net feedback is referred to as karma points.\nWe use eight subreddits that reflect Reddit\u2019s diverse topics, while limiting the amount of data to a reasonable size. In addition, we create an artificial distractor merged others that serves as an open class in our classification tasks and for normalizing scores in correlation analysis. Statistics are listed in Table 1. The merged others set includes 9 other subreddits that are similar in size and content diversity to the previous ones: books, chicago, nyc, seattle, explainlikeimfive, science, running, nfl, and todayilearned. Among these extra subreddits, the smallest in size is nyc (1.5M tokens, 76K comments), and\nthe largest is todayilearned (88M tokens, 5M comments). All data is from the period between January 1, 2014 and January 31, 2015. In each subreddit, 20% of the threads are held out for testing.\nWe use discussion threads with at least 100 comments, hypothesizing that smaller threads will not elicit enough community personality for our study. (Virtually all threads kept had only upvotes.) For training our models, we also exclude individual comments with non-positive karma (k \u2264 0) in order to learn only from content that is less likely to be downvoted by the Reddit communities; percentages are noted in Table 1."}, {"heading": "4 Models", "text": "We wish to characterize community language via style and topic. For modeling style, a popular approach has been combining the selected words with part-of-speech (POS) tags to construct models for genre detection (Stamatatos et al., 2000; Feldman et al., 2009; Bergsma et al., 2012) and data selection (Iyer and Ostendorf, 1999; Axelrod, 2014). For topic, a common approach is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We follow such approaches in our work, acknowledging the challenge of completely separating style/genre and topic factors raised previously (Iyer and Ostendorf, 1999; Sarawgi et al., 2011; Petrenz and Webber, 2011; Axelrod, 2014), which also comes out in our analysis. Generative language models are used for characterizing both style and topic, since they are well suited to handling texts of widely varying lengths."}, {"heading": "4.1 Representing Style", "text": "Replacing words with POS tags reduces the possibility that the style model is learning topic, but re-\nplacing too many words loses useful community jargon. To explore this tradeoff, we compared four trigram language models representing different uses of words vs. POS tags in the vocabulary: \u2022 word_only: a regular token-based language\nmodel (vocabulary: 156K words) \u2022 hyb-15k: a hybrid word-POS language model\nover a vocabulary of 15K most frequent words across all communities in our data; all other words are converted to POS tags (vocabulary: 15K words + 38 tags) \u2022 hyb-500.30: a hybrid word-POS language\nmodel over a vocabulary of 500 most frequent words in a subset of data balanced across communities, combined with the union of the 30 next most common words from each of the 17 subreddits; all other words are converted to POS tags (vocabulary: 854 words + 38 tags) \u2022 tag_only: a language model using only POS\ntags as its vocabulary (vocabulary: 38 tags) The hybrid models represent two intermediate sample points between the extremes of word-only and tag-only n-grams. For the hyb-500.30 model, the mix of general and community-specific words was designed to capture distinctive community jargon. The general words include punctuation, function words, and words that are common in many subreddits (e.g., sex, culture, see, dumb, simply). The subreddit-specific words seem to reflect both broad topical themes and jargon or style words, as in (themes vs. style/jargon): askmen: wife, single vs. whatever, interested askwomen: mom, husband vs. especially, totally askscience: particle, planet vs. basically, x fitness: exercises, muscles vs. cardio, reps, rack\nTokenization and tagging are done using Stanford coreNLP (Manning et al., 2014). Punctuation is separated from the words and treated as a word. All language models are trigrams trained using the SRILM toolkit (Stolcke, 2002); modified KneserNey smoothing is applied to the word_only language model, while Witten-Bell smoothing is applied to the tag_only and both hybrid models."}, {"heading": "4.2 Representing Topic", "text": "We train 100- and 200-dimensional LDA topic models (Blei et al., 2003) using gensim (R\u030cehu\u030ar\u030cek and Sojka, 2010). We remove all stopwords (250\nwords) and use tf-idf normalized word counts in each comment (as documents). The vocabulary consists of 156K words, similar to the vocabulary of the word_only language model. The topic models were trained on a subset of the training data, using all collected subreddits but randomly excluding roughly 15% of the training data of larger subreddits worldnews, todayilearned, and nfl.\nThe topics learned exhibit a combination of ones that reflect general characteristics of online discussions or topics that arise in many forums, some that have more specific topics, and others that do not seem particularly coherent. Topics (from LDA-100) that consistently have high probability in all subreddits are shown in Table 2 with their top 10 words by frequency (normalized by the topic average). Topic 19 is likely capturing Reddit\u2019s writing conventions and formatting rules. Broadly used topics reflect women\u2019s issues (29) and news events (32, 34).\nOnline communities are typically organized around a common theme, but multiple topics might fall under that theme, particularly since some of the \u201ctopics\u201d actually reflect style. A subreddit as a whole is characterized by a distribution of topics as learned via LDA, but any particular discussion thread would not necessarily reflect the full distribution. Therefore, we characterize each subreddit with multiple topic vectors. Specifically, we compute LDA topic vectors for each discussion thread in a subreddit, and learn 50 representative topic vectors for each subreddit via k-means clustering."}, {"heading": "5 Community Classification", "text": "One method for exploring the relative importance of topic vs. style in online communication is through community classification experiments: given a discussion thread (or a user\u2019s comments), can we iden-\ntify the community that it comes from more easily using style characteristics or topic characteristics? We formulate this task as a multi-class classification problem (8 communities and \u201cother\u201d), where samples are either at the discussion thread level or the user level. At the thread level, all comments (from multiple people) and the post in a discussion thread are aggregated and treated as a document to be classified. At the user level, we aggregate all comments made by a user in a certain subreddit and treat the collection (which may reflect multiple topics) as a document to be classified.\nWe classify document di to a subreddit according to j\u0302 = argmaxj si,j , where si,j is a score of the similarity of di to community j. For the style models, si,j is the log-probability under the respective trigram language model of community j. For the topic model, si,j is computed using di\u2019s topic vector vi as follows. For a subreddit j, we compute the cosine similarities simj,k between vi and the subreddit\u2019s topic vectors wj,k for k = 1, . . . , 50. The final topic similarity score si,j is the mean of the top 3 highest similarities: si,j = (simj,[1]+simj,[2]+simj,[3])/3, where [\u00b7] denotes the sorted cosine similarities\u2019 indices. The top-3 average captures the most prominent subreddit topics (as in a nearest-neighbor classifier). Averaging over all 50 simj,k is ill suited to subreddits with broad topic coverage, and leads to poor classification results.\nTable 3 summarizes the community classification results (as average accuracy across all subreddits) for each model described in Section 4. While all models beat the random baseline of 11%, the poor performance of the tag_only model confirms that POS tags alone are insufficient to characterize the community. Both for classifying threads and authors, hyb-500.30 yields the best average classification accuracy, due to its ability to generalize POS structure while covering sufficient lexical content to capture the community\u2019s jargon and key topical themes. Neither topic model beats hyb-500.30, indicating that topic alone is not discriminative enough for community identification, even though specific communities coalesce around certain common topics. The word_only and hyb-15k models have performance on the threads that is similar to the topic models, since word features are sensitive to topic, as shown in (Petrenz and Webber, 2011).\nClassifying authors is harder than classifying threads. Two factors are likely to contribute. First, treating a whole discussion thread as a document yields more data to base the decision on than a collection of author comments, since there are many authors who only post a few comments. Second, authors that have multi-community involvement may be less adapted to a specific community. The fact that word-based style models outperform topic models may be because the comments are from different threads so not matching typical topic distributions.\nSubreddit confusion statistics indicate that certain communities are easier to identify than others. Both style and topic models do well in recognizing askscience: classification accuracy for threads is as much as 97%. Communities that were most confusable are intuitively similar: politics and worldnews, askmen and askwomen."}, {"heading": "6 Community Feedback Correlation", "text": "In this section, we investigate whether the style and/or topic scores of a discussion or user are correlated with community response. For thread-level feedback, we use karma points of the discussion thread itself; for the user-level feedback, we compute each user\u2019s subreddit-dependent k-index (Jaech et al., 2015), defined similarly to the well-known hindex (Hirsch, 2005). Specifically, a user\u2019s k-index kj in subreddit j is the maximum integer k such that the user has at least k comments with karma greater than k in that subreddit. User k-index scores have Zipfian distribution, as illustrated in Figure 1 for the worldnews subreddit.\nWe compute a normalized community similarity score s\u0303i,j = si,j \u2212 si,m, where si,m is the corresponding score from the subreddit merged others. The correlation between s\u0303i,j and community feedback is reported for three models in Table 4 for the\nthread level, and in Table 5 for the user level. On the thread level, the hyb-500.30 style model consistently finds positive, statistically significant, correlation between the post\u2019s stylistic similarity score and its karma. This result suggests that language style adaptation does contribute to being well-received by the community. None of the other models explored in the previous section had this property, and for the topic models the correlation is mostly negative. On the user level, all correlations between a user\u2019s k-index and their style/topic match are statistically significant, though the hyb-500.30 style model shows more positive correlation than other models. In both cases, the word_onlymodel gives results between the style and topic models. The hyb-15k model has results that are similar to the word_only model, and the tag_only model has mostly negative correlation.\nExamining users\u2019 multi-community involvement, we also find that users with high k-indices tend to participate in fewer subreddits. Among relatively\nactive users (having at least 100 comments), those with a max k-index of at least 100 participated in a median of 3 communities, while those with a max k-index of at most 5 participated in a median of 6 subreddits. Of the 42 users with max k-index of at least 100, only 4 achieve a k-index of at least 50 in one other community, and only 6 achieve a k-index of at least 20 in one other community."}, {"heading": "7 Conclusion", "text": "In this work, we use hybrid n-grams and topic models to characterize style and topic of language in online communities. Since communities center on a common theme, topic characteristics are reflected in language style, but we find that the best model for determining community identity uses very few words and mostly relies on POS patterns. Using Reddit\u2019s community response system (karma), we also show that discussions and users with higher community endorsement are more likely to match the language style of the community, where the language model that best classifies the community is also most correlated with community response. In addition, online users tend to have more positive community response when they specialize in fewer subreddits. These results have implications for detecting newcomers in a community and the popularity of posts, as well as for language generation."}, {"heading": "Acknowledgments", "text": "This paper is based on work supported by the DARPA DEFT Program. Views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We thank the reviewers for their helpful feedback."}], "references": [{"title": "Data Selection for Statistical Machine Translation", "author": ["Amittai Axelrod"], "venue": "Ph.D. thesis,", "citeRegEx": "Axelrod.,? \\Q2014\\E", "shortCiteRegEx": "Axelrod.", "year": 2014}, {"title": "Stylometric analysis of scientific articles", "author": ["Matt Post", "David Yarowsky"], "venue": "In Proc. Conf. North American Chapter Assoc. for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Bergsma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergsma et al\\.", "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialog", "author": ["Danescu-Niculescu-Mizil", "Lillian Lee"], "venue": null, "citeRegEx": "Danescu.Niculescu.Mizil et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Danescu.Niculescu.Mizil et al\\.", "year": 2011}, {"title": "Mark my words! Linguistic style accommodation in social media", "author": ["Michael Gamon", "Susan Dumais"], "venue": "In Proceedings of WWW", "citeRegEx": "DanescuNiculescu.Mizil et al\\.,? \\Q2011\\E", "shortCiteRegEx": "DanescuNiculescu.Mizil et al\\.", "year": 2011}, {"title": "No country for old members: User lifecycle and linguistic change in online communities", "author": ["Robert West", "Dan Jurafsky", "Jure Leskovec", "Christopher Potts"], "venue": "Proceedings of WWW", "citeRegEx": "DanescuNiculescu.Mizil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "DanescuNiculescu.Mizil et al\\.", "year": 2013}, {"title": "Part-of-speech histogram features for genre classification of text", "author": ["Alex Marin", "Mari Ostendorf", "Maya Gupta"], "venue": "In Proc. ICASSP,", "citeRegEx": "Feldman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2009}, {"title": "An index to quantify an individual\u2019s scientific research output", "author": ["Jorge E. Hirsch"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Hirsch.,? \\Q2005\\E", "shortCiteRegEx": "Hirsch.", "year": 2005}, {"title": "Relevance weighting for combining multidomain data for n-gram language modeling", "author": ["Iyer", "Ostendorf1999] Rukmini Iyer", "Mari Ostendorf"], "venue": null, "citeRegEx": "Iyer et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 1999}, {"title": "Talking to the crowd: What do people react to in online discussions", "author": ["Jaech et al.2015] Aaron Jaech", "Victoria Zayats", "Hao Fang", "Mari Ostendorf", "Hannaneh Hajishirzi"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural", "citeRegEx": "Jaech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaech et al\\.", "year": 2015}, {"title": "What\u2019s in a name? Understanding the interplay between titles", "author": ["Julian McAuley", "Jure Leskovec"], "venue": null, "citeRegEx": "Lakkaraju et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lakkaraju et al\\.", "year": 2013}, {"title": "Entrainment in speech preceding backchannels", "author": ["Agust\u0131\u0301n Gravano", "Julia Hirschberg"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Levitan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levitan et al\\.", "year": 2011}, {"title": "The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Language use as a reflection of socialization in online communities", "author": ["Nguyen", "Ros\u00e92011] Dong Nguyen", "Carolyn P. Ros\u00e9"], "venue": "In Proceedings of the Workshop on Languages in Social Media,", "citeRegEx": "Nguyen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2011}, {"title": "Stable classification of text genres", "author": ["Petrenz", "Webber2011] Philipp Petrenz", "Bonnie Webber"], "venue": "Computational Linguistics,", "citeRegEx": "Petrenz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petrenz et al\\.", "year": 2011}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["\u0158eh\u016f\u0159ek", "Sojka2010] Radim \u0158eh\u016f\u0159ek", "Petr Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "\u0158eh\u016f\u0159ek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "\u0158eh\u016f\u0159ek et al\\.", "year": 2010}, {"title": "Gender attribution: Tracing stylometric evidence beyond topic and genre", "author": ["Kailash Gajulapalli", "Yejin Choi"], "venue": "In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Sarawgi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sarawgi et al\\.", "year": 2011}, {"title": "Text genre detection using common word frequencies", "author": ["Nikos Fakotakis", "George Kokkinakis"], "venue": "In Proceedings of the 18th Conference on Computational Linguistics - Volume", "citeRegEx": "Stamatatos et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Stamatatos et al\\.", "year": 2000}, {"title": "SRILM-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proceedings International Conference on Spoken Language Processing,", "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "All who wander: On the prevalence and characteristics", "author": ["Tan", "Lee2015] Chenhao Tan", "Lillian Lee"], "venue": null, "citeRegEx": "Tan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "It is well known that conversation partners become more linguistically similar to each other as their dialogue evolves, via many aspects such as lexical, syntactic, as well as acoustic characteristics (Niederhoffer and Pennebaker, 2002; Levitan et al., 2011).", "startOffset": 201, "endOffset": 258}, {"referenceID": 3, "context": "those of the community they participate in, reaching \u201clinguistic maturity\u201d over time (Nguyen and Ros\u00e9, 2011; Danescu-Niculescu-Mizil et al., 2013). In a multi-community setting, Tan and Lee (2015) found", "startOffset": 109, "endOffset": 197}, {"referenceID": 10, "context": "Lakkaraju et al. (2013) proposed a community model to predict the popularity of a resubmitted content, revealing that its title plays", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Jaech et al. (2015) considered timing and a variety of language features in ranking comments for popularity, finding significant differences across different communities.", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "For modeling style, a popular approach has been combining the selected words with part-of-speech (POS) tags to construct models for genre detection (Stamatatos et al., 2000; Feldman et al., 2009; Bergsma et al., 2012) and data selection (Iyer and Ostendorf, 1999; Axelrod, 2014).", "startOffset": 148, "endOffset": 217}, {"referenceID": 6, "context": "For modeling style, a popular approach has been combining the selected words with part-of-speech (POS) tags to construct models for genre detection (Stamatatos et al., 2000; Feldman et al., 2009; Bergsma et al., 2012) and data selection (Iyer and Ostendorf, 1999; Axelrod, 2014).", "startOffset": 148, "endOffset": 217}, {"referenceID": 1, "context": "For modeling style, a popular approach has been combining the selected words with part-of-speech (POS) tags to construct models for genre detection (Stamatatos et al., 2000; Feldman et al., 2009; Bergsma et al., 2012) and data selection (Iyer and Ostendorf, 1999; Axelrod, 2014).", "startOffset": 148, "endOffset": 217}, {"referenceID": 0, "context": ", 2012) and data selection (Iyer and Ostendorf, 1999; Axelrod, 2014).", "startOffset": 27, "endOffset": 68}, {"referenceID": 2, "context": "For topic, a common approach is Latent Dirichlet Allocation (LDA) (Blei et al., 2003).", "startOffset": 66, "endOffset": 85}, {"referenceID": 16, "context": "We follow such approaches in our work, acknowledging the challenge of completely separating style/genre and topic factors raised previously (Iyer and Ostendorf, 1999; Sarawgi et al., 2011; Petrenz and Webber, 2011; Axelrod, 2014), which also comes out in our analysis.", "startOffset": 140, "endOffset": 229}, {"referenceID": 0, "context": "We follow such approaches in our work, acknowledging the challenge of completely separating style/genre and topic factors raised previously (Iyer and Ostendorf, 1999; Sarawgi et al., 2011; Petrenz and Webber, 2011; Axelrod, 2014), which also comes out in our analysis.", "startOffset": 140, "endOffset": 229}, {"referenceID": 12, "context": "Tokenization and tagging are done using Stanford coreNLP (Manning et al., 2014).", "startOffset": 57, "endOffset": 79}, {"referenceID": 18, "context": "All language models are trigrams trained using the SRILM toolkit (Stolcke, 2002); modified KneserNey smoothing is applied to the word_only language model, while Witten-Bell smoothing is applied to the tag_only and both hybrid models.", "startOffset": 65, "endOffset": 80}, {"referenceID": 2, "context": "We train 100- and 200-dimensional LDA topic models (Blei et al., 2003) using gensim (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 51, "endOffset": 70}, {"referenceID": 9, "context": "For thread-level feedback, we use karma points of the discussion thread itself; for the user-level feedback, we compute each user\u2019s subreddit-dependent k-index (Jaech et al., 2015), defined similarly to the well-known hindex (Hirsch, 2005).", "startOffset": 160, "endOffset": 180}, {"referenceID": 7, "context": ", 2015), defined similarly to the well-known hindex (Hirsch, 2005).", "startOffset": 52, "endOffset": 66}], "year": 2016, "abstractText": "This work investigates style and topic aspects of language in online communities: looking at both utility as an identifier of the community and correlation with community reception of content. Style is characterized using a hybrid word and part-of-speech tag n-gram language model, while topic is represented using Latent Dirichlet Allocation. Experiments with several Reddit forums show that style is a better indicator of community identity than topic, even for communities organized around specific topics. Further, there is a positive correlation between the community reception to a contribution and the style similarity to that community, but not so for topic similarity.", "creator": "LaTeX with hyperref package"}}}