{"id": "1609.06530", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Weakly supervised spoken term discovery using cross-lingual side information", "abstract": "Recent there opened unsupervised direct discovery (UTD) program on identify each cluster sporadic tell - like units from audio come. These communications those benefit out have seem demand - resource classical every phonemes audio is unavailable, ; where so recorded so-called included soon language existence. However, in instance defendants for for still however solution (client. h2o. , few stirner) did obtain (appears protesters) text proverbs of the data. If putting, the information never did include as a source fact meant reference to maintaining UTD. Here, reason with full simple requires though rescoring saw output bringing a UTD same make text language, both critical it on when corpus well Spanish portable took English translations. We show that clear consequently hinders the average lasers of the crucial bringing a wide combined of which configurations own component transmetalation methods.", "histories": [["v1", "Wed, 21 Sep 2016 12:43:53 GMT  (708kb,D)", "http://arxiv.org/abs/1609.06530v1", "5 pages, 4 figures, submitted for ICASSP 2017"]], "COMMENTS": "5 pages, 4 figures, submitted for ICASSP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sameer bansal", "herman kamper", "sharon goldwater", "adam lopez"], "accepted": false, "id": "1609.06530"}, "pdf": {"name": "1609.06530.pdf", "metadata": {"source": "CRF", "title": "WEAKLY SUPERVISED SPOKEN TERM DISCOVERY USING CROSS-LINGUAL SIDE INFORMATION", "authors": ["Sameer Bansal", "Herman Kamper", "Sharon Goldwater", "Adam Lopez"], "emails": ["h.kamper}@sms.ed.ac.uk,", "alopez}@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "Index Terms\u2014 Unsupervised term discovery, low-resource speech processing, speech translation, weakly supervised learning"}, {"heading": "1. INTRODUCTION", "text": "High-quality automatic speech recognition (ASR) systems require hundreds of hours of transcribed training data. As a result, they are currently available for only a tiny fraction of the world\u2019s several thousand languages [1]. To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data. While considerable progress has been made in this area recently [8\u201311], learning from audio alone is very challenging. Here, we ask whether using side information could improve performance.\nIn particular, we address the task of unsupervised term discovery (UTD), which aims to identify and cluster repeated word-like units from audio. We show that UTD can be improved using side information from text translations of the audio into another language. Such translations can often be obtained rapidly through crowd-sourcing, for example in disaster relief scenarios such as the 2010 Haiti earthquake [12]. And when the low-resource language has no written form, text translations (ideally into a related language, as in [13]) may be considerably easier to obtain than a phonetic transcription.\nIn addition to improving UTD, our work may feed into the development of cross-lingual tools for low-resource languages, in particular systems for translating speech from a low-resource language to a higher-resource language. A traditional pipeline would use ASR to transcribe the audio, followed by machine translation of the transcriptions. However, training such a system requires both transcribed audio in the low-resource language and parallel text. Recent work [14] has begun to explore how to translate key words and phrases based on\nthe kind of training data we use here. Our work could inform future approaches to this task.\nAlthough our ultimate goal is to work with truly low-resource languages, ours is the first attempt we know of to address this task setting, so as a proof of concept we present results using a dataset of Spanish speech paired with English text translations [15]. We use an open-source UTD system to discover potential word-like units from the audio, then use a simple rescoring method to improve the UTD output based on the translation information. Our results show large improvements in average precision across a wide range of hyperparameter settings, and also across cross-speaker matches."}, {"heading": "2. UNSUPERVISED TERM DISCOVERY", "text": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21]. Unsupervised term discovery is one of the most well-developed areas. Essentially, UTD systems search for pairs of audio segments that are similar, as measured by their dynamic time warping (DTW) [22] distance. This task is inherently quadratic in the input size, and early systems [2,23] were prohibitively slow. Here, we use the open-source implementation in the Zero Resource Toolkit (ZRTools)1 [4], a stateof-the-art system which uses a more efficient two-pass approach. It is also the only freely available UTD system we know of."}, {"heading": "2.1. Overview of the ZRTools UTD system", "text": "In its first pass, ZRTools uses an approximate randomized algorithm and image processing techniques to extract potential matching segments. Image processing is used based on the intuition that if we plot the cosine similarity between every frame of the input feature vector representation (e.g. MFCCs), any repeated segments in the pair of utterances will show up as diagonal line patterns. Figure 1(a) illustrates this, showing a clear diagonal pattern corresponding to similar words in two utterances.\nIn its second pass, ZRTools computes a normalized DTW score over potential matches to extract the final output. It returns segment pairs longer than a minimum duration (we used the recommended value of 500ms) along with their DTW score (between 0 and 1, with higher scores indicating greater similarity). These word-like or phrase-like segments can then be used for downstream tasks like keyword search and topic modeling [2, 23, 24].\nFull details of the system can be found in [4]. 1https://github.com/arenjansen/ZRTools\nar X\niv :1\n60 9.\n06 53\n0v 1\n[ cs\n.C L\n] 2\n1 Se\np 20"}, {"heading": "2.2. Limitations of UTD", "text": "Like all UTD systems, ZRTools identifies patterns using acoustic information only. This can lead to various types of errors which we hope to reduce using cross-lingual side information."}, {"heading": "2.2.1. Mismatch between acoustic and semantic information", "text": "Some phonetically similar pairs identified by UTD are nevertheless different semantically, as illustrated in Figure 1(b). The words in the utterances are different, but UTD identifies similar phoneme sequences a n o p w e and e n o p w e. Acoustic information alone cannot overcome these errors, yet they will cause semantic errors in downstream applications such as machine translation or spoken document retrieval.\nOn the other hand, due to noise and variability (both within and across speakers), not all semantically correct matches will be assigned a high DTW score. The ZRTools documentation recommends using a DTW score threshold of 0.88 to filter good matches, yet there are many correct pairs with scores lower than 0.88. One example is illustrated in Figure 1(c), where a correct match has a score just below the cut-off threshold. Of course, we can lower the DTW threshold to return more pairs (raise recall), but this will also increase the number of incorrect pairs returned (lower precision)."}, {"heading": "2.2.2. Silence and filler words as valid matches", "text": "UTD is sensitive to silence regions, background noises, and filler words, all three of which commonly occur in conversational speech. We observed that these phenomena generate a large number of discovered pairs, since they are frequent and are often good acoustic matches with each other.\nThe number of non-word pairs found by UTD due to these phenomena depends on the preprocessing of the data. To show that our method improves UTD output regardless of preprocessing, we experiment with two different preprocessing methods.\nFirst, we use the automatic voice activity detection (VAD) script that comes with ZRTools, which uses Root Mean Squared (RMS) Energy detection to label VAD regions. Only these regions are then used when searching for patterns. This method aggressively filters out silence but removes a considerable amount of valid speech. It also retains many filler words, which often have high energy and long duration.\nAlternatively, we can use a forced alignment (FA) of the speech data with the transcripts to filter out non-speech regions. This method would not be available in a true zero-resource situation, but might better reflect the output of a more sophisticated automatic VAD system, and has the advantage of retaining more of the training data. However, we observed that it removes fewer silent regions than the VAD system.\nRegardless of which preprocessing method is used, UTD will tend to find a large number of matches based on non-word regions. However, the translations for such non-word pairs will rarely contain any content words in common, so rescoring them based on their translations should reduce these spurious matches."}, {"heading": "3. IMPROVING UTD USING TRANSLATIONS", "text": "Given pair of speech utterances, we hypothesize that the similarity in their translations provides a (noisy) signal of the semantic similarity between the discovered acoustic units, and that this signal can improve UTD.\nConsider the examples in Table 1, which shows the English translations of the utterances containing the segments depicted in Figure 1. (The utterances are cropped, so not all Spanish words are shown.) Stop words are shown in parentheses; we filter these out using the NLTK toolkit2 before computing translation similarity. Notice that the two pairs (a and c) that have matching Spanish words also have matching English content words, even though one of them falls below the recommended 0.88 threshold for a UTD (acoustic) match. On the other hand, pair (b) has a high UTD score due to phonetic similarity, but there is no match between the English words.\nTo exploit these observations, we rescore the pairs returned by ZRTools using their translation similarity. If dtwi is the acoustic similarity score for pair i computed by ZRTools, and Ji is the translation similarity score (described below), then the new score of pair i is computed as the \u03b1-weighted mean between the two:\nscorei = (1\u2212 \u03b1) \u00d7 dtwi + \u03b1 \u00d7 Ji (1)\nTo compute the similarity J between a pair of English translations, we treat each translation as bag of words (after filtering for stop\n2http://www.nltk.org/\nwords), and use Jaccard similarity [25]:\nJ = |E1 \u2229 E2| |E1 \u222a E2|\n(2)\nwhere E1 is the set of content words in translation 1 and E2 is the set of content words in translation 2.\nNote that even seemingly low translation similarity scores (such as 0.125 for pair (a) in Table 1) are still a strong signal of semantic similarity between acoustic matches, because any non-zero score indicates some content words in common. Empirically we have observed J \u2265 0.1 to be a good indicator of a correct match (although in practice we do not impose any threshold on J)."}, {"heading": "4. EXPERIMENTAL SETUP AND EVALUATION", "text": "In all experiments, the input consists of speech from the CALLHOME Spanish corpus and the crowdsourced English translations of [15]. The corpus consists of speech from telephone conversations between native speakers. We use the default feature representation as used by ZRtools: 39-dimensional Relative Spectral Transform - Perceptual Linear Prediction (PLP) feature vectors.\nWe carry out four sets of experiments as summarized in Table 2. Note that the energy-based VAD filters out far more of the data than forced alignment. For each phone call, we have two channels of audio, each with at least one speaker, but sometimes more. The same speaker may be on multiple calls. However, for the purposes of our cross-speaker evaluation, we assume that each channel corresponds to a unique speaker.\nTo evaluate the results of the raw UTD system and our rescoring method, we use the original Spanish CALLHOME transcripts to check if a pair of discovered speech segments is actually a true match. Note that the transcripts are not otherwise used as input to our system, except to filter non-speech in the forced alignment setting, where it serves as a kind of oracle for speech detection. For each pair\nof segments, we retrieve the corresponding words (as per the time stamps) from the transcripts. We retrieve any words which either partially or completely overlap with ZRTools output. The retrieved words are then filtered for stop words using NLTK. A discovered pair is marked as correct if the two segments have at least one content word in common; otherwise, it is marked as incorrect.\nTo implement our rescoring method, we begin by running UTD with an acoustic matching threshold of D = 0.8, which is considerably lower than the ZRTools recommended level of D = 0.88. An empirical check suggested that very few correct pairs had scores below 0.8, and this value of D gives us enough potential pairs to perform rescoring.\nFor evaluation, we treat the set of correct pairs returned with D = 0.8 as the total number of possible correct pairs\u2014that is, recall values are computed with respect to this number. Therefore, a recall value of 1 does not mean that all correct pairs in the entire dataset have been identified, only those whose DTW score is above 0.8.\nUsing our recomputed scores (as defined in Equation 1) we can choose a new threshold value S, and return pairs that score above S. For each value of S, we can compute precision and recall:\nPrecision@S = \u2211N\ni=1(correcti \u2227 scorei \u2265 S)\u2211N i=1(scorei \u2265 S)\n(3)\nRecall@S = \u2211N\ni=1(correcti \u2227 scorei \u2265 S)\u2211N i=1(correcti \u2227 dtwi \u2265 0.80)\n(4)\nwhere correcti indicates if a UTD output pair is correct or not, and N is the total number of pairs discovered with DTW threshold D = 0.80. We find the Precision/Recall curve by considering all possible values of S, and then compute the average precision (AP) as the area under this curve."}, {"heading": "5. RESULTS AND DISCUSSION", "text": ""}, {"heading": "5.1. Baseline UTD system", "text": "Table 3 lists the number of pairs discovered by running the baseline UTD system in each configuration. The number of pairs discovered using energy-based VAD is low, as expected, due to large parts of speech data being filtered out. Using forced alignments gives us a higher number of discovered pairs, but at a cost of precision. The low number of cross-speaker pairs listed in Table 3 highlights the difficulty of discovering these, though they are important for downstream tasks [26, 27]. Translation information may be particularly helpful for identifying cross-speaker pairs, but our method is limited by the small number of pairs that are discovered in the first place, as shown\nin the xspk columns in Table 3. In future work we plan to investigate whether translation information could be fed into the UTD system at an earlier stage to help discover more cross-speaker pairs."}, {"heading": "5.2. Improvements using translations", "text": "Figure 2 illustrates the benefit of our system, showing Precision/Recall curves for the (50, FA) setting. By using only acoustic information and varying the value of D, only points on the lower (blue) curve can be achieved (e.g., the red point is for D = 0.88). Using translations (here \u03b1 = 0.4) clearly improves results.\nTo show that these benefits are not highly sensitive to \u03b1, Figure 3 plots the AP for all configurations listed in Table 2, for \u03b1 between 0 and 1. For every configuration and every \u03b1 > 0 setting, we obtain higher AP than the baseline \u03b1 = 0, often by a large margin. AP scores for \u03b1 = 0.4, which is one of a range of good values, are listed in Table 4. Note that the AP numbers are only comparable between systems using the same data/preprocessing configuration, since the total number of pairs that need to be discovered to achieve 100% recall is different in each case (it is given by the number of correct pairs at D = 0.8 in Table 3).\nFigure 4 compares our system directly to the UTD system\u2019s recommended setting ofD = 0.88, which returns about 11K matches yielding a precision of 0.17. If we set S in our system to return the same number of matches, the precision rises to 0.21. We similarly found that for cross-speaker matches, at 6.9K predictions (the baseline output), our system using translation improves precision from 0.04 to 0.07.\nFigure 3 shows that at \u03b1 = 1, the AP is higher than at \u03b1 = 0, which seems to imply that ignoring the DTW score and only using\ntranslation similarity yields better results. However, recall that we started by pruning the ZRTools output using a DTW threshold of 0.80, so even with \u03b1 = 1, our system is not actually ignoring acoustic information. In addition, the UTD system provides important information about segment boundaries which translations alone cannot.\nAs discussed in Section 2.2.2, we observed that UTD discovers many filler words with high DTW score, but their translation score should be low. Unfortunately it is difficult to quantify how well our system filters out these matches since filler words are usually not transcribed in our data."}, {"heading": "6. CONCLUSION", "text": "We have shown that side information in the form of translations improves the output of UTD across a wide range of settings. In future work, we will use the improved UTD output to learn better cross-speaker speech features for low-resource settings, and explore the use of translations as a preprocessing step for UTD, by helping guide the search for matches. We also aim to expand this work into a semi-supervised setting, using additional unlabeled speech data to improve UTD. We believe this can be done using approaches such as label propagation [28] and label spreading [29, 30]."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "We thank David Chiang and Antonios Anastasopoulos for sharing alignments of the CALLHOME speech and transcripts; Aren Jansen for assistance with ZRTools; and Marco Damonte, Federico Fancellu, Sorcha Gilroy, Ida Szubert, and Clara Vania for comments on previous drafts. This work was supported in part by a James S McDonnell Foundation Scholar Award and a Google faculty research award."}, {"heading": "8. REFERENCES", "text": "[1] L. Besacier, E. Barnard, A. Karpov, and T. Schultz, \u201cAutomatic speech recognition for under-resourced languages: A survey,\u201d Speech Communication, vol. 56, pp. 85\u2013100, 2014.\n[2] A. S. Park and J. R. Glass, \u201cUnsupervised pattern discovery in speech,\u201d IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.\n[3] Y. Zhang and J. R. Glass, \u201cTowards multi-speaker unsupervised speech pattern discovery,\u201d in Proc. ICASSP, 2010.\n[4] A. Jansen and B. Van Durme, \u201cEfficient spoken term discovery using randomized algorithms,\u201d in Proc. ASRU, 2011.\n[5] Y. Zhang, R. Salakhutdinov, H.-A. Chang, and J. R. Glass, \u201cResource configurable spoken query detection using deep Boltzmann machines,\u201d in Proc. ICASSP, 2012.\n[6] F. Metze, X. Anguera, E. Barnard, M. Davel, and G. Gravier, \u201cThe spoken web search task at MediaEval 2012,\u201d in Proc. ICASSP, 2013.\n[7] K. Levin, A. Jansen, and B. Van Durme, \u201cSegmental acoustic indexing for zero resource keyword search,\u201d in Proc. ICASSP, 2015.\n[8] M. Versteegh, R. Thiollie\u0300re, T. Schatz, X. N. Cao, X. Anguera, A. Jansen, and E. Dupoux, \u201cThe Zero Resource Speech Challenge 2015,\u201d in Proc. Interspeech, 2015.\n[9] O. J. Ra\u0308sa\u0308nen, G. Doyle, and M. C. Frank, \u201cUnsupervised word discovery from speech using automatic segmentation into syllable-like units,\u201d in Proc. Interspeech, 2015.\n[10] H. Kamper, A. Jansen, and S. J. Goldwater, \u201cFully unsupervised small-vocabulary speech recognition using a segmental bayesian model,\u201d in Proc. Interspeech, 2015.\n[11] H. Kamper, A. Jansen, and S. J. Goldwater, \u201cUnsupervised word segmentation and lexicon discovery using acoustic word embeddings,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.\n[12] R. Munro, \u201cCrowdsourced translation for emergency response in Haiti: the global collaboration of local knowledge,\u201d in AMTA Workshop on Collaborative Crowdsourcing for Translation, 2010, pp. 1\u20134.\n[13] L. J. Martin, A. Wilkinson, S. S. Miryala, V. Robison, and A. W. Black, \u201cUtterance classification in speech-to-speech translation for zero-resource languages in the hospital administration domain,\u201d in Proc. ASRU, 2015.\n[14] L. Duong, A. Anastasopoulos, D. Chiang, S. Bird, and T. Cohn, \u201cAn attentional model for speech translation without transcription,\u201d in Proc. NAACL HLT, 2016.\n[15] M. Post, G. Kumar, A. Lopez, D. Karakos, C. Callison-Burch, and S. Khudanpur, \u201cImproved speech-to-text translation with the Fisher and Callhome Spanish\u2013English speech translation corpus,\u201d in Proc. IWSLT, 2013.\n[16] C.-y. Lee and J. R. Glass, \u201cA nonparametric Bayesian approach to acoustic model discovery,\u201d in Proc. ACL, 2012.\n[17] M.-H. Siu, H. Gish, A. Chan, W. Belfield, and S. Lowe, \u201cUnsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery,\u201d Comput. Speech Lang., vol. 28, no. 1, pp. 210\u2013223, 2014.\n[18] L. Badino, A. Mereta, and L. Rosasco, \u201cDiscovering discrete subword units with binarized autoencoders and hidden-Markovmodel encoders,\u201d in Proc. Interspeech, 2015.\n[19] R. Thiollie\u0300re, E. Dunbar, G. Synnaeve, M. Versteegh, and E. Dupoux, \u201cA hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling,\u201d in Proc. Interspeech, 2015.\n[20] O. Walter, T. Korthals, R. Haeb-Umbach, and B. Raj, \u201cA hierarchical system for word discovery exploiting DTW-based initialization,\u201d in Proc. ASRU, 2013.\n[21] C.-y. Lee, T. O\u2019Donnell, and J. R. Glass, \u201cUnsupervised lexicon discovery from acoustic input,\u201d Trans. ACL, vol. 3, pp. 389\u2013403, 2015.\n[22] H. Sakoe and S. Chiba, \u201cDynamic programming algorithm optimization for spoken word recognition,\u201d IEEE Trans. Acoustics, Speech, Signal Process., vol. 26, no. 1, pp. 43\u201349, 1978.\n[23] Y. Zhang and J. R. Glass, \u201cUnsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams,\u201d in Proc. ASRU, 2009.\n[24] M. Dredze, A. Jansen, G. Coppersmith, and K. Church, \u201cNLP on spoken documents without ASR,\u201d in Proc. EMNLP, 2010.\n[25] P. Jaccard, Distribution de la Flore Alpine: dans le Bassin des dranses et dans quelques re\u0301gions voisines, Rouge, 1901.\n[26] H. Kamper, M. Elsner, A. Jansen, and S. J. Goldwater, \u201cUnsupervised neural network based feature extraction using weak top-down constraints,\u201d in Proc. ICASSP, 2015.\n[27] D. Renshaw, H. Kamper, A. Jansen, and S. J. Goldwater, \u201cA comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge,\u201d in Proc. Interspeech, 2015.\n[28] X. Zhu and Z. Ghahramani, \u201cLearning from labeled and unlabeled data with label propagation,\u201d Tech. Rep., CMU-CALD02-107, 2002.\n[29] Y. Bengio, O. Delalleau, and N. Le Roux, \u201cLabel propagation and quadratic criterion,\u201d Semi-supervised learning, vol. 10, 2006.\n[30] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scho\u0308lkopf, \u201cLearning with local and global consistency,\u201d Advances in Neural Information Processing Systems, vol. 16, no. 16, pp. 321\u2013328, 2004."}], "references": [{"title": "Automatic speech recognition for under-resourced languages: A survey", "author": ["L. Besacier", "E. Barnard", "A. Karpov", "T. Schultz"], "venue": "Speech Communication, vol. 56, pp. 85\u2013100, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Towards multi-speaker unsupervised speech pattern discovery", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ICASSP, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Resource configurable spoken query detection using deep Boltzmann machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J.R. Glass"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "The spoken web search task at MediaEval 2012", "author": ["F. Metze", "X. Anguera", "E. Barnard", "M. Davel", "G. Gravier"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "The Zero Resource Speech Challenge 2015", "author": ["M. Versteegh", "R. Thiolli\u00e8re", "T. Schatz", "X.N. Cao", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised word discovery from speech using automatic segmentation into syllable-like units", "author": ["O.J. R\u00e4s\u00e4nen", "G. Doyle", "M.C. Frank"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully unsupervised small-vocabulary speech recognition using a segmental bayesian model", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Crowdsourced translation for emergency response in Haiti: the global collaboration of local knowledge", "author": ["R. Munro"], "venue": "AMTA Workshop on Collaborative Crowdsourcing for Translation, 2010, pp. 1\u20134.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Utterance classification in speech-to-speech translation for zero-resource languages in the hospital administration domain", "author": ["L.J. Martin", "A. Wilkinson", "S.S. Miryala", "V. Robison", "A.W. Black"], "venue": "Proc. ASRU, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "An attentional model for speech translation without transcription", "author": ["L. Duong", "A. Anastasopoulos", "D. Chiang", "S. Bird", "T. Cohn"], "venue": "Proc. NAACL HLT, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved speech-to-text translation with the Fisher and Callhome Spanish\u2013English speech translation corpus", "author": ["M. Post", "G. Kumar", "A. Lopez", "D. Karakos", "C. Callison-Burch", "S. Khudanpur"], "venue": "Proc. IWSLT, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A nonparametric Bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J.R. Glass"], "venue": "Proc. ACL, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery", "author": ["M.-H. Siu", "H. Gish", "A. Chan", "W. Belfield", "S. Lowe"], "venue": "Comput. Speech Lang., vol. 28, no. 1, pp. 210\u2013223, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Discovering discrete subword units with binarized autoencoders and hidden-Markovmodel encoders", "author": ["L. Badino", "A. Mereta", "L. Rosasco"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling", "author": ["R. Thiolli\u00e8re", "E. Dunbar", "G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "A hierarchical system for word discovery exploiting DTW-based initialization", "author": ["O. Walter", "T. Korthals", "R. Haeb-Umbach", "B. Raj"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T. O\u2019Donnell", "J.R. Glass"], "venue": "Trans. ACL, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "IEEE Trans. Acoustics, Speech, Signal Process., vol. 26, no. 1, pp. 43\u201349, 1978.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1978}, {"title": "Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "NLP on spoken documents without ASR", "author": ["M. Dredze", "A. Jansen", "G. Coppersmith", "K. Church"], "venue": "Proc. EMNLP, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Distribution de la Flore Alpine: dans le Bassin des dranses et dans quelques r\u00e9gions voisines", "author": ["P. Jaccard"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1901}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["H. Kamper", "M. Elsner", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge", "author": ["D. Renshaw", "H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["X. Zhu", "Z. Ghahramani"], "venue": "Tech. Rep., CMU-CALD- 02-107, 2002.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Label propagation and quadratic criterion", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "Semi-supervised learning, vol. 10, 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems, vol. 16, no. 16, pp. 321\u2013328, 2004.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "As a result, they are currently available for only a tiny fraction of the world\u2019s several thousand languages [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 137, "endOffset": 142}, {"referenceID": 2, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 137, "endOffset": 142}, {"referenceID": 3, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 137, "endOffset": 142}, {"referenceID": 4, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 163, "endOffset": 168}, {"referenceID": 5, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 163, "endOffset": 168}, {"referenceID": 6, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 163, "endOffset": 168}, {"referenceID": 7, "context": "While considerable progress has been made in this area recently [8\u201311], learning from audio alone is very challenging.", "startOffset": 64, "endOffset": 70}, {"referenceID": 8, "context": "While considerable progress has been made in this area recently [8\u201311], learning from audio alone is very challenging.", "startOffset": 64, "endOffset": 70}, {"referenceID": 9, "context": "While considerable progress has been made in this area recently [8\u201311], learning from audio alone is very challenging.", "startOffset": 64, "endOffset": 70}, {"referenceID": 10, "context": "While considerable progress has been made in this area recently [8\u201311], learning from audio alone is very challenging.", "startOffset": 64, "endOffset": 70}, {"referenceID": 11, "context": "Such translations can often be obtained rapidly through crowd-sourcing, for example in disaster relief scenarios such as the 2010 Haiti earthquake [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "And when the low-resource language has no written form, text translations (ideally into a related language, as in [13]) may be considerably easier to obtain than a phonetic transcription.", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "Recent work [14] has begun to explore how to translate key words and phrases based on the kind of training data we use here.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "Although our ultimate goal is to work with truly low-resource languages, ours is the first attempt we know of to address this task setting, so as a proof of concept we present results using a dataset of Spanish speech paired with English text translations [15].", "startOffset": 256, "endOffset": 260}, {"referenceID": 15, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 124, "endOffset": 131}, {"referenceID": 16, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 124, "endOffset": 131}, {"referenceID": 17, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 169, "endOffset": 177}, {"referenceID": 18, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 169, "endOffset": 177}, {"referenceID": 8, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 248, "endOffset": 263}, {"referenceID": 10, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 248, "endOffset": 263}, {"referenceID": 19, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 248, "endOffset": 263}, {"referenceID": 20, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 248, "endOffset": 263}, {"referenceID": 21, "context": "Essentially, UTD systems search for pairs of audio segments that are similar, as measured by their dynamic time warping (DTW) [22] distance.", "startOffset": 126, "endOffset": 130}, {"referenceID": 1, "context": "This task is inherently quadratic in the input size, and early systems [2,23] were prohibitively slow.", "startOffset": 71, "endOffset": 77}, {"referenceID": 22, "context": "This task is inherently quadratic in the input size, and early systems [2,23] were prohibitively slow.", "startOffset": 71, "endOffset": 77}, {"referenceID": 3, "context": "Here, we use the open-source implementation in the Zero Resource Toolkit (ZRTools) [4], a stateof-the-art system which uses a more efficient two-pass approach.", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "These word-like or phrase-like segments can then be used for downstream tasks like keyword search and topic modeling [2, 23, 24].", "startOffset": 117, "endOffset": 128}, {"referenceID": 22, "context": "These word-like or phrase-like segments can then be used for downstream tasks like keyword search and topic modeling [2, 23, 24].", "startOffset": 117, "endOffset": 128}, {"referenceID": 23, "context": "These word-like or phrase-like segments can then be used for downstream tasks like keyword search and topic modeling [2, 23, 24].", "startOffset": 117, "endOffset": 128}, {"referenceID": 3, "context": "Full details of the system can be found in [4].", "startOffset": 43, "endOffset": 46}, {"referenceID": 24, "context": "words), and use Jaccard similarity [25]:", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "In all experiments, the input consists of speech from the CALLHOME Spanish corpus and the crowdsourced English translations of [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 25, "context": "The low number of cross-speaker pairs listed in Table 3 highlights the difficulty of discovering these, though they are important for downstream tasks [26, 27].", "startOffset": 151, "endOffset": 159}, {"referenceID": 26, "context": "The low number of cross-speaker pairs listed in Table 3 highlights the difficulty of discovering these, though they are important for downstream tasks [26, 27].", "startOffset": 151, "endOffset": 159}, {"referenceID": 27, "context": "We believe this can be done using approaches such as label propagation [28] and label spreading [29, 30].", "startOffset": 71, "endOffset": 75}, {"referenceID": 28, "context": "We believe this can be done using approaches such as label propagation [28] and label spreading [29, 30].", "startOffset": 96, "endOffset": 104}, {"referenceID": 29, "context": "We believe this can be done using approaches such as label propagation [28] and label spreading [29, 30].", "startOffset": 96, "endOffset": 104}], "year": 2016, "abstractText": "Recent work on unsupervised term discovery (UTD) aims to identify and cluster repeated word-like units from audio alone. These systems are promising for some very low-resource languages where transcribed audio is unavailable, or where no written form of the language exists. However, in some cases it may still be feasible (e.g., through crowdsourcing) to obtain (possibly noisy) text translations of the audio. If so, this information could be used as a source of side information to improve UTD. Here, we present a simple method for rescoring the output of a UTD system using text translations, and test it on a corpus of Spanish audio with English translations. We show that it greatly improves the average precision of the results over a wide range of system configurations and data preprocessing methods.", "creator": "LaTeX with hyperref package"}}}