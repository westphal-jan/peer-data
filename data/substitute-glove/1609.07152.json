{"id": "1609.07152", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Input Convex Neural Networks", "abstract": "This fine image new hence polytope somatic link edifice. These instead formula_16 - income (potentially deep) neural subscribers with unnecessary on the distribution parameters such that the output most the providers \u201d brought convex determining within (some many) the depending. The operates if two relying randomness switching optimization to some mechanisms to own independent given much, some sure be applied set editing one structured statistical, data imputation, reinforcement involves, way others. In it laid how still be core groundwork made themselves models, oppose testing its parametric, regression both learning, and identify their aesthetic power. We tv that some existing constructs network powerpc can be he data - manifolds with without minor termination, work utilize specialized optimization optimization dkny coming though complete. Finally, we highlight the performance has the methods on holds - music prediction, image ayp, or reinforcement classes problems, entered we well steady over the establishing state important the folk both usually charges.", "histories": [["v1", "Thu, 22 Sep 2016 20:10:57 GMT  (2074kb,D)", "http://arxiv.org/abs/1609.07152v1", null], ["v2", "Thu, 13 Oct 2016 19:46:58 GMT  (2167kb,D)", "http://arxiv.org/abs/1609.07152v2", null], ["v3", "Wed, 14 Jun 2017 17:59:12 GMT  (2532kb,D)", "http://arxiv.org/abs/1609.07152v3", "ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["brandon amos", "lei xu", "j zico kolter"], "accepted": true, "id": "1609.07152"}, "pdf": {"name": "1609.07152.pdf", "metadata": {"source": "CRF", "title": "Input Convex Neural Networks", "authors": ["Brandon Amos", "Lei Xu"], "emails": ["bamos@cs.cmu.edu", "leonard.xu.thu@gmail.com", "zkolter@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we propose a new neural network architecture that we call the input convex neural network (ICNN). In the simplest case, these are scalar valued neural networks f(x, y; \u03b8) where x and y denotes inputs to the function and \u03b8 denotes the parameters, built in such a way that the network is convex in (a subset of) inputs y.1 The fundamental benefit to these ICNNs is that we can optimize over the convex inputs to the network given some fixed value for other inputs. That is, given some fixed x (and possibly some fixed elements of y) we can globally and efficiently (because the problem is convex) solve the optimization problem\nargmin y f(x, y; \u03b8). (1)\n\u2217Work done while author was at Carnegie Mellon University. 1We emphasize the term \u201cinput convex\u201d since convexity in machine learning typically refers to convexity (of the loss minimization learning problem) in the parameters, which is not the case here. Note that in our notation, f needs only be a convex function in y, and may still be non-convex in the remaining inputs x. Training these neural networks remains a nonconvex problem, and the convexity is only being exploited at inference time.\nar X\niv :1\n60 9.\n07 15\n2v 1\n[ cs\n.L G\nWe detail a number of potential use cases below, but fundamentally, this formalism lets us perform inference in the network via optimization. That is, instead of making predictions in a neural network via a purely feedforward process, we can make predictions by optimizing a scalar function (which effectively plays the roll of an energy function) over some inputs to the function given others.\nThere are a number of potential use cases for these types of network. In what follows we will use notation that differs slightly from the above in order to make the connections with existing methodologies more clear, but all cases have the same characteristic in that inference in the model is performed by optimizing over some inputs to the network given others. In all cases the stated optimization problems are convex.\n\u2022 Structured prediction. As is perhaps apparent from our notation above, a key application of this work is in structured prediction. Given (typically high-dimensional) structured input and output spaces X \u00d7 Y, we can build a network over (x, y) pairs that encodes the energy function for this pair, following typical energy-based learning formalisms [LeCun et al., 2006]. Prediction involves finding the y \u2208 Y that minimizes the energy for a given x\ny\u0302 = argmin y\u2208Y f(x, y; \u03b8). (2)\nIn our setting, assuming that Y is a convex space (a common assumption in structured prediction), this optimization problem is convex. This is similar in nature to the neural network structured prediction methodology [Belanger and McCallum, 2015], with the difference being that in our setting f is both convex in y, so the optimization can be performed globally, and has a deep network structure over y itself.\n\u2022 Data imputation. Similar to the above but slightly more generic, if we are given some space Y we can learn a network f(y; \u03b8) (removing the additional x inputs, though these can be added as well) that, given an example with some subset I missing, imputes the likely values of these variables by solving the optimization problem as above\ny\u0302I = argmin yI f(yI , yI\u0304 ; \u03b8). (3)\nThis could be used e.g., in image inpainting where the goal is to fill in some arbitrary set of missing pixels given observed ones.\n\u2022 Continuous action reinforcement learning. Given a reinforcement learning problem with potentially continuous state and action spaces S \u00d7 A, we can model the (negative) Q function, \u2212Q(s, a; \u03b8) as an input convex neural network. In this case the action selection procedure can be formulated as a convex optimization problem\na?(s) = argmin a \u2212Q(s, a; \u03b8). (4)\n\u2022 Generative embeddings. Suppose we have some input space X that we wish to map to some embedding space Y of our own design. We can learn an input convex network (where here alone for notational simplicity we mean a network that is convex in both x and y) such that the forward embedding procedure is determined by the optimization problem\ny\u0302(x) = argmin y\u2208Y f(x, y; \u03b8) (5)\nwhile the generative process is simply the reverse\nx\u0302(y) = argmin x\u2208X f(x, y; \u03b8). (6)\nBecause these are both convex optimization problems, both problems can be solved using the exact same underlying network.\nThis paper does not explore all of these possibilities, but rather lays the foundation for optimization, inference, and learning in these input convex models. A natural question that arises when considering such models is whether we lose substantial representation ability when limiting ourselves to networks with this property. As we argue in this paper, the answer appears to be no, at least in terms of the representational power of the resulting systems. As we show, ICNNs can be built using networks very similar to existing nonconvex networks, and although convex functions themselves are limited, we emphasize that the predictions made in an ICNN are the argmin over a convex function, which itself can capture non-convex relationships. Furthermore, as is hopefully clear from the examples above, in most cases the network need not be convex in all its inputs, but only in the inputs that are being optimized over for inference; in the paper we detail a partial ICNN model that we show generalizes traditional feedforward networks. Indeed, optimization is an operation that can greatly increase the complexity of functions: for a convex piecewise linear function F (x, z), G(x) = minz\u2208Z F (x, z) can be substantially more complex than F (x, z) itself. Together, this suggests that the representational power of such functions can be quite large.\nThe main contributions of this paper are as follows: we propose the ICNN architecture as well as partially convex or biconvex variants; we develop efficient optimization/inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the existing state of the art."}, {"heading": "2 Background and related work", "text": "Energy based learning The interplay between inference, optimization, and structured prediction has a long history in neural networks. Several early incarnations of neural networks were explicitly trained to produce structured sequences (e.g. Simard and LeCun [1991]), and there was an early appreciation that structured models like hidden Markov models could be combined with the outputs of neural networks [Bengio et al., 1994]. Much of this earlier work is surveyed and synthesized by LeCun et al. [2006], who give a tutorial on these energy based learning methods. In recent years, there has been a strong push to further incorporate structured prediction methods like conditional random fields as the \u201clast layer\u201d of a deep network architecture [Peng et al., 2009, Zheng et al., 2015, Chen et al., 2015]. Several methods have proposed to build general neural networks over joint input and output spaces, and perform inference over outputs using generic optimization techniques [Goodfellow et al., 2014, Belanger and McCallum, 2015].\nThe current work is highly related to these past approaches, but also differs in a very particular way. To the best of our knowledge, each of these structured prediction methods based upon energybased models operates in one of two ways, either: 1) the architecture is built in a very particular way such that optimization over the output is guaranteed to be \u201ceasy\u201d (e.g. convex, or the result of running some inference procedure), usually by introducing a structured linear objective at the last layer of the network; or 2) no attempt is made to make the architecture \u201ceasy\u201d to run inference\nover, and instead a general model is built over the output space. In contrast, our approach lies somewhere in between: by ensuring convexity of the resulting decision space, we are constraining the inference problem to be easy in some respect, but we specify very little about the architecture other than the constraints required to make it convex. In particular, as we will show, the network architecture over the variables to be optimized over can be deep and involve multiple non-linearities. The goal of the proposed work is to allow for complex functions over the output without needing to specify them manually (exactly analogous to how current deep neural networks treat their input space).\nStructured prediction and MAP inference Our work also draws some connection to MAPinference-based learning and approximate inference. There are two broad classes of learning approaches in structured prediction: method that use probabilistic inference techniques (typically exploiting the fact that the gradient of log likelihood is given by the actual feature expectations minus their expectation under the learned model [Koller and Friedman, 2009, Ch 20]), and methods that rely solely upon MAP inference (such as max-margin structured prediction [Taskar et al., 2005, Tsochantaridis et al., 2005]). MAP inference in particular also has close connections to optimization, as various convex relaxations of the general MAP inference problem often perform well in practice or in theory. The proposed methods can be viewed as an extreme case of this second class of algorithm, where inference is based solely upon a convex optimization problem that may not have any probabilistic semantics at all. Finally, although it is more abstract, we feel there is a philosophical similarity between our proposed approach and sum-product networks [Poon and Domingos, 2011]; both settings define networks where inference is accomplished \u201ceasily\u201d either by a sum-product message passing algorithm (by construction) or via convex optimization.\nFitting convex functions Finally, the proposed work relates to a topic less considered in the machine learning literature, that of fitting convex functions to data [Boyd and Vandenberghe, 2004, pg. 338]. Indeed our learning problem can be viewed as parameter estimation under a model that is guaranteed to be convex by its construction. The most similar work of which we are aware specifically fit sums of rectified half-planes to data [Magnani and Boyd, 2009], which is similar to one layer of our rectified linear units. However, the actual training scheme is much different, and our deep network architecture allows for a much richer class of representations, while still maintaining convexity."}, {"heading": "3 Convex neural network architectures", "text": "Here we more formally present different ICNN architectures and prove their convexity properties given certain constraints on the parameter space. Our chief claim is that the class of (full and partial) input convex models is rich and lets us capture complex joint models over the input to a network."}, {"heading": "3.1 Fully input convex neural networks", "text": "To begin, we consider a fully convex, k-layer, fully connected ICNN, shown in Figure 1. This model defines a neural network over the input y (i.e., omitting any x term in this function) using the architecture\nzi+1 = gi\n( W\n(z) i zi +W (y) i y + bi\n) , i = 0, . . . , k \u2212 1\nf(y; \u03b8) = zk (7)\nwhere zi denotes the layer activations (with z0,W (z) 0 \u2261 0), \u03b8 = {W (y) 0:k\u22121,W (z) 1:k\u22121, b0:k\u22121} are the parameters, and gi are non-linear activation functions. The central result on convexity of the network is the following:\nProposition 1. The function f is convex in y provided that all W (z) 1:k\u22121 are non-negative, and all functions gi are convex and non- decreasing.\nThe proof is simple and follows from the fact that non-negative sums of convex functions are also convex and that the composition of a convex and convex non-decreasing function is also convex (see e.g. Boyd and Vandenberghe [2004, 3.2.4]). The constraint that the gi be convex non-decreasing is not particularly restrictive, as current non-linear activation units like the rectified linear unit or max-pooling unit already satisfy this constraint. The constraint that the W (z) terms be nonnegative is somewhat restrictive, but because the bias terms and W (y) terms can be negative, the network still has substantial representation power, as we will shortly demonstrate empirically.\nOne notable addition in the ICNN are the \u201cpassthrough\u201d layers that directly connect the input y to hidden units in deeper layers. Such layers are unnecessary in traditional feedforward networks because previous hidden units can always be mapped to subsequent hidden units with the identity mapping; however, for ICNNs, the non-negativity constraint subsequent W (z) weights restricts the allowable use of hidden units that mirror the identity mapping, and so we explicitly include this additional passthrough. Some passthrough layers have been recently explored in the deep residual networks [He et al., 2015] and densely connected convolutional networks [Huang et al., 2016], though these differ from those of an ICNN as they pass through hidden layers deeper in the network, whereas to maintain convexity our passthrough layers can only apply to the input directly.\nAs we detail below, other linear operators like convolutions can be included in ICNNs without changing the convexity properties. Indeed, modern feedforward architectures such as AlexNet [Krizhevsky et al., 2012], VGG [Simonyan and Zisserman, 2014], and GoogLeNet [Szegedy et al., 2015] with ReLUs [Nair and Hinton, 2010] can be made input convex with Proposition 1. In the experiment that follow, we will explore ICNNs with both fully connected and convolutional layers."}, {"heading": "3.2 Partially input convex architectures", "text": "The FICNN provides joint convexity over the entire input to the function, which indeed may be a restriction on the allowable class of models; indeed, neural networks derive much of their power from the fact that they are general function approximators. Furthermore, this full joint convexity is unnecessary in settings like structured prediction where the neural network is used to build a joint model over an input and output example space and only convexity over the outputs is necessary.\nIn this section we propose an extension to the pure FICNN, the partially input convex neural network (PICNN), that is convex only over some inputs to the network. Indeed, when we refer to ICNNs broadly we will typically mean these partially convex functions. As we will show, these networks generalize both traditional feedforward networks and FICNNs, and thus provide substantial representational benefits. We define a PICNN to be a network over (x, y) pairs f(x, y; \u03b8) where f is convex in y but not convex in x. Figure 2 illustrates one potential k-layer PICNN architecture defined by the recurrences\nui+1 = g\u0303i(W\u0303iui + b\u0303i)\nzi+1 = gi\n( W\n(z) i ( zi \u25e6 [W (zu)i ui + b (z) i ]+ ) +W (y) i ( yi \u25e6 (W (yu)i ui + b (y) i ) ) +W (u) i ui + bi ) f(x, y; \u03b8) = zk, u0 = x (8)\nwhere ui \u2208 Rni and zi \u2208 Rmi denote the hidden units for the \u201cx-path\u201d and \u201cy-path\u201d, where y \u2208 Rp, and where \u25e6 denotes the Hadamard product, the elementwise product between two vectors. The crucial element here is that unlike the FICNN, we only need the W (z) terms to be non-negative, and we can introduce arbitrary products between the ui hidden units and the zi hidden units. Although more general formulations are possible (e.g., we could involve arbitrary linear functions of the outer product uiz T i , these would result in very large numbers of parameters, and can always be captured by above architecture by simply adding additional layers that contain more hidden units). The following proposition highlights the representational power of the PICNN.\nProposition 2. A PICNN network with k layers can represent any FICNN with k layers and any purely feedforward network with k layers.\nProof. To recover a FICNN we simply set the weights over the entire x path to be zero and set b(z) = b(y) = 1. We can recover a feedforward network by noting that a traditional feedforward network f\u0302(x; \u03b8) where f : X \u2192 Y, can be viewed as a network with an inner product f(x; \u03b8)T y in its last layer (see e.g. LeCun et al. [2006] for more details). Thus, a feedforward network can be represented as a PICNN by setting the x path to be exactly the feedforward component, then having the y path be all zero except W (yu) k\u22121 = I and W (y) k\u22121 = 1 T .\nBiconvex architectures Although we do not discuss it in detail here, we can also develop an intermediate model between the PICNN and FICNN that is not convex in (x, y) jointly, but which is convex in either x or y when the other variables are fixed. Such an architecture would be useful for e.g., the generative embedding model described above, since it would allow for efficient inference over either x or y given the other, but is less restrictive that requiring joint convexity."}, {"heading": "3.3 Convolutional architectures", "text": "Convolutions are important to many visual structured tasks. We have left convolutions out to keep the prior ICNN notation light by using matrix-vector operations. ICNNs can be similarly created with convolutions by viewing the convolution as a linear operator.\nThe construction of convolutional layers in ICNNs depend on the type of input and output space. If the input and output space are similarly structured (e.g. both spatial), the jth feature map of a convolutional PICNN layer i can be defined by\nzji+1 = gi ( zi \u2217W (z)i,j + (Sx) \u2217W (x) i,j + (Sy) \u2217W (y) i,j + bi,j ) (9)\nwhere the convolution kernels W are the same size and S scales the input and output to be the same size as the previous feature map, and were we omit some of the Hadamard product terms that can appear above for simplicity of presentation.\nIf the input space is spatial, but the output space has another structure (e.g. the simplex), the convolution over the output space can be replaced by a matrix-vector operation, such as\nzji+1 = gi ( zi \u2217W (z)i,j + (Sx) \u2217W (x) i,j +B (y) i,j y + bi,j ) (10)\nwhere the product B (y) i,j y is a scalar."}, {"heading": "4 Inference in ICNNs", "text": "Unlike in traditional feedforward neural networks, where predictions are produced via a single forward pass in the network, prediction (which we generally refer to as inference here), requires solving the convex optimization problem\nminimize y\u2208Y f(x, y; \u03b8) (11)\nWhile the resulting tasks are convex optimization problems (and thus \u201ceasy\u201d to solve in some sense), in practice this still involves the solution of a potentially very complex optimization problem. Thus, we here discuss several approaches for (sometimes approximately) solving these optimization problems. We find that in practice, we can obtain reasonably accurate solutions in many setting using a procedure that only involves a small number of forward and backward passes through the network, and which thus has a complexity that in practice is at most a constant factor worse than that for feedforward networks. The same consideration will apply to training such network, which we will discuss in Section 5."}, {"heading": "4.1 Exact inference in ICNNs", "text": "Although it is not a practical approach for solving the optimization tasks, we first highlight the fact that the inference problem for the networks presented above (where the non-linear are either ReLU or linear units) can be posed as as linear program. Specifically, considering the FICNN network in (7) can be written as the optimization problem\nminimize y,z1,...,zk zk\nsubject to zi+1 \u2265W (z)i zi +W (y) i y + bi, i = 0, . . . , k \u2212 1\nzi \u2265 0, i = 1, . . . , k \u2212 1.\n(12)\nThis problem exactly replicates the equations of the FICNN, with the exception that we have replaced ReLU and the equality constraint between layers with a positivity constraint on the zi\nterms and an inequality. However, because we are minimizing the final zk term, and because each inequality constraint is convex, at the solution one of these constraints must be tight, i.e., (zi)j = (W (z) i zi + W (y) i y + bi)j or (zi)j = 0, which recovers the ReLU non-linearity exactly. The exact same procedure can be used to write to create an exact inference procedure for the PICNN. Although the LP formulation is appealing in its simplicity, in practice these optimization problems will have a number of variables equal to the total number of activations in the entire network. Furthermore, most LP solution methods to solve such problems require that we form and invert structured matrices with blocks such as W Ti Wi \u2014 the case for most interior-point methods [Wright, 1997] or even approximate algorithms such as the alternating direction method of multipliers [Boyd et al., 2011] \u2014 which are large dense matrices or have structured forms such as non-cyclic convolutions that are expensive to invert. Even incremental approaches like the Simplex method require that we form inverses of subsets of columns of these matrices, which are additionally different for structured operations like convolutions, and which overall still involve substantially more computation than a single forward pass. Furthermore, such solvers typically do not exploit the substantial effort that has gone in to accelerating the forward and backward computation passes for neural networks using hardware such as GPUs. Thus, as a whole, these do not present a viable option for optimizing the networks."}, {"heading": "4.2 Approximate inference in ICNNs", "text": "Because of this impracticality of exact inference, we consider approximate approaches to optimizing over the inputs to these networks, but ideally ones that still exploit the convexity of the resulting problem. We specifically focus on gradient-based approaches, which use the fact that we can easily compute the gradient of an ICNN with respect to its inputs, \u2207xf(x, y; \u03b8), using exactly the same backpropagation routines used to compute gradients of a loss function of the network with respect to its parameters.\nGradient descent The simplest gradient-based method for solving (11) is just (projected sub-) gradient descent, or modifications such as those that use a momentum term [Polyak, 1964, Rumelhart et al., 1988], or spectral step size modifications [Barzilai and Borwein, 1988, Birgin et al., 2000]. That is, we start with some initial y\u0302 repeat the update\ny\u0302 := PY (y\u0302 \u2212 \u03b1\u2207yf(x, y\u0302; \u03b8)) (13)\nThis method is appealing in its simplicity, but suffers from the typical problems of gradient descent on non-smooth objectives: we need to pick a stepsize and possibly use a sequence of decreasing step sizes, and don\u2019t have an obvious method to assess how accurate of a current solution we have obtained (since the ICNN is piecewise linear it will not have zero gradient at the solution). The method is also more challenging to integrate with some learning procedures, as we often need to differentiate through an entire chain of the gradient descent algorithm Domke [2012]. Thus, while the method can sometimes work in practice, we have found that other approaches typically far outperform this method, and we will focus on alternative approximate approaches for the remainder of this section.\nBundle method One optimization approach that seems particular promising in this setting is the bundle method [Smola et al., 2008]. The bundle method takes advantage of the fact that for a convex objective, the first-order approximation at any point is a global underestimator of the function; this lets us maintain a piecewise linear lower bound on the function by adding cutting\nplanes formed by this first order approximation, and then repeatedly optimizing this lower bound. Specifically, the process follows the procedure shown in Algorithm 1. Denoting the iterates of the algorithm as yk, at each iteration of the algorithm, we compute the first order approximation to the function\nf(x, yk; \u03b8) +\u2207yf(x, yk; \u03b8)T (y \u2212 yk) (14)\nand update the next iteration by solving the optimization problem\nyk+1 := argmin y\u2208Y max 1\u2264i\u2264k {f(x, yi; \u03b8) +\u2207yf(x, yi; \u03b8)T (y \u2212 yi)}. (15)\nA bit more concretely, optimization problem can be written via a set of linear inequality constraints\nyk+1, tk+1 := argmin y\u2208Y,t {t | Gy + h \u2264 t1} (16)\nwhere G \u2208 Rk\u00d7n has rows equal to gTi = \u2207yf(x, yi; \u03b8)T (17)\nand h \u2208 Rk has entries equal to\nhi = f(x, y i; \u03b8)\u2212\u2207yf(x, yi; \u03b8)T yi. (18)\nThis method has the advantage that it typically requires much fewer gradient iterations than a gradient method, because each gradient is maintained to continually improve a lower bound on the function. The downside is that now need a solve a (simpler, but still potentially complex) optimization problem within each iteration of the algorithm, which is still relatively low using offthe-shelf solvers. Furthermore, if the constraint set Y represents some convex polytope, then we will need at least n+ 1 gradient evaluations to ever reach a point in the interior of the polytope, as the solution to the resulting linear program will always lie on the boundary of the constraint set, and we would require n+ 1 constraints to ensure that no constraint defined by the original feasible polytope is active at the solution.\nAlgorithm 1 A typical bundle method to optimize f : Rm\u00d7n \u2192 R over Rn for K iterations with a fixed x and initial starting point y1.\nfunction BundleMethod(f , x, y1, K) G \u2190 0 \u2208 RK\u00d7n h \u2190 0 \u2208 RK for k = 1,K do\nGTk \u2190 \u2207yf(x, yk; \u03b8)T . kth row of G hk \u2190 f(x, yk; \u03b8)\u2212\u2207yf(x, yk; \u03b8)T yk yk+1, tk+1 \u2190 argminy\u2208Y,t {t | G1:ky + h1:k \u2264 t1}\nend for return yK+1\nend function"}, {"heading": "4.3 Approximate inference via the bundle entropy method", "text": "To overcome these challenges, we have developed a new optimization algorithm for this domain that we term the bundle entropy method. This algorithm specifically applies to the (very common)\ncase where Y consists simply of a set of bounds on each variable, which we assume to be Y = [0, 1]n (other upper or lower bounds can be attained through scaling). The method is also easily extensible to the setting where elements of Y belong to a higher-dimensional probability simplex as well.\nIn this setting, we consider adding an additional \u201cbarrier\u201d function to the optimization in the form of the negative entropy \u2212H(y), where\nH(y) = \u2212 n\u2211 i=1 (y log y + (1\u2212 y) log(1\u2212 y)). (19)\nIn other words instead want to solve the optimization problem\nminimize y\nf(x, y; \u03b8)\u2212H(y) (20)\nwhere we can additionally add a scaling between f and the entropy term if desired (in practice, because we can scale the network weights instead, this seems to rarely be necessary). The negative entropy is a convex function, with the limits of limy\u21920H(y) = limy\u21921H(y) = 0, and negative values in the interior of this range. The function acts as a barrier because, although it does not approach infinity as it reaches the barrier of the feasible set, its gradient does approach infinity as it reaches the barrier, and thus the optimal solution will always lie in the interior of the unit hypercube Y.\nAn appealing feature of the entropy regularization comes from its close connection with sigmoid units in typical neural networks. It follows easily from first-order optimality conditions that the optimization problem\nminimize y\ncT y \u2212H(y) (21)\nis given by y? = 1/(1 + exp(c)). Thus if we consider the \u201ctrivial\u201d PICNN mentioned in Section 3.2, which simply consists of the function f(x, y; \u03b8) = yT f\u0303(x; \u03b8) for some purely feedforward network f\u0303(x; \u03b8), then the entropy-regularized minimization problem gives a solution that is equivalent to simply taking the sigmoid of the neural network outputs, which is typically used to compute the logistic or cross-entropy loss. Thus, the move to ICNNs can be interpreted as providing a more structured joint energy functional over the linear function implicitly used by sigmoid layers.\nWe now combine the bundle method described above with the entropy regularization term. Using the notation above, this involves repeating the updates\nyk+1, tk+1 := argmin y,t {t\u2212H(y) | Gy + h \u2264 t1} (22)\nWe solve these optimization problems using a dual approach. The Lagrangian of the optimization problem is\nL(y, t, \u03bb) = t\u2212H(y) + \u03bbT (Gy + h\u2212 t1). (23)\nDifferentiating with respect to y and t gives the optimality conditions\n\u2207yL(y, t, \u03bb) = 0 =\u21d2 y = 1\n1 + exp(GT\u03bb)\n\u2207tL(y, t, \u03bb) = 0 =\u21d2 1T\u03bb = 1 (24)\nwhich in turn leads to the dual problem\nmaximize \u03bb\n(G1 + h)T\u03bb\u2212 1T log(1 + exp(GT\u03bb))\nsubject to \u03bb \u2265 0, 1T\u03bb = 1. (25)\nThis is a smooth optimization problem over the unit simplex, and can be solved, e.g., the Projected Newton method of [Bertsekas, 1982, pg. 241, eq. 97]. The basic strategy is to maintain a set of \u03bb variables that are not bound at zero, and perform a Newton update over k \u2212 1 of these indices (to account for the equality constraint). Given that we typically have relatively few active constraints (this is one of the benefits of the entropy regularization term), and the fact that we can prune constraints that are no longer active, this algorithm can typically find a solution to machine precision very quickly. A complete description of the bundle entropy method is given in Algorithm 2.\nAlgorithm 2 Our bundle entropy method to optimize f : Rm \u00d7 [0, 1]n \u2192 R over [0, 1]n for K iterations with a fixed x and initial starting point y1.\nfunction BundleEntropyMethod(f , x, y1, K) G` \u2190 [ ] h` \u2190 [ ] for k = 1,K do\nAppend(G`, \u2207yf(x, yk; \u03b8)T ) Append(h`, f(x, y\nk; \u03b8)\u2212\u2207yf(x, yk; \u03b8)T yk) ak \u2190 Length(G`) . The number of active constraints. Gk \u2190 Concat(G`)\u2208 Rak\u00d7n hk \u2190 Concat(h`)\u2208 Rak if ak = 1 then\n\u03bbk \u2190 1 else\n\u03bbk \u2190 ProjNewtonLogistic(Gk, hk) end if yk+1 \u2190 (1 + exp(GTk \u03bbk))\u22121 Delete(G`[i] and h`[i] where \u03bbi \u2264 0) . Prune inactive constraints.\nend for return yK+1\nend function\nFor lower dimensional problems, the bundle entropy method often attains an exact solution after a relatively small number of iterations. And even for larger problems, we find in practice that the approximate solutions generated by a very small number of iterations (typically, we use 5 iterations in practice), still substantially outperform gradient descent approaches.\nAn additional advantage of the bundle method is that, unlike gradient-based methods, we can use the Bundle method to obtain an estimate of suboptimality for each iterate. Specifically, because tk+1 \u2212 H(yk+1) (i.e., the objective value returned by the kth iterate of the bundle method) is a lower bound on the true objective, we have the optimality bound\nf(x, yk+1; \u03b8)\u2212H(yk+1)\u2212min y {f(x, y; \u03b8)\u2212H(y)} \u2264 f(x, yk+1)\u2212H(yk+1)\u2212 (tk+1 \u2212H(yk+1))\n= f(x, yk+1; \u03b8)\u2212 tk+1. (26)\nThis lets us control the precision of the bundle method, if desired, though as mentioned, in practice we typically get good performance limiting this to about 5 iterations."}, {"heading": "5 Learning ICNNs", "text": "Generally speaking, ICNN learning shapes the objective\u2019s energy function to produce the desired values when optimizing over the relevant inputs. That is, for a given input output pair (x, y?), our goal is to find ICNN parameters \u03b8 such that\ny? \u2248 argmin y f\u0303(x, y; \u03b8) (27)\nwhere for the entirely of this section, we use the notation f\u0303 to denote the combination of the neural network function plus the regularization term such as \u2212H(y), if it is included, i.e.\nf\u0303(x, y; \u03b8) = f(x, y; \u03b8)\u2212H(y). (28)\nAlthough we only discuss the entropy regularization in this work , we emphasize that other regularizers are also possible.\nDepending on the setting, there are several different approaches we can use to ensure that the ICNN achieves the desired targets, and we consider three approaches below: direct functional fitting, max-margin structured prediction, and argmin differentiation."}, {"heading": "5.1 Direct functional fitting", "text": "Although it is simple enough that it does not require substantial discussion, we first note that in some domains, we do not need a specialized procedure for fitting ICNNs, but can use existing approaches that directly fit the ICNN function. An example of this is the Q-learning setting, where the goal of our algorithm is to fit Q(s, a) = \u2212f\u0303(s, a; \u03b8) such that\nQ(s, a) = R(s, a) + \u03b3E [ argmax\na\u2032 Q(s\u2032, a\u2032))\n] . (29)\nGiven some observed tuple (s, a, r, s\u2032), Q learning performs the updates\n\u03b8 := \u03b8 \u2212 \u03b1 ( Q(s, a)\u2212 r \u2212 \u03b3 argmax\na\u2032 Q(s\u2032, a\u2032)\n) \u2207\u03b8Q(s, a). (30)\nThese same exact updates can be applied to the ICNN case, with the only additional requirement that we project the weights onto their feasible sets after this update (i.e., clip any W terms that are required to be positive). Other than this, learning proceeds exactly as before."}, {"heading": "5.2 Max-margin structured prediction", "text": "In the more traditional structured prediction setting, where we do not aim to fit the energy function directly but fit the predictions made by the system to some target outputs, there are different possibilities for learning the ICNN parameters. One such method is based upon the max-margin structured prediction framework [Tsochantaridis et al., 2005, Taskar et al., 2005]. Given some training example (x, y?), we would like to require that this example has a joint energy that is lower than all other possible values for y. That is, we want the function f\u0303 to satisfy the constraint\nf\u0303(x, y?; \u03b8) \u2264 min y f\u0303(x, y; \u03b8) (31)\nUnfortunately, these conditions can be trivially fit by choosing a constant f\u0303 (although the entropy term alleviates this problem slightly, we can still choose an approximately constant function), so\ninstead the max-margin approach adds a margin-scaling term that requires this gap to be larger for y further from y?, as measured by some loss function \u2206(y, y?). Additionally adding slack variables to allow for potential violation of these constraints, we arrive at the typical max-margin structured prediction optimization problem\nminimize \u03b8,\u03be\u22650\n\u03bb 2 \u2016\u03b8\u201622 + m\u2211 i=1 \u03bei\nsubject to f\u0303(xi, yi; \u03b8) \u2264 min y\u2208Y\n( f\u0303(xi, y; \u03b8)\u2212\u2206(yi, y) ) \u2212 \u03bei\n(32)\nAs a simple example, for multiclass classification tasks where y? denotes a \u201cone-hot\u201d encoding of examples, we can use a multi-variate entropy term and let \u2206(y, y?) = y?T (1\u2212y). Training requires solving this \u201closs-augmented\u201d inference problem, which is convex for suitable choices of the margin scaling term.\nThe optimization problem (2) is naturally still not convex in \u03b8, but can be solved via the subgradient method for structured prediction [Ratliff et al., 2007]. This algorithm iteratively selects a training example xi, yi, then 1) solves the optimization problem\ny? = argmin y\u2208Y f(xi, y; \u03b8)\u2212\u2206(yi, y) (33)\nand 2) if the margin is violated, updates the network\u2019s parameters according to the subgradient\n\u03b8 := P+ [\u03b8 \u2212 \u03b1 (\u03bb\u03b8 +\u2207\u03b8f(xi, yi, \u03b8)\u2212\u2207\u03b8f(xi, y?; \u03b8))] (34)\nwhere P+ denotes the projection of W (z)1:k\u22121 onto the non-negative orthant. This method can be easily adapted to use mini-batches instead of a single example per subgradient step, and also adapted to alternative optimization methods like AdaGrad [Duchi et al., 2011] or ADAM [Kingma and Ba, 2014]. Further, a fast approximate solution to y? can be used instead of the exact solution.\nAlthough max-margin structured prediction is a simple and well-studied approach, in our experiences using these methods within an ICNN, we had substantial difficulty choosing the proper margin scaling term (especially for domains with continuous-valued outputs), or allowing for losses other than the hinge loss. For this reason, the majority of our experiments here focus on the next approach, which more directly encodes the loss suffered by the full structured-prediction pipeline."}, {"heading": "5.3 Argmin differentiation", "text": "In our final proposed approach, that of argmin differentiation, we propose to directly minimize a loss function between true outputs and the outputs predicted by our model, where these predictions themselves are the result of an optimization problem. We explicitly consider the case where the approximate solution to the inference problem is attained via the previously-described bundle entropy method, typically run for some fixed (usually small) number of iterations. To simplify notation, in the following we will let\ny\u0302(x; \u03b8) = argmin y min t {t\u2212H(y) | Gy + h \u2264 t1} \u2248 argmin y f\u0303(x, y; \u03b8) (35)\nrefer to the approximate minimization over y that results from running the bundle entropy method, specifically at the last iteration of the method.\nGiven some example (x, y?), our goal is to compute the gradient, with respect to the ICNN parameters, of the loss between y? and y\u0302(x; \u03b8): `(y\u0302(x; \u03b8), y?). This is in some sense the most\ndirect analogue to traditional neural network learning, since we typically optimize networks by minimizing some loss between the network\u2019s (feedforward) predictions and the true desired labels. Doing this in the predictions-via-optimization setting requires that we differentiate \u201cthrough\u201d the argmin operator, which can be accomplished via implicit differentiation of the KKT optimality conditions. Although the derivation in somewhat involved, the final result is fairly compact, and is given by the following proposition (for simplicity, we will write y\u0302 below instead of y\u0302(x; \u03b8) when the notation should be clear):\nProposition 3. The gradient of the neural network loss for predictions generated through the minimization process is given by\n\u2207\u03b8`(y\u0302(x; \u03b8), y?) = k\u2211 i=1 ( c\u03bbi\u2207\u03b8f(x, yi; \u03b8) +\u2207\u03b8 ( \u2207yf(x, yi; \u03b8)T ( \u03bbic y + c\u03bbi ( y\u0302(x; \u03b8)\u2212 yi )))) (36)\nwhere yi denote the solution returned by the ith iteration of the entropy bundle method, \u03bb denotes the dual variable solution of the entropy bundle method, and where the c variables are determined by the solution to the linear system diag ( 1 y\u0302 + 1 1\u2212y\u0302 ) GT 0\nG 0 \u22121 0 \u22121T 0   cyc\u03bb ct  =  \u2212\u2207y\u0302`(y\u0302, y\u0302)0 0  . (37) Before proving this result, we highlight a few key points. The complexity of computing this gradient will be linear in k, which is the number of active constraints at the solution of the bundle entropy method. The inverse of this matrix can also be computed efficiently by just inverting the\nk\u00d7 k matrix Gdiag (\n1 y + 1 1\u2212y\n) GT via the a variable elimination procedure, instead of by inverting\nthe full matrix. The gradients \u2207\u03b8f(x, yi; \u03b8) are standard neural network gradients, and further, can be computed in the same forward/backward pass as we use to compute the gradients for the bundle entropy method. The main challenge of the method is to compute the terms of the form\n\u2207\u03b8(\u2207yf(x, yi; \u03b8)T v) (38)\nfor some vector v. This quantity can be computed by most autodifferentiation tools (the gradient inner product \u2207yf(x, yi; \u03b8)T v itself just becomes a graph computation than can be differentiated itself), or it can be computed via the finite difference approximation\n\u2207\u03b8(\u2207yf(x, yi; \u03b8)T v) = lim \u21920\n1\n(\u2207\u03b8f(x, yi + v; \u03b8)\u2212\u2207\u03b8f(x, yi; \u03b8)) . (39)\nIn both cases the complexity of computing this entire gradient is a small constant multiple of computing k gradients with respect to \u03b8.\nProof (of Proposition 3). We have by the chain rule that\n\u2202` \u2202\u03b8 = \u2202` \u2202y\u0302\n( \u2202y\u0302\n\u2202G\n\u2202G \u2202\u03b8 + \u2202y\u0302 \u2202h \u2202h \u2202\u03b8\n) . (40)\nThe challenging terms to compute in this equation are the \u2202y\u0302\u2202G and \u2202y\u0302 \u2202h terms. These can be computed (although we will ultimately not compute them explicitly, but just compute the product\nof these matrices and other terms in the Jacobian), by implicit differentiation of the KKT conditions. Specifically, the KKT conditions of the bundle entropy method (considering only the active constraints at the solution) are given by\n1 + log y\u0302 \u2212 log(1\u2212 y\u0302) +GT\u03bb = 0 Gy\u0302 + h\u2212 t1 = 0\n1T\u03bb = 1.\n(41)\nFor simplicity of presentation, we consider first the Jacobian with respect to h. Taking differentials of these equations with respect to h gives\ndiag\n( 1\ny\u0302 +\n1\n1\u2212 y\u0302\n) dy +GTd\u03bb = 0\nGdy + dh\u2212 dt1 = 0 1Td\u03bb = 0\n(42)\nor in matrix form  diag ( 1 y\u0302 + 1 1\u2212y\u0302 ) GT 0\nG 0 \u22121 0 \u22121T 0\n  dyd\u03bb\ndt\n =  0\u2212dh\n0  . (43) To compute the Jacobian \u2202y\u0302\u2202h we can solve the system above with the right hand side given by dh = I, and the resulting dy term will be the corresponding Jacobian. However, in our ultimate objective we always left-multiply the proper terms in the above equation by \u2202`\u2202y\u0302 . Thus, we instead define  cyc\u03bb\nct\n =  diag ( 1 y + 1 1\u2212y ) GT 0\nG 0 \u22121 0 \u22121T 0\n \u22121  \u2212( \u2202`\u2202y? )T0\n0  (44) and we have the the simple formula for the Jacobian product\n\u2202`\n\u2202y\u0302\n\u2202y\u0302 \u2202h = (c\u03bb)T . (45)\nA similar set of operations taking differentials with respect to G leads to the matrix equations diag ( 1 y + 1 1\u2212y ) GT 0\nG 0 \u22121 0 \u22121T 0\n  dyd\u03bb\ndt\n =  \u2212dGT\u03bb\u2212dGy\n0  (46) and the corresponding Jacobian products / gradients are given by\n\u2202`\n\u2202y\u0302\n\u2202y\u0302 \u2202G = cy\u03bbT + y\u0302(c\u03bb)T . (47)\nFinally, using the definitions that\ngTi = \u2207yf(x, yi; \u03b8)T , hi = f(x, yk; \u03b8)\u2212\u2207yf(x, yi; \u03b8)T yi (48)\nwe recover the formula presented in the proposition.\nGiven this ability to compute gradients with respect to an arbitrary loss function, we can fit the parameter using traditional stochastic gradient methods examples. Specifically, given an example (or a minibatch of examples) xi, yi, we compute gradients\n\u2207\u03b8`(y\u0302(xi; \u03b8), yi) (49)\nand update the parameters using e.g. the ADAM optimizer."}, {"heading": "6 Experiments", "text": "Our experiments study the representational power of ICNNs to better understand the interplay between the model\u2019s restrictiveness and accuracy. This section presents our results on multi-label classification on the bibtex dataset [Katakis et al., 2008], image completion using the Olivetti face dataset [Samaria and Harter, 1994], and continuous action reinforcement learning in the OpenAI Gym [Brockman et al., 2016].\nOur multi-label classification experiments use a fully-connected PICNN. Our image completion experiments use a convolutional DQN-style PICNN where the input space X is the right side of the image and the output space Y is the left side of the image. Our reinforcement learning experiments use a fully-connected PICNN where the input space X is over the possible current states and the output space Y is the continuous action space.\nWe have made the source code to reproduce our experiments openly available on GitHub at https://github.com/locuslab/icnn under the arxiv/v1 tag on the master branch. Our Python [Van Rossum and Drake Jr, 1995] implementation uses numpy [Oliphant, 2006] for linear algebra primitives and TensorFlow [Abadi et al., 2016] for neural networks and auto-differentiation."}, {"heading": "6.1 Synthetic classification examples", "text": "We begin with a simple example to illustrate the classification performance of a two-hidden-layer FICNN and PICNN on two-dimensional binary binary classification tasks from the scikit-learn toolkit [Pedregosa et al., 2011]. Figure 3 shows the classification performance on the dataset. The FICNN\u2019s energy function which is fully convex in X \u00d7 Y jointly is able to capture complex, but sometimes restrictive decision boundaries. The PICNN, which is nonconvex over X but convex over Y overcomes these restrictions and can capture more complex decision boundaries."}, {"heading": "6.2 Multi-Label Classification", "text": "We next study how ICNNs perform on multi-label classification with the bibtex dataset and benchmark presented in Katakis et al. [2008]. This benchmark maps text classification from an input space X of 1836 bag-of-works indicator (binary) features to an output space Y of 159 binary labels. We use the train/test split of 4880/2515 from Katakis et al. [2008] and evaluate with the exampleaveraged (macro) F1 score. We use the ARFF version of this dataset from Mulan [Tsoumakas et al., 2011]. Our PICNN architecture for multi-label classification uses fully-connected layers with ReLU activation functions and batch normalization [Ioffe and Szegedy, 2015] along with input path. As a baseline, we use a fully-connected neural network with batch normalization and ReLU activation functions.\nFigure 4 compares the training of a feedforward baseline to a PICNN with the same structure (600 fully connected, 159 (#labels) fully connected). While the PICNN does not improve the F1 score, we emphasize that the performance is similar to the feedforward network. Our baseline feedforward network\u2019s best macro F1 score of 45.5 outperforms the best macro-F1 score of 43.4 from a survey presented in Madjarov et al. [2012]. SPENs [Belanger and McCallum, 2015] obtain a macro-F1 score of 42.2. The high performance of our baseline indicates that it\u2019s likely the bibtex benchmark does not have much structure for ICNNs or other techniques to learn."}, {"heading": "6.3 Image completion on the Olivetti faces", "text": "As a test of the system on a structured prediction task over a much more complex output space Y, we apply a convolutional DQN-style PICNN to face completion on the sklearn version [Pedregosa et al., 2011] of the Olivetti data set [Samaria and Harter, 1994], which contains 400 64x64 grayscale\nimages Our PICNN is equivalent to the DQN architecture in Mnih et al. [2015] (except we have added batch normalization to the input path) and is over (x, y) pairs where x (32x64) is the left half and y (32x64) is the right half of the image. The input and output paths are: 32x8x8 conv (stride 4x2), 64x4x4 conv (stride 2x2), 64x3x3 conv, 512 fully connected. Any modern architecture can be used in place of a DQN and we have not well-explored the space of possible architectures. Our weak motivation for choosing a DQN architecture comes from its use of strided convolutions over pooling. Our intuition is that networks for image completion do not need (and can be hurt by) the translation-invariance provided by pooling layers present in other modern architectures.\nThis experiment uses the same training/test splits and minimizes the mean squared error (MSE) as in Poon and Domingos [2011] so that our results can be directly compared to (a non-exhaustive list of) other techniques. We also explore the tradeoffs between our bundle entropy method and back optimization and use a non-convex baseline to better understand the impacts of convexity. Our back optimization techniques use gradient descent. Back optimization will adapt the network to any learning rate and momentum. We arbitrarily use a learning rate of 0.01 and a momentum of 0.9.\nFigure 5 shows the training/testing performance of networks that use five iterations in the\ninner optimization (bundle entropy method or gradient descent). Figure 6 shows the best image completions on the full test set by the network trained with the bundle entropy method.\nThe minimum test MSE for the bundle entropy, back optimization, and back optimization with convexity relaxed are 833.0, 872.0, and 850.9, respectively. Our network improves upon performance of sum-product networks, which obtain an MSE of 942 [Poon and Domingos, 2011]. These results show that the bundle entropy method can leverage more information from these five iterations than gradient descent, even when the convexity constraint is relaxed. The PICNN trained with back-optimization with the relaxed convexity constraint slightly outperforms the network with the convexity constraint, but not the network trained with the bundle-entropy method. This shows that for image completion with PICNNs, convexity only slightly inhibits the representational power.\nA potential concern when using ICNNs is that the inner optimization is too expensive and may require too many iterations. In this experiment, we have shown that only five iterations in the inner optimization is sufficient. Given more iterations, the error gap between the bundle-entropy method and back-optimization may close and networks trained with back-optimization may outperform networks trained with the bundle-entropy method."}, {"heading": "6.4 Continuous Action Reinforcement Learning", "text": "In this experiment, we present our preliminary ICNN results on standard benchmarks in continuous action reinforcement learning from the OpenAI Gym [Brockman et al., 2016] that use the MuJoCo physics simulator [Todorov et al., 2012]. Table 1 shows the dimensionality of the continuous control benchmarks from the MuJoCo portions of the OpenAI gym. As introduced earlier, given continuous state and action spaces S \u00d7 A, we model the (negative) Q function, \u2212Q(s, a; \u03b8) as an ICNN and select actions with the convex optimization problem (4). We use Q learning to optimize the ICNN with the update in (30).\nFor our preliminary results, we only focus on a few environments with small state and action spaces. All of our experiments use a PICNN with two fully-connected layers that each have 200 hidden units. We use Deep Deterministic Policy Gradient (DDPG) [Lillicrap et al., 2015] and Normalized Advantage Functions (NAF) [Gu et al., 2016] as state-of-the-art baselines. Because there are not official DDPG or NAF implementations or results on the OpenAI gym tasks, we use the Simon Ramstedt\u2019s DDPG implementation from https://github.com/SimonRamstedt/ddpg and have re-implemented NAF.\nNAF poses a particularly interesting comparison point to ICNNs. In particular, NAF decomposes theQ function in terms of the value function an an advantage functionQ(s, a) = V (s)+A(s, a)\nwhere the advantage function is restricted to be concave quadratic in the actions, and thus always have closed-form solution. In a sense, this closely mirrors the setup of the PICNN architecure: like NAF, we have a separate non-convex path for the s variables, and an overall function that is convex in a; however, the distinction is that while NAF requires that the convex portion be quadratic, the ICNN architecture allows any convex functional form. As our experiments show, this representation power does allow for better performance of the resulting system, though the trade-off, of course, is that determining the optimal action in an ICNN is substantially more computationally complex that for a quadratic.\nFigure 7 compares the test performance of PICNNs, DDPG, and NAF. The error bars show the 95% confidence interval of a Gaussian distribution: \u00b11.96\u03c3, where \u03c3 is the standard deviation of the means. PICNNs excel on tasks like InvertedPendulum-v1 and Reacher-v1, and Hopper. HalfCheetah-v1 (Split) explains the high PICNN variance on this task by showing how our experiments can be split into high-performing (H, 6/10 trials) and low-performing (L, 4/10 trials) models. The method currently does not perform well on the InvertedDoublePendulum-v1 task, and we are still investigating performance on the larger continuous domains. Nonetheless, this initial performance does highlight the fact that there can be substantial representational benefit to using the ICNN architecture within a simple Q learning framework."}, {"heading": "7 Conclusion and future work", "text": "This paper laid the groundwork for the input convex neural network model. By incorporating relatively simple constraints into existing network architectures, we can fit very general convex functions and the apply optimization as an inference procedure. Since many existing models already fit into this overall framework (e.g., CRF models perform an optimization over an output space where parameters are given by the output of a neural network), the proposed method presents an extension where the entire inference procedure is \u201clearned\u201d along with the network itself, without the need for explicitly building typical structured prediction architectures. This work explored only a small subset of the possible applications of these network, and the networks offer promising directions for many additional domains."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Two-point step size gradient methods", "author": ["Jonathan Barzilai", "Jonathan M Borwein"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "Barzilai and Borwein.,? \\Q1988\\E", "shortCiteRegEx": "Barzilai and Borwein.", "year": 1988}, {"title": "Structured prediction energy networks", "author": ["David Belanger", "Andrew McCallum"], "venue": null, "citeRegEx": "Belanger and McCallum.,? \\Q2015\\E", "shortCiteRegEx": "Belanger and McCallum.", "year": 2015}, {"title": "Globally trained handwritten word recognizer using spatial representation, convolutional neural networks, and hidden markov models", "author": ["Yoshua Bengio", "Yann LeCun", "Donnie Henderson"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Projected newton methods for optimization problems with simple constraints", "author": ["Dimitri P Bertsekas"], "venue": "SIAM Journal on control and Optimization,", "citeRegEx": "Bertsekas.,? \\Q1982\\E", "shortCiteRegEx": "Bertsekas.", "year": 1982}, {"title": "Nonmonotone spectral projected gradient methods on convex sets", "author": ["Ernesto G Birgin", "Jos\u00e9 Mario Mart\u0301\u0131nez", "Marcos Raydan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Birgin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Birgin et al\\.", "year": 2000}, {"title": "Learning deep structured models", "author": ["Liang-Chieh Chen", "Alexander G Schwing", "Alan L Yuille", "Raquel Urtasun"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Generic methods for optimization-based modeling", "author": ["Justin Domke"], "venue": "In Proceedings of the Conference on AI and Statistics,", "citeRegEx": "Domke.,? \\Q2012\\E", "shortCiteRegEx": "Domke.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Shixiang Gu", "Timothy Lillicrap", "Ilya Sutskever", "Sergey Levine"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Multilabel text classification for automated tag suggestion", "author": ["Ioannis Katakis", "Grigorios Tsoumakas", "Ioannis Vlahavas"], "venue": "ECML PKDD discovery challenge,", "citeRegEx": "Katakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Katakis et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Daphne Koller", "Nir Friedman"], "venue": "MIT press,", "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A tutorial on energy-based learning", "author": ["Yann LeCun", "Sumit Chopra", "Raia Hadsell", "M Ranzato", "F Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "An extensive experimental comparison of methods for multi-label learning", "author": ["Gjorgji Madjarov", "Dragi Kocev", "Dejan Gjorgjevikj", "Sa\u0161o D\u017eeroski"], "venue": "Pattern Recognition,", "citeRegEx": "Madjarov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madjarov et al\\.", "year": 2012}, {"title": "Convex piecewise-linear fitting", "author": ["Alessandro Magnani", "Stephen P Boyd"], "venue": "Optimization and Engineering,", "citeRegEx": "Magnani and Boyd.,? \\Q2009\\E", "shortCiteRegEx": "Magnani and Boyd.", "year": 2009}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "A guide to NumPy, volume 1", "author": ["Travis E Oliphant"], "venue": "Trelgol Publishing USA,", "citeRegEx": "Oliphant.,? \\Q2006\\E", "shortCiteRegEx": "Oliphant.", "year": 2006}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Conditional neural fields", "author": ["Jian Peng", "Liefeng Bo", "Jinbo Xu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Peng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["Boris T Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak.,? \\Q1964\\E", "shortCiteRegEx": "Polyak.", "year": 1964}, {"title": "Sum-product networks: A new deep architecture", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "UAI", "citeRegEx": "Poon and Domingos.,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2011}, {"title": "Approximate) subgradient methods for structured prediction", "author": ["Nathan D Ratliff", "J Andrew Bagnell", "Martin Zinkevich"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ratliff et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["Ferdinando S Samaria", "Andy C Harter"], "venue": "In Applications of Computer Vision,", "citeRegEx": "Samaria and Harter.,? \\Q1994\\E", "shortCiteRegEx": "Samaria and Harter.", "year": 1994}, {"title": "Reverse tdnn: an architecture for trajectory generation", "author": ["Patrice Simard", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Simard and LeCun.,? \\Q1991\\E", "shortCiteRegEx": "Simard and LeCun.", "year": 1991}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Bundle methods for machine learning", "author": ["Alex J. Smola", "S.v.n. Vishwanathan", "Quoc V. Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Smola et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2008}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Mulan: A java library for multi-label learning", "author": ["Grigorios Tsoumakas", "Eleftherios Spyromitros-Xioufis", "Jozef Vilcek", "Ioannis Vlahavas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsoumakas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2011}, {"title": "Python reference manual", "author": ["Guido Van Rossum", "Fred L Drake Jr."], "venue": "Centrum voor Wiskunde en Informatica Amsterdam,", "citeRegEx": "Rossum and Jr.,? \\Q1995\\E", "shortCiteRegEx": "Rossum and Jr.", "year": 1995}, {"title": "Primal-dual interior-point methods", "author": ["Stephen J Wright"], "venue": null, "citeRegEx": "Wright.,? \\Q1997\\E", "shortCiteRegEx": "Wright.", "year": 1997}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip HS Torr"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Given (typically high-dimensional) structured input and output spaces X \u00d7 Y, we can build a network over (x, y) pairs that encodes the energy function for this pair, following typical energy-based learning formalisms [LeCun et al., 2006].", "startOffset": 217, "endOffset": 237}, {"referenceID": 2, "context": "This is similar in nature to the neural network structured prediction methodology [Belanger and McCallum, 2015], with the difference being that in our setting f is both convex in y, so the optimization can be performed globally, and has a deep network structure over y itself.", "startOffset": 82, "endOffset": 111}, {"referenceID": 3, "context": "Simard and LeCun [1991]), and there was an early appreciation that structured models like hidden Markov models could be combined with the outputs of neural networks [Bengio et al., 1994].", "startOffset": 165, "endOffset": 186}, {"referenceID": 26, "context": "Simard and LeCun [1991]), and there was an early appreciation that structured models like hidden Markov models could be combined with the outputs of neural networks [Bengio et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Simard and LeCun [1991]), and there was an early appreciation that structured models like hidden Markov models could be combined with the outputs of neural networks [Bengio et al., 1994]. Much of this earlier work is surveyed and synthesized by LeCun et al. [2006], who give a tutorial on these energy based learning methods.", "startOffset": 166, "endOffset": 265}, {"referenceID": 28, "context": "Finally, although it is more abstract, we feel there is a philosophical similarity between our proposed approach and sum-product networks [Poon and Domingos, 2011]; both settings define networks where inference is accomplished \u201ceasily\u201d either by a sum-product message passing algorithm (by construction) or via convex optimization.", "startOffset": 138, "endOffset": 163}, {"referenceID": 21, "context": "The most similar work of which we are aware specifically fit sums of rectified half-planes to data [Magnani and Boyd, 2009], which is similar to one layer of our rectified linear units.", "startOffset": 99, "endOffset": 123}, {"referenceID": 11, "context": "Some passthrough layers have been recently explored in the deep residual networks [He et al., 2015] and densely connected convolutional networks [Huang et al.", "startOffset": 82, "endOffset": 99}, {"referenceID": 12, "context": ", 2015] and densely connected convolutional networks [Huang et al., 2016], though these differ from those of an ICNN as they pass through hidden layers deeper in the network, whereas to maintain convexity our passthrough layers can only apply to the input directly.", "startOffset": 53, "endOffset": 73}, {"referenceID": 17, "context": "Indeed, modern feedforward architectures such as AlexNet [Krizhevsky et al., 2012], VGG [Simonyan and Zisserman, 2014], and GoogLeNet [Szegedy et al.", "startOffset": 57, "endOffset": 82}, {"referenceID": 33, "context": ", 2012], VGG [Simonyan and Zisserman, 2014], and GoogLeNet [Szegedy et al.", "startOffset": 13, "endOffset": 43}, {"referenceID": 35, "context": ", 2012], VGG [Simonyan and Zisserman, 2014], and GoogLeNet [Szegedy et al., 2015] with ReLUs [Nair and Hinton, 2010] can be made input convex with Proposition 1.", "startOffset": 59, "endOffset": 81}, {"referenceID": 23, "context": ", 2015] with ReLUs [Nair and Hinton, 2010] can be made input convex with Proposition 1.", "startOffset": 19, "endOffset": 42}, {"referenceID": 18, "context": "LeCun et al. [2006] for more details).", "startOffset": 0, "endOffset": 20}, {"referenceID": 41, "context": "Furthermore, most LP solution methods to solve such problems require that we form and invert structured matrices with blocks such as W T i Wi \u2014 the case for most interior-point methods [Wright, 1997] or even approximate algorithms such as the alternating direction method of multipliers [Boyd et al.", "startOffset": 185, "endOffset": 199}, {"referenceID": 7, "context": "The method is also more challenging to integrate with some learning procedures, as we often need to differentiate through an entire chain of the gradient descent algorithm Domke [2012]. Thus, while the method can sometimes work in practice, we have found that other approaches typically far outperform this method, and we will focus on alternative approximate approaches for the remainder of this section.", "startOffset": 172, "endOffset": 185}, {"referenceID": 34, "context": "Bundle method One optimization approach that seems particular promising in this setting is the bundle method [Smola et al., 2008].", "startOffset": 109, "endOffset": 129}, {"referenceID": 29, "context": "The optimization problem (2) is naturally still not convex in \u03b8, but can be solved via the subgradient method for structured prediction [Ratliff et al., 2007].", "startOffset": 136, "endOffset": 158}, {"referenceID": 8, "context": "This method can be easily adapted to use mini-batches instead of a single example per subgradient step, and also adapted to alternative optimization methods like AdaGrad [Duchi et al., 2011] or ADAM [Kingma and Ba, 2014].", "startOffset": 170, "endOffset": 190}, {"referenceID": 15, "context": ", 2011] or ADAM [Kingma and Ba, 2014].", "startOffset": 16, "endOffset": 37}, {"referenceID": 14, "context": "This section presents our results on multi-label classification on the bibtex dataset [Katakis et al., 2008], image completion using the Olivetti face dataset [Samaria and Harter, 1994], and continuous action reinforcement learning in the OpenAI Gym [Brockman et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 31, "context": ", 2008], image completion using the Olivetti face dataset [Samaria and Harter, 1994], and continuous action reinforcement learning in the OpenAI Gym [Brockman et al.", "startOffset": 58, "endOffset": 84}, {"referenceID": 24, "context": "Our Python [Van Rossum and Drake Jr, 1995] implementation uses numpy [Oliphant, 2006] for linear algebra primitives and TensorFlow [Abadi et al.", "startOffset": 69, "endOffset": 85}, {"referenceID": 0, "context": "Our Python [Van Rossum and Drake Jr, 1995] implementation uses numpy [Oliphant, 2006] for linear algebra primitives and TensorFlow [Abadi et al., 2016] for neural networks and auto-differentiation.", "startOffset": 131, "endOffset": 151}, {"referenceID": 25, "context": "1 Synthetic classification examples We begin with a simple example to illustrate the classification performance of a two-hidden-layer FICNN and PICNN on two-dimensional binary binary classification tasks from the scikit-learn toolkit [Pedregosa et al., 2011].", "startOffset": 234, "endOffset": 258}, {"referenceID": 39, "context": "We use the ARFF version of this dataset from Mulan [Tsoumakas et al., 2011].", "startOffset": 51, "endOffset": 75}, {"referenceID": 13, "context": "Our PICNN architecture for multi-label classification uses fully-connected layers with ReLU activation functions and batch normalization [Ioffe and Szegedy, 2015] along with input path.", "startOffset": 137, "endOffset": 162}, {"referenceID": 2, "context": "SPENs [Belanger and McCallum, 2015] obtain a macro-F1 score of 42.", "startOffset": 6, "endOffset": 35}, {"referenceID": 12, "context": "2 Multi-Label Classification We next study how ICNNs perform on multi-label classification with the bibtex dataset and benchmark presented in Katakis et al. [2008]. This benchmark maps text classification from an input space X of 1836 bag-of-works indicator (binary) features to an output space Y of 159 binary labels.", "startOffset": 142, "endOffset": 164}, {"referenceID": 12, "context": "2 Multi-Label Classification We next study how ICNNs perform on multi-label classification with the bibtex dataset and benchmark presented in Katakis et al. [2008]. This benchmark maps text classification from an input space X of 1836 bag-of-works indicator (binary) features to an output space Y of 159 binary labels. We use the train/test split of 4880/2515 from Katakis et al. [2008] and evaluate with the exampleaveraged (macro) F1 score.", "startOffset": 142, "endOffset": 387}, {"referenceID": 12, "context": "Our PICNN architecture for multi-label classification uses fully-connected layers with ReLU activation functions and batch normalization [Ioffe and Szegedy, 2015] along with input path. As a baseline, we use a fully-connected neural network with batch normalization and ReLU activation functions. Figure 4 compares the training of a feedforward baseline to a PICNN with the same structure (600 fully connected, 159 (#labels) fully connected). While the PICNN does not improve the F1 score, we emphasize that the performance is similar to the feedforward network. Our baseline feedforward network\u2019s best macro F1 score of 45.5 outperforms the best macro-F1 score of 43.4 from a survey presented in Madjarov et al. [2012]. SPENs [Belanger and McCallum, 2015] obtain a macro-F1 score of 42.", "startOffset": 138, "endOffset": 720}, {"referenceID": 25, "context": "3 Image completion on the Olivetti faces As a test of the system on a structured prediction task over a much more complex output space Y, we apply a convolutional DQN-style PICNN to face completion on the sklearn version [Pedregosa et al., 2011] of the Olivetti data set [Samaria and Harter, 1994], which contains 400 64x64 grayscale", "startOffset": 221, "endOffset": 245}, {"referenceID": 31, "context": ", 2011] of the Olivetti data set [Samaria and Harter, 1994], which contains 400 64x64 grayscale", "startOffset": 33, "endOffset": 59}, {"referenceID": 22, "context": "images Our PICNN is equivalent to the DQN architecture in Mnih et al. [2015] (except we have added batch normalization to the input path) and is over (x, y) pairs where x (32x64) is the left half and y (32x64) is the right half of the image.", "startOffset": 58, "endOffset": 77}, {"referenceID": 22, "context": "images Our PICNN is equivalent to the DQN architecture in Mnih et al. [2015] (except we have added batch normalization to the input path) and is over (x, y) pairs where x (32x64) is the left half and y (32x64) is the right half of the image. The input and output paths are: 32x8x8 conv (stride 4x2), 64x4x4 conv (stride 2x2), 64x3x3 conv, 512 fully connected. Any modern architecture can be used in place of a DQN and we have not well-explored the space of possible architectures. Our weak motivation for choosing a DQN architecture comes from its use of strided convolutions over pooling. Our intuition is that networks for image completion do not need (and can be hurt by) the translation-invariance provided by pooling layers present in other modern architectures. This experiment uses the same training/test splits and minimizes the mean squared error (MSE) as in Poon and Domingos [2011] so that our results can be directly compared to (a non-exhaustive list of) other techniques.", "startOffset": 58, "endOffset": 893}, {"referenceID": 28, "context": "Our network improves upon performance of sum-product networks, which obtain an MSE of 942 [Poon and Domingos, 2011].", "startOffset": 90, "endOffset": 115}, {"referenceID": 37, "context": ", 2016] that use the MuJoCo physics simulator [Todorov et al., 2012].", "startOffset": 46, "endOffset": 68}, {"referenceID": 19, "context": "We use Deep Deterministic Policy Gradient (DDPG) [Lillicrap et al., 2015] and Normalized Advantage Functions (NAF) [Gu et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 10, "context": ", 2015] and Normalized Advantage Functions (NAF) [Gu et al., 2016] as state-of-the-art baselines.", "startOffset": 49, "endOffset": 66}], "year": 2016, "abstractText": "This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with only minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.", "creator": "LaTeX with hyperref package"}}}