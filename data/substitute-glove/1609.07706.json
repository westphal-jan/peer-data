{"id": "1609.07706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2016", "title": "Learning by Stimulation Avoidance: A Principle to Control Spiking Neural Networks Dynamics", "abstract": "Learning inc. take operate whose find neurons, own by extension homogenous brought models of neural phone, has possibility to what leadership learning allow opening to cited typically. In particular paper, wo argue giving the believed of a limitation allowing they fail be equilibrium of yet biologically revival neural news. Using carefully quicker current interpersonal, the systems get way quickly rather a certain dynamical state. We rule exactly concerning \" Learning appeared Stimulation Avoidance \" (LSA ). We reluctance they simulation that form typically ample conditions since to LSA where artificial optic their addition ensure to evolve abilities results similar put those upon in biological neurons by Shahaf and Marom [no. ]. We decide the mechanism ' same appropriate dynamics at a reduced programs, and demonstrate looking to thicknesses from intended a main of 100 olfactory. We show saying LSA and good high descriptive has larger existing concepts coming the response of discovery threading messaging to data multimedia, over enough otherwise several as a life decades for particular symbolized application: learning more soaring avoidance by place simulated spacecraft. The surge in popularity large artificial neural communication is mostly collaboration although disembodied models of dopaminergic with unquestionably ambiguous measurement: allow another authors ' knowledge, change both entered first made prowess correlates - models own with pages oversupply tv apart blended Hebbian philosophy.", "histories": [["v1", "Sun, 25 Sep 2016 06:44:42 GMT  (653kb,D)", "http://arxiv.org/abs/1609.07706v1", "17 pages, 11 figures"]], "COMMENTS": "17 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["lana sinapayen", "atsushi masumori", "takashi ikegami"], "accepted": false, "id": "1609.07706"}, "pdf": {"name": "1609.07706.pdf", "metadata": {"source": "CRF", "title": "Learning by Stimulation Avoidance: A Principle to Control Spiking Neural Networks Dynamics", "authors": ["Lana Sinapayen", "Atsushi Masumori", "Takashi Ikegami"], "emails": ["lana@sacral.c.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "Learning based on networks of real neurons, and by extension biologically inspired models of neural networks, has yet to find general learning rules leading to widespread applications. In this paper, we argue for the existence of a principle allowing to steer the dynamics of a biologically inspired neural network. Using carefully timed external stimulation, the network can be driven towards a desired dynamical state. We term this principle \u201cLearning by Stimulation Avoidance\u201d (LSA). We demonstrate through simulation that the minimal sufficient conditions leading to LSA in artificial networks are also sufficient to reproduce learning results similar to those obtained in biological neurons by Shahaf and Marom [1]. We examine the mechanism\u2019s basic dynamics in a reduced network, and demonstrate how it scales up to a network of 100 neurons. We show that LSA has a higher explanatory power than existing hypotheses about the response of biological neural networks to external simulation, and can be used as a learning rule for an embodied application: learning of wall avoidance by a simulated robot. The surge in popularity of artificial neural networks is mostly directed to disembodied models of neurons with biologically irrelevant dynamics: to the authors\u2019 knowledge, this is the first work demonstrating sensory-motor learning with random spiking networks through pure Hebbian learning."}, {"heading": "Author Summary", "text": "Networks of spiking neurons are currently the model that most closely reproduces real neuron\u2019s dynamics [2]. Theoretical approaches have shown that spiking models have a tremendous potential for learning and processing information [3], but efforts are still ongoing to use this potential in mainstream applications. By finding general learning rules governing the dynamics of spiking networks, we get closer to this goal. In this paper, we propose such a biologically plausible rule (LSA), explain how it influences the dynamics of the network, demonstrate how to use it in practice to obtain desired dynamics from initially random networks and finally, show that it can be used to obtained desired behaviour in a simulated robot. Far from being constrained to simulated models, LSA also offers an interpretation to so far unexplained experimental results obtained with in vitro networks."}, {"heading": "Introduction", "text": "In two papers published in 2001 and 2002, Shahaf and Marom conduct experiments with a training method that drives rats\u2019 cortical neurons cultivated in vitro to learn given tasks [1, 4]. They show that stimulating the network with a focal current and removing that stimulation when a desired behaviour is executed is sufficient to strengthen said behaviour. By the end of training, the behaviour is obtained\nar X\niv :1\n60 9.\n07 70\n6v 1\n[ cs\n.N E\n] 2\n5 Se\np 20\nreliably and quickly in response to the stimulation. More specifically, networks learn to increase the firing rate of a group of neurons (output neurons) inside a time window of 50 ms, in response to an external electric stimulation applied to an other part of the network (input neurons). This result is powerful, first due to its generality: the network is initially random, the input and output zones\u2019 size and position are chosen by the experimenter, as well as the output\u2019s time window and the desired output pattern. A second attractive feature of the experiment is the simplicity of the training method. To obtain learning in the network, Shahaf and Marom repeat the following two steps: (1) Apply a focal electrical stimulation to the network. (2) When the desired behavior appears, remove the stimulation.\nAt first the desired output seldom appears in the required time window, but after several training cycles (repeating steps (1) and (2)), the output is reliably obtained. Marom explains these results by invoking the Stimulus Regulation Principle (SRP, from [5, 6]). At the low level of a neural network, the SRP postulates that stimulation drives the network to \u201ctry out\u201d different topologies by modifying neuronal connections (\u201cmodifiability\u201d), and that removing the stimulus simply freezes the network in its last configuration (\u201cstability\u201d). The SRP explicitly postulates that no strengthening of neural connections occurs as a result of stimulus removal.\nThe generality of the results obtained by Shahaf and Marom suggests that this form of learning must be a crucial and very basic property of biological neural networks. But the SRP does not entirely explain the experimental results. Why are several training cycles necessary if \u201cstability\u201d guarantees that the configuration of the network is preserved after stopping the stimulation? How does \u201cmodifiability\u201d not conflict with the idea of learning, if we cannot prevent the \u201cgood\u201d topology to be modified by the stimulation at each new training cycle?\nWe propose a different explanatory mechanism: the principle of Learning By Stimulation Avoidance (LSA, [7, 8]). LSA is an emergent property of spiking networks coupled to Hebbian rule [9] and external stimulation. LSA states that these networks will reliably learn to exhibit firing patterns that lead to the removal of an external stimulation (positive LSA), and avoid exhibiting firing patterns that lead to the application of an external stimulation (negative LSA).\nIn opposition to the SRP, LSA does not postulate that stimulus intensity is the major drive for changes in the network, but rather that the timing of the stimulation relative to network activity is crucial. LSA relies entirely on time dependent strengthening and weakening of neural connections. In addition, LSA proposes an explanatory mechanism for synaptic pruning, which is not covered by the SRP.\nShahaf postulates that the SRP might not be at work in \u201creal brains\u201d. Indeed, while SRP has not yet been found to take place in the brain, the fundamental rule from which LSA emerges, Spike-Timing Dependant Plasticity (STDP), has been found in both in vivo and in vitro networks. STDP is a Hebbian learning rule so fundamental that it has been consistently found in the brains of a wide range of species, from insects to humans [10\u201312]. STDP causes changes in the synaptic weight between two firing neurons depending on the timing of their activity: if the presynaptic neuron fires within 20 ms before the postsynaptic neuron, the synaptic weight increases; if the presynaptic neuron fires within 20 ms aftter the postsynaptic neuron , the synaptic weight decreases.\nAlthough STDP occurs at neuronal level, it has very direct consequences on the sensory-motor coupling of animals with the environment. In vitro and in vivo experiments based on STDP can reliably enhance sensory coupling [13], decrease it [14], and these bidirectional changes can even be combined to create receptive fields in sensory neurons [15,16].\nIn all these studies the reaction of cortical neurons to stimuli is gradually enhanced when they are made to fire within 20 ms after the end of the stimulation, and inhibited when they are made to fire within 20 ms after the start of the stimulation. In this paper, we show that these are the only conditions required for the emergence of both positive LSA and negative LSA between pre- and post-synaptic neurons, as illustrated in Fig. 1. Therefore, although STDP is a rule that operates at the scale of one neuron, LSA can be expected to emerge at network level in real brains as well as it emerges in artificial networks. LSA at a network level requires an additional condition that is burst suppression. In this paper, we have tested two mechanisms. One is simply adding white noise to all neurons and the other one is to use a short term plasticity rule (STP)."}, {"heading": "Output", "text": "The structure of the paper is as follows: the model is presented in Section 1; we show that the conditions necessary to obtain LSA are sufficient reproduce biological results in Section 2.1 and study the dynamics of LSA in a minimal network of 3 neurons in Section 2.2. We then show that LSA supports hypotheses to explain biological mechanisms that are not covered by the SRP (Section 2.3) and implement a simple embodied application using LSA as the sole learning mechanism in Section 2.4. In Section 2.5 we explore the effect of parameter changes on the learning performance of the network."}, {"heading": "1 Model", "text": ""}, {"heading": "Network model", "text": "We use the model of spiking neuron devised by Izhikevich [17] to simulate excitatory neurons (regular spiking neurons) and inhibitory neurons (fast spiking neurons) with a simulation time step of 1 ms. The equations of the neural model and the resulting dynamics are shown in Fig. 2. In the experiments of Section 2.1 and Section 2.4, we simulate fully connected networks of 100 neurons (self-connections are forbidden) with 80 excitatory and 20 inhibitory neurons. This ratio of 20% of inhibitory neurons is standard in simulations [2, 17] and close to real biological values (15%, [18]). The initial weights are random (uniform distribution: 0 < w < 5 for excitatory neurons, \u22125 < w < 0 for inhibitory neurons). The maximum weight is fixed to 10. There are no delays in signal transmission between neurons. The neurons receive three kinds of input: (1) Gaussian noise m with a standard deviation \u03c3 = 3 mV, representing noisy thalamic input. (2) External stimulation e with a value of 0 or 1 mV and a frequency of 1000 Hz. The timing of the stimulation depends on the experiment. (3) Stimulation from other neurons: when a neuron a spikes, the weight wa,b is added as an input for neuron b. All these inputs are added for each neuron ni at each iteration as:\nIi = I \u2217 i + ei +mi . (1)\nI\u2217i = n\u2211 j=0 wj,i \u00d7 fj , fj =\n{ 1, if nj is firing\n0, otherwise. (2)\nIn the experiments with a reduced network (Section 2.2), we simulate only 3 neurons, all excitatory. The initial weights are fixed to 5, except if noted otherwise. In the experiments of Section 2.3, burst suppression is obtained in the 100-neuron network by reducing the number of connections to obtain a sparsely connected network: each neuron has 20 random connections to other neurons (uniform distribution, 0 < w < 10), a high maximum weight of 50, high external input e = 10 mV and high noise \u03c3 = 5 mV1."}, {"heading": "Plasticity model", "text": "We add synaptic plasticity to all networks in the form of STDP as proposed in [19]. STDP is applied only between excitatory neurons; other connections keep their initial weight during all the simulation. We use additive STDP: Fig. 3 shows the variation of weight \u2206w for a synapse going from neuron a to neuron b. As shown on the figure, \u2206w is negative if b fires first, and positive a fires first. The total weight w varies as:\nwt = wt\u22121 + \u2206w . (3)\nWe fix a maximum value to the weight: if w > wmax, w is reset to wmax. In the experiments with 100-neurons networks, we also apply a decay function to all the weights in the network. The decay function is applied at each iteration t as:\n1Variations in the number of connections and the strength variance are examined in Section 2.5\n\\Delta w = A (1- \\frac{1}{\\tau})^s \\Delta w = -A (1- \\frac{1}{\\tau})^{-s}\n\u2200wt, wt+1 = (1\u2212 \u00b5)wt (4)\n\u00b5 = 5\u00d7 10\u22127 . (5)\nIn the experiments of Section 2.4, burst suppression is obtained by adding a phenomenological model of Short Term Plasticity (STP, [20]) to the network, as a way to suppress bursts while keeping the network fully connected. STP is a reversible plasticity rule that decreases the intensity of neuronal spikes if they are too close in time, preventing the network to enter a state of global synchronized activity:\nw\u2217i,j = uxwi,j (6)\ndx dt = 1\u2212 x \u03c4d \u2212 uxfi (7)\ndu dt = U \u2212 u \u03c4f + U(1\u2212 u)fi (8)\nSTP acts as a short term reversible factor on the original synaptic weight, with the side effect of preventing global bursting of the network. Eq. 2 becomes\nI\u2217i = n\u2211 j=0 w\u2217j,i \u00d7 fj . (9)"}, {"heading": "Robot", "text": "In Section 2.4, we simulate a simple robot moving inside an arena surrounded by walls. The arena is a square of size 1000 px (pixels). The robot is a 25 px radius circle, constantly moving at 1 px/ms except in case of collision with a wall. The robot has two distance sensors oriented respectively at \u03c0/4 and \u2212\u03c0/4 from the front direction of the robot. The sensors have a range of 80 px; they are activated when the robot is at less than 80 px from a wall, on the direction supported by each sensor\u2019s orientation. Two input zones in the network (10 neurons each) receive input in mV from the sensors as follows:\noutput = sensitivity/distance (10)\nThe sensitivity of the sensors is fixed at a constant value for the duration of each experiment. For simplicity, the robot\u2019s steering is non-differential. It is controlled by the spikes of two output zones in the network (10 neurons each). For each spike in the left output zone, the robots steers \u03c0/6 radian (to the left); for each spike in the right output zone, the robots steers \u2212\u03c0/6 radian (to the right)."}, {"heading": "2 Results", "text": ""}, {"heading": "2.1 LSA is Sufficient to Explain Biological Results", "text": "In [7] we showed that a simulated random spiking network built from [2, 21] combined to STDP could be driven to learn desired output patterns using a training method similar to that of Shahaf et al. Shahaf shows that his training protocol can reduce the response time of a network. The response time is defined as the delay between the the application of the stimulation and the observation of a desired output from the network. In his first series of experiments (\u201csimple learning\u201d experiments), the desired output is defined by the fulfilment of one condition, Condition 1: the electrical activity must increase in a chosen Output Zone A. This is the experiment we reproduced in [7], demonstrating that this learning behaviour is a direct effect of STDP and is captured by the principle of LSA: firing patterns leading to the removal of external stimulation are strengthened, firing patterns that lead to the application of an external stimulation are avoided.\nIn this section we show that the same methods are sufficient to obtain results similar to the second series of experiments performed by Shahaf (\u201cselective learning\u201d experiments), in which the desired output is the simultaneous fulfilment of Condition 1 as defined before and Condition 2: a different output zone (Output Zone B) must not exhibit enhanced electrical activity. When both conditions are fulfilled, the result is called selective learning because only Output Zone A must learn to increase its activity inside the time window, while Output Zone B must not change its activity.\nWe reproduce the experiment as follows. In a network of 100 neurons, a group of 10 excitatory neurons are stimulated with 1 mV at a frequency of 1,000 Hz. Two different groups of 10 neurons are monitored (Output Zone A and Output Zone B). We define the desired output pattern as: 4 neurons or more firing in Output Zone A (Condition 1), and less than 4 neurons firing in Output Zone B (Condition 2). Both conditions must be fulfilled simultaneously, where simultaneously means at the same millisecond. We stop the external stimulation as soon as the desired output is observed. If the desired output is not observed after 10,000 ms of stimulation, the stimulation is also stopped. After a random delay of 1,000 to 2,000 ms, the stimulation starts again. In addition, each neuron is stimulated with noisy input and the weights are subject to slow decay as indicated in the Model section.\nThis paper provides a theoretical explanation to Shahaf\u2019s in vitro experiment by simulating spiking neurons. There are important differences: the stimulation frequency (Shahaf uses lower frequencies), its intensity (this parameter is unknown in Shahaf\u2019s experiment) and the time window for the output (in Shahaf\u2019s results the activity of Output Zone A is arguably higher even outside of the selected output window). We also use a fully connected network, while the biological network grown in vitro is likely to be sparsely connected [22].\nThe task is learned (learning time) when the reaction time of the network reaches a value inferior to 4,000 ms and keeps under this limit. The success rate is the percentage of networks that successfully learned the task in 400,000 ms or less (N = 20 networks per condition). The final reaction time is calculated for successful networks after learning. Standard error is indicated.\nDespite these differences, we obtain results comparable to those of Shahaf: the reaction time, initially random, becomes shorter with training (Fig. 4). We also perform the experiment with no stimulation at all and find a success rate of 0%; the statistics of the selective learning experiment are summarized in Table 1.\nAs shown by these results, the network exhibits selective learning as defined by Shahaf. But we also find that despite a success rate of 90% at exhibiting the desired firing pattern, both the firing rates of Output Zone A and Output Zone B increase in equivalent proportions: the two output zones fire at the same rate but in a desynchronized way (see also Fig. 7-b). Although data about firing rates is not specifically discussed in the paper, Shahaf himself reports in his experiment that only half of the in-vitro networks succeeded at selective learning, while all succeeded at the \u201csimple learning\u201d task. Our hypothesis is that bursts are detrimental to learning [23] and explain the difficulty of obtaining selective learning. If this hypothesis is true, burst suppression should improve learning. Before discussing burst suppression, in the next section we quickly survey the dynamics of LSA in a minimal network of 3 neurons, and how these dynamics can affect learning if there are global bursts. Then we discuss a 100-neurons network with global bursting suppression."}, {"heading": "2.2 Dynamics of LSA in a Minimal Network", "text": "In [7] we showed that a minimal network of 2 neurons consistently follows the principle of LSA; we also showed that a single neuron is able to prune one synapse and enhance another synapse simultaneously depending on the stimulation received by the two presynaptic neurons. In this experiment we examine\nthe weights dynamics in a chain of 3 excitatory neurons all connected to each other, as a simplification of what may be happening in a fully connected network: one neuron is used as input, one as output, and they are separated by a \u201chidden neuron\u201d.\nNeurons are labeled 0 (input neuron), 1 (hidden neuron) and 2 (output neuron). Fig 5 shows the results of experiments with different learning conditions and different initial states. The results can be summarised as follows: 1. In positive LSA, direct connections between input and output are privileged over indirect connections. All connections are updated with the same time step (1 ms), therefore the fastest path (direct connection) will always cause neuron 2 to fire before the longer path (made of several connections) can be completely activated. On the other hand, when no direct connection exists, weights on longer paths are correctly increased. 2. Negative LSA only prunes weights of direct connections between the input and output, as this is sufficient to stop all stimulation to the output neuron. 3. For neurons that are strongly stimulated (here, neuron 0) the default behaviour of individual weights is to increase, except if submitted to the influence of negative LSA. Neurons that fire constantly bias other neurons to fire after them, mechanically increasing their output weights.\nThis raises concerns about the stability of larger, fully connected networks; all weights could simply increase to the maximum value. But introducing inhibitory neurons in the network can improve network stability [24]. In our experiments with 100-neuron networks, 20 are inhibitory neurons with fixed input\nweights and output weights. In addition, we make the hypothesis that global bursts in the network can impair LSA, as all neurons fire together make it impossible to tease apart individual neuron\u2019s contributions to the postsynaptic neuron\u2019s excitation. Global bursts are also considered to be a pathologic behaviour for in vitro networks, and do not occur with healthy in vivo networks [23]."}, {"heading": "2.3 LSA has More Explanatory Power than the SRP", "text": "We have seen that our simple model can reproduce the results of Shahaf\u2019s experiments, and that LSA can potentially explain this behaviour. But LSA also explains a behaviour that is not discussed in the SRP: synapse pruning. LSA predicts that networks evolve as much as possible towards dynamical states that cause the less external stimulation.\nIn this experiment, we change the network\u2019s parameters to have a sparsely connected network with strong noise (see Methods). These networks are less prone to global bursts and exhibit strong desynchronized activity, as shown in Fig. 6.\nHere we monitor two output zones and fix two stimulation conditions: (1) Input Zone A is stimulated. When a neuron fires in Output Zone A, the external stimulation to Input Zone A is stopped. (2) Input Zone B is not stimulated. When a neuron fires in Output Zone B, the whole network (excluding inhibitory neurons and Output Zone B itself) is stimulated for 10 ms. The goal is to obtain true selective learning, by increasing the firing rate of Output Zone A compared to Output Zone B.\nDue to the maximum weight value, only a few (comparatively to the network size) spiking presynaptic neurons are necessary to make a postsynaptic neuron fire. In consequence, condition (2) must be able to prune as many input synapses to Output Zone B as possible. The importance of suppressing global bursts\nbecomes obvious: global bursts cause Output Zone B to fire at the same time as the whole network, making it impossible to update only relevant weights without also updating unrelated weights.\nAs a result of LSA, we expect that the network will move from a state where both output zones fire at the same rate, to a state where Output Zone A fires at high rates and Output Zone B fires at lower rates. This prediction is realized, as we can see in Fig. 7-a: the trajectory of firing rates goes to the space of low external stimulation. In Fig. 7-b, we show for comparison the trajectory for networks with only condition (1) applied: on average the firing rates of both output zones are equivalent, with individual networks trajectories ending up indiscriminately at the top left or bottom right of the space.\nThese results could potentially be reproduced in a network in vitro: the Izhikevich model of spiking network that we use has been found to exhibit the same dynamics as real neurons, and our experiments with STDP can reproduce some results of biological experiments; therefore there is a probability that this results predicted by LSA still holds in biological networks with suppressed bursts, especially since we have shown that LSA gives promising results on biological networks embodied in simple robots (see [8] and following section)."}, {"heading": "2.4 Embodied Application: Wall Avoidance with a Robot", "text": "We show that LSA can be used in a practical embodied application: wall avoidance learning. We simulate a simple robot moving inside a closed arena. The robot has two distance sensors on the front, allowing it to detect walls (Fig. 8). A 100-neuron network takes the two sensors\u2019 values as respective input for two input zones (right and left). Activity in two output zones of the network allows the robot to turn right or left. In these conditions, steering when encountering a wall can stop the stimulation received by the input zones from the distance sensors, if the new direction of the robot points away from the walls. The behaviour enhanced by LSA should therefore be wall avoidance. We call this experiment a \u201cclosed loop\u201d experiment, because steering away from the walls automatically stops the external stimulation at the right timing.\nIn this network, burst suppression is mediated via STP (see Methods). We compare the results of this experiment (closed loop experiment, sensor sensitivity = 8 mV) with a control experiment where a constant stimulation (8 mV) is applied to the network\u2019s input zones, independently of the distance or orientation of the robot relative to the walls (open loop experiment). Both conditions are tested 20 times and averaged. Fig. 9 shows two important effects: (1) Constant stimulation leads to higher activity in the network, provoking random steering of the robot which leads to some level of wall avoidance, but (2) Closed loop feedback is necessary to obtain actual wall avoidance learning. Indeed, by the end of the closed loop experiment the robot spends only 43% of its time at less than 80 pixels from any wall (the range of the distance sensors), against 64% in the open loop experiment. The importance of feedback over simply having high stimulation is further demonstrated by the fact that the open loop robot is receiving overall a greater amount of stimulation than the closed loop robot. At 400 s, when the learning curve of the closed loop robot starts sloping down, the robot has received on average 1.54 mV of stimulation per millisecond. The open loop robot, which by that time has reached its best performance, has received 16 mV/ms. In addition, the more the robot learns to avoid walls, the less stimulation it receives on average. Despite this we reach a final state where the robot spends most of its time far from the walls. In a different experiment, we give to open loop robots the same amount of average stimulation that is received by closed loop robots; by the end of the experiment (1000 s) the open loop robots still spend more than 80% of the time close to arena walls.\nWe further study the effect of stimulation strength and feedback on learning performance by varying the sensitivity of the distance sensors. The average state in the last 300 seconds of each task (total duration: 1000 s) is reported on Fig. 10. The open loop result of the previous experiment is included for reference. Fig. 10 indicates that the learnability of the task is improved by having more sensitive sensors, up to a limit of about 40%. Having sensors with a sensitivity of more than 7 mV does not improve the performance of the robot. This result is in direct contradiction with the SRP\u2019s leading hypothesis, which postulates that the intensity of stimulation is the driving force behind network modification. If that was the case, more sensitive sensors should always lead to better learnability. By contrast, LSA emphasises timing, not strength of the simulation. We hypothesise that the 40% limit is due at least in part to\nunreliable feedback, as steering away from a wall can put the robot directly in contact with another wall if it is stuck in a corner: the same action can lead to start or removal of stimulation depending on the context."}, {"heading": "2.5 Parameter Exploration", "text": "Finally, we perform a parameter search to explore the working conditions of the \u201csimple learning\u201d task. In the previous experiments, the network was fully connected and the initial connection weights followed a uniform distribution between 0 and 5 for the excitatory neurons and -5 and 0 for the inhibitory neurons. In this section, we vary the number of connections in the network and the variance v of the weights. For each neuron an output connection is chosen at random and the weight is initialised at w = 5 + \u03c9 (w = \u22125 + \u03c9), with \u03c9 following a uniform distribution between \u2212v and v. This process is repeated M times for each neuron, 0 < M < 150. The same connection can be chosen twice at random, so the actual number of connections can be inferior to M .\nFor each set (\u03c9,M) we perform N=20 experiments (\u201csimple learning\u201d experiment) of length T=500 seconds. The average difference between the firing rate of the Output Zone during the first 100 seconds and the last 100 seconds is reported on the heat map Fig. 11. This figure shows that the ideal region of the parameter space to obtain good learning results is between 20 and 30 connections per neuron. Below these values, we hypothesise that the lowest number of neurons connecting the input neurons to the output neurons becomes too big, weakening the correlation between input and output. Above these\nvalues, the increased connectivity of the network might cause too many bursts, affecting the learning results. By comparison, the variance in the initial weights has low influence on the final learning results."}, {"heading": "Discussion", "text": "In this paper, we introduce a new principle explaining the dynamics of spiking neural networks under the influence of external stimulation. The model presented in this paper is very simplified compared to biological neurons, as it uses only two types of neurons, one type of STDP, no homeostatic mechanism, etc. Nevertheless, the model is able to reproduce key features of experiments conducted biological neurons and and explain results obtained in vitro with neurons submitted to external stimulation. LSA also offers an explanation to a biological mechanism that is ignored by the theory of SRP, namely the pruning of synapses. LSA has direct practical applications: by engineering causal relationships between neural dynamics and external stimulations, we can induce learning and change the dynamics of the neurons from the outside.\nLSA relies on the mechanism of STDP, and we demonstrated that the conditions to obtain LSA are: (1) External stimulation above a minimal threshold; (2) Causal coupling between neural network\u2019s behaviour and environmental stimulation; (3) Burst suppression. We obtain burst suppression by increasing the input noise in the model or by using STP. We assume that in healthy biological neurons, the neuronal noise may be introduced by spontaneous neuronal activity. As we have shown, LSA does not support the theory of the Stimulus Regulation Principle. It could be closer to the Principle of Free Energy\nMinimisation introduced by Friston [25]. The Free Energy Principle states that networks strive to avoid surprising inputs by learning to predict external stimulation. An expected behaviour of networks obeying the Free Energy Principle, or obeying LSA is that they can fall into the dark room paradox, avoiding incoming input by cutting all sources of external simulation. A key difference between LSA and the Free Energy Principle is that our network does not predict incoming input. Most importantly, LSA automatically let stimuli from environment terminate at the right timing, so that a network can self-organize using environmental information."}, {"heading": "Acknowledgments", "text": "This work was supported by Grant-in-Aid for Scientific Research (Studies on Homeo-Dynamics with Cultivated Neural Circuits and Embodied Artificial Neural Net- works; 24300080)."}], "references": [{"title": "Learning in networks of cortical neurons", "author": ["G Shahaf", "S. Marom"], "venue": "The Journal of Neuroscience", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Which model to use for cortical spiking neurons", "author": ["Izhikevich EM"], "venue": "IEEE transactions on neural networks", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Networks of spiking neurons: the third generation of neural network models", "author": ["W. Maass"], "venue": "Neural networks", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Development, learning and memory in large random networks of cortical neurons: lessons beyond anatomy", "author": ["S Marom", "G. Shahaf"], "venue": "Quarterly reviews of biophysics", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Principles of behavior: an introduction to behavior theory", "author": ["Hull CL"], "venue": "Appleton-Century;", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1943}, {"title": "Psychological facts and psychological theory", "author": ["Guthrie ER"], "venue": "Psychological Bulletin", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1946}, {"title": "Learning by Stimulation Avoidance as a primary principle of spiking neural networks dynamics", "author": ["L Sinapayen", "A Masumori", "N Virgo", "T. Ikegami"], "venue": "European Conference on Artificial Life (ECAL", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Experiments on a robot behavior controlled by cultured neuronal cells", "author": ["A Masumori", "N Maruyama", "L Sinapayen", "T Mita", "U Frey", "D Bakkum", "Principle et al. Emergence of sense-making behavior by the Stimulus Avoidance"], "venue": "13th European Conference on Artificial Life", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "The organization of behavior", "author": ["Hebb DO"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1949}, {"title": "Spike timing-dependent plasticity: a Hebbian learning rule", "author": ["N Caporale", "Y. Dan"], "venue": "Annu Rev Neurosci", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Spike timing-dependent plasticity: from synapse to perception", "author": ["Dan Y", "Poo MM"], "venue": "Physiological reviews", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Competitive Hebbian learning through spike-timing-dependent synaptic plasticity", "author": ["S Song", "KD Miller", "LF. Abbott"], "venue": "Nature neuroscience", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Hebbian STDP in mushroom bodies facilitates the synchronous flow of olfactory information in locusts", "author": ["S Cassenaer", "G. Laurent"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Spike timing-dependent synaptic depression in the in vivo barrel cortex of the rat", "author": ["V Jacob", "DJ Brasier", "I Erchova", "D Feldman", "DE. Shulz"], "venue": "The Journal of Neuroscience", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Temporal specificity in the cortical plasticity of visual space representation", "author": ["YX Fu", "K Djupsund", "H Gao", "B Hayden", "K Shen", "Y. Dan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Pairing-induced changes of orientation maps in cat visual cortex", "author": ["S Schuett", "T Bonhoeffer", "M. H\u00fcbener"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Simple model of spiking neurons", "author": ["EM Izhikevich"], "venue": "IEEE Transactions on neural networks", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "GABA immunoreactive neurons in rat visual cortex", "author": ["DL Meinecke", "A. Peters"], "venue": "Journal of Comparative Neurology", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1987}, {"title": "Reconciling the STDP and BCM models of synaptic plasticity in a spiking recurrent neural network", "author": ["D Bush", "A Philippides", "P Husbands", "M. O\u2019Shea"], "venue": "Neural computation", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Simulation of networks of spiking neurons: a review of tools and strategies", "author": ["R Brette", "M Rudolph", "T Carnevale", "M Hines", "D Beeman", "JM Bower"], "venue": "Journal of computational neuroscience", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Morphological characterization of in vitro neuronal networks", "author": ["O Shefi", "I Golding", "R Segev", "E Ben-Jacob", "A. Ayali"], "venue": "Physical Review E", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Controlling bursting in cortical cultures with closed-loop multi-electrode stimulation", "author": ["DA Wagenaar", "R Madhavan", "J Pine", "SM. Potter"], "venue": "The Journal of neuroscience", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons", "author": ["N. Brunel"], "venue": "Journal of computational neuroscience", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "The free-energy principle: a unified brain theory", "author": ["K. Friston"], "venue": "Nature Reviews Neuroscience", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "We demonstrate through simulation that the minimal sufficient conditions leading to LSA in artificial networks are also sufficient to reproduce learning results similar to those obtained in biological neurons by Shahaf and Marom [1].", "startOffset": 229, "endOffset": 232}, {"referenceID": 1, "context": "Networks of spiking neurons are currently the model that most closely reproduces real neuron\u2019s dynamics [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "Theoretical approaches have shown that spiking models have a tremendous potential for learning and processing information [3], but efforts are still ongoing to use this potential in mainstream applications.", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "In two papers published in 2001 and 2002, Shahaf and Marom conduct experiments with a training method that drives rats\u2019 cortical neurons cultivated in vitro to learn given tasks [1, 4].", "startOffset": 178, "endOffset": 184}, {"referenceID": 3, "context": "In two papers published in 2001 and 2002, Shahaf and Marom conduct experiments with a training method that drives rats\u2019 cortical neurons cultivated in vitro to learn given tasks [1, 4].", "startOffset": 178, "endOffset": 184}, {"referenceID": 4, "context": "Marom explains these results by invoking the Stimulus Regulation Principle (SRP, from [5, 6]).", "startOffset": 86, "endOffset": 92}, {"referenceID": 5, "context": "Marom explains these results by invoking the Stimulus Regulation Principle (SRP, from [5, 6]).", "startOffset": 86, "endOffset": 92}, {"referenceID": 6, "context": "Why are several training cycles necessary if \u201cstability\u201d guarantees that the configuration of the network is preserved after stopping the stimulation? How does \u201cmodifiability\u201d not conflict with the idea of learning, if we cannot prevent the \u201cgood\u201d topology to be modified by the stimulation at each new training cycle? We propose a different explanatory mechanism: the principle of Learning By Stimulation Avoidance (LSA, [7, 8]).", "startOffset": 422, "endOffset": 428}, {"referenceID": 7, "context": "Why are several training cycles necessary if \u201cstability\u201d guarantees that the configuration of the network is preserved after stopping the stimulation? How does \u201cmodifiability\u201d not conflict with the idea of learning, if we cannot prevent the \u201cgood\u201d topology to be modified by the stimulation at each new training cycle? We propose a different explanatory mechanism: the principle of Learning By Stimulation Avoidance (LSA, [7, 8]).", "startOffset": 422, "endOffset": 428}, {"referenceID": 8, "context": "LSA is an emergent property of spiking networks coupled to Hebbian rule [9] and external stimulation.", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "STDP is a Hebbian learning rule so fundamental that it has been consistently found in the brains of a wide range of species, from insects to humans [10\u201312].", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "STDP is a Hebbian learning rule so fundamental that it has been consistently found in the brains of a wide range of species, from insects to humans [10\u201312].", "startOffset": 148, "endOffset": 155}, {"referenceID": 11, "context": "STDP is a Hebbian learning rule so fundamental that it has been consistently found in the brains of a wide range of species, from insects to humans [10\u201312].", "startOffset": 148, "endOffset": 155}, {"referenceID": 12, "context": "In vitro and in vivo experiments based on STDP can reliably enhance sensory coupling [13], decrease it [14], and these bidirectional changes can even be combined to create receptive fields in sensory neurons [15,16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "In vitro and in vivo experiments based on STDP can reliably enhance sensory coupling [13], decrease it [14], and these bidirectional changes can even be combined to create receptive fields in sensory neurons [15,16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "In vitro and in vivo experiments based on STDP can reliably enhance sensory coupling [13], decrease it [14], and these bidirectional changes can even be combined to create receptive fields in sensory neurons [15,16].", "startOffset": 208, "endOffset": 215}, {"referenceID": 15, "context": "In vitro and in vivo experiments based on STDP can reliably enhance sensory coupling [13], decrease it [14], and these bidirectional changes can even be combined to create receptive fields in sensory neurons [15,16].", "startOffset": 208, "endOffset": 215}, {"referenceID": 16, "context": "We use the model of spiking neuron devised by Izhikevich [17] to simulate excitatory neurons (regular spiking neurons) and inhibitory neurons (fast spiking neurons) with a simulation time step of 1 ms.", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "This ratio of 20% of inhibitory neurons is standard in simulations [2, 17] and close to real biological values (15%, [18]).", "startOffset": 67, "endOffset": 74}, {"referenceID": 16, "context": "This ratio of 20% of inhibitory neurons is standard in simulations [2, 17] and close to real biological values (15%, [18]).", "startOffset": 67, "endOffset": 74}, {"referenceID": 17, "context": "This ratio of 20% of inhibitory neurons is standard in simulations [2, 17] and close to real biological values (15%, [18]).", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "We add synaptic plasticity to all networks in the form of STDP as proposed in [19].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "1 LSA is Sufficient to Explain Biological Results In [7] we showed that a simulated random spiking network built from [2, 21] combined to STDP could be driven to learn desired output patterns using a training method similar to that of Shahaf et al.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "1 LSA is Sufficient to Explain Biological Results In [7] we showed that a simulated random spiking network built from [2, 21] combined to STDP could be driven to learn desired output patterns using a training method similar to that of Shahaf et al.", "startOffset": 118, "endOffset": 125}, {"referenceID": 19, "context": "1 LSA is Sufficient to Explain Biological Results In [7] we showed that a simulated random spiking network built from [2, 21] combined to STDP could be driven to learn desired output patterns using a training method similar to that of Shahaf et al.", "startOffset": 118, "endOffset": 125}, {"referenceID": 6, "context": "This is the experiment we reproduced in [7], demonstrating that this learning behaviour is a direct effect of STDP and is captured by the principle of LSA: firing patterns leading to the removal of external stimulation are strengthened, firing patterns that lead to the application of an external stimulation are avoided.", "startOffset": 40, "endOffset": 43}, {"referenceID": 20, "context": "We also use a fully connected network, while the biological network grown in vitro is likely to be sparsely connected [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "Our hypothesis is that bursts are detrimental to learning [23] and explain the difficulty of obtaining selective learning.", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "2 Dynamics of LSA in a Minimal Network In [7] we showed that a minimal network of 2 neurons consistently follows the principle of LSA; we also showed that a single neuron is able to prune one synapse and enhance another synapse simultaneously depending on the stimulation received by the two presynaptic neurons.", "startOffset": 42, "endOffset": 45}, {"referenceID": 22, "context": "But introducing inhibitory neurons in the network can improve network stability [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 21, "context": "Global bursts are also considered to be a pathologic behaviour for in vitro networks, and do not occur with healthy in vivo networks [23].", "startOffset": 133, "endOffset": 137}, {"referenceID": 7, "context": "These results could potentially be reproduced in a network in vitro: the Izhikevich model of spiking network that we use has been found to exhibit the same dynamics as real neurons, and our experiments with STDP can reproduce some results of biological experiments; therefore there is a probability that this results predicted by LSA still holds in biological networks with suppressed bursts, especially since we have shown that LSA gives promising results on biological networks embodied in simple robots (see [8] and following section).", "startOffset": 511, "endOffset": 514}, {"referenceID": 23, "context": "Minimisation introduced by Friston [25].", "startOffset": 35, "endOffset": 39}], "year": 2016, "abstractText": "Learning based on networks of real neurons, and by extension biologically inspired models of neural networks, has yet to find general learning rules leading to widespread applications. In this paper, we argue for the existence of a principle allowing to steer the dynamics of a biologically inspired neural network. Using carefully timed external stimulation, the network can be driven towards a desired dynamical state. We term this principle \u201cLearning by Stimulation Avoidance\u201d (LSA). We demonstrate through simulation that the minimal sufficient conditions leading to LSA in artificial networks are also sufficient to reproduce learning results similar to those obtained in biological neurons by Shahaf and Marom [1]. We examine the mechanism\u2019s basic dynamics in a reduced network, and demonstrate how it scales up to a network of 100 neurons. We show that LSA has a higher explanatory power than existing hypotheses about the response of biological neural networks to external simulation, and can be used as a learning rule for an embodied application: learning of wall avoidance by a simulated robot. The surge in popularity of artificial neural networks is mostly directed to disembodied models of neurons with biologically irrelevant dynamics: to the authors\u2019 knowledge, this is the first work demonstrating sensory-motor learning with random spiking networks through pure Hebbian learning.", "creator": "LaTeX with hyperref package"}}}