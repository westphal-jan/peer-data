{"id": "1609.07876", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Lexicon-Free Fingerspelling Recognition from Video: Data, Models, and Signer Adaptation", "abstract": "We study. problem of freedoms footage permutations its fingerspelled letters in American Sign Language (ASL ). Fingerspelling composed though significant but exceptionally kajol within of ASL. Recognizing sdsl is experience for new number of none: It involves quick, small proceedings that kinds same becoming coarticulated; be artistic making similarity months signers; along there ago taken in dearth example continuous fingerspelling embedded collected. In still. mean bulk and entreating a new indicate place of flow vaporizers soundtracks, distinguish addition related of recognizers, few concepts following ways within bankshares size. Our only - ballet camry are segmental (fifth - Markov) conditional random plant conventional surface membrane communication - publishing features. In the relinquishment - reducing changing, our recognizers commitment from then least 92% letter predictive. The multi - non-compete follow is although more challenging, few although processes services adventures our understanding from still 63% briefing potencies under this creating.", "histories": [["v1", "Mon, 26 Sep 2016 07:34:24 GMT  (4941kb,D)", "http://arxiv.org/abs/1609.07876v1", "arXiv admin note: substantial text overlap witharXiv:1608.08339"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1608.08339", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["taehwan kim", "jonathan keane", "weiran wang", "hao tang", "jason riggle", "gregory shakhnarovich", "diane brentari", "karen livescu"], "accepted": false, "id": "1609.07876"}, "pdf": {"name": "1609.07876.pdf", "metadata": {"source": "CRF", "title": "Lexicon-Free Fingerspelling Recognition from Video: Data, Models, and Signer Adaptation", "authors": ["Taehwan Kima", "Jonathan Keaneb", "Weiran Wanga", "Hao Tanga", "Jason Riggleb", "Gregory Shakhnarovicha", "Diane Brentarib", "Karen Livescua"], "emails": [], "sections": [{"heading": null, "text": "We study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL. Recognizing fingerspelling is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work we collect and annotate a new data set of continuous fingerspelling videos, compare several types of recognizers, and explore the problem of signer variation. Our best-performing models are segmental (semiMarkov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 92% letter accuracy. The multi-signer setting is much more challenging, but with neural network adaptation we achieve up to 83% letter accuracies in this setting.\nKeywords: American Sign Language, fingerspelling recognition, segmental model, deep neural network, adaptation"}, {"heading": "1. Introduction", "text": "Sign languages are the primary means of communication for millions of Deaf people in the world. In the US, there are about 350,000\u2013500,000 people for whom American Sign Language (ASL) is the primary language [1]. While there has been extensive research over several decades on automatic recognition and analysis of spoken language, much less progress has been made for sign languages. Both signers and nonsigners would benefit from technology that improves communication between these populations and facilitates search and retrieval in sign language video.\nSign language recognition involves major challenges. The linguistics of sign language is less well understood than that of spoken language, hampering both scientific and technological progress. Another challenge is the high variability in the appearance of signers\u2019 bodies and the large number of degrees of freedom. The closely related\nar X\niv :1\n60 9.\n07 87\n6v 1\n[ cs\n.C L\n] 2\n6 Se\ncomputer vision problem of articulated pose estimation and tracking remains largely unsolved.\nSigning in ASL (as in many other sign languages) involves the simultaneous use of handshape, location, movement, orientation, and non-manual behaviors. There has now been a significant amount of research on sign language recognition, for a number of sign languages. Prior research has focused more on the larger motions of sign and on interactions between multiple body parts, and less so on handshape. In some contexts, however, handshape carries much of the content, and it is this aspect of ASL that we study here.\nSign language handshape has its own phonology that has been studied and enjoys a broadly agreed-upon understanding relative to the other manual parameters of movement and place of articulation [3]. Recent linguistic work on sign language phonology has developed approaches based on articulatory features, related to motions of parts of the hand [4, 2]. At the same time, computer vision research has studied pose estimation and tracking of hands [5], but usually not in the context of a grammar that constrains the motion. There is therefore a great need to better understand and model the handshape properties of sign language.\nThis project focuses mainly on one constrained, but very practical, component of ASL: fingerspelling. In certain contexts (e.g., when no sign exists for a word such as a name, to introduce a new word, or for emphasis), signers use fingerspelling: They spell out the word as a sequence of handshapes or hand trajectories corresponding to individual letters. Fig. 1 shows the ASL fingerspelled alphabet, and Figs. 2 show examples of real fingerspelling sequences. We will refer to the handshapes in Fig. 1 as fingerspelled letters (FS-letters), which are canonical target handshapes for each of the 26 letters, and the starred actual handshapes in Fig. 2 as peak handshapes.\nFingerspelled words arise naturally in the context of technical conversation or conversation about current events, such as in Deaf blogs or news sites.1 Automatic fingerspelling recognition could add significant value to such resources. Overall, fingerspelling comprises 12-35% of ASL [6], depending on the context, and includes 72% of the handshapes used in ASL [7]. These factors make fingerspelling an excellent testbed for handshape recognition.\nMost previous work on fingerspelling and handshape has focused on restricted con-\n1E.g., http://deafvideo.tv, http://aslized.org.\nChapter 2 Recognition methods\nOur task is to take as input a video (a sequence of images) corresponding to a fingerspelled word, as in Figure 2-1, and predict the signed letters. This is a sequence prediction task analogous to connected phone or word recognition, but there are some interesting sign language-specific properties to the data domain. For example, one striking aspect of fingerspelling sequences, such as those in Figure 2-1, is the large amount of motion and lack of any prolonged \u201csteady state\u201d for each letter. Typically, each letter is represented by a brief \u201cpeak of articulation\u201d of one or a few frames, during which the hand\u2019s motion is at a minimum and the handshape is the closest to the target handshape for the letter. This peak is surrounded by longer period of motion between the current letter and the previous/next letters.\nWe consider signer-dependent, signer-independent, and signer-adapted recognition. We next describe the recognizers we compare, as well as the techniques we explore for signer adaptation. All of the recognizers use deep neural network (DNN) classifiers of letters or handshape features."}, {"heading": "2.1 Recognizers", "text": "In designing recognizers, we keep several considerations in mind. First, the data set, while large by sign language research standards, is still quite small compared to typical speech data sets. This means that large models with many context-dependent units are infeasible to train on our data (as confirmed by our initial experiments). We therefore restrict attention here to \u201cmono-letter\u201d models, that is models in which each unit is a context-independent letter. We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [41, 66, 10]. Second, we would like our models to be able to capture rich sign language-specific information, such as the dynamic aspects of fingerspelled letters as discussed above; this suggests the segmental models that we consider below. Finally, we would like our models to be easy to adapt to new signers. In order to enable this, all of our recognizers use independently trained deep neural network (DNN) classifiers, which can be adapted and plugged into different sequence models. Our DNNs are trained using an L2-regularized cross-entropy loss. The inputs are the image features concatenated over a multi-frame window centered at the current frame, which are fed through several fully connected layers followed by a softmax output layer with as many units as labels."}, {"heading": "2.1.1 Tandem model", "text": "The first recognizer we consider is based on the popular tandem approach to speech recognition [21]. In tandem-based speech recognition, Neural Networks (NN) are trained to classify phones, and their outputs (phone posteriors) are post-processed and used as observations in a standard HMM-based recognizer with Gaussian mixture observation distributions. The post-processing may include taking the logs of the posteriors (or simply taking the linear outputs of the NNs rather than posteriors), applying principal components analysis, and/or appending acoustic features to the NN outputs.\nIn this work, we begin with a basic adaptation of the tandem approach, where instead of phone posteriors estimated from acoustic frames, we use letter posteriors estimated from image features. We also propose to use classifiers of phonological features of fin-\nditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies, or signer-dependent applications [8, 9, 10] (see Section 2 for more on related work). In such restricted settings, letter error rates (Levenshtein distances between hypothesized and true FS-letter sequences, as a proportion of the number of true FS-letters) of 10% or less have been obtained. In this work we consider lexicon-free fingerspelling sequences produced by multiple signers. This is a natural setting, since fingerspelling is often used for names and other \u201cnew\u201d terms that do not appear in any closed vocabulary. Our l ng-term goals are to develop techniques for robust automatic detecti n and recognition of fingerspelle words i vi o, and to enable generalization across signers, styles, and recording conditions, both in controlled visual settings and \u201cin the wild.\u201d To this end, we are also interested in developing multi-signer, multi-style corpora of fingerspelling, as well as efficient annotation schemes for the new data.\nThe work in this paper represents our first steps toward this goal: studio collection and annotation of a new multi-signer connected fingerspelling data set (Section 3) and high-quality fingerspelling recognition in the signer-dependent and multi-signer settings (Section 5). The new data set, while small relative to typical speech data sets, comprises the l rg st finger lling video data set of which we are wa e containing onnected multi-sign r finger pelling tha is not stricted to any particul r lexicon. Next, we compare several recognition models inspired by automatic speech recognition, with some custom-made characteristics for ASL (Section 4). We begin with a tandem hidden Markov model (HMM) approach, where the features are based on pos-\n3\nteriors of deep neural network (DNN) classifiers of letter and handshape phonological features. We also develop discriminative segmental models, which allow us to introduce more flexible features of fingerspelling segments. In the category of segmental models, we compare models for rescoring lattices and a first-pass segmental model, and the latter ultimately outperforms the others. Finally, we address the problem of signer variation via adaptation of the DNN classifiers, which allows us to bridge much of the gap between signer-dependent and signer-independent performance. 2"}, {"heading": "2. Related work", "text": "Automatic sign language recognition has been approached in a variety of ways, including approaches based on computer vision techniques and ones inspired by automatic speech recognition. Thorough surveys on the topic are provided by Koller et al. [14] and Ong and Ranganath [15]. Here we focus on the most closely related work.\nA great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22]. This approach is very attractive for designing new communication interfaces for Deaf individuals. In many settings, however, video is more practical, and for online or archival recordings is the only choice. In this paper we restrict the discussion to the video-only setting.\nA number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28]. Some of the largest data collection efforts have been for European languages, such as the Dicta-Sign [29] and SignSpeak [30] projects. For American Sign Language, the American Sign Language Lexicon Video Dataset (ASLLVD) [27, 28, 31] includes recordings of almost 3,000 isolated signs and can be searched via a queryby-example interface. The National Center for Sign Language and Gesture Resources (NCSLGR) Corpus includes videos of continuous ASL signing, with over 10,000 sign tokens including about 1,500 fingerspelling sequences, annotated using the SignStream linguistic annotation tool [32, 33]. The latter includes the largest previous data set of fingerspelling sequences of which we are aware. In order to explicitly study fingerspelling, however, it is helpful to have a collection that is both larger and annotated specifically with fingerspelling in mind. To our knowledge, the data collection and annotation we report in this paper (see Sec. 3) is the largest currently available for ASL fingerspelling.\nSign language recognition from video begins with front-end features. Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42]. In this work, we are aiming at relatively small motions that are difficult to track a priori, and therefore begin with general image appearance features based on histograms of oriented gradients (HOG) [43], which have also been used in other recent sign language recognition\n2Parts of this work have appeared in our conference papers [11, 12, 13]. This paper includes additional model comparisons and improvements, different linguistic feature sets, and detailed presentation of the collected data and annotation.\nwork [14, 44]. Much prior work has used hidden Markov model (HMM)-based approaches [45, 46, 39, 14], and this is the starting point for our work as well. Ney and colleagues have shown that it is possible to borrow many of the standard HMM-based techniques from automatic speech recognition to obtain good performance on naturalistic German Sign Language videos [39, 14]. In addition, there have been efforts involving conditional models [35] and more complex (non-linear-chain) models [47, 48, 49].\nAs in acoustic and even visual speech recognition, the choice of basic linguistic unit is an important research question. For acoustic speech, the most commonly used unit is the context-dependent phoneme [50], although other choices such as syllables, articulatory features, and automatically learned units have also been considered [51, 52]. As the research community started to consider visual speech recognition (\u201clipreading\u201d), analogous units have been explored: visemes, articulatory features, and automatic clusters [53, 54, 55]. While sign language shares some aspects of spoken language, it has some unique characteristics. A number of linguistically motivated representations of handshape and motion have been used in some prior work, e.g., [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2]. One of the unique aspects of sign language is that transitional movements occupy a larger portion of the signal than steady states, and some researchers have developed approaches for explicitly modeling the transitions as units [61, 62, 63, 64].\nA subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10]. Letter error rates of 10% or less have been achieved when the recognition is constrained to a small (up to 100-word) lexicon of allowed sequences. Our work is the first of which we are aware to address the task of lexicon-free fingerspelling sequence recognition.\nThe problem of signer adaptation has been addressed in prior work, using techniques borrowed from speaker adaptation for speech recognition, such as maximum likelihood linear regression and maximum a posteriori estimation [14, 67, 68], and to explicitly study the contrast between signer-dependent and multi-signer fingerspelling recognition.\nThe models we use and develop in this paper are related to prior work in both vision and speech (unrelated to sign language). Our HMM baselines are tandem models, analogous to ones developed for speech recognition [69, 70]. The segmental models we propose are related to segmental conditional random fields (SCRFs) and their variants, which have now been applied fairly widely for speech recognition [71, 72, 73, 74, 75]. In natural language processing, semi-Markov CRFs have been used for named entity recognition [76], where the labeling is binary. Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [77, 78] with a small set of possible activities to choose from, including work on spotting and recognition of a small vocabulary of (non-fingerspelled) signs in sign language video [79] or instrumented data capture [80]. One aspect that our work shares with the speech recognition work is that we have a relatively large set of labels (26 FS-letters plus non-letter \u201cN/A\u201d labels), and widely varying lengths of segments corresponding to each label (our data includes segment durations anywhere from 2 to 40 frames),\nwhich makes the search space larger and the recognition task more difficult than in the vision and text tasks. In prior speech recognition work, this computational difficulty has often been addressed by adopting a lattice rescoring approach, where a frame-based model such as an HMM system generates first-pass lattices and a segmental model rescores them [71, 74]. We compare this approach to an efficient first-pass segmental model [72]."}, {"heading": "3. Data collection and annotation", "text": "We recorded and analyzed videos of 3 native ASL signers and 1 early learner, fingerspelling a total of 3,684 word instances. We annotated the video by identifying the time of peak articulation (also known as hold, posture, or target) for each fingerspelled letter. There were 21,453 peaks in total. The following sections describe in detail the data collection and annotation process.\nThe data and annotations will be released publicly, in order to make it easy for others to study fingerspelling more extensively, and to replicate and compare against our results."}, {"heading": "3.1. Video recording", "text": "The data was collected across several sessions, each including all of the words on one word list (the lists are described in detail below). Our use of \u201cwords\u201d here includes both real words and nonsense letter sequences. The signers were presented with an isolated word on a computer screen. They were asked to fingerspell the word, and then press either a green button to advance or a red button to repeat the word if they felt they had made a mistake. For most sessions the signers were asked to fingerspell at a normal, natural speed.3 Each session lasted 25\u201340 minutes, with a self-timed rest/stretch break in the middle of each session.\nThree word lists were created and used to collect data. The first list had 300 words: 100 names, 100 nouns, and 100 non-English words. These words were chosen to get examples of many letters in many contexts. The second list consisted of 300 mostly non-English words in an effort to get examples of each possible letter bigram. The third list had the 300 most common nouns in the CELEX corpus in order to get a list of words that are reasonably familiar to the signers. Additionally, a set of carefully articulated, isolated fingerspelled letters were also collected from each signer. A full listing of the word lists can be found in [2].\nThe video was recorded in a laboratory setting, with the signers wearing green or blue clothing and with the signers set against a green background. For most sessions the signers sat in a chair with an armrest that they could use if they felt the desire to. In a small number of sessions the signers were asked to stand rather than sit. Video was recorded using at least two cameras, each at an approximately 45 degree angle from a direct frontal view. Each camera recorded video at 1920x540 pixels per field4, 60 fields\n3The instructions, given in ASL, were to: \u201cproceed at normal speed and in your natural way of fingerspelling.\u201d\n4Through our deinterlacing process, additional lines were interpolated to give us a final result of 1920x1080 pixels per frame at 60 frames per second\nper second, interlaced, using the AVCHD format. For purposes of annotation, the video files were processed with FFMPEG to deinterlace, crop, resize, and reencode them for compatibility with the ELAN annotation software [81]. For purposes of recognition experiments, the videos were kept at full size and deinterlaced only.\nThe recording settings, including differences in environment and camera placement across sessions, are illustrated in Fig. 3."}, {"heading": "3.2. Annotation", "text": "Our annotation method is separated into two main parts: 1. a simple task to identify approximate times of each peak (peak detection) and 2. a verification task to determine precise timing for each peak (peak verification). The first is designed to be extremely quick, allowing multiple annotator judgements to be aggregated together. The second is more time-consuming and performed by a single annotator."}, {"heading": "3.2.1. Peak detection", "text": "3\u20134 human annotators identified the peak of each letter. For this purpose, we defined a peak as the point where the articulators change direction to proceed on to the next peak (i.e. where the instantaneous velocity of the articulators is zero or at its minimum). This point is typically where the hand most closely resembles the canonical handshape, although at normal speed the peak handshape is often very different from canonical. Two FS-letters, -J- and -Z-, are not well-represented in terms of peaks, since they have movement. For these two FS-letters, annotators were asked to mark a peak at the point that they could determine that it was one of these two FS-letters. Peak detection is simple, requiring minimal training; annotators reported that this task was very intuitive. Most of the annotators at this stage had no exposure to ASL or fingerspelling.\nThe peak times from the multiple annotators were averaged, after aligning the annotations to minimize the mean absolute difference in time between the individual annotators\u2019 peaks. We accounted for misidentified peaks by penalizing missing or extra ones in the alignment. Using logs from the recording session, a best guess at the FS-letter corresponding to each peak was added by aligning the intended FS-letter sequence to the peak times (starting at the left edge of the word, matching each peak with a FS-letter)."}, {"heading": "3.2.2. Peak verification", "text": "Finally, a more experienced, second-language learner of ASL or an annotator specifically trained in fingerspelling annotation verified the location and identity of each peak from the peak detection stage. For the verification stage, we further refined the definition of peak to the point in time when the handshape configuration is closest to the canonical handshape for a given FS-letter. If a peak handshape remained stable for more than one frame, each stable frame was marked. However, for recognition experiments, only the original single peak frame was used.\nWhen using the data for recognizer training, the peak annotations are used to segment each word into FS-letters: The boundary between consecutive FS-letters is defined as the midpoint between their peak frames. Appendix A provides additional details about the annotation and analysis conventions, as well as detailed definitions of the fingerspelled letters.\nFS-letter frequencies in the collected data are given in Tab. 1. A histogram of the durations of all peaks in our data (excluding outliers) is given in Fig. 4.\nSeveral studies [82, 83, 84, 2] have been conducted looking at how frequently FSletters are realized canonically (i.e. as the correct peak handshape) in our data. The frequency of the canonical peak handshape of the FS-letter depends on a number of factors (FS-letter identity, speed, signer, etc.). Some FS-letters show up nearly always as the canonical variant (e.g., -C-), some almost never (-D-), while others have 75%\u2013 85% ( -E- or -O- respectively) canonical peak handshape realizations [83]. Similarly to the phonetics of spoken language, contextual effects have an impact on the phonetic configuration of each peak handshape. For example, the pinky extension property of FS-letters has been shown to spread from peak handshapes that have an extended pinky to the ones before and after [2]. These coarticulation effects, combined with the quick motions of fingerspelling, are some of the factors that make the recognition task quite challenging."}, {"heading": "4. Recognition methods", "text": "Our task is to take as input a video (a sequence of images) corresponding to a fingerspelled word, as in Fig. 2, and predict the signed FS-letters. This is a sequence prediction task analogous to connected phone or word recognition, but there are some interesting sign language-specific properties to the data domain. For example, one striking aspect of fingerspelling sequences, such as those in Fig. 2, is the large amount of motion and lack of any prolonged \u201csteady state\u201d for each FS-letter. As described in Sec. 3, each FS-letter is represented by a brief \u201cpeak of articulation\u201d, during which the\nhand\u2019s motion is at a minimum and the handshape is the closest to the target handshape for the FS-letter. This peak is surrounded by longer periods of motion between the current FS-letter and the previous/next FS-letters.\nAnother striking property of sign language is the wide variation between signers. Inspection of data such as Fig. 2 reveals some types of signer variation, including differences in speed, hand appearance, and non-signing motion before and after signing. The speed variation is large: In our data, the ratio between the average per-letter duration is about 1.8 between the fastest and slowest signers.\nWe consider signer-dependent, signer-independent, and signer-adapted recognition. We next describe the recognizers we compare, as well as the techniques we explore for signer adaptation. All of the recognizers use deep neural network (DNN) classifiers of FS-letters or handshape features."}, {"heading": "4.1. Recognizers", "text": "In designing recognizers, we keep several considerations in mind. First, the data set, while large in comparison to prior fingerspelling data sets, is still quite small compared to typical speech data sets. This means that large models with many contextdependent units are infeasible to train on our data (as confirmed by our initial experiments). We therefore restrict attention here to \u201cmono-letter\u201d models, that is models in which each unit is a context-independent FS-letter. We also consider the use of articulatory (phonological and phonetic) feature units, and there is evidence from speech recognition research that these may be useful in low-data settings [85, 86]. Second, we would like our models to be able to capture detailed sign language-specific information, such as the dynamic aspects of FS-letters as discussed above; this suggests the segmental models that we consider below. Finally, we would like our models to be easy to adapt to new signers. In order to enable this, all of our recognizers use independently trained deep neural network (DNN) classifiers, which can be adapted and\nplugged into different sequence models. Our DNNs are trained using an L2-regularized cross-entropy loss. The inputs are the image features concatenated over a multi-frame window centered at the current frame, which are fed through several fully connected layers followed by a softmax output layer with as many units as labels.5"}, {"heading": "4.1.1. Tandem model", "text": "The first recognizer we consider is a fairly typical tandem model [69, 70]. Framelevel image features are fed to seven DNN classifiers, one of which predicts the frame\u2019s FS-letter label and six others which predict handshape phonological features. The phonological features are defined in Tab. 2, and example frames for values of one feature are shown in Fig. 5. The classifier outputs and image features are reduced in dimensionality via PCA and then concatenated. The concatenated features form the observations in an HMM-based recognizer with Gaussian mixture observation densities. We use a 3-state HMM for each (context-independent) FS-letter, plus one HMM each for initial and final \u201csilence\u201d (non-signing segments).\nIn addition, we use a bigram letter language model. In general the language model of FS-letter sequences is difficult to define or estimate, since fingerspelling does not follow the same distribution as English words and there is no large database of natural fingerspelled sequences on which to train. In addition, in our data set, the words were selected so as to maximize coverage of letter n-grams and word types rather than following a natural distribution. For this work, the language model is trained using ARPA CSR-III text, which includes English words and names [87]. The issue of language modeling for fingerspelling deserves more attention in future work.\n5We have also considered convolutional neural networks with raw image pixels as input, but these did not significantly outperform the DNNs on image features. See Sec. 5.3.3."}, {"heading": "4.1.2. Segmental CRF", "text": "The second recognizer is a segmental CRF (SCRF). SCRFs [76, 71] are conditional log-linear models with feature functions that can be based on variable-length segments\nof input frames, allowing for great flexibility in defining feature functions. Formally, for a sequence of frames o1, o2, . . . , oT , a segmentation of size k is a sequence of time points 0 = q0, q1, . . . , qk\u22121, qk = T used to denote time boundaries of segments. In other words, the i-th segment starts at time qi\u22121 and ends at qi. The labeling of segmentation q is a sequence of labels s1, s2, . . . , sk. We will denote the length of label sequences and segmentations |s|, |q|, respectively. For a segmentation of size k, |s| = |q| = k. A SCRF defines, over a sequence of labels s given a sequence of frames o, a probability distribution\np(s|o) = \u2211 q:|q|=|s| exp (\u2211|q| i=1 \u03bb >f(si\u22121, si, qi\u22121, qi, o) ) \u2211\ns\u2032 \u2211 q\u2032:|q\u2032|=|s\u2032| exp (\u2211|q\u2032| i=1 \u03bb >f(s\u2032i\u22121, s \u2032 i, q \u2032 i\u22121, q \u2032 i, o) ) where \u03bb is a weight vector, and f(s, s\u2032, q, q\u2032, o) is a feature vector. We assume s0 is the sentence-start string, so that f(s0, s1, q0, q1) is well-defined. In the case of sign language, it is natural to define feature functions that are sensitive to the duration and dynamics of each FS-letter segment."}, {"heading": "4.1.3. Rescoring segmental CRF", "text": "One common way of using SCRFs is to rescore the outputs from a baseline HMMbased recognizer, and this is one way we apply SCRFs here. We first use the baseline recognizer, in this case the tandem HMM, to generate lattices of high-scoring segmentations and labelings, and then rescore them with our SCRFs.\nWe use the same feature functions as in [12], described here for completeness. Some of the feature functions are quite general to sequence recognition tasks, while some are tailored specifically to fingerspelling recognition.\nDNN classifier-based feature functions. The first set of feature functions measure how well the frames within a segment match the hypothesized label. For this purpose we use the same DNN classifiers as in the tandem model above.\nLet y be a FS-letter and v be the value of a FS-letter or linguistic feature, and g(v|oi) the softmax output of a DNN classifier at frame i for class v. We define several feature functions based on aggregating the DNN outputs over a segment in various ways:\n\u2022 mean: fmeanyv (s, s\u2032, q, q\u2032, o) = \u03b4(s\u2032 = y) \u00b7 1q\u2032\u2212q+1 \u2211q\u2032 i=q g(v|oi)\n\u2022 max: fmaxyv (s, s\u2032, q, q\u2032, o) = \u03b4(s\u2032 = y) \u00b7maxi\u2208{q,q+1,...,q\u2032} g(v|oi)\n\u2022 divs: a concatenation of three mean feature functions, each computed over a third of the segment\n\u2022 divm: a concatenation of three max feature functions, each computed over a third of the segment\nThese are similar to features used in prior work on SCRFs for ASR with DNN-based feature functions [88, 89, 72]. We tune the choice of aggregated feature functions on tuning data in our experiments.\nPeak detection features. A sequence of fingerspelled FS-letters yields a corresponding sequence of peak handshapes as described above. The peak frame and the frames around it for each FS-letter tend to have relatively little motion, while the transition frames between peaks have much more motion. To encourage each predicted FS-letter segment to have a single peak, we define letter-specific \u201cpeak detection features\u201d based on approximate derivatives of the visual descriptors. We compute the approximate derivative as the l2 norm of the difference between descriptors in every pair of consecutive frames, smoothed by averaging over 5-frame windows. Typically, there should be a single local minimum in this derivative over the span of the segment. We define the feature function corresponding to FS-letter y as\nf peaky (s, s \u2032, q, q\u2032, o) = \u03b4(s\u2032 = y) \u00b7 \u03b4peak(o, q, q\u2032)\nwhere \u03b4peak(o, q, q\u2032) is 1 if there is exactly one local minimum between time point q and q\u2032 and 0 otherwise.\nLanguage model feature. The language model feature is a bigram probability of the FS-letter pair corresponding to an edge:\nf lm(s, s\u2032, q, q\u2032, o) = pLM (s, s \u2032).\nwhere pLM is our smoothed bigram language model.\nBaseline consistency feature. To take advantage of the already high-quality baseline that generated the lattices, we use a baseline feature like the one in [71], which is based on the 1-best output from the baseline tandem recognizer. The feature has value 1 when the corresponding segment spans exactly one FS-letter label in the baseline output and the label matches it:\nf bias(s, s\u2032, q, q\u2032, o) =  +1 if C(q, q \u2032) = 1, and B(q, q\u2032) = s\u2032\n\u22121 otherwise\nwhere C(q, q\u2032) is the number of distinct baseline labels in the time span from q to q\u2032, B(q, q\u2032) is the label corresponding to time span (q, q\u2032) when C(q, q\u2032) = 1."}, {"heading": "4.1.4. First-pass segmental CRF", "text": "One of the drawbacks of a rescoring approach is that the quality of the final outputs depends both on the quality of the baseline lattices and the fit between the segmentations in the baseline lattices and those preferred by the second-pass model. We therefore also consider a first-pass segmental model, using similar features to the rescoring model. The difference between a rescoring model and a first-pass model lies in the set of segmentations and labellings to marginalize over. The rescoring model only marginalizes over the hypotheses generated by the tandem model, while the first-pass model marginalizes over all possible hypotheses. The first-pass model thus does not depend on the tandem HMM. We use a first-pass SCRF inspired by the phonetic recognizer of Tang et al. [72], and the same feature functions as in [72], namely average DNN outputs over each segment, samples of DNN outputs within the segment, boundaries of DNN outputs in each segment, duration and bias. Recall that g(v|oi) is the\nsoftmax output of a DNN classifier at frame i for class v. We use the same DNN frame classifiers as in the rescoring setting. Also recall that y is a FS-letter and v is the value of a FS-letter or linguistic feature. The exact definition of each feature is listed below.\nAverages of DNN outputs. These are the same as the mean features used in the rescoring setting.\nSamples of DNN outputs. These are samples of the DNN outputs at the mid-points of three equal-sized sub-segments\nf splyvi(s, s \u2032, q, q\u2032, o) = \u03b4(s\u2032 = y)g(v|oq+(q\u2032\u2212q)i)\nfor i = 16%, 50%, 84%.\nLeft boundary. Three DNN output vectors around the left boundary of the segment\nf l-bndryyvk (s, s \u2032, q, q\u2032, o) = \u03b4(s\u2032 = y)g(v|oq+k)\nfor k \u2208 {\u22121, 0, 1}.\nRight boundary. Three DNN output vectors around the right boundary of the segment\nf r-bndryyvk (s, s \u2032, q, q\u2032, o) = \u03b4(s\u2032 = y)g(v|oq\u2032+k)\nfor k \u2208 {\u22121, 0, 1}.\nDuration. An indicator for the duration of a segment\nf duryk (s, s \u2032, q, q\u2032, o) = \u03b4(q\u2032 \u2212 q = k)\u03b4(s\u2032 = y)\nfor k \u2208 {1, 2, . . . , 30}.\nBias. A constant 1\nf biasy (s, s \u2032, q, q\u2032, o) = 1 \u00b7 \u03b4(s\u2032 = y).\nAs shown in the above definitions, all features are lexicalized with the unigram label, i.e., they are multiplied by an indicator for the hypothesized FS-letter label."}, {"heading": "4.2. DNN adaptation", "text": "In experiments, we will consider both signer-dependent and signer-independent recognition. In the latter case, the test signer is not seen in the training set. As we will see, there is a very large gap between signer-dependent and signer-independent recognition on our data. We also consider the case where we have some labeled data from the test signer, but not a sufficient amount for training signer-dependent models. In this case we consider adapting a signer-independent model toward the test signer. The most straightforward form of adaptation for our models is to adapt only the DNN classifiers, and then use the adapted ones in pre-trained signer-independent models. We consider adaptation with carefully annotated adaptation data, using ground-truth\nframe-level labels, as well as the setting where only word labels are available but no frame-level alignments. In the latter case, we obtain frame labels via forced alignment using the signer-independent model.\nOur adaptation approaches are inspired by several DNN adaptation techniques that have been developed for speech recognition (e.g., [90, 91, 92, 93]). We consider several approaches, shown in Fig. 6. Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [94, 95, 96]. In these techniques most of the network parameters are fixed to the signer-independent ones, and a limited set of weights at the input and/or output layers are learned.\nIn the approach we refer to as LIN+UP in Fig. 6, we apply an affine transformation WLIN to the static features at each frame, as a pre-processing step before frame concatenation, and use the transformed features as input to the trained signer-independent DNNs. We simultaneously learn WLIN and fine-tune the last (softmax) layer weights by minimizing the same cross-entropy loss on the adaptation data, initialized with the signer-independent weights..\nThe second approach, referred to as LIN+LON in Fig. 6, uses the same adaptation layer at the input. However, instead of adapting the softmax weights at the top layer, it removes the softmax output activation and adds a new softmax output layer WLON trained for the test signer. The new input and output layers are trained jointly with the same cross-entropy loss.\nFinally, we also consider adaptation by fine-tuning all of the DNN weights on the adaptation data, using as initialization the signer-independent DNN weights (that is, using the signer-independent DNN as a \u201cwarm start\u201d)."}, {"heading": "5. Experimental Results", "text": "We report on experiments using the fingerspelling data from the four ASL signers described above. We begin by describing some of the front-end details of hand segmentation and feature extraction, followed by experiments with the frame-level DNN classifiers (Sec. 5.1) and FS-letter sequence recognizers (Sec. 5.2).\nHand localization and segmentation As in prior work [9, 11, 12], we used a simple signer-dependent model for hand detection. First we manually annotated hand regions\nin 30 frames, and we trained a mixture of Gaussians Phand for the color of the hand pixels in L*a*b color space, and a single-Gaussian color model P xbg for every pixel x in the image excluding pixel values in or near marked hand regions. Given the color triplet cx = [lx, ax, bx] at pixel x from a test frame, we assign the pixel to the hand if\nPhand(cx)\u03c0hand > P x bg(cx)(1\u2212 \u03c0hand), (1)\nwhere the prior \u03c0hand for hand size is estimated from the same 30 training frames. We clean up the output of this simple model via several filtering steps. First, we suppress pixels that fall within regions detected as faces by the Viola-Jones face detector [97], since these tend to be false positives. We also suppress pixels that passed the log-odds test (Eq. 1) but have a low estimated value of Phand, since these tend to correspond to movements in the scene. Finally, we suppress pixels outside of a (generous) spatial region where the signing is expected to occur. The largest surviving connected component of the resulting binary map is treated as a mask that defines the detected hand region. Some examples of the resulting hand regions are shown in Fig. 2. Although this approach currently involves manual annotation for a small number of frames, it could be fully automated in an interactive system, and may not be required if given a larger number of training signers.\nHandshape descriptors We use histograms of oriented gradients (HOG [43]) as the visual descriptor (feature vector) for a given hand region. We first resize the tight bounding box of the hand region to a canonical size of 128\u00d7128 pixels, and then compute HOG features on a spatial pyramid of regions, 4\u00d74, 8\u00d78, and 16\u00d716 grids, with eight orientation bins per grid cell, resulting in 2688-dimensional descriptors. Pixels outside of the hand mask are ignored in this computation. For the HMM-based recognizers, to speed up computation, these descriptors were projected to at most 200 principal dimensions; the exact dimensionality in each experiment was tuned on a development set. For DNN frame classifiers, we found that finer grids did not improve much with increasing complexities, so we use 128-dimensional descriptors."}, {"heading": "5.1. DNN frame classification performance", "text": "Since all of our fingerspelling recognition models use DNN frame classifiers as a building block, we first examine the performance of the frame classifiers.\nFor training and tuning the DNNs, we use the recognizer training data, split into 90% for DNN training and 10% for DNN tuning. The DNNs are trained for seven tasks (FS-letter classification and classification of each of the six phonological features). The input is the 128-dimensional HOG features concatenated over a 21-frame window. The DNNs have three hidden layers, each with 3000 ReLUs [98]. Network learning is done with cross-entropy training with a weight decay penalty of 10\u22125, via stochastic gradient descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [99] at a rate of 0.5 at each hidden layer, fixed momentum of 0.95, and initial learning rate of 0.01, which is halved when held-out accuracy stops improving. We pick the bestperforming epoch on held-out data. The network structure and hyperparameters were tuned on held-out (signer-independent) data in initial experiments.\nWe consider the signer-dependent setting (where the DNN is trained on data from the test signer), signer-independent setting (where the DNN is trained on data from all except the test signer), and signer-adapted setting (where the signer-independent DNNs are adapted using adaptation data from the test signer). For LIN+UP and LIN+LON, we adapt by running SGD over minibatches of 100 samples with a fixed momentum of 0.9 for up to 20 epochs, with initial learning rate of 0.02 (which is halved when accuracy stops improving on the adaptation data). For fine-tuning, we use the same SGD procedure as for the signer-independent DNNs. We pick the epoch with the highest accuracy on the adaptation data.\nThe frame error rates for all settings are given in Fig. 7. For the signer-adapted case, we consider DNN adaptation with different types and amounts of supervision. The types of supervision include fully labeled adaptation data (\u201cGT\u201d, for \u201cground truth\u201d, in the figure), where the peak locations for all FS-letters are manually annotated; as well as adaptation data labeled only with the FS-letter sequence but not the timing\ninformation. In the latter case, we use the baseline tandem system to generate forced alignments (\u201cFA\u201d in the figure). We consider amounts of adaptation data from 5% to 20% of the test signer\u2019s full data.\nThese results show that among the adaptation methods, LIN+UP slightly outperforms LIN+LON, and fine-tuning outperforms both LIN+UP and LIN+LON. For FSletter sequence recognition experiments in the next section, we adapt via fine-tuning using 20% of the test signer\u2019s data.\nWe have analyzed the types of errors made by the DNN classifiers. One of the main effects is that all of the signer-independent classifiers have a large number of incorrect predictions of the non-signing classes (<s>, </s>). This may be due to the previously mentioned observation that non-linguistic gestures are variable and easy to confuse with signing when given a new signer\u2019s image frames. As the DNNs are adapted, this is the main type of error that is corrected. Specifically, of the frames with label <s> that are misrecognized by the signer-independent DNNs, 18.3% are corrected after adaptation; of the misrecognized frames labeled </s>, 11.2% are corrected."}, {"heading": "5.2. FS-letter recognition experiments", "text": ""}, {"heading": "5.2.1. Signer-dependent recognition", "text": "Our first continuous FS-letter recognition experiments are signer-dependent; that is, we train and test on the same signer, for each of four signers. For each signer, we use a 10-fold setup: In each fold, 80% of the data is used as a training set, 10% as a development set for tuning parameters, and the remaining 10% as a final test set. We independently tune the parameters in each fold. To make the results comparable to the later adaptation results, we use 8 out of 10 folds to compute the final test results and report the average letter error rate (LER) over those 8 folds. For language models, we train letter bigram language models from large online dictionaries of varying sizes that include both English words and names [100]. We use HTK [101] to implement the baseline HMM-based recognizers and SRILM [102] to train the language models. The HMM parameters (number of Gaussians per state, size of language model vocabulary, transition penalty and language model weight), as well as the dimensionality of the HOG descriptor input and HOG depth, were tuned to minimize development set letter error rates for the baseline HMM system. The front-end and language model hyperparameters were then kept fixed for the experiments with SCRFs (in this sense the SCRFs are slightly disadvantaged). Additional parameters tuned for the SCRF rescoring models included the N-best list sizes, type of feature functions, choice of language models, and L1 and L2 regularization parameters. Finally, for the first-pass SCRF, we tuned step size, maximum length of segmentations, and number of training epochs.\nTab. 3 (last row) shows the signer-dependent FS-letter recognition results. SCRF rescoring improves over the tandem HMM, and the first-pass SCRF outperforms both.\nNote that in our experimental setup, there is some overlap of word types between training and test data. This is a realistic setup, since in real applications some of the test words will have been previously seen and some will be new. However, for comparison, we have also conducted the same experiments while keeping the training, development, and test vocabularies disjoint; in this modified setup, letter error rates increase by about 2-3% overall, but the SCRFs still outperform the other models."}, {"heading": "5.2.2. Signer-independent recognition", "text": "In the signer-independent setting, we would like to recognize FS-letter sequences from a new signer, given a model trained only on data from other signers. For each of the four test signers, we train models on the remaining three signers, and report the performance for each test signer and averaged over the four test signers. For direct comparison with the signer-dependent experiments, each test signer\u2019s performance is itself an average over the 8 test folds for that signer.\nAs shown in the first line of Tab. 3, the signer-independent performance of the three types of recognizers is quite poor, with the rescoring SCRF somewhat outperforming the tandem HMM and first-pass SCRF. The poor performance is perhaps to be expected with such a small number of training signers."}, {"heading": "5.2.3. Signer-adapted recognition", "text": "The remainder of Tab. 3 (second and third rows) gives the connected FS-letter recognition performance obtained with the three types of models using DNNs adapted via fine-tuning, using different types of adaptation data (ground-truth, GT, vs. forcedaligned, FA). For all models, we do not retrain the models with the adapted DNNs, but tune hyperparameters6 on 10% of the test signer\u2019s data. The tuned models are evaluated on an unseen 10% of the test signer\u2019s remaining data; finally, we repeat this for eight choices of tuning and test sets, covering the 80% of the test signer\u2019s data that we do not use for adaptation, and report the mean FS-letter accuracy over the test sets.\nAs shown in Tab. 3, adaptation improves the performance to up to 30.3% letter error rate with forced-alignment adaptation labels and up to 17.3% letter error rate with ground-truth adaptation labels. All of the adapted models improve similarly. However, interestingly, the first-pass SCRF is slightly worse than the others before adaptation but better (by 4.4% absolute) after ground-truth adaptation. One hypothesis is that the first-pass SCRF is more dependent on the DNN performance, while the tandem model uses the original image features and the rescoring SCRF uses that tandem model\u2019s hypotheses. Once the DNNs are adapted, however, the first-pass SCRF outperforms the other models. Fig. 8 shows an example fingerspelling sequence and the hypotheses of the tandem, rescoring SCRF, and first-pass SCRF."}, {"heading": "5.3. Extensions and analysis", "text": "We next analyze our results and consider potential extensions for improving the models.\n6See [11, 12, 72] for details of the tuning parameters.\n5.3.1. Analysis: Could we do better by training entirely on adaptation data? Until now we have considered adapting the DNNs while using sequence models (HMMs/SCRFs) trained only on signer-independent data. In this section we consider alternatives to this adaptation setting. We fix the model to a first-pass SCRF and the adaptation data to 20% of the test signer\u2019s data annotated with ground-truth labels. In this setting, we consider two alternative ways of using the adaptation data: (1) using the adaptation data from the test signer to train both the DNNs and sequence model from scratch, ignoring the signer-independent training set; and (2) training the DNNs from scratch on the adaptation data, but using the SCRF trained on the training signers. We compare these options with our best results using the signer-independent SCRF and DNNs fine-tuned on the adaptation data. The results are shown in Tab. 4.\nWe find that ignoring the signer-independent training set and training both DNNs and SCRFs from scratch on the test signer (option (1) above) works remarkably well, better than the signer-independent models and even better than adaptation via forced alignment (see Tab. 3). However, training the SCRF on the training signers but DNNs from scratch on the adaptation data (option (2) above) improves performance further. However, neither of these outperforms our previous best approach of signerindependent SCRFs plus DNNs fine-tuned on the adaptation data."}, {"heading": "5.3.2. Analysis: FS-letter vs. feature DNNs", "text": "We next compare the FS-letter DNN classifiers and the phonological feature DNN classifiers in the context of the first-pass SCRF recognizers. We also consider an alternative sub-letter feature set, in particular a set of phonetic features introduced by Keane [2], whose feature values are listed in Section Appendix A, Tab. B.9. We use\nthe first-pass SCRF with either only FS-letter classifiers, only phonetic feature classifiers, FS-letter + phonological feature classifiers, and FS-letter + phonetic feature classifiers. We do not consider the case of phonological features alone, because they are not discrimative for some FS-letters. Fig. 9 shows the FS-letter recognition results for the signer-dependent and signer-adapted settings.\nWe find that using FS-letter classifiers alone outperforms the other options in the signer-dependent setting, achieving 7.7% letter error rate. For signer-adapted recognition, phonological or phonetic features are helpful in addition to FS-letters for two of the signers (signers 2 and 4) but not for the other two (signers 1,3); on average, using FS-letter classifiers alone is best in both cases, achieving 16.6% accuracy on average. In contrast, in earlier work [11] we found that phonological features outperform FSletters in the tandem HMM. However, those experiments used a tandem model with neural networks with a single hidden layer; we conjecture that with more layers, we are able to do a better job at the more complicated task of FS-letter classification."}, {"heading": "5.3.3. Analysis: DNNs vs. CNNs", "text": "In all experiments thus far, we have used HOG image descriptors fed into fully connected feedforward DNNs. However, it has recently become common to use convolutional neural networks (CNNs) on raw image pixels without any hand-crafted image descriptors, which produces improved performance for certain visual recognition tasks (e.g., [103, 104]). In our case, we have relatively little training data compared with typical benchmark visual recognition tasks, motivating our choice to preprocess with image descriptors rather than rely on networks that learn features from raw pixels. To test this assumption, we compare the performance of FS-letter and phonological feature classifiers based on DNNs and CNNs. We use a signer-dependent setting and the same 8-fold setup.\nFor CNN experiments, our inputs are grayscale 64 \u00d7 64 \u00d7 T pixels. T is the number of frames used in the input window, as in our DNNs. The CNNs use 32 kernels of 3 \u00d7 3 filters with stride 1 for the first and second convolutional layers. Next we add a max pooling layer with a 2 \u00d7 2 pixel window, with stride 2. For the third and fourth convolutional layer, we use 64 kernels of 3 \u00d7 3 filters with stride 1. Again, we then add a max pooling layer with a 2 \u00d7 2 pixel window, with stride 2. For all convolutional layers, ReLUs are used for the nonlinearity (as in our DNNs). Finally, two fully connected layers with 2000 ReLUs are added, followed by a softmax output layer. We use dropout to prevent overfitting, with a 0.25 dropout probability for all convolutional layers and 0.5 for fully connected layers. Training is done via stochastic gradient descent with learning rate 0.01, learning rate decay 1 \u00d7 10\u22126, momentum 0.9, and miini-batches of size 100. The network structure and all other settings were tuned on held-out data in preliminary experiments. T was also tuned and set to 21. We implemented the CNNs using Keras [105] with Theano [106].\nFig. 10 shows the results for the signer-dependent setting. We find that the performances of the DNNs and CNNs are comparable. Specifically, for FS-letter classification, DNNs are slightly better, while for phonological feature classification, CNNs are slightly better. However, in both cases the gaps are very small. These results provide evidence for our hypothesis that on this data set, CNNs on raw pixels do not provide a substantial benefit, and we do not use them in further experiments."}, {"heading": "5.3.4. Improving performance in the force-aligned adaptation case", "text": "We next attempt to improve the performance of adaptation in the absence of groundtruth (manually annotated) frame labels. This is an important setting, since in practice\nit can be very difficult to obtain ground-truth labels at the frame level. Using only the FS-letter label sequence for the adaptation data, we use the signer-independent tandem recognizer to get force-aligned frame labels. We then adapt (fine-tune) the DNNs using the force-aligned adaptation data (as before in the FA case). We then re-align the test signer\u2019s adaptation data with the adapted recognizer. Finally, we adapt the DNNs again with the re-aligned data. Throughout this experiment, we do not change the recognizer but only update the DNNs. Using this iterative realignment approach, we are able to furthur improve the recognition accuracy in the FA case by about 1.3%, as shown in Tab. 5."}, {"heading": "5.3.5. Improving performance with segmental cascades", "text": "Finally, we consider whether we can improve upon the performance of our best models, the first-pass SCRFs, by rescoring their results in a second pass with more powerful features. We follow the discriminative segmental cascades (DSC) approach of [72], where a simpler first-pass SCRF is used for lattice generation and a second SCRF, with more computationally demanding features, is used for rescoring.\nFor these experiments we start with the most successful first-pass SCRF in the above experiments, which uses FS-letter DNNs and is adapted with 20% of the test signer\u2019s data with ground-truth peak handshape labels. For the second-pass SCRF, we use the first-pass score as a feature, and add to it two more complex features: a segmental DNN, which takes as input an entire hypothesized segment and produces posterior probabilities for all of the FS-letter classes; and the \u201cpeak detection\u201d feature described in Section 4. We use the same bigram language model as in the tandem HMM and rescoring SCRF models. For the segmental DNN, the training segments are given by the ground-truth segmentations derived from manual peak annotations. The input layer consists of a concatenation of three mean HOG vectors, each averaged over one third of the segment. We use the same DNN structure and learning strategy as for the DNN frame classifiers.\nAs shown in Tab. 6, this approach slightly improves the average FS-letter accuracy over four signers, from 16.6% in the first pass to 16.2% in the second pass. This improvement, while small, is statistically significant at the p=0.05 level (using the MAPSSWE significance test from the NIST Scoring Toolkit [107]). These results combine our most successful ideas and form our final best results for signer-adapted recognition. For the signer-dependent setting, this approach achieves comparable performance to the first-pass SCRF."}, {"heading": "6. Conclusion", "text": "This paper tackles the problem of unconstrained fingerspelled letter sequence recognition in ASL, where the FS-letter sequences are not restricted to any closed vocabulary. This problem is challenging due to both the small amount of available training data and the significant variation between signers. Our data collection effort has thus far produced a set of carefully annotated fingerspelled letter sequences for four native ASL signers. Our recognition experiments have compared HMM-based and segmental models with features based on DNN classifiers, and have investigated a range of settings including signer-dependent, signer-independent, and signer-adapted. Our main contributions are:\n\u2022 We have developed an approach for quick annotation of fingerspelling data using a two-pass approach, where multiple non-expert annotators quickly find candidate peak times, which are then automatically combined and verified by an ASL expert.\n\u2022 Signer-dependent recognition, even with only a small amount of training data, is quite successful, reaching letter error rates below 10%. Signer-independent recognition, in contrast, is quite challenging, with letter error rates around 60%.\nDNN adaptation allows us to bridge a large part of the gap between signerindependent and signer-dependent performance.\n\u2022 Our best results are obtained using two-pass discriminative segmental cascades with features based on frame-level DNN FS-letter classifiers. This approach achieves an average letter error rate of 7.6% in the signer-dependent setting and 16.2% LER in the signer-adapted setting. The adapted models use signerindependent SCRFs and DNNs adapted by fine-tuning on the adaptation data. The adapted results are obtained using about 115 words of adaptation data with manual annotations of FS-letter peaks.\n\u2022 If an even smaller amount of adaptation data is available, we can still get improvements from adaptation down to about 30 words of adaptation data.\n\u2022 In the absence of manual frame-level annotations, we can automatically align the adaptation data and still get a significant boost in performance from adaptation. We can also iteratively improve the performance by re-aligning the data with the adapted models. Our best adapted models using automatically aligned adaptation data achieve 27.3% LER.\n\u2022 The main types of errors that are addressed by adapting the DNN classifiers are confusions between signing and non-signing segments.\nWe are continuing to investigate additional sequence recognition approaches as well as adaptation methods, especially for the case where no manual annotations are available. Future work will also expand our data collection to a larger number of signers and to data collected \u201cin the wild\u201d, such as videos from Deaf online media. Finally, future work will consider jointly detecting and recognizing fingerspelling sequences embedded within running ASL."}, {"heading": "Acknowledgements", "text": "We are grateful for the work of several undergraduate assistants for annotation of the data. This research was funded by a Google Faculty Award and by NSF grants NSF-1433485 and NSF/BCS-1251807. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency."}, {"heading": "Appendix A. Fingerspelled handshape definitions and annotation conventions", "text": "Handshape \u2013 We defined a handshape as stable if all of digits assumed a position and maintained it with only minor fluctuations. As soon as any digit moves, the handshape is considered to not be stable anymore. We were conservative with respect to holds, in that if a digit moves a small amount, but that movement is part of a larger movement that preceded or followed, that was not considered stable.\n\u2022 Fingerspelled (FS)-letters are defined as the target, canonical handshapes that are linked to the 26 letters of the English alphabet.\n\u2022 Peak handshapes are the phonetically realized handshapes used in a given sequence at the moment when it most closely approximates the FS-letter.\nOrientation \u2013 Most FS-letters are produced with the palm facing away from the signer\u2019s body. The few exceptions to this are -G-, -H-, -P-, and -Q-7 where the palm faces the signer ( -G- and -H-; labeled side in our analysis), or faces down ( -P- and -Q-; labeled down in our analysis). Because handshape and orientation changes are not always synchronized, we have annotated handshape stability as a hold, even if the hand is continuing to undergo an orientation change. Future annotation is necessary for orientation changes in detail and determine the pattern of stability and motion that exists there.\nMovement \u2013 Two FS-letters are described as having movement: -J- and -Z-. -Jinvolves an orientation change, and -Z- traces the path of the letter8. For both of these FS-letters, again we have annotated a hold to be where the handshape is stable, regardless of orientation change, or path movement.\nHandshape detail \u2013 A detailed (although not exhaustive) description of handshapes is given in table A.7. This is meant to be guidance to annotators, and is intended to catch the core features for each handshape, allowing for the systematic variation known to exist in handshape. If some of these features match, but the handshape is significantly different than expected, annotators added a diacritic (+) to note a large amount of deviance. This is not intended to exhaustively mark all of the deviant handshapes, but only those that should be looked into further. There are some instances where a peak is found, but no peak was detected. Although we have not analyzed this systematically, these instances are frequently peak handshapes that are instantaneous, or peak handshapes that occur extremely close to each other. These peaks are noted with a different diacritic (*). Finally if two handshapes have compressed to form a single peak handshape, a digraph is used to annotate the combined peak handshape. Examples that we have seen so far are -GH-, - IT-, - IN-, - IO-, - IL-, and - CI-. Here the digraph is simply two FS-letters that seem to make up the single peak; for consistency they should be written in alphabetical order regardless of the orthographic order of the letters in the word being fingerspelled. See table A.8 for a description of those found so far."}, {"heading": "Appendix B. Phonetic feature definitions", "text": "7These are the FS-letters traditionally described as having different orientation; there are other possibilities that we have found as well: -X- and -Y-.\n8This is frequently abbreviated to just a horizontal line, representing the top bar of the z."}], "references": [{"title": "How many people use ASL in the United States? Why estimates need updating", "author": ["R.E. Mitchell", "T.A. Young", "B. Bachleda", "M.A. Karchmer"], "venue": "Sign Language Studies 6 (3) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards an articulatory model of handshape: What fingerspelling tells us about the phonetics and phonology of handshape in American Sign Language", "author": ["J. Keane"], "venue": "Ph.D. thesis, University of Chicago ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "A Prosodic Model of Sign Language Phonology", "author": ["D. Brentari"], "venue": "MIT Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Modelling and recognition of the linguistic components in American Sign Language", "author": ["L. Ding", "A.M. Martinez"], "venue": "Image and Vision Computing 27 (12) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Real-time articulated hand pose estimation using semi-supervised transductive regression forests", "author": ["D. Tang", "T.-H. Yu", "T.-K. Kim"], "venue": "in: ICCV", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "How the alphabet came to be used in a sign language", "author": ["C. Padden", "D.C. Gunsauls"], "venue": "Sign Language Studies 4 (1) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Native and foreign vocabulary in American Sign Language: A lexicon with multiple origins", "author": ["D. Brentari", "C. Padden"], "venue": "in: Foreign vocabulary in sign languages: A cross-linguistic investigation of word formation, Lawrence Erlbaum, Mahwah, NJ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Dynamic fingerspelling recognition using geometric and motion features", "author": ["P. Goh", "E.-J. Holden"], "venue": "in: ICIP", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic recognition of fingerspelled words in British Sign Language", "author": ["S. Liwicki", "M. Everingham"], "venue": "in: 2nd IEEE Workshop on CVPR for Human Communicative Behavior Analysis", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Fingerspelling recognition through classification of letterto-letter transitions", "author": ["S. Ricco", "C. Tomasi"], "venue": "in: ACCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "American Sign Language fingerspelling recognition with phonological feature-based tandem models", "author": ["T. Kim", "K. Livescu", "G. Shakhnarovich"], "venue": "in: Proc. IEEE Workshop on Spoken Language Technology (SLT)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Fingerspelling recognition with semi- Markov conditional random fields", "author": ["T. Kim", "G. Shakhnarovich", "K. Livescu"], "venue": "in: ICCV", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Signer-independent fingerspelling recognition with deep neural network adaptation", "author": ["T. Kim", "W. Wang", "H. Tang", "K. Livescu"], "venue": "in: ICASSP", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers", "author": ["O. Koller", "J. Forster", "H. Ney"], "venue": "Computer Vision and Image Understanding 141 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic sign language analysis: A survey and the future beyond lexical meaning", "author": ["S.C. Ong", "S. Ranganath"], "venue": "IEEE transactions on pattern analysis and machine intelligence 27 (6) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Recognition of finger spelling of American Sign Language with artificial neural network using position/orientation sensors and data glove", "author": ["C. Oz", "M.C. Leu"], "venue": "in: 2nd International Conference on Advances in Neural Networks", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Real-time hand-tracking with a color glove", "author": ["R. Wang", "J. Popovic"], "venue": "in: SIG- GRAPH", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Spelling it out: Real-time ASL fingerspelling recognition", "author": ["N. Pugeault", "R. Bowden"], "venue": "in: ICCV Workshops", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Hand pose estimation and hand shape classification using multi-layered randomized decision forests", "author": ["C. Keskin", "F. K\u0131ra\u00e7", "Y.E. Kara", "L. Akarun"], "venue": "in: ECCV", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Collecting and evaluating the cuny asl corpus for research on american sign language animation", "author": ["P. Lu", "M. Huenerfauth"], "venue": "Computer Speech and Language 28 (3) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Histogram of 3d facets: a depth descriptor for human action and hand gesture recognition", "author": ["C. Zhang", "Y. Tian"], "venue": "Computer Vision and Image Understanding 139 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "American Sign Language alphabet recognition using Microsoft Kinect", "author": ["C. Dong", "M.C. Lieu", "Z. Yin"], "venue": "in: CVPR Workshops", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Purdue ASL database for the recognition of American sign language", "author": ["A.M. Martinez", "R.B. Wilbur", "R. Shay", "A.C. Kak"], "venue": "in: ICMI", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "H", "author": ["P. Dreuw", "C. Neidle", "V. Athitsos", "S. Sclaroff"], "venue": "Ney, Benchmark databases for video-based automatic sign language recognition., in: International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "The significance of facial features for automatic sign language recognition", "author": ["U. Von Agris", "M. Knorr", "K.-F. Kraiss"], "venue": "in: Proc. IEEE International Conference on Automatic Face and Gesture Recognition", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "SignSpeak - understanding", "author": ["P. Dreuw", "J. Forster", "Y. Gweth", "D. Stein", "H. Ney", "G. Martinez", "J.V. Llahi", "O. Crasborn", "E. Ormel", "W. Du", "T. Hoyoux", "J. Piater", "J.M.M. Lazaro", "M. Wheatley"], "venue": "recognition, and translation of sign languages, in: Proc. Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Large lexicon project: American sign language video corpus and sign language indexing/retrieval algorithms", "author": ["V. Athitsos", "C. Neidle", "S. Sclaroff", "J. Nash", "A. Stefan", "A. Thangali", "H. Wang", "Q. Yuan"], "venue": "in: Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "A new web interface to facilitate access to corpora: Development of the ASLLRP data access interface (DAI)", "author": ["C. Neidle", "C. Vogler"], "venue": "in: LREC Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Sign language technologies and resources of the Dicta-Sign project", "author": ["E. Efthimiou", "S.-E. Fotinea", "T. Hanke", "J. Glauert", "R. Bowden", "A. Braffort", "C. Collet", "P. Maragos", "F. Lefebvre-Albaret"], "venue": "in: LREC Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "H", "author": ["J. Forster", "C. Schmidt", "T. Hoyoux", "O. Koller", "U. Zelle", "J.H. Piater"], "venue": "Ney, RWTH-PHOENIX-Weather: A large vocabulary sign language recognition and translation corpus., in: International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "SignStream: A tool for linguistic and computer vision research on visual-gestural language data", "author": ["C. Neidle", "S. Sclaroff", "V. Athitsos"], "venue": "Behavior Research Methods, Instruments, & Computers 33 (3) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "A linguistic feature vector for the visual interpretation of sign language", "author": ["R. Bowden", "D. Windridge", "T. Kadir", "A. Zisserman", "M. Brady"], "venue": "in: ECCV", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Detecting coarticulation in sign language using conditional random fields", "author": ["R. Yang", "S. Sarkar"], "venue": "in: Proc. Intl. Conf. on Pattern Recognition (ICPR)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Geometric features for improving continuous appearance-based sign language recognition", "author": ["M. Zahedi", "P. Dreuw", "D. Rybach", "T. Deselaers", "H. Ney"], "venue": "in: BMVC", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Sign language recognition using a combination of new vision based features", "author": ["M.M. Zaki", "S.I. Shaheen"], "venue": "Pattern Recognition Letters 32 (4) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Transfer learning in sign language", "author": ["A. Farhadi", "D. Forsyth", "R. White"], "venue": "in: CVPR", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "M", "author": ["P. Dreuw", "D. Rybach", "T. Deselaers"], "venue": "Zahedi, , H. Ney, Speech recognition techniques for a sign language recognition system, in: Proc. Interspeech", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated extraction of signs from continuous sign language sentences using iterated conditional modes", "author": ["S. Nayak", "S. Sarkar", "B. Loeding"], "venue": "in: CVPR", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Upper body detection and tracking in extended signing sequences", "author": ["P. Buehler", "M. Everingham", "D.P. Huttenlocher", "A. Zisserman"], "venue": "International Journal of Computer Vision 95 (2) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic and efficient long term arm and hand tracking for continuous sign language TV broadcasts", "author": ["T. Pfister", "J. Charles", "M. Everingham", "A. Zisserman"], "venue": "in: BMVC", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "in: CVPR", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "Sign language recognition using dynamic time warping and hand shape distance based on histogram of oriented gradient features", "author": ["P. Jangyodsuk", "C. Conly", "V. Athitsos"], "venue": "in: Proc. 7th International Conference on PErvasive Technologies Related to Assistive Environments", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time american sign language recognition using desk and wearable computer based video", "author": ["T. Starner", "J. Weaver", "A. Pentland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 20 (12) ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1998}, {"title": "Parallel hidden Markov models for American Sign Language recognition", "author": ["C. Vogler", "D. Metaxas"], "venue": "in: ICCV", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1999}, {"title": "Exploiting phonological constraints for handshape inference in ASL video", "author": ["A. Thangali", "J.P. Nash", "S. Sclaroff", "C. Neidle"], "venue": "in: CVPR", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Handshapes and movements: Multiple-channel ASL recognition", "author": ["C. Vogler", "D. Metaxas"], "venue": "in: Gesture Workshop", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2003}, {"title": "Product-HMMs for automatic sign language recognition", "author": ["S. Theodorakis", "A. Katsamanis", "P. Maragos"], "venue": "in: ICASSP", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "The use of context in large vocabulary speech recognition", "author": ["J. Odell"], "venue": "Ph.D. thesis, University of Cambridge ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1995}, {"title": "Moving beyond the \u2018beads-on-a-string\u2019 model of speech", "author": ["M. Ostendorf"], "venue": "in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1999}, {"title": "Subword modeling for automatic speech recognition: Past", "author": ["K. Livescu", "E. Fosler-Lussier", "F. Metze"], "venue": "present, and emerging approaches, IEEE Signal Processing Magazine 29 (6) ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "Audiovisual Speech Processing", "author": ["G. Potamianos", "C. Neti", "J. Luettin", "I. Matthews"], "venue": "Cambridge University Press", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "A segment-based audio-visual speech recognizer: Data collection", "author": ["T.J. Hazen", "K. Saenko", "C.-H. La", "J.R. Glass"], "venue": "development, and initial experiments, in: ICMI, ACM", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2004}, {"title": "Multistream articulatory featurebased models for visual speech recognition", "author": ["K. Saenko", "K. Livescu", "J. Glass", "T. Darrell"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 31 (9) ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for recognizing the simultaneous aspects of American Sign Language", "author": ["C. Vogler", "D. Metaxas"], "venue": "Computer Vision and Image Understanding 81 ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2001}, {"title": "Toward scalability in ASL recognition: Breaking down signs into phonemes", "author": ["C. Vogler", "D. Metaxas"], "venue": "in: Gesture Workshop", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1999}, {"title": "Advances in phoneticsbased sub-unit modeling for transcription alignment and sign language recognition", "author": ["V. Pitsikalis", "S. Theodorakis", "C. Vogler", "P. Maragos"], "venue": "in: IEEE CVPR Workshop on Gesture Recognition", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2011}, {"title": "Model-level data-driven sub-units for signs in videos of continuous sign language", "author": ["S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "in: ICASSP", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "A prosodic model of sign language phonology", "author": ["D. Brentari"], "venue": "The MIT Press", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1998}, {"title": "Adapting hidden Markov models for ASL recognition by using three-dimensional computer vision methods", "author": ["C. Vogler", "D. Metaxas"], "venue": "in: IEEE International Conference on Systems, Man and Cybernetics", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1997}, {"title": "Handling movement epenthesis and hand segmentation ambiguities in continuous sign language recognition using nested dynamic programming", "author": ["R. Yang", "S. Sarkar", "B. Loeding"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 32 (3) ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Advances in dynamic-static integration of manual cues for sign language recognition", "author": ["S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "in: The 9th International Gesture Workshop: Gesture in Embodied Communication and Human-Computer Interaction (GW)", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2011}, {"title": "Sign transition modeling and a scalable solution to continuous sign language recognition for real-world applications", "author": ["K. Li", "Z. Zhou", "C.-H. Lee"], "venue": "ACM Transactions on Accessible Computing (TACCESS) 8 (2) ", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "BoostMap: A method for efficient approximate similarity rankings", "author": ["V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios"], "venue": "in: CVPR", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2004}, {"title": "Affine-invariant modeling of shape-appearance images applied on sign language handshape classification", "author": ["A. Roussos", "S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "in: ICIP", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2010}, {"title": "Rapid signer adaptation for continuous sign language recognition using a combined approach of eigenvoices", "author": ["U. Von Agris", "C. Blomer", "K.-F. Kraiss"], "venue": "MLLR, and MAP, in: Proc. Intl. Conf. on Pattern Recognition (ICPR)", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2008}, {"title": "Deciphering gestures with layered meanings and signer adaptation", "author": ["S.C. Ong", "S. Ranganath"], "venue": "in: Proc. IEEE International Conference on Automatic Face and Gesture Recognition", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2004}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["H. Hermansky", "D.P.W. Ellis", "S. Sharma"], "venue": "in: ICASSP", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2000}, {"title": "Probabilistic and bottle-neck features for LVCSR of meetings", "author": ["F. Gr\u00e9zl", "M. Karafi\u00e1t", "S. Kont\u00e1r", "J. Cernocky"], "venue": "in: ICASSP", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2007}, {"title": "Segmental CRF approach to large vocabulary continuous speech recognition", "author": ["G. Zweig", "P. Nguyen"], "venue": "in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative segmental cascades for feature-rich phone recognition", "author": ["H. Tang", "W. Wang", "K. Gimpel", "K. Livescu"], "venue": "in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2015}, {"title": "H", "author": ["G. Zweig", "P. Nguyen", "D. Van Compernolle", "K. Demuynck", "L. Atlas", "P. Clark", "G. Sell", "M. Wang", "F. Sha"], "venue": "Hermansky, et al., Speech recognition with segmental conditional random fields: A summary of the jhu clsp 2010 summer workshop, in: ICASSP", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient segmental conditional random fields for phone recognition", "author": ["Y. He", "E. Fosler-Lussier"], "venue": "in: Proc. Interspeech", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-Markov conditional random fields for information extraction", "author": ["S. Sarawagi", "W.W. Cohen"], "venue": "in: NIPS", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2004}, {"title": "Activity recognition and abnormality detection with the switching hidden semi-Markov model", "author": ["T.V. Duong", "H.H. Bui", "D.Q. Phung", "S. Venkatesh"], "venue": "in: CVPR", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2005}, {"title": "Sign language spotting based on semi- Markov conditional random field", "author": ["S.-S. Cho", "H.-D. Yang", "S.-W. Lee"], "venue": "in: Workshop on the Applications of Computer Vision", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2009}, {"title": "P", "author": ["H. Sloetjes"], "venue": "Wittenburg, Annotation by category: ELAN and ISO DCR., in: International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2008}, {"title": "Coarticulation in ASL fingerspelling", "author": ["J. Keane", "D. Brentari", "J. Riggle"], "venue": "in: Annual Meeting of the North East Linguistic Society", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2012}, {"title": "Handshape and coarticulation in ASL fingerspelling", "author": ["J. Keane", "D. Brentari", "J. Riggle"], "venue": "conference presentation, linguistic Society of America 2012 Annual Meeting ", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2012}, {"title": "Dispelling prescriptive rules in ASL fingerspelling: the case of -E", "author": ["J. Keane", "D. Brentari", "J. Riggle"], "venue": "poster, theoretical Issues in Sign Language Research 11; London, UK ", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2013}, {"title": "A", "author": ["S. St\u00fcker", "F. Metze", "T. Schultz"], "venue": "Waibel, Integrating multilingual articulatory features into speech recognition., in: Proc. Interspeech", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2003}, {"title": "An articulatory feature-based tandem approach and factored observation modeling", "author": ["O. \u00c7etin", "A. Kantor", "S. King", "C. Bartels", "M. Magimai-doss", "J. Frankel", "K. Livescu"], "venue": "in: ICASSP", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2007}, {"title": "CSR-III text", "author": ["D. Graff", "R. Rosenfeld", "D. Paul"], "venue": "http://http://www.ldc. upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC95T6 ", "citeRegEx": "87", "shortCiteRegEx": null, "year": 1995}, {"title": "Deep segmental neural networks for speech recognition", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu", "H. Jiang"], "venue": "in: Proc. Interspeech", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental conditional random fields with deep neural networks as acoustic models for first-pass word recognition", "author": ["Y. He", "E. Fosler-Lussier"], "venue": "in: Proc. Interspeech", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker adaptation of context dependent deep neural networks", "author": ["H. Liao"], "venue": "in: ICASSP", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "in: ICASSP", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P. Swietojanski", "S. Renals"], "venue": "in: Proc. IEEE Workshop on Spoken Language Technology (SLT)", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition", "author": ["R. Doddipatla", "M. Hasan", "T. Hain"], "venue": "in: Proc. Interspeech", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system", "author": ["J. Neto", "L. Almeida", "M. Hochberg", "C. Martins", "L. Nunes", "S. Renals", "T. Robinson"], "venue": "in: Proc. Eurospeech", "citeRegEx": "94", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["K. Yao", "D. Yu", "F. Seide", "H. Su", "L. Deng", "Y. Gong"], "venue": "in: Proc. IEEE Workshop on Spoken Language Technology (SLT)", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems", "author": ["B. Li", "K.C. Sim"], "venue": "in: Proc. Interspeech", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2010}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M.J. Jones"], "venue": "in: CVPR", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2001}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton"], "venue": "in: ICASSP", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "Journal of Machine Learing Research 15 ", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2014}, {"title": "SRILM at sixteen: update and outlook", "author": ["A. Stolcke", "J. Zheng", "W. Wang", "V. Abrash"], "venue": "in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2012}, {"title": "keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras ", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In the US, there are about 350,000\u2013500,000 people for whom American Sign Language (ASL) is the primary language [1].", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "Reproduced from [2].", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Sign language handshape has its own phonology that has been studied and enjoys a broadly agreed-upon understanding relative to the other manual parameters of movement and place of articulation [3].", "startOffset": 193, "endOffset": 196}, {"referenceID": 3, "context": "Recent linguistic work on sign language phonology has developed approaches based on articulatory features, related to motions of parts of the hand [4, 2].", "startOffset": 147, "endOffset": 153}, {"referenceID": 1, "context": "Recent linguistic work on sign language phonology has developed approaches based on articulatory features, related to motions of parts of the hand [4, 2].", "startOffset": 147, "endOffset": 153}, {"referenceID": 4, "context": "At the same time, computer vision research has studied pose estimation and tracking of hands [5], but usually not in the context of a grammar that constrains the motion.", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "Overall, fingerspelling comprises 12-35% of ASL [6], depending on the context, and includes 72% of the handshapes used in ASL [7].", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "Overall, fingerspelling comprises 12-35% of ASL [6], depending on the context, and includes 72% of the handshapes used in ASL [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 38, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [41, 66, 10].", "startOffset": 183, "endOffset": 195}, {"referenceID": 63, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [41, 66, 10].", "startOffset": 183, "endOffset": 195}, {"referenceID": 9, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [41, 66, 10].", "startOffset": 183, "endOffset": 195}, {"referenceID": 20, "context": "The first recognizer we consider is based on the popular tandem approach to speech recognition [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "ditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies, or signer-dependent applications [8, 9, 10] (see Section 2 for more on related work).", "startOffset": 126, "endOffset": 136}, {"referenceID": 8, "context": "ditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies, or signer-dependent applications [8, 9, 10] (see Section 2 for more on related work).", "startOffset": 126, "endOffset": 136}, {"referenceID": 9, "context": "ditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies, or signer-dependent applications [8, 9, 10] (see Section 2 for more on related work).", "startOffset": 126, "endOffset": 136}, {"referenceID": 13, "context": "[14] and Ong and Ranganath [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[14] and Ong and Ranganath [15].", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 16, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 17, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 18, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 19, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 20, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 21, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 22, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 23, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 24, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 25, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 26, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 27, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 28, "context": "Some of the largest data collection efforts have been for European languages, such as the Dicta-Sign [29] and SignSpeak [30] projects.", "startOffset": 101, "endOffset": 105}, {"referenceID": 29, "context": "Some of the largest data collection efforts have been for European languages, such as the Dicta-Sign [29] and SignSpeak [30] projects.", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "For American Sign Language, the American Sign Language Lexicon Video Dataset (ASLLVD) [27, 28, 31] includes recordings of almost 3,000 isolated signs and can be searched via a queryby-example interface.", "startOffset": 86, "endOffset": 98}, {"referenceID": 27, "context": "For American Sign Language, the American Sign Language Lexicon Video Dataset (ASLLVD) [27, 28, 31] includes recordings of almost 3,000 isolated signs and can be searched via a queryby-example interface.", "startOffset": 86, "endOffset": 98}, {"referenceID": 30, "context": "The National Center for Sign Language and Gesture Resources (NCSLGR) Corpus includes videos of continuous ASL signing, with over 10,000 sign tokens including about 1,500 fingerspelling sequences, annotated using the SignStream linguistic annotation tool [32, 33].", "startOffset": 254, "endOffset": 262}, {"referenceID": 31, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 135, "endOffset": 151}, {"referenceID": 32, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 135, "endOffset": 151}, {"referenceID": 33, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 135, "endOffset": 151}, {"referenceID": 34, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 135, "endOffset": 151}, {"referenceID": 35, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 200, "endOffset": 215}, {"referenceID": 36, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 200, "endOffset": 215}, {"referenceID": 37, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 200, "endOffset": 215}, {"referenceID": 3, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 200, "endOffset": 215}, {"referenceID": 38, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 233, "endOffset": 241}, {"referenceID": 39, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 233, "endOffset": 241}, {"referenceID": 40, "context": "In this work, we are aiming at relatively small motions that are difficult to track a priori, and therefore begin with general image appearance features based on histograms of oriented gradients (HOG) [43], which have also been used in other recent sign language recognition", "startOffset": 201, "endOffset": 205}, {"referenceID": 10, "context": "2Parts of this work have appeared in our conference papers [11, 12, 13].", "startOffset": 59, "endOffset": 71}, {"referenceID": 11, "context": "2Parts of this work have appeared in our conference papers [11, 12, 13].", "startOffset": 59, "endOffset": 71}, {"referenceID": 12, "context": "2Parts of this work have appeared in our conference papers [11, 12, 13].", "startOffset": 59, "endOffset": 71}, {"referenceID": 13, "context": "work [14, 44].", "startOffset": 5, "endOffset": 13}, {"referenceID": 41, "context": "work [14, 44].", "startOffset": 5, "endOffset": 13}, {"referenceID": 42, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [45, 46, 39, 14], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 43, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [45, 46, 39, 14], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 36, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [45, 46, 39, 14], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 13, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [45, 46, 39, 14], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 36, "context": "Ney and colleagues have shown that it is possible to borrow many of the standard HMM-based techniques from automatic speech recognition to obtain good performance on naturalistic German Sign Language videos [39, 14].", "startOffset": 207, "endOffset": 215}, {"referenceID": 13, "context": "Ney and colleagues have shown that it is possible to borrow many of the standard HMM-based techniques from automatic speech recognition to obtain good performance on naturalistic German Sign Language videos [39, 14].", "startOffset": 207, "endOffset": 215}, {"referenceID": 32, "context": "In addition, there have been efforts involving conditional models [35] and more complex (non-linear-chain) models [47, 48, 49].", "startOffset": 66, "endOffset": 70}, {"referenceID": 44, "context": "In addition, there have been efforts involving conditional models [35] and more complex (non-linear-chain) models [47, 48, 49].", "startOffset": 114, "endOffset": 126}, {"referenceID": 45, "context": "In addition, there have been efforts involving conditional models [35] and more complex (non-linear-chain) models [47, 48, 49].", "startOffset": 114, "endOffset": 126}, {"referenceID": 46, "context": "In addition, there have been efforts involving conditional models [35] and more complex (non-linear-chain) models [47, 48, 49].", "startOffset": 114, "endOffset": 126}, {"referenceID": 47, "context": "For acoustic speech, the most commonly used unit is the context-dependent phoneme [50], although other choices such as syllables, articulatory features, and automatically learned units have also been considered [51, 52].", "startOffset": 82, "endOffset": 86}, {"referenceID": 48, "context": "For acoustic speech, the most commonly used unit is the context-dependent phoneme [50], although other choices such as syllables, articulatory features, and automatically learned units have also been considered [51, 52].", "startOffset": 211, "endOffset": 219}, {"referenceID": 49, "context": "For acoustic speech, the most commonly used unit is the context-dependent phoneme [50], although other choices such as syllables, articulatory features, and automatically learned units have also been considered [51, 52].", "startOffset": 211, "endOffset": 219}, {"referenceID": 50, "context": "As the research community started to consider visual speech recognition (\u201clipreading\u201d), analogous units have been explored: visemes, articulatory features, and automatic clusters [53, 54, 55].", "startOffset": 179, "endOffset": 191}, {"referenceID": 51, "context": "As the research community started to consider visual speech recognition (\u201clipreading\u201d), analogous units have been explored: visemes, articulatory features, and automatic clusters [53, 54, 55].", "startOffset": 179, "endOffset": 191}, {"referenceID": 52, "context": "As the research community started to consider visual speech recognition (\u201clipreading\u201d), analogous units have been explored: visemes, articulatory features, and automatic clusters [53, 54, 55].", "startOffset": 179, "endOffset": 191}, {"referenceID": 31, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 3, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 44, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 45, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 53, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 54, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 43, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 55, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 56, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 57, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 131, "endOffset": 138}, {"referenceID": 1, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 131, "endOffset": 138}, {"referenceID": 58, "context": "One of the unique aspects of sign language is that transitional movements occupy a larger portion of the signal than steady states, and some researchers have developed approaches for explicitly modeling the transitions as units [61, 62, 63, 64].", "startOffset": 228, "endOffset": 244}, {"referenceID": 59, "context": "One of the unique aspects of sign language is that transitional movements occupy a larger portion of the signal than steady states, and some researchers have developed approaches for explicitly modeling the transitions as units [61, 62, 63, 64].", "startOffset": 228, "endOffset": 244}, {"referenceID": 60, "context": "One of the unique aspects of sign language is that transitional movements occupy a larger portion of the signal than steady states, and some researchers have developed approaches for explicitly modeling the transitions as units [61, 62, 63, 64].", "startOffset": 228, "endOffset": 244}, {"referenceID": 61, "context": "One of the unique aspects of sign language is that transitional movements occupy a larger portion of the signal than steady states, and some researchers have developed approaches for explicitly modeling the transitions as units [61, 62, 63, 64].", "startOffset": 228, "endOffset": 244}, {"referenceID": 62, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 108, "endOffset": 124}, {"referenceID": 63, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 108, "endOffset": 124}, {"referenceID": 44, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 108, "endOffset": 124}, {"referenceID": 17, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 108, "endOffset": 124}, {"referenceID": 7, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 165, "endOffset": 175}, {"referenceID": 8, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 165, "endOffset": 175}, {"referenceID": 9, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 165, "endOffset": 175}, {"referenceID": 13, "context": "The problem of signer adaptation has been addressed in prior work, using techniques borrowed from speaker adaptation for speech recognition, such as maximum likelihood linear regression and maximum a posteriori estimation [14, 67, 68], and to explicitly study the contrast between signer-dependent and multi-signer fingerspelling recognition.", "startOffset": 222, "endOffset": 234}, {"referenceID": 64, "context": "The problem of signer adaptation has been addressed in prior work, using techniques borrowed from speaker adaptation for speech recognition, such as maximum likelihood linear regression and maximum a posteriori estimation [14, 67, 68], and to explicitly study the contrast between signer-dependent and multi-signer fingerspelling recognition.", "startOffset": 222, "endOffset": 234}, {"referenceID": 65, "context": "The problem of signer adaptation has been addressed in prior work, using techniques borrowed from speaker adaptation for speech recognition, such as maximum likelihood linear regression and maximum a posteriori estimation [14, 67, 68], and to explicitly study the contrast between signer-dependent and multi-signer fingerspelling recognition.", "startOffset": 222, "endOffset": 234}, {"referenceID": 66, "context": "Our HMM baselines are tandem models, analogous to ones developed for speech recognition [69, 70].", "startOffset": 88, "endOffset": 96}, {"referenceID": 67, "context": "Our HMM baselines are tandem models, analogous to ones developed for speech recognition [69, 70].", "startOffset": 88, "endOffset": 96}, {"referenceID": 68, "context": "The segmental models we propose are related to segmental conditional random fields (SCRFs) and their variants, which have now been applied fairly widely for speech recognition [71, 72, 73, 74, 75].", "startOffset": 176, "endOffset": 196}, {"referenceID": 69, "context": "The segmental models we propose are related to segmental conditional random fields (SCRFs) and their variants, which have now been applied fairly widely for speech recognition [71, 72, 73, 74, 75].", "startOffset": 176, "endOffset": 196}, {"referenceID": 70, "context": "The segmental models we propose are related to segmental conditional random fields (SCRFs) and their variants, which have now been applied fairly widely for speech recognition [71, 72, 73, 74, 75].", "startOffset": 176, "endOffset": 196}, {"referenceID": 71, "context": "The segmental models we propose are related to segmental conditional random fields (SCRFs) and their variants, which have now been applied fairly widely for speech recognition [71, 72, 73, 74, 75].", "startOffset": 176, "endOffset": 196}, {"referenceID": 72, "context": "In natural language processing, semi-Markov CRFs have been used for named entity recognition [76], where the labeling is binary.", "startOffset": 93, "endOffset": 97}, {"referenceID": 73, "context": "Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [77, 78] with a small set of possible activities to choose from, including work on spotting and recognition of a small vocabulary of (non-fingerspelled) signs in sign language video [79] or instrumented data capture [80].", "startOffset": 120, "endOffset": 128}, {"referenceID": 74, "context": "Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [77, 78] with a small set of possible activities to choose from, including work on spotting and recognition of a small vocabulary of (non-fingerspelled) signs in sign language video [79] or instrumented data capture [80].", "startOffset": 302, "endOffset": 306}, {"referenceID": 68, "context": "In prior speech recognition work, this computational difficulty has often been addressed by adopting a lattice rescoring approach, where a frame-based model such as an HMM system generates first-pass lattices and a segmental model rescores them [71, 74].", "startOffset": 245, "endOffset": 253}, {"referenceID": 70, "context": "In prior speech recognition work, this computational difficulty has often been addressed by adopting a lattice rescoring approach, where a frame-based model such as an HMM system generates first-pass lattices and a segmental model rescores them [71, 74].", "startOffset": 245, "endOffset": 253}, {"referenceID": 69, "context": "We compare this approach to an efficient first-pass segmental model [72].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "A full listing of the word lists can be found in [2].", "startOffset": 49, "endOffset": 52}, {"referenceID": 75, "context": "For purposes of annotation, the video files were processed with FFMPEG to deinterlace, crop, resize, and reencode them for compatibility with the ELAN annotation software [81].", "startOffset": 171, "endOffset": 175}, {"referenceID": 76, "context": "Several studies [82, 83, 84, 2] have been conducted looking at how frequently FSletters are realized canonically (i.", "startOffset": 16, "endOffset": 31}, {"referenceID": 77, "context": "Several studies [82, 83, 84, 2] have been conducted looking at how frequently FSletters are realized canonically (i.", "startOffset": 16, "endOffset": 31}, {"referenceID": 78, "context": "Several studies [82, 83, 84, 2] have been conducted looking at how frequently FSletters are realized canonically (i.", "startOffset": 16, "endOffset": 31}, {"referenceID": 1, "context": "Several studies [82, 83, 84, 2] have been conducted looking at how frequently FSletters are realized canonically (i.", "startOffset": 16, "endOffset": 31}, {"referenceID": 77, "context": ", -C-), some almost never (-D-), while others have 75%\u2013 85% ( -E- or -O- respectively) canonical peak handshape realizations [83].", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "For example, the pinky extension property of FS-letters has been shown to spread from peak handshapes that have an extended pinky to the ones before and after [2].", "startOffset": 159, "endOffset": 162}, {"referenceID": 79, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, and there is evidence from speech recognition research that these may be useful in low-data settings [85, 86].", "startOffset": 185, "endOffset": 193}, {"referenceID": 80, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, and there is evidence from speech recognition research that these may be useful in low-data settings [85, 86].", "startOffset": 185, "endOffset": 193}, {"referenceID": 66, "context": "Tandem model The first recognizer we consider is a fairly typical tandem model [69, 70].", "startOffset": 79, "endOffset": 87}, {"referenceID": 67, "context": "Tandem model The first recognizer we consider is a fairly typical tandem model [69, 70].", "startOffset": 79, "endOffset": 87}, {"referenceID": 81, "context": "For this work, the language model is trained using ARPA CSR-III text, which includes English words and names [87].", "startOffset": 109, "endOffset": 113}, {"referenceID": 2, "context": "Table 2: Definition and possible values for phonological features based on [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "For detailed descriptions, see [3].", "startOffset": 31, "endOffset": 34}, {"referenceID": 72, "context": "SCRFs [76, 71] are conditional log-linear models with feature functions that can be based on variable-length segments", "startOffset": 6, "endOffset": 14}, {"referenceID": 68, "context": "SCRFs [76, 71] are conditional log-linear models with feature functions that can be based on variable-length segments", "startOffset": 6, "endOffset": 14}, {"referenceID": 11, "context": "We use the same feature functions as in [12], described here for completeness.", "startOffset": 40, "endOffset": 44}, {"referenceID": 82, "context": "These are similar to features used in prior work on SCRFs for ASR with DNN-based feature functions [88, 89, 72].", "startOffset": 99, "endOffset": 111}, {"referenceID": 83, "context": "These are similar to features used in prior work on SCRFs for ASR with DNN-based feature functions [88, 89, 72].", "startOffset": 99, "endOffset": 111}, {"referenceID": 69, "context": "These are similar to features used in prior work on SCRFs for ASR with DNN-based feature functions [88, 89, 72].", "startOffset": 99, "endOffset": 111}, {"referenceID": 68, "context": "To take advantage of the already high-quality baseline that generated the lattices, we use a baseline feature like the one in [71], which is based on the 1-best output from the baseline tandem recognizer.", "startOffset": 126, "endOffset": 130}, {"referenceID": 69, "context": "[72], and the same feature functions as in [72], namely average DNN outputs over each segment, samples of DNN outputs within the segment, boundaries of DNN outputs in each segment, duration and bias.", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "[72], and the same feature functions as in [72], namely average DNN outputs over each segment, samples of DNN outputs within the segment, boundaries of DNN outputs in each segment, duration and bias.", "startOffset": 43, "endOffset": 47}, {"referenceID": 84, "context": ", [90, 91, 92, 93]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 85, "context": ", [90, 91, 92, 93]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 86, "context": ", [90, 91, 92, 93]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 87, "context": ", [90, 91, 92, 93]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 88, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [94, 95, 96].", "startOffset": 96, "endOffset": 108}, {"referenceID": 89, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [94, 95, 96].", "startOffset": 96, "endOffset": 108}, {"referenceID": 90, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [94, 95, 96].", "startOffset": 96, "endOffset": 108}, {"referenceID": 8, "context": "Hand localization and segmentation As in prior work [9, 11, 12], we used a simple signer-dependent model for hand detection.", "startOffset": 52, "endOffset": 63}, {"referenceID": 10, "context": "Hand localization and segmentation As in prior work [9, 11, 12], we used a simple signer-dependent model for hand detection.", "startOffset": 52, "endOffset": 63}, {"referenceID": 11, "context": "Hand localization and segmentation As in prior work [9, 11, 12], we used a simple signer-dependent model for hand detection.", "startOffset": 52, "endOffset": 63}, {"referenceID": 91, "context": "First, we suppress pixels that fall within regions detected as faces by the Viola-Jones face detector [97], since these tend to be false positives.", "startOffset": 102, "endOffset": 106}, {"referenceID": 40, "context": "Handshape descriptors We use histograms of oriented gradients (HOG [43]) as the visual descriptor (feature vector) for a given hand region.", "startOffset": 67, "endOffset": 71}, {"referenceID": 92, "context": "The DNNs have three hidden layers, each with 3000 ReLUs [98].", "startOffset": 56, "endOffset": 60}, {"referenceID": 93, "context": "Network learning is done with cross-entropy training with a weight decay penalty of 10\u22125, via stochastic gradient descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [99] at a rate of 0.", "startOffset": 190, "endOffset": 194}, {"referenceID": 94, "context": "We use HTK [101] to implement the baseline HMM-based recognizers and SRILM [102] to train the language models.", "startOffset": 75, "endOffset": 80}, {"referenceID": 10, "context": "6See [11, 12, 72] for details of the tuning parameters.", "startOffset": 5, "endOffset": 17}, {"referenceID": 11, "context": "6See [11, 12, 72] for details of the tuning parameters.", "startOffset": 5, "endOffset": 17}, {"referenceID": 69, "context": "6See [11, 12, 72] for details of the tuning parameters.", "startOffset": 5, "endOffset": 17}, {"referenceID": 1, "context": "We also consider an alternative sub-letter feature set, in particular a set of phonetic features introduced by Keane [2], whose feature values are listed in Section Appendix A, Tab.", "startOffset": 117, "endOffset": 120}, {"referenceID": 10, "context": "In contrast, in earlier work [11] we found that phonological features outperform FSletters in the tandem HMM.", "startOffset": 29, "endOffset": 33}, {"referenceID": 95, "context": ", [103, 104]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 96, "context": "We implemented the CNNs using Keras [105] with Theano [106].", "startOffset": 36, "endOffset": 41}, {"referenceID": 2, "context": "We compare: FS-letter only, phonetic features only, FS-letters + phonological features [3] and FS-letters + phonetic features [2].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "We compare: FS-letter only, phonetic features only, FS-letters + phonological features [3] and FS-letters + phonetic features [2].", "startOffset": 126, "endOffset": 129}, {"referenceID": 69, "context": "We follow the discriminative segmental cascades (DSC) approach of [72], where a simpler first-pass SCRF is used for lattice generation and a second SCRF, with more computationally demanding features, is used for rescoring.", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "9: Phonetic features [2].", "startOffset": 21, "endOffset": 24}], "year": 2016, "abstractText": "We study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL. Recognizing fingerspelling is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work we collect and annotate a new data set of continuous fingerspelling videos, compare several types of recognizers, and explore the problem of signer variation. Our best-performing models are segmental (semiMarkov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 92% letter accuracy. The multi-signer setting is much more challenging, but with neural network adaptation we achieve up to 83% letter accuracies in this setting.", "creator": "LaTeX with hyperref package"}}}