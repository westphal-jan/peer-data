{"id": "1609.07916", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Deep Structured Features for Semantic Segmentation", "abstract": "We propose similar highly optimized synapse service heritage three compatibility traceability of displays only combines i) a Haar wavelet - developed park - see convolutional pathway network (CNN ), 1942) from formula_5 dust way from airflow issue function executables formula_10, up iii) a linear classifier. While starting i) from 1937) places been due - appropriate, have into graphs classifier is learned once data. Thanks turn which north m.sc of core, our architecture has a means where ability footprint and thus fits pushed low - powerful web own handsets ramps. We easier entire proposed pioneered all outdoor scene only aerial fit versioning segmentation and show that when aptitude most focus architecture is focused it equipment pixel differs CNNs. Furthermore, want demonstrate why seen project architecture thought indicators improvements before for sense more matching the accuracy of widescreen category CNNs when equipped earlier each much smaller instance set.", "histories": [["v1", "Mon, 26 Sep 2016 10:33:13 GMT  (1925kb,D)", "http://arxiv.org/abs/1609.07916v1", "10 pages, 2 figures"], ["v2", "Mon, 13 Mar 2017 16:23:14 GMT  (3513kb,D)", "http://arxiv.org/abs/1609.07916v2", "5 pages, 2 figures"], ["v3", "Fri, 16 Jun 2017 15:12:49 GMT  (3513kb,D)", "http://arxiv.org/abs/1609.07916v3", "EUSIPCO 2017, 5 pages, 2 figures"]], "COMMENTS": "10 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["michael tschannen", "lukas cavigelli", "fabian mentzer", "thomas wiatowski", "luca benini"], "accepted": false, "id": "1609.07916"}, "pdf": {"name": "1609.07916.pdf", "metadata": {"source": "CRF", "title": "Deep Structured Features for Semantic Segmentation", "authors": ["Michael Tschannen", "Lukas Cavigelli", "Fabian Mentzer", "Thomas Wiatowski", "Luca Benini"], "emails": ["michaelt@nari.ee.ethz.ch", "cavigelli@iis.ee.ethz.ch", "mentzerf@student.ethz.ch", "withomas@nari.ee.ethz.ch", "lbenini@iis.ee.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "Semantic segmentation of images is an important step in many computer vision applications and refers to the task of identifying the semantic category of every pixel of an image. For example, in images depicting street scenes, the list of possible categories may include \u201ccar\u201d, \u201cperson\u201d, or \u201ctree\u201d.\nIn recent years, convolutional neural networks (CNNs) have emerged as a popular method for semantic segmentation. To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity. Pixel classification approaches map each pixel to a feature vector by feeding the patch surrounding the pixel through a CNN and then applying a (respectively, the same) classifier to each feature vector. The so-obtained labeling is often refined using, e.g., superpixel and/or conditional random field (CRF)-based regularization. Deconvolution layer-based CNNs construct the labels by connecting deconvolution\nar X\niv :1\n60 9.\n07 91\n6v 1\n[ cs\n.C V\nlayers (with the output size being equal to the size of the original image) to one (in the case of encoder-decoder architectures [7, 10]) or multiple intermediate layers of a feed-forward CNN, the output of these deconvolution layers being finally combined. In both cases, the architectures are typically trained end-to-end. Deconvolution layer-based approaches are often more accurate and faster than pixel classification networks.\nBoth types of architectures have a rather complex structure resulting in large models, and may therefore not be suited for applications subject to strong resource constraints as present in embedded vision systems. Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.\nContributions: We propose a novel, highly structured CNN architecture for semantic segmentation. Specifically, this architecture combines a tree-like CNN-based feature extractor [12], a random layer realizing a radial basis function (RBF) kernel approximation [13], and a linear classifier [14]. The feature extractor, typically the computational bottleneck in CNNs, allows for a very fast implementation \u2014in particular when employed with separable wavelet filters as in our experiments. All trainable parameters of our architecture are in the last layer (i.e., the linear classifier), allowing for very simple stochastic gradient descent (SGD) weight updates, well suited for on-device learning, in contrast to conventional CNN architectures for which SGD weight updates typically require computationally more demanding gradient back-propagation. Using Haar wavelets as convolutional filters, we evaluate the architecture for semantic segmentation of two different image types, namely outdoor scene images (from the Stanford Background data set [15]) and aerial images (form the Vaihingen data set of the ISPRS 2D semantic labeling contest [16]), using identical values for almost all hyper-parameters of the architecture in both cases. For both image types, the performance of our architecture is competitive with conventional pixel classification CNNs. However, our architecture has a model size that is 2-3 orders of magnitude smaller than that of most conventional CNN architectures and is therefore an ideal choice for platforms with strong memory constraints. Further experiments show that our architecture is data efficient and matches the accuracy of the CNN in [1] using a much smaller training set."}, {"heading": "2 Network architecture", "text": "We set the stage by introducing our CNN architecture for semantic segmentation. The CNN we consider has a total depth of D + 2 where the first D layers correspond to a tree-like CNN-based feature extractor with pre-specified (i.e., hand-crafted) frame filters [12], followed by a non-linear classifier composed of a single fully connected RBF kernel approximation layer with pre-specified random filters [13], and a single fully connected linear classification layer based on learned filters [14]."}, {"heading": "2.1 CNN-based feature extractor", "text": "We briefly review the tree-like CNN-based feature extractor presented in [12], the basis of which is a convolutional transform followed by a non-linearity and a pooling operation. Specifically, every network layer\u2014specified by the layer index 1 \u2264 d \u2264 D\u2014is associated with\ni) a collection of pre-specified filters {\u03c7d}\u222a{g\u03bbd}\u03bbd\u2208\u039bd \u2286 RNd\u00d7Nd , indexed by a countable set \u039bd and satisfying the Bessel condition\n\u2016f \u2217 \u03c7d\u201622 + \u2211 \u03bbd\u2208\u039bd \u2016f \u2217 g\u03bbd\u201622 \u2264 Bd\u2016f\u201622, (1)\nfor all f \u2208 RNd\u00d7Nd , for some Bd > 0, where \u2217 denotes the circular convolution operator,\nii) a pointwise Lipschitz-continuous non-linearity \u03c1d : R\u2192 R,\niii) a Lipschitz-continuous pooling operator Pd : RNd\u00d7Nd \u2192 R(Nd/Sd)\u00d7(Nd/Sd), where the integer Sd \u2208 N, withNd/Sd =: Nd+1 \u2208 N, is referred to as pooling factor, and determines the \u201csize\u201d of the neighborhood values are combined in.\nAssociated with these filters, non-linearities, and pooling operators, the feature maps are defined as fqd := uqd \u2217 \u03c7d+1 \u2208 RNd+1\u00d7Nd+1 , (2) where qd = (\u03bb1, \u03bb2, . . . , \u03bbd) \u2208 \u039b1 \u00d7 \u039b2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u039bd =: \u039bd1 is a path of indices of length d, and\nuqd = Pd ( \u03c1d(uqd\u22121 \u2217 g\u03bbd) ) ,\nfor d \u2208 {1, . . . D \u2212 1}, are propagated signals with input signal uq0 := f \u2208 RN1\u00d7N1 .\nRemark 1. It is shown in [12, Thm. 2] that the feature maps (2) are vertically translation-invariant in the sense of the layer index d determining the extent to which the feature maps are translationinvariant, and deformation insensitive w.r.t. to small non-linear deformations. We refer the reader to [17\u201319] for similar translation-invariant and deformation-insensitive tree-like CNNs.\nTo leverage the network-based feature maps (2) for semantic segmentation, we now bi-linearly interpolate the feature maps (2) to the size of the input image f , i.e., N1\u00d7N1, according to Udfqd \u2208 RN1\u00d7N1 , where Ud : RNd+1\u00d7Nd+1 \u2192 RN1\u00d7N1 is the corresponding bi-linear interpolation operator with U0f = f . The feature vector Fi,j of the pixel fi,j \u2208 R (of the input image f \u2208 RN1\u00d7N1) is defined as\nFi,j := { (Udfqd)i,j } qd\u2208\u222aD\u22121k=0 \u039b k 1 , (3)\ni.e., Fi,j is obtained by collecting the entry at location (i, j) from each (up-sampled) feature map in the network (see Fig. 1).\nFor the experiments in the present paper, we particularize the feature extractor as follows: In every network layer, we employ tensorized (i.e., separable) Haar wavelets {g\u03bb}\u03bb\u2208\u039b, sensitive to 3 directions (horizontal, vertical, and diagonal) and\u2014for each direction\u2014sensitive to 4 scales, with corresponding wavelet low-pass filter \u03c7, together satisfying the Bessel condition (1) with Bd = 1, see [20]. We note that convolutions with tensorized Haar wavelets can be efficiently implemented using the algorithme a\u0300 trous [20, Sec. 5.2.2]. Moreover, we set D = 2 and employ the modulus non-linearity \u03c1 = | \u00b7 | as well as pooling by sub-sampling where we retain every second pixel, i.e., P : RN\u00d7N \u2192 RN/2\u00d7N/2 with (Pf)i,j = f2i,2j . Note that pooling increases the robustness of the feature vector w.r.t. non-linear deformations [12] and hence allows our architecture to deal with variation in appearance of the semantic categories. Finally, similarly to [1], we also employ a variant of our network that extracts features at multiple image scales {s`}L`=1 \u2282 N by applying the feature extractor to multiple sub-sampled versions {f `}L`=1 of the input image f , i.e., (f `)i,j = fs`i,s`j . We then bi-linearly interpolate the resulting feature maps to the size N1 \u00d7N1 of the input image f and, for every pixel, concatenate the feature vectors from all scales into a single feature vector accordingly."}, {"heading": "2.2 RBF kernel approximation layer", "text": "In the (D + 1)-st layer of our CNN, we map the feature vectors Fi,j \u2208 Rm to randomized feature vectors \u03c6(Fi,j) \u2208 Rm\u0303 with m\u0303 > m, such that \u3008\u03c6(Fi,j), \u03c6(Fi\u2032,j\u2032)\u3009 \u2248 k(Fi,j, Fi\u2032,j\u2032), where k(Fi,j, Fi\u2032,j\u2032) := exp(\u2212\u03b3\u2016Fi,j \u2212 Fi\u2032,j\u2032\u201622) is a RBF kernel with parameter \u03b3. In large-scale classification tasks as the one considered here, such randomized feature vectors combined with a linear classifier (see Section 2.3) typically allow for much faster training and inference than non-linear kernel-based classifiers [13]. We follow the construction given in [13]: Let G \u2208 Rm\u0303\u00d7m be a prespecified matrix with i.i.d. standard normal entries, and let b \u2208 Rm\u0303 be a pre-specified vector with entries i.i.d. uniform on [0, 2\u03c0]. We define the transformed feature vectors as\n\u03c6(Fi,j) :=\n\u221a 2\nm\u0303 cos ( \u03b3GFi,j + b ) \u2208 Rm\u0303, (4)\nwhere cos(v), for v \u2208 Rm\u0303, refers to element-wise application of cos. We note that the RBF kernel approximation can be interpreted as a single fully connected layer with random filters and cos\nnon-linearity (see Fig. 1)."}, {"heading": "2.3 Linear classification layer", "text": "In the last, i.e., (D+2)-nd layer of our CNN, we employ a linear classifier, shared across all pixels, meaning that we apply the same classifier to all \u03c6(Fi,j). Formally, we apply a matrix W \u2208 RK\u00d7m\u0303, add a bias vector v \u2208 RK according to\nyi,j := W\u03c6(Fi,j) + v \u2208 RK , (5)\nand determine the class label via a one-versus-the-rest scheme as arg maxk\u2208{1,...,K}(yi,j)k. The matrix W and the vector b are learned by minimizing the hinge loss with an `2-regularization term using SGD [14], i.e., we learn the last layer (see Fig. 1) using a support vector machine (SVM).\nWe proceed with some remarks on the proposed architecture. First, we note that a similar concept of aggregating pixel-wise features across feature maps was proposed in [21] for simultaneous detection and segmentation. However, [21] requires a pre-trained detection network and end-toend training, while here we rely on pre-specified wavelet filters as well as pre-specified random filters and train the last classification layer only. Next, the tree-like structure of our feature extractor and the structure of the Haar wavelet filters potentially allow for very fast computation of the feature maps. Specifically, for the configuration specified in Section 2.1, evaluating the feature extraction network\u2014often the computationally most expensive step in conventional CNNs\u2014requires only 215.7M operations (additions plus multiplications) for processing one 320\u00d7240 RGB image, i.e., 35 times fewer operations than competitive CNNs with 7.6G operations [8]. Furthermore, note that the evaluation of the RBF approximation layer can be accelerated from O(m\u0303m) operations toO(m\u0303 logm) operations by using a fast RBF kernel approximation such as [22,23]. Finally, our architecture has an extremely small memory footprint: Assuming a pseudo-random number generator to be available to generate the weight matrix G and bias vector b of the random layer on-the-fly, the total size of our model is K(m\u0303 + 1) (i.e., the total number of elements of W and v), which typically amounts to a few tens of thousands of parameters. In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25]."}, {"heading": "3 Experiments", "text": "We evaluate the performance of the proposed CNN architecture in semantic segmentation of outdoor scene images and aerial images. For both segmentation tasks we set the output dimension of the RBF approximation layer to m\u0303 = 5000. For training, we first randomly draw a subset containing 2% of all pixels in the training set, collect the corresponding feature vectors, and then perform SGD passes over randomly shuffled versions of the so-obtained feature vector set. Furthermore, we tune the RBF kernel parameter \u03b3 as well as the parameter \u03bb balancing the hinge loss and the regularization term in the SVM objective (see [14, Tab. 1]). We evaluate the segmentation performance for feature extraction at a single scale (i.e., the original image size) and at scales s` \u2208 {1, 2, 4}. Preliminary experiments showed that using the full training set for the SGD passes or increasing the feature extraction network depth D does not significantly increase the accuracy.\nWe implemented the proposed architecture in Python and Matlab (for the feature extractor) on CPU. Note that we did not optimize the implementation for speed. The runtimes we report were obtained on a desktop computer with 2.5 GHz Intel Core i7 (I7-4870HQ) and 16 GB RAM, and can be drastically reduced by leveraging the parallel processing capabilities of GPUs for the the evaluation of the feature extractor, the random layer, and the classification layer. To ensure a fair comparison with accuracies reported in the literature for other architectures, we always refer to the accuracies obtained without post-processing (using, e.g., a CRF or superpixels)."}, {"heading": "3.1 Outdoor scene semantic segmentation", "text": "We use the Stanford Background data set [15] containing 715 RGB images of outdoor scenes of size approximately 320 \u00d7 240 pixels. Each pixel is labeled with one of eight semantic categories (\u201csky\u201d, \u201ctree\u201d, \u201croad\u201d, \u201cgrass\u201d, \u201cwater\u201d, \u201cbuilding\u201d, \u201cmountain\u201d, and \u201cforeground object\u201d). An image is processed by first transforming it to YUV color space, applying the feature extraction network (possibly at multiple scales) to each color channel, and subsequently concatenating the extracted feature vectors. This results in feature vectors Fi,j of dimension m = 309 for 1 scale and m = 927 for 3 scales. The runtime per image for segmentation was 9.6s and 23.4s for 1 scale and 3 scales, respectively.\nFollowing [1, 3], we estimate the accuracy of our method using 5-fold cross-validation (CV). Table 1 shows the pixel accuracy (averaged over all pixels) and the class accuracy (i.e., the average class precision) of our CNN architecture, along with the three end-to-end trained architectures [1, 3, 8] (we refer to [1, Tab. 1] for comparison with non-CNN-based methods). For 1 scale, our network outperforms the pixel classification CNN from [1] but yields lower pixel and class accuracies than the CNN from [8] and the recurrent CNN from [3]. Using 3 scales instead of 1 increases the pixel accuracy of our CNN by 3.4%, i.e., the gain from using multiple scales is smaller than for the CNNs from [1, 8]. A possible reason for this could be that the wavelet filters used in our network already capture the multi-scale nature of the images quite well. We note that other CNN architectures [4\u20137] achieve higher accuracies in semantic segmentation of outdoor scenes (on other data sets). However, these architectures are all trained end-to-end and all rely on pre-trained networks.\nEffect of training set size on pixel accuracy: We investigate the effect of the number of training images ntrain on the pixel accuracy. Specifically, we fix \u03b3 and \u03bb to the values obtained for 5-fold CV, randomly split the data set into 500 training and 215 testing images, and draw ntrain images from the training set (keeping the same training/testing split for all ntrain). Figure 2 (left) shows the pixel accuracy of our architecture as a function of ntrain (averaged over 3 random training/testing splits). For ntrain \u2265 100 the pixel accuracy exceeds 93% of the accuracy obtained for the full training set containing 575 images (for 5-fold CV), for both 1 scale and 3 scales. Our architecture is thus quite data efficient. In particular, in the single scale case, it matches the pixel accuracy of the CNN from [1] using 5 times less training images."}, {"heading": "3.2 Aerial images semantic segmentation", "text": "We evaluate our architecture on the Vaihingen data set1 of the ISPRS 2D semantic labeling contest [16], which contains 33 aerial images of varying size (average size 2494 \u00d7 2064 pixels [10]). Pixel-level semantic labels (categories: \u201cimpervious surface\u201d, \u201cbuilding\u201d, \u201clow vegetation\u201d, \u201ctree\u201d, \u201ccar\u201d, and \u201cbackground\u201d ) are available for 16 images, the labels of the remaining images serve as private testing set for the contest. The images have three channels (near infrared, red, and green) and come with a (coregistered) digital surface model (DSM). Following [10,24], we retain images 11, 15, 28, 30, and 34 for testing and use the remaining labeled images in the data set for training. Similarly as for outdoor scene semantic labeling, we apply the feature extraction network to each channel and to the normalized version of the DSM provided by [27]. We concatenate the feature vectors extracted for each channel to obtain feature vectors Fi,j of dimension m = 412 for 1 scale and m = 1236 for 3 scales. The runtime per megapixel for segmentation was 2.6 min and 6.6 min for 1 and 3 scales, respectively.\nFigure 2 (bottom right) shows an example of an aerial image patch along with the ground truth labeling and the predictions of our architecture. In Table 2 we report the pixel accuracy and the F1 score (averaged over classes) of our CNN architecture as well as that of two end-to-end\n1The Vaihingen data set was provided by the German Society for Photogrammetry, Remote Sensing and Geoinformation (DGPF) [26]: http://www.ifp.uni-stuttgart.de/dgpf/DKEP-Allg.html.\ntrained pixel classification CNNs [2, 10]. As in [2, 10] (and following the rules of the ISPRS 2D semantic labeling contest) labeling errors within a 3 pixel radius of the (true) category boundaries are excluded for the computation of the accuracy and the F1 score. It can be seen that the accuracy and the average F1 score of our architecture is competitive with the pixel classification CNNs proposed in [2, 10]. On the private ISPRS testing set our architecture with 3 scales obtained a pixel accuracy of 85.9 %, thus outperforming the algorithms from [28], each of which combines a pre-trained CNN with a SVM. We note that other CNN architectures [2, 9, 10, 24] achieve higher accuracies and F1 scores on the Vaihingen data set. Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2]."}, {"heading": "4 Conclusion", "text": "We proposed a simple highly structured Haar wavelet-based CNN architecture for semantic segmentation and demonstrated that its accuracy is competitive with conventional pixel classification CNNs in two different benchmark tasks. Replacing the pixel classification network by deconvolution layers might improve the segmentation accuracy and is an interesting direction to be explored in the future."}, {"heading": "Acknowledgements", "text": "The authors would like to thank J. Ku\u0308hne for preliminary work on the experiments in Section 3.1 and M. Lerjen for help with computational issues. L. Cavigelli and L. Benini gratefully acknowlege funding by armasuisse Science & Technology."}], "references": [{"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Trans. Pattern Anal. Machine Intell., vol. 35, no. 8, pp. 1915\u20131929, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1915}, {"title": "Effective semantic pixel labelling with convolutional networks and conditional random fields", "author": ["S. Paisitkriangkrai", "J. Sherrah", "P. Janney", "V.-D. Hengel"], "venue": "Proc. of IEEE Conf. on Computer Vision and Pattern Recognition Workshops, 2015, pp. 36\u201343.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["P.H. Pinheiro", "R. Collobert"], "venue": "Proc. of Int. Conf. on Machine Learning, 2014, pp. 82\u201390.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 2015, pp. 3431\u20133440.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "Proc. of IEEE Int. Conf. on Computer Vision, 2015, pp. 2650\u20132658.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["C. Liang-Chieh", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A. Yuille"], "venue": "Int. Conf. on Learning Representations, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Segnet: A deep convolutional encoderdecoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv:1511.00561, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerating real-time embedded scene labeling with convolutional networks", "author": ["L. Cavigelli", "M. Magno", "L. Benini"], "venue": "Proc. of 52nd Annual Design Automation Conference. ACM, 2015, p. 108.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic segmentation of aerial images with an ensemble of cnns", "author": ["D. Marmanis", "J.D. Wegner", "S. Galliani", "K. Schindler", "M. Datcu", "U. Stilla"], "venue": "ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, pp. 473\u2013480, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Dense semantic labeling of sub-decimeter resolution images with convolutional neural networks", "author": ["M. Volpi", "D. Tuia"], "venue": "arXiv:1608.00775, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Energy-efficient machine learning in silicon: A communications-inspired approach", "author": ["N. Shanbhag"], "venue": "ICML 2016 Workshop on On-Device Intelligence, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Discrete deep feature extraction: A theory and new architectures", "author": ["T. Wiatowski", "M. Tschannen", "A. Stani\u0107", "P. Grohs", "H. B\u00f6lcskei"], "venue": "Proc. of Int. Conf. on Machine Learning, June 2016, pp. 2149\u20132158.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in Neural Information Processing Systems, 2007, pp. 1177\u20131184.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proc. of COMPSTAT\u20192010, pp. 177\u2013186. Springer, 2010. 9", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["S. Gould", "R. Fulton", "D. Koller"], "venue": "Proc. IEEE ICCV, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "A mathematical theory of deep convolutional neural networks for feature extraction", "author": ["T. Wiatowski", "H. B\u00f6lcskei"], "venue": "arXiv:1512.06293, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1872\u20131886, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1872}, {"title": "Group invariant scattering", "author": ["S. Mallat"], "venue": "Comm. Pure Appl. Math., vol. 65, no. 10, pp. 1331\u2013 1398, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "A wavelet tour of signal processing: The sparse way", "author": ["S. Mallat"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "Proc. IEEE CVPR, 2015, pp. 447\u2013456.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "Proc. of Int. Conf. on Machine Learning, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Recycling randomness with structure for sublinear time kernel expansions", "author": ["K. Choromanski", "V. Sindhwani"], "venue": "Proc. of Int. Conf. on Machine Learning, 2016, pp. 2502\u20132510.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery", "author": ["J. Sherrah"], "venue": "arXiv:1606.02585, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1135\u2013 1143.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "The DGPF-test on digital airborne camera evaluation\u2013overview and test design", "author": ["M. Cramer"], "venue": "Photogrammetrie-Fernerkundung-Geoinformation, vol. 2010, no. 2, pp. 73\u201382, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Use of the stair vision library within the ISPRS 2D semantic labeling benchmark (Vaihingen)", "author": ["M. Gerke"], "venue": "Tech. Rep., University of Twente, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for semantic labeling", "author": ["A. Lagrange", "B. Le Saux"], "venue": "Tech. Rep., Onera \u2013 The French Aerospace Lab, 2015. 10", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 107, "endOffset": 112}, {"referenceID": 1, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 107, "endOffset": 112}, {"referenceID": 2, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 107, "endOffset": 112}, {"referenceID": 3, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 4, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 5, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 6, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 7, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 8, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 9, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 6, "context": "layers (with the output size being equal to the size of the original image) to one (in the case of encoder-decoder architectures [7, 10]) or multiple intermediate layers of a feed-forward CNN, the output of these deconvolution layers being finally combined.", "startOffset": 129, "endOffset": 136}, {"referenceID": 9, "context": "layers (with the output size being equal to the size of the original image) to one (in the case of encoder-decoder architectures [7, 10]) or multiple intermediate layers of a feed-forward CNN, the output of these deconvolution layers being finally combined.", "startOffset": 129, "endOffset": 136}, {"referenceID": 3, "context": "Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.", "startOffset": 43, "endOffset": 51}, {"referenceID": 4, "context": "Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.", "startOffset": 43, "endOffset": 51}, {"referenceID": 5, "context": "Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.", "startOffset": 43, "endOffset": 51}, {"referenceID": 8, "context": "Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.", "startOffset": 43, "endOffset": 51}, {"referenceID": 10, "context": "Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.", "startOffset": 211, "endOffset": 215}, {"referenceID": 11, "context": "Specifically, this architecture combines a tree-like CNN-based feature extractor [12], a random layer realizing a radial basis function (RBF) kernel approximation [13], and a linear classifier [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "Specifically, this architecture combines a tree-like CNN-based feature extractor [12], a random layer realizing a radial basis function (RBF) kernel approximation [13], and a linear classifier [14].", "startOffset": 163, "endOffset": 167}, {"referenceID": 13, "context": "Specifically, this architecture combines a tree-like CNN-based feature extractor [12], a random layer realizing a radial basis function (RBF) kernel approximation [13], and a linear classifier [14].", "startOffset": 193, "endOffset": 197}, {"referenceID": 14, "context": "Using Haar wavelets as convolutional filters, we evaluate the architecture for semantic segmentation of two different image types, namely outdoor scene images (from the Stanford Background data set [15]) and aerial images (form the Vaihingen data set of the ISPRS 2D semantic labeling contest [16]), using identical values for almost all hyper-parameters of the architecture in both cases.", "startOffset": 198, "endOffset": 202}, {"referenceID": 0, "context": "Further experiments show that our architecture is data efficient and matches the accuracy of the CNN in [1] using a much smaller training set.", "startOffset": 104, "endOffset": 107}, {"referenceID": 11, "context": ", hand-crafted) frame filters [12], followed by a non-linear classifier composed of a single fully connected RBF kernel approximation layer with pre-specified random filters [13], and a single fully connected linear classification layer based on learned filters [14].", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": ", hand-crafted) frame filters [12], followed by a non-linear classifier composed of a single fully connected RBF kernel approximation layer with pre-specified random filters [13], and a single fully connected linear classification layer based on learned filters [14].", "startOffset": 174, "endOffset": 178}, {"referenceID": 13, "context": ", hand-crafted) frame filters [12], followed by a non-linear classifier composed of a single fully connected RBF kernel approximation layer with pre-specified random filters [13], and a single fully connected linear classification layer based on learned filters [14].", "startOffset": 262, "endOffset": 266}, {"referenceID": 11, "context": "1 CNN-based feature extractor We briefly review the tree-like CNN-based feature extractor presented in [12], the basis of which is a convolutional transform followed by a non-linearity and a pooling operation.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "We refer the reader to [17\u201319] for similar translation-invariant and deformation-insensitive tree-like CNNs.", "startOffset": 23, "endOffset": 30}, {"referenceID": 16, "context": "We refer the reader to [17\u201319] for similar translation-invariant and deformation-insensitive tree-like CNNs.", "startOffset": 23, "endOffset": 30}, {"referenceID": 17, "context": "We refer the reader to [17\u201319] for similar translation-invariant and deformation-insensitive tree-like CNNs.", "startOffset": 23, "endOffset": 30}, {"referenceID": 18, "context": ", separable) Haar wavelets {g\u03bb}\u03bb\u2208\u039b, sensitive to 3 directions (horizontal, vertical, and diagonal) and\u2014for each direction\u2014sensitive to 4 scales, with corresponding wavelet low-pass filter \u03c7, together satisfying the Bessel condition (1) with Bd = 1, see [20].", "startOffset": 253, "endOffset": 257}, {"referenceID": 11, "context": "non-linear deformations [12] and hence allows our architecture to deal with variation in appearance of the semantic categories.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "Finally, similarly to [1], we also employ a variant of our network that extracts features at multiple image scales {s`}`=1 \u2282 N by applying the feature extractor to multiple sub-sampled versions {f }`=1 of the input image f , i.", "startOffset": 22, "endOffset": 25}, {"referenceID": 12, "context": "3) typically allow for much faster training and inference than non-linear kernel-based classifiers [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "We follow the construction given in [13]: Let G \u2208 Rm\u0303\u00d7m be a prespecified matrix with i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "The matrix W and the vector b are learned by minimizing the hinge loss with an `2-regularization term using SGD [14], i.", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "First, we note that a similar concept of aggregating pixel-wise features across feature maps was proposed in [21] for simultaneous detection and segmentation.", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "However, [21] requires a pre-trained detection network and end-toend training, while here we rely on pre-specified wavelet filters as well as pre-specified random filters and train the last classification layer only.", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "6G operations [8].", "startOffset": 14, "endOffset": 17}, {"referenceID": 20, "context": "Furthermore, note that the evaluation of the RBF approximation layer can be accelerated from O(m\u0303m) operations toO(m\u0303 logm) operations by using a fast RBF kernel approximation such as [22,23].", "startOffset": 184, "endOffset": 191}, {"referenceID": 21, "context": "Furthermore, note that the evaluation of the RBF approximation layer can be accelerated from O(m\u0303m) operations toO(m\u0303 logm) operations by using a fast RBF kernel approximation such as [22,23].", "startOffset": 184, "endOffset": 191}, {"referenceID": 3, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 134, "endOffset": 144}, {"referenceID": 4, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 134, "endOffset": 144}, {"referenceID": 5, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 134, "endOffset": 144}, {"referenceID": 8, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 134, "endOffset": 144}, {"referenceID": 22, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 134, "endOffset": 144}, {"referenceID": 23, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 206, "endOffset": 210}, {"referenceID": 14, "context": "1 Outdoor scene semantic segmentation We use the Stanford Background data set [15] containing 715 RGB images of outdoor scenes of size approximately 320 \u00d7 240 pixels.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "Following [1, 3], we estimate the accuracy of our method using 5-fold cross-validation (CV).", "startOffset": 10, "endOffset": 16}, {"referenceID": 2, "context": "Following [1, 3], we estimate the accuracy of our method using 5-fold cross-validation (CV).", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": ", the average class precision) of our CNN architecture, along with the three end-to-end trained architectures [1, 3, 8] (we refer to [1, Tab.", "startOffset": 110, "endOffset": 119}, {"referenceID": 2, "context": ", the average class precision) of our CNN architecture, along with the three end-to-end trained architectures [1, 3, 8] (we refer to [1, Tab.", "startOffset": 110, "endOffset": 119}, {"referenceID": 7, "context": ", the average class precision) of our CNN architecture, along with the three end-to-end trained architectures [1, 3, 8] (we refer to [1, Tab.", "startOffset": 110, "endOffset": 119}, {"referenceID": 0, "context": "For 1 scale, our network outperforms the pixel classification CNN from [1] but yields lower pixel and class accuracies than the CNN from [8] and the recurrent CNN from [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "For 1 scale, our network outperforms the pixel classification CNN from [1] but yields lower pixel and class accuracies than the CNN from [8] and the recurrent CNN from [3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "For 1 scale, our network outperforms the pixel classification CNN from [1] but yields lower pixel and class accuracies than the CNN from [8] and the recurrent CNN from [3].", "startOffset": 168, "endOffset": 171}, {"referenceID": 0, "context": ", the gain from using multiple scales is smaller than for the CNNs from [1, 8].", "startOffset": 72, "endOffset": 78}, {"referenceID": 7, "context": ", the gain from using multiple scales is smaller than for the CNNs from [1, 8].", "startOffset": 72, "endOffset": 78}, {"referenceID": 3, "context": "We note that other CNN architectures [4\u20137] achieve higher accuracies in semantic segmentation of outdoor scenes (on other data sets).", "startOffset": 37, "endOffset": 42}, {"referenceID": 4, "context": "We note that other CNN architectures [4\u20137] achieve higher accuracies in semantic segmentation of outdoor scenes (on other data sets).", "startOffset": 37, "endOffset": 42}, {"referenceID": 5, "context": "We note that other CNN architectures [4\u20137] achieve higher accuracies in semantic segmentation of outdoor scenes (on other data sets).", "startOffset": 37, "endOffset": 42}, {"referenceID": 6, "context": "We note that other CNN architectures [4\u20137] achieve higher accuracies in semantic segmentation of outdoor scenes (on other data sets).", "startOffset": 37, "endOffset": 42}, {"referenceID": 0, "context": "In particular, in the single scale case, it matches the pixel accuracy of the CNN from [1] using 5 times less training images.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "CNN [1] 66.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "CNN [8] 74.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "CNN (rCNN3 (\u25e6)) [3] 80.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "2 Aerial images semantic segmentation We evaluate our architecture on the Vaihingen data set1 of the ISPRS 2D semantic labeling contest [16], which contains 33 aerial images of varying size (average size 2494 \u00d7 2064 pixels [10]).", "startOffset": 223, "endOffset": 227}, {"referenceID": 9, "context": "Following [10,24], we retain images 11, 15, 28, 30, and 34 for testing and use the remaining labeled images in the data set for training.", "startOffset": 10, "endOffset": 17}, {"referenceID": 22, "context": "Following [10,24], we retain images 11, 15, 28, 30, and 34 for testing and use the remaining labeled images in the data set for training.", "startOffset": 10, "endOffset": 17}, {"referenceID": 25, "context": "Similarly as for outdoor scene semantic labeling, we apply the feature extraction network to each channel and to the normalized version of the DSM provided by [27].", "startOffset": 159, "endOffset": 163}, {"referenceID": 24, "context": "In Table 2 we report the pixel accuracy and the F1 score (averaged over classes) of our CNN architecture as well as that of two end-to-end 1The Vaihingen data set was provided by the German Society for Photogrammetry, Remote Sensing and Geoinformation (DGPF) [26]: http://www.", "startOffset": 259, "endOffset": 263}, {"referenceID": 1, "context": "CNN [2] 83.", "startOffset": 4, "endOffset": 7}, {"referenceID": 9, "context": "74 CNN-PC [10] 86.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "trained pixel classification CNNs [2, 10].", "startOffset": 34, "endOffset": 41}, {"referenceID": 9, "context": "trained pixel classification CNNs [2, 10].", "startOffset": 34, "endOffset": 41}, {"referenceID": 1, "context": "As in [2, 10] (and following the rules of the ISPRS 2D semantic labeling contest) labeling errors within a 3 pixel radius of the (true) category boundaries are excluded for the computation of the accuracy and the F1 score.", "startOffset": 6, "endOffset": 13}, {"referenceID": 9, "context": "As in [2, 10] (and following the rules of the ISPRS 2D semantic labeling contest) labeling errors within a 3 pixel radius of the (true) category boundaries are excluded for the computation of the accuracy and the F1 score.", "startOffset": 6, "endOffset": 13}, {"referenceID": 1, "context": "It can be seen that the accuracy and the average F1 score of our architecture is competitive with the pixel classification CNNs proposed in [2, 10].", "startOffset": 140, "endOffset": 147}, {"referenceID": 9, "context": "It can be seen that the accuracy and the average F1 score of our architecture is competitive with the pixel classification CNNs proposed in [2, 10].", "startOffset": 140, "endOffset": 147}, {"referenceID": 26, "context": "9 %, thus outperforming the algorithms from [28], each of which combines a pre-trained CNN with a SVM.", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "We note that other CNN architectures [2, 9, 10, 24] achieve higher accuracies and F1 scores on the Vaihingen data set.", "startOffset": 37, "endOffset": 51}, {"referenceID": 8, "context": "We note that other CNN architectures [2, 9, 10, 24] achieve higher accuracies and F1 scores on the Vaihingen data set.", "startOffset": 37, "endOffset": 51}, {"referenceID": 9, "context": "We note that other CNN architectures [2, 9, 10, 24] achieve higher accuracies and F1 scores on the Vaihingen data set.", "startOffset": 37, "endOffset": 51}, {"referenceID": 22, "context": "We note that other CNN architectures [2, 9, 10, 24] achieve higher accuracies and F1 scores on the Vaihingen data set.", "startOffset": 37, "endOffset": 51}, {"referenceID": 8, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 86, "endOffset": 92}, {"referenceID": 9, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 86, "endOffset": 92}, {"referenceID": 8, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 115, "endOffset": 121}, {"referenceID": 22, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 115, "endOffset": 121}, {"referenceID": 1, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 144, "endOffset": 151}, {"referenceID": 22, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 144, "endOffset": 151}, {"referenceID": 1, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 193, "endOffset": 196}], "year": 2016, "abstractText": "We propose a highly structured neural network architecture for semantic segmentation of images that combines i) a Haar wavelet-based tree-like convolutional neural network (CNN), ii) a random layer realizing a radial basis function kernel approximation, and iii) a linear classifier. While stages i) and ii) are completely pre-specified, only the linear classifier is learned from data. Thanks to its high degree of structure, our architecture has a very small memory footprint and thus fits onto low-power embedded and mobile platforms. We apply the proposed architecture to outdoor scene and aerial image semantic segmentation and show that the accuracy of our architecture is competitive with conventional pixel classification CNNs. Furthermore, we demonstrate that the proposed architecture is data efficient in the sense of matching the accuracy of pixel classification CNNs when trained on a much smaller data set.", "creator": "LaTeX with hyperref package"}}}