{"id": "1609.08397", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Generalization Error Bounds for Optimization Algorithms via Stability", "abstract": "Many pipe ability preparation need given hypotheses rather Regularized Empirical Risk Minimization (R - ERM ), now conflicts same back-end iterative instead as maximal huguenot (GD ), equations currents portuguese (SGD ), and discrete variance reduction (SVRG ). Conventional analysis to use discrete interfaces provides on need convergence fixed previous with coaching facilitate, that, living in brought memory mental recognized may benefits as what following modularity play years entire parents evolution out unseen failed tool. In both that, we perpetrators followed today u.s., by to maintain similar a technologies. In of, ? decompose is generalization calculation full R - ERM, and underlying however river directly besides of triangles brought exception - 1-dimensional persons. In polytope lawsuits, we regardless however first generalization intentionally will far avenue the to macroeconomic consumption particular second mathematical algorithm include next stressing now into R - ERM further, shared having expectation (by the future given $ \\ mathcal {O} (( 34 / n) + \\ mathbb {E} \\ \u00edmair (T) ) $, , $ \\ acta (T) $ is the clustering probable others $ T $ name next there addition computation) more where up formula_19 (next one certain well $ \\ mathcal {O} \\ again (\\ 14:21 {\\ onto {1 / \\ region} } {\\ sqrt {n} } + \\ grau (T) \\ but) $ with probability $ 34 - \\ jal $ ). For non - geometrically instance, we or there obtain a. higher approximation corrected instead. Our qualitative reports others 10) off with long personnel process, the generalizes intentional. lowest for all the dynamical simplest with way investigation; plus) Comparatively conversation, SVRG this able inverse own smaller GD in SGD. We other reports microbes in other whorls and longer - polytope concern, when form model test verify whole theoretical facts.", "histories": [["v1", "Tue, 27 Sep 2016 13:10:57 GMT  (850kb,D)", "http://arxiv.org/abs/1609.08397v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["qi meng", "yue wang", "wei chen", "taifeng wang", "zhiming ma", "tie-yan liu"], "accepted": true, "id": "1609.08397"}, "pdf": {"name": "1609.08397.pdf", "metadata": {"source": "CRF", "title": "Generalization Error Bounds for Optimization Algorithms via Stability", "authors": ["Qi Meng", "Yue Wang", "Wei Chen", "Taifeng Wang", "Zhi-Ming Ma", "Tie-Yan Liu"], "emails": ["qimeng13@pku.edu.cn", "11271012@bjtu.edu.cn", "tie-yan.liu}@microsoft.com", "mazm@amt.ac.cn"], "sections": [{"heading": null, "text": "( log 1/\u03b4\u221a\nn + \u03c1(T )\n) with\nprobability 1\u2212 \u03b4). For non-convex cases, we can also obtain a similar expected generalization error bound. Our theorems indicate that 1) along with the training process, the generalization error will decrease for all the optimization algorithms under our investigation; 2) Comparatively speaking, SVRG has better generalization ability than GD and SGD. We have conducted experiments on both convex and non-convex problems, and the experimental results verify our theoretical findings."}, {"heading": "1 Introduction", "text": "Many machine learning tasks can be formulated as Regularized Empirical Risk Minimization (R-ERM). Specifically, given a training dataset, the goal of R-ERM is to learn a model from a hypothesis space by minimizing the regularized empirical risk defined as the average loss on the training data plus a regularization term.\nIn most cases, it is hard to achieve an exact minimization of the objective function since the problem might be too complex to have a closed-form solution. Alternatively, we seek an approximate minimization by using some optimization algorithms. Widely used optimization algorithms include the first-order methods such as gradient descent (GD),\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nstochastic gradient descent (SGD), stochastic variance reduction (SVRG) (Johnson and Zhang 2013), and the secondorder methods such as Newton\u2019s methods (Nocedal and Wright 2006) and quasi-Newton\u2019s methods (Nocedal and Wright 2006). In this paper, for ease of analysis and without loss of generality, we will take GD, SGD and SVRG as examples. GD calculates the gradient of the objective function at each iteration and updates the model towards the direction of negative gradient by a constant step size. It has been proved that, if the step size is not very large, GD can achieve a linear convergence rate (Nesterov 2013). SGD exploits the additive nature of the objective function in R-ERM, and randomly samples an instance at each iteration to calculate the gradient. Due to the variance introduced by stochastic sampling, SGD has to adopt a decreasing step size in order to guarantee the convergence, and the corresponding convergence rate is sublinear in expectation (Rakhlin, Shamir, and Sridharan 2011). In order to reduce the variance in SGD, SVRG divides the optimization process into multiple stages and updates the model towards a direction of the gradient at a randomly sampled instance regularized by a full gradient over all the instances. In this way, SVRG can achieve linear convergence rate in expectation with a constant step size (Johnson and Zhang 2013).\nWhile the aforementioned convergence analysis can characterize the behaviors of the optimization algorithms in the training process, what the machine learning community cares more is the generalization performance of the learned model on unseen test data. 1 As we know, the generalization error of a machine learning algorithm can be decomposed into three parts, the approximation error, the estimation error, and the optimization error. The approximation error is caused by the limited representation power of the hypothesis space F ; the estimation error (which measures the difference between the empirical risk and the expected risk) is caused by the limited amount of training data (Vapnik and Kotz 1982)(Bousquet and Elisseeff 2002); and the optimization error (which measures the difference between expected risks of the model obtained by the optimization algorithm af-\n1Under a related but different setting, i.e., the data instances are successively generated from the underlying distribution, people have proven regret bounds for algorithms like SGD (Kakade and Tewari 2009; Cesa-Bianchi, Conconi, and Gentile 2004) and SVRG (Frostig et al. 2015).\nar X\niv :1\n60 9.\n08 39\n7v 1\n[ st\nat .M\nL ]\n2 7\nSe p\n20 16\nter T iterations and the true optimum of the regularized empirical risk) is caused by the limited computational power. In (Bousquet and Bottou 2008), Bottou and Bousquet proved generalization error bounds for GD and SGD based on VCdimension (Kearns and Ron 1999), which unavoidably are very loose in their nature.2 The goal of our paper is to develop more general and tighter generalization error bounds for the widely used optimization algorithms in R-ERM.\nTo this end, we leverage stability (Bousquet and Elisseeff 2002) as a tool and obtain the following results:\n(1) For convex objective functions, we prove that, the generalization error of an optimization algorithm can be upper bounded by a quantity related to its stability plus its convergence rate in expectation. Specifically, the generalization error bound is in the order of O(1/n + E\u03c1(T )), where \u03c1(T ) is the optimization convergence error and T is the number of iterations. This indicates that along with the optimization process on the training data, the generalization error will decrease, which is consistent with our intuition.\n(2) For convex objective functions, we can also obtain a high probability bound for the generalization error. In particular, the bound is in the order of O ( log 1/\u03b4\u221a\nn + \u03c1(T ) ) with probability at least 1 \u2212 \u03b4. That is, if an algorithm has a high-probability convergence bound, we can get a highprobability generalization error bound too, and our bound is sharper than those derived in the previous literature.\n(3) Based on our theorems, we analyze the time for different optimization algorithms to achieve the same generalization error, given the same amount of training data. We find that SVRG outperforms GD and SGD in most cases, and although SGD can quickly reduce the test error at the beginning of the training process, it slows down due to the decreasing step size and can hardly obtain the same test error as GD and SVRG when n is large.\n(4) Some of our theoretical results can be extended to the nonconvex objective functions, with some additional assumptions on the distance between the global minimizer and the stationary local minimizers.\nWe have conducted experiments on linear regression, logistic regression, and fully-connected neural networks to verify our theoretical findings. The experimental results are consistent with our theory: (1) when the training process goes on, the test error decreases; (2) in most cases, SVRG has better generalization performance than GD and SGD."}, {"heading": "2 Preliminaries", "text": "In this section, we briefly introduce the R-ERM problem, and popular optimization algorithms to solve it."}, {"heading": "2.1 R-ERM and its Stability", "text": "Suppose that we have a training set S = {z1 = (x1, y1), ..., zn = (xn, yn)} with n instances that are i.i.d. sampled from Z = X \u00d7 Y according to an unknown distribution P . The goal is to learn a good prediction model\n2In (Hardt, Recht, and Singer 2015), Hardt et.al studied convex risk minimization via stability, but they did not consider the influence of hypothesis space and the tradeoff between approximation error and estimation error.\nf \u2208 F : X \u2192 Y , whose prediction accuracy at instance (x, y) is measured by a loss function l(y, f(x)) = l(f, z). Different learning tasks may use different loss functions, such as the least square loss (f(x) \u2212 y)2 for regression, and the logistic loss log (1 + e\u2212yf(x)) for classification. We learn the prediction model from the training set S, and will use this model to give predictions for unseen test data.\nR-ERM is a very common way to achieve the above goal. Given loss function l(f, z), we aim to learn a model f\u2217 that minimizes the expected risk\nR(f) = Ez\u223cP l(f, z).\nBecause the underlying distribution P is unknown, in practice, we learn the prediction model by minimizing the regularized empirical risk over the training instances, which is defined as below,\nRrS(f) = 1\nn n\u2211 i=1 l(f, zi) + \u03bbN(f). (1)\nHere, the regularization term \u03bbN(f) helps to restrict the capacity of the hypothesis space F to avoid overfitting. In this paper, we consider N(f) as a norm in a reproducing kernel Hilbert space (RKHS): N(f) = \u2016f\u20162k where k refers to the kernel (Wahba 2000).\nAs aforementioned, our goal is expected risk minimization but what we can do in practice is empirical risk minimization instead. The gap between these two goals is measured by the so-called estimation error, which is usually expressed in the following way: the expected risk is upper bounded by the empirical risk plus a quantity related to the capacity of the hypothesis space (Vapnik and Kotz 1982)(Bousquet and Bottou 2008). One can choose different ways to measure the capacity of the hypothesis space, and stability is one of them, which is proved to be able to produce tighter estimation error bound than VC dimension (Kearns and Ron 1999). There has been a venerable line of research on estimation error analysis based on stability, dated back more than thirty years ago (Bousquet and Elisseeff 2002; Devroye and Wagner 1979; Kearns and Ron 1999; Mukherjee et al. 2006; Shalev-Shwartz et al. 2010). The landmark work by Bousquet and Elisseeff (Bousquet and Elisseeff 2002) introduced the following definitions of uniform loss stability and output stability. Definition 2.1 (Uniform Loss Stability) An algorithmA has uniform stability \u03b20 with respect to loss function l if the following holds \u2200S \u2208 Zn,\u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 , n},\n|EA [l(AS , \u00b7)]\u2212 EA [l(AS\\j , \u00b7)]| \u2264 \u03b20, (2)\nwhere AS , AS\\j are the outputs of algorithm A based on S and S\\j = {z1, \u00b7 \u00b7 \u00b7 , zj\u22121, zj+1, \u00b7 \u00b7 \u00b7 , zn}, respectively. Definition 2.2 (Output Stability) An algorithm has output stability \u03b21 if the following holds \u2200S \u2208 Zn,\u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 , n},\n\u2016AS \u2212AS\\j\u2016Fc \u2264 \u03b21, (3) where \u2016 \u00b7 \u2016Fc denotes the norm in hypothesis space Fc.\nFrom the above definitions, we can see that stability measures the change of the loss function or the produced model of a given learning algorithm if one instance in the training\nset is changed. For example, if the loss function is convex and L-Lipschitz w.r.t. f , the corresponding R-ERM algorithm with regularization term N(f) = \u2016f\u20162k has stability \u03b20 \u2264 L 2K2 2\u03bbn and \u03b21 \u2264 LK 2\u03bbn , where K is the upper bound of the kernel norm (Bousquet and Elisseeff 2002)."}, {"heading": "2.2 Optimization Algorithms", "text": "Many optimization methods can be used to solve the R-ERM problem, including the first-order methods such as Gradient Descent (GD) (Nesterov 2013), Stochastic Gradient Descent (SGD) (Rakhlin, Shamir, and Sridharan 2011), and Stochastic Variance Reduction (SVRG) (Johnson and Zhang 2013), as well as the second-order methods such as Newton\u2019s methods (Nocedal and Wright 2006) and quasi-Newton\u2019s methods (Byrd et al. 2016). We will take the first-order methods as examples in this paper, although many of our analysis can be easily extended to other optimization algorithms.\nLet us consider model f parameterized by w. The update rules of GD, SGD, and SVRG are summarized as follows. Gradient Descent (GD)\nwt+1 = wt \u2212 \u03b7\u2207RrS(wt). (4)\nStochastic Gradient Descent (SGD)\nwt+1 = wt \u2212 \u03b7tg(wt). (5)\nStochastic Variance Reduced Gradient (SVRG)\nvts = g(w t s)\u2212\u2207RrS(wts) +\u2207RrS(w\u0303t\u22121) (6) wts+1 = w t s \u2212 \u03b7vts. (7)\nwhere g(\u00b7) is the gradient of \u2207RrS(\u00b7) at randomly sampled training instances, wts is the output parameter at the s-th iteration in the t-th stage, and w\u0303t\u22121 is the final output in stage t\u2212 1.\nWhen the loss function is strongly convex and smooth with respect to the model parameters, GD can achieve linear convergence rate; SGD can only achieve sublinear convergence rate due to the variance introduced by stochastic sampling (but in each iteration, it only needs to compute the gradient over one instance and thus can be much faster in speed); SVRG can achieve linear convergence rate by reducing the variance and in most iterations it only needs to compute the gradient over one instance. 3 When the loss functions are nonconvex w.r.t. the model parameters (e.g., neural networks), GD (Nesterov 2013), SGD (Ghadimi and Lan 2013), and SVRG (Reddi et al. 2016) still have convergence properties (although regarding a different measure of convergence). For ease of reference, we summarize the convergence rates of the aforementioned optimization algorithms in both convex and nonconvex cases in Table 1."}, {"heading": "3 Generalization Analysis", "text": "In this section, we will analyze the generalization error for optimization algorithms by using stability as a tool.\n3The second-order methods can get quadratic convergence rate (Nocedal and Wright 2006). However, as compared with the firstorder methods, the computation complexity of the second-order methods could be much higher due to the calculation of the secondorder information.\nFirstly, we introduce the definition of generalization error and its decomposition. Then, we prove the generalization error bounds of optimization algorithms in both convex and nonconvex cases. The proof details of all the lemmas and theorems are placed in the supplementary materials due to space limitation."}, {"heading": "3.1 Generalization Error and its Decomposition", "text": "As we mentioned in Section 2, R-ERM minimizes the regularized empirical risk, i.e.,\nf\u2217S,r := argminf\u2208FR r S(f) (8)\nas an approximation of the expected risk minimization:\nf\u2217 := argminfR(f). (9)\nDenote the empirical risk RS(f) = 1n \u2211n i=1 l(f, zi). It is clear that, the minimization of RrS(f) in F is equivalent to the minimization of RS(f) in Fc = {f \u2208 F , N(f) \u2264 c} for some constant c. That is,\nf\u2217S,r = f \u2217 S,Fc := argminf\u2208FcRS(f). (10)\nDenote the minimizer of the expected risk R(f) in the hypothesis space Fc as f\u2217Fc , i.e.,\nf\u2217Fc := argminf\u2208FcR(f). (11)\nIn many practical cases, neither f\u2217S,r nor f \u2217 S,Fc has a closed\nform. What people do is to implement an iterative optimization algorithmA to produce the prediction model. We denote the output model of algorithm A at iteration T over n training instances as fT (A,n,Fc). We use generalization error to denote the difference between the expected risk of this learnt model and the optimal expected risk, as follows,\nE(A,n,Fc, T ) = R(fT (A,n,Fc))\u2212R(f\u2217). (12) As known, the generalization error can be decomposed\ninto the three components,\nE(A,n,Fc, T ) (13) = R(fT )\u2212R(f\u2217S,Fc) +R(f \u2217 S,Fc)\u2212R(f \u2217 Fc) (14)\n+R(f\u2217Fc)\u2212R(f \u2217)\n:= Eopt(A,n,Fc, T ) + Eest(n,Fc) + Eapp(Fc). (15)\nThe item Eapp(Fc) := R(f\u2217Fc) \u2212 R(f \u2217), is called approximation error, which is caused by the limited representation power of the hypothesis spaceFc. With the hypothesis space increasing, (i.e., c is increasing), the approximation error will decrease. The item Eest(n,Fc) := R(f\u2217S,Fc) \u2212 R(f \u2217 Fc), is called estimation error, which is caused by the limited amount of the training data (which leads to the gap between the empirical risk and the expected risk). It will decrease with the increasing training data size n, and the decreasing capacity of the hypothesis space Fc. The item Eopt(A,n,Fc, T ) := R(fT )\u2212R(f\u2217S,Fc), is called optimization error, which measures the sub-optimality of the optimization algorithms in terms of the expected risk. It is caused by the limited computational resources. 4\n4For simplicity, we sometimes denote fT (A,n,Fc), E(A,n,Fc, T ), Eapp(Fc), Eest(n,Fc), Eopt(A,n,Fc, T ) as fT , E , Eapp, Eest, Eopt, respectively.\nPlease note that, the optimization error under our study differs from the target in the conventional convergence analysis of optimization algorithms. In the optimization community, the following two objectives\n\u03c10(T ) = RS(fT )\u2212RS(f\u2217S,Fc); \u03c11(T ) = \u2016fT \u2212 f \u2217 S,Fc\u2016 2 Fc (16)\nare commonly used in convex cases, and\n\u03c12(T ) = \u2016\u2207RrS(fT )\u20162 (17)\nis commonly used in nonconvex cases. To avoid confusion, we call them convergence error and their corresponding upper bounds convergence error bounds. Please note although convergence error is different from optimization error, having a convergence error bound plays an important role in guaranteeing a generalization error bound. In the following subsections, we will prove the generalization error bound for typical optimization algorithms, by using the stability techniques, based on their convergence error bounds."}, {"heading": "3.2 Expected Generalization Bounds for Convex Case", "text": "The following theorem gives an expected generalization error bounds in the convex case.\nTheorem 3.1 Consider an R-ERM problem, if the loss function is L-Lipschitz continuous, \u03b3-smooth, and convex with respect to the prediction output vector, we have\nES,AE \u2264 Eapp + 2\u03b20 + ES,A\u03c10(T ) + \u03b3ES,A\u03c11(T )\n2\n+ \u221a ES,A\u03c11(T ) ( L2\n2n + 6L\u03b3\u03b21\n) , (18)\nwhere \u03b20, \u03b21 are the uniform stability and output stability of the R-ERM process as defined in 2.1 and 2.2, \u03c10(T ) and \u03c11(T ) are the convergence errors defined in Eqn 16.\nFrom Theorem 3.1, we can see that the generalization error can be upper bounded by the stability \u03b20 and \u03b21, the convergence errors of the optimization algorithms \u03c10(T ) and \u03c11(T ), and the well-studied approximation error (Vapnik and Vapnik 1998). As the training process goes on, both E\u03c10(T ) and E\u03c11(T ) will decrease. Therefore, the expected generalization error will decrease too. This is consistent with our intuition. Better optimizations will lead to better expected generalization performance.\nIn order to prove Theorem 3.1, we need the following two lemmas, whose proofs are placed in the supplementary materials due to space restrictions.\nLemma 3.2 For R-ERM problems, we have \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 , n}:\nES [ R(f\u2217S,Fc)\u2212RS(f \u2217 S,Fc) ] = ES [ l(f\u2217S,Fc , z \u2032 j)\u2212 l(f\u2217Sj ,Fc , z \u2032 j) ]\n(19) and\nES [\u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc)] = ES [\u2207f l(f \u2217 S,Fc , z \u2032 j)\u2212\u2207f l(f\u2217Sj ,Fc , z \u2032 j)],\n(20) where Sj = {z1, \u00b7 \u00b7 \u00b7 , zj\u22121, z\u2032j , zj+1, \u00b7 \u00b7 \u00b7 , zn}, and f\u2217Sj ,Fc is the minimizer of RSj (f) in Fc . Lemma 3.3 Assume that the loss function is L-Lipschitz and \u03b3-smooth w.r.t. the prediction output vector, we have\nES [\u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc)]\n2 \u2264 L 2\n2n + 6L\u03b3\u03b21. (21)\nProof Sketch of Theorem 3.1: Step 1: Since the loss function is convex and \u03b3-smooth w.r.t. f , we can get that R(f) is \u03b3-smooth and RS(f) is convex w.r.t f . We decompose Eopt as below:\nEopt \u2264 ( \u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc) )T (fT \u2212 f\u2217S,Fc)\n+RS(fT )\u2212RS(f\u2217S,Fc) + \u03b3 2 \u2016fT \u2212 f\u2217S,Fc\u2016 2 Fc ,\nWe can use \u03c10(T ), \u03c11(T ) and Lemma 3.3, to get an upper bound of ES,AEopt.\nStep 2: Since RS(f\u2217S,Fc) \u2264 RS(f \u2217 Fc), we have\nEest \u2264 [ R(f\u2217S,Fc)\u2212RS(f \u2217 S,Fc) ] + [RS(f \u2217 Fc)\u2212R(f \u2217 Fc)] .\nWe have ES [ RS(f \u2217 Fc)\u2212R(f \u2217 Fc) ] = 0. By using Lemma 3.2,\nwe can bound ESEest. By combining the upper bounds of ES,AEopt and ESEopt, we can get the results.\nAfter proving the general theorem, we consider a special case - an R-ERM problem with kernel regularization term \u03bb\u2016f\u20162k. In this case, we can derive the concrete expressions of the stability and convergence error. In particular, \u03b20 = O(1/\u03bbn), \u03b21 = O(1/\u03bbn) and \u03c11(T ) is equivalent to \u2016wT \u2212 w\u2217S,r\u20162. If the loss function is convex and smooth w.r.t. parameter w, RrS(w) with N(f) = \u2016f\u20162k is strongly convex and smooth w.r.t w. In this case, \u03c10(T ) dominates \u03c11(T ), i.e., \u03c10(T ) is larger than \u03c11(T ) w.r.t the order of T . Therefore, we can obtain the following corollary.\nCorollary 3.4 For an R-ERM problem with a regularization term \u03bb\u2016f\u20162k, under the same assumptions in Theorem 3.1, and further assuming that the loss function is convex and smooth w.r.t parameter w, we have\nES,AE \u2264 Eapp +O ( 1\n\u03bbn + ES,A\u03c10(T )\n) . (22)"}, {"heading": "3.3 High-Probability Generalization Bounds for Convex Case", "text": "The following theorem gives a high-probability bound of E in the convex case. Due to space limitation, we put the proof in the supplementary materials. Theorem 3.5 For an R-ERM problem, if the loss function is L-Lipschitz continuous, \u03b3-smooth and convex with respect to the prediction output vector, and 0 \u2264 l(f\u2217S,Fc , z) \u2264 M for arbitrary z \u2208 Z and S \u2208 Zn, then with probability at least 1\u2212 \u03b4, we have\nE \u2264 Eapp + 2\u03b20 + \u03c10(T ) + \u03b3\n2 \u03c11(T ) + 2\u03b3\u03b21\n\u221a \u03c11(T )\n+ ( 4n\u03b20 + 2M + (4n\u03b3\u03b21 + L) \u221a \u03c11(T ) )\u221a ln 4/\u03b4 2n .\nThe high-probability bound is consistent with the expected bound given in the previous subsection. That is, the highprobability generalization bound will also decrease along with the training process. In addition, we can also get a corollary for the special case of R-ERM with kernel regularization.\nCorollary 3.6 For an R-ERM problem with kernel regularization term \u03bb\u2016f\u20162k, under the same assumptions in Theorem 3.5, and further assuming that the loss function is convex and smooth w.r.t parameter w, we have, with probability at least 1\u2212 \u03b4,\nE \u2264 Eapp +O\n(\u221a log 1/\u03b4\nn + \u03c10(T )\n) .\nRakhlin et.al. (Rakhlin, Shamir, and Sridharan 2011) proved a high-probability convergence rate for SGD. For GD, the training process is deterministic. By plugging the order of \u03b20 and \u03b21 in SGD and GD, we have the following corollary.\nCorollary 3.7 For an R-ERM problem with kernel regularization, under the assumptions in Corollary 3.6, with probability at least 1\u2212\u03b4, the generalization error of SGD and GD can be upper bounded as follows,\nESGD \u2264 Eapp +O\n(\u221a ln 1/\u03b4\nn\n) +O ( \u03ba2 log(log(T )/\u03b4)\nT\n) ;\nEGD \u2264 Eapp +O\n(\u221a ln 1/\u03b4\nn\n) +O ( e\u2212\u03baT ) ,\nwhere \u03ba is the condition number."}, {"heading": "3.4 Expected Generalization Bounds for Nonconvex Case", "text": "In this subsection, we consider the case in which the loss function is convex w.r.t. the prediction output vector, but non-convex w.r.t. the model parameter. This case can cover deep neural networks, which are state-of-the-art AI techniques nowadays.\nFor the non-convex case, the definition of convergence error is a little different, as shown by Eq. (17). It measures whether the solution is close to a critical point, which is defined and further categorized as follows.\nDefinition 3.8 Consider the objectiveRrS and parameterw. If \u2207RrS(w) = 0, we say w is a critical point of RrS; if \u2207RrS(w) has at least one strictly negative eigenvalue, we say w is a strict saddle point. If each critical point w is either a local minimum or a strict saddle point, we say that RrS satisfies the strict saddle property.\nThe following theorem gives the expected generalization error bound for non-convex cases under the widely used assumptions (Lian et al. 2015; Reddi et al. 2016; Lee et al. 2016). Theorem 3.9 If RrS is \u00b5-strongly convex in the 0- neighborhood of arbitrary local minimum wloc, satisfies strict saddle point property, L- Lipschitz continuous, \u03b3-smooth and continuously twice differential w.r.t the model parameter w, and the loss function is convex w.r.t f , then we have\nES,AE \u2264 Eapp+2\u03b20+R(wloc)\u2212R(w\u2217S,Fc)+ L\n\u00b5\n\u221a min\nt=1,\u00b7\u00b7\u00b7 ,T ES,A\u03c12(t),\nwhere T \u2265 T1 and T1 is the number of iterations to achieve mint=1,\u00b7\u00b7\u00b7 ,T1 ES,A [\u03c12(t)] \u2264 \u03b32 20.\nSimilarly to the convex case, from the above theorem we can see that with the training process going on, the generalization error in the nonconvex case will also decrease. In addition, we can also derive specific bound for the R-ERM with kernel regularization."}, {"heading": "4 Sufficient Training and Optimal Generalization Error", "text": "In this section, we make further discussions on the generalization bound. In particular, we will explore the sufficient training iterations, and the optimal generalization error given the training data size.\nAs shown in Section 3, the generalization error bounds consist of an estimation error related to the training data size n and an optimization error related to the training iteration T . Given a machine learning task with fixed training size n, at the early stage of the training process (i.e., T is relatively small), the optimization error will dominate the generalization error; when T becomes larger than a threshold, the optimization error will decrease to be smaller than the estimation error (i.e. O(1/n)), and then the estimation error will dominate the generalization error. We call this threshold sufficient training iteration and the corresponding training time sufficient training time. The generalization error with the optimization algorithm sufficiently trained is called optimal generalization error. Given the generalization error bound, we can derive the sufficient training iteration/time. For ease of analysis, we list the sufficient training iteration/time of GD, SGD, and SVRG for both convex and nonconvex cases in Table 2.\nFrom Table 2, we have the following observations. For the convex case, when the condition number \u03ba is much smaller than n, GD, SGD and SVRG have no big differences from each other in their sufficient training iterations; when \u03ba is comparable with n, e.g., \u03ba = O( \u221a n), 5 the sufficient train-\n5In some cases, \u03ba is related to the regularization coefficient \u03bb and \u03bb is determined by the data size n (Vapnik and Vapnik 1998)(Shamir, Srebro, and Zhang 2014).\ning time for GD, SGD and SVRG is O(n \u221a nd lnn), O(n2d), O(nd lnn), respectively. That is, SVRG corresponds to a shorter sufficient training time than GD and SVRG. For the non-convex case, if 0 \u2264 O(1/n), which is more likely to happen for small data size n, the first term in the sufficient training time dominates, and it is fine to terminate the training process at T = T1. SVRG requires shorter training time than GD and SGD by at least an order of O(n1/3) and O(n4/3), respectively. If 0 is larger than O(1/n), which is more likely to happen for large data size n, the sufficient training time for GD, SGD, and SVRG is O(n3), O(n4), and O(n8/3), respectively. In this case, SVRG requires shorter training time than GD and SGD by an order of O(n1/3) and O(n4/3), respectively."}, {"heading": "5 Experiments", "text": "In this section, we report experimental results to validate our theoretical findings. We conducted experiments on three tasks: linear regression, logistic regression, and fully connected neural networks, whose objective functions are least square loss, logistic loss, and cross-entropy loss respectively, plus an L2 regularization term with \u03bb = 1/ \u221a n. The first two tasks are used to verify our results for convex problems, and the third task is used to verify our theory on nonconvex problems. For each task, we report three figures. The horizontal axis of each figure corresponds to the number of data passes and the vertical axis corresponds to the training loss, test loss, and log-scaled test loss, respectively. For linear regression, we independently sample data instances with size n = 40000 from a 100\u2212dimension Gaussian distribution. We use half of them as the training data and the other as the test data. We set the step size for GD, SGD, SVRG as 0.032, 0.01/t and 0.005, respectively, according to the smoothness and strong-convexity coefficients. For our simulated data, the condition number \u03ba \u2248 116. The results are shown in Fig.1(c)1(a)1(b). For logistic regression, we conduct binary classification on benchmark dataset rcv1. We set the step sizes for GD, SGD, SVRG as 400, 200/t and 1, respectively. The results are shown in Fig. 1(f)1(e)1(d). For neural networks, we work on a model with one fully connected hidden layer of 100 nodes, ten softmax output nodes, and sigmoid activation (Johnson and Zhang 2013). We tune the step size for GD, SGD, SVRG and eventually choose 0.03, 0.25/ \u221a t and 0.001, respectively, which correspond to the best performances in our experiments. The inner loop size for SVRG for convex problems is set as 2n and that for nonconvex problem is set as 5n. The results are shown in Fig.1(i)1(g)1(h).\nFrom the results for all the three tasks, we have the following observations. (1) As training error decreases, the test er-\nror also decreases. (2) According to Fig.1(c), SVRG is faster than GD by a factor ofO(\u03ba) and faster than SGD by a factor of more than O(\u03ba). (3) According to Fig. 1(c)1(f)1(i), SGD is the slowest although it is fast in the beginning, which is consistent with our discussions in Section 4.\nBy comparing the results of logistic regression and linear regression, we have the following observations. (1) The test error for logistic regression converges after fewer rounds of data passes than linear regression. This is because the condition number \u03ba for logistic regression is smaller than linear regression. (2) SVRG is faster than GD and SGD but the differences between them are less significant for logistic regression, due to a smaller \u03ba. As compared to the results for logistic regression and linear regression, we have the following observations on the results of neural networks. (1) The convergence rate is slower and the accuracy is lower. This is because of the nonconvexity and the gap between global optimum and local optimum. (2) SVRG is faster than GD and SGD but the differences between them are not as significant as in the convex cases, which is consistent with our discussions in Section 4 by considering the data size of CIFAR 10."}, {"heading": "6 Conclusion", "text": "In this paper, we have studied the generalization error bounds for optimization algorithms to solve R-ERM prob-\nlems, by using stability as a tool. For convex problems, we have obtained both expected bounds and high-probability bounds. Some of our results can be extended to the nonconvex case. Roughly speaking, our theoretical analysis has shown: (1) Along with the training process, the generalization error will decrease; (2) SVRG outperforms GD and SGD in most cases. We have verified the theoretical findings by using experiments on linear regression, logistic regression and fully connected neural networks. In the future, we plan to study the stability of R-ERM with other regularization terms, e.g., the L1 regularizer, which is usually associated with non-smooth optimization methods."}, {"heading": "7 Appendices", "text": ""}, {"heading": "7.1 Proofs of Lemma 3.2, Lemma 3.3 and", "text": "Theorem 3.1 Lemma 3.2: For R-ERM problems, we have \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 , n}: ES [ R(f\u2217S,Fc)\u2212RS(f \u2217 S,Fc) ] = ES [ l(f\u2217S,Fc , z \u2032 j)\u2212 l(f\u2217Sj ,Fc , z \u2032 j) ]\n(23) and\nES [\u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc)]\n= ES [\u2207f l(f\u2217S,Fc , z \u2032 j)\u2212\u2207f l(f\u2217Sj ,Fc , z \u2032 j)], (24)\nwhere Sj = {z1, \u00b7 \u00b7 \u00b7 , zj\u22121, z\u2032j , zj+1, \u00b7 \u00b7 \u00b7 , zn}, and f\u2217Sj ,r is the minimizer of RrSj .\nProof: The proofs of Eq.(23) and Eq.(24) are very similar, we only prove Eq.(24). ES [\u2207RS(f\u2217S,Fc)] (25)\n= 1\nn n\u2211 j=1 ES [\u2207f l(f\u2217S,Fc , zj)] (26)\n= 1\nn n\u2211 j=1 ES,z\u2032j [\u2207f l(f \u2217 S,Fc , zj)] (27)\n= 1\nn n\u2211 j=1 ES,z\u2032j [\u2207f l(f \u2217 Sj ,Fc , z \u2032 j)] (28)\nUsing the definition of R, we can get ES\u2207R(f\u2217S,Fc) = ES,z\u2207f l(f \u2217 S,Fc , z) = ES,z\u2032j\u2207f l(f \u2217 S,Fc , z \u2032 j). (29) By combining Eq.(28) and (29), we can get the results.\nLemma 3.3 : Assume that the loss function is L-Lipschitz and \u03b3-smooth w.r.t. the prediction output vector, we have\nES [\u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc)]\n2 \u2264 L 2\n2n + 6L\u03b3\u03b21. (30)\nProof: The proof is following Lemma 9 and Lemma 25 in (Bousquet and Elisseeff 2002). We just need to relpalce M which is the upper bound of the loss function by the upper bound of the derivative of loss function \u2207f l(f, z), and replace the lip By the assumption that the loss function isL\u2212Lipschitz continous and \u03b3\u2212smooth w.r.t. the prediction output vector, we can get that \u2207f l(f, z) \u2264 L and \u2207f l(f, z) is \u03b3\u2212 Lipschitz continous. Following Lemma 9 in (Bousquet and Elisseeff 2002), we have\nES [\u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc)] 2 (31)\n\u2264 L 2\n2n + 3LES,z\u2032j [|\u2207l(f\n\u2217 S,Fc , zj)\u2212\u2207l(f \u2217 Sj ,Fc , zj)|](32)\n\u2264 L 2\n2n + 3L\u03b3ES,z\u2032j [\u2016f\n\u2217 S,Fc \u2212 f \u2217 Sj ,Fc\u2016Fc ] (33)\n\u2264 L 2\n2n + 3L\u03b3ES,z\u2032j [\u2016f\n\u2217 S,Fc \u2212 f \u2217 S\\j ,Fc\u2016Fc ] (34)\n+3L\u03b3ESj ,z\u2032j [\u2016f \u2217 S,Fc \u2212 f \u2217 S\\j ,Fc\u2016Fc ] (35)\n\u2264 L 2\n2n + 6L\u03b3\u03b21. (36)\nTheorem 3.1 : Consider an R-ERM problem, if the loss function is L-Lipschitz continuous, \u03b3-smooth, and convex with respect to the prediction output vector, we have\nES,AE \u2264 Eapp + 2\u03b20 + ES,A\u03c10(T ) + \u03b3ES,A\u03c11(T )\n2\n+ \u221a ES,A\u03c11(T ) ( L2\n2n + 6L\u03b3\u03b21\n) , (37)\nwhere \u03b20, \u03b21 are the uniform stability and output stability of the R-ERM process as defined in Def.2.1 and Def.2.2, \u03c10(T ) and \u03c11(T ) are the convergence errors defined in Eqn.(16).\nProof: If l(f, z) is convex and \u03b3-smooth w.r.t. f , we can get that R(f) is \u03b3-smooth and RS(f) is convex. First, we decompose Eopt as follows:\nEopt (38) = R(fT )\u2212R(f\u2217S,Fc) (39) \u2264 \u2207R(f\u2217S,Fc) T (fT \u2212 f\u2217S,Fc) + \u03b3\n2 \u2016fT \u2212 f\u2217S,Fc\u2016 2 Fc (40)\n= ( \u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc) )T (fT \u2212 f\u2217S,Fc) (41)\n+\u2207RS(f\u2217S,Fc) T (fT \u2212 f\u2217S,Fc) +\n\u03b3 2 \u2016fT \u2212 f\u2217S,Fc\u2016 2 Fc(42)\n\u2264 ( \u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc) )T (fT \u2212 f\u2217S,Fc) (43)\n+RS(fT )\u2212RS(f\u2217S,Fc) + \u03b3 2 \u2016fT \u2212 f\u2217S,Fc\u2016 2 Fc ,\nwhere the first inequality is established by using the \u03b3smoothness condition and the third inequality is established by using the convexity condition. Taking expectation w.r.t. S and the optimization algorithm A, we can get\nES,A \u03b3 2 \u2016fT \u2212 f\u2217S,Fc\u2016 2 = \u03b3 2 ES,A\u03c11(T ) (44)\nES,A[RS(fT )\u2212RS(f\u2217S,Fc)] = ES,A\u03c10(T ) (45)\nFor the term ( \u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc) )T (fT \u2212 f\u2217S,Fc), by\nusing Cauthy-Schwarz inequality, we can get:\nES,A ( \u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc) )T (fT \u2212 f\u2217S,Fc)\n\u2264 \u221a ES,A\u2016fT \u2212 f\u2217S,Fc\u20162ES ( \u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc) )2 \u2264 \u221a ES,A\u03c11(T ) ( L2\n2n + 6L\u03b3\u03b21\n) (46)\nwhere the second inequality holds according to Lemma 3.3. Next we decompose Eest as follows:\nEest (47) = [ R(f\u2217S,Fc)\u2212RS(f \u2217 S,Fc) ] + [ RS(f \u2217 S,Fc)\u2212RS(f \u2217 Fc) ]\n+ [RS(f \u2217 Fc)\u2212R(f \u2217 Fc)] (48) \u2264 [ R(f\u2217S,Fc)\u2212RS(f \u2217 S,Fc) ] + [RS(f \u2217 Fc)\u2212R(f \u2217 Fc)] , (49)\nwhere the second inequality is established because f\u2217S,Fc is the minimizer of RS restricted to the hypothesis space Fc.\nSince f\u2217Fc is independent of S, we have ES [ RS(f \u2217 Fc)\u2212R(f \u2217 Fc) ] = 0. Then by using Eq.(19)\nand the definition of uniform stability, we can get ESEest \u2264 ES [ R(f\u2217S,Fc)\u2212RS(f \u2217 S,Fc) ] \u2264 ES,z\u2032j [|l(f \u2217 S,Fc , z \u2032 j)\u2212 l(f\u2217Sj ,Fc , z \u2032 j)|]\n\u2264 ES,z\u2032j [|l(f \u2217 S,Fc , z \u2032 j)\u2212 l(f\u2217S\\j ,Fc , z \u2032 j)|]\n+ES,z\u2032j [|l(f \u2217 S\\j ,Fc , z \u2032 j)\u2212 l(f\u2217Sj ,Fc , z \u2032 j)|]\n\u2264 2\u03b20. (50)\nBy combining Ineq.(46) and Ineq.(50), we can get the result in the theorem."}, {"heading": "7.2 Proof of Theorem 3.5", "text": "Theorem 3.5: For an R-ERM problem, if the loss function is L-Lipschitz continuous, \u03b3-smooth and convex with respect to the prediction output vector, and 0 \u2264 l(f\u2217S,Fc , z) \u2264M for arbitrary z \u2208 Z and S \u2208 Zn, then with probability at least 1\u2212 \u03b4, we have\nE \u2264 Eapp + 2\u03b20 + \u03c10(T ) + \u03b3\n2 \u03c11(T ) + 2\u03b3\u03b21\n\u221a \u03c11(T )\n+ ( 4n\u03b20 + 2M + (4n\u03b3\u03b21 + L) \u221a \u03c11(T ) )\u221a ln 4/\u03b4 2n .\nIn order to prove Theorem 3.5, we need to use the following theorem which is proposed by McDiarmid.\nTheorem (McDiarmid,1989): Let S and Si are two data sets which are different at only one point j. Let F : Zn \u2192 R be any measurable function which there exists constants cj(j = 1, \u00b7 \u00b7 \u00b7 , n) such that supS\u2208Zn,z\u2032j\u2208Z |F (S) \u2212 F (S j)| \u2264 cj , then PS(F (S)\u2212 ESF (S) \u2265 ) \u2264 e\u22122 2/ \u2211n j=1 c 2 j .\nProof: Firstly, we give the high probability bound for estimation error Eest. As we have the decomposition Ineq. (49), we need to analyze R(f\u2217S,Fc)\u2212RS(f \u2217 S,Fc) and RS(f \u2217 Fc)\u2212R(f \u2217 Fc). By using Theorem 12 in (Bousquet and Elisseeff 2002), we have with probability at least 1\u2212 \u03b4,\nR(f\u2217S,Fc)\u2212RS(f \u2217 S,Fc) \u2264 2\u03b20 + (4m\u03b20 +M)\n\u221a ln 1/\u03b4\n2n . (51)\nFor RS(f\u2217Fc)\u2212R(f \u2217 Fc), by using Hoeffding\u2019s inequality, we\ncan get with probability at least 1\u2212 \u03b4,\nRS(f \u2217 Fc)\u2212R(f \u2217 Fc) \u2264M\n\u221a ln 1/\u03b4\n2n (52)\nWe can use Hoeffding\u2019s bound since f\u2217Fc is independent with the training set S. By combining Ineq.(51) and Ineq.(52), we have with probability at least 1\u2212 2\u03b4,\nEest \u2264 2\u03b20 + (4m\u03b20 + 2M) \u221a ln 1/\u03b4\n2n . (53)\nSecondly, we give high probability bound for the term Eopt by using Theorem (McDiarmid, 1989). According to Theorem (McDiarmid, 1989), we need to calculate ES [\u2207R(f\u2217S,Fc) \u2212 \u2207RS(f \u2217 S,Fc)], and cj = |\u2207R(f \u2217 S,Fc) \u2212\n\u2207RS(f\u2217S,Fc)\u2212 ( \u2207R(f\u2217Sj ,Fc)\u2212\u2207RS(f \u2217 Sj ,Fc) ) |.\nFor \u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc), we have\nES [\u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc)] (54) = ES [ \u2207f l(f\u2217S,Fc , z \u2032 j)\u2212\u2207f l(f\u2217Sj ,Fc , z \u2032 j) ] \u2264 2\u03b3\u03b21.(55)\nWe also have\n|\u2207R(f\u2217S,Fc)\u2212\u2207R(f \u2217 S\\j,r )| (56) \u2264 ES [ |\u2207f l(f\u2217S,Fc , z \u2032 j)\u2212\u2207f l(f\u2217Sj ,Fc , z \u2032 j)| ] \u2264 \u03b3|f\u2217S,Fc(x)\u2212 f \u2217 S\\j ,Fc(x)| \u2264 \u03b3\u03b21 (57)\nand |\u2207RS(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S\\j ,Fc )| \u2264 \u03b3\u03b21+ Ln , which yields\n|\u2207R(f\u2217S,Fc)\u2212\u2207R(f \u2217 Sj ,Fc)| \u2264 2\u03b3\u03b21 (58)\n|\u2207RS(f\u2217S,Fc)\u2212\u2207RS(f \u2217 Sj ,Fc)| \u2264 2\u03b3\u03b21 +\nL n . (59)\nThus we can get cj = 4\u03b3\u03b21+ Ln . By using Theorem (McDiarmid, 1989), we can get that with probability at least 1\u2212 \u03b4\n\u2207R(f\u2217S,Fc)\u2212\u2207RS(f \u2217 S,Fc) \u2264 2\u03b3\u03b21 + (4n\u03b3\u03b21 + L)\n\u221a ln 1/\u03b4\n2n .\n(60) By putting Ineq.(60), \u03c10(T ) and \u03c11(T ) in Ineq.(43), we\nhave with probability at least 1\u2212 2\u03b4,\nEopt \u2264 ( 2\u03b3\u03b21 + (4n\u03b3\u03b21 + L) \u221a ln 1/\u03b4\n2n\n)\u221a \u03c11(T )\n+\u03c10(T ) + (\u03b3 2 ) \u03c11(T ) (61)\nBy combining Ineq.(53) and Ineq.(61), we have with probability 1\u2212 4\u03b4,\nE \u2264 Eapp + 2\u03b20 + (4n\u03b20 + 2M) \u221a ln 1/\u03b4\n2n + \u03c10(T )\n+ (\u03b3 2 ) \u03c11(T ) + ( 2\u03b3\u03b21 + (4n\u03b3\u03b21 + L) \u221a ln 1/\u03b4 2n )\u221a \u03c11(T )\n\u2264 Eapp + 2\u03b20 + \u03c10(T ) + (\u03b3 2 ) \u03c11(T ) + 2\u03b3\u03b21 \u221a \u03c11(T )\n+ ( 4n\u03b20 + 2M + (4n\u03b3\u03b21 + L) \u221a \u03c11(T ) )\u221a ln 1/\u03b4 2n\nBy using \u03b4/4 to replace \u03b4, we can get the result."}, {"heading": "7.3 Proof of Theorem 3.9", "text": "Theorem 3.9: If RrS is \u00b5-strongly convex in the 0- neighborhood of arbitrary local minimum wloc, satisfies strict saddle point property, L- Lipschitz continuous, \u03b3-smooth and continuously twice differential w.r.t the model parameter w, and the loss function is convex w.r.t f , then we have\nES,AE \u2264 Eapp+2\u03b20+R(wloc)\u2212R(w\u2217S,Fc)+ L\n\u00b5\n\u221a min\nt=1,\u00b7\u00b7\u00b7 ,T ES,A\u03c12(t),\n(62) where T \u2265 T1 and T1 is the number of iterations to achieve mint=1,\u00b7\u00b7\u00b7 ,T1 ES,A [\u03c12(t)] \u2264 \u03b32 20.\nProof: The upper bound for expected estimation error is the same as convex cases since the loss function is convex w.r.t f , i.e., ESEest \u2264 2\u03b20.\nReferring to a recent work of Lee et.al (Lee et al. 2016), GD with a random initialization and sufficiently small constant step size converges to a local minimizer almost surely under the assumptions in Theorem 3.9. Thus, the assumption that RrS is \u00b5-strongly convex in the 0-neighborhood of arbitrary local minimum wloc is easily to be satisfied in sense of \u201dalmost surely\u201d. We decompose mint=1,\u00b7\u00b7\u00b7 ,T EEopt as\nmin t=1,\u00b7\u00b7\u00b7 ,T\nE[R(wt)\u2212R(wloc)] + E[R(wloc)\u2212R(w\u2217S,Fc)]. (63)\nBy the L-Lipschitz condition, we have R(wt) \u2212 R(wloc) \u2264 L\u2016wt \u2212 wloc\u2016. Firstly, We need to calculate how many iterations are needed to guarantee that\nmin t=1,\u00b7\u00b7\u00b7 ,T1\nE\u2016wt \u2212 wloc\u2016 \u2264 0. (64)\nBy the \u03b3-smooth assumption, we have \u03b3\u2016wt \u2212 wloc\u20162 \u2265 \u3008\u2207RrS(wt), wt \u2212 wloc\u3009. Thus for wt \u2208 B(wloc, 0), we have \u2016\u2207RrS(wt)\u2016 \u2264 \u03b3\u2016wt \u2212 wloc\u2016 \u2264 \u03b3 0. By the continuously twice differential assumption, we can assume that \u2016\u2207RrS(wt)\u2016 \u2264 \u03b3 0 for wt \u2208 B(wloc, 0) and \u2016\u2207RrS(wt)\u2016 \u2265 \u03b3 0 for wt /\u2208 B(wloc, 0) without loss of generality. 6 Therefore mint=1,\u00b7\u00b7\u00b7 ,T1 E\u2016\u2207RrS(wt)\u20162 \u2264 \u03b32 20 is a sufficient condition for mint=1,\u00b7\u00b7\u00b7 ,T1 E\u2016wt \u2212 wloc\u2016 \u2264 0.\nIf T \u2265 T1, by the \u00b5-strongly convex assumption, we have \u2016wt\u2212wloc\u20162 \u2264 1\u00b5 \u3008\u2207R r S(wt), wT\u2212wloc\u3009 \u2264 1\u00b5\u2016\u2207R r S(wt)\u2016\u2016wt\u2212 wloc\u2016 for wt \u2208 B(wloc, 0), which yields \u2016wt \u2212 wloc\u2016 \u2264 1 \u00b5 \u2016\u2207RrS(wt)\u2016. Based on the above discussions, we can get\nmin t=1,\u00b7\u00b7\u00b7 ,T\nEEopt\n= min t=1,\u00b7\u00b7\u00b7 ,T E[R(wt)\u2212R(wloc)] + E[R(wloc)\u2212R(wS,F\u2217c )]\n\u2264 L min t=1,\u00b7\u00b7\u00b7 ,T E\u2016wt \u2212 wloc\u2016+ E[R(wloc)\u2212R(wS,F\u2217c )]\n\u2264 L \u00b5 min t=1,\u00b7\u00b7\u00b7 ,T E\u2016\u2207RrS(wt)\u2016+ E[R(wloc)\u2212R(wS,F\u2217c )]\n\u2264 L \u00b5\n\u221a min\nt=1,\u00b7\u00b7\u00b7 ,T E\u2016\u2207RrS(wt)\u20162 + E[R(wloc)\u2212R(wS,F\u2217c )]\n= L\n\u00b5\n\u221a min\nt=1,\u00b7\u00b7\u00b7 ,T E\u03c12(t) + E[R(wloc)\u2212R(wS,F\u2217c )],\nwhere T \u2265 T1.\n6Otherwise, we can choose 0 small enough to make it satisfied."}], "references": [{"title": "and Bottou", "author": ["O. Bousquet"], "venue": "L.", "citeRegEx": "Bousquet and Bottou 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "and Elisseeff", "author": ["O. Bousquet"], "venue": "A.", "citeRegEx": "Bousquet and Elisseeff 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "R", "author": ["Byrd"], "venue": "H.; Hansen, S.; Nocedal, J.; and Singer, Y.", "citeRegEx": "Byrd et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Conconi Cesa-Bianchi", "N. Gentile 2004] Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory 50(9):2050\u20132057", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "and Wagner", "author": ["L. Devroye"], "venue": "T.", "citeRegEx": "Devroye and Wagner 1979", "shortCiteRegEx": null, "year": 1979}, {"title": "S", "author": ["R. Frostig", "R. Ge", "Kakade"], "venue": "M.; and Sidford, A.", "citeRegEx": "Frostig et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Lan", "author": ["S. Ghadimi"], "venue": "G.", "citeRegEx": "Ghadimi and Lan 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Recht Hardt", "M. Singer 2015] Hardt", "B. Recht", "Y. Singer"], "venue": "arXiv preprint arXiv:1509.01240", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "and Zhang", "author": ["R. Johnson"], "venue": "T.", "citeRegEx": "Johnson and Zhang 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Tewari", "author": ["S.M. Kakade"], "venue": "A.", "citeRegEx": "Kakade and Tewari 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Ron", "author": ["M. Kearns"], "venue": "D.", "citeRegEx": "Kearns and Ron 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Gradient descent converges to minimizers", "author": ["B. Recht"], "venue": "University of California,", "citeRegEx": "Recht,? \\Q2016\\E", "shortCiteRegEx": "Recht", "year": 2016}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["Lian"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lian,? \\Q2015\\E", "shortCiteRegEx": "Lian", "year": 2015}, {"title": "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization", "author": ["Mukherjee"], "venue": null, "citeRegEx": "Mukherjee,? \\Q2006\\E", "shortCiteRegEx": "Mukherjee", "year": 2006}, {"title": "and Wright", "author": ["J. Nocedal"], "venue": "S.", "citeRegEx": "Nocedal and Wright 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647", "author": ["Shamir Rakhlin", "A. Sridharan 2011] Rakhlin", "O. Shamir", "K. Sridharan"], "venue": null, "citeRegEx": "Rakhlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2011}, {"title": "S", "author": ["Reddi"], "venue": "J.; Hefny, A.; Sra, S.; P\u00f3cz\u00f3s, B.; and Smola, A.", "citeRegEx": "Reddi et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learnability, stability and uniform convergence", "author": ["Shalev-Shwartz"], "venue": "Journal of Machine Learning Research 11(Oct):2635\u20132670", "citeRegEx": "Shalev.Shwartz,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz", "year": 2010}, {"title": "Communication-efficient distributed optimization using an approximate newton-type method", "author": ["Srebro Shamir", "O. Zhang 2014] Shamir", "N. Srebro", "T. Zhang"], "venue": "In ICML,", "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "and Kotz", "author": ["V.N. Vapnik"], "venue": "S.", "citeRegEx": "Vapnik and Kotz 1982", "shortCiteRegEx": null, "year": 1982}, {"title": "and Vapnik", "author": ["V.N. Vapnik"], "venue": "V.", "citeRegEx": "Vapnik and Vapnik 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "G", "author": ["Wahba"], "venue": "2000. An introduction to model building with reproducing kernel hilbert spaces. Statistics Department TR", "citeRegEx": "Wahba 2000", "shortCiteRegEx": null, "year": 1020}], "referenceMentions": [], "year": 2016, "abstractText": "Many machine learning tasks can be formulated as Regularized Empirical Risk Minimization (R-ERM), and solved by optimization algorithms such as gradient descent (GD), stochastic gradient descent (SGD), and stochastic variance reduction (SVRG). Conventional analysis on these optimization algorithms focuses on their convergence rates during the training process, however, people in the machine learning community may care more about the generalization performance of the learned model on unseen test data. In this paper, we investigate on this issue, by using stability as a tool. In particular, we decompose the generalization error for R-ERM, and derive its upper bound for both convex and non-convex cases. In convex cases, we prove that the generalization error can be bounded by the convergence rate of the optimization algorithm and the stability of the R-ERM process, both in expectation (in the order of O(1/n) + E\u03c1(T )), where \u03c1(T ) is the convergence error and T is the number of iterations) and in high probability (in the order of O ( log 1/\u03b4 \u221a n + \u03c1(T ) ) with probability 1\u2212 \u03b4). For non-convex cases, we can also obtain a similar expected generalization error bound. Our theorems indicate that 1) along with the training process, the generalization error will decrease for all the optimization algorithms under our investigation; 2) Comparatively speaking, SVRG has better generalization ability than GD and SGD. We have conducted experiments on both convex and non-convex problems, and the experimental results verify our theoretical findings.", "creator": "LaTeX with hyperref package"}}}