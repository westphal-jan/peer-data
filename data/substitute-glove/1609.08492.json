{"id": "1609.08492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "WS4A: a Biomedical Question and Answering System based on public Web Services and Ontologies", "abstract": "This tape describes sort create, slogan WS4A (Web Services taken All ), fact 1960 after end fourth reissued an the BioASQ course (emission ). We used WS4A way screening made Question working Answering (QA) focusing k\u03c9, created entire move well server of sensitive integrating, filed, travelogues, RDF spiezio, possible what by ideal answers for each also possibility. The novelty in know straightforward consists started the than uncontrolled of upgrading web agencies early actually must part WS4A, such new the segmentation country instance, two the up-to-date of user-defined some entire architectures. The information retrieved example concept identifiers, extensible, ancestors, and this rely, PubMed waypoints. The newspapers inspiration since WS4A gulf taken instance drama the precision, to and category - measure assumption obtained 2004 overseeing w3.", "histories": [["v1", "Tue, 27 Sep 2016 15:14:04 GMT  (326kb,D)", "https://arxiv.org/abs/1609.08492v1", "7 pages, 1 figure, 1 table, accepted as poster at BioASQ '16"], ["v2", "Thu, 17 Nov 2016 12:12:15 GMT  (355kb,D)", "http://arxiv.org/abs/1609.08492v2", "7 pages, 1 figure, 1 table, accepted as poster at BioASQ '16"]], "COMMENTS": "7 pages, 1 figure, 1 table, accepted as poster at BioASQ '16", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["miguel j rodrigues", "miguel fal\\'e", "re lamurias", "francisco m couto"], "accepted": false, "id": "1609.08492"}, "pdf": {"name": "1609.08492.pdf", "metadata": {"source": "CRF", "title": "WS4A: a Biomedical Question and Answering System based on public Web Services and Ontologies", "authors": ["Miguel J. Rodrigues", "Miguel Fal\u00e9", "Andre Lamurias", "Francisco M. Couto"], "emails": ["mrodrigues@lasige.di.fc.ul.pt"], "sections": [{"heading": null, "text": "Keywords: natural language processing, web services, name entity recognition, information retrieval, support vector machines, semantic similarity, question and answering"}, {"heading": "1 Introduction", "text": "This paper describes our participation in the BioASQ challenge edition of 20161, a challenge on large-scale biomedical semantic indexing and question answering. Our participation focused on the Question and Answering (QA) task 4b, which consisted on the retrieval of relevant concepts, documents, snippets, RDF triples, exact answers and \u201dideal answers\u201d for each given question. The competition was composed of two phases: in the first one the goal was to take as input a set of queries, and respond with set of the most relevant concepts, articles, snippets and RDF triples. The ontologies and terminologies from where the list of concepts were to be retrieved are: the Medical Subject Headings (MeSH); the Gene Ontology (GO); the Universal Protein Resource (UniProt); the Joint Chemical Dictionary (Jochem); the Disease Ontology (DO). In the second phase, gold responses to the questions answered of the first phase were provided, so this time, the goal was to provide a response by exploring the correct documents and snippets gathered by these experts. This challenge required the use of selected ontologies and answers in a required format according to the type of query.\n? Corresponding author: mrodrigues@lasige.di.fc.ul.pt 1 http://www.bioasq.org/participate/challenges\nar X\niv :1\n60 9.\n08 49\n2v 2\n[ cs\n.C L\n] 1\n7 N\nov 2\n01 6\nWe developed the system WS4A (Web Services for (4) All), which novelty was to explore every possible option to use public web services and incorporate available domain knowledge. WS4A addressed the above tasks by first recognizing relevant terms in the query and also in the abstracts associated with it based on available Web Services. Next, the system mapped those terms to the respective concepts in ontologies and terminologies presented below. Then, WS4A compared those concepts to identify the responses that shared most concepts with the ones associated to the query. WS4A employed semantic similarity to measure how close in meaning they are even if they do no share the same exact concepts. Additionally, WS4A used Machine Learning [1] techniques to classify if an abstract is either relevant or not for the given query.\nSection 2 describes the exploited Web Services by WS4A, and Section 3 explains its composing modules. In Sections 4 and 5, we present and discuss our results, along with future work."}, {"heading": "2 Web Services", "text": "The first Web Service used was the one provided by BioPortal [2], that given a text returns the ontology concepts mentioned in it. There are some other parameters that WS4A explored, but with the purpose of better filtering the results, such as: i) longest annotation only (set to false); ii) number exclusion (set to false); iii) whole word only (set to true) and iv) synonym exclusion (set to false). The results were provides as a JSON dictionary, divided by annotation and ontology.\nWith the UniProt Web Services [3], we may gather PubMedIds from protein descriptions, but to get these, we need to use another Web Service in order the obtain this description, the Whatizit2 Web Service.\nWhatizit [4] is an alternative to BioPortal, when it comes to identifying specific terms in sentences. The Web Service present in Whatizit identifies the words in a sentence according to a given vocabulary and as a result provides a XML with an identifier of the Uniprot database in one of its tags.\nBack to UniProt, the request is made in the following format: i) the Web Service URL; ii) followed by a protein identifier; iii) ending with the available format that we chose (from our understanding, XML and HTML formats were available). For example: http://www.uniprot.org/uniprot/P12345.xml\nWhen it comes to retrieving PubMed identifiers (PubMed IDs), the PubChem Web Service [5] was the last one to be used. This Web Service provided an easy interface, since it required trivial parameters and URLs. Each request was structured in the following way:\n\u2013 Base URL: http://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/ name/ \u2013 A word in the question \u2013 Data Format: xrefs/PubMedID/JSON. Depending on the word, the result set\n(in XML) would be empty or not. If not, it meant that some PubMed IDs were present.\n2 http://www.ebi.ac.uk/webservices/whatizit/info.jsf\nAn example of the request for \u201doxygen\u201d in the JSON data format would be: https:// pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/oxygen/xrefs/ PubMedID/JSON,\nNCBI has a service, eutils [6], that provides a broad range of options. Two of them were used in order to retrieve all PubMed articles by their identifiers. The dates of these articles had to be before November 19th, 2015. The URLs are easy to identify and build, since they have a specific format, for example http://www.ncbi.nlm. nih.gov/pubmed/23687640where 223687640 is the PubMed ID. From eutils we used two services, one that searches for PubMed IDs with MeSH annotations for example: http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch. fcgi?db=pubmed&mindate=2014&maxdate=2015/11/19&term=+AND+%28GENES% 5Bmesh%5D+%29; and one that fetches the abstracts from the Ids retrieved from the previous URL, for example: http://eutils.ncbi.nlm.nih.gov/entrez/ eutils/efetch.fcgi?db=pubmed&retmode=xml&id=26580448,26580161, 26575237,26577665.\nFinally, in the Answer Builder module, we use yet another Web Service where we use Linked Life Data and their SPARQL query endpoint, to generate rdf triples according to the ranked abstracts and annotations.3)"}, {"heading": "3 Pipeline", "text": "Figure 1 shows the modular nature of WS4A, where each module was designed to be as much independent from the others as possible.\nWe have a module, Annotate Text, where question annotations by ontologies takes place. In this module, the Web Services used are the ones described in Section 2, in particular the BioPortal web services, which returns the annotations by their respective\n3 http://linkedlifedata.com\nontologies. Following the annotation retrieval, each annotation is used to gather the respective PubMed IDs related to these annotations using the NCBI module, which also includes a local data structure containing the CheBi ontology.\nAfter the PubMed IDs are gathered into one structure, only the ten most recent articles are used, independently of the ontology it comes from. This measure was taken just to improve temporal performance. The Abstract Annotator module comes in use: we take these 10 abstracts in order to obtain annotations from the abstracts per ontology. This module is in fact the Annotate Text module, which reuses the same functions and methods present previously, but changing the input to abstracts instead of questions.\nIn the Abstract Evaluator module, only the abstracts that are deemed to contain useful information are used in the response. For each abstract, the following scores are registered for each abstract:\n\u2013 Jaccard score between the query and abstract annotations 4\n\u2013 hierarchical distance score between the query and abstract annotations\n\u2013 frequency between the top semantic annotations of the abstract and query\n\u2013 sentence semantic similarity score 5\nAfter these steps, a grade is given to the abstract using these scores. Depending on the operation being computed (training or classification of abstracts), there are two possible courses of action: approval (or disapproval) of the abstract, so it can take part in the response in case of being in the training phase; otherwise, it adds the scores to a main structure, along with the abstract to be used to generate the n-grams.\nAfter ranking and selecting the abstracts, we use another module, Answer Builder, that selects the 10 (according to the BioASQ\u2019s rules) best valued snippets from the semantic analysis. Also, we generate the concepts required for the response. We take the initial annotated terms from the query with no regard of ontology. WS4A does the same to the annotations from the abstracts, but sums up the semantic similarity from each annotation, selecting the top 10 annotations by their score.\nThe following step consists of taking these top 10 annotations from the abstracts, intercept them with the original annotations. With this result, we use only the MeSH annotations, to generate the RDF triples. To filter the resulting great amount of rdf triples, we use tf-idf in order to obtain those top 10 that add more content.\nThe challenge provided training and test datasets that we used for our machine learning approach. In CLEF 2015 [7] among the systems that entered the BioASQ task 3b [8], we noted that OAQA [9] used supervised learning techniques to predict answer types (answer type coercion) and score candidate answers. We used Support vector machines [10,1] to classify the relevance of the abstracts to a given query. From the Abstract Evaluation module we select to work as features the four scores of the Abstract Evaluator module and the top 5000 n-grams."}, {"heading": "4 Results", "text": "Table 1 shows WS4A results for every batch and using the final version of the system. The times were obtained in a desktop computer equipped with an Intel(R) Core(TM)2 Duo CPU and 6 GB RAM.\nOur results, among all participants placed on the lower half of the result table, with the exception of the fourth batch results in which we did not submit any result set. The best result F-measure scored 0.24 in average, and so the overall results were also not very high. This results reflect phase A. In phase B, we were able to achieve a top 3 result for the first batch, obtaining lower level table results for the other batches. We achieved two second places in the first batch for the exact answers and ideal answers category.\nComparing to last year\u2019s results [8], WS4A was far from achieving a comparable performance mainly because we registered for this competition some weeks before the date of the first batch release and no time for tuning the system was available. Also, in this year\u2019s competition, the type of answer wanted would already be given, so last year\u2019s results are not strictly comparable comparing to ours.\nThe average time to answer a question was of around 90 seconds, with slow internet connection sometimes piking the time for obtaining an answer. W4SA was not developed to obtain highly accurate results but instead to use as much web services as possible. We demonstrated that such approach is feasible, however, we faced some issues related to availability faults in these services that hindered the progress of our system and the generation of the answers file.\nWe did not notice any other limitations, such as query limits (at least that we found or read about), but the servers would sometimes be unavailable for some reason for which we could never figure out the cause, and since this problem was quite random, we think it did not have anything to do with our research.\n4 http://ag.arizona.edu/classes/rnr555/lecnotes/10.html 5 http://sujitpal.blogspot.pt/2014/12/semantic-similarity-for-short-sentences. html"}, {"heading": "5 Conclusions", "text": "In this paper we showed the feasibility of developing a question and answering system mostly based on web services. Many of the techniques employed in WS4A have been used previously in other systems. For example, IIT [11] retrieved documents using PubMed\u2019s web services like WS4A did, and extracted snippets based on similarity between sentences of the retrieved documents and the query. LIMSI-CNRS [12] used WordNet relations (namely, synonyms) when comparing between words from the query and the answer choice, and the given short text. WS4A also used WordNet, when comparing between the words in the query and the words in the abstracts. The main difference between WS4A and the aforementioned systems, is that our system is mainly based on web services and fully explores the semantics given by the ontologies. Thus WS4A is a light system that can be easily deployed, and which is continuously updated given the extensive use of web services.\nIn the future, we intend to explore semantic similarity measures [13], and build a cache database in order to save and revisit results. Snippet generation can also be improved through semantic similarity, as CoMiC [14] used semantic similarity for the short text\u2019s segmentation in the Entrance Exams task 2015 (using the C99 algorithm). We also plan on improving the annotation gathering functionality, and include other sources such as DBpedia6 and YAGO7 for obtaining such annotations. YodaQA [15] resorts to DBPedia Spotlight, a service that automatically annotates DBPedia concepts from plain text."}, {"heading": "Acknowledgments", "text": "This work was supported by FCT through funding of LaSIGE Research Unit, ref. UID/CEC/00408/2013."}], "references": [{"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V Dubourg"], "venue": "The Journal of Machine Learning Research 12", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Bioportal: ontologies and integrated data resources at the click of a mouse", "author": ["N.F. Noy", "N.H. Shah", "P.L. Whetzel", "B. Dai", "M. Dorf", "N. Griffith", "C. Jonquet", "D.L. Rubin", "M.A. Storey", "Chute", "C.G"], "venue": "Nucleic acids research", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Uniprot: a hub for protein information", "author": ["U Consortium"], "venue": "Nucleic acids research", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Text processing through web services: calling whatizit", "author": ["D. Rebholz-Schuhmann", "M. Arregui", "S. Gaudan", "H. Kirsch", "A. Jimeno"], "venue": "Bioinformatics 24(2)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Pubchem: a public information system for analyzing bioactivities of small molecules", "author": ["Y. Wang", "J. Xiao", "T.O. Suzek", "J. Zhang", "J. Wang", "S.H. Bryant"], "venue": "Nucleic acids research", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Building customized data pipelines using the entrez programming utilities (eutils)", "author": ["E. Sayers", "D. Wheeler"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Preface", "author": ["Linda Cappellato", "G.F.J. Nicola Ferro", "E.S. Juan"], "venue": "CLEF 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Results of the bioasq tasks of the question answering lab at clef 2015", "author": ["G. Balikas", "A. Kosmopoulos", "A. Krithara", "G. Paliouras", "I. Kakadiaris"], "venue": "CLEF 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to answer biomedical factoid & list questions: Oaqa at bioasq 3b", "author": ["Zi Yang", "Niloy Gupta", "X.S.D.X.C.Z.", "E. Nyberg"], "venue": "CLEF 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Making large scale svm learning practical", "author": ["T. Joachims"], "venue": "Technical report, Universit\u00e4t Dortmund", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Iiith at bioasq challange 2015 task 3b: Bio-medical question answering system", "author": ["Harish Yenala", "M.S. Avinash Kamineni", "M. Chinnakotla"], "venue": "CLEF 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Limsi-cnrs@clef 2015: Tree edit beam search for multiple choice question answering", "author": ["M. Gleize", "B. Grau"], "venue": "CLEF 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "The next generation of similarity measures that fully explore the semantics in biomedical ontologies", "author": ["F.M. Couto", "H.S. Pinto"], "venue": "Journal of bioinformatics and computational biology 11(05)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Comic: Exploring text segmentation and similarity in the english entrance exams task", "author": ["B.R. Ramon Ziai"], "venue": "CLEF 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Biomedical question answering using the yodaqa system: Prototype notes", "author": ["J.S. Petr Baudis"], "venue": "CLEF 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Additionally, WS4A used Machine Learning [1] techniques to classify if an abstract is either relevant or not for the given query.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "The first Web Service used was the one provided by BioPortal [2], that given a text returns the ontology concepts mentioned in it.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "With the UniProt Web Services [3], we may gather PubMedIds from protein descriptions, but to get these, we need to use another Web Service in order the obtain this description, the Whatizit2 Web Service.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "Whatizit [4] is an alternative to BioPortal, when it comes to identifying specific terms in sentences.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "When it comes to retrieving PubMed identifiers (PubMed IDs), the PubChem Web Service [5] was the last one to be used.", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "PubMedID/JSON, NCBI has a service, eutils [6], that provides a broad range of options.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "In CLEF 2015 [7] among the systems that entered the BioASQ task 3b [8], we noted that OAQA [9] used supervised learning techniques to predict answer types (answer type coercion) and score candidate answers.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "In CLEF 2015 [7] among the systems that entered the BioASQ task 3b [8], we noted that OAQA [9] used supervised learning techniques to predict answer types (answer type coercion) and score candidate answers.", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "In CLEF 2015 [7] among the systems that entered the BioASQ task 3b [8], we noted that OAQA [9] used supervised learning techniques to predict answer types (answer type coercion) and score candidate answers.", "startOffset": 91, "endOffset": 94}, {"referenceID": 9, "context": "We used Support vector machines [10,1] to classify the relevance of the abstracts to a given query.", "startOffset": 32, "endOffset": 38}, {"referenceID": 0, "context": "We used Support vector machines [10,1] to classify the relevance of the abstracts to a given query.", "startOffset": 32, "endOffset": 38}, {"referenceID": 7, "context": "Comparing to last year\u2019s results [8], WS4A was far from achieving a comparable performance mainly because we registered for this competition some weeks before the date of the first batch release and no time for tuning the system was available.", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "For example, IIT [11] retrieved documents using PubMed\u2019s web services like WS4A did, and extracted snippets based on similarity between sentences of the retrieved documents and the query.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "LIMSI-CNRS [12] used WordNet relations (namely, synonyms) when comparing between words from the query and the answer choice, and the given short text.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "In the future, we intend to explore semantic similarity measures [13], and build a cache database in order to save and revisit results.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Snippet generation can also be improved through semantic similarity, as CoMiC [14] used semantic similarity for the short text\u2019s segmentation in the Entrance Exams task 2015 (using the C99 algorithm).", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "YodaQA [15] resorts to DBPedia Spotlight, a service that automatically annotates DBPedia concepts from plain text.", "startOffset": 7, "endOffset": 11}], "year": 2016, "abstractText": "This paper describes our system, dubbed WS4A (Web Services for All), that participated in the fourth edition of the BioASQ challenge (2016). We used WS4A to perform the Question and Answering (QA) task 4b, which consisted on the retrieval of relevant concepts, documents, snippets, RDF triples, exact answers and \u201dideal answers\u201d for each given question. The novelty in our approach consists on the maximum exploitation of existing web services in each step of WS4A, such as the annotation of text, and the retrieval of metadata for each annotation. The information retrieved included concept identifiers, ontologies, ancestors, and most importantly, PubMed identifiers. The paper describes the WS4A pipeline and also presents the precision, recall and f-measure values obtained in task 4b. Our system achieved two second places in two subtasks on one of the five batches.", "creator": "LaTeX with hyperref package"}}}