{"id": "1609.08843", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Hierarchical Memory Networks for Answer Selection on Unknown Words", "abstract": "Recently, under - when - first emotional spectrum least much promising week on Question Answering role, instead wirelessly an over implications into giving regarding memory and rarely explanations ability by could require methodologies steps taking the virtual. However, computers networks subject the practical on sentence - level memory from generated sprinkling computation one-dimensional and mean way further take making latest enable both focus coming wrote, either any thanks it was modified lose some exactly information, especially when the listen are notable same an explaining. In is book, we proposal turn adventures Hierarchical Memory Networks, branded HMN. First, thing encode the past facts into sentence - reduced repair several example - level image 37. Then, (k) - jensen pooling is benefited following grasp module one the sentence - focus memory the sample the ([) most appropriate prosecutors to a obviously and feed same prisoner along making application month though slang - level applications to economic under describe day the represented sentences. Finally, for scenario kind also anyone over, interfacing taken given sentence - level teleological kvant-1 and over something - reach attention mechanism. The experimental results prove that opportunity ways successfully conducts certainly selection on described instance without achieves when enough exciting as memory networks.", "histories": [["v1", "Wed, 28 Sep 2016 10:03:05 GMT  (531kb)", "http://arxiv.org/abs/1609.08843v1", "10 pages, to appear in COLING 2016"]], "COMMENTS": "10 pages, to appear in COLING 2016", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["jiaming xu", "jing shi", "yiqun yao", "suncong zheng", "bo xu", "bo xu"], "accepted": false, "id": "1609.08843"}, "pdf": {"name": "1609.08843.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Memory Networks for Answer Selection on Unknown Words", "authors": ["Jiaming Xu", "Jing Shi", "Yiqun Yao", "Suncong Zheng", "Bo Xu"], "emails": ["yaoyiqun2014}@ia.ac.cn", "xubo}@ia.ac.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n08 84\n3v 1\n[ cs\n.I R\n] 2\n8 Se\np 20\n16"}, {"heading": "1 Introduction", "text": "With the recent resurgence of interest in Deep Neural Networks (DNN), many researchers have concentrated on using deep learning to solve natural language processing (NLP) tasks (Collobert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015). The main merits of these representation learning based methods are that they do not rely on any linguistic tools and can be applied to different languages or domains. However, the memory of these methods, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) compressing all the external sentences into a fixed-length vector, is typically too small to accurately remember facts from the past, and may lose important details for response generation (Shang et al., 2015). Due to the drawback, these traditional DNN models encounter great limitation on Question Answering (QA), as a complex NLP task, which requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question (Hermann et al., 2015; Yu et al., 2015).\nRecently, lots of deep learning methods with explicit memory and attention mechanism are explored for Question Answering (QA) task, such as Memory Networks (MemNN) (Sukhbaatar et al., 2015), Neural Machine Translation (NMT) and Neural Turing Machine (NTM) (Yu et al., 2015). These methods exploit a external memory to store the past sentences with a continuous representation and utilize attention mechanism to automatically soft-search for parts of the memory for prediction. Compared with NMT and NTM, MemNN, making multiple computational steps (termed as \u201chops\u201d) on the memory before making an output, is better qualified for textual reasoning tasks. However, for QA task, MemNN only conducts the reasoning on sentence-level memory and does not further take any attention mechanism to focus on words in the retrieved facts. More recently, Yu et al. (2015) constructed a Search-Response\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ \u2217The first two authors contributed equally.\npipeline where Search component uses MemNN to search the supporting sentences and Response component uses NMT or NTM to generate answer on the selected sentences. However, that work needs the supervision of the supporting facts to guide the training of Search component and the combination of these two components through a separate training way may hurt the performance. Along the direction of that work, we believe that a joint learning model can achieve a better performance by designing a hierarchical architecture, with sentence-level and word-level components, which has shown promising results on document modeling (Lin et al., 2015) and document classification (Yang et al., 2016).\nBesides, rare and unknown word problem as an important issue should be considered in NLP tasks, especially for QA task, where the words that we are mainly interested in are usually named entities which are mostly unknown or rare words (Marrero et al., 2013; Gulcehre et al., 2016). In order to control the computational complexity, many methods limit the trained vocabulary size, which further leads to lots of low-frequency words outside the trained vocabulary (Li et al., 2016). Traditional methods directly mask the rare or unknown words with meaningless unk which may lose the important information for answer selection task. For example, given a set of sentences as follows:\n1. Miss, what is your name? 2. Uh, my name is Wainwright. 3. Please tell me your passport number. 4. Ok, it is 899917359.\nAssume that the words Wainwright and 8999173591 are rare words or outside the trained vocabulary. If these words are discarded or replaced with unk symbol, any models may not be able to select the correct answers for response during testing.\nBased on the above observations, this paper proposes a Hierarchical Memory Networks2 (dubbed to HMN) for answer selection. Our method first maps the sentences into a sentence-level memory and reasoning module takes multiple hops on the sentence-level memory to soft-search the related sentences. Meanwhile, all words in the sentences are encoded into a word-level memory with recurrent neural networks. Then, we exploit k-max pooling to sample the most relevant sentences and feed these selected sentences into attention mechanism on the word-level memory to focus the words. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. Our main contributions are three-fold:\n(1). We proposed a novel hierarchical memory networks for answer selection, where the reasoning module is performed on sentence-level memory to retrieve the relevant sentences and the attention mechanism is applied on word-level memory to focus the words. This hierarchical architecture allows the model to have explicit reasoning ability on sentences and also focus on more fine-grained words.\n(2). k-max pooling is exploited to sample the most relevant facts based on the results of the sentencelevel reasoning and then feed these facts into word-level attention mechanism, which can filter the noise information and also reduce the computational complexity on word-level attention.\n(3). We release four synthetic domain dialogue datasets3 , two from air-ticket booking domain and two from hotel reservation domain, where the answers are mostly rare or unknown words, and lots of answers should be reasoned based on some supporting sentences. The experimental results show that our approach can successfully conduct answer selection on unknown words."}, {"heading": "2 Background: Memory Networks", "text": "Here, we give a brief description of memory networks which have shown promising results on QA tasks (Weston et al., 2015; Bordes et al., 2015; Sukhbaatar et al., 2015). Memory network first introduced by Weston et al. (2015) is a new class of learning models which can easily read and write to part of a long-term memory component, and combine this seamlessly with inference for prediction. Formally, besides the explicit memory which is an array of cells to memorize the pre-trained vector representations\n1Note that the personal information used in our examples and datasets throughout this paper is all synthetic and not real. 2It is worth noticing that the term \u201cHierarchical Memory Networks\u201d has been mentioned in (Chandar et al., 2016) where the intention was to organize the memory into multi-level groups based on hashing, tree or clustering structures to make the reader efficiently access the memory, whereas in our paper the term has a different meaning.\n3Our code and dataset are available: https://github.com/jacoxu/HMN4QA\nof the external data, a general memory network consists of four major components: (1). Input feature map which converts the incoming input to the internal feature representation. (2). Generalization which updates old memories given the new input. (3). Output feature map which produces a new output based on the new input and the current state. (4). Response which converts the output into the response format desired. Along the above framework, Sukhbaatar et al. (2015) put forward end-to-end memory networks which do not require the supervision of the supporting facts and are more generally applicable in realistic setting. Thus, we choose end-to-end memory networks, denoted as MemNN throughout our paper, as the foundation of our proposed approach."}, {"heading": "3 Hierarchical Memory Networks for Answer Selection", "text": ""}, {"heading": "3.1 Approach Overview", "text": "As described in Figure 1(a), we give an illustration of our HMN for answer selection. Given a set of n sentences denoted as: X = {xi}i=(1,2,...,n) and a query q, where i is the timestep of sentence xi in the set. We first map these sentences X into the sentence-level memory M(S) and the word-level memory M\n(W ) with low-dimensional distributed representations respectively. Then, reasoning on the sentencelevel memory is utilized to soft-search the related sentences. We further exploit k-max pooling to sample the most relevant sentences based on the soft-searching results and take attention mechanism to focus on word-level memory of the selected sentences. The target answer y is used to guide the learning of the reasoning on sentence-level memory and the attention on word-level memory learning simultaneously."}, {"heading": "3.2 Sentence-level Memory and Reasoning", "text": "In this section, we apply reasoning module to make multiple interaction on sentence-level memory based on the adjacent weight tying scheme of MemNN (Sukhbaatar et al., 2015), as shown in Figure 1(b). Given two word embedding matrices A \u2208 R|V |\u00d7d and C \u2208 R|V |\u00d7d, where |V | is the vocabulary size and d is the dimension of the word embedding, we first encode the word xij at timestep j in the sentence xi into dual channels of word representation as Axij \u2208 Rd and Cxij \u2208 Rd.\nIn order to combine the order of the words into their representations, a positional encoding matrix l is applied to update the dual-channel word embeddings as lgj \u00b7 (Axij) and lgj \u00b7 (Cxij), where\nlgj = (1\u2212 j/Ji)\u2212 (g/d)(1 \u2212 2j/Ji), 1 \u2264 j \u2264 Ji, 1 \u2264 g \u2264 d, (1)\nand Ji is the length of the sentence xi and g is the embedding index. This positional encoding scheme is also successfully applied in (Xiong et al., 2016).\nTwo temporal encoding matrices TA \u2208 Rn\u00d7d and TC \u2208 Rn\u00d7d are further utilized to encode the order of the sentences. Then, the sentence-level memory M(S) = {{ai}, {ci}}i=(1,2,...,n) is reformed as:\nai = \u2211\nj lj \u00b7 (Axij) +TA(i), ci = \u2211 j lj \u00b7 (Cxij) +TC(i), (2)\nwhere lj is the j-th column vector of the position encoding matrix l according to the sentence xi and the operation \u201c\u00b7\u201d means the element-wise multiplication.\nFor the query q, the j-th word qj is also embedded as Aqj \u2208 Rd, where the A is the embedding matrix used in Eqn. (2). By encoding the word position j into the query representation, we get the probe representation of the query q as follows:\nu (S) 1 = \u2211 j lj \u00b7 (Aqj), (3)\nwhere lj is the j-th column vector of the position encoding matrix l according to the query q. Then the attention weights of the sentences according to the query can be calculated through the inner product of the two vectors as \u03b1(S)i = softmax(a T i u (S) 1 ), and the output of the sentence-level memory based on the activation of the query can be obtained as: o1 = \u2211 i \u03b1 (S) i ci.\nIn order to perform reasoning on sentence-level memory to find the most relevant sentences, we make R hops to soft-search the sentences and output the final vector oR. To be specific, during the r+1 hop of the reasoning operation, the process can be formalized as: u(S)r+1 = or+u (S) r , \u03b1 (S) i = softmax(a T i u (S) r+1), or+1 = \u2211 i \u03b1 (S) i ci, and the dual-channel memories are updated as follows:\nai = \u2211\nj lj \u00b7 (A\nr+1xij) +T r+1 A (i), ci = \u2211 j lj \u00b7 (C r+1xij) +T r+1 C (i), (4)\nwhere 1 \u2264 r \u2264 (R \u2212 1). Specifically, during the r + 1 hop, the word embedding matrices Ar+1 and C\nr+1 are mutually independent, so as the temporal encoding matrices Tr+1A and T r+1 C . But during the adjacent two hops, Ar+1 = Cr and Tr+1A = T r C . Finally, we can get the predicted word probability distribution by applying softmax on the output vector of the reasoning on the sentence-level memory as:\np(S)(w) = softmax((CR)T (oR + u (S) R )), (5)\nwhere w = {wt}t=(1,2,...,|V |) is the word set with a vocabulary size of |V |, the weight matrix is the same as the embedding matrix CR \u2208 R|V |\u00d7d on the last hop, and T is the operation of matrix transposition.\n3.3 k-max Pooling\nHere, we exploit a pooling operation over the top attention weights \u03b1(S) of the reasoning module on the sentence-level memory to sample the most relevant sentences. Given a value k and the top attention weights \u03b1(S) of length n \u2265 k, we use the k-max pooling to select a subset of sentence sequences X\u0302 = {x\u0302i}i=(1,2,...,k), corresponds with their top-k maximum values of \u03b1\n(S) on the sentences. The k-max pooling operation makes it possible to pool the k most relevant sentences to the query and filter the noise information, which maybe more beneficial to select the correct answers. Moreover, this sampling module feeds a subset of sentences X\u0302 to the following attention mechanism on the word-level memory, which can reduce the computation complexity of the attention to focus on the relevant words."}, {"heading": "3.4 Attention on Word-level Memory", "text": "For word-level memory, we first apply a Bi-directional GRU (BiGRU) to compute the hidden states of all the ordered words w\u0304 = {w\u0304t}t=(1,2,...,|t|) in the sentence set X, where |t| is the time steps of the words in the sentences. In particular, for the t-th word w\u0304t, the forward GRU and the backward GRU encode\nit as hidden states ~ht = \u2192 GRU(CRw\u0304t) and \u2190 ht = \u2190 GRU(CRw\u0304t) respectively, where CR is the word embedding matrix of the last hop on the sentence-level memory, and we set the dimension of ~ht and \u2190\nht equals to the dimension of the word embedding. By summing the forward hidden states and the backward hidden states, we obtain the word-level memory as M(W ) = {mt}t=(1,2,...,|t|), where mt = ~ht + \u2190\nht. In this way, the memory mt contains the context information of the t-th word w\u0304t in the sentence set X.\nThen, we perform attention on the subset of the ordered words w\u0302 = {w\u0302t}t=(1,2,...,|t\u0302|) in the selected sentences X\u0302 by using the probe vector u(S)R of the last hop on the sentence-level memory and a subset of word-level memory {m\u0302t}t=(1,2,...,|t\u0302|) selected form M (W ) according to the word subset w\u0302. The normalized attention weights \u03b1(W ) = {\u03b1(W )t }t=(1,2,...,|t\u0302|) on the word-level memory are calculated as:\n\u03b1 (W ) t = softmax(v T tanh(Wu (S) R +Um\u0302t)), (6)\nwhere v \u2208 Rd\u00d71, W \u2208 Rd\u00d7d and U \u2208 Rd\u00d7d are all learning parameters updated during the training. Inspired by Pointer Networks (Vinyals et al., 2015), we adopt the normalized attention weights \u03b1(W ) on the word collection w\u0302 as the probability distribution of the output words:\np(W )(w) = trans(p(W )(w\u0302)) = trans(\u03b1(W )), (7)\nwhere trans(\u00b7) means the operation to map the words probability distribution p(W )(w\u0302) \u2208 R|t\u0302| into the probability distribution p(W )(w) \u2208 R|V |. To be specific, the map operation makes the probability distribution p(W )(w\u0302) of the word subset (w\u0302 = {w\u0302t}t=(1,2,...,|t\u0302|)) to be added into their corresponding positions in the vocabulary (w = {wt}t=(1,2,...,|V |)), and the probabilities of the words not in selected word subset w\u0302 will be set to zero4. Finally, we get the new probability distribution p(W )(w) \u2208 R|V |."}, {"heading": "3.5 Joint Learning", "text": "In this paper, we combine the probability distributions of the output words both on the sentence-level memory and the word-level memory to predict the joint probability distribution p(w) as follows:\np(w) = p(S)(w) + p(W )(w). (8)\nFinally, we use the target answer y to guide the learning of the reasoning module on sentence-level memory and the attention module on word-level memory simultaneously. We choose the cross entropy as the cost function and apply Stochastic Gradient Descent (SGD) (Bottou, 1991) as the optimization method to train our joint model. The learned parameters include word embedding matrices A1 and {Cr}r=(1,2,...,R), temporal encoding matrices T 1 A and {T r C}r=(1,2,...,R) in Eqn. (2) and (4), the parameters {\u03b8BiGRU} of the BiGRU model and the attention parameters v, W and U in Eqn. (6).\n4For example, if the word \u201cairport\u201d, at two different timesteps in the ordered word subset w\u0302, has two probabilities \u201c0.1\u201d and \u201c0.3\u201d, the map operation would add up these probabilities and set \u201c0.4\u201d as the probability of the word \u201cit\u201d in the vocabulary."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets and Setup", "text": "We conduct answer selection tasks on four synthetic domain dialogue datasets, two from air-ticket booking domain and two from hotel reservation domain. One complete dialogue history of each dataset has eight round responses. Besides greeting and ending sentences, one dialogue history consists six round responses to query and answer client\u2019s personal information, such as name, phone and passport number. The datasets contain hundreds of response patterns and thousands of entity information. More detailed descriptions can be found in our released datasets. The statistics of the datasets are summarized in Table 1. We use 45% of the data for training, 5% for validation and the remaining 50% for test. From the statistics, we can see that the proportions of the unseen answers on dev/test sets all overtake 57%.\nIn our experiments, the most of hyper parameters are set uniformly for the datasets as described in Table 3. The training gradients with an l2 norm larger than 40 are clipped to 40 and the learning rate is annealed every 15 epochs by \u03bb/2 until 60 epochs are reached. The learned parameters are all initialized randomly from a Gaussian distribution with zero mean and 0.1 standard deviation. In order to make the comparison more intuitive, we use the number of predicted error samples on each dataset to evaluate the performance for answer selection and calculate the average result by repeating each experiment 5 times."}, {"heading": "4.2 Comparison with Memory Networks", "text": "In order to evaluate the effect of multiple hops for reasoning module and temporal encoding on sentencelevel memory, we design three MemNN (Sukhbaatar et al., 2015) based variants: MemNN-H1 (1 hop and temporal encoding), MemNN-NT (3 hops but not temporal encoding) and MemNN (3 hops and temporal encoding) on the datasets. We further evaluate the prediction performance of our HMN on different level memory components via HMN-Sent (prediction of reasoning module on sentence-level memory as Eqn. (5)), HMN-Word (prediction of attention module on word-level memory as Eqn. (7)) and HMN-Joint (joint prediction as Eqn. (8)). The comparison of these methods are reported in Table 2. From the results, we can see that MemNN-H1 without temporal encoding and MemNN-NT without multi-hop reasoning make the worst performances, which clearly demonstrate that multiple hops for reasoning module and temporal encoding on sentence-level memory play a very important role on our tasks. Despite that MemNN represents surprising results on this task, our HMN-Word and HMN-joint\nfurther improve the answer selection performance on all the datasets. Compared with the results of HMNSent and HMN-Word, the results also show that the joint prediction can make a better performance."}, {"heading": "4.3 How to Select the Correct Answers on Unknown Words", "text": "Here, we try to answer two questions: (1) How does the reasoning module focus on the related sentences and predict the rare and unknown words on sentence-level memory? (2) How does the attention module focus on the correct answers and distinguish multiple rare and unknown words on word-level memory? We give a visual example of our HMN over one dialogue history for answer selection in Figure 2 to get a better understanding of the effect of each module. The example is one dialogue history from air-ticket booking domain which contains 16 sentences associated with their temporal indexs.\nFrom Figure 2, we can see that in the first hop, the reasoning module mainly focuses on the sentences 13 and 14, which have most semantic relevance to the question \u201cWhen does the client depart?\u201d. As the effect of temporal encoding as Eqn. (4), the reasoning allocates more weight to the most related sentence 14 in the following hops. Another interesting result as shown in Table 2 is that MemNN and HMN-Sent represent surprising performance to predict the rare and unknown words on sentence-level memory. An explanation maybe that the way in which these methods use a simple way, rather than sophisticated LSTM or GRU, to encoding the sentence into memory as Eqn. (1). This simple sentence encoding strategy can remain the raw embedding representation of words, and MemNN and HMN-Sent utilize the transpose of the raw embedding matrix as the decoding weights to conduct answer match as Eqn. (5) in the raw embedding space. Nonetheless, sentence-level encoding may introduce other semantic information which may lead to predict an error answer, as the prediction (Sent) in the example.\nAfter k-max pooling for sampling the most relevant sentences, a sophisticated attention mechanism is applied on the selected word-level memory. Two possible reasons make the attention successfully focus on the correct answers: One is that the probe vector u(S)R as Eqn. (6), used in attention mechanism to interact with word-level memory, is generated from the reasoning module and has semantic relevance to the\ntarget answers. Another reason is that attention mechanism performs inhibitory effect on our task which can successfully filter the almost useless words, such as \u201ctime\u201d, \u201cgo\u201d and \u201con\u201d in the example. Besides the above reasons, we also investigate the influence of different word-level memory encoding methods, such as BiGRU ( \u2192 GRU (CRw\u0304t)+ \u2190\nGRU (CRw\u0304t)), GRU (GRU(CRw\u0304t)) and Embedding (CRw\u0304t), by varying the values of k for max pooling, and the comparison of answer selection performance are present in Figure 3. The results show the expected effect that encoding the context information into word-level memory via BiGRU or GRU can help the attention module distinguish multiple rare and unknown words when we enlarge the value of k and introduce more unknown words to the attention mechanism.\nFrom Figure 3, we further investigate the influence of k to the answer selection performance. We can see that the performances almost unchanged by using HMN-Joint with BiGRU when we vary the value of k. Considering that the more sentences k-max pooling samples from sentence-level memory, the more computational complexity the attention mechanism as Eqn. (6) costs on word-level memory, 4-max pooling used in our experiments is a good trade-off."}, {"heading": "5 Related Works", "text": "Recently, lots of deep learning methods with explicit memory and attention mechanism have shown promising performance in Question Answering (QA) tasks. For example, Yu et al. (2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al., 2014) with distributed external memory to solve QA tasks, and Sukhbaatar et al. (2015) designed end-to-end memory networks and introduced multi-hop reasoning component to solve various types of QA task. These representation learning based methods do not rely on any linguistic tools and can be applied to different languages or domains (Feng et al., 2015). However, most works of these deep learning based methods rarely focus on solving answer selection on unknown word problem. Recently, the unknown word problem has attracted more researchers\u2019 attention. Hermann et al. (2015) used NLP tools to recognize all the entity and establish co-references to replace all the rare entities by placeholders and trained an attention based model with softmax to predict the placeholder id. Li et al. (2016) replaced the rare words in a test sentence with similarity in-vocabulary words to solve machine translation task, where the representation of the rare words still can be learned from a large mono-lingual corpus. Gulcehre et al. (2016) utilized and extended the attention-based pointing mechanism (Vinyals et al., 2015) to point the unknown words for machine translation and text summarization. However, the sophisticated attention mechanism is applied on the all word-level representations which may result in high computational complexity, and lots of the fine-grained noise words should be filtered out by reasoning the relevant facts to the query in a high-level semantic space."}, {"heading": "6 Conclusion", "text": "In this paper, we introduce hierarchical memory networks to solve answer selection problem on unknown words. We first encode the sentences into a sentence-level memory with temporal encoding. Then reasoning module conducts multi-hop interaction on the memory to retrieve the related sentences, and k-max pooling samples the k most related sentences. For word-level memory, BiGRU is utilized to encode the words and introduce context into the memory, then a sophisticated attention mechanism is applied on the selected word-level memory to focus the fine-grained words. We conduct answer selection experiments on four synthetic domain dialogue datasets which contain lots of unseen answers. The experimental results show that our hierarchical memory networks can achieve a satisfying performance."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their insightful comments, and this work was supported by the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005), the National High Technology Research and Development Program of China (863 Program) (Grant No. 2015AA015402) and the National Natural Science Foundation (Grant No. 61602479 and 61403385)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Large-scale simple question answering with memory", "author": ["Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Stochastic gradient learning in neural networks", "author": ["L\u00e9on Bottou"], "venue": "Proceedings of Neuro-N\u0131mes,", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["Feng et al.2015] Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Feng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Towards zero unknown in neural machine translation", "author": ["Li et al.2016] Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong"], "venue": "In Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Lin et al.2015] Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Named entity recognition: fallacies, challenges and opportunities", "author": ["Juli\u00e1n Urbano", "Sonia S\u00e1nchez-Cuadrado", "Jorge Morato", "Juan Miguel G\u00f3mez-Berb\u0131\u0301s"], "venue": "Computer Standards & Interfaces,", "citeRegEx": "Marrero et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Marrero et al\\.", "year": 2013}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Xiong et al.2016] Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML)", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Hierarchical attention networks for document classification", "author": ["Yang et al.2016] Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT)", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Empirical study on deep learning models for question answering", "author": ["Yu et al.2015] Yang Yu", "Wei Zhang", "Chung-Wei Hang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1510.07526", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of the 25th International Conference on Computational Linguistics (COLING),", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "1 Introduction With the recent resurgence of interest in Deep Neural Networks (DNN), many researchers have concentrated on using deep learning to solve natural language processing (NLP) tasks (Collobert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015).", "startOffset": 192, "endOffset": 278}, {"referenceID": 13, "context": "1 Introduction With the recent resurgence of interest in Deep Neural Networks (DNN), many researchers have concentrated on using deep learning to solve natural language processing (NLP) tasks (Collobert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015).", "startOffset": 192, "endOffset": 278}, {"referenceID": 17, "context": "1 Introduction With the recent resurgence of interest in Deep Neural Networks (DNN), many researchers have concentrated on using deep learning to solve natural language processing (NLP) tasks (Collobert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015).", "startOffset": 192, "endOffset": 278}, {"referenceID": 5, "context": "1 Introduction With the recent resurgence of interest in Deep Neural Networks (DNN), many researchers have concentrated on using deep learning to solve natural language processing (NLP) tasks (Collobert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015).", "startOffset": 192, "endOffset": 278}, {"referenceID": 3, "context": "However, the memory of these methods, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) compressing all the external sentences into a fixed-length vector, is typically too small to accurately remember facts from the past, and may lose important details for response generation (Shang et al.", "startOffset": 142, "endOffset": 160}, {"referenceID": 11, "context": ", 2014) compressing all the external sentences into a fixed-length vector, is typically too small to accurately remember facts from the past, and may lose important details for response generation (Shang et al., 2015).", "startOffset": 197, "endOffset": 217}, {"referenceID": 6, "context": "Due to the drawback, these traditional DNN models encounter great limitation on Question Answering (QA), as a complex NLP task, which requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question (Hermann et al., 2015; Yu et al., 2015).", "startOffset": 243, "endOffset": 282}, {"referenceID": 16, "context": "Due to the drawback, these traditional DNN models encounter great limitation on Question Answering (QA), as a complex NLP task, which requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question (Hermann et al., 2015; Yu et al., 2015).", "startOffset": 243, "endOffset": 282}, {"referenceID": 12, "context": "Recently, lots of deep learning methods with explicit memory and attention mechanism are explored for Question Answering (QA) task, such as Memory Networks (MemNN) (Sukhbaatar et al., 2015), Neural Machine Translation (NMT) and Neural Turing Machine (NTM) (Yu et al.", "startOffset": 164, "endOffset": 189}, {"referenceID": 16, "context": ", 2015), Neural Machine Translation (NMT) and Neural Turing Machine (NTM) (Yu et al., 2015).", "startOffset": 74, "endOffset": 91}, {"referenceID": 3, "context": "However, the memory of these methods, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) compressing all the external sentences into a fixed-length vector, is typically too small to accurately remember facts from the past, and may lose important details for response generation (Shang et al., 2015). Due to the drawback, these traditional DNN models encounter great limitation on Question Answering (QA), as a complex NLP task, which requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question (Hermann et al., 2015; Yu et al., 2015). Recently, lots of deep learning methods with explicit memory and attention mechanism are explored for Question Answering (QA) task, such as Memory Networks (MemNN) (Sukhbaatar et al., 2015), Neural Machine Translation (NMT) and Neural Turing Machine (NTM) (Yu et al., 2015). These methods exploit a external memory to store the past sentences with a continuous representation and utilize attention mechanism to automatically soft-search for parts of the memory for prediction. Compared with NMT and NTM, MemNN, making multiple computational steps (termed as \u201chops\u201d) on the memory before making an output, is better qualified for textual reasoning tasks. However, for QA task, MemNN only conducts the reasoning on sentence-level memory and does not further take any attention mechanism to focus on words in the retrieved facts. More recently, Yu et al. (2015) constructed a Search-Response", "startOffset": 143, "endOffset": 1515}, {"referenceID": 9, "context": "Along the direction of that work, we believe that a joint learning model can achieve a better performance by designing a hierarchical architecture, with sentence-level and word-level components, which has shown promising results on document modeling (Lin et al., 2015) and document classification (Yang et al.", "startOffset": 250, "endOffset": 268}, {"referenceID": 15, "context": ", 2015) and document classification (Yang et al., 2016).", "startOffset": 36, "endOffset": 55}, {"referenceID": 10, "context": "Besides, rare and unknown word problem as an important issue should be considered in NLP tasks, especially for QA task, where the words that we are mainly interested in are usually named entities which are mostly unknown or rare words (Marrero et al., 2013; Gulcehre et al., 2016).", "startOffset": 235, "endOffset": 280}, {"referenceID": 8, "context": "In order to control the computational complexity, many methods limit the trained vocabulary size, which further leads to lots of low-frequency words outside the trained vocabulary (Li et al., 2016).", "startOffset": 180, "endOffset": 197}, {"referenceID": 1, "context": "2 Background: Memory Networks Here, we give a brief description of memory networks which have shown promising results on QA tasks (Weston et al., 2015; Bordes et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 130, "endOffset": 197}, {"referenceID": 12, "context": "2 Background: Memory Networks Here, we give a brief description of memory networks which have shown promising results on QA tasks (Weston et al., 2015; Bordes et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 130, "endOffset": 197}, {"referenceID": 1, "context": ", 2015; Bordes et al., 2015; Sukhbaatar et al., 2015). Memory network first introduced by Weston et al. (2015) is a new class of learning models which can easily read and write to part of a long-term memory component, and combine this seamlessly with inference for prediction.", "startOffset": 8, "endOffset": 111}, {"referenceID": 12, "context": "Along the above framework, Sukhbaatar et al. (2015) put forward end-to-end memory networks which do not require the supervision of the supporting facts and are more generally applicable in realistic setting.", "startOffset": 27, "endOffset": 52}, {"referenceID": 12, "context": "2 Sentence-level Memory and Reasoning In this section, we apply reasoning module to make multiple interaction on sentence-level memory based on the adjacent weight tying scheme of MemNN (Sukhbaatar et al., 2015), as shown in Figure 1(b).", "startOffset": 186, "endOffset": 211}, {"referenceID": 14, "context": "This positional encoding scheme is also successfully applied in (Xiong et al., 2016).", "startOffset": 64, "endOffset": 84}, {"referenceID": 2, "context": "We choose the cross entropy as the cost function and apply Stochastic Gradient Descent (SGD) (Bottou, 1991) as the optimization method to train our joint model.", "startOffset": 93, "endOffset": 107}, {"referenceID": 12, "context": "2 Comparison with Memory Networks In order to evaluate the effect of multiple hops for reasoning module and temporal encoding on sentencelevel memory, we design three MemNN (Sukhbaatar et al., 2015) based variants: MemNN-H1 (1 hop and temporal encoding), MemNN-NT (3 hops but not temporal encoding) and MemNN (3 hops and temporal encoding) on the datasets.", "startOffset": 173, "endOffset": 198}, {"referenceID": 0, "context": "(2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 5, "context": "These representation learning based methods do not rely on any linguistic tools and can be applied to different languages or domains (Feng et al., 2015).", "startOffset": 133, "endOffset": 152}, {"referenceID": 11, "context": "For example, Yu et al. (2015) applied Neural Machine Translation (NMT) (Bahdanau et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 0, "context": "(2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al., 2014) with distributed external memory to solve QA tasks, and Sukhbaatar et al. (2015) designed end-to-end memory networks and introduced multi-hop reasoning component to solve various types of QA task.", "startOffset": 49, "endOffset": 246}, {"referenceID": 0, "context": "(2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al., 2014) with distributed external memory to solve QA tasks, and Sukhbaatar et al. (2015) designed end-to-end memory networks and introduced multi-hop reasoning component to solve various types of QA task. These representation learning based methods do not rely on any linguistic tools and can be applied to different languages or domains (Feng et al., 2015). However, most works of these deep learning based methods rarely focus on solving answer selection on unknown word problem. Recently, the unknown word problem has attracted more researchers\u2019 attention. Hermann et al. (2015) used NLP tools to recognize all the entity and establish co-references to replace all the rare entities by placeholders and trained an attention based model with softmax to predict the placeholder id.", "startOffset": 49, "endOffset": 739}, {"referenceID": 0, "context": "(2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al., 2014) with distributed external memory to solve QA tasks, and Sukhbaatar et al. (2015) designed end-to-end memory networks and introduced multi-hop reasoning component to solve various types of QA task. These representation learning based methods do not rely on any linguistic tools and can be applied to different languages or domains (Feng et al., 2015). However, most works of these deep learning based methods rarely focus on solving answer selection on unknown word problem. Recently, the unknown word problem has attracted more researchers\u2019 attention. Hermann et al. (2015) used NLP tools to recognize all the entity and establish co-references to replace all the rare entities by placeholders and trained an attention based model with softmax to predict the placeholder id. Li et al. (2016) replaced the rare words in a test sentence with similarity in-vocabulary words to solve machine translation task, where the representation of the rare words still can be learned from a large mono-lingual corpus.", "startOffset": 49, "endOffset": 957}, {"referenceID": 0, "context": "(2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al., 2014) with distributed external memory to solve QA tasks, and Sukhbaatar et al. (2015) designed end-to-end memory networks and introduced multi-hop reasoning component to solve various types of QA task. These representation learning based methods do not rely on any linguistic tools and can be applied to different languages or domains (Feng et al., 2015). However, most works of these deep learning based methods rarely focus on solving answer selection on unknown word problem. Recently, the unknown word problem has attracted more researchers\u2019 attention. Hermann et al. (2015) used NLP tools to recognize all the entity and establish co-references to replace all the rare entities by placeholders and trained an attention based model with softmax to predict the placeholder id. Li et al. (2016) replaced the rare words in a test sentence with similarity in-vocabulary words to solve machine translation task, where the representation of the rare words still can be learned from a large mono-lingual corpus. Gulcehre et al. (2016) utilized and extended the attention-based pointing mechanism (Vinyals et al.", "startOffset": 49, "endOffset": 1192}], "year": 2016, "abstractText": "Recently, end-to-end memory networks have shown promising results on Question Answering task, which encode the past facts into an explicit memory and perform reasoning ability by making multiple computational steps on the memory. However, memory networks conduct the reasoning on sentence-level memory to output coarse semantic vectors and do not further take any attention mechanism to focus on words, which may lead to the model lose some detail information, especially when the answers are rare or unknown words. In this paper, we propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the past facts into sentencelevel memory and word-level memory respectively. Then, k-max pooling is exploited following reasoning module on the sentence-level memory to sample the k most relevant sentences to a question and feed these sentences into attention mechanism on the word-level memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks.", "creator": "LaTeX with hyperref package"}}}