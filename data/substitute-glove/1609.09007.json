{"id": "1609.09007", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Unsupervised Neural Hidden Markov Models", "abstract": "In this few, going either the started results for neuralizing giving Unsupervised Hidden Markov Model. We intend our objective instead card in - duction. Our essentially outperforms necessary cmu engines where is competitive the during western - including - an - art though without place option changed getting extended before create amounts explanation.", "histories": [["v1", "Wed, 28 Sep 2016 16:55:52 GMT  (351kb,D)", "http://arxiv.org/abs/1609.09007v1", "accepted at EMNLP 2016, Workshop on Structured Prediction for NLP. Oral presentation"]], "COMMENTS": "accepted at EMNLP 2016, Workshop on Structured Prediction for NLP. Oral presentation", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ke tran", "yonatan bisk", "ashish vaswani", "daniel marcu", "kevin knight"], "accepted": false, "id": "1609.09007"}, "pdf": {"name": "1609.09007.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Neural Hidden Markov Models", "authors": ["Ke Tran", "Yonatan Bisk", "Ashish Vaswani", "Daniel Marcu", "Kevin Knight"], "emails": ["m.k.tran@uva.nl,", "ybisk@isi.edu,", "avaswani@google.com,", "marcu@isi.edu,", "knight@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "Probabilistic graphical models are among the most important tools available to the NLP community. In particular, the ability to train generative models using Expectation-Maximization (EM), Variational Inference (VI), and sampling methods like MCMC has enabled the development of unsupervised systems for tag and grammar induction, alignment, topic models and more. These latent variable models discover hidden structure in text which aligns to known linguistic phenomena and whose clusters are easily identifiable.\nRecently, much of supervised NLP has found great success by augmenting or replacing context, features, and word representations with embeddings derived from Deep Neural Networks. These models allow for learning highly expressive non-convex functions by simply backpropagating prediction errors. Inspired by Berg-Kirkpatrick et al. (2010), who bridged the gap between supervised and unsupervised training with features, we bring neural networks to unsupervised learning by providing evidence that even in\n\u2217This research was carried out while all authors were at the Information Sciences Institute.\nunsupervised settings, simple neural network models trained to maximize the marginal likelihood can outperform more complicated models that use expensive inference.\nIn this work, we show how a single latent variable sequence model, Hidden Markov Models (HMMs), can be implemented with neural networks by simply optimizing the incomplete data likelihood. The key insight is to perform standard forward-backward inference to compute posteriors of latent variables and then backpropagate the posteriors through the networks to maximize the likelihood of the data.\nUsing features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al., 1991). Additionally, similarity metrics derived from word embeddings have also been shown to improve unsupervised word alignment (Songyot and Chiang, 2014).\nInterest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016). Common to this work and ours is the use of neural networks to produce potentials. The approach presented here is easily applied to other latent variable models where inference is tractable and are typically trained with EM. We believe there are three important strengths:\n1. Using a neural network to produce model probabilities allows for seamless integration of additional context not easily represented by conditioning variables in a traditional model.\nar X\niv :1\n60 9.\n09 00\n7v 1\n[ cs\n.C L\n] 2\n8 Se\np 20\n2. Gradient based training trivially allows for multiple objectives in the same loss function.\n3. Rich model representations do not saturate as quickly and can therefore utilize large quantities of unlabeled text.\nOur focus in this preliminary work is to present a generative neural approach to HMMs and demonstrate how this framework lends itself to modularity (e.g. the easy inclusion of morphological information via Convolutional Neural Networks \u00a75), and the addition of extra conditioning context (e.g. using an RNN to model the sentence \u00a76). Our approach will be demonstrated and evaluated on the simple task of part-of-speech tag induction. Future work, should investigate the second and third proposed strengths."}, {"heading": "2 Framework", "text": "Graphical models have been widely used in NLP. Typically potential functions \u03c8(z,x) over a set of latent variables, z, and observed variables, x, are defined based on hand-crafted features. Moreover, independence assumptions between variables are often made for the sake of tractability. Here, we propose using neural networks (NNs) to produce the potentials since neural networks are universal function approximators. Neural networks can extract useful taskspecific abstract representations of data. Additionally, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) based Recurrent Neural Networks (RNNs), allow for modeling unbounded context with far fewer parameters than naive one-hot feature encodings. The reparameterization of potentials with neural networks (NNs) is seamless:\n\u03c8(z,x) = fNN(z,x | \u03b8) (1)\nThe sequence of observed variables are denoted as x = {x1, . . . , xn}. In unsupervised learning, we aim to find model parameters \u03b8 that maximize the evidence p(x | \u03b8). We focus on cases when the posterior is tractable and we can use Generalized EM (Dempster et al., 1977) to estimate \u03b8.\np(x) = \u2211 z p(x, z) (2)\n= Eq(z)[ln p(x, z | \u03b8)] + H[q(z)] (3) + KL (q(z) \u2016 p(z |x, \u03b8)) (4)\nwhere q(z) is an arbitrary distribution, and H is the entropy function. The E-step of EM estimates the posterior p(z |x) based on the current parameters \u03b8. In the M-step, we choose q(z) to be the posterior p(z |x), setting the KL-divergence to zero. Additionally, the entropy term H[q(z)] is a constant and can therefore be dropped. This means updating \u03b8 only requires maximizing Ep(z |x)[ln p(x, z | \u03b8)]. The gradient is therefore defined in terms of the gradient of the joint probability scaled by the posteriors:\nJ(\u03b8) = \u2211 z p(z |x)\u2202 ln p(x, z | \u03b8) \u2202\u03b8\n(5)\nIn order to perform the gradient update in Eq 5, we need to compute the posterior p(z |x). This can be done efficiently with the Message Passing algorithm. Note that, in cases where the derivative \u2202 \u2202\u03b8 ln p(x, z | \u03b8) is easy to evaluate, we can perform direct marginal likelihood optimization (Salakhutdinov et al., 2003). We do not address here the question of semi-supervised training, but believe the framework we present lends itself naturally to the incorporation of constraints or labeled data. Next, we demonstrate the application of this framework to HMMs in the service of part-of-speech tag induction."}, {"heading": "3 Part-of-Speech Induction", "text": "Part-of-speech tags encode morphosyntactic information about a language and are a fundamental tool in downstream NLP applications. In English, the Penn Treebank (Marcus et al., 1994) distinguishes 36 categories and punctuation. Tag induction is the task of taking raw text and both discovering these latent clusters and assigning them to words in situ. Classes can be very specific (e.g. six types of verbs in English) to their syntactic role. Example tags are shown in Table 1. In this example, board is labeled as a singular noun while Pierre Vinken is a singular proper noun.\nTwo natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word embeddings."}, {"heading": "3.1 The Hidden Markov Model", "text": "A common model for this task, and our primary workhorse, is the Hidden Markov Model trained with the unsupervised message passing algorithm, BaumWelch (Welch, 2003).\nModel HMMs model a sentence by assuming that (a) every word token is generated by a latent class, and (b) the current class at time t is conditioned on the local history t\u22121. Formally, this gives us an emission p(xt | zt) and transition p(zt | zt\u22121) probability. The graphical model is drawn pictorially in Figure 1, where shaded circles denote observations and empty ones are latent. The probability of a given sequence of observations x and latent variables z is given by multiplying transitions and emissions across all time steps (Eq. 6). Finding the optimal sequence of latent classes corresponds to computing an argmax over the values of z.\np(x, z) = n+1\u220f t=1 p(zt | zt\u22121) n\u220f t=1 p(xt | zt) (6)\nBecause our task is unsupervised we do not have a priori access to these distributions, but they can be estimated via Baum-Welch. The algorithm\u2019s outline is provided in Algorithm 1.\nTraining an HMM with EM is highly non-convex and likely to get stuck in local optima (Johnson, 2007). Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states. Recently, Lin et al. (2015) extended several models\nAlgorithm 1 Baum-Welch Algorithm Randomly Initialize distributions (\u03b8) repeat\nCompute forward messages: \u2200i,t \u03b1i(t) Compute backward messages: \u2200i,t \u03b2i(t) Compute posteriors:\np(zt = i |x, \u03b8) \u221d \u03b1i(t)\u03b2i(t) p(zt = i, zt+1 = j |x, \u03b8)\n\u221d \u03b1i(t)p(zt+1=j|zt= i) \u00d7\u03b2j(t+ 1)p(xt+1|zt+1=j)\nUpdate \u03b8 until Converged\nincluding the HMM to include pre-trained word embeddings learned by different skip-gram models. Our work will fully neuralize the HMM and learn embeddings during the training of our generative model. There has also been recent work on by Rastogi et al. (2016) on neuralizing Finite-State Transducers."}, {"heading": "3.2 Additional Comparisons", "text": "While the main focus of our paper is the seamless extension of an unsupervised generative latent variable model with neural networks, for completeness we will also include comparisons to other techniques which do not adhere to the generative assumption. We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al. (2012).\nOf particular interest to us is the work of Brown et al. (1992). Brown clusters group word types through a greedy agglomerative clustering according to their mutual information across the corpus based on bigram probabilities. Brown clusters do not account for a word\u2019s membership in multiple syntactic classes, but are a very strong baseline for tag induction. It is possible our approach could be improved by augmenting our objective function to include mutual information computations or a bias towards a harder clustering."}, {"heading": "4 Neural HMM", "text": "The aforementioned training of an HMM assumes access to two distributions: (1) Emissions with K \u00d7 V parameters, and (2) Transitions with K \u00d7K parameters. Here we assume there are K clusters and V\nword types in our vocabulary. Our neural HMM (NHMM) will replace these matrices with the output of simple feed-forward neural networks. All conditioning variables will be presented as input to the network and its final softmax layer will provide probabilities. This should replicate the behavior of the standard HMM, but without an explicit representation of the necessary distributions."}, {"heading": "4.1 Producing Probabilities", "text": "Producing emission and transition probabilities allows for standard inference to take place in the model.\nEmission Architecture Let vk \u2208 RD be vector embedding of tag zk, wi \u2208 RD and bi vector embedding and bias of word i respectively. The emission probability p(wi | zk) is given by\np(wi | zk) = exp(v>k wi + bi)\u2211V j=1 exp(v > k wj + bj)\n(7)\nThe emission probability can be implemented by a neural network where wi is the weight of unit i at the output layer of the network. The tag embeddings vk are obtained by a simple feed-forward neural network consisting of a lookup table following by a nonlinear activation (ReLU). When using morphology information (\u00a75), we will first use another network to produce the word embedddings wi.\nTransition Architecture We produce the transition probability directly by using a linear layer of D \u00d7K2. More specifically, let q \u2208 RD be a query embedding. The unnormalized transition matrix T is computed as\nT = U>q+ b (8)\nwhere U \u2208 RD\u00d7K2 and b \u2208 RK2 . We then reshape T to a K \u00d7K matrix and apply a softmax layer per row to produce valid transition probabilities."}, {"heading": "4.2 Training the Neural Network", "text": "The probabilities can now be used to perform the aforementioned forward and backward passes over the data to compute posteriors. In this way, we perform the E-step as though we were training a vanilla HMM. Traditionally, these values would simply\nbe re-normalized during the M-step to re-estimate model parameters. Instead, we use them to re-scale our gradients (following the discussion from \u00a72). Combining the HMM factorization of the joint probability p(x, z) from Eq. 6 with the gradient from Eq. 5, yields the following update rule:\nJ(\u03b8) = \u2211 z p(z |x)\u2202 ln p(x, z | \u03b8) \u2202\u03b8\n= \u2211 t \u2211 zt p(zt |x) \u2202 ln p(xt | zt, \u03b8) \u2202\u03b8\n+ p(zt, zt\u22121 |x) \u2202 ln p(zt | zt\u22121, \u03b8)\n\u2202\u03b8 (9)\nThe posteriors p(zt |x) and p(zt, zt\u22121 |x) are obtained by running Baum-Welch as shown in Algorithm 1. Where traditional supervised training can follow a clear gradient signal towards a specific assignment, here we are propagating the model\u2019s (un)certainty instead. An additional complication introduced by this paradigm is the question of how many gradient steps to take on a given minibatch. In incremental EM the posteriors are simply accumulated and normalized. Here, we repeatedly recompute gradients on a minibatch until reaching the maximum number of epochs or a convergence threshold is met.\nFinally, notice that the factorization of the HMM allows us to evaluate the joint distribution p(x, z | \u03b8) easily. We therefore employ Direct Marginal Likelihood (DML) (Salakhutdinov et al., 2003) to optimize the model\u2019s parameters. After trying both EM and DML we found EM to be slower to converge and perform slightly weaker. For this reason, the presented results will all be trained with DML."}, {"heading": "4.3 HMM and Neural HMM Equivalence", "text": "An important result we see in Table 2 is that the Neural HMM (NHMM) performs almost identically to the HMM. At this point, we have replaced the underlying machinery, but the model still has the same information bottlenecks as a standard HMM, which limit the amount and type of information carried between words in the sentence. Additionally, both approaches are optimizing the same objective function, data likelihood, via the computation of posteriors. The equivalency is an important sanity check. The\nfollowing two sections will demonstrate the extensibility of this approach."}, {"heading": "5 Convolutions for Morphology", "text": "The first benefit of moving to neural networks is the ease with which new information can be provided to the model. The first experiment we will perform is replacing words with embedding vectors derived from a Convolutional Neural Network (CNN) (Kim et al., 2016; Jozefowicz et al., 2016). We use a convolutional kernel with widths from 1 to 7, which covers up to 7 character n-grams (Figure 2). This allows the model to automatically learn lexical representations based on prefix, suffix, and stem information about a word. No additional changes to learning are required for extension.\nAdding the convolution does not dramatically slow down our model because the emission distributions can be computed for the whole batch in one operation. We simply pass the whole vocabulary through the convolution in a single operation."}, {"heading": "6 Infinite Context with LSTMs", "text": "One of the most powerful strengths of neural networks is their ability to create compact representation of data. We will explore this here in the creation of transition matrices. In particular, we chose to augment the transition matrix with all preceding words in the sentence: p(zt | zt\u22121, w0, . . . , wt\u22121). Incorporating this amount of context in a traditional HMM is intractable and impossible to estimate, as the number of parameters grows exponentially.\nFor this reason, we use a stacked LSTM to form a low dimensional representation of the sentence (C0...t\u22121) which can be easily fed to our network when producing a transition matrix:\np(zt | zt\u22121, C0...t\u22121) in Figure 3. By having the LSTM only consume up to the previous word, we do not break any sequential generative model assumptions.1 In terms of model architecture, the query embedding q will be replaced by a hidden state ht\u22121 of the LSTM at time step t\u2212 1."}, {"heading": "7 Evaluation", "text": "Once a model is trained, the one best latent sequence is extracted for every sentence and evaluated on three metrics.\nMany-to-One (M-1) Many-to-one computes the most common true part-of-speech tag for each cluster. It then computes tagging accuracy as if the cluster were replaced with that tag. This metric is easily gamed by introducing a large number of clusters.\nOne-to-One (1-1) One-to-One performs the same computation as Many-to-One but only one cluster is allowed to be assigned to a given tag. This prevents the gaming of M-1.\nV-Measure (VM) V-Measure is an F-measure which trades off conditional entropy between the clusters and gold tags. Christodoulopoulos et al. (2010) found VM is to be the most informative and consistent metric, in part because it is agnostic to the number of induced tags."}, {"heading": "8 Data and Parameters", "text": "To evaluate our approaches, we follow the existing literature and train and test on the full WSJ corpus.\n1This interpretation does not complicate the computation of forward-backward messages when running Baum-Welch, though it does, by design, break Markovian assumption about knowledge of the past.\nThere are three components of our models which can be tuned. Something we have to be careful of when train and test are the same data. To avoid cheating, no values were tuned in this work.\nArchitecture The first parameter is the number of hidden units. We chose 512 because it was the largest power of two we could fit in memory. When we extended our model to include the convolutional emission network, we only used 128 units, due to the intensive computation of Char-CNN over the whole vocabulary per minibatch.\nThe second design choice was the number of LSTM layers. We used a three layer LSTM as it worked well for (Tran et al., 2016), and we applied dropout (Srivastava et al., 2014) over the vertical connections of the LSTMs (Pham et al., 2014) with a rate of 0.5.\nFinally, the maximum number of inner loop updates applied per batch is set to six. We train all the models for five epochs and perform gradient clipping whenever the gradient norm is greater than five. To determine when to stop applying the gradient during training we simply check when the log probability has converged ( new\u2212oldold < 10\n\u22124) or if the maximum number of inner loops has been reached. All optimization was done using Adam (Kingma and Ba, 2015) with default hyper-parameters.\nInitialization In addition to architectural choices we have to initialize all of our parameters. Word embeddings (and character embeddings in the CNN) are drawn from a Gaussian N (0, 1). The weights of all linear layers in the model are drawn from a uniform distribution with mean zero and a standard deviation of \u221a 1/nin, where nin is the input dimension of the linear layer.2 Additionally, weights for the LSTMs are initialized using N (0, 1/2n), where n is the number of hidden units, and the bias of the forget gate is set to 1, as suggested by J\u00f3zefowicz et al. (2015). We present some parameter and modeling ablation analysis in \u00a710.\nIt is worth emphasizing that parameters are shared at the lower level of our network architectures (see Figure 2 and Figure 3). Sharing parameters not only allows the networks to share statistical strength, but also reduces the computational cost of comput-\n2This is the default parameter initialization in Torch.\ning sufficient statistics during training due to the marginalization over latent variables.\nIn all of our experiments, we use minibatch size of 256 and sentences of 40 words or less due to memory constraints. Evaluation was performed on all sentence lengths. Additionally, we map all the digits to 0, but do not lower-case the data or perform any other preprocessing. All model code is available online for extension and replication at https://github.com/ketranm/neuralHMM."}, {"heading": "9 Results", "text": "Our results are presented in Table 2 along with two baseline systems, and the four top performing and state-of-the-art approaches. As noted earlier, we are happy to see that our NHMM performs almost identically with the standard HMM. Second, we find that our approach, while simple and fast, is competitive with Blunsom (2011). Their Hierarchical Pitman-Yor Process for trigram HMMs with character modeling is a very sophisticated Bayesian approach and the most appropriate comparison to our work.\nWe see that both extended context (+LSTM) and the addition of morphological information (+Conv) provide substantial boosts to performance. Interestingly, the gains are not completely complementary, as we note that the six and twelve point gains of these additions only combine to a total of sixteen points in\nVM improvement. This might imply that at least some of the syntactic context being captured by the LSTM is mirrored in the morphology of the language. This hypothesis is something future work should investigate with morphologically rich languages.\nFinally, the newer work of Yatbaz et al. (2012) outperforms our approach. It is possible our performance could be improved by following their lead and including knowledge of the future."}, {"heading": "10 Parameter Ablation", "text": "Our model design decisions and weight initializations were chosen based on best practices set forth in the supervised training literature. We are lucky that these also behaved well in the unsupervised setting. Within unsupervised structure prediction, to our best knowledge, there has been no empirical study on neural network architecture design and weight initialization. We therefore provide an initial overview on the topic for several of our decisions.\nWeight Initialization If we run our best model (NHMM+Conv+LSTM) with all the weights initialized from a uniform distribution U(\u221210\u22124, 10\u22124)3 we find a dramatic drop in V-Measure performance (61.7 vs 71.7 in Table 3). This is consistent with the common wisdom that unlike supervised learning (Luong et al., 2015), weight initialization is important to achieve good performance on unsupervised tasks. It is possible that performance could be further enhance via the popular technique of ensembling, would allow for combining models which converged to different local optima.\nLSTM Layers And Dropout We find that dropout is important in training an unsupervised NHMM.\n3We choose small standard derivation here for numerical stability when computing forward-backward messages.\nRemoving dropout causes performance to drop six points. To avoid tuning the dropout rate, future work might investigate the effect of variational dropout (Kingma et al., 2015) in unsupervised learning. We also observed that the number of LSTM layers has an impact on V-Measure. Had we simply used a single layer we would have lost nearly five points. It is possible that more layers, perhaps coupled with more data, would yield even greater gains."}, {"heading": "11 Future Work", "text": "In addition to parameter tuning and multilingual evaluation, the biggest open questions for our approach are the effects of additional data and augmenting the loss function. Neural networks are notoriously data hungry, indicating that while we achieve competitive results, it is possible our model will scale well when run with large corpora. This would likely require the use of techniques like NCE (Gutmann and Hyv\u00e4rinen, 2010) which have been shown to be highly effective in related tasks like neural language modeling (Mnih and Teh, 2012; Vaswani et al., 2013). Secondly, despite focusing on ways to augment an HMM, Brown clustering and systems inspired by it perform very well. They aim to maximize mutual information rather than likelihood. It is possible that augmenting or constraining our loss will yield additional performance gains.\nOutside of simply maximizing performance on tag induction, a more subtle, but powerful contribution of this work may be its demonstration of the easy and effective nature of using neural networks with Bayesian models traditionally trained by EM. We hope this approach scales well to many other domains and tasks."}, {"heading": "Acknowledgments", "text": "This work was supported by Contracts W911NF-151-0543 and HR0011-15-C-0115 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO). Additional thanks to Christos Christodoulopoulos."}], "references": [{"title": "Global optimization of a neural", "author": ["Ralf Kompe"], "venue": null, "citeRegEx": "Kompe.,? \\Q1991\\E", "shortCiteRegEx": "Kompe.", "year": 1991}, {"title": "Phylogenetic grammar induction", "author": ["Taylor Berg-Kirkpatrick", "Dan Klein."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1288\u20131297, Uppsala, Sweden, July.", "citeRegEx": "Berg.Kirkpatrick and Klein.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick and Klein.", "year": 2010}, {"title": "Painless unsupervised learning with features", "author": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Labeled grammar induction with minimal supervision", "author": ["Yonatan Bisk", "Christos Christodoulopoulos", "Julia Hockenmaier."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Bisk et al\\.,? 2015", "shortCiteRegEx": "Bisk et al\\.", "year": 2015}, {"title": "A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction", "author": ["Phil Blunsom", "Trevor Cohn."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 865\u2013874,", "citeRegEx": "Blunsom and Cohn.,? 2011", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2011}, {"title": "ClassBased n-gram Models of Natural Language", "author": ["Peter F Brown", "Peter V deSouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."], "venue": "Computational Linguistics, 18.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Two Decades of Unsupervised POS induction: How far have we come", "author": ["Christos Christodoulopoulos", "Sharon Goldwater", "Mark Steedman"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Christodoulopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Christodoulopoulos et al\\.", "year": 2010}, {"title": "A Bayesian Mixture Model for Part-of-Speech Induction Using Multiple Features", "author": ["Christos Christodoulopoulos", "Sharon Goldwater", "Mark Steedman."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Edinburgh,", "citeRegEx": "Christodoulopoulos et al\\.,? 2011", "shortCiteRegEx": "Christodoulopoulos et al\\.", "year": 2011}, {"title": "Unsupervised structure prediction with nonparallel multilingual guidance", "author": ["Shay B. Cohen", "Dipanjan Das", "Noah A. Smith."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50\u201361, Edinburgh, Scot-", "citeRegEx": "Cohen et al\\.,? 2011", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 600\u2013609, Portland,", "citeRegEx": "Das and Petrov.,? 2011", "shortCiteRegEx": "Das and Petrov.", "year": 2011}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A Dempster", "N Laird", "D Rubin."], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), January.", "citeRegEx": "Dempster et al\\.,? 1977", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u2013 1780, November.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Composing graphical models with neural networks for structured representations and fast inference", "author": ["Matthew J Johnson", "David Duvenaud", "Alexander B Wiltschko", "Sandeep R Datta", "Ryan P Adams."], "venue": "ArXiv e-prints, March.", "citeRegEx": "Johnson et al\\.,? 2016", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Why doesn\u2019t EM find good HMM POS-taggers", "author": ["Mark Johnson."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), January.", "citeRegEx": "Johnson.,? 2007", "shortCiteRegEx": "Johnson.", "year": 2007}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 2342\u20132350.", "citeRegEx": "J\u00f3zefowicz et al\\.,? 2015", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Exploring the Limits of Language Modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "ArXiv e-prints, February.", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "The International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Autoencoding variational bayes", "author": ["Diederik P Kingma", "Max Welling."], "venue": "The International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Welling.,? 2014", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Diederik P Kingma", "Tim Salimans", "Max Welling."], "venue": "Advances in Neural Information Processing Systems 28, pages 2575\u20132583. Curran Associates, Inc.", "citeRegEx": "Kingma et al\\.,? 2015", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Unsupervised pos induction with word embeddings", "author": ["Chu-Cheng Lin", "Waleed Ammar", "Chris Dyer", "Lori Levin."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The Penn Treebank: Annotating Predicate Argument Structure", "author": ["Mitchell P Marcus", "Grace Kim", "Mary Ann Marcinkiewicz", "Robert MacIntyre", "Ann Bies", "Mark Ferguson", "Karen Katz", "Britta Schasberger."], "venue": "ARPA Human Language Technology Workshop.", "citeRegEx": "Marcus et al\\.,? 1994", "shortCiteRegEx": "Marcus et al\\.", "year": 1994}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1751\u2013 1758, New York, NY, USA, July.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Christopher Bluche", "Th\u00e9odore Kermorvant", "J\u00e9r\u00f4me Louradour."], "venue": "International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 285\u2013290, Sept.", "citeRegEx": "Pham et al\\.,? 2014", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Weighting finite-state transductions with neural context", "author": ["Pushpendre Rastogi", "Ryan Cotterell", "Jason Eisner."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Rastogi et al\\.,? 2016", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "Optimization with em and expectationconjugate-gradient", "author": ["Ruslan Salakhutdinov", "Sam Roweis", "Zoubin Ghahramani."], "venue": "Proceedings, Intl. Conf. on Machine Learning (ICML, pages 672\u2013679.", "citeRegEx": "Salakhutdinov et al\\.,? 2003", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2003}, {"title": "Improving word alignment using word similarity", "author": ["Theerawat Songyot", "David Chiang."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1840\u20131845, Doha, Qatar, October.", "citeRegEx": "Songyot and Chiang.,? 2014", "shortCiteRegEx": "Songyot and Chiang.", "year": 2014}, {"title": "Unsupervised dependency parsing without gold part-of-speech tags", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Angel X. Chang", "Daniel Jurafsky."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281\u20131290, Ed-", "citeRegEx": "Spitkovsky et al\\.,? 2011", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "JMLR, (1):1929\u20131958, January.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Recurrent memory networks for language modeling", "author": ["Ke Tran", "Arianna Bisazza", "Christof Monz."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 321\u2013", "citeRegEx": "Tran et al\\.,? 2016", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Hidden Markov Models and the Baum-Welch Algorithm", "author": ["Lloyd R Welch."], "venue": "IEEE Information Theory Society Newsletter, 53(4):1\u201324, December.", "citeRegEx": "Welch.,? 2003", "shortCiteRegEx": "Welch.", "year": 2003}, {"title": "Learning Syntactic Categories Using Paradigmatic Representations of Word Context", "author": ["Mehmet Ali Yatbaz", "Enis Sert", "Deniz Yuret."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Nat-", "citeRegEx": "Yatbaz et al\\.,? 2012", "shortCiteRegEx": "Yatbaz et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Inspired by Berg-Kirkpatrick et al. (2010), who bridged the gap between supervised and unsupervised training with features, we bring neural networks to unsupervised learning by providing evidence that even in", "startOffset": 12, "endOffset": 43}, {"referenceID": 9, "context": "Using features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al.", "startOffset": 71, "endOffset": 146}, {"referenceID": 8, "context": "Using features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al.", "startOffset": 71, "endOffset": 146}, {"referenceID": 28, "context": "Additionally, similarity metrics derived from word embeddings have also been shown to improve unsupervised word alignment (Songyot and Chiang, 2014).", "startOffset": 122, "endOffset": 148}, {"referenceID": 19, "context": "Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016).", "startOffset": 132, "endOffset": 180}, {"referenceID": 13, "context": "Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016).", "startOffset": 132, "endOffset": 180}, {"referenceID": 12, "context": "Additionally, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) based Recurrent Neural Networks (RNNs), allow for modeling unbounded context with far fewer parameters than naive one-hot feature encodings.", "startOffset": 44, "endOffset": 78}, {"referenceID": 10, "context": "We focus on cases when the posterior is tractable and we can use Generalized EM (Dempster et al., 1977) to estimate \u03b8.", "startOffset": 80, "endOffset": 103}, {"referenceID": 27, "context": "\u2202 \u2202\u03b8 ln p(x, z | \u03b8) is easy to evaluate, we can perform direct marginal likelihood optimization (Salakhutdinov et al., 2003).", "startOffset": 96, "endOffset": 124}, {"referenceID": 23, "context": "In English, the Penn Treebank (Marcus et al., 1994) distinguishes 36 categories and punctuation.", "startOffset": 30, "endOffset": 51}, {"referenceID": 29, "context": "Two natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word embeddings.", "startOffset": 80, "endOffset": 124}, {"referenceID": 3, "context": "Two natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word embeddings.", "startOffset": 80, "endOffset": 124}, {"referenceID": 33, "context": "Welch (Welch, 2003).", "startOffset": 6, "endOffset": 19}, {"referenceID": 14, "context": "Training an HMM with EM is highly non-convex and likely to get stuck in local optima (Johnson, 2007).", "startOffset": 85, "endOffset": 100}, {"referenceID": 4, "context": "Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011).", "startOffset": 85, "endOffset": 109}, {"referenceID": 4, "context": "Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states.", "startOffset": 86, "endOffset": 135}, {"referenceID": 4, "context": "Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states. Recently, Lin et al. (2015) extended several models Algorithm 1 Baum-Welch Algorithm", "startOffset": 86, "endOffset": 364}, {"referenceID": 26, "context": "There has also been recent work on by Rastogi et al. (2016) on neuralizing Finite-State Transducers.", "startOffset": 38, "endOffset": 60}, {"referenceID": 5, "context": "We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 5, "context": "We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al.", "startOffset": 27, "endOffset": 156}, {"referenceID": 5, "context": "We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al. (2012).", "startOffset": 27, "endOffset": 181}, {"referenceID": 5, "context": "Of particular interest to us is the work of Brown et al. (1992). Brown clusters group word types through a greedy agglomerative clustering according to their mutual information across the corpus based on bigram probabilities.", "startOffset": 44, "endOffset": 64}, {"referenceID": 27, "context": "We therefore employ Direct Marginal Likelihood (DML) (Salakhutdinov et al., 2003) to optimize the model\u2019s parameters.", "startOffset": 53, "endOffset": 81}, {"referenceID": 6, "context": "Christodoulopoulos et al. (2010) found VM is to be the most informative and consistent metric, in part because it is agnostic to the number of induced tags.", "startOffset": 0, "endOffset": 33}, {"referenceID": 31, "context": "We used a three layer LSTM as it worked well for (Tran et al., 2016), and we applied dropout (Srivastava et al.", "startOffset": 49, "endOffset": 68}, {"referenceID": 30, "context": ", 2016), and we applied dropout (Srivastava et al., 2014) over the vertical connections of the LSTMs (Pham et al.", "startOffset": 32, "endOffset": 57}, {"referenceID": 25, "context": ", 2014) over the vertical connections of the LSTMs (Pham et al., 2014) with a rate of 0.", "startOffset": 51, "endOffset": 70}, {"referenceID": 18, "context": "All optimization was done using Adam (Kingma and Ba, 2015) with default hyper-parameters.", "startOffset": 37, "endOffset": 58}, {"referenceID": 15, "context": "2 Additionally, weights for the LSTMs are initialized using N (0, 1/2n), where n is the number of hidden units, and the bias of the forget gate is set to 1, as suggested by J\u00f3zefowicz et al. (2015). We present some parameter and modeling ablation analysis in \u00a710.", "startOffset": 173, "endOffset": 198}, {"referenceID": 34, "context": "Finally, the newer work of Yatbaz et al. (2012) outperforms our approach.", "startOffset": 27, "endOffset": 48}, {"referenceID": 22, "context": "This is consistent with the common wisdom that unlike supervised learning (Luong et al., 2015), weight initialization is important to achieve good performance on unsupervised tasks.", "startOffset": 74, "endOffset": 94}, {"referenceID": 20, "context": "To avoid tuning the dropout rate, future work might investigate the effect of variational dropout (Kingma et al., 2015) in unsupervised learning.", "startOffset": 98, "endOffset": 119}, {"referenceID": 11, "context": "This would likely require the use of techniques like NCE (Gutmann and Hyv\u00e4rinen, 2010) which have been shown to be highly effective in related tasks like neural language modeling (Mnih and Teh, 2012; Vaswani et al.", "startOffset": 57, "endOffset": 86}, {"referenceID": 24, "context": "This would likely require the use of techniques like NCE (Gutmann and Hyv\u00e4rinen, 2010) which have been shown to be highly effective in related tasks like neural language modeling (Mnih and Teh, 2012; Vaswani et al., 2013).", "startOffset": 179, "endOffset": 221}, {"referenceID": 32, "context": "This would likely require the use of techniques like NCE (Gutmann and Hyv\u00e4rinen, 2010) which have been shown to be highly effective in related tasks like neural language modeling (Mnih and Teh, 2012; Vaswani et al., 2013).", "startOffset": 179, "endOffset": 221}], "year": 2016, "abstractText": "In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag induction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.", "creator": "LaTeX with hyperref package"}}}