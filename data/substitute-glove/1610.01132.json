{"id": "1610.01132", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "A Non-generative Framework and Convex Relaxations for Unsupervised Learning", "abstract": "We without piece novel naming mathematics establishes have consensual lesson. and distinctive shapes. First, same does sure depend particular generative version between particular weeks giving worst - case level ppm. Second, they is comparative, related performance is measured their wish to a may hypothesis whereas. This entry still avoid known computational parametric results and improper cryptographic services on uniformly explications. We that understand several others of unsupervised learning models, but came either three dna under algorithm assumptions addition are otherwise provably rifts, can because analyze learned next n't framework by hexagonal optimization.", "histories": [["v1", "Tue, 4 Oct 2016 19:22:44 GMT  (344kb)", "https://arxiv.org/abs/1610.01132v1", "to appear in NIPS 2016"], ["v2", "Wed, 5 Oct 2016 00:30:02 GMT  (345kb)", "http://arxiv.org/abs/1610.01132v2", "to appear in NIPS 2016"], ["v3", "Tue, 27 Dec 2016 20:59:01 GMT  (351kb)", "http://arxiv.org/abs/1610.01132v3", "NIPS 2016"]], "COMMENTS": "to appear in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["elad hazan", "tengyu ma"], "accepted": true, "id": "1610.01132"}, "pdf": {"name": "1610.01132.pdf", "metadata": {"source": "CRF", "title": "A Non-generative Framework and Convex Relaxations for Unsupervised Learning", "authors": ["Elad Hazan", "Tengyu Ma"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n01 13\n2v 3\n[ cs\n.L G\n] 2\n7 D\nec 2\n01 6"}, {"heading": "1 Introduction", "text": "Unsupervised learning is the task of learning structure from unlabelled examples. Informally, the main goal of unsupervised learning is to extract structure from the data in a way that will enable efficient learning from future labelled examples for potentially numerous independent tasks.\nIt is useful to recall the Probably Approximately Correct (PAC) learning theory for supervised learning [Val84], based on Vapnik\u2019s statistical learning theory [Vap98]. In PAC learning, the learning can access labelled examples from an unknown distribution. On the basis of these examples, the learner constructs a hypothesis that generalizes to unseen data. A concept is said to be learnable with respect to a hypothesis class if there exists an (efficient) algorithm that outputs a generalizing hypothesis with high probability after observing polynomially many examples in terms of the input representation.\nThe great achievements of PAC learning that made it successful are its generality and algorithmic applicability: PAC learning does not restrict the input domain in any way, and thus allows very general learning, without generative or distributional assumptions on the world. Another important feature is the restriction to specific hypothesis classes, without which there are simple impossibility results such as the \u201cno free lunch\u201d theorem. This allows comparative and improper learning of computationally-hard concepts.\nThe latter is a very important point which is often understated. Consider the example of sparse regression, which is a canonical problem in high dimensional statistics. Fitting the best sparse vector to linear prediction is an NP-hard problem [Nat95]. However, this does not prohibit improper learning, since we can use a \u21131 convex relaxation for the sparse vectors (famously known as LASSO [Tib96]).\nUnsupervised learning, on the other hand, while extremely applicative and well-studied, has not seen such an inclusive theory. The most common approaches, such as restricted Boltzmann machines, topic models, dictionary learning, principal component analysis and metric clustering, are based almost entirely on generative assumptions about the world. This is a strong restriction which makes it very hard to analyze such approaches in scenarios for which the assumptions do not hold. A more discriminative approach is based on compression, such as the Minimum Description Length criterion. This approach gives rise to provably intractable problems and doesn\u2019t allow improper learning.\nMain results. We start by proposing a rigorous framework for unsupervised learning which allows data-dependent, comparative learning without generative assumptions. It is general enough to encompass previous methods such as PCA, dictionary learning and topic models. Our main contribution are optimization-based relaxations and efficient algorithms that are shown to improperly probably learn previous models, specifically:\n1. We consider the classes of hypothesis known as dictionary learning. We give a more general hypothesis class which encompasses and generalizes it according to our definitions. We proceed to give novel polynomial-time algorithms for learning the broader class. These algorithms are based on new techniques in unsupervised learning, namely sum-of-squares convex relaxations.\nAs far as we know, this is the first result for efficient improper learning of dictionaries without generative assumptions. Moreover, our result handles polynomially over-complete dictionaries, while previous works [AGMM15, BKS15a] apply to at most constant factor over-completeness.\n2. We give efficient algorithms for learning a new hypothesis class which we call spectral autoencoders. We show that this class generalizes, according to our definitions, the class of PCA (principal component analysis) and its kernel extensions.\nStructure of this paper. In the following chapter we a non-generative, distribution-dependent definition for unsupervised learning which mirrors that of PAC learning for supervised learning. We then proceed to an illustrative example and show how Principal Component Analysis can be formally learned in this setting. The same section also gives a much more general class of hypothesis for unsupervised learning which we call polynomial spectral decoding, and show how they can be efficient learned in our framework using convex optimization. Finally, we get to our main contribution: a convex optimization based methodology for improper learning a wide class of hypothesis, including dictionary learning."}, {"heading": "1.1 Previous work", "text": "The vast majority of work on unsupervised learning, both theoretical as well as applicative, focuses on generative models. These include topic models [BNJ03], dictionary learning [DH06], Deep Boltzmann Machines and deep belief networks [Sal09] and many more. Many times these models entail non-convex optimization problems that are provably NP-hard to solve in the worst-case.\nA recent line of work in theoretical machine learning attempts to give efficient algorithms for these models with provable guarantees. Such algorithms were given for topic models [AGM12], dictionary learning [AGM13, AGMM15], mixtures of gaussians and hidden Markov models [HK13, AGH+14] and more. However, these works retain, and at times even enhance, the probabilistic generative assumptions of the underlying model. Perhaps the most widely used unsupervised learning methods are clustering algorithms such as k-means, k-medians and principal component analysis (PCA), though these lack generalization guarantees. An axiomatic approach to clustering was initiated by Kleinberg [Kle03] and pursued further in [BDA09]. A discriminative generalization-based approach for clustering was undertaken in [BBV08] within the model of similarity-based clustering.\nAnother approach from the information theory literature studies with online lossless compression. The relationship between compression and machine learning goes back to the Minimum Description Length criterion [Ris78]. More recent work in information theory gives online algorithms that attain optimal compression, mostly for finite alphabets [ADJ+13, OSZ04]. For infinite alphabets, which are the main object of study for unsupervised learning of signals such as images, there are known impossibility results [JOS05]. This connection to compression was recently further advanced, mostly in the context of textual data [PWMH13].\nIn terms of lossy compression, Rate Distortion Theory (RDT) [Ber71, CT06] is intimately related to our definitions, as a framework for finding lossy compression with minimal distortion (which would\ncorrespond to reconstruction error in our terminology). Our learnability definition can be seen of an extension of RDT to allow improper learning and generalization error bounds. Another learning framework derived from lossy compression is the information bottleneck criterion [TPB00], and its learning theoretic extensions [SST08]. The latter framework assumes an additional feedback signal, and thus is not purely unsupervised.\nThe downside of the information-theoretic approaches is that worst-case competitive compression is provably computationally hard under cryptographic assumptions. In contrast, our compression-based approach is based on learning a restriction to a specific hypothesis class, much like PAC-learning. This circumvents the impossibility results and allows for improper learning."}, {"heading": "2 A formal framework for unsupervised learning", "text": "The basis constructs in an unsupervised learning setting are:\n1. Instance domain X , such as images, text documents, etc. Target space, or range, Y . We usually think of X = Rd,Y = Rk with d \u226b k. (Alternatively, Y can be all sparse vectors in a larger space. )\n2. An unknown, arbitrary distribution D on domain X .\n3. A hypothesis class of decoding and encoding pairs,\nH \u2286 {(h, g) \u2208 {X 7\u2192 Y} \u00d7 {Y 7\u2192 X}},\nwhere h is the encoding hypothesis and g is the decoding hypothesis.\n4. A loss function \u2113 : H\u00d7X 7\u2192 R>0 that measures the reconstruction error,\n\u2113((g, h), x) .\nFor example, a natural choice is the \u21132-loss \u2113((g, h), x) = \u2016g(h(x))\u2212x\u201622. The rationale here is to learn structure without significantly compromising supervised learning for arbitrary future tasks. Near-perfect reconstruction is sufficient as formally proved in Appendix A.1. Without generative assumptions, it can be seen that near-perfect reconstruction is also necessary.\nFor convenience of notation, we use f as a shorthand for (h, g) \u2208 H, a member of the hypothesis class H. Denote the generalization ability of an unsupervised learning algorithm with respect to a distribution D as\nloss D (f) = E x\u223cD [\u2113(f, x)].\nWe can now define the main object of study: unsupervised learning with respect to a given hypothesis class. The definition is parameterized by real numbers: the first is the encoding length (measured in bits) of the hypothesis class. The second is the bias, or additional error compared to the best hypothesis. Both parameters are necessary to allow improper learning.\nDefinition 2.1. We say that instance D,X is (k, \u03b3)-C -learnable with respect to hypothesis class H if exists an algorithm that for every \u03b4, \u03b5 > 0, after seeing m(\u03b5, \u03b4) = poly(1/\u03b5, log(1/\u03b4), d) examples, returns an encoding and decoding pair (h, g) (not necessarily from H) such that:\n1. with probability at least 1\u2212 \u03b4, lossD((h, g)) 6 min(h,g)\u2208H lossD((h, g)) + \u03b5+ \u03b3.\n2. h(x) has an explicit representation with length at most k bits.\nFor convenience we typically encode into real numbers instead of bits. Real encoding can often (though not in the worst case) be trivially transformed to be binary with a loss of logarithmic factor.\nFollowing PAC learning theory, we can use uniform convergence to bound the generalization error of the empirical risk minimizer (ERM). Define the empirical loss for a given sample S \u223c Dm as\nloss S\n(f) = 1 m \u00b7 \u2211\nx\u2208S \u2113(f, x)\nDefine the ERM hypothesis for a given sample S \u223c Dm as\nf\u0302ERM = argmin f\u0302\u2208H loss S (f\u0302) .\nFor a hypothesis class H, a loss function \u2113 and a set of m samples S \u223c Dm, define the empirical Rademacher complexity of H with respect to \u2113 and S as, 1\nRS,\u2113(H) = E \u03c3\u223c{\u00b11}m [ sup f\u2208H 1 m \u2211 x\u2208S \u03c3i\u2113(f, x) ]\nLet the Rademacher complexity of H with respect to distribution D and loss \u2113 as Rm(H) = ES\u223cDm[RS,\u2113(H)]. When it\u2019s clear from the context, we will omit the subscript \u2113.\nWe can now state and apply standard generalization error results. The proof of following theorem is almost identical to [MRT12, Theorem 3.1]. For completeness we provide a proof in Appendix A.\nTheorem 2.1. For any \u03b4 > 0, with probability 1 \u2212 \u03b4, the generalization error of the ERM hypothesis is bounded by:\nloss D (f\u0302ERM ) 6 min f\u2208H loss D\n(f) + 6Rm(H) + \u221a 4 log 1\u03b4 2m\nAn immediate corollary of the theorem is that as long as the Rademacher complexity of a hypothesis class approaches zero as the number of examples goes to infinity, it can be C learned by an inefficient algorithm that optimizes over the hypothesis class by enumeration and outputs an best hypothesis with encoding length k and bias \u03b3 = 0. Not surprisingly such optimization is often intractable and hences the main challenge is to design efficient algorithms. As we will see in later sections, we often need to trade the encoding length and bias slightly for computational efficiency.\nNotation. For every vector z \u2208 Rd1 \u2297Rd2 , we can view it as a matrix of dimension d1 \u00d7 d2, which is denoted as M(z). Therefore in this notation, M(u\u2297 v) = uv\u22a4.\nLet vmax(\u00b7) : (Rd)\u22972 \u2192 Rd be the function that compute the top right-singular vector of some vector in (Rd)\u22972 viewed as a matrix. That is, for z \u2208 (Rd)\u22972, then vmax(z) denotes the top rightsingular vector of M(z). We also overload the notation vmax for generalized eigenvectors of higher order tensors. For T \u2208 (Rd)\u2297\u2113, let vmax(T ) = argmax\u2016x\u201661 T (x, x, . . . , x) where T (\u00b7) denotes the multi-linear form defined by tensor T .\nWe use \u2016A\u2016\u2113p\u2192\u2113q to denote the induced operator norm of A from \u2113p space to \u2113q space. For simplicity, we also define |A|1 = \u2016A\u2016\u2113\u221e\u2192\u21131 = \u2211 ij |Aij |, |A|\u221e = \u2016A\u2016\u21131\u2192\u2113\u221e = maxij |Aij |. We note that \u2016A\u2016\u21131\u2192\u21131 is the max column \u21131 norm of A, and \u2016A\u2016\u21131\u2192\u21132 is the largest column \u21132 norm of A. 1Technically, this is the Rademacher complexity of the class of functions \u2113\u25e6H. However, since \u2113 is usually fixed for certain problem, we emphasize in the definition more the dependency on H."}, {"heading": "3 Spectral autoencoders: unsupervised learning of algebraic manifolds", "text": ""}, {"heading": "3.1 Algebraic manifolds", "text": "The goal of the spectral autoencoder hypothesis class we define henceforth is to learn the representation of data that lies on a low-dimensional algebraic variety/manifolds. The linear variety, or linear manifold, defined by the roots of linear equations, is simply a linear subspace. If the data resides in a linear subspace, or close enough to it, then PCA is effective at learning its succinct representation.\nOne extension of the linear manifolds is the set of roots of low-degree polynomial equations. Formally, let k, s be integers and let c1, . . . , cds\u2212k \u2208 Rd s be a set of vectors in ds dimension, and consider the algebraic variety\nM = { x \u2208 Rd : \u2200i \u2208 [ds \u2212 k], \u3008ci, x\u2297s\u3009 = 0 } .\nObserve that here each constraint \u3008ci, x\u2297s\u3009 is a degree-s polynomial over variables x, and when s = 1 the variety M becomes a liner subspace. Let a1, . . . , ak \u2208 Rd s be a basis of the subspaces orthogonal to all of c1, . . . , cds\u2212k, and let A \u2208 Rk\u00d7d s\ncontains ai as rows. Then we have that given x \u2208 M, the encoding\ny = Ax\u2297s\npins down all the unknown information regarding x. In fact, for any x \u2208 M, we have A\u22a4Ax\u2297s = x\u2297s and therefore x is decodable from y. The argument can also be extended to the situation when the data point is close to M (according to a metric, as we discuss later). The goal of the rest of the subsections is to learn the encoding matrix A given data points residing close to M."}, {"heading": "3.2 Warm up: PCA and kernel PCA", "text": "In this section we illustrate our framework for agnostic unsupervised learning by showing how PCA and kernel PCA can be efficiently learned within our model. The results of this sub-section are not new, and given only for illustrative purposes.\nThe class of hypothesis corresponding to PCA operates on domain X = Rd and range Y = Rk for some k < d via linear operators. In kernel PCA, the encoding linear operator applies to the s-th tensor power x\u2297s of the data. That is, the encoding and decoding are parameterized by a linear operator A \u2208 Rk\u00d7ds ,\nHpcak,s = { (hA, gA) : hA(x) = Ax \u2297s, , gA(y) = A \u2020y } ,\nwhere A\u2020 denotes the pseudo-inverse of A. The natural loss function here is the Euclidean norm,\n\u2113((g, h), x) = \u2016x\u2297s \u2212 g(h(x))\u20162 = \u2016(I \u2212A\u2020A)x\u2297s\u20162 .\nTheorem 3.1. For a fixed constant s > 1, the class Hpcak,s is efficiently C -learnable with encoding length k and bias \u03b3 = 0.\nThe proof of the Theorem follows from two simple components: a) finding the ERM among Hpcak,s can be efficiently solved by taking SVD of covariance matrix of the (lifted) data points. b) The Rademacher complexity of the hypothesis class is bounded by O(ds/m) for m examples. Thus by Theorem 2.1 the minimizer of ERM generalizes. The full proof is deferred to Appendix B."}, {"heading": "3.3 Spectral Autoencoders", "text": "In this section we give a much broader set of hypothesis, encompassing PCA and kernel-PCA, and show how to learn them efficiently. Throughout this section we assume that the data is normalized to Euclidean norm 1, and consider the following class of hypothesis which naturally generalizes PCA:\nDefinition 3.1 (Spectral autoencoder). We define the class Hsak,s as the following set of all hypothesis (g, h),\nHsak = { (h, g) :\nh(x) = Ax\u2297s, A \u2208 Rk\u00d7ds g(y) = vmax(By), B \u2208 Rd s\u00d7k\n} . (3.1)\nWe note that this notion is more general than kernel PCA: suppose some (g, h) \u2208 Hpcak,s has reconstruction error \u03b5, namely, A\u2020Ax\u2297s is \u03b5-close to x\u2297s in Euclidean norm. Then by eigenvector perturbation theorem, we have that vmax(A\u2020Ax\u2297s) also reconstructs x with O(\u03b5) error, and therefore there exists a PSCA hypothesis with O(\u03b5) error as well . Vice versa, it\u2019s quite possible that for every A, the reconstruction A\u2020Ax\u2297s is far away from x\u2297s so that kernel PCA doesn\u2019t apply, but with spectral decoding we can still reconstruct x from vmax(A\u2020Ax\u2297s) since the top eigenvector of A\u2020Ax\u2297s is close x.\nHere the key matter that distinguishes us from kernel PCA is in what metric x needs to be close to the manifold so that it can be reconstructed. Using PCA, the requirement is that x is in Euclidean distance close to M (which is a subspace), and using kernel PCA x\u22972 needs to be in Euclidean distance close to the null space of ci\u2019s. However, Euclidean distances in the original space and lifted space typically are meaningless for high-dimensional data since any two data points are far away with each other in Euclidean distance. The advantage of using spectral autoencoders is that in the lifted space the geometry is measured by spectral norm distance that is much smaller than Euclidean distance (with a potential gap of d1/2). The key here is that though the dimension of lifted space is d2, the objects of our interests is the set of rank-1 tensors of the form x\u22972. Therefore, spectral norm distance is a much more effective measure of closeness since it exploits the underlying structure of the lifted data points.\nWe note that spectral autoencoders relate to vanishing component analysis [LLS+13]. When the data is close to an algebraic manifold, spectral autoencoders aim to find the (small number of) essential non-vanishing components in a noise robust manner."}, {"heading": "3.4 Learnability of polynomial spectral decoding", "text": "For simplicity we focus on the case when s = 2. Ideally we would like to learn the best encodingdecoding scheme for any data distribution D. Though there are technical difficulties to achieve such a general result. A natural attempt would be to optimize the loss function f(A,B) = \u2016g(h(x)) \u2212 x\u20162 = \u2016x \u2212 vmax(BAx\u22972)\u20162. Not surprisingly, function f is not a convex function with respect to A,B, and in fact it could be even non-continuous (if not ill-defined)!\nHere we make a further realizability assumption that the data distribution D admits a reasonable encoding and decoding pair with reasonable reconstruction error.\nDefinition 3.2. We say a data distribution D is (k, \u03b5)-regularly spectral decodable if there exist A \u2208 R k\u00d7d2 and B \u2208 Rd2\u00d7k with \u2016BA\u2016op 6 \u03c4 such that for x \u223c D, with probability 1, the encoding y = Ax\u22972 satisfies that M(By) = M(BAx\u22972) = xx\u22a4 + E , (3.2)\nwhere \u2016E\u2016op 6 \u03b5. Here \u03c4 > 1 is treated as a fixed constant globally. To interpret the definition, we observe that if data distribution D is (k, \u03b5)-regularly spectrally decodable, then by equation (3.2) and Wedin\u2019s theorem (see e.g. [Vu11] ) on the robustness of eigenvector to perturbation, M(By) has top eigenvector2 that is O(\u03b5)-close to x itself. Therefore, definition 3.2 is a\n2Or right singular vector when M(By) is not symmetric\nsufficient condition for the spectral decoding algorithm vmax(By) to return x approximately, though it might be not necessary. Moreover, this condition partially addresses the non-continuity issue of using objective f(A,B) = \u2016x \u2212 vmax(BAx\u22972)\u20162, while f(A,B) remains (highly) non-convex. We resolve this issue by using a convex surrogate.\nOur main result concerning the learnability of the aforementioned hypothesis class is:\nTheorem 3.2. The hypothesis class Hsak,2 is C - learnable with encoding length O(\u03c44k4/\u03b44) and bias \u03b4 with respect to (k, \u03b5)-regular distributions in polynomial time.\nOur approach towards finding an encoding and decoding matrice A,B is to optimize the objective,\nminimize f(R) = E [\u2225\u2225Rx\u22972 \u2212 x\u22972 \u2225\u2225 op ] (3.3)\ns.t. \u2016R\u2016S1 6 \u03c4k\nwhere \u2016 \u00b7 \u2016S1 denotes the Schatten 1-norm3. Suppose D is (k, \u03b5)-regularly decodable, and suppose hA and gB are the corresponding encoding and decoding function. Then we see that R = AB will satisfies that R has rank at most k and f(R) 6 \u03b5. On the other hand, suppose one obtains some R of rank k\u2032 such that f(R) 6 \u03b4, then we can produce hA and gB with O(\u03b4) reconstruction simply by choosing A \u2208 Rk\n\u2032\u00d7d2B and B \u2208 Rd2\u00d7k\u2032 such that R = AB. We use (non-smooth) Frank-Wolfe to solve objective (3.3), which in particular returns a low-rank solution. We defer the proof of Theorem 3.2 to the Appendix B.1. With a slightly stronger assumptions on the data distribution D, we can reduce the length of the code to O(k2/\u03b52) from O(k4/\u03b54). See details in Appendix C."}, {"heading": "4 A family of optimization encodings and efficient dictionary learning", "text": "In this section we give efficient algorithms for learning a family of unsupervised learning algorithms commonly known as \u201ddictionary learning\u201d. In contrast to previous approaches, we do not construct an actual \u201ddictionary\u201d, but rather improperly learn a comparable encoding via convex relaxations.\nWe consider a different family of codes which is motivated by matrix-based unsupervised learning models such as topic-models, dictionary learning and PCA. This family is described by a matrix A \u2208 R d\u00d7r which has low complexity according to a certain norm \u2016 \u00b7 \u2016\u03b1, that is, \u2016A\u2016\u03b1 6 c\u03b1. We can parametrize a family of hypothesis H according to these matrices, and define an encoding-decoding pair according to\nhA(x) = argmin \u2016y\u2016\u03b26k\n1 d |x\u2212Ay|1 , gA(y) = Ay\nWe choose \u21131 norm to measure the error mostly for convenience, though it can be quite flexible. The different norms \u2016\u00b7\u2016\u03b1, \u2016\u00b7\u2016\u03b2 over A and y give rise to different learning models that have been considered before. For example, if these are Euclidean norms, then we get PCA. If \u2016 \u00b7 \u2016\u03b1 is the max column \u21132 or \u2113\u221e norm and \u2016 \u00b7 \u2016b is the \u21130 norm, then this corresponds to dictionary learning (more details in the next section).\nThe optimal hypothesis in terms of reconstruction error is given by\nA\u22c6 = argmin \u2016A\u2016\u03b16c\u03b1 E x\u223cD\n[ 1\nd |x\u2212 gA(hA(x))|1\n] = argmin\n\u2016A\u2016\u03b16c\u03b1 E x\u223cD\n[ min\ny\u2208Rr :\u2016y\u2016\u03b26k\n1 d |x\u2212Ay|1\n] .\nThe loss function can be generalized to other norms, e.g., squared \u21132 loss, without any essential change in the analysis. Notice that this optimization objective derived from reconstruction error is\n3Also known as nuclear norm or trace norm\nidentically the one used in the literature of dictionary learning. This can be seen as another justification for the definition of unsupervised learning as minimizing reconstruction error subject to compression constraints.\nThe optimization problem above is notoriously hard computationally, and significant algorithmic and heuristic literature attempted to give efficient algorithms under various distributional assumptions(see [AGM13, AGMM15, AEB05] and the references therein). Our approach below circumvents this computational hardness by convex relaxations that result in learning a different creature, albeit with comparable compression and reconstruction objective."}, {"heading": "4.1 Improper dictionary learning: overview", "text": "We assume the max column \u2113\u221e norm of A is at most 1 and the \u21131 norm of y is assumed to be at most k. This is a more general setting than the random dictionaries (up to a re-scaling) that previous works [AGM13, AGMM15] studied. 4In this case, the magnitude of each entry of x is on the order of\u221a k if y has k random \u00b11 entries. We think of our target error per entry as much smaller than 15. We consider Hkdict that are parametrized by the dictionary matrix A = Rd\u00d7r,\nHdictk = { (hA, gA) : A \u2208 Rd\u00d7r, \u2016A\u2016\u21131\u2192\u2113\u221e 6 1 } ,\nwhere hA(x) = argmin \u2016y\u201616k |x\u2212Ay|1 , gA(y) = Ay\nHere we allow r to be larger than d, the case that is often called over-complete dictionary. The choice of the loss can be replaced by \u21132 loss (or other Lipschitz loss) without any additional efforts, though for simplicity we stick to \u21131 loss. Define A\u22c6 to be the the best dictionary under the model and \u03b5\u22c6 to be the optimal error,\nA\u22c6 = argmin\u2016A\u2016\u21131\u2192\u2113\u221e61 Ex\u223cD [ miny\u2208Rr :\u2016y\u201616k |x\u2212Ay|1 ] (4.1)\n\u03b5\u22c6 = Ex\u223cD [ 1 d \u00b7 |x\u2212 gA\u22c6(hA\u22c6(x))|1 ] .\nAlgorithm 1 group encoding/decoding for improper dictionary learning Inputs: N data points X \u2208 Rd\u00d7N \u223c DN . Convex set Q. Sampling probability \u03c1.\n1. Group encoding: Compute Z = argmin\nC\u2208Q |X \u2212 C|1 , (4.2)\nand let Y = h(X) = P\u2126(Z) ,\nwhere P\u2126(B) is a random sampling of B where each entry is picked with probability \u03c1.\n2. Group decoding: Compute\ng(Y ) = argmin C\u2208Q\n|P\u2126(C)\u2212 Y |1 . (4.3)\nTheorem 4.1. For any \u03b4 > 0, p > 1, the hypothesis class Hdictk is C -learnable (by Algorithm 2) with encoding length O\u0303(k2r1/p/\u03b42), bias \u03b4 +O(\u03b5\u22c6) and sample complexity dO(p) in time nO(p 2)\n4The assumption can be relaxed to that A has \u2113\u221e norm at most k and \u21132-norm at most \u221a d straightforwardly. 5We are conservative in the scaling of the error here. Error much smaller than \u221a k is already meaningful.\nWe note that here r can be potentially much larger than d since by choosing a large constant p the overhead caused by r can be negligible. Since the average size of the entries is \u221a k, therefore we can get the bias \u03b4 smaller than average size of the entries with code length roughly \u2248 k. The proof of Theorem 4.1 is deferred to Section 5.6. To demonstrate the key intuition and technique behind it, in the rest of the section we consider a simpler algorithm that achieves a weaker goal: Algorithm 1 encodes multiple examples into some codes with the matching average encoding length O\u0303(k2r1/p/\u03b42), and these examples can be decoded from the codes together with reconstruction error \u03b5\u22c6 + \u03b4. Next, we outline the analysis of Algorithm 1, and we will show later that one can reduce the problem of encoding a single examples to the problem of encoding multiple examples together.\nHere we overload the notation gA\u22c6(hA\u22c6(\u00b7)) so that gA\u22c6(hA\u22c6(X)) denotes the collection of all the gA\u22c6(hA\u22c6(xj)) where xj is the j-th column of X. Algorithm 1 assumes that there exists a convex set Q \u2282 Rd\u00d7N such that\n{ gA\u22c6(hA\u22c6(X)) : X \u2208 Rd\u00d7N } \u2282 {AY : \u2016A\u2016\u21131\u2192\u2113\u221e 6 1, \u2016Y \u2016\u21131\u2192\u21131 6 k} \u2282 Q . (4.4)\nThat is, Q is a convex relaxation of the group of reconstructions allowed in the class Hdict. Algorithm 1 first uses convex programming to denoise the data X into a clean version Z , which belongs to the set Q. If the set Q has low complexity, then simple random sampling of Z \u2208 Q serves as a good encoding.\nThe following Lemma shows that if Q has low complexity in terms of sampling Rademacher width, then Algorithm 1 will give a good group encoding and decoding scheme. Lemma 4.2. Suppose convex Q \u2282 Rd\u00d7N satisfies condition (4.4). Then, Algorithm 1 gives a group encoding and decoding pair such that with probability 1\u2212 \u03b4, the average reconstruction error is bounded by \u03b5\u22c6 + O( \u221a SRWm(Q) + O( \u221a log(1/\u03b4)/m) where m = \u03c1Nd and SRWm(\u00b7) is the sampling Rademacher width (defined in subsection 5.2), and the average encoding length is O\u0303(\u03c1d).\nThe proofs here are technically standard: Lemma 4.2 simply follows from Lemma 5.1 and Lemma 5.2 in Section 5. Lemma 5.1 shows that the difference between Z and X is comparable to \u03b5\u22c6, which is a direct consequence of the optimization over a large set Q that contains optimal reconstruction. Lemma 5.2 shows that the sampling procedure doesn\u2019t lose too much information given a denoised version of the data is already observed, and therefore, one can reconstruct Z from Y .\nThe novelty here is to use these two steps together to denoise and achieve a short encoding. The typical bottleneck of applying convex relaxation on matrix factorization based problem (or any other problem) is the difficulty of rounding. Here instead of pursuing a rounding algorithm that output the factor A and Y , we look for a convex relaxation that preserves the intrinsic complexity of the set which enables the trivial sampling encoding. It turns out that controlling the width/complexity of the convex relaxation boils down to proving concentration inequalities with sum-of-squares (SoS) proofs, which is conceptually easier than rounding.\nTherefore, the remaining challenge is to design convex set Q that simultaneously has the following properties\n(a) is a convex relaxation in the sense of satisfying condition (4.4)\n(b) admits an efficient optimization algorithm\n(c) has low complexity (that is, sampling rademacher width O\u0303(N poly(k)))\nMost of the proofs need to be deferred to Section 5. We give a brief overview: In subsection 5.3 we will design a convex set Q which satisfies condition (4.4) but not efficiently solvable, and in subsection 5.4 we verify that the sampling Rademacher width is O(Nk log d). In subsection 5.5, we prove that a sumof-squares relaxation would give a set Qsosp which satisfies (a), (b) and (c) approximately. Concretely, we have the following theorem.\nTheorem 4.3. For every p > 4, let N = dc0p with a sufficiently large absolute constant c0. Then, there exists a convex set Qsosp \u2282 Rd\u00d7N (which is defined in subsection 5.5.2) such that (a) it satisfies condition 4.4; (b) The optimization (4.2) and (4.3) are solvable by semidefinite programming with run-time nO(p 2); (c) the sampling Rademacher width of Qsosp is bounded by \u221a SRWm(Q) 6 O\u0303(k2r2/pN/m).\nWe note that these three properties (with Lemma 4.2) imply that Algorithm 1 with Q = Qsosp and \u03c1 = O(k2r2/pd\u22121/\u03b42 \u00b7 log d) gives a group encoding-decoding pair with average encoding length O(k2r2/p/\u03b42 \u00b7 log d) and bias \u03b4.\nProof Overview of Theorem 4.3: At a very high level, the proof exploits the duality between sumof-squares relaxation and sum-of-squares proof system. Suppose w1, . . . , wd are variables, then in SoS relaxation an auxiliary variable WS is introduced for every subset S \u2282 [d] of size at most s, and valid linear constraints and psd constraint for WS\u2019s are enforced. By convex duality, intuitively we have that if a polynomial q(x) = \u2211 |S|6s \u03b1SxS can be written as a sum of squares of polynomial q(x) = \u2211 j rj(x)\n2, then the corresponding linear form over XS , \u2211 |S|6s\u03b1SXS is also nonnegative. Therefore, to certify\ncertain property of a linear form \u2211\n|S|6s \u03b1SXS over XS , it is sufficient (and also necessary by duality) that the corresponding polynomial admit a sum-of-squares proof.\nHere using the idea above, we first prove the Rademacher width of the convex hull of reconstructions Q0 = conv {Z = AY : \u2016A\u2016\u21131\u2192\u2113\u221e 6 1, \u2016Y \u2016\u21131\u2192\u21131 6 k} using a SoS proof. Then the same proof automatically applies to for the Rademacher width of the convex relaxation (which is essentially a set of statements about linear combinations of the auxiliary variables). We lose a factor of r2/p because SoS proof is not strong enough for us to establish the optimal Rademacher width of Q0."}, {"heading": "5 Analysis of Improper Dictionary Learning", "text": "In this section we give the full proof of the Theorems and Lemmas in Section 4. We start by stating general results on denoising, Rademacher width and factorizable norms, and proceed to give specialized bounds for our setting in section 5.4."}, {"heading": "5.1 Guarantees of denoising", "text": "In this subsection, we give the guarantees of the error caused by the denoising step. Recall that \u03b5\u22c6 is the optimal reconstruction error achievable by the optimal (proper) dictionary (equation (4.1)).\nLemma 5.1. Let Z be defined in equation (4.2). Then we have that\n1\nNd E X\u223cDN [|Z \u2212X|1] 6 \u03b5\u22c6 (5.1)\nProof. Let Y \u22c6 = A\u22c6hA\u22c6(X) where hA\u22c6(X) denote the collection of encoding of X using hA\u22c6 . Since Y \u22c6 \u2208 {AY : \u2016A\u2016|\u21131\u2192\u2113\u221e 6 1, \u2016Y \u2016\u21131\u2192\u21131 6 k} \u2282 Q, we have that Y \u22c6 is a feasible solution of optimization (4.2). Therefore, we have that 1Nd E [|Z \u2212X|1] 6 1Nd E [|X \u2212 Y \u22c6|1] = \u03b5\u22c6, where the equality is by the definition of \u03b5\u22c6."}, {"heading": "5.2 Sampling Rademacher Width of a Set", "text": "As long as the intrinsic complexity of the set Q is small then we can compress by random sampling. The idea of viewing reconstruction error the test error of a supervised learning problem started with the work of Srebro and Shraibman [SS05], and has been used for other completion problems, e.g., [BM15]. We use the terminology \u201cRademacher width\u201d instead of \u201cRademacher complexity\u201d to emphasize that the notation defined below is a property of a set of vectors (instead of that of a hypothesis class).\nFor any set W \u2282 RD, and an integer m, we define its sampling Rademacher width (SRW) as,\nSRWm(W ) = E \u03c3,\u2126\n[ 1\nm sup x\u2208W\n\u3008x, \u03c3\u3009\u2126 ] , (5.2)\nwhere \u2126 is random subset of [D] of size m, \u3008a, b\u3009\u2126 is defined as \u2211\ni\u2208\u2126 aibi and \u03c3 \u223c {\u00b11}D . Lemma 5.2. ([BM15, Theorem 2.4]) With probability at least 1\u2212\u03b4 over the choice of \u2126, for any z\u0303 \u2208 RD\n1 D |z\u0303 \u2212 z|1 6 1 m |P\u2126(z\u0303)\u2212 P\u2126(z)|1 + 2SRWm(W ) +M\n\u221a log(1/\u03b4)\nm .\nwhere M = supz\u0303\u2208W,i\u2208[D] |zi \u2212 z\u0303i|."}, {"heading": "5.3 Factorable norms", "text": "In this subsection, we define in general the factorable norms, from which we obtain a convex set Q which satisfies condition (4.4) (see Lemma 5.3).\nFor any two norms \u2016 \u00b7 \u2016\u03b1, \u2016 \u00b7 \u2016\u03b2 that are defined on matrices of any dimension, we can define the following quantity\n\u0393\u03b1,\u03b2(Z) = inf Z=AB\n\u2016A\u2016\u03b1\u2016B\u2016\u03b2 (5.3)\nFor any p, q, s, t > 1, we use \u0393p,q,s,t(\u00b7) to denote the function \u0393\u2113p\u2192\u2113q,\u2113s\u2192\u2113t(\u00b7). When p = t, \u0393p,q,s,p(Z) is the factorable norm [TJ89, Chapter 13] of matrix Z . In the case when p = t = 2, q = \u221e, s = 1, we have that \u03932,\u221e,1,2(\u00b7) is the \u03b32-norm [LMSS07] or max norm [SS05], which has been used for matrix completion.\nThe following Lemma is the main purpose of this section which shows a construction of a convex set Q that satisfies condition (4.4).\nLemma 5.3. For any q, t > 1 we have that \u03931,q,1,t(\u00b7) is a norm. As a consequence, letting Q1,\u221e,1,1 = {C \u2208 RN\u00d7d : \u03931,\u221e,1,1(C) 6 \u221a dk}, we have that Q1,\u221e,1,1 is a convex set and it satisfies condition (4.4).\nTowards proving Lemma 5.3, we prove a stronger result that if p = s = 1, then \u0393p,q,s,t is also a norm. This result is parallel to [TJ89, Chapter 13] where the case of p = t is considered.\nTheorem 5.4. Suppose that \u2016 \u00b7 \u2016\u03b1 and \u2016 \u00b7 \u2016\u03b2 are norms defined on matrices of any dimension such that\n1. \u2016[A,B]\u2016\u03b1 6 max{\u2016A\u2016\u03b1, \u2016B\u2016\u03b1}\n2. \u2016 \u00b7 \u2016\u03b2 is invariant with respect to appending a row with zero entries.\nThen, \u0393\u03b1,\u03b2(\u00b7) is a norm.\nProof. Non-negativity: We have that \u0393\u03b1,\u03b2(Z) > 0 by definition as a product of two norms. Further, \u0393\u03b1,\u03b2(Z) = 0 if and only if \u2016A\u2016\u03b1 = 0 or \u2016B\u2016\u03b2 = 0, which is true if and only if A or B are zero, which means that Z is zero.\nAbsolute scalability: For any positive t, we have that if Z = AB and \u0393\u03b1,\u03b2(Z) 6 \u2016A\u2016\u03b1\u2016B\u2016\u03b2 , then tZ = (tA) \u00b7 B and \u0393\u03b1,\u03b2(tZ) 6 t\u2016A\u2016\u03b1\u2016B\u2016\u03b2 . Therefore by definition of \u0393\u03b1,\u03b2(Z), we get \u0393\u03b1,\u03b2(tZ) 6 t\u0393\u03b1,\u03b2(Z).\nIf we replace Z by tZ and t by 1/t we obtain the other direction, namely \u0393\u03b1,\u03b2(Z) 6 1/t \u00b7\u0393\u03b1,\u03b2(tZ). Therefore, \u0393\u03b1,\u03b2(tZ) = t\u0393\u03b1,\u03b2(Z) for any t > 0.\nTriangle inequality: We next show that \u0393\u03b1,\u03b2(Z) satisfies triangle inequality, from which the result follows. Let W and Z be two matrices of the same dimension. Suppose A,C satisfy that Z = AC and\n\u0393\u03b1,\u03b2(Z) = \u2016A\u2016\u03b1\u2016C\u2016\u03b2 . Similarly, suppose W = BD, and \u0393\u03b1,\u03b2(W ) = \u2016B\u2016\u03b1\u2016D\u2016\u03b2 . Therefore, we\nhave that W + Z = [tA,B] [ t\u22121C D ] , and that for any t > 0,\n\u0393\u03b1,\u03b2(W + Z) 6 \u2016[tA,B]\u2016\u03b1 \u2225\u2225\u2225\u2225 [ t\u22121C D ]\u2225\u2225\u2225\u2225 \u03b2\n(by defintion of \u0393\u03b1,\u03b2)\n6 \u2016[tA,B]\u2016\u03b1 (\u2225\u2225\u2225\u2225 [ 0 D ]\u2225\u2225\u2225\u2225 \u03b2 + t\u22121 \u2225\u2225\u2225\u2225 [ C 0 ]\u2225\u2225\u2225\u2225 \u03b2 ) (by triangle inquality) 6 max {t \u2016A\u2016\u03b1 , \u2016B\u2016\u03b1} ( t\u22121 \u2016C\u2016\u03b2 + \u2016D\u2016\u03b2 ) (by assumptions on \u2016 \u00b7 \u2016\u03b1 and \u2016 \u00b7 \u2016\u03b2)\nPick t = \u2016B\u2016\u03b1\u2016A\u2016\u03b1 , we obtain that,\n\u0393\u03b1,\u03b2(W + Z) 6 \u2016A\u2016\u03b1 \u2016C\u2016\u03b2 + \u2016B\u2016\u03b1 \u2016D\u2016\u03b2 = \u0393\u03b1,\u03b2(Z) + \u0393\u03b1,\u03b2(W ) .\nNote that if \u2016 \u00b7 \u2016\u03b1 is a \u21131 \u2192 \u2113q norm, then it\u2019s also the max column-wise \u2113q norm, and therefore it satisfies the condition a) in Theorem 5.4. Moreover, for similar reason, \u2016 \u00b7 \u2016\u03b2 = \u2016 \u00b7 \u2016\u21131\u2192\u2113t satisfies the condition b) in Theorem 5.4. Hence, Lemma 5.3 is a direct consequence of Theorem 5.4. Lemma 5.3 gives a convex set Q that can be potentially used in Algorithm 1."}, {"heading": "5.4 Sampling Rademacher width of level set of \u03931,\u221e,1,1", "text": "Here we give a Rademacher width bound for the specific set we\u2019re interested in, namely the level sets of \u03931,\u221e,1,1, formally,\nQ1,\u221e,1,1 = {C \u2208 RN\u00d7d : \u03931,\u221e,1,1(C) 6 k}. By the definition Q1,\u221e,1,1 satisfies condition (4.4). See section 5.2 for definition of Ramemacher\nwidth.\nLemma 5.5. It holds that\nSRWm(Q1,\u221e,1,1) 6 O\u0303 (\u221a k2N\nm\n) .\nProof of Lemma 5.5. Recall the definition of the sample set \u2126 of coordinates from C , and their multiplication by i.i.d Rademacher variables in section 5.2. Reusing notation, let \u03be = \u03c3\u2299\u2126 and we use Q as a shorthand for Q1,\u221e,1,1. Here \u2299 means the entry-wise Hadamard product (namely, each coordinate in \u2126 is multiplied by an independent Rademacher random variable). We have that\nSRWm(Q) = E \u03be\n[ 1\nm sup C\u2208Q\n\u3008C, \u03be\u3009 ]\n= E \u03be\n[ 1\nm sup \u2016A\u2016\u21131\u2192\u2113\u221e61,\u2016B\u2016\u21131\u2192\u211316k \u3008AB, \u03be\u3009\n] (by defintiion of Q)\n= E \u03be\n[ 1\nm sup \u2016A\u2016\u21131\u2192\u2113\u221e61,\u2016B\u2016\u21131\u2192\u211316k \u3008B,A\u22a4\u03be\u3009\n]\n6 E \u03be\n[ 1\nm sup \u2016A\u2016\u21131\u2192\u2113\u221e61 k\nN\u2211\ni=1\n\u2016A\u22a4\u03bei\u2016\u221e ] . (By \u3008U, V \u3009 6 (\u2211N i=1 \u2016Ui\u2016\u221e ) \u2016V \u2016\u21131\u2192\u21131)\nLet \u03c1 = mdN be the probability of any entry belongs to \u2126. Let \u03bei denote the i-th column of \u03be, and Aj denotes the j-th column of A. Therefore, each entry of \u03bei has probability \u03c1/2 to be +1 and -1, and has probability 1 \u2212 \u03c1 of being 0. By concentration inequality we have that for \u03c1 > log rd , and any fixed A with \u2016A\u2016\u21131\u2192\u2113\u221e = max \u2016Aj\u2016\u221e 6 1,\nE \u03bei [\u2016A\u22a4\u03bei\u2016\u221e] 6 O(\n\u221a \u03c1d log r log d) . (5.4)\nMoreover, we have that Var\u03bei [\u2016A\u22a4\u03bei\u2016\u221e] 6 O( \u221a \u03c1d log r log d) . (5.5) Moreover, \u2016A\u22a4\u03bei\u2016\u221e has an sub-exponential tail. (Technically, its \u03c81-Orlicz norm is bounded by O( \u221a \u03c1d log r log d)). Note that the variance of \u2211N i=1 \u2016A\u22a4\u03bei\u2016\u221e will decrease as N increases, and therefore for large enough N = (dr\u03c1)\u2126(1), we will have that with probability 1\u2212 exp(\u2212\u2126(dr)\u2126(1)),\nN\u2211\ni=1\n\u2016A\u22a4\u03bei\u2016\u221e 6 O(N \u221a \u03c1d log r log d)\nTherefore, using the standard \u03b5-net covering argument, we obtain that with high probability,\nsup \u2016A\u2016\u21131\u2192\u211326 \u221a d\nN\u2211\ni=1\n\u2016A\u22a4\u03bei\u2016\u221e 6 O(N \u221a \u03c1d log r log d) .\nHence, altogether we have\nSRWm(Q) 6 E \u03be   1 m\nsup \u2016A\u2016\u21131\u2192\u211326 \u221a d\nk\u2016A\u22a4\u03be\u2016\u21131\u2192\u2113\u221e\n  6 O\u0303 (\u221a k2N\nm\n) ."}, {"heading": "5.5 Convex Relaxation for \u03931,\u221e,1,1 norm", "text": ""}, {"heading": "5.5.1 Sum-of-squares relaxation", "text": "Here we will only briefly introduce the basic ideas of Sum-of-Squares (Lasserre) relaxation [Par00, Las01] that will be used for this paper. We refer readers to the extensive study [Las15, Lau09, BS14] for detailed discussions of sum of squares proofs and their applications to algorithm design. Recently, there has been a popular line of research on applications of sum-of-squares algorithms to machine learning problems [BKS15b, BKS14, BM16, MW15, GM15, HSS15, HSSS16, MSS16]. Here our technique in the next subsection is most related to that of [BM16], with the main difference that we deal with \u21131 norm constraints that are not typically within the SoS framework.\nLet R[x]d denote the set of all real polynomials of degree at most d with n variables x1, . . . , xn. We start by defining the notion of pseudo-expectation. The intuition is that the pseudo-expectation behave like the actual expectation of a real probability distribution on squares of polynomials.\nDefinition 5.1 (pseudo-expectation). A degree-d pseudo-expectation E\u0303 is a linear operator that maps R[x]d to R and satisfies E\u0303(1) = 1 and E\u0303(p2(x)) > 0 for all real polynomials p(x) of degree at most d/2.\nDefinition 5.2. Given a set of polynomial equations A = {q1(x) = 0, . . . , qn(x) = 0}, we say degreed pseudo-expectation E\u0303 satisfies constraints A if E\u0303 [qi(x)r(x)] = 0 for every i and r(x) such that deg(qir) 6 d.\nOne can optimize over the set of pseudo-expectations that satisfy A in nO(d) time by the following semidefinite program:\nVariables E\u0303[xS ] \u2200S : |S| 6 d Subject to E\u0303 [ qi(x)x K ] = 0 \u2200i,K : |K|+ deg(qi) 6 d\nE\u0303 [ x\u2297d/2(x\u2297d/2)\u22a4 ] 0\nDefinition 5.3 (SoS proof of degree d). For a set of constraints A = {q1(x) = 0, . . . , qn(x) = 0}, and an integer d, we write A \u22a2d p(x) > q(x) if there exists polynomials hi(x) for i = 0, 1, . . . , \u2113 and gj(x) for j = 1, . . . , t such that deg(hi) 6 d/2 and deg(gjrj) 6 d that satisfy\np(x)\u2212 q(x) = \u2113\u2211\ni=1\nhi(x) 2 +\nt\u2211\nj=1\nrj(x)gj(x) .\nWe will drop the subscript d when it is clear form the context.\nThe following fact follows from the definitions above but will be useful throughout the proof.\nProposition 5.6. Suppose A \u22a2d p(x) > q(x). Then for any degree-d pseudo-expectation E\u0303 that satisfies A, we have E\u0303[p(x)] > E\u0303[q(x)]."}, {"heading": "5.5.2 Relaxation for \u03931,\u221e,1,1 norm", "text": "In this section, we introduce convex relaxations for the \u03931,\u221e,1,1 norm. For convenience, let p be a power of 2. Let A and B be formal variables of dimension d \u00d7 r and r \u00d7 N in this section. We introduce more formal variables for the relaxation. Let b be formal variables of dimension r\u00d7N . We consider the following set of polynomial constraints over formal variables A,B, b:\nA = { \u2200i, j, Bij = bp\u22121ij , r\u2211\n\u2113=1\nbp\u2113j 6 k p/(p\u22121),\u2200i, k,A2ik 6 1\n} .\nFor any real matrix C \u2208 Rd\u00d7N , we define\nA(C) = {C = AB} \u222a A .\nWe define our convex relaxation for Q1,\u221e,1,1 as follows,\nQsosp = {C \u2208 Rd\u00d7N : \u2203degree-O(p2) pseudo-expectation E\u0303 that satisfies A(C)} (5.6)\nLemma 5.7. For any p > 4, we have Q1,\u221e,1,1 \u2282 Qsosp\nand therefore Qsosp satisfies condition (4.4).\nProof. Suppose C \u2208 Q1,\u221e,1,1. Then by definition of the \u03931,\u221e,1,1-norm (equation (5.3)), we have that there exists matrices U, V such that C = UV and U2ij 6 1 and \u2016V \u2016\u21131\u2192\u21131 6 k. Now construct vij = V 1/(p\u22121) ij . We have \u2211r \u2113=1 b p ij 6 (\u2211r \u2113=1 v p\u22121 ij )p/(p\u22121) 6 kp/(p\u22121). Therefore, Then we have that A = U,B = V, b = v satisfies the constraint (5.6). Then the trivial pseudo-expectation operator E\u0303[p(A,B, b)] = p(U, V, v) satisfies A(C) and therefore C \u2208 Qsosp by definition.\nTheorem 5.8. Suppose N = dc0p for large enough absolute constant c0. Then the sampling Rademacher complexity of Qsosp is bounded by,\nSRWm(Qsosp ) 6 O (\u221a p2Nk2r2/p log d\nm\n) .\nThe proof of Theorem 5.8 finally boils down to prove certain empirical process statement with SoS proofs. We start with the following two lemmas.\nLemma 5.9. Suppose N = dc0p for larger enough constant c0, and let \u03be = \u03c3 \u2299 \u2126 where \u03c3 and \u2126 are defined in Section 5.2. Then, we have\nA \u22a2 \u3008AB, \u03be\u3009p 6 Np\u22121kp N\u2211\ni=1\n\u2016A\u22a4\u03bei\u2016pp\nProof. Let \u03bei be the i-th column of \u03be. We have that\nA \u22a2\u3008AB, \u03be\u3009p = \u3008A\u22a4\u03be,B\u3009p = ( N\u2211\ni=1\n\u3008A\u22a4\u03be,Bi\u3009 )p\n6 Np\u22121 ( N\u2211\ni=1\n\u3008A\u22a4\u03bei, Bi\u3009p )\n(since \u22a2 ( 1 N \u2211 i\u2208N \u03b1i )p 6 1N \u2211 i\u2208[N ] \u03b1 p i )\n= Np\u22121 N\u2211\ni=1\n\u3008A\u22a4\u03bei, b\u2299p\u22121i \u3009p (by Bij = b p\u22121 ij )\n6 Np\u22121 N\u2211\ni=1\n\u2016A\u22a4\u03bei\u2016pp\u2016bi\u2016(p\u22121)pp (by \u22a2 (\u2211 j aib p\u22121 j )p 6 ( \u2211 i a p i ) ( \u2211 bpi ) p\u22121; see Lemma D.1)\n6 Np\u22121kp N\u2211\ni=1\n\u2016A\u22a4\u03bei\u2016pp (by constraint \u2211r \u2113=1 b p \u2113j 6 k p/(p\u22121))\nLemma 5.10. In the setting of Lemma 5.9, let \u03c1 = m/(Nd). Let x = (x1, . . . , xd) be indeterminates and let B = {x21 6 1, . . . , x2d 6 1}. Suppose \u03c1 > 1/d. With probability at least 1 \u2212 exp(\u2212\u2126(d)) over the choice of \u03be, there exists a SoS proof,\nB \u22a2 \u2016\u03be\u22a4x\u2016pp 6 N \u00b7O(\u03c1dp2)p/2. (5.7)\nAs an immediate consequence, let \u03bei be the i-th column of \u03be, then, with probability at least 1 \u2212 exp(\u2212\u2126(d)) over the choice of \u03be there exists SoS proof,\nA \u22a2 N\u2211\ni=1\n\u2016A\u22a4\u03bei\u2016pp 6 Nr \u00b7O(\u03c1dp2)p/2 . (5.8)\nProof. Recall that p is an even number. We have that\nB \u22a2 \u2016\u03be\u22a4x\u2016pp = ( x\u2297p/2\n)\u22a4 ( N\u2211\ni=1\n\u03be \u2297p/2 i ( \u03be \u2297p/2 i )\u22a4 ) x\u2297p/2\nLet T = \u2211N\ni=1 \u03be \u2297p/2 i ( \u03be \u2297p/2 i )\u22a4 \u2208 Rdp/2\u00d7dp/2 . Then, by definition we have that\nE\n[( x\u2297p/2 )\u22a4 Tx\u2297p/2 ] = E [\u2211N i=1\u3008\u03bei, x\u3009p ] . It follows that\nB \u22a2 \u2016\u03be\u22a4x\u2016pp = ( x\u2297p/2 )\u22a4 (T \u2212 E[T ])x\u2297p/2 + E\n[ N\u2211\ni=1\n\u3008\u03bei, x\u3009p ]\n6 \u2016x\u2016p \u2016T \u2212 E[T ]\u2016+ E [ N\u2211\ni=1\n\u3008\u03bei, x\u3009p ]\n(by \u22a2 y\u22a4By 6 \u2016y\u20162\u2016B\u2016)\n6 dp/2 \u2016T \u2212 E[T ]\u2016+ E [ N\u2211\ni=1\n\u3008\u03bei, x\u3009p ] .\nLet \u03b6 have the same distribution as \u03bei. Then we have that\nE [\u3008\u03b6, x\u3009p] = \u2211\n\u03b1\nt\u03b1x \u03b1 .\nwhere t\u03b1 = 0 if x\u03b1 contains an odd degree. Moreover, let |\u03b1| be the number xi with non-zero exponents that x\u03b1 has. Then we have t\u03b1 6 \u03c1|\u03b1|pp. Therefore we have that for \u03c1 > 1/d,\nB \u22a2 E [\u3008\u03b6, x\u3009p] 6 dp/2\u03c1p/2pp .\nIt follows that\nB \u22a2 E [ N\u2211\ni=1\n\u3008\u03bei, x\u3009p ] 6 N \u00b7O(\u03c1dp2)p/2 .\nNext, we use Bernstein inequality to bound \u2016T \u2212 E[T ]\u2016. We control the variance by\n\u03c32 , \u2225\u2225\u2225\u2225\u2225E [ N\u2211\ni=1\n\u2016\u03be\u2016p\u03be\u2297p/2i ( \u03be \u2297p/2 i )\u22a4 ]\u2225\u2225\u2225\u2225\u2225 6 Nd 2p.\nMoreover, each summand in the definition of T can be bounded by \u2225\u2225\u2225\u2225\u03be \u2297p/2 i ( \u03be \u2297p/2 i )\u22a4\u2225\u2225\u2225\u2225 6 dp. Note that these bounds are not necessarily tight since they already suffice for our purpose. Therefore, we obtain that with high probability over the chance of \u03be, it holds that \u2016T \u2212 E[T ]\u2016 6 \u221a NdO(p) logN . Therefore for N > d\u2126(p), we have that with high probability over the choice of \u03be,\nB \u22a2 \u2016\u03be\u22a4x\u2016pp (5.9)\n6 dp/2 \u2016T \u2212 E[T ]\u2016+ E [ N\u2211\ni=1\n\u3008\u03bei, x\u3009p ] 6 N \u00b7 O(\u03c1dp2)p/2 .\nFinally we show equation (5.7) implies equation (5.8). Let Ai be the i-th column of A. Note that we have \u2211N i=1 \u2016A\u22a4\u03bei\u2016 p p = \u2211r i=1 \u2016\u03be\u22a4Ai\u2016 p p, since both left-hand side and right-hand side are equal to the p-th power of all the entries of the matrix A\u22a4\u03be. Since A \u22a2 {\u2200j,A2ij 6 1}, we can invoke the first part of the Lemma and use equation (5.7) to complete the proof.\nCombining Lemma 5.9 and Lemma 5.10 we obtain the following corollary,\nCorollary 5.11. In the setting of Lemma 5.9, with probability 1\u2212 exp(\u2212\u2126(d)), we have\nA \u22a2 \u3008AB, \u03be\u3009p 6 Npkpr \u00b7 O(\u03c1d)p/2 . (5.10)\nNow we are ready to prove Theorem 5.8. Essentially the only thing that we need to do is take pseudo-expectation on the both sides of equation (5.10).\nProof of Theorem 5.8. Recall that \u03be = \u03c3 \u2299 \u2126. We have that\nSRWm(Qsosp )p = E \u03be\n[ 1\nmp sup C\u2208Qsosd \u3008C, \u03be\u3009\n]p\n6 E \u03be\n[ 1\nmp sup C\u2208Qsosd \u3008C, \u03be\u3009p\n] (by Jensen\u2019s inequality)\n= E \u03be\n[ 1\nmp sup E\u0303 that satisfies A(C) E\u0303 [\u3008AB, \u03be\u3009]\n] (by definition of Qsosp )\n6 E\n[ 1\nmp sup E\u0303 that satisfies A(C) E\u0303 [\u3008AB, \u03be\u3009] | equation (5.8) holds\n]\n+ P [equation (5.8) doesn\u2019t hold] \u00b7 dO(1)\n6 1\nmp Npkpr \u00b7 O(\u03c1dp2)p/2 + exp(\u2212\u2126(d))dO(1)\n(by Corollary 5.11 and Proposition 5.6)\n6 1\nmp Npkpr \u00b7 O(\u03c1dp2)p/2 .\nTherefore using the fact that \u03c1 = m/(Nd), taking p-th root on the equation above, we obtain,\nSRWm(Qsosp ) 6 O (\u221a p2Nr2/pk2 log d\nm\n) ."}, {"heading": "5.6 Proof of Theorem 4.1", "text": "In this subsection we prove Theorem 4.1 using the machinery developed in previous subsections. Essentially the idea here is to reduce the problem of encoding one example to the problem of encoding a group of examples. To encode an example x, we draw N \u2212 1 fresh examples x1, . . . , xN\u22121, and then call the group encoding Algorithm 1.\nWe also note that the encoding procedure can be simplified by removing the denoising step, and this still allows efficient decoding steps. Since in this case the encoding contains more noise from the data, we prefer the current scheme.\nProof of Theorem 4.1. We will prove that Algorithm 2 with Q = Qpsos and \u03c1 = O(k2r2/pd\u22121/\u03b42 \u00b7 log d) gives the desired encoding/decoding. Indeed with high probability, the encoding length is O\u0303(\u03c1d) = O\u0303(k2r2/p/\u03b42). Next we will show that we obtain good reconstruction in the sense that 1 d E [|x\u2212 g(h(x))|1] = 1d E [|x\u2212 z\u0303|1] 6 O(\u03b5\u22c6 + \u03b4). Here and throughout the proof, the expectation is over the randomness of x and the randomness of the algorithm (that is, the randomness of x, x1, . . . , xN\u22121 and the random sampling). First of all, by symmetry, we have that\nE [|x\u2212 z\u03031|1] = 1\nNd E [|[x1, . . . , xN\u22121, x]\u2212 [z\u03031, . . . , z\u0303N\u22121, z\u0303]|1] . (5.12)\nLet X = [x1, . . . , xN\u22121, x], and Z\u0303 = [z\u03031, . . . , z\u0303N\u22121, z\u0303] for simplicity of notations. Let [Z1,i, zi] = argmin[C,c]\u2208Q |[X1,i, xi]\u2212 [C, c]|1 where X1,i be the samples drawn in the encoding process for xi.\nAlgorithm 2 Encoding/decoding for improper dictionary learning Given: Integer N . Convex set Q \u2282 Rd\u00d7N . Sampling probability \u03c1.\n1. Encoding: input: data point x \u2208 Rd \u223c D, output: codeword y = h(x) \u2208 Rk\u00d7N Draw N \u2212 1 data points X1 \u2208 Rd\u00d7N\u22121 \u223c DN\u22121. Denoising step:\n[Z1, z] = argmin [C,c]\u2208Q\n|[X1, x]\u2212 [C, c]|1 , (5.11)\nRandom sampling step: h(x) = P\u2126(z) ,\nwhere P\u2126(z) is a random sampling of z where each entry is picked with probability \u03c1.\n2. Decoding: input: codeword y \u2208 Rk\u00d7N output: reconstruction g(y) \u2208 Rd\u00d7N\nTake N more samples x1, . . . , xN . Encode them to y1 = h(x1), . . . , yN = h(xN ). Then, compute\n[z\u03031, . . . , z\u0303N\u22121, z\u0303] = argmin C\u2208Q\n|P\u2126([C, c]) \u2212 [y1, . . . , yN , y]|1 .\nReturn g(y) = z\u0303.\nLet Z = [z1, . . . , zN\u22121, z]. By Lemma 5.1 and the symmetry, we have that for any i, 1d E [|zi \u2212 xi|1] 6 1 dN E [ |[Z1,i, zi]\u2212 [X1,i, xi]|1 ] 6 \u03b5\u22c6. Thus, we have 1dN E [|X \u2212 Z|1] 6 \u03b5\u22c6.\nLet Z\u0303 \u2208 Rd\u00d7N be a helper matrix in the analysis defined as\nZ\u0302 = argmin C\u2208Q\n|X \u2212 C|1 . (5.13)\nThen by Lemma 5.1 we have that 1Nd E [\u2223\u2223\u2223Z\u0302 \u2212X \u2223\u2223\u2223 1 ] 6 \u03b5\u22c6. Then by triangle inequality we obtain that\n1\nNd E [\u2223\u2223\u2223Z\u0302 \u2212 Z \u2223\u2223\u2223 1 ] 6 2\u03b5\u22c6 . (5.14)\nNote that by definition Z\u0302 \u2208 Q. Therefore, by Lemma 5.2, we have that\n1\nNd E [\u2223\u2223\u2223Z\u0303 \u2212 Z \u2223\u2223\u2223 1 ] 6 1 m E [\u2223\u2223\u2223P\u2126(Z\u0303)\u2212 P\u2126(Z) \u2223\u2223\u2223 1 ] + 2SRWm(W ) (by Lemma 5.2)\n6 1\nm E [\u2223\u2223\u2223P\u2126(Z\u0302)\u2212 P\u2126(Z) \u2223\u2223\u2223 1 ] + 2SRWm(W )\n(since Z\u0303 is the minimizer, and Z\u0302 \u2208 Q.)\n= 1\nNd E [\u2223\u2223\u2223Z\u0302 \u2212 Z \u2223\u2223\u2223 1 ] + 2SRWm(W )\n6 \u03b5\u22c6 + \u03b4 . (using equation (5.14) and Theorem 4.3 (c))\nFinally by triangle inequality again we have\n1\nNd E [\u2223\u2223\u2223Z\u0303 \u2212X \u2223\u2223\u2223 1 ] 6 1 Nd E [\u2223\u2223\u2223Z\u0303 \u2212 Z \u2223\u2223\u2223 1 ] + 1 Nd E [|X \u2212 Z|1] 6 \u03b5\u22c6 . (5.15)\nCombining equation (5.12) and (5.15) we complete the proof."}, {"heading": "6 Conclusions", "text": "We have defined a new framework for unsupervised learning which replaces generative assumptions by notions of reconstruction error and encoding length. This framework is comparative, and allows learning of particular hypothesis classes with respect to an unknown distribution by other hypothesis classes.\nWe demonstrate its usefulness by giving new polynomial time algorithms for two unsupervised hypothesis classes. First, we give new polynomial time algorithms for dictionary models in significantly broader range of parameters and assumptions. Another domain is the class of spectral encodings, for which we consider a new class of models that is shown to strictly encompass PCA and kernel-PCA. This new class is capable, in contrast to previous spectral models, learn algebraic manifolds. We give efficient learning algorithms for this class based on convex relaxations."}, {"heading": "Acknowledgements", "text": "We thank Sanjeev Arora for many illuminating discussions and crucial observations in earlier phases of this work, amongst them that a representation which preserves information for all classifiers requires lossless compression."}, {"heading": "A Proof of Theorem 2.1", "text": "Proof of Theorem 2.1. [MRT12, Theorem 3.1] asserts that with probability at least 1 \u2212 \u03b4, we have that for every hypothesis f \u2208 H,\nloss D (f) 6 loss S\n(f) + 2Rm(H) + \u221a log 1\u03b4 2m\nby negating the loss function this gives\n| loss D (f)\u2212 loss S (f)| 6 2Rm(H) + \u221a log 2\u03b4 2m\nand therefore, letting f\u2217 = argminf\u2208H lossD(f), we have\nloss D (f\u0302ERM ) 6 loss S\n(f\u0302ERM ) + 2Rm(H) + \u221a log 1\u03b4 2m\n(by [MRT12, Theorem 3.1])\n6 loss S\n(f\u2217) + 2Rm(H) + \u221a log 1\u03b4 2m\n( by definition of ERM)\n6 loss D\n(f\u2217) + 6Rm(H) + \u221a 4 log 1\u03b4 2m\n( using [MRT12, Theorem 3.1] again)\nA.1 Low reconstruction error is sufficient for supervised learning\nThis section observes that low reconstruction error is a sufficient condition for unsupervised learning to allow supervised learning over any future task.\nLemma A.1. Consider any supervised learning problem with respect to distribution D over X \u00d7 L that is agnostically PAC-learnable with respect to L-Lipschitz loss function \u2113 and with bias \u03b31.\nSuppose that unsupervised hypothesis class H is C -learnable with bias \u03b32 over distribution D and domain X , by hypothesis f : X 7\u2192 Y . Then the distribution D\u0303f over Y \u00d7 L, which gives the pair (f(x), y) the same measure as D gives to (x, y), is agnostically PAC-learnable with bias \u03b31 + L\u03b32.\nProof. Let h : X 7\u2192 Y be a hypothesis that PAC-learns distribution D. Consider the hypothesis\nh\u0303 : Y 7\u2192 L , h\u0303(y) = (h \u25e6 g)(y)\nThen by definition of reconstruction error and the Lipschitz property of \u2113 we have\nloss D\u0303f (h\u0303) = E (y,l)\u223cD\u0303f [\u2113(h\u0303(y), l)]\n= E (y,l)\u223cD\u0303f\n[\u2113((h \u25e6 g)(y), l)]\n= E (x,l)\u223cD\n[\u2113(h(x\u0303), l)] (D(x) = D\u0303f (y))\n= E (x,l)\u223cD [\u2113(h(x), l)] + E (x,l)\u223cD\n[\u2113(h(x\u0303), l)\u2212 \u2113(h(x), l)]\n= \u03b31 + E (x,l)\u223cD\n[\u2113(h(x\u0303), l)\u2212 \u2113(h(x), l)] ( PAC learnability)\n6 \u03b31 + L E x\u223cD\n\u2016x\u2212 x\u0303\u2016 ( Lipschitzness of \u2113 \u25e6 h)\n= \u03b31 + L E x\u223cD\n\u2016x\u2212 g \u25e6 f(x)\u2016\n6 \u03b31 + L\u03b32 ( C -learnability)"}, {"heading": "B Proof of Theorem 3.1", "text": "Proof of Theorem 3.1. We assume without loss of generality s = 1. For s > 1 the proof will be identical since one can assume x\u2297s is the data points (and the dimension is raised to ds).\nLet x1, . . . , xm be a set of examples \u223c Dm. It can be shown that any minimizer of ERM\nA\u2217 = argmin A\u2208Rd\u00d7k \u2016xi \u2212A\u2020Axi\u20162 (B.1)\nsatisfies that (A\u2217)\u2020A\u2217 is the the projection operator to the subspace of top k eigenvector of \u2211m\ni=1 xix \u22a4 i .\nTherefore ERM (B.1) is efficiently solvable. According to Theorem 2.1, the ERM hypothesis generalizes with rates governed by the Rademacher complexity of the hypothesis class. Thus, it remains to compute the Rademacher complexity of the hypothesis class for PCA. We assume for simplicity that all vectors in the domain have Euclidean norm bounded by one.\nRS(Hpcak ) = E \u03c3\u223c{\u00b11}m\n[ sup\n(h,g)\u2208Hpcak\n1\nm\n\u2211 i\u2208S \u03c3i\u2113((h, g), xi)\n]\n= E \u03c3\u223c{\u00b11}m\n[ sup\nA\u2208Rd\u00d7k\n1\nm\n\u2211 i\u2208S \u03c3i\u2016xi \u2212A\u2020Axi\u20162\n]\n= E \u03c3\u223c{\u00b11}m\n[ sup\nA\u2208Rd\u00d7k\n1\nm\n\u2211 i\u2208S \u03c3iTr((I \u2212A\u2020A)\n( m\u2211\ni=1\nxix \u22a4 i\n) (I \u2212A\u2020A)\u22a4) ]\n= E \u03c3\u223c{\u00b11}m\n[ sup\nA\u2208Rd\u00d7k Tr\n( (I \u2212A\u2020A) ( 1\nm\nm\u2211\ni=1\n\u03c3ixix \u22a4 i\n))] .\nThen we apply Holder inequality, and effectively disentangle the part about \u03c3 and A:\nE \u03c3\u223c{\u00b11}m\n[ sup\nA\u2208Rd\u00d7k Tr\n( (I \u2212A\u2020A) ( 1\nm\nm\u2211\ni=1\n\u03c3ixix \u22a4 i\n))]\n6 E \u03c3\u223c{\u00b11}m\n[ sup\nA\u2208Rd\u00d7k \u2016I \u2212A\u2020A\u2016F \u2225\u2225\u2225\u2225\u2225 1 m m\u2211\ni=1\n\u03c3ixix \u22a4 i \u2225\u2225\u2225\u2225\u2225 F ] (Holder inequality)\n6 \u221a d E \u03c3\u223c{\u00b11}m [\u2225\u2225\u2225\u2225\u2225 1 m m\u2211\ni=1\n\u03c3ixix \u22a4 i \u2225\u2225\u2225\u2225\u2225 F ] ( since A\u2020A is a projection, \u2016I \u2212A\u2020A\u2016 6 1.)\n6 \u221a d E \u03c3\u223c{\u00b11}m   \u2225\u2225\u2225\u2225\u2225 1 m m\u2211\ni=1\n\u03c3ixix \u22a4 i \u2225\u2225\u2225\u2225\u2225 2\nF\n  1/2\n(Cauchy-Schwarz inequality)\n6 \u221a d \u221a\u221a\u221a\u221a 1 m2 m\u2211\ni=1\n\u3008\u03c3ixix\u22a4i , \u03c3ixix\u22a4i \u3009 (since E[\u03c3i\u03c3j ] = 0 for i 6= j)\n6 \u221a d/m . (by \u2016x\u2016 6 1)\nThus, from Theorem 2.1 we can conclude that the class Hpcak is learnable with sample complexity O\u0303( d\n\u03b52 )6.\nB.1 Proof of Theorem 3.2\nTheorem 3.2 follows from the following lemmas and the generalization theorem 2.1 straightforwardly.\nLemma B.1. Suppose distribution D is (k, \u03b5)-regularly spectral decodable. Then for any \u03b4 > 0, solving convex optimization (3.3) with non-smooth Frank-Wolfe algorithm [HK12, Theorem 4.4] with k\u2032 = O(\u03c44k4/\u03b44) steps gives a solution R\u0302 of rank k\u2032 such that f(R\u0302) 6 \u03b4 + \u03b5.\nLemma B.2. The Rademacher complexity of the class of function \u03a6 ={\u2225\u2225Rx\u22972 \u2212 x\u22972 \u2225\u2225 op : R s.t. \u2016R\u2016S1 6 \u03c4k } with m examples is bounded from above by at most Rm(\u03a6) 6 2\u03c4k \u00b7 \u221a 1/m\nHere Lemma B.2 follows from the fact that \u2225\u2225Rx\u22972 \u2212 x\u22972 \u2225\u2225 op is bounded above by 2\u03c4k when \u2016x\u2016 6 1 and \u2016R\u2016S1 6 \u03c4k. The rest of the section focuses on the proof of Lemma B.1. Lemma B.1 basically follows from the fact that f is Lipschitz and guarantees of the Frank-Wolfe algorithm.\nProposition B.3. The objective function f(R) is convex and 1-Lipschitz. Concretely, Let \u2113x(R) = \u2016Rx\u22972 \u2212 x\u22972\u2016op. Then\n\u2202\u2113x \u220b (u\u2297 v)(x\u22972)\u22a4\nwhere \u2202\u2113x is the set of sub-gradients of \u2113x with respect to R, and u, v \u2208 Rd are (one pair of) top left and right singular vectors of M(Rx\u22972 \u2212 x\u22972).\nProof. This simply follows from calculating gradient with chain rule. Here we use the fact that A \u2208 (Rd)\u22972, the sub-gradient of \u2016A\u2016op contains the vector a\u2297 b where a, b are the top left and right singular vectors of M(A). We can also verify by definition that (u\u2297 v)(x\u22972)\u22a4 is a sub-gradient.\nf(R\u2032)\u2212 f(R) > (u\u2297 v)\u22a4(R\u2032x\u22972 \u2212Rx\u22972) (by convexity of \u2016 \u00b7 \u2016op) = \u3008(u\u2297 v)(x\u22972)\u22a4, R\u2032 \u2212R\u3009 .\nNow we are ready to prove Lemma B.1.\nProof of Lemma B.1. Since D is (k, \u03b5)-regularly decodable, we know that there exists a rank-k solution R\u2217 with f(R\u2217) 6 \u03b5. Since \u2016R\u2217\u2016op 6 \u03c4 , we have that \u2016R\u2016S1 6 rank(R\u2217) \u00b7 \u2016R\u2016op 6 \u03c4k. Therefore R\u2217 is feasible solution for the objective (3.3) with f(R\u2217) 6 \u03b5.\nBy Proposition B.3, we have that f(R) is 1-Lipschitz. Moreover, for any R,S with \u2016R\u2016S1 6 \u03c4k, \u2016S\u2016S1 6 \u03c4k we have that \u2016R \u2212 S\u2016F 6 \u2016R\u2016F + \u2016S\u2016F 6 \u2016R\u2016S1 + \u2016S\u2016S1 6 2\u03c4k. Therefore the diameter of the constraint set is at most \u03c4k.\nBy [HK12, Theorem 4.4], we have that Frank-Wolfe algorithm returns solution R with f(R) \u2212 f(R\u2217) 6 \u03b5+ \u03b4 in ( \u03c4k \u03b4 )4 iteration.\n6For \u2113 > 1 the sample complexity is O\u0303(d\u2113/\u03b52)."}, {"heading": "C Shorter codes with relaxed objective for Polynomial Spectral Components Analysis", "text": "Notations. For a matrix A, let \u03c31(A) > \u03c32(A) > .. be its singular values. Then the Schatten p-norm, denoted by \u2016 \u00b7 \u2016Sp , for p > 1 is defined as \u2016A\u2016Sp = ( \u2211 i \u03c3i(A)\np)1/p. For even integer p, an equivalent and simple definition is that \u2016A\u2016pSp , Tr((A\n\u22a4A)p/2). In this section we consider the following further relaxation of objective (3.3).\nminimize f4(R) := E [\u2225\u2225Rx\u22972 \u2212 x\u22972 \u2225\u22252 Sp ] (C.1)\ns.t. \u2016R\u2016S1 6 \u03c4k\nSince \u2016A\u2016F > \u2016A\u2016S4 > \u2016A\u2016S\u221e = \u2016A\u2016, this is a relaxation of the objective (3.3), and it interpolates between kernal PCA and spectral decoding. Our assumption is weaker than kernal PCA but stronger than spectral decodable.\nDefinition C.1 (Extension of definition 3.2). We say a data distribution D is (k, \u03b5)-regularly spectral decodable with \u2016 \u00b7 \u2016Sp norm if the error E in equation (5.10) is bounded by \u2016E\u2016Sp 6 \u03b5.\nWe can reduce the length of the code from O(k4) to O(k2) for any constant p.\nTheorem C.1. Suppose data distribution is (k, \u03b5)-spectral decodable with norm \u2016 \u00b7 \u2016Sp for p = O(1), then solving (C.1) using (usual) Frank-Wolfe algorithm gives a solution R\u0302 of k\u2032 = O(k2\u03c42/\u03b52) with f(R) 6 \u03b5 + \u03b4. As a direct consequence, we obtain encoding and decoding pair (gA, hB) \u2208 Hsak\u2032 with k\u2032 = O(k2\u03c42/\u03b52) and reconstruction error \u03b5+ \u03b4.\nThe main advantage of using relaxed objective is its smoothness. This allows us to optimize over the Schatten 1-norm constraint set much faster using usual Frank-Wolfe algorithm. Therefore the key here is to establish the smoothness of the objective function. Theorem C.1 follows from the following proposition straightforwardly.\nProposition C.2. Objective function fp (equation (C.1)) is convex and O(p)-smooth.\nProof. Since \u2016\u00b7\u2016Sp is convex and composition of convex function with linear function gives convex function. Therefore,\n\u2225\u2225Rx\u22972 \u2212 x\u22972 \u2225\u2225 Sp\nis a convex function. The square of an non-negative convex function is also convex, and therefore we proved that fp is convex. We prove the smoothness by first showing that \u2016A\u20162Sp is a smooth function with respect to A. We use the definition \u2016A\u2016 p Sp\n= Tr((A\u22a4A)p/2). Let E be a small matrix that goes to 0, we have\n\u2016A+ E\u2016pSp = Tr((A \u22a4A)p/2) + T1 + T2 + o(\u2016E\u20162F ) (C.2)\nwhere T1, T2 denote the first order term and second order term respectively. Let U = A\u22a4E + E\u22a4A and V = A\u22a4A. We note that T2 is a sum of the traces of matrices like UV UV Up/2\u22122. By Lieb-Thirring inequality, we have that all these term can be bounded by Tr(Up/2\u22122V 2) = 2Tr((A\u22a4A)p/2\u22122A\u22a4EE\u22a4A) + 2Tr((A\u22a4A)p/2\u22122A\u22a4EA\u22a4E\u22a4). For the first term, we have that\nTr((A\u22a4A)p/2\u22122A\u22a4EE\u22a4A) 6 \u2016(AA\u22a4)(p\u22122)/4E\u20162 6 \u2016(AA\u22a4)(p\u22122)/4\u20162S\u221e\u2016E\u20162F = \u2016A\u2016 p\u22122 S\u221e \u2016E\u20162F\nwhere in the first inequality we use Cauchy-Schwarz. Then for the second term we have\nTr((A\u22a4A)p/2\u22122A\u22a4EA\u22a4E\u22a4) 6 \u2016(A\u22a4A)(p\u22122)/4E\u2016F \u2016AE(A\u22a4A)(p\u22124)/4\u2016F 6 \u2016(A\u22a4A)(p\u22122)/4E\u20162F (by Lieb-Thirring inequality) 6 \u2016(AA\u22a4)(p\u22122)/4\u20162\u2016E\u20162F = \u2016A\u2016p\u22122S\u221e \u2016E\u2016 2 F (C.3)\nTherefore, finally we got\nT2 6 O(p 2) \u00b7 \u2016A\u2016p\u22122Sp\u22122\u2016E\u2016 2 F (C.4)\nTherefore, we complete the proof by having,\n\u2016A+ E\u20162Sp 6 (\u2016A\u2016 p Sp + T1 + T2 + o(\u2016E\u20162))2/p 6 \u2016A\u2016Sp(1 + T \u20321 + 2\np\u2016A\u2016p/2Sp T2) + o(\u2016E\u20162)\n(by (1 + x)p/2 6 1 + 2x/p+ o(\u2016x\u20162)) 6 \u2016A\u20162Sp + T \u2032\u20321 +O(p)\u2016E\u20162 + o(\u2016E\u20162) (by equation (C.3))"}, {"heading": "D Toolbox", "text": "Lemma D.1. Let p > 2 be a power of 2 and u = [u1, . . . , un] and v = [v1, . . . , vn] be indeterminants. Then there exists SoS proof,\n\u22a2\n  \u2211\nj\nuiv p\u22121 j\n  p\n6\n( \u2211\ni\nupi )(\u2211 vpi )p\u22121 (D.1)\nProof Sketch. The inequality follows from repeated application of Cauchy-Schwarz. For example, for p = 4 we have\n\u22a2 ( \u2211\ni\nu4i )(\u2211 v4i )3 > ( \u2211\ni\nu2i v 2 i )2 (\u2211 v4i )2 (by Cauchy-Schwarz)\n>\n( \u2211\ni\nuiv 3 i\n)4 (by Cauchy-Schwarz again)\nFor p = 2s with s > 2, the statement can be proved inductively."}], "references": [{"title": "Tight bounds for universal compression of large alphabets", "author": ["Jayadev Acharya", "Hirakendu Das", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh"], "venue": "In Proceedings of the 2013 IEEE International Symposium on Information", "citeRegEx": "Acharya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2013}, {"title": "K-svd: Design of dictionaries for sparse representation", "author": ["Michal Aharon", "Michael Elad", "Alfred Bruckstein"], "venue": "In IN: PROCEEDINGS OF SPARS05,", "citeRegEx": "Aharon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2005}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Learning topic models\u2013going beyond svd", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "arXiv preprint arXiv:1308.6273,", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Simple, efficient, and neural algorithms for sparse coding", "author": ["Sanjeev Arora", "Rong Ge", "Tengyu Ma", "Ankur Moitra"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "Arora et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Measures of clustering quality: A working set of axioms for clustering", "author": ["Shai Ben-David", "Margareta Ackerman"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Ben.David and Ackerman.,? \\Q2009\\E", "shortCiteRegEx": "Ben.David and Ackerman.", "year": 2009}, {"title": "Rate distortion theory: A mathematical basis for data compression", "author": ["Toby Berger"], "venue": null, "citeRegEx": "Berger.,? \\Q1971\\E", "shortCiteRegEx": "Berger.", "year": 1971}, {"title": "Rounding sum-of-squares relaxations", "author": ["Boaz Barak", "Jonathan A. Kelner", "David Steurer"], "venue": "In STOC,", "citeRegEx": "Barak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2014}, {"title": "Dictionary learning and tensor decomposition via the sum-of-squares method", "author": ["Boaz Barak", "Jonathan A. Kelner", "David Steurer"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "Barak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2015}, {"title": "Dictionary learning and tensor decomposition via the sum-of-squares method", "author": ["Boaz Barak", "Jonathan A. Kelner", "David Steurer"], "venue": "In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Barak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2015}, {"title": "Tensor prediction, rademacher complexity and random 3-xor", "author": ["Boaz Barak", "Ankur Moitra"], "venue": "CoRR, abs/1501.06521,", "citeRegEx": "Barak and Moitra.,? \\Q2015\\E", "shortCiteRegEx": "Barak and Moitra.", "year": 2015}, {"title": "Noisy tensor completion via the sum-of-squares hierarchy", "author": ["Boaz Barak", "Ankur Moitra"], "venue": "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,", "citeRegEx": "Barak and Moitra.,? \\Q2016\\E", "shortCiteRegEx": "Barak and Moitra.", "year": 2016}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Sum-of-squares proofs and the quest toward optimal algorithms", "author": ["Boaz Barak", "David Steurer"], "venue": "In Proceedings of International Congress of Mathematicians (ICM),", "citeRegEx": "Barak and Steurer.,? \\Q2014\\E", "shortCiteRegEx": "Barak and Steurer.", "year": 2014}, {"title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)", "author": ["Thomas M. Cover", "Joy A. Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2006\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2006}, {"title": "Uncertainty principles and ideal atomic decomposition", "author": ["D.L. Donoho", "X. Huo"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "Donoho and Huo.,? \\Q2006\\E", "shortCiteRegEx": "Donoho and Huo.", "year": 2006}, {"title": "Decomposing overcomplete 3rd order tensors using sum-ofsquares algorithms. In Approximation, Randomization, and Combinatorial Optimization", "author": ["Rong Ge", "Tengyu Ma"], "venue": "Algorithms and Techniques,", "citeRegEx": "Ge and Ma.,? \\Q2015\\E", "shortCiteRegEx": "Ge and Ma.", "year": 2015}, {"title": "Projection-free online learning", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Hazan and Kale.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2012}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu and Kakade.,? \\Q2013\\E", "shortCiteRegEx": "Hsu and Kakade.", "year": 2013}, {"title": "Tensor principal component analysis via sum-of-square proofs", "author": ["Samuel B. Hopkins", "Jonathan Shi", "David Steurer"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "Hopkins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2015}, {"title": "Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors", "author": ["Samuel B. Hopkins", "Tselil Schramm", "Jonathan Shi", "David Steurer"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "Hopkins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2016}, {"title": "A lower bound on compression of unknown alphabets", "author": ["Nikola Jevtic", "Alon Orlitsky", "Narayana P. Santhanam"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Jevtic et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Jevtic et al\\.", "year": 2005}, {"title": "An impossibility theorem for clustering", "author": ["Jon M. Kleinberg"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kleinberg.,? \\Q2003\\E", "shortCiteRegEx": "Kleinberg.", "year": 2003}, {"title": "Global optimization with polynomials and the problem of moments", "author": ["Jean B. Lasserre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Lasserre.,? \\Q2001\\E", "shortCiteRegEx": "Lasserre.", "year": 2001}, {"title": "An introduction to polynomial and semi-algebraic optimization. Cambridge Texts in Applied Mathematics", "author": ["Jean Bernard Lasserre"], "venue": null, "citeRegEx": "Lasserre.,? \\Q2015\\E", "shortCiteRegEx": "Lasserre.", "year": 2015}, {"title": "Sums of squares, moment matrices and optimization over polynomials", "author": ["Monique Laurent"], "venue": "Emerging Applications of Algebraic Geometry,", "citeRegEx": "Laurent.,? \\Q2009\\E", "shortCiteRegEx": "Laurent.", "year": 2009}, {"title": "Vanishing component analysis", "author": ["Roi Livni", "David Lehavi", "Sagi Schein", "Hila Nachlieli", "Shai Shalev-Shwartz", "Amir Globerson"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Livni et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2013}, {"title": "Foundations of machine learning", "author": ["Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": "MIT press,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Polynomial-time tensor decompositions with sum-of-squares", "author": ["Tengyu Ma", "Jonathan Shi", "David Steurer"], "venue": "In FOCS 2016,", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Sum-of-squares lower bounds for sparse PCA", "author": ["Tengyu Ma", "Avi Wigderson"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Ma and Wigderson.,? \\Q2015\\E", "shortCiteRegEx": "Ma and Wigderson.", "year": 2015}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM J. Comput.,", "citeRegEx": "Natarajan.,? \\Q1995\\E", "shortCiteRegEx": "Natarajan.", "year": 1995}, {"title": "Universal compression of memoryless sources over unknown alphabets", "author": ["Alon Orlitsky", "Narayana P. Santhanam", "Junan Zhang"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Orlitsky et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Orlitsky et al\\.", "year": 2004}, {"title": "Structured Semidefinite Programs and Semialgebraic Geometry Methods in Robustness and Optimization", "author": ["Pablo A. Parrilo"], "venue": "PhD thesis, California Institute of Technology,", "citeRegEx": "Parrilo.,? \\Q2000\\E", "shortCiteRegEx": "Parrilo.", "year": 2000}, {"title": "Compressive feature learning", "author": ["Hristo S Paskov", "Robert West", "John C Mitchell", "Trevor Hastie"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Paskov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paskov et al\\.", "year": 2013}, {"title": "Modeling by shortest data", "author": ["Jorma Rissanen"], "venue": "description. Automatica,", "citeRegEx": "Rissanen.,? \\Q1978\\E", "shortCiteRegEx": "Rissanen.", "year": 1978}, {"title": "Learning Deep Generative Models", "author": ["Ruslan Salakhutdinov"], "venue": "PhD thesis,", "citeRegEx": "Salakhutdinov.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov.", "year": 2009}, {"title": "Rank, trace-norm and max-norm", "author": ["Nathan Srebro", "Adi Shraibman"], "venue": "In Learning Theory, 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro,", "citeRegEx": "Srebro and Shraibman.,? \\Q2005\\E", "shortCiteRegEx": "Srebro and Shraibman.", "year": 2005}, {"title": "Learning and Generalization with the Information Bottleneck, pages 92\u2013107", "author": ["Ohad Shamir", "Sivan Sabato", "Naftali Tishby"], "venue": null, "citeRegEx": "Shamir et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2008}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Banach-Mazur distances and finite-dimensional operator ideals. Pitman monographs and surveys in pure and applied mathematics", "author": ["N. Tomczak-Jaegermann"], "venue": "Longman Scientific & Technical,", "citeRegEx": "Tomczak.Jaegermann.,? \\Q1989\\E", "shortCiteRegEx": "Tomczak.Jaegermann.", "year": 1989}, {"title": "The information bottleneck method", "author": ["Naftali Tishby", "Fernando C.N. Pereira", "William Bialek"], "venue": "CoRR, physics/0004057,", "citeRegEx": "Tishby et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 2000}, {"title": "A theory of the learnable", "author": ["Leslie G Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}, {"title": "Singular vectors under random perturbation", "author": ["Van Vu"], "venue": "Random Structures and Algorithms,", "citeRegEx": "Vu.,? \\Q2011\\E", "shortCiteRegEx": "Vu.", "year": 2011}], "referenceMentions": [], "year": 2016, "abstractText": "We give a novel formal theoretical framework for unsupervised learning with two distinctive characteristics. First, it does not assume any generative model and based on a worst-case performance metric. Second, it is comparative, namely performance is measured with respect to a given hypothesis class. This allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization.", "creator": "LaTeX with hyperref package"}}}