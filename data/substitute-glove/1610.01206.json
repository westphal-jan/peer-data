{"id": "1610.01206", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Multi-View Representation Learning: A Survey from Shallow Methods to Deep Methods", "abstract": "Recently, dual - direction integral psychology most it no reducing stronger very same machine learning once product manufacturing areas. This paper early reviews an root useful through scholars on expanding - view present perspective, especially recently scriptural correlation mathematical (CCA) and today several proprietary. And he exactly suspect later strengthening similar multi - idea representation academic both geographical on shallow methods including sharing - modal topic practical, multi - change grassy coding, and newest - approach pervasive sphere Markov networks, may deep methods including multi - concatenation regulated Boltzmann used, largest - modal autoencoders, but multi - modal heartbeats computation networks. Further, we the must that instance perspective from matrix bounded given multi - view therefore learning. Overall, a overall pledged them provide action insightful guidebook but calculations basis and current developments until long field large multi - today discrete technical where the wants tested 'll been most appropriate tools offered particular desktop.", "histories": [["v1", "Mon, 3 Oct 2016 17:14:15 GMT  (1036kb,D)", "http://arxiv.org/abs/1610.01206v1", "27 pages, 10 figures. arXiv admin note: text overlap witharXiv:1206.5538,arXiv:1304.5634by other authors"], ["v2", "Sun, 27 Nov 2016 03:11:53 GMT  (1032kb,D)", "http://arxiv.org/abs/1610.01206v2", "20pages, 9 figures"], ["v3", "Thu, 24 Aug 2017 08:08:22 GMT  (812kb,D)", "http://arxiv.org/abs/1610.01206v3", "20pages, 9 figures. arXiv admin note: text overlap witharXiv:1206.5538by other authors"], ["v4", "Fri, 1 Sep 2017 05:52:06 GMT  (812kb,D)", "http://arxiv.org/abs/1610.01206v4", "20pages, 9 figures"]], "COMMENTS": "27 pages, 10 figures. arXiv admin note: text overlap witharXiv:1206.5538,arXiv:1304.5634by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.IR", "authors": ["yingming li", "ming yang", "zhongfei zhang"], "accepted": false, "id": "1610.01206"}, "pdf": {"name": "1610.01206.pdf", "metadata": {"source": "CRF", "title": "Multi-View Representation Learning: A Survey from Shallow Methods to Deep Methods", "authors": ["Yingming Li", "Ming Yang", "Zhongfei (Mark) Zhang"], "emails": ["zhongfei}@zju.edu.cn"], "sections": [{"heading": null, "text": "Multi-View Representation Learning: A Survey from Shallow Methods to Deep Methods\nYingming Li, Ming Yang, Zhongfei (Mark) Zhang, Senior Member, IEEE\nAbstract\u2014Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper first reviews the root methods and theories on multi-view representation learning, especially on canonical correlation analysis (CCA) and its several extensions. And then we investigate the advancement of multi-view representation learning that ranges from shallow methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to deep methods including multi-modal restricted Boltzmann machines, multi-modal autoencoders, and multi-modal recurrent neural networks. Further, we also provide an important perspective from manifold alignment for multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical basis and current developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.\nIndex Terms\u2014Multi-view representation learning, canonical correlation analysis, multi-modal deep learning.\nF"}, {"heading": "1 INTRODUCTION", "text": "Multi-view data have become increasingly available in real-world applications where examples are described by different feature sets or different \u201cviews\u201d, such as image+text, audio+video, and webpage+click-through data. The different views usually contain complementary information, and multi-view learning can exploit this information to learn representation that is more expressive than that of single-view learning method. Therefore, multi-view representation learning has become a very promising topic with wide applicability.\nCanonical correlation analysis (CCA) [1] and its kernel extensions [2\u20134] are representative techniques in early studies of multi-view representation learning. A variety of theories and approaches are later introduced to investigate their theoretical properties, explain their success, and extend them to improve the generalization performance in particular tasks. While CCA and its kernel versions show their abilities of effectively modeling the relationship between two or more sets of variables, they have limitations on capturing high level associations between multiview data. Inspired by the success of deep neural networks [5\u2013 7], deep CCAs [8] have been proposed to this problem, with a common strategy to learn a joint representation that is coupled between multiple views at higher level after learning several layers of view-specific features in the lower layers.\nHowever, how to learn a good association between multi-view data still remains an open problem. In 2016, a workshop on multiview representation learning is held in conjunction with the 33rd international conference on machine learning to help promote a better understanding of various approaches and the challenges in specific applications. So far, there have been increasing research activities in this direction and a large number of multi-view representation learning algorithms have been presented based on\n\u2022 Y. Li, M. Yang, Z. Zhang are with College of Information Science & Electronic Engineering, Zhejiang University, China. E-mail: {yingming, cauchym, zhongfei}@zju.edu.cn\nthe fundamental theories of CCAs and progress of deep neural networks. For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323]. Further, manifold alignment also provides an important perspective for multi-view representation learning [24].\nTherefore, we review the literature of multi-view representation learning from shallow methods to deep methods in accordance with its progress. In particular, each part is surveyed by following two broad, parallel lines of this learning mechanism: one rooted in probabilistic graphical models and the other rooted in non-linear embedding models including the kernel trick and neural networks. In fact, the fundamental difference between the two paradigms is whether the layered architecture of a learning model is to be interpreted as a probabilistic graphical model or as a computation network. The connection between these two paradigms becomes more obvious when we consider deeper multi-view representation learning models. The computational networks have become increasingly important for big data learning since the exact inference of probabilistic models usually becomes intractable with deep structures.\nThe goal of this survey is to review the theoretical basis and key advances in the area of multi-view representation learning and to provide a global picture of this active direction. We expect this survey to help researchers find the most appropriate approaches for their particular applications and deliver perspectives of what can be done in the future to promote the development of multi-view representation learning.\nThe remainder of this paper is organized as follows. In Section 2, we introduce theories and advances on CCA and its extensions including probabilistic CCA, sparse CCA, kernel CCA and deep CCA. Section 3 surveys representative shallow multi-view representation learning approaches according to the modeling mechanisms involved. Then, in Section 4, we investigate the popular deep multi-view representation learning methods which have\nar X\niv :1\n61 0.\n01 20\n6v 1\n[ cs\n.L G\n] 3\nO ct\n2 01\n6\nattracted much attention and been widely applied in cross-media retrieval. Further, Section 5 surveys manifold alignment which provides an important perspective for multi-view representation learning. Finally, we provide a conclusion in Section 6."}, {"heading": "2 CANONICAL CORRELATION ANALYSIS AND ITS EXTENSIONS", "text": "In this sections we will review the root of multi-view representation learning techniques, canonical correlation analysis (CCA) and its extensions that range from probabilistic graphical modeling to non-linear deep embedding. In particular, we will investigate the related work of probabilistic CCA, sparse CCA, kernel CCA and deep CCA to illustrate the probabilistic, sparse, and non-linear views of multi-view representation learning theories."}, {"heading": "2.1 Canonical Correlation Analysis", "text": "Canonical Correlation Analysis [1] has become increasingly popular for its capability of effectively modeling the relationship between two or more sets of variables. From the perspective of multi-view representation learning, CCA computes a shared embedding of both sets of variables through maximizing the correlations among the variables between the two sets. More specifically, CCA has been widely used in multi-view learning tasks to generate low dimensional feature representations [25\u2013 27]. Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc. For example, Figure 1 shows a fascinating cross-modality application of CCA in multi-media retrieval.\nGiven a pair of datasets X = [x1, . . . , xn] and Y = [y1, . . . , yn], CCA tends to find linear projections wx and wy , which make the corresponding examples in the two datasets maximally correlated in the projected space. The correlation coefficient between the two datasets in the projected space is given by\n\u03c1 = corr ( w>xX,w > y Y ) = w>x Cxywy\u221a\n(w>x Cxxwx) ( w>y Cyywy ) (1)\nwhere the covariance matrix Cxy is defined as\nCxy = 1\nn n\u2211 i=1 (xi \u2212 \u00b5x) (yi \u2212 \u00b5y)> (2)\nwhere \u00b5x = 1n \u2211n i=1 xi and \u00b5y = 1 n \u2211n i=1 yi are the means of the two views, respectively. The definition of Cxx and Cyy can be obtained similarly.\nSince the correlation \u03c1 is invariant to the scaling of wx and wy , CCA can be posed equivalently as a constrained optimization problem.\nmax wx,wy\nwTxCxywy\ns.t. wTxCxxwx = 1, w T y Cyywy = 1 (3)\nBy formulating the Lagrangian dual of Eq.(3), it can be shown that the solution to Eq.(3) is equivalent to solving a pair of generalized eigenvalue problems [3],\nCxyC \u22121 yy Cyxwx = \u03bb 2Cxxwx CyxC \u22121 xx Cxywy = \u03bb 2Cyywy (4)\nBesides the above definition of CCA, there are also other different ways to define the canonical correlations of a pair of matrices, and all these ways are shown to be equivalent [42]. In particular, Kettenring [43] show that CCA is equivalent to a constrained least-square optimization problem. Further, Golub and Zha [42] also provide a classical algorithm for computing CCA by first QR decomposition of the data matrices which whitens the data and then an SVD of the whitened covariance matrix. However, with typically huge data matrices this procedure becomes extremely slow. Avron et al. [30, 44] propose a fast algorithm for CCA with a pair of tall-and-thin matrices using subsampled randomized Walsh-Hadamard transform [45], which only subsamples a small proportion of the training data points to approximate the matrix product. Further, Lu and Foster [46] consider sparse design matrices and introduce an efficient iterative regression algorithm for large scale CCA.\nAnother line of research for large scale CCA considers stochastic optimization algorithm for CCA [47]. Ma et al. [48] introduce an augmented approximate gradient scheme and further extend it to a stochastic optimization regime. Recent work [49, 50] attempts to transform the original problem of CCA into sequences of least squares problems and solve these problems with accelerated gradient descent (AGD).\nSince local correlations usually exist for specific applications besides the global correlation, e.g. in earth science applications, Fern et al. [31] propose to use a mixture of local linear canonical correlation models to find correlation clusters from the data. The proposed method uses a two-step procedure: the compound data set is first partitioned into clusters such that each cluster contains instances whose x features and y features are linearly correlated; then CCA is independently applied to each cluster to form a mixture of correlations that are locally linear. This procedure is named as the correlation clustering to differentiate it from the traditional clustering. Further, Sun et al. [27] formulate CCA as a least squares problem in multi-label classification.\nWhile CCA has the capability of conducting multi-view feature learning and has been widely applied in different fields, it still has some limitations in different applications. For example, it ignores the nonlinearities of multi-view data. Consequently, many algorithms based on CCA have been proposed to extend the original CCA in real-world applications. In the following sections, we review its several widely-used extensions including probabilistic CCA, sparse CCA, kernel CCA and Deep CCA."}, {"heading": "2.2 Probabilistic CCA", "text": "CCA can be interpreted as the maximum likelihood solution to a probabilistic latent variable model for two Gaussian random vectors. This formulation of CCA as a probabilistic model is proposed by Bach and Jordan [51].\nProbabilistic CCA can be formulated by first introducing an explicit latent variable z corresponding to the principal-component subspace. And then a Gaussian prior distribution p(z) over the latent variable, together with two Gaussian conditional distributions p(x|z) and p(y|z) for the observed variables x and y conditioned on the value of the latent variable. In particular, the prior distribution over z is given by a zero-mean unit-covariance Gaussian\np(z) = N (z|0, IM ) (5)\nSimilarly, the conditional distributions of the observed variable x and y, conditioned on the value of the latent variable z, is also Gaussian and given as follows,\np(x|z) = N (x|Wxz + \u00b5x,\u03a8x) p(y|z) = N (y|Wyz + \u00b5y,\u03a8y) (6)\nwhere the mean of x and y is in general linear functions of z governed by the dx\u00d7dz matrixWx and the dx-dimensional vector \u00b5x, and by the dy \u00d7 dz matrix Wy and the dy-dimensional vector \u00b5y , respectively. The crucial idea lies in that the latent variables z is shared by the two data sets, while other variables or parameters are independent.\nConsequently, Bach and Jordan [51] provide a detailed connection between CCA and probabilistic CCA based on the result of the maximum likelihood estimation of Eq.(6) that Wx = \u03a3xxUxP\n1/2R and Wy = \u03a3yyUyP 1/2R, where U is composed of the canonical directions, P is a diagonal matrix containing\nthe canonical correlations, and R is an arbitrary rotation matrix. Further, the posterior expectations E(z|x) and E(z|y) lie in the same subspace that the traditional CCA searches since the obtained canonical weights are the same as the ones for CCA. EM algorithm is applied to find the maximum likelihood solution and is shown to converge to the global optimum. In addition, Browne [52] also proves that the maximum likelihood solution of Eq.(6) is identical to the solution of classical CCA.\nHowever, since probabilistic CCA is based on a Gaussian density model, outliers may lead to serious biases in the parameter estimates when using a maximum likelihood estimation method. To handle the influence of outliers, Archambeau et al. [53] introduce robust canonical correlation analysis by replacing Gaussian distributions with Student-t distributions, which constructs mixtures of robust CCA and can deal with missing data quite easily. Fyfe and Leen [54] consider a Dirichlet process method for performing a non-Bayesian probabilistic mixture CCA.\nBy treating the mixture case in length, Klami and Kaski [55] present a full Bayesian treatment of probabilistic canonical analyzer. Further, Wang [56] applies a hierarchical Bayesian model to probabilistic CCA and learn this model by variational approximation. By performing the Bayesian model algorithm for probabilistic CCA, the number of canonical correlations can be determined automatically and one would not encounter the issue of selecting the number of canonical correlations, which has been a well known downside for using CCA. Both [55] and [56] exploit the inverse Wishart matrix distribution as a prior for the covariance matrices \u03a8x(y) in Eq.(6) and use the automatic relevance determination prior for the linear transformation matrices Wx(y).\nLater Viinikanoja et al. [57] introduce a mixture of robust canonical correlation analyzers and provide a variational Bayesian inference for learning from noisy data. Such kinds of mixture models can be considered as locally linear models that find several correlation clusters and fit a separate CCA model for each cluster. The clustering process is incorporated into the solution procedure and is also closely related to the CCA models themselves.\nBased on exponential family extensions of principal component analysis [58, 59], Klami et al. [60] extend Bayesian CCA to the exponential family by generalizing the Gaussian noise assumption to noise with any distribution in the exponential family. Using the natural parameter formulation of exponential family distributions, certain common choices can be incorporated as special cases, which leading to an efficient way of exploiting the robustness of exponential family distribution in practical models [61, 62]. Recently, Mukuta and Harada [63] follow Wang\u2019s approach [56] and introduce a Bayesian extension of partial CCA [64]. Kamada et al. [65] propose a probabilistic semi-CCA by incorporating the mechanism of data missing.\nDespite the apparent superiority of Bayesian CCA for model selection, it is worth pointing out that the inference of the Bayesian CCA is difficult for high-dimensional data. Most of the practical applications of Bayesian CCA in the earlier efforts focus on relatively low-dimensional data [55, 56]. To make Bayesian CCA feasible for high-dimensional data, Huopaniemi et al. [66, 67] propose to use multi-way analysis setup by exploiting dimensionality reduction techniques."}, {"heading": "2.3 Sparse CCA", "text": "Recent years have witnessed a growing interest in learning sparse representations of data. Correspondingly, the problem of sparse\nCCA also has received much attention in the multi-view representation learning. The quest for sparsity can be motivated from several aspects. First is the ability to account for the predicted results. The big picture usually relies on a small number of crucial variables, with details to be allowed for variation. The second motivation for sparsity is regularization and stability. Reasonable regularization plays an important role in eliminating the influence of noisy data and reducing the sensitivity of CCA to a small number of observations. Further, sparse CCA can be formulated as a subset selection scheme which reduces the dimensionality of the vectors and makes possible a stable solution.\nThe problem of sparse CCA can be considered as finding a pair of linear combinations of wx and wy with prescribed cardinality which maximize the correlation. In particular, sparse CCA can be defined as the solution to\n\u03c1 = max wx,wy w>x Cxywy\u221a wxCxxwxwyCywy\ns.t. ||wx||0 \u2264 sx, ||wy||0 \u2264 sy. (7)\nMost of the approaches to sparse CCA are based on the well known LASSO trick [68] which is a shrinkage and selection method for linear regression. By formulating CCA as two constrained simultaneous regression problems, Hardoon and ShaweTaylor [69] propose to approximate the non-convex constraints with \u221e-norm. This is achieved by fixing each index of the optimized vector to 1 in turn and constraining the 1-norm of the remaining coefficients. Similarly, Waaijenborg et al. [70] propose to use the elastic net type regression.\nIn addition, Sun et al. [27] introduce a sparse CCA by formulating CCA as a least squares problem in multi-label classification and directly computing it with the Least Angle Regression algorithm (LARS) [71]. Further, this least squares formulation facilitates the incorporation of the unlabeled data into the CCA framework to capture the local geometry of the data. For example, graph laplacian [72] can be used in this framework to tackle with the unlabeled data.\nIn fact, the development of sparse CCA is intimately related to the advance of sparse PCA [73\u201375]. The classical solutions to generalized eigenvalue problem with sparse PCA can be easily extended to that of sparse CCA [76, 77]. Torres et al. [76] derive a sparse CCA algorithm by extending an approach for solving sparse eigenvalue problems using D.C. programming. Based on the sparse PCA algorithm in [74], Wiesel et al. [77] propose a backward greedy approach to sparse CCA by bounding the correlation at each stage. Witten et al. [78] propose to apply a penalized matrix decomposition to the covariance matrix Cxy , which results in a method for penalized sparse CCA. Consequently, structured sparse CCA has been proposed by extending the penalized CCA with structured sparsity inducing penalty [79].\nAnother line of research for sparse CCA is based on sparse Bayesian learning [80\u201382]. In particular, they are built on the probabilistic interpretation of CCA outlined by [51]. Fyfe and Leen [80] investigate two methods for sparsifying probabilistic CCA. Fujiwara et al. [83] present a variant of sparse Bayesian CCA, using an element-wise automatic relevance determination (ARD) prior configuration, where non-effective parameters are automatically driven to zero. Rai and Daume\u0301 III [82] propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components and capture the sparsity underlying the projections. This framework exploits the\nIndian Buffet Process [84] to discover latent feature representation of a set of observations and to control the overall complexity of the model.\nRecently, some authors have used probabilistic inter-battery factor analysis (IBFA) [52, 85], which can be considered as an extended CCA model which complements CCA by providing a certain variation not captured by the correlation components, to build more complex hierarchical sparse Bayesian CCA models. By extending IBFA with group-wise sparsity, Klami et al. [86] introduce a Bayesian treatment of the IBFA model, which describes not only the correlations between data sets but also provides components explaining the linear structure within each of the data sets.\nBuilt on multi-battery factor analysis (MBFA) by McDonald [87] and Browne [88] as a generalization of IBFA, Klami et al. [89] introduce a problem formulation of group factor analysis, which extends CCA to more than two sets with structural sparsity, resulting in that it is more flexible than the previous extensions [86]. The solution to this framework can be formulated as variational inference of a latent variable model with a structural sparsity prior."}, {"heading": "2.4 Kernel CCA", "text": "Canonical Correlation Analysis is a linear multi-view representation learning algorithm, but for many scenarios of real-world multi-view data revealing nonlinearities, it is impossible for a linear embedding to capture all the properties of the multi-view data [90]. Since kernerlization is a principled trick for introducing non-linearity into linear methods, kernel CCA [91\u201393] provides an alternative solution. As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].\nThe key idea of kernel CCA lies in embedding the data into a higher dimensional feature space \u03c6x : X \u2192 H, where Hx is the reproducing kernel Hilbert space (RKHS) associated with the real numbers, kx : X \u00d7 X \u2192 R and kx(xi, xj) =< \u03c6x(xi), \u03c6x(xj) >. ky,Hy , and \u03c6y can be defined analogously.\nBy adapting the representer theorem [103] to the case of multiview data to state that the following minimization problem,\nmin f1,...,fk L((x1, y1, fx(x1), fy(y1)), . . . , (xn, yn, fx(xn), fy(yn)))\n+ \u2126x(||f ||2K , ||f ||2K) (8)\nwhere L is an arbitrary loss function and \u2126 is a strictly monotonically increasing function, admits representation of the form\nfx(x) = \u2211 i \u03b1ikx(xi, x), fy(y) = \u2211 i \u03b2iky(yi, y) (9)\nCorrespondingly, we replace vectors wx and wy in our previous CCA formulation Eq.(1) with fx = \u2211 i \u03b1i\u03c6x(xi) and\nfy = \u2211 i \u03b2i\u03c6y(yi), respectively, and replace the covariance matrices accordingly. The kernel CCA objective can be written as follows:\n\u03c1 = f>x C\u0302xyfy\u221a\nf>x C\u0302 > xxfxf > y C\u0302yyfy\n(10)\nIn particular, the kernel covariance matrix C\u0302xy is defined as\nC\u0302xy = 1\nn n\u2211 i=1 ( \u03c6x(xi)\u2212 \u00b5\u03c6x(x) ) ( \u03c6y(yi)\u2212 \u00b5\u03c6y(y) )> , (11)\nwhere \u00b5\u03c6x(x) = 1 n \u2211n i=1 \u03c6x(xi) and \u00b5\u03c6y(y) = 1 n \u2211n i=1 \u03c6y(yi) are the means of the two views\u2019 kernel mappings, respectively. The form of C\u0302xx and C\u0302yy can be obtained analogously.\nLet Kx denote the kernel matrix such that Kx = HK\u0303xH , where [K\u0303x]ij = kx(xi, xj) and H = I \u2212 1n11\n> is a centering matrix, 1 \u2208 Rn being a vector of all ones. And Ky is defined similarly. Further, we substitute them into Eq.(10) and formulate the objective of kernel CCA as the following optimization problem:\nmax \u03b1,\u03b2 \u03b1>KxKy\u03b2\u221a \u03b1TK2x\u03b1\u03b2 >K2y\u03b2 (12)\nAs discussed in [3], the above optimization leads to degenerate solutions when either Kx or Ky is invertible. Thus, we introduce regularization terms and maximize the following regularized expression\nmax \u03b1,\u03b2 \u03b1>KxKy\u03b2\u221a \u03b1T (K2x + xKx)\u03b1\u03b2 > ( K2y + yKy ) \u03b2\n(13)\nSince this new regularized objective function is not affected by re-scaling of \u03b1 or \u03b2, we assume that the optimization problem is subject to\n\u03b1>K2x\u03b1+ x\u03b1 >Kx\u03b1 = 1 \u03b2>K2y\u03b2 + y\u03b2 >Ky\u03b2 = 1 (14)\nSimilar to the optimized case of CCA, by formulating the Lagrangian dual of Eq.(13) with the constraints in Eq.(14), it can be shown that the solution to Eq.(13) is also equivalent to solving a pair of generalized eigenproblems [3],\n(Kx + xI) \u22121 Ky (Ky + yI) \u22121 Kx\u03b1 = \u03bb 2\u03b1 (Ky + yI) \u22121 Kx (Kx + xI) \u22121 Ky\u03b2 = \u03bb 2\u03b2 (15)\nConsequently, the statistical properties of KCCA have been investigated from several aspects [2, 104]. Fukumizu et al. [105] introduce a mathematical proof of the statistical convergence of kernel CCA by providing rates for the regularization parameters. Later Hardoon and Shawe-Taylor [106] provide a detailed theoretical analysis of KCCA and propose a finite sample statistical analysis of KCCA by using a regression formulation. Cai and Sun [107] also provide a convergence rate analysis of kernel CCA under an approximation assumption. However, the problems of choosing the regularization parameter in practice remain largely unsolved.\nIn addition, KCCA has a closed-form solution via the eigenvalue system in Eq.(15), but this solution does not scale up to the large size of the training set, due to the problem of time complexity and memory cost. Thus, various approximation methods have been proposed by constructing low-rank approximations of the kernel matrices, including incomplete Cholesky decomposition [2], partial Gram-Schmidt orthogonolisation [3], and block incremental SVD [99, 108]. In addition, the Nystro\u0308m method [109] is widely used to speed up the kernel machines [110\u2013113]. This approach is achieved by carrying out an eigen-decomposition on a lowerdimensional system, and then expanding the results back up to the original dimensions.\nIn early applications, KCCA has shown its ability as a feature preprocessing step that improves subsequent analysis in, for example, classification using a support vector machine [114]. While the combination of KCCA and SVM seems effective, there appears no guarantee that the multi-view representation learned by KCCA will be best suited to the classification task. Consequently, Farquhar et al. [115] investigate the possibility of combining the two distinct stages of KCCA and SVM into a single optimization named as SVM-2K, which has been widely used in classification tasks where two views of the same phenomenon are available.\nWith the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117]. Hardoon et al. [3] first apply KCCA to cross-modality retrieval task, in which images are retrieved by a given multiple text query and without using any label information around the retrieved images. Consequently, KCCA is exploited by Socher and Li [94] to learn a mapping between textual words and visual words so that both modalities are connected by a shared, low dimensional feature space. Further, Hodosh et al. [117] make use of KCCA in a stringent task of associating images with natural language sentences that describe what is depicted.\nMoreover, kernel CCA also has attracted much attention in multi-view clustering [32, 98]. Blaschko and Lampert [32] propose a correlational spectral clustering method, which exploits KCCA to do unsupervised clustering of images and text in latent meaning space. For webpage clustering task, Trivedi et al. [98] use a regularized variant of the KCCA algorithm to learn a lower dimensional subspace from heterogeneous data sources. This approach leverages tag information as a complementary part of webpage contents to extract highly discriminative features.\nWhile KCCA can learn representation that is closely related to the underlying generative process of the multi-view data, it may suffer from the small sample effect when the data acquisition in one or more modalities is expensive or otherwise limited. In order to robustly learn the relevant directions maximizing the correlation, Blaschko et al. [97] propose to modify the objective of KCCA with semi-supervised Laplacian regularization [118] to favor directions that lie along the data manifold [118]."}, {"heading": "2.5 Deep CCA", "text": "The CCA-like objectives can be naturally applied to neural networks to capture high-level associations between data from multiple views. In the early work, by assuming that different parts of perceptual input have common causes in the external world, Becker and Hinton [119] present a multilayer nonlinear extension of canonical correlation by maximizing the normalized covariation between the outputs from two neural network modules. Further, Becker [120] explores the idea of maximizing the mutual information between the outputs of different network modules to extract higher order features from coherence inputs.\nLater Lai and Fyfe [121, 122] investigate a neural network implementation of CCA and maximize the correlation (rather than canonical correlation) between the outputs of the networks for each view. Hsieh [123] formulates a nonlinear canonical correlation analysis (NLCCA) method using three feedforward neural networks. The first network maximizes the correlation between the canonical variates (the two output neurons), while the remaining two networks map the canonical variates back to the original two sets of variables.\nAlthough multiple CCA-based neural network models have been proposed for decades, the full deep neural network extension of CCA, referred as deep CCA, has recently been developed by Andrew et al. [8]. Inspired by the recent success of deep neural networks [6, 124], Andrew et al. [8] introduce deep CCA to learn deep nonlinear mappings between two views {X,Y } which are maximally correlated. The deep CCA learns representations of the two views by using multiple stacked layers of nonlinear mappings. In particular, assume for simplicity that the network has d intermediate layers, deep CCA first learns deep representation from fx(x) = hWx,bx(x) with parameters (Wx, bx) = (W 1 x , b 1 x,W 2 x , b 2 x, . . . ,W d x , b d x), where W l x(ij) denotes the parameters associated with the connection between unit i in layer l, and unit j un layer l+ 1. Also, blx(j) denotes the bias associated with unit j in layer l+ 1. Given a sample of the second view, the representation fy(y) is computed in the same way, with different parameters (Wy, by). The goal of deep CCA is to jointly learn parameters for both views such that corr(fx(X), fy(Y )) is as high as possible. Let \u03b8x be the vector of all the parameters (W x, bx) of the first view and similarly for \u03b8y , then\n(\u03b8\u2217x, \u03b8 \u2217 y) = arg max\n(\u03b8x,\u03b8y) corr(fx(X; \u03b8x), fy(Y ; \u03b8y)). (16)\nFor training deep neural network models, parameters are typically estimated with gradient-based optimization methods. Thus, the parameters (\u03b8\u2217x, \u03b8 \u2217 y) are also estimated on the training data by following the gradient of the correlation objective, with batchbased algorithms like L-BFGS as in [8] or stochastic optimization with mini-batches [125\u2013127].\nDeep CCA and its extensions have been widely applied in learning representation tasks in which multiple views of data are provided. Yan and Mikolajczyk [128] learn a joint latent space for matching images and captions with a deep CCA framework, which adopts a GPU implementation and could deal with overfitting. To exploit multilingual context when learning word embeddings, Lu et al. [126] learn deep non-linear embeddings of two languages using the deep CCA.\nRecently, a deep canonically correlated autoencoder (DCCAE) [20] is proposed by combining the advantages of the deep CCA and autoencoder-based approaches. In particular, DCCAE consists\nof two autoencoders and optimizes the combination of canonical correlation between the learned bottleneck representations and the reconstruction errors of the autoencoders. This optimization offers a trade-off between information captured in the embedding within each view on one aspect, and the information in the relationship across views on the other."}, {"heading": "3 SHALLOW METHODS ON MULTI-VIEW REPRESENTATION LEARNING", "text": "In this section, we will first review the shallow multi-view representation learning methods from probabilistic modeling perspective, and then survey the related methods from directly parameterized representation learning perspective."}, {"heading": "3.1 Probabilistic Models", "text": "From the probabilistic modeling perspective, the problem of multiview feature learning can be interpreted as an attempt to recover a compact set of latent random variables that describe a distribution over the observed multi-view data. We can express p(x, y, h) as a probabilistic model over the joint space of the latent variables h, and observed two-view data x, y. Feature values are determined by the posterior probability p(h|x, y). Parameters are usually estimated by maximizing the regularized likelihood of the training data."}, {"heading": "3.1.1 Multi-View Probabilistic Latent Semantic Analysis", "text": "Probabilistic latent semantic analysis (PLSA) [129] is a statistical variant of Latent Semantic Analysis (LSA) [130] and a significant step toward probabilistic modeling of text. It performs a probabilistic decomposition by modeling each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics\u201d z \u2208 Z = {z1, . . . , zK}. Each topic zk is associated with relative estimates P (ti|zk) for each term in the corpus. A document dj is then represented as a convex combination of factors with mixing weights P (zk|dj), i.e., the predictive probabilities for terms in a particular document have the form P (ti|dj) = \u2211 k P (ti|zk)P (zk|dj), with non-negative\nprobabilities and constraints \u2211 i P (ti|zk) = 1 for all k and\u2211\nk P (zk|dj) = 1 for all j. Multi-modal PLSA was originally proposed for the problem of jointly modeling content and connectivity. In the original LinkPLSA model [9], both term-based PLSA [129] and citationbased PHITS [131] are merged into a joint probabilistic model, explaining terms and citations in terms of a common set of underlying factors. In particular, Link-PLSA defines the following joint model for predicting citations/links and terms in documents:\nP (ti|dj) = \u2211 k P (ti|zk)P (zk|dj),\nP (cl|dj) = \u2211 k P (cl|zk)P (zk|dj). (17)\nwhere P (zk|dj) denotes the shared document-specific mixing proportions. It couples the conditional probabilities for terms and citations: each topic has some probability P (cl|zk) of linking to document dl as well as some probability P (ti|zk) of containing an occurrence of term ti. This coupling integrates content and link information in a principled manner.\nFurther, Link-PLSA aims at maximizing the following (normalized) log-likelihood function,\nL = \u2211 j [ \u03b1 \u2211 i Cij\u2211 i\u2032 Ci\u2032j log \u2211 k P (ti|zk)P (zk|dj)\n+(1\u2212 \u03b1) \u2211 i Alj\u2211 l\u2032 Al\u2032j log \u2211 k\nP (cl|zk)P (zk|dj) ] (18)\nwhere C is the term-document matrix of word counts and A is a matrix of document-citation pair. Cij denotes how often a term ti occurs in document dj and Aij is nonzero if dj contains a hyperlink di. Following the EM approach it is straightforward to derive the parameter estimation.\nFurther, Monay et al. [132] present a multi-view PLSA-based approach called PLSA-words. The principal idea of PLSA-words is to constrain the latent space by focusing on textual features and then learning visual variations conditioned on the space learned from text. If the conditional dependence assumption holds, then the parameter learning for visual features is effective, subject to the generated informative latent space from textual features. If the dependence assumption is violated, then on average the parameter learning is less effective and multi-modal PLSA learning may not be succesful. Instead of conditional single directed dependence from textual modality to visual modality, Zhang et al. [133] propose a PLSA based probabilistic multi-modal semantic model, in which the visual features and the textual words are fully connected via a hidden layer to allow much more mutual dependence between each other than PLSA-words.\nConsidering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141]. For example, inspired by the knowledge from neuroscience that our neocortex consists of multiple layers, Lienhart et al. [136] propose a multilayer multimodal PLSA model, which can handle different modalities as well as different features within a mode."}, {"heading": "3.1.2 Multi-Modal Latent Dirichlet Allocation", "text": "Latent Dirichlet allocation (LDA) [142] is a generative probabilistic model for collections of a corpus. It proceeds beyond PLSA through providing a generative model at words and document level simultaneously. In particular, LDA is a three-level hierarchical Bayesian network that models each document as a finite mixture over an underlying set of topics.\nAs a generative model, LDA is extendable to multi-view data. Barnard et al. [10] present a mixture of multi-modal LDA model (MoM-LDA), which describes the following generative process for the multi-modal data: each document (consisting of visual and textual information) has a distribution for a fixed number of topics (mixture components), and given a specific topic the visual features and textual words are generated. Further, the probability distribution of the topics is different for each image-caption pair, which is achieved by imposing a Dirichlet prior for the distribution of topics.\nConsequently, Blei and Jordan [11] propose a correspondence LDA (Corr-LDA) model, which not only allows simultaneous dimensionality reduction in the representation of region descriptions and words, but also models the conditional correspondence between their respectively reduced representations. The graphical model of Corr-LDA is depicted in Figure 3. This model can be viewed in terms of a generative process that first generates the region descriptions and subsequently generates the caption words.\nIn particular, let z = {z1, z2, . . . , zN} be the latent variables that generate the image, and let y = {y1, y2, . . . , yM} be discrete indexing variables that take values from 1 to N with equal probability. Each image and its corresponding caption are represented as a pair (r,w). The first element r = {r1, . . . , rN} is a collection of N feature vectors associated with the regions of the image. The second element w = {w1, . . . , wM} is the collection of M words of the caption. Given N and M , a Kfactor Corr-LDA model assumes the following generative process for an image/caption (r,w):\n1) Sample \u03b8 \u223c Dir(\u03b8|\u03b1). 2) For each image region rn, n \u2208 {1, . . . , N}:\na) Sample zn \u223c Mult(\u03b8) b) Sample rn \u223c p(r|zn, \u00b5, \u03c3) from a multivariate\nGaussian distribution conditioned on zn.\n3) For each caption word wm, m \u2208 {1, . . . ,M}:\na) Sample ym \u223c Unif(1, . . . , N) b) Sample wm \u223c p(w|ym, z, \u03b2) from a multino-\nmial distribution conditioned on the zym factor.\nConsequently, Corr-LDA specifies the following joint distribution on image regions, caption words, and latent variables:\np(r,w,\u03b8, z,y) = p(\u03b8|\u03b1) (\nN\u220f n=1\np(zn|\u03b8)p(rn|zn, \u00b5, \u03c3) )\n\u00b7 (\nM\u220f m=1 p(ym|N)p(wm|ym, z, \u03b2) ) (19)\nNote that exact probabilistic inference for Corr-LDA is intractable and we employ variational inference methods to approximate the posterior distribution over the latent variables given a particular image/caption.\nBoth MoM-LDA and Corr-LDA achieve great success in image annotation. However, there are still limitations with these models. First, selecting the number of mixture components to be used in a MoM-LDA or Corr-LDA model is difficult. In particular, different choices of the number of mixture components correspond to different MoM-LDA or Corr-LDA models. Second, MoMLDA and Corr-LDA models neglect the discriminative information which may be helpful for multi-view learning. For example, image captions provide evidence for the class label, and the class label provides evidence for image captions. Third, both models discard substantial visually auxiliary information existing in the images,\ne.g., the position information of the visual features. For multimodal topic modeling, we are ultimately interested in defining a latent space that is consistent in semantic terms, while discarding as little useful information about the multi-modal data as possible.\nAmount of work have been presented to solve the above limitations. Based on hierarchical Dirichlet process (HDP) [143], Yakhnenko and Honavar [144] introduce a multi-modal hierarchical Dirichlet process model (MoM-HDP), which is an extension of MoM-LDA with an infinite number of mixture components. Thus, MoM-HDP is capable of removing the need for a prior choice of the number of mixture components or the computational expense of model selection. Note that in practice, the Dirichlet process is approximated by truncating it [145]. Qi et al. [146] also present a correspondence hierarchical Dirichlet process model (Corr-HDP), which is an extension of the Corr-LDA model using a hierarchical Dirichlet process instead.\nFollowing the supervised LDA algorithms [147], supervised multi-modal LDA models are subsequently proposed to make effective use of the discriminative information. Since supervised topic models can be classified into two categories: downstream models and upstream models [147], supervised multi-modal LDA models can also be catergorized accordingly. For a downstream supervised multimodal model, the supervised response variables are generated from topic assignment variables. For instance, Wang et al. [148] develop a multi-modal probabilistic model for jointly modeling the image, its class label, and its annotations, called multi-class supervised LDA with annotations, which treats the class label as a global description of the image, and treats annotation terms as local descriptions of parts of the image. Its underlying assumptions naturally integrate the multi-modal data with their discriminative information so that it takes advantage of the merits of Corr-LDA and supervised LDA [149] simultaneously.\nFor an upstream supervised multi-modal model, the response variables directly or indirectly generate latent topic variables [150\u2013152]. For instance, Cao et al. [151] propose a spatially coherent latent topic model (Spatial-LTM), which represents an image containing objects in two different modalities: appearance features and salient image patches. Further, a supervised spatialLTM model is presented by incorporating label information into distribution of topics. Nguyen et al. [153] propose a Multi-modal Multi-instance Multi-Label LDA model (M3LDA), in which the model consists of a visual-label part, a textual-label part, and a label-topic part. The underlying idea is to make the topic decided by the visual information to be consistent with the topic decided by the textual information, leading to the correct label assignment."}, {"heading": "3.1.3 Shared Gaussian Process Latent Variable Model", "text": "Gaussian Processes (GPs) [154, 155] are generalizations of Gaussian distributions defined over infinite index sets. GPs have become powerful models for classification and regression that subsume numerous classes of function approximators, such as single hidden-layer neural networks and RBF networks. Lawrence [156] proposes the Gaussian Process Latent Variable Model (GPLVM) as a non-linear dimensionality reduction technique. Consequently, Shon et al. [157] present the shared GPLVM (SGPLVM) model as a generalization of GPLVM that can handle multiple observation spaces, where each set of observations is parameterized by a different set of kernel parameters. This shared GPLVM model can be considered as a nonlinear extension to CCA.\nLet X , Y be matrices of observations drawn from spaces of dimensionality dX , dY respectively, and Z be a latent space of dimensionality dZ \u2264 dX , dY . Assume that each latent variable zi generates a pair of observations xi and yi through GPs parameterized as non-linear functions fX : Z \u2192 X and fY : Z \u2192 Y . By employing an RBF kernel, the similarity between two data points z, z\u2032 can be defined as:\nk(z, z\u2032) = \u03b1Xexp ( \u2212\u03b3X\n2 \u2016z \u2212 z\u2032\u20162\n) + \u03b4z,z\u2032\u03b2 \u22121 X (20)\nwhere \u03b8X = {\u03b1X , \u03b2X , \u03b3X} are the given hyperparameters for the X space and \u03b4 represents the delta function.\nThe priors P (\u03b8X), P (\u03b8Y ), P (X), the likelihoods P (X) , P (Y ) for the X , Y observation spaces, and the joint likelihood PGP (X,Y, Z, \u03b8X , \u03b8Y ) are given by:\nP (X|\u03b8X , Z) = |W |N\u221a\n(2\u03c0)NDX |K|DX exp\n( \u22121\n2 DX\u2211 k=1 w2kX > k KXXk\n)\nP (Y |\u03b8Y , Z) = |V |N\u221a\n(2\u03c0)NDY |K|DY exp\n( \u22121\n2 DY\u2211 m=1 v2mY > mKY Ym\n)\nP (\u03b8X) \u221d 1\n\u03b1X\u03b2X\u03b3X P (\u03b8Y ) \u221d\n1\n\u03b1Y \u03b2Y \u03b3Y\nP (Z) = 1\u221a 2\u03c0 exp\n( \u22121\n2 \u2211 i\n||zi||2 )\nPGP (X,Y, Z, \u03b8X , \u03b8Y )\n= P (X|\u03b8X , Z)P (Y |\u03b8Y , Z)P (\u03b8X)P (\u03b8X)P (Z) (21)\nwhere \u03b1X , \u03b2X , \u03b3X are hyperparameters for the X space, and wk, vm respectively denote the diagonal entries for matrices W , V .\nGiven a trained SGPLVM, the parameters in one observation space can be inferred from the given parameters in the other observation space (e.g., infer the corresponding y given x). This problem can be solved in two steps. First, the most likely latent coordinate z given the observation x can be determined by maximizing the log likelihood of (z, x). Once the correct latent coordinate z has been inferred for a given x, the corresponding observation y can be predicted with the trained SGPLVM.\nFollowing the idea of discriminative Gaussian process latent variable model (DGPLVM) [158], Eleftheriadis et al. [159] propose a discriminative shared Gaussian process latent variable model (DS-GPLVM) for multi-view analysis by exploiting a shared discriminative manifold by multi-view data. Further, Xu et al. [160] propose a large margin multi-view Gaussian process via an integration of Gaussian process and large-margin principle. Recently, by imposing cross-modal similarity/dissimilarity constraint as a prior to the latent space, Song et al. [161] propose a multi-modal regularized similarity Gaussian Process variable model to enforce that the semantically similar/dissimilar crossmodal observations are also similar/dissimilar in the latent space, which maximizes the cross-modal semantic consistency."}, {"heading": "3.1.4 Collective Matrix Factorization", "text": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169]. The learned factor matrices can be used for estimating missing values in collaborative filtering, or as multi-view representation for classification and clustering.\nIn general, a simple form of CMF considers multiple low rank data matrices {Xi \u2208 Rn\u00d7di}i\u2208I of the same row dimension, and simultaneously factorizes them based on the following form\nXi = UV > i for all i \u2208 I (22)\nwhere it is assumed that these matrices share the same factor matrix U \u2208 Rn\u00d7k and each matrix has its own loading matrix Vi \u2208 di \u00d7 k. Bias for different factors and loadings can also be incorporated [170].\nTo use CMF, we must compute the factors and loadings given multiple observed data matrices. The common approach is to minimize the regularized squared error loss with respect to U and {Vi}i\u2208I ,\nmin U,Vi \u2211 i\u2208I \u03bbXi\u2016Xi \u2212 UV >i \u20162F + \u03bbU\u2016U\u20162F + \u2211 i\u2208I \u03bbVi\u2016Vi\u20162F\n(23)\nwhere \u03bbU and \u03bbVi are regularization parameters. The above regularized form of CMF can be generalized as a probabilistic model. In probabilistic collective matrix factorization (PCMF), we assume the following generative distributions,\np(U) =Nn,k(0, \u03bb\u22121U In \u2297 Ik) p(Vi) =Ndi,k(0, \u03bb\u22121Vi Idi \u2297 Ik) p(Xi) =Nn,di(UV >i , \u03bb\u22121Xi In \u2297 Idi) (24)\nwhere Ik is a k-dimensional identity matrix. This is the probabilistic interpretation of CMF that other complex models build on [171]. CMF model is usually fit by finding a locally optimal solution of factor matrix U and loading matrices Vi, with an iterative algorithm. Further, from the above generative distributions of model components, we can see that the main difference between probabilistic CMF and probabilistic CCA in Eq.(6) depends on the choice of the noise distribution on top of the shared components.\nIn early studies of joint matrix factorization, Yu et al. [163] propose a multi-label informed latent semantic indexing algorithm which exploits the idea of joint matrix factorization to learn a shared latent semantic factor. Consequently, Zhu et al. [172] introduce a joint matrix factorization model to combine the information of content and links for web page analysis. By sharing factors between text content and link structure, this model could learn a consistent and compact feature representation. Further, supervised information is incorporated into the joint matrix factorization process to obtain a better representation for the classification task.\nLater Singh and Gordon [162] propose collective matrix factorization in which several matrices are factorized simultaneously, sharing parameters among factors. This CMF approach allows nonlinear relationships between the parameters and outputs, using Bregman divergences to measure the error. Further, a hierarchical Bayesian variant [173] is proposed to provide an estimate of uncertainty during the relational modeling process. In particular, a block Metropolis-Hastings sampler has been presented to speed up the parameter estimation.\nFurther, CMF has been applied for multi-view hashing [174, 175]. A multimodal latent binary embedding (MLBE) method is proposed by Zhen and Yang [174] to learn hash functions from a probabilistic generative perspective. MLBE encodes the intra-modality and inter-modality similarities through a latent factor model in which hash codes are binary latent factors in a common Hamming space. Further, Ding et al. [175] put forward collective matrix factorization hashing to learn unified hash codes\nby CMF with a latent factor model from different modalities of one instance.\nIn addition, joint nonnegative matrix factorization (NMF) also has been proposed for multi-view analysis [176\u2013179]. For example, Liu et al. [177] introduce a joint NMF algorithm to learn a factorization that gives compatible clustering solutions across multiple views. The key idea lies in formulating a joint matrix factorization process with the constraint that pushes clustering solution of each view towards a common consensus instead of fixing it directly."}, {"heading": "3.1.5 Multi-View Sparse Coding", "text": "Like CMF, multi-view sparse coding also has both a probabilistic and non-probabilistic interpretation. Multi-view sparse coding [12\u201314] relates a latent representation (either a vector of random variables or a feature vector, depending on the interpretation) to the multi-view data through a set of linear mappings, which we refer to as the dictionaries. It has the property of finding shared representation h\u2217 which picks out the most appropriate bases and zeros others, given a high degree of correlation with the input. This property is owing to the explaining away effect which aries naturally in directed graphical models [180].\nThe main difference between multi-view sparse coding and CMF is that multi-view sparse coding adopts a penalty to ensure a sparse activation of h. Given a pair of datasets {X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy}, a non-probabilistic multi-view sparse coding scheme can be seen as recovering the code or feature vector associated with a new multi-view input via:\nh\u2217 = arg min h \u2016x\u2212Wxh\u201622 + \u2016y \u2212Wyh\u201622 + \u03bb\u2016h\u20161 (25)\nLearning the pair of dictionaries {Wx,Wy} can be accomplished by optimizing the following training criterion with respect to Wx and Wy:\nJWx,Wy = \u2211 i ( \u2016xi \u2212Wxh\u2217i \u201622 + \u2016yi \u2212Wyh\u2217i \u201622 ) (26)\nwhere xi and yi are the two modal inputs and h\u2217 is the corresponding sparse code determined by Eq.(25). In particular, Wx and Wy are usually constrained to have unit-norm columns.\nThe probabilistic interpretation of multi-modal sparse coding differs from that of CMF, in that instead of a Gaussian prior on the latent variable h, we use a sparsity inducing Laplace prior (corresponding to an L1 penalty):\np(h) = dh\u220f j 2 \u03bb exp(\u2212\u03bb|hj |)\n\u2200ni=1 : p(xi|h) = N (xi;Wxh+ \u00b5xi , \u03c32xiI) p(yi|h) = N (yi;Wyh+ \u00b5yi , \u03c32yiI) (27)\nIn the case of two-view sparse coding, because we seek a sparse multi-view representation (i.e., one with many features set to zero), we are interested in recovering the MAP (maximum a posteriori) value of h: i.e., h\u2217 = arg maxh p(h|x, y) rather than its expected value E[h|x, y]. Under this interpretation, learning dictionaries Wx and Wy proceeds as maximizing the likelihood of the data given these MAP values of h\u2217: arg maxWx,Wy \u220f i p(xi|h\u2217)p(yi|h\u2217) subject to the norm constraint on Wx and Wy . Note that this parameter learning scheme, subject to the MAP values of the latent h, is not a standard practice in the probabilistic graphical model literature.\nTypically, the likelihood of the multi-view data is maximized directly. In the presence of latent variables, expectation maximization is employed where the parameters are optimized with respect to the marginal likelihood, i.e., summing or integrating the joint log-likelihood over the all values of the latent variables under their posterior, rather than considering only the single MAP value of h. The theoretical properties of this form of parameter learning are not yet well understood but seem to work in practice (e.g., Gaussian mixture models).\nOne might expect that multi-view sparse representation would significantly leverage the performance especially when features for different views are complementary to one another and indeed it seems to be the case. There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184]."}, {"heading": "3.1.6 Multi-View Latent Space Markov Networks", "text": "Undirected graphical models, also called Markov random fields that have many special cases, including the exponential family Harmonium [185] and restricted Boltzmann machine [186]. Within the context of unsupervised multi-view feature learning, Xing et al. [15] first introduce a particular form of multi-view latent space Markov network model called multi-wing harmonium model. This model can be viewed as an undirected counterpart of the aforementioned directed aspect models such as multi-modal LDA [11], with the advantages that inference is fast due to the conditional independence of the hidden units and that topic mixing can be achieved by document- and feature-specific combination of aspects.\nFor simplicity, we begin with dual-wing harmonium model, which consists of two modalities of input units X = {xi}ni=1, Y = {yj}nj=1, and a set of hidden units H = {hk}nk=1. In this dual-wing harmonium, each modality of input units and the hidden units constructs a complete bipartite graph where units in the same set have no connections but are fully connected to units in the other set. In addition, there are no connections between two input modalities. In particular, consider all the case where all observed and hidden variables are from exponential family; we have\np(xi) =exp{\u03b8Ti \u03c6(xi)\u2212A(\u03b8i)} p(yj) =exp{\u03b7Tj \u03c8(yj)\u2212B(\u03b7j)} p(hk) =exp{\u03bbTk \u03d5(hk)\u2212 C(\u03bbk)} (28)\nwhere \u03c6(\u00b7), \u03c8(\u00b7), and \u03d5(\u00b7) are potentials over cliques formed by individual nodes, \u03b8i, \u03b7j , and \u03bbk are the associated weights of potential functions, and A(\u00b7), B(\u00b7), and C(\u00b7) are log partition functions.\nThrough coupling the random variables in the log-domain and introducing other additional terms, we obtain the joint distribution p(X,Y,H) as follows:\np(X,Y,H) \u221d exp {\u2211\ni \u03b8Ti \u03c6(xi) + \u2211 j \u03b7Tj \u03c8(yj) + \u2211 k \u03bbTk \u03d5(hk)\n+ \u2211 ik \u03c6(xi) TWik\u03d5(hk) + \u2211 jk \u03c8(yj) TUjk\u03d5(hk) } (29)\nwhere \u03c6(xi)\u03d5(hk), \u03c8(yj)\u03d5(hk) are potentials over cliques consisting of pairwise linked nodes, and Wik, Ujk are the associated\nweights of potential functions. From the joint distribution, we can derive the conditional distributions\np(xi|H) \u221d exp{\u03b8\u0303Ti \u03c6(xi)\u2212A(\u03b8\u0303i)} p(yj |H) \u221d exp{\u03b7\u0303Tj \u03c8(yj)\u2212B(\u03b7\u0303j)}\np(hk|X,Y ) \u221d exp{\u03bb\u0303Tk \u03d5(hk)\u2212 C(\u03bb\u0303k)} (30)\nwhere the shifted parameters \u03b8\u0303i = \u03b8i+ \u2211 kWik\u03d5(hk), \u03b7\u0303j = \u03b7j+\u2211\nk Ujk\u03d5(hk), and \u03bb\u0303k = \u03bbk + \u2211 iWik\u03c6(xi) + \u2211 j Ujk\u03c8(yj).\nIn training probabilistic models parameters are typically updated in order to maximize the likelihood of the training data. The updating rules can be obtained by taking derivative of the log-likelihood of the sample defined in Eq.(29) with respect to the model parameters. The multi-wing model can be directly obtained by extending the dual-wing model when the multi-modal input data are observed.\nFurther, Chen et al. [16] present a multi-view latent space Markov network and its large-margin extension that satisfies a weak conditional independence assumption that data from different views and the response variables are conditionally independent given a set of latent variables. In addition, Xie and Xing [187] propose a multi-modal distance metric learning (MMDML) framework based on the multi-wing harmonium model and metric learning method by [188]. This MMDML provides a principled way to embed data of arbitrary modalities into a single latent space where distance supervision is leveraged."}, {"heading": "3.2 Directly Learning A Parametric Embedding from", "text": "Multi-View Input to Representation\nFrom the framework of multi-view probabilistic models discussed in the previous section, we can see that the learned representation is usually associated with shared latent variables, specifically with their posterior distribution given the multi-view observed input. Further, this posterior distribution often becomes complicated and intractable if the designed models have hierarchical structures. It has to resort to sampling or approximate inference techniques, which need to endure the associated computational and approximation error cost. In addition, a posterior distribution over shared latent variables is not yet a reasonable feature vector that can be directly fed to the classifier. Therefore, if we intend to obtain stable deterministic feature values, an alternative non-probabilistic multiview embedding learning paradigm is the directly parameterized feature or representation functions. The common perspective between these methods is that they learn a direct encoding for multiview input. Consequently, in this section we review the related work from this perspective."}, {"heading": "3.2.1 Partial Least Squares", "text": "Partial Least Squares (PLS) [189\u2013191] is a wide class of methods for modeling relations between sets of observed variables. It has been a popular tool for regression and classification as well as dimensionality reduction, especially in the field of chemometrics [192, 193]. The underlying assumption of all PLS methods is that the observed data are generated by a process which is driven by a small number of latent variables. In particular, PLS creates orthogonal latent vectors by maximizing the covariance between different sets of variables.\nGiven a pair of datasets X = [x1, . . . , xn] \u2208 Rdx\u00d7n and Y = [y1, . . . , yn] \u2208 Rdy\u00d7n, a k-dimensional PLS solution can\nbe parameterized by a pair of matrices Wx \u2208 Rdx\u00d7k and Wy \u2208 Rdy\u00d7k [47]. The PLS problem can now be expressed as:\nmax Wx,Wy\ntr ( W>x CxyWy ) s.t. W>x Wx = I,W > y Wy = I. (31)\nIt can be shown that the columns of the optimal Wx and Wy correspond to the singular vectors of covariance matrix Cxy = E[xy>]. Like the CCA objective, PLS is also an optimization of an expectation subject to fixed constraints.\nIn essence, CCA finds the directions of maximum correlation while PLS finds the directions of maximum covariance. Covariance and correlation are two different statistical measures for describing how variables covary. It has been shown that there is close connections between PLS and CCA in several aspects [190, 193]. Guo and Mu [194] investigate the CCA based methods, including linear CCA, regularized CCA, and kernel CCA, and compare to the PLS models in solving the joint estimation problem. In particular, they provide a consistent ranking of the above methods in estimating age, gender, and ethnicity.\nFurther, Li et al. [195] introduce a least square form of PLS, called cross-modal factor analysis (CFA). CFA aims at finding orthogonal transformation matrices Wx and Wy that minimize the following expression:\n||X>Wx \u2212 Y >Wy||2F subject to: W>x Wx = I, W > y Wy = I (32)\nwhere || \u00b7 ||F denotes the Frobenius norm. It can be easily verified that the above optimization problem in Eq.(32) has the same solution as that of PLS. Several extensions of CFA are presented by incorporating non-linearity and supervised information [196\u2013 198].\nAmong the variants of PLS, Orthonormalized PLS (OPLS) [199\u2013201] is a popular representative. OPLS does not consider the variance of one of the two views and has been shown to be competitive with the other PLS variants. In particular, OPLS computes the orthogonal transform vectors for X by solving the following optimization problem:\nmax Wx\ntr ( W>x XY >Y X>Wx )\ns.t. W>x XX >Wx = I. (33)\nIt can be shown that the columns of the optimal W are given by the eigenvectors of the following generalized eigenvalue problem:\nXY >Y X>wx = \u03b7XX >wx (34)\nFrom the above computation process of the projection of X in OPLS, we can see that the obtained transformation matrix Wx is directed by the information encoded in Y. This is especially useful in the supervised learning context in which OPLS is applied for supervised dimensionality reduction.\nConnections between CCA and OPLS also have been studied in the past decade. Based on the unified framework for CCA and PLS in [190], CCA and OPLS also can be considered as special cases of this framework by selecting different values of regularization parameters. Further, Sun et al. [202] investigate the equivalence relationship between CCA and OPLS and show that the difference between the CCA solution and the OPLS solution is a mere orthogonal transformation.\nAnother commonly used variant of PLS is PLS Regression (PLSR), which has been proven to be an efficient robust method to do quantity analysis for signal processing [192, 203]. PLSR uses the two-block predictive PLS model to capture the relationship between X and Y [192]. It is especially true for the case that one set of variables are the target variables of a regression problem as it comprises regression tasks as well as dimensionality reduction techniques. By following the way of probabilistic CCA, Li et al. [204] present a probabilistic PLS to explain PLSR from a probabilistic viewpoint and describes the physical meaning of PLSR model."}, {"heading": "3.2.2 Multi-View Discriminant Analysis", "text": "CCA and its kernel extensions are used to match sets of features by maximizing within-set correlation and minimizing between-set correlation, which are unsupervised scenarios. A number of multiview analysis approaches [205\u2013207] have also been proposed by considering the scenarios in which the data have multiple different views, along with supervised information.\nGiven a pair of datasets X = [x1, . . . , xn] and Y = [y1, . . . , yn] with the corresponding labels l = (l1, . . . , ln), the two-view regularized Fisher discriminant analysis (FDA) [205] chooses two sets of weights wx and wy to solve the following optimization problem,\n\u03c1 = w>x Xll >Y >wy\u221a (w>x XBX>wx + \u00b5||wx||2) \u00b7 ( w>y Y BY >wy + \u00b5||wy||2 ) (35)\nwhere wx and wy are the weight vectors for each view, and B is a matrix incorporating the label information and the balance of the datasets. Since the equation is not affected by rescaling of wx or wy , the optimization is subjected to the following constraints\nw>xXBX >wx + \u00b5||wx||2 = 1\nw>y Y BY >wy + \u00b5||wy||2 = 1 (36)\nBy introducing the Lagrange multipliers \u03bbx and \u03bby , the corresponding Lagrangian for this optimization is:\nL =w>xXll>Y >wy \u2212 \u03bbx 2\n( w>xXBX >wx + \u00b5||wx||2 \u2212 1 )\n\u2212 \u03bby 2\n( w>y Y BY >wy + \u00b5||wy||2 \u2212 1 ) , (37)\nwhich can be solved by differentiation with respect to the weight vectors wx and wy .\nBy substituting the two weight vectors with wx = X\u03b1 and wy = Y \u03b2, and a shared regularization parameter \u03ba for both sets of weights, we obtain\n\u03c1 = \u03b1>X>Xll>Y >Y \u03b2\u221a\n(\u03b1>X>XBX>X\u03b1+ \u03ba||wx||2)\n\u00b7 1\u221a (\u03b2>Y >Y BY >Y \u03b2 + \u03ba||wy||2)\n(38)\nThen the kernel form of it is:\n\u03c1 = \u03b1>Kxll >Ky\u03b2\u221a (\u03b1>KxBKx\u03b1+ \u03ba\u03b1>Kx\u03b1) \u00b7 (\u03b2>KyBKy\u03b2 + \u03ba||\u03b2>Ky\u03b2)\n(39)\nAs in the primal form, the equation is not affected by rescalingwx or wy; the optimization is subjected to the following constraints\n\u03b1>KxBKx\u03b1+ \u03ba\u03b1 >Kx\u03b1 = 1 \u03b2>KyBKy\u03b2 + \u03ba\u03b2 >Ky\u03b2 = 1 (40)\nwhereKx andKy are the corresponding kernels for the two views. The corresponding Lagrangian for this optimization can be written as\nL =\u03b1>Kxll>Ky\u03b2 \u2212 \u03bbx 2\n( \u03b1>KxBKx\u03b1+ \u03ba\u03b1 >Kx\u03b1\u2212 1 )\n\u2212 \u03bby 2\n( \u03b2>KyBKy\u03b2 + \u03ba\u03b2 >Ky\u03b2 \u2212 1 )\n(41)\nBy differentiation with respect to the weight vectors \u03b1 and \u03b2, we can solve the above problem. In addition, based on the approach outlined in [208], both regularized two-view FDA and its kernel extension can be casted as equivalent disciplined convex optimization problems. Consequently, Diethe et al. [206] introduce Multiview Fisher Discriminant Analysis (MFDA) that learns classifiers in multiple views, by minimizing the variance of the data along the projection while maximizing the distance between the average outputs for classes over all of the views.\nHowever, MFDA can only be used for binary classification problems. In [207], Kan et al. propose a Multi-view Discriminant Analysis (MvDA) method, which seeks for a discriminant common space by maximizing the between-class and minimizing the within-class variations, across all the views. Later based on bilinear models [209] and general graph embedding framework [210], Sharma et al. [211] introduce Generalized Multi-view Analysis (GMA). As an instance of GMA, Generalized Multi-view Linear Discriminant Analysis (GMLDA) finds a set of projection directions in each view that tries to separate different contents\u2019 class means and unify different views of the same class in the common subspace."}, {"heading": "3.2.3 Cross-Modal Ranking", "text": "Motivated by incorporating ranking information into multi-modal embedding learning, cross-modal ranking has attracted much attention in the literature [212\u2013214]. Bai et al. [212] present a supervised semantic indexing (SSI) model which defines a class of non-linear models that are discriminatively trained to map multimodal input pairs into ranking scores. Unlike CCA and CFA, SSI is trained from a supervised way on the ranking task of interest so that it retains more discriminative information.\nIn particular, SSI attempts to learn a similarity function f(q, d) between a text query q and an image d according to a pre-defined ranking loss. The learned function f directly maps each text-image pair to a ranking score based on their semantic relevance. Given a text query q \u2208 Rm and an image d \u2208 Rn, SSI intends to find a linear scoring function to measure the relevance of d given q:\nf(q, d) = q>Wd = m\u2211 i=1 n\u2211 j=1 qiWijdj (42)\nwhere f(q, d) is defined as the score between the query q and the image d, and the parameter W \u2208 Rm\u00d7n captures the correspondence between the two different modalities of the data: Wij represents the correlation between the i-th dimension of the text space and the j-th dimension of the image space. Note that this way of embedding allows both positive and negative correlations between different modalities since both positive and negative values are allowed in Wij .\nGiven the similarity function in Eq.(42) and a set of tuples, where each tuple contains a query q, a relevant image d+ and an irrelevant image d\u2212, SSI attempts to choose the scoring function f(q, d) such that f(q, d+) > f(q, d\u2212), expressing that d+ should be ranked higher than d\u2212. For this purpose, SSI exploits the\nmargin ranking loss [215] which has already been widely used in information retrieval, and minimizes:\u2211\n(q,d+,d\u2212)\nmax(0, 1\u2212 qTWd+ + qTWd\u2212) (43)\nThis optimization problem can be solved through stochastic gradient descent [216],\nW \u2190W + \u03bb ( q(d+)> \u2212 q(d\u2212)> ) ,\nif 1\u2212 qTWd+ + qTWd\u2212 > 0 (44)\nIn fact, this method is a special margin ranking perceptron [217], which has been shown to be equivalent to SVM [218]. In contrast to classical SVM, stochastic training is highly scalable and is easy to implement for millions of training examples. However, dealing with the models on all the pairs of multimodalities input features is still computationally challenging. Thus, SSI also proposes several improvements to the above basic model for addressing this issue, including low-rank representation, sparsification, and correlated feature hashing. In the following, we take the low-rank constrained SSI model for example.\nMotivated by dealing with the model memory, speed, and capacity control issues, a low-rank configuration for W is set up with W = U>V in Eq.(42), which results in the following low-rank scoring function:\nf(q, d) = q>U>V d = (Uq) > V d (45)\nwhere U \u2208 Rk\u00d7m is used to map the query text q from the m-dimensional text space to the k-dimensional subspace and V \u2208 Rk\u00d7n is used to map the retrieved image d from the n-dimensional image space to the k-dimensional subspace. Consequently, the text query and the retrieved image are mapped to a common k-dimensional latent semantic space, and then their similarity is measured with a cosine similarity score in the kdimensional space.\nWhen the W matrix is constrained in the low-rank way, training is performed in a similar way as before. Gradient steps are still applied to optimize the parameters U and V :\nU \u2190 U + \u03bbV (d+ \u2212 d\u2212)q>, if 1\u2212 f(q, d+) + f(q, d\u2212) > 0 V \u2190 V + \u03bbUq(d+ \u2212 d\u2212)>, if 1\u2212 f(q, d+) + f(q, d\u2212) > 0\n(46)\nInspired by the success of the SSI models for cross-media retrieval tasks, a variety of \u201ccross-modal ranking\u201d methods have been proposed to achieve better generalization performance. To exploit the advantage of online learning of kernel-based classifiers, Grangier and Bengio [214] propose a discriminative crossmodal ranking model called Passive-Aggressive Model for Image Retrieval (PAMIR), which not only adopts a learning criterion related to the final retrieval performance, but also considers different image kernels. In contrast to SSI, the parameterization of PAMIR can benefit from effective kernels for image comparison. Further, several recent efforts [219\u2013221] incorporate latent semantic representation learning into cross-modal ranking. Lu et al. [219] propose a Latent Semantic Cross-Modal Ranking (LSCMR) algorithm, which regards the cross-modal retrieval as a list-wise ranking problem and optimizes the list-wise ranking loss with a low-rank embedding setup. In particular, the low-rank embedding space is discriminatively learned by the structural large margin method. Moreover, Wu et al. [220] introduce a Bi-directional Cross-media Semantic Representation Model, which incorporates\nthe bi-directional list-wise ranking loss into the learning of crossmodal embeddings to take advantage of bi-directional ranking examples."}, {"heading": "3.2.4 Cross-Modal Hashing", "text": "CCA and its extensions haven been widely applied to conduct cross-view similarity search [25, 38, 95]. A promising way to speed up the cross-view similarity search is the hashing technique which makes a tradeoff between accuracy and efficiency. Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231]. The principle of the multi-modal hashing methods is to map the high dimensional multi-modal data into a common hash codes so that similar crossmodal data objects have the same or similar hash codes.\nBronstein et al. [222] propose a hashing-based model, called cross-modal similarity sensitive hashing (CMSSH), which approaches the cross-modality similarity learning problem by embedding the multi-modal data into a common metric space. The similarity is parameterized by the embedding itself. The goal of cross-modality similarity learning is to construct the similarity function between points from different spaces, X \u2208 Rd1 and Y \u2208 Rd2 . Assume that the unknown binary similarity function is s : X \u00d7 Y \u2192 {\u00b11}; the classical cross-modality similarity learning aims at finding a binary similarity function s\u0302 on X \u00d7 Y approximating s. Recent work attempts to solve the problem of cross-modality similarity leaning as an multi-view representation learning problem.\nIn particular, CMSSH proposes to construct two maps: \u03be : X \u2192 Hn and \u03b7 : Y \u2192 Hn, where Hn denotes the n-dimensional Hamming space. Such mappings encode the multi-modal data into two n-bit binary strings so that dHn(\u03be(x), \u03b7(y)) is small for s(x, y) = +1 and large for s(x, y) = \u22121 with a high probability. Consequently, this hamming embedding can be interpreted as cross-modal similarity-sensitive hashing, under which positive pairs have a high collision probability, while negative pairs are unlikely to collide. Such hashing also acts as a way of multi-modal dimensionality reduction when d1, d2 n.\nThe n-dimensional Hamming embedding for X can be thought of as a vector \u03be(x) = (\u03be1(x), . . . , \u03ben(x)) of binary embeddings of the form\n\u03bei(x) = { 0 if fi(x) \u2264 0, 1 if fi(x) > 0,\n(47)\nparameterized by a projection fi : X \u2192 R. Similarly, \u03b7i is a binary map parameterized by projection gi : Y \u2192 R.\nFollowing the greedy approach [232], the Hamming metric can be constructed sequentially as a superposition of weak binary classifiers on pairs of data points,\nhi(x, y) = { +1 if \u03bei(x) = \u03b7i(y), \u22121 otherwise ,\n= (2\u03bei(x)\u2212 1)(2\u03b7i(y)\u2212 1), (48)\nHere, a simple strategy for the maps is affine projection, such as fi(x) = p > i + ai and gi(y) = q > i y + bi. It can be extended to complex projections easily. Observing the resemblance with sequentially binary classifiers, boosted cross-modality similarity learning algorithms are introduced based on the standard AdaBoost procedure [233]. CMSSH has shown its utility and efficiency in several multi-view learning applications including cross-representation shape retrieval and alignment of multi-modal medical images.\nHowever, CMSSH only considers the inter-view correlation but ignores the intra-view similarity [234]. Kumar and Udupa [223] extend Spectral Hashing [235] from the single view setting to the multi-view scenario and present cross view hashing (CVH), which attempts to find hash functions that map similar objects to similar codewords over all the views so that inter-view and intraview similarities are both preserved. Gong and Lazebink [224] combine iterative quantization with CCA to exploit cross-modal embeddings for learning similarity preserving binary codes. Consequently, Zhen and Yang [225] present co-regularized hashing (CRH) for multi-modal data based on a boosted co-regularization framework. The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs, while the learning for multiple bits is performed via a boosting procedure. Later Zhu et al. [226] introduce linear cross-modal hashing (LCMH), which is adopts a \u201ctwo-stage\u201d strategy to learn cross-view hashing functions. The data within each modality are first encoded into a low-rank representation using the idea of anchor graph [236] and then hash functions for each modality are learned to map each modality\u2019s low-rank space into a shared Hamming space. Song et al. [237] introduce an inter-media hashing (IMH) model by jointly capturing inter-media and intra-media consistency.\nFurther, recent work has incorporated sparse coding into multimodal hashing. Wu et al. [184] present sparse multi-modal hashing by introducing dictionary learning into multi-view hashing, where the intra-modality and inter-modality similarities are modeled with a hypergraph. In particular, the learned dictionary for each modality is considered as the hash functions and the sparse code of each data point over its corresponding dictionary equals to the acquired hash code. Inspired by this effectiveness of incorporating sparse learning into multi-view hashing, Yu et al. [234] additionally exploit discriminative information when learning coupled dictionaries to make the shared dictionary space interpretable. Zhou et al. [238] present a latent semantic sparse hashing model by unifying sparse coding and matrix factorization.\nBased on taking both hashing function learning and quantization of hash codes into consideration, Wu et al. [239] present quantized correlation hashing (QCH). QCH adopts a joint learning scheme that consolidates the minimization of quantization loss and maximization of domain correlation into a single objective function. In this way, the quantizer and binary code learning are simultaneously optimized so that the quantizer is more fit for cross-modality data search."}, {"heading": "4 DEEP METHODS ON MULTI-VIEW REPRESENTATION LEARNING", "text": "Inspired by the success of deep neural networks [5, 6, 124], a variety of deep multi-view feature learning methods have been proposed to capture the high-level correlation between multi-view data. In this section, we continue to review the deep multi-view representation models from probabilistic and directly embedding perspectives."}, {"heading": "4.1 Probabilistic Models", "text": "Restricted Boltzmann Machines (RBM) [240] is an undirected graphical model that can learn the distribution of training data. The model consists of stochastic visible units v \u2208 {0, 1}dv and\nstochastic hidden units h \u2208 {0, 1}dh , which seeks to minimize the following energy function E : {0, 1}dv+dh \u2192 R :\nE(v,h; \u03b8) = \u2212 dv\u2211 i=1 dh\u2211 j=1 viWijhj \u2212 dv\u2211 i=1 bivi \u2212 dh\u2211 j=1 ajhj (49)\nwhere \u03b8 = {a,b,W} are the model parameters. Consequently, the joint distribution over the visible and hidden units is defined by:\nP (v,h; \u03b8) = 1\nZ(\u03b8) exp (\u2212E(v,h; \u03b8)) . (50)\nWhen considering modeling visible real-valued or sparse count data, this RBM can be easily extended to corresponding variants, e.g., Gaussian RBM [185] and replicated softmax RBM [241].\nA deep Boltzmann machine (DBM) is a generative network of stochastic binary units. It consists of a set of visible units v \u2208 {0, 1}dv , and a sequence of layers of hidden units h(1) \u2208 {0, 1}dh1 ,h(2) \u2208 {0, 1}dh2 , . . . ,h(L) \u2208 {0, 1}dhL . Here only connections between hidden units are allowed in adjacent layers. Let us take a DBM with two hidden layers for example. By ignoring bias terms, the energy of the joint configuration {v,h} is defined as\nE(v,h; \u03b8) = \u2212v>W (1)h(1) \u2212 h(1)W (2)h(2) (51)\nwhere h = {h(1),h(2)} represents the set of hidden units, and \u03b8 = {W (1),W (2)} are the model parameters that denote visibleto-hidden and hidden-to-hidden symmetric interaction terms. Further, this binary-to-binary DBM can also be easily extended to modeling dense real-valued or sparse count data.\nBy extending the setup of the DBM, Srivastava and Salakhutdinov [17] propose a deep multi-modal RBM to model the relationship between image and text. In particular, each data modality is modeled using a separate two-layer DBM and then an additional layer of binary hidden units on top of them is added to learn the shared representation.\nLet vm \u2208 Rdvm denote an image input and vt \u2208 Rdvt denote a text input. By ignoring bias terms on the hidden units for clarity, the distribution of vm in the image-specific two-layer DBM is\ngiven as follows: P (vm; \u03b8) = \u2211\nh(1),h(2)\nP (vm,h (1),h(2); \u03b8)\n= 1 Z(\u03b8) \u2211\nh(1),h(2)\nexp ( \u2212 dvm\u2211 i=1 (vmi \u2212 bi)2 2\u03c32i + dvm\u2211 i=1 dh1\u2211 j=1 vmi \u03c3i W (1) ij\n+ dh1\u2211 j=1 dh2\u2211 l=1 h (1) j W (2) jl h (2) l ) . (52)\nSimilarly, the text-specific two-layer DBM can also be defined by combining a replicated softmax model with a binary RBM.\nConsequently, the deep multi-modal DBM has been presented by combining the image-specific and text specific two-layer DBM with an additional layer of binary hidden units on top of them. The particular graphical model is shown in Figure 4. The joint distribution over the multi-modal input can be written as:\nP (vm,vt; \u03b8) = \u2211\nh (2) m ,h (2) t ,h (3)\nP ( h(2)m ,h (2) t ,h (3) )(\u2211\nh (1) m\nP ( vm,\nh(1)m ,h (2) m ))(\u2211 h\n(1) t\nP ( vt,h (1) t ,h (2) t )) (53)\nLike RBM, exact maximum likelihood learning in this model is also intractable, while efficient approximate learning can be implemented by using mean-field inference to estimate datadependent expectations, and an MCMC based stochastic approximation procedure to approximate the model\u2019s expected sufficient statistics [6].\nMulti-modal DBM has been widely used for multi-view representation learning [242\u2013244]. Hu et al. [243] employ the multimodal DBM to learn joint representation for predicting answers in cQA portal. Ge et al. [244] apply the multi-modal RBM to determining information trustworthiness, in which the learned joint representation denotes the consistent latent reasons that underline users\u2019 ratings from multiple sources. Pang and Ngo [245] propose to learn a joint density model for emotion prediction in usergenerated videos with a deep multi-modal Boltzmann machine. This multi-modal DBM is exploited to model the joint distribution over visual, auditory, and textual features. Here Gaussian RBM is used to model the distributions over the visual and auditory features, and replicated softmax topic model is applied for mining the textual features.\nFurther, Sohn et al. [246] investigate an improved multi-modal RBM framework via minimizing the variation of information between data modalities through the shared latent representation. Recently, Ehrlichet et al. [247] present a multi-task multi-modal RBM (MTM-RBM) approach for facial attribute classification. In particular, a multi-task RBM is proposed by extending the formulation of discriminative RBM [248] to account for multiple tasks. And then multi-task RBM is naturally extended to MTMRBM by combining a collection of unimodal MT-RBM, one for each visible modality. Unlike the typical multi-modal RBM, the learned joint feature representation of MTM-RBM enables interaction between different tasks so that it is suited for multiple attribute classification.\n4.2 Directly Deep Parametric Embedding from MultiView Inputs to Representation With the development of deep neural networks, many shallow multi-view embedding methods have been extended to deep ones,\nrespectively. In this section, we review the deep multi-view feature learning methods from the directly parametric embedding perspective."}, {"heading": "4.2.1 Multi-Modal Deep Autoencoders", "text": "Although Multi-modal RBMs achieve great success in learning a shared representation, there are still limitations with them, e.g., there is no explicit objective for the models to discover correlations across the modalities such that some hidden units are tuned only for one modality while others are tuned only for the other. Multi-modal deep autoencoders [18, 249\u2013251] gradually become good alternatives for learning a shared representation between modalities due to the sufficient flexibility of their objectives.\nInspired by denoising autoencoders [5], Ngiam et al. [18] propose to extract shared representations via training a bimodal deep autoencoder (Figure 5) using an augmented but noisy dataset with additional examples that have only a single modality as input. The key idea is to use greedy layer-wise training with an extension to RBMs with sparsity [252] followed by fine-tuning.\nFurther, Feng et al. [19] propose a correspondence autoencoder (Corr-AE) via constructing correlations between hidden representations of two uni-modal deep autoencoders. The details of the architecture of the basic Corr-AE is shown in Figure 6. As illustrated in Figure 6, this Corr-AE architecture consists of two subnetworks (each a basic deep autoencoder) that are connected by a predefined similarity measure on a specific coder layer. Each subnetwork in the Corr-AE serves for each modality.\nLet f(x;Wf ) and g(y;Wg) denote the mapping from the inputs {X,Y } to the code layers, respectively. \u03b8 = {Wf ,Wg} denotes the weight parameters in these two networks. Here the similarity measure between the i-th pair of image feature xi and the given text feature yi is defined as follows:\nC(xi, yi; \u03b8) = \u2016f(xi;Wf )\u2212 g(yi;Wg)\u201622 (54)\nwhere f and g are logistic activation functions. Consequently, the loss function on any pair of inputs is then defined as follows:\nL(xi, yi; \u03b8) =(1\u2212 \u03b1) (LI(xi, yi; \u03b8) + LT (xi, yi; \u03b8)) + \u03b1LC(xi, yi; \u03b8) (55)\nwhere\nLI(xi, yi; \u03b8) = \u2016xi \u2212 x\u0302i\u201622 LT (xi, yi; \u03b8) = \u2016yi \u2212 y\u0302i\u201622 LC(xi, yi; \u03b8) = C(xi, yi; \u03b8)\nLI and LT are the losses caused by data reconstruction errors for the given inputs of the two subnetworks, specifically image and text modality. LC is the correlation loss and \u03b1 is a parameter for trade-off between two groups of objectives. x\u0302i and y\u0302i are the reconstruction data from xi and yi respectively.\nOverall, optimizing the objective in Eq.(55) enables Corr-AE to learn similar representations from bimodal features. Besides, based on two other multi-modal autoencoders [18], Corr-AE is extended to two other correspondence deep models, called CorrCross-AE and Corr-Full-AE.\nConsequently, Silberer and Lapata train stacked multi-modal autoencoder with semi-supervised objective to learn grounded meaning representations. Wang et al. [253] propose an effective mapping mechanism based on stacked autoencoder for multimodal retrieval. In particular, the cross-modal mapping functions are learned by optimizing an objective function which captures both intra-modal and inter-modal semantic relationships of data from heterogeneous sources.\nFurther, based on CCA and deep autoencoders, Wang et al. [20] propose deep canonically correlated autoencoders that also consist of two autoencoders and optimize the combination of canonical correlations between the learned bottleneck representations and the reconstruction errors of the autoencoders. Intuitively, this is the same principle as that of Corr-AE. The difference between them relies on the different similarity measures.\nRecently, Rastegar et al. [254] suggest to exploit the cross weights between representations of modalities for gradually learning interactions of the modalities in a multi-modal deep autoencoder network. Theoretical analysis shows that considering these interactions in deep network manner (from low to high level) provides more intra-modality information. As opposed to the existing deep multi-modal autoencoders, this approach attempts to reconstruct the representation of each modality at a given level, with the representation of the other modalities in the previous layer."}, {"heading": "4.2.2 Deep Cross-View Embedding Models", "text": "Deep cross-view embedding models have become increasingly popular in the applications including cross-media retrieval [255, 256] and multi-modal distributional semantic learning [257, 258]. Frome et al. [255] propose a deep visual-semantic embedding model (DeViSE), which connects two deep neural networks by a cross-modal mapping. As shown in Figure 7, DeViSE is first\ninitialized with a pre-trained neural network language model [259] and a pre-trained deep visual-semantic model [124]. Consequently, a linear transformation is exploited to map the representation at the top of the core visual model into the learned dense vector representations by the neural language model.\nFollowing the setup of loss function in [212], DeViSE employs a combination of dot-product similarity and hinge rank loss so that the model has the ability of producing a higher dot-product similarity between the visual model output and the vector representation of the correct label than between the visual output and the other randomly chosen text terms. The per training example hinge rank loss is defined as follows:\u2211 j 6=label max [ 0,margin\u2212 ~tlabelM~v(image) + ~tjM~v(image)\n] (56)\nwhere ~v(image) is a column vector denoting the output of the top layer of the core visual network for the given image, M is the mapping matrix of the linear transformation layer, ~tlabel is a row vector denoting the learned embedding vector for the provided text label, and ~tj are the embeddings of the other text terms. This DeViSE model is trained by asynchronous stochastic gradient descent on a distributed computing platform [260].\nInspired by the success of DeViSE, Norouzi et al. [261] propose a convex combination of semantic embedding model (ConSE) for mapping images into continuous semantic embedding spaces. Unlike DeViSE, this ConSE model keeps the softmax layer of the convolutional net intact. Given a test image, ConSE simply runs the convolutional classifier and considers the convex combination of the semantic embedding vectors from the top T predictions as its corresponding semantic embedding vector. Further, Fang et al. [262] develop a deep multi-modal similarity model that learns two neural networks to map images and text fragments to a common vector representation.\nWith the development of multi-modal distributional semantic models [263\u2013265], deep cross-modal mapping is naturally exploited to learn the improved multimodal distributed semantic representation. Lazaridou et al. [258] introduce multimodal skipgram models to extend the skip-gram model of [259] by taking visual information into account. In this extension, for a subset of the target words, relevant visual evidence from natural images is presented together with the corpus contexts. In particular, the model is designed to encourage the propagation of visual information to all words via a deep cross-modal ranking so that it can improve image labeling and retrieval in the zero-shot setup,\nwhere the test concepts are never seen during model training. Further, Dong et al. [256] present Word2VisualVec, a deep neural network architecture that learns to predict a deep visual encoding of textual input, and thus enables cross-media retrieval in a visual space.\nIn addition, Xu et al. [266] propose a unified framework that jointly models video and the corresponding text sentence. In this joint architecture, the goal is to learn a function f(V) : V \u2192 T , where V represents the low-level features extracted from video, and T is the high-level text description of the video. A joint model P is designed to connect these two levels of information. It consists of three parts: compositional language model ML : T \u2192 Tf , deep video model MV : V \u2192 Vf , and a joint embedding model E(Vf , Tf ), such that\nP : MV (V ) \u2212\u2192 Vf \u2194 E(Vf , Tf )\u2194 Tf \u2190\u2212ML(T ) (57)\nwhere Vf and Tf are the output of the deep video model and the compositional language model, respectively. In this joint embedding model, the distance of the outputs of the deep video model and the compositional language model in the joint space is minimized to make them alignment.\nAlong this line of compositional cross-modal semantic representation learning, Yu et al. [267] present a unified deep neural network model called cross space mapping, in which the image and query are mapped to a common vector space via a convolution part and a query-embedding part, respectively. Jiang et al. [268] also introduce a deep cross-modal retrieval method, called deep compositional cross-modal learning to rank (C2MLR). C2MLR considers learning a multi-modal embedding from the perspective of optimizing a pairwise ranking problem while enhancing both local alignment and global alignment. In addition, Wei et al. [269] introduce a deep semantic matching method, in which two independent deep networks are learned to map image and text into a common semantic space with a high level abstraction. Wu et al. [270] consider learning multi-modal representation from the perspective of encoding the explicit/implicit relevance relationship between the vertices in the click graph, in which vertices are images/text queries and edges indicate the clicks between an image and a query."}, {"heading": "4.2.3 Deep Multi-Modal Hashing", "text": "Motivated by the recent advance of deep learning for multi-modal data [17, 18, 255], deep neural networks have gradually been exploited to learn hash functions. Kang et al. [271] introduce a deep multi-view hashing method in which each layer of hidden nodes consists of view-specific and shared hidden nodes to learn individual and shared hidden spaces from multiple views of data.\nConsequently, Zhuang et al. [272] propose a cross-media hashing approach based on a correspondence multi-modal neural network, referred as Cross-Media Neural Network Hashing (CMNNH). The network structure of CMNNH can be considered as a combination of two modality-specific neural networks with an additional correspondence layer as shown in Figure 8.\nDenote the two neural networks corresponding to multi-modal input {X,Y } as NNx and NNy , respectively. For each x \u2208 X (y \u2208 Y in a similar way), x is forwarded layer-by-layer through NNx to generate the representation of each layer. The lth layer takes hl as the input and uses a projection function to transform it to hl+1 in the next layer:\nhl+1 = f l(W lhl) (58)\nwhere hl and hl+1 are the feature representations in the lth and l + 1th hidden layer, respectively. W l is the projection matrix and f l is the activation function.\nFrom the perspective of the hash function Hx, it takes x as input, forwards x to the hash code layer, and outputs the kdimensional binary hash codes:\nHx(x) = sign(~x) (59)\nwhere ~x \u2208 Rk is a k-dimensional, real-value vector of the hash code layer, and it can be converted to a binary hash code by the sign function. The hash function Hy is formulated by analogy. However, the sign function is not differentiable, and thus is hard to optimize directly. By following the setup of most of the existing hashing methods [222, 235], the sign function can be removed at the hash function learning stage and added at the testing stage.\nFurther, to preserve the inter-modal pair-wise correspondence between NNx and NNy , a loss function is defined based on the least square error of pairwise inter-modal correspondence:\nL1(x, y) = 1\n2 ||~x \u2212 ~y||2F (60)\nBesides, to preserve the intra-modal discriminative capability, softmax regression function is employed as the loss function on the output layer as follows:\nL2(x, y, c) = KL(t\u0302x, t) + KL(t\u0302y, t) (61)\nwhere t is the class label for x and y, t\u0302x and t\u0302y are the estimated values by NNx and NNy , respectively, and KL(\u00b7) is the KLdivergence function. Further, the two loss functions for all the data points in X and Y are inegrated and the overall loss function of CMNNH is minimized as follows:\nJ = n\u2211 i=1 L1(xi, yi) + \u03bb n\u2211 i=1 L2(xi, yi, ti) (62)\nwhere \u03bb is a hyper-parameter to balance the two losses. The training of CMNNH is conducted by the classical back-propagation method.\nTo solve the redundancy problem of multi-modal hashing representation learning by deep models, Wang et al. [273] introduce a deep multi-modal hashing model with orthogonal regularization for mapping multi-modal data into a common hamming space. On one hand, this model captures intra-modality and inter-modality correlations to extract useful information from multi-modal data; on the other hand, to address the redundancy problem, orthogonal regularizers are imposed on the weighting matrices.\nRecently, Jiang and Li [274] propose a deep cross-modal hashing method (DCMH) by integrating feature learning and hashcode learning into the same framework. Unlike most existing methods that typically solve the discrete optimization problem of hash-code learning via relaxing the original discrete learning problem into a continuous learning problem, DCMH directly learns the discrete hash codes without relaxation. In addition, Cao et al. present a fully correlation autoencoder hashing method by extending the correspondence autoencoder by [19]."}, {"heading": "4.2.4 Multi-Modal Recurrent Neural Network", "text": "A recurrent neural network (RNN) [275] is a neural network which processes a variable-length sequence x = (x1, . . . , xT ) through hidden state representation h. At each time step t, the hidden state ht of the RNN is estimated by\nht = f (ht\u22121, xt) (63)\nwhere f is a non-linear activation function and is selected based on the requirement of data modeling. For example, a simple case may be a common element-wise logistic sigmoid function and a complex case may be a long short-term memory (LSTM) unit [276].\nAn RNN is known as its ability of learning a probability distribution over a sequence by being trained to predict the next symbol in a sequence. In this training, the prediction at each time step t is decided by the conditional distribution p(xt|xt\u22121, . . . , x1). For example, a multinomial distribution can be learned as output with a softmax activation function\np(xt,j = 1|xt\u22121, . . . , x1) = exp(wjht)\u2211K\nj\u2032=1 exp(wj\u2032ht) (64)\nwhere j = 1, . . . ,K denotes the possible symbol components and wj are the corresponding rows of a weight matrix W . Further, based the above probabilities, the probability of the sequence x can be computed as\np(x) = T\u220f t=1 p(xt|xt\u22121, . . . , x1). (65)\nWith this learned distribution, it is straightforward to generate a new sequence by iteratively generating a symbol at each time step.\nCho et al. [277] propose a RNN encoder-decoder model by exploiting RNN to connect multi-modal sequence. As shown in Figure 9, this neural network first encodes a variable-length source sequence into a fixed-length vector representation and then decodes this fixed-length vector representation back into a variablelength target sequence. In fact, it is a general method to learn the conditional distribution over an output sequence conditioned on another input sequence, e.g., p(y1, . . . , yT \u2032 |x1, . . . , xT ), where the input and output sequence lengths T and T \u2032 can be different. In particular, the encoder of the proposed model is an RNN which sequentially encodes each symbol of an input sequence x into the corresponding hidden state according to Eq.(63). After reading the end of the input sequence, a summary hidden state of the whole source sequence c is acquired. The decoder of the proposed model is another RNN which is exploited to generate the target sequence by predicting the next symbol yt with the hidden state ht. Based on the recurrent property, both yt and ht are also conditioned on yt\u22121 and on the summary c of the input sequence. Thus, the hidden state of the decoder at time t is computed by,\nht = f(ht\u22121, yt\u22121, c) (66)\nand the conditional distribution of the next symbol is\np(yt|yt\u22121, yt\u22122, . . . , y1, c) = g(ht, yt\u22121, c) (67)\nwhere g is an activation function and produces valid probabilities with a softmax. The main idea of the RNN-based EncoderDecoder framework can be summarized by jointly training two RNNs to maximize the conditional log-likelihood\nmax \u03b8\n1\nN N\u2211 n=1 logp\u03b8(yn|xn) (68)\nwhere \u03b8 is the set of the model parameters and each pair (xn,yn) consists of an input sequence and an output sequence from the training set. The model parameters can be estimated by a gradientbased algorithm.\nFurther, Sutskever et al. [278] also present a general endto-end approach for multi-modal sequence to sequence learning based on deep LSTM networks, which is very useful for learning problems with long range temporal dependencies [276, 279]. The goal of this method is also to estimate the conditional probability p(y1, . . . , yT \u2032 |x1, . . . , xT ). Similar to [277], the conditional probability is computed by first obtaining the fixed dimensional representation v of the input sequence (x1, . . . , xT ) with the encoding LSTM-based networks, and then computing the probability of y1, . . . , yT \u2032 with the decoding LSTM-based networks whose initial hidden state is set to the representation v of x1, . . . , xT :\np(y1, . . . , yT \u2032 |x1, . . . , xT ) = T \u2032\u220f\nt=1\np(yt|v, y1, . . . , yt\u22121) (69)\nwhere each p(yt|v, y1, . . . , yt\u22121) distribution is represented with a softmax over all the words in the vocabulary.\nBesides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283]. Karpathy and Li [21] propose a multi-modal recurrent neural network architecture to generate new descriptions of image regions. Chen and Zitnick [284] explore the bi-directional mapping between images and their sentencebased descriptions with RNNs. Venugopalan et al. [282] introduce an end-to-end sequence model to generate captions for videos.\nBy applying attention mechanism [285] to visual recognition [286, 287], Xu et al. [288] introduce an attention based multimodal RNN model, which trains the multi-modal RNN in a deterministic manner using the standard back propagation. In particular, it incorporates a form of attention with two variants: a \u201dhard\u201d attention mechanism and a \u201dsoft\u201d attention mechanism. The advantage of the proposed model lies in attending to salient part of an image while generating its caption."}, {"heading": "5 MULTI-VIEW REPRESENTATION LEARNING AS MANIFOLD ALIGNMENT", "text": "Another important perspective on multi-view representation learning is based on the manifold alignment. The key idea underlying this approach is to embed inputs from different domains to a new latent common space, simultaneously preserving topology of each input domain. Its premise is the manifold hypothesis, according to which high-dimensional real-world data are expected to concentrate in the vicinity of a low-dimensional manifold, embedded in high dimensional input space. In particular, this prior is well suited for datasets such as images, sounds and texts, as this information are not natural signals. Since most data sources can be modeled by manifolds, manifold alignment can be used to align the underlying structures across different data views. With this perspective, the multi-view representation learning task can be seen as finding the relationship among the structures of manifolds from the different views of data. The associated multi-view embeddings being learned can be associated with the intrinsic coordinate systems on the embedded manifolds. Further, manifold alignment makes full use of paired and unpaired data and this capability is particularly useful for multi-view feature learning in certain applications, where the paired multi-view instances are limited.\nOne of the pioneering efforts in manifold alignment with given coordinates is the semi-supervised alignment by Ham et al. [24]. Given certain labeled samples by the ordinal index l, semisupervised alignment aims to find a map defined on the vertices of the graph f : V 7\u2192 R that matches known target values for the labeled vertices. This can be solved by directly finding arg minf |fi \u2212 si| (i \u2208 l) where s is the vector of target value. Since only a small number of labeled examples are provided, it is important to exploit manifold structure in the data when constructing the mapping function. In particular, graph Laplacian matrix L provides this structural information. The semi-supervised alignment problem can be solved by minimizing the following objective function:\nC(f) = \u2211 i \u00b5|fi \u2212 si|2 + fTLf (70)\nwhere the first term is the loss function, and the second term enforces smoothness along the manifold. The optimum mapping function f is easily obtained by a linear transform.\nThis semi-supervised alignment is similar to manifold ranking [290], which learns to rank on data manifolds. They both take advantage of the manifold learning to do alignment between different types of data. To exploit the high level geometry structure embedded in the data samples, Mao et al. [289] extend the original semi-supervised alignment and proposed a method called parallel field alignment retrieval (PFAR), which investigates alignment framework from the perspective of parallel vector fields. An example of parallel vector alignment is shown in Figure 10.\nIn addition, Ham et al. [24] also introduce manifold alignment with pairwise correspondence, which builds connections between multiple data sets by aligning their underlying manifolds. In particular, multi-view input data sets {X,Y } have subsets Xl and Yl which are in pairwise alignment by the indices xi \u2194 yi, (i \u2208 l). Then we intend to determine how to match the remaining examples using aligned manifold embeddings. Considering that embedding coordinates for each data set should take similar values for corresponding pairs, the multi-view manifold embedding can be defined by generalizing the single graph embedding algorithm as follows:\nC(f, g) = \u00b5 \u2211 i\u2208l |fi \u2212 gi|+ fTLxf + gTLyg (71)\nwhere f and g denote real-valued functions defined on the respective graphs of X and Y , Lx and Ly are the graph Laplacian matrices of X and Y . The first term penalizes the difference between f and g on the corresponding vertices, and the second and third terms impose smoothness constraints on f and g based on the respective graphs.\nThis graph algorithm is able to robustly align the underlying manifold structure across multi-view data sets. Even with a small number of paired samples provided, the algorithm is capable of estimating a common low-dimensional embedding space which can be used in cross-view retrieval task. The main concern about this algorithm is the computational cost, which lies in finding the spectral decomposition of a large matrix. Methods for calculating eigenvectors of large sparse matrices can be employed to speed up the computation of the embeddings.\nIn addition, since the above manifold alignment is defined only on the known data points and it is hard to handle the new test points, Lafon et al. [291] propose a two-step manifold alignment to solve this problem. Diffusion maps are first used to embed the nodes of the graphs corresponding to the aligned multi-modal data sets, followed by an affine matching to align the obtained clouds of points. Further, Wang and Mahadevan [292] also introduce a twostage approach for manifold alignment using procrustes analysis.\nIn the first stage, it uses a standard dimensionality reduction method to map the data sets to low dimensional spaces which reflect their intrinsic geometries. In the second stage, procrustes analysis is applied to align the two low dimensional embeddings of the data sets based on a number of landmark points. This twostage based manifold alignment method obtains mappings that are defined everywhere rather than only training data points. Consequently, Wang and Mahadevan [293] also propose a manifold alignment based approach for heterogeneous domain adaptation."}, {"heading": "6 CONCLUSION", "text": "Multi-view representation learning has attracted much attention in machine learning and data mining areas. This paper first reviews the root methods and theories on multi-view representation learning, especially on canonical correlation analysis (CCA) and its extensions. And then we investigate the advances of multiview representation learning that ranges from shallow methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to deep methods including multi-modal restricted Boltzmann machines, multi-modal autoencoders, and multi-modal recurrent neural networks. Further, we also provide an important perspective for multi-view representation learning from manifold alignment. This survey aims to provide an insightful picture of the theoretical basis and the current development in the field of multi-view representation learning and to help researchers find the most appropriate methodologies for particular applications."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported in part by the National Basic Research Program of China (2012CB316400), Zhejiang University \u2014 Alibaba Financial Joint lab, and Zhejiang Provincial Engineering Center on Media Data Cloud Processing and Analysis. ZZ is also supported in part by US NSF (IIS-0812114, CCF-1017828)."}], "references": [{"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, vol. 28, no. 3/4, pp. 321\u2013377, 1936.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1936}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1\u201348, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S.R. Szedmak", "J.R. Shawe-taylor"], "venue": "Neural Comput., vol. 16, no. 12, pp. 2639\u20132664, Dec. 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "A survey of multi-view machine learning", "author": ["S. Sun"], "venue": "Neural Computing and Applications, vol. 23, no. 7-8, pp. 2031\u2013 2038, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "ICML, 2008, pp. 1096\u20131103.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep boltzmann machines.", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "in AISTATS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1828}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J.A. Bilmes", "K. Livescu"], "venue": "ICML, 2013, pp. 1247\u2013 1255.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "The missing link - A probabilistic model of document content and hypertext connectivity", "author": ["D.A. Cohn", "T. Hofmann"], "venue": "NIPS, 2000, pp. 430\u2013436.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. de Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1107\u20131135, Mar. 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling annotated data", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "SIGIR, 2003, pp. 127\u2013134.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Factorized latent spaces with structured sparsity", "author": ["Y. Jia", "M. Salzmann", "T. Darrell"], "venue": "NIPS, 2010, pp. 982\u2013 990.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust multimodal dictionary learning", "author": ["T. Cao", "V. Jojic", "S. Modla", "D. Powell", "K. Czymmek", "M. Niethammer"], "venue": "MICCAI, 2013, pp. 259\u2013266.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiview hessian discriminative sparse coding for image annotation", "author": ["W. Liu", "D. Tao", "J. Cheng", "Y. Tang"], "venue": "Computer Vision and Image Understanding, vol. 118, pp. 50\u201360, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining associated text and images with dual-wing harmoniums", "author": ["E.P. Xing", "R. Yan", "A.G. Hauptmann"], "venue": "UAI \u201905, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence, Edinburgh, Scotland, July 26-29, 2005, 2005, pp. 633\u2013641.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Predictive subspace learning for multi-view data: a large margin approach", "author": ["N. Chen", "J. Zhu", "E.P. Xing"], "venue": "NIPS, 2010, pp. 361\u2013369.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "NIPS, 2012, pp. 2231\u2013 2239.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "ICML, 2011, pp. 689\u2013 696.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Cross-modal retrieval with correspondence autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "ACM Multimedia, 2014, pp. 7\u201316.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "On deep multi-view representation learning", "author": ["W. Wang", "R. Arora", "K. Livescu", "J.A. Bilmes"], "venue": "ICML, 2015, pp. 1083\u20131092.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F. Li"], "venue": "CoRR, vol. abs/1412.2306, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "arXiv preprint arXiv:1412.6632, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR, 2015, pp. 2625\u2013 2634.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Semisupervised alignment of manifolds", "author": ["J. Ham", "D. Lee", "L. Saul"], "venue": "10th International Workshop on Artificial Intelligence and Statistics, 2005, pp. 120\u2013127.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G.R. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "ACM Multimedia, 2010, pp. 251\u2013260.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Pixels that sound.", "author": ["E. Kidron", "Y.Y. Schechner", "M. Elad"], "venue": "IEEE Computer Society,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "A least squares formulation for canonical correlation analysis", "author": ["L. Sun", "S. Ji", "J. Ye"], "venue": "ICML, 2008, pp. 1024\u2013 1031.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-view dimensionality reduction via canonical correlation analysis", "author": ["D.P. Foster", "R. Johnson", "T. Zhang"], "venue": "Tech. Rep., 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "A scalable two-stage approach for a class of dimensionality reduction techniques", "author": ["L. Sun", "B. Ceran", "J. Ye"], "venue": "KDD, 2010, pp. 313\u2013322.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["H. Avron", "C. Boutsidis", "S. Toledo", "A. Zouzias"], "venue": "ICML, 2013, pp. 347\u2013355.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Correlation clustering for learning mixtures of canonical correlation models", "author": ["X.Z. Fern", "C.E. Brodley", "M.A. Friedl"], "venue": "SDM, 2005, pp. 439\u2013448.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Correlational spectral clustering", "author": ["M.B. Blaschko", "C.H. Lampert"], "venue": "CVPR, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "ICML, 2009, pp. 129\u2013136.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["S.M. Kakade", "D.P. Foster"], "venue": "COLT, 2007, pp. 82\u201396.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Correlated random features for fast semi-supervised learning", "author": ["B. McWilliams", "D. Balduzzi", "J.M. Buhmann"], "venue": "NIPS, 2013, pp. 440\u2013448.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["P.S. Dhillon", "D.P. Foster", "L.H. Ungar"], "venue": "NIPS, 2011, pp. 199\u2013207.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Using CCA to improve CCA: A new spectral method for estimating vector models of words", "author": ["P.S. Dhillon", "J. Rodu", "D.P. Foster", "L.H. Ungar"], "venue": "ICML, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "A multi-view embedding space for modeling internet images, tags, and their semantics", "author": ["Y. Gong", "Q. Ke", "M. Isard", "S. Lazebnik"], "venue": "International Journal of Computer Vision, vol. 106, no. 2, pp. 210\u2013233, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative learning and recognition of image set classes using canonical corre-  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  21 lations", "author": ["T. Kim", "J. Kittler", "R. Cipolla"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 6, pp. 1005\u20131018, 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-label output codes using canonical correlation analysis.", "author": ["Y. Zhang", "J.G. Schneider"], "venue": "in AISTATS,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Discriminant learning through multiple principal angles for visual recognition", "author": ["Y. Su", "Y. Fu", "X. Gao", "Q. Tian"], "venue": "IEEE Trans. Image Processing, vol. 21, no. 3, pp. 1381\u2013 1390, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "The canonical correlations of matrix pairs and their numerical computation", "author": ["G.H. Golub", "H. Zha"], "venue": "Stanford, CA, USA, Tech. Rep., 1992.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1992}, {"title": "Canonical analysis of several sets of variables", "author": ["J.R. Kettenring"], "venue": "Biometrika, vol. 58, no. 3, pp. 433\u2013451, 1971.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1971}, {"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["H. Avron", "C. Boutsidis", "S. Toledo", "A. Zouzias"], "venue": "SIAM J. Scientific Computing, vol. 36, no. 5, 2014.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved analysis of the subsampled randomized hadamard transform", "author": ["J.A. Tropp"], "venue": "CoRR, vol. abs/1011.1595, 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "large scale canonical correlation analysis with iterative least squares", "author": ["Y. Lu", "D.P. Foster"], "venue": "NIPS, 2014, pp. 91\u201399.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "50th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2012, Allerton Park & Retreat Center, Monticello, IL, USA, October 1-5, 2012, 2012, pp. 861\u2013868.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Z. Ma", "Y. Lu", "D.P. Foster"], "venue": "ICML, 2015, pp. 169\u2013178.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Globally convergent stochastic optimization for canonical correlation analysis", "author": ["W. Wang", "J. Wang", "N. Srebro"], "venue": "CoRR, vol. abs/1604.01870, 2016.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1870}, {"title": "Efficient algorithms for large-scale generalized eigenvector computation and canonical correlation analysis", "author": ["R. Ge", "C. Jin", "S.M. Kakade", "P. Netrapalli", "A. Sidford"], "venue": "CoRR, vol. abs/1604.03930, 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Department of Statistics, University of California, Berkeley, Tech. Rep., 2005.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2005}, {"title": "The maximum-likelihood solution in interbattery factor analysis", "author": ["M.W. Browne"], "venue": "British Journal of Mathematical and Statistical Psychology, vol. 32, no. 1, pp. 75\u201386, 1979.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1979}, {"title": "Robust probabilistic projections", "author": ["C. Archambeau", "N. Delannay", "M. Verleysen"], "venue": "(ICML, 2006, pp. 33\u201340.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Stochastic processes for canonical correlation analysis", "author": ["C. Fyfe", "G. Leen"], "venue": "ESANN, 2006, pp. 245\u2013250.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2006}, {"title": "Local dependent components", "author": ["A. Klami", "S. Kaski"], "venue": "ICML, 2007, pp. 425\u2013432.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2007}, {"title": "Variational bayesian approach to canonical correlation analysis", "author": ["C. Wang"], "venue": "IEEE Trans. Neural Networks, vol. 18, no. 3, pp. 905\u2013910, 2007.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "Variational bayesian mixture of robust CCA models", "author": ["J. Viinikanoja", "A. Klami", "S. Kaski"], "venue": "ECML PKDD, 2010, pp. 370\u2013385.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "A generalization of principal components analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta", "R.E. Schapire"], "venue": "NIPS, 2001, pp. 617\u2013624.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2001}, {"title": "Bayesian  exponential family PCA", "author": ["S. Mohamed", "K.A. Heller", "Z. Ghahramani"], "venue": "NIPS, 2008, pp. 1089\u20131096.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian exponential family projections for coupled data sources", "author": ["A. Klami", "S. Virtanen", "S. Kaski"], "venue": "UAI, 2010, pp. 286\u2013293.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse bayesian multi-task learning", "author": ["C. Archambeau", "S. Guo", "O. Zoeter"], "venue": "NIPS, 2011, pp. 1755\u20131763.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust bayesian matrix factorisation", "author": ["B. Lakshminarayanan", "G. Bouchard", "C. Archambeau"], "venue": "AISTATS, 2011, pp. 425\u2013433.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic partial canonical correlation analysis", "author": ["Y. Mukuta", "T. Harada"], "venue": "ICML, 2014, pp. 1449\u20131457.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Partial canonical correlations", "author": ["RAO B.R."], "venue": "vol. 20, no. 2, 1969, pp. 211\u2013219.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1969}, {"title": "Probabilistic semi-canonical correlation analysis", "author": ["C. Kamada", "A. Kanezaki", "T. Harada"], "venue": "ACM Multimedia, 2015, pp. 1131\u20131134.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-way analysis of high-dimensional collinear data", "author": ["I. Huopaniemi", "T. Suvitaival", "J. Nikkil\u00e4", "M. Oresic", "S. Kaski"], "venue": "Data Min. Knowl. Discov., vol. 19, no. 2, pp. 261\u2013 276, 2009.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2009}, {"title": "Multivariate multi-way analysis of multi-source data", "author": ["\u2014\u2014"], "venue": "Bioinformatics, vol. 26, no. 12, pp. 391\u2013398, 2010.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society (Series B), vol. 58, pp. 267\u2013288, 1996.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1996}, {"title": "Sparse canonical correlation analysis", "author": ["D.R. Hardoon", "J. Shawe-Taylor"], "venue": "Department of Computer Science, University College London, Tech. Rep., 2007.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2007}, {"title": "Quantifying the association between gene expressions and DNA-markers by penalized canonical correlation analysis.", "author": ["S. Waaijenborg", "P.C. Verselewel de Witt Hamer", "A.H. Zwinderman"], "venue": "Statistical applications in genetics and molecular biology,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2008}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics, vol. 32, pp. 407\u2013499, 2004.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2004}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, Dec. 2006.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparse eigen methods by D.C. programming", "author": ["B.K. Sriperumbudur", "D.A. Torres", "G.R.G. Lanckriet"], "venue": "ICML, 2007, pp. 831\u2013838.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2007}, {"title": "Full regularization path for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F.R. Bach", "L.E. Ghaoui"], "venue": "ICML, 2007, pp. 177\u2013184.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2007}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["A. d\u2019Aspremont", "L.E. Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet"], "venue": "SIAM Review, vol. 49, no. 3, pp. 434\u2013448, 2007.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2007}, {"title": "Finding musically meaningful words by sparse cca", "author": ["D. Torres", "D. Turnbull", "L. Barrington", "B. Sriperumbudur", "G. Lanckriet"], "venue": "NIPS WMBC \u201907, December 2007.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2007}, {"title": "A greedy approach to sparse canonical correlation analysis", "author": ["A. Wiesel", "M. Kliger", "III A.O. Hero"], "venue": "ArXiv e-prints, 2008.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2008}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D.M. Witten", "T. Hastie", "R. Tibshirani"], "venue": "Biostatistics, 2009.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2009}, {"title": "Structured sparse  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  22 canonical correlation analysis", "author": ["X. Chen", "H. Liu", "J.G. Carbonell"], "venue": "AISTATS, 2012, pp. 199\u2013 207.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "Two methods for sparsifying probabilistic canonical correlation analysis", "author": ["C. Fyfe", "G. Leen"], "venue": "ICONIP, 2006, pp. 361\u2013370.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparse probabilistic projections", "author": ["C. Archambeau", "F.R. Bach"], "venue": "NIPS, 2008, pp. 73\u201380.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-label prediction via sparse infinite CCA", "author": ["P. Rai", "H.D. III"], "venue": "NIPS, 2009, pp. 1518\u20131526.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimating image bases for visual image reconstruction from human brain activity", "author": ["Y. Fujiwara", "Y. Miyawaki", "Y. Kamitani"], "venue": "NIPS, 2009, pp. 576\u2013584.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2009}, {"title": "A nonparametric bayesian approach to modeling overlapping clusters", "author": ["K.A. Heller", "Z. Ghahramani"], "venue": "AISTATS, 2007, pp. 187\u2013194.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2007}, {"title": "An inter-battery method of factor analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika, vol. 23, no. 2, pp. 111\u2013136, 1958.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 1958}, {"title": "Bayesian canonical correlation analysis", "author": ["A. Klami", "S. Virtanen", "S. Kaski"], "venue": "Journal of Machine Learning Research, vol. 14, no. 1, pp. 965\u20131003, 2013.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2013}, {"title": "Three common factor models for groups of variables", "author": ["R. McDonald"], "venue": "Psychometrika, vol. 37, no. 1, pp. 173\u2013178, 1970.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 1970}, {"title": "Factor analysis of multiple batteries by maximum likelihood", "author": ["M.W. Browne"], "venue": "British Journal of Mathematical and Statistical Psychology, vol. 33, pp. 184\u2013199, 1979.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 1979}, {"title": "Group factor analysis", "author": ["A. Klami", "S. Virtanen", "E. Lepp\u00e4aho", "S. Kaski"], "venue": "IEEE Trans. Neural Netw. Learning Syst., vol. 26, no. 9, pp. 2136\u20132147, 2015.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on multi-view learning", "author": ["C. Xu", "D. Tao", "C. Xu"], "venue": "arXiv preprint arXiv:1304.5634, 2013.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P.L. Lai", "C. Fyfe"], "venue": "IJCNN (4), 2000, p. 614.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2000}, {"title": "A kernel method for canonical correlation analysis", "author": ["S. Akaho"], "venue": "International Meeting of the Psychometric Society, 2001.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2001}, {"title": "Nonlinear feature extraction using generalized canonical correlation analysis", "author": ["T. Melzer", "M. Reiter", "H. Bischof"], "venue": "ICANN, 2001, pp. 353\u2013360.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2001}, {"title": "Connecting modalities: Semisupervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "F. Li"], "venue": "CVPR, 2010, pp. 966\u2013973.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning the relative importance of objects from tagged images for retrieval and crossmodal search.", "author": ["S.J. Hwang", "K. Grauman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2012}, {"title": "Extraction of correlated gene clusters from multiple genomic data by generalized kernel canonical correlation analysis", "author": ["Y. Yamanishi", "J. Vert", "A. Nakaya", "M. Kanehisa"], "venue": "ICISMB, 2003, pp. 323\u2013330.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised kernel canonical correlation analysis with application to human fmri", "author": ["M.B. Blaschko", "J.A. Shelton", "A. Bartels", "C.H. Lampert", "A. Gretton"], "venue": "Pattern Recognition Letters, vol. 32, no. 11, pp. 1572\u20131583, 2011.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting tag and word correlations for improved webpage clustering", "author": ["A. Trivedi", "P. Rai", "S.L. DuVall", "III H. Daum\u00e9"], "venue": "Proceedings of the 2Nd International Workshop on Search and Mining User-generated Contents, ser. SMUC \u201910, 2010, pp. 3\u201312.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2010}, {"title": "Kernel CCA for multi-view learning of acoustic features using articulatory measure-  ments", "author": ["R. Arora", "K. Livescu"], "venue": "MLSLP, 2012, pp. 34\u201337.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-view cca-based acoustic features for phonetic recognition across speakers and domains", "author": ["\u2014\u2014"], "venue": "ICASSP, 2013, pp. 7135\u20137139.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2013}, {"title": "The kernel mutual information", "author": ["A. Gretton", "R. Herbrich", "A.J. Smola"], "venue": "ICASSP, 2003, pp. 880\u2013884.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2003}, {"title": "Kernel methods for measuring independence", "author": ["A. Gretton", "R. Herbrich", "A.J. Smola", "O. Bousquet", "B. Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 2075\u20132129, 2005.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2001}, {"title": "The geometry of kernel canonical correlation analysis", "author": ["M. Kuss", "T. Graepel"], "venue": "Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany, Tech. Rep. 108, may 2003.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2003}, {"title": "Statistical consistency of kernel canonical correlation analysis", "author": ["K. Fukumizu", "F.R. Bach", "A. Gretton"], "venue": "Journal of Machine Learning Research, vol. 8, pp. 361\u2013383, 2007.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2007}, {"title": "Convergence analysis of kernel canonical correlation analysis: theory and practice", "author": ["D.R. Hardoon", "J. Shawe-Taylor"], "venue": "Machine Learning, vol. 74, no. 1, pp. 23\u201338, 2009.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2009}, {"title": "Convergence rate of kernel canonical correlation analysis", "author": ["J. Cai", "H. Sun"], "venue": "Science China Mathematics, vol. 54, no. 10, pp. 2161\u20132170, 2011.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental singular value decomposition of uncertain data with missing values", "author": ["M. Brand"], "venue": "ECCV, 2002, pp. 707\u2013720.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2002}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M.W. Seeger"], "venue": "NIPS, 2000, pp. 682\u2013688.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2000}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["T. Yang", "Y. Li", "M. Mahdavi", "R. Jin", "Z. Zhou"], "venue": "NIPS, 2012, pp. 485\u2013493.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2012}, {"title": "Fastfood - computing hilbert space expansions in loglinear time", "author": ["Q.V. Le", "T. Sarl\u00f3s", "A.J. Smola"], "venue": "ICML, 2013, pp. 244\u2013252.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2013}, {"title": "Randomized nonlinear component analysis", "author": ["D. Lopez-Paz", "S. Sra", "A.J. Smola", "Z. Ghahramani", "B. Sch\u00f6lkopf"], "venue": "ICML, 2014, pp. 1359\u20131367.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale approximate kernel canonical correlation analysis", "author": ["W. Wang", "K. Livescu"], "venue": "CoRR, vol. abs/1511.04773, 2015.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2015}, {"title": "Using KCCA for japaneseenglish cross-language information retrieval and document classification", "author": ["Y. Li", "J. Shawe-Taylor"], "venue": "J. Intell. Inf. Syst., vol. 27, no. 2, pp. 117\u2013 133, 2006.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2006}, {"title": "Two view learning: Svm-2k, theory and practice", "author": ["J.D.R. Farquhar", "D.R. Hardoon", "H. Meng", "J. Shawe- Taylor", "S. Szedm\u00e1k"], "venue": "NIPS, 2005, pp. 355\u2013362.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2005}, {"title": "A correlation approach for automatic image annotation", "author": ["D.R. Hardoon", "C. Saunders", "S. Szedm\u00e1k", "J. Shawe- Taylor"], "venue": "ADMA, 2006, pp. 681\u2013692.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2006}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "J. Artif. Intell. Res. (JAIR), vol. 47, pp. 853\u2013899, 2013.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2013}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  23 Research, vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-organizing neural network that discovers surfaces in random-dot stereograms", "author": ["S. Becker", "G.E. Hinton"], "venue": "Nature, vol. 355, no. 6356, pp. 161\u2013163, 1992.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 1992}, {"title": "Mutual information maximization: Models of cortical self-organization.", "author": ["S. Becker"], "venue": "Network : Computation in Neural Systems,", "citeRegEx": "120", "shortCiteRegEx": "120", "year": 1996}, {"title": "Canonical correlation analysis using artificial neural networks", "author": ["P.L. Lai", "C. Fyfe"], "venue": "ESANN, 1998, pp. 363\u2013368.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 1998}, {"title": "A neural implementation of canonical correlation analysis", "author": ["\u2014\u2014"], "venue": "Neural Networks, vol. 12, no. 10, pp. 1391\u20131397, 1999.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 1999}, {"title": "Nonlinear canonical correlation analysis by neural networks", "author": ["W.W. Hsieh"], "venue": "Neural Networks, vol. 13, no. 10, pp. 1095\u20131105, 2000.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2000}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012, pp. 1097\u20131105.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["W. Wang", "R. Arora", "K. Livescu", "J.A. Bilmes"], "venue": "ICASSP, 2015, pp. 4590\u20134594.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["A. Lu", "W. Wang", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "HLT-NAACL, 2015, pp. 250\u2013256.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic optimization for deep CCA via nonlinear orthogonal iterations", "author": ["W. Wang", "R. Arora", "K. Livescu", "N. Srebro"], "venue": "Allerton, 2015, pp. 688\u2013695.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep correlation for matching images and text", "author": ["F. Yan", "K. Mikolajczyk"], "venue": "CVPR, 2015, pp. 3441\u20133450.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "UAI, 1999, pp. 289\u2013296.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 1999}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science, vol. 41, no. 6, p. 391, 1990.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning to probabilistically identify authoritative documents", "author": ["D. Cohn", "H. Chang"], "venue": "ICML, 2000, pp. 167\u2013 174.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2000}, {"title": "Plsa-based image autoannotation: Constraining the latent space", "author": ["F. Monay", "D. Gatica-Perez"], "venue": "ACM Multimedia, 2004, pp. 348\u2013351.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2004}, {"title": "A probabilistic semantic model for image annotation and multi-modal image retrieva", "author": ["R. Zhang", "Z.M. Zhang", "M. Li", "W. Ma", "H. Zhang"], "venue": "ICCV, 2005, pp. 846\u2013851.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2005}, {"title": "Modeling semantic aspects for cross-media image indexing", "author": ["F. Monay", "D. Gatica-Perez"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 10, pp. 1802\u20131817, 2007.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 1802}, {"title": "Semisupervised topic modeling for image annotation", "author": ["Y. Shao", "Y. Zhou", "X. He", "D. Cai", "H. Bao"], "venue": "ACM Multimedia, 2009, pp. 521\u2013524.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2009}, {"title": "Multilayer plsa for multimodal image retrieval", "author": ["R. Lienhart", "S. Romberg", "E. H\u00f6rster"], "venue": "ACM CIVR, 2009.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 2009}, {"title": "Heterogeneous transfer learning for image clustering via the socialweb", "author": ["Q. Yang", "Y. Chen", "G. Xue", "W. Dai", "Y. Yu"], "venue": "ACL, 2009, pp. 1\u20139.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2009}, {"title": "A feature-word-topic model for image annotation", "author": ["C. Nguyen", "N. Kaothanthong", "X.H. Phan", "T. Tokuyama"], "venue": "ACM CIKM, 2010, pp. 1481\u20131484.", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic image annotation with continuous PLSA", "author": ["Z. Li", "Z. Shi", "X. Liu", "Z. Shi"], "venue": "ICASSP, 2010, pp. 806\u2013809.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi modal semantic indexing for image retrieval", "author": ["C. Pulla", "C.V. Jawahar"], "venue": "ACM CIVR, 2010, pp. 342\u2013349.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-feature plsa for combining visual features in image annotation", "author": ["R. Zhang", "L. Zhang", "X. Wang", "L. Guan"], "venue": "ACM Multimedia, 2011, pp. 1513\u20131516.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association, vol. 101, 2004.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-modal hierarchical dirichlet process model for predicting image annotation and image-object label correspondence", "author": ["O. Yakhnenko", "V. Honavar"], "venue": "SDM, 2009, pp. 283\u2013293.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2009}, {"title": "Gibbs sampling methods for stick-breaking priors", "author": ["H. Ishwaran", "L.F. James"], "venue": "Journal of the American Statistical Association, vol. 96, no. 453, pp. 161\u2013173, 2001.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2001}, {"title": "Mining partially annotated images", "author": ["Z. Qi", "M. Yang", "Z.M. Zhang", "Z. Zhang"], "venue": "KDD, 2011, pp. 1199\u20131207.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2011}, {"title": "Large margin learning of upstream scene understanding models", "author": ["J. Zhu", "L. Li", "F. Li", "E.P. Xing"], "venue": "NIPS, 2010, pp. 2586\u20132594.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2010}, {"title": "Simultaneous image classification and annotation", "author": ["C. Wang", "D.M. Blei", "F. Li"], "venue": "CVPR, 2009, pp. 1903\u2013 1910.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2009}, {"title": "Supervised topic models", "author": ["D.M. Blei", "J.D. McAuliffe"], "venue": "NIPS, 2007, pp. 121\u2013128.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2007}, {"title": "What, where and who? classifying events by scene and object recognition", "author": ["L. Li", "F. Li"], "venue": "ICCV, 2007, pp. 1\u20138.", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2007}, {"title": "Spatially coherent latent topic model for concurrent segmentation and classification of objects and scenes", "author": ["L. Cao", "F. Li"], "venue": "ICCV, 2007, pp. 1\u20138.", "citeRegEx": "151", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards total scene understanding: Classification, annotation and segmentation in an automatic framework", "author": ["L. Li", "R. Socher", "F. Li"], "venue": "CVPR, 2009, pp. 2036\u20132043.", "citeRegEx": "152", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-modal image annotation with multi-instance multi-label LDA", "author": ["C. Nguyen", "D. Zhan", "Z. Zhou"], "venue": "IJCAI, 2013.", "citeRegEx": "153", "shortCiteRegEx": null, "year": 2013}, {"title": "Gaussian processes for machine learning.", "author": ["C.E. Rasmussen"], "venue": null, "citeRegEx": "154", "shortCiteRegEx": "154", "year": 2006}, {"title": "Gaussian process latent variable models for human pose estimation", "author": ["C.H. Ek", "P.H.S. Torr", "N.D. Lawrence"], "venue": "MLMI, 2007, pp. 132\u2013143.", "citeRegEx": "155", "shortCiteRegEx": null, "year": 2007}, {"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["N.D. Lawrence"], "venue": "NIPS, 2003, pp. 329\u2013336.", "citeRegEx": "156", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning shared latent structure for image synthesis and robotic imitation", "author": ["A.P. Shon", "K. Grochow", "A. Hertzmann", "R.P.N. Rao"], "venue": "NIPS, 2005, pp. 1233\u20131240.", "citeRegEx": "157", "shortCiteRegEx": null, "year": 2005}, {"title": "Discriminative gaussian process latent variable model for classification", "author": ["R. Urtasun", "T. Darrell"], "venue": "ICML, 2007, pp. 927\u2013934.", "citeRegEx": "158", "shortCiteRegEx": null, "year": 2007}, {"title": "Discriminative shared gaussian processes for multiview and viewinvariant facial expression recognition", "author": ["S. Eleftheriadis", "O. Rudovic", "M. Pantic"], "venue": "IEEE Trans. Image Processing, vol. 24, no. 1, pp. 189\u2013204, 2015.", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-margin multiview gaussian process for image classification", "author": ["C. Xu", "D. Tao", "Y. Li", "C. Xu"], "venue": "ICIMCS, 2013, pp. 7\u201312.", "citeRegEx": "160", "shortCiteRegEx": null, "year": 2013}, {"title": "Similarity  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  24 gaussian process latent variable model for multi-modal data analysis", "author": ["G. Song", "S. Wang", "Q. Huang", "Q. Tian"], "venue": "ICCV, 2015, pp. 4050\u20134058.", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2015}, {"title": "Relational learning via collective matrix factorization", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "SIGKDD, 2008, pp. 650\u2013 658.", "citeRegEx": "162", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-label informed latent semantic indexing", "author": ["K. Yu", "S. Yu", "V. Tresp"], "venue": "SIGIR, 2005, pp. 258\u2013265.", "citeRegEx": "163", "shortCiteRegEx": null, "year": 2005}, {"title": "Spectral clustering for multi-type relational data", "author": ["B. Long", "Z.M. Zhang", "X. Wu", "P.S. Yu"], "venue": "ICML, 2006, pp. 585\u2013592.", "citeRegEx": "164", "shortCiteRegEx": null, "year": 2006}, {"title": "A probabilistic framework for relational clustering", "author": ["B. Long", "Z.M. Zhang", "P.S. Yu"], "venue": "KDD, 2007, pp. 470\u2013479.", "citeRegEx": "165", "shortCiteRegEx": null, "year": 2007}, {"title": "Relation-Prediction in Multi-Relational Domains using Matrix-Factorization", "author": ["C. Lippert", "S.H. Weber", "Y. Huang", "V. Tresp", "M. Schubert", "H.-P. Kriegel"], "venue": "NIPS 2008 Workshop: Structured Input - Structured Output, 2008.", "citeRegEx": "166", "shortCiteRegEx": null, "year": 2008}, {"title": "Group-sparse embeddings in collective matrix factorization", "author": ["A. Klami", "G. Bouchard", "A. Tripathi"], "venue": "CoRR, vol. abs/1312.5921, 2013.", "citeRegEx": "167", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex collective matrix factorization", "author": ["G. Bouchard", "D. Yin", "S. Guo"], "venue": "AISTATS, 2013, pp. 144\u2013152.", "citeRegEx": "168", "shortCiteRegEx": null, "year": 2013}, {"title": "Consistent collective matrix completion under joint low rank structure", "author": ["S. Gunasekar", "M. Yamada", "D. Yin", "Y. Chang"], "venue": "AISTATS, 2015.", "citeRegEx": "169", "shortCiteRegEx": null, "year": 2015}, {"title": "Partial collective matrix factorization and its pac bound", "author": ["C. Lan", "X. Li", "Y. Deng", "J. Huan"], "venue": "2016.", "citeRegEx": "170", "shortCiteRegEx": null, "year": 2016}, {"title": "Image annotation using multi-correlation probabilistic matrix factorization", "author": ["Z. Li", "J. Liu", "X. Zhu", "T. Liu", "H. Lu"], "venue": "ACM Multimedia, 2010, pp. 1187\u20131190.", "citeRegEx": "171", "shortCiteRegEx": null, "year": 2010}, {"title": "Combining content and link for classification using matrix factorization", "author": ["S. Zhu", "K. Yu", "Y. Chi", "Y. Gong"], "venue": "SIGIR, 2007, pp. 487\u2013494.", "citeRegEx": "172", "shortCiteRegEx": null, "year": 2007}, {"title": "A bayesian matrix factorization model for relational data", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "CoRR, vol. abs/1203.3517, 2012.", "citeRegEx": "173", "shortCiteRegEx": null, "year": 2012}, {"title": "A probabilistic model for multimodal hash function learning", "author": ["Y. Zhen", "D. Yeung"], "venue": "SIGKDD, 2012, pp. 940\u2013 948.", "citeRegEx": "174", "shortCiteRegEx": null, "year": 2012}, {"title": "Collective matrix factorization hashing for multimodal data", "author": ["G. Ding", "Y. Guo", "J. Zhou"], "venue": "CVPR, 2014, pp. 2083\u20132090.", "citeRegEx": "175", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-negative Matrix Factorization in Multimodality Data for Segmentation and Label Prediction", "author": ["Z. Akata", "C. Thurau", "C. Bauckhage"], "venue": "16th Computer Vision Winter Workshop, 2011.", "citeRegEx": "176", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-view clustering via joint nonnegative matrix factorization", "author": ["J. Liu", "C. Wang", "J. Gao", "J. Han"], "venue": "SDM, 2013, pp. 252\u2013260.", "citeRegEx": "177", "shortCiteRegEx": null, "year": 2013}, {"title": "Data fusion by matrix factorization", "author": ["M. Zitnik", "B. Zupan"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 1, pp. 41\u201353, 2015.", "citeRegEx": "178", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust and non-negative collective matrix factorization for text-to-image transfer learning", "author": ["L. Yang", "L. Jing", "M.K. Ng"], "venue": "IEEE Trans. Image Processing, vol. 24, no. 12, pp. 4701\u20134714, 2015.", "citeRegEx": "179", "shortCiteRegEx": null, "year": 2015}, {"title": "Pattern recognition and machine", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "180", "shortCiteRegEx": "180", "year": 2006}, {"title": "Sparse unsupervised dimensionality reduction for multiple view data", "author": ["Y. Han", "F. Wu", "D. Tao", "J. Shao", "Y. Zhuang", "J. Jiang"], "venue": "IEEE Trans. Circuits Syst. Video Techn., vol. 22,  no. 10, pp. 1485\u20131496, 2012.", "citeRegEx": "181", "shortCiteRegEx": null, "year": 2012}, {"title": "Click prediction for web image reranking using multimodal sparse coding", "author": ["J. Yu", "Y. Rui", "D. Tao"], "venue": "IEEE Transactions on Image Processing, vol. 23, no. 5, pp. 2019\u2013 2032, 2014.", "citeRegEx": "182", "shortCiteRegEx": null, "year": 2019}, {"title": "Supervised coupled dictionary learning with group structures for multi-modal retrieval", "author": ["Y. Zhuang", "Y. Wang", "F. Wu", "Y. Zhang", "W. Lu"], "venue": "AAAI, 2013.", "citeRegEx": "183", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse multi-modal hashing", "author": ["F. Wu", "Z. Yu", "Y. Yang", "S. Tang", "Y. Zhang", "Y. Zhuang"], "venue": "IEEE Trans. Multimedia, vol. 16, no. 2, pp. 427\u2013439, 2014.", "citeRegEx": "184", "shortCiteRegEx": null, "year": 2014}, {"title": "Exponential family harmoniums with an application to information retrieval", "author": ["M. Welling", "M. Rosen-Zvi", "G.E. Hinton"], "venue": "NIPS, 2004, pp. 1481\u20131488.", "citeRegEx": "185", "shortCiteRegEx": null, "year": 2004}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.", "citeRegEx": "186", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-modal distance metric learning", "author": ["P. Xie", "E.P. Xing"], "venue": "IJCAI, 2013, pp. 1806\u20131812.", "citeRegEx": "187", "shortCiteRegEx": null, "year": 2013}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell"], "venue": "NIPS, 2002, pp. 505\u2013512.", "citeRegEx": "188", "shortCiteRegEx": null, "year": 2002}, {"title": "Soft modeling: the basic design and some extensions", "author": ["H. Wold"], "venue": "Systems under indirect observation, vol. 2, pp. 589\u2013591, 1982.", "citeRegEx": "189", "shortCiteRegEx": null, "year": 1982}, {"title": "Human detection using partial least squares analysis", "author": ["W.R. Schwartz", "A. Kembhavi", "D. Harwood", "L.S. Davis"], "venue": "ICCV, 2009, pp. 24\u201331.", "citeRegEx": "191", "shortCiteRegEx": null, "year": 2009}, {"title": "Pls-regression: a basic tool of chemometrics", "author": ["S. Wold", "M. Sjstrm", "L. Eriksson"], "venue": "Chemometrics and Intelligent Laboratory Systems, vol. 58, pp. 109\u2013130, 2001.", "citeRegEx": "192", "shortCiteRegEx": null, "year": 2001}, {"title": "Partial least squares for discrimination", "author": ["M. Barker", "W. Rayens"], "venue": "Journal of Chemometrics, vol. 17, no. 3, pp. 166 \u2013 173, 2003.", "citeRegEx": "193", "shortCiteRegEx": null, "year": 2003}, {"title": "Joint estimation of age, gender and ethnicity: CCA vs. PLS", "author": ["G. Guo", "G. Mu"], "venue": "FG, 2013, pp. 1\u20136.", "citeRegEx": "194", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimedia content processing through cross-modal association", "author": ["D. Li", "N. Dimitrova", "M. Li", "I.K. Sethi"], "venue": "ACM Multimedia, 2003, pp. 604\u2013611.", "citeRegEx": "195", "shortCiteRegEx": null, "year": 2003}, {"title": "Kernel cross-modal factor analysis for information fusion with application to bimodal emotion recognition", "author": ["Y. Wang", "L. Guan", "A.N. Venetsanopoulos"], "venue": "IEEE Transactions on Multimedia, vol. 14, no. 3-1, pp. 597\u2013607, 2012.", "citeRegEx": "196", "shortCiteRegEx": null, "year": 2012}, {"title": "On the role of correlation and abstraction in cross-modal multimedia retrieval", "author": ["J.C. Pereira", "E. Coviello", "G. Doyle", "N. Rasiwasia", "G.R.G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 3, pp. 521\u2013535, 2014.", "citeRegEx": "197", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised cross-modal factor analysis", "author": ["J. Wang", "H. Wang", "Y. Tu", "K. Duan", "Z. Zhan", "S. Chekuri"], "venue": "CoRR, vol. abs/1502.05134, 2015.", "citeRegEx": "198", "shortCiteRegEx": null, "year": 2015}, {"title": "Characterizing the response of PET and fMRI data using multivariate linear models", "author": ["K. Worsley", "J.-B. Poline", "K. Friston", "A. Evans"], "venue": "Neuroimage, vol. 6, no. 4, pp. 305\u2013319, 1997.", "citeRegEx": "199", "shortCiteRegEx": null, "year": 1997}, {"title": "Efficient kernel orthonormalized PLS for remote sensing applications", "author": ["J. Arenas-Garc\u0131\u0301a", "G. Camps-Valls"], "venue": "IEEE Trans. Geoscience and Remote Sensing, vol. 46, no. 10, pp. 2872\u20132881, 2008.  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  25", "citeRegEx": "200", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse and kernel opls feature extraction based on eigenvalue problem solving", "author": ["S. Mu\u00f1oz Romero", "J. Arenas-Garc\u0131\u0301a", "V. G\u00f3mez- Verdejo"], "venue": "Pattern Recognition, vol. 48, no. 5, pp. 1797\u20131811, May 2015.", "citeRegEx": "201", "shortCiteRegEx": null, "year": 1811}, {"title": "On the equivalence between canonical correlation analysis and orthonormalized partial least squares", "author": ["L. Sun", "S. Ji", "S. Yu", "J. Ye"], "venue": "IJCAI, 2009, pp. 1230\u20131235.", "citeRegEx": "202", "shortCiteRegEx": null, "year": 2009}, {"title": "Eigenspectra, a robust regression method for multiplexed raman spectra analysis", "author": ["S. Li", "J. Gao", "J.O. Nyagilo", "D.P. Dave"], "venue": "BIBM, 2010, pp. 525\u2013530.", "citeRegEx": "203", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic partial least square regression: A robust model for quantitative analysis of raman spectroscopy data", "author": ["\u2014\u2014"], "venue": "BIBM, 2011, pp. 526\u2013531.", "citeRegEx": "204", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiview fisher discriminant analysis", "author": ["T. Diethe", "D.R. Hardoon", "J. Shawe-taylor"], "venue": "In NIPS Workshop on Learning from Multiple Sources, 2008.", "citeRegEx": "205", "shortCiteRegEx": null, "year": 2008}, {"title": "Constructing nonlinear discriminants from multiple data views", "author": ["T. Diethe", "D.R. Hardoon", "J. Shawe-Taylor"], "venue": "ECML PKDD, 2010, pp. 328\u2013343.", "citeRegEx": "206", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiview discriminant analysis", "author": ["M. Kan", "S. Shan", "H. Zhang", "S. Lao", "X. Chen"], "venue": "ECCV, 2012, pp. 808\u2013821.", "citeRegEx": "207", "shortCiteRegEx": null, "year": 2012}, {"title": "An improved training algorithm for kernel fisher discriminants", "author": ["S. Mika", "A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "AIS- TATS, 2001.", "citeRegEx": "208", "shortCiteRegEx": null, "year": 2001}, {"title": "Separating style and content with bilinear models", "author": ["J.B. Tenenbaum", "W.T. Freeman"], "venue": "Neural Computation, vol. 12, no. 6, pp. 1247\u20131283, 2000.", "citeRegEx": "209", "shortCiteRegEx": null, "year": 2000}, {"title": "Graph embedding and extensions: A general framework for dimensionality reduction", "author": ["S. Yan", "D. Xu", "B. Zhang", "H. Zhang", "Q. Yang", "S. Lin"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 1, pp. 40\u201351, 2007.", "citeRegEx": "210", "shortCiteRegEx": null, "year": 2007}, {"title": "Generalized multiview analysis: A discriminative latent space", "author": ["A. Sharma", "A. Kumar", "H.D. III", "D.W. Jacobs"], "venue": "CVPR, 2012, pp. 2160\u20132167.", "citeRegEx": "211", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to rank with (a lot of) word features", "author": ["B. Bai", "J. Weston", "D. Grangier", "R. Collobert", "K. Sadamasa", "Y. Qi", "O. Chapelle", "K.Q. Weinberger"], "venue": "Inf. Retr., vol. 13, no. 3, pp. 291\u2013314, 2010.", "citeRegEx": "212", "shortCiteRegEx": null, "year": 2010}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Machine Learning, vol. 81, no. 1, pp. 21\u201335, 2010.", "citeRegEx": "213", "shortCiteRegEx": null, "year": 2010}, {"title": "A discriminative kernel-based approach to rank images from text queries", "author": ["D. Grangier", "S. Bengio"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 8, pp. 1371\u20131384, 2008.", "citeRegEx": "214", "shortCiteRegEx": null, "year": 2008}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "Advances in Large Margin Classifiers. Cambridge, MA: MIT Press, 2000.", "citeRegEx": "215", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning to rank using gradient descent", "author": ["C.J.C. Burges", "T. Shaked", "E. Renshaw", "A. Lazier", "M. Deeds", "N. Hamilton", "G.N. Hullender"], "venue": "ICML, 2005, pp. 89\u201396.", "citeRegEx": "216", "shortCiteRegEx": null, "year": 2005}, {"title": "New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron", "author": ["M. Collins", "N. Duffy"], "venue": "ACL, 2002, pp. 263\u2013270.", "citeRegEx": "217", "shortCiteRegEx": null, "year": 2002}, {"title": "Links between perceptrons, mlps and svms", "author": ["R. Collobert", "S. Bengio"], "venue": "ICML, 2004.", "citeRegEx": "218", "shortCiteRegEx": null, "year": 2004}, {"title": "A low rank structural large margin method for cross-modal ranking", "author": ["X. Lu", "F. Wu", "S. Tang", "Z. Zhang", "X. He", "Y. Zhuang"], "venue": "SIGIR, 2013, pp. 433\u2013442.", "citeRegEx": "219", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-modal learning to rank via latent joint representation", "author": ["F. Wu", "X. Jiang", "X. Li", "S. Tang", "W. Lu", "Z. Zhang", "Y. Zhuang"], "venue": "IEEE Trans. Image Processing, vol. 24, no. 5, pp. 1497\u20131509, 2015.", "citeRegEx": "221", "shortCiteRegEx": null, "year": 2015}, {"title": "Data fusion through cross-modality metric learning using similarity-sensitive hashing", "author": ["M.M. Bronstein", "A.M. Bronstein", "F. Michel", "N. Paragios"], "venue": "CVPR, 2010, pp. 3594\u20133601.", "citeRegEx": "222", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning hash functions for cross-view similarity search", "author": ["S. Kumar", "R. Udupa"], "venue": "IJCAI, 2011, pp. 1360\u2013 1365.", "citeRegEx": "223", "shortCiteRegEx": null, "year": 2011}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "CVPR, 2011, pp. 817\u2013824.", "citeRegEx": "224", "shortCiteRegEx": null, "year": 2011}, {"title": "Co-regularized hashing for multimodal data", "author": ["Y. Zhen", "D. Yeung"], "venue": "NIPS, 2012, pp. 1385\u20131393.", "citeRegEx": "225", "shortCiteRegEx": null, "year": 2012}, {"title": "Linear cross-modal hashing for efficient multimedia search", "author": ["X. Zhu", "Z. Huang", "H.T. Shen", "X. Zhao"], "venue": "ACM Multimedia Conference, MM \u201913, Barcelona, Spain, October 21-25, 2013, 2013, pp. 143\u2013152.", "citeRegEx": "226", "shortCiteRegEx": null, "year": 2013}, {"title": "Parametric local multimodal hashing for cross-view similarity search", "author": ["D. Zhai", "H. Chang", "Y. Zhen", "X. Liu", "X. Chen", "W. Gao"], "venue": "IJCAI, 2013, pp. 2754\u20132760.", "citeRegEx": "227", "shortCiteRegEx": null, "year": 2013}, {"title": "Collaborative hashing", "author": ["X. Liu", "J. He", "C. Deng", "B. Lang"], "venue": "CVPR, 2014, pp. 2147\u20132154.", "citeRegEx": "228", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale supervised multimodal hashing with semantic correlation maximization", "author": ["D. Zhang", "W. Li"], "venue": "AAAI, 2014, pp. 2177\u20132183.", "citeRegEx": "229", "shortCiteRegEx": null, "year": 2014}, {"title": "Semanticspreserving hashing for cross-view retrieval", "author": ["Z. Lin", "G. Ding", "M. Hu", "J. Wang"], "venue": "CVPR, 2015, pp. 3864\u20133872.", "citeRegEx": "230", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral multimodal hashing and its application to multimedia retrieval", "author": ["Y. Zhen", "Y. Gao", "D. Yeung", "H. Zha", "X. Li"], "venue": "IEEE Trans. Cybernetics, vol. 46, no. 1, pp. 27\u201338, 2016.", "citeRegEx": "231", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning task-specific similarity", "author": ["G. Shakhnarovich"], "venue": "Ph.D. dissertation, Cambridge, MA, USA, 2005.", "citeRegEx": "232", "shortCiteRegEx": null, "year": 2005}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. Comput. Syst. Sci., vol. 55, no. 1, pp. 119\u2013139, 1997.", "citeRegEx": "233", "shortCiteRegEx": null, "year": 1997}, {"title": "Discriminative coupled dictionary hashing for fast crossmedia retrieval", "author": ["Z. Yu", "F. Wu", "Y. Yang", "Q. Tian", "J. Luo", "Y. Zhuang"], "venue": "SIGIR, 2014, pp. 395\u2013404.", "citeRegEx": "234", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "NIPS, 2008, pp. 1753\u20131760.", "citeRegEx": "235", "shortCiteRegEx": null, "year": 2008}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S. Chang"], "venue": "ICML, 2011, pp. 1\u20138.", "citeRegEx": "236", "shortCiteRegEx": null, "year": 2011}, {"title": "Intermedia hashing for large-scale retrieval from heterogeneous data sources", "author": ["J. Song", "Y. Yang", "Y. Yang", "Z. Huang", "H.T. Shen"], "venue": "SIGMOD, 2013, pp. 785\u2013796.", "citeRegEx": "237", "shortCiteRegEx": null, "year": 2013}, {"title": "Latent semantic sparse hashing for cross-modal similarity search", "author": ["J. Zhou", "G. Ding", "Y. Guo"], "venue": "SIGIR, 2014, pp. 415\u2013424.", "citeRegEx": "238", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantized correlation hashing for fast cross-modal search", "author": ["B. Wu", "Q. Yang", "W. Zheng", "Y. Wang", "J. Wang"], "venue": "IJCAI, 2015, pp. 3946\u20133952.", "citeRegEx": "239", "shortCiteRegEx": null, "year": 2015}, {"title": "Replicated softmax: an undirected topic model", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "NIPS, 2009, pp. 1607\u20131614.", "citeRegEx": "241", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio-visual deep learning for noise robust speech recognition", "author": ["J. Huang", "B. Kingsbury"], "venue": "ICASSP, 2013, pp. 7596\u20137599.", "citeRegEx": "242", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal dbn for predicting high-quality answers in cqa portals.", "author": ["H. Hu", "B. Liu", "B. Wang", "M. Liu", "X. Wang"], "venue": null, "citeRegEx": "243", "shortCiteRegEx": "243", "year": 2013}, {"title": "Multi-source deep learning for information trustworthiness estimation", "author": ["L. Ge", "J. Gao", "X. Li", "A. Zhang"], "venue": "KDD, 2013, pp. 766\u2013774.", "citeRegEx": "244", "shortCiteRegEx": null, "year": 2013}, {"title": "Mutlimodal learning with deep boltzmann machine for emotion prediction in user generated videos", "author": ["L. Pang", "C.-W. Ngo"], "venue": "ACM ICMR, 2015, pp. 619\u2013622.", "citeRegEx": "245", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved multimodal deep learning with variation of information", "author": ["K. Sohn", "W. Shang", "H. Lee"], "venue": "NIPS, 2014, pp. 2141\u20132149.", "citeRegEx": "246", "shortCiteRegEx": null, "year": 2014}, {"title": "Facial attributes classification using multi-task representation learning", "author": ["M. Ehrlich", "T.J. Shields", "T. Almaev", "M.R. Amer"], "venue": "CVPR Workshops, 2016, pp. 47\u201355.", "citeRegEx": "247", "shortCiteRegEx": null, "year": 2016}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "ICML, 2008, pp. 536\u2013543.", "citeRegEx": "248", "shortCiteRegEx": null, "year": 2008}, {"title": "Multimodal deep autoencoder for human pose recovery", "author": ["C. Hong", "J. Yu", "J. Wan", "D. Tao", "M. Wang"], "venue": "IEEE Transactions on Image Processing, vol. 24, no. 12, pp. 5659\u20135670, 2015.", "citeRegEx": "249", "shortCiteRegEx": null, "year": 2015}, {"title": "Coupled auto-associative neural networks for heterogeneous face recognition", "author": ["B.S. Riggan", "C. Reale", "N.M. Nasrabadi"], "venue": "IEEE Access, vol. 3, pp. 1620\u20131632, 2015.", "citeRegEx": "250", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning text pair similarity with context-sensitive autoencoders", "author": ["H. Amiri", "P. Resnik", "J. Boyd-Graber", "H.D. III"], "venue": "2016.", "citeRegEx": "251", "shortCiteRegEx": null, "year": 2016}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "NIPS, 2008, pp. 873\u2013880.", "citeRegEx": "252", "shortCiteRegEx": null, "year": 2008}, {"title": "Effective multi-modal retrieval based on stacked autoencoders", "author": ["W. Wang", "B.C. Ooi", "X. Yang", "D. Zhang", "Y. Zhuang"], "venue": "Proceedings of the VLDB Endowment, vol. 7, no. 8, pp. 649\u2013660, 2014.", "citeRegEx": "253", "shortCiteRegEx": null, "year": 2014}, {"title": "Mdl-cw: A multimodal deep learning framework with cross weights", "author": ["S. Rastegar", "M. Soleymani", "H.R. Rabiee", "S. Mohsen Shojaee"], "venue": "CVPR, 2016, pp. 2601\u2013 2609.", "citeRegEx": "254", "shortCiteRegEx": null, "year": 2016}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "NIPS, 2013, pp. 2121\u20132129.", "citeRegEx": "255", "shortCiteRegEx": null, "year": 2013}, {"title": "Word2visualvec: Crossmedia retrieval by visual feature prediction", "author": ["J. Dong", "X. Li", "C.G. Snoek"], "venue": "arXiv preprint arXiv:1604.06838, 2016.", "citeRegEx": "256", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal neural language models.", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "in ICML,", "citeRegEx": "257", "shortCiteRegEx": "257", "year": 2014}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A. Lazaridou", "N.T. Pham", "M. Baroni"], "venue": "arXiv preprint arXiv:1501.02598, 2015.", "citeRegEx": "258", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "259", "shortCiteRegEx": null, "year": 2013}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G.S. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1312.5650, 2013.", "citeRegEx": "261", "shortCiteRegEx": null, "year": 2013}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "CVPR, 2015, pp. 1473\u20131482.", "citeRegEx": "262", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual information in semantic representation", "author": ["Y. Feng", "M. Lapata"], "venue": "HLT-NAACL, 2010, pp. 91\u201399.", "citeRegEx": "263", "shortCiteRegEx": null, "year": 2010}, {"title": "Multimodal distributional semantics.", "author": ["E. Bruni", "N.-K. Tran", "M. Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "264", "shortCiteRegEx": "264", "year": 2014}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics.", "author": ["D. Kiela", "L. Bottou"], "venue": "in EMNLP,", "citeRegEx": "265", "shortCiteRegEx": "265", "year": 2014}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework.", "author": ["R. Xu", "C. Xiong", "W. Chen", "J.J. Corso"], "venue": null, "citeRegEx": "266", "shortCiteRegEx": "266", "year": 2015}, {"title": "Learning cross space mapping via dnn using large scale click-through logs", "author": ["W. Yu", "K. Yang", "Y. Bai", "H. Yao", "Y. Rui"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 2000\u20132007, 2015.", "citeRegEx": "267", "shortCiteRegEx": null, "year": 2000}, {"title": "Deep compositional cross-modal learning to rank via local-global alignment", "author": ["X. Jiang", "F. Wu", "X. Li", "Z. Zhao", "W. Lu", "S. Tang", "Y. Zhuang"], "venue": "ACM Multimedia, 2015, pp. 69\u201378.", "citeRegEx": "268", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-modal retrieval with cnn visual features: A new baseline", "author": ["Y. Wei", "Y. Zhao", "C. Lu", "S. Wei", "L. Liu", "Z. Zhu", "S. Yan"], "venue": "2016.", "citeRegEx": "269", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning of multimodal representations with random walks on the click graph", "author": ["F. Wu", "X. Lu", "J. Song", "S. Yan", "Z.M. Zhang", "Y. Rui", "Y. Zhuang"], "venue": "IEEE Transactions on Image Processing, vol. 25, no. 2, pp. 630\u2013642, 2016.", "citeRegEx": "270", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning to hash with multiple representations", "author": ["Y. Kang", "S. Kim", "S. Choi"], "venue": "ICDM, 2012, pp. 930\u2013935.", "citeRegEx": "271", "shortCiteRegEx": null, "year": 2012}, {"title": "Cross-media hashing with neural networks", "author": ["Y. Zhuang", "Z. Yu", "W. Wang", "F. Wu", "S. Tang", "J. Shao"], "venue": "ACM Multimedia, 2014, pp. 901\u2013904.", "citeRegEx": "272", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multimodal hashing with orthogonal regularization", "author": ["D. Wang", "P. Cui", "M. Ou", "W. Zhu"], "venue": "IJCAI, 2015.", "citeRegEx": "273", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep cross-modal hashing", "author": ["Q.-Y. Jiang", "W.-J. Li"], "venue": "arXiv preprint arXiv:1602.02255, 2016.", "citeRegEx": "274", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "in Interspeech,", "citeRegEx": "275", "shortCiteRegEx": "275", "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "276", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, 2014, pp. 1724\u20131734.", "citeRegEx": "277", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 2014, pp. 3104\u20133112.", "citeRegEx": "278", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning longterm dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  27 Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013 166, 1994.", "citeRegEx": "279", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539, 2014.", "citeRegEx": "280", "shortCiteRegEx": null, "year": 2014}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "arXiv preprint arXiv:1412.4729, 2014.", "citeRegEx": "281", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence-video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "ICCV, 2015, pp. 4534\u20134542.", "citeRegEx": "282", "shortCiteRegEx": null, "year": 2015}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV, 2015, pp. 2425\u20132433.", "citeRegEx": "283", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1411.5654, 2014.", "citeRegEx": "284", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, vol. abs/1409.0473, 2014.", "citeRegEx": "285", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "CoRR, vol. abs/1412.7755, 2014.", "citeRegEx": "286", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "CoRR, vol. abs/1406.6247, 2014.", "citeRegEx": "287", "shortCiteRegEx": null, "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "CoRR, vol. abs/1502.03044, 2015.", "citeRegEx": "288", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel field alignment for cross media retrieval", "author": ["X. Mao", "B. Lin", "D. Cai", "X. He", "J. Pei"], "venue": "ACM Multimedia, 2013, pp. 897\u2013906.", "citeRegEx": "289", "shortCiteRegEx": null, "year": 2013}, {"title": "Ranking on data manifolds", "author": ["D. Zhou", "J. Weston", "A. Gretton", "O. Bousquet", "B. Sch\u00f6lkopf"], "venue": "NIPS, 2003, pp. 169\u2013176.", "citeRegEx": "290", "shortCiteRegEx": null, "year": 2003}, {"title": "Data fusion and multicue data matching by diffusion maps", "author": ["S. Lafon", "Y. Keller", "R.R. Coifman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 11, pp. 1784\u20131797, 2006.", "citeRegEx": "291", "shortCiteRegEx": null, "year": 2006}, {"title": "Manifold alignment using procrustes analysis", "author": ["C. Wang", "S. Mahadevan"], "venue": "ICML, 2008, pp. 1120\u20131127.", "citeRegEx": "292", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Canonical correlation analysis (CCA) [1] and its kernel extensions [2\u20134] are representative techniques in early studies of multi-view representation learning.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "Canonical correlation analysis (CCA) [1] and its kernel extensions [2\u20134] are representative techniques in early studies of multi-view representation learning.", "startOffset": 67, "endOffset": 72}, {"referenceID": 2, "context": "Canonical correlation analysis (CCA) [1] and its kernel extensions [2\u20134] are representative techniques in early studies of multi-view representation learning.", "startOffset": 67, "endOffset": 72}, {"referenceID": 3, "context": "Canonical correlation analysis (CCA) [1] and its kernel extensions [2\u20134] are representative techniques in early studies of multi-view representation learning.", "startOffset": 67, "endOffset": 72}, {"referenceID": 6, "context": "Inspired by the success of deep neural networks [5\u2013 7], deep CCAs [8] have been proposed to this problem, with a common strategy to learn a joint representation that is coupled between multiple views at higher level after learning several layers of view-specific features in the lower layers.", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "Inspired by the success of deep neural networks [5\u2013 7], deep CCAs [8] have been proposed to this problem, with a common strategy to learn a joint representation that is coupled between multiple views at higher level after learning several layers of view-specific features in the lower layers.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 132, "endOffset": 138}, {"referenceID": 9, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 132, "endOffset": 138}, {"referenceID": 10, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 132, "endOffset": 138}, {"referenceID": 11, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 165, "endOffset": 172}, {"referenceID": 12, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 165, "endOffset": 172}, {"referenceID": 13, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 165, "endOffset": 172}, {"referenceID": 14, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 217, "endOffset": 225}, {"referenceID": 15, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 217, "endOffset": 225}, {"referenceID": 16, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 295, "endOffset": 299}, {"referenceID": 17, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 325, "endOffset": 332}, {"referenceID": 18, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 325, "endOffset": 332}, {"referenceID": 19, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 325, "endOffset": 332}, {"referenceID": 20, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 376, "endOffset": 383}, {"referenceID": 21, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 376, "endOffset": 383}, {"referenceID": 22, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 376, "endOffset": 383}, {"referenceID": 23, "context": "Further, manifold alignment also provides an important perspective for multi-view representation learning [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "An illustrative example application of CCA in cross-modal retrieval (adapted from [25]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "Canonical Correlation Analysis [1] has become increasingly popular for its capability of effectively modeling the relationship between two or more sets of variables.", "startOffset": 31, "endOffset": 34}, {"referenceID": 26, "context": "More specifically, CCA has been widely used in multi-view learning tasks to generate low dimensional feature representations [25\u2013 27].", "startOffset": 125, "endOffset": 133}, {"referenceID": 27, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 99, "endOffset": 106}, {"referenceID": 28, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 99, "endOffset": 106}, {"referenceID": 29, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 99, "endOffset": 106}, {"referenceID": 32, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 119, "endOffset": 127}, {"referenceID": 33, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 140, "endOffset": 148}, {"referenceID": 34, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 140, "endOffset": 148}, {"referenceID": 35, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 166, "endOffset": 173}, {"referenceID": 36, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 166, "endOffset": 173}, {"referenceID": 37, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 166, "endOffset": 173}, {"referenceID": 38, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 197, "endOffset": 204}, {"referenceID": 39, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 197, "endOffset": 204}, {"referenceID": 40, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 197, "endOffset": 204}, {"referenceID": 2, "context": "(3) is equivalent to solving a pair of generalized eigenvalue problems [3],", "startOffset": 71, "endOffset": 74}, {"referenceID": 41, "context": "Besides the above definition of CCA, there are also other different ways to define the canonical correlations of a pair of matrices, and all these ways are shown to be equivalent [42].", "startOffset": 179, "endOffset": 183}, {"referenceID": 42, "context": "In particular, Kettenring [43] show that CCA is equivalent to a constrained least-square optimization problem.", "startOffset": 26, "endOffset": 30}, {"referenceID": 41, "context": "Further, Golub and Zha [42] also provide a classical algorithm for computing CCA by first QR decomposition of the data matrices which whitens the data and then an SVD of the whitened covariance matrix.", "startOffset": 23, "endOffset": 27}, {"referenceID": 29, "context": "[30, 44] propose a fast algorithm for CCA with a pair of tall-and-thin matrices using subsampled randomized Walsh-Hadamard transform [45], which only subsamples a small proportion of the training data points to approximate the matrix product.", "startOffset": 0, "endOffset": 8}, {"referenceID": 43, "context": "[30, 44] propose a fast algorithm for CCA with a pair of tall-and-thin matrices using subsampled randomized Walsh-Hadamard transform [45], which only subsamples a small proportion of the training data points to approximate the matrix product.", "startOffset": 0, "endOffset": 8}, {"referenceID": 44, "context": "[30, 44] propose a fast algorithm for CCA with a pair of tall-and-thin matrices using subsampled randomized Walsh-Hadamard transform [45], which only subsamples a small proportion of the training data points to approximate the matrix product.", "startOffset": 133, "endOffset": 137}, {"referenceID": 45, "context": "Further, Lu and Foster [46] consider sparse design matrices and introduce an efficient iterative regression algorithm for large scale CCA.", "startOffset": 23, "endOffset": 27}, {"referenceID": 46, "context": "Another line of research for large scale CCA considers stochastic optimization algorithm for CCA [47].", "startOffset": 97, "endOffset": 101}, {"referenceID": 47, "context": "[48] introduce an augmented approximate gradient scheme and further extend it to a stochastic optimization regime.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Recent work [49, 50] attempts to transform the original problem of CCA into sequences of least squares problems and solve these problems with accelerated gradient descent (AGD).", "startOffset": 12, "endOffset": 20}, {"referenceID": 49, "context": "Recent work [49, 50] attempts to transform the original problem of CCA into sequences of least squares problems and solve these problems with accelerated gradient descent (AGD).", "startOffset": 12, "endOffset": 20}, {"referenceID": 30, "context": "[31] propose to use a mixture of local linear canonical correlation models to find correlation clusters from the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] formulate CCA as a least squares problem in multi-label classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "This formulation of CCA as a probabilistic model is proposed by Bach and Jordan [51].", "startOffset": 80, "endOffset": 84}, {"referenceID": 50, "context": "Consequently, Bach and Jordan [51] provide a detailed connection between CCA and probabilistic CCA based on the result of the maximum likelihood estimation of Eq.", "startOffset": 30, "endOffset": 34}, {"referenceID": 51, "context": "In addition, Browne [52] also proves that the maximum likelihood solution of Eq.", "startOffset": 20, "endOffset": 24}, {"referenceID": 52, "context": "[53] introduce robust canonical correlation analysis by replacing Gaussian distributions with Student-t distributions, which constructs mixtures of robust CCA and can deal with missing data quite easily.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "Fyfe and Leen [54] consider a Dirichlet process method for performing a non-Bayesian probabilistic mixture CCA.", "startOffset": 14, "endOffset": 18}, {"referenceID": 54, "context": "By treating the mixture case in length, Klami and Kaski [55] present a full Bayesian treatment of probabilistic canonical analyzer.", "startOffset": 56, "endOffset": 60}, {"referenceID": 55, "context": "Further, Wang [56] applies a hierarchical Bayesian model to probabilistic CCA and learn this model by variational approximation.", "startOffset": 14, "endOffset": 18}, {"referenceID": 54, "context": "Both [55] and [56] exploit the inverse Wishart matrix distribution as a prior for the covariance matrices \u03a8x(y) in Eq.", "startOffset": 5, "endOffset": 9}, {"referenceID": 55, "context": "Both [55] and [56] exploit the inverse Wishart matrix distribution as a prior for the covariance matrices \u03a8x(y) in Eq.", "startOffset": 14, "endOffset": 18}, {"referenceID": 56, "context": "[57] introduce a mixture of robust canonical correlation analyzers and provide a variational Bayesian inference for learning from noisy data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "Based on exponential family extensions of principal component analysis [58, 59], Klami et al.", "startOffset": 71, "endOffset": 79}, {"referenceID": 58, "context": "Based on exponential family extensions of principal component analysis [58, 59], Klami et al.", "startOffset": 71, "endOffset": 79}, {"referenceID": 59, "context": "[60] extend Bayesian CCA to the exponential family by generalizing the Gaussian noise assumption to noise with any distribution in the exponential family.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "Using the natural parameter formulation of exponential family distributions, certain common choices can be incorporated as special cases, which leading to an efficient way of exploiting the robustness of exponential family distribution in practical models [61, 62].", "startOffset": 256, "endOffset": 264}, {"referenceID": 61, "context": "Using the natural parameter formulation of exponential family distributions, certain common choices can be incorporated as special cases, which leading to an efficient way of exploiting the robustness of exponential family distribution in practical models [61, 62].", "startOffset": 256, "endOffset": 264}, {"referenceID": 62, "context": "Recently, Mukuta and Harada [63] follow Wang\u2019s approach [56] and introduce a Bayesian extension of partial CCA [64].", "startOffset": 28, "endOffset": 32}, {"referenceID": 55, "context": "Recently, Mukuta and Harada [63] follow Wang\u2019s approach [56] and introduce a Bayesian extension of partial CCA [64].", "startOffset": 56, "endOffset": 60}, {"referenceID": 63, "context": "Recently, Mukuta and Harada [63] follow Wang\u2019s approach [56] and introduce a Bayesian extension of partial CCA [64].", "startOffset": 111, "endOffset": 115}, {"referenceID": 64, "context": "[65] propose a probabilistic semi-CCA by incorporating the mechanism of data missing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "Most of the practical applications of Bayesian CCA in the earlier efforts focus on relatively low-dimensional data [55, 56].", "startOffset": 115, "endOffset": 123}, {"referenceID": 55, "context": "Most of the practical applications of Bayesian CCA in the earlier efforts focus on relatively low-dimensional data [55, 56].", "startOffset": 115, "endOffset": 123}, {"referenceID": 65, "context": "[66, 67] propose to use multi-way analysis setup by exploiting dimensionality reduction techniques.", "startOffset": 0, "endOffset": 8}, {"referenceID": 66, "context": "[66, 67] propose to use multi-way analysis setup by exploiting dimensionality reduction techniques.", "startOffset": 0, "endOffset": 8}, {"referenceID": 67, "context": "Most of the approaches to sparse CCA are based on the well known LASSO trick [68] which is a shrinkage and selection method for linear regression.", "startOffset": 77, "endOffset": 81}, {"referenceID": 68, "context": "By formulating CCA as two constrained simultaneous regression problems, Hardoon and ShaweTaylor [69] propose to approximate the non-convex constraints with \u221e-norm.", "startOffset": 96, "endOffset": 100}, {"referenceID": 69, "context": "[70] propose to use the elastic net type regression.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] introduce a sparse CCA by formulating CCA as a least squares problem in multi-label classification and directly computing it with the Least Angle Regression algorithm (LARS) [71].", "startOffset": 0, "endOffset": 4}, {"referenceID": 70, "context": "[27] introduce a sparse CCA by formulating CCA as a least squares problem in multi-label classification and directly computing it with the Least Angle Regression algorithm (LARS) [71].", "startOffset": 179, "endOffset": 183}, {"referenceID": 71, "context": "For example, graph laplacian [72] can be used in this framework to tackle with the unlabeled data.", "startOffset": 29, "endOffset": 33}, {"referenceID": 72, "context": "In fact, the development of sparse CCA is intimately related to the advance of sparse PCA [73\u201375].", "startOffset": 90, "endOffset": 97}, {"referenceID": 73, "context": "In fact, the development of sparse CCA is intimately related to the advance of sparse PCA [73\u201375].", "startOffset": 90, "endOffset": 97}, {"referenceID": 74, "context": "In fact, the development of sparse CCA is intimately related to the advance of sparse PCA [73\u201375].", "startOffset": 90, "endOffset": 97}, {"referenceID": 75, "context": "The classical solutions to generalized eigenvalue problem with sparse PCA can be easily extended to that of sparse CCA [76, 77].", "startOffset": 119, "endOffset": 127}, {"referenceID": 76, "context": "The classical solutions to generalized eigenvalue problem with sparse PCA can be easily extended to that of sparse CCA [76, 77].", "startOffset": 119, "endOffset": 127}, {"referenceID": 75, "context": "[76] derive a sparse CCA algorithm by extending an approach for solving sparse eigenvalue problems using D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "Based on the sparse PCA algorithm in [74], Wiesel et al.", "startOffset": 37, "endOffset": 41}, {"referenceID": 76, "context": "[77] propose a backward greedy approach to sparse CCA by bounding the correlation at each stage.", "startOffset": 0, "endOffset": 4}, {"referenceID": 77, "context": "[78] propose to apply a penalized matrix decomposition to the covariance matrix Cxy , which results in a method for penalized sparse CCA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 78, "context": "Consequently, structured sparse CCA has been proposed by extending the penalized CCA with structured sparsity inducing penalty [79].", "startOffset": 127, "endOffset": 131}, {"referenceID": 79, "context": "Another line of research for sparse CCA is based on sparse Bayesian learning [80\u201382].", "startOffset": 77, "endOffset": 84}, {"referenceID": 80, "context": "Another line of research for sparse CCA is based on sparse Bayesian learning [80\u201382].", "startOffset": 77, "endOffset": 84}, {"referenceID": 81, "context": "Another line of research for sparse CCA is based on sparse Bayesian learning [80\u201382].", "startOffset": 77, "endOffset": 84}, {"referenceID": 50, "context": "In particular, they are built on the probabilistic interpretation of CCA outlined by [51].", "startOffset": 85, "endOffset": 89}, {"referenceID": 79, "context": "Fyfe and Leen [80] investigate two methods for sparsifying probabilistic CCA.", "startOffset": 14, "endOffset": 18}, {"referenceID": 82, "context": "[83] present a variant of sparse Bayesian CCA, using an element-wise automatic relevance determination (ARD) prior configuration, where non-effective parameters are automatically driven to zero.", "startOffset": 0, "endOffset": 4}, {"referenceID": 81, "context": "Rai and Daum\u00e9 III [82] propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components and capture the sparsity underlying the projections.", "startOffset": 18, "endOffset": 22}, {"referenceID": 83, "context": "This framework exploits the Indian Buffet Process [84] to discover latent feature representation of a set of observations and to control the overall complexity of the model.", "startOffset": 50, "endOffset": 54}, {"referenceID": 51, "context": "Recently, some authors have used probabilistic inter-battery factor analysis (IBFA) [52, 85], which can be considered as an extended CCA model which complements CCA by providing a certain variation not captured by the correlation components, to build more complex hierarchical sparse Bayesian CCA models.", "startOffset": 84, "endOffset": 92}, {"referenceID": 84, "context": "Recently, some authors have used probabilistic inter-battery factor analysis (IBFA) [52, 85], which can be considered as an extended CCA model which complements CCA by providing a certain variation not captured by the correlation components, to build more complex hierarchical sparse Bayesian CCA models.", "startOffset": 84, "endOffset": 92}, {"referenceID": 85, "context": "[86] introduce a Bayesian treatment of the IBFA model, which describes not only the correlations between data sets but also provides components explaining the linear structure within each of the data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 86, "context": "Built on multi-battery factor analysis (MBFA) by McDonald [87] and Browne [88] as a generalization of IBFA, Klami et al.", "startOffset": 58, "endOffset": 62}, {"referenceID": 87, "context": "Built on multi-battery factor analysis (MBFA) by McDonald [87] and Browne [88] as a generalization of IBFA, Klami et al.", "startOffset": 74, "endOffset": 78}, {"referenceID": 88, "context": "[89] introduce a problem formulation of group factor analysis, which extends CCA to more than two sets with structural sparsity, resulting in that it is more flexible than the previous extensions [86].", "startOffset": 0, "endOffset": 4}, {"referenceID": 85, "context": "[89] introduce a problem formulation of group factor analysis, which extends CCA to more than two sets with structural sparsity, resulting in that it is more flexible than the previous extensions [86].", "startOffset": 196, "endOffset": 200}, {"referenceID": 89, "context": "Canonical Correlation Analysis is a linear multi-view representation learning algorithm, but for many scenarios of real-world multi-view data revealing nonlinearities, it is impossible for a linear embedding to capture all the properties of the multi-view data [90].", "startOffset": 261, "endOffset": 265}, {"referenceID": 90, "context": "Since kernerlization is a principled trick for introducing non-linearity into linear methods, kernel CCA [91\u201393] provides an alternative solution.", "startOffset": 105, "endOffset": 112}, {"referenceID": 91, "context": "Since kernerlization is a principled trick for introducing non-linearity into linear methods, kernel CCA [91\u201393] provides an alternative solution.", "startOffset": 105, "endOffset": 112}, {"referenceID": 92, "context": "Since kernerlization is a principled trick for introducing non-linearity into linear methods, kernel CCA [91\u201393] provides an alternative solution.", "startOffset": 105, "endOffset": 112}, {"referenceID": 1, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 175, "endOffset": 186}, {"referenceID": 93, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 175, "endOffset": 186}, {"referenceID": 94, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 175, "endOffset": 186}, {"referenceID": 2, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 210, "endOffset": 221}, {"referenceID": 95, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 210, "endOffset": 221}, {"referenceID": 96, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 210, "endOffset": 221}, {"referenceID": 31, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 244, "endOffset": 252}, {"referenceID": 97, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 244, "endOffset": 252}, {"referenceID": 98, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 280, "endOffset": 289}, {"referenceID": 99, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 280, "endOffset": 289}, {"referenceID": 1, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 306, "endOffset": 319}, {"referenceID": 100, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 306, "endOffset": 319}, {"referenceID": 101, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 306, "endOffset": 319}, {"referenceID": 102, "context": "By adapting the representer theorem [103] to the case of multiview data to state that the following minimization problem,", "startOffset": 36, "endOffset": 41}, {"referenceID": 2, "context": "As discussed in [3], the above optimization leads to degenerate solutions when either Kx or Ky is invertible.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "(13) is also equivalent to solving a pair of generalized eigenproblems [3],", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "Consequently, the statistical properties of KCCA have been investigated from several aspects [2, 104].", "startOffset": 93, "endOffset": 101}, {"referenceID": 103, "context": "Consequently, the statistical properties of KCCA have been investigated from several aspects [2, 104].", "startOffset": 93, "endOffset": 101}, {"referenceID": 104, "context": "[105] introduce a mathematical proof of the statistical convergence of kernel CCA by providing rates for the regularization parameters.", "startOffset": 0, "endOffset": 5}, {"referenceID": 105, "context": "Later Hardoon and Shawe-Taylor [106] provide a detailed theoretical analysis of KCCA and propose a finite sample statistical analysis of KCCA by using a regression formulation.", "startOffset": 31, "endOffset": 36}, {"referenceID": 106, "context": "Cai and Sun [107] also provide a convergence rate analysis of kernel CCA under an approximation assumption.", "startOffset": 12, "endOffset": 17}, {"referenceID": 1, "context": "Thus, various approximation methods have been proposed by constructing low-rank approximations of the kernel matrices, including incomplete Cholesky decomposition [2], partial Gram-Schmidt orthogonolisation [3], and block incremental SVD [99, 108].", "startOffset": 163, "endOffset": 166}, {"referenceID": 2, "context": "Thus, various approximation methods have been proposed by constructing low-rank approximations of the kernel matrices, including incomplete Cholesky decomposition [2], partial Gram-Schmidt orthogonolisation [3], and block incremental SVD [99, 108].", "startOffset": 207, "endOffset": 210}, {"referenceID": 98, "context": "Thus, various approximation methods have been proposed by constructing low-rank approximations of the kernel matrices, including incomplete Cholesky decomposition [2], partial Gram-Schmidt orthogonolisation [3], and block incremental SVD [99, 108].", "startOffset": 238, "endOffset": 247}, {"referenceID": 107, "context": "Thus, various approximation methods have been proposed by constructing low-rank approximations of the kernel matrices, including incomplete Cholesky decomposition [2], partial Gram-Schmidt orthogonolisation [3], and block incremental SVD [99, 108].", "startOffset": 238, "endOffset": 247}, {"referenceID": 108, "context": "In addition, the Nystr\u00f6m method [109] is widely used to speed up the kernel machines [110\u2013113].", "startOffset": 32, "endOffset": 37}, {"referenceID": 109, "context": "In addition, the Nystr\u00f6m method [109] is widely used to speed up the kernel machines [110\u2013113].", "startOffset": 85, "endOffset": 94}, {"referenceID": 110, "context": "In addition, the Nystr\u00f6m method [109] is widely used to speed up the kernel machines [110\u2013113].", "startOffset": 85, "endOffset": 94}, {"referenceID": 111, "context": "In addition, the Nystr\u00f6m method [109] is widely used to speed up the kernel machines [110\u2013113].", "startOffset": 85, "endOffset": 94}, {"referenceID": 112, "context": "In addition, the Nystr\u00f6m method [109] is widely used to speed up the kernel machines [110\u2013113].", "startOffset": 85, "endOffset": 94}, {"referenceID": 113, "context": "In early applications, KCCA has shown its ability as a feature preprocessing step that improves subsequent analysis in, for example, classification using a support vector machine [114].", "startOffset": 179, "endOffset": 184}, {"referenceID": 114, "context": "[115] investigate the possibility of combining the two distinct stages of KCCA and SVM into a single optimization named as SVM-2K, which has been widely used in classification tasks where two views of the same phenomenon are available.", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "With the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117].", "startOffset": 222, "endOffset": 243}, {"referenceID": 93, "context": "With the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117].", "startOffset": 222, "endOffset": 243}, {"referenceID": 94, "context": "With the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117].", "startOffset": 222, "endOffset": 243}, {"referenceID": 115, "context": "With the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117].", "startOffset": 222, "endOffset": 243}, {"referenceID": 116, "context": "With the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117].", "startOffset": 222, "endOffset": 243}, {"referenceID": 2, "context": "[3] first apply KCCA to cross-modality retrieval task, in which images are retrieved by a given multiple text query and without using any label information around the retrieved images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 93, "context": "Consequently, KCCA is exploited by Socher and Li [94] to learn a mapping between textual words and visual words so that both modalities are connected by a shared, low dimensional feature space.", "startOffset": 49, "endOffset": 53}, {"referenceID": 116, "context": "[117] make use of KCCA in a stringent task of associating images with natural language sentences that describe what is depicted.", "startOffset": 0, "endOffset": 5}, {"referenceID": 31, "context": "Moreover, kernel CCA also has attracted much attention in multi-view clustering [32, 98].", "startOffset": 80, "endOffset": 88}, {"referenceID": 97, "context": "Moreover, kernel CCA also has attracted much attention in multi-view clustering [32, 98].", "startOffset": 80, "endOffset": 88}, {"referenceID": 31, "context": "Blaschko and Lampert [32] propose a correlational spectral clustering method, which exploits KCCA to do unsupervised clustering of images and text in latent meaning space.", "startOffset": 21, "endOffset": 25}, {"referenceID": 97, "context": "[98] use a regularized variant of the KCCA algorithm to learn a lower dimensional subspace from heterogeneous data sources.", "startOffset": 0, "endOffset": 4}, {"referenceID": 96, "context": "[97] propose to modify the objective of KCCA with semi-supervised Laplacian regularization [118] to favor directions that lie along the data manifold [118].", "startOffset": 0, "endOffset": 4}, {"referenceID": 117, "context": "[97] propose to modify the objective of KCCA with semi-supervised Laplacian regularization [118] to favor directions that lie along the data manifold [118].", "startOffset": 91, "endOffset": 96}, {"referenceID": 117, "context": "[97] propose to modify the objective of KCCA with semi-supervised Laplacian regularization [118] to favor directions that lie along the data manifold [118].", "startOffset": 150, "endOffset": 155}, {"referenceID": 118, "context": "In the early work, by assuming that different parts of perceptual input have common causes in the external world, Becker and Hinton [119] present a multilayer nonlinear extension of canonical correlation by maximizing the normalized covariation between the outputs from two neural network modules.", "startOffset": 132, "endOffset": 137}, {"referenceID": 119, "context": "Further, Becker [120] explores the idea of maximizing the mutual information between the outputs of different network modules to extract higher order features from coherence inputs.", "startOffset": 16, "endOffset": 21}, {"referenceID": 120, "context": "Later Lai and Fyfe [121, 122] investigate a neural network implementation of CCA and maximize the correlation (rather than canonical correlation) between the outputs of the networks for each view.", "startOffset": 19, "endOffset": 29}, {"referenceID": 121, "context": "Later Lai and Fyfe [121, 122] investigate a neural network implementation of CCA and maximize the correlation (rather than canonical correlation) between the outputs of the networks for each view.", "startOffset": 19, "endOffset": 29}, {"referenceID": 122, "context": "Hsieh [123] formulates a nonlinear canonical correlation analysis (NLCCA) method using three feedforward neural networks.", "startOffset": 6, "endOffset": 11}, {"referenceID": 7, "context": "The framework of deep CCA (adapted from [8]), in which the output layers of two deep networks are maximally correlated.", "startOffset": 40, "endOffset": 43}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Inspired by the recent success of deep neural networks [6, 124], Andrew et al.", "startOffset": 55, "endOffset": 63}, {"referenceID": 123, "context": "Inspired by the recent success of deep neural networks [6, 124], Andrew et al.", "startOffset": 55, "endOffset": 63}, {"referenceID": 7, "context": "[8] introduce deep CCA to learn deep nonlinear mappings between two views {X,Y } which are maximally correlated.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Thus, the parameters (\u03b8\u2217 x, \u03b8 \u2217 y) are also estimated on the training data by following the gradient of the correlation objective, with batchbased algorithms like L-BFGS as in [8] or stochastic optimization with mini-batches [125\u2013127].", "startOffset": 176, "endOffset": 179}, {"referenceID": 124, "context": "Thus, the parameters (\u03b8\u2217 x, \u03b8 \u2217 y) are also estimated on the training data by following the gradient of the correlation objective, with batchbased algorithms like L-BFGS as in [8] or stochastic optimization with mini-batches [125\u2013127].", "startOffset": 225, "endOffset": 234}, {"referenceID": 125, "context": "Thus, the parameters (\u03b8\u2217 x, \u03b8 \u2217 y) are also estimated on the training data by following the gradient of the correlation objective, with batchbased algorithms like L-BFGS as in [8] or stochastic optimization with mini-batches [125\u2013127].", "startOffset": 225, "endOffset": 234}, {"referenceID": 126, "context": "Thus, the parameters (\u03b8\u2217 x, \u03b8 \u2217 y) are also estimated on the training data by following the gradient of the correlation objective, with batchbased algorithms like L-BFGS as in [8] or stochastic optimization with mini-batches [125\u2013127].", "startOffset": 225, "endOffset": 234}, {"referenceID": 127, "context": "Yan and Mikolajczyk [128] learn a joint latent space for matching images and captions with a deep CCA framework, which adopts a GPU implementation and could deal with overfitting.", "startOffset": 20, "endOffset": 25}, {"referenceID": 125, "context": "[126] learn deep non-linear embeddings of two languages using the deep CCA.", "startOffset": 0, "endOffset": 5}, {"referenceID": 19, "context": "Recently, a deep canonically correlated autoencoder (DCCAE) [20] is proposed by combining the advantages of the deep CCA and autoencoder-based approaches.", "startOffset": 60, "endOffset": 64}, {"referenceID": 128, "context": "Probabilistic latent semantic analysis (PLSA) [129] is a statistical variant of Latent Semantic Analysis (LSA) [130] and a significant step toward probabilistic modeling of text.", "startOffset": 46, "endOffset": 51}, {"referenceID": 129, "context": "Probabilistic latent semantic analysis (PLSA) [129] is a statistical variant of Latent Semantic Analysis (LSA) [130] and a significant step toward probabilistic modeling of text.", "startOffset": 111, "endOffset": 116}, {"referenceID": 8, "context": "In the original LinkPLSA model [9], both term-based PLSA [129] and citationbased PHITS [131] are merged into a joint probabilistic model, explaining terms and citations in terms of a common set of underlying factors.", "startOffset": 31, "endOffset": 34}, {"referenceID": 128, "context": "In the original LinkPLSA model [9], both term-based PLSA [129] and citationbased PHITS [131] are merged into a joint probabilistic model, explaining terms and citations in terms of a common set of underlying factors.", "startOffset": 57, "endOffset": 62}, {"referenceID": 130, "context": "In the original LinkPLSA model [9], both term-based PLSA [129] and citationbased PHITS [131] are merged into a joint probabilistic model, explaining terms and citations in terms of a common set of underlying factors.", "startOffset": 87, "endOffset": 92}, {"referenceID": 131, "context": "[132] present a multi-view PLSA-based approach called PLSA-words.", "startOffset": 0, "endOffset": 5}, {"referenceID": 132, "context": "[133] propose a PLSA based probabilistic multi-modal semantic model, in which the visual features and the textual words are fully connected via a hidden layer to allow much more mutual dependence between each other than PLSA-words.", "startOffset": 0, "endOffset": 5}, {"referenceID": 133, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 134, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 135, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 136, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 137, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 138, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 139, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 140, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 135, "context": "[136] propose a multilayer multimodal PLSA model, which can handle different modalities as well as different features within a mode.", "startOffset": 0, "endOffset": 5}, {"referenceID": 141, "context": "Latent Dirichlet allocation (LDA) [142] is a generative probabilistic model for collections of a corpus.", "startOffset": 34, "endOffset": 39}, {"referenceID": 9, "context": "[10] present a mixture of multi-modal LDA model (MoM-LDA), which describes the following generative process for the multi-modal data: each document (consisting of visual and textual information) has a distribution for a fixed number of topics (mixture components), and given a specific topic the visual features and textual words are generated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Consequently, Blei and Jordan [11] propose a correspondence LDA (Corr-LDA) model, which not only allows simultaneous dimensionality reduction in the representation of region descriptions and words, but also models the conditional correspondence between their respectively reduced representations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "The graphical model representation of the Corr-LDA model (adapted from [11]).", "startOffset": 71, "endOffset": 75}, {"referenceID": 142, "context": "Based on hierarchical Dirichlet process (HDP) [143], Yakhnenko and Honavar [144] introduce a multi-modal hierarchical Dirichlet process model (MoM-HDP), which is an extension of MoM-LDA with an infinite number of mixture components.", "startOffset": 46, "endOffset": 51}, {"referenceID": 143, "context": "Based on hierarchical Dirichlet process (HDP) [143], Yakhnenko and Honavar [144] introduce a multi-modal hierarchical Dirichlet process model (MoM-HDP), which is an extension of MoM-LDA with an infinite number of mixture components.", "startOffset": 75, "endOffset": 80}, {"referenceID": 144, "context": "Note that in practice, the Dirichlet process is approximated by truncating it [145].", "startOffset": 78, "endOffset": 83}, {"referenceID": 145, "context": "[146] also present a correspondence hierarchical Dirichlet process model (Corr-HDP), which is an extension of the Corr-LDA model using a hierarchical Dirichlet process instead.", "startOffset": 0, "endOffset": 5}, {"referenceID": 146, "context": "Following the supervised LDA algorithms [147], supervised multi-modal LDA models are subsequently proposed to make effective use of the discriminative information.", "startOffset": 40, "endOffset": 45}, {"referenceID": 146, "context": "Since supervised topic models can be classified into two categories: downstream models and upstream models [147], supervised multi-modal LDA models can also be catergorized accordingly.", "startOffset": 107, "endOffset": 112}, {"referenceID": 147, "context": "[148] develop a multi-modal probabilistic model for jointly modeling the image, its class label, and its annotations, called multi-class supervised LDA with annotations, which treats the class label as a global description of the image, and treats annotation terms as local descriptions of parts of the image.", "startOffset": 0, "endOffset": 5}, {"referenceID": 148, "context": "Its underlying assumptions naturally integrate the multi-modal data with their discriminative information so that it takes advantage of the merits of Corr-LDA and supervised LDA [149] simultaneously.", "startOffset": 178, "endOffset": 183}, {"referenceID": 149, "context": "For an upstream supervised multi-modal model, the response variables directly or indirectly generate latent topic variables [150\u2013152].", "startOffset": 124, "endOffset": 133}, {"referenceID": 150, "context": "For an upstream supervised multi-modal model, the response variables directly or indirectly generate latent topic variables [150\u2013152].", "startOffset": 124, "endOffset": 133}, {"referenceID": 151, "context": "For an upstream supervised multi-modal model, the response variables directly or indirectly generate latent topic variables [150\u2013152].", "startOffset": 124, "endOffset": 133}, {"referenceID": 150, "context": "[151] propose a spatially coherent latent topic model (Spatial-LTM), which represents an image containing objects in two different modalities: appearance features and salient image patches.", "startOffset": 0, "endOffset": 5}, {"referenceID": 152, "context": "[153] propose a Multi-modal Multi-instance Multi-Label LDA model (M3LDA), in which the model consists of a visual-label part, a textual-label part, and a label-topic part.", "startOffset": 0, "endOffset": 5}, {"referenceID": 153, "context": "Gaussian Processes (GPs) [154, 155] are generalizations of Gaussian distributions defined over infinite index sets.", "startOffset": 25, "endOffset": 35}, {"referenceID": 154, "context": "Gaussian Processes (GPs) [154, 155] are generalizations of Gaussian distributions defined over infinite index sets.", "startOffset": 25, "endOffset": 35}, {"referenceID": 155, "context": "Lawrence [156] proposes the Gaussian Process Latent Variable Model (GPLVM) as a non-linear dimensionality reduction technique.", "startOffset": 9, "endOffset": 14}, {"referenceID": 156, "context": "[157] present the shared GPLVM (SGPLVM) model as a generalization of GPLVM that can handle multiple observation spaces, where each set of observations is parameterized by a different set of kernel parameters.", "startOffset": 0, "endOffset": 5}, {"referenceID": 157, "context": "Following the idea of discriminative Gaussian process latent variable model (DGPLVM) [158], Eleftheriadis et al.", "startOffset": 85, "endOffset": 90}, {"referenceID": 158, "context": "[159] propose a discriminative shared Gaussian process latent variable model (DS-GPLVM) for multi-view analysis by exploiting a shared discriminative manifold by multi-view data.", "startOffset": 0, "endOffset": 5}, {"referenceID": 159, "context": "[160] propose a large margin multi-view Gaussian process via an integration of Gaussian process and large-margin principle.", "startOffset": 0, "endOffset": 5}, {"referenceID": 160, "context": "[161] propose a multi-modal regularized similarity Gaussian Process variable model to enforce that the semantically similar/dissimilar crossmodal observations are also similar/dissimilar in the latent space, which maximizes the cross-modal semantic consistency.", "startOffset": 0, "endOffset": 5}, {"referenceID": 161, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 162, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 163, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 164, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 165, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 166, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 167, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 168, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 169, "context": "Bias for different factors and loadings can also be incorporated [170].", "startOffset": 65, "endOffset": 70}, {"referenceID": 170, "context": "This is the probabilistic interpretation of CMF that other complex models build on [171].", "startOffset": 83, "endOffset": 88}, {"referenceID": 162, "context": "[163] propose a multi-label informed latent semantic indexing algorithm which exploits the idea of joint matrix factorization to learn a shared latent semantic factor.", "startOffset": 0, "endOffset": 5}, {"referenceID": 171, "context": "[172] introduce a joint matrix factorization model to combine the information of content and links for web page analysis.", "startOffset": 0, "endOffset": 5}, {"referenceID": 161, "context": "Later Singh and Gordon [162] propose collective matrix factorization in which several matrices are factorized simultaneously, sharing parameters among factors.", "startOffset": 23, "endOffset": 28}, {"referenceID": 172, "context": "Further, a hierarchical Bayesian variant [173] is proposed to provide an estimate of uncertainty during the relational modeling process.", "startOffset": 41, "endOffset": 46}, {"referenceID": 173, "context": "Further, CMF has been applied for multi-view hashing [174, 175].", "startOffset": 53, "endOffset": 63}, {"referenceID": 174, "context": "Further, CMF has been applied for multi-view hashing [174, 175].", "startOffset": 53, "endOffset": 63}, {"referenceID": 173, "context": "A multimodal latent binary embedding (MLBE) method is proposed by Zhen and Yang [174] to learn hash functions from a probabilistic generative perspective.", "startOffset": 80, "endOffset": 85}, {"referenceID": 174, "context": "[175] put forward collective matrix factorization hashing to learn unified hash codes by CMF with a latent factor model from different modalities of one instance.", "startOffset": 0, "endOffset": 5}, {"referenceID": 175, "context": "In addition, joint nonnegative matrix factorization (NMF) also has been proposed for multi-view analysis [176\u2013179].", "startOffset": 105, "endOffset": 114}, {"referenceID": 176, "context": "In addition, joint nonnegative matrix factorization (NMF) also has been proposed for multi-view analysis [176\u2013179].", "startOffset": 105, "endOffset": 114}, {"referenceID": 177, "context": "In addition, joint nonnegative matrix factorization (NMF) also has been proposed for multi-view analysis [176\u2013179].", "startOffset": 105, "endOffset": 114}, {"referenceID": 178, "context": "In addition, joint nonnegative matrix factorization (NMF) also has been proposed for multi-view analysis [176\u2013179].", "startOffset": 105, "endOffset": 114}, {"referenceID": 176, "context": "[177] introduce a joint NMF algorithm to learn a factorization that gives compatible clustering solutions across multiple views.", "startOffset": 0, "endOffset": 5}, {"referenceID": 11, "context": "Multi-view sparse coding [12\u201314] relates a latent representation (either a vector of random variables or a feature vector, depending on the interpretation) to the multi-view data through a set of linear mappings, which we refer to as the dictionaries.", "startOffset": 25, "endOffset": 32}, {"referenceID": 12, "context": "Multi-view sparse coding [12\u201314] relates a latent representation (either a vector of random variables or a feature vector, depending on the interpretation) to the multi-view data through a set of linear mappings, which we refer to as the dictionaries.", "startOffset": 25, "endOffset": 32}, {"referenceID": 13, "context": "Multi-view sparse coding [12\u201314] relates a latent representation (either a vector of random variables or a feature vector, depending on the interpretation) to the multi-view data through a set of linear mappings, which we refer to as the dictionaries.", "startOffset": 25, "endOffset": 32}, {"referenceID": 179, "context": "This property is owing to the explaining away effect which aries naturally in directed graphical models [180].", "startOffset": 104, "endOffset": 109}, {"referenceID": 11, "context": "There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184].", "startOffset": 132, "endOffset": 136}, {"referenceID": 180, "context": "There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184].", "startOffset": 159, "endOffset": 164}, {"referenceID": 181, "context": "There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184].", "startOffset": 182, "endOffset": 187}, {"referenceID": 182, "context": "There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184].", "startOffset": 222, "endOffset": 232}, {"referenceID": 183, "context": "There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184].", "startOffset": 222, "endOffset": 232}, {"referenceID": 184, "context": "Undirected graphical models, also called Markov random fields that have many special cases, including the exponential family Harmonium [185] and restricted Boltzmann machine [186].", "startOffset": 135, "endOffset": 140}, {"referenceID": 185, "context": "Undirected graphical models, also called Markov random fields that have many special cases, including the exponential family Harmonium [185] and restricted Boltzmann machine [186].", "startOffset": 174, "endOffset": 179}, {"referenceID": 14, "context": "[15] first introduce a particular form of multi-view latent space Markov network model called multi-wing harmonium model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "This model can be viewed as an undirected counterpart of the aforementioned directed aspect models such as multi-modal LDA [11], with the advantages that inference is fast due to the conditional independence of the hidden units and that topic mixing can be achieved by document- and feature-specific combination of aspects.", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "[16] present a multi-view latent space Markov network and its large-margin extension that satisfies a weak conditional independence assumption that data from different views and the response variables are conditionally independent given a set of latent variables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 186, "context": "In addition, Xie and Xing [187] propose a multi-modal distance metric learning (MMDML) framework based on the multi-wing harmonium model and metric learning method by [188].", "startOffset": 26, "endOffset": 31}, {"referenceID": 187, "context": "In addition, Xie and Xing [187] propose a multi-modal distance metric learning (MMDML) framework based on the multi-wing harmonium model and metric learning method by [188].", "startOffset": 167, "endOffset": 172}, {"referenceID": 188, "context": "Partial Least Squares (PLS) [189\u2013191] is a wide class of methods for modeling relations between sets of observed variables.", "startOffset": 28, "endOffset": 37}, {"referenceID": 189, "context": "Partial Least Squares (PLS) [189\u2013191] is a wide class of methods for modeling relations between sets of observed variables.", "startOffset": 28, "endOffset": 37}, {"referenceID": 190, "context": "It has been a popular tool for regression and classification as well as dimensionality reduction, especially in the field of chemometrics [192, 193].", "startOffset": 138, "endOffset": 148}, {"referenceID": 191, "context": "It has been a popular tool for regression and classification as well as dimensionality reduction, especially in the field of chemometrics [192, 193].", "startOffset": 138, "endOffset": 148}, {"referenceID": 46, "context": "be parameterized by a pair of matrices Wx \u2208 Rdx\u00d7k and Wy \u2208 Rdy\u00d7k [47].", "startOffset": 65, "endOffset": 69}, {"referenceID": 191, "context": "It has been shown that there is close connections between PLS and CCA in several aspects [190, 193].", "startOffset": 89, "endOffset": 99}, {"referenceID": 192, "context": "Guo and Mu [194] investigate the CCA based methods, including linear CCA, regularized CCA, and kernel CCA, and compare to the PLS models in solving the joint estimation problem.", "startOffset": 11, "endOffset": 16}, {"referenceID": 193, "context": "[195] introduce a least square form of PLS, called cross-modal factor analysis (CFA).", "startOffset": 0, "endOffset": 5}, {"referenceID": 196, "context": "Several extensions of CFA are presented by incorporating non-linearity and supervised information [196\u2013 198].", "startOffset": 98, "endOffset": 108}, {"referenceID": 197, "context": "Among the variants of PLS, Orthonormalized PLS (OPLS) [199\u2013201] is a popular representative.", "startOffset": 54, "endOffset": 63}, {"referenceID": 198, "context": "Among the variants of PLS, Orthonormalized PLS (OPLS) [199\u2013201] is a popular representative.", "startOffset": 54, "endOffset": 63}, {"referenceID": 199, "context": "Among the variants of PLS, Orthonormalized PLS (OPLS) [199\u2013201] is a popular representative.", "startOffset": 54, "endOffset": 63}, {"referenceID": 200, "context": "[202] investigate the equivalence relationship between CCA and OPLS and show that the difference between the CCA solution and the OPLS solution is a mere orthogonal transformation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 190, "context": "Another commonly used variant of PLS is PLS Regression (PLSR), which has been proven to be an efficient robust method to do quantity analysis for signal processing [192, 203].", "startOffset": 164, "endOffset": 174}, {"referenceID": 201, "context": "Another commonly used variant of PLS is PLS Regression (PLSR), which has been proven to be an efficient robust method to do quantity analysis for signal processing [192, 203].", "startOffset": 164, "endOffset": 174}, {"referenceID": 190, "context": "PLSR uses the two-block predictive PLS model to capture the relationship between X and Y [192].", "startOffset": 89, "endOffset": 94}, {"referenceID": 202, "context": "[204] present a probabilistic PLS to explain PLSR from a probabilistic viewpoint and describes the physical meaning of PLSR model.", "startOffset": 0, "endOffset": 5}, {"referenceID": 203, "context": "A number of multiview analysis approaches [205\u2013207] have also been proposed by considering the scenarios in which the data have multiple different views, along with supervised information.", "startOffset": 42, "endOffset": 51}, {"referenceID": 204, "context": "A number of multiview analysis approaches [205\u2013207] have also been proposed by considering the scenarios in which the data have multiple different views, along with supervised information.", "startOffset": 42, "endOffset": 51}, {"referenceID": 205, "context": "A number of multiview analysis approaches [205\u2013207] have also been proposed by considering the scenarios in which the data have multiple different views, along with supervised information.", "startOffset": 42, "endOffset": 51}, {"referenceID": 203, "context": ", ln), the two-view regularized Fisher discriminant analysis (FDA) [205] chooses two sets of weights wx and wy to solve the following optimization problem,", "startOffset": 67, "endOffset": 72}, {"referenceID": 206, "context": "In addition, based on the approach outlined in [208], both regularized two-view FDA and its kernel extension can be casted as equivalent disciplined convex optimization problems.", "startOffset": 47, "endOffset": 52}, {"referenceID": 204, "context": "[206] introduce Multiview Fisher Discriminant Analysis (MFDA) that learns classifiers in multiple views, by minimizing the variance of the data along the projection while maximizing the distance between the average outputs for classes over all of the views.", "startOffset": 0, "endOffset": 5}, {"referenceID": 205, "context": "In [207], Kan et al.", "startOffset": 3, "endOffset": 8}, {"referenceID": 207, "context": "Later based on bilinear models [209] and general graph embedding framework [210], Sharma et al.", "startOffset": 31, "endOffset": 36}, {"referenceID": 208, "context": "Later based on bilinear models [209] and general graph embedding framework [210], Sharma et al.", "startOffset": 75, "endOffset": 80}, {"referenceID": 209, "context": "[211] introduce Generalized Multi-view Analysis (GMA).", "startOffset": 0, "endOffset": 5}, {"referenceID": 210, "context": "Motivated by incorporating ranking information into multi-modal embedding learning, cross-modal ranking has attracted much attention in the literature [212\u2013214].", "startOffset": 151, "endOffset": 160}, {"referenceID": 211, "context": "Motivated by incorporating ranking information into multi-modal embedding learning, cross-modal ranking has attracted much attention in the literature [212\u2013214].", "startOffset": 151, "endOffset": 160}, {"referenceID": 212, "context": "Motivated by incorporating ranking information into multi-modal embedding learning, cross-modal ranking has attracted much attention in the literature [212\u2013214].", "startOffset": 151, "endOffset": 160}, {"referenceID": 210, "context": "[212] present a supervised semantic indexing (SSI) model which defines a class of non-linear models that are discriminatively trained to map multimodal input pairs into ranking scores.", "startOffset": 0, "endOffset": 5}, {"referenceID": 213, "context": "For this purpose, SSI exploits the margin ranking loss [215] which has already been widely used in information retrieval, and minimizes: \u2211", "startOffset": 55, "endOffset": 60}, {"referenceID": 214, "context": "This optimization problem can be solved through stochastic gradient descent [216],", "startOffset": 76, "endOffset": 81}, {"referenceID": 215, "context": "In fact, this method is a special margin ranking perceptron [217], which has been shown to be equivalent to SVM [218].", "startOffset": 60, "endOffset": 65}, {"referenceID": 216, "context": "In fact, this method is a special margin ranking perceptron [217], which has been shown to be equivalent to SVM [218].", "startOffset": 112, "endOffset": 117}, {"referenceID": 212, "context": "To exploit the advantage of online learning of kernel-based classifiers, Grangier and Bengio [214] propose a discriminative crossmodal ranking model called Passive-Aggressive Model for Image Retrieval (PAMIR), which not only adopts a learning criterion related to the final retrieval performance, but also considers different image kernels.", "startOffset": 93, "endOffset": 98}, {"referenceID": 217, "context": "Further, several recent efforts [219\u2013221] incorporate latent semantic representation learning into cross-modal ranking.", "startOffset": 32, "endOffset": 41}, {"referenceID": 218, "context": "Further, several recent efforts [219\u2013221] incorporate latent semantic representation learning into cross-modal ranking.", "startOffset": 32, "endOffset": 41}, {"referenceID": 217, "context": "[219] propose a Latent Semantic Cross-Modal Ranking (LSCMR) algorithm, which regards the cross-modal retrieval as a list-wise ranking problem and optimizes the list-wise ranking loss with a low-rank embedding setup.", "startOffset": 0, "endOffset": 5}, {"referenceID": 24, "context": "CCA and its extensions haven been widely applied to conduct cross-view similarity search [25, 38, 95].", "startOffset": 89, "endOffset": 101}, {"referenceID": 37, "context": "CCA and its extensions haven been widely applied to conduct cross-view similarity search [25, 38, 95].", "startOffset": 89, "endOffset": 101}, {"referenceID": 94, "context": "CCA and its extensions haven been widely applied to conduct cross-view similarity search [25, 38, 95].", "startOffset": 89, "endOffset": 101}, {"referenceID": 219, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 220, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 221, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 222, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 223, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 224, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 225, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 226, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 227, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 228, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 219, "context": "[222] propose a hashing-based model, called cross-modal similarity sensitive hashing (CMSSH), which approaches the cross-modality similarity learning problem by embedding the multi-modal data into a common metric space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 229, "context": "Following the greedy approach [232], the Hamming metric can be constructed sequentially as a superposition of weak binary classifiers on pairs of data points,", "startOffset": 30, "endOffset": 35}, {"referenceID": 230, "context": "Observing the resemblance with sequentially binary classifiers, boosted cross-modality similarity learning algorithms are introduced based on the standard AdaBoost procedure [233].", "startOffset": 174, "endOffset": 179}, {"referenceID": 231, "context": "However, CMSSH only considers the inter-view correlation but ignores the intra-view similarity [234].", "startOffset": 95, "endOffset": 100}, {"referenceID": 220, "context": "Kumar and Udupa [223] extend Spectral Hashing [235] from the single view setting to the multi-view scenario and present cross view hashing (CVH), which attempts to find hash functions that map similar objects to similar codewords over all the views so that inter-view and intraview similarities are both preserved.", "startOffset": 16, "endOffset": 21}, {"referenceID": 232, "context": "Kumar and Udupa [223] extend Spectral Hashing [235] from the single view setting to the multi-view scenario and present cross view hashing (CVH), which attempts to find hash functions that map similar objects to similar codewords over all the views so that inter-view and intraview similarities are both preserved.", "startOffset": 46, "endOffset": 51}, {"referenceID": 221, "context": "Gong and Lazebink [224] combine iterative quantization with CCA to exploit cross-modal embeddings for learning similarity preserving binary codes.", "startOffset": 18, "endOffset": 23}, {"referenceID": 222, "context": "Consequently, Zhen and Yang [225] present co-regularized hashing (CRH) for multi-modal data based on a boosted co-regularization framework.", "startOffset": 28, "endOffset": 33}, {"referenceID": 223, "context": "[226] introduce linear cross-modal hashing (LCMH), which is adopts a \u201ctwo-stage\u201d strategy to learn cross-view hashing functions.", "startOffset": 0, "endOffset": 5}, {"referenceID": 233, "context": "The data within each modality are first encoded into a low-rank representation using the idea of anchor graph [236] and then hash functions for each modality are learned to map each modality\u2019s low-rank space into a shared Hamming space.", "startOffset": 110, "endOffset": 115}, {"referenceID": 234, "context": "[237] introduce an inter-media hashing (IMH) model by jointly capturing inter-media and intra-media consistency.", "startOffset": 0, "endOffset": 5}, {"referenceID": 183, "context": "[184] present sparse multi-modal hashing by introducing dictionary learning into multi-view hashing, where the intra-modality and inter-modality similarities are modeled with a hypergraph.", "startOffset": 0, "endOffset": 5}, {"referenceID": 231, "context": "[234] additionally exploit discriminative information when learning coupled dictionaries to make the shared dictionary space interpretable.", "startOffset": 0, "endOffset": 5}, {"referenceID": 235, "context": "[238] present a latent semantic sparse hashing model by unifying sparse coding and matrix factorization.", "startOffset": 0, "endOffset": 5}, {"referenceID": 236, "context": "[239] present quantized correlation hashing (QCH).", "startOffset": 0, "endOffset": 5}, {"referenceID": 4, "context": "Inspired by the success of deep neural networks [5, 6, 124], a variety of deep multi-view feature learning methods have been proposed to capture the high-level correlation between multi-view data.", "startOffset": 48, "endOffset": 59}, {"referenceID": 5, "context": "Inspired by the success of deep neural networks [5, 6, 124], a variety of deep multi-view feature learning methods have been proposed to capture the high-level correlation between multi-view data.", "startOffset": 48, "endOffset": 59}, {"referenceID": 123, "context": "Inspired by the success of deep neural networks [5, 6, 124], a variety of deep multi-view feature learning methods have been proposed to capture the high-level correlation between multi-view data.", "startOffset": 48, "endOffset": 59}, {"referenceID": 16, "context": "The graphical model of deep multi-modal RBM (adapted from [17]), which models the joint distribution over image and text inputs.", "startOffset": 58, "endOffset": 62}, {"referenceID": 184, "context": ", Gaussian RBM [185] and replicated softmax RBM [241].", "startOffset": 15, "endOffset": 20}, {"referenceID": 237, "context": ", Gaussian RBM [185] and replicated softmax RBM [241].", "startOffset": 48, "endOffset": 53}, {"referenceID": 16, "context": "By extending the setup of the DBM, Srivastava and Salakhutdinov [17] propose a deep multi-modal RBM to model the relationship between image and text.", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "Like RBM, exact maximum likelihood learning in this model is also intractable, while efficient approximate learning can be implemented by using mean-field inference to estimate datadependent expectations, and an MCMC based stochastic approximation procedure to approximate the model\u2019s expected sufficient statistics [6].", "startOffset": 316, "endOffset": 319}, {"referenceID": 238, "context": "Multi-modal DBM has been widely used for multi-view representation learning [242\u2013244].", "startOffset": 76, "endOffset": 85}, {"referenceID": 239, "context": "Multi-modal DBM has been widely used for multi-view representation learning [242\u2013244].", "startOffset": 76, "endOffset": 85}, {"referenceID": 240, "context": "Multi-modal DBM has been widely used for multi-view representation learning [242\u2013244].", "startOffset": 76, "endOffset": 85}, {"referenceID": 239, "context": "[243] employ the multimodal DBM to learn joint representation for predicting answers in cQA portal.", "startOffset": 0, "endOffset": 5}, {"referenceID": 240, "context": "[244] apply the multi-modal RBM to determining information trustworthiness, in which the learned joint representation denotes the consistent latent reasons that underline users\u2019 ratings from multiple sources.", "startOffset": 0, "endOffset": 5}, {"referenceID": 241, "context": "Pang and Ngo [245] propose to learn a joint density model for emotion prediction in usergenerated videos with a deep multi-modal Boltzmann machine.", "startOffset": 13, "endOffset": 18}, {"referenceID": 242, "context": "[246] investigate an improved multi-modal RBM framework via minimizing the variation of information between data modalities through the shared latent representation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 243, "context": "[247] present a multi-task multi-modal RBM (MTM-RBM) approach for facial attribute classification.", "startOffset": 0, "endOffset": 5}, {"referenceID": 244, "context": "In particular, a multi-task RBM is proposed by extending the formulation of discriminative RBM [248] to account for multiple tasks.", "startOffset": 95, "endOffset": 100}, {"referenceID": 17, "context": "The bimodal deep autoencoder (adapted from [18]).", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "Multi-modal deep autoencoders [18, 249\u2013251] gradually become good alternatives for learning a shared representation between modalities due to the sufficient flexibility of their objectives.", "startOffset": 30, "endOffset": 43}, {"referenceID": 245, "context": "Multi-modal deep autoencoders [18, 249\u2013251] gradually become good alternatives for learning a shared representation between modalities due to the sufficient flexibility of their objectives.", "startOffset": 30, "endOffset": 43}, {"referenceID": 246, "context": "Multi-modal deep autoencoders [18, 249\u2013251] gradually become good alternatives for learning a shared representation between modalities due to the sufficient flexibility of their objectives.", "startOffset": 30, "endOffset": 43}, {"referenceID": 247, "context": "Multi-modal deep autoencoders [18, 249\u2013251] gradually become good alternatives for learning a shared representation between modalities due to the sufficient flexibility of their objectives.", "startOffset": 30, "endOffset": 43}, {"referenceID": 4, "context": "Inspired by denoising autoencoders [5], Ngiam et al.", "startOffset": 35, "endOffset": 38}, {"referenceID": 17, "context": "[18] propose to extract shared representations via training a bimodal deep autoencoder (Figure 5) using an augmented but noisy dataset with additional examples that have only a single modality as input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 248, "context": "The key idea is to use greedy layer-wise training with an extension to RBMs with sparsity [252] followed by fine-tuning.", "startOffset": 90, "endOffset": 95}, {"referenceID": 18, "context": "[19] propose a correspondence autoencoder (Corr-AE) via constructing correlations between hidden representations of two uni-modal deep autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The correspondence deep autoencoder (adapted from [19]).", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "Besides, based on two other multi-modal autoencoders [18], Corr-AE is extended to two other correspondence deep models, called CorrCross-AE and Corr-Full-AE.", "startOffset": 53, "endOffset": 57}, {"referenceID": 249, "context": "[253] propose an effective mapping mechanism based on stacked autoencoder for multimodal retrieval.", "startOffset": 0, "endOffset": 5}, {"referenceID": 19, "context": "[20] propose deep canonically correlated autoencoders that also consist of two autoencoders and optimize the combination of canonical correlations between the learned bottleneck representations and the reconstruction errors of the autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 250, "context": "[254] suggest to exploit the cross weights between representations of modalities for gradually learning interactions of the modalities in a multi-modal deep autoencoder network.", "startOffset": 0, "endOffset": 5}, {"referenceID": 251, "context": "Deep cross-view embedding models have become increasingly popular in the applications including cross-media retrieval [255, 256] and multi-modal distributional semantic learning [257, 258].", "startOffset": 118, "endOffset": 128}, {"referenceID": 252, "context": "Deep cross-view embedding models have become increasingly popular in the applications including cross-media retrieval [255, 256] and multi-modal distributional semantic learning [257, 258].", "startOffset": 118, "endOffset": 128}, {"referenceID": 253, "context": "Deep cross-view embedding models have become increasingly popular in the applications including cross-media retrieval [255, 256] and multi-modal distributional semantic learning [257, 258].", "startOffset": 178, "endOffset": 188}, {"referenceID": 254, "context": "Deep cross-view embedding models have become increasingly popular in the applications including cross-media retrieval [255, 256] and multi-modal distributional semantic learning [257, 258].", "startOffset": 178, "endOffset": 188}, {"referenceID": 251, "context": "[255] propose a deep visual-semantic embedding model (DeViSE), which connects two deep neural networks by a cross-modal mapping.", "startOffset": 0, "endOffset": 5}, {"referenceID": 251, "context": "The DeViSE model (adapted from [255]), which is initialized with parameters pre-trained at the lower layers of the visual object categorization network and the skip-gram language model.", "startOffset": 31, "endOffset": 36}, {"referenceID": 255, "context": "initialized with a pre-trained neural network language model [259] and a pre-trained deep visual-semantic model [124].", "startOffset": 61, "endOffset": 66}, {"referenceID": 123, "context": "initialized with a pre-trained neural network language model [259] and a pre-trained deep visual-semantic model [124].", "startOffset": 112, "endOffset": 117}, {"referenceID": 210, "context": "Following the setup of loss function in [212], DeViSE employs a combination of dot-product similarity and hinge rank loss so that the model has the ability of producing a higher dot-product similarity between the visual model output and the vector representation of the correct label than between the visual output and the other randomly chosen text terms.", "startOffset": 40, "endOffset": 45}, {"referenceID": 256, "context": "[261] propose a convex combination of semantic embedding model (ConSE) for mapping images into continuous semantic embedding spaces.", "startOffset": 0, "endOffset": 5}, {"referenceID": 257, "context": "[262] develop a deep multi-modal similarity model that learns two neural networks to map images and text fragments to a common vector representation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 258, "context": "With the development of multi-modal distributional semantic models [263\u2013265], deep cross-modal mapping is naturally exploited to learn the improved multimodal distributed semantic representation.", "startOffset": 67, "endOffset": 76}, {"referenceID": 259, "context": "With the development of multi-modal distributional semantic models [263\u2013265], deep cross-modal mapping is naturally exploited to learn the improved multimodal distributed semantic representation.", "startOffset": 67, "endOffset": 76}, {"referenceID": 260, "context": "With the development of multi-modal distributional semantic models [263\u2013265], deep cross-modal mapping is naturally exploited to learn the improved multimodal distributed semantic representation.", "startOffset": 67, "endOffset": 76}, {"referenceID": 254, "context": "[258] introduce multimodal skipgram models to extend the skip-gram model of [259] by taking visual information into account.", "startOffset": 0, "endOffset": 5}, {"referenceID": 255, "context": "[258] introduce multimodal skipgram models to extend the skip-gram model of [259] by taking visual information into account.", "startOffset": 76, "endOffset": 81}, {"referenceID": 252, "context": "[256] present Word2VisualVec, a deep neural network architecture that learns to predict a deep visual encoding of textual input, and thus enables cross-media retrieval in a visual space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 261, "context": "[266] propose a unified framework that jointly models video and the corresponding text sentence.", "startOffset": 0, "endOffset": 5}, {"referenceID": 262, "context": "[267] present a unified deep neural network model called cross space mapping, in which the image and query are mapped to a common vector space via a convolution part and a query-embedding part, respectively.", "startOffset": 0, "endOffset": 5}, {"referenceID": 263, "context": "[268] also introduce a deep cross-modal retrieval method, called deep compositional cross-modal learning to rank (CMLR).", "startOffset": 0, "endOffset": 5}, {"referenceID": 264, "context": "[269] introduce a deep semantic matching method, in which two independent deep networks are learned to map image and text into a common semantic space with a high level abstraction.", "startOffset": 0, "endOffset": 5}, {"referenceID": 265, "context": "[270] consider learning multi-modal representation from the perspective of encoding the explicit/implicit relevance relationship between the vertices in the click graph, in which vertices are images/text queries and edges indicate the clicks between an image and a query.", "startOffset": 0, "endOffset": 5}, {"referenceID": 16, "context": "Motivated by the recent advance of deep learning for multi-modal data [17, 18, 255], deep neural networks have gradually been exploited to learn hash functions.", "startOffset": 70, "endOffset": 83}, {"referenceID": 17, "context": "Motivated by the recent advance of deep learning for multi-modal data [17, 18, 255], deep neural networks have gradually been exploited to learn hash functions.", "startOffset": 70, "endOffset": 83}, {"referenceID": 251, "context": "Motivated by the recent advance of deep learning for multi-modal data [17, 18, 255], deep neural networks have gradually been exploited to learn hash functions.", "startOffset": 70, "endOffset": 83}, {"referenceID": 266, "context": "[271] introduce a deep multi-view hashing method in which each layer of hidden nodes consists of view-specific and shared hidden nodes to learn individual and shared hidden spaces from multiple views of data.", "startOffset": 0, "endOffset": 5}, {"referenceID": 267, "context": "[272] propose a cross-media hashing approach based on a correspondence multi-modal neural network, referred as Cross-Media Neural Network Hashing (CMNNH).", "startOffset": 0, "endOffset": 5}, {"referenceID": 267, "context": "The illustration of cross-media neural network hashing architecture (adapted from [272]).", "startOffset": 82, "endOffset": 87}, {"referenceID": 219, "context": "By following the setup of most of the existing hashing methods [222, 235], the sign function can be removed at the hash function learning stage and added at the testing stage.", "startOffset": 63, "endOffset": 73}, {"referenceID": 232, "context": "By following the setup of most of the existing hashing methods [222, 235], the sign function can be removed at the hash function learning stage and added at the testing stage.", "startOffset": 63, "endOffset": 73}, {"referenceID": 268, "context": "[273] introduce a deep multi-modal hashing model with orthogonal regularization for mapping multi-modal data into a common hamming space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 272, "context": "The illustration of the RNN encoder-decoder (adapted from [277]).", "startOffset": 58, "endOffset": 63}, {"referenceID": 269, "context": "Recently, Jiang and Li [274] propose a deep cross-modal hashing method (DCMH) by integrating feature learning and hashcode learning into the same framework.", "startOffset": 23, "endOffset": 28}, {"referenceID": 18, "context": "present a fully correlation autoencoder hashing method by extending the correspondence autoencoder by [19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 270, "context": "A recurrent neural network (RNN) [275] is a neural network which processes a variable-length sequence x = (x1, .", "startOffset": 33, "endOffset": 38}, {"referenceID": 271, "context": "For example, a simple case may be a common element-wise logistic sigmoid function and a complex case may be a long short-term memory (LSTM) unit [276].", "startOffset": 145, "endOffset": 150}, {"referenceID": 272, "context": "[277] propose a RNN encoder-decoder model by exploiting RNN to connect multi-modal sequence.", "startOffset": 0, "endOffset": 5}, {"referenceID": 273, "context": "[278] also present a general endto-end approach for multi-modal sequence to sequence learning based on deep LSTM networks, which is very useful for learning problems with long range temporal dependencies [276, 279].", "startOffset": 0, "endOffset": 5}, {"referenceID": 271, "context": "[278] also present a general endto-end approach for multi-modal sequence to sequence learning based on deep LSTM networks, which is very useful for learning problems with long range temporal dependencies [276, 279].", "startOffset": 204, "endOffset": 214}, {"referenceID": 274, "context": "[278] also present a general endto-end approach for multi-modal sequence to sequence learning based on deep LSTM networks, which is very useful for learning problems with long range temporal dependencies [276, 279].", "startOffset": 204, "endOffset": 214}, {"referenceID": 272, "context": "Similar to [277], the conditional probability is computed by first obtaining the fixed dimensional representation v of the input sequence (x1, .", "startOffset": 11, "endOffset": 16}, {"referenceID": 20, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 71, "endOffset": 84}, {"referenceID": 21, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 71, "endOffset": 84}, {"referenceID": 275, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 71, "endOffset": 84}, {"referenceID": 22, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 104, "endOffset": 118}, {"referenceID": 276, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 104, "endOffset": 118}, {"referenceID": 277, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 104, "endOffset": 118}, {"referenceID": 278, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 150, "endOffset": 155}, {"referenceID": 20, "context": "Karpathy and Li [21] propose a multi-modal recurrent neural network architecture to generate new descriptions of image regions.", "startOffset": 16, "endOffset": 20}, {"referenceID": 279, "context": "Chen and Zitnick [284] explore the bi-directional mapping between images and their sentencebased descriptions with RNNs.", "startOffset": 17, "endOffset": 22}, {"referenceID": 277, "context": "[282] introduce an end-to-end sequence model to generate captions for videos.", "startOffset": 0, "endOffset": 5}, {"referenceID": 280, "context": "By applying attention mechanism [285] to visual recognition [286, 287], Xu et al.", "startOffset": 32, "endOffset": 37}, {"referenceID": 281, "context": "By applying attention mechanism [285] to visual recognition [286, 287], Xu et al.", "startOffset": 60, "endOffset": 70}, {"referenceID": 282, "context": "By applying attention mechanism [285] to visual recognition [286, 287], Xu et al.", "startOffset": 60, "endOffset": 70}, {"referenceID": 283, "context": "[288] introduce an attention based multimodal RNN model, which trains the multi-modal RNN in a deterministic manner using the standard back propagation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 285, "context": "This semi-supervised alignment is similar to manifold ranking [290], which learns to rank on data manifolds.", "startOffset": 62, "endOffset": 67}, {"referenceID": 284, "context": "[289] extend the original semi-supervised alignment and proposed a method called parallel field alignment retrieval (PFAR), which investigates alignment framework from the perspective of parallel vector fields.", "startOffset": 0, "endOffset": 5}, {"referenceID": 284, "context": "An illustrative example of parallel field alignment (adapted from [289]).", "startOffset": 66, "endOffset": 71}, {"referenceID": 23, "context": "[24] also introduce manifold alignment with pairwise correspondence, which builds connections between multiple data sets by aligning their underlying manifolds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 286, "context": "[291] propose a two-step manifold alignment to solve this problem.", "startOffset": 0, "endOffset": 5}, {"referenceID": 287, "context": "Further, Wang and Mahadevan [292] also introduce a twostage approach for manifold alignment using procrustes analysis.", "startOffset": 28, "endOffset": 33}], "year": 2016, "abstractText": "Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper first reviews the root methods and theories on multi-view representation learning, especially on canonical correlation analysis (CCA) and its several extensions. And then we investigate the advancement of multi-view representation learning that ranges from shallow methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to deep methods including multi-modal restricted Boltzmann machines, multi-modal autoencoders, and multi-modal recurrent neural networks. Further, we also provide an important perspective from manifold alignment for multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical basis and current developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.", "creator": "LaTeX with hyperref package"}}}