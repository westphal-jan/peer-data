{"id": "1610.01874", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Neural-based Noise Filtering from Word Embeddings", "abstract": "Word groundmass have been significant to benefit NLP assignments impressively. Yet, there is fill for improvement opened it formula_4 representations, because moreover joke embeddings large contain draconian information, i. call. , ventilation. We formulate their literature example before increasing word pgl by chaperoned learning, in use to 30-year what denoising dyadic. The word denoising hypercube are certificate latter deteriorating salient information having ease noise where entire dates: embeddings, group on a deep bird - try neural programming contaminants. Results from closes process new say the condense bible 3000-meter naturale fargo this referred why role-play.", "histories": [["v1", "Thu, 6 Oct 2016 13:52:52 GMT  (184kb,D)", "http://arxiv.org/abs/1610.01874v1", "9 pages, 4 figures, COLING 2016"]], "COMMENTS": "9 pages, 4 figures, COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kim anh nguyen", "sabine schulte im walde", "ngoc thang vu"], "accepted": false, "id": "1610.01874"}, "pdf": {"name": "1610.01874.pdf", "metadata": {"source": "CRF", "title": "Neural-based Noise Filtering from Word Embeddings", "authors": ["Kim Anh Nguyen", "Sabine Schulte"], "emails": ["nguyenkh@ims.uni-stuttgart.de", "schulte@ims.uni-stuttgart.de", "thangvu@ims.uni-stuttgart.de"], "sections": [{"heading": "1 Introduction", "text": "Word embeddings aim to represent words as low-dimensional dense vectors. In comparison to distributional count vectors, word embeddings address the problematic sparsity of word vectors and achieved impressive results in many NLP tasks such as sentiment analysis (e.g., Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)). Moreover, word embeddings are attractive because they can be learned in an unsupervised fashion from unlabeled raw corpora. There are two main approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices.\nIn recent years, a number of approaches have focused on improving word embeddings, often by integrating lexical resources. For example, Adel and Schu\u0308tze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexical resources, by applying ideas from sparse coding to transform dense word embeddings into sparse word embeddings. The dense vectors in their models can be transformed into sparse overcomplete vectors or sparse binary overcomplete vectors. They showed that the resulting vector representations were more similar to interpretable features in NLP and outperformed the original vector representations on several benchmark tasks.\nIn this paper, we aim to improve word embeddings by reducing their noise. The hypothesis behind our approaches is that word embeddings contain unnecessary information, i.e. noise. We start out with the idea of learning word embeddings as suggested by Mikolov et al. (2013), relying on the distributional hypothesis (Harris, 1954) that words with similar distributions have related meanings. We address those distributions in embedded vectors of words that decrease the value of such vector representations. For instance, consider the sentence the quick brown fox gazing at the cloud jumped over the lazy dog. The\nar X\niv :1\n61 0.\n01 87\n4v 1\n[ cs\n.C L\n] 6\nO ct\n2 01\n6\ncontext jumped can be used to predict the words fox, cloud and dog in a window size of 5 words; however, a cloud cannot jump. The context jumped is therefore considered as noise in the embedded vector of cloud. We propose two novel models to smooth word embeddings by filtering noise: We strengthen salient contexts and weaken unnecessary contexts.\nThe first proposed model is referred to as complete word denoising embeddings model (CompEmb). Given a set of original word embeddings, we use a filter to learn a denoising matrix, and then project the set of original word embeddings into this denoising matrix to produce a set of complete word denoising embeddings. The second proposed model is referred to as overcomplete word denoising embeddings model (OverCompEmb). We make use of a sparse coding method to transform an input set of original word embeddings into a set of overcomplete word embeddings, which is considered as the \u201covercomplete process\u201d. We then apply a filter to train a denoising matrix, and thereafter project the set of original word embeddings into the denoising matrix to generate a set of overcomplete word denoising embeddings. The key idea in our models is to use a filter for learning the denoising matrix. The architecture of the filter is a feed-forward, non-linear and parameterized neural network with a fixed depth that can be used to learn the denoising matrices and reduce noise in word embeddings. Using state-of-the-art word embeddings as input vectors, we show that the resulting word denoising embeddings outperform the original word embeddings on several benchmark tasks such as word similarity and word relatedness tasks, synonymy detection and noun phrase classification. Furthermore, the implementation of our models is made publicly available1.\nThe remainder of this paper is organized as follows: Section 2 presents the two proposed models, the loss function, and the sparse coding technique for overcomplete vectors. In Section 3, we demonstrate the experiments on evaluating the effects of our word denoising embeddings, tuning hyperparameters, and we analyze the effects of filter depth. Finally, Section 4 concludes the paper."}, {"heading": "2 Learning Word Denoising Embeddings", "text": "In this section, we present the two contributions of this paper. Figure 1 illustrates our two models to learn denoising for word embeddings. The first model on the top, the complete word denoising embeddings model \u201cCompEmb\u201d (Section 2.1), filters noise from word embeddings X to produce complete word denoising embeddings X\u2217, in which the vector length of X\u2217 in comparison to X is unchanged after denoising (called complete). The second model at the bottom of the figure, the overcomplete word denoising embeddings model \u201cOverCompEmb\u201d (Section 2.2), filters noise from word embeddings X to yield overcomplete word denoising embeddings Z\u2217, in which the vector length of Z\u2217 tends to be greater than the vector length of X (called overcomplete).\nFor the notations, let X \u2208 RV\u00d7L is an input set of word embeddings in which V is the vocabulary size, andL is the vector length of X. Furthermore, Z \u2208 RV\u00d7K is the overcomplete word embeddings in which K is the vector length of Z (K > L); finally, D \u2208 RL\u00d7L is the pre-trained dictionary (Section 2.4)."}, {"heading": "2.1 Complete Word Denoising Embeddings", "text": "In this subsection, we aim to reduce noise in the given input word embeddings X by learning a denoising matrix Qc. The complete word denoising embeddings X\u2217 are then generated by projecting X into Qc. More specifically, given an input X \u2208 RV\u00d7L, we seek to optimize the following objective function:\nargmin X,Qc,S V\u2211 i=1 \u2016xi \u2212 f(xi,Qc,S)\u2016+ \u03b1\u2016S\u20161 (1)\nwhere f is a filter; S is a lateral inhibition matrix; and \u03b1 is a regularization hyperparameter. Inspired by studies on sparse modeling, the matrix S is chosen to be symmetric and has zero on the diagonal. The goal of this matrix is to implement excitatory interaction between neurons, and to increase the convergence speed of the neural network (Szlam et al., 2011). More concretely, the matrices Qc and S\n1https://github.com/nguyenkh/NeuralDenoising\nare initialized with I and E, which are identity matrices, and the Lipschitz constant:\nQc = 1 ED; S = I\u2212 1EDTD E > the largest eigenvalue of DTD D \u2208 RL\u00d7L be pre-trained dictionary\nThe underlying idea for reducing noise is to make use of a filter f to learn a denoising matrix Qc; hence, we design the filter f as a non-linear, parameterized, feed-forward architecture with a fixed depth that can be trained to approximate f(X,Qc,S) to X as in Figure 2a. As a result, noise from word embeddings will be filtered by layers of the filter f . The filter f is encoded as a recursive function by iterating over the number of fixed depth T , as the following recursive Equation 2 shows:\nY = f(X,Qc,S) Y(0) = G(XQc) Y(k + 1) = G(XQc + Y(k)S) 0 \u2264 k < T\n(2)\nG is a non-linear activation function. The matrices Qc and S are learned to produce the lowest possible error in a given number of iterations. Matrix S, in the architecture of filter f , acts as a controllable matrix to filter unnecessary information on embedded vectors, and to impose restrictions on further reducing the computational burden (e.g., solving low-rank approximation problem or keeping the number of terms at zero (Gregor and LeCun, 2010)). Moreover, the initialization of the matrices Qc, S and E enhances a\nhighly efficient minimization of the objective function in Equation 1, due to the pre-trained dictionary D that carries the information of reconstructing X.\nThe architecture of the filter f is a recursive feed-forward neural network with the fixed depth T , so the number of T plays a significant role in controlling the approximation of X\u2217. The effects of T will be discussed later in Section 3.4. When Qc is trained, the complete word denoising embeddings X\u2217 are yielded by projecting X into Qc, as shown by the following Equation 3:\nX\u2217 = G(XQc) (3)"}, {"heading": "2.2 Overcomplete Word Denoising Embeddings", "text": "Now we introduce our method to reduce noise and overcomplete vectors in the given input word embeddings. To obtain overcomplete word embeddings, we first use a sparse coding method to transform the given input word embeddings X into overcomplete word embeddings Z. Secondly, we use overcomplete word embeddings Z as the intermediate word embeddings to optimize the objective function: A set of input word embeddings X \u2208 RV\u00d7L is transformed to overcomplete word embeddings Z \u2208 RV\u00d7K by applying sparse coding method in Section 2.4. We then make use of the pre-trained dictionary D \u2208 RL\u00d7K and Z \u2208 RV\u00d7K to learn the denoising matrix Qo by minimizing the following Equation 4:\nargmin X,Qo,S V\u2211 i=1 \u2016zi \u2212 f(xi,Qo,S)\u2016+ \u03b1\u2016S\u20161 (4)\nThe initialization of the parameters Qo, S, E and \u03b1 follows the same procedure as described in Section 2.1, and with the same interpretation of the filter architecture in Figure 2b. The overcomplete word denoising embeddings Z\u2217 are then generated by projecting X into the denoising matrix Qo and using the non-linear activation function G in the following Equation 5:\nZ\u2217 = G(XQo) (5)"}, {"heading": "2.3 Loss Function", "text": "For each pair of term vectors xi \u2208 X and yi \u2208 Y = f(X,Qc,S), we make use of the cosine similarity to measure the similarity between xi and yi as follows:\nsim(xi,yi) = xi \u00b7 yi \u2016xi\u2016\u2016yi\u2016\n(6)\nLet \u2206 be the difference between sim(xi,xi) and sim(xi,yi), equivalently \u2206 = 1 \u2212 sim(xi,yi). We then optimize the objective function in Equation 1 by minimizing \u2206; and the same loss function is also applied to optimize the objective function in Equation 4. Training is done through Stochastic Gradient Descent with the Adadelta update rule (Zeiler, 2012)."}, {"heading": "2.4 Sparse Coding", "text": "Sparse coding is a method to represent vector representations as a sparse linear combination of elementary atoms of a given dictionary. The underlying assumption of sparse coding is that the input vectors can be reconstructed accurately as a linear combination of some basis vectors and a few number of non-zero coefficients (Olshausen and Field, 1996).\nThe goal is to approximate a dense vector in RL by a sparse linear combination of a few columns of a matrix D \u2208 RL\u00d7K in which K is a new vector length and the matrix D be called a dictionary. Concretely, given V input vectors of L dimensions X = [x1, x2, ..., xV ], the dictionary and sparse vectors can be formulated as the following minimization problem:\nmin D\u2208C,Z\u2208RK\u00d7V V\u2211 i=1 \u2016xi \u2212Dzi\u201622 + \u03bb\u2016zi\u20161 (7)\nZ = [z1, ..., zV ] carries the decomposition coefficients of X = [x1, x2, ..., xV ]; and \u03bb represents a scalar to control the sparsity level of Z. The dictionary D is typically learned by minimizing Equation 7 over input vectors X. In the case of overcomplete representations Z, the vector length K is typically implied as K = \u03b3L (\u03b3 > 0).\nIn the method of overcomplete word denoising embeddings (Section 2.2), our approach makes use of overcomplete word embeddings Z as the intermediate word embeddings reconstructed by applying a sparse coding method to word embeddings X. The overcomplete word embeddings Z are then utilized to optimize Equation 4. To obtain overcomplete word embeddings Z and dictionaries, we use the SPAMS package2 to implement sparse coding for word embeddings X and to train the dictionaries D."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Experimental Settings", "text": "As input word embeddings, we rely on two state-of-the-art word embeddings methods: word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). We use the word2vec tool3 and the web corpus ENCOW14A (Scha\u0308fer and Bildhauer, 2012; Scha\u0308fer, 2015) which contains approximately 14.5 billion tokens, in order to train Skip-gram models with 100 and 300 dimensions. For the GloVe method, we use pre-trained vectors of 100 and 300 dimensions4 that were trained on 6 billion words from Wikipedia and English Gigaword. The tanh function is used as the non-linear activation function in both approaches. The fixed depth of filter T is set to 3; further hyperparameters are chosen as discussed in Section 3.2. To train the networks, we use the Theano framework (Theano Development Team, 2016) to implement our models with a mini-batch size of 100. Regularization is applied by dropouts of 0.5 and 0.2 for input and output layers (without tuning), respectively."}, {"heading": "3.2 Hyperparameter Tuning", "text": "In both methods of denoising word embeddings, the `1 regularization penalty \u03b1 is set to 0.5 without tuning in Equation 1 and 4. The method of learning overcomplete word denoising embeddings relies on the mediate word embeddings Z to minimize the objective function in Equation 4. The sparsity of Z depends on the `1 regularization \u03bb in Equation 7; and the length vector K of Z is implied as K = \u03b3L. Therefore, we aim to tune \u03bb and \u03b3 such that Z represents the nearest approximation of the original vector representation X. We perform a grid search on \u03bb \u2208 {1.0, 0.5, 0.1, 10\u22123, 10\u22126} and \u03b3 \u2208 {2, 3, 5, 7, 10, 13, 15}, developing on the word similarity task WordSim353 (to be discussed on Section 3.3). The hyperparameter tunings are illustrated in Figures 3a and 3b for sparsity and overcomplete vector length tuning, respectively. In both approaches, we set \u03bb to 10\u22126 and \u03b3 to 10 for the sparsity and length of overcomplete word embeddings.\n2http://spams-devel.gforge.inria.fr 3https://code.google.com/p/word2vec/ 4http://www-nlp.stanford.edu/projects/glove/"}, {"heading": "3.3 Effects of Word Denoising Embeddings", "text": "In this section, we quantify the effects of word denoising embeddings on three kinds of tasks: similarity and relatedness tasks, detecting synonymy, and bracketed noun phrase classification task. In comparison to the performance of word denoising embeddings, we take into account state-of-the-art word embeddings (Skip-gram and GloVe word embeddings) as baselines. Besides, we also use the public source code5 to re-implement the two methods suggested by Faruqui et al. (2015) which are vectors A (sparse overcomplete vectors) and B (sparse binary overcomplete vectors).\nThe effects of the word denoising embeddings on the tasks are shown in Table 1. The results show that the vectors X\u2217 and Z\u2217 outperform the original vectors X,A and B, except for the NP task, in which the vectors B based on the 300-dimensional GloVe vectors are best. The effect of the vectors Z\u2217 is slightly less impressive, when compared to the overcomplete vectors X\u2217. The overcomplete word embeddings Z strongly differ from the word embeddings X; hence, the denoising is affected. However, the performance of the vectors Z\u2217 still outperforms the original vectors X,A and B after the denoising process."}, {"heading": "3.3.1 Relatedness and Similarity Tasks", "text": "For the relatedness task, we use two kinds of datasets: MEN (Bruni et al., 2014) consists of 3000 word pairs comprising 656 nouns, 57 adjectives and 38 verbs. The WordSim-353 relatedness dataset (Finkelstein et al., 2001) contains 252 word pairs. Concerning the similarity tasks, we evaluate the denoising vectors again on two kinds of datasets: SimLex-999 (Hill et al., 2015) contains 999 word pairs including 666 noun, 222 verb and 111 adjective pairs. The WordSim-353 similarity dataset consists of 203 word pairs. In addition, we evaluate our denoising vectors on the WordSim-353 dataset which contains 353 pairs for both similarity and relatedness relations. We calculate cosine similarity between the vectors of two words forming a test pair, and report the Spearman rank-order correlation coefficient \u03c1 (Siegel and Castellan, 1988) against the respective gold standards of human ratings."}, {"heading": "3.3.2 Synonymy", "text": "We evaluate on 80 TOEFL (Test of English as a Foreign Language) synonym questions (Landauer and Dumais, 1997) and 50 ESL (English as a Second Language) questions (Turney, 2001). The first dataset represents a subset of 80 multiple-choice synonym questions from the TOEFL test: a word is paired with four options, one of which is a valid synonym. The second dataset contains 50 multiple-choice synonym questions, and the goal is to choose a valid synonym from four options. For each question, we compute the cosine similarity between the target word and the four candidates. The suggested answer is the candidate with the highest cosine score. We use accuracy to evaluate the performance.\n5https://github.com/mfaruqui/sparse-coding"}, {"heading": "3.3.3 Phrase parsing as Classification", "text": "Lazaridou et al. (2013) introduced a dataset of noun phrases (NP) in which each NP consists of three elements: the first element is either an adjective or a noun, and the other elements are all nouns. For a given NP (such as blood pressure medicine), the task is to predict whether it is a left-bracketed NP, e.g., (blood pressure) medicine, or a right-bracketed NP, e.g., blood (pressure medicine).\nThe dataset contains 2227 noun phrases split into 10 folds. For each NP, we use the average of word vectors as features to feed into the classifier by tuning the hyperparameters (w1, w2 and w3) for each element (e1, e2 and e3) within the NP: ~eNP = 13(w1~e1+w2~e2+w3~e3). We then employ the classification of the NPs by using a Support Vector Machine (SVM) with Radial Basis Function kernel. The classifier is tuned on the first fold, and cross-validation accuracy is reported on the nine remaining folds."}, {"heading": "3.4 Effects of Filter Depth", "text": "As mentioned above, the architecture of the filter f is a feed-forward network with a fixed depth T . For each stage T , the filter f attempts to reduce the noise within input vectors by approximating these vectors based on vectors of a previous stage T \u2212 1. In order to investigate the effects of each stage T , we use pre-trained GloVe vectors with 100 dimensions to evaluate the denoising performance of the vectors on detecting synonymy in the TOEFL dataset across several stages of T .\nThe results are presented in Figure 4. The accuracy of synonymy detection increases sharply from 63.2% to 88.6% according to the number of stages T from 0 to 3. However, the denoising performance of vectors falls with the number of stages T > 3. This evaluation shows that the filter f with a consistently fixed depth T can be trained to efficiently filter noise for word embeddings. In other words, the number of stages T exceeds a consistent number T (with T > 3 in our case), leading to the loss of salient information in the vectors."}, {"heading": "4 Conclusion", "text": "To the best of our knowledge, we are the first to work on filtering noise in word embeddings. In this paper, we have presented two novel models to improve word embeddings by reducing noise in stateof-the-art word embeddings models. The underlying idea in our models was to make use of a deep feed-forward neural network filter to reduce noise. The first model generated complete word denoising embeddings; the second model yielded overcomplete word denoising embeddings. We demonstrated that the word denoising embeddings outperform the originally state-of-the-art word embeddings on several benchmark tasks."}, {"heading": "Acknowledgements", "text": "The research was supported by the Ministry of Education and Training of the Socialist Republic of Vietnam (Scholarship 977/QD-BGDDT; Nguyen Kim Anh) and the DFG Heisenberg Fellowship SCHU2580/1 (Sabine Schulte im Walde). It is also a collaboration between project D12 and project A8 in the DFG Collaborative Research Centre SFB 732."}], "references": [{"title": "Using mined coreference chains as a resource for a semantic task", "author": ["Heike Adel", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1447\u20131452, Doha, Qatar.", "citeRegEx": "Adel and Sch\u00fctze.,? 2014", "shortCiteRegEx": "Adel and Sch\u00fctze.", "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."], "venue": "Journal of Artifical Intelligence Research (JAIR), 49:1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Sparse overcomplete word vector representations", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 1491\u20131500, Beijing, China.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th International Conference on the World Wide Web, pages 406\u2013414.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Learning fast approximations of sparse coding", "author": ["Karol Gregor", "Yann LeCun."], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), Haifa, Israel, pages 399\u2013406.", "citeRegEx": "Gregor and LeCun.,? 2010", "shortCiteRegEx": "Gregor and LeCun.", "year": 2010}, {"title": "Distributional structure", "author": ["Zellig S. Harris."], "venue": "Word, 10(23):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics, 41(4):665\u2013695.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "A solution to Platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K. Landauer", "Susan T. Dumais."], "venue": "Psychological Review, 104(2):211\u2013240.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing", "author": ["Angeliki Lazaridou", "Eva Maria Vecchi", "Marco Baroni."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1908\u20131913, Doha, Qatar.", "citeRegEx": "Lazaridou et al\\.,? 2013", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 746\u2013751, Atlanta, Georgia.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Integrating distributional lexical contrast into word embeddings for antonym-synonym distinction", "author": ["Kim Anh Nguyen", "Sabine Schulte im Walde", "Ngoc Thang Vu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 454\u2013459, Berlin, Germany.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["Bruno A. Olshausen", "David J. Field."], "venue": "Nature, 381(6583):607\u2013609.", "citeRegEx": "Olshausen and Field.,? 1996", "shortCiteRegEx": "Olshausen and Field.", "year": 1996}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A multitask objective to inject lexical contrast into distributional semantics", "author": ["Nghia The Pham", "Angeliki Lazaridou", "Marco Baroni."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 21\u201326, Beijing, China.", "citeRegEx": "Pham et al\\.,? 2015", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "Building large corpora from the web using a new efficient tool chain", "author": ["Roland Sch\u00e4fer", "Felix Bildhauer."], "venue": "Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 486\u2013493, Istanbul, Turkey.", "citeRegEx": "Sch\u00e4fer and Bildhauer.,? 2012", "shortCiteRegEx": "Sch\u00e4fer and Bildhauer.", "year": 2012}, {"title": "Processing and querying large web corpora with the COW14 architecture", "author": ["Roland Sch\u00e4fer."], "venue": "Proceedings of the 3rd Workshop on Challenges in the Management of Large Corpora, pages 28\u201334, Mannheim, Germany.", "citeRegEx": "Sch\u00e4fer.,? 2015", "shortCiteRegEx": "Sch\u00e4fer.", "year": 2015}, {"title": "Nonparametric Statistics for the Behavioral Sciences", "author": ["Sidney Siegel", "N. John Castellan."], "venue": "McGraw-Hill, Boston, MA.", "citeRegEx": "Siegel and Castellan.,? 1988", "shortCiteRegEx": "Siegel and Castellan.", "year": 1988}, {"title": "Structured sparse coding via lateral inhibition", "author": ["Arthur D. Szlam", "Karol Gregor", "Yann L. Cun."], "venue": "Advances in Neural Information Processing Systems (NIPS), 24:1116\u20131124.", "citeRegEx": "Szlam et al\\.,? 2011", "shortCiteRegEx": "Szlam et al\\.", "year": 2011}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints, abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Mining the web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["Peter D. Turney."], "venue": "Proceedings of the 12th European Conference on Machine Learning (ECML), pages 491\u2013502.", "citeRegEx": "Turney.,? 2001", "shortCiteRegEx": "Turney.", "year": 2001}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013).", "startOffset": 110, "endOffset": 132}, {"referenceID": 13, "context": "The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices.", "startOffset": 53, "endOffset": 78}, {"referenceID": 5, "context": "(2013), relying on the distributional hypothesis (Harris, 1954) that words with similar distributions have related meanings.", "startOffset": 49, "endOffset": 63}, {"referenceID": 4, "context": ", Kim (2014)), word similarity (e.", "startOffset": 2, "endOffset": 13}, {"referenceID": 4, "context": ", Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.", "startOffset": 2, "endOffset": 63}, {"referenceID": 4, "context": ", Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)).", "startOffset": 2, "endOffset": 108}, {"referenceID": 0, "context": "For example, Adel and Sch\u00fctze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification.", "startOffset": 13, "endOffset": 37}, {"referenceID": 0, "context": "For example, Adel and Sch\u00fctze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet.", "startOffset": 13, "endOffset": 166}, {"referenceID": 0, "context": "For example, Adel and Sch\u00fctze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings.", "startOffset": 13, "endOffset": 447}, {"referenceID": 0, "context": "For example, Adel and Sch\u00fctze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexical resources, by applying ideas from sparse coding to transform dense word embeddings into sparse word embeddings.", "startOffset": 13, "endOffset": 717}, {"referenceID": 0, "context": "For example, Adel and Sch\u00fctze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexical resources, by applying ideas from sparse coding to transform dense word embeddings into sparse word embeddings. The dense vectors in their models can be transformed into sparse overcomplete vectors or sparse binary overcomplete vectors. They showed that the resulting vector representations were more similar to interpretable features in NLP and outperformed the original vector representations on several benchmark tasks. In this paper, we aim to improve word embeddings by reducing their noise. The hypothesis behind our approaches is that word embeddings contain unnecessary information, i.e. noise. We start out with the idea of learning word embeddings as suggested by Mikolov et al. (2013), relying on the distributional hypothesis (Harris, 1954) that words with similar distributions have related meanings.", "startOffset": 13, "endOffset": 1465}, {"referenceID": 18, "context": "The goal of this matrix is to implement excitatory interaction between neurons, and to increase the convergence speed of the neural network (Szlam et al., 2011).", "startOffset": 140, "endOffset": 160}, {"referenceID": 4, "context": ", solving low-rank approximation problem or keeping the number of terms at zero (Gregor and LeCun, 2010)).", "startOffset": 80, "endOffset": 104}, {"referenceID": 21, "context": "Training is done through Stochastic Gradient Descent with the Adadelta update rule (Zeiler, 2012).", "startOffset": 83, "endOffset": 97}, {"referenceID": 12, "context": "The underlying assumption of sparse coding is that the input vectors can be reconstructed accurately as a linear combination of some basis vectors and a few number of non-zero coefficients (Olshausen and Field, 1996).", "startOffset": 189, "endOffset": 216}, {"referenceID": 10, "context": "1 Experimental Settings As input word embeddings, we rely on two state-of-the-art word embeddings methods: word2vec (Mikolov et al., 2013) and GloVe (Pennington et al.", "startOffset": 116, "endOffset": 138}, {"referenceID": 13, "context": ", 2013) and GloVe (Pennington et al., 2014).", "startOffset": 18, "endOffset": 43}, {"referenceID": 15, "context": "We use the word2vec tool3 and the web corpus ENCOW14A (Sch\u00e4fer and Bildhauer, 2012; Sch\u00e4fer, 2015) which contains approximately 14.", "startOffset": 54, "endOffset": 98}, {"referenceID": 16, "context": "We use the word2vec tool3 and the web corpus ENCOW14A (Sch\u00e4fer and Bildhauer, 2012; Sch\u00e4fer, 2015) which contains approximately 14.", "startOffset": 54, "endOffset": 98}, {"referenceID": 1, "context": "1 Relatedness and Similarity Tasks For the relatedness task, we use two kinds of datasets: MEN (Bruni et al., 2014) consists of 3000 word pairs comprising 656 nouns, 57 adjectives and 38 verbs.", "startOffset": 95, "endOffset": 115}, {"referenceID": 3, "context": "The WordSim-353 relatedness dataset (Finkelstein et al., 2001) contains 252 word pairs.", "startOffset": 36, "endOffset": 62}, {"referenceID": 6, "context": "Concerning the similarity tasks, we evaluate the denoising vectors again on two kinds of datasets: SimLex-999 (Hill et al., 2015) contains 999 word pairs including 666 noun, 222 verb and 111 adjective pairs.", "startOffset": 110, "endOffset": 129}, {"referenceID": 17, "context": "We calculate cosine similarity between the vectors of two words forming a test pair, and report the Spearman rank-order correlation coefficient \u03c1 (Siegel and Castellan, 1988) against the respective gold standards of human ratings.", "startOffset": 146, "endOffset": 174}, {"referenceID": 8, "context": "2 Synonymy We evaluate on 80 TOEFL (Test of English as a Foreign Language) synonym questions (Landauer and Dumais, 1997) and 50 ESL (English as a Second Language) questions (Turney, 2001).", "startOffset": 93, "endOffset": 120}, {"referenceID": 20, "context": "2 Synonymy We evaluate on 80 TOEFL (Test of English as a Foreign Language) synonym questions (Landauer and Dumais, 1997) and 50 ESL (English as a Second Language) questions (Turney, 2001).", "startOffset": 173, "endOffset": 187}, {"referenceID": 1, "context": "Besides, we also use the public source code5 to re-implement the two methods suggested by Faruqui et al. (2015) which are vectors A (sparse overcomplete vectors) and B (sparse binary overcomplete vectors).", "startOffset": 90, "endOffset": 112}, {"referenceID": 2, "context": "Vectors X represent the baselines; vectors A and B were suggested by Faruqui et al. (2015); the vector length Z\u2217 is equal to 10 times of vector length X.", "startOffset": 69, "endOffset": 91}], "year": 2016, "abstractText": "Word embeddings have been demonstrated to benefit NLP tasks impressively. Yet, there is room for improvement in the vector representations, because current word embeddings typically contain unnecessary information, i.e., noise. We propose two novel models to improve word embeddings by unsupervised learning, in order to yield word denoising embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings.", "creator": "TeX"}}}