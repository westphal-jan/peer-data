{"id": "1610.02003", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Scalable Machine Translation in Memory Constrained Environments", "abstract": "Machine literary is two judicial would has develops automated multimedia but combining went way human pronunciation when saw. Statistical type translation (SMT) makes came controlled paradigm in this pitch. In SMT, encyclopedias example generated by means of coding models whose thermodynamic kinds learned but fluently data. Scalability takes a but concern over SMT, as one would n't from this these has either much data as possible which convoy would translation mechanisms.", "histories": [["v1", "Thu, 6 Oct 2016 19:22:49 GMT  (1168kb,D)", "http://arxiv.org/abs/1610.02003v1", "Master Thesis"]], "COMMENTS": "Master Thesis", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["paul baltescu"], "accepted": false, "id": "1610.02003"}, "pdf": {"name": "1610.02003.pdf", "metadata": {"source": "CRF", "title": "Scalable Machine Translation in Memory Constrained Environments", "authors": ["Paul-Dan Baltescu"], "emails": [], "sections": [{"heading": null, "text": "Machine translation is the discipline concerned with developing automated tools for translating from one human language to another. Statistical machine translation (SMT) is the dominant paradigm in this field. In SMT, translations are generated by means of statistical models whose parameters are learned from bilingual data. Scalability is a key concern in SMT, as one would like to make use of as much data as possible to train better translation systems. In recent years, mobile devices with adequate computing power have become widely available. Despite being very successful, mobile applications relying on NLP systems continue to follow a client-server architecture, which is of limited use because access to internet is often limited and expensive. The goal of this dissertation is to show how to construct a scalable machine translation system that can operate with the limited resources available on a mobile device. The main challenge for porting translation systems on mobile devices is memory usage. The amount of memory available on a mobile device is far less than what is typically available on the server side of a client-server application. In this thesis, we investigate alternatives for the two components which prevent standard translation systems from working on mobile devices due to high memory usage. We show that once these standard components are replaced with our proposed alternatives, we obtain a scalable translation system that can work on a device with limited memory. The first two chapters of this thesis are introductory. Chapter 1 discusses the task we undertake in greater detail and highlights our contributions. Chapter 2 provides a brief introduction to statistical machine translation. In Chapter 3, we explore online grammar extractors as a memory efficient alternative to phrase tables. We propose a faster and simpler extraction algorithm for translation rules containing gaps, thereby improving the extraction time for hierarchical phase-based translation systems. In Chapter 4, we conduct a thorough investigation on how neural language models should be integrated in translation systems. We settle on a novel combination of noise contrastive estimation and factoring the output layer using Brown clusters. We obtain a high quality translation system that is fast both when training and decoding and we use it to show that neural language models outperform traditional n-gram models in memory constrained environments. Chapter 5 concludes our work showing that online grammar extractors and neural language models allow us to build scalable, high quality systems that can translate text with the limited resources available on a mobile device.\nContents"}, {"heading": "1 Introduction 1", "text": "1.1 Thesis Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3 Thesis Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"}, {"heading": "2 Statistical Machine Translation 7", "text": "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 Alignment Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3 Translation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.3.1 Finite State Transducers . . . . . . . . . . . . . . . . . . . . . . . 11 2.3.2 Synchronous Context Free Grammars . . . . . . . . . . . . . . . . 12 2.3.3 Phrase Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.4 Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.5 Scoring Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.6 Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.6.1 Decoding with FST Models . . . . . . . . . . . . . . . . . . . . . 21 2.6.2 Decoding with SCFG Models . . . . . . . . . . . . . . . . . . . . 23\n2.7 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"}, {"heading": "3 Online Grammar Extractors 28", "text": "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.2 Grammar Extraction for Contiguous Phrases . . . . . . . . . . . . . . . . . 29 3.3 Grammar Extraction for Phrases with Gaps . . . . . . . . . . . . . . . . . 32 3.4 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37"}, {"heading": "4 Neural Language Models 39", "text": "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.2 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 4.3 Model Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.4 Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n4.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 4.4.2 Class Factored Models . . . . . . . . . . . . . . . . . . . . . . . . 46 4.4.3 Tree Factored Models . . . . . . . . . . . . . . . . . . . . . . . . 47\ni\n4.4.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 4.5 Noise Contrastive Training . . . . . . . . . . . . . . . . . . . . . . . . . . 49 4.6 Diagonal Context Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . 51 4.7 Quality vs. Memory Trade-Off . . . . . . . . . . . . . . . . . . . . . . . . 53 4.8 Direct N-gram Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 4.9 Source Sentence Conditioning . . . . . . . . . . . . . . . . . . . . . . . . 56 4.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58"}, {"heading": "5 Conclusions 60", "text": "5.1 Building a Compact Translation System . . . . . . . . . . . . . . . . . . . 60 5.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 5.3 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nReferences 65\nii\nList of Figures\n2.1 Alignment matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.2 Translating with phrase-based models . . . . . . . . . . . . . . . . . . . . 12 2.3 A simple SCFG grammar and an example derivation. . . . . . . . . . . . . 13 2.4 MERT subroutine for optimizing a single weight . . . . . . . . . . . . . . 20 2.5 Phrase-based decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.6 Hierarchical phrase-based decoding . . . . . . . . . . . . . . . . . . . . . 24 2.7 Cube pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n3.1 Suffix array example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.2 Phrase extraction algorithm for contiguous phrases . . . . . . . . . . . . . 31 3.3 Trie cache with suffix links . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.4 Phrase extraction algorithm for discontiguous phrases (case 1) . . . . . . . 33 3.5 Phrase extraction algorithm for discontiguous phrases (case 2) . . . . . . . 33\n4.1 Feedforward neural language model architecture . . . . . . . . . . . . . . . 43 4.2 Class factored models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.3 Tree factored models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 4.4 BLEU score vs. model size (log scale) for n-gram and neural models. . . . 54 4.5 Feedforward source conditioned neural language model architecture . . . . 57\niii\nList of Tables\n3.1 Results for the preprocessing step. . . . . . . . . . . . . . . . . . . . . . . 36 3.2 Results for the phrase extraction step. . . . . . . . . . . . . . . . . . . . . 37 3.3 Results for parallel extraction using 8 processes/threads. . . . . . . . . . . 37\n4.1 Statistics for the parallel corpora. . . . . . . . . . . . . . . . . . . . . . . . 42 4.2 Statistics for the monolingual corpora. . . . . . . . . . . . . . . . . . . . . 42 4.3 BLEU scores for the proposed normalization schemes. . . . . . . . . . . . 48 4.4 BLEU scores for common clustering strategies on fr\u2192en data. . . . . . . . 48 4.5 Average decoding time per sentence for the proposed normalization schemes. 49 4.6 A comparison between stochastic gradient descent (SGD) and noise contrastive estimation (NCE) for class factored models on the fr\u2192en data. . . . 50 4.7 Training times for neural models on fr\u2192en data. . . . . . . . . . . . . . . . 51 4.8 A side by side comparison of class factored models with and without diagonal contexts trained with noise contrastive estimation on the fr\u2192en data. . 51 4.9 Decoding time speed-ups when using diagonal context matrices. . . . . . . 52 4.10 BLEU scores and memory requirements for back-off n-gram models constructed from different percentages of the training corpus. . . . . . . . . . . 52 4.11 BLEU scores and memory requirements for class factored models with different word embeddings sizes. . . . . . . . . . . . . . . . . . . . . . . . . 53 4.12 BLEU scores and memory requirements for class factored neural language models with n-gram features. . . . . . . . . . . . . . . . . . . . . . . . . . 56 4.13 BLEU scores for source conditioned neural language models. . . . . . . . . 58\n5.1 Memory footprints for individual components and overall and end-to-end BLEU scores for the baseline and the compact translation systems across the 3 language pairs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\niv\nChapter 1\nIntroduction\nMachine translation is the discipline concerned with developing automated processes for translating from one natural (human) language to another. Although machine translation has been a subject of interest in the research community since the late 1940s (Weaver, 1949), the field has only seen significant progress since the 1990s with the rise in popularity of statistical methods for translation (Brown et al., 1993). Statistical machine translation makes use of parallel data to automatically infer rules which can later be combined to produce translations of input sentences. As with most natural language systems employing statistical models, the quality of translation systems increases as more data is used to train the underlying models. Scalability is a key concern in machine translation, as one would like to use as much data as possible when building a translation system.\nIn the last decade, mobile devices with adequate computing power (e.g., smartphones, tablets, etc.) have become widely available and have started playing an important role in the daily life of millions of people. Mobile applications using NLP systems such as speech recognizers or translation systems have been incredibly successful because they lower the barrier for accessing information on the go. Most of these applications have a client-server architecture, where the heavy computation specific to NLP tasks is done on the server side, while the client side is only used to render the user interface of the application. The main bottleneck of this approach is access to internet, which is often limited and expensive. To work around this problem, one has to construct NLP systems that can operate with the limited resources available on mobile devices. The key limitation is the amount of memory available, typically limited to 1GB, which is 2-3 orders of magnitude less than what is usually available on the server side of a client-server application. A similar problem is encountered when developing NLP systems that are expected to run on\n1\ncommodity machines. Despite having slightly more memory than mobile devices, average home computers are still far less powerful than the high-end machines used by software companies or research institutes. In this thesis, we seek to develop translation systems that work on memory constrained devices."}, {"heading": "1.1 Thesis Goals", "text": "The primary aim of this thesis is to present a scalable approach for constructing high quality machine translation systems that can run in memory constrained environments such as mobile devices or commodity machines. We tackle the two main challenges that prevent standard translation systems from working in such environments: the representation of the translation model in memory and structure of the language model. First, we explore compact representations of translation models which rely on suffix arrays to efficiently locate phrases in the source side of a parallel corpus and extract translation rules on the fly. Second, we investigate neural language models as a memory efficient alternative to traditional n-gram language models and analyze the effect of the most popular scaling techniques for neural models on end-to-end translation quality. We show that by introducing our proposed alternatives in a standard translation system to replace their equivalent components, we obtain a fast, compact and high quality translation system.\nThroughout this thesis, we seek to evaluate the approaches we propose using several metrics. Above all, we are interested in producing high quality translation systems and we follow the standard practice of reporting BLEU scores (Papineni et al., 2002). We also report the amount of memory needed to store the models we investigate, as it is our goal to show that these models are compact enough to be used in memory constrained environments. Finally, we must ensure that our models are fast enough to be practical, both when training a translation system and when using it to translate new sentences. We achieve this by keeping track of the time needed to train each component individually and of the average time needed to decode a test sentence. We note that building a translation system is a time consuming task (which may take up to several days), but we are not interested to perform this task with the limited resources available on the client device. Instead, we would like to train our models on a powerful machine and download them on the client device when access to internet is available, leaving decoding as the only operation to be performed on the client.\n2\nIn our dissertation, we also explore decisions which lead to trade-offs between these metrics. For example, certain scaling techniques for neural models result in higher translation quality, but make decoding slower, or depending on the amount of memory available, using either neural language models or back-off n-gram models will lead to higher BLEU scores."}, {"heading": "1.2 Contributions", "text": "In this section, we summarize the main contributions of this thesis.\nIn Chapter 3, we discuss compact alternatives to phrase tables, the traditional data structures used to represent translation models in memory. We highlight the importance of these alternatives, in particular in the context of hierarchical phrase-based translation systems (Chiang, 2007). We choose online grammar extractors as the basis of our work and provide supporting arguments to motivate the decision. We present an efficient implementation of an online grammar extractor using suffix arrays based on Lopez (2007). We introduce a novel algorithm for extracting hierarchical translation rules with significantly lower running time. Our approach is also much simpler to implement than Lopez (2007).\nIn Chapter 4, we conduct a thorough analysis on integrating neural language models in translation systems. Although the idea of incorporating these models in MT is not new, we are the first to explore it with the goal of producing a compact translation system and to focus on the properties of neural language models as the sole language models in the system. The latter is important because our hypothesis is that most of the language modeling is otherwise done by the back-off n-gram model, with the neural language model only acting as a differentiating factor when the n-gram model cannot provide a decisive probability. We show that neural language models clearly outperform traditional n-gram models in memory constrained environments, but when the memory restriction is lifted, back-off n-gram models are more effective than their neural counterparts. Scaling neural language models is a difficult task, but crucial for obtaining practical translation systems. We investigate the impact of several frequently used scaling techniques on end-to-end translation quality. We discover that a novel combination of noise contrastive estimation (Mnih and Teh, 2012) and factoring the softmax layer using Brown clusters (Brown et al., 1992) is the most pragmatic solution for efficient training and decoding with neural language models. Finally, we\n3\nexplore two extensions to neural language models (one investigated for the first time in the context of translation systems) with the goal of boosting translation quality further.\nIn Chapter 5, we show that by combining the techniques introduced in the earlier chapters, we obtain a high quality system that fits within the 1 GB memory constraint. We evaluate our system on three language pairs and show that it outperforms a traditional system trained on sampled data to match the memory requirements by 0.7-1.5 BLEU points. The proposed techniques are scalable both when training the model and when using it to decode new sentences.\nAnother important contribution of our thesis is that we open source our code and make it easy to integrate with the most popular translation toolkits. Our suffix array grammar extractor1 is released as part of cdec (Dyer et al., 2010) and has been included as part of the default instructions for building a baseline system with the toolkit. The extractor is designed as a standalone tool, and in order to incorporate it in other translation toolkits, one has to write only the new interface between the translation system and the grammar extractor. We also release OxLM, a scalable neural language modeling toolkit2. The models trained with OxLM can be integrated as features in the cdec (Dyer et al., 2010) and Moses (Koehn et al., 2007) decoders. In contrast to other open source neural language toolkits for MT (Vaswani et al., 2013), we allow our models to be explicitly normalized. This is crucial for obtaining high quality translation systems when additional back-off n-gram models cannot be used due to memory constraints. Also, unlike Schwenk (2010), our models can be integrated directly in the decoder. Also, we do not use a backup n-gram model to score rare words, because it is not feasible to do so with limited memory."}, {"heading": "1.3 Thesis Structure", "text": "In this section, we discuss the structure of the thesis and summarize the contents of each chapter. Part of the work presented in this dissertation is based on publications of which we are the main author. We indicate which parts rely on previously published material, as we overview the topics covered by each chapter.\nChapter 2: Statistical Machine Translation 1The code has been released at: https://github.com/redpony/cdec/tree/master/ extractor. 2The language modeling toolkit is available at: https://github.com/pauldb89/oxlm.\n4\nThis chapter presents a brief introduction to statistical machine translation. Our goal is to explain how a translation system works, as we prepare the reader for the topics covered in the next chapters. We discuss the standard approaches employed by each component of a translation system and show where difficulties arise when the amount of memory is limited. We compare two formalisms that lie at the foundation of most translation systems in use today: finite state transducers and synchronous context free grammars. We focus our exposition on the topics relevant for reaching the goal of constructing a compact translation system and refer the interested reader to Lopez (2008b) for a thorough review of statical machine translation.\nChapter 3: Online Grammar Extractors\nThe traditional approach for storing translation models in memory is achieved with the help of phrase tables, dictionary-like data structures that map all source phrases from the training corpus to their target side correspondents. Phrase tables often become unmanageably large, especially in the case of hierarchical phrase-based systems, which make use of translation rules containing gaps. Online grammar extractors avoid loading all translation rules into memory by constructing memory efficient data structures on top of the source side of the parallel data, which are used to efficiently locate phrases in the corpus and extract translation rules on the fly during decoding. The online grammar extractor presented in this chapter extends Lopez (2007) and introduces a new technique for matching phrases containing gaps that significantly reduces the extraction time for hierarchical phrase-based systems. This chapter is based on the following publication:\nPaul Baltescu and Phil Blunsom. 2014. A Fast and Simple Online Synchronous Context Free Grammar Extractor. Prague Bulletin of Mathematical Linguistics.\nChapter 4: Neural Language modeling for Machine Translation\nIn this chapter, we explore neural language models as a memory efficient alternative to traditional n-gram models. Recent research has shown positive results when neural language models are integrated as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014) or when used for n-best list rescoring (Schwenk, 2010). These publications follow different approaches for scaling neural language\n5\nmodels and one goal of this chapter is to conduct a thorough analysis to understand which of these techniques is best in a practical setup. We show that when memory is limited, neural language models clearly outperform traditional n-gram models, but this is not true when the memory constraint is removed. Finally, we explore extensions to neural language models with the goal of improving translation quality. The work presented in this chapter is based on the following publications:\nPaul Baltescu, Phil Blunsom and Hieu Hoang. 2014. OxLM: A Neural Language modeling Framework for Machine Translation. Prague Bulletin of Mathematical Linguistics.\nPaul Baltescu and Phil Blunsom. 2015. Pragmatic Neural Language modeling in Machine Translation. Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics.\nChapter 5: Conclusions\nIn the previous chapters, we have discussed online grammar extractors and neural language models as memory efficient alternatives to phrase tables and traditional ngram language models. We now show that by putting these components together we obtain a scalable translation system that can operate in a memory constrained environment. We conclude our thesis by reviewing our findings and discussing avenues for future work.\n6\nChapter 2\nStatistical Machine Translation"}, {"heading": "2.1 Introduction", "text": "In this chapter, we discuss the core components of a standard machine translation system. We take a pragmatic approach and go step by step through the stages involved in building a translation system from scratch, starting with large amounts of parallel and monolingual text. Our presentation closely follows the steps for building a baseline system with cdec (Dyer et al., 2010) and Moses (Koehn et al., 2007), two popular open source translation toolkits. Relating to these tools is also useful considering that the work presented in the latter chapters of this thesis is tightly integrated with these frameworks. In fact, the reader can take the key components discussed here and the extensions from Chapter 3 and Chapter 4 and construct an efficient, high quality system that can work on a memory constrained device. We show this in Chapter 5.\nThis chapter is not a comprehensive review of techniques and trends employed in statistical machine translation. Instead, we aim to provide the minimal information that is sufficient to understand how translation systems work. We also focus on the parts relevant for laying the foundations for the following chapters. For a detailed literature review of statistical machine translation, we defer the reader to Lopez (2008b).\nIn order to train a translation system, one needs large quantities of parallel data. A parallel corpus is a set of pairs of sentences that are translations of each other. Parallel corpora can be obtained from a number of sources: parliamentary proceedings (Koehn, 2005), news articles from international agencies, weather forecasts, instruction manuals, etc. In order for the system to produce fluent translations, one also needs large amounts of monolingual text in the target language. Monolingual data is cheaper to obtain (e.g. by\n7\ncrawling the web) and larger quantities are usually available. For the most common pairs of languages, one can find enough parallel and monolingual data to train a translation system on the website of the Workshop in Statistical Machine Translation. 1\nThe first step for training a translation system is cleaning the training data. This step usually consists of tokenizing and lowercasing/truecasing the data. Tokenization is the process of breaking up sentences into individual tokens (words). The main challenge for tokenization is dealing with punctuation, i.e. good tokenizers separate out words from punctuation, but keep abbreviations (Msc., a.m.), compound words (apre\u0300s-midi), etc. as single tokens. Lowercasing refers to the process of replacing all the capital letters in a word with lowercase letters. Truecasing allows each letter to be either lowercased or uppercased in order to obtain the most likely base form of a token. Lowercasing is the easier approach because of its deterministic nature. On the other hand, truecasing is more powerful because it has the ability to distinguish between words with the same lowercase form (e.g. US, us). These techniques are applied in order to make the data statistics less sparse and the overall system more robust. Common practice dictates that pairs of very long sentences or having unusual length ratios are removed from the parallel data. These sentence pairs are frequently the outcome of flaws in the process of collecting the training data. For certain languages, additional text processing may be needed (e.g. word segmentation for Chinese).\nThe next step is aligning the parallel corpus. Alignment models are discussed in further detail in Section 2.2. The parallel corpus and the word alignments are used to extract translation rules. The structure of translation rules depends on the formalism chosen as foundation for the translation system. In Section 2.3, we discuss finite state transducers and synchronous context free grammars as underlying formalisms for translation systems. We also discuss phrase tables as the default approach to storing translation rules in memory. Section 2.4 reviews the use of language models in machine translation and discusses backoff n-gram models as the standard approach for language modeling. Section 2.5 shows how translation rules, language models and other signals are combined together as part of a unified scoring model. Section 2.6 explains the algorithms for translating a source sentence into a target sentence, a process also known as decoding. Section 2.7 concludes by discussing methods for evaluating the quality of translation systems.\n1The workshop is organized on an yearly basis. The website for the 2015 edition is http://statmt. org/wmt15/, and from there one can access previous editions.\n8"}, {"heading": "2.2 Alignment Models", "text": "Parallel corpora are an essential resource for training translation models. However, they cannot be used for this task in their raw form, because they cover an insignificant part of the space of sentence pairs. The observed sentence pairs are also unlikely to repeat again. Therefore, we need to use the parallel data to learn smaller translation units which can later be composed in order to translate full sentences. Alignment models help with this crucial task by learning the process of translating (or aligning) single words. Larger translation units can then be obtained by composing several aligned words located in the same context (Section 2.3).\nFormally, let s = sN1 and t = tM1 be a pair of sentences from the parallel corpus. An alignment a is a subset of index pairs from [1, N ]\u00d7 [1,M ]. We say (i, j) \u2208 a if the words si and tj are translations of each other. Word alignments can be illustrated graphically using an alignment matrix as shown in Figure 2.1.\nBrown et al. (1993) made the first substantial leap in statistical machine translation\n9\nby introducing several word level translation models (known as the IBM Models), which are now regarded as the standard set of models for word alignment. Vogel et al. (1996) used Hidden Markov Models to define an alignment model known as the HMM alignment model. By default, the Moses toolkit (Koehn et al., 2007) relies on Giza++ (Och and Ney, 2003) for aligning the parallel corpus. Giza++ is a fine tuned implementation of the IBM alignment models, using the first models to initialize the more advanced ones. In Giza++, the IBM Model 2 is replaced by the HMM alignment model. The cdec toolkit computes alignments using an adaptation of IBM Model 2 with fewer parameters (Dyer et al., 2013).\nFor demonstration purposes, let us analyze the HMM alignment model into greater detail. The HMM model is an asymmetric alignment model (and so are the IBM Models) because each target word is aligned to exactly one source word, while a source word can be aligned to multiple target words. A dummy null token is introduced on the source side to permit unaligned target words. The model encapsulates a bias for monotonic alignments, i.e. consecutive target words are more likely to be aligned to consecutive source words. Formally, the model defines the joint probability of a target sentence t and an alignment a given a source sentence s as follows:\np(t, a|s) = M\u220f j=1 p(aj|aj\u22121, N)p(tj|saj) (2.1)\nThe model is defined in terms of the translation probabilities p(t|s) and the alignment penalties p(i|j,N). If the word alignments were known ahead of time, computing these parameters would be straightforward. Conversely, if we knew the translation probabilities and alignment penalties, we could find the optimal alignment using the Viterbi algorithm (Viterbi, 1967). However, since neither are known ahead of time, we must resort to the EM algorithm for parameter estimation (Baum, 1972). The other alignment models follow the same general approach with regards to learning the parameters and finding the maximum probability alignment.\nThe standard practice for obtaining symmetric alignments is to apply the alignment models in both directions and to use some heuristic to combine the resulting alignments (e.g., set union, set intersection, etc.). The default heuristic used by the Moses and cdec toolkits starts from the intersection of the two alignments and then adds additional links along the main diagonal of the alignment matrix (Koehn, 2010). The heuristic includes a final step adding links from the set union to those words that remain unaligned.\n10"}, {"heading": "2.3 Translation Models", "text": "Translation models define the set of translation rules employed by a translation system. Most translation models stem from one of the following two formalisms: finite state transducers (FSTs) or synchronous context free grammars (SCFGs). These formalisms are similar in nature to their better known monolingual counterparts, the finite state automata and the context free grammars, but have the ability to model a target language in addition to the source language, which makes them suitable for machine translation. In this section, we present their formal definitions and briefly describe one model of each type. We also introduce phrase tables, the standard approach for storing translation models in memory, and explain how they are constructed from a word aligned parallel corpus."}, {"heading": "2.3.1 Finite State Transducers", "text": "Formally, a FST is a tuple (Q, Vs, Vt, D), where Q represents a set of states, Vs and Vt are sets of symbols and D \u2286 Q \u00d7 Vs \u00d7 Vt \u00d7 Q is a set of transitions. In the context of machine translation, Vs and Vt define the set of source and target tokens (words, phrases, etc.), the statesQ are a succinct representation of translation hypotheses, and the transitions D specify the set of base rules that define the space of valid translations. For example, transitions can model the process of translating single word units as follows: a transition q1 s/t\u2212\u2192 q2 with q1, q2 \u2208 Q, s \u2208 Vs, t \u2208 Vt represents the source word s being translated to the target word t, extending the current translation hypothesis represented by q1 into q2. The states Q also indicate how close we are to obtain a full translation of the input sentence. In practice, most FST-based translation systems are described as compositions of FSTs. The resulting systems are also FSTs because FSTs are closed to the composition operator.\nHistorically, the first translation models to show promising results were based on the FST formalism and consequently these models laid the foundations of statistical machine translation. Examples of FST-based models include the IBM Models (Brown et al., 1993), the HMM alignment model (Vogel et al., 1996) and the phrase-based translation models (Koehn et al., 2003). We briefly touch upon phrase-based models as described in Koehn et al. (2003) because these models were employed in many state of the art systems and are still widely in use today. In fact, the Moses toolkit (Koehn et al., 2007) relies by default on phrase-based models. Compared to their word-based predecessors, phrase-based models can translate contiguous groups of words (phrases) in a single step. The intuition behind\n11\nthese models is that if a phrase pair is observed enough times in the parallel corpus, it is more likely to produce the intended translation than if we replace individual source words with their most likely translations. Another reason why phrase-based models perform better is their ability to capture local reorderings and to improve grammatical agreement.\nAt a high level, phrase-based translation can be seen as a three steps process: first, the source sentence is segmented into several phrases, then each source phrase is translated into a target phrase, and finally, these phrases are reordered to produce a fluent translation in the target language (Figure 2.2). Each of these steps can be modelled using an FST and thus phrase-based translation models are a composition of FSTs. The first two tasks are straightforward to solve with an FST, but FSTs are not well suited to deal with reorderings. In fact, an FST must have O(2N) states in order to capture all the O(N !) permutations of a set of N words. Furthermore, Knight (1999) showed that the problem of finding the optimal reordering is equivalent to the traveling salesman problem and thus NP-complete. As a result, decoding with phrase-based models cannot be done efficiently and one must resort to heuristics like beam search (Koehn, 2004) (Subsection 2.6.1). Reordering poses the same challenge to all FST-based models."}, {"heading": "2.3.2 Synchronous Context Free Grammars", "text": "SCFG-based models were introduced with the goal of alleviating some of the problems with FST models. First, SCFG models can capture long distance reorderings more easily than FST models. Second, they provide a framework for learning discontiguous phrases, allowing the models to learn useful translation templates, e.g. he is X years old\u2190\u2192 il a X ans. Finally, SCFGs provide support for bridging the gap between translation and syntax.\n12\nSynchronous context free grammars are an extension of context free grammars capable of modeling an additional language. They are formally defined as a tuple (N,S, Vs, Vt, D), where N is a set of nonterminals, S \u2208 N is the starting nonterminal for any SCFG derivation, Vs is a set of source terminals, Vt is a set of target terminals and D \u2286 N \u00d7 (N \u00d7 Vs) \u2217 \u00d7 (N \u00d7 Vt)\u2217 is a set of productions (rules). A production of the form X \u2192 \u03b1 | \u03b2 indicates that a nonterminal X \u2208 N may be rewritten as a string of terminals and nonterminals \u03b1 \u2208 (N \u00d7 Vs)\u2217 in the source language and \u03b2 \u2208 (N \u00d7 Vt)\u2217 in the target language. The source and target side of a rule must contain the exact same multiset of nonterminals and each source nonterminal must be aligned to exactly one target nonterminal of the same type. Nonterminals on the right hand side of a rule are usually labelled with numbers to indicate how they align to each other. Figure 2.3 shows an example SCFG and illustrates how this grammar can be used for translating sentences.\nModeling word reordering with an SCFG is a trivial task as one can simply enumerate rules for every possible permutation up to some given length. However, decoding algorithms for models based on this formalism are adaptations of the well known parsing algorithms for CFGs (Subsection 2.6.2) and therefore their complexity depends on the size of the grammar. Adding all these permutations to the grammar would result in an exponential number of rules yielding impractical decoding algorithms. A common compromise is to include only the following two reordering rules: X \u2192 X1X2 | X1X2 and X \u2192 X1X2 | X2X1. A number of other permutations may be obtained by repeatedly applying these two rules, but not all (e.g. X \u2192 X1X2X3X4 | X2X4X1X3). Although the\n13\nnumber of permutations that cannot be obtained with this procedure increases exponentially with the size of the permutation (Wu, 1997), Zens and Ney (2003) show that only a negligible part of real world reorderings cannot be captured with this simplified model.\nA number of translation models use the SCFG formalism: inversion transduction grammars (Wu, 1997), hierarchical phrase-based models (Chiang, 2007), syntactic tree-to-string grammars (Galley et al., 2004; Galley et al., 2006), etc. We briefly describe hierarchical phrase-based models because of their relative success over phrase-based models. The cdec toolkit (Dyer et al., 2010) relies by default on a hierarchical phrase-based model.\nHierarchical phrase-based models are SCFGs with a single nonterminal (usually labeled as X). The source and target vocabularies directly map to the vocabularies of the language pair for which the model is built. To allow for efficient decoding, hierarchical phrase-based models enforce the restriction discussed previously and limit the number of nonterminal pairs in the right hand side of a rule to 2. Decoding with hierarchical phrase-based models is polynomial in the size of the sentence, but the main challenge is incorporating the language model. We cover this subject in greater detail in Subsection 2.6.2."}, {"heading": "2.3.3 Phrase Tables", "text": "In Subsection 2.3.1 and Subsection 2.3.2, we presented the two most common formalisms that define the structure of the rules employed by translation systems. In this subsection, we focus on how these rules are stored in memory and explain how they are extracted from a word-aligned parallel corpus (a task known as phrase or grammar extraction).\nThe traditional approach to storing translation rules in memory is achieved via phrase tables. A phrase table maps each source phrase to all the target phrases that align to it in the parallel text. For each target phrase, the phrase table also stores an additional set of scores whose role is discussed in Section 2.5. Phrase tables are often pruned so only the most frequent phrase pairs are kept. Pruning can be done based on the weights associated with each target phrase, or by keeping a limited number of target phrases for each source phrase. Decoders require the ability to efficiently look up the target phrases associated with any source phrase. As a result, phrase tables are usually implemented as hash tables or tries to allow constant time lookups.\nPhrase tables are constructed offline in a preprocessing step taking as input the parallel corpus and the word alignments. The algorithm processes the corpus sentence by sentence\n14\nand extracts all valid phrase pairs. A phrase pair is considered valid if none of its source or target words aligns to a word outside of the phrase pair. For FST rules, the extraction algorithm (Och and Ney, 2004) iterates through all the source phrases sji of a given sentence sN1 , 1 \u2264 i \u2264 j \u2264 N . For every word sk, i \u2264 k \u2264 j, all the target words aligned with sk are added to a set T . After the whole source phrase is processed, the algorithm checks if the target words in T form a contiguous phrase in the target sentence and that none of the target words are aligned with source words outside sji . Additional phrase pairs will be extracted if there are unaligned target words adjacent to the target phrase defined by T . The extraction algorithm for SCFG rules works in a similar fashion, but it contains a large number of additional edge cases because it must ensure that gaps are also correctly aligned. A detailed account of the extraction algorithm for SCFG rules is given in Lopez (2008a).\nParallel corpora used for training translation systems usually contain millions of sentences. As a result, phrase tables easily grow too large to be held in memory. For FST models, the number of contiguous subphrases of a sentence of length N is O(N2). A common solution is to limit the length of the extracted phrases by a threshold L, reducing the number of subphrases to O(N \u00d7 L). This is a sensible restriction because longer phrase patterns are less likely to be observed during decoding. For example, Koehn et al. (2003) recommend extracting phrase pairs having up to four words on the source side.\nFor SCFG-based models, the number of phrases that can be extracted from a single sentence is exponential in the sentence length. This number can be significantly reduced when a threshold is placed on the maximum number of nonterminal pairs in a rule (like in hierarchical phrase-based models) and when the maximum span of a rule is limited (similar to FST models). However, even with these limitations in place, discontiguous translation rules continue to be orders of magnitude more than contiguous rules. Traditional phrase tables are not suitable for SCFG grammar extraction at scale, even without taking into account additional memory constraints, such as the ones imposed by mobile devices. A compact and scalable alternative to phrase tables is presented in Chapter 3."}, {"heading": "2.4 Language Models", "text": "The language model is a key component in a translation system which is responsible for the fluency of the output translations. It drives a good part of the end to end translation quality and considerable improvements can often be achieved by using more monolingual data to\n15\ntrain the language model. In this section, we explain how language models work, while Section 2.5 shows how translation models and language models are combined together in a unified scoring model.\nLanguage models are statistical models used to score how likely a sequence of words is to occur in a certain language by means of a probability distribution. Let w = wM1 be a sentence in the target language and P (w) the probability distribution defined by the model. According to the chain rule of probability, P (w) can be decomposed as the product of the probabilities of each target word given its preceding context:\nP (w) = M\u220f i=1 P (wi|wi\u221211 ). (2.2)\nTo prevent the model from relying on distributions computed from very sparse statistics, a n-1th order Markov assumption is typically incorporated in the model:\nP (w) = M\u220f i=1 P (wi|wi\u22121i\u2212n+1). (2.3)\nBack-off n-gram models are the default language modeling implementation used in\nmachine translation. These models estimate the conditional probability as:\nP (wi|wi\u22121i\u2212n+1) = c(wii\u2212n+1)\nc(wi\u22121i\u2212n+1) , (2.4)\nwhere c(wii\u2212n+1) and c(w i\u22121 i\u2212n+1) represent the number of times w i i\u2212n+1 and w i\u22121 i\u2212n+1 are observed in the monolingual corpus. In their raw form, n-gram language models do not accurately estimate rare n-grams. Over the years, a number of smoothing techniques have been proposed in order to address this problem (Jelinek and Mercer, 1980; Katz, 1987; Kneser and Ney, 1995; Chen and Goodman, 1999).\nMachine translation toolkits like Moses (Koehn et al., 2007) and cdec (Dyer et al., 2010) have plugins for several open source implementations of back-off n-gram models: SRILM (Stolcke, 2002), IRSTLM (Federico et al., 2008) and KenLM (Heafield, 2011). Heafield (2011) shows his implementation is superior to SRILM and IRSTLM both in terms of speed and memory usage. In fact, two separate implementations are provided as part of KenLM. One is optimized for speed and uses a hash table with linear probing to look up n-gram weights. The other, optimized for memory, but still faster than the alternatives, relies on a trie and makes use of floating point quantization.\n16\nBack-off n-gram models are used as the default language modeling choice in machine translation because they produce very good results if enough monolingual data is available. They are also fast to train and query and their definition is very intuitive. On the other hand, a n-gram language model stores a numerical value for every n-gram in the training corpus. As a result, even the most compact implementations (e.g. the KenLM trie implementation) require tens of gigabytes of memory for a decently sized monolingual corpus (see Section 4.7). In conclusion, back-off n-gram models are not suitable for memory constrained environments. Chapter 4 investigates neural language models as a space-efficient alternative to n-gram language models."}, {"heading": "2.5 Scoring Model", "text": "The translation formalisms introduced in Section 2.3 consist of a set of rules which define the entire set of valid translations. However, not all of these translations are equally good. The scoring model provides a framework for comparing these translations by assigning a probability to all output sentences. The decoder\u2019s goal (Section 2.6) is to infer the best translation with respect to the scoring model.\nThe goal of a translation system is to find arg maxt P (t|s), where s is the source sentence and t is a possible translation. It is useful to extend this definition to include the translation rules used by the system in order to produce t. Let d denote a set of rules (derivation) and S(t) be the set of derivations d which produce t. We can rewrite our objective as arg maxt \u2211 d\u2208S(t) P (t,d|s). Unfortunately, this optimization problem is intractable for both FSTs and SCFGs. The common practice is to use the Viterbi approximation instead: arg maxt,d\u2208S(t) P (t,d|s). Most translation systems can trace their roots to the IBM Models (Brown et al., 1993). Brown et al. (1993) represent the translation process as a generative model:\nP (t,d|s) = P (s, t,d) P (s) = P (t)P (s,d|t) P (s) \u221d P (t)P (s,d|t). (2.5)\nHere, P (s,d|t) is a target-to-source translation model (the reverse of the models discussed in Section 2.3) and P (t) is the target language model (Section 2.4). By combining these two terms we aim to obtain a translation that is both accurate and fluent in the target language.\n17\nThe language model is traditionally estimated as described in Section 2.4. The translation model probabilities are estimated from a word aligned parallel corpus, also via frequency counts. For example, to compute the probability that the word president translates as pre\u0301sident, we divide the number of times the two words are aligned to each other in a French-English parallel corpus with the number of times the word president appears in the target side of the corpus. For phrases (contiguous or not), we can multiply the word level translation probabilities (averaging the probabilities for words with several alignments) or we can use phrase level frequency counts instead. The latter approach is usually more accurate. These weights are stored in the phrase table in addition to each target phrase.\nThe problem with generative translation models is that they need to make too many independence assumptions in order to be tractable. For example, when applying a translation rule, one can only use the source words belonging to the current phrase pair as signal. Other information is ignored; in this case, the entire source sentence can be useful when choosing the target words. The solution for this problem is to rely on discriminative models which permit the use of overlapping features. Indeed, most translation systems today, including Moses (Koehn et al., 2007) and cdec (Dyer et al., 2010), use a discriminative model for scoring derivations.\nThe most common form of discriminative translation models is log-linear models. A log-linear model defines a set of K feature functions fK1 (s, t,d) producing strictly positive outputs and learns a set of weights \u03bbK1 . The weights \u03bb K 1 capture the correlation between the feature functions and the model\u2019s output: a large positive \u03bbk implies the function fk is a strong predictor of the model\u2019s output, a large negative \u03bbk shows a strong inverse correlation between fk and the model\u2019s predictions, while a \u03bbk close to 0 shows that fk is not useful for predictions. The feature functions fK1 typically include a target language model and a generative target-to-source translation model, just like generative scoring models. However, since log-linear models support overlapping features, it is common to also include a source-to-target generative translation model, to use several scoring techniques for translation models (e.g. word level, phrase level, word embeddings) or to include multiple target language models. Using all these signals, a log-linear model defines a probability distribution as follows:\nP (t,d|s) = exp \u2211K\nk=1 \u03bbkfk(s, t,d)\u2211 t\u2032,d\u2032\u2208S(t\u2032) exp \u2211K k=1 \u03bbkfk(s, t \u2032,d\u2032) . (2.6)\n18\nFortunately, most decoding algorithms do not require a well-defined probabilistic model, so the intractable normalization term can be ignored.\nThe first step towards training a log-linear translation model is computing the values of the generative features as explained above. Once these features are known (or can be computed online quickly), we can proceed with learning the coefficients \u03bbK1 . One option is to use maximum likelihood estimates learned via gradient descent on a small development corpus. Unfortunately, this method requires computing the full normalization term. The standard optimization applied in this case is to approximate the normalization term with the list of n-best translations, hoping that they account for most of the probability mass.\nSection 2.7 discusses how the quality of machine translation systems is evaluated. Och (2003) shows that optimizing the weights of the scoring model directly towards the evaluation metric results in considerable qualitative gains. The key difficulty is that these metrics are non-differentiable and require new training algorithms. The default algorithm for training translation models with the Moses toolkit is minimum error rate training (MERT) (Och, 2003). MERT was also the default setting in the cdec toolkit, until recently when it was replaced with the maximum infused relaxed algorithm (MIRA) (Eidelman, 2012).\nFor demonstration purposes, let us discuss the MERT algorithm in greater detail. MERT assumes the existence of an error functionE(t\u0302, t) defining the amount of mismatch between a translation candidate t\u0302 and the reference translation t. The goal of MERT is to find \u03bbK1 which minimizes the total error on the development corpus D:\n\u03bbK1 = arg min \u03bbK1 \u2211 s,t\u2208D E(arg max t\u0302 P\u03bbK1 (t\u0302|s), t). (2.7)\nThe algorithm has several iterations. During one iteration, we generate several candidates \u03bbK1 randomly. For each candidate, we iterate over each \u03bbk and try to optimize it with respect to the total error function while keeping all the other parameters \u03bbk\u2032 6=k constant. We keep track of the parameters \u03bbK1 that minimize the error function and, if during one iteration none of the optimized candidates yield any improvement over the current solution, we terminate the algorithm.\nThe trick for optimizing each individual parameter \u03bbk is noting that for a given translation candidate t\u0302, the function P (t\u0302|s) = \u03bbkfk(s, t\u0302) + \u2211 k\u2032 6=k \u03bbk\u2032fk\u2032(s, t\u0302) is linear in \u03bbk, if all \u03bbk\u2032 6=k are constant. For a fixed source sentence s, we take each t\u0302 belonging to the n-best list of s after the previous iteration of the algorithm (as a representative subsample of the search\n19\nspace) and we intersect the lines defined by P (t\u0302|s). We obtain several compact intervals where different t\u0302 are the most likely translations of s (Figure 2.4a, Figure 2.4c). For each such interval, the error function E(t\u0302, t) is constant because t\u0302 does not change, implying that E(t\u0302, t) is a step function with at most n different values (Figure 2.4b, Figure 2.4d), n being the number of candidates in the n-best list. By adding up the step functions for each source sentence s, we observe that the total error function is also a step function, albeit finer grained (Figure 2.4e). \u03bbk is found by iterating over the distinct values of the total error function and choosing any point (e.g. the middle point for robustness) from the interval with the smallest error."}, {"heading": "2.6 Decoding", "text": "In a machine translation system, the decoder is the component responsible for translating a source sentence into a target sentence or for producing a list of n-best translations. At\n20\na high level, the decoder generates a set of partial translation hypotheses by repeatedly applying rules licensed by the translation model (Section 2.3) and by scoring them with the scoring model (Section 2.5). In this section, we review how decoding is done for both FST and SCFG-based translation models.\nBoth FST and SCFG models have a high degree of ambiguity and define a massively exponential space of valid translations for every source sentence. Aggressive fine-tuned optimization techniques need to be applied in order to keep decoders practical and to maintain a high bar for translation quality. This section reviews some of these techniques."}, {"heading": "2.6.1 Decoding with FST Models", "text": "In order to illustrate how decoding with FST models works, we again choose phrase-based translation models (Subsection 2.3.1) as a representative example from this class of models. Our review of FST decoding is based on Koehn (2004), which describes the Pharaoh decoder, now the default decoder in the Moses toolkit.\nThe biggest challenge for FST decoding is reordering the words in the target language. Assuming an output sentence has M words, there are M ! permutations of these words, too many for the decoder to analyze even if it was able to find the correct translation for each word. Instead, FST decoders take a different approach and generate the target sentence word by word, while keeping track of the source words responsible for generating the partial translation so far. This trick effectively swaps the reordering step with the translation step: the algorithm iterates through the source phrases in the order in which their translations would occur in the target language and translates them one by one. The algorithm needs to remember the source words that have already been translated so it can avoid translating them multiple times. Assuming the source sentence has N words, the most compact form in which the decoder can keep track of the processed source words is a N bit mask, where the already translated words are marked with 1 and the remaining words are marked with 0. For example, for the source sentence So it can happen anywhere, a 01110 mask implies that the words it, can and happen have already been translated (in any order). This trick reduces the number of reordering states processed by the decoder to 2N .\nIn order to apply target language modeling features, the decoder must have access to the most recent words in each translation hypothesis. For the n-gram back-off language models from Section 2.4 or the feedforward neural language models discussed in Chapter 4, the\n21\ndecoder must store the last n\u2212 1 target words with each state (bit mask). The fact that we do not need to store entire translation hypotheses and that the source coverage bit masks and the n\u2212 1 target word histories are sufficient leads to the first key optimization applied in FST decoders, known as hypothesis recombination. If two partial hypotheses share the same source coverage bit mask and finish with the same n\u2212 1 target words, we only need to store the highest scoring one, because for any sequence of future translation rules, this hypothesis will continue to have a higher score. This optimization does not degrade the quality of the decoder.\nThe number of decoder states after applying the hypothesis recombination trick is bounded by O(2N \u00d7 |Vt|n\u22121) and the total complexity is O(2N \u00d7N \u00d7 |Vt|n\u22121). Although many target word histories are not valid under the translation model, there are still too many states for the decoder to process. To address this problem, FST decoders limit the size of the window of source words in which the reordering can take place. This by itself is\n22\nnot enough, and another lossy optimization trick known as beam search is employed (Figure 2.5). Translation hypotheses, identified by their source bit mask and their n \u2212 1-gram history, are stacked in priority queues based on the number of covered source words. The decoder iterates over these priority queues in increasing order of covered source words. From each queue, it processes only the top K candidates or those candidates with score above a certain threshold. When choosing which hypotheses to process, the decoder does not rely only on the score given by the scoring model, but it also includes a heuristic score which evaluates how hard it is to translate the remaining part of the source sentence. The purpose of the heuristic is to prevent the decoder from keeping only hypotheses that start with the easy-to-translate parts of the source sentence. The heuristic score is precomputed assuming no reordering is needed for the remaining (uncovered) source words.\nThe top scoring hypothesis from the priority queue spanning the entire source sentence corresponds to the highest scoring translation. In order to reconstruct the actual translation, for each partial hypothesis we need to store an arc to its parent hypothesis (the hypothesis on which the last derivation was applied before obtaining the current hypothesis). The output sentence is obtained by traversing these arcs starting with the top scoring full hypothesis and by accumulating the target phrases from each derivation in reverse order. If a n-best list is needed instead, we can find the n highest scoring paths in this DAG in polynomial time using dynamic programming."}, {"heading": "2.6.2 Decoding with SCFG Models", "text": "SCFG decoding algorithms are similar to parsing algorithms for CFGs. Their exact formulation depends on the translation model in question, but they share the same general approach. Our exposition is based on hierarchical phrase based models (Subsection 2.3.2).\nThe decoding algorithm for hierarchical phrase based models (Chiang, 2007) is a bottom-\nup dynamic programming algorithm. The algorithm translates contiguous intervals of words from the source sentence in increasing order of their length (span). A state in the decoder is characterized by two indexes (i, j), 1 \u2264 i \u2264 j \u2264 N , representing the cost of the best partial hypothesis spanning the source words sji . To compute the state (i, j), the decoder applies each of the grammar rules matching the source context sji , weights them using its scoring model (Section 2.5) and combines them with any subspans covered by the nonterminals on the right hand side of the rule (Figure 2.6). The derivations licensed\n23\nby hierarchical phrase-based models contain at most 2 pairs of nonterminals on their right hand side (Subsection 2.3.2). Rules containing 0 or 1 nonterminal pairs can match sji in only one way. For rules containing 2 nonterminal pairs, the algorithm varies the length of the subspan covered by the first nonterminal, which then uniquely identifies the subspan covered by the second nonterminal. For such rules, O(N) pairs of decoder states are analyzed. The overall complexity of SCFG decoding algorithms is cubic in the length of the source sentence, a major improvement over the exponential complexity of FST decoders.\nThe key challenge with SCFG decoding is incorporating the target language modeling features. In order to apply a n-gram language model, the decoder must store the leftmost and the rightmost n \u2212 1 target words for any partial hypothesis because new target words can be added at both ends of the hypothesis. After applying the hypothesis recombination trick explained in Subsection 2.6.1, the number of decoder states is O(N2 \u00d7 |Vt|2n\u22122) and the total complexity is O(N3 \u00d7 |G| \u00d7 |Vt|2n\u22122). Compared to FST decoders, SCFG decoders only analyze a polynomial number of states in the source sentence, but they store an additional history of n\u2212 1 target words. In order to make SCFG decoders practical, we apply an optimization trick similar to beam search known as cube pruning. For each pair of source indexes (i, j), we stack the partial hypotheses (identified by their two n \u2212 1 target word histories) in a priority queue. As with beam search, only the top candidates from each priority queue are analyzed,\n24\nhypotheses below a certain index or a certain score threshold are discarded. The additional complexity for decoding with hierarchical phrase-based models is that a hypothesis can have up to two parent hypotheses, depending on the number of nonterminal pairs in the top level rule. For rules with two nonterminal pairs, we would like to avoid computing the cartesian product between the priority queues storing the parent hypotheses. Instead, we insert the pair of top candidates from each queue into a new priority queue, identified by its indexes (1, 1). Then, as long as we need to generate new hypotheses, we extract the top pair (i, j) from the new priority queue and insert the pairs (i + 1, j) and (i, j + 1) avoiding duplicates (Figure 2.7). This algorithm guarantees to combine only the top scoring pairs. The number of processed (inserted or extracted) pairs is linear in the size of the output and the operations performed on the additional priority queue have logarithmic time complexity. Overall, the complexity of combining parent hypotheses is reduced from O(K2) to O(K logK). This trick can also be generalized to SCFG models where the number of nonterminal pairs in a rule is unlimited.\nSCFG decoders use the same algorithm for reconstructing the highest scoring transla-\ntion (or n-best list) from the dynamic programming table as FST decoders.\n25"}, {"heading": "2.7 Evaluation", "text": "Evaluating the quality of a machine translation system is a hard problem because sentences can often be translated in many ways. It is possible for equivalent translations not to share any words, while for sentences with many words in common to have completely different meanings. Human translators are able to judge when a system produces good translations, but a more scalable solution was needed to support the massive investments in machine translation over the last decades. As a result, the research community defined and adopted several automatic metrics for evaluating translation quality. Despite their controversy, automatic metrics have several obvious benefits over human evaluation: (i) they facilitate quick iteration by making it easy to see which new features yield improvements, (ii) they are a cost effective way of comparing systems developed by different research groups (assuming the same training and test data is used) and (iii) they eliminate the subjective bias and errors inherent to human evaluation. Quality metrics are also central to feature tuning algorithms like MERT (Section 2.5), and optimizing for more accurate evaluation metrics can theoretically produce higher quality systems.\nMachine translation systems are evaluated on test parallel corpora separated from training and development data in order to prevent overfitting. The decoder translates the source side of the test corpus, and a similarity score is computed between the output translations and the target side of the test corpus (also known as the reference corpus), usually by means of string matching techniques. Evaluation metrics need to be extendable to cases where multiple reference translations are available for each source sentence.\nThroughout this thesis, we use BLEU (Papineni et al., 2002) to report on translation quality. BLEU is by far the most widely used evaluation metric in the research literature. Other metrics that have seen considerable success include METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006).\nBLEU is a precision based metric defined on a [0, 1] scale, where 0 indicates no overlap between an output translation and its references, while 1 corresponds to the ideal case where the sentence matches its references exactly. At a high level, BLEU measures how many n-grams from the output translation occur in the reference translations. It defines a modified n-gram precision term pn, for every n \u2264 4, as:\npn =\n\u2211 t \u2211\ng\u2208n-grams(t) min(c(g), cref(g))\u2211 t \u2211 g\u2208n-grams(t) c(g) , (2.8)\n26\nwhere c(g) is the number of times the n-gram g occurs in the output translation t and cref(g) is the maximum number of times g occurs in any reference translation of t. Each term pn penalizes the translation t if a n-gram g does not occur enough times in t. The numerator is capped by cref(g) in order to prevent falsely increasing the precision by overgenerating words from the reference translations. For robustness, each factor pn is computed by summing the capped counts and the reference counts over the entire test corpus. The factors pn are mixed together using the geometric mean.\nIn order to be a useful metric, BLEU must also address recall. For example, all the factors pn become 1 if the decoder strictly generates a 4-gram from the reference translations. BLEU addresses the recall problem by penalizing sentences shorter than the reference translations. For each output translation, we consider the reference translation with length closest to that of the output. Let c be the total length of the output translations and r be the total length of the chosen references. BLEU defines a brevity penalty as:\nBP = { 1, if c > r, e1\u2212 r c , otherwise.\n(2.9)\nThe brevity term has no effect if the output translations are already longer than the reference translations. Otherwise, the penalty increases exponentially as the output becomes shorter.\nFinally, BLEU is defined as the product between the brevity term and the averaged\nn-gram precision:\nBLEU = BP \u00d7 exp(1 4 \u2211 n\u22644 log pn). (2.10)\nIn order to make gains easier to observe, BLEU is commonly scaled by a factor of 100. We follow this practice in our work.\n27\nChapter 3\nOnline Grammar Extractors"}, {"heading": "3.1 Introduction", "text": "Phrase tables are the default approach for representing translation models in memory. As explained in Subsection 2.3.3, phrase tables load all the phrase pairs extractable from a parallel corpus in memory and organize them as a dictionary, mapping source phrases to lists of target phrases. Phrase tables are very efficient to query because they support constant time access to translation rules, but their main weakness is their huge memory footprint which makes them unsuitable for memory constrained environments. The problem is further aggravated in SCFG-based systems, where the number of extractable rules is exponential in the maximum span of a phrase. In fact, scaling phrase tables for hierarchical phrase-based systems is problematic even without imposing any additional memory constraints.\nA naive solution frequently employed by the research community to facilitate decoding with limited resources (e.g. on commodity machines) is to filter the phrase table and remove all translation rules that are not applicable for a given test set. This approach is not satisfactory in our case, because it does not scale to unseen sentences which are to be expected in any practical setting.\nIn the research literature, a few scalable, compact alternatives to phrase tables have been proposed. Zens and Ney (2007) store phrase tables on disk organized in a trie data structure for efficient read access. Callison-Burch et al. (2005) and Zhang and Vogel (2005) introduce a phrase extraction algorithm based on suffix arrays which extracts translation rules on the fly during decoding. Lopez (2007) shows how online extractors based on suffix arrays can be extended to extract hierarchical translation rules.\nIn our work, we choose to rely on online grammar extractors for retrieving translation\n28\nrules during decoding. Phrase tables stored on disk and suffix array extractors have comparable lookup times, despite the former having better asymptotic complexity (constant vs. logarithmic), because reading from disk is slower. However, the suffix array approach yields several practical benefits for our particular setup. First, the amount of disk space available on a mobile device would continue to be an inconvenient limitation if we choose to store phrase tables on disk. Second, assuming we aggressively prune the tables to fit in the available space (and thereby also degrade the model), the initial cost of downloading the models is far greater with this approach. Finally, in order to maintain a manageable size, phrase tables must limit the maximum number of words spanned by a phrase.1 The memory footprint of online grammar extractors does not depend on this parameter, allowing decoders to use longer phrase pairs, resulting in more accurate translations overall.\nIn the remainder of this chapter, we discuss an efficient and compact suffix array extractor that works for both standard and hierarchical phrase-based systems. Section 3.2 reviews how suffix arrays are used for contiguous phrase extraction (Lopez, 2007). Section 3.3 introduces a novel algorithm for extracting phrases with gaps. Section 3.4 presents details regarding our open source implementation, released as part of the cdec toolkit (Dyer et al., 2010). Section 3.5 illustrates the strengths of our approach with experiments. Section 3.6 concludes with a summary of the ideas discussed in this chapter."}, {"heading": "3.2 Grammar Extraction for Contiguous Phrases", "text": "A suffix array (Manber and Myers, 1990) is a memory efficient data structure which can be used to efficiently locate all the occurrences of a pattern, given as part of a query, in some larger string (named text string in the string matching literature, e.g. Gusfield (1997)). A suffix array is the list of suffixes in the text string sorted in lexicographical order. Formally,\n1Koehn et al. (2003) recommend setting the maximum width of a phrase to 4 words.\n29\nif a = aN1 is the suffix array of a string w = wN1 , then ai stores the starting position of the i-th smallest suffix in w, i.e. wNai\u22121 < w N ai ,\u22001 < i \u2264 N . Since each suffix of w is encoded by its starting position in a, the overall size of the suffix array is linear in the size of w. A crucial property of suffix arrays is that all suffixes starting with a given prefix form a compact interval within the suffix array. Formally, for any 1 \u2264 i \u2264 j \u2264 N , if wNai and w N aj share the same prefix string p, then \u2200i \u2264 k \u2264 j, wNak starts with the prefix p. An example suffix array constructed from a toy sentence is shown in Figure 3.1.\nSuffix arrays are well suited to solve the central problem of contiguous phrase extraction: efficiently matching phrases against the source side of the parallel corpus. Once all the occurrences of a certain phrase are found, translation rules are extracted from a subsample of phrase matches. The rule extraction algorithm (Subsection 2.3.3) is linear in the size of the phrase pattern and adds little overhead to the phrase matching step.\nBefore a suffix array can be applied to solve the phrase matching problem, the source side of the parallel corpus is preprocessed by replacing words with numerical ids and concatenating all sentences together into a single array. The suffix array is constructed on top of this new array. In our implementation, we use a memory efficient suffix array construction algorithm proposed by Larsson and Sadakane (2007) having O(N logN) time complexity.\nThe algorithm for finding the occurrences of a phrase in the parallel corpus uses binary search to locate the interval of suffixes in the suffix array starting with that phrase pattern. Let w1, w2, . . . , wK be the phrase pattern. Since a suffix array is a sorted list of suffixes, we can binary search for the interval of suffixes starting with w1. This contiguous subset of suffix indices continues to be lexicographically sorted and binary search can be used again to find the subinterval of suffixes starting with w1, w2. However, all suffixes in this interval are known to start with w1, so it is sufficient to base all comparisons on only the second word in the suffix. The algorithm is repeated until the whole pattern is matched successfully or until the suffix interval becomes empty, implying that the phrase does not exist in the training data. The complexity of the phrase matching algorithm is O(K logN). The algorithm is illustrated in Figure 3.2.\nWe note that if w1, . . . , wK is a subphrase of a given sentence presented as input to the decoder, then w1, . . . , wK\u22121 is also a legitimate subphrase, which the extractor will match as part of a separate query. Matching w1, . . . , wK\u22121 executes the first K \u2212 1 steps of the phrase matching algorithm for w1, . . . , wK . Therefore, the complexity of the algorithm can\n30\nbe reduced to O(logN) per phrase, by caching the suffix array interval found when searching for w1, . . . , wK\u22121 and only executing the last step of the algorithm for w1, . . . , wK .\nLet M be the length of a sentence received as input by the decoder. If the decoder explores the complete set of contiguous subphrases of the input sentence, the suffix array is queried M(M+1) 2 times. We make two trivial observations to further optimize the extractor by avoiding redundant queries. These optimizations do not lead to major speed-ups for contiguous phrase extraction, but are important for laying the foundations of the extraction algorithm for phrases containing gaps. First, we note that if a certain subphrase of the input sentence does not occur in the training corpus, any phrase spanning this subphrase will not occur in the corpus as well. Second, phrases may occur more than once in a test sentence, but all such repeated occurrences share the same matches in the training corpus. We add a caching layer on top of the suffix array to store the set of phrase matches for each queried phrase. Before applying the pattern matching algorithm for a phrase w1, . . . , wK , we verify if the cache does not already contain the result for w1, . . . , wK and check if the search for\n31\nw1, . . . , wK\u22121 and w2, . . . , wK returned any results. The caching layer is implemented as a trie with suffix links and constructed in a breadth first manner so that shorter phrases are processed before longer ones (Lopez, 2008a) (Figure 3.3)."}, {"heading": "3.3 Grammar Extraction for Phrases with Gaps", "text": "In Chapter 2, we showed that hierarchical translation systems rely on the synchronous context free grammar formalism which enables them to make use of translation rules containing gaps. In this section, we present an algorithm for extracting synchronous context free rules from a parallel corpus, which requires us to improve the phrase extraction algorithm from Section 3.2 to handle discontiguous phrases. We first published this algorithm in Baltescu and Blunsom (2014) and it was later adopted as a central piece in He et al. (2015)\u2019s work to massively scale discontiguous phrase extraction using GPUs.\nLet us make some notations to ease the exposition of the phrase extraction algorithm. Let a, b and c be words in the source language, X a nonterminal used to denote the gaps in translation rules and \u03b1 and \u03b2 source phrases containing zero or more occurrences of X . Let M\u03b1 be the set of matches of the phrase \u03b1 in the source side of the training corpus,\n32\nwhere a phrase match is defined by a sequence of indices marking the positions where the contiguous subphrases of \u03b1 are found in the training data. Our goal is to find M\u03b1 for every phrase \u03b1. Section 3.2 shows how to achieve this if X does not occur in \u03b1.\nLet us now consider the case when \u03b1 contains at least one nonterminal. If \u03b1 = X\u03b2 or \u03b1 = \u03b2X , then M\u03b1 = M\u03b2 , because the phrase matches are defined only in terms of the indices where the contiguous subpatterns match the training data. The words spanned by the leading or trailing nonterminal are not relevant because they do not appear in the translation rule. Since |\u03b2| < |\u03b1|, M\u03b2 is already available in the trie cache as a consequence of the breadth first search approach we use to compute the sets M .\nThe remaining case is \u03b1 = a\u03b2c, where both Ma\u03b2 and M\u03b2c have been computed at a previous step. We take into consideration two cases depending on whether the next-to-last symbol of \u03b1 is a terminal or not (i.e. \u03b1 = a\u03b2bc or \u03b1 = a\u03b2Xc, respectively). In the former case, we calculate M\u03b1 by iterating over all the phrase matches in Ma\u03b2b and selecting those matches that are followed by the word c (Figure 3.4). In the second case, we take note of the experimental results of Lopez (2008a) who shows that translation rules that span more than 15 words have no effect on the overall quality of a translation system. In our implementation, we introduce a parameter max rule span setting the maximum span of a translation rule. For each phrase match in Ma\u03b2X , we check if any of the following\n33\nmax rule span words is c (subject to sentence boundaries and taking into account the current span of a\u03b2X) and insert any new phrase matches in M\u03b1 accordingly (Figure 3.5).\nNote that M\u03b1 can also be computed by considering two cases based on the second symbol in \u03b1 (i.e. \u03b1 = ab\u03b2c or \u03b1 = aX\u03b2c) and by searching the word a at the beginning of the phrase matches in Mb\u03b2c or MX\u03b2c. In our implementation, we consider both options and apply the one that is likely to lead to a smaller number of comparisons. The complexity of the algorithm for computing M\u03b1=a\u03b2c is O(min(|Ma\u03b2|, |M\u03b2c|)). Lopez (2007) presents a similar grammar extraction algorithm for discontiguous phrases, but the complexity for computing M\u03b1 is O(|Ma\u03b2|+ |M\u03b2c|). Lopez (2007) introduces a separate optimization based on double binary search (Baeza-Yates, 2004) of time complexity O(min(|Ma\u03b2|, |M\u03b2c|) log max(|Ma\u03b2|, |M\u03b2c|)), designed to speed up the extraction algorithm when one of the lists is much shorter than the other. Our approach is asymptotically faster than both algorithms. In addition to this, we do not require the lists M\u03b1 to be sorted, allowing for a much simpler implementation. Lopez (2007) needs van Emde Boas trees (Cormen et al., 2009) and an inverted index to sort these lists efficiently.\nThe extraction algorithm can be optimized by precomputing an index for the most frequent discontiguous phrases (Lopez, 2007). To construct the index, we first identify the most frequent contiguous phrases in the training data. We use the LCP array (Manber and Myers, 1990), an auxiliary data structure constructed in linear time from a suffix array (Kasai et al., 2001), to find all the contiguous phrases in the training data that occur above a certain frequency threshold. We add these phrases to a max-heap together with their frequencies and extract the most frequent K contiguous patterns, where K is a parameter received as input by the grammar extractor. We iterate over the source side of the training data and populate the index with all the discontiguous phrases of the form uXv and uXvXw, where u, v and w are amongst the most frequent K contiguous phrases in the training data."}, {"heading": "3.4 Implementation Details", "text": "Our grammar extractor is designed as a standalone tool which takes as input a word-aligned parallel corpus and a test set and produces as output the set of translation rules applicable to each sentence in the test set. The extractor produces the output in the format expected by\n34\nthe cdec decoder, but the implementation is self-contained and easily extendable to other hierarchical phrase-based translation systems.\nOur tool performs grammar extraction in two steps. The preprocessing step takes as input the parallel corpus and the file containing the word alignments and writes to disk binary representations of the data structures needed in the extraction step: a dictionary mapping tokens to numerical ids, the source suffix array, the target data array, the word alignment, the precomputed index of frequent discontiguous phrase matches and a translation table storing count based estimates for the conditional probabilities p(s|t) and p(t|s), for every source word s and target word t collocated in the same sentence pair in the training data. cdec uses both phrase level and word level generative translation models as features in the decoder (see Section 2.5 for details). The translation table is needed to efficiently compute word based features online. The preprocessing step needs to be performed only once when extracting grammars for multiple test corpora. The extraction step takes as input the precomputed data structures and a test corpus and produces a set of grammar files containing the applicable translation rules for each sentence in the test set. The cdec decoder expects that all grammars are made available ahead of time, which is why we process each test corpus as a batch. We do not take advantage that the corpus is known ahead of time and do not apply pruning techniques as commonly done for phrase tables.\nThe grammar extractor is written in C++. Our implementation leverages the benefits of a multithreaded environment to speed up grammar extraction. The test corpus is dynamically distributed across the number of available threads (specified by the user via the --threads parameter). All the data structures computed in the preprocessing step are immutable during extraction and can be effectively shared across multiple threads at no additional time or memory cost. In contrast, cdec\u2019s cython extractor implementing the algorithm proposed by Lopez (2007) uses the multiprocessing library for parallelization. The precomputed data structures are copied across all the processes used for extraction, increasing the memory usage by a factor proportional to the number of processes. As a result, the parallelization feature of the cython extractor is not usable when a limited amount of memory is available.\nOur code is released together with a suite of unit tests meant to encourage developers to add their own features to our grammar extractor, without fear that their code changes might have unexpected consequences.\n35"}, {"heading": "3.5 Experiments", "text": "In this section, we present a set of experiments which illustrate the benefits of our new extraction algorithm. We compare our implementation with the cdec cython extractor which implements the algorithm proposed by Lopez (2007). In order to make the comparison fair and to prove that the speed-ups we obtain are indeed a result of our new algorithm, we also report results for a C++ implementation of the algorithm in Lopez (2007).\nIn our experiments, we used the French-English data from the europarl corpus, a set of 2M sentence pairs containing a total of 105M tokens. The training data was tokenized, lowercased and sentence pairs with unusual length ratios were filtered out using the corpus preparation scripts available in cdec.2 The corpus was aligned with fast align (Dyer et al., 2013) and the alignments were symmetrized using the grow-diag-final-and heuristic (Section 2.2). We extracted translation rules for the newstest2012 corpus. The test corpus consists of 3,003 sentences and was tokenized and lowercased using the same scripts. All the data used in these experiments is available on the WMT website.3 In all implementations, if more than 300 matches in the parallel corpus are found for a given input phrase, 300 of these matches are deterministically sampled without replacement for the purposes of phrase extraction.\nTable 3.1 shows results for the preprocessing step of the three implementations. We note a 10-fold time reduction when reimplementing Lopez (2007)\u2019s algorithm in C++. We believe this is a case of inefficient programming when the precomputed index is constructed in the cython code and not a result of using different programming languages. Our new implementation does not significantly outperform the C++ reimplementation of the preprocessing step because we construct the same set of data structures.\n2We followed the instructions at: http://www.cdec-decoder.org/guide/tutorial.html. 3The website is accessible at: http://statmt.org/wmt15/.\n36\nThe second set of results (Table 3.2) show the running times and memory requirements of the extraction step. Our C++ reimplementation of Lopez (2007)\u2019s algorithm is slightly less efficient than the original cython extractor, supporting the idea that the two programming languages have comparable performance. We note that our novel extraction algorithm is over 4 times faster than the original approach of Lopez (2007).\nTable 3.3 demonstrates the benefits of parallel phrase extraction. We repeated the experiments from Table 3.2 using 8 processes in cython and 8 threads in C++. As expected, the running times decrease roughly 8 times. The benefits of sharing the data structures in the parallel extraction step are obvious, our new implementation using 29.1 GB less memory."}, {"heading": "3.6 Summary", "text": "In this chapter, we investigated compact alternatives for phrase tables. We began with a brief exposition of existing techniques and showed why online grammar extractors are a natural choice for the task of constructing an autonomous translation system that can run on a commodity machine or on a mobile device. We first reviewed how suffix array grammar extractors work in phrase-based translation systems and then introduced a novel extraction algorithm for hierarchical phrases that is 4 times faster than Lopez (2007). We provided details on our open source implementation and showed how to maximise parallelism without\n37\nany negative impact on the memory footprint. Finally, we presented several experiments illustrating the benefits of the approach we proposed in this chapter.\n38\nChapter 4\nNeural Language Models"}, {"heading": "4.1 Introduction", "text": "Most translation systems today use standard back-off n-gram models to model the target language. We showed in Section 2.4 that traditional n-gram models estimate and store a weight for every n-gram in the training data. The amount of data used to train language models has significant impact on the overall translation quality. Monolingual data is cheap to obtain and it has become a standard to train language models on corpora consisting of billions of words. On these large corpora, even the most compact implementations of n-gram models require many gigabytes of memory, making them unsuitable for memory constrained environments.\nIn this chapter, we focus on neural language models as a compact alternative to traditional n-gram models. Neural language models (Bengio et al., 2003) are a more recent class of language models which use neural networks to learn distributed representations for words. Neural language models project words and contexts into a continuous vector space. The conditional probability P (wi|wi\u22121i\u2212n) is defined to be proportional to the distance between the continuous representation of the word wi and the context wi\u22121i\u2212n. Neural language models learn to cluster word vectors according to their syntactic and semantic role. The strength of neural language models lies in their ability to generalize to unseen n-grams, because similar words share the probability of following the same context. Neural language models have been shown to outperform n-gram language models using intrinsic evaluation (Chelba et al., 2013; Mikolov et al., 2011a; Schwenk, 2007) or when used in addition to traditional models in natural language systems such as speech recognizers (Mikolov et al., 2011a; Schwenk, 2007).\n39\nNeural language models are much more computationally intensive than back-off ngram models. Scaling these models is a hard problem that has received a lot of attention in the research community. For instance, every forward pass through the underlying neural network computes an expensive softmax activation in the output layer of the network with a cost proportional to the product between the size of the vocabulary and the size of the word embeddings. This operation is performed for every n-gram scored by the network and for every example used to update the model during training. Several methods have been proposed to alleviate this problem: some applicable only when training the model (Mnih and Teh, 2012; Bengio and Senecal, 2008), while others can also speed up arbitrary queries (Morin and Bengio, 2005; Mnih and Hinton, 2009). Scaling neural language models is a topic covered extensively in this chapter.\nIn machine translation, it has been shown that neural language models improve translation quality if incorporated as additional features in machine translation decoders (Botha and Blunsom, 2014; Vaswani et al., 2013) or if used for n-best list rescoring (Schwenk, 2010). One problem is that most of the work done in this line of research uses different techniques for scaling neural language models, leaving the question of finding the optimal neural language model architecture for machine translation unanswered. As part of our goal to design a compact and scalable high quality translation system, we aim to address this problem by conducting a thorough investigation of what optimization techniques work best when integrating neural language models in translation systems.\nUnlike previous research, we focus on how neural language models perform when used as the sole language models in translation systems because we cannot afford to use additional back-off n-gram models in memory constrained environments. We believe this constraint allows us to draw more insightful conclusions about the optimal architecture for neural language models in general. We argue that when neural language models are used in addition to back-off n-gram models, most of the language modeling is actually done by the back-off n-gram models, with the neural models only acting as a differentiating factor when the back-off models cannot provide a decisive probability. This argument is based on the results in Section 4.7, which show that a back-off n-gram model trained without any memory constraints clearly outperforms even our largest neural language models, so it is reasonable to expect that when these models are used together in a translation system, most of the benefits come from the back-off n-gram model. Furthermore, when we trained\n40\ntranslation systems with both of these features (e.g. for Baltescu and Blunsom (2015), Table 5), we observed that the log-linear scoring model (Section 2.5) learned on average a weight twice as large for the back-off n-gram model compared to the neural model, implying that the log-linear scoring model sees the unconstrained back-off n-gram model as a more reliable signal than the neural model.\nVaswani et al. (2013) show that directly integrating neural language models in machine translation decoders leads to better results over using these models only for reranking nbest lists. This result matches the intuition that the neural language modeling features help decoders converge on a better set of translations during the exploration of the search space. Based on this result, we focus our investigation on the direct integration of neural language models in machine translation decoders.\nWe use feedforward neural networks as the underlying architecture for our neural language models because it makes the decoder integration much more straightforward. The decoder needs to store only the n\u22121-gram histories for translation hypotheses (Section 2.6). Feedforward neural language models can be integrated in any decoder (that supports backoff n-gram models) without making any changes to the decoder itself. Auli and Gao (2014) show that recurrent neural language models can also be integrated in decoders, but to prevent an exponential blow up in the number of decoder states, they only keep the top scoring hypothesis for any n\u2212 1-gram history. The chapter is structured as follows. Section 4.2 introduces the data we use to perform the experiments which lay the grounds for our investigation. Section 4.3 presents the basic architecture of neural language models which we later seek to improve upon. Section 4.4 explores several tricks for reducing the amount of computation in the softmax step. Section 4.5 investigates noise contrastive training, a sampling technique which drastically reduces the complexity of training neural language models, and shows how this method can be used in conjunction with the class-based factorization introduced in Section 4.4. In Section 4.6, we explore diagonal context matrices as a source for further speed improvements. In Section 4.7, we analyze the performance of neural language models and traditional backoff models on a wide memory spectrum and show that neural language models are superior in memory constrained environments. Section 4.8 shows how neural language models can be extended to include direct n-gram features which allow them to learn weights for ngrams from the conditioning context. Section 4.9 presents how the conditioning context of\n41\nLanguage pairs # tokens # sentences\nneural language models can be extended with words from the source sentence. Section 4.10 concludes the chapter with a summary of our learnings."}, {"heading": "4.2 Experimental Setup", "text": "In our experiments, we use data from the 2014 edition of the Workshop in Machine Translation.1 We train standard phrase-based translation systems for French\u2192 English, English \u2192 Czech and English\u2192 German using the Moses toolkit (Koehn et al., 2007).\nWe used the europarl and the news commentary corpora as parallel data for training the translation systems. The parallel corpora were tokenized, lowercased and sentences longer than 80 words were removed using standard text processing tools.2 Table 4.1 contains statistics about the training corpora after the preprocessing step. We tuned the translation systems on the newstest2013 data using minimum error rate training (Och, 2003) and we used the newstest2014 corpora to report uncased BLEU scores averaged over 3 runs.\nThe language models were trained on the europarl, news commentary and the 2007- 2013 editions of the news crawl corpora. The corpora were tokenized and lowercased using the same text processing scripts. The words which did not occur in the target side\n1The data is available at: http://www.statmt.org/wmt14/translation-task.html. 2We followed the first two steps from http://www.cdec-decoder.org/guide/tutorial.\nhtml.\n42\nof the parallel data were replaced with a special \u3008unk\u3009 token. We report the sizes of the monolingual corpora and of the vocabularies after preprocessing in Table 4.2.\nThroughout this chapter, we report results for 5-gram language models, regardless of whether they are back-off n-gram models or neural language models. To construct the backoff n-gram models, we used the compact trie-based implementation available in KenLM (Heafield, 2011). This not only enables us to perform a fair comparison between the two types of models when memory is a key limitation (Section 4.7), but also prevents us from facing difficulties fitting these models in the memory of the machines we use to run experiments. When training neural language models, we set the size of the distributed representations to 500, we use 10 negative samples for noise contrastive estimation (Section 4.5) and we use diagonal context matrices (Section 4.6), unless otherwise noted. In cases where we report results on only one language pair, the reader should assume we used French\u2192English data."}, {"heading": "4.3 Model Description", "text": "As a basis for our investigation, we implement a probabilistic neural language model as defined in Bengio et al. (2003).3 For every word w in the target vocabulary Vt, we learn two distributed representations qw and rw in RD. The vector qw captures the syntactic and semantic role of the word w when w is part of a conditioning context, while rw captures its\n3We released our implementation as a scalable open source neural language modeling toolkit at: https: //github.com/pauldb89/oxlm.\n43\nrole as a prediction. To simplify our notations, let hi denote the conditioning context wi\u22121i\u2212n for a word wi in a given corpus. To find the conditional probability P (wi|hi), our model first computes a context projection vector:\np = f ( n\u22121\u2211 j=1 Cjqwi\u2212j ) ,\nwhere Cj \u2208 RD\u00d7D are context specific transformation matrices and f is a component-wise rectified linear activation. The model computes a set of similarity scores measuring how well each word w \u2208 Vt matches the context projection of hi. The similarity score is defined as \u03c6(w, hi) = rTwp + bw, where bw is a bias term initialized with the unigram probability of w. A softmax activation is used to transform the similarity scores into probabilities:\nP (wi|hi) = exp(\u03c6(wi, hi))\u2211 w\u2208V exp(\u03c6(w, hi)) .\nThe model\u2019s architecture is illustrated in Figure 4.1.\nThe complete set of parameters is (Cj,qw, rw, bw)1\u2264j<n, w\u2208Vt . The parameters are initialized randomly from a Gaussian distribution with mean 0 and standard deviation 0.1. The model is trained to maximize log-likelihood with L2 regularization. We shuffle the training data randomly at each iteration and use adaptive gradient descent (AdaGrad) (Duchi et al., 2011) to improve convergence. In order to reduce the training time, we split the training data into minibatches of 10 000 samples and evenly distribute the computation of gradient updates within each minibatch over the number of available threads. The model\u2019s parameters are updated once at the end of each minibatch.\nThe architecture presented in this section is not fast enough to scale to real world translation applications. In the next sections, we discuss the key issues with this architecture and investigate several optimizations to address these problems."}, {"heading": "4.4 Normalization", "text": ""}, {"heading": "4.4.1 Introduction", "text": "Optimizing the computation of the softmax function in the output layer of the network is the most crucial step for making neural language models practical. This operation is performed for every data point presented as input to the network both during training and decoding. For a given input, the time complexity of the softmax step is O(|Vt| \u00d7 D). The amount\n44\nof computation involved is prohibitive given that when decoding with a neural language modeling feature, the model is queried several hundred thousand times for a sentence of average length.\nPrevious publications on neural language modeling in machine translation have approached this problem in two different ways. Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalization when decoding, albeit Devlin et al. (2014) alter their training objective to learn self-normalized models, i.e. models where the sum of the values in the output layer is (ideally) close to 1. Vaswani et al. (2013) use noise contrastive estimation to speed up training, while Devlin et al. (2014) train their models with standard gradient descent on a GPU.\nThe second approach is to explicitly normalize the models, but to limit the set of words over which the normalization is performed, either via class-based factorization (Botha and Blunsom, 2014) or using a shortlist4 containing only the most frequent words in the vocabulary and scoring the remaining words with a back-off n-gram model (Schwenk, 2010). Hybrid architectures where only infrequent words are grouped into class hierarchies have also been proposed (Le et al., 2011). These normalization techniques can be applied both at training and decoding time.\nIn this section, we explore both options. The first approach requires little elaboration: the probabilities P (w|h) are replaced by \u03c6(w, h) when scoring partial hypotheses in the decoder. The second approach is presented in greater detail in the next subsections. Subsection 4.4.2 introduces class factored models (Botha and Blunsom, 2014). Subsection 4.4.3 takes this idea further by constructing a multiple level class hierarchy over the vocabulary. Schwenk (2010)\u2019s approach is not applicable to our particular setup, since it relies on backoff n-gram models for scoring infrequent words. Le et al. (2011)\u2019s approach does not scale well enough for our goals, since it employs large shortlists and word classes (4000-8000 words), but the core idea is captured in the extension presented in Subsection 4.4.3. We conclude this section with a set of experiments aimed at comparing the effect on speed and translation quality for the optimizations discussed in this section.\n4Here, the term shortlist should be understood as defined in Schwenk (2010).\n45"}, {"heading": "4.4.2 Class Factored Models", "text": "The class based factorization trick (Goodman, 2001) is one option for reducing the excessive amount of computation in the softmax step. We partition the vocabulary intoK classes\n{C1, . . . , CK} such that V = \u22c3K i=1 Ci and Ci \u2229 Cj = \u2205,\u22001 \u2264 i < j \u2264 K. We define the conditional probabilities as:\nP (wi|hi) = P (ci|hi)P (wi|ci, hi),\nwhere ci is the class the word wi belongs to, i.e. wi \u2208 Cci . We adjust the model definition to also account for the class probabilities P (ci|hi). We associate a distributed representation sc and a bias term tc to every class c. The class conditional probabilities are computed reusing the projection vector p with a new scoring function \u03c8(c, hi) = sTcp + tc. The probabilities are normalized separately:\nP (ci|hi) = exp(\u03c8(ci, hi))\u2211K j=1 exp(\u03c8(cj, hi))\nP (wi|ci, hi) = exp(\u03c6(wi, hi))\u2211 w\u2208Cci exp(\u03c6(w, hi))\nFigure 4.2 illustrates the class decomposition trick. WhenK \u2248 \u221a |Vt| and the word classes have roughly equal sizes, the softmax step has a more manageable time complexity of\nO( \u221a |Vt| \u00d7 D) for both training and computing probabilities associated with test n-gram queries.\n46"}, {"heading": "4.4.3 Tree Factored Models", "text": "One can take the idea presented in the previous subsection one step further and construct a tree over the vocabulary Vt. The words in the vocabulary are used to label the leaves of the tree. Let n1, . . . , nk be the nodes on the path descending from the root (n1) to the leaf labelled with wi (nk). The probability of the word wi to follow the context hi is defined as:\nP (wi|hi) = k\u220f j=2 P (nj|n1, . . . , nj\u22121, hi).\nWe associate a distributed representation sn and bias term tn to each node in the tree. The conditional probabilities are obtained reusing the scoring function \u03c8(nj, hi):\nP (nj|n1, . . . , nj\u22121, hi) = exp(\u03c8(nj, hi))\u2211\nn\u2208S(nj) exp(\u03c8(n, hi)) ,\nwhere S(nj) is the set containing the siblings of nj and the node itself. Note that the class decomposition trick described earlier can be seen as a tree factored model with two layers, where the first layer contains the word classes and the second layer contains the words in the vocabulary (see Figure 4.2 and Figure 4.3).\nThe optimal time complexity is obtained by using balanced binary trees. The overall complexity of the normalization step becomes O(log |Vt| \u00d7 D) because the length of any path in the tree is bounded by O(log |Vt|) and because exactly two terms are present in the denominator of every normalization operation.\n47\nInducing high quality binary trees is a difficult problem which has received some attention in the research literature (Mnih and Hinton, 2009; Morin and Bengio, 2005). Results have been somewhat unsatisfactory, with the exception of Mnih and Hinton (2009), who use a top down clustering algorithm for constructing trees. Since Mnih and Hinton (2009)\u2019s code has not been open sourced and is difficult to reproduce, in our experiments we use Huffman trees (Huffman, 1952) instead. Huffman trees do not have any linguistic motivation, but have the property that a minimum number of nodes are inspected during training. In our experiments, the Huffman trees have depths close to log |Vt|."}, {"heading": "4.4.4 Experiments", "text": "In this section, we evaluate three types of models: unnormalized, class factored and tree factored models. These models use diagonal context matrices (Section 4.6) and are trained using noise contrastive estimation (Section 4.5), except for tree factored models, where noise contrastive estimation does not improve training time relative to standard gradient descent. In Table 4.3, we observe that class factored models perform best in terms of translation quality, with 0.5 BLEU points for fr\u2192en and 0.3 BLEU points for en\u2192de over unnormalized models, which are the next best performing models in terms of translation quality. These results also indicate that tree factored models perform poorly compared to the other candidates: -1.2 BLEU points for fr\u2192en, -1.3 BLEU points for en\u2192cs and -1 BLEU point for en\u2192de. We believe this to be a consequence of imposing an artificial hierarchy over the vocabulary.\n48\nTable 4.4 compares two popular techniques for obtaining word classes: Brown clustering (Brown et al., 1992; Liang, 2005) and frequency binning (Mikolov et al., 2011b). From these results, we learn that the clustering technique employed to partition the vocabulary can have a huge impact on translation quality and that Brown clustering is clearly superior to frequency binning (0.5 BLEU points for fr\u2192en). We report the average time needed to decode a sentence for each of these models in Table 4.5. We observe that both class and tree factored models are slow compared to unnormalized models. One option for speeding up factored models is using a GPU to perform the vector-matrix operations. We did not explore this option since GPU implementations are architecture specific and would limit the usability of our open source toolkit by the community. Overall, if quality is more important than speed, we recommend using class factored models; otherwise, we recommend using unnormalized models."}, {"heading": "4.5 Noise Contrastive Training", "text": "Training neural language models to maximise data likelihood involves several iterations over the entire training corpus and applying the backpropagation algorithm for every training sample. Even with the previous factorization tricks, training neural models is slow. We investigate an alternative approach for training language models based on noise contrastive estimation, a technique which does not require normalized probabilities when computing gradients (Mnih and Teh, 2012). This method has already been used for training neural language models for machine translation by Vaswani et al. (2013).\nThe idea behind noise contrastive training is to transform a density estimation problem into a classification problem, by learning a classifier to discriminate between samples drawn from the data distribution and samples drawn for a known noise distribution. Following Mnih and Teh (2012), we set the unigram distribution Pn(w) as the noise distribution and\n49\nuse k times more noise samples than data samples to train our models. The new objective function is:\nJ(\u03b8) = m\u2211 i=1 logP (C = 1|\u03b8, wi, hi) + m\u2211 i=1 k\u2211 j=1 logP (C = 0|\u03b8, nij, hi),\nwhere nij are the noise samples drawn from Pn(w). The posterior probability that a word is generated from the data distribution given its context is:\nP (C = 1|\u03b8, wi, hi) = P (wi|\u03b8, hi)\nP (wi|\u03b8, hi) + kPn(wi) .\nMnih and Teh (2012) show that the gradient of J(\u03b8) converges to the gradient of the loglikelihood objective when k \u2192\u221e. When using noise contrastive estimation, additional parameters can be used to capture the normalization terms. Mnih and Teh (2012) fix these parameters to 1 and obtain the same perplexities, thereby circumventing the need for explicit normalization. However, this method does not provide any guarantees that the models are normalized at test time. In fact, the outputs may sum up to arbitrary values, unless the model is explicitly normalized.\nNoise contrastive estimation is more efficient than the factorization tricks at training time, but at test time one still has to normalize the model to obtain valid probabilities. We propose combining this approach with the class decomposition trick resulting in a fast algorithm for both training and testing. In the new training algorithm, when we account for the class conditional probabilities P (ci|hi), we draw noise samples from the class unigram distribution, and when we account for P (wi|ci, hi), we sample from the unigram distribution of only the words in the class Cci .\nClass factored models enable us to investigate noise contrastive estimation at a much larger scale than previous results (e.g. the WSJ corpus used by Mnih and Teh (2012) has slightly over 1M tokens), thereby gaining useful insights on how this method truly performs\n50\nat scale. In our experiments, we use a 2B words corpus and a 100k words vocabulary (Section 4.2). Table 4.6 summarizes our findings. We obtain a 0.2 BLEU points improvement using stochastic gradient descent, however, noise contrastive estimation reduces training time by a factor of 7.\nTable 4.7 reviews the neural models introduced in the previous section and shows the time needed to train each one. We note that noise contrastive training requires roughly the same amount of time for unnormalized and class factored models. Also, we note that this method is at least as fast as maximum likelihood training, even when the latter is applied on tree factored models. Since multi-level tree hierarchies have lower quality, take longer to query and do not yield any substantial benefits at training time compared to unnormalized models, we conclude they represent a suboptimal language modeling architecture for machine translation."}, {"heading": "4.6 Diagonal Context Matrices", "text": "In this section, we investigate diagonal context matrices as a source for reducing the computational cost of calculating the projection vector p. In the standard definition of a neural language model, this cost is dominated by the softmax step, but as soon as tricks like noise contrastive estimation or hierarchical factorizations are used, this operation becomes the main bottleneck for training and querying the model. In a diagonal matrix, all the elements located outside the main diagonal are zero. As a result, the matrix vector products Cjqwi\u2212j\n51\nAcceptance Ratio Memory BLEU\nfor computing the projection vector p become scalar products and the time complexity is reduced fromO(D2) toO(D). A similar time reduction is achieved in the backpropagation algorithm, as only O(D) context parameters need to be updated for every training instance.\nDevlin et al. (2014) also identified the need for finding a scalable solution for computing the projection vector. Their approach is to cache the product between every word embedding and every context matrix and to look up these terms in a cache as needed. Devlin et al. (2014)\u2019s approach works well when decoding, but it is not applicable during training and it requires additional memory.\nTable 4.8 compares the training time for diagonal and full context matrices for class factored models. Both models have similar BLEU scores, but the training time is reduced by a factor of 3 when diagonal context matrices are used. Table 4.9 shows the average decoding time per sentence with diagonal and full context matrices for class factored models and unnormalized models. We observe a time reduction by a factor of 4 for class factored models and a time reduction by a factor of 50 when the normalization step is skipped altogether.\n52\nEmbeddings Size Memory BLEU"}, {"heading": "4.7 Quality vs. Memory Trade-Off", "text": "In this section, we compare neural language models and back-off n-gram models on a wide memory spectrum and track how the end to end quality of a translation system changes as these models grow in size. We perform this analysis with two goals in mind. First, we seek to verify the hypothesis that neural language models are indeed better suited for memory constrained environments. Second, we aim to understand how neural language models perform relative to back-off n-gram models when no memory restrictions are enforced, and if increasing the size of the word embeddings beyond what is typically reported in the research literature is sufficient to correct any BLEU score discrepancies.\nIn this analysis, we used a compact trie-based implementation with floating point quantization for constructing memory efficient back-off n-gram models (Heafield, 2011). A 5-gram model trained on the English monolingual data introduced in Section 4.2 requires 12 GB of memory. We randomly sample sentences with an acceptance ratio ranging between 0.01 and 1 to construct smaller models. The BLEU scores obtained using these models are reported in Table 4.10.\nWe use class factored models with diagonal context matrices as the representative architecture for neural language models. The normalization techniques discussed in Section 4.4 and noise contrastive estimation do not affect the memory footprint of these models. Using full context matrices would increase the size of the models, but by not more than 3%. We do not use any additional compression techniques (e.g. floating point quantization), although this is technically possible. To observe how these models perform for various memory thresholds, we experiment with setting the size of the word embeddings between 100 and 5000. The results are displayed in Table 4.11.\n53\nWe plot the quality vs. memory trade-off for back-off n-gram models and class factored models in Figure 4.4. We observe that neural models perform significantly better (over 1 BLEU point) when less than 1 GB of memory is used, which is roughly the amount of memory available on mobile phones and commodity machines. This result confirms the potential of neural language models for applications designed to run in memory constrained environments. At the other end of the scale, we see that back-off models outperform even the largest neural language models by a decent margin (almost 1 BLEU point) and that we can expect only modest gains if we increase the size of the word embeddings further. This result suggests that increasing the size of the word embeddings is not sufficient for correcting the quality gap between neural language models and back-off n-gram models."}, {"heading": "4.8 Direct N-gram Features", "text": "Neural language models learn embeddings for single words which they combine using context matrices and an activation function. Direct features extend neural language models by allowing them to learn weights for n-grams. Direct features play a similar role to frequency counts in back-off n-gram models: larger predictive weights correspond to more likely ngrams. These weights are used to amend the similarity scores produced by neural language models in cases where the occurrence of several words together in the conditioning context provides more accurate signal. From the perspective of the underlying neural network, pre-\n54\ndictive weights are edges connecting the input and the output layers of the network directly.\nDirect features for unigrams were originally introduced in neural language models by Bengio et al. (2003). Mikolov et al. (2011a) extend these features to n-grams and show they are useful for reducing perplexity and improving word error rate in speech recognizers. Direct n-gram features are reminiscent of maximum entropy language models (Berger et al., 1996) and are sometimes called maximum entropy features (Mikolov et al., 2011a).\nTo our knowledge, prior to our work, neural language models with direct n-gram features have not been explored in machine translation. In this section, our primary goal is to understand the effect of direct n-gram features on translation quality. A common belief in the NLP community is that back-off n-gram models are better at estimating the probabilities of frequent n-grams compared to neural language models. We would like to see what gains are obtained when our models learn the weights of the most frequent n-grams in the training data.\nFormally, we define a set of binary feature functions f = fM1 , whereM is the number of n-grams we choose to model. We assign each function a weight from a real valued vector u \u2208 RM . To account for word classes, we also define a set of binary feature functions gc and a vector of weights vc for each word cluster c. An n-gram (w, h) has a corresponding feature function gc(w, h) iff w \u2208 Cc. The scoring functions introduced in Section 4.3 are updated to include an additional term:\n\u03c8(c, hi) = sTc p + tc + u T f(c, hi)\n\u03c6(w, hi) = rTwp + bw + v T c gc(w, hi)\nIn all other respects, our model definition remains unchanged. The weight vectors u and vc are learned together with the other parameters using noise contrastive estimation.\nFundamentally, neural language models with direct n-gram features require at least as much memory as n-gram language models because they learn and store individual weights for every n-gram. This is why we choose to focus only on the most frequent n-grams in the training data. In our implementation, we need two indexes: one for mapping n-grams to their weights, the other for mapping n-gram contexts hi to the lists of words and classes that can follow hi. We needed the latter index in order to normalize our models both when computing the gradient at training time and when querying the model at decoding time. We used feature hashing (Weinberger et al., 2009; Ganchev and Dredze, 2008) to compress\n55\nNumber of n-grams Minimum frequency Memory BLEU\nthese indexes. Research literature shows that as long as the number of collisions resulting from feature hashing is low, the overall quality of the models remains unaffected. We experienced the same outcome when hand tuning the models looking for a good memory vs. quality trade-off.\nIn our experiments, we take the most frequent M n-grams in the corpus, where M varies from 1 million to 100 millions (Table 4.12). For each model, we report the minimum frequency of the n-grams included in the model, together with the memory footprint and the BLEU score. These models are built on top the 500-dimensional class factored model with diagonal contexts which we have used as a baseline for neural language models in this chapter. Overall, we observe a solid improvement in translation quality of 1 BLEU point. If we limit the memory footprint to at most 1 GB, we get over half a BLEU point improvement. These gains are comparable to those we obtain when increasing the size of the word embeddings. On the other hand, for our largest model, the increase in training and decoding time is roughly 2x, whereas a model with 5000-dimensional embeddings takes 10x more time to train and query compared to the baseline."}, {"heading": "4.9 Source Sentence Conditioning", "text": "In this section, we follow Devlin et al. (2014) and investigate extending the conditioning context of neural language models with a window of words from the source sentence. This idea is similar to the recent trend in neural translation models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014), however, our models do not capture the full translation process, because we rely on a separate alignment model (Section 2.2) to locate the relevant source words.\n56\nFormally, a source conditioned language model computes the probability of a target\nsentence t given a source sentence s and an alignment a as follows:\nP (t|s, a) = \u220f i P (ti|ti\u221211 , s, a).\nIn our models, we continue to use the traditional n-th order Markov assumption on the target side. Also, instead of modeling the entire source sentence, we focus only on a window of most relevant 2m+1 source words for each target word ti. The window of source words is selected using the alignment links a and is centered around a\u0304i. Specifically:\n\u2022 If ti has a single alignment link ai, then a\u0304i = ai.\n\u2022 If ti has multiple alignment links, a\u0304i is chosen to be the middle alignment link. In case of an even number of alignment links, we choose the middle link to the left.\n\u2022 If ti is unaligned, we set a\u0304i = a\u0304j where tj is the closest aligned target word to ti. In case of ties, preference is given to the target word on the right.\nUsing these notations, the conditional probabilities become:\nP (ti|ti\u221211 , s, a) = P (ti|ti\u22121i\u2212n, s a\u0304i+m a\u0304i\u2212m).\nWe extend the model presented in Section 4.3 by defining a vector representation qw \u2208 RD for every source wordw \u2208 Vs. These representations capture the syntactic and semantic roles of source words and, together with the embeddings in the target conditioning context, they are used to predict the next target word. We also define 2m + 1 additional (diagonal)\n57\ncontext matrices Cj . The description from Section 4.3 remains unchanged if we extend the histories hi to be the concatenation of ti\u22121i\u2212n and s a\u0304i+m a\u0304i\u2212m. The new model architecture is illustrated in Figure 4.5.\nAt decoding time, we need access to the alignment links that served as the basis for extracting each translation rule from the parallel corpus. Fortunately, alignment links are included by default in the phrase tables constructed with the Moses toolkit. cdec\u2019s online grammar extractor presented in Chapter 3 also extracts alignment links. The decoder feature applying the source conditioned neural language model uses these links and the location of the source phrase in the source sentence to compute the window of source words used to score each word in the target phrase.\nIn our experiments, we use the bilingual data from Section 4.2 to train the language models. We replace the words occurring only once in each side of the corpus with a special \u3008UNK\u3009 token to learn an embedding for unseen words. We use 500 dimensional word embeddings for both source and target words and we train the models using noise contrastive estimation. We used the class based factorization trick to speed up the normalization step. In our experiments, the decoder uses both a monolingual and a source conditioned neural language model as features.\nThe additional source conditioned neural language model results in 0.2-0.3 gains in BLEU score across all language pairs (Table 4.13). While these gains may appear modest, we note that the size of the parallel corpora used to train these models is 15-18 times smaller than the monolingual corpora used to train the target language models. On the positive side, these models require about as much memory as the monolingual models despite learning embeddings in two languages because the vocabularies are smaller."}, {"heading": "4.10 Summary", "text": "In this chapter, we introduced neural language models as a compact alternative to traditional back-off n-gram models. Neural language models are notoriously difficult to scale.\n58\nWe addressed the key structural inefficiencies in these models and analyzed the effect of the proposed optimizations on end to end translation quality. First, we focused on speeding up the softmax step in the output layer of the neural network. We investigated factoring the output layer using word classes and multiple level trees as well as simply ignoring the normalization altogether. We found out that class factored models perform best in terms of quality, but unnormalized models are considerably faster. Tree factored models did not perform nearly as well as the other candidates and did not provide significant speed ups. We also found out that the clustering algorithm used for defining the word classes can have a significant effect on quality and that Brown clustering (Brown et al., 1992) is a good choice. We showed that noise contrastive estimation is a fast and effective training algorithm for neural language models that works well for large datasets. We showed how noise contrastive estimation can be extended to class factored models. We also showed that diagonal context matrices can significantly speed up the computation of the hidden layer without considerable impact on quality. Putting this information together, we established a strong baseline for neural language models, which we compared with a compact implementation of back-off n-gram models (Heafield, 2011). We observed that class factored models outperform back-off n-gram models by 1 BLEU point when the amount of available memory is limited to 1GB. However, when this restriction is lifted, back-off n-gram models outperform neural language models by a significant margin. Furthermore, increasing the size of the word embeddings is not sufficient to close the gap between these types of models. Finally, we explored direct n-gram features and source sentence conditioning as extensions to neural language models. Both techniques showed reasonable gains in BLEU score.\n59\nChapter 5\nConclusions"}, {"heading": "5.1 Building a Compact Translation System", "text": "Our primary goal in this thesis is to produce a scalable, high quality machine translation system that can operate with the limited resources available on mobile devices. The amount of memory available on such devices is commonly limited to 1GB. This is the main bottleneck that prevents standard machine translation systems from being employed on mobile devices. In Chapter 2, we reviewed the components that comprise a standard translation system and identified phrase tables and traditional back-off n-gram language models as the components responsible for rendering these systems impractical in memory constrained environments. Chapter 3 proposed online suffix array grammar extractors as a compact, scalable alternative to phrase tables and introduced a new algorithm that makes phrase extraction considerably faster for hierarchical phrase based systems. Chapter 4 investigated neural language models as a memory efficient alternative to traditional back-off language models. We addressed the key scalability concerns regarding neural language models and explored a few extensions that lead to translation quality improvements. In this section, we use the suffix array grammar extractors and the neural language models introduced in the earlier chapters in a standard translation system and show that our system produces high quality translations with limited memory.\nWe train hierarchical phrase-based systems with the cdec toolkit (Dyer et al., 2010) for three language pairs: fr\u2192en, en\u2192cs and en\u2192de and limit the total memory used by the systems to 1GB. The grammars are extracted using the new, faster algorithm for discontiguous phrases introduced in Section 3.3. Despite being more compact than phrase tables, the memory footprint of suffix array extractors is linear in the size of the training\n60\ndata, which limits the amount of parallel data we can use for extracting translation rules. In our experiments, we use the news commentary corpora from WMT 20141 for grammar extraction. For language modeling, we replace the default implementation using back-off n-gram models (Heafield, 2011) available in cdec with the class factored neural language models introduced in Chapter 4. The output layer of the neural models is factored using Brown clustering (Liang, 2005). We use diagonal context matrices and project the word embeddings and the hidden layer into a 500-dimensional vector space. The models are trained using noise contrastive estimation on the europarl, news commentary and news crawl corpora.\nWe use the cdec toolkit to train baseline hierarchical phrase-based translation systems using the same parallel and monolingual data. The baseline systems employ traditional back-off n-gram models based on the compact trie implementation available in KenLM (Heafield, 2011). In order not to exceed the total 1GB memory limit, the monolingual data used to train the n-gram language models is sampled randomly with the acceptance ratios shown in Table 5.1. Phrase tables do not scale for hierarchical phrase based systems and, consequently, the cdec toolkit lacks a phrase table implementation. To overcome this challenge, the baseline systems also rely on the phrase extraction algorithm from Chapter 3 for grammar extraction.\nThe results of our experiments are shown in Table 5.1. The first line shows the amount\n1The parallel and monolingual data used in these experiments is available at http://statmt.org/ wmt14/.\n61\nof memory used by the suffix array grammar extractors. As explained earlier, the same grammar extractors are used by both systems. The next section shows the acceptance ratios used to sample the data for training the n-gram language models, their memory footprint as well as the total footprint of the baseline systems, along with the baseline BLEU scores. The final section shows the memory requirements of the compact systems using the class factored neural language models from Chapter 4 and the corresponding BLEU scores. Our compact systems produce BLEU score improvements of 0.74 for fr\u2192en, 1.36 for en\u2192cs and 0.89 for en\u2192de over the baseline. We note that a larger neural language model could be used for fr\u2192en (e.g. by projecting the word embeddings and hidden layer into a higher dimensional space) resulting in a higher BLEU score difference, but we decided to share the model configurations among the 3 language pairs to make the results easier to reproduce."}, {"heading": "5.2 Related Work", "text": "Our work is focused on replacing the memory intensive components of standard machine translation systems in order to make them usable on mobile devices. In recent years, an alternative approach for building translation systems has emerged as the so-called neural translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). These systems employ a single neural network for modeling the entire translation process unlike standard translation systems which are a collection of purpose specific models (Chapter 2). Neural translation models are an instance of the more generic framework for sequence to sequence mapping (Sutskever et al., 2014), where the models first construct a distributed representation over the source sentence which is then used to generate the target sentence. These models can match state-of-the-art standard translation systems, but require larger and deeper neural architectures than the ones explored in this thesis (e.g. Sutskever et al. (2014) employ a 4 layer LSTM with 1000 dimensional embeddings) and occupy considerably more memory. A potential solution for this problem is to use several compression tricks introduced for scaling deep architectures for object recognition on mobile devices and embedded systems (Chen et al., 2015; Han et al., 2016). Chen et al. (2015) reduce the amount of redundancy in a neural network by assigning the same value to multiple parameters via hashing and obtain a 8-16x memory reduction rate at no loss in accuracy. Han et al. (2016) obtain 35-49x reduction rates by\n62\npruning the models, applying dynamic weight sharing and using Huffman encoding. These ideas can also be applied to compress the neural language models from Chapter 4.\nAnother area of related research is focused on producing compact n-gram language models by pruning weights for n-grams that are unlikely to appear at test time. Seymore and Rosenfeld (1996) explore two simple pruning techniques: filtering n-grams below a certain frequency threshold and n-grams whose weight is close to the back-off weight. Stolcke (1998) proposes a similar approach which aims to minimize the difference in perplexity between the original model and the pruned model. Gao and Lee (2000) construct models for estimating the probability of a n-gram not occurring in the test corpus which they use for pruning the language model. These techniques can be combined with a memory efficient implementation (e.g. the trie based language models available in KenLM (Heafield, 2011)) to obtain accurate, compact back-off n-gram language models."}, {"heading": "5.3 Conclusions", "text": "In this thesis, we show how to build a scalable, high quality machine translation system that can operate in memory constrained environments. We present the components in a standard machine translation system (Chapter 2) and show that phrase tables and n-gram language models are responsible for rendering these systems impractical on devices with limited memory. We propose suffix array grammar extractors and neural language models as compact alternatives for these components.\nChapter 3 discusses suffix array grammar extractors into greater detail. We explain why online extractors are better suited for our goal than storing compact representations of phrase tables on disk. We illustrate how suffix array extractors work for phrase based systems (Lopez, 2007). We introduce a novel phrase extraction algorithm for hierarchical phrase based systems which is several times faster than previous work. Our online grammar extractor is open source and has replaced Lopez (2007)\u2019s implementation as the default extractor in the cdec toolkit. Compared to the previous extractor, one important feature of our implementation is supporting memory efficient parallelism.\nChapter 4 is focused on neural language models. We investigate several strategies for scaling neural language models and their effect on end-to-end translation quality. We experiment with 3 frequently used strategies for reducing the computation in the output layer\n63\nof the neural network: class factorizations, tree factorizations and ignoring the normalization step altogether. We observe that class factored models are best in terms of translation quality, while unnormalized models are (unsurprisingly) the fastest. The clustering algorithm used to partition the target vocabulary into word classes has considerable impact on quality. In this respect, our experiments show that Brown clustering (Brown et al., 1992) clearly outperforms frequency binning. Next, we review noise contrastive training, an effective optimization trick which allows us to skip the normalization step at training time with negligible loss in BLEU score, and show how this technique can be extended to class factored models. We also investigate diagonal context matrices for speeding up the computation of the hidden layer of the neural network. We compare our best performing neural language model with a compact n-gram language model on a wide memory spectrum and show that neural language models are superior when the amount of memory available is limited to 1GB, but if the memory is unbounded, increasing the size of the word embeddings and the hidden layer is not sufficient to obtain parity with n-gram language models. We also investigate two enhancements to neural language models: learning weights for ngrams in the conditioning context jointly with our model and extending the conditioning context to cover windows of relevant source words. These extensions show improvements in translation quality, but also incur an additional memory cost which makes them less suitable for memory constrained environments. We release the OxLM neural language modeling toolkit containing highly optimized implementations of all these models, which can then be integrated as features in the cdec and moses decoders.\nIn Chapter 5, we compare our compact translation system relying on suffix array grammar extractors and class factored neural language models with a baseline hierarchical phase-based translation system trained with the cdec toolkit and show that we obtain considerable gains in BLEU score on 3 language pairs. We review related research and conclude with our findings.\n64"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Machine translation is the discipline concerned with developing automated tools for translating from one human language to another. Statistical machine translation (SMT) is the dominant paradigm in this field. In SMT, translations are generated by means of statistical models whose parameters are learned from bilingual data. Scalability is a key concern in SMT, as one would like to make use of as much data as possible to train better translation systems. In recent years, mobile devices with adequate computing power have become widely available. Despite being very successful, mobile applications relying on NLP systems continue to follow a client-server architecture, which is of limited use because access to internet is often limited and expensive. The goal of this dissertation is to show how to construct a scalable machine translation system that can operate with the limited resources available on a mobile device. The main challenge for porting translation systems on mobile devices is memory usage. The amount of memory available on a mobile device is far less than what is typically available on the server side of a client-server application. In this thesis, we investigate alternatives for the two components which prevent standard translation systems from working on mobile devices due to high memory usage. We show that once these standard components are replaced with our proposed alternatives, we obtain a scalable translation system that can work on a device with limited memory. The first two chapters of this thesis are introductory. Chapter 1 discusses the task we undertake in greater detail and highlights our contributions. Chapter 2 provides a brief introduction to statistical machine translation. In Chapter 3, we explore online grammar extractors as a memory efficient alternative to phrase tables. We propose a faster and simpler extraction algorithm for translation rules containing gaps, thereby improving the extraction time for hierarchical phase-based translation systems. In Chapter 4, we conduct a thorough investigation on how neural language models should be integrated in translation systems. We settle on a novel combination of noise contrastive estimation and factoring the output layer using Brown clusters. We obtain a high quality translation system that is fast both when training and decoding and we use it to show that neural language models outperform traditional n-gram models in memory constrained environments. Chapter 5 concludes our work showing that online grammar extractors and neural language models allow us to build scalable, high quality systems that can translate text with the limited resources available on a mobile device.", "creator": "LaTeX with hyperref package"}}}