{"id": "1610.02806", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Modelling Sentence Pairs with Tree-structured Attentive Encoder", "abstract": "We aspects an attentive encoder how quality twigs - structured sccs neural networks and automate superficial neural focus for adapting supreme pairs. Since existing attentive vehicles exert attention weeks the sequential structure, we step brought simply rest encourage need so on mud topology. Specially, given a made of sentences, doing attentive encoder devices the expression although another 25-year, with contrast originally also RNN, help dictionary by structural functionality of second other sentence on by causes escs beneath. We examining first proposal anxious dataset weeks remaining tasks: clustering characterization, mentions reveal and believe - phony understand selection. Experimental expect show that your jbl outperforms these baselines and phenomenal state - of - way - book despite place before tasks.", "histories": [["v1", "Mon, 10 Oct 2016 08:52:36 GMT  (375kb,D)", "http://arxiv.org/abs/1610.02806v1", "10 pages, 3 figures, COLING2016"]], "COMMENTS": "10 pages, 3 figures, COLING2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yao zhou", "cong liu", "yan pan"], "accepted": false, "id": "1610.02806"}, "pdf": {"name": "1610.02806.pdf", "metadata": {"source": "CRF", "title": "Modelling Sentence Pairs with Tree-structured Attentive Encoder", "authors": ["Yao Zhou", "Cong Liu", "Yan Pan"], "emails": ["yoosan.zhou@gmail.com", "panyan5}@mail.sysu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Modelling a sentence pair is to score two pieces of sentences in terms of their semantic relationship. The applications include measuring the semantic relatedness of two sentences (Marelli et al., 2014), recognizing the textual entailment (Bowman et al., 2015) between the premise and hypothesis sentences, paraphrase identification (He et al., 2015), answer selection and query ranking (Yin et al., 2015) etc.\nThe approach of modelling a sentence pair based on neural networks usually consist of two steps. First, a sentence encoder transforms each sentence into a vector representation. Second, a classifier receives two sentence representations as features to make the classification. The sentence encoder can be regarded as a semantic compositional function which maps a sequence of word vectors to a sentence vector. This compositional function takes a range of different forms, including (but not limited to) sequential recurrent neural networks (Seq-RNNs) (Mikolov, 2012), tree-structured recursive neural networks (TreeRNNs) (Socher et al., 2014; Tai et al., 2015) and convolutional neural networks (CNNs) (Kim, 2014).\nWe introduce an approach that combines recursive neural networks and recurrent neural networks with the attention mechanism, which has been widely used in the sequence to sequence learning (seq2seq) framework whose applications ranges from machine translation (Bahdanau et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015) to natural language conversation (Shang et al., 2015) and other NLP tasks such as question answering (Sukhbaatar et al., 2015; Hermann et al., 2015), classification (Rockta\u0308schel et al., 2016; Shimaoka et al., 2016). In the machine translation, the attention mechanism is used to learn the alignments between source words and target words in the decoding phase. More generally, we consider that the motivation of attention mechanism is to allow the model to attend over a set of elements with the intention of attaching different emphases to each element. We argue that the attention mechanism used in a tree-structured model is different from a sequential model. Our idea is inspired by Rockta\u0308schel et al. (2016) and Hermann et al. (2015). In this paper, we utilise the attention mechanism to select semantically more relevant child by the representation of one sentence learned by a Seq-RNNs, when constructing the head representation of the other sentence in the pair on a dependency tree. Since our model adopts the attention in the sentence encoding phase, we refer to it as an attentive \u2217 indicates the corresponding author. Code is available at https://github.com/yoosan/sentpair This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ ar X iv :1\n61 0.\n02 80\n6v 1\n[ cs\n.C L\n] 1\n0 O\nct 2\n01 6\nencoder. In this work, we implement this attentive encoder with two architectures: tree-structured LSTM and tree-structured GRU.\nWe evaluate the proposed encoder on three sentence pair modelling tasks: semantic similarity on the SICK dataset, paraphrase identification on the MSRP dataset and true-false question selection on the AI2-8grade science questions dataset. Experimental results demonstrate that our attentive encoder is able to outperform all non-attentional counterparts and achieves the state-of-the-art performance on the SICK dataset and AI2-8grade dataset."}, {"heading": "2 Models", "text": "Let\u2019s begin with a high-level discussion of our tree-structured attentive encoder. As shown in Figure 1, given a sentence pair (Sa, Sb), our goal is to score this sentence pair. Our tree-structured attentive model has two components. In the first component, a pair of sentences is fed to a Seq-RNNs, which encodes each sentence and results in a pair of sentence representations. In second component, the Attentive TreeRNNs encodes a sentence again, aimed by the representation of the other sentence generated by the first component. Compared with the existing approaches of modelling sentence pairs, our attentive encoder consider not only the sentence itself but also the other sentence in the pair. Finally, the two sentence vectors produced by the second component are fed to the multilayer perceptron network to produce a distribution over possible values. These components will be detailed in the following sections."}, {"heading": "2.1 Seq-RNNs", "text": "We first describe the RNN composer, which is the basic unit of Seq-RNNs. Given an input sequence of arbitrary length, an RNN composer iteratively computes a hidden state ht using the input vector xt and its previous hidden state ht\u22121. In this paper, the input vector xt is a word vector of the t-th word in a sentence. The hidden state ht can be interpreted as a distributed representation of the sequence of tokens observed up to time t. Commonly, the RNN transition function is the following:\nht = tanh(Wxt + Uht\u22121 + b) (1)\nWe refer to the model that recursively apply the RNN composer to a sequence as the Seq-RNNs. Unfortunately, standard Seq-RNNs suffers from the problem that the gradients of the hidden states of earlier part of the sequence vanishes in long sequences (Hochreiter, 1998). Long Short-term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Chung et al., 2014) are two powerful and popular architectures that address this problem by introducing gates and memory. In this paper, we only show the illustrations of LSTM (Figure 2(a)) and GRU (Figure 2(d)). The implementations of standard LSTM and GRU in this paper are same as (Luong et al., 2015) and (Chung et\nal., 2014). When we replace the standard RNN composer with LSTM or GRU, the Seq-RNNs becomes Seq-LSTMs or Seq-GRUs."}, {"heading": "2.2 Standard Tree-RNNs", "text": "Compared with standard RNN composer, which computes its hidden state from the input at the current time step and the hidden state of previous time step, the Tree-RNN composer computes its hidden state from an input and the hidden states of arbitrarily many child units. We now describe the Child-Sum TreeLSTM and Child-Sum Tree-GRU architectures which are formed by applying the Child-Sum algorithm to LSTM and GRU respectively.\nChild-Sum Tree-LSTM. In this paper, the implementation of Child-Sum Tree-LSTM is same as (Tai et al., 2015). We consider that a Child-Sum Tree-LSTM composer contains two parts: the external part and internal part. The external part consists of the inputs and outputs, and the internal part is the controllers and memory of the composer. As shown in Figure 2(b), the inputs of the composer are: a input vector x, multiple hidden states h1, h2, . . . , hn and multiple memory cells c1, c2, . . . , cn, where n is the number of child units. The outputs consist of a memory cell c and a hidden state h which can be interpreted as the representation of a phrase. The internal part aims at controlling the flow of information by an input gate i, an output gate o and multiple forget gates f1, f2, . . . , fn. The gating mechanisms used in the Child-Sum Tree-LSTM are similar to sequential LSTM. Intuitively, the sum of children\u2019s hidden states h\u0303 is the previous hidden state, the forget gate fk controls the degree of memory kept from that of the child k, the input gate i controls how much the internal input u is updated and the output gate controls the exposure of internal memory c. We define the transition equations as follows:\nh\u0303 = \u2211\n1\u2264k\u2264n hk, i = \u03c3(W (i)x+ U (i)h\u0303+ b(i)),\no = \u03c3(W (o)x+ U (o)h\u0303+ b(o)), u = tanh(W (u)x+ U (u)h\u0303+ b(u)), fk = \u03c3(W (f)x+ U (f)hk + b (f)), c = i u+\u22111\u2264k\u2264n fk ck, h = o tanh(c),\n(2)\nChild-Sum Tree-GRU. The way of Child-Sum Tree-GRU extending to the standard GRU is similar to the way of Child-Sum Tree-LSTM extending to the standard LSTM. Since we only introduce the Child-Sum algorithm applied to the LSTM and GRU, in the following, we omit the \u201cChild-Sum\u201d prefix of Child-Sum Tree-LSTM and Child-Sum Tree-GRU for simplicity. Compared with the Tree-LSTM, the Tree-GRU removes the memory cell c and introduces an update gate z and multiple reset gates\nr1, r2, . . . , rn that allow the composer to reset the hidden states of the child units. The candidate hidden state h\u0302 is computed similarly to the standard RNN (Equation 1) and the update gate z is used to control how much the previous hidden state h\u0303 and the candidate h\u0302 should be passed. The transition equations of Tree-GRU are in the following:\nh\u0303 = \u2211\n1\u2264k\u2264n hk, rk = \u03c3(W (r)x+ U (r)hk + b (r)), h\u0302 = tanh(W (h)x+ U (h) \u2211n\nk=1 rk hk), z = \u03c3(W (z)x+ U (z)h\u0303+ b(z)) h = z h\u0303+ (1\u2212 z) h\u0302,\n(3)\nwhere \u03c3 denotes the sigmoid function and denotes element-wise multiplication. We can easily apply the Child-Sum Tree-RNN to the dependency trees that have branching factors of arbitrary number and order-insensitive nodes. We refer to the model adopting the Tree-LSTM and TreeGRU composer to the dependency tree as the Dependency Tree-LSTMs and Dependency Tree-GRUs. For simplicity, we also omit the prefix \u201cDependency\u201d in the following sections."}, {"heading": "2.3 Attentive Tree-RNNs", "text": "We now details how we extend the standard Tree-RNN. The idea that we incorporate the attention into the standard Tree-RNN comes from: (1) there will be semantic relevance between two sentences in the sentence pair modelling tasks; (2) the effect of semantic relevance could be implemented in the process of constructing the sentence representation by Tree-RNN where each child should be assigned a different weight; and (3) the attention mechanism is well suited for learning weights on a contextual collection where a guided vector is attending over.\nSoft Attention Layer In this work, the attention mechanism is implemented by a soft attention layer A. Given a collection of hidden states h1, h2, . . . , hn and an external vector s, the soft attention layer produce a weight \u03b1k for each hidden state as well as a weighted vector g via the Equations 4:\nmk = tanh(W (m)hk + U (m)s), \u03b1k = exp(w\u1d40mk)\u2211n j=1 exp(w \u1d40mj) ,\ng = \u2211 1\u2264k\u2264n \u03b1khk, (4)\nAttentive Tree-LSTM and -GRU As illustrated in Figure 2(c) and Figure 2(f), when the attention layer is embedded to the standard Tree-LSTM and Tree-GRU composer, these composers become the Attentive Tree-LSTM and Attentive Tree-GRU. The attention layer (notated with A) receives the children\u2019s hidden states and an external vector s, producing a weighted representation g (Equation 4). In our implementation, the external vector s is a vector representation of sentence learned by a Seq-RNNs. Specifically, if the tree composer is Attentive Tree-LSTM, then the Seq-RNNs is Seq-LSTMs. Instead of taking the sum of children\u2019s hidden states as the previous hidden state h\u0303 in the standard Tree-RNN, we compute a new hidden state by a transformation h\u0303 = tanh(W (a)g + b(a)). Similar to the standard Tree-RNN, the attentive composers can also be easily applied to dependency trees. We refer to the model which applies the Attentive Tree-RNN composer to the dependency tree as the Attentive (Dependency) Tree-RNNs."}, {"heading": "2.4 MLP", "text": "The multilayer perceptron network (MLP) receives a pair of vectors produced by the sentence encoder to compute a multinomial distribution over possible values. Given two sentence representations hL and hR, we compute their componentwise product hL hR and their absolute difference |hL \u2212 hR|. These features are also used by Tai et al.(2015). We then compress these features into a low dimensional vector hs, which is used to compute the probability distribution p\u0302\u03b8. The equations are the following:\nh\u00d7 = hL hR, h+ = |hL \u2212 hR|, hs = \u03c3(W (\u00d7)h\u00d7 +W (+)h+ + b (h)),\np\u0302\u03b8 = softmax(W (p)hs + b(p)),\n(5)"}, {"heading": "3 Experiments and Results", "text": "Our baselines In order to make a meaningful comparison between the sequential models, treestructured models and attentive models, we present four baselines. They are: (i) Seq-LSTMs, learning two sentence representations by the sequential LSTMs; (ii) Seq-GRUs, like Seq-LSTMs but using GRU composer; (iii) Tree-LSTMs, learning two sentence representations by the Dependency Tree-LSTMs; and (iv) Tree-GRUs, like Tree-LSTMs but using Child-Sum Tree-GRU composer. The two sentence representations are fed to the MLP to produce a probability distribution."}, {"heading": "3.1 Task 1: Semantic Similarity", "text": "First we conduct our semantic similarity experiment on the Sentences Involving Compositional Knowledge(SICK) dataset (Marelli et al., 2014)12. This task is to predict a similarity score of a pair of sentences, based on human generated scores. The SICK dataset consists of 9927 sentence pairs with the split of 4500 training pairs, 500 development pairs and 4927 testing pairs. Each sentence pair is annotated with a similarity score ranging from 1 to 5. A high score indicates that the sentence pair is highly related. All sentences are derived from existing image and video annotation dataset. The evaluation metrics are Pearson\u2019s r, Spearman\u2019s \u03c1 and mean squared error (MSE).\nRecall that the output of MLP (Section 2.4) is a probability distribution p\u0302\u03b8. Our goal in this task is to predict a similarity score of two sentences. Let r\u1d40 = [1, . . . , 5] be an integer vector, the similarity score y\u0302 is computed by y\u0302 = r\u1d40p\u0302\u03b8. We take the same setup as (Tai et al., 2015) that computes a target distribution p as a function of prediction score y given by:\npi =  y \u2212 byc, i = byc+ 1 byc \u2212 y + 1, i = byc 0 otherwise\nThe loss function of semantic similarity is the KL-divergence that measures the continuous distance between the predicted distribution p\u0302\u03b8 and the distribution of the ground truth p:\nJ(\u03b8) = 1\nN N\u2211 k=1 KL(p(k) \u2223\u2223\u2223\u2223\u2223\u2223p\u0302(k)\u03b8 ) + \u03bb2 ||\u03b8||22 (6)\nThe results are summarized in Table 2. We first compare our results against the previous results. ECNU (Zhao et al., 2014), the best result of SemEval 2014 submissions, achieves a 0.8414 r score by a heavily feature-engineered approach. Kiros et al. (2015) presents an unsupervised approach to learn\n1Dependency trees are parsed by the Stanford Parser package, http://nlp.stanford.edu/software/ lex-parser.html\n2Glove vectors are available at http://nlp.stanford.edu/projects/glove/\nthe universal sentence vectors without depending on a specific task. Their Combine\u2013skip+COCO model improve the Pearson\u2019s r to 0.8655, but a weakness is that their sentence vectors are high-dimensional vectors (2400D). Training the skip-thoughts vectors needs a lot of time and space. He et al. (2015) show the effectiveness of convolutional nets with the similarity measurement layer for modelling sentence similarity. Their ConvNet outperforms ECNU with +0.027 Pearson\u2019s r. We can observe that dependency Tree-LSTM, combine-skip+COCO and ConvNet almost achieve the same performance and our Attentive Tree-LSTMs outperforms these three methods around +0.005 points. Comparison to ECNU, our Attentive Tree-LSTMs gains an improvement of +0.032 and achieves the state-of-the-art performance. We find a phenomenon also appeared in (Tai et al., 2015) that tree-structured models can outperform sequential counterparts. Comparison to the non-attentional baselines (such as Tree-LSTMs), the attention mechanism (such as Attentive Tree-LSTMs) gives us a boost of around +0.007. All results highlight that our attentive Tree-RNNs are well suited for the semantic similarity task."}, {"heading": "3.2 Task 2: Paraphrase Identification", "text": "The next task we evaluate is paraphrase identification on the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004). Given two sentences, this task is to predict whether or not they are paraphrases. The dataset is collected from news sources and contains 5801 pairs of sentences, with 4076 for training and the remaining 1725 for testing. We randomly select 10% of training set and use them as our dev set. This task is a binary classification task, therefore we report the accuracy and F1 score.\nSince that the p\u0302\u03b8 indicates the distribution over the possible labels, we take argmax(p\u0302\u03b8) as the predicted label in the testing phase. The loss function for the binary classification is the binary cross-entropy:\nJ(\u03b8) = \u2212 1 N N\u2211 k=1 (y(k)logp\u0302(k)\u03b8 + (1\u2212 y(k))log(1\u2212 p\u0302 (k) \u03b8 )) + \u03bb 2 ||\u03b8||22 (7)\nTable 3 (left) presents our results on the MSRP dataset. The previous approaches are: (1) Baseline, cosine similarity with tf-idf weighting; (2) RAE, recursive autoencoder with dynamic pooling; (3) combine-skip+feats, skip-thought vectors with features; (4) ABCNN-3, attention-based convolutional nets; and (5) TF-KLD, matrix factorization with supervised reweighting. First, all our models are able to outperform the baseline. We only compare our models with the neural networks-based approaches, including RAE and ABCNN-3 for a fair comparison. We find that our models do not prove to be very competitive. After a careful analysis, we conclude that the reasons are (1) our models are pure neural networks-based, we don\u2019t add any features to identify paraphrases while the other methods have used additional features; (2) The MLP is not very suitable in this task. We attempt to replace the MLP with the cosine distance and euclidean distance in our future work. Although our models have not yet matched the SOTA performance, we obtain an improvement of +2.3 accuracy by Attentive Tree-LSTMs when we incorporate the attention into the standard Tree-LSTM."}, {"heading": "3.3 Task 3: True-False Question Selection", "text": "We last consider a challenging task: selecting true or false given a scientific question and its evidence. In this task, we use the AI2-8grade dataset built by (Baudis et al., 2016). This dataset is derived from the AI2 Elementary School Science Questions released by Allen Institute. Each sentence pair consists of a hypothesis sentence processed by substituting the wh-word in the question by answer and its evidence sentence extracted from a collection of CK12 textbooks. The number of sample pairs in the training, development, and test set are 12689, 2483 and 11359 respectively. This dataset contains 626 words not appearing in Glove vectors, most of which are named entities and scientific jargons.\nThe loss function is the same as the paraphrase identification since this task is also a binary classification task. We reports the accuracy on development set and test set shown in Table 3 (right). Since this dataset is a fresh and uncompleted dataset, we only compare our models with Baudis et al. (2016) who have evaluated several models on it. Comparison to (Baudis et al., 2016), all of our models gain a significant improvement. Specially, our best result achieved by the Attentive Tree-LSTMs is higher than the best of (Baudis et al., 2016) by +28 percents. It is observed that tree-structured models are more competitive than the sequential counterparts. As we expected, the attentive models can outperform all non-attentional counterparts."}, {"heading": "4 Quantitative Analysis", "text": "Example Analysis Table 4 presents example predictions that are produced by our Attentive TreeLSTMs. The first group shows that our model is able to predict semantic similarity score nearly perfectly on the SICK dataset. We argue the reason is that the sentences of SICK dataset are image and video descriptions whose sentence structure is relatively simple and there are less uncommon words and named entities in the vocabulary. The second group gives us three examples on the MSRP test set. We\nfind that our model can identify whether two fact statements are paraphrases, but fails to recognize the numbers (in group 2, line 3). We presents the examples on AI2-8grade dataset in the last group. We can observe that our model is efficient to select the false questions, while our model is difficult to select the true answers, unless the evidence of question is very strong.\nEffect of Sentence Length In order to analyse the effect of mean sentence length on the SICK dataset, we draw the Figure 3(a). We observe that the Pearson score become lower as sentence become longer. Compared with the Seq-RNNs, the Tree-RNNs obtain a little improvements. Specially, the Attentive Tree-GRUs proves to be more effective than Tree-GRUs when the mean sentence length reaches to 20.\nEffect of N -grams In the MSR paraphrase corpus, a hypothesis is that two sentence tend to be paraphrases when the value of their n-gram overlap is high. As a result we present the Figure 3(b), x-axis is the normalized n-grams overlap whose value is computed by c \u2217 (unigram+bigram+trigram)mean sent length , where c equals to 50, and y-axis is the accuracy. We can observe that the Attentive Tree-GRUs are more effective than Tree-GRUs when the value of normalized n-grams overalp is less than 40. The results suggest that our attentive models are more general.\nAttention Visualization It is instructive to analyse which child the attentive model is attending over when constructing the head representation. We visualize the heatmaps of attention weights shown in Figure 4. The words at x-axis are modified by the words at y-axis with a weight (greater than zero). For example in Figure 4(a), the 5th word at x-axis is \u201cplaying\u201d whose children are \u201cboy\u201d, \u201coutdoors\u201d, \u201cand\u201d and \u201cis\u201d. We can observe that the word \u201cboy\u201d holds a higher weight among all the modifiers. It means that the branch rooted with \u201cboy\u201d contributes more when constructing the representation of subtree whose\nroot node is \u201cplaying\u201d. This phenomenon is very reasonable because the sentence is describing a image of \u201ca boy is playing something\u201d."}, {"heading": "5 Conclusion", "text": "In this paper, we introduced a way of incorporating attention into the Child-Sum Tree-LSTM and TreeGRU that can be applied to the dependency tree. We evaluate the proposed models on three sentence pair modelling tasks and achieve state-of-the-art performance on two of them. Experiment results show that our attentive models are effective for modelling sentence pairs and can outperform all non-attentional counterparts. In the future, we will evaluate our models on the other sentence pair modelling tasks (such as RTE) and extend them to the seq2seq learning framework."}, {"heading": "Acknowledgements", "text": "This work was funded in part by the National Key Research and Development Program of China (2016YFB0201900), the National Science Foundation of China (grant 61472459, 61370021, U1401256, 61472453), Natural Science Foundation of Guangdong Province under Grant S2013010011905."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Joint learning of sentence embeddings for relevance and entailment", "author": ["Baudis et al.2016] Petr Baudis", "Silvestr Stanko", "Jan Sedivy"], "venue": "arXiv preprint arXiv:1605.04655", "citeRegEx": "Baudis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Baudis et al\\.", "year": 2016}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "\u00c7alar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Technical Report Arxiv report 1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Dolan et al.2004] Bill Dolan", "Chris Quirk", "Chris Brockett"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Multi-perspective sentence similarity modeling with convolutional neural networks", "author": ["He et al.2015] Hua He", "Kevin Gimpel", "Jimmy Lin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Hochreiter.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Ji", "Eisenstein2013] Yangfeng Ji", "Jacob Eisenstein"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ji et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. SemEval-2014", "author": ["Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["Courtney Corley", "Carlo Strapparava"], "venue": "In AAAI,", "citeRegEx": "Mihalcea et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April", "author": ["Tom\u00e1\u0161 Mikolov"], "venue": null, "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "An attentive neural architecture for fine-grained entity type classification", "author": ["Pontus Stenetorp", "Kentaro Inui", "Sebastian Riedel"], "venue": "In Proceedings of the 5th Workshop on Automated Knowledge Base Construction,", "citeRegEx": "Shimaoka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shimaoka et al\\.", "year": 2016}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207\u2013218", "author": ["Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["arthur szlam", "Jason Weston", "Rob Fergus"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs. arXiv preprint arXiv:1512.05193", "author": ["Yin et al.2015] Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment", "author": ["Zhao et al.2014] Jiang Zhao", "Tian Tian Zhu", "Man Lan"], "venue": "Proceedings of the SemEval,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "The applications include measuring the semantic relatedness of two sentences (Marelli et al., 2014), recognizing the textual entailment (Bowman et al.", "startOffset": 77, "endOffset": 99}, {"referenceID": 2, "context": ", 2014), recognizing the textual entailment (Bowman et al., 2015) between the premise and hypothesis sentences, paraphrase identification (He et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 6, "context": ", 2015) between the premise and hypothesis sentences, paraphrase identification (He et al., 2015), answer selection and query ranking (Yin et al.", "startOffset": 80, "endOffset": 97}, {"referenceID": 25, "context": ", 2015), answer selection and query ranking (Yin et al., 2015) etc.", "startOffset": 44, "endOffset": 62}, {"referenceID": 15, "context": "This compositional function takes a range of different forms, including (but not limited to) sequential recurrent neural networks (Seq-RNNs) (Mikolov, 2012), tree-structured recursive neural networks (TreeRNNs) (Socher et al.", "startOffset": 141, "endOffset": 156}, {"referenceID": 22, "context": "This compositional function takes a range of different forms, including (but not limited to) sequential recurrent neural networks (Seq-RNNs) (Mikolov, 2012), tree-structured recursive neural networks (TreeRNNs) (Socher et al., 2014; Tai et al., 2015) and convolutional neural networks (CNNs) (Kim, 2014).", "startOffset": 211, "endOffset": 250}, {"referenceID": 24, "context": "This compositional function takes a range of different forms, including (but not limited to) sequential recurrent neural networks (Seq-RNNs) (Mikolov, 2012), tree-structured recursive neural networks (TreeRNNs) (Socher et al., 2014; Tai et al., 2015) and convolutional neural networks (CNNs) (Kim, 2014).", "startOffset": 211, "endOffset": 250}, {"referenceID": 11, "context": ", 2015) and convolutional neural networks (CNNs) (Kim, 2014).", "startOffset": 49, "endOffset": 60}, {"referenceID": 0, "context": "We introduce an approach that combines recursive neural networks and recurrent neural networks with the attention mechanism, which has been widely used in the sequence to sequence learning (seq2seq) framework whose applications ranges from machine translation (Bahdanau et al., 2015; Luong et al., 2015), text summarization (Rush et al.", "startOffset": 260, "endOffset": 303}, {"referenceID": 12, "context": "We introduce an approach that combines recursive neural networks and recurrent neural networks with the attention mechanism, which has been widely used in the sequence to sequence learning (seq2seq) framework whose applications ranges from machine translation (Bahdanau et al., 2015; Luong et al., 2015), text summarization (Rush et al.", "startOffset": 260, "endOffset": 303}, {"referenceID": 18, "context": ", 2015), text summarization (Rush et al., 2015) to natural language conversation (Shang et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 19, "context": ", 2015) to natural language conversation (Shang et al., 2015) and other NLP tasks such as question answering (Sukhbaatar et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 23, "context": ", 2015) and other NLP tasks such as question answering (Sukhbaatar et al., 2015; Hermann et al., 2015), classification (Rockt\u00e4schel et al.", "startOffset": 55, "endOffset": 102}, {"referenceID": 7, "context": ", 2015) and other NLP tasks such as question answering (Sukhbaatar et al., 2015; Hermann et al., 2015), classification (Rockt\u00e4schel et al.", "startOffset": 55, "endOffset": 102}, {"referenceID": 17, "context": ", 2015), classification (Rockt\u00e4schel et al., 2016; Shimaoka et al., 2016).", "startOffset": 24, "endOffset": 73}, {"referenceID": 20, "context": ", 2015), classification (Rockt\u00e4schel et al., 2016; Shimaoka et al., 2016).", "startOffset": 24, "endOffset": 73}, {"referenceID": 0, "context": "We introduce an approach that combines recursive neural networks and recurrent neural networks with the attention mechanism, which has been widely used in the sequence to sequence learning (seq2seq) framework whose applications ranges from machine translation (Bahdanau et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015) to natural language conversation (Shang et al., 2015) and other NLP tasks such as question answering (Sukhbaatar et al., 2015; Hermann et al., 2015), classification (Rockt\u00e4schel et al., 2016; Shimaoka et al., 2016). In the machine translation, the attention mechanism is used to learn the alignments between source words and target words in the decoding phase. More generally, we consider that the motivation of attention mechanism is to allow the model to attend over a set of elements with the intention of attaching different emphases to each element. We argue that the attention mechanism used in a tree-structured model is different from a sequential model. Our idea is inspired by Rockt\u00e4schel et al. (2016) and Hermann et al.", "startOffset": 261, "endOffset": 1057}, {"referenceID": 0, "context": "We introduce an approach that combines recursive neural networks and recurrent neural networks with the attention mechanism, which has been widely used in the sequence to sequence learning (seq2seq) framework whose applications ranges from machine translation (Bahdanau et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015) to natural language conversation (Shang et al., 2015) and other NLP tasks such as question answering (Sukhbaatar et al., 2015; Hermann et al., 2015), classification (Rockt\u00e4schel et al., 2016; Shimaoka et al., 2016). In the machine translation, the attention mechanism is used to learn the alignments between source words and target words in the decoding phase. More generally, we consider that the motivation of attention mechanism is to allow the model to attend over a set of elements with the intention of attaching different emphases to each element. We argue that the attention mechanism used in a tree-structured model is different from a sequential model. Our idea is inspired by Rockt\u00e4schel et al. (2016) and Hermann et al. (2015). In this paper, we utilise the attention mechanism to select semantically more relevant child by the representation of one sentence learned by a Seq-RNNs, when constructing the head representation of the other sentence in the pair on a dependency tree.", "startOffset": 261, "endOffset": 1083}, {"referenceID": 9, "context": "Unfortunately, standard Seq-RNNs suffers from the problem that the gradients of the hidden states of earlier part of the sequence vanishes in long sequences (Hochreiter, 1998).", "startOffset": 157, "endOffset": 175}, {"referenceID": 3, "context": "Long Short-term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Chung et al., 2014) are two powerful and popular architectures that address this problem by introducing gates and memory.", "startOffset": 96, "endOffset": 116}, {"referenceID": 12, "context": "The implementations of standard LSTM and GRU in this paper are same as (Luong et al., 2015) and (Chung et", "startOffset": 71, "endOffset": 91}, {"referenceID": 24, "context": "In this paper, the implementation of Child-Sum Tree-LSTM is same as (Tai et al., 2015).", "startOffset": 68, "endOffset": 86}, {"referenceID": 24, "context": "These features are also used by Tai et al.(2015). We then compress these features into a low dimensional vector hs, which is used to compute the probability distribution p\u0302\u03b8.", "startOffset": 32, "endOffset": 49}, {"referenceID": 16, "context": "Config Value Config Value Word vectors Glove (Pennington et al., 2014) Dims of word vectors 300 OOV word vectors uniform(-0.", "startOffset": 45, "endOffset": 70}, {"referenceID": 5, "context": "5 Optim method Adagrad (Duchi et al., 2011) Num of epoch 10", "startOffset": 23, "endOffset": 43}, {"referenceID": 13, "context": "1 Task 1: Semantic Similarity First we conduct our semantic similarity experiment on the Sentences Involving Compositional Knowledge(SICK) dataset (Marelli et al., 2014)12.", "startOffset": 147, "endOffset": 169}, {"referenceID": 24, "context": "We take the same setup as (Tai et al., 2015) that computes a target distribution p as a function of prediction score y given by:", "startOffset": 26, "endOffset": 44}, {"referenceID": 26, "context": "ECNU (Zhao et al., 2014), the best result of SemEval 2014 submissions, achieves a 0.", "startOffset": 5, "endOffset": 24}, {"referenceID": 26, "context": "ECNU (Zhao et al., 2014), the best result of SemEval 2014 submissions, achieves a 0.8414 r score by a heavily feature-engineered approach. Kiros et al. (2015) presents an unsupervised approach to learn Dependency trees are parsed by the Stanford Parser package, http://nlp.", "startOffset": 6, "endOffset": 159}, {"referenceID": 26, "context": "Method r \u03c1 MSE ECNU (Zhao et al., 2014) 0.", "startOffset": 20, "endOffset": 39}, {"referenceID": 24, "context": "8414 - Dependency Tree-LSTMs (Tai et al., 2015) 0.", "startOffset": 29, "endOffset": 47}, {"referenceID": 6, "context": "2561 ConvNet (He et al., 2015) 0.", "startOffset": 13, "endOffset": 30}, {"referenceID": 24, "context": "We find a phenomenon also appeared in (Tai et al., 2015) that tree-structured models can outperform sequential counterparts.", "startOffset": 38, "endOffset": 56}, {"referenceID": 4, "context": "2 Task 2: Paraphrase Identification The next task we evaluate is paraphrase identification on the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004).", "startOffset": 142, "endOffset": 162}, {"referenceID": 5, "context": "He et al. (2015) show the effectiveness of convolutional nets with the similarity measurement layer for modelling sentence similarity.", "startOffset": 0, "endOffset": 17}, {"referenceID": 14, "context": "Method Acc(%) F1(%) Baseline (Mihalcea et al., 2006) 65.", "startOffset": 29, "endOffset": 52}, {"referenceID": 21, "context": "3 RAE (Socher et al., 2011) 76.", "startOffset": 6, "endOffset": 27}, {"referenceID": 25, "context": "0 ABCNN-3 (Yin et al., 2015) 78.", "startOffset": 10, "endOffset": 28}, {"referenceID": 1, "context": "7 Method Dev Acc(%) Test Acc(%) RNN (Baudis et al., 2016) 38.", "startOffset": 36, "endOffset": 57}, {"referenceID": 1, "context": "1 CNN (Baudis et al., 2016) 44.", "startOffset": 6, "endOffset": 27}, {"referenceID": 1, "context": "4 RNN-CNN (Baudis et al., 2016) 43.", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "6 attn1511 (Baudis et al., 2016) 38.", "startOffset": 11, "endOffset": 32}, {"referenceID": 1, "context": "RNN (Baudis et al., 2016) 49.", "startOffset": 4, "endOffset": 25}, {"referenceID": 1, "context": "In this task, we use the AI2-8grade dataset built by (Baudis et al., 2016).", "startOffset": 53, "endOffset": 74}, {"referenceID": 1, "context": "Comparison to (Baudis et al., 2016), all of our models gain a significant improvement.", "startOffset": 14, "endOffset": 35}, {"referenceID": 1, "context": "Specially, our best result achieved by the Attentive Tree-LSTMs is higher than the best of (Baudis et al., 2016) by +28 percents.", "startOffset": 91, "endOffset": 112}, {"referenceID": 1, "context": "In this task, we use the AI2-8grade dataset built by (Baudis et al., 2016). This dataset is derived from the AI2 Elementary School Science Questions released by Allen Institute. Each sentence pair consists of a hypothesis sentence processed by substituting the wh-word in the question by answer and its evidence sentence extracted from a collection of CK12 textbooks. The number of sample pairs in the training, development, and test set are 12689, 2483 and 11359 respectively. This dataset contains 626 words not appearing in Glove vectors, most of which are named entities and scientific jargons. The loss function is the same as the paraphrase identification since this task is also a binary classification task. We reports the accuracy on development set and test set shown in Table 3 (right). Since this dataset is a fresh and uncompleted dataset, we only compare our models with Baudis et al. (2016) who have evaluated several models on it.", "startOffset": 54, "endOffset": 906}], "year": 2016, "abstractText": "We describe an attentive encoder that combines tree-structured recursive neural networks and sequential recurrent neural networks for modelling sentence pairs. Since existing attentive models exert attention on the sequential structure, we propose a way to incorporate attention into the tree topology. Specially, given a pair of sentences, our attentive encoder uses the representation of one sentence, which generated via an RNN, to guide the structural encoding of the other sentence on the dependency parse tree. We evaluate the proposed attentive encoder on three tasks: semantic similarity, paraphrase identification and true-false question selection. Experimental results show that our encoder outperforms all baselines and achieves state-of-the-art results on two tasks.", "creator": "LaTeX with hyperref package"}}}