{"id": "1610.03009", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Investigation of Synthetic Speech Detection Using Frame- and Segment-Specific Importance Weighting", "abstract": "Speaker verifiable operational are vulnerable to spoofing attacks though compelling a major though having only fact - person deployment. To scheduled, most another soon compromise manufactured press detectors (SSDs) should excludes that understanding main be array of invitation equally. However, ones attempted measurement how can strengths and weaknesses some the explosive they would take now be as or turning term melodies museums. Moreover, same cannot depending for only particular magnitudes take rock. Here, nobody proposal three algorithms has likely likelihood - ratio scores of potential tables, plurals, and piece - disciplines depending opened as importance for the SSD. Significant improvement over and birdie system has others registration for considered attempt techniques way many. in activities the SSDs. However, improvement however most attack additionally previously clearly giving. Thus, made description of disorganization instead also brought when the unknown used were different have could. one soviets better with the proposed SSD 3.7 while the jump SSD.", "histories": [["v1", "Mon, 10 Oct 2016 18:03:29 GMT  (156kb,D)", "http://arxiv.org/abs/1610.03009v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["ali khodabakhsh", "cenk demiroglu"], "accepted": false, "id": "1610.03009"}, "pdf": {"name": "1610.03009.pdf", "metadata": {"source": "CRF", "title": "Investigation of Synthetic Speech Detection Using Frame- and Segment-Specific Importance Weighting", "authors": ["Ali Khodabakhsh", "Cenk Demiro\u011flu"], "emails": ["ali.khodabakhsh@ozu.edu.tr,", "cenk.demiroglu@ozyegin.edu.tr"], "sections": [{"heading": null, "text": "Index Terms\u2014speaker verification, spoofing, synthetic speech detector\nI. INTRODUCTION\nOne of the biggest obstacles in deployment of speaker verification technology in real-life scenarios, especially in high-security applications such as telephone banking, is the difficulty in countering spoofing attacks. Even though verification of speaker identity through human voice has been shown to be successful [1], state-of-the art verification systems have been shown to be vulnerable to spoofing attacks using speech synthesis and voice conversion [2].\nMost of the literature on the spoofing problem has focused on algorithms that were designed to counter specific types of attacks. For example, one method of synthesizing speech is the HMM-based approach where smooth speech parameters are generated and speech is synthesized with a vocoder. Even though HMM-based synthesis can successfully spoof the modern verification systems, it is also easy to detect by exploiting the unnaturally smooth trajectories of the parameters [3][4][5]. Moreover, because the vocoder typically has a minimum-phase filter, phase was also used for detecting HMM-based synthesis since natural speech spectrum is not minimum phase [6].\nEven though unit selection synthesis is relatively harder to detect, it is also challenging to deploy in the context of spoofing since unlike the HMM-based approach that can adapt to the target with seconds of data, unit selection requires hours of training data. Such large amounts of data is hard to collect for each target speaker in most practical cases. Existing synthetic speech detectors (SSDs) typically use jumps\nin fundamental frequency at the concatenation points for detection [7][8].\nVoice conversion algorithms can also be used for spoofing [2]. Because they typically use minimum-phase vocoders, phase was used in [9][10] for detecting voice-converted speech. Moreover, some voice conversion systems exhibit low parameter variability across an utterance compared to natural speech and that was also exploited for detecting voice conversion [11].\nThere are also SSDs that are independent of the attack type. One promising approach is to use a local binary pattern (LBP) analysis for feature extraction [12]. In that approach, a one-class classifier is trained with features derived only from natural speech. The classifier learns the spectro-temporal model of speech and can detect synthetic signals that do not fit well to that model. In [13], i-vectors are used both for speaker verification and synthetic speech detection. The detector and speaker verification scores are fused to make a final decision.\nHere, we investigate several detectors without attackspecific prior assumptions. Our approach is based on the hypothesis that long- and/or short-duration artifacts will be observed in the synthetic speech without any constraints on the type of artifacts. Artifacts that typically occur in stop sounds during synthesis because of their rapidly changing dynamics and sudden glitches that occur frequently with the unit selection systems are examples of short-duration artifacts. Overlysmooth parameters generated with HMM-based synthesis is an example to long-duration artifacts. The SSD algorithm should be sensitive to both types of artifacts to be effective.\nIn this paper, we have investigated SSDs that can capture both short and long-duration artifacts. The first SSD was developed using an unsupervised approach where a Gaussian mixture model (GMM) is trained for natural speech and a GMM is trained for synthetic speech. After aligning each speech frame with a Gaussian, each Gaussian component is treated as an independent detector and detector scores are fused with logistic regression.\nOur second method is based on designing detectors that are focused on detecting artifacts in specific phonemes. This approach can be successful at detecting phoneme-specific artifacts in synthetic speech. However, some of the phonemes are not observed frequently enough in most utterances. To reduce the data sparsity issue, broad-level sound class detectors are used in a third approach. Similar to the Gaussian\nar X\niv :1\n61 0.\n03 00\n9v 1\n[ cs\n.S D\n] 1\n0 O\nct 2\n01 6\napproach, score fusion is done for the phoneme- and classbased methods.\nAll three methods performed substantially better than the baseline detector that treats all Gaussians and phonemes equally for the known attack types. However, the proposed systems did not substantially improve the baseline system for unknown attack types. Fusing the three proposed detectors further improved the SSD performance both in known and unknown conditions."}, {"heading": "II. SYNTHETIC SPEECH DETECTORS", "text": "An overview of the proposed system is shown in Fig. 1. Mel-frequency cepstral coefficients (MFCC) are first extracted from the speech utterance. Then, the feature vectors are grouped together into J groups. In one approach, vectors that are aligned with the same Gaussian component of a GMM are grouped together. In another approach, feature vectors that belong to the same phoneme or sound class constitute a group. Details of grouping are described in the next section.\nAfter grouping, log-likelihood ratio (LLR) detection is done for each group of feature vectors. To compute LLR, a GMM is trained for natural speech and a GMM is trained for synthetic speech. Same GMMs are used for all J groups. Once the score of each group is computed, score fusion is done using a logistic regression function to compute the final score S(u). A hard threshold is used to compute the final decision.\nIn the baseline detector, which does not use any grouping, given an utterance u, assuming independent speech frames\nLLR(u) = 1\nN N\u2211 i=1 log(xi|\u039bnat)\u2212 log(xi|\u039bsyn), (1)\nwhere N is the total number of frames, xi is the feature vector for the ith frame, \u039bnat is the canonical model of GMM for the natural speech, and \u039bsyn is the canonical model of GMM for the synthetic speech. The final decision is done using a hard threshold for LLR(u).\nIn the proposed approach, the decision is based on the utterance score\nS(u) = \u03a6(S1, S2, ..., SJ) (2)\nwhere \u03a6 is a nonlinear function and score Sj for each group j is\n1\nNj Nj\u2211 i=1 log(x (j) i |\u039bnat)\u2212 log(x (j) i |\u039bsyn). (3)\nThe rationale of this approach is to develop detectors that are focused on different segments of speech and weigh each segment depending on its information content. For example, nasals are typically not modeled well by vocoders because of the spectral dip in nasals that are not modeled with an allpole model. A detector that is focused only on nasals can detect those artifacts. Similarly, synthetic speech may contain some short-duration glitches that are not observed in natural speech. Even though those artifacts may be detectable by some of the Gaussian components in synthetic GMMs. when the frame likelihoods are averaged as in Eq. 1, those short-duration events may not be detected because of the low weight they get and noise introduced in other frames. Focusing on those highly informative Gaussians regardless of their durations and assigning them high weight can improve the detection performance in those cases."}, {"heading": "A. Duration-based Weighting", "text": "Distribution of the frame-level LLR values approximately follow a Gaussian distribution in most utterances. By averaging the LLR scores, as done in Eq. 3, assuming Gaussianity, a maximum-likelihood (ML) estimate of the mean is found. Considering the fact that the ML estimate of the mean of a Gaussian has an estimation variance that is inversely proportional with the number of observations, reliability of the detector j increases when Nj increases. To take the estimation variance, hence the uncertainty of the detector scores, into account, we propose the duration-weighted score\nS \u2032\nj = ln(Nj + 1)Sj (4)\nwhere ln(.) is the natural logarithm."}, {"heading": "III. FEATURE GROUPING METHODS", "text": "Three feature grouping strategies are investigated. In the phoneme-based approach, each phoneme constitutes a group. Thus, feature vectors that occur within a particular phoneme type in the utterance are grouped together.\nOne of the problems with the phoneme-based approach is that some of the utterances provided in the challenge were short ( 2-3seconds) which means that many of the phonemes were not observed in those cases. Because broad acousticphonetic sound classes share similar acoustic properties, we hypothesized that if a system performs poorly in synthesizing a phoneme, it will most likely perform poorly for the other phonemes that are acoustically similar. Thus, to make more data available for each group, a class-based approach is used for grouping in the second approach. In the class-based approach, five sound classes are used: vowels, nasals, glides, stops, and rest. The rest class contains all phonemes that do not belong to the other four classes.\nThe phoneme- and class-based methods are good at detecting artifacts that occur in relatively long segments. However, they are not designed for detecting sudden glitches that can easily occur with unit selection systems or some of the voice conversion systems. Location of those glitches are random for\nthe most part and they may not be detected with detectors that are focused on long-duration segments.\nTo address the issue of short-duration artifact detection, we propose Gaussian-based grouping where each frame in the utterance is first aligned with the GMM of natural speech. Then, frames that are aligned with the same Gaussian are grouped together. This approach allows detection of framelevel artifacts and assign them high weight even though they may occur infrequently in the utterance."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. Experiment Setup", "text": "The synthetic speech detectors were trained with 19 MFCCs together with the delta and delta-delta features. In short-time analysis, frame length was 25msec and frame rate was 10msec. Bigaussian voice activity detection (VAD) was used where energy of the speech and noise frames are modeled with single Gaussians and likelihood ratio detector is used to detect speech frames.\nThe baseline synthetic speech detector had a 512-component GMM to model natural speech. Similarly, synthetic speech was modeled with 512-component GMM. For natural speech, GMM training was initialized using k-means clustering. The GMM for synthetic speech was adapted from the GMM of the natural speech using a maximum a posteriori (MAP) approach. Experiments with synthetic speech GMM that was trained independent of the natural speech GMM were also performed for comparison.\nThe phoneme-based approach requires a phoneme recognizer since the transcriptions of the challenge data were not available. The Hungarian phoneme recognizer [14] was trained with WSJ-CAM database and used here for phoneme recognition. A total of 37 phonemes were used. Outputs of the phoneme recognizer were mapped to sound classes and used in sound-class based detector also.\nThe spoofing challenge database1 was used for training, development and evaluation of all systems. The BOSARIS toolkit [15] was used to train the logistic regression algorithm that was used for fusing the scores of detectors."}, {"heading": "B. Results and Discussion", "text": "Experimental results for the development and evaluation data are shown in Table I. The baseline LLR detector is trained with two different methods. In one approach (LLRnoAdapt), two independent GMMs are trained for the natural and synthetic speech. In the second approach (LLR-Adapt), a GMM is trained for natural speech and then adapted to the synthetic speech using MAP adaptation.\nThe LLR-Adapt system performed better for known conditions while LLR-noAdapt performed better for unknown conditions. Thus, even though LLR-Adapt performed better than LLR-noAdapt on average, it could not generalize as good as the LLR-noAdapt. This result indicates that, during GMM training, some of the novel clusters in the synthetic data that\n1http://www.spoofingchallenge.org/asvSpoof.pdf\nwere useful for ambiguity detection, could not be modeled well with adaptation of GMM for natural speech.\nGaussian-based system performed better than class- and phoneme-based methods both for known and unknown conditions. In particular, Gaussian-based approach performed better for the S1, S2, and S5 methods, all of which are voice conversion algorithms. Unlike the phoneme- and class-based systems, Gaussian-based detector can learn to detect shortduration artifacts. Thus, the presence of short-duration acoustic distortions seems to be more informative for detecting voice conversion attacks.\nClass-based system performed better for S3 and phonemebased system performed better for S4 attack methods. Both S3 and S4 are generated with HMM-based TTS. Unlike the voice conversion systems, HMM-based TTS systems generate smooth trajectories. Thus, sudden acoustic distortions are rarely generated with those systems. In this case, overlysmooth longer segments seem to be more informative for detection. Small distortions in a long segment can be detected well with class- and phoneme-specific detectors that are focused on particular segments. However, Gaussian-based approach is not expected to be as successful with this type of attack because speech frames are generated with a maximumlikelihood approach in HMM-based synthesis. Thus, the parameter generation algorithm is designed to generate high likelihoods for each frame and individual Gaussians are not expected to detect the artifacts in features.\nDuration-based weighting consistently improved class- and phoneme-based performance. However, for the Gaussianbased approach, performance improved slightly for the unknown systems and degraded slightly for the known systems. We believe there are at least two major factors behind this result. Firstly, because an important strength of the Gaussianapproach is its ability to detect short-time artifacts, weighting with duration can hurt its performance. Secondly, duration of observed Gaussians can change significantly depending on the spoofing system used which can increase the variability of features and make the detection task harder. Because ASR systems take phoneme durations into account during recognition, that effect is not as important in the phonemeand class-based methods.\nThe core hypothesis in the proposed system was that different Gaussians, phonemes, sound-classes contribute different amounts of information for synthetic speech detection. To test that hypothesis, experiments were performed with each Gaussian, phoneme, and sound-class separately. For the Gaussian case, results are shown in Fig. 2, for the phoneme case, results are shown in Fig. 3. In both cases, large variation in detection performance can be observed which verifies our hypothesis.\nDetector results for the class-based system is shown in Table II. Performance of each class is significantly different from each other and they change substantially depending on the attack method. Also note that, even though vowel class is observed more than other classes, their performance is better than other systems only for HMM-based TTS attacks. For the voice-conversion attacks, short-duration stop sounds become\nLog-Number of Occurrence\n6 7 8 9 10 11 12\nE E\nR [\n% ]\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\nFig. 2. Detection performance of each Gaussian component versus its logarithm of number of occurrence in the development utterances is shown.\nLog-Number of Occurrence\n7 8 9 10 11 12 13 14 15\nE E\nR [\n% ]\n5\n10\n15\n20\n25\n30\naa\nae\nah\naw\nay\neh er\ney\nih\niy\now\nuh\nuw\nm n\nng\nhh\nl\nr\nw\ny\nb\nd g\nk p tch\ndh\ndx\nf\njh ssh\nthv z\nVowel Nasal Glide Stop Rest\nFig. 3. Detection performance of each phoneme versus its logarithm of number of occurrence in the development utterances is shown. Phonemes that are in the same sound-class are shown with the same color and shape.\nmore informative even though they occur far less frequently than the vowels.\nFig. 2 shows the correlation of number of occurrences vs EER computed with each of the 512 Gaussians. Even though EER and durations have a negative correlation, the pattern is weak and does not impact the overall detector performance significantly. This result is inline with the finding that durationbased weighting does not improve the performance of the Gaussian-based system.\nThe effect of duration is more significant with phonemebased detector compared to the Gaussian-based detector. Du-\nration versus EER is shown in Fig. 3 where a stronger negative correlation is observed compared to the Gaussian case especially for the vocalic sounds. The correlation disappears for some of the highly informative stop and fricative sounds.\nThe proposed detectors performed substantially better than the baseline detectors for known attack types. However, the difference is not substantial for the unknown attack types. To further boost the performance, the detectors were fused with a second stage of logistic regression algorithm. The fusion improved performance both for known and unknown attack types which indicate that the detectors generate complementary information."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "We have investigated a multi-detector approach for synthetic speech detection where each detector is focused on a particular acoustic segment. The Gaussian-based detector performed better in voice conversion attacks. Phoneme- and class-based detectors performed better for HMM-based synthesis attacks. Duration-based feature normalization improved the phonemeand class-based systems but not the Gaussian-based system. The proposed systems performed substantially better than the baseline system in known attack types. In unknown attacks, the improvement was not substantial. Fusing the scores of proposed detectors further improved the performance in both known and unknown conditions.\nOur goal in this paper was to take a commonly used likelihood ratio based SSD and use it in a segment-specific manner. The hypothesis here was that different segments contribute different amounts of information and their scores should be weighted accordingly. Results confirmed our hypothesis. Because we did not assume any prior information, we have used the commonly used MFCC features. In the future work, we will investigate a richer set of features and other classifiers such as SVM to further improve the detection performance."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by TUBITAK 1001 grant No 112E160."}], "references": [{"title": "Spoofing and countermeasures for speaker verification: a survey", "author": ["Z. Wu", "N. Evans", "T. Kinnunen", "J. Yamagishi", "F. Alegre", "H. Li"], "venue": "Speech Communication, vol. 66, pp. 130\u2013153, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A robust speaker verification system against imposture using an hmm-based speech synthesis system.", "author": ["T. Satoh", "T. Masuko", "T. Kobayashi", "K. Tokuda"], "venue": "INTERSPEECH,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "A speech parameter generation algorithm considering global variance for hmm-based speech synthesis", "author": ["T. Tomoki", "K. Tokuda"], "venue": "IEICE TRANSACTIONS on Information and Systems, vol. 90, no. 5, pp. 816\u2013 824, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Speaker verification against synthetic speech", "author": ["L.-W. Chen", "W. Guo", "L.-R. Dai"], "venue": "Chinese Spoken Language Processing (ISCSLP), 2010 7th International Symposium on. IEEE, 2010, pp. 309\u2013312.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation of speaker verification security and detection of hmmbased synthetic speech", "author": ["P.L. De Leon", "M. Pucher", "J. Yamagishi", "I. Hernaez", "I. Saratxaga"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 8, pp. 2280\u20132290, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Discrimination method of synthetic speech using pitch frequency against synthetic speech falsification", "author": ["A. Ogihara", "U. Hitoshi", "A. Shiozaki"], "venue": "IEICE transactions on fundamentals of electronics, communications and computer sciences, vol. 88, no. 1, pp. 280\u2013286, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Synthetic speech discrimination using pitch pattern statistics derived from image analysis.", "author": ["P.L. De Leon", "B. Stewart", "J. Yamagishi"], "venue": "INTERSPEECH,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition.", "author": ["Z. Wu", "C.E. Siong", "H. Li"], "venue": "INTERSPEECH,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case", "author": ["Z. Wu", "T. Kinnunen", "E.S. Chng", "H. Li", "E. Ambikairajah"], "venue": "Signal & Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Pacific. IEEE, 2012, pp. 1\u20135.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Spoofing countermeasures to protect automatic speaker verification from voice conversion", "author": ["F. Alegre", "A. Amehraye", "N. Evans"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3068\u20133072.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A one-class classification approach to generalised speaker verification spoofing countermeasures using local binary patterns", "author": ["\u2014\u2014"], "venue": "Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE Sixth International Conference on. IEEE, 2013, pp. 1\u20138.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint speaker verification and antispoofing in the i -vector space", "author": ["A. Sizov", "E. Khoury", "T. Kinnunen", "Z. Wu", "S. Marcel"], "venue": "Information Forensics and Security, IEEE Transactions on, vol. 10, no. 4, pp. 821\u2013 832, April 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Phoneme recognizer based on long temporal context", "author": ["P. Schwarz", "P. Matejka", "L. Burget", "O. Glembek"], "venue": "Speech Processing Group, Faculty of Information Technology, Brno University of Technology.[Online]. Available: http://speech. fit. vutbr. cz/en/software, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "The bosaris toolkit: Theory, algorithms and code for surviving the new dcf", "author": ["N. Br\u00fcmmer", "E. de Villiers"], "venue": "arXiv preprint arXiv:1304.2865, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Even though verification of speaker identity through human voice has been shown to be successful [1], state-of-the art verification systems have been shown to be vulnerable to spoofing attacks using speech synthesis and voice conversion [2].", "startOffset": 237, "endOffset": 240}, {"referenceID": 1, "context": "Even though HMM-based synthesis can successfully spoof the modern verification systems, it is also easy to detect by exploiting the unnaturally smooth trajectories of the parameters [3][4][5].", "startOffset": 182, "endOffset": 185}, {"referenceID": 2, "context": "Even though HMM-based synthesis can successfully spoof the modern verification systems, it is also easy to detect by exploiting the unnaturally smooth trajectories of the parameters [3][4][5].", "startOffset": 185, "endOffset": 188}, {"referenceID": 3, "context": "Even though HMM-based synthesis can successfully spoof the modern verification systems, it is also easy to detect by exploiting the unnaturally smooth trajectories of the parameters [3][4][5].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "Moreover, because the vocoder typically has a minimum-phase filter, phase was also used for detecting HMM-based synthesis since natural speech spectrum is not minimum phase [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "Existing synthetic speech detectors (SSDs) typically use jumps in fundamental frequency at the concatenation points for detection [7][8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 6, "context": "Existing synthetic speech detectors (SSDs) typically use jumps in fundamental frequency at the concatenation points for detection [7][8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 0, "context": "Voice conversion algorithms can also be used for spoofing [2].", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "Because they typically use minimum-phase vocoders, phase was used in [9][10] for detecting voice-converted speech.", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "Because they typically use minimum-phase vocoders, phase was used in [9][10] for detecting voice-converted speech.", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "Moreover, some voice conversion systems exhibit low parameter variability across an utterance compared to natural speech and that was also exploited for detecting voice conversion [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 10, "context": "One promising approach is to use a local binary pattern (LBP) analysis for feature extraction [12].", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "In [13], i-vectors are used both for speaker verification and synthetic speech detection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "The Hungarian phoneme recognizer [14] was trained with WSJ-CAM database and used here for phoneme recognition.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "The BOSARIS toolkit [15] was used to train the logistic regression algorithm that was used for fusing the scores of detectors.", "startOffset": 20, "endOffset": 24}], "year": 2016, "abstractText": "Speaker verification systems are vulnerable to spoofing attacks which presents a major problem in their real-life deployment. To date, most of the proposed synthetic speech detectors (SSDs) have weighted the importance of different segments of speech equally. However, different attack methods have different strengths and weaknesses and the traces that they leave may be short or long term acoustic artifacts. Moreover, those may occur for only particular phonemes or sounds. Here, we propose three algorithms that weigh likelihood-ratio scores of individual frames, phonemes, and sound-classes depending on their importance for the SSD. Significant improvement over the baseline system has been obtained for known attack methods that were used in training the SSDs. However, improvement with unknown attack types was not substantial. Thus, the type of distortions that were caused by the unknown systems were different and could not be captured better with the proposed SSD compared to the baseline SSD.", "creator": "LaTeX with hyperref package"}}}