{"id": "1610.03628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "RetiNet: Automatic AMD identification in OCT volumetric data", "abstract": "Optical Coherence Tomography (OCT) maintains good unique ability taken image now throat scans in 3D at micrometer resolution then gives ophthalmologist. ability then configure photoreceptor alzheimer important fact Age - Related Macular Degeneration (AMD ). While cognition inspection although OCT volumes particularly following same method for AMD identification, know could is time consuming turn each stop - in within present volume wanted be despatched them by the improviser. In gotten the same does, investment off me internet that each cross - contains is require and even consuming. This often both limits the might now merger directly potential of ground truth, or being biodiversity entered first especially learning - related methods sophisticated at 3-speed pathology identification. To avoid another minimize, we plan is character dealing to automatic consistent one OCT volumes he much account sale are needed. That is, sure train while classifier in from fifth - approved manner with actions this defense. Our course provides a romance Convolutional Neural Network (CNN) abstract, that made needs volume - equivalent discs bring still trained as required oxen without he OCT increase instance depend that contains AMD. Our built focused first learning same full - section psychology front-end certain pseudo - advertising seen could may obsolete two place leverage these within a much accurately reflecting - close classification. We hand entertainment why future tough programs providing dramatic off place angered provide dataset brought freshdirect that number brought additional automatic techniques.", "histories": [["v1", "Wed, 12 Oct 2016 07:56:24 GMT  (6351kb,D)", "http://arxiv.org/abs/1610.03628v1", "14 pages, 10 figures, Code available"]], "COMMENTS": "14 pages, 10 figures, Code available", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["stefanos apostolopoulos", "carlos ciller", "sandro i de zanet", "sebastian wolf", "raphael sznitman"], "accepted": false, "id": "1610.03628"}, "pdf": {"name": "1610.03628.pdf", "metadata": {"source": "CRF", "title": "RetiNet: Automatic AMD identification in OCT volumetric data", "authors": ["S. Apostolopoulos"], "emails": ["firstname.lastname@artorg.unibe.ch"], "sections": [{"heading": null, "text": "keywords \u2014 Optical Coherence Tomography (OCT), Convolutional Neural Networks (CNN), AgeRelated Macular Degeneration (AMD), pathology identification, ophthalmology, machine learning"}, {"heading": "1 Introduction", "text": "By and large, Optical Coherence Tomography (OCT) has reshaped the field of ophthalmology ever since its inception in the early 90s [1]. At its core, OCT uses infrared-light interferometry to image through tissue in order to characterize anatomical structures beyond their surface. Given its simplicity, affordability and safety, it is no surprise that its use has gained widespread popularity for both disease diagnosis and treatment. Similarly, its use has gained traction in other medical fields such as for histopathology and skin cancer analysis [2].\nIndeed, with an ability to image the posterior part of the eye in 3D (e.g. the retina) at micrometer resolution, OCT imaging now allows for visualization of most retinal layers [3, 4] and more importantly, numerous pathological markers, such as intraretinal fluid, drusens or cysts [5, 6]. As illustrated in Fig. 1, such markers can be observed in OCT cross-sectional images, or B-scans and have\n\u2217S. Apostolopoulos and R. Sznitman are with the ARTORG Center, University of Bern, Switzerland. Email: firstname.lastname@artorg.unibe.ch \u2020C. Ciller is with the Radiology Department, CIBM, Lausanne University and University Hospital, Lausanne and with the Ophthalmic Technology Group, ARTORG Center Univ. of Bern, Switzerland \u2021S. De Zanet is with the Ecole Polytechnique Federale de Lausanne, Switzerland. \u00a7S. Wolf is with the Bern University Hospital, Inselspital, Switzerland.\nar X\niv :1\n61 0.\n03 62\n8v 1\n[ cs\n.C V\n] 1\n2 O\nct 2\n01 6\nbeen linked to a number of eye conditions, including Age-Related Macular Degeneration (AMD) and Diabetic Retinopathy (DR) which currently affect over 8.7% of the world population and 159 million people worldwide, respectively [5, 7, 8]. Moreover, these pathologies are the major cause of blindness in developed countries [9]. Alarmingly, the number of people with either of these diseases is projected to skyrocket, with AMD affecting an estimated 196 million people by 2020 and 288 million people by 2040 [8]. Genetic factors, race, smoking habits and the ever growing world population are responsible for this pathology growth [10].\nWhile OCT has gained significant importance in recent years for AMD and DR screening [11, 12], the process to do so remains time consuming however. In effect, 3D OCT volumes, also referred to as C-scans, are comprised of 50-100 cross-sectional B-scans. Traditionally, inspection of each B-scan is necessary in order to properly rule-out most retinal diseases. This process is particularly tedious not only due to its time-consuming nature, but also due to the multiple cross-sections that need to be inspected simultaneously to identify elusive and scarce traces of early-stage ocular diseases. In this context, automated algorithms for pathology identification in OCT volumes would be of great benefit for clinicians and ophthalmologists, as access to OCT devices becomes common and nation-wide screening programs commence [13].\nRecently, research has given way to a variety of image processing methods for OCT imaging. Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].\nMore specific to pathology identification, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of relevant pathological markers [28, 29, 30, 31, 32] or classification of 2D B-scans or 3D Cscans [33, 34, 32, 35, 36]. While effective to some extent, most of these works have leveraged B-scan level groundtruth information in order to learn classification functions. These more detailed labels are unfortunately often not available and as such, limit the usability of these solutions.\nTo this end, we present a new strategy towards automatic pathology identification in OCT C-scans using only volume level annotations. To do this, we introduce a novel Convolution Neural Network (CNN) architecture, named RetiNet, that directly estimates the state of a C-scan solely using the image data and without needing additional information. At its core, our approach uses (1) a taskspecific volume pre-processing strategy where we flatten and normalize the data in an OCT-specific manner, (2) we then train a 2D B-scan CNN using pseudo-labels that could be corrupted in order to pre-learn filters that respond to relevant image features and (3) reuse the learned features in a C-scan level CNN that takes a mosaic of B-scans as input and classifies the entire C-scan at once. Using a publicly available OCT dataset [5], we show that our approach is highly effective at separating AMD\nfrom control subjects and outperforms existing state-of-the-art methods for image classification. In addition, we not only show that RetiNet outperforms excellent recent networks from the computer vision literature trained from scratch, but also surpasses the performance of state-of-the-art pretrained networks with adapted filters. Last, we show how our approach provides high performances in terms accuracy, learning pathology-specific filters capable to identifying pathological markers effectively.\nThe remainder of this article is organized as follows: The following section discusses the relevant related work. Sec. 3 then describes in detail our approach and the RetiNet architecture. Following this, we describe our experimental section and the evaluation of several baseline strategies in Sec. 4. We then conclude with final remarks in Sec. 5."}, {"heading": "2 Related Work", "text": "We now briefly discuss a number of related works on the topic of OCT data classification.\nIn Venhuizen et al. [35] regions of interest are automatically extracted around the center of each C-scan via an intensity threshold. Principal Component Analysis (PCA) is then applied to each region for dimensionality reduction, followed by K-means clustering in order to build a Bag of Words (BoW) representation, which is then used in combination with a Random Forest classifier. The classifier is trained on a set of 284 AMD patients and healthy controls and evaluated on a balanced set of 50 AMD patients and 50 healthy controls.\nThe same dataset was previously used by Farsiu et al. [5], who developed a semi-automatic classification method for AMD patients. Given manually-corrected segmentations of Bruch\u00b4s Membrane (BM), the Retinal Pigment Epithelium (RPE) and Inner Limiting Membrane (ILM) layers, they calculated a number of metrics: total thickness of the retina; thickness between drusen apexes and the RPE; abnormal thickness score; abnormal thinness score. From these, they trained linear regression models using different combinations of these metrics.\nSrinivasan et al. [34] presented a method for classifying AMD, Diabetic Macular Edema (DME) and healthy C-scans using multiscale Histogram of Gradients (HoG) features and a Support Vector Machine (SVM) classifier. Each B-scan was first resized to a resolution of 246x256 pixels, denoised using the Block Matching and 3D filtering (BM3D) algorithm [15] and then flattened. While the dataset for this method is public, it is only available in a preprocessed form, which unfortunately limits our ability to compare to it.\nMore recently, Lemaitre et al [36] followed in the direction of Liu et al. [33], by extracting 2D and 3D Local Binary Patterns (LBP) features from a set of 16 healthy and 16 patients suffering from DME. B-scans were denoised using the non-local means (NLM) algorithm [37] and flattened. The use of different linear and non-linear classifiers was then explored to identify which performed best.\nFinally, Schlegl et al. [32] employed a 2D patch-based Convolutional Neural Network (CNN) to classify retinal tissue into Intra-retinal Cysts (IRC), Subretinal Fluid (SRF) and healthy categories, while providing information to the location of the pathology. They train their classifier using three different ground truths: weak-labeling, wherein a single label is applied to the whole C-scan; weaklabeling with semantic information, wherein coarse information about the location of the pathology is applied along with the weak label; full-labeling, wherein the classifier is trained on the per-voxel ground truth of the whole C-scan. The latter approach yields the best results, with 97.7%, 89.61% and 91.98% for the healthy, IRC and SRF classes, respectively. While the weak-labeling approach performs significantly worse, at 65.63%, 21.94% and 90.30%, respectively, this setting is the most closely related to the one in the present work.\nMore specifically, we present a novel method to automatically evaluate AMD or healthy volumes. Our strategy has two important advantages over existing methods: (1) it relies only on volume level labels to be trained and (2) it evaluates complete volumes in one shot, making it simpler to use. As we will show in Sec. 4, our approach allows for significant performance gains over these existing methods."}, {"heading": "3 Our Approach", "text": "The overall goal in this work is to automatically evaluate whether an OCT volume contains AMD. The main challenges in tackling this problem lies in the fact that (1) relatively few volumes are typically available for training classification models even though volumes are large in size (e.g. 500 \u00d7 1000 \u00d7 100 pixels) and (2) that labels denoting the presence of pathology are only available at the volume level and not at the cross-section level.\nTo perform effective volume classification, we will follow a Deep CNN approach and will describe in the following section our novel architecture to do so. In general, our approach relies on a threestage process.\nThe first is an OCT-specific normalization and data-augmentation strategy for OCT volumes in order to improve overall generalization and classification performance. Here, we reduce image dimensionality and flatten OCT scans in order to regularize the data. Similarly, we make use of symmetries particular to the eyes in order to augment the data effectively. The second stage attempts to learn pathology-specific features at cross-section B-scan levels using volume-level labels. Here, we make use of a relatively simple network to learn filters that are relevant for 2D OCT image data. In the last stage, we remap the volume to a large image mosaic and train a new volume-level network by leveraging the previously learned filters that operate at the B-scan level.\nWe now begin by formalizing our problem and establish the necessary notation to precisely describe our strategy."}, {"heading": "3.1 Notation and formulation", "text": "Without loss of generality, we assume that our training data V = {V1, . . . , VN} is comprised of N OCT volumes. Each volume Vn is of dimension {W \u00d7H \u00d7D} = V , where a B-scan cross-section consists of a W \u00d7 D image, with D being the depth of the penetrating OCT light source. For a volume Vn, we denote Bhn, h = 1, . . . ,H , as the h th B-scan in the volume.\nEach volume in Vn is associated with a class label Yn \u2208 {0, 1} = Y such that 0 corresponds to control volumes and 1 corresponds to pathological volumes (i.e. AMD). Our goal is to learn a classification function f : V \u2192 Y using the training set and labels available. Importantly, we assume that no information is available on the labels of Bhn, as these are expensive to gather."}, {"heading": "3.2 Data preprocessing", "text": ""}, {"heading": "3.2.1 Normalization", "text": "As can be seen in Fig. 2 (top row), there is both high variability in positions of Bhn with respect to the anatomy and distortion of the retinal layers. In particular, the retinal layers may be tilted, shifted vertically, distorted due to the acquisition process and of varying intensity. In addition, significant portions of Bhn images contain little informative content, such as the area above the ILM and below\nthe BM layer. This is due to the OCT imaging device and consists of either noise or regions too deep for the OCT laser to penetrate.\nAs such, in order to provide a more compact and consistent set of training volumes, we are interested in normalizing and reducing the size of the OCT volumes. Unfortunately, simply cropping each B-scan would be ill-suited, since the retina is curved and this would either remove most of the informative data or result in marginal resizing. To this end, we propose an effective normalization or flattening strategy.\nOur flattening approach consists in aligning the individual BM layers, rectifying for the eye curvature and normalizing for variations in volume intensities. To do so, we first detect the BM layer by applying an anisotropic filter on Bhn using 200 diffusion iterations [38]\nc (\u2016\u2207I\u2016) = 1 1 + ( \u2016\u2207I\u2016 \u03ba )2 , where \u03ba = 50 was empirically set for all experiments. We then compute the Difference of Gaussians (DoG) from the filtered responses and estimate the BM layer as maximal gradient pixels. Naturally, these responses are noisy and incorrect in some cases. For this reason, we fit a secondorder polynomial model to the noisy responses using RANSAC outlier detection [39]. We then warp the estimated BM to a vertical line centered at 60% of the image height. In order to reduce the dimension of the image, we resize each Bhn to be of smaller size w \u00d7 d, w < W, d < D. As can be seen in Fig. 2, areas above the ILM and below the BM only contain noise. For this reason, we crop every Bhn by discarding every voxel v in the C-scan so that d 4 \u2264 vd \u2264 3d 4 , with vd being the depth of the voxel.\nFinally, intensity variations in OCT are common when looking at acquisitions over different patients. In order to regularize across these variations, the voxel intensities are normalized to be zero mean and with a standard deviation of one."}, {"heading": "3.2.2 Data augmentation", "text": "We use data augmentation to increase the number of samples in our training dataset and reduce overfitting [40, 41]. In particular, we take advantage of the bilateral symmetry of the eye to effectively double the number of samples. The resulting samples are biologically plausible, i.e. the optic disc, fovea and vessels remain at the correct spots relative to each other, and removes any latent sample bias due to different counts of left and right eyes in the dataset."}, {"heading": "3.3 B-scan classification with weak labels: RetiNet B", "text": "Recall that our data is inherently volumetric and that the amount of available data is relatively small. Given this challenging learning context, we will first learn features that are efficient at detecting typical 2D OCT structures by learning to \u201cclassify\u201d B-scans. While our labels are only at the volume level, we propose to learn a B-scan level classifier by using approximately correct, or \u201cweak\u201d labels to do so. In particular, we let Y\u0302 hn = Yn, where Y\u0302 h n is the weak label for B h n. Note that for all control volumes, the Y\u0302 hn = 0 labels indicate the lack of an AMD diagnosis \u2013 individual B-scan cross-sections should be pathology free. Conversely, volumes from subjects diagnosed with AMD may contain a number of control or non-pathological B-scans. In particular, up to 50% of the labels could be incorrect for such volumes.\nTo learn this classification function, we proceed by constructing a feed-forward CNN whose architecture is illustrated in Fig. 3(top). In this network, every single gray-scale Bhn is fed as input and passed through a set of 7 convolutional layers with small kernels (3 \u00d7 3, 5 \u00d7 5) and max-pooling layers. We define this set of consecutive layers as the FEATURE layers.\nFollowing these layers, classification is achieved by using two consecutive fully connected layers and using a soft-max activation with two outputs for our two classes (i.e. control and AMD). We denote these latter layers as the CLASSIFICATION layers. Throughout the entire network, convolutional and dense layers make use of leaky Rectified Linear Units (ReLUs) activations. From this point on, we refer to this network as RetiNet B.\nTo train this network, we first begin by initializing all layer parameters randomly using Glorot Uniform sampling [42]. We then make use of Extreme Learning [43], as it has been shown to increase regularization by forcing the convolutional layers to map to a broader features space. In practice, once the CLASSIFICATION layers are initialized, we do not allow them to change. That is, we freeze these layers and only allow the FEATURE layers to be modified during the learning phase. In Sec. 4.4, we show the effect of this learning strategy when compared to traditional regimes."}, {"heading": "3.4 Volume classification: RetiNet C", "text": "As we will show later in our experiments, the performance of the above network is limited, as it must learn from weak labels and does not make use of volumetric information to make a final decision. More so, it is not possible to test if the classification is in fact correct as the true label per B-scan is not known.\nFor this reason, we proceed to a second stage that attempts to classify the complete C-scan in one shot. Our proposed network is depicted in Fig. 3. Instead of setting each Bhn as an input channel, our network takes as input a vertical image of stacked B-scans, Mn, where\nMn = B1n... Bhn  , resulting in a {w\u00d7Dh} sized image. Using the learned FEATURE layers from the previous section, we include these into our new network as they are invariant to the size of the input and because ideally, these have learned what anatomical and pathological structures are relevant. These are then followed by 5 consecutive blocks of convolutional layers, average-pooling and batch normalization. Finally, we add the CLASSIFICATION layer without transferring weights from RetiNet B. We define this network configuration as RetiNet C.\nTo train RetiNet C, we freeze the FEATURE layers, as these were learned on B-scans and should respond in the same way as above to preserve useful features extracted in the previous phase. The rest of the network is then trained using the true labels Yn to learn the remainder of the network layers."}, {"heading": "4 Evaluation", "text": "We now detail the performance of our strategy in the task of AMD classification in OCT volumes. We compare our approach to a number of existing state-of-the-art baselines coming from both the\nOCT pathology identification literature and the more general computer vision literature. We also provide qualitative results of our method, illustrating the different activation maps produced by our network and show how the different stages of our approach benefit the overall performance."}, {"heading": "4.1 Data set", "text": "Our method was trained and evaluated on the publicly available dataset from Duke University [5]. This dataset was made available to find methods to define quantitative indicators for the presence of intermediate AMD. In this set, 384 Spectral Domain OCT volumes are present, of which 269 volumes come from subjects with intermediate AMD while the remaining 115 subjects volumes were collected from healthy subjects. All scans are centered on the foveal pit. Each volume is acquired with 1000 A-scans per B-scan and 100 B-scans per volume. This results volume dimensions of 100\u00d7 1000\u00d7 500px3. In general, the volumes are not isotropic."}, {"heading": "4.2 Baselines", "text": "To illustrate how each part of oue strategy influences overall performance, as well as to compare how our approach performs in contrast to other existing techniques in the literature, we now outline a number of baselines to which we will compare to directly:\n- VGG19: is the 19-layer variant of the deep CNN approach for image classification described in [44]. We pre-trained this network on the ImageNet dataset and fine-tuned the resulting filters using the OCT dataset. To do that, we modified the receptive field of the network to match our B-scan resolution of 384x298 and exchanged the classification layer of the network with a fully-connected layer of size 2.\n- ResNet: similar to VGG19, we evaluated a pre-trained version of the 152-layer residual network described by He et al. [45]. Due to the highly tuned parameters of this network, we maintained the size of the receptive field at 224x224, opting instead to resize our input volume dimension to match. As before, we exchange the classification layer with a fullyconnected layer of size 2.\n- DenseNet: is a recent architecture network by Huang et al. [46], which extends the residual network concept using a complete graph of skip connections. We implemented DenseNet with 3 dense blocks and a growth rate of 12, and trained the entire network on the OCT dataset.\n- 2DSeg: is the patch-based classification scheme for pathological OCT identification described in Schlegl et al. [32], which we re-implemented and trained on the OCT dataset. Due to the lack of location information or per-voxel classifications in our ground truth data, we focused on the weak-labeling approach described in this same paper.\nIn addition to our complete RetiNet C approach, we also compare its performance to;\n- RetiNet B (extreme): This consists of the RetiNet B classifier described in Sec. 3.3 learned with Extreme Learning [43]. By comparing RetiNet to this baseline, we can see the performance gain provided by the RetiNet C network construction.\n- RetiNet B: Similarly to RetiNet B (extreme), this classifier is identical in structure to the network of RetiNet B but trained without Extreme Learning.\nIn addition, we attempted to train both VGG19 and ResNet using only the OCT data, but given the large size of these networks and the small size of the dataset, this yielded in extremely poor classification methods. To avoid bias, we omit these methods from our experiments."}, {"heading": "4.3 Experimental setup", "text": "We partition the dataset into five randomized, equi-sized subsets, using four for training and one for testing, for a total of five cross-validations per network. All methods were trained on the same partitions using the same folds. The random seed was preserved across all runs in order to remove any dataset-dependent bias.\n0.0\n0.00 0.05 0.10 0.15 0.20 0.25\n0.75\nWeak BSL2D (AUC=0.978)\n0.00 0.02 0.04 0.06 0.08 0.10\n0.0\n0.00 0.05 0.10 0.15 0.20 0.25\n0.75\nWeak BSL2D (AUC=0.978)\nWe trained each network for a maximum of 100 epochs per fold, using early stopping with a patience of 15 epochs to avoid over-fitting [47, 48]. We relied on the adadelta algorithm [49] to optimize the parameters of each network. All networks except that of 2DSeg were optimized by minimizing the categorical cross-entropy of their predictions versus the ground truth. 2DSeg was optimized by minimizing the mean squared error, as described in [32].\nVGG19, DenseNet and ResNet networks were trained and evaluated at the B-scan level using a weak labeling scheme, where the label of the complete C-scan was applied to each B-scan of that subject. The final C-scan classification prediction was defined as the mean score of the B-scan level predictions. The maximum achievable B-scan level accuracy is limited to roughly 94pc, due to mislabelings of individual B-scans (i.e. the C-scan of an AMD patient may contain a number of healthy B-scans), as well as acquisition artifacts (i.e. blinks).\nWe provide a version of our RetiNet C implementation online1. A complete list of the parameters used can be found in Table. 1. These were selected using experimental validation. In Fig. 4, we show the learning rate of our network with the above parameters on the training data and on the a validation set."}, {"heading": "4.4 RetiNet characterization", "text": "As an initial set of experiments, we are interested in characterizing the performance of our strategy. In Fig. 5, we directly compare performances between RetiNet C, RetiNet B and RetiNet B (extreme) in terms of classification performance. The results shown in this figure are attained with 5-fold cross-validation.\nFirst, we show in Fig. 5(top) the ROC curves of each of the three strategies. In addition to this traditional metric, we also show in Fig. 5(bottom) the False Negative Rate versus the False Positive\n1Visit https://github.com/thefiddler/retinet for an implementation of RetiNet C.\n0.00 0.02 0.04 0.06 0.08 0.10\nFalse negative rate\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFa ls\ne po\nsi tiv\ne ra\nte\nFNR/FPR RetiNet Weak BSL2D (extreme) Weak BSL2D\n0.00 0.05 0.10 0.15 0.20 0.25\nFalse positive rate\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nTr ue\npo si\ntiv e\nra te\nROC curve\nRetiNet (AUC=0.991) Weak BSL2D (extreme) (AUC=0.995) Weak BSL2D (AUC=0.978)\n0 10 20 30 40 50 60 70\nEpochs\n10 5\n10 4\n10 3\n10 2\n10 1\n100\n101\nLo ss\nTrain Loss\nRetiNet Weak BSL2D (extreme) Weak BSL2D\n0 10 20 30 40 50 60 70\nEpochs\n10 4\n10 3\n10 2\n10 1\n100\n101\nLo ss\nValidation Loss\nRetiNet Weak BSL2D (extreme) Weak BSL2D\nRetiNet C (AUC=0.997) RetiNet B (extr m ) (AUC=0.995) RetiNet B (AUC=0.978)\n(a) Receiver Operating Characteristic\n0.00 0.02 0.04 0.06 0.08 0.10\nFalse negative rate\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFa ls\ne po\nsi tiv\ne ra\nte\nFNR/FPR RetiNet Weak BSL2D (extreme) Weak BSL2D\n0.00 0.05 0.10 0.15 0.20 0.25\nFalse positive rate\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nTr ue\npo si\ntiv e\nra te\nROC curve\nRetiNet (AUC=0.991) Weak BSL2D (extreme) (AUC=0.995) Weak BSL2D (AUC=0.978)\n0 10 20 30 40 50 60 70\nEpochs\n10 5\n10 4\n10 3\n10 2\n10 1\n100\n101\nLo ss\nTrain Loss\nRetiNet Weak BSL2D (extreme) Weak BSL2D\n0 10 20 30 40 50 60 70\nEpochs\n10 4\n10 3\n10 2\n10 1\n100\n101\nLo ss\nValidation Loss\nRetiNet Weak BSL2D (extreme) Weak BSL2D\n0.00 0.02 0.04 0.06 0.08 0.10\nFalse negative rate\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFa ls\ne po\nsi tiv\ne ra\nte\nFNR/FPR RetiNet Weak BSL2D (extreme) Weak BSL2D\n0.00 0.05 0.10 0.15 0.20 0.25\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nTr ue\npo si\ntiv e\nra te\n0 10 20 30 40 50 60 70\nEpochs\n10 5\n10 4\n10 3\n10 2\n10 1\n100\n101\nLo ss\nTrain Loss\nRetiNet Weak BSL2D (extreme) Weak BSL2D\n0 10 20 30 40 50 60 70\nEpochs\n10 4\n10 3\n10 2\n10 1\n100\n101\nLo ss\nRetiNet Weak BSL2D (extreme) Weak BSL2D\ni C RetiNet B (extrem ) RetiNet B\n(b) False Positive to False Negative ratio\nRate. This metric is more informative from a clinical perspective, as the clinical cost of classifying a pathological volume as a healthy volume is much higher than the other way around. In particular, we can see that for a 1% false negative rate (i.e. misclassifying pathological as healthy), RetiNet C has a 0. 5% false positive rate (i.e. misclassifying healthy as pathological). This is interesting in the context of screening because it indicates that a human would need to evaluate pathological scans in any case and that the false positive rate indicates the reduced proportion of healthy scans one would still need to examine should one use an automatic classification algorithm. That is, allowing for a 1% error in predicted subjects would only require a 5% inspection of the healthy population.\nWith this, we can observe that RetiNet B has difficulty in learning correctly the volume labels given that it is trained on weak labels. As illustrated in the training and validation loss plots in Fig. 4, we can see that RetiNet B effectively lacks generalization capabilities as it heavily overfits the data.\nIn contrast, RetiNet B (extreme), with its Extreme Learning framework, allows for a stronger regularization and mitigates a significant amount of overfitting that is present with RetiNet B. As such, the difference in classification performance between RetiNet B (extreme) and RetiNet C can be attributed to the weak labels and the 2D nature of the strategy.\nIn this sense, these results highlight that RetiNet C overcomes the lack of B-scan level labels and that weak labels can be exploited at the C-scan level. To illustrate what our network learns however, we can visualize the network activation maps in Fig. 7. Here we show four (two healthy and two AMD) examples of volumes and how our network responds to them. For each case, we show three B-scans from a volume (h = 25, 50, 75), the associated fundus view of the RPE retinal layer and the projection of the activation map of the last convolutional layer RetiNet C. In particular, we can see for the AMD cases that the activation maps are responding very strongly at different locations of the volume and differently to healthy volumes.\nLast, the learning of this two-stage network appears to learn even after 70 epochs, indicating that the overfitting is most likely limited. At the same time, we notice that learning is in general consistently noisy with our framework and this is most likely due to the limited number of C-scans available during training."}, {"heading": "4.5 Baseline comparison", "text": "Fig. 6 outlines the performance of RetiNet C and the baseline methods in terms of ROC, as well as FNR/FPR. Across both metrics, RetiNet C appears to outperform these baselines.\nA number of interesting conclusions can in addition be drawn from these results. First, off-the-shelf computer vision networks that perform exceptionally well on natural images VGG19, DenseNet and ResNet, have a strong tendency to overfit this data. Second, DenseNet performs very similarly to both VGG19 and ResNet even though it is trained from scratch and converges very quickly. Both VGG19 and ResNet could not be successfully trained from scratch however due to the relatively\n0 10 20 30 40 50 60 70\nEpochs\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nLo ss\nDenseNet ResNet RetiNet VGG19 Vienna15\n0 10 20 30 40 50 60 70\nEpochs\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nLo ss\nDenseNet ResNet RetiNet VGG19 Vienna15\nsmall dataset. Last, the 2DSeg approach, which was sp cifically developed for this application, appears to have difficulties generalizing when training with weak labels. This is consistent with authors conclusion as well [32].\nFrom our experiments, we can see that RetiNet C achieves an Area Under the Curve (AUC) of of 99.7%. This compares favorably to the semi-automatic method of Farsiu et al. [5], which achieved an AUC of 99.17% in their reported best case and the automatic method of Venhuizen et al. [35] which achieved an AUC of 98.4%. While we do not compare to these methods directly, we report these scores from their published results on the same data.\nIn this sense, it appears as though RetiNet C provides a stable strategy capable of leveraging volumetric information and only uses the volume level labels for training. Fig. 8 shows a few examples where RetiNet C correctly and incorrectly predicts different volume labels."}, {"heading": "5 Conclusions", "text": "In this article, we have proposed a novel strategy for automatic identification of AMD in OCT volumes. Our strategy is advantageous as it only requires volume-level labels as opposed to crosssectional labels, making it far easier to train from a groundtruth acquisition point of view. Our approach involves a novel two-stage deep learning architecture that in the first phase, focuses on learning features that are domain specific and then focuses on the volume classification task in the latter phase.\nWe validated our approach using publicly available OCT data and compared the performance of our method against both techniques from the OCT domain and the computer vision literature. We showed that not only does our approach do well in terms of ROC performance, but that it also does well with respect to a more clinically relevant metric.\nThis being said, our method still has difficulties identifying mild AMD cases as shown in Fig. 8(e) where the difference between healthy and pathological is visibly challenging. In this sense, we will focus in the future on developing strategies for identifying early-stages of the disease and we will look at how diseases differ to one another."}], "references": [{"title": "Optical coherence tomography in dermatology : a review", "author": ["J. Welzel"], "venue": "Skin Res Technol., vol. 7, no. 1, pp. 1\u20139, 2001.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Retinal Imaging and Image Analysis", "author": ["M.D. Abramoff", "M.K. Garvin", "M. Sonka"], "venue": "IEEE Transactions on Medical Imaging, vol. 3, no. 1, pp. 169\u2013208, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Quantitative classification of eyes with and without intermediate age-related macular degeneration using optical coherence tomography", "author": ["S. Farsiu", "S.J. Chiu", "R.V. O\u2019Connell", "F.A. Folgar", "E. Yuan", "J.A. Izatt", "C.A. Toth"], "venue": "Ophthalmology, vol. 121, no. 1, pp. 162\u2013172, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Age-Related Macular Degeneration", "author": ["R.D. Jager", "W.F. Mieler", "J.W. Miller"], "venue": "The New England Journal of Medicine, vol. 358, no. 24, pp. 2606\u20132617, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Global prevalence and major risk factors of diabetic retinopathy", "author": ["J.W.Y. Yau", "S.L. Rogers", "R. Kawasaki", "E.L. Lamoureux", "J.W. Kowalski", "T. Bek", "S.-J. Chen", "J.M. Dekker", "A. Fletcher", "J. Grauslund", "S. Haffner", "R.F. Hamman", "M.K. Ikram", "T. Kayama", "B.E.K. Klein", "R. Klein", "S. Krishnaiah", "K. Mayurasakorn", "J.P. O\u2019Hare", "T.J. Orchard", "M. Porta", "M. Rema", "M.S. Roy", "T. Sharma", "J. Shaw", "H. Taylor", "J.M. Tielsch", "R. Varma", "J.J. Wang", "N. Wang", "S. West", "L. Xu", "M. Yasuda", "X. Zhang", "P. Mitchell", "T.Y. Wong"], "venue": "Diabetes Care, vol. 35, no. 3, pp. 556\u2013 564, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Global prevalence of age-related macular degeneration and disease burden projection for 2020 and 2040: A systematic review and meta-analysis", "author": ["W.L. Wong", "X. Su", "X. Li", "C.M.G. Cheung", "R. Klein", "C.Y. Cheng", "T.Y. Wong"], "venue": "The Lancet Global Health, vol. 2, no. 2, pp. e106\u2013e116, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Age-related macular degeneration is the leading cause of blindness", "author": ["N. Bressler"], "venue": "JAMA, vol. 291, no. 15, pp. 1900\u20131901, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1900}, {"title": "Early detection and treatment of neovascular age-related macular degeneration.", "author": ["N.M. Bressler"], "venue": "The Journal of the American Board of Family Practice / American Board of Family Practice,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Diabetic retinopathy and diabetic macular edema: Pathophysiology, screening, and novel therapies", "author": ["T.A. Ciulla", "A.G. Amador", "B. Zinman"], "venue": "Diabetes Care, vol. 26, no. 9, pp. 2653\u2013 2664, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Guidelines for the management of neovascular age-related macular degeneration by the European Society of Retina Specialists (EURETINA)", "author": ["U. Schmidt-Erfurth", "V. Chong", "A. Loewenstein", "M. Larsen", "E. Souied", "R. Schlingemann", "B. Eldem", "J. Mones", "G. Richard", "F. Bandello", "S. European Society of Retina"], "venue": "Br J Ophthalmol, vol. 98, no. 9, pp. 1144\u20131167, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Speckle reduction in optical coherence tomography images by use of a spatially adaptive wavelet filter", "author": ["D.C. Adler", "T.H. Ko", "J.G. Fujimoto"], "venue": "Opt. Lett., vol. 29, no. 24, pp. 2878\u20132880, Dec 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Image denoising with block-matching and 3D filtering", "author": ["K. Dabov", "A. Foi"], "venue": "Electronic Imaging, vol. 6064, pp. 1\u201312, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Nonlocal Transform- Domain Filter for Volumetric Data Denoising and Reconstruction", "author": ["M. Maggioni", "V. Katkovnik", "K. Egiazarian", "S. Member", "A. Foi"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 1, pp. 119\u2013133, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Ultrahigh-resolution, high-speed, Fourier domain optical coherence tomography and methods for dispersion compensation", "author": ["M. Wojtkowski", "V.J. Srinivasan", "T.H. Ko", "J.G. Fujimoto", "A. Kowalczyk", "J.S. Duker", "D.J. Fujimoto JG", "Kowalczyk A"], "venue": "Optics Express, vol. 12, no. 11, p. 2404, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Correction of motion artifacts and scanning beam distortions in 3D ophthalmic optical coherence tomography imaging", "author": ["R.J. Zawadzki", "A.R. Fuller", "S.S. Choi", "D.F. Wiley", "B. Hamann", "J.S. Werner"], "venue": "Ophthalmic Technologies XVII, vol. 6426, no. x, p. 42607, 2007.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Motion correction in optical coherence tomography volumes on a per A-scan basis using orthogonal scan patterns.", "author": ["M.F. Kraus", "B. Potsaid", "M.A. Mayer", "R. Bock", "B. Baumann", "J.J. Liu", "J. Hornegger", "J.G. Fujimoto"], "venue": "Biomedical optics express,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Averaging techniques for OCT imaging", "author": ["M. Szkulmowski", "M. Wojtkowski"], "venue": "Opt Express, vol. 21, no. 8, pp. 9757\u20139773, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Motion artefact correction in retinal optical coherence tomography using local symmetry", "author": ["A. Montuoro", "J. Wu", "S. Waldstein", "B. Gerendas", "G. Langs", "C. Simader", "U. Schmidt-Erfurth"], "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic estimation of noise parameters in Fourier-domain optical coherence tomography cross sectional images using statistical information", "author": ["P. Steiner", "J.H. Kowal", "B. Pova\u017eay", "C. Meier", "R. Sznitman"], "venue": "Applied Optics, vol. 54, no. 12, pp. 3650\u20133657, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Imaging thermal expansion and retinal tissue changes during photocoagulation by high speed OCT", "author": ["H.H. M\u00fcller", "L. Ptaszynski", "K. Schlott", "C. Debbeler", "M. Bever", "S. Koinzer", "R. Birngruber", "R. Brinkmann", "G. H\u00fcttmann"], "venue": "Biomedical Optics Express, vol. 3, no. 5, p. 1025, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Time-Resolved Ultra-High Resolution Optical Coherence Tomography for Real-Time Monitoring of Selective Retina Therapy", "author": ["P. Steiner", "A. Ebneter", "L.E. Berger", "M. Zinkernagel", "C. Meier", "J.H. Kowal", "C. Framme", "R. Brinkmann", "S. Wolf", "R. Sznitman"], "venue": "Investigative Ophthalmology and Visual Science, vol. 56, pp. 6654\u20136662, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic assessment of time-resolved oct images for selective retina therapy", "author": ["S. Zbinden", "\u015e.S. Kucur", "P. Steiner", "S. Wolf", "R. Sznitman"], "venue": "International Journal of Computer Assisted Radiology and Surgery, vol. 11, no. 6, pp. 863\u2013871, 2016. [Online]. Available: http://dx.doi.org/10.1007/s11548-016-1383-6", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Microscope-integrated intraoperative OCT with electrically tunable focus and heads-up display for imaging of ophthalmic surgical maneuvers", "author": ["Y.K. Tao", "S.K. Srivastava", "J.P. Ehlers", "C. Clinic"], "venue": "Biomed Opt Express., vol. 5, no. 6, pp. 1342\u20131350, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Automated stereo vision instrument tracking for intraoperative OCT guided anterior segment ophthalmic surgical maneuvers.", "author": ["M.T. El-Haddad", "Y.K. Tao"], "venue": "Biomedical optics express,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Threedimensional analysis of retinal layer texture: Identification of fluid-filled regions in SD-OCT of the macula", "author": ["G. Quellec", "K. Lee", "M. Dolejsi", "M.K. Garvin", "M.D. Abr\u00e0moff", "M. Sonka"], "venue": "IEEE Transactions on Medical Imaging, vol. 29, no. 6, pp. 1321\u20131330, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Validated automatic segmentation of AMD pathology including drusen and geographic atrophy in SD- OCT images", "author": ["S.J. Chiu", "J.A. Izatt", "R.V. O\u2019Connell", "K.P. Winter", "C.A. Toth", "S. Farsiu"], "venue": "Investigative Ophthalmology and Visual Science, vol. 53, no. 1, pp. 53\u201361, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Graph-based multi-surface segmentation of OCT data using trained hard and soft constraints", "author": ["P.A. Dufour", "L. Ceklic", "H. Abdillahi", "S. Schroder", "S. De Zanet", "U. Wolf-Schnurrbusch", "J. Kowal"], "venue": "IEEE Transactions on Medical Imaging, vol. 32, no. 3, pp. 531\u2013543, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Vendor Independent Cyst Segmentation in Retinal SD-OCT Volumes using a Combination of Multiple Scale Convolutional Neural Networks", "author": ["F.G. Venhuizen", "M.J.J.P.V. Grinsven", "C.B. Hoyng"], "venue": "Medical Image Computing and Computer Assisted Intervention - Challenge on Retinal Cyst Segmentation, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting Semantic Descriptions from Medical Images with Convolutional Neural Networks", "author": ["T. Schlegl", "S.M. Waldstein", "U.M. Schmidt-erfurth"], "venue": "Information Processing in Medical Imaging, 2015, pp. 437\u2013448.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated macular pathology diagnosis in retinal OCT images using multi-scale spatial pyramid with local binary patterns", "author": ["Y.-Y. Liu", "M. Chen", "H. Ishikawa", "G. Wollstein", "J.S. Schuman", "J.M. Rehg"], "venue": "Medical Image Computing and Computer-Assisted Intervention\u2013 MICCAI 2010. Springer, 2010, pp. 1\u20139.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Automated age-related macular degeneration classification in oct using unsupervised feature learning", "author": ["F.G. Venhuizen", "B. van Ginneken", "B. Bloemen", "M.J.J.P. van Grinsven", "R. Philipsen", "C. Hoyng", "T. Theelen", "C.I. Snchez"], "venue": "Proc. SPIE, vol. 9414, 2015, pp. 94 141I\u201394 141I\u20137.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Classification of SD-OCT Volumes using Local Binary Patterns: Experimental Validation for DME Detection", "author": ["G. Lemaitre", "M. Rastgoo", "J. Massich", "C.Y. Cheung", "Y. Wong", "E. Lamoureux", "D. Milea", "M. Fabrice", "G. Lemaitre", "M. Rastgoo", "J. Massich", "C.Y. Cheung", "T.Y. Wong", "G. Lema"], "venue": "Journal of Ophthalmology, vol. 6, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "A non-local algorithm for image denoising", "author": ["A. Buades", "B. Coll", "J.-M.J.-M. Morel"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 2, no. 0, pp. 60\u201365 vol. 2, 2005.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Scale-space and edge detection using anisotropic diffusion", "author": ["P. Perona", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 12, no. 7, pp. 629\u2013639, 1990.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1990}, {"title": "Random Sample Consensus: A Paradigm for Model Fitting with Applicatlons to Image Analysis and Automated Cartography", "author": ["M.A. Fischler", "R.C. Bolles"], "venue": "Communications of the ACM, vol. 24, no. 6, pp. 381 \u2013 395, 1981. [Online]. Available: http://dx.doi.org/10.1145/358669.358692", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1981}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings., vol. 1, no. Icdar, pp. 958\u2013963, 2003. [Online]. Available: http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1227801", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances In Neural Information Processing Systems, pp. 1\u20139, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), vol. 9, pp. 249\u2013256, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Extreme learning machine: Theory and applications", "author": ["G.-B. Huang", "Q.-y. Zhu", "C.-k. Siew", "G.-b. H. \u00c3", "Q.-y. Zhu", "C.-k. Siew", "G.-B. Huang", "Q.-y. Zhu", "C.-k. Siew"], "venue": "Neurocomputing, vol. 70, no. 1-3, pp. 489\u2013501, 2006. 13  (a) True positive  (b) True positive (c) True negative  (d) True negative (e) False positive  (f) False negative Figure 8: Example B-scans from correctly and incorrectly classified volumes. While (a-d) show correctly identified cases, (e-f) are incorrectly classified. Surprisingly, our approach correctly identifies (d) as non-AMD, even though it illustrates an epiretinal membrane and vitreoretinal traction, neither of which is AMD.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2006}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ArXiv preprint arXiv:1409.1556, pp. 1\u201314, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Arxiv.Org, vol. 7, no. 3, pp. 171\u2013180, 2015.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Densely Connected Convolutional Networks", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger"], "venue": "ArXiv preprint, pp. 1\u201312, 2016.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic early stopping using cross validation: Quantifying the criteria", "author": ["L. Prechelt"], "venue": "Neural Networks, vol. 11, no. 4, pp. 761\u2013767, 1998.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1998}, {"title": "Exploring Strategies for Training Deep Neural Networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "Journal of Machine Learning Research, vol. 1, pp. 1\u201340, 2009.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["M.D. Zeiler"], "venue": "arXiv, p. 6, 2012. [Online]. Available: http://arxiv.org/abs/1212.5701 14", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Similarly, its use has gained traction in other medical fields such as for histopathology and skin cancer analysis [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "the retina) at micrometer resolution, OCT imaging now allows for visualization of most retinal layers [3, 4] and more importantly, numerous pathological markers, such as intraretinal fluid, drusens or cysts [5, 6].", "startOffset": 102, "endOffset": 108}, {"referenceID": 2, "context": "the retina) at micrometer resolution, OCT imaging now allows for visualization of most retinal layers [3, 4] and more importantly, numerous pathological markers, such as intraretinal fluid, drusens or cysts [5, 6].", "startOffset": 207, "endOffset": 213}, {"referenceID": 3, "context": "the retina) at micrometer resolution, OCT imaging now allows for visualization of most retinal layers [3, 4] and more importantly, numerous pathological markers, such as intraretinal fluid, drusens or cysts [5, 6].", "startOffset": 207, "endOffset": 213}, {"referenceID": 2, "context": "7% of the world population and 159 million people worldwide, respectively [5, 7, 8].", "startOffset": 74, "endOffset": 83}, {"referenceID": 4, "context": "7% of the world population and 159 million people worldwide, respectively [5, 7, 8].", "startOffset": 74, "endOffset": 83}, {"referenceID": 5, "context": "7% of the world population and 159 million people worldwide, respectively [5, 7, 8].", "startOffset": 74, "endOffset": 83}, {"referenceID": 6, "context": "Moreover, these pathologies are the major cause of blindness in developed countries [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "Alarmingly, the number of people with either of these diseases is projected to skyrocket, with AMD affecting an estimated 196 million people by 2020 and 288 million people by 2040 [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "While OCT has gained significant importance in recent years for AMD and DR screening [11, 12], the process to do so remains time consuming however.", "startOffset": 85, "endOffset": 93}, {"referenceID": 8, "context": "While OCT has gained significant importance in recent years for AMD and DR screening [11, 12], the process to do so remains time consuming however.", "startOffset": 85, "endOffset": 93}, {"referenceID": 9, "context": "In this context, automated algorithms for pathology identification in OCT volumes would be of great benefit for clinicians and ophthalmologists, as access to OCT devices becomes common and nation-wide screening programs commence [13].", "startOffset": 229, "endOffset": 233}, {"referenceID": 10, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 60, "endOffset": 72}, {"referenceID": 11, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 60, "endOffset": 72}, {"referenceID": 12, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 60, "endOffset": 72}, {"referenceID": 13, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 14, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 15, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 16, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 17, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 18, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 19, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 177, "endOffset": 189}, {"referenceID": 20, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 177, "endOffset": 189}, {"referenceID": 21, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 177, "endOffset": 189}, {"referenceID": 22, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 241, "endOffset": 249}, {"referenceID": 23, "context": "Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27].", "startOffset": 241, "endOffset": 249}, {"referenceID": 24, "context": "More specific to pathology identification, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of relevant pathological markers [28, 29, 30, 31, 32] or classification of 2D B-scans or 3D Cscans [33, 34, 32, 35, 36].", "startOffset": 215, "endOffset": 235}, {"referenceID": 25, "context": "More specific to pathology identification, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of relevant pathological markers [28, 29, 30, 31, 32] or classification of 2D B-scans or 3D Cscans [33, 34, 32, 35, 36].", "startOffset": 215, "endOffset": 235}, {"referenceID": 26, "context": "More specific to pathology identification, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of relevant pathological markers [28, 29, 30, 31, 32] or classification of 2D B-scans or 3D Cscans [33, 34, 32, 35, 36].", "startOffset": 215, "endOffset": 235}, {"referenceID": 27, "context": "More specific to pathology identification, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of relevant pathological markers [28, 29, 30, 31, 32] or classification of 2D B-scans or 3D Cscans [33, 34, 32, 35, 36].", "startOffset": 215, "endOffset": 235}, {"referenceID": 28, "context": "More specific to pathology identification, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of relevant pathological markers [28, 29, 30, 31, 32] or classification of 2D B-scans or 3D Cscans [33, 34, 32, 35, 36].", "startOffset": 215, "endOffset": 235}, {"referenceID": 29, "context": "More specific to pathology identification, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of relevant pathological markers [28, 29, 30, 31, 32] or classification of 2D B-scans or 3D Cscans [33, 34, 32, 35, 36].", "startOffset": 281, "endOffset": 301}, {"referenceID": 28, "context": "More specific to pathology identification, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of relevant pathological markers [28, 29, 30, 31, 32] or classification of 2D B-scans or 3D Cscans [33, 34, 32, 35, 36].", "startOffset": 281, "endOffset": 301}, {"referenceID": 30, "context": "More specific to pathology identification, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of relevant pathological markers [28, 29, 30, 31, 32] or classification of 2D B-scans or 3D Cscans [33, 34, 32, 35, 36].", "startOffset": 281, "endOffset": 301}, {"referenceID": 31, "context": "More specific to pathology identification, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of relevant pathological markers [28, 29, 30, 31, 32] or classification of 2D B-scans or 3D Cscans [33, 34, 32, 35, 36].", "startOffset": 281, "endOffset": 301}, {"referenceID": 2, "context": "Using a publicly available OCT dataset [5], we show that our approach is highly effective at separating AMD", "startOffset": 39, "endOffset": 42}, {"referenceID": 30, "context": "[35] regions of interest are automatically extracted around the center of each C-scan via an intensity threshold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[5], who developed a semi-automatic classification method for AMD patients.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Each B-scan was first resized to a resolution of 246x256 pixels, denoised using the Block Matching and 3D filtering (BM3D) algorithm [15] and then flattened.", "startOffset": 133, "endOffset": 137}, {"referenceID": 31, "context": "More recently, Lemaitre et al [36] followed in the direction of Liu et al.", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "[33], by extracting 2D and 3D Local Binary Patterns (LBP) features from a set of 16 healthy and 16 patients suffering from DME.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "B-scans were denoised using the non-local means (NLM) algorithm [37] and flattened.", "startOffset": 64, "endOffset": 68}, {"referenceID": 28, "context": "[32] employed a 2D patch-based Convolutional Neural Network (CNN) to classify retinal tissue into Intra-retinal Cysts (IRC), Subretinal Fluid (SRF) and healthy categories, while providing information to the location of the pathology.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "To do so, we first detect the BM layer by applying an anisotropic filter on B n using 200 diffusion iterations [38]", "startOffset": 111, "endOffset": 115}, {"referenceID": 34, "context": "For this reason, we fit a secondorder polynomial model to the noisy responses using RANSAC outlier detection [39].", "startOffset": 109, "endOffset": 113}, {"referenceID": 35, "context": "We use data augmentation to increase the number of samples in our training dataset and reduce overfitting [40, 41].", "startOffset": 106, "endOffset": 114}, {"referenceID": 36, "context": "We use data augmentation to increase the number of samples in our training dataset and reduce overfitting [40, 41].", "startOffset": 106, "endOffset": 114}, {"referenceID": 37, "context": "To train this network, we first begin by initializing all layer parameters randomly using Glorot Uniform sampling [42].", "startOffset": 114, "endOffset": 118}, {"referenceID": 38, "context": "We then make use of Extreme Learning [43], as it has been shown to increase regularization by forcing the convolutional layers to map to a broader features space.", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": "Our method was trained and evaluated on the publicly available dataset from Duke University [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 39, "context": "- VGG19: is the 19-layer variant of the deep CNN approach for image classification described in [44].", "startOffset": 96, "endOffset": 100}, {"referenceID": 40, "context": "[45].", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[46], which extends the residual network concept using a complete graph of skip connections.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[32], which we re-implemented and trained on the OCT dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "3 learned with Extreme Learning [43].", "startOffset": 32, "endOffset": 36}, {"referenceID": 42, "context": "We trained each network for a maximum of 100 epochs per fold, using early stopping with a patience of 15 epochs to avoid over-fitting [47, 48].", "startOffset": 134, "endOffset": 142}, {"referenceID": 43, "context": "We trained each network for a maximum of 100 epochs per fold, using early stopping with a patience of 15 epochs to avoid over-fitting [47, 48].", "startOffset": 134, "endOffset": 142}, {"referenceID": 44, "context": "We relied on the adadelta algorithm [49] to optimize the parameters of each network.", "startOffset": 36, "endOffset": 40}, {"referenceID": 28, "context": "2DSeg was optimized by minimizing the mean squared error, as described in [32].", "startOffset": 74, "endOffset": 78}, {"referenceID": 28, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "This is consistent with authors conclusion as well [32].", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "[5], which achieved an AUC of 99.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[35] which achieved an AUC of 98.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Optical Coherence Tomography (OCT) provides a unique ability to image the eye retina in 3D at micrometer resolution and gives ophthalmologist the ability to visualize retinal diseases such as Age-Related Macular Degeneration (AMD). While visual inspection of OCT volumes remains the main method for AMD identification, doing so is time consuming as each cross-section within the volume must be inspected individually by the clinician. In much the same way, acquiring ground truth information for each cross-section is expensive and time consuming. This fact heavily limits the ability to acquire large amounts of groundtruth, which subsequently impacts the performance of learning-based methods geared at automatic pathology identification. To avoid this burden, we propose a novel strategy for automatic analysis of OCT volumes where only volume labels are needed. That is, we train a classifier in a semi-supervised manner to conduct this task. Our approach uses a novel Convolutional Neural Network (CNN) architecture, that only needs volume-level labels to be trained to automatically asses whether an OCT volume is healthy or contains AMD. Our architecture involves first learning a cross-section pathology classifier using pseudo-labels that could be corrupted and then leverage these towards a more accurate volume-level classification. We then show that our approach provides excellent performances on a publicly available dataset and outperforms a number of existing automatic techniques. keywords \u2014 Optical Coherence Tomography (OCT), Convolutional Neural Networks (CNN), AgeRelated Macular Degeneration (AMD), pathology identification, ophthalmology, machine learning", "creator": "LaTeX with hyperref package"}}}