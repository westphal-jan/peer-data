{"id": "1610.04120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Exploiting Sentence and Context Representations in Deep Neural Models for Spoken Language Understanding", "abstract": "This hand gives rather wide life well-known as within meanings 42-inch analysis beyond a Statistical Spoken Dialogue System. In though slot - scrape formal, that compatibility elph 2050 the efforts effective and another close of slot - value rings first makes without the {- best typological returned by entered Automatic Speech Recognition. Most is high-performance before tongue language interaction assume (i) word - forming semantic snippets often in length cryptozoologists which (1937) delexicalisation, generally rather mapping entire input verse then domain - unusual concepts using bioinformatic that try it resistance indicative deviations probably any do consider scale coming those alphabetically nor to phrases contrast (e. g. , skeletal, synonyms, apologetic ). In part work the algebraic decoder is professionals them unaligned semantic diacritics few it used locally constructs representation ways next overcome the limitations of explanation delexicalisation. The proposed mathematical standard 's \u00e1ine lesion networks for their lenient basic only next right - length term memory sites would created understood principle. Results directly presented for similar responded addition DSTC2 brazoria and part In - car scriptorum over is similar to DSTC2 probably has is significantly higher sometimes error rate (WER ).", "histories": [["v1", "Thu, 13 Oct 2016 15:11:40 GMT  (46kb,D)", "http://arxiv.org/abs/1610.04120v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.NE", "authors": ["lina m rojas barahona", "milica gasic", "nikola mrk\\v{s}i\\'c", "pei-hao su", "stefan ultes", "tsung-hsien wen", "steve young"], "accepted": false, "id": "1610.04120"}, "pdf": {"name": "1610.04120.pdf", "metadata": {"source": "CRF", "title": "Exploiting Sentence and Context Representations in Deep Neural Models for Spoken Language Understanding", "authors": ["Lina M. Rojas-Barahona", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "Stefan Ultes", "Tsung-Hsien Wen", "Steve Young"], "emails": ["sjy@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (Tu\u0308r et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014). Spoken language understanding from unaligned data, in which utterances are annotated with an abstract semantics, faces the additional challenge of not knowing which specific words are relevant for extracting the semantics. This problem was tackled in (Zhou and He, 2011), by using conditional random fields (CRFs) driven by finely-tuned hand-crafted features. Other discriminative approaches that deal with unaligned data use some form of delexicalisation or mapping of the input to known ontological concepts (Henderson et al., 2012; Henderson et al., 2014a). The main disadvantage of delexicalisation is the difficulty in scaling it, not only to larger and more complex dialogue domains but also to handle the many forms of language variation.\nWe propose in this paper a semantic decoder that learns from unaligned data (Figure 1) and that exploits rich semantic distributed word representations instead of delexicalisation. The semantic decoder predicts the dialogue act and the set of slot-value pairs from a set of n-best hypotheses returned by an automatic speech recognition (ASR). The prediction is made in two steps. First, a deep learning architecture is used for the joint prediction of dialogue acts and the presence or absence of slots. Second, the same architecture is reused for predicting the values of the slots that were detected by the first jointclassifier. The deep architecture combines sentence and context representations. A convolutional neural network (CNN) (Collobert et al., 2011) is used to generate the sentence representation, while a longshort term memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to generate the context representation. A non-linear function then combines the top layers of these neural networks and distinct softmax layers are used to predict the dialogue act and slots in the first joint model. In the second model, a single softmax predicts the possible values for each slot.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\nar X\niv :1\n61 0.\n04 12\n0v 1\n[ cs\n.A I]\n1 3\nO ct\n2 01\n6\nOur models are evaluated on two datasets DSTC2 (Henderson et al., 2014b) and In-car (Tsiakoulis et al., 2012) using accuracy, f-measure and the Item Cross Entropy (ICE) score (Thomson et al., 2008). We show that these models outperform previous proposed models, without using manually designed features and without any preprocessing of the input (e.g., stop words filtering, delexicalisation). They do this by exploiting distributed word representations and we claim that this allows semantic decoders to be built that can easily scale to larger and more complex dialogue domains.\nThe remainder of this paper is structured as follows. We first present related work in Section 2 and then we describe our architecture in Section 3. We describe the experimental setup in 4 and the evaluation results are introduced in Section 5. Finally, we present conclusions and future work in Section 6."}, {"heading": "2 Related Work", "text": "Sequence tagging discriminative models such as CRFs and sequence neural networks have been widely explored for spoken language understanding. For instance, Recurrent Neural Networks have been proposed in (Yao et al., 2013; Mesnil et al., 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014). A combination of neural networks and triangular CRFs is presented in (Celikyilmaz and Hakkani-Tur, 2010), in which a convolutional neural network is used for extracting the input features of a triangular CRF in order to perform joint intent detection and slot filling. All these models use word-level semantic annotations. However, providing these word-level semantic annotations is costly since it requires specialised annotators. (Zhou and He, 2011) has proposed learning CRFs from unaligned data, however they use manually tuned lexical or syntactic features. In this work we avoid the need for word-level annotation by exploiting distributed word embeddings and using deep learning for feature representation.\nConvolutional Neural Networks (CNNs) have been used previously for sentiment analysis (Kim, 2014; Kalchbrenner et al., 2014) and in this work we explore a similar CNN to the one presented by Kim (2014) for generating a sentence representation. However unlike Kim (2014), the input in not a single well formed sentence but a set of ill-formed ASR hypotheses. Additionally, the softmax layer used for binary classification (i.e., positive or negative sentiment) is replaced by a softmax layer for multiclass dialogue act prediction and a further softmax layer is added for each distinct slot in the domain. (Chen and He, 2015) proposed a CNN for generating intent embeddings in SLU, which uses tri-letter input vectors. Instead, in this paper the models are initialised with GloVe word embeddings (Pennington et al., 2014). These GloVe embeddings were trained in an unsupervised fashion on a large amount of data to model the contextual similarity and correlation between words. Chen and He\u2019s model aims to learn the embeddings for utterances and intents such that utterances with similar intents are close to each other in the continuous space. Although we share the same spirit, we use sentence embeddings not only for intent\ncreativecommons.org/licenses/by/4.0/\n(or dialogue act) recognition but also for slot-filling within a dialogue system and we combine them with embeddings for dialogue context.\nApproaches for adaptive SLU have been proposed in (Ferreira et al., 2015; Zhu et al., 2014), however they focused more on domain adaptation on top of an existing SLU component. Moreover, they use classical discriminative models for SLU such as CRFs and SVMs that require manually designed features. In contrast, the focus of this paper is to exploit deep learning models for SLU, which learn feature representations automatically.\nRecently, some researchers have focused on mapping word level hypotheses directly to beliefs without using an explicit semantic decoder step (Henderson et al., 2014a; Mrks\u030cic\u0301 et al., 2015). These systems track the user\u2019s goal through the course of the dialogue by maintaining a distribution over slot-value pairs. Such systems are interesting, but it is not clear that they can be scaled to very large domains due to the constraint of delexicalisation. Furthermore, they still require an explicit semantic decoding layer for domain identification and general Topic Management."}, {"heading": "3 Deep Learning Semantic Decoder", "text": "We split the task of semantic decoding into two steps: (i) training a joint model for predicting the dialogue act and presence or absence of slots and (ii) predicting the values for the most probable slots detected in (i). As shown in Figure 2, we use the same deep learning architecture in both steps for combining sentence and context representations to generate the final hidden unit that feeds one or many softmax layers. In the first step, as shown in the Figure, there are distinct softmax layers for the joint optimisation of the dialogue act and each possible slot. In the second step there is a single softmax layer that predicts the value of each specific slot. In the following we explain this architecture in more detail."}, {"heading": "3.1 Sentence Representation", "text": "A CNN is used for generating the hypothesis representation, then these representations are weighted by their confidence scores and then summed up to obtain the sentence representation (Figure 3).\nThe CNN is a variant of (Kim, 2014), in which the inputs are the word vectors in each ASR hypothesis. Let xi be a k\u2212dimensional word embedding for the i-th word in a hypothesis. A hypothesis of length m is represented as: x1:m = x1 \u2295 x2 \u2295 ... \u2295 xm where \u2295\nis the concatenation operator. A convolutional operation is applied to a window of l words to produce a new feature.\nci = f(w \u00b7 xi:i+l\u22121 + b) (1)\nwhere f is the hyperbolic tangent function; w \u2208 Rlk is a filter applied to a window of l words and b \u2208 R is a bias term. The filter is applied to every window of words in the sentence to produce a feature map.\nc = [c1, c2, ..., cn\u2212l+1] (2)\nwith c \u2208 Rn\u2212l+1. A max pooling operation is then applied to give the maximum value c = max{c} as the representative feature for that filter. Multiple filters can be applied by varying the window size to obtain several adjacent features for a given hypothesis. These features f\u0302j for the hypothesis j \u2208 H are then multiplied by the ASR confidence score pj1 and summed over all ASR hypotheses to generate a representation for the sentence st (Equation 3), as shown in Figure 3.\nst = \u2211 j\u2208H f\u0302j \u2217 pj (3)"}, {"heading": "3.2 Context Representation", "text": "An LSTM (Hochreiter and Schmidhuber, 1997) is used for tracking the context implied by previous dialogue system actions. The top layer of this LSTM network then provides the context representation for decoding the current input utterance.\nAn LSTM is a sequence model that utilises a memory cell capable of preserving states over long periods of time. This cell is recurrently connected to itself and it has three multiplication units, an input gate, a forget gate and an output gate. These gating vectors are in [0,1]. The cell makes selective decisions about what information is preserved, and when to allow access to units, via gates that open and close. The LSTM transition equations are as follows:\nit = \u03c3(W (i) \u00b7 xt +U(i) \u00b7 ht\u22121 + b(i)),\nft = \u03c3(W (f) \u00b7 xt +U(f) \u00b7 ht\u22121 + b(f)), ot = \u03c3(W (o) \u00b7 xt +U(o) \u00b7 ht\u22121 + b(o)),\nut = tanh(W (u) \u00b7 xt +U(u) \u00b7 ht\u22121 + b(u)), ct = it \u2299 ut + ft \u2299\nct\u22121, ht = ot \u2299 tanh(ct)\n(4)\nwhere ht is the hidden unit at time step t, xt is the input at the current time step, b is a bias, \u03c3 is the logistic sigmoid function and \u2299 denotes elementwise multiplication.\nAs shown in Figure 1, system actions are encoded in the form of a system dialogue act plus one or more slot-value pairs. To track the history of system actions, slots and values are treated as words and the input xt is formed from its corresponding word vectors. The length of the context can vary. We consider\n1The posterior probability of hypothesis j in the N-best list.\nall the system actions previous to the current user utterance, or a window l of the previous system actions. For instance, if we are currently processing the last user input in Figure 1, in which L is the total number of system actions, we can consider all previous system actions (L=4), or the last l system actions, where l < L."}, {"heading": "3.3 Combining Sentence and Context", "text": "We study in this paper two ways of combining the sentence st and the context ht representations. The first straightforward way is to apply a non linear function to their weighted sum:\nh\u0302t = tanh(Ws \u00b7 st +Wc \u00b7 ht) (5)\nThe second way is to let the sentence representation be the last input to the LSTM network, then h\u0302t = ht. For classification a softmax layer is used for each prediction:\nP (Y = k|h\u0302,W, b) = e (Wkh\u0302+bk)\u2211\nk\u2032 e (Wk\u2032h\u0302+bk\u2032)\n(6)\nwhere k is the index of the output neuron representing one class. For dialogue act classification k is one of the possible values: inform, request, offer, ... etc. For the slot prediction k is either 0 for absent or 1 for present. For slot-value prediction k will correspond to one of the possible values for each slot. For instance, for the slot price-range the possible values are cheap, moderate, expensive and dontcare. The result of the prediction is the most probable class:\ny\u0302 = argmaxk(P (Y = k|h\u0302,W, b)) (7)\nThe back-propagation optimisation is done by minimising the negative log-likelihood loss function through stochastic gradient descent."}, {"heading": "4 Experimental Evaluation", "text": "In this section we introduce the corpora, and describe the experiments performed and the evaluation metrics used."}, {"heading": "4.1 Corpora", "text": "Experimental evaluation used two similar datasets: DSTC2 (Henderson et al., 2014b) and In-car (Tsiakoulis et al., 2012). Both corpora were collected using a spoken dialogue system which provides restaurant information system for the city of Cambridge. Users can specify restaurant suggestions by area, pricerange and food type and can then query the system for additional restaurant specific information such as phone number, post code and address. The first dialogue corpus was released for the dialogue state tracking challenge and we use here the semantic annotations that were also provided 2. The trainset has 2118 dialogues and 15611 turns in total while the testset has 1117 dialogues and 9890 turns in total.\nThe second corpus contains dialogues collected under various noisy in-car conditions. In a stationary car with the air conditioning fan on and off, in a moving car and in a car simulator (Tsiakoulis et al., 2012) 3. The trainset has 1508 dialogues and 10532 turns in total and the testset has 641 dialogues and 4861 turns in total. Because of the noise, the average word error rate (WER = 37%) is significantly higher than for DSTC2 (around 29%)."}, {"heading": "4.2 Hyperparameters and Training", "text": "Dropout was used on the penultimate layers of both the CNN and the LSTM networks to prevent coadaptation of hidden units by randomly dropping out a proportion of the hidden units during forward propagation (Hinton et al., 2012). The models were implemented in Theano (Bastien et al., 2012). We\n2The DSTC2 corpus is publicly available in: http://camdial.org/\u02dcmh521/dstc/ 3This corpus has been obtained in an industry funded project and therefore it is not available for public use.\nused filter windows of 3, 4, and 5 with 100 feature maps each for the CNN. A dropout rate of 0.5 and a batch size of 50 was employed, 10% of the trainset was used as validation set and early stopping was adopted. Training is done through stochastic gradient descent over shuffled mini-batches with Adadelta update rule (we used an adadelta decay parameter of 0.95). To initialise the models, GloVE word vectors were used (Pennington et al., 2014) with a dimension d = 100. System-action word-embeddings are tuned during training, instead hypothesis word-embeddings are not because of the heavy computations."}, {"heading": "4.3 Experiments", "text": "Step I: Joint classification of dialogue-acts and slots: We evaluated five different model configurations for the joint classification of dialogue-acts and presence or absence of slots.\n\u2022 CNN: the softmax layers for the joint classification of dialogue acts and slots are connected directly to the CNN sentence representation with no context.\n\u2022 CNN+LSTM: we study the influence of context by considering the previous system actions (Section 3.2, Eq. 5), here we study the different context length, by using a context window of 1, 4, and all the previous system actions, namely CNN+LSTM w1, CNN+LSTM w4 and CNN+LSTM w respectively.\n\u2022 LSTM all: Finally, we study the impact of long distance dependencies, by using mainly the LSTM model, with the previous system actions as input, but we inject the sentence representation as the last LSTM input.\nStep II: Classification of slot value pairs: We select the best model in step I for predicting the presence of slots, then for each slot present we predict the value, by using again the best architecture from the previous step."}, {"heading": "4.4 Evaluation Metrics", "text": "We evaluate the performance of our models by using the conventional metrics for classification, namely accuracy, precision, recall and F-measure (F1-score).\nIn addition, we used the ICE score (Eq. 8) between the hypotheses and the reference semantics (ie. ground-truth) to measure the overall quality of the distribution returned by the models(Thomson et al., 2008). Let U be the number of utterances and W be the number of available semantic items. Given u = 1..U and w = 1...W , let:\ncuw = { p, the confidence assigned to the hypothesis that the wth semantic item is part of utterance u, 0, if none was assigned.\n\u03b4uw = { 1, if the wth item is in the reference semantics for u, 0, otherwise\nand N = \u2211\nuw \u03b4uw, be the total number of semantic items in the reference semantics.\nICE = 1Nw \u2211 \u2212 log(\u03b4uwcuw + (1\u2212 \u03b4uw)(1\u2212 cuw)\n(8)"}, {"heading": "5 Results and Discussion", "text": "In this section we report the results on DSTC2 and In-car dialogue corpora.\nStep I: Joint classification of dialogue-acts and slots: For this step, the classifiers must predict jointly 14 dialogue acts and 5 slots for the DSTC2 dataset as well as 14 dialogue acts and 7 slots for the In-car dataset. We evaluate both (i) using 10 fold cross-validation on the trainsets and (ii) on the corpora\u2019 testsets.\nTable 1 shows the 10 fold cross-validation results on both corpora. These results suggest that for DTSC2, the context representation is not significantly impacting the prediction. Although, the model with a window of 4 ,CNN+LSTM w4, improves slightly the accuracy and f1-score. On the In-car dataset, however, including the context does help to disambiguate the semantic predictions from illformed hypotheses. This is expected, since this data set has a much higher error rate and hence higher levels of confusion in the ASR output. Although there is no significant difference on the f1-score when using the immediate previous system act (w1) or a longer context, CNN+LSTM w gives a better accuracy and a lower ICE score on this dataset.\nTable 2 shows the results on the test sets. Consequently, when evaluating on the DSTC2 test set, a window of 4 (w4), performs slightly better than other window sizes and better than the simple CNN model. On the In-car testset, a context window of 4 outperforms all the other settings: CNN+LSTM. However, on this test set using the sentence representation as the last input to the LSTM context neural network (section 3.3) improves the f1-score and reduces the ICE error.\nStep II: Prediction of slot value pairs For evaluating Step II, we selected the best model obtained during the 10-fold cross-validation experiments in terms of F1 score. For both corpora, this was the CNN+LSTM w4 configuration. For DSTC2, it was the 4th-fold crossvalidation with Acc = 90.42%,\nF1 = 88.69% and ICE = 0.251. For In-car, it was the 5th-fold crossvalidation with Acc = 93.13%, F1 = 81.49% and ICE = 0.393. We used these models to classify whether a given slot appears in a given hypothesis or not. Then for that slot, we train another CNN+LSTM w4 classifier for predicting its values. In the In-car corpus the slot \u201dtype\u201d has only one possible value \u201drestaurant\u201d. Similarly, the slot \u201dtask\u201d can only be the value \u201dfind\u201d. For these slots with only one value, we report values using the model of Step I, since it is enough to detect the slot in the utterance.\nGiven that there is no domain specific delexicalisation, the models achieve a good level of performance overall (Table 3). Note that the slot \u201dfood\u201d has 74 possible values in DSTC2 and 25 in In-car. Hence, this slot has much higher cardinality than all the other slots.\nOverall performance A baseline for assessing overall performance is provided by the model presented in (Henderson et al., 2012), in which the vector representation is obtained by summing up the frequency of n-grams extracted from the 10-best hypotheses, weighted by their confidence scores. Here we compare our performance against Henderson\u2019s model with and without context features, namely WNGRAMS+Ctxt and WNGRAMS repectively. Henderson reported his results on the In-car dataset. A similar model, namely SLU1, was evaluated on DSTC2 in (Williams, 2014). Both implementations consist of many binary classifiers for dialogue act and slot-value pairs.\nIn terms of the ICE score, the model CNN+LSTM W4 outperforms all the baselines (Table 4). In terms of the F1 score, the model significantly outperforms the SLU1 and WNGRAMS baselines. However it is slightly worse than WNGRAMS+Ctxt, which has been enhanced with context features on In-car. Remember however, that our model uses only word-embeddings for automatically generating sentence and context representations without having any manually designed features or using explicit application specific semantic dictionaries.\n4\u201dSlot\u201d is used when no value is given for the slot (e.g., \u201dWhat kind of food do they serve?\u201d/request(slot=food)). 5\u201dThis\u201d is used for annotating elliptical utterances (e.g., \u201dI dont care\u201d/inform(this=\u2019dontcare\u2019))."}, {"heading": "6 Conclusion and Future Work", "text": "This paper has presented a deep learning architecture for semantic decoding in spoken dialogue systems that exploits semantically rich distributed word vectors. We compared different models for combining sentence and context representations. We found that context representations significantly impact slot F-measure on ASR hypotheses generated under very noisy conditions. The combination of sentence and context representations, with a context window of 4 words, outperforms all the baselines in terms of the ICE score. In terms of the F1 scores, our model outperforms the baseline on the DSTC2 corpus and the baseline without manually designed features on the In-car corpus. Although the F-score of our model does not outperforms the baseline enriched with context features on the In-car corpus, the proposed model remains competitive, especially considering that our model requires no manually designed features or application specific semantic dictionaries."}, {"heading": "7 Future Work", "text": "Semantic distributed vector representations can be used for detecting similarity between domains. As future work, we want to study the adoption of the sentence and the contex representations generated in the Step I (i.e., the joint prediction of dialogue act and slots) within a Topic Management in multidomain dialogue systems. The Topic Manager is in charge of detecting the domain and the intention behind users\u2019 utterances. Furthermore, it would be interesting to study these embeddings for domain adaptation on potentially open-domains."}, {"heading": "Acknowledgments", "text": "This research was partly funded by the EP-SRC grant EP/M018946/1 Open Domain Statistical Spoken Dialogue Systems. The data used in this paper was produced in an industry funded project and it is not available for public use."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio."], "venue": "nips workshop on deep learning and unsupervised feature learning.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Convolutional neural network based semantic tagging with entity embeddings", "author": ["Asli Celikyilmaz", "Dilek Hakkani-Tur."], "venue": "genre.", "citeRegEx": "Celikyilmaz and Hakkani.Tur.,? 2010", "shortCiteRegEx": "Celikyilmaz and Hakkani.Tur.", "year": 2010}, {"title": "Learning bidirectional intent embeddings by convolutional deep structured semantic models for spoken language understanding", "author": ["Yun-Nung Chen", "Xiaodong He."], "venue": "Extended Abstract of The 29th Annual Conference on Neural Information Processing Systems\u2013Machine Learning for Spoken Language Understanding and Interactions Workshop (NIPS-SLU).", "citeRegEx": "Chen and He.,? 2015", "shortCiteRegEx": "Chen and He.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12(Aug):2493\u2013 2537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep belief network based semantic taggers for spoken language understanding", "author": ["Anoop Deoras", "Ruhi Sarikaya."], "venue": "INTERSPEECH, pages 2713\u20132717.", "citeRegEx": "Deoras and Sarikaya.,? 2013", "shortCiteRegEx": "Deoras and Sarikaya.", "year": 2013}, {"title": "Online adaptative zero-shot learning spoken language understanding using word-embedding", "author": ["Emmanuel Ferreira", "Bassam Jabaian", "Fabrice Lef\u00e8vre."], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5321\u20135325. IEEE.", "citeRegEx": "Ferreira et al\\.,? 2015", "shortCiteRegEx": "Ferreira et al\\.", "year": 2015}, {"title": "Discriminative Spoken Language Understanding Using Word Confusion Networks", "author": ["Matthew Henderson", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Pirros Tsiakoulis", "Kai Yu", "Steve Young."], "venue": "Spoken Language Technology Workshop, 2012. IEEE.", "citeRegEx": "Henderson et al\\.,? 2012", "shortCiteRegEx": "Henderson et al\\.", "year": 2012}, {"title": "Word-based Dialog State Tracking with Recurrent Neural Networks", "author": ["M. Henderson", "B. Thomson", "S.J. Young."], "venue": "Proceedings of SIGdial.", "citeRegEx": "Henderson et al\\.,? 2014a", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "The second dialog state tracking challenge", "author": ["Matthew Henderson", "Blaise Thomson", "Jason Williams."], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, volume 263.", "citeRegEx": "Henderson et al\\.,? 2014b", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv preprint arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1404.2188.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Mesnil et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2015}, {"title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young."], "venue": "Proceedings of ACL.", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? 2015", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Deep belief nets for natural language call-routing", "author": ["R. Sarikaya", "G.E. Hinton", "B. Ramabhadran."], "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5680\u20135683, May.", "citeRegEx": "Sarikaya et al\\.,? 2011", "shortCiteRegEx": "Sarikaya et al\\.", "year": 2011}, {"title": "Application of deep belief networks for natural language understanding", "author": ["Ruhi Sarikaya", "Geoffrey E Hinton", "Anoop Deoras."], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4):778\u2013 784.", "citeRegEx": "Sarikaya et al\\.,? 2014", "shortCiteRegEx": "Sarikaya et al\\.", "year": 2014}, {"title": "Evaluating semantic-level confidence scores with multiple hypotheses", "author": ["Blaise Thomson", "Kai Yu", "Milica Gasic", "Simon Keizer", "Francois Mairesse", "Jost Schatzmann", "Steve J Young."], "venue": "INTERSPEECH, pages 1153\u2013 1156.", "citeRegEx": "Thomson et al\\.,? 2008", "shortCiteRegEx": "Thomson et al\\.", "year": 2008}, {"title": "Statistical methods for building robust spoken dialogue systems in an automobile", "author": ["Pirros Tsiakoulis", "Milica Ga\u0161ic", "Matthew Henderson", "Joaquin Planells-Lerma", "Jorge Prombonas", "Blaise Thomson", "Kai Yu", "Steve Young", "Eli Tzirkel."], "venue": "Proceedings of the 4th applied human factors and ergonomics.", "citeRegEx": "Tsiakoulis et al\\.,? 2012", "shortCiteRegEx": "Tsiakoulis et al\\.", "year": 2012}, {"title": "Semantic parsing using word confusion networks with conditional random fields", "author": ["G\u00f6khan T\u00fcr", "Anoop Deoras", "Dilek Hakkani-T\u00fcr."], "venue": "INTERSPEECH, pages 2579\u20132583.", "citeRegEx": "T\u00fcr et al\\.,? 2013", "shortCiteRegEx": "T\u00fcr et al\\.", "year": 2013}, {"title": "Web-style ranking and slu combination for dialog state tracking", "author": ["Jason D Williams."], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 282\u2013291.", "citeRegEx": "Williams.,? 2014", "shortCiteRegEx": "Williams.", "year": 2014}, {"title": "Recurrent neural networks for language understanding", "author": ["Kaisheng Yao", "Geoffrey Zweig", "Mei-Yuh Hwang", "Yangyang Shi", "Dong Yu."], "venue": "INTERSPEECH, pages 2524\u20132528.", "citeRegEx": "Yao et al\\.,? 2013", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Learning conditional random fields from unaligned data for natural language understanding", "author": ["Deyu Zhou", "Yulan He."], "venue": "European Conference on Information Retrieval, pages 283\u2013288. Springer.", "citeRegEx": "Zhou and He.,? 2011", "shortCiteRegEx": "Zhou and He.", "year": 2011}, {"title": "Semantic parser enhancement for dialogue domain extension with little data", "author": ["Su Zhu", "Lu Chen", "Kai Sun", "Da Zheng", "Kai Yu."], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 336\u2013341. IEEE.", "citeRegEx": "Zhu et al\\.,? 2014", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 13, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 22, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 16, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 4, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 17, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 23, "context": "This problem was tackled in (Zhou and He, 2011), by using conditional random fields (CRFs) driven by finely-tuned hand-crafted features.", "startOffset": 28, "endOffset": 47}, {"referenceID": 6, "context": "Other discriminative approaches that deal with unaligned data use some form of delexicalisation or mapping of the input to known ontological concepts (Henderson et al., 2012; Henderson et al., 2014a).", "startOffset": 150, "endOffset": 199}, {"referenceID": 7, "context": "Other discriminative approaches that deal with unaligned data use some form of delexicalisation or mapping of the input to known ontological concepts (Henderson et al., 2012; Henderson et al., 2014a).", "startOffset": 150, "endOffset": 199}, {"referenceID": 3, "context": "A convolutional neural network (CNN) (Collobert et al., 2011) is used to generate the sentence representation, while a longshort term memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to generate the context representation.", "startOffset": 37, "endOffset": 61}, {"referenceID": 10, "context": ", 2011) is used to generate the sentence representation, while a longshort term memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to generate the context representation.", "startOffset": 102, "endOffset": 136}, {"referenceID": 8, "context": "Our models are evaluated on two datasets DSTC2 (Henderson et al., 2014b) and In-car (Tsiakoulis et al.", "startOffset": 47, "endOffset": 72}, {"referenceID": 19, "context": ", 2014b) and In-car (Tsiakoulis et al., 2012) using accuracy, f-measure and the Item Cross Entropy (ICE) score (Thomson et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 18, "context": ", 2012) using accuracy, f-measure and the Item Cross Entropy (ICE) score (Thomson et al., 2008).", "startOffset": 73, "endOffset": 95}, {"referenceID": 22, "context": "For instance, Recurrent Neural Networks have been proposed in (Yao et al., 2013; Mesnil et al., 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al.", "startOffset": 62, "endOffset": 101}, {"referenceID": 13, "context": "For instance, Recurrent Neural Networks have been proposed in (Yao et al., 2013; Mesnil et al., 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al.", "startOffset": 62, "endOffset": 101}, {"referenceID": 16, "context": ", 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 132, "endOffset": 205}, {"referenceID": 4, "context": ", 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 132, "endOffset": 205}, {"referenceID": 17, "context": ", 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 132, "endOffset": 205}, {"referenceID": 1, "context": "A combination of neural networks and triangular CRFs is presented in (Celikyilmaz and Hakkani-Tur, 2010), in which a convolutional neural network is used for extracting the input features of a triangular CRF in order to perform joint intent detection and slot filling.", "startOffset": 69, "endOffset": 104}, {"referenceID": 23, "context": "(Zhou and He, 2011) has proposed learning CRFs from unaligned data, however they use manually tuned lexical or syntactic features.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Convolutional Neural Networks (CNNs) have been used previously for sentiment analysis (Kim, 2014; Kalchbrenner et al., 2014) and in this work we explore a similar CNN to the one presented by Kim (2014) for generating a sentence representation.", "startOffset": 86, "endOffset": 124}, {"referenceID": 11, "context": "Convolutional Neural Networks (CNNs) have been used previously for sentiment analysis (Kim, 2014; Kalchbrenner et al., 2014) and in this work we explore a similar CNN to the one presented by Kim (2014) for generating a sentence representation.", "startOffset": 86, "endOffset": 124}, {"referenceID": 2, "context": "(Chen and He, 2015) proposed a CNN for generating intent embeddings in SLU, which uses tri-letter input vectors.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "Instead, in this paper the models are initialised with GloVe word embeddings (Pennington et al., 2014).", "startOffset": 77, "endOffset": 102}, {"referenceID": 1, "context": "A combination of neural networks and triangular CRFs is presented in (Celikyilmaz and Hakkani-Tur, 2010), in which a convolutional neural network is used for extracting the input features of a triangular CRF in order to perform joint intent detection and slot filling. All these models use word-level semantic annotations. However, providing these word-level semantic annotations is costly since it requires specialised annotators. (Zhou and He, 2011) has proposed learning CRFs from unaligned data, however they use manually tuned lexical or syntactic features. In this work we avoid the need for word-level annotation by exploiting distributed word embeddings and using deep learning for feature representation. Convolutional Neural Networks (CNNs) have been used previously for sentiment analysis (Kim, 2014; Kalchbrenner et al., 2014) and in this work we explore a similar CNN to the one presented by Kim (2014) for generating a sentence representation.", "startOffset": 70, "endOffset": 916}, {"referenceID": 1, "context": "A combination of neural networks and triangular CRFs is presented in (Celikyilmaz and Hakkani-Tur, 2010), in which a convolutional neural network is used for extracting the input features of a triangular CRF in order to perform joint intent detection and slot filling. All these models use word-level semantic annotations. However, providing these word-level semantic annotations is costly since it requires specialised annotators. (Zhou and He, 2011) has proposed learning CRFs from unaligned data, however they use manually tuned lexical or syntactic features. In this work we avoid the need for word-level annotation by exploiting distributed word embeddings and using deep learning for feature representation. Convolutional Neural Networks (CNNs) have been used previously for sentiment analysis (Kim, 2014; Kalchbrenner et al., 2014) and in this work we explore a similar CNN to the one presented by Kim (2014) for generating a sentence representation. However unlike Kim (2014), the input in not a single well formed sentence but a set of ill-formed ASR hypotheses.", "startOffset": 70, "endOffset": 984}, {"referenceID": 5, "context": "Approaches for adaptive SLU have been proposed in (Ferreira et al., 2015; Zhu et al., 2014), however they focused more on domain adaptation on top of an existing SLU component.", "startOffset": 50, "endOffset": 91}, {"referenceID": 24, "context": "Approaches for adaptive SLU have been proposed in (Ferreira et al., 2015; Zhu et al., 2014), however they focused more on domain adaptation on top of an existing SLU component.", "startOffset": 50, "endOffset": 91}, {"referenceID": 7, "context": "Recently, some researchers have focused on mapping word level hypotheses directly to beliefs without using an explicit semantic decoder step (Henderson et al., 2014a; Mrk\u0161i\u0107 et al., 2015).", "startOffset": 141, "endOffset": 187}, {"referenceID": 14, "context": "Recently, some researchers have focused on mapping word level hypotheses directly to beliefs without using an explicit semantic decoder step (Henderson et al., 2014a; Mrk\u0161i\u0107 et al., 2015).", "startOffset": 141, "endOffset": 187}, {"referenceID": 12, "context": "The CNN is a variant of (Kim, 2014), in which the inputs are the word vectors in each ASR hypothesis.", "startOffset": 24, "endOffset": 35}, {"referenceID": 10, "context": "2 Context Representation An LSTM (Hochreiter and Schmidhuber, 1997) is used for tracking the context implied by previous dialogue system actions.", "startOffset": 33, "endOffset": 67}, {"referenceID": 8, "context": "1 Corpora Experimental evaluation used two similar datasets: DSTC2 (Henderson et al., 2014b) and In-car (Tsiakoulis et al.", "startOffset": 67, "endOffset": 92}, {"referenceID": 19, "context": ", 2014b) and In-car (Tsiakoulis et al., 2012).", "startOffset": 20, "endOffset": 45}, {"referenceID": 19, "context": "In a stationary car with the air conditioning fan on and off, in a moving car and in a car simulator (Tsiakoulis et al., 2012) 3.", "startOffset": 101, "endOffset": 126}, {"referenceID": 9, "context": "2 Hyperparameters and Training Dropout was used on the penultimate layers of both the CNN and the LSTM networks to prevent coadaptation of hidden units by randomly dropping out a proportion of the hidden units during forward propagation (Hinton et al., 2012).", "startOffset": 237, "endOffset": 258}, {"referenceID": 0, "context": "The models were implemented in Theano (Bastien et al., 2012).", "startOffset": 38, "endOffset": 60}, {"referenceID": 15, "context": "To initialise the models, GloVE word vectors were used (Pennington et al., 2014) with a dimension d = 100.", "startOffset": 55, "endOffset": 80}, {"referenceID": 18, "context": "ground-truth) to measure the overall quality of the distribution returned by the models(Thomson et al., 2008).", "startOffset": 87, "endOffset": 109}, {"referenceID": 6, "context": "Overall performance A baseline for assessing overall performance is provided by the model presented in (Henderson et al., 2012), in which the vector representation is obtained by summing up the frequency of n-grams extracted from the 10-best hypotheses, weighted by their confidence scores.", "startOffset": 103, "endOffset": 127}, {"referenceID": 21, "context": "A similar model, namely SLU1, was evaluated on DSTC2 in (Williams, 2014).", "startOffset": 56, "endOffset": 72}, {"referenceID": 21, "context": "Corpus Model F1 ICE DSTC2 SLU1 (Williams, 2014) 80.", "startOffset": 31, "endOffset": 47}, {"referenceID": 6, "context": "In-car WNGRAMS (Henderson et al., 2012) 70.", "startOffset": 15, "endOffset": 39}, {"referenceID": 6, "context": "WNGRAMS+Ctxt (Henderson et al., 2012) 74.", "startOffset": 13, "endOffset": 37}], "year": 2016, "abstractText": "This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition. Most current models for spoken language understanding assume (i) word-aligned semantic annotations as in sequence taggers and (ii) delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation (e.g., morphology, synonyms, paraphrasing ). In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation. The proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation. Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate (WER).", "creator": "TeX"}}}