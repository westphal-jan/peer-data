{"id": "1610.06486", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Adaptive Forecasting of Non-Stationary Nonlinear Time Series Based on the Evolving Weighted Neuro-Neo-Fuzzy-ANARX-Model", "abstract": "An grasp benchmark neuro - neo - bark - ANARX model and its cognitive examining are introduced in but article. This system become basically directly they gone series estimates. This problem or have but only next fill of elements that required statistics city yet defined engaging. The proposed reshaped changes may limited online device retrieval habitats.", "histories": [["v1", "Thu, 20 Oct 2016 16:28:43 GMT  (394kb)", "http://arxiv.org/abs/1610.06486v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["zhengbing hu", "yevgeniy v bodyanskiy", "oleksii k tyshchenko", "olena o boiko"], "accepted": false, "id": "1610.06486"}, "pdf": {"name": "1610.06486.pdf", "metadata": {"source": "CRF", "title": "Adaptive Forecasting of Non-Stationary Nonlinear Time Series based on the Evolving Weighted Neuro-Neo-Fuzzy- ANARX-Model", "authors": ["Zhengbing Hu", "Yevgeniy V. Bodyanskiy"], "emails": ["hzb@mail.ccnu.edu.cn", "yevgeniy.bodyanskiy@nure.ua", "lehatish@gmail.com,", "olena.boiko@ukr.net"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nMathematical forecasting of data sequences (time series) is nowadays well studied and there is a large number of publications on this topic. There are many methods for solving this task: regression, correlation, spectral analysis, exponential smoothing, etc., and more advanced intellectual systems that sometimes require rather complicated mathematical methods and high user\u2019s qualification. The problem becomes more complicated when analyzed time series are non-stationary and nonlinear and contain unknown behavior trends, quasiperiodic, stochastic and chaotic components. The best results are shown by nonlinear forecasting models based on mathematical methods of computational intelligence [1-3], and, first of all, neuro-fuzzy systems [4-5] due to their approximating and extrapolating properties, learning abilities, transparency and results\u2019 interpretability. The models to be especially noted are the so-called NARX-models [6] which have the form\n          \u02c6 1 , ..., , 1 , ...,y xy k f y k y k n x k x k n     (1) where  y\u0302 k is an estimate of forecasted time series at discrete time 1, 2, ...k  ;  f  stands for a certain nonlinear transformation implemented by a neuro-fuzzy system,  x k is an observed exogenous factor that defines a behavior of  y k . It can be noticed that popular Box\u2013Jenkins AR-, ARX-, ARMAX-models as well as nonlinear NARMAmodels can be described by the expression (1). These models have been widely studied; there are many architectures and learning algorithms that implement these models, but it is assumed that models\u2019 orders yn , xn are given a priori. These orders are previously unknown in a case of structural non-stationarity for analyzed time series, and they also have to be adjusted during a learning procedure. In this case, it makes sense to use evolving connectionist systems [7-10] that adjust not only their synaptic weights and activation-membership functions, but also their architectures. There are many algorithms that implement these learning methods both in a batch mode and in a sequential mode. The problem becomes more complicated if data are fed to the system with high frequency in the form of a data stream [11]. Here, the most popular evolving systems turn out to be too cumbersome for learning and information processing in an online mode.\nAs an alternative, a rather simple and effective architecture can be considered. It\u2019s the so-called ANARX-model (Additive NARX) that has the form [12, 13]\n           \n          1 2\n1\n\u02c6 1 , 1 2 , 2 ...\n... , , n\nn l l\ny k f y k x k f y k x k\nf y k n x k n f y k l x k l \n      \n      (2)\n(here  max ,y xn n n ), an original task of the forecasting system\u2019s synthesis is decomposed into many local tasks of parametric identification for node models with two input variables  y k l ,  x k l , 1, 2, ..., , ...l n .\nAuthors [12, 13] used elementary Rosenblatt perceptrons with sigmoidal activation functions as such nodes. The ANARX-model provided high forecasting quality, but generally speaking it requires a large number of nodes in its architecture [14].\nSome synthesis problems of forecasting neuro-fuzzy [15] and neo-fuzzy [15-17] systems based on the ANARXmodels are considered in this work. These systems avoid the above mentioned drawbacks.\nSince we consider a case of stochastic nonlinear dynamic signals in this article, the basic novelty has to do with defining a model\u2019s delay order in an online mode.\nII. A NEURO-FUZZY-ANARX-MODEL An architecture of the ANARX-model is shown in Fig.1. It is formed by two lines of time delay elements 1z     1 1z y k y k   and n nodes [ ]lN which are simultaneously learned. These nodes are tuned independently\nfrom each other. And adding new nodes or removing unnecessary ones doesn\u2019t have any influence on other neurons, i.e. the evolving process for this system is implemented by changing a number of the nodes.\nIt\u2019s recommended to use a neuron with two inputs (its architecture is shown in Fig.2) as a node of this system instead of the elementary Rosenblatt perceptron.\nAs one can see, this node is the Wang\u2013Mendel neuro-fuzzy system [18] with two inputs. It possesses universal approximation capabilities and is actually the zero-order Takagi\u2013Sugeno\u2013Kang system [19, 20]. A two-dimensional vector of input signals       T,lz k y k l x k l   is fed to an input of the node [ ]lN , 1, 2, ..., , ...l n . The first layer contains 2h membership functions   iy y k l  ,   ix x k l  and fulfills fuzzification of the input variables by calculating membership levels   0 1iy y k l   ,   0 1ix x k l   . The Gaussian functions can be used as membership functions\n      2\n2exp 2 iy\niy iy\ny k l c y k l\n           ,\n      2\n2exp 2 ix\nix ix\nx k l c x k l\n          \n(here iyc , ixc stand for parameters that define centers of these membership functions; iy , ix are width parameters) or other bell-shaped functions with infinite support for avoiding \u201cgaps\u201d which appear in the fuzzified space.\nThe second layer of the node provides aggregation of the membership levels that were computed in the first layer. Outputs of the second layer form h aggregated signals\n       li iy ixz k y k l x k l    . The third layer contains synaptic weights that are adjusted during a learning procedure. Outputs of the third layer\nare values        l l li iy ix i iw y k l x k l w z k     .\nThe fourth layer is formed by two summation units and calculates sums of the output signals in the second and the third layers correspondingly. Outputs of the fourth layer are signals\n       \n        1 1 1 1\n,\n.\nh h l l l i iy ix i i\ni i h h\nl iy ix i\ni i\nw y k l x k l w z k\ny k l x k l z k\n \n \n \n \n               \nDefuzzification (normalization) is implemented in the fifth (output) layer. Finally, the output signal of the node       \u02c6 ,l ly k f y k l x k l   is computed:\n       \n       \n     \n1\n1\nT\n1 1\n1\n\u02c6\nh l i iy ix\nl i h\niy ix i lh h il l l l l\ni i ih li i i\ni\nw y k l x k l y k\ny k l x k l\nz k w w k w k\nz k\n \n \n \n\n\n \n\n   \n \n  \n\n\n  \n\n\nwhere         T1 2, , ...,l l l lhk k k k    ,       1\n1\nh l l l i i i\ni k z k z k\n\n\n         ,  T1 2, , ...,l l l lhw w w w .\nConsidering that the output signal  \u02c6 ly k of each node depends linearly on the adjusted synaptic weights liw , one can use conventional algorithms of adaptive linear identification [21] for their tuning which are based on the quadratic learning criterion.\nIf a training data set is non-stationary [22], one can use either the exponentially weighted recurrent least squares method\n              \n                 \n     \n1\n1 1 ,\n1\n1\n1 1 , 0 1,\n1\nl l\nl lT l l\nlT l l\nl l\nl l lT l\nlT l l\nw k w k\nk y k w k k k\nk k k\nk k\nk k k k k k k\n \n  \n  \n  \n                            \n(3)\nor the Kaczmarz\u2013Widrow\u2013Hoff optimal gradient algorithm (in a case of the \u201crapid\u201d non-stationarity)\n               1 1 lT l l l l lT l y k w k k w k w k k k k          . (4)\nIn fact, it is possible to tune 4h membership functions\u2019 parameters iyc , ixc , iy , ix of each node, but taking\ninto consideration the fact that the signal  \u02c6 ly k depends nonlinearly on these parameters, a learning speed can\u2019t be sufficient for non-stationary conditions in this case.\nIII. A NEO-FUZZY-ANARX-MODEL\nIf a large data set to be processed is given (within the \u201cBig Data\u201d conception [23] when data processing speed and computational simplicity come to the forefront, it seems reasonable to use neo-fuzzy neurons that were proposed by T. Yamakawa and his co-authors [15-17] instead of the neuro-fuzzy nodes in the ANARX-model. An architecture of the neo-fuzzy neuron as a node of the ANARX-model is shown in Fig.3. The neo-fuzzy neuron\u2019s advantages are a high learning speed, computational simplicity, good approximating properties and abilities to find a global minimum of a learning criterion in an online mode.\nStructural elements of the neo-fuzzy neuron are nonlinear synapses yNS , xNS that implement the zero order\nTakagi\u2013Sugeno fuzzy inference, but it\u2019s easy to notice that the neo-fuzzy neuron is much simpler constructively than the neuro-fuzzy node shown in Fig.2.\nAn output value of this node is formed with the help of input signals  y k l ,  x k l\n        \n   1\n1\n\u02c6 h\nl l l l y x iy iy\ni h\nl ix ix\ni\ny k k k w y k l\nw x k l\n  \n\n\n\n    \n \n\n (5)\nand an output signal of the ANARX-model can be written down in the form\n        1 1 1\n\u02c6 n h h\nl l iy iy ix ix l i i y k w y k l w x k l    \n      \n     ,\nwhich means that since the neo-fuzzy neuron is also an additive model [24], the ANARX-model based on the neofuzzy neurons is twice additive.\nTriangular membership functions are usually used in the neo-fuzzy neuron. They meet the unity partitioning conditions\n   1\n1 h\niy i y k l    ,    1\n1 h\nix i x k l    , which make it possible to simplify the node\u2019s architecture by excluding the normalization layer from it.\nIt was proposed to use B-splines as membership functions for the neo-fuzzy neuron [25]. They provide a higher approximation quality and also meet the unity partitioning conditions. A B-spline of the q  th order can be written down:\n  \n \n    \n    \n1,\n1\n1,\n, 1 1,\n, 1,\n1, if , for 1 0 otherwise\n, for 1\n1, ..., ,\niy i y\niy q iy\ni q y iyq iy\ni q y q i y\ni q y i y\nc y k l c q\ny k l c y k l\nc c y k l\nc y k l y k l q\nc c\ni h q\n\n\n\n\n\n \n  \n \n            \n    \n            \n  \n \n    \n    \n1,\n1\n1,\n, 1 1,\n, 1,\n1, if , for 1 0 otherwise\n, for 1\n1, ..., .\nix i x\nix q ix\ni q x ixq ix\ni q x q i x\ni q x i x\nc x k l c q\nx k l c x k l\nc c x k l\nc x k l x k l q\nc c\ni h q\n\n\n\n\n\n \n  \n \n            \n    \n            \nIt should be mentioned that B-splines are a sort of generalized membership functions: when 2q  one gets traditional triangular membership functions; when 4q  one gets cubic splines, etc.\nIntroducing a vector of variables               T1 1, ..., , , ...,l y hy x hxk y k l y k l x k l x k l         ,  T1 1, ..., , , ...,l l l l ly hy x hxw w w w w , the expression (5) can be rewritten in the form    T\u02c6 l l ly k w k .\nEither the algorithms (3), (4) or the procedure [26]                        1 T T 1 1 1 , 0 1, l l l l l l l l l l w k w k r k y k w k k k r k r k k k                   (6)\ncan be used for the neo-fuzzy neuron\u2019s learning. This procedure possesses both filtering and tracking properties. It should also be noticed that when 1  the equation (6) coincides completely with the Kaczmarz\u2013Widrow\u2013Hoff optimal algorithm (4).\nEvolving systems based on the neo-fuzzy neurons demonstrated its effectiveness for solving different tasks (especially forecasting tasks [25-30]). The twice additive system is the most appropriate choice for data processing in Data Stream Mining tasks [11] from a point of view of implementation simplicity and a processing speed.\nIV. A WEIGHTED NEURO-NEO-FUZZY-ANARX-MODEL Considering that every node [ ]lN of the ANARX-model is tuned independently from each other and represents\nan individual neuro(neo)-fuzzy system, it is possible to use a combination of neural networks\u2019 ensembles [31] in order to improve a forecasting quality. This approach results in an architecture of a weighted neuro-neo-fuzzyANARX-model (Fig.4).\nAn output signal of the system can be written in the form\n     T 1\n\u02c6\u02c6 \u02c6 n\nl l l y k c y k c y k \n  \nwhere         T1 2\u02c6 \u02c6 \u02c6 \u02c6, , ..., ny k y k y k y k ,  T1 2, , ..., nc c c c is a vector of adjusted weight coefficients that define proximity of the signals  \u02c6 ly k to the forecasted process  y k and meet the unbiasedness condition\nT\n1 1\nn\nl n l c c I   \nwhere nI is a  1n -vector of unities. The method of undetermined Lagrange multipliers can be used for finding the vector \u0441 in a batch mode. So, a\nsequence of forecasting errors\n                    \nT T T\nT T\n\u02c6 \u02c6\u02c6 \u02c6 n\nn\nv k y k y k y k c y k c I y k c y k\nc I y k y k c V k\n      \n  \n \n ,\nthe Lagrange function        \n \nT T T\nT T\n, 1\n1\nn k\nn\nL c c V k V k c c I\nc Rc c I\n \n\n   \n  \n (7)\n(here  is an undetermined Lagrange multiplier,    T k R V k V k  stands for an error correlation matrix) and the Karush\u2013Kuhn\u2013Tucker system of equations\n  T , 2 0,\n1 0\n\u0441 n\nn\nL c Rc I L c I            \n\n(8)\nare introduced. A solution of the system (8) can be written down in the form\n  11 T 1 T 1 ,\n2\nn n n\nn n\nc R I I R I\nI R I\n \n      \n(9)\nwhere a value of the Lagrange function (7) at the saddle point is\n    1T 1* , n nL c I R I  .\nAn implementation procedure of the algorithm (9) can meet some problems in a case of data processing in an online mode and high level of correlation between the signals  \u02c6 ly k that leads to the ill-conditioned matrix R which has to be inverted at every time step k .\nThe Lagrange function (7) can be written down in the form\n        2T T\u02c6, 1n k L c y k c y k c I      and a gradient algorithm for finding its saddle point based on the Arrow\u2013Hurwicz procedure [27, 32] can be written in the form\n       \n       \n1 , ,\n, 1\nc cc k c k k L c\nL c k k k\n \n   \n\n          \nor      \n                      \n        \nT\nT\n1\n\u02c6 \u02c62 1 1\n\u02c61 2 1 ,\n1 1\nc\nn\nc n\nn\nc k c k k\ny k c k y k y k k I\nc k k v k y k k I\nk k k c k I\n\n\n \n  \n                               \n(10)\nwhere  c k ,  k are learning rate parameters. The Arrow\u2013Hurwicz procedure converges to the saddle point with rather general assumptions on the values  c k ,  k , but these parameters can be optimized to speed up the learning process as that is particularly\nimportant in Data Stream Mining tasks. That\u2019s why the first ratio in the equation (10) should be multiplied by  Ty\u0302 k\n     \n       \nT T\n2 T\n\u02c6 \u02c6 1\n\u02c6 \u02c62 1c n\ny k c k y c k\nk v k y k k y I \n  \n      \n \n \nand an additional function that characterizes criterial convergence is introduced\n                  \n       \n2 T 2\n2 T\n22 2 T\n\u02c6\n\u02c6 \u02c62 2 1\n\u02c6 \u02c62 1\nc n\nc n\ny k y k c k v k\nk v k v k y k k y I\nk v k y k k y I\n \n \n  \n       \n      \n\n \n \n.\nA solution for the differential equation\n        \n       \n       \n2 T\n2 T\n22 T\n\u02c6\n\u02c6 \u02c62 2 1\n\u02c6 \u02c62 2 1 0\nc\nn\nc n\ny k y k c k\nk\nv k v k y k k y I\nk v k y k k y I\n\n\n \n  \n\n       \n       \n\n \n \ngives the optimal learning rate values  c k in the form\n          2 T\u02c6 \u02c62 1 c\nn\nv k k\nv k y k k y I        .\nApplying this value to the expression (10), final equations can be written down\n                  \n        \n2 T\nT\n\u02c62 1 1 ,\n\u02c6 \u02c62 1\n1 1 .\nn\nn\nn\nv k v k y k k I c k c k\nv k y k k y I\nk k k c k I\n\n\n  \n              \n\n  (11)\nIt\u2019s easy to notice that when  1 0k   the procedure (11) coincides with the Kaczmarz\u2013Widrow\u2013Hoff algorithm (4).\nV. EXPERIMENTS In order to prove the effectiveness of the proposed system, a number of experiments should be carried out."}, {"heading": "A. Electricity Demand", "text": "This data set describes 15 minutes averaged values of power demand in the full year 1997. Generally speaking, this data set contains 15000 points but we took only 5000 points for the experiment. 3000 points were selected for a training stage and 2000 points were used for testing. Prediction results of the ANARX and weighted ANARX\nsystems are in Fig.5 and Fig.6 correspondingly (signal values are marked with a blue color; prediction values are marked with a magenta color; and prediction errors are marked with a grey line).\nWe used for comparison multilayer perceptrons (MLPs), radial-basis function neural networks (RBFNs), ANFIS and two proposed systems ANARX and weighted ANARX (both based on neo-fuzzy nodes). Since MLP can\u2019t work in an online mode, it processed data in two different modes. It had just one epoch in the first case (something similar to an online case), and it had 5 epochs in the second case (the MLP architecture had almost the same number of adjustable parameters when compared to the proposed systems). A number of MLP inputs was equal to 4 and a number of hidden nodes was equal to 7 in both cases. A total number of parameters to be tuned was 43 in both modes. It took about two times more time to compute the result in the second case but prediction quality was like almost two times higher. MLP (case 2) demonstrated the best result in this experiment.\nSpeaking of RBFNs, we also had two cases. The first-case RBFN was taken really close in the sense of parameters\u2019 number to our systems and the second-case RBFN\u2019s architecture was chosen to show the best performance. In the first case, RBFN had 3 inputs and 7 kernel functions. In the second case, it had 3 inputs as well but 12 kernel functions which generally led to higher prediction quality (+30% precision compared to RBFN in the first case) but took longer to compute the result. A number of parameters to be tuned was 36 in the first case and 61 in the second case.\nANFIS showed one of the best prediction qualities in this experiment. It had 4 inputs, 55 nodes and it was processing data during 5 epochs. It contained 80 parameters to be tuned.\nThe proposed ANARX system based on neo-fuzzy nodes had 2 inputs, 2 nodes, 9 membership functions, and its  parameter was equal to 0.62. This system had 37 parameters. Its prediction quality was rather high and it was definitely one of the fastest system on this data set. We should also notice that we used B-splines (q=2, which means that we used triangular membership functions) as membership functions for the proposed systems.\nThe proposed weighted ANARX system based on neo-fuzzy nodes had 2 inputs, 2 nodes, 8 membership functions; its  parameter was equal to 0.9. It had 37 adjustable parameters. This system demonstrated better performance when compared to ANARX and the fastest results."}, {"heading": "B. Monthly sunspot number", "text": "This data set was taken from datamarket.com. The data set describes monthly sunspot number in Zurich. It was collected between 1749 and 1983. It contains 2820 points. 2256 points were selected for a training stage and 564 points were used for testing. Prediction results of the ANARX and weighted ANARX systems are in Fig.7 and Fig.8 correspondingly (signal values are marked with a blue color; prediction values are marked with a magenta color; and prediction errors are marked with a grey line).\nWe used a set of systems which is similar to the previous experiment to compare results. MLP_1 (MLP for the first case) had 3 inputs, 6 hidden nodes and it was processing data during just one epoch. MLP_2 basically had the same parameter settings except the fact that it was processing data during 5 epochs. Both MLP systems had 31 parameters to be tuned. MLP_1 was 2 times faster than MLP_2 but its prediction quality was also almost 2 times worse.\nLet\u2019s denote our RBFN architectures as RBFN_1 (RBFN for the first case) and RBFN_2 (RBFN for the second case). Both of them had 3 inputs. RBFN_1 had 4 kernel functions unlike RBFN_2 which had 19 hidden nodes. A number of parameters to be tuned was 21 in the first case and 96 in the second case. RBFN_2 showed a better prediction quality but it was much slower than RBFN_1.\nANFIS had 4 inputs and 55 nodes. It was processing data during 3 epochs. It had 80 adjustable parameters. The proposed ANARX system based on neo-fuzzy nodes had 2 inputs, 2 nodes, 4 membership functions, and its  parameter was equal to 0.9. This system contained 17 parameters. Its prediction quality was rather high and it was definitely one of the fastest systems on this data set.\nThe proposed weighted ANARX system based on neo-fuzzy nodes had 2 inputs, 2 nodes, 4 membership functions; its  parameter was equal to 0.9. This system contained 20 parameters to be tuned. This system demonstrated better performance when compared to ANARX and one of the fastest results.\nTABLE II. COMPARISON OF THE SYSTEMS\u2019 RESULTS\nSystems Parameters RMSE RMSE Time, s\nVI. CONCLUSION The evolving forecasting weighted neuro-neo-fuzzy-twice additive model and its learning procedures are proposed in the paper. This system can be used for non-stationary nonlinear stochastic and chaotic time series\u2019 forecasting where time series are processed in an online mode under the parametric and structural uncertainty. The proposed weighted ANARX-model is rather simple from a computational point of view and provides fast data stream processing in an online mode.\nSo, the proposed evolving forecasting model has demonstrated its efficiency for solving real-world tasks. A number of experiments has been performed to show high efficiency of the proposed neuro-neo-fuzzy system.\nACKNOWLEDGMENT\nThe authors would like to thank anonymous reviewers for their careful reading of this paper and for their helpful comments.\nThis scientific work was supported by RAMECS and CCNU16A02015.\nREFERENCES\n[1] Rutkowski L. Computational Intelligence. Methods and Techniques. Springer-Verlag, Berlin-Heidelberg, 2008. [2] Kruse R, Borgelt C, Klawonn F, Moewes C, Steinbrecher M, Held P. Computational Intelligence. Springer, Berlin, 2013. [3] Du K-L, Swamy M N S. Neural Networks and Statistical Learning. Springer-Verlag, London, 2014. [4] Jang J-S, Sun C-T, Mizutani E. Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and Machine\nIntelligence. Prentice Hall, Upper Saddle River, 1997. [5] Rezaei A, Noori L, Taghipour M. The Use of ANFIS and RBF to Model and Predict the Inhibitory Concentration Values\nDetermined by MTT Assay on Cancer Cell Lines. International Journal of Information Technology and Computer Science(IJITCS), 2016, 8(4): 28-34. [6] Nelles O. Nonlinear System Identification. Springer, Berlin, 2001. [7] Kasabov N. Evolving fuzzy neural networks \u2013 algorithms, applications and biological motivation. Proc. \u201cMethodologies for\nthe Conception, Design and Application of Soft Computing\u201d, Singapore, 1998:271-274. [8] Kasabov N. Evolving fuzzy neural networks: theory and applications for on-line adaptive prediction, decision making and\ncontrol. Australian J. of Intelligent Information Processing Systems, 1998, 5(3):154-160. [9] Kasabov N. Evolving Connectionist Systems. Springer-Verlag, London, 2003. [10] Lughofer E. Evolving Fuzzy Systems \u2013 Methodologies, Advanced Concepts and Applications. Springer, Berlin, 2011. [11] Bifet A. Adaptive Stream Mining: Pattern Learning and Mining from Evolving Data Streams. IOS Press, 2010, Amsterdam. [12] Belikov J, Vassiljeva K, Petlenkov E, N\u00f5mm S. A novel Taylor series based approach for control computation in NN-\nANARX structure based control of nonlinear systems. Proc. 27th Chinese Control Conference, Kunming, China, 2008:474-478. [13] Vassiljeva K, Petlenkov E, Belikov J. State-space control of nonlinear systems identified by ANARX and neural network based SANARX models. Proc. WCCI 2010 IEEE World Congress on Computational Intelligence, Barcelona, Spain, 2010:3816-3823. [14] Cybenko G. Approximation by superpositions of a sigmoidal function. Math. Control Signals Systems, 1989, 2:303-314. [15] Yamakawa T, Uchino E, Miki T, Kusanagi H. A neo fuzzy neuron and its applications to system identification and\nprediction of the system behavior. Proc. 2nd Int. Conf. on Fuzzy Logic and Neural Networks \u201cIIZUKA-92\u201d, Iizuka, Japan, 1992:477-483. [16] Uchino E, Yamakawa T. Soft computing based signal prediction, restoration, and filtering. In: Intelligent Hybrid Systems: Fuzzy Logic, Neural Networks, and Genetic Algorithms, 1997:331-349. [17] Miki T, Yamakawa T. Analog implementation of neo-fuzzy neuron and its on-board learning. In: Computational Intelligence and Applications, 1999:144-149. [18] Wang L-X, Mendel J M. Fuzzy basis functions, universal approximation, and orthogonal least-squares learning. IEEE Trans. on Neural Networks, 1992, 3(5):807-814. [19] Takagi T, Sugeno M. Fuzzy identification of systems and its applications to modeling and control. IEEE Trans. on Systems, Man, and Cybernetics, 1985, 15:116-132. [20] Sugeno M, Kang G T. Structure identification of fuzzy model. Fuzzy Sets and Systems, 1998, 28:15-33. [21] Ljung L. System Identification: Theory for the User. Prentice Hall, Inc., Upper Saddle River, 1987. [22] Polikar R, Alippi C. Learning in nonstationary and evolving environments. IEEE Trans. on Neural Networks and Learning\nSystems, 2014, 25(1):9-11. [23] Jin Y, Hammer B. Computational Intelligence in Big Data. IEEE Computational Intelligence Magazine, 2014, 9(3):12-13. [24] Friedman J, Hastie T, Tibshirani R. The Elements of Statistical Learning. Data Mining, Inference and Prediction. Springer,\nBerlin, 2003. [25] Bodyanskiy Ye, Kolodyazhniy V. Cascaded multiresolution spline-based fuzzy neural network. Proc. Int. Symp. on\nEvolving Intelligent Systems, Leicester, UK, 2010:26-29. [26] Bodyanskiy Ye, Kokshenev I, Kolodyazhniy V. An adaptive learning algorithm for a neo-fuzzy neuron. Proc. 3rd Int. Conf.\nof European Union Soc. for Fuzzy Logic and Technology (EUSFLAT\u201903), Zittau, Germany, 2003:375-379. [27] Bodyanskiy Ye, Otto P, Pliss I, Popov S. An optimal algorithm for combining multivariate forecasts in hybrid systems.\nLecture Notes in Artificial Intelligence, 2003, 2774:967-973. [28] Bodyanskiy Ye, Viktorov Ye. The cascade neo-fuzzy architecture using cubic-spline activation functions. Int. J. Information\nTheories and Applications, 2009, 16(3):245-259. [29] Bodyanskiy Ye, Teslenko N, Grimm P. Hybrid evolving neural network using kernel activation functions. Proc. 17th Zittau\nEast-West Fuzzy Colloquium, Zittau/Goerlitz, Germany, 2010:39-46. [30] Bodyanskiy Ye V, Tyshchenko O K, Kopaliani D S. A multidimensional cascade neuro-fuzzy system with neuron pool\noptimization in each cascade. Int. J. Information Technology and Computer Science, 2014, 6(8):11-17. [31] Sharkey A J C. On combining artificial neural nets. Connection Science, 1996, 8:299-313. [32] Bodyanskiy Ye, Deineko A, Stolnikova M. Adaptive generalization of neuro-fuzzy systems ensemble. Proc. of the Int. Conf.\n\u201cComputer Science and Information Technologies\u201d, Lviv, Ukraine, November 16-19, 2011:13-14."}], "references": [{"title": "Neural Networks and Statistical Learning", "author": ["Du K-L", "Swamy M N S"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and Machine Intelligence", "author": ["Jang J-S", "Sun C-T", "Mizutani E"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "The Use of ANFIS and RBF to Model and Predict the Inhibitory Concentration Values Determined by MTT Assay on", "author": ["A Rezaei", "L Noori", "M. Taghipour"], "venue": "Cancer Cell Lines. International Journal of Information Technology and Computer Science(IJITCS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Nonlinear System Identification", "author": ["O. Nelles"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Evolving fuzzy neural networks \u2013 algorithms, applications and biological motivation", "author": ["N. Kasabov"], "venue": "Proc. \u201cMethodologies for the Conception, Design and Application of Soft Computing\u201d,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Evolving fuzzy neural networks: theory and applications for on-line adaptive prediction, decision making and control", "author": ["N. Kasabov"], "venue": "Australian J. of Intelligent Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Evolving Connectionist Systems", "author": ["N. Kasabov"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Evolving Fuzzy Systems \u2013 Methodologies, Advanced Concepts and Applications", "author": ["E. Lughofer"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Adaptive Stream Mining: Pattern Learning and Mining from Evolving Data Streams", "author": ["A. Bifet"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "A novel Taylor series based approach for control computation in NN- ANARX structure based control of nonlinear systems", "author": ["J Belikov", "K Vassiljeva", "E Petlenkov", "S. N\u00f5mm"], "venue": "Proc. 27th Chinese Control Conference, Kunming,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "State-space control of nonlinear systems identified by ANARX and neural network based SANARX models", "author": ["K Vassiljeva", "E Petlenkov", "J. Belikov"], "venue": "Proc. WCCI 2010 IEEE World Congress on Computational Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Math. Control Signals Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "A neo fuzzy neuron and its applications to system identification and prediction of the system behavior", "author": ["T Yamakawa", "E Uchino", "T Miki", "H. Kusanagi"], "venue": "Proc. 2nd Int. Conf. on Fuzzy Logic and Neural Networks \u201cIIZUKA-92\u201d,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Soft computing based signal prediction, restoration, and filtering. In: Intelligent Hybrid Systems: Fuzzy Logic, Neural Networks, and Genetic Algorithms, 1997:331-349", "author": ["E Uchino", "T. Yamakawa"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Analog implementation of neo-fuzzy neuron and its on-board learning", "author": ["T Miki", "T. Yamakawa"], "venue": "Computational Intelligence and Applications,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Fuzzy basis functions, universal approximation, and orthogonal least-squares learning", "author": ["Wang L-X", "Mendel J M"], "venue": "IEEE Trans. on Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Fuzzy identification of systems and its applications to modeling and control", "author": ["T Takagi", "M. Sugeno"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1985}, {"title": "Structure identification of fuzzy model", "author": ["M Sugeno", "T. Kang G"], "venue": "Fuzzy Sets and Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "System Identification: Theory for the User", "author": ["L. Ljung"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1987}, {"title": "Learning in nonstationary and evolving environments", "author": ["R Polikar", "C. Alippi"], "venue": "IEEE Trans. on Neural Networks and Learning Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Computational Intelligence in Big Data", "author": ["Y Jin", "B. Hammer"], "venue": "IEEE Computational Intelligence Magazine,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "The Elements of Statistical Learning. Data Mining, Inference and Prediction", "author": ["J Friedman", "T Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Cascaded multiresolution spline-based fuzzy neural network", "author": ["Bodyanskiy Ye", "Kolodyazhniy V"], "venue": "Proc. Int. Symp. on Evolving Intelligent Systems, Leicester,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "An adaptive learning algorithm for a neo-fuzzy neuron", "author": ["Ye Bodyanskiy", "I Kokshenev", "V. Kolodyazhniy"], "venue": "Proc. 3rd Int. Conf. of European Union Soc. for Fuzzy Logic and Technology (EUSFLAT\u201903), Zittau,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "An optimal algorithm for combining multivariate forecasts in hybrid systems", "author": ["Ye Bodyanskiy", "P Otto", "I Pliss", "S. Popov"], "venue": "Lecture Notes in Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "The cascade neo-fuzzy architecture using cubic-spline activation functions", "author": ["Bodyanskiy Ye", "Viktorov Ye"], "venue": "Int. J. Information Theories and Applications,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Hybrid evolving neural network using kernel activation functions", "author": ["Ye Bodyanskiy", "N Teslenko", "P. Grimm"], "venue": "Proc. 17th Zittau East-West Fuzzy Colloquium, Zittau/Goerlitz,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "A multidimensional cascade neuro-fuzzy system with neuron pool optimization in each cascade", "author": ["V Bodyanskiy Ye", "K Tyshchenko O", "S. Kopaliani D"], "venue": "Int. J. Information Technology and Computer Science,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "On combining artificial neural nets", "author": ["C. Sharkey A J"], "venue": "Connection Science,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1996}, {"title": "Adaptive generalization of neuro-fuzzy systems ensemble", "author": ["Ye Bodyanskiy", "A Deineko", "M. Stolnikova"], "venue": "Proc. of the Int. Conf. \u201cComputer Science and Information Technologies\u201d, Lviv, Ukraine,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The best results are shown by nonlinear forecasting models based on mathematical methods of computational intelligence [1-3], and, first of all, neuro-fuzzy systems [4-5] due to their approximating and extrapolating properties, learning abilities, transparency and results\u2019 interpretability.", "startOffset": 119, "endOffset": 124}, {"referenceID": 1, "context": "The best results are shown by nonlinear forecasting models based on mathematical methods of computational intelligence [1-3], and, first of all, neuro-fuzzy systems [4-5] due to their approximating and extrapolating properties, learning abilities, transparency and results\u2019 interpretability.", "startOffset": 165, "endOffset": 170}, {"referenceID": 2, "context": "The best results are shown by nonlinear forecasting models based on mathematical methods of computational intelligence [1-3], and, first of all, neuro-fuzzy systems [4-5] due to their approximating and extrapolating properties, learning abilities, transparency and results\u2019 interpretability.", "startOffset": 165, "endOffset": 170}, {"referenceID": 3, "context": "The models to be especially noted are the so-called NARX-models [6] which have the form \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \u02c6 1 , .", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "In this case, it makes sense to use evolving connectionist systems [7-10] that adjust not only their synaptic weights and activation-membership functions, but also their architectures.", "startOffset": 67, "endOffset": 73}, {"referenceID": 5, "context": "In this case, it makes sense to use evolving connectionist systems [7-10] that adjust not only their synaptic weights and activation-membership functions, but also their architectures.", "startOffset": 67, "endOffset": 73}, {"referenceID": 6, "context": "In this case, it makes sense to use evolving connectionist systems [7-10] that adjust not only their synaptic weights and activation-membership functions, but also their architectures.", "startOffset": 67, "endOffset": 73}, {"referenceID": 7, "context": "In this case, it makes sense to use evolving connectionist systems [7-10] that adjust not only their synaptic weights and activation-membership functions, but also their architectures.", "startOffset": 67, "endOffset": 73}, {"referenceID": 8, "context": "The problem becomes more complicated if data are fed to the system with high frequency in the form of a data stream [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 9, "context": "It\u2019s the so-called ANARX-model (Additive NARX) that has the form [12, 13]", "startOffset": 65, "endOffset": 73}, {"referenceID": 10, "context": "It\u2019s the so-called ANARX-model (Additive NARX) that has the form [12, 13]", "startOffset": 65, "endOffset": 73}, {"referenceID": 9, "context": "Authors [12, 13] used elementary Rosenblatt perceptrons with sigmoidal activation functions as such nodes.", "startOffset": 8, "endOffset": 16}, {"referenceID": 10, "context": "Authors [12, 13] used elementary Rosenblatt perceptrons with sigmoidal activation functions as such nodes.", "startOffset": 8, "endOffset": 16}, {"referenceID": 11, "context": "The ANARX-model provided high forecasting quality, but generally speaking it requires a large number of nodes in its architecture [14].", "startOffset": 130, "endOffset": 134}, {"referenceID": 12, "context": "Some synthesis problems of forecasting neuro-fuzzy [15] and neo-fuzzy [15-17] systems based on the ANARXmodels are considered in this work.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "Some synthesis problems of forecasting neuro-fuzzy [15] and neo-fuzzy [15-17] systems based on the ANARXmodels are considered in this work.", "startOffset": 70, "endOffset": 77}, {"referenceID": 13, "context": "Some synthesis problems of forecasting neuro-fuzzy [15] and neo-fuzzy [15-17] systems based on the ANARXmodels are considered in this work.", "startOffset": 70, "endOffset": 77}, {"referenceID": 14, "context": "Some synthesis problems of forecasting neuro-fuzzy [15] and neo-fuzzy [15-17] systems based on the ANARXmodels are considered in this work.", "startOffset": 70, "endOffset": 77}, {"referenceID": 15, "context": "As one can see, this node is the Wang\u2013Mendel neuro-fuzzy system [18] with two inputs.", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "It possesses universal approximation capabilities and is actually the zero-order Takagi\u2013Sugeno\u2013Kang system [19, 20].", "startOffset": 107, "endOffset": 115}, {"referenceID": 17, "context": "It possesses universal approximation capabilities and is actually the zero-order Takagi\u2013Sugeno\u2013Kang system [19, 20].", "startOffset": 107, "endOffset": 115}, {"referenceID": 18, "context": "Considering that the output signal \uf028 \uf029 \u02c6 l y k of each node depends linearly on the adjusted synaptic weights l i w , one can use conventional algorithms of adaptive linear identification [21] for their tuning which are based on the quadratic learning criterion.", "startOffset": 188, "endOffset": 192}, {"referenceID": 19, "context": "If a training data set is non-stationary [22], one can use either the exponentially weighted recurrent least squares method \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 1 1 , 1 1 1 1 , 0 1, 1 l l", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "If a large data set to be processed is given (within the \u201cBig Data\u201d conception [23] when data processing speed and computational simplicity come to the forefront, it seems reasonable to use neo-fuzzy neurons that were proposed by T.", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "Yamakawa and his co-authors [15-17] instead of the neuro-fuzzy nodes in the ANARX-model.", "startOffset": 28, "endOffset": 35}, {"referenceID": 13, "context": "Yamakawa and his co-authors [15-17] instead of the neuro-fuzzy nodes in the ANARX-model.", "startOffset": 28, "endOffset": 35}, {"referenceID": 14, "context": "Yamakawa and his co-authors [15-17] instead of the neuro-fuzzy nodes in the ANARX-model.", "startOffset": 28, "endOffset": 35}, {"referenceID": 21, "context": "and an output signal of the ANARX-model can be written down in the form \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 1 1 \u02c6 n h h l l iy iy ix ix l i i y k w y k l w x k l \uf06d \uf06d \uf03d \uf03d \uf03d \uf0e6 \uf0f6 \uf03d \uf02d \uf02b \uf02d \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf0e5 \uf0e5 \uf0e5 , which means that since the neo-fuzzy neuron is also an additive model [24], the ANARX-model based on the neofuzzy neurons is twice additive.", "startOffset": 256, "endOffset": 260}, {"referenceID": 22, "context": "It was proposed to use B-splines as membership functions for the neo-fuzzy neuron [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "Either the algorithms (3), (4) or the procedure [26] \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 T", "startOffset": 48, "endOffset": 52}, {"referenceID": 22, "context": "Evolving systems based on the neo-fuzzy neurons demonstrated its effectiveness for solving different tasks (especially forecasting tasks [25-30]).", "startOffset": 137, "endOffset": 144}, {"referenceID": 23, "context": "Evolving systems based on the neo-fuzzy neurons demonstrated its effectiveness for solving different tasks (especially forecasting tasks [25-30]).", "startOffset": 137, "endOffset": 144}, {"referenceID": 24, "context": "Evolving systems based on the neo-fuzzy neurons demonstrated its effectiveness for solving different tasks (especially forecasting tasks [25-30]).", "startOffset": 137, "endOffset": 144}, {"referenceID": 25, "context": "Evolving systems based on the neo-fuzzy neurons demonstrated its effectiveness for solving different tasks (especially forecasting tasks [25-30]).", "startOffset": 137, "endOffset": 144}, {"referenceID": 26, "context": "Evolving systems based on the neo-fuzzy neurons demonstrated its effectiveness for solving different tasks (especially forecasting tasks [25-30]).", "startOffset": 137, "endOffset": 144}, {"referenceID": 27, "context": "Evolving systems based on the neo-fuzzy neurons demonstrated its effectiveness for solving different tasks (especially forecasting tasks [25-30]).", "startOffset": 137, "endOffset": 144}, {"referenceID": 8, "context": "The twice additive system is the most appropriate choice for data processing in Data Stream Mining tasks [11] from a point of view of implementation simplicity and a processing speed.", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": "A WEIGHTED NEURO-NEO-FUZZY-ANARX-MODEL Considering that every node [ ] l N of the ANARX-model is tuned independently from each other and represents an individual neuro(neo)-fuzzy system, it is possible to use a combination of neural networks\u2019 ensembles [31] in order to improve a forecasting quality.", "startOffset": 253, "endOffset": 257}, {"referenceID": 24, "context": "and a gradient algorithm for finding its saddle point based on the Arrow\u2013Hurwicz procedure [27, 32] can be written in the form \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 , , , 1 c c c k c k k L c L c k k k \uf06c \uf068 \uf06c \uf06c \uf06c \uf06c \uf068 \uf06c \uf0ec \uf03d \uf02d \uf02d \uf0d1 \uf0ef\uf0ef \uf0ed \uf0b6 \uf0ef \uf03d \uf02d \uf02b \uf0ef \uf0b6 \uf0ee", "startOffset": 91, "endOffset": 99}, {"referenceID": 29, "context": "and a gradient algorithm for finding its saddle point based on the Arrow\u2013Hurwicz procedure [27, 32] can be written in the form \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 , , , 1 c c c k c k k L c L c k k k \uf06c \uf068 \uf06c \uf06c \uf06c \uf06c \uf068 \uf06c \uf0ec \uf03d \uf02d \uf02d \uf0d1 \uf0ef\uf0ef \uf0ed \uf0b6 \uf0ef \uf03d \uf02d \uf02b \uf0ef \uf0b6 \uf0ee", "startOffset": 91, "endOffset": 99}], "year": 2016, "abstractText": "An evolving weighted neuro-neo-fuzzy-ANARX model and its learning procedures are introduced in the article. This system is basically used for time series forecasting. It\u2019s based on neo-fuzzy elements. This system may be considered as a pool of elements that process data in a parallel manner. The proposed evolving system may provide online processing data streams. Index Terms \u2014 Computational Intelligence, time series prediction, neuro-neo-fuzzy System, Machine Learning, ANARX, Data Stream.", "creator": "PScript5.dll Version 5.2.2"}}}