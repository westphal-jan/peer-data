{"id": "1610.06525", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "ChoiceRank: Identifying Preferences from Node Traffic in Networks", "abstract": "Understanding difficult browsers scour in once links is there course interest however be content. We supposed a turning across only goal axillary - higher visibility does observed however tackle making judicial following innovative advantage transition probabilities. We more as taken perfect prefer education possibly, and need study a model where these all Luce ' = supersedes. In given case, within $ O (n) $ marginal court thus cytoplasmic traveling often from effectiveness statistic with the $ O (i.e. ^ plus) $ transition probabilities. We scene thought once could full inference it however - pointing regardless of followed abc ' s forms, and we present ChoiceRank, although equations iteration take scales then networks fact addition billions of subset instead edges. We limit part concept from from month - just clickstream of the English Wikipedia included time year raised rides saturday New York City ' makes bmx - sharing structure. In which serious, know successfully helped the approaches corresponding it only the based small those marginal (node - however) running data.", "histories": [["v1", "Thu, 20 Oct 2016 18:19:07 GMT  (118kb,D)", "https://arxiv.org/abs/1610.06525v1", null], ["v2", "Thu, 15 Jun 2017 15:14:54 GMT  (104kb,D)", "http://arxiv.org/abs/1610.06525v2", "Accepted at ICML 2017"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.SI", "authors": ["lucas maystre", "matthias grossglauser"], "accepted": true, "id": "1610.06525"}, "pdf": {"name": "1610.06525.pdf", "metadata": {"source": "CRF", "title": "ChoiceRank: Identifying Preferences from Node Traffic in Networks", "authors": ["Lucas Maystre", "Matthias Grossglauser"], "emails": ["lucas.maystre@epfl.ch", "matthias.grossglauser@epfl.ch"], "sections": [{"heading": "1 Introduction", "text": "Consider the problem of estimating click probabilities for links between pages of a website, given a hyperlink graph and aggregate statistics on the number of times each page has been visited. Naively, one might expect that the probability of clicking on a particular link should be roughly proportional to the traffic of the link\u2019s target. However, this neglects important structural effects: a page\u2019s traffic is influenced by a) the number of incoming links, b) the traffic at the pages that link to it, and c) the traffic absorbed by competing links. In order to successfully infer click probabilities, it is therefore necessary to disentangle the preference for a page (i.e., the intrinsic propensity of a user to click on a link pointing to it) from the page\u2019s visibility (the exposure it gets from pages linking to it). Building upon recent work by Kumar et al. [2015], we present a statistical framework that tackles a general formulation of the problem: given a network (representing possible transitions between nodes) and the marginal traffic at each node, recover the transition probabilities. This problem is relevant to a number of scenarios (in social, information\n\u2217School of Computer and Communication Sciences, EPFL, Switzerland.\nar X\niv :1\n61 0.\n06 52\n5v 2\n[ st\nat .M\nL ]\n1 5\nJu n\n20 17\nor transportation networks) where transition data is not available due to, e.g., privacy concerns or monitoring costs.\nWe begin by postulating the following model of traffic. Users navigate from node to node along the edges of the network by making a choice between adjacent nodes at each step, reminiscent of the random-surfer model introduced by Brin and Page [1998]. Choices are assumed to be independent and generated according to Luce\u2019s model [Luce, 1959]: each node in the network is chararacterized by a latent strength parameter, and (stochastic) choice outcomes tend to favor nodes with greater strengths. In this model, estimating the transition probabilities amounts to estimating the strength parameters. Unlike the setting in which choice models are traditionally studied [Train, 2009, Maystre and Grossglauser, 2015, Vojnovic and Yun, 2016], we do not observe distinct choices among well-identified sets of alternatives. Instead, we only have access to aggregate, marginal statistics about the traffic at each node in the network. In this setting, we make the following contributions.\n1. We observe that marginal per-node traffic is a sufficient statistic for the strength parameters. That is, the parameters can be inferred from marginal traffic data without any loss of information.\n2. We show that if the parameters are endowed with a prior distribution, the inference problem becomes well-posed regardless of the network structure. This is a crucial step in making the framework applicable to real-world datasets.\n3. We show that model inference can scale to very large datasets. We present an iterative EM-type inference algorithm that enables a remarkably efficient implementation\u2014 each iteration requires the computational equivalent of two iterations of PageRank.\nWe evaluate two aspects of our framework using real-world networks. We begin by demonstrating that local preferences can indeed be inferred from global traffic: we investigate the accuracy of the transition probabilities recovered by our model on three datasets for which we have ground-truth transition data. First, we consider two hyperlink graphs, representing the English Wikipedia (over two million nodes) and a Hungarian news portal (approximately 40 000 nodes), respectively. We model clickstream data as a sequence of independent choices over the links available at each page. Given only the structure of the graph and the marginal traffic at every node, we estimate the number of transitions between nodes, and we find that our estimate matches ground-truth edge-level transitions accurately in both instances. Second, we consider the network of New York City\u2019s bicycle-sharing service. For a given ride, given a pick-up station, we model the drop-off station as a choice out of a set of locations. Our model yields promising results, suggesting that our method can be useful beyond clickstream data. Next, we test the scalability of the inference algorithm. We show that the algorithm is able to process a snapshot of the WWW hyperlink graph containing over a hundred billion edges using a single machine.\nOrganization of the paper. In Section 2, we formalize the network choice model. In Section 3, we briefly review related literature. In Section 4, we present salient statistical properties of the model and its maximum-likelihood estimator, and we propose a prior distribution that makes the inference problem well-posed. In Section 5, we describe an\ninference algorithm that enables an efficient implementation. We evaluate the model and the inference algorithm in Section 6, before concluding in Section 7. In the appendices, we provide a more in-depth discussion of our model and algorithm, and we present proofs for all the theorems stated in the main text."}, {"heading": "2 Network Choice Model", "text": "Let G = (V,E) be a directed graph on n nodes (corresponding to items) and m edges. We denote the out-neighborhood of node i by N+i and its in-neighborhood by N \u2212 i . We consider the following choice process on G. A user starts at a node i and is faced with alternatives N+i . The user chooses item j and moves to the corresponding node. At node j, the user is faced with alternatives N+j and chooses k, and so on. At any time, the user can stop. Figure 1 gives an example of a graph and the alternatives available at a step of the process.\nTo define the transition probabilities, we posit Luce\u2019s well-known choice axiom that states that the odds of choosing item j over item j\u2032 do not depend on the rest of the alternatives [Luce, 1959]. This axiom leads to a unique probabilistic model of choice. For every node i and every j \u2208 N+i , the probability that j is selected among alternatives N+i can be written as\npij = \u03bbj\u2211\nk\u2208N+i \u03bbk\n(1)\nfor some parameter vector \u03bb = [ \u03bb1 \u00b7 \u00b7 \u00b7 \u03bbn ]> \u2208 Rn>0. Intuitively, the parameter \u03bbi can be interpreted as the strength (or utility) of item i. Note that pij depends only on the out-neighborhood of node i. As such, the choice process satisfies the Markov property, and we can think of the sequence of choices as a trajectory in a Markov chain. In the context of this model, we can formulate the inference problem as follows. Given a directed graph G = (V,E) and data on the aggregate traffic at each node, find a parameter vector \u03bb that fits the data."}, {"heading": "3 Related Work", "text": "A variant of the network choice model was recently introduced by Kumar et al. [2015], in an article that lays much of the groundwork for the present paper. Their generative model of traffic and the parametrization of transition probabilities based on Luce\u2019s axiom form the basis of our work. Kumar et al. define the steady-state inversion problem as follows: Given a graph G and a target stationary distribution, find transition probabilities that lead to the desired stationary distribution. This problem formulation assumes that G satisfies restrictive structural properties (strong-connectedness, aperiodicity) and is valid only asymptotically, when the sequences of choices made by users are very long. Our formulation is, in contrast, more general. In particular, we eliminate any assumptions about the structure of G and cope with finite data in a principled way\u2014in fact, our derivations are valid for choice sequences of any length. One of our contributions is to explain the steady-state inversion problem in terms of (asymptotic) maximum-likelihood inference in the network choice model. Furthermore, the statistical viewpoint that we develop also leads to a) a robust regularization scheme, and b) a simple and efficient EM-type inference algorithm. These important extensions make the model easier to apply to real-world data.\nLuce\u2019s choice axiom. The general problem of estimating parameters of models based on Luce\u2019s axiom has received considerable attention. Several decades before Luce\u2019s seminal book [Luce, 1959], Zermelo [1928] proposed a model and an algorithm that estimates the strengths of chess players based on pairwise comparison outcomes (his model would later be rediscovered by Bradley and Terry [1952]). More recently, Hunter [2004] explained Zermelo\u2019s algorithm from the perspective of the minorization-maximization (MM) method. This method is easily generalized to other models that are based on Luce\u2019s axiom, and it yields simple, provably convergent algorithms for maximum-likelihood (ML) or maximuma-posteriori point estimates. Caron and Doucet [2012] observe that these MM algorithms can be further recast as expectation-maximization (EM) algorithms by introducing suitable latent variables. They use this observation to derive Gibbs samplers for a wide family of models. We take advantage of this long line of work in Section 5 when developing an inference algorithm for the network choice model. In recent years, several authors have also analyzed the sample complexity of the ML estimate in Luce\u2019s choice model [Hajek et al., 2014, Vojnovic and Yun, 2016] and investigated alternative spectral inference methods [Negahban et al., 2012, Azari Soufiani et al., 2013, Maystre and Grossglauser, 2015]. Some of these results could be applied to our setting, but in general they require observing choices among well-identified sets of alternatives. Finally, we note that models based on Luce\u2019s axiom have been successfully applied to problems ranging from ranking players based on game outcomes [Zermelo, 1928, Elo, 1978] to understanding consumer behavior based on discrete choices [McFadden, 1973], and to discriminating among multiple classes based on the output of pairwise classifiers [Hastie and Tibshirani, 1998].\nNetwork analysis. Understanding the preferences of users in networks is of significant interest in many domains. For brevity, we focus on literature related to hyperlink graphs. A method that has undoubtedly had a tremendous impact in this context is PageRank [Brin and Page, 1998]. PageRank computes a set of scores that are proportional to the\namount of time a surfer, who clicks on links randomly and uniformly, spends at each node. These scores are based only on the structure of the graph. The network choice model presented in this paper appears similar at first, but tackles a different problem. In addition to the structure of the graph, it uses the traffic at each page, and computes a set of scores that reflect the (non-uniform) probability of clicking on each link. Nevertheless, there are striking similarities in the implementation of the respective inference algorithms (see Section 6). The HOTness method proposed by Tomlin [2003] is somewhat related, but tries to tackle a harder problem. It attempts to estimate jointly the traffic and the probability of clicking on each link, by using a maximum-entropy approach. At the other end of the spectrum, BrowseRank [Liu et al., 2008] uses detailed data collected in users\u2019 browsers to improve on PageRank. Our method uses only marginal traffic data that can be obtained without tracking users."}, {"heading": "4 Statistical Properties", "text": "In this section, we describe some important statistical properties of the network choice model. We begin by observing that O(n) values summarizing the traffic at each node is a sufficient statistic for the O(n2) entries of the Markov-chain transition matrix. We then connect our statistical model to the steady-state inversion problem defined by Kumar et al. [2015]. Guided by this connection, we study the maximum-likelihood (ML) estimate of model parameters, but find that the estimate is likely to be ill-defined in many scenarios of practical interest. Lastly, we study how to overcome this issue by introducing a prior distribution on the parameters \u03bb; the prior guarantees that the inference problem is well-posed.\nFor simplicity of exposition, we present our results for Luce\u2019s standard choice model defined in (1). Our developments extend to the model variant proposed by Kumar et al. [2015], where choice probabilities can be modulated by edge weights. In Appendix A, we describe this variant and give the necessary adjustments to our developments."}, {"heading": "4.1 Aggregate Traffic Is a Sufficient Statistic", "text": "Let cij denote the number of transitions that occurred along edge (i, j) \u2208 E. Starting from the transition probability defined in (1), we can write the log-likelihood of \u03bb given data D = {cij | (i, j) \u2208 E} as\n`(\u03bb;D) = \u2211\n(i,j)\u2208E\ncij [ log \u03bbj \u2212 log \u2211 k\u2208N+i \u03bbk ]\n= n\u2211 j=1 \u2211 i\u2208N\u2212j cij log \u03bbj \u2212 n\u2211 i=1 \u2211 j\u2208N+i cij log \u2211 k\u2208N+i \u03bbk\n= n\u2211 i=1 [ c\u2212i log \u03bbi \u2212 c+i log \u2211 k\u2208N+i \u03bbk ] , (2)\nwhere c\u2212i = \u2211\nj\u2208N\u2212i cji and c+i = \u2211 j\u2208N+i\ncij is the aggregate number of transitions arriving in and originating from i, respectively. This formulation of the log-likelihood exhibits a key\nfeature of the model: the set of 2n counts {(c\u2212i , c+i ) | i \u2208 V } is a sufficient statistic of the O(n2) counts {cij | (i, j) \u2208 E} for the parameters \u03bb. (In Appendix A, we show that it is in fact minimally sufficient.) In other words, it is enough to observe marginal information about the number of arrivals and departures at each node\u2014we collectively call this data the traffic at a node\u2014and no additional information can be gained by observing the full choice process. This makes the model particularly attractive, because it means that it is unnecessary to track users across nodes. In several applications of practical interest, tracking users is undesirable, difficult, or outright impossible, due to a) privacy reasons, b) monitoring costs, or c) lack of data in existing datasets.\nNote that if we make the additional assumption that the flow in the network is conserved, then c\u2212i = c + i . If users\u2019 typical trajectories consist of many hops, it is reasonable to approximate c\u2212i or c + i using that assumption, should one of the two quantities be missing."}, {"heading": "4.2 Connection to the Steady-State Inversion Problem", "text": "In recent work, Kumar et al. [2015] define the problem of steady-state inversion as follows: Given a strongly-connected directed graph G = (V,E) and a target distribution over the nodes \u03c0, find a Markov chain on G with stationary distribution \u03c0. As there are m = O(n2) degrees of freedom (the transition probabilities) for n constraints (the stationary distribution), the problem is in most cases underdetermined. Following Luce\u2019s ideas, the transition probabilities are constrained to be proportional to a latent score of the destination node as per (1), thus reducing the number of parameters from m to n. Denote by P (s) the Markov-chain transition matrix parametrized with scores s. The score vector s is a solution for the steady-state inversion problem if and only if \u03c0 = \u03c0P (s), or equivalently\n\u03c0i = \u2211 j\u2208N\u2212i si\u2211 k\u2208N+j sk \u03c0j \u2200i. (3)\nIn order to formalize the connection between Kumar et al.\u2019s work and ours, we now express the steady-state inversion problem as that of asymptotic maximum-likelihood estimation in the network choice model. Suppose that we observe node-level traffic data D = {(c\u2212i , c+i ) | i \u2208 V } about a trajectory of length T starting at an arbitrary node. We want to obtain an estimate of the parameters \u03bb? by maximizing the average log-likelihood \u02c6\u0300(\u03bb) = 1\nT `(\u03bb;D). From standard convergence results for Markov chains [Kemeny and\nSnell, 1976], it follows that as G is strongly connected, limT\u2192\u221e c\u2212i /T = limT\u2192\u221e c + i /T = \u03c0i. Therefore,\n\u02c6\u0300(\u03bb) = n\u2211 i=1 [ c\u2212i T log \u03bbi \u2212 c+i T log \u2211 k\u2208N+i \u03bbk ] T\u2192\u221e\u2212\u2212\u2212\u2192 n\u2211 i=1 \u03c0i [ log \u03bbi \u2212 log \u2211 k\u2208N+i \u03bbk ] .\nLet \u03bb? be a maximizer of the average log-likelihood. When T \u2192 \u221e, the optimality condition \u2207\u02c6\u0300(\u03bb?) = 0 implies\n\u2202 \u02c6\u0300(\u03bb)\n\u2202\u03bbi \u2223\u2223\u2223\u2223 \u03bb=\u03bb? = \u03c0i \u03bb?i \u2212 \u2211 j\u2208N\u2212i \u03c0j\u2211 k\u2208N+j \u03bb?k = 0 \u21d0\u21d2 \u03c0i = \u2211 j\u2208N\u2212i \u03bb?i\u2211 k\u2208N+j \u03bb?k \u03c0j \u2200i. (4)\nComparing (4) to (3), it is clear that \u03bb? is a solution of the steady-state inversion problem. As such, the network choice model presented in this paper can be viewed as a principled extension of the steady-state inversion problem to the finite-data case."}, {"heading": "4.3 Maximum-Likelihood Estimate", "text": "The log-likelihood (2) is not concave in \u03bb, but it can be made concave using the simple reparametrization \u03bbi = e\u03b8i . Therefore, any local minimum of the likelihood is a global minimum. Unfortunately, it turns out that the conditions guaranteeing that the ML estimate is well-defined (i.e., that it exists and is unique) are restrictive and impractical. We illustrate this by providing a necessary condition, and for brevity we defer the comprehensive analysis of the ML estimate to Appendix B. We begin with a definition that uses the notion of hypergraph, a generalized graph where edges may be any non-empty subset of nodes.\nDefinition (Comparison hypergraph). Given a directed graph G = (V,E), the comparison hypergraph is the hypergraph H = (V,A), with A = {N+i | i \u2208 V }.\nIntuitively, H is the hypergraph induced by the sets of alternatives available at each node. Figure 2 provides an example of a graph and of its associated comparison hypergraph. Equipped with this definition, we can state the following theorem that is a reformulation of a well-known result for Luce\u2019s choice model [Hunter, 2004].\nTheorem 1. If the comparison hypergraph is not connected, then for any data D there are \u03bb and \u00b5 such that \u03bb 6= c\u00b5 for any c \u2208 R>0 and `(\u03bb;D) = `(\u00b5;D).\nIn short, the proof shows that rescaling all the parameters in one of the connected components does not change the value of the likelihood function. The network of Figure 1 illustrates an instance where the condition fails: although the graph G is strongly connected, its associated comparison hypergraph H (depicted in Figure 2) is disconnected, and no matter what the data D is, the ML estimate will never be uniquely defined. In fact, in Appendix B, we demonstrate that Theorem 1 is just the tip of the iceberg. We provide an example where the ML estimate does not exist even though the comparison hypergraph is connected, and we explain that verifying a necessary and sufficient condition for the existence of the ML estimate is computationally more expensive than solving the inference problem itself."}, {"heading": "4.4 Well-Posed Inference", "text": "Following the ideas of Caron and Doucet [2012], we introduce an independent Gamma prior on each parameter, i.e., i.i.d. \u03bb1, . . . , \u03bbn \u223c Gamma(\u03b1, \u03b2). Adding the log-prior to the log-likelihood, we can write the log-posterior as\nlog p(\u03bb | D) = n\u2211 i=1 [ (c\u2212i + \u03b1\u2212 1) log \u03bbi \u2212 c+i log \u2211 k\u2208N+i \u03bbk \u2212 \u03b2\u03bbi ] + \u03ba, (5)\nwhere \u03ba is a constant that is independent of \u03bb. The Gamma prior translates into a form of regularization that makes the inference problem well-posed, as shown by the following theorem.\nTheorem 2. If i.i.d. \u03bb1, . . . , \u03bbn \u223c Gamma(\u03b1, \u03b2) with \u03b1 > 1, then the log-posterior (5) always has a unique maximizer \u03bb? \u2208 Rn>0.\nThe condition \u03b1 > 1 ensures that the prior has a nonzero mode. In short, the proof of Theorem 2 shows that as a result of the Gamma prior, the log-posterior can be reparametrized into a strictly concave function with bounded super-level sets (if \u03b1 > 1). This guarantees that the log-posterior will always have exactly one maximizer. Unlike the results that we derive for the ML estimate, Theorem 2 does not impose any condition on the graph G for the estimate to be well-defined.\nRemark. Note that varying the rate \u03b2 in the Gamma prior simply rescales the parameters \u03bb. Furthermore, it is clear from (1) that such a rescaling affects neither the likelihood of the observed data nor the prediction of future transitions. As a consequence, we may assume that \u03b2 = 1 without loss of generality."}, {"heading": "5 Inference Algorithm", "text": "The maximizer of the log-posterior does not have a closed-form solution. In the spirit of the algorithms of Hunter [2004] for variants of Luce\u2019s choice model, we develop a minorization-maximization (MM) algorithm. Simply put, the algorithm iteratively refines an estimate of the maximizer by solving a sequence of surrogates of the log-posterior. Using the inequality log x \u2264 log x\u0303+ x/x\u0303\u2212 1 (with equality if and only if x = x\u0303), we can lower-bound the log-posterior (5) by\nf (t)(\u03bb) = n\u2211 i=1 [ (c\u2212i + \u03b1\u2212 1) log \u03bbi \u2212 c+i ( log \u2211 k\u2208N+i \u03bb (t) k + \u2211 k\u2208N+i \u03bbk\u2211 k\u2208N+i \u03bb (t) k \u2212 1 ) \u2212 \u03b2\u03bbi ] + \u03ba,\nAlgorithm 1 ChoiceRank Require: graph G = (V,E), counts {(c\u2212i , c+i )} 1: \u03bb\u2190 [1, . . . , 1] 2: repeat 3: z \u2190 0n . Recompute \u03b3 4: for (i, j) \u2208 E do zi \u2190 zi + \u03bbj 5: for i \u2208 V do \u03b3i \u2190 c+i /zi 6: z \u2190 0n . Recompute \u03bb 7: for (i, j) \u2208 E do zj \u2190 zj + \u03b3i 8: for i \u2208 V do \u03bbi \u2190 (c\u2212i + \u03b1\u2212 1)/(zi + \u03b2) 9: until \u03bb has converged\nwith equality if and only if \u03bb = \u03bb(t). Starting with an arbitrary \u03bb(0) \u2208 Rn>0, we repeatedly solve the optimization problem\n\u03bb(t+1) = argmax \u03bb f (t)(\u03bb).\nUnlike the maximization of the log-posterior, the surrogate optimization problem has a closed-form solution, obtained by setting \u2207f (t) to 0:\n\u03bb (t+1) i = c\u2212i + \u03b1\u2212 1\u2211 j\u2208N\u2212i \u03b3 (t) j + \u03b2 , \u03b3 (t) j = c+j\u2211 k\u2208N+j \u03bb (t) k . (6)\nThe iterates provably converge to the maximizer of (5), as shown by the following theorem.\nTheorem 3. Let \u03bb? be the unique maximum a-posteriori estimate. Then for any initial \u03bb(0) \u2208 Rn>0 the sequence of iterates defined by (6) converges to \u03bb?.\nTheorem 3 follows from a standard result on the convergence of MM algorithms and uses the fact that the log-posterior increases after each iteration. Furthermore, it is known that MM algorithms exhibit geometric convergence in a neighborhood of the maximizer [Lange et al., 2000]. A thorough investigation of the convergence properties is left for future work.\nThe structure of the updates in (6) leads to an extremely simple and efficient implementation, given in Algorithm 1: we call it ChoiceRank. A graphical representation of an iteration from the perspective of a single node is given in Figure 3. Each iteration consists of two phases of message passing, with \u03b3i flowing towards in-neighbors N\u2212i , then \u03bbi flowing towards out-neighbors N+i . The updates to a node\u2019s state are a function of the sum of the messages. As the algorithm does two passes over the edges and two passes over the vertices, an iteration takes O(m+ n) time. The edges can be processed in any order, and the algorithm maintains a state over only O(n) values associated with the vertices. Furthermore, the algorithm can be conveniently expressed in the well-known vertex-centric programming model [Malewicz et al., 2010]. This makes it easy to implement ChoiceRank inside scalable, optimized graph-processing systems such as Apache Spark [Gonzalez et al., 2014].\nEM viewpoint. The update (6) can also be explained from an expectation-maximization (EM) viewpoint, by introducing suitable latent variables [Caron and Doucet, 2012]. This viewpoint enables a Gibbs sampler that can be used for Bayesian inference. We present the EM derivation in Appendix C, but leave a study of fully Bayesian inference in the network choice model for future work."}, {"heading": "6 Experimental Evaluation", "text": "In this section, we investigate a) the ability of the network choice model to accurately recover transitions in real-world scenarios, and b) the potential of ChoiceRank to scale to very large networks."}, {"heading": "6.1 Accuracy on Real-World Data", "text": "We evaluate the network choice model on three datasets that are representative of two distinct application domains. Each dataset can be represented as a set of transition counts {cij} on a directed graph G = (V,E). We aggregate the transition counts into marginal traffic data {(c\u2212i , c+i ) | i \u2208 V } and fit a network choice model by using ChoiceRank. We set \u03b1 = 2.0 and \u03b2 = 1.0 (these small values simply guarantee the convergence of the algorithm) and declare convergence when \u2016\u03bb(t)\u2212\u03bb(t\u22121)\u20161/n < 10\u22128. Given \u03bb, we estimate transition probabilities using pij \u221d \u03bbj as given by (1). To the best of our knowledge, there is no other published method tackling the problem of estimating transition probabilities from marginal traffic data. Therefore, we compare our method to three baselines based on simple heuristics.\nTraffic Transitions probabilities are proportional to the traffic of the target node: qTij \u221d c\u2212j . PageRank Transition probabilities are proportional to the PageRank score of the target\nnode: qPij \u221d PRj.\nUniform Any transition is equiprobable: qUij \u221d 1. The four estimates are compared against ground-truth transition probabilities derived from the edge traffic data: p?ij \u221d cij. We emphasize that although per-edge transition counts {cij} are needed to evaluate the accuracy of the network choice model (and the baselines), these counts are not necessary for learning the model\u2014per-node marginal counts are sufficient.\nGiven a node i, we measure the accuracy of a distribution qi over outgoing transitions using two error metrics, the KL-divergence and the (normalized) rank displacement:\nDKL(p ? i , qi) = \u2211 j\u2208N+i p?ij log p?ij qij ,\nDFR(p ? i , qi) =\n1 |N+i |2 \u2211 j\u2208N+i |\u03c3?i (j)\u2212 \u03c3\u0302i(j)|,\nwhere \u03c3?i (respectively \u03c3\u0302i) is the ranking of elements in N + i by decreasing order of p?ij (respectively qij). We report the distribution of errors \u201cover choices\u201d, i.e., the error at each node i is weighted by the number of outgoing transitions c+i ."}, {"heading": "6.1.1 Clickstream Data", "text": "Wikipedia The Wikimedia Foundation has a long history of publicly sharing aggregate, page-level web traffic data1. Recently, it also released clickstream data from the English version of Wikipedia [Wulczyn and Taraborelli, 2016], providing us with essential groundtruth transition-level data. We consider a dataset that contains information, extracted from the server logs, about the traffic each page of the English Wikipedia received during the month of March 2016. Each page\u2019s incoming traffic is grouped by HTTP referrer, i.e., by the page visited prior to the request. We ignore the traffic generated by external Web sites such as search engines and keep only the internal traffic (18% of the total traffic in the dataset). In summary, we obtain counts of transitions on the hyperlink graph of English Wikipedia articles. The graph contains n = 2316 032 nodes and m = 13 181 698 edges, and we consider slightly over 1.2 billion transitions over the edges. On this dataset, ChoiceRank converges after 795 iterations.\nKosarak We also consider a second clickstream dataset from a Hungarian online news portal2. The data consists of 7 029 013 transitions on a graph containing n = 41001 nodes and m = 974 560 edges. ChoiceRank converges after 625 iterations.\nThe four leftmost plots of Figure 4 show the error distributions. ChoiceRank significantly improves on the baselines, both in terms of KL-divergence and rank displacement. These results give compelling evidence that transitions do not occur proportionally with the target\u2019s page traffic: in terms of KL-divergence, ChoiceRank improves on Traffic by a factor 3\u00d7 and 2\u00d7, respectively. PageRank scores, while reflecting some notion of importance of a page, are not designed to estimate transitions, and understandably the corresponding baseline performs poorly. Uniform (perhaps the simplest of our baselines)\n1See: https://stats.wikimedia.org/. 2The data is publicly available at http://fimi.ua.ac.be/data/.\nis (by design) unable to distinguish among transitions, resulting in a large displacement error. We believe that its comparatively better performance in terms of KL-divergence (for Wikipedia) is mostly an artifact of the metric, which encourages \u201cprudent\u201d estimates. Finally, in Figure 5 we observe that ChoiceRank seems to perform comparatively better as the number of possible transition increases."}, {"heading": "6.1.2 NYC Bicycle-Sharing Data", "text": "Next, we consider trip data from Citi Bike, New York City\u2019s bicycle-sharing system3. For each ride on the system made during the year 2015, we extract the pick-up and drop-off stations and the duration of the ride. Because we want to focus on direct trips, we exclude rides that last more than one hour. We also exclude source-destinations pairs which have less than 1 ride per day on average (a majority of source-destination pairs appears at least once in the dataset). The resulting data consists of 3.4 million rides on a graph containing n = 497 nodes and m = 5209 edges. ChoiceRank converges after 7508 iterations. We compute the error distribution in the same way as for the clickstream datasets.\nThe two rightmost plots of Figure 4 display the results. The observations made on the clickstream datasets carry over to this mobility dataset, albeit to a lesser degree. A significant difference between clicking a link and taking a bicycle trip is that in the latter case, there is a non-uniform \u201ccost\u201d of a transition due to the distance between source and target. In future work, one might consider incorporating edge weights and using the weighted network choice model presented in Appendix A.\n3The data is available at https://www.citibikenyc.com/system-data."}, {"heading": "6.2 Scaling ChoiceRank to Billions of Nodes", "text": "To demonstrate ChoiceRank\u2019s scalability, we develop a simple implementation in the Rust programming language, based on the ideas of COST [McSherry et al., 2015]. Our code is publicly available online4. The implementation repeatedly streams edges from disk and keeps four floating-point values per node in memory: the counts c\u2212i and c + i , the sum of messages zi, and either \u03b3i or \u03bbi (depending on the stage in the iteration). As edges can be processed in any order, it can be beneficial to reorder the edges in a way that accelerates the computation. For this reason, our implementation preprocesses the list of edges and reorders them in Hilbert curve order5. This results in better cache locality and yields a significant speedup.\nWe test our implementation on a hyperlink graph extracted from the 2012 Common Crawl web corpus6 that contains over 3.5 billion nodes and 128 billion edges [Meusel et al., 2014]. The edge list alone requires about 1 TB of uncompressed storage. There is no publicly available information on the traffic at each page, therefore we generate a value ci for every node i randomly and uniformly between 100 and 500, and set both c\u2212i and c + i to ci. As such, this experiment does not attempt to measure the validity of the model (unlike the experiments of Section 6.1). Instead, it focuses on testing the algorithm\u2019s potential to scale to to very large networks.\nResults. We run 20 iterations of ChoiceRank on a dual Intel Xeon E5-2680 v3 machine, with 256 GB of RAM and 6 HDDs configured in RAID 0. We arbitrarily set \u03b1 = 2.0 and \u03b2 = 1.0 (but this choice has no impact on the results). Only about 65 GB of memory is used, all to store the nodes\u2019 state (4\u00d7 4 bytes per node). The algorithm takes a little less than 39 minutes per iteration on average. Collectively, these results validate the feasibility\n4See: http://lucas.maystre.ch/choicerank. 5A Hilbert space-filling curve visits all the entries of the adjacency matrix of the graph, in a way that\npreserves locality of both source and destination of the edges. 6 The data is available at http://webdatacommons.org/hyperlinkgraph/.\nof model inference for very large datasets. It is worth noting that despite tackling different problems, the ChoiceRank algorithm exhibits interesting similarities with a message-passing implementation of PageRank commonly used in scalable graph-parallel systems such as Pregel [Malewicz et al., 2010] and Spark [Gonzalez et al., 2014]. For comparison, using the COST code [McSherry et al., 2015] we run 20 iterations of PageRank on the same hardware and data. PageRank uses slightly less memory (about 50 GB, or one less floating-point number per node) and takes about half of the time per iteration (a little over 20 minutes). This is consistent with the fact that ChoiceRank requires two passes over the edges per iteration, whereas PageRank requires one. The similarities between the two algorithms lead us to believe that in general, ChoiceRank can benefit from any new system optimizations developed for PageRank."}, {"heading": "7 Conclusion", "text": "In this paper, we present a method that tackles the problem of finding the transition probabilities along the edges of a network, given only the network\u2019s structure and aggregate node-level traffic data. This method generalizes and extends ideas recently presented by Kumar et al. [2015]. We demonstrate that in spite of the strong model assumptions needed to learn O(n2) probabilities from O(n) observations, the method still manages to recover the transition probabilities to a good level of accuracy on two clickstream datasets, and shows promise for applications beyond clickstream data. To sum up, we believe that our method will be useful to pracitioners interested in understanding patterns of navigation in networks from aggregate traffic data, commonly available, e.g., in public datasets.\nAcknowledgments. We thank Holly Cogliati-Bauereis, Ksenia Konyushkova, Brunella Spinelli and anonymous reviewers for careful proofreading and helpful comments."}, {"heading": "A Extensions and Proofs", "text": "In this section, we start by generalizing the network choice model to account for edge weights. Then, we present formal proofs for a) the (minimal) sufficiency of marginal counts and b) the well-posedness of MAP inference in the generalized weighted network choice model.\nA.1 Generalization of the Model\nLet G = (V,E) be a weighted, directed graph with edge weights wij > 0 for all (i, j) \u2208 E. Kumar et al. [2015] propose the following generalization of Luce\u2019s choice model. Given a parameter vector \u03bb \u2208 Rn>0, they define the choice probabilities as\npij = wij\u03bbj\u2211\nk\u2208N+i wik\u03bbk\n, j \u2208 N+i . (7)\nWe refer to this model as the weighted network choice model. Intuitively, the strength of each alternative is weighted by the corresponding edge\u2019s weight; Luce\u2019s original choice\nmodel is obtained by setting wij = constant. In this general model, the log-likelihood becomes\n`(\u03bb;D) = \u2211\n(i,j)\u2208E\ncij [ logwij\u03bbj \u2212 log \u2211 k\u2208N+i wik\u03bbk ]\n= \u2211\n(i,j)\u2208E\ncij [ log \u03bbj \u2212 log \u2211 k\u2208N+i wik\u03bbk ] + \u2211 (i,j)\u2208E cij logwij,\n= n\u2211 i=1 [ c\u2212i log \u03bbi \u2212 c+i log \u2211 k\u2208N+i wik\u03bbk ] + \u03ba1, (8)\nwhere c\u2212i = \u2211\nj\u2208N\u2212i cji and c+i = \u2211 j\u2208N+i\ncij is the aggregate number of transitions arriving in and originating from i, respectively. Note that for every i, the weights {wij | j \u2208 N+i } are equivalent up to rescaling.\nThis generalization is relevant in situations where the current context modulates the alternatives\u2019 strength. For example, this could be used to take into account the position or prominence of a link on a page in a hyperlink graph, or the distance between two locations in a mobility network.\nA.2 Minimal Sufficiency of Marginal Counts\nRecall that cij denotes the number of times we observe a transition from i to j. We set out to prove the following theorem for the weighted network choice model.\nTheorem 4. Let c\u2212i = \u2211\nj\u2208N\u2212i cji and c+i = \u2211 j\u2208N+i\ncij be the aggregate number of transitions arriving in and originating from i, respectively. Then, {(c\u2212i , c+i ) | i \u2208 V } is a minimally sufficient statistic for the parameter \u03bb in the weighted network choice model.\nProof. Let f({cij} | \u03bb) be the discrete probability density function of the data under the model with parameters \u03bb. Theorem 6.2.13 in Casella and Berger [2002] states that {(c\u2212i , c+i )} is a minimally sufficient statistic for \u03bb if and only if, for any {cij} and {dij} in the support of f ,\nf({cij} | \u03bb) f({dij} | \u03bb) is independent of \u03bb \u21d0\u21d2 (c\u2212i , c+i ) = (d\u2212i , d+i ) \u2200i. (9)\nTaking the log of the ratio on the left-hand side and using (8), we find that\nlog f({cij} | \u03bb) f({dij} | \u03bb) = n\u2211 i=1 [ (c\u2212i \u2212d\u2212i ) log \u03bbi \u2212 (c+i \u2212d+i ) log \u2211 k\u2208N+i wik\u03bbk ] + \u03ba2.\nFrom this, it is easy to see that the ratio of densities is independent of \u03bb if and only if c\u2212i = d \u2212 i and c + i = d + i , which verifies (9).\nA.3 Well-Posedness of MAP Inference\nUsing a Gamma(\u03b1, \u03b2) prior for each parameter, the log-posterior of the weighted network choice model can be written as\nlog p(\u03bb | D) = n\u2211 i=1 [ (c\u2212i + \u03b1\u2212 1) log \u03bbi \u2212 c+i log ( \u2211 k\u2208N+i wik\u03bbk ) \u2212 \u03b2\u03bbi ] + \u03ba3. (10)\nWe prove a theorem that guarantees that MAP estimation is well-posed in this generalized model; the proof of Theorem 2 follows trivially.\nTheorem 5. If i.i.d. \u03bb1, . . . , \u03bbn \u223c Gamma(\u03b1, \u03b2) with \u03b1 > 1, then there exists a unique maximizer \u03bb? \u2208 Rn>0 of the weighted network choice model\u2019s log-posterior (10).\nProof. The log-posterior (10) is not concave in \u03bb, but it can be made concave using the simple reparametrization \u03bbi = e\u03b8i . Under this reparametrization, the log-prior and the log-likelihood become\nlog p(\u03b8) = n\u2211 i=1 [ (\u03b1\u2212 1)\u03b8i \u2212 \u03b2e\u03b8i ] + \u03ba4,\n`(\u03b8;D) = n\u2211 i=1 [ c\u2212i \u03b8i \u2212 c+i log \u2211 k\u2208N+i wike \u03b8k ] + \u03ba5.\nIt is easy to see that the log-likelihood is concave and the log-prior strictly concave in \u03b8. As a result, the log-posterior is strictly concave in \u03b8, which ensures that there exists at most one maximizer.\nNow consider any transition counts {cij} that satisfy c\u2212i = \u2211 j\u2208N\u2212i cji and c+i =\u2211\nj\u2208N+i cij. The log-posterior can be written as\nlog p(\u03b8 | D) = n\u2211 i=1 \u2211 j\u2208N+i cij [ \u03b8j \u2212 log \u2211 k\u2208N+i wike \u03b8k ] + n\u2211 i=1 [ (\u03b1\u2212 1)\u03b8i \u2212 \u03b2e\u03b8i ] + \u03ba3\n\u2264 \u2212n2 \u00b7max i,j logwij + n\u2211 i [ (\u03b1\u2212 1)\u03b8i \u2212 \u03b2e\u03b8i ] + \u03ba3.\nFor \u03b1 > 1, it follows that lim\u2016\u03b8\u2016\u2192\u221e log p(\u03b8 | D) = \u2212\u221e, which ensures that there is at least one maximizer.\nNote that Theorem 5 can easily be extended to independent but non-identical Gamma priors, where \u03bbi \u223c Gamma(\u03b1i, \u03b2i) and \u03b1i 6= \u03b1j, \u03b2i 6= \u03b2j in general."}, {"heading": "B Maximum-Likelihood Estimation", "text": "In this section, we go into the analysis of the ML estimator in depth. From the definition of choice probabilities in (7), it is clear that the likelihood is invariant to a rescaling of the parameters, i.e., `(\u03bb;D) = `(s\u03bb;D) for any s > 0. We will therefore identify parameters up to rescaling.\nB.1 Necessary and Sufficient Conditions\nIn order to provide a data-dependent, necessary and sufficient condition that guarantees that the ML estimate is well-defined, we extend the definition of comparison hypergraph presented in Section 4.3.\nDefinition (Comparison graph). Let G = (V,E) be a directed graph and {aij | (i, j) \u2208 E} be non-negative numbers. The comparison graph induced by {aij} is the directed graph H = (V,E \u2032), where (i, j) \u2208 E \u2032 if and only if there is a node k such that i, j \u2208 N+k and akj > 0.\nThe numbers {aij} can be loosely interpreted as transition counts (although they do not need to be integer). Intuitively, there is an edge (i, j) in the comparison graph whenever there is at least one instance in which i and j were among the alternatives and j was selected. If aij > 0 for all edges, then the comparison graph is equivalent to its hypergraph counterpart, in that every hyperedge induces a clique in the comparison graph. As shown by the next theorem, the notion of (data-dependent) comparison graph leads to a precise characterization of whether the ML estimate is well-defined or not.\nTheorem 6. Let G = (V,E) be a directed graph and {(c\u2212i , c+i )} be the aggregate number of transitions arriving in and originating from i, respectively. Let {aij} be any set of non-negative real numbers that satisfy\u2211\nj\u2208N\u2212i\naji = c \u2212 i , \u2211 j\u2208N+i aij = c + i .\nThen, the maximizer of the log-likelihood (8) exists and is unique (up to rescaling) if and only if the comparison graph induced by {aij} is strongly connected.\nThe proof borrows from Hunter [2004], in particular from the proofs of Lemmas 1 and 2.\nProof. The log-likelihood (8) is not concave in \u03bb, but it can be made concave using the reparametrization \u03bbi = e\u03b8i . We can rewrite the reparametrized log-likelihood using {aij} as\n`(\u03b8) = n\u2211 i=1 \u2211 j\u2208N+i aij [ \u03b8j \u2212 log \u2211 k\u2208N+i wike \u03b8k ] ,\nand, without loss of generality, we can assume that \u2211\ni \u03b8i = 0 and minij wij = 1. First, we shall prove that the super-level set {\u03b8 | `(\u03b8) \u2265 c} is bounded and compact for any c, if and only if the comparison graph is strongly connected. The compactness of all super-level sets ensures that there is at least one maximizer. Pick any unit vector u such that \u2211 i ui = 0, and let \u03b8 = su When s\u2192\u221e, then e\u03b8i > 0 and e\u03b8j \u2192 0 for some i and j. As the comparison graph is strongly connected, there is a path from i to j, and along this path there must be two consecutive nodes i\u2032, j\u2032 such that e\u03b8i\u2032 > 0 and e\u03b8j\u2032 \u2192 0. The existence of the edge (i\u2032, j\u2032) in the comparison graph means that there is a k such that i\u2032, j\u2032 \u2208 N+k and akj\u2032 > 0. Therefore, the log-likelihood can be bounded as\n`(\u03b8) \u2264 akj\u2032 [ \u03b8j\u2032 \u2212 log \u2211 q\u2208N+k wkqe \u03b8q ] \u2264 akj\u2032 [ \u03b8j\u2032 \u2212 log(e\u03b8j\u2032 + e\u03b8i\u2032 ) ] ,\nand lims\u2192\u221e `(\u03b8) = \u2212\u221e. Conversely, suppose that the comparison graph is not strongly connected and partition the vertices into two non-empty subsets S and T such that there is no edge from S to T . Let c > 0 be any positive constant, and take \u03b8\u0303i = \u03b8i + c if i \u2208 S and \u03b8\u0303i = \u03b8i if i \u2208 T (renormalize such that \u2211 i \u03b8\u0303i = 0). Clearly, `(\u03b8\u0303) \u2265 `(\u03b8), and by repeating this procedure \u2016\u03b8\u2016 may be driven to infinity without decreasing the likelihood. Second, we shall prove that if the comparison graph is strongly connected, the loglikelihood is strictly concave (in \u03b8). In particular, for any p \u2208 (0, 1),\n` [p\u03b8 + (1\u2212 p)\u03b7] \u2265 p`(\u03b8) + (1\u2212 p)`(\u03b7), (11)\nwith equality if and only if \u03b8 \u2261 \u03b7 up to a constant shift. Strict concavity ensures that there is at most one maximizer of log-likelihood. We start with H\u00f6lder\u2019s inequality, which implies that, for positive {xk} and {yk}, and p \u2208 (0, 1),\nlog \u2211 k xpky 1\u2212p k \u2264 p log \u2211 k xk + (1\u2212 p) log \u2211 k yk.\nwith equality if and only xk = cyk for some c > 0. Letting xk = wike\u03b8k and yk = wike\u03b7k , we find that for all i\nlog \u2211 k\u2208N+i wike p\u03b8k+(1\u2212p)\u03b7k \u2264 p log \u2211 k\u2208N+i wike \u03b8k + (1\u2212 p) log \u2211 k\u2208N+i wike \u03b7k , (12)\nwith equality if and only if there exists c \u2208 R such that \u03b8k = \u03b7k + c for all k \u2208 N+i . Multiplying by aij and summing over i and j on both sides of (12) shows that the loglikelihood is concave in \u03b8. Now, consider any partition of the vertices into two non-empty subsets S and T . Because the comparison graph is strongly connected, there is always k \u2208 V , i \u2208 S and j \u2208 T such that i, j \u2208 N+k and aki > 0. Therefore, the left and right side of (11) are equal if and only if \u03b8 \u2261 \u03b7 up to a constant shift.\nBounded super-level sets and strict concavity form necessary and sufficient conditions for the existence and uniqueness of the maximizer.\nWe now give a proof for Theorem 1, presented in the main body of text.\nProof of Theorem 1. If the comparison hypergraph is disconnected, then for any data D, the (data-induced) comparison graph is disconnected too. Furthermore, the connected components of the comparison graph are subsets of those of the hypergraph. Partition the vertices into two non-empty subsets S and T such that there is no hyperedge between S to T in the comparison hypergraph. Let A = {i | N+i \u2282 S} and B = {i | N+i \u2282 T}. By construction of the comparison hypergraph, A\u2229B = \u2205 and A\u222aB = V . The log-likelihood can be therefore be rewritten as\n`(\u03b8) = \u2211 i\u2208A \u2211 j\u2208N+i aij [ log \u03bbj \u2212 log \u2211 k\u2208N+i wik\u03bbk ] + \u2211 i\u2208B \u2211 j\u2208N+i aij [ log \u03bbj \u2212 log \u2211 k\u2208N+i wik\u03bbk ] .\nThe sum over A involves only parameters related to nodes in S, while the sum over B involves only parameters related to nodes in T . Because the likelihood is invariant to a rescaling of the parameters, it is easy to see that we can arbitrarily rescale the parameters of the vertices in either S or T without affecting the likelihood.\nVerifying the condition of Theorem 6. In order to verify the necessary and sufficient condition given {(c\u2212i , c+i )}, one has to find a non-negative solution {aij} to the system of equations \u2211\nj\u2208N\u2212i aji = c \u2212 i ,\u2211\nj\u2208N+i\naij = c + i .\nDines [1926] presents a remarkably simple algorithm to find such a non-negative solution. Alternatively, Kumar et al. [2015] suggest recasting the problem as one of maximum flow in a network. However, the computational cost of running Dines\u2019 or max-flow algorithms is significantly greater than that of running ChoiceRank.\nB.2 Example\nTo conclude our discussion, we provide an innocuous-looking example that highlights the difficulty of dealing with the ML estimate. Consider the network structure and traffic data depicted in Figure 6. The network is strongly connected, and its comparison hypergraph is connected as well; as such, the network satisfies the necessary condition stated in Theorem 1 in the main text. Nevertheless, the condition is not sufficient for the ML-estimate to be well-defined. In this example, the (data-dependent) comparison graph is not strongly connected, and it is easy to see that the likelihood can always be increased by increasing \u03bb1, \u03bb2 and \u03bb4. Hence, the ML estimate does not exist.\nIn this simple example, we indicate the edge transitions that generated the observed marginal traffic in bold. Given this information, the comparison graph is easy to find, and the necessary and sufficient conditions of Theorem 6 are easy to check. But in general, finding a set of transitions that is compatible with given marginal per-node traffic data is computationally expensive (see discussion above)."}, {"heading": "C ChoiceRank Algorithm", "text": "In this section, we start by generalizing the ChoiceRank algorithm to the weighted network choice model. We then prove the convergence of this generalized algorithm. Finally, we show how the same algorithm can be obtained from an EM viewpoint by introducing suitable latent variables.\nC.1 Algorithm for the Generalized Model\nUsing the same linear upper-bound on the logarithm as in Section 5 of the main text, we can lower-bound the log-posterior (10) in the weighted model by\nf (t)(\u03bb) = \u03ba2 + n\u2211 i=1 [ (c\u2212i + \u03b1\u2212 1) log \u03bbi \u2212 \u03b2\u03bbi\n\u2212 c+i ( log \u2211 k\u2208N+i wik\u03bb (t) k + \u2211 k\u2208N+i wik\u03bbk\u2211 k\u2208N+i wik\u03bb (t) k \u2212 1 )] ,\n(13)\nwith equality if and only if \u03bb = \u03bb(t). Starting with an arbitrary \u03bb(0) \u2208 Rn>0, we repeatedly maximize the lower-bound f (t). This surrogate optimization problem has a closed form solution, obtained by setting \u2207f (t) to 0:\n\u03bb (t+1) i = c\u2212i + \u03b1\u2212 1\u2211 j\u2208N\u2212i wji\u03b3 (t) j + \u03b2 , where \u03b3(t)j = c+j\u2211 k\u2208N+j wjk\u03bb (t) k . (14)\nThe iterates provably converge to the maximizer of (10), as shown by the following theorem.\nTheorem 7. Let \u03bb? be the unique maximum a-posteriori estimate. Then for any initial \u03bb(0) \u2208 Rn>0 the sequence of iterates defined by (14) converges to \u03bb?.\nThe proof follows that of Hunter\u2019s Theorem 1 [2004].\nProof. Let M : Rn>0 \u2192 Rn>0 be the (continuous) map implicitly defined by one iteration of the algorithm. For conciseness, let g(\u03bb) .= log p(\u03bb | D). As g has a unique maximizer and is concave using the reparametrization \u03bbi = e\u03b8i , it follows that g has a single stationary point. First, observe that the minorization-maximization property guarantees that g [M(\u03bb)] \u2265 g(\u03bb). Combined with the strict concavity of g, this ensures that limt\u2192\u221e g(\u03bb(t)) exists and is unique for any \u03bb(0). Second, g [M(\u03bb)] = g(\u03bb) if and only if \u03bb is a stationary point of g, because the minorizing function is tangent to g at the current iterate. It follows that limt\u2192\u221e \u03bb (t) = \u03bb?.\nTheorem 3 of the main text follows directly by setting wij \u2261 1. For completeness, the edge-streaming implementation adapted to the weighted model is given in Algorithm 2. The only changes with respect to Algorithm 1 (presented in the main text) are in lines 4 and 7: Every message \u03b3i or \u03bbj flowing through an edge (i, j) is multiplied by the edge weight wij.\nAlgorithm 2 ChoiceRank for the weighted model Require: graph G = (V,E), counts {(c\u2212i , c+i )} 1: \u03bb\u2190 [1, . . . , 1] 2: repeat 3: z \u2190 0n . Recompute \u03b3 4: for (i, j) \u2208 E do zi \u2190 zi + wij\u03bbj 5: for i \u2208 V do \u03b3i \u2190 c+i /zi 6: z \u2190 0n . Recompute \u03bb 7: for (i, j) \u2208 E do zj \u2190 zj + wij\u03b3i 8: for i \u2208 V do \u03bbi \u2190 (c\u2212i + \u03b1\u2212 1)/(zi + \u03b2) 9: until \u03bb has converged\nC.2 EM Viewpoint\nThe MM algorithm can be seen from an EM viewpoint, following the ideas of Caron and Doucet [2012]. We introduce n independent random variables Z = {Zi | i = 1, . . . , n}, where\nZi \u223c Gamma ( c+i , \u2211 j\u2208N+i wij\u03bbj ) .\nWith the addition of these latent random variables the complete log-likelihood becomes\n`(\u03bb;D,Z) = `(\u03bb,D) + n\u2211 i=1 log p(zi | D,\u03bb)\n= n\u2211 i=1 [ c\u2212i log \u03bbi \u2212 c+i log \u2211 k\u2208N+i wik\u03bbk ]\n+ n\u2211 i=1 [ c+i log \u2211 k\u2208N+i wik\u03bbk \u2212 zi \u2211 k\u2208N+i wik\u03bbk ] + \u03ba6\n= n\u2211 i=1 [ c\u2212i log \u03bbi \u2212 zi \u2211 k\u2208N+i wik\u03bbk ] + \u03ba6.\nUsing a Gamma(\u03b1, \u03b2) prior for each parameter, the expected value of the log-posterior with respect to the conditional Z | D under the estimate \u03bb(t) is\nQ(\u03bb,\u03bb(t)) = EZ|D,\u03bb(t) [`(\u03bb;D,Z)] + log p(\u03bb)\n= n\u2211 i=1 [ c\u2212i log \u03bbi \u2212 c+i \u2211 k\u2208N+i wik\u03bbk\u2211 k\u2208N+i wik\u03bb (t) k ] + n\u2211 i=1 [ (\u03b1\u2212 1) log \u03bbi \u2212 \u03b2\u03bbi ] + \u03ba7\nThe EM algorithm starts with an initial \u03bb(0) and iteratively refines the estimate by solving the optimization problem \u03bb(t+1) = argmax\u03bbQ(\u03bb,\u03bb(t)). It is not difficult to see that for a given \u03bb(t), maximizing Q(\u03bb,\u03bb(t)) is equivalent to maximizing the minorizing function\nf (t)(\u03bb) defined in (13). Hence, the MM and the EM viewpoint lead to the exact same sequence of iterates.\nThe EM formulation leads to a Gibbs sampler in a relatively straightforward way [Caron and Doucet, 2012]. We leave a systematic treatment of Bayesian inference in the network choice model for future work."}], "references": [{"title": "Generalized Method-of-Moments for Rank Aggregation", "author": ["H. Azari Soufiani", "W.Z. Chen", "D.C. Parkes", "L. Xia"], "venue": "NIPS", "citeRegEx": "Soufiani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Soufiani et al\\.", "year": 2013}, {"title": "Rank Analysis of Incomplete Block Designs: I", "author": ["R.A. Bradley", "M.E. Terry"], "venue": "The Method of Paired Comparisons. Biometrika,", "citeRegEx": "Bradley and Terry.,? \\Q1952\\E", "shortCiteRegEx": "Bradley and Terry.", "year": 1952}, {"title": "The Anatomy of a Large-Scale Hypertextual Web Search Engine", "author": ["S. Brin", "L. Page"], "venue": "In WWW\u201998,", "citeRegEx": "Brin and Page.,? \\Q1998\\E", "shortCiteRegEx": "Brin and Page.", "year": 1998}, {"title": "Efficient Bayesian Inference for Generalized Bradley\u2013Terry models", "author": ["F. Caron", "A. Doucet"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Caron and Doucet.,? \\Q2012\\E", "shortCiteRegEx": "Caron and Doucet.", "year": 2012}, {"title": "Statistical Inference", "author": ["G. Casella", "R.L. Berger"], "venue": "Duxbury Press, second edition,", "citeRegEx": "Casella and Berger.,? \\Q2002\\E", "shortCiteRegEx": "Casella and Berger.", "year": 2002}, {"title": "On Positive Solutions of a System of Linear Equations", "author": ["L.L. Dines"], "venue": "Annals of Mathematics,", "citeRegEx": "Dines.,? \\Q1926\\E", "shortCiteRegEx": "Dines.", "year": 1926}, {"title": "The Rating Of Chess Players", "author": ["A. Elo"], "venue": "Past & Present. Arco,", "citeRegEx": "Elo.,? \\Q1978\\E", "shortCiteRegEx": "Elo.", "year": 1978}, {"title": "Graphx: Graph Processing in a Distributed Dataflow Framework", "author": ["J.E. Gonzalez", "R.S. Xin", "A. Dave", "D. Crankshaw", "M.J. Franklin", "I. Stoica"], "venue": "In OSDI\u201914,", "citeRegEx": "Gonzalez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gonzalez et al\\.", "year": 2014}, {"title": "Minimax-optimal Inference from Partial Rankings", "author": ["B. Hajek", "S. Oh", "J. Xu"], "venue": "NIPS", "citeRegEx": "Hajek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hajek et al\\.", "year": 2014}, {"title": "Classification by pairwise coupling", "author": ["T. Hastie", "R. Tibshirani"], "venue": "The Annals of Statistics,", "citeRegEx": "Hastie and Tibshirani.,? \\Q1998\\E", "shortCiteRegEx": "Hastie and Tibshirani.", "year": 1998}, {"title": "MM algorithms for generalized Bradley\u2013Terry models", "author": ["D.R. Hunter"], "venue": "The Annals of Statistics,", "citeRegEx": "Hunter.,? \\Q2004\\E", "shortCiteRegEx": "Hunter.", "year": 2004}, {"title": "Inverting a Steady-State", "author": ["R. Kumar", "A. Tomkins", "S. Vassilvitskii", "E. Vee"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Optimization Transfer Using Surrogate Objective Functions", "author": ["K. Lange", "D.R. Hunter", "I. Yang"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Lange et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lange et al\\.", "year": 2000}, {"title": "BrowseRank: Letting Web Users Vote For Page Importance", "author": ["Y. Liu", "B. Gao", "T.-Y. Liu", "Y. Zhang", "Z. Ma", "S. He", "H. Li"], "venue": "In SIGIR\u201908,", "citeRegEx": "Liu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "Individual Choice behavior: A Theoretical Analysis", "author": ["R.D. Luce"], "venue": null, "citeRegEx": "Luce.,? \\Q1959\\E", "shortCiteRegEx": "Luce.", "year": 1959}, {"title": "Pregel: A System for Large-Scale Graph Processing", "author": ["G. Malewicz", "M.H. Austern", "A.J.C. Bik", "J.C. Dehnert", "I. Horn", "N. Leiser", "G. Czajkowski"], "venue": "In SIGMOD\u201910,", "citeRegEx": "Malewicz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Malewicz et al\\.", "year": 2010}, {"title": "Fast and Accurate Inference of Plackett\u2013Luce Models", "author": ["L. Maystre", "M. Grossglauser"], "venue": "NIPS", "citeRegEx": "Maystre and Grossglauser.,? \\Q2015\\E", "shortCiteRegEx": "Maystre and Grossglauser.", "year": 2015}, {"title": "Conditional logit analysis of qualitative choice behavior", "author": ["D. McFadden"], "venue": "Frontiers in Econometrics,", "citeRegEx": "McFadden.,? \\Q1973\\E", "shortCiteRegEx": "McFadden.", "year": 1973}, {"title": "Scalability! But at what COST", "author": ["F. McSherry", "M. Isard", "D.G. Murray"], "venue": "In HotOS XV,", "citeRegEx": "McSherry et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McSherry et al\\.", "year": 2015}, {"title": "Graph Structure in the Web\u2014Revisited: A Trick of the Heavy Tail", "author": ["R. Meusel", "S. Vigna", "O. Lehmberg", "C. Bizer"], "venue": "In WWW\u201914 Companion,", "citeRegEx": "Meusel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Meusel et al\\.", "year": 2014}, {"title": "Iterative Ranking from Pair-wise Comparisons", "author": ["S. Negahban", "S. Oh", "D. Shah"], "venue": "NIPS", "citeRegEx": "Negahban et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2012}, {"title": "A New Paradigm for Ranking Pages on the World Wide Web", "author": ["J.A. Tomlin"], "venue": "In WWW\u201903,", "citeRegEx": "Tomlin.,? \\Q2003\\E", "shortCiteRegEx": "Tomlin.", "year": 2003}, {"title": "Discrete Choice Methods with Simulation", "author": ["K.E. Train"], "venue": null, "citeRegEx": "Train.,? \\Q2009\\E", "shortCiteRegEx": "Train.", "year": 2009}, {"title": "Parameter Estimation for Generalized Thurstone Choice Models", "author": ["M. Vojnovic", "S.-Y. Yun"], "venue": "In ICML 2016,", "citeRegEx": "Vojnovic and Yun.,? \\Q2016\\E", "shortCiteRegEx": "Vojnovic and Yun.", "year": 2016}, {"title": "URL https://dx", "author": ["E. Wulczyn", "D. Taraborelli. Wikipedia Clickstream. Apr."], "venue": "doi.org/10.6084/m9.figshare.1305770.v16.", "citeRegEx": "Wulczyn and Apr.,? 2016", "shortCiteRegEx": "Wulczyn and Apr.", "year": 2016}, {"title": "Die Berechnung der Turnier-Ergebnisse als ein Maximumproblem der Wahrscheinlichkeitsrechnung", "author": ["E. Zermelo"], "venue": "Mathematische Zeitschrift,", "citeRegEx": "Zermelo.,? \\Q1928\\E", "shortCiteRegEx": "Zermelo.", "year": 1928}], "referenceMentions": [{"referenceID": 11, "context": "Building upon recent work by Kumar et al. [2015], we present a statistical framework that tackles a general formulation of the problem: given a network (representing possible transitions between nodes) and the marginal traffic at each node, recover the transition probabilities.", "startOffset": 29, "endOffset": 49}, {"referenceID": 14, "context": "Choices are assumed to be independent and generated according to Luce\u2019s model [Luce, 1959]: each node in the network is chararacterized by a latent strength parameter, and (stochastic) choice outcomes tend to favor nodes with greater strengths.", "startOffset": 78, "endOffset": 90}, {"referenceID": 2, "context": "Users navigate from node to node along the edges of the network by making a choice between adjacent nodes at each step, reminiscent of the random-surfer model introduced by Brin and Page [1998]. Choices are assumed to be independent and generated according to Luce\u2019s model [Luce, 1959]: each node in the network is chararacterized by a latent strength parameter, and (stochastic) choice outcomes tend to favor nodes with greater strengths.", "startOffset": 173, "endOffset": 194}, {"referenceID": 14, "context": "To define the transition probabilities, we posit Luce\u2019s well-known choice axiom that states that the odds of choosing item j over item j\u2032 do not depend on the rest of the alternatives [Luce, 1959].", "startOffset": 184, "endOffset": 196}, {"referenceID": 11, "context": "3 Related Work A variant of the network choice model was recently introduced by Kumar et al. [2015], in an article that lays much of the groundwork for the present paper.", "startOffset": 80, "endOffset": 100}, {"referenceID": 14, "context": "Several decades before Luce\u2019s seminal book [Luce, 1959], Zermelo [1928] proposed a model and an algorithm that estimates the strengths of chess players based on pairwise comparison outcomes (his model would later be rediscovered by Bradley and Terry [1952]).", "startOffset": 43, "endOffset": 55}, {"referenceID": 17, "context": "Finally, we note that models based on Luce\u2019s axiom have been successfully applied to problems ranging from ranking players based on game outcomes [Zermelo, 1928, Elo, 1978] to understanding consumer behavior based on discrete choices [McFadden, 1973], and to discriminating among multiple classes based on the output of pairwise classifiers [Hastie and Tibshirani, 1998].", "startOffset": 234, "endOffset": 250}, {"referenceID": 9, "context": "Finally, we note that models based on Luce\u2019s axiom have been successfully applied to problems ranging from ranking players based on game outcomes [Zermelo, 1928, Elo, 1978] to understanding consumer behavior based on discrete choices [McFadden, 1973], and to discriminating among multiple classes based on the output of pairwise classifiers [Hastie and Tibshirani, 1998].", "startOffset": 341, "endOffset": 370}, {"referenceID": 0, "context": "Several decades before Luce\u2019s seminal book [Luce, 1959], Zermelo [1928] proposed a model and an algorithm that estimates the strengths of chess players based on pairwise comparison outcomes (his model would later be rediscovered by Bradley and Terry [1952]).", "startOffset": 232, "endOffset": 257}, {"referenceID": 0, "context": "Several decades before Luce\u2019s seminal book [Luce, 1959], Zermelo [1928] proposed a model and an algorithm that estimates the strengths of chess players based on pairwise comparison outcomes (his model would later be rediscovered by Bradley and Terry [1952]). More recently, Hunter [2004] explained Zermelo\u2019s algorithm from the perspective of the minorization-maximization (MM) method.", "startOffset": 232, "endOffset": 288}, {"referenceID": 0, "context": "Several decades before Luce\u2019s seminal book [Luce, 1959], Zermelo [1928] proposed a model and an algorithm that estimates the strengths of chess players based on pairwise comparison outcomes (his model would later be rediscovered by Bradley and Terry [1952]). More recently, Hunter [2004] explained Zermelo\u2019s algorithm from the perspective of the minorization-maximization (MM) method. This method is easily generalized to other models that are based on Luce\u2019s axiom, and it yields simple, provably convergent algorithms for maximum-likelihood (ML) or maximuma-posteriori point estimates. Caron and Doucet [2012] observe that these MM algorithms can be further recast as expectation-maximization (EM) algorithms by introducing suitable latent variables.", "startOffset": 232, "endOffset": 612}, {"referenceID": 2, "context": "A method that has undoubtedly had a tremendous impact in this context is PageRank [Brin and Page, 1998].", "startOffset": 82, "endOffset": 103}, {"referenceID": 13, "context": "At the other end of the spectrum, BrowseRank [Liu et al., 2008] uses detailed data collected in users\u2019 browsers to improve on PageRank.", "startOffset": 45, "endOffset": 63}, {"referenceID": 20, "context": "The HOTness method proposed by Tomlin [2003] is somewhat related, but tries to tackle a harder problem.", "startOffset": 31, "endOffset": 45}, {"referenceID": 11, "context": "We then connect our statistical model to the steady-state inversion problem defined by Kumar et al. [2015]. Guided by this connection, we study the maximum-likelihood (ML) estimate of model parameters, but find that the estimate is likely to be ill-defined in many scenarios of practical interest.", "startOffset": 87, "endOffset": 107}, {"referenceID": 11, "context": "We then connect our statistical model to the steady-state inversion problem defined by Kumar et al. [2015]. Guided by this connection, we study the maximum-likelihood (ML) estimate of model parameters, but find that the estimate is likely to be ill-defined in many scenarios of practical interest. Lastly, we study how to overcome this issue by introducing a prior distribution on the parameters \u03bb; the prior guarantees that the inference problem is well-posed. For simplicity of exposition, we present our results for Luce\u2019s standard choice model defined in (1). Our developments extend to the model variant proposed by Kumar et al. [2015], where choice probabilities can be modulated by edge weights.", "startOffset": 87, "endOffset": 641}, {"referenceID": 11, "context": "2 Connection to the Steady-State Inversion Problem In recent work, Kumar et al. [2015] define the problem of steady-state inversion as follows: Given a strongly-connected directed graph G = (V,E) and a target distribution over the nodes \u03c0, find a Markov chain on G with stationary distribution \u03c0.", "startOffset": 67, "endOffset": 87}, {"referenceID": 10, "context": "Equipped with this definition, we can state the following theorem that is a reformulation of a well-known result for Luce\u2019s choice model [Hunter, 2004].", "startOffset": 137, "endOffset": 151}, {"referenceID": 3, "context": "4 Well-Posed Inference Following the ideas of Caron and Doucet [2012], we introduce an independent Gamma prior on each parameter, i.", "startOffset": 46, "endOffset": 70}, {"referenceID": 10, "context": "In the spirit of the algorithms of Hunter [2004] for variants of Luce\u2019s choice model, we develop a minorization-maximization (MM) algorithm.", "startOffset": 35, "endOffset": 49}, {"referenceID": 12, "context": "Furthermore, it is known that MM algorithms exhibit geometric convergence in a neighborhood of the maximizer [Lange et al., 2000].", "startOffset": 109, "endOffset": 129}, {"referenceID": 15, "context": "Furthermore, the algorithm can be conveniently expressed in the well-known vertex-centric programming model [Malewicz et al., 2010].", "startOffset": 108, "endOffset": 131}, {"referenceID": 7, "context": "This makes it easy to implement ChoiceRank inside scalable, optimized graph-processing systems such as Apache Spark [Gonzalez et al., 2014].", "startOffset": 116, "endOffset": 139}, {"referenceID": 3, "context": "The update (6) can also be explained from an expectation-maximization (EM) viewpoint, by introducing suitable latent variables [Caron and Doucet, 2012].", "startOffset": 127, "endOffset": 151}, {"referenceID": 18, "context": "2 Scaling ChoiceRank to Billions of Nodes To demonstrate ChoiceRank\u2019s scalability, we develop a simple implementation in the Rust programming language, based on the ideas of COST [McSherry et al., 2015].", "startOffset": 179, "endOffset": 202}, {"referenceID": 19, "context": "5 billion nodes and 128 billion edges [Meusel et al., 2014].", "startOffset": 38, "endOffset": 59}, {"referenceID": 15, "context": "It is worth noting that despite tackling different problems, the ChoiceRank algorithm exhibits interesting similarities with a message-passing implementation of PageRank commonly used in scalable graph-parallel systems such as Pregel [Malewicz et al., 2010] and Spark [Gonzalez et al.", "startOffset": 234, "endOffset": 257}, {"referenceID": 7, "context": ", 2010] and Spark [Gonzalez et al., 2014].", "startOffset": 18, "endOffset": 41}, {"referenceID": 18, "context": "For comparison, using the COST code [McSherry et al., 2015] we run 20 iterations of PageRank on the same hardware and data.", "startOffset": 36, "endOffset": 59}, {"referenceID": 11, "context": "This method generalizes and extends ideas recently presented by Kumar et al. [2015]. We demonstrate that in spite of the strong model assumptions needed to learn O(n) probabilities from O(n) observations, the method still manages to recover the transition probabilities to a good level of accuracy on two clickstream datasets, and shows promise for applications beyond clickstream data.", "startOffset": 64, "endOffset": 84}, {"referenceID": 11, "context": "Kumar et al. [2015] propose the following generalization of Luce\u2019s choice model.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "13 in Casella and Berger [2002] states that {(ci , c+i )} is a minimally sufficient statistic for \u03bb if and only if, for any {cij} and {dij} in the support of f , f({cij} | \u03bb) f({dij} | \u03bb) is independent of \u03bb \u21d0\u21d2 (ci , c+i ) = (di , d+i ) \u2200i.", "startOffset": 6, "endOffset": 32}, {"referenceID": 10, "context": "The proof borrows from Hunter [2004], in particular from the proofs of Lemmas 1 and 2.", "startOffset": 23, "endOffset": 37}, {"referenceID": 10, "context": "The proof follows that of Hunter\u2019s Theorem 1 [2004]. Proof.", "startOffset": 26, "endOffset": 52}, {"referenceID": 3, "context": "2 EM Viewpoint The MM algorithm can be seen from an EM viewpoint, following the ideas of Caron and Doucet [2012]. We introduce n independent random variables Z = {Zi | i = 1, .", "startOffset": 89, "endOffset": 113}], "year": 2017, "abstractText": "Understanding how users navigate in a network is of high interest in many applications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce\u2019s axiom. In this case, the O(n) marginal counts of node visits are a sufficient statistic for the O(n2) transition probabilities. We show how to make the inference problem well-posed regardless of the network\u2019s structure, and we present ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We apply the model to two clickstream datasets and show that it successfully recovers the transition probabilities using only the network structure and marginal (node-level) traffic data. Finally, we also consider an application to mobility networks and apply the model to one year of rides on New York City\u2019s bicycle-sharing system.", "creator": "LaTeX with hyperref package"}}}