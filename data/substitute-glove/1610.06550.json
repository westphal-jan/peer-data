{"id": "1610.06550", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Neural Machine Translation with Characters and Hierarchical Encoding", "abstract": "Most directly Neural Machine Translation models use organizations addition presents or almost listen as their unit many input and 2.2. We propose short class with though corporatist char2word encoder, that one individual films both as linear with output. We first understood how this filesystem whereby of still mind inductive prevents psychometrics variables, most seen that much growth words though. Secondly, by relatable engineering attention plots months however decoder so go that long model imagines time filter mainly words into hand piece templates less that often, such same names work country, are represented character by character.", "histories": [["v1", "Thu, 20 Oct 2016 19:33:02 GMT  (592kb,D)", "http://arxiv.org/abs/1610.06550v1", "8 pages, 7 figures"]], "COMMENTS": "8 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander rosenberg johansen", "jonas meinertz hansen", "elias khazen obeid", "casper kaae s{\\o}nderby", "ole winther"], "accepted": false, "id": "1610.06550"}, "pdf": {"name": "1610.06550.pdf", "metadata": {"source": "CRF", "title": "NEURAL MACHINE TRANSLATION WITH CHARACTERS AND HIERARCHICAL ENCODING", "authors": ["Alexander Rosenberg Johansen", "Jonas Meinertz Hansen", "Elias Khazen Obeid", "Casper Kaae S\u00f8nderby", "Ole Winther"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "Neural Machine Translation (NMT) is the application of deep neural networks to translation of text. NMT is based on an end-to-end trainable algorithm that can learn to translate just by being presented with translated language pairs. Despite being a relatively new approach, NMT has in recent years surpassed classical statistical machine translation models and now holds state-of-the-art results [Chung et al., 2016, Luong and Manning, 2016, Wu et al., 2016].\nEarly NMT models introduced by Cho et al. [2014a], Kalchbrenner and Blunsom [2013], Sutskever et al. [2014] are based on the encoder-decoder network architecture. Here the encoder compresses an input sequence of variable length from the source language to a fixed-length vector representing the sentiment of the sentence. The decoder, takes the fixed-length representation as input and produces a variable length translation in the target language. However, due to the fixed length representation the na\u00efve encoder-decoder approach have limitations when translating longer sequences.\nTo overcome this shortcoming, the attention mechanism proposed by Bahdanau et al. [2014] assists the decoder by learning to selectively attend to parts of the input sequence, which it deems most relevant for generating the next element in the output sequence and effectively reducing the distance from encoding to decoding. This approach has made it possible for encoder-decoder models to produce high quality\ntranslations over longer sequences. However, it suffers from the significant amount of computational power and memory needed to compute the relevance of every element of the input sequence for every element of the output sequence.\nFor this reason, training the models on individual characters is not practical, and most current solutions instead use word segmentation [Bahdanau et al., 2014, Sutskever et al., 2014] or multiple characters [Schuster and Nakajima, 2012, Sennrich et al., 2015, Wu et al., 2016] to represent sentences. However, this high-level segmentation approach has a set of drawbacks; most Latin based languages have millions of words with the majority occurring rarely. To handle this, current models use confined dictionaries with only the k most common words with the remaining words being represented by a special <UNK>-token [Bahdanau et al., 2014, Sutskever et al., 2014]. As a result, names, places, and other rare words are typically translated as unknown by these models. Further, when all words are represented as separate entities, the model has to learn how every word is related to one-another, which can be challenging for rare words even if they are obvious variations of more frequent words [Luong and Manning, 2016]. Figure 2 illustrates some of the challenges of characters versus words for the encoder-decoder model.\nTwo branches of methods to circumvent these drawbacks have been proposed. The first branch is based on extending\nar X\niv :1\n61 0.\n06 55\n0v 1\n[ cs\n.C L\n] 2\n0 O\nct 2\n01 6\nthe current word-based encoder-decoder model to incorporate modules, such as dictionary look-up for out-of-dictionary words [Jean et al., 2014, Luong et al., 2014]. The second branch, which we will investigate in this work, is moving towards smaller units of computation.\nIn this paper we demonstrate models that use a new char2word mechanism (illustrated in figure 1) during encoding, which reduces long character-level input sequences to word-level representations. This approach has the advantages of keeping a small alphabet of possible input values and preserving relations between words that are spelled similarly, while significantly reducing the number of elements that the attention mechanism needs to consider for each output element it generates. Using this method the decoder\u2019s memory and computational requirements are reduced, making it feasible to train the models on long sequences of characters as input and output. Thus avoiding the drawbacks of word based models described above. And lastly, we give a qualitative analysis of attention plots produced by a character-level encoder-decoder model with and without the hierarchical encoding mechanism, char2word. This shed light on how a character-level model uses attention, which might explain some of the success behind the BPE and hybrid models."}, {"heading": "2. RELATED WORK", "text": "Other approaches to circumventing the increase in sequence lengths while reducing the dictionary size have been proposed: First, byte-pair Encoding (BPE) [Sennrich et al., 2015], currently holding state-of-the-art in the WMT\u201914 English-to-French and English-to-German [Wu et al., 2016], where the dictionary is a combination of the most common characters. In practice this means that most frequent words are encoded using fewer bits than less frequent words. Secondly, a hybrid model [Luong and Manning, 2016] where a word encoder-decoder consults a character encoder-decoder when confronted with out-of-dictionary words. Thirdly, pretrained word-based encoder-decoder models with character input used for creating word embeddings [Ling et al., 2015]\nhave been shown to achieve similar results to word-based approaches. As a last mention, character decoder with BPE encoder has shown to be end-to-end trained successfully [Chung et al., 2016].\nWu et al. [2016] provides a good summary and large-scale demonstration of many of the techniques that are known to work well for NMT and RNNs in general. The RNN encoderdecoder with attention approach is used not only within machine translation, but can be regarded as a general architecture to extract information from a sequence and answer some type of question related to it [Kumar et al., 2015]."}, {"heading": "3. MATERIALS AND METHODS", "text": "First we give a brief description of the Neural Machine Translation model, afterwards we will go into detail of explaining our proposed architecture for character and word segmentation."}, {"heading": "3.1. Neural Machine Translation", "text": "From a probabilistic perspective, translation can be defined by maximizing the conditional probability of arg maxy p(y|x), where y1, y2, . . . , yTy is the target sequence and x1, x2, . . . , xTx is the source sequence. The conditional probability p(y|x) is modelled by an encoder-decoder model where the encoder and the decoder are modelled by separate Recurrent Neural Networks (RNNs) and the whole model is trained end-to-end on a large parallel corpus. The model uses memory based RNN variants, as they enable modelling of longer dependencies in the sequences [Cho et al., 2014b, Hochreiter and Schmidhuber, 1997].\nThe encoder part (input RNN) computes a set of hidden representations, h1, h2, . . . , hTx based on the input\nht = h(xt, ht\u22121) (1)\nwhere h is a RNN with memory cells, ht \u2208 Rmh is a hidden state representation at time step t, with mh hidden units.\nThe decoder part (output RNN) then computes a context vector, ct, based on the hidden representations from the encoder:\nct = q(h1, h2, . . . , hTx), (2)\nwhere q is a function that takes a set of hidden representations and returns a context vector ct \u2208 Rmc where mc number of context units. For a decoder without attention, the value of ct is the same for all time steps.\nFinally the decoder combines the previous predictions, y<t, and the context vector, ct, to predict the next unit (word, BPE, or character), such that it maximises the log conditional\nprobability\nlog p(y | x) = Ty\u2211 t=1 log p(yt | y<t, c), (3)\np(yt | y<t, ct) = g(yt\u22121, st, ct), (4)\nwhere g is a non-linear, potentially multi-layered function that outputs the probability of yt, ande st is the hidden state of the decoder RNN, such that\nst = f(st\u22121, yt\u22121, ct). (5)\nWe minimise the cross entropy loss averaged over all time steps with n sized mini-batches and add L2 regularisation, such that\nJ = \u2212 n\u2211\ni=1\nlog p(yi | xi) + \u03bb ( N \u2032\u2211\nn\u2032=1\n\u03b82n\u2032\n) ,\nwhere \u03bb is a tune-able hyper-parameter, N \u2032 is the number of non-bias weights and \u03b8 is the weights in the neural network."}, {"heading": "3.1.1. Attention", "text": "As motivated in section 1, the attention mechanism can compute a new context vector ct for every time step by combining the hidden representations from the encoder as well as the previous hidden state, st\u22121, of the decoder\nct = Tx\u2211 j=1 atjhj (6)\nwhere the weight parameter atj of each annotation hj is computed as\natj = exp(etj)\u2211Tx k exp(etk)\n(7)\nand we have that\netj = a(st\u22121, hj) (8)\nwhere atj and etj reflect the importance of hj , w.r.t. the previous decoder state st\u22121. The attention function, a, is a nonlinear, possibly multi-layered neural network.\nThe encoder-decoder and attention model is trained jointly to minimise the loss function."}, {"heading": "4. OUR MODEL", "text": "We propose two models: The char-to-char NMT model and the char2word-to-char NMT model. Both models build on the encoder-decoder model with attention as defined in section 3.1 and section 3.1.1. Below we will give specific model definitions.\n4.1. The char encoder\nOur character-level encoder (referred to as the char encoder) is built upon a bi-directional RNN [Schuster and Paliwal, 1997]. The encoder function, ht, in equation (1) becomes\nht =\n[ hf (Ext, \u2212\u2212\u2192 ht\u22121)\nhb(Ext, \u2190\u2212\u2212 ht+1),\n] (9)\nwhere xt \u2208 {0, 1}mx is a one-hot encoded vector and mx is the amount of input classes, E \u2208 Rme\u00d7mx is an embedding matrix with me being the size of the embedding, hf and hb are RNN functions and h is initialised as h0 = 0.\nThe char encoder is illustrated with the yellow arrows and blue circles in figure 3.\n4.2. The char2word encoder\nThe character-to-word-level encoder (referred to as the char2word encoder) samples states from the forward pass of the char encoder defined in the above section. The states it samples are based on the locations of spaces in the original text, resulting in a sequence of outputs from the char encoder that essentially represents the words from the text and acts as their embeddings. We sample the indices from \u2212\u2192 h , such that\nhspacest = \u2212\u2192 h \u03d5t, (10)\nwhere \u2212\u2192 h t is defined from above equation (9) and \u03d5 is an ordered list of indices defining spaces in the input sequence x. Given hspaces equation (9) is used with hspacest replacing Ext.\nThe char2word encoder is illustrated with the yellow arrows and blue circles in figure 1.\nA result of this \u201cdownsampling\u201d by using spaces, the char2word encoder only has about a fifth of the hidden states the char encoder has. As we described in the introduction, the computationally expensive part of the encoderdecoder with attention is the attention part. By significantly reducing the encoder we could train the char2word encoder in half the time compared to the char encoder.\n4.3. The char decoder\nOur character-level decoder (referred to as the char decoder) works with characters as the smallest unit of computation and decodes one character at a time. The decoder uses a RNN and the attention mechanism [Bahdanau et al., 2014] when decoding each character.\nThe new state in our decoder RNN, st, as defined in equation (5) is computed as follows\nst = f(st\u22121, yt\u22121, ct), (11)\nst = f( [ E\u2032pt\u22121 ct ] , si\u22121) (12)\npt\u22121 = arg max(yt\u22121), (13)\nwhere pt\u22121 \u2208 0, 1kp is a one-hot encoded vector with kp being the amount of input classes, E\u2032 \u2208 Rm\u2032e\u00d7mp is an embedding matrix with m\u2032e being the size of the embedding, f is a RNN function and s is initialised as s0 = hTx ."}, {"heading": "4.3.1. Attention mechanism", "text": "The attention model a (defined in equation (14)) is used to compute the context ct for time step t, which is utilised by the decoder to perform variable length attention. The attention function, a, was parametrized as\na(st\u22121, hj) = v T a tanh(Wast\u22121 + Uahj + ba), (14)\nwhere Wa \u2208 Rms\u00d7ms , Ua \u2208 Rms\u00d7mh , va \u2208 Rmh , ba \u2208 Rmh , ms is the amount of hidden units in the decoder and mh is the amount of hidden units in the encoder. As Uahj does not depend on t, we can pre-compute it in advance for optimisation purposes."}, {"heading": "4.3.2. Output function", "text": "The output of the decoder g(yt\u22121, st, ct) uses a linear combination of the current hidden state in the decoder, st, followed by a softmax function.\ng(yt\u22121, st, ct) = exp(Wyst + by)\u2211K k=1 exp(Wysi + by) , (15)\nwhere Wy \u2208 RK\u00d7ms , by \u2208 RK and K is the amount of output classes.\nWe use the same decoder with attention for both the char-to-char and char2word-to-charmodel, which is illustrated in figure 1 and figure 3. The main difference is that our figure 1 model has significantly lower amount of units to attend over."}, {"heading": "5. EXPERIMENTS", "text": "All models were evaluated using the BLEU score1 [Papineni et al., 2002]."}, {"heading": "5.1. Data and Preprocessing", "text": "We trained our models on two different datasets of language pairs from the WMT\u201915: En-De (4.5M) and De-En (4.5M). For validation we used the newstest2013 and for testing we used newstest2014 and newstest2015.\nThe data preprocessing applies is identical to Chung et al. [2016] on En-De and De-Enwith the source sentence length set to 250 characters instead of 50 BPE units. In short, that means; We normalise punctuations and tokenise using Moses scripts2. We exclude all samples where the source sentence exceed 250 characters and the target sentence exceed 500 characters. The source and target language has separate dictionaries, each containing the 300 most common characters. Characters not in the dictionary is replaced with an unknown token."}, {"heading": "5.2. Training details", "text": "The model hyperparameters are listed in tables 1 and 2. For the RNN functions in the encoder and the decoder we use gated recurrent units (GRU) [Cho et al., 2014b]. For training we use back-propagation with stochastic-gradient descent using the Adam optimiser [Kingma and Ba, 2014] with a learning rate of \u03b1 = 0.001. For L2 regularization we set \u03bb = 1\u00d7 10\u22126. In order to stabilise training and avoid exploding gradients, the norms of the gradients are clipped with a threshold of 1 before updating the parameters. All models are implemented using TensorFlow [Abadi et al., 2016] and the code and details of the setup are available on GitHub3."}, {"heading": "5.2.1. Batch details", "text": "When training with batches, all sequences must be padded to match the longest sequence in the batch, and the recurrent layers must do the full set of computations for all samples and all timesteps, which can result in a lot of wasted resources [Hannun et al., 2014] (see figure 4). Training translation models is further complicated by the fact that source and target sentences, while correlated, may have different lengths, and it is necessary to consider both when constructing batches in order to utilize computation power and RAM optimally.\nTo circumvent this issue, we start each epoch by shuffling all samples in the dataset and sorting them with a stable sorting algorithm according to both the source and target sentence\n1We used the multi-bleu.perl script from Moses (https:// github.com/moses-smt/mosesdecoder).\n2From Moses (https://github.com/moses-smt/ mosesdecoder) using normalize-punctuation.perl and tokenizer.perl\n3https://github.com/Styrke/master-code\nlengths. This ensures that any two samples in the dataset that have almost the same source and target sentence lengths are located close to each other in the sorted list while the exact order of samples varies between epochs. To pack a batch we simply started adding samples from the sorted sample list to the batch, until we reached the maximal total allowed character threshold (which we set to 50,000) for the full batch with padding after which we would start on a new batch. Finally all the batches are fed in random order to the model for training until all samples have been trained on, and a new epoch begins. Figure 5 illustrates what such dynamic batches might look like."}, {"heading": "5.3. Results", "text": ""}, {"heading": "5.3.1. Quantitative", "text": "The quantitative results of our models are illustrated in table 3. Notice that the char2word-to-char model outperforms the char-to-charmodel on all datasets (average 1.28 BLEU performance increase). This could be an indication that either having hierarchical, word-like, representations on the encoder or simply the fact that the encoder was significantly smaller, helps in NMT when using a character decoder\nwith attention."}, {"heading": "5.3.2. Qualitative", "text": "Plotting the weights of atj (defined at equation (14)) is popular in NMT research, as these gives an indication of where the model found relevant information while decoding. We have provided plots of both our char-to-char- and char2word-to-char models in figures 6 and 7. The more intense the blue colour, the higher the values of atj at that point. Notice that each column corresponds to the decoding of a single unit, resulting in each column summing to 1.\nThe char-to-char attention plot, attending over every character, interestingly indicates that words that would normally\nbe considered out-of-dictionary (see Lisette Verhaig in figure 6) are translated character by character-by-character, whereas common words are attended at the end/start of each word 4 to use as a single embedding. This observation might explain why using hierarchical encoding improves performance. BPE based models and the hybrid word-char model by Luong and Manning [2016] effectively works in the same manner, when translating common words BPE- and hybrid word-char models will work on a word level, whereas with rare words the BPE will work with sub-parts of the word (maybe even characters) and the hybrid approach will use character representations.\nThe char2word-to-char attention plot has words, or character-made embeddings of words, to perform attention over. The attention plot seems very similar to the BPE-toChar plot proposed by Chung et al. [2016]. This might indicate that it is possible to imitate lexeme (word) based models using smaller dictionaries and preserving relationship between words."}, {"heading": "6. CONCLUSION", "text": "We have proposed a pure character based encoder-decoder model with attention using a hierarchical encoding. We find that the hierarchical encoding, using our newly proposed char2word encoding mechanism, improves the the BLEU score by an average of 1.28 compared to models using a standard character encoder.\nQualitatively, we find that the attention of a character model without hierarchical encoding learns to make hierarchical representations even without being explicitly told to do so, by switching between word and character embeddings for common and rare words. This observation is in line with current research on Byte-Pair-Encoding- and hybrid word-character models, as these models uses word like embeddings for common words and sub-words or characters for rare words.\nFurthermore, qualitatively we find that our hierarchical encoding finds lexemes in the source sentence when decoding similarly to current models with much larger dictionaries using Byte-Pair-Encoding.\n4As we use a bi-directional RNN, full information will be available at both the end and start of a word"}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["ter", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul A. Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "CoRR, abs/1603.04467,", "citeRegEx": "ter et al\\.,? \\Q2016\\E", "shortCiteRegEx": "ter et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "CoRR, abs/1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoderdecoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "CoRR, abs/1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1603.06147,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Y. Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "Andrew Y. Ng"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "CoRR, abs/1412.2007,", "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "CoRR, abs/1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black"], "venue": "CoRR, abs/1511.04586,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning"], "venue": "CoRR, abs/1604.00788,", "citeRegEx": "Luong and Manning.,? \\Q2016\\E", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "CoRR, abs/1410.8206,", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Trans. Sig. Proc.,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Japanese and korean voice search", "author": ["Mike Schuster", "Kaisuke Nakajima"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Schuster and Nakajima.,? \\Q2012\\E", "shortCiteRegEx": "Schuster and Nakajima.", "year": 2012}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "CoRR, abs/1508.07909,", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Early NMT models introduced by Cho et al. [2014a], Kalchbrenner and Blunsom [2013], Sutskever et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 1, "context": "Early NMT models introduced by Cho et al. [2014a], Kalchbrenner and Blunsom [2013], Sutskever et al.", "startOffset": 31, "endOffset": 83}, {"referenceID": 1, "context": "Early NMT models introduced by Cho et al. [2014a], Kalchbrenner and Blunsom [2013], Sutskever et al. [2014] are based on the encoder-decoder network architecture.", "startOffset": 31, "endOffset": 108}, {"referenceID": 1, "context": "To overcome this shortcoming, the attention mechanism proposed by Bahdanau et al. [2014] assists the decoder by learning to selectively attend to parts of the input sequence, which it deems most relevant for generating the next element in the output sequence and effectively reducing the distance from encoding to decoding.", "startOffset": 66, "endOffset": 89}, {"referenceID": 12, "context": "Further, when all words are represented as separate entities, the model has to learn how every word is related to one-another, which can be challenging for rare words even if they are obvious variations of more frequent words [Luong and Manning, 2016].", "startOffset": 226, "endOffset": 251}, {"referenceID": 17, "context": "Other approaches to circumventing the increase in sequence lengths while reducing the dictionary size have been proposed: First, byte-pair Encoding (BPE) [Sennrich et al., 2015], currently holding state-of-the-art in the WMT\u201914 English-to-French and English-to-German [Wu et al.", "startOffset": 154, "endOffset": 177}, {"referenceID": 19, "context": ", 2015], currently holding state-of-the-art in the WMT\u201914 English-to-French and English-to-German [Wu et al., 2016], where the dictionary is a combination of the most common characters.", "startOffset": 98, "endOffset": 115}, {"referenceID": 12, "context": "Secondly, a hybrid model [Luong and Manning, 2016] where a word encoder-decoder consults a character encoder-decoder when confronted with out-of-dictionary words.", "startOffset": 25, "endOffset": 50}, {"referenceID": 11, "context": "Thirdly, pretrained word-based encoder-decoder models with character input used for creating word embeddings [Ling et al., 2015] have been shown to achieve similar results to word-based approaches.", "startOffset": 109, "endOffset": 128}, {"referenceID": 4, "context": "As a last mention, character decoder with BPE encoder has shown to be end-to-end trained successfully [Chung et al., 2016].", "startOffset": 102, "endOffset": 122}, {"referenceID": 10, "context": "The RNN encoderdecoder with attention approach is used not only within machine translation, but can be regarded as a general architecture to extract information from a sequence and answer some type of question related to it [Kumar et al., 2015].", "startOffset": 224, "endOffset": 244}, {"referenceID": 4, "context": "As a last mention, character decoder with BPE encoder has shown to be end-to-end trained successfully [Chung et al., 2016]. Wu et al. [2016] provides a good summary and large-scale demonstration of many of the techniques that are known to work well for NMT and RNNs in general.", "startOffset": 103, "endOffset": 141}, {"referenceID": 15, "context": "Our character-level encoder (referred to as the char encoder) is built upon a bi-directional RNN [Schuster and Paliwal, 1997].", "startOffset": 97, "endOffset": 125}, {"referenceID": 1, "context": "The decoder uses a RNN and the attention mechanism [Bahdanau et al., 2014] when decoding each character.", "startOffset": 51, "endOffset": 74}, {"referenceID": 14, "context": "All models were evaluated using the BLEU score1 [Papineni et al., 2002].", "startOffset": 48, "endOffset": 71}, {"referenceID": 4, "context": "The data preprocessing applies is identical to Chung et al. [2016] on En-De and De-Enwith the source sentence length set to 250 characters instead of 50 BPE units.", "startOffset": 47, "endOffset": 67}, {"referenceID": 9, "context": "For training we use back-propagation with stochastic-gradient descent using the Adam optimiser [Kingma and Ba, 2014] with a learning rate of \u03b1 = 0.", "startOffset": 95, "endOffset": 116}, {"referenceID": 5, "context": "When training with batches, all sequences must be padded to match the longest sequence in the batch, and the recurrent layers must do the full set of computations for all samples and all timesteps, which can result in a lot of wasted resources [Hannun et al., 2014] (see figure 4).", "startOffset": 244, "endOffset": 265}, {"referenceID": 11, "context": "BPE based models and the hybrid word-char model by Luong and Manning [2016] effectively works in the same manner, when translating common words BPE- and hybrid word-char models will work on a word level, whereas with rare words the BPE will work with sub-parts of the word (maybe even characters) and the hybrid approach will use character representations.", "startOffset": 51, "endOffset": 76}, {"referenceID": 4, "context": "The attention plot seems very similar to the BPE-toChar plot proposed by Chung et al. [2016]. This might indicate that it is possible to imitate lexeme (word) based models using smaller dictionaries and preserving relationship between words.", "startOffset": 73, "endOffset": 93}], "year": 2016, "abstractText": "Most existing Neural Machine Translation models use groups of characters or whole words as their unit of input and output. We propose a model with a hierarchical char2word encoder, that takes individual characters both as input and output. We first argue that this hierarchical representation of the character encoder reduces computational complexity, and show that it improves translation performance. Secondly, by qualitatively studying attention plots from the decoder we find that the model learns to compress common words into a single embedding whereas rare words, such as names and places, are represented character by character.", "creator": "LaTeX with hyperref package"}}}