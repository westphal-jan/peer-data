{"id": "1610.07686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Co-Occuring Directions Sketching for Approximate Matrix Multiply", "abstract": "We appropriate co - contain used sketching, every hamiltonian decoding four approximate dimensional limited (AMM ), in in downloads car. We audience that managing - hual\u0101lai speed tradeoff without reason breakdown bound for AMM than mostly randomized each well-founded approaches for AMM. Co - occurring into should is $ 28 + \\ io $ - amplitude \u2014 same optimal low consequently approximation part a quaternion product. Empirically our estimation outperforms have aspects meant AMM, which any except sketch zero. We quantify empirically really geography opinion only algorithms", "histories": [["v1", "Tue, 25 Oct 2016 00:01:33 GMT  (429kb)", "http://arxiv.org/abs/1610.07686v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["youssef mroueh", "etienne marcheret", "vaibhava goel"], "accepted": false, "id": "1610.07686"}, "pdf": {"name": "1610.07686.pdf", "metadata": {"source": "CRF", "title": "Co-Occuring Directions Sketching for Approximate Matrix Multiply", "authors": ["Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n07 68\n6v 1\n[ cs\n.L G\n] 2\n5 O\nct 2\n01 6\nWe introduce co-occurring directions sketching, a deterministic algorithm for approximate matrix product (AMM), in the streaming model. We show that co-occuring directions achieves a better error bound for AMM than other randomized and deterministic approaches for AMM. Co-occurring directions gives a (1 + \u03b5)-approximation of the optimal low rank approximation of a matrix product. Empirically our algorithm outperforms competing methods for AMM, for a small sketch size. We validate empirically our theoretical findings and algorithms."}, {"heading": "1 Introduction", "text": "The vast and continuously growing amount of multimodal content poses some challenges with respect to the collection and the mining of this data. Multimodal datasets are often viewed as multiple large matrices describing the same content with different modality representations (multiple views) such as images and their textual descriptions. The product of large multimodal matrices is of practical interest as it models the correlation between different modalities. Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.\nThe data streaming paradigm assumes a single pass over the data and a small memory footprint, resulting in a space/accuracy tradeoff. Multimodal data can occupy a large amount of memory or may be generated sequentially, hence it is important for the streaming\nPreliminary work. Under review by AISTATS 2017. Do not distribute.\nmodel to capture the data correlation .\nApproximate Matrix Multiplication (AMM), is gaining an increasing interest in streaming applications (See the recent monograph [Woo14] for more details ). In AMM we are given matrices X ,Y , with a large number of columns n, and the goal is to compute matrices BX , BY , with smaller number of columns \u2113, such that ||XY \u22a4 \u2212 BXB\u22a4Y ||Z is small for some norm \u2016.\u2016Z . In streaming AMM, columns of BX , BY , need to be updated as the data arrives sequentially. We refer to BX and BY as sketches of X and Y .\nRandomized approaches for AMM were pioneered by the work of [DKM06]. The approach of [DKM06] is based on the sampling of \u2113 columns of X and Y . [DKM06] shows that by choosing an appropriate sampling matrix \u03a0 \u2208 Rn\u00d7\u2113, we obtain a Frobenius error guarantee (\u2016.\u2016Z = \u2016.\u2016F ):\n\u2225 \u2225XY \u22a4 \u2212X\u03a0(Y \u03a0)\u22a4 \u2225 \u2225 F \u2264 \u03b5 \u2016X\u2016F \u2016Y \u2016F , (1)\nfor \u2113 = \u2126(1/\u03b52), with high probability. The same guarantee of Eq. (1) was achieved in [Sar06], by using a random projection \u03a0 \u2208 Rn\u00d7\u2113 that satisfies the guarantees of a Johnson- Lindenstrauss (JL) transform (\u2200x \u2208 Rn \u2016\u03a0x\u20162 \u223c (1 \u00b1 \u03b5) \u2016x\u20162 , with probability 1 \u2212 \u03b4), where \u2113 = O(1/\u03b52 log(1/\u03b4)). Other randomized approaches focused on error guarantees given in spectral norm (\u2016.\u2016Z = \u2016.\u2016) , such as JL embeddings or efficient subspace embeddings [Sar06, MZ11, ATKZ14, CNW15] that can be applied to any type of matrices X in input sparisty time [CW13]. [CNW15] showed that using a subspace embedding \u03a0 \u2208 Rn\u00d7\u2113 we have with a probability 1\u2212 \u03b4:\n\u2225 \u2225XY \u22a4 \u2212X\u03a0(Y \u03a0)\u22a4 \u2225 \u2225 \u2264 \u03b5 \u2016X\u2016 \u2016Y \u2016 , (2)\nfor \u2113 = O((sr(X) + sr(Y ) + log(1/\u03b4))/\u03b52), where sr(X) = \u2016X\u20162F \u2016X\u20162 is the stable rank of X . Note that sr(X) \u2264 rank(X), hence results stated in term of stable rank are sharper and more robust than the one stated with the rank [Sar06, MZ11, ATKZ14].\nCovariance sketching refers to AMM for X = Y . An elegant deterministic approach for covariance sketching called frequent directions was introduced recently\nin [Lib13, GLPW15], drawing the connection between covariance matrix sketching, and the classic problem of estimation of frequent items [MG82]. Another approach for AMM, consists of concatenating matrices X and Y, and of applying a covariance sketch technique on the resulting matrix, this approach results in a looser guarantee; The right hand side in Equations (1),(2) is replaced by \u03b5(\u2016X\u20162F + \u2016Y \u2016 2 F ). Based on this observation, [YLZ16] proposed to use the frequent directions algorithm of [Lib13] to perform AMM in a deterministic way, we refer to this approach as FDAMM. FD-AMM [YLZ16] outputs BX , BY such that\n\u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225 \u2225 \u2264 \u03b5(\u2016X\u20162F + \u2016Y \u2016 2 F ), (3)\nfor \u2113 = \u2308 1\u03b5\u2309. The sketch length \u2113 dependency on \u03b5 in randomized methods is quadratic, FD-AMM improves this dependency to linear.\nIn this paper we introduce co-occuring directions, a deterministic algorithm for AMM. Our algorithm is inspired by frequent directions and enables similar guarantees to (2) in spectral norm, but with a linear dependency of \u2113 on \u03b5 as in FD-AMM. Given with stable ranks, co-occuring direction achieves the guarantee of (2) for \u2113 = O( \u221a sr(X)sr(Y )/\u03b5).\nThe paper is organized as follows: In Section 2 we review frequent directions, introduce our co-occuring directions sketching algorithm, and give error bounds analysis in AMM and in low rank approximation of a matrix product. We state our proofs in Section 3. In section 2.2.2 and Section 4 we discuss error bounds, space and time requirements, and compare our approach to related work on AMM and low rank approximation. Finally we validate the empirical performance of co-occuring directions in Section 5, on both synthetic and real world multimodal datasets.\nNotation. We note by C = U\u03a3V \u22a4, the thin svd of C, and by \u03c3max(C) the maximum singular value, Tr refers to the trace. \u03c3j are the singular values that are assumed to be given in decreasing order. Note that for C \u2208 Rmx\u00d7my the spectral norm is defined as follows \u2016C\u2016 = maxu,v,\u2016u\u2016=\u2016v\u2016=1 \u2223 \u2223u\u22a4Cv \u2223\n\u2223 = \u03c3max(C). The nuclear norm (known also as trace or 1\u2212 schatten norm) is defined as follows: \u2016C\u2016\u2217 = Tr(\u03a3). sr(C) = \u2016C\u20162F \u2016C\u20162 is the stable rank of C. Assume C and D have the same number of column, [C;D] denotes their concatenation on their row dimensions. For n \u2208 N, [n] = {1, . . . n}."}, {"heading": "2 Sketching from Covariance to Correlation", "text": "In this section we review covariance sketching with the frequent directions algorithm of [Lib13] and state its\ntheoretical guarantees [Lib13, GLPW15]. We then introduce correlation sketching and present and analyze our co-occuring directions algorithm.\n2.1 Covariance Sketching: Frequent Directions\nLet X \u2208 Rmx\u00d7n, where n is the number of samples and mx the dimension. We assume that n > mx. The goal of covariance sketching is to find a small matrix DX \u2208 Rmx\u00d7\u2113, where \u2113 << n (\u2113 is assumed to be an even number ), such that XX\u22a4 \u2248 DXD\u22a4X . Frequent directions algorithm introduced in [Lib13] (Algorithm 1) achieves this goal. Intuitively frequent directions algorithm sets a noise level using the median of the spectrum of the covariance of the sketch DX . It then discards directions below that level and replaces them with fresh samples. This results in the updated the covariance estimate. This process is repeated as the data is streaming.\nAlgorithm 1 Frequent Directions\n1: procedure FD(X \u2208 Rmx\u00d7n) 2: DX \u2190 0 \u2208 Rmx\u00d7\u2113 . 3: for i \u2208 [n] do 4: Insert column Xi into a zero column of DX 5: if DX has no zero valued column then 6: [U,\u03a3, V ] \u2190 SVD(DX) 7: \u03b4 \u2190 \u03c32\u2113/2 \u22b2 median value of \u03a32 8: \u03a3\u0303 \u2190 \u221a\nmax(\u03a32 \u2212 \u03b4I\u2113, 0) \u22b2 shrinkage 9: DX \u2190 U \u03a3\u0303\n10: end if 11: end for 12: return DX 13: end procedure\nTheorem 1 ([Lib13]) DX the output of algorithm 1 satisfies:\n\u2225 \u2225XX\u22a4 \u2212DXD\u22a4X \u2225\n\u2225 \u2264 2 \u2016X\u2016 2 F\n\u2113 . (4)\n2.2 Correlation Sketching: Co-occuring Directions\nWe start by defining correlation sketching:\nDefinition 1 (Correlation Sketching/AMM) Let X \u2208 Rmx\u00d7n, Y \u2208 Rmy\u00d7n, where n > max(mx,my). Let BX \u2208 Rmx\u00d7\u2113 and BY \u2208 Rmy\u00d7\u2113 (\u2113 < n, \u2113 \u2264 min(mx,my)). Let \u03b7 > 0 . The matrix pair (BX , BY ) is called an \u03b7-correlation sketch of (X,Y ) if it satisfies in spectral norm:\n\u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225 \u2225 \u2264 \u03b7.\nWe now present our co-occuring directions algorithm (Algorithm 2). Intuitively Algorithm 2 sets a noise level using the median of the singular values of the correlation matrix of the sketch BXB \u22a4 Y . The SVD of BXB \u22a4 Y is computed efficiently in lines 8,9 and 10 of Algorithm 2 using QR decomposition. Left and right singular vectors below this noise threshold are replaced by fresh samples from X and Y , correlation sketches are updated and the process continues. Theorem 2 shows that our co-occuring directions algorithm outputs (BX , BY ) a correlation sketch of (X,Y ) as defined above in Definition 1.\nAlgorithm 2 Co-occuring Directions\n1: procedure Co-D(X \u2208 Rmx\u00d7n, Y \u2208 Rmy\u00d7n) 2: BX \u2190 0 \u2208 Rmx\u00d7\u2113 . 3: BY \u2190 0 \u2208 Rmy\u00d7\u2113 . 4: for i \u2208 [n] do 5: Insert a column Xi into a zero valued col-\numn of BX 6: Insert a column Yi into a zero valued col-\numn of BY 7: if BX , BY have no zero valued column then 8: [Qx, Rx] \u2190 QR(BX) 9: [Qy, Ry] \u2190 QR(BY ) 10: [U,\u03a3, V ] \u2190 SVD(RxR\u22a4y ) 11: \u22b2 Qx \u2208 Rmx\u00d7\u2113, Rx \u2208 R\u2113\u00d7\u2113, 12: \u22b2 Qy \u2208 Rmy\u00d7\u2113, Ry \u2208 R\u2113\u00d7\u2113, U,\u03a3, V \u2208 R\u2113\u00d7\u2113. 13: Cx \u2190 QxU \u221a \u03a3\n14: Cy \u2190 QyV \u221a \u03a3 15: \u22b2 Cx, Cy not computed 16: \u03b4 \u2190 \u03c3\u2113/2(\u03a3) \u22b2 the median value of \u03a3 17: \u03a3\u0303 \u2190 max(\u03a3\u2212 \u03b4I\u2113, 0) \u22b2 shrinkage 18: BX \u2190 QxU \u221a \u03a3\u0303 19: BY \u2190 QyV \u221a\n\u03a3\u0303 20: \u22b2 at least last \u2113/2 columns are zero 21: end if 22: end for 23: return BX , BY 24: end procedure\nIt is important to see that while frequent directions shrinks \u03a32, co-occuring directions filters \u03a3. We prove in the following an approximation bound in spectral norm for co-occurring directions."}, {"heading": "2.2.1 Main Results", "text": "We give in the following our main results, on the approximation error of co-occurring direction in AMM (Theorem 2), and in the k\u2212th rank approximation of a matrix product (Theorem 3). Proofs are given in Section 3.\nTheorem 2 (AMM) The output of co-occuring directions (Algorithm 2) gives a correlation sketch (BX , BY ) of (X,Y ), for \u2113 \u2264 min(mx,my) satisfying: For a correlation sketch of length \u2113, we have:\n\u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225 \u2225 \u2264 2 \u2016X\u2016F \u2016Y \u2016F \u2113 .\n2) Algorithm 2 runs in O(n(mx +my + \u2113)\u2113) time and requires a space of O((mx +my + \u2113)\u2113).\nTheorem 3 (Low Rank Product Approximation) Let (BX , BY ) be the output of Algorithm 2. Let k \u2264 \u2113. Let Uk, Vk be the matrices whose columns are the k-th largest left and right singular vectors of BXB \u22a4 Y . Let \u03c0kU (X) = UkU \u22a4 k X, \u03c0 k V (Y ) = VkV \u22a4 k Y . Let\n\u03b5 > 0, for \u2113 \u2265 8 \u221a sr(X)sr(Y )\n\u03b5 ||X||||Y || \u03c3k+1(XY \u22a4) we have:\n\u2225 \u2225XY \u22a4 \u2212 \u03c0kU (X)\u03c0kV (Y )\u22a4 \u2225 \u2225 \u2264 \u03c3k+1(XY \u22a4)(1 + \u03b5)."}, {"heading": "2.2.2 Discussion of Main Results", "text": "For \u2113 = \u2308 1\u03b5\u2309, \u03b5 \u2208 [ 1min(mx,my) , 1] from Theorem 2 we see that (BX , BY ) produced by Algorithm 2 is an \u03b7correlation sketch of (X,Y ) for \u03b7 = 2\u03b5 \u2016X\u2016F \u2016Y \u2016F . In AMM, bounds are usually stated in term of the product of spectral norms ofX an Y as in Equation (2). Let sr(X) = \u2016X\u20162F \u2016X\u20162 be the stable rank ofX . It is easy to see\nthat co-occuring directions for \u2113 = 2 \u221a sr(X)sr(Y )\n\u03b5 , gives an error bound of \u03b5 \u2016X\u2016 \u2016Y \u2016. While in randomized methods the error is O(1/ \u221a \u2113), co-occuring direction\u2019s error is O(1/\u2113). Moreover the dependency on stable ranks in co-occuring directions is 2 \u221a\nsr(X)sr(Y ) \u2264 sr(X) + sr(Y ), the lattter appears in subspace embedding based AMM [CNW15, MZ11, ATKZ14]. For X = Y co-occuring directions reduces to frequent directions of [Lib13], and Theorem 2 recovers Theorem 1 of [Lib13].\nStronger bounds for frequent directions were given in [GLPW15] where the bound in Equation (4) is improved, for \u2113 > 2k, for any k:\n\u2225 \u2225XX\u22a4 \u2212DXD\u22a4X \u2225 \u2225 \u2264 2 \u2113\u2212 2k \u2016X \u2212Xk\u2016 2 F ,\nwhere Xk is the k\u2212th rank approximation of X (with X0 = 0). Hence by defining Z = [X ;Y ] \u2208 R\n(mx+my)\u00d7n and applying frequent directions to Z (FD-AMM [YLZ16]), we obtain BX , BY satisfying: \u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225 \u2225 \u2264 2\u2113\u22122k \u2016Z \u2212 Zk\u2016 2 F , hence the perfomance of FD-AMM depends on the low rank structure of Z. A sharper analysis for co-occuring directions remains an open question, but the following discussion of Theorem 3 will shed some light on the advantages of co-occuring directions on FD-AMM [YLZ16].\nTheorem 3 shows that co-occuring directions sketching gives a (1+ \u03b5)- approximation of the optimal low rank approximation of the matrix product XY \u22a4. Note that \u03c3k+1(XY \u22a4) \u2264 \u2016XY \u22a4\u2016 \u2217\nk+1 . Hence for \u2113 \u2265 8(k + 1)/\u03b5, we obtain a 1 + \u03b5- approximation of the optimal k rank approximation of XY \u22a4. This highlights the relation between the sketch length in co-occurring directions \u2113 and the rank of XY \u22a4. Note that the maximum rank of XY \u22a4 is min(rank(X), rank(Y )). When using FD-AMM, based on the covariance sketch of the concatenation of X and Y , the sketch length \u2113 is related to the rank of Z = [X ;Y ]. Note that the maximum rank of the concatenation (Z) is bounded by rank(X) + rank(Y ). Hence we see that co-occuring directions guarantees a 1+\u03b5 approximation of the optimal k-rank approximation ofXY \u22a4 for a smaller sketch size then FD-AMM (min(rank(X), rank(Y )) for cooccuring directions versus rank(X)+ rank(Y ) for FDAMM).\nIn the following we comment on the running time of co-occuring directions."}, {"heading": "2.2.3 Running Time Analysis and Parralelization.", "text": "Running Time. We compare the space and the running time of our sketch to to a naive implementation of the correlation sketch. 1) Naive Correlation Sketch: In the if statement of Algorithm 2, compute the \u2113 thin svd SVD(BXB \u22a4 Y ) = [U,\u03a3, V ], BX \u2190 U \u221a \u03a3\u0303, BY \u2190 V \u221a\n\u03a3\u0303. We need a space O(mxmy) to store BXB \u22a4 y . The running time is dominated by computing an \u2113 thin svd O(mxmy\u2113) each n \u2113/2 that is O(nmxmy), hence no gain with respect to brute force. 2) Co-occuring Directions: Algorithm 2 avoids computing BXB \u22a4 Y by using the QR decomposition of BX and BY . The space needed is O(\u2113(mx + my + \u2113)). We have a computation done every n\u2113/2 , that is dominated by computing QR factorization and svd : O((mx +my + \u2113)\u2113 2) (computing RxR \u22a4 y requires O(\u2113\n3) operations). This results in a total running time : O(n(mx + my + \u2113)\u2113). There is a computational and memory advantage when \u2113 <\nmxmy mx+my .\nParallelization of Co-occuring Directions (Sketches of Sketches). Similarly to the frequent directions [Lib13], co-occuring directions algorithm is simply parallelizable. Let X = [X1, X2] \u2208 R\nmx\u00d7(n1+n2), and Y = [Y1, Y2] \u2208 Rmx\u00d7(n1+n2). Let (B1X , B 1 Y ) be the correlation sketch of (X1, Y1), and (B2X , B 2 Y ) be the correlation sketch of (X2, Y2). Then the correlation sketch (CX , CY ) of ([B1X , B 2 X ], [B 1 Y , B 2 Y ]) is a correlation sketch of (X,Y ), and is as good as (BX , BY ) the correlation\nsketch of (X,Y ). Hence we can sketch the data in M -independent chunks on M machines then merge by concatenating the sketches and performing another sketch on the concatenation, by doing so we divide the running time by M ."}, {"heading": "3 Proofs", "text": "In this Section we give proofs of our main results:\nProof 1 (Proof of Theorem 2) By construction we have:\nCxC \u22a4 y =\n( QxU \u221a \u03a3 )( QyV \u221a \u03a3 )\u22a4\n= Qx ( U\u03a3V \u22a4 ) Q\u22a4y = Qx ( RxR \u22a4 y ) Q\u22a4y = (QxRx) (QyRy) \u22a4 .\nHence the algorithm is computing a form of R-SVD of BXB \u22a4 Y , followed by a shrinkage of the correlation matrix. Let Bix, B i y, C i x, C i y,\u03a3 i, \u03a3\u0303i, \u03b4i, the values of BX , BY , Cx, Cy,\u03a3, \u03a3\u0303, \u03b4 after the execution of the main loop. \u03b4i = 0 if we don\u2019t enter the if statement (Bix = C i x and B i y = C i y if we don\u2019t enter the if statement). Hence we have at an iteration i:\nCixC i,\u22a4 y = B i\u22121 x B i\u22121,\u22a4 y +XiY \u22a4 i .\nNote that:\nXY \u22a4 \u2212BXB\u22a4Y = XY \u22a4 \u2212BnxBn,\u22a4y\n=\nn \u2211\ni=1\n(\nXiY \u22a4 i +B i\u22121 x B i\u22121,\u22a4 y \u2212BixBi,\u22a4y\n)\n=\nn \u2211\ni=1\n(\nCixC i,\u22a4 y \u2212BixBi,\u22a4y\n)\n.\nBy the triangular inequality we can bound the spectral norm:\n\u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225\n\u2225 \u2264 n \u2211\ni=1\n\u2225 \u2225CixC i,\u22a4 y \u2212BixBi,\u22a4y \u2225 \u2225 .\nWe are left with bounding \u2225 \u2225CixC i,\u22a4 y \u2212BixBi,\u22a4y \u2225 \u2225:\nCixC i,\u22a4 y =\n( QixU i ) \u03a3i ( QiyV i )\u22a4 , BixB i,\u22a4 y = ( QixU i ) \u03a3\u0303i ( QiyV i )\u22a4 .\nNote that: \u2225 \u2225CixC i,\u22a4 y \u2212BixBi,\u22a4y \u2225 \u2225 = \u2225 \u2225 \u2225(QixU i)(\u03a3i \u2212 \u03a3\u0303i)(QiyV i)\u22a4 \u2225 \u2225 \u2225\n= \u2225 \u2225 \u2225\u03a3i \u2212 \u03a3\u0303i \u2225 \u2225 \u2225\n\u2264 \u03b4i,\nwhere the first equality follows from the fact that, QixU i, QiyV i, are orthonormal. And \u03a3i \u2212 \u03a3\u0303i is a diagonal matrix with at least \u2113/2 entries equal \u03b4i or 0,\nand the other entries are less than \u03b4i. It follows that we have in spectral norm:\n\u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225\n\u2225 \u2264 n \u2211\ni=1\n\u03b4i. (5)\nNow we want to relate \u2211n\ni=1 \u03b4i to \u2113, and propreties of X,Y . Let \u2016.\u2016\u2217, the 1\u2212 schatten norm. For a matrix A of rank r, and singular values \u03c3i : \u2016A\u2016\u2217 = \u2211r i=1 \u03c3i(A). We have: \u2225\n\u2225BXB \u22a4 Y\n\u2225 \u2225\n\u2217 =\n\u2225 \u2225BnxB n,\u22a4 y \u2225 \u2225\n\u2217\n=\nn \u2211\ni=1\n\u2225 \u2225BixB i,\u22a4 y \u2225 \u2225 \u2217 \u2212 \u2225 \u2225Bi\u22121x B i\u22121,\u22a4 y \u2225 \u2225 \u2217\n=\nn \u2211\ni=1\n(\n\u2225 \u2225CixC i,\u22a4 y \u2225 \u2225 \u2217 \u2212 \u2225 \u2225Bi\u22121x B i\u22121,\u22a4 y \u2225 \u2225 \u2217\n)\n\u2212 n \u2211\ni=1\n(\n\u2225 \u2225CixC i,\u22a4 y \u2225 \u2225 \u2217 \u2212 \u2225 \u2225BixB i,\u22a4 y \u2225 \u2225 \u2217\n)\n(6)\nWe have at an iteration i, the R-SVD of CixC i,\u22a4 y and Bi,xB i,\u22a4 y :\n\u2225 \u2225CixC i,\u22a4 y \u2225 \u2225 \u2217 = Tr(\u03a3i) and \u2225 \u2225BixB i,\u22a4 y \u2225 \u2225 \u2217 = Tr(\u03a3\u0303i).\nHence we have by the definition of the shrinking operation: \u2225\n\u2225CixC i,\u22a4 y\n\u2225 \u2225 \u2217 \u2212 \u2225 \u2225BixB i,\u22a4 y \u2225 \u2225 \u2217\n= Tr(\u03a3i \u2212 \u03a3\u0303i) = \u2113 \u2211\nj=1\n\u03c3ij \u2212 \u03c3\u0303ij\n= \u2211\nj,\u03c3i j >\u03b4i\n\u03b4i + \u2211\nj,\u03c3i j \u2264\u03b4i\n\u03c3ij \u2265 \u2113\n2 \u03b4i. (7)\nOn the other hand using the reverse triangle inequality for the 1\u2212 shatten norm we have: \u2225 \u2225CixC i,\u22a4 y \u2225 \u2225 \u2217 \u2212 \u2225 \u2225Bi\u22121x B i\u22121,\u22a4 y \u2225 \u2225 \u2217 \u2264 \u2225 \u2225CixC i,\u22a4 y \u2212Bi\u22121x Bi\u22121,\u22a4y \u2225 \u2225 \u2217\nRecall that: CixC i,\u22a4 y = B i\u22121 x B i\u22121,\u22a4 y +XiY \u22a4 i , hence we have: \u2225 \u2225CixC i,\u22a4 y \u2225 \u2225 \u2217 \u2212 \u2225 \u2225Bi\u22121x B i\u22121,\u22a4 y \u2225 \u2225 \u2217 \u2264 \u2225 \u2225XiY \u22a4 i \u2225 \u2225 \u2217 = \u2016Xi\u20162 \u2016Yi\u20162 , (8) since XiY \u22a4 i is rank one. Finally putting together Equations (6), (7),(8), we have:\n\u2225 \u2225BXB \u22a4 Y \u2225 \u2225 \u2217 \u2264\nn \u2211\ni=1\n\u2016Xi\u20162 \u2016Yi\u20162 \u2212 \u2113\n2\nn \u2211\ni=1\n\u03b4i. (9)\nIt follows from Equation (9) that:\nn \u2211\ni=1\n\u03b4i \u2264 2\n\u2113\n(\nn \u2211\ni=1\n\u2016Xi\u20162 \u2016Yi\u20162 \u2212 \u2225 \u2225BXB \u22a4 Y \u2225 \u2225\n\u2217\n)\n\u2264 2 \u2113\n\n\n\u221a \u221a \u221a \u221a n \u2211\ni=1\n\u2016Xi\u201622\n\u221a \u221a \u221a \u221a n \u2211\ni=1\n\u2016Yi\u201622\n\n\n= 2\n\u2113 \u2016X\u2016F \u2016Y \u2016F , (10)\nwhere in the last inequality we used the CauchySchwarz inequality. Putting together Equations (5) and (10) we have finally:\n\u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225 \u2225 \u2264 2 \u2113 \u2016X\u2016F \u2016Y \u2016F . (11)\n2) Refer to Section 2.2.3.\nProof 2 (Proof of Theorem 3) Let \u03c0kU (X) = UkU \u22a4 k X, \u03c0 k V (Y ) = VkV \u22a4 k Y . Let Hxk be the span of {u1, . . . uk}, and Hxmx\u2212k be the orthogonal of Hxk. Similarly define Hyk the span of {v1, . . . vk}, and H y my\u2212k its orthogonal. For all u \u2208 Rmx , \u2016u\u2016 = 1, there exits ax, bx \u2208 R, a2x + b2x = 1, such that u = axwx + bxzx, where wx \u2208 Hxk , ||wx|| = 1 and zx \u2208 Hxmx\u2212k, ||zx|| = 1. Similarly for v \u2208 Rmy , \u2016v\u2016 = 1 there exits ay, by \u2208 R, a2y + b 2 y = 1, such that v = aywy + byzy, where wy \u2208 Hyk, ||wy|| = 1 and zy \u2208 H y my\u2212k , ||vy|| = 1 .\nLet \u2206 = XY \u22a4 \u2212 \u03c0kU (X)\u03c0kV (Y )\u22a4, we have \u2016\u2206\u2016 = maxu\u2208Rmx ,v\u2208Rmy ,||u||=||v||=1 |u\u22a4\u2206v|\n|u\u22a4\u2206v| = |(axwx + bxzx)\u22a4\u2206(aywy + byzy)| \u2264 |axay||w\u22a4x \u2206wy |+ |bxby||z\u22a4x \u2206zy| + |axby||w\u22a4x \u2206zy|+ |bxay||z\u22a4x \u2206wy|\nSince wx \u2208 Hxk , wy \u2208 Hyk, we have w\u22a4x \u2206wy = 0. Since zx \u2208 Hxmx\u2212k, zy \u2208 H y my\u2212k , z\u22a4x \u2206zy = z \u22a4 x XY \u22a4zy. Similarly w\u22a4x \u2206zy = w \u22a4 x XY \u22a4zy, and z \u22a4 x \u2206wy = z \u22a4 x XY\n\u22a4wy. Note that |ax|, |bx|, |ay|, |by| are bounded by 1. Hence we have (maximum is taken on each appropriate set defined above, all vectors are unit norm):\nmax u,v |u\u22a4\u2206v| \u2264 max zx,zy |z\u22a4x XY \u22a4zy|+ max wx,zy |w\u22a4x XY \u22a4zy|\n+ max zx,wy\n|z\u22a4x XY \u22a4wy|\nFor zx \u2208 Hxmx\u2212k, zy \u2208 H y my\u2212k we have:\n|z\u22a4x XY \u22a4zy| \u2264 |z\u22a4x (XY \u22a4 \u2212BXB\u22a4Y )zy|+ |z\u22a4x BXB\u22a4Y zy| \u2264 \u2225\n\u2225XY \u22a4 \u2212BXB\u22a4Y \u2225 \u2225+ \u03c3k+1(BXB \u22a4 Y )\n\u2264 2 \u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225 \u2225+ \u03c3k+1(XY \u22a4),\nwhere we used that maxzx\u2208Hxmx\u2212k,zy\u2208H y my\u2212k |z\u22a4x BXB\u22a4Y zy| = \u03c3k+1(BXB \u22a4 Y ) by definition of \u03c3k+1. The last inequality follows from weyl inequality |\u03c3k+1(BXB\u22a4Y )\u2212 \u03c3k+1(XY \u22a4)| \u2264 \u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225 \u2225. Note that for wx \u2208 Hxk and zy \u2208 Hymy\u2212k we have w\u22a4x BXB \u22a4 Y zy = 0. To see that, note that wx \u2208 span{u1, . . . uk} , zy \u2208 span{vk+1, . . . v\u2113}. There exists \u03b2j, such that zy = \u2211\u2113 j=k+1 \u03b2jvj, hence BXB \u22a4 Y zy = \u2211\u2113 i=1 \u2211\u2113 j=k+1 \u03c3i\u03b2juiv \u22a4 i vj = \u2211\u2113 j=k+1 \u03c3j\u03b2juj \u22a5 wx. Hence we have:\n|w\u22a4x XY \u22a4zy| = |w\u22a4x (XY \u22a4 \u2212BXB\u22a4Y )zy| \u2264 \u2225\n\u2225XY \u22a4 \u2212BXB\u22a4Y \u2225 \u2225 .\nSimilarly for for zx \u2208 Hxmx\u2212k and wy \u2208 H y k we conclude that: |w\u22a4x XY \u22a4zy| \u2264 \u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225\n\u2225. Finally we have:\n\u2016\u2206\u2016 \u2264 4 \u2225 \u2225XY \u22a4 \u2212BXB\u22a4Y \u2225 \u2225+ \u03c3k+1(XY \u22a4)\n\u2264 8 \u2016X\u2016F \u2016Y \u2016F \u2113 + \u03c3k+1(XY \u22a4) \u2264 \u03c3k+1(XY \u22a4)(1 + 8 \u221a sr(X)sr(Y )\n\u2113\n||X ||||Y || \u03c3k+1(XY \u22a4) )\nFor \u2113 \u2265 8 \u221a sr(X)sr(Y )\n\u03b5 ||X||||Y || \u03c3k+1(XY \u22a4) , we have: \u2016\u2206\u2016 \u2264\n\u03c3k+1(XY \u22a4)(1 + \u03b5)."}, {"heading": "4 Previous Work on Approximate Matrix Multilply", "text": "We list here a catalog of baselines for AMM:\nBrute Force. We keep a running correlation C \u2190 C+XiY \u22a4 i . We perform an \u2113 thin svd at the end of the stream. Space O(mxmy), running time: O(nmxmy) + O(mxmy\u2113), the cost of the sketch update and the \u2113 thin svd.\nSampling [DKM06]. We define a distribution over [n], pi = \u2016Xi\u2016\u2016Yi\u2016 S , where S = \u2211n\ni=1 \u2016Xi\u2016 \u2016Yi\u2016. Form BX and BY by taking \u2113 iids samples (column indices), using pi. In the streaming model, since S is not known, we use \u2113 independent reservoir samples. Hence the space needed is O(\u2113(mx + my)), the running time is O(\u2113(mx +my)n).\nRandom Projection [Sar06]. BX , BY are of the form X\u03a0 and Y\u03a0, where \u03a0 \u2208 Rn\u00d7\u2113 , and \u03a0ij \u2208 {\u22121/ \u221a \u2113, 1/ \u221a \u2113}, uniformly. This is easily implemented in the streaming model and requires O(\u2113(mx + my)) space and O(\u2113(mx +my)n) time.\nHashing [CW13]. Let h : [n] \u2192 [\u2113], and s : [n] \u2192 {\u22121, 1} be perfect hash functions. We initialize BX , BY to all zeros matrices. When processing columns of X and Y we update columns of BX and BY as follows: BX,h(i) \u2190 BX,h(i) + s(i)Xi, BY,h(i) \u2190 BY,h(i)+s(i)Yi. Hashing requires O(\u2113(mx+my)) space and O(n(mx +my)) time.\nFD-AMM [YLZ16]. Let Z = [X ;Y ] \u2208 R(mx+my)\u00d7n, let DZ be the output of frequent directions (Algoritm 1). We partition DZ = [BX ;BY ], and use BX and BY in AMM. This requires O(\u2113(mx +my)) space and O(n(mx +my)\u2113) time."}, {"heading": "5 Experiments", "text": "AMM of Low Rank Matrices. We consider X \u2208 R\nmx\u00d7n and Y \u2208 Rmy\u00d7n, generated using a non-noisy low rank model [GLPW15] as follows: X = VxSxU \u22a4 x ,\nwhere Ux \u2208 Rn\u00d7kx , (Ux)i,j \u223c N (0, 1), Sx \u2208 Rkx\u00d7kx is a diagonal matrix with (Sx)jj = 1\u2212(j\u22121)/kx, and Vx \u2208 R\nmx\u00d7kx is such that V \u22a4x Vx = Ikx . Similarly we generate Y = VySyU \u22a4 y , Uy \u2208 Rn\u00d7ky , Sy \u2208 Rky\u00d7ky , Vy \u2208 R my\u00d7ky . Hence X and Y are at most rank kx, and ky respectively. We consider n = 10000, mx = 1000, my = 2000, and three regimes: both matrices have a large rank (kx = 400, ky = 400), one matrix has a smaller rank then the other (kx = 400, ky = 40), and both matrices have a small rank (kx = 40, ky = 40). We compare the performance of co-occuring directions to baselines given in Section 4 in those three regimes. For randomized baselines we run each experiments 50 times and report mean and standard deviations of performances. Experiments were conducted on a single core Intel Xeon CPU E5-2667, 3.30GHz, with 265 GB of RAM and 25.6 MB of cache.\nWe see in Figure 1, that hashing timing is, as expected, independent from the sketch length. Random projection requires the most amount of time. Cooccuring directions timing is on par with sampling and slightly better than FD-AMM. From Figure 2 1 we see that the deterministic baselines (a,c,e) consistently outperform the randomized baselines (b,d,f) in all three regimes. As discussed previously randomized methods error bound are of the order of O(1/ \u221a \u2113), while both co-occuring directions and FD-AMM have an error bound order O(1/\u2113). Note that the brute force error becomes zero (up to machine precision) when \u2113 exceeds min(rank(X), rank(Y )). When comparing co-occuring direction to FD-AMM we see a clear phase transition for co-occuring direction as \u2113 exceeds O(min(rank(X), rank(Y ))). For FD-AMM the phase transition happens when \u2113 exceeds O(rank(X)+ rank(Y )). The phase transition happens earlier for cooccuring directions and hence co-occuring directions outperforms FD-AMM for a smaller sketch size. This is in line with our discussion in Section 2.2.2. For in-\n1Better seen in color.\nstance plot (c) illustrates this effect, kx = 400, ky = 40, as \u2113 exceeds 50, the error of co-occuring directions sharply decreases , while FD-AMM error is still high.\nThe latter starts a steep decreasing tendency when \u2113 exceeds 400. We give plots for the low rank approximation as given in Theorem 3 for k = min(kx, ky) in the\nappendix, we see a similar trend in the approximation error.\nAMM of Noisy Low Rank Matrices (Robustness). We consider the same model as before but we add a gaussian noise to the low rank matrices, i.e X = VxSxU \u22a4 x + Nx/\u03b6x, where \u03b6x > 0, and Nx \u2208 Rmx\u00d7n, (Nx)i,j \u223c N (0, 1). Similarly for Y = VySyU\u22a4y +Ny/\u03b6y. In this scenario X and Y have still decaying singular values but with non zeros tails. We consider \u03b6x = 1000, and \u03b6y = 100. We compare here deterministic baselines in Figures 3,4, and 5, in the three scenarios we see that co-occuring directions still outperforms FDAMM, but the gap between the two approaches becomes smaller in the low rank regimes (Figures 4, and 5), this hints to a weakness in the shrinking of singular values in both algorithms getting affected by the noise (Step 17 in Alg. 2). We give plots for the low rank approximation in the appendix.\nMultimodal Data Experiments. In this section we study the empirical performance of co-occuring directions in approximating correlation between images and captions. We consider Microsoft COCO [LMB+14] dataset. For visual features we use the residual CNN Resnet101, [HZRS16]. The last layer of Resnet results in a feature vector of dimension mx = 2048. For text we use the Hierarchical Kernel Sentence Embed-\nding HSKE of [MMG16] that results in a feature vector of dimension my = 3000. The training set size is n = 113287. We see in Fig. 6 that co-occuring directions outperforms FD-AMM in this case as well (timing experiment is given in the appendix)."}, {"heading": "6 Conclusion", "text": "In this paper we introduced a deterministic sketching algorithm for AMM that we termed co-occuring directions . We showed its error bounds (in spectral norm) for AMM and the low rank approximation of a product. We showed empirically that co-occuring directions outperforms deterministic and randomized baselines in the streaming model. Indeed co-occuring direction has the best error/space tradeoff among known baselines with errors given in spectral norm in the streaming model. We are left with two open questions. First, whether guarantees of Theorem 2 can be improved akin to the improved guarantees for frequent directions given [GLPW15]. This would give an explicit link of the sketch length \u2113, to the low rank structure of the matrix product XY \u22a4, and/or the low rank structure of the individual matrices. Second, whether robustness of co-occuring directions can be improved using\nrobust shrinkage operators as in [GDP14]."}, {"heading": "A Low Rank product Approximation", "text": "B MS-Coco Timing Experiments"}], "references": [{"title": "Approximate matrix multiplication with application to linear embeddings", "author": ["Michail Vlachos Anastasios T. Kyrillidis", "Anastasios Zouzias"], "venue": "Corr,", "citeRegEx": "ATKZ14", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal approximate matrix product in terms of stable rank", "author": ["Michael B. Cohen", "Jelani Nelson", "David P. Woodruff"], "venue": "CoRR,", "citeRegEx": "CNW15", "shortCiteRegEx": null, "year": 2015}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "STOC,", "citeRegEx": "CW13", "shortCiteRegEx": null, "year": 2013}, {"title": "Co-clustering documents and words using bipartite spectral graph partitioning", "author": ["Inderjit S. Dhillon"], "venue": "KDD,", "citeRegEx": "Dhi01", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast monte carlo algorithms for matrices i: Approximating matrix multiplication", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM J. Comput.,", "citeRegEx": "DKM06", "shortCiteRegEx": null, "year": 2006}, {"title": "Phillips", "author": ["Mina Ghashami", "Amey Desai", "Jeff M"], "venue": "Improved Practical Matrix Sketching with Guarantees.", "citeRegEx": "GDP14", "shortCiteRegEx": null, "year": 2014}, {"title": "Frequent directions : Simple and deterministic matrix sketching", "author": ["Mina Ghashami", "Edo Liberty", "Jeff M. Phillips", "David P. Woodruff"], "venue": "CoRR,", "citeRegEx": "GLPW15", "shortCiteRegEx": null, "year": 2015}, {"title": "Relations between two sets of variates", "author": ["Harold Hotteling"], "venue": "Biometrika,", "citeRegEx": "Hot36", "shortCiteRegEx": null, "year": 1936}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CVPR,", "citeRegEx": "HZRS16", "shortCiteRegEx": null, "year": 2016}, {"title": "In KDD", "author": ["Edo Liberty. Simple", "deterministic matrix sketching"], "venue": "ACM,", "citeRegEx": "Lib13", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding repeated elements", "author": ["J. Misra", "David Gries"], "venue": "Science of Computer Programming,", "citeRegEx": "MG82", "shortCiteRegEx": null, "year": 1982}, {"title": "Multimodal retrieval with asymmetrically weighted CCA and hierarchical kernel sentence embedding", "author": ["Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel"], "venue": "ArXiv,", "citeRegEx": "MMG16", "shortCiteRegEx": null, "year": 2016}, {"title": "Low rank matrix-valued chernoff bounds and approximate matrix multiplication", "author": ["Avner Magen", "Anastasios Zouzias"], "venue": "SODA,", "citeRegEx": "MZ11", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tamas Sarlos"], "venue": null, "citeRegEx": "Sarlos.,? \\Q2006\\E", "shortCiteRegEx": "Sarlos.", "year": 2006}, {"title": "A survey of partial least squares (pls) methods", "author": ["Jacob A. Wegelin"], "venue": "with emphasis on the two-block case. Technical report,", "citeRegEx": "Weg00", "shortCiteRegEx": null, "year": 2000}, {"title": "Comput", "author": ["David P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends Theor"], "venue": "Sci.,", "citeRegEx": "Woo14", "shortCiteRegEx": null, "year": 2014}, {"title": "Frequent direction algorithms for approximate matrix multiplication with applications in CCA", "author": ["Qiaomin Ye", "Luo Luo", "Zhihua Zhang"], "venue": "IJCAI,", "citeRegEx": "YLZ16", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.", "startOffset": 44, "endOffset": 51}, {"referenceID": 7, "context": "Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.", "startOffset": 89, "endOffset": 96}, {"referenceID": 3, "context": "Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.", "startOffset": 121, "endOffset": 128}, {"referenceID": 15, "context": "Approximate Matrix Multiplication (AMM), is gaining an increasing interest in streaming applications (See the recent monograph [Woo14] for more details ).", "startOffset": 127, "endOffset": 134}, {"referenceID": 4, "context": "Randomized approaches for AMM were pioneered by the work of [DKM06].", "startOffset": 60, "endOffset": 67}, {"referenceID": 4, "context": "The approach of [DKM06] is based on the sampling of l columns of X and Y .", "startOffset": 16, "endOffset": 23}, {"referenceID": 4, "context": "[DKM06] shows that by choosing an appropriate sampling matrix \u03a0 \u2208 R, we obtain a Frobenius error guarantee (\u2016.", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "\u2016) , such as JL embeddings or efficient subspace embeddings [Sar06, MZ11, ATKZ14, CNW15] that can be applied to any type of matrices X in input sparisty time [CW13].", "startOffset": 158, "endOffset": 164}, {"referenceID": 1, "context": "[CNW15] showed that using a subspace embedding \u03a0 \u2208 R we have with a probability 1\u2212 \u03b4:", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "in [Lib13, GLPW15], drawing the connection between covariance matrix sketching, and the classic problem of estimation of frequent items [MG82].", "startOffset": 136, "endOffset": 142}, {"referenceID": 16, "context": "Based on this observation, [YLZ16] proposed to use the frequent directions algorithm of [Lib13] to perform AMM in a deterministic way, we refer to this approach as FDAMM.", "startOffset": 27, "endOffset": 34}, {"referenceID": 9, "context": "Based on this observation, [YLZ16] proposed to use the frequent directions algorithm of [Lib13] to perform AMM in a deterministic way, we refer to this approach as FDAMM.", "startOffset": 88, "endOffset": 95}, {"referenceID": 16, "context": "FD-AMM [YLZ16] outputs BX , BY such that", "startOffset": 7, "endOffset": 14}, {"referenceID": 9, "context": "2 Sketching from Covariance to Correlation In this section we review covariance sketching with the frequent directions algorithm of [Lib13] and state its theoretical guarantees [Lib13, GLPW15].", "startOffset": 132, "endOffset": 139}, {"referenceID": 9, "context": "Frequent directions algorithm introduced in [Lib13] (Algorithm 1) achieves this goal.", "startOffset": 44, "endOffset": 51}, {"referenceID": 9, "context": "Theorem 1 ([Lib13]) DX the output of algorithm 1 satisfies:", "startOffset": 11, "endOffset": 18}, {"referenceID": 9, "context": "For X = Y co-occuring directions reduces to frequent directions of [Lib13], and Theorem 2 recovers Theorem 1 of [Lib13].", "startOffset": 67, "endOffset": 74}, {"referenceID": 9, "context": "For X = Y co-occuring directions reduces to frequent directions of [Lib13], and Theorem 2 recovers Theorem 1 of [Lib13].", "startOffset": 112, "endOffset": 119}, {"referenceID": 6, "context": "Stronger bounds for frequent directions were given in [GLPW15] where the bound in Equation (4) is improved, for l > 2k, for any k:", "startOffset": 54, "endOffset": 62}, {"referenceID": 16, "context": "Hence by defining Z = [X ;Y ] \u2208 R (mx+my)\u00d7n and applying frequent directions to Z (FD-AMM [YLZ16]), we obtain BX , BY satisfying:", "startOffset": 90, "endOffset": 97}, {"referenceID": 16, "context": "A sharper analysis for co-occuring directions remains an open question, but the following discussion of Theorem 3 will shed some light on the advantages of co-occuring directions on FD-AMM [YLZ16].", "startOffset": 189, "endOffset": 196}, {"referenceID": 9, "context": "Similarly to the frequent directions [Lib13], co-occuring directions algorithm is simply parallelizable.", "startOffset": 37, "endOffset": 44}, {"referenceID": 4, "context": "Sampling [DKM06].", "startOffset": 9, "endOffset": 16}, {"referenceID": 2, "context": "Hashing [CW13].", "startOffset": 8, "endOffset": 14}, {"referenceID": 16, "context": "FD-AMM [YLZ16].", "startOffset": 7, "endOffset": 14}, {"referenceID": 6, "context": "We consider X \u2208 R mx\u00d7n and Y \u2208 Ry, generated using a non-noisy low rank model [GLPW15] as follows: X = VxSxU \u22a4 x , where Ux \u2208 Rx , (Ux)i,j \u223c N (0, 1), Sx \u2208 Rxx is a diagonal matrix with (Sx)jj = 1\u2212(j\u22121)/kx, and Vx \u2208 R mx\u00d7kx is such that V \u22a4 x Vx = Ikx .", "startOffset": 78, "endOffset": 86}, {"referenceID": 8, "context": "For visual features we use the residual CNN Resnet101, [HZRS16].", "startOffset": 55, "endOffset": 63}, {"referenceID": 11, "context": "ding HSKE of [MMG16] that results in a feature vector of dimension my = 3000.", "startOffset": 13, "endOffset": 20}, {"referenceID": 6, "context": "First, whether guarantees of Theorem 2 can be improved akin to the improved guarantees for frequent directions given [GLPW15].", "startOffset": 117, "endOffset": 125}], "year": 2016, "abstractText": "We introduce co-occurring directions sketching, a deterministic algorithm for approximate matrix product (AMM), in the streaming model. We show that co-occuring directions achieves a better error bound for AMM than other randomized and deterministic approaches for AMM. Co-occurring directions gives a (1 + \u03b5)-approximation of the optimal low rank approximation of a matrix product. Empirically our algorithm outperforms competing methods for AMM, for a small sketch size. We validate empirically our theoretical findings and algorithms.", "creator": "LaTeX with hyperref package"}}}