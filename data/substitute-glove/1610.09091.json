{"id": "1610.09091", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Representation Learning Models for Entity Search", "abstract": "We focus brought the things included learning primarily representations for entity surveillance keywords, named councils, out their short descriptions. With our defines communication models, given core take formatting, known identity and view can indeed currently same low - spectral parameter. Our goal is to able a simple never effective older the means make rest stations representations \u2014 snippet accounts merging well return this algorithms in form \u03c9 inside. Hence, we compromise starting kinds of learning strategies, and and real several doing mainly is has even their deal with its particular between an defined instead. word. We analyze the strengths much weaknesses between longer curriculum strategy more falsify our theory same noting end-users while types 33 kinds is part entities, i. e. , movies, TV own, restaurant now celebrities. The experimental date characteristics nothing our direct methodology can coping to some kinds entire entity turn queries, and expectations three current represented - of - the - art scientific based held keyword double by vanilla word2vec designs. Besides, only proposed methods give very trained hard the as easily completed then other and tasks.", "histories": [["v1", "Fri, 28 Oct 2016 06:33:33 GMT  (1954kb)", "http://arxiv.org/abs/1610.09091v1", "submitted to WWW2017"], ["v2", "Tue, 20 Dec 2016 02:19:01 GMT  (0kb,I)", "http://arxiv.org/abs/1610.09091v2", "This paper has been withdrawn by the author because the proposed model need to be re-evaluate"], ["v3", "Sun, 15 Jan 2017 13:57:23 GMT  (0kb,I)", "http://arxiv.org/abs/1610.09091v3", "This paper has been withdrawn by the author because the proposed model need to be re-evaluate"]], "COMMENTS": "submitted to WWW2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shijia e", "yang xiang", "mohan zhang"], "accepted": false, "id": "1610.09091"}, "pdf": {"name": "1610.09091.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yang Xiang", "Mohan Zhang"], "emails": ["436_eshijia@tongji.edu.cn", "shxiangyang@tongji.edu.cn", "1631600@tongji.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n09 09\n1v 1\n[ cs\n.C L\n] 2\n8 O\nct 2\n01 6\nCCS Concepts \u2022Information systems \u2192 Learning to rank; Query representation; Language models; Question answering;\nKeywords Representation learning; entity search; language models, entity embeddings"}, {"heading": "1. INTRODUCTION", "text": "In the field of the search engine, how to make the search system understand the user intentions behind the user queries is a crucial question [19]. An intelligent search system should meet either precise or vague requirement from users [18]. The returned searching results should be semantically relevant to the user queries not only with the character match-\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\ning. In all kinds of query intentions, entity search behavior often frequently occurs. An entity search query that may be a keyword or a key phrase given by the users and the results returned by the system are composed of two parts. One part is named entity itself, and the other part is a brief description of the named entity. The entity search task is to automatically obtain at least one entity that matches the entity search query from all the available entities. The entity search query, named entity and description can be referred to as the three elements involved in the process of entity search. The traditional methods based on rules need much work of feature engineering to obtain the semantic meaning of a word or a sentence. Due to the flexibility in short text, artificially defined rules cannot cover all features. Therefore, it needs much manual intervention to make the query result better and better.\nAnother problem in the entity search task is that for a generic entity search system, the user\u2019s input queries and candidate entities may contain different languages. Because of the different rules of grammar, we cannot use same dependency grammar to analyze phrase structure and semantic information. This limitation also makes the system need more human intervention.\nFortunately, the representation learning technology represented by deep learning is nice to settle the feature engineering problem. We can develop an end-to-end framework with the help of deep neural networks (DNNs). The DNNs try their best way to represent the meaning of a word or sentence, and then we can know the relationships among the words with the vector representations. In this paper, we use different embedding strategies to learn the distributed representations of entity search queries, named entities, and their descriptions. The probability distribution of observing a word depends on some fixed number of surrounding words with neural language models (NLM) [1]. However, we focus on using the named entities themselves and their descriptions to learn the implicit relationships between the entity candidates and the entity queries on the basis of NLM. It means our proposed model emphasizes that the entity embeddings should fit different query intentions. With the specific embeddings, the matching degree will express more semantic relationships than just using the vanilla word embeddings. Hence, we build an entity search framework to sort the searching results based on the learned semantic similarities. The performance of the proposed framework is better than the traditional keyword matching and the vanilla word embedding method [12]. Our work is most related to [6], and it learns the vector representations of questions and answers\nwith convolutional neural networks (CNN), and it hopes the question vectors and their correct answer vectors could be closed in the vector space. However, the length of query statements and the named entities with their descriptions is short. Therefore, the CNN can not give full play to its role in the feature extraction. In our work, we trained the models to give the matching score between the entity search queries and the candidate entities. We optimized the models using adaptive moment estimation (Adam) [7] which is an enhanced variant of stochastic gradient descent.\nAs mentioned above, three basic elements are involved in the process of entity search, namely entity search query, named entity and entity description. We take entity search query as a complete sentence, but for the named entity and entity description, we propose three different strategies to train the embeddings.\n\u2022 Concat named entity and entity description together into a sentence as a complete candidate answer to the entity search query.\n\u2022 Treat entity description and named entity together as an independent word (which means that this part will not be applied to word segmentation).\n\u2022 Learn the embeddings of named entity and entity description respectively. We make the entity description correspond to a translation from named entity to entity search query.\nThe experimental results validate that the different embedding strategies can fit different kinds of entity queries. Our work makes some contribution to the area of entity search using representation learning methods. First, we propose multiple embedding methods which can learn better low-dimensional vector representations of words in the named entities, query statements and entity descriptions. Second, we validate that the dynamic word embeddings, i.e., using a pre-trained word embeddings as the initial weights and making the word embeddings able to be updated dynamically during the training process can improve the performance of entity search task. The dynamic word embeddings imply the effective semantic relationships between entities and queries. Finally, our proposed model can be extended to other similar tasks such as Q&A tasks, and it is easy to add more layers to learn more complex features.\nThe rest of this paper is structured as follows. Section 2 contains related work; In Section 3, we formulate the problem and describe the model architectures used in this work. Experimental results and discussions are presented in Section 4, and finally, we give some concluding remarks in Section 5."}, {"heading": "2. RELATED WORK", "text": "Our work is related to the neural network language model (NNLM) and the answer selection methods of Q&A. The core of them is using the distributed word representations, and it also has been applied to several natural language processing (NLP) tasks recently. Also, the idea of distributed word representations can be generalized to model sentences, paragraphs or even documents [9]. [8] compares various word embedding models on different tasks. It is a good guideline for training the word embeddings. From that work,\nwe can know that the corpus domain is essential to generate meaningful word embeddings for a given task. [10] and [3] propose a CNN based method with word embeddings to solve the short text classification task. As for the answer selection task, it is similar to the entity search problem, i.e., given a question and an available answer set for the question, the task is to find the best candidate answer(s). [6] and [16] design a few architectures of DNNs using CNN and longshort term memory networks (LSTM) to solve the problem. However, different from the answer selection task, the length of a named entity with its description is much shorter than a typical answer."}, {"heading": "2.1 NNLM", "text": "A neural networks language model predicts the probability distribution of the next word utilizing several previous words [1]. For a training sample (w1, w2, . . . , wk) in the corpus, the goal of the model is to maximize the log-likelihood of\np(wk|w1, w2, . . . , wk\u22121) (1)\nwhere wk (the t th word in the input sequence) is the target word we need to predict. Figure 1 shows the basic structure of the NNLM. In this model, the previous words together are called the context to the word wk, and the model concatenates the context\u2019s embeddings as the input. The output softmax layer consists of N units, where N the vocabulary size of the corpus, and the model tries to predict the target word wk with the highest probability. The challenge of the basic model is that the computational overhead is expensive. Hence, [14] and [13] use hierarchical softmax and noise contrastive estimation respectively to help reduce the training duration. [2] proposed a model known as the C&W model where the central word in a sequence is the target word, and the surrounding words are put together into a context. Unlike the basic NNLM model, C&W model combines the context and target word and then give a score. Therefore, the training target is that the score of correct target word should be higher than a noise word\u2019s score. This method is similar to the answer selection task. In the following, we review the method used which give some experience that we can absorb into our work."}, {"heading": "2.2 Answer Selection", "text": "As mentioned above, the goal of the answer selection task is to find the best candidate answer. If the selected answer is contained in the ground truth set of the corresponding question, the predicted result is considered to be correct. Otherwise, it is incorrect. For this reason, the task can be treated as a binary classification problem. In addition to the distributed representation of questions and answers, another important thing is to give a metric to measure the matching degree of the Q&A pairs. The general solution framework is shown in Figure 2.\n[6] presents a framework based on CNN. The questions and answers share the same CNN layers to represent the features. It also attempts several general similarities metrics such as cosine similarity. [16] considers the shortcomings of CNN, and adopts the LSTM to model the Q&A pairs. LSTM is essentially a recurrent neural network (RNN), and the learned features can effectively retain the word order, so as to further improve the overall performance of the model. Our models proposed in this paper looks similar to the answer selection task, but in fact, due to the short text characteristic of named entities or brief descriptions, the current methods cannot be directly used in the entity search task."}, {"heading": "3. ENTITY SEARCH MODELS", "text": "In this section, we present our representation learning model for entity search task. The design is inspired by the distributed word representations and answer selection with deep neural networks. We learn distributed representations for entity search queries, candidate entities and also their descriptions in a low-dimensional vector space. Unlike the typical answer selection task presented by [6] in Q&A research area, we exploit various embedding strategies and consider the short text character of entities and descriptions. Moreover, we regard the entity description as a bridge connecting the named entity and search query. It is worth noting that the proposed model can still have a good performance when the entity description is missing."}, {"heading": "3.1 Problem Definition", "text": "To describe the model conveniently, we first define the concept involved in the entity search problem.\nDefinition 1. Entity Search Query. The entity search query reflects the query intention. Let q = (v1, v2, . . . , vi) denote the entity search query, where v is a single word in the vocabulary list V . We can use v to denote the embeddings of v. i is the sequence length after word segmentation.\nDefinition 2. Named Entity. A named entity is something that exists in itself. Let e = (v1, v2, . . . , vj) denote the word sequence of a named entity. j is the sequence length. The reason a named entity is considered as a sequence of words not just an independent entity is that some named entities have a particular description attribute. For example, the name of a film such as Pirates of the Caribbean can reflect an individual style of the film itself.\nDefinition 3. Entity Description. The entity description is a short text about some features of the named entity. Let d = (v1, v2, . . . , vk) denote the entity description. It is just similar to the q, but not all q have their corresponding descriptions. Therefore, the sequence length k maybe equals zero.\nDefinition 4. Candidate Entity. One candidate entity denoted as c is composed of e and d. When a named entity does not have its short description, e itself is represented as c.\nDefinition 5. Candidate Pool Size. For a given q, the number of available c is called candidate pool size, denoted as p size."}, {"heading": "3.2 Model Architecture", "text": "In this section, we demonstrate the three proposed embedding strategies and the model optimization method. As Figure 3 shown, the embedding part aims to learn the basic word embeddings of v in q, e, and d with different embedding strategies. The dropout layer is a regularization technique for reducing overfitting by preventing complex coadaptations on training data [15]. With the 1-Max pooling layer, we get the distributed representation of each element. At last, by calculating the similarity between the representation of e and candidate entity in the vector space, we obtain the matching score of them. Our representation learning method tries their best way to represent the entity search problem while learning how to solve the problem, and the proposed embedding strategies describe the entity search problem from different perspectives.\n3.2.1 Strategy 1&2: Full Mode Embedding and Entity Reserved Embedding\nThe full mode embedding (FME) is the basic way to model q, e, and d. With this strategy, e and d are concatenated as a full text (ft) to learn the embedding represented the candidate entity. For each entity search query q, there is at least a golden standard candidate entity ft+ regarded as a positive entity. A training instance is constructed by pairing this ft+ with a negative entity ft\u2212 sampled from the all available candidate entities to the corresponding q. For each e, because there may be multiple positive candidate entities, the ft+ will not always be the same one to a negative entity. As Figure 3 shown, the model will generate the\nrepresentations for q, ft+, and ft\u2212 denoted as Rq, Rft+ and Rft\u2212 . This strategy makes the entity search task very similar to the answer selection task. As a result, we minimize a ranking objective function defined as follows:\nL = max{0,m\u2212 cos(Rq, Rft+) + cos(Rq, Rft\u2212)} (2)\nwhere m is a positive margin. Our goal is to make the positive entity more closed to the search query (higher similarity) in the vector space. If cos(Rq , Rft\u2212)\u2212 cos(Rq , Rft+) >= 0, it means the Rft\u2212 will not be ranked below the Rft+ , so m\u2212 cos(Rq , Rft+) + cos(Rq, Rft\u2212) > 0, and the neural network needs to update the parameters and a new negative example is sampled randomly. If cos(Rq , Rft\u2212)\u2212 cos(Rq, Rft+) < 0, it seems nice. However, only if it still be less than 0 after plus m, the loss L will be 0, and that is the real perfection. In other words, the margin m is a hyper-parameter to control the distinguishability of the positive and negative entities, i.e., we hope our system can distinguish between positive and negative entities as much as possible.\nThe strategy 2, entity reserved embedding (ERE) pays more attention to the named entity itself, so we do not split e which exists in a ft. However, d can still be segmented into several v. This strategy is mainly to investigate whether a single named entity is sufficient to contain valid semantic information. As Figure 3 shown, the other parts of ERE are just same as FME.\n3.2.2 Strategy 3: Translation with Description Embedding\nThere are two parts in the first two strategies, i.e., the query part e and the answer part composed of e and d. Translation with description embedding (TDE) shown in Figure 3 is different from previous strategies. q, e, and d are mapped to points in the vector space, and TDE tries to ac-\ncurately describe the relationship among the three elements by vector operation. More formally, the model expects Rd, the distributed representation of d to be a translation from Re to Rq . Under this assumption, Re+ +Rd+ should be the closest point of Rq , while Rq should be away from Re\u2212+Rd\u2212 . To learn such representations, the objective function is:\nL = max{0,m\u2212 cos(Re+ +Rd+ , Rq) + cos(Re\u2212 +Rd\u2212 , Rq)} (3)\nThe detailed optimization procedure of TDE is described in Algorithm 1. At each iteration, a small set of training samples serves as the mini-batch. FME and ERE also follow this mini-batch training for stochastic optimization. More detailed information about the parameter settings will be presented in Section 4.\n3.2.3 Feature representation and similarity metric With different embedding strategies, q, e, and d have their\ndistributed representations. Then, the dropout layer is to improve the generalization ability of the model. q, e, and d may contain more than one v, so we use 1-Max pooling to select the maximum one of the embeddings as the final representation.\nWhat we do in the last step is calculate the cosine similarity between the two representation vectors. In our experiments, we also tried other similarity metrics such as manhattan distance or euclidean distance, but the results were not better than the cosine similarity. Another important reason we use cosine similarity is that the inner product facilitates the derivation of gradients [5]."}, {"heading": "4. EXPERIMENTS", "text": "In this section, we describe various experiments to evalu-\nAlgorithm 1 Translation with Description Embedding\nInput: All q in training set and their corresponding (e+, d+) and (e\u2212, d\u2212). margin m, embeddings dim n. 1: v \u2190 uniform (\u22120.05, 0.05) for each v \u2208 V 2: repeat 3: qbatch \u2190 (q1, q2, . . . , qb size) // sample a mini-batch of size b size 4: e+batch \u2190 (e + 1 , e + 2 , . . . , e + b size)\n5: d+batch \u2190 (d + 1 , d + 2 , . . . , d + b size) 6: Trainbatch \u2190 \u2205 7: for all q \u2208 qbatch do 8: e\u2212batch \u2190 (e \u2212 1 , e \u2212 2 , . . . , e \u2212 b size) 9: d\u2212batch \u2190 (d \u2212 1 , d \u2212 2 , . . . , d \u2212 b size)\n10: Trainbatch \u2190 Trainbatch \u222a {((q, e + 1 , d + 1 ), (q, e \u2212 1 , d \u2212 1 )), . . . , ((q, e + b size, d + b size), (q, e \u2212 b size, d \u2212 b size))} 11: end for 12: Rq , Re and Rd \u2190 Representation learning with Strategy 3 // Also can be Strategy 1 or 2 for FME and TDE. 13: Update representation parameters w.r.t\n\u2211\n((q,e+,d+),(q,e\u2212,d\u2212))\u2208Trainbatch\n\u2207max{0,m\u2212 cos(Re+ +Rd+ , Rq) + cos(Re\u2212 +Rd\u2212 , Rq)}\n14: until convergence Output: All the embeddings of q, e, and d.\nate the proposed representation learning model with different strategies. The datasets1 and codes2 are publicly available."}, {"heading": "4.1 Datasets", "text": "The datasets used in our experiments comes from Baidu Cup\u2019 16. It consists of four kinds of entity search datasets (as shown in Table 1).\ntvShow. In this dataset, e is a keyword or key phrase which belongs to some TV show related attributes, e.g., q can be\u201creligious subjects,\u201d and its candidate entities (psize \u223c 100) are selected from the TV show entity set. All of the e have their d, which describe the year that the TV show is on.\nmovie. The movie dataset is same as the tvShow dataset, and only the named entity is changed into a movie. All named entities also have the description about the time of the movie\u2019s release.\nrestaurant. This dataset is different from the previous two. The q in the dataset is about some characteristics of restaurants such as \u201cspecial French fries.\u201d The d of e is the specific address of the restaurant.\ncelebrity. The q in this dataset is to find some people with certain characteristic, e.g., \u201csenior engineer.\u201d The entity descriptions are also informative. Some of them are information about job information, and others may reflect some experience of e. Also, one of the biggest features of this dataset is that not all e have d. This feature also increases the difficulty of the entity search task.\nIn the datasets mentioned above, the candidate pool size (p size) is about 100, so it is a challenging setting. In a real entity search system, the p size is maybe only more than a dozen candidates. As a result, if a model achieves good performance on those datasets, it will probably be able to do a good job when the p size becomes smaller.\n4.2 Experimental Setup\n1https://github.com/eshijia/baidu entity dataset 2https://github.com/eshijia/entity search\nEvaluation metrics. The quality of an entity search model will be evaluated by Mean Average Precision (MAP).\nMAP = 1\n|Q|\n|Q|\u2211\ni=1\nAverP (Ci, Ai) (4)\n|Q| denotes the total number of entity queries in the test set. AverP (C,A) = \u2211 n k=1 (P (k)\u00b7rel(k))\nmin(m,n) denotes the average\nprecision (AP). k is the rank in the sequence of retrieved candidate entities. m is the number of correct entities. n is the number of retrieved candidate entities. P (k) is the precision at cut-off k in the candidate entity list. rel(k) is an indicator function equaling one if the entity at rank k is a ground truth entity, and zero otherwise. The reason we use AP is the typical precision just considers the number of correct items in the return list, without taking into account the order between items. For an entity search engine, the candidate entities must be returned orderly, and the most relevant entity should be ranked in the front of the return list. Another two evaluation metrics we use are Top-1 accuracy and Hit@10. For a high availability entity search system, users are often concerned about whether the first entity (Top-1 ) in the list meets the requirements, and Hit@10 means the proportion of correct candidate entities ranked in the top 10.\nImplementation. Our entity search model in this paper was built from scratch using Python with Keras3, and all experiments were processed in a Tesla k20C GPU device.\nWe used a large-scale corpus crawled from the web-based encyclopedia to learn the pre-trained word embeddings with word2vec4, and the embedding size was 300. We tried several margin values, such as 0.5, 0.2, 0.05, 0.02, and we chose 0.02 at last. We fixed the dropout rate of the dropout layer as 0.5. We trained our models with Adam as the final optimization strategy. In the following, there will be a detailed comparison of different optimization strategies. The batch size (b size) was 64. The chosen hyper-parameters were\n3https://keras.io 4https://code.google.com/archive/p/word2vec\nTable 1: Statistics of the four kinds of datasets\nDatasets tvShow movie restaurant celebrity\n# entities (e) 9254 24347 54539 52831 # entity search queries (q) (Train/Test) 100/1000\n# candidate entities per q \u223c100\nbased on our experience and limited computing resources."}, {"heading": "4.3 Results", "text": "In addition to the contrast between different embedding strategies, we also compared two baseline methods. Baseline 1 is based on the keyword matching method (KWM) used in many search systems in practice, such as [11]. Baseline 2 utilizes vanilla word embeddings (W2V) trained by word2vec [12], i.e., we directly use the pre-trained word embeddings of v, and pairwise calculate the cosine similarity between the embeddings in q and c, then select the maximum similarity as the matching score between q and c. Table 2 shows the detail results of different entity search models.\n4.3.1 Performance analysis In Table 2, the model named with the prefix of p indicates\nthat it uses the pre-trained word vectors, and with a suffix of 1 means the dropout rate of its dropout layer is 0.5, 2 means 0.25 and 3 means it does not use the dropout layer. Therefore, compared to traditional KWM and vanilla W2V model, the proposed models with three embedding strategies are more suitable for the entity search task with higher MAP. For tvShow, movie and restaurant datasets where d is relatively simple and lack of information, the ERE models achieve the best performance. However, it is worth noting that for celebrity dataset where the description (d) is informative (this means that the description text is more likely to contain some words in q), the KWM model can get the best Top-1 accuracy. However, some e in this dataset do not have d, and the KWM model cannot return more correct candidate entities. The experiment results show that the TDE models achieve the best performance for celebrity dataset, and it verifies that the distributed representation of q, e, and d can be calculated by each other in the low-dimensional vector space.\nAlso, we can make following research results from Table 2. The dropout layer can effectively improve the performance of models (e.g., pERE-2 vs. pERE-3). Without the pretrained word embeddings (e.g., FME-2, ERE-2 or TDE-2), the overall performance of the model will naturally decrease. The fact that the FME models do not achieve excellent performance proves that methods used in the answer selection task cannot directly apply to the entity search task, and we should consider more characteristics of named entities to solve the entity search task.\n4.3.2 Optimizer comparison For the training process of our models, the optimizer will\nhave a significant influence on the convergence of learning models. Different optimizers fit different training tasks. Figure 4 shows the trends of the loss value in ERE-2 model with different popular optimizers. The performance of vanilla SGD [4] is poor. Within 100 epochs, there was no effective reduction of the loss value. Adagrad [4] adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters, and\nFigure 4: Training loss vs epochs with various optimizers\nit had a better convergence effect in this entity search task. Adadelta [20] and RMSprop [17] are both extensions of Adagrad, and they seek to reduce the aggressive, monotonically decreasing learning rate. The difference is that RMSprop divides the learning rate by an exponentially decaying average of squared gradients. However, there was no significant difference in the convergence effects of Adadelta and RMSprop. Adam [7] also calculates adaptive learning rates for each parameter. Unlike Adadelta and RMSprop update the learning rate just based on the exponentially decaying average of past squared gradients, Adam also keeps an exponentially decaying average of past gradients that similar to momentum, and it got the highest convergence rate. Therefore we could achieve better training results with a shorter training time, which is the main reason we chose the Adam optimizer. It should be noted that we must regularly assess the performance of the test set to prevent overfitting with the fast optimizer."}, {"heading": "5. CONCLUSIONS", "text": "In this paper, we study the problem of entity search by employing representation learning methods. With three embedding strategies, our proposed models can be adapted to different kinds of entity query tasks. Experimental results on four different datasets validate the superiority of the proposed models, and we also give an analysis about how to choose a proper optimizer to train the representation models.\nThe core idea of our models is to consider the relationships among named entities, entity descriptions and entity search queries in the low-dimensional vector space. Also, we pay attention to the short text features of them. The over-\nall framework in this paper is language independent, and it can be easily generalized to other similar tasks. There are many potential future research directions for this work. An significant trend is that combining with more external data sources, such as knowledge base, to obtain more entity features, thus improving the overall performance of the models. Our work is a excellent guidance of utilizing representation learning methods to solve the entity search task."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "This work was supported by the National Basic Research Program of China (2014CB340404), the National Natural Science Foundation of China (71571136), the Project of Science and Technology Commission of Shanghai Municipality (16JC1403000), and the Shanghai Municipal Science and Technology Research Project (14511108002). We thank Jianting Chen for useful discussions."}, {"heading": "7. REFERENCES", "text": "[1] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.\nA neural probabilistic language model. journal of machine learning research, 3(Feb):1137\u20131155, 2003.\n[2] R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM, 2008.\n[3] C. N. dos Santos and M. Gatti. Deep convolutional neural networks for sentiment analysis of short texts. In COLING, pages 69\u201378, 2014.\n[4] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.\n[5] M. Fan, Q. Zhou, E. Chang, and T. F. Zheng. Transition-based knowledge graph embedding with relational mapping properties. In Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, pages 328\u2013337, 2014.\n[6] M. Feng, B. Xiang, M. R. Glass, L. Wang, and B. Zhou. Applying deep learning to answer selection: A study and an open task. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 813\u2013820. IEEE, 2015.\n[7] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[8] S. Lai, K. Liu, S. He, and J. Zhao. How to generate a good word embedding? 2015.\n[9] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. In ICML, volume 14, pages 1188\u20131196, 2014.\n[10] J. Y. Lee and F. Dernoncourt. Sequential short-text classification with recurrent and convolutional neural networks. arXiv preprint arXiv:1603.03827, 2016.\n[11] Z. Liu, Y. Cai, Y. Shan, and Y. Chen. Ranking friendly result composition for xml keyword search. In International Conference on Conceptual Modeling, pages 441\u2013449. Springer, 2015.\n[12] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[13] A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426, 2012.\n[14] F. Morin and Y. Bengio. Hierarchical probabilistic neural network language model. In Aistats, volume 5, pages 246\u2013252. Citeseer, 2005.\n[15] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.\n[16] M. Tan, B. Xiang, and B. Zhou. Lstm-based deep learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108, 2015.\n[17] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4(2), 2012.\n[18] J.-R. Wen, J.-Y. Nie, and H.-J. Zhang. Clustering user queries of a search engine. In Proceedings of the 10th international conference on World Wide Web, pages 162\u2013168. acm, 2001.\n[19] Y. Yang and J. Tang. Beyond query: Interactive user intention understanding. In Data Mining (ICDM), 2015 IEEE International Conference on, pages 519\u2013528. IEEE, 2015.\n[20] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "journal of machine learning research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["C.N. dos Santos", "M. Gatti"], "venue": "In COLING,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Transition-based knowledge graph embedding with relational mapping properties", "author": ["M. Fan", "Q. Zhou", "E. Chang", "T.F. Zheng"], "venue": "In Proceedings of the 28th Pacific Asia Conference on Language,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["M. Feng", "B. Xiang", "M.R. Glass", "L. Wang", "B. Zhou"], "venue": "In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "How to generate a good word embedding", "author": ["S. Lai", "K. Liu", "S. He", "J. Zhao"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sequential short-text classification with recurrent and convolutional neural networks", "author": ["J.Y. Lee", "F. Dernoncourt"], "venue": "arXiv preprint arXiv:1603.03827,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Ranking friendly result composition for xml keyword search", "author": ["Z. Liu", "Y. Cai", "Y. Shan", "Y. Chen"], "venue": "In International Conference on Conceptual Modeling,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "arXiv preprint arXiv:1206.6426,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In Aistats,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1929}, {"title": "Lstm-based deep learning models for non-factoid answer selection", "author": ["M. Tan", "B. Xiang", "B. Zhou"], "venue": "arXiv preprint arXiv:1511.04108,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Clustering user queries of a search engine", "author": ["J.-R. Wen", "J.-Y. Nie", "H.-J. Zhang"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Beyond query: Interactive user intention understanding", "author": ["Y. Yang", "J. Tang"], "venue": "In Data Mining (ICDM),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "In the field of the search engine, how to make the search system understand the user intentions behind the user queries is a crucial question [19].", "startOffset": 142, "endOffset": 146}, {"referenceID": 17, "context": "An intelligent search system should meet either precise or vague requirement from users [18].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "The probability distribution of observing a word depends on some fixed number of surrounding words with neural language models (NLM) [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 11, "context": "The performance of the proposed framework is better than the traditional keyword matching and the vanilla word embedding method [12].", "startOffset": 128, "endOffset": 132}, {"referenceID": 5, "context": "Our work is most related to [6], and it learns the vector representations of questions and answers", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "We optimized the models using adaptive moment estimation (Adam) [7] which is an enhanced variant of stochastic gradient descent.", "startOffset": 64, "endOffset": 67}, {"referenceID": 8, "context": "Also, the idea of distributed word representations can be generalized to model sentences, paragraphs or even documents [9].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "[8] compares various word embedding models on different tasks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] and [3] propose a CNN based method with word embeddings to solve the short text classification task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[10] and [3] propose a CNN based method with word embeddings to solve the short text classification task.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "[6] and [16] design a few architectures of DNNs using CNN and longshort term memory networks (LSTM) to solve the problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[6] and [16] design a few architectures of DNNs using CNN and longshort term memory networks (LSTM) to solve the problem.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "A neural networks language model predicts the probability distribution of the next word utilizing several previous words [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 13, "context": "Hence, [14] and [13] use hierarchical softmax and noise contrastive estimation respectively to help reduce the training duration.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "Hence, [14] and [13] use hierarchical softmax and noise contrastive estimation respectively to help reduce the training duration.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "[2] proposed a model known as the C&W model where the central word in a sequence is the target word, and the surrounding words are put together into a context.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] presents a framework based on CNN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] considers the shortcomings of CNN, and adopts the LSTM to model the Q&A pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Unlike the typical answer selection task presented by [6] in Q&A research area, we exploit various embedding strategies and consider the short text character of entities and descriptions.", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "The dropout layer is a regularization technique for reducing overfitting by preventing complex coadaptations on training data [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 4, "context": "Another important reason we use cosine similarity is that the inner product facilitates the derivation of gradients [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": "Baseline 1 is based on the keyword matching method (KWM) used in many search systems in practice, such as [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "Baseline 2 utilizes vanilla word embeddings (W2V) trained by word2vec [12], i.", "startOffset": 70, "endOffset": 74}, {"referenceID": 3, "context": "The performance of vanilla SGD [4] is poor.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Adagrad [4] adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters, and Figure 4: Training loss vs epochs with various opti-", "startOffset": 8, "endOffset": 11}, {"referenceID": 19, "context": "Adadelta [20] and RMSprop [17] are both extensions of Adagrad, and they seek to reduce the aggressive, monotonically decreasing learning rate.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "Adadelta [20] and RMSprop [17] are both extensions of Adagrad, and they seek to reduce the aggressive, monotonically decreasing learning rate.", "startOffset": 26, "endOffset": 30}, {"referenceID": 6, "context": "Adam [7] also calculates adaptive learning rates for each parameter.", "startOffset": 5, "endOffset": 8}], "year": 2016, "abstractText": "We focus on the problem of learning distributed representations for entity search queries, named entities, and their short descriptions. With our representation learning models, the entity search query, named entity and description can be represented as low-dimensional vectors. Our goal is to develop a simple but effective model that can make the distributed representations of query related entities similar to the query in the vector space. Hence, we propose three kinds of learning strategies, and the difference between them mainly lies in how to deal with the relationship between an entity and its description. We analyze the strengths and weaknesses of each learning strategy and validate our methods on public datasets which contain four kinds of named entities, i.e., movies, TV shows, restaurants and celebrities. The experimental results indicate that our proposed methods can adapt to different types of entity search queries, and outperform the current state-of-the-art methods based on keyword matching and vanilla word2vec models. Besides, the proposed methods can be trained fast and be easily extended to other similar tasks.", "creator": "LaTeX with hyperref package"}}}