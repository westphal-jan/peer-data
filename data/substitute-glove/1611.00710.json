{"id": "1611.00710", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Deep counter networks for asynchronous event-based processing", "abstract": "Despite their advantages in terms over processes resources, latency, by greater reduces, event - based implementations instance neural networks have but been leave from achieve the own performance suggesting it able below board - under - new - history deep operating customized. We provision resistance peripheral as minimal spiking somatic models which even certain major where only part, thus avoiding tactic traceless. We episode too prescriptive there out in inner tool interactive converges to set made consistent increasing or are achieved five southern - 's - for - art concept communications. As having concert - related tone \u2014 computation close made larger latency took sparse updates, counter computers either shape sophisticated any extremely compact up decrease - power hardware implementation. We of theory and training uses for counter networks, in emphasize on the MNIST unchanged though counter marketing directions although, both held terms that much and which mainly patrols required, to official - of - main - book classification consistent.", "histories": [["v1", "Wed, 2 Nov 2016 18:22:33 GMT  (438kb,D)", "http://arxiv.org/abs/1611.00710v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["jonathan binas", "giacomo indiveri", "michael pfeiffer"], "accepted": false, "id": "1611.00710"}, "pdf": {"name": "1611.00710.pdf", "metadata": {"source": "CRF", "title": "Deep counter networks for asynchronous event-based processing", "authors": ["Jonathan Binas", "Giacomo Indiveri", "Michael Pfeiffer"], "emails": ["jbinas@ini.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "Despite the remarkable success of deep neural networks [14] in areas such as computer vision [13, 7] or speech recognition [8], biological neural systems clearly outshine their artificial counterparts in terms of compactness, speed and energy consumption. One putative reason for such efficiency may lie in the way signals are represented and transmitted in animal brains: data is transmitted sparsely and asynchronously, in small packets by means of spikes. This is in stark contrast with the frame-based approach of classical neural networks, which always compute the complete output of one layer synchronously, before passing it on to the next layer. Indeed, spike-based processing allows for more efficient utilization of communication channels and computing resources, and can lead to speedups in processing [17]. These advantages have sparked interest in dedicated spiking neural network electronic devices based on such event-driven\nar X\niv :1\n61 1.\n00 71\n0v 1\n[ cs\n.N E\n] 2\nprocessing schemes [16, 11]. Achieving the same accuracy as state-of-the-art deep learning models with event-based updates has remained challenging, but recently a number of methods have been proposed which convert a previously trained conventional analog neural network (ANN) into a spiking one [1, 5, 9, 6]. The principle of such conversion techniques is to approximate the continuous-valued activations of ANN units by the spike rates of event-based neurons. Although successful on several classical benchmark tasks, all these methods suffer from approximation errors, and typically require a multitude of spikes to represent a single continuous value, thereby losing some of the advantages of event-based computation.\nIn this work, we propose a set of minimalistic event-based asynchronous neural network models, which process input data streams continuously as they arrive, and which are formally equivalent to conventional frame-based models. This class of models can be used to build highly efficient event-based processing systems, potentially in conjunction with event-based sensors [4, 18]. The resulting systems process the stream of incoming data in real time, and yield first predictions of the output typically already after a few data packets have been received, and well before the full input pattern has been presented to the network.\nHere we demonstrate how the computation carried out by counter networks exactly reproduces the computation done in the conventional frame-based network. This allows maintaining the high accuracy of deep neural networks, but does so with a power and resource efficient representation and communication scheme. Specifically we show how, as a consequence of the event-driven style of processing, the resulting networks do not require computationally expensive multiplication operations. We initially demonstrate the principle for networks with binary activations, and then extend the model to non-binary activations. The performance of our novel models is evaluated on the mnist dataset. The numerical results indicate that counter networks require fewer operations than previous approaches to process a given input, while enabling state-of-the-art classification accuracy."}, {"heading": "2 Counter neural networks", "text": "Multiplications are the most expensive operations when using conventional neural networks for inference on digital hardware. It is therefore desirable to reduce the number of required multiplications to a minimum. In this section, we introduce an event-based neuron model, which only makes use of addition operations, counter variables, and simple comparisons, all of which can be implemented very efficiently in simple digital electronic circuits."}, {"heading": "2.1 Multiplication-free networks", "text": "Previous work has shown how frame-based neural networks can be implemented using additions only, by either restricting all weights to binary (or ternary) values [10, 15, 3], or by using binary activations [2]. The binary variable (weight\nor activation) then represents an indicator function, and all neural activations can be computed by simply adding up the selected weights or activations. To introduce our event-based model in its most basic form, we first investigate the case where neurons have binary activations, such that the output of all neurons y(k)i in layer k is given by\ny(k) = \u03c3 ( W(k)y(k\u22121) \u2212 \u03b8(k) ) , (1)\nwhere W is the weight matrix, \u03b8 are threshold values corresponding to bias terms, and\n\u03c3(x) = { 1, if x > 0 , 0, otherwise.\n(2)\nThus, the output of a neuron is 1 if its net input is greater than its threshold value \u03b8 and 0 otherwise. While this model does not pose any constraints on the weights and thresholds, we use low precision integer values in all experiments to keep the computational cost low and allow for highly efficient digital implementations. We consider here multi-layer networks trained through stochastic gradient descent using the backpropagation algorithm [19]. Since the error gradient is zero almost everywhere in the discrete network given by eqs. (1) and (2), we replace the binarization function \u03c3 by a logistic sigmoid function \u03c3\u0303 in the backward pass,\n\u03c3\u0303(x) = 1\n1 + exp (\u2212\u03bbx) , (3)\nwhere \u03bb is a scaling factor. Furthermore, during training, we keep copies of the high-resolution weights and activations, and use them to compute the gradient in the backward pass, as proposed by [3, 20]. In addition to the activations and network parameters, the inputs to the network are binarized by scaling them to lie in the range [0, 1] and rounding them to the nearest integer."}, {"heading": "2.2 Lossless event-based implementation through counter neurons", "text": "The multiplication-free network proposed above can directly be turned into an asynchronous event-based neural network by turning every unit of the ANN into a counter neuron, which we describe below. The weights and biases obtained through the training procedure above can be used in the event-based network without further conversion.\nEach counter neuron is defined by an internal counter variable c, which is updated whenever the neuron receives positive or negative inputs in the form of binary events (or spikes). The neuron operation is illustrated in fig. 1 and is described by alg. 1. A counter neuron essentially counts, or adds up, all the inputs it receives. Whenever this sum crosses the threshold \u03b8, a binary event \u00b11 is emitted. The value of the event depends on the direction in which\nthe threshold was crossed, i.e. a positive event is emitted when the threshold is crossed from below, and a negative event when c falls below the threshold. Whenever neuron j emits an event the quantity \u00b1wij is provided as input to neuron i of the next layer, with the sign determined by the output of neuron j. Thus, the neurons themselves do not continuously provide output signals, which makes information transmission and computation in the network very sparse. The input to the network is also provided in event-based form as a stream of binary events (or spikes), i.e. the network at discrete points in time receives a list of indices of one or more pixels, indicating that these pixels are active. Specifically, a binary input image (or other data) is streamed to the network pixel-by-pixel in an asynchronous manner, whereby the order and exact timing of the pixels does not matter. In the following we will show analytically that an event-based network based on counter neurons produces exactly the same output as its frame-based counterpart.\nProof of the equivalence. To prove the equivalence of the frame-based and the event-based model we have to show that the outputs of individual neurons are the same in both settings. In the following, we assume \u03b8k > 0\u2200k without loss of generality. Let an event-based network be initialized at time t = 0, such that all ck(0) = \u2212\u03b8k. Within the time interval [0, T ], neuron i of layer m in the event-based network receives a sequence of N inputs, (w(m)ij1 s1, . . . , w (m) ijN sN ) from a set of source neurons j1, . . . , jN at times t1, . . . , tN , where sk is the sign of\ninitialize: ci \u2190 \u2212\u03b8i whenever neuron i receives an input event inpi = signj\u2217wij\u2217 from a neuron j\u2217 or a sum of simultaneous input events inpi = \u2211 j\u2208J\u2217 signjwij from a set of\nneurons J\u2217 do\ncprevi \u2190 ci ci \u2190 ci + inpi if cprevi \u2264 0 and ci > 0 then\nemit +1 end\nif cprevi > 0 and ci \u2264 0 then emit \u22121\nend end\nAlgorithm 1: Basic counter neuron implementation.\nthe kth event, and 0 \u2264 tk \u2264 T, \u2200k. It follows from alg. 1 that the value of the counter variable c(m)i at time T is\nc (m) i (T ) = c (m) i (tN ) = \u2211 k=1,...,N w (m) ijk sk \u2212 \u03b8(m)i , (4)\nas it simply sums up the inputs. The sign of c(m)i might change several times during the presentation of the input, and trigger the emission of a positive or negative event at every zero-crossing. Since c(m)i is initialized at \u2212\u03b8(m)i < 0, there are 2n + \u03c3(c(m)i (tN )) sign changes in total, where n is a non-negative integer, and thus the total input communicated to neuron k of the next layer is\n\u03c3 ( c (m) i (tN ) ) w (m+1) ki = \u03c3 (\u2211N k=1 w (m) ijk sk \u2212 \u03b8(m)i ) w (m+1) ki =: y\u0302 (m) i w (m+1) ki , (5)\nas the 2n sign changes cancel out. On the other hand, recursively applying eq. (5) leads to\ny\u0302 (m) i = \u03c3 (\u2211 j w (m) ij \u03c3(c (m\u22121) j (tN ))\u2212 \u03b8 (m) i ) = \u03c3 (\u2211 j w (m) ij y\u0302 (m\u22121) j \u2212 \u03b8 (m) i ) . (6)\nSince y\u0302(0)k = y (0) k for the input layer, the equivalence must also hold for all higher layers, according to eq. (6).\nWith the notion of equivalence, it is clear that the event-based network, if configured with the parameters obtained for the frame-based network, is able to exactly reproduce the output of the latter. Unlike in previous work [1, 5], the resulting event-based network is guaranteed to provide the same result as the \u2018ideal\u2019 frame-based implementation. Thereby, the respective output value can be obtained by adding up the events emitted by the output layer. Technically,\nthe equivalence holds only in the case where the full stream of input events has been presented to the network, and propagated through all layers. In practice, however, a few input events are often enough to activate the right neurons and produce the correct output long before the full set of input events has been presented. As a consequence, on average far fewer operations than in the frame-based model are required to compute the correct output (see fig. 2 for an example)."}, {"heading": "2.3 Extension to non-binary inputs", "text": "The constraint that the input patterns are binary, and each input unit either produces an event or not, can be safely relaxed to integer-valued inputs without further modifications of the model. The framework thus supports finer grained input scales, which is important e.g. for input images using multiple gray-levels or RGB values to encode different colors. The simple modification is that each individual input unit produces over time a number of events that corresponds to the encoded integer value. While such integer-valued inputs would require\ninitialize: zi \u2190 0; ci \u2190 \u2212\u03b8i whenever there is an input event inpi = signj\u2217wij\u2217 from a neuron j\u2217 or a sum of simultaneous input events inpi =\u2211\nj\u2208J\u2217 signjwij from a set of neurons J \u2217 do\nci \u2190 ci + inpi while ci \u2265 \u03bb do\nemit +1 ci \u2190 ci \u2212 \u03bb zi \u2190 zi + 1\nend\nwhile zi > 0 and ci < 0 do emit \u22121 ci \u2190 ci + \u03bb zi \u2190 zi \u2212 1\nend end\nAlgorithm 2: Extended counter neuron implementation based on the discretized ReLU activation. The parameter \u03b8i represents the neuron\u2019s threshold. The scaling factor \u03bb allows adjusting the step size at which the neuron emits events, i.e. how much more input is required to trigger the next event.\nmultiplication in the frame-based network with non-binary (or non-ternary) weights, the event-based network remains free of multiplication operations, as the instantaneous output of any input unit is still binary."}, {"heading": "2.4 Extended counter neuron network with non-binary activations", "text": "Using binary outputs for neurons might be a disadvantage, because due to the the limited output bandwidth of individual neurons it might be necessary to use more neurons in hidden layers to obtain the same accuracy as a non-binary network. The counter network model can easily be extended to non-binary activations, without requiring multiplication in the event-based implementation. In order to train a network based on neurons featuring multiple, discrete levels of activation, we use a discretized version of the ReLU activation function during training:\nf(x) = \u03c1\n(\u230a x+\n\u03bb\n\u230b) , (7)\nwhere 1 is a small, positive constant, \u03bb \u2208 Z+ is a scaling factor, and \u03c1 is the typical ReLU half-wave rectification,\n\u03c1(x) = { x, if x > 0 , 0, otherwise.\n(8)\nAs in the binary case, the discrete activation is used during training in the forward pass, and a continuous approximation in the backward pass. Specifically, f can be approximated by a shifted and scaled ReLU,\nf\u0303(x) = \u03c1\n( x\n\u03bb \u2212 1 2\n) , (9)\nin the backward pass. The learned parameters can then again be directly transferred to configure a network of event-based neurons without further conversion of weights or biases. The dynamics of this network are illustrated in fig. 3, and described in alg. 2. The equivalence of the frame-based and the event-based implementation can be proven similarly to the binary case. A sketch of the proof is outlined below:\nProof of the equivalence. From alg. 2 it can be seen that after a neuron has processed a number of input events, its internal variable zi has the value \u03c1(b(xi\u2212\n\u03b8i + )/\u03bbc), where xi is the accumulated input provided over time. On the other hand, the value of zi changes only when an event is emitted, and its value corresponds to the number of positive events emitted, minus the number of negative events emitted. Thus, the accumulated output communicated by the neuron corresponds precisely to zi, and thereby to the output of the frame-based model given by eq. (7), since xi corresponds to the total input provided by neurons from the previous layer, xi = \u2211 j wijyj .\nThe discretized ReLU offers a wider range of values than the binary activation, and therefore allows for a more fine-grained response, thereby facilitating training. On the other hand, using non-binary outputs might lead to larger output delays compared to the binary case, as the trained neurons might now require a multitude of events from individual neurons to arrive at the correct output."}, {"heading": "3 Results", "text": "Various networks were trained on the mnist dataset of handwritten digits to demonstrate competitive classification accuracy. In particular, we evaluated fully connected networks (FCNs) of three hidden layers (784-1000-1000-1000-10) and five hidden layers (784-750-750-750-750-750-10) to investigate how the depth of the network affects the processing speed and efficiency. In addition we trained convolutional networks (CNNs) with two (784-12c5-12c7-10) and four (784-12c312c5-12c7-12c9-10) hidden layers. The network dimensions were chosen such that the number of parameters remains roughly the same in the shallower and deeper networks (\u22482.8 mio. parameters for the FCNs, and \u224850k for the CNNs.)"}, {"heading": "3.1 Training details", "text": "The networks were trained through stochastic gradient descent using the Adam method [12]. The gradients for the backward pass were computed using the continuous approximations described by eqs. (3) and (9). All parameters were restricted to 8-bit integer precision in the forward pass, and floating point representations were used only in the backward pass, as suggested by [3, 20]. The biases \u03b8 were restricted to non-negative values through an additional constraint in the objective function, otherwise categorical cross-entropy was used as the loss function. The learning rate was set to 0.01 for the CNNs and to 0.005 for the FCNs. The mnist dataset was split into a training set of 50000 samples, a validation set of 10000 samples, and a test set of 10000 samples, and a batch size of 200 samples was used during training. The networks were trained until a classification error of \u2248 1.5% on the validation set was obtained. The low-precision parameters were then directly used in an event-based network of equivalent architecture."}, {"heading": "3.2 Fast and efficient classification", "text": "The main advantage of event-based deep networks is that outputs can be obtained fast and efficiently. We quantify this in Figure 4, where the processing speed is measured as the time taken to produce the same output as the frame-based model, and efficiency is quantified as the number of addition operations required to compute the output. For this analysis, individual pixels of the input image are\nprovided to the network one by one in random order. In the systems based on the basic counter network model with binary neurons, the majority of events is emitted at the beginning of the input data presentation, with activity gradually declining in the course of the presentation. This reflects the fact that counter neurons cannot emit two events of the same sign in a row, leading to quick saturation in the output. The opposite is the case for the extended counter neuron based on the discretized ReLU, where activity gradually ramps up. Overall, this allows the extended model to produce the desired output with fewer operations than the basic model, as can be seen in fig. 5. The achieved efficiency is beyond what had been possible with previous conversion-based methods: our method achieves classification of mnist at <500k events (CNN based on the extended neuron model), while the best reported result of a conversion-based network, to our knowledge, is >1 mio. events [17]. Despite the different network architectures, the behavior of FCNs and CNNs is qualitatively similar, with the main differences being due to the neuron model. In general, deeper networks seem to require a greater number of operations than shallower networks to achieve equivalent results."}, {"heading": "4 Discussion", "text": "The two presented counter neuron models allow efficient event-based implementations of deep neural network architectures. While previous methods constructed deep spiking networks by converting parameters and approximating activations with firing rates, the output of our model is provably equivalent to its frame-based counterpart. Training is done in the frame-based domain, where state-of-the-art neural network optimization methods can be exploited. The discrete nature of counter networks allows hardware-friendly digital implementations, and makes them very suitable to process data from event-based sensors [18]. The resulting\nsystems differ from traditional neural networks in the sense that units are updated \u2018depth first\u2019, rather than \u2018breadth first\u2019, meaning that any neuron can fire when its threshold is crossed, instead of waiting for all neurons in previous layers to be updated, as in conventional neural networks. This allows processing of input data as they arrive, rather than waiting for a whole frame to be transferred to the input layer. This can significantly speed up processing in digital applications. Compared to other deep spiking neural networks based on parameter conversion [17], counter networks require fewer operations to process input images, even in our non-optimized setting. We expect that adding further constraints to enforce sparsity or reduce neuron activations can make counter networks even more efficient. Further research is required to investigate the applicability of the counter neuron model in recurrent networks. Finally, event-based systems are appealing because they allow for purely local, event-based weight update rules, such as spike-timing dependent plasticity (STDP). Preliminary results indicate that STDP-based training of counter networks is possible, which not only would allow efficient inference but also training of deep event-based networks."}, {"heading": "Acknowledgements", "text": "The research was supported by the Swiss National Science Foundation Grant 200021-146608 and the European Union ERC Grant 257219."}], "references": [{"title": "Spiking deep convolutional neural networks for energy-efficient object recognition", "author": ["Yongqiang Cao", "Yang Chen", "Deepak Khosla"], "venue": "International Journal of Computer Vision,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "BinaryNet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Matthieu Courbariaux", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Activity-driven, event-based vision sensors", "author": ["Tobi Delbr\u00fcck", "Bernabe Linares-Barranco", "Eugenio Culurciello", "Christoph Posch"], "venue": "In Proceedings of 2010 IEEE International Symposium on Circuits and Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing", "author": ["Peter U Diehl", "Daniel Neil", "Jonathan Binas", "Matthew Cook", "Shih-Chii Liu", "Michael Pfeiffer"], "venue": "In 2015 International Joint Conference on Neural Networks (IJCNN). IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Convolutional networks for fast, energy-efficient neuromorphic computing", "author": ["Steven K. Esser", "Paul A. Merolla", "John V. Arthur", "Andrew S. Cassidy", "Rathinakumar Appuswamy", "Alexander Andreopoulos", "David J. Berg", "Jeffrey L. McKinstry", "Timothy Melano", "Davis R. Barch", "Carmelo di Nolfo", "Pallab Datta", "Arnon Amir", "Brian Taba", "Myron D. Flickner", "Dharmendra S. Modha"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey E Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-Rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Spiking deep networks with LIF neurons", "author": ["Eric Hunsberger", "Chris Eliasmith"], "venue": "arXiv preprint arXiv:1510.08829,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "IEEE Workshop on Signal Processing Systems (SiPS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Neuromorphic architectures for spiking deep neural networks", "author": ["Giacomo Indiveri", "Federico Corradi", "Ning Qiao"], "venue": "In 2015 IEEE International Electron Devices Meeting (IEDM). IEEE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1510.03009,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["Paul A Merolla", "John V Arthur", "Rodrigo Alvarez-Icaza", "Andrew S Cassidy", "Jun Sawada", "Filipp Akopyan", "Bryan L Jackson", "Nabil Imam", "Chen Guo", "Yutaka Nakamura"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Learning to be efficient: algorithms for training low-latency, low-compute deep spiking neural networks", "author": ["Daniel Neil", "Michael Pfeiffer", "Shih-Chii Liu"], "venue": "In Proceedings of the 31st Annual ACM Symposium on Applied Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Retinomorphic event-based vision sensors: bioinspired cameras with spiking output", "author": ["Christoph Posch", "Teresa Serrano-Gotarredona", "Bernabe Linares-Barranco", "Tobi Delbruck"], "venue": "Proceedings of the IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Learning representations by back-propagating errors", "author": ["D E Rumelhart", "G E Hinton", "R J Williams"], "venue": "Nature, 323:533\u2013536,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1986}, {"title": "Robustness of spiking deep belief networks to noise and reduced bit precision of neuro-inspired hardware platforms", "author": ["Evangelos Stromatias", "Daniel Neil", "Michael Pfeiffer", "Francesco Galluppi", "Steve B Furber", "Shih-Chii Liu"], "venue": "Frontiers in neuroscience,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Despite the remarkable success of deep neural networks [14] in areas such as computer vision [13, 7] or speech recognition [8], biological neural systems clearly outshine their artificial counterparts in terms of compactness, speed and energy consumption.", "startOffset": 93, "endOffset": 100}, {"referenceID": 6, "context": "Despite the remarkable success of deep neural networks [14] in areas such as computer vision [13, 7] or speech recognition [8], biological neural systems clearly outshine their artificial counterparts in terms of compactness, speed and energy consumption.", "startOffset": 93, "endOffset": 100}, {"referenceID": 7, "context": "Despite the remarkable success of deep neural networks [14] in areas such as computer vision [13, 7] or speech recognition [8], biological neural systems clearly outshine their artificial counterparts in terms of compactness, speed and energy consumption.", "startOffset": 123, "endOffset": 126}, {"referenceID": 15, "context": "Indeed, spike-based processing allows for more efficient utilization of communication channels and computing resources, and can lead to speedups in processing [17].", "startOffset": 159, "endOffset": 163}, {"referenceID": 14, "context": "processing schemes [16, 11].", "startOffset": 19, "endOffset": 27}, {"referenceID": 10, "context": "processing schemes [16, 11].", "startOffset": 19, "endOffset": 27}, {"referenceID": 0, "context": "Achieving the same accuracy as state-of-the-art deep learning models with event-based updates has remained challenging, but recently a number of methods have been proposed which convert a previously trained conventional analog neural network (ANN) into a spiking one [1, 5, 9, 6].", "startOffset": 267, "endOffset": 279}, {"referenceID": 4, "context": "Achieving the same accuracy as state-of-the-art deep learning models with event-based updates has remained challenging, but recently a number of methods have been proposed which convert a previously trained conventional analog neural network (ANN) into a spiking one [1, 5, 9, 6].", "startOffset": 267, "endOffset": 279}, {"referenceID": 8, "context": "Achieving the same accuracy as state-of-the-art deep learning models with event-based updates has remained challenging, but recently a number of methods have been proposed which convert a previously trained conventional analog neural network (ANN) into a spiking one [1, 5, 9, 6].", "startOffset": 267, "endOffset": 279}, {"referenceID": 5, "context": "Achieving the same accuracy as state-of-the-art deep learning models with event-based updates has remained challenging, but recently a number of methods have been proposed which convert a previously trained conventional analog neural network (ANN) into a spiking one [1, 5, 9, 6].", "startOffset": 267, "endOffset": 279}, {"referenceID": 3, "context": "This class of models can be used to build highly efficient event-based processing systems, potentially in conjunction with event-based sensors [4, 18].", "startOffset": 143, "endOffset": 150}, {"referenceID": 16, "context": "This class of models can be used to build highly efficient event-based processing systems, potentially in conjunction with event-based sensors [4, 18].", "startOffset": 143, "endOffset": 150}, {"referenceID": 9, "context": "Previous work has shown how frame-based neural networks can be implemented using additions only, by either restricting all weights to binary (or ternary) values [10, 15, 3], or by using binary activations [2].", "startOffset": 161, "endOffset": 172}, {"referenceID": 13, "context": "Previous work has shown how frame-based neural networks can be implemented using additions only, by either restricting all weights to binary (or ternary) values [10, 15, 3], or by using binary activations [2].", "startOffset": 161, "endOffset": 172}, {"referenceID": 2, "context": "Previous work has shown how frame-based neural networks can be implemented using additions only, by either restricting all weights to binary (or ternary) values [10, 15, 3], or by using binary activations [2].", "startOffset": 161, "endOffset": 172}, {"referenceID": 1, "context": "Previous work has shown how frame-based neural networks can be implemented using additions only, by either restricting all weights to binary (or ternary) values [10, 15, 3], or by using binary activations [2].", "startOffset": 205, "endOffset": 208}, {"referenceID": 17, "context": "We consider here multi-layer networks trained through stochastic gradient descent using the backpropagation algorithm [19].", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "Furthermore, during training, we keep copies of the high-resolution weights and activations, and use them to compute the gradient in the backward pass, as proposed by [3, 20].", "startOffset": 167, "endOffset": 174}, {"referenceID": 18, "context": "Furthermore, during training, we keep copies of the high-resolution weights and activations, and use them to compute the gradient in the backward pass, as proposed by [3, 20].", "startOffset": 167, "endOffset": 174}, {"referenceID": 0, "context": "In addition to the activations and network parameters, the inputs to the network are binarized by scaling them to lie in the range [0, 1] and rounding them to the nearest integer.", "startOffset": 131, "endOffset": 137}, {"referenceID": 0, "context": "Unlike in previous work [1, 5], the resulting event-based network is guaranteed to provide the same result as the \u2018ideal\u2019 frame-based implementation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 4, "context": "Unlike in previous work [1, 5], the resulting event-based network is guaranteed to provide the same result as the \u2018ideal\u2019 frame-based implementation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 11, "context": "The networks were trained through stochastic gradient descent using the Adam method [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 2, "context": "All parameters were restricted to 8-bit integer precision in the forward pass, and floating point representations were used only in the backward pass, as suggested by [3, 20].", "startOffset": 167, "endOffset": 174}, {"referenceID": 18, "context": "All parameters were restricted to 8-bit integer precision in the forward pass, and floating point representations were used only in the backward pass, as suggested by [3, 20].", "startOffset": 167, "endOffset": 174}, {"referenceID": 15, "context": "events [17].", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "The discrete nature of counter networks allows hardware-friendly digital implementations, and makes them very suitable to process data from event-based sensors [18].", "startOffset": 160, "endOffset": 164}, {"referenceID": 15, "context": "Compared to other deep spiking neural networks based on parameter conversion [17], counter networks require fewer operations to process input images, even in our non-optimized setting.", "startOffset": 77, "endOffset": 81}], "year": 2016, "abstractText": "Despite their advantages in terms of computational resources, latency, and power consumption, event-based implementations of neural networks have not been able to achieve the same performance figures as their equivalent state-of-the-art deep network models. We propose counter neurons as minimal spiking neuron models which only require addition and comparison operations, thus avoiding costly multiplications. We show how inference carried out in deep counter networks converges to the same accuracy levels as are achieved with state-of-the-art conventional networks. As their event-based style of computation leads to reduced latency and sparse updates, counter networks are ideally suited for efficient compact and low-power hardware implementation. We present theory and training methods for counter networks, and demonstrate on the mnist benchmark that counter networks converge quickly, both in terms of time and number of operations required, to state-of-the-art classification accuracy.", "creator": "LaTeX with hyperref package"}}}