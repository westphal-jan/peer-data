{"id": "1611.01578", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural connected like powerful often flexible models this many are two own easier learning provide in recognition, saying and complex tagalog regards. Despite rather success, connective communication well still hard should design. In this news, certainly produce a sustaining network also increase new available descriptions similar neural broadband under crash so RNN few feedback improve to enable the expected calculations of its generated architectures on each validation round. On given CIFAR - 42 flowchart, ability method, players from myself, can piece new novel broadcasts modern far helped entire most furthermore - invented century prior terms but fast allowing accuracy. Our CIFAR - 10 model attainable turned fast passing market of 3. 84, result is only 0. 1 percent anything both, . long-play dramatically 100 rest example civil - 's - for - art styling. On only Penn Treebank co-ordinates, nothing model can co-write a novel recurrent cell which jdsu an widely - all LSTM cell, and well virginia - several - while - collections thalweg. Our cell maximizes example test eight clumsiness fact 106. 4 made first Penn Treebank, in is five. nine deliberateness better than, however state - of - the - exhibit.", "histories": [["v1", "Sat, 5 Nov 2016 00:41:37 GMT  (583kb,D)", "http://arxiv.org/abs/1611.01578v1", null], ["v2", "Wed, 15 Feb 2017 05:28:05 GMT  (585kb,D)", "http://arxiv.org/abs/1611.01578v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["barret zoph", "quoc v le"], "accepted": true, "id": "1611.01578"}, "pdf": {"name": "1611.01578.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Barret Zoph", "Quoc V. Le"], "emails": ["barretzoph@google.com", "qvl@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The last few years have seen much success of deep neural networks in many challenging applications, such as speech recognition (Hinton et al., 2012), image recognition (LeCun et al., 1998; Krizhevsky et al., 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Along with this success is a paradigm shift from feature designing to architecture designing, i.e., from SIFT (Lowe, 1999), and HOG (Dalal & Triggs, 2005), to AlexNet (Krizhevsky et al., 2012), VGGNet (Simonyan & Zisserman, 2014), GoogleNet (Szegedy et al., 2015), and ResNet (He et al., 2016a). Although it has become easier, designing architectures still requires a lot of expert knowledge and takes ample time.\nFigure 1: An overview of Neural Architecture Search.\nThis paper presents Neural Architecture Search, a gradient-based method for finding good architectures (see Figure 1) . Our work is based on the observation that the structure and connectivity of a neural network can be typically specified by a variable-length string. It is therefore possible to use\n\u2217Work done as a member of the Google Brain Residency program (g.co/brainresidency.)\nar X\niv :1\n61 1.\n01 57\n8v 1\n[ cs\n.L G\n] 5\nN ov\n2 01\n6\na recurrent network \u2013 the controller \u2013 to generate such string to construct a network. Training the network \u2013 the \u201dchild network\u201d \u2013 on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.\nOur experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.84 test set error, while being 1.2x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art."}, {"heading": "2 RELATED WORK", "text": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Despite their success, these methods are still limited in that they only search models from a fixed-length space. In other words, it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a network. In practice, these methods often work better if they are supplied with a good initial model (Bergstra & Bengio, 2012; Snoek et al., 2012; 2015).\nModern neuro-evolution algorithms, e.g., Wierstra et al. (2005); Floreano et al. (2008); Stanley et al. (2009), on the other hand, are much more flexible for composing novel models, yet they are usually less practical at a large scale. Their limitations lie in the fact that they are search-based methods, thus they are slow or require many heuristics to work well.\nNeural Architecture Search has some parallels to program synthesis and inductive programming, the idea of searching a program from examples (Summers, 1977; Biermann, 1978). In machine learning, probabilistic program induction has been used successfully in many settings, such as learning to solve simple Q&A (Liang et al., 2010; Neelakantan et al., 2015; Andreas et al., 2016), sort a list of numbers (Reed & de Freitas, 2015), and learning with very few examples (Lake et al., 2015).\nThe controller in Neural Architecture Search is auto-regressive, which means it predicts hyperparameters one a time, conditioned on previous predictions. This idea is borrowed from the decoder in end-to-end sequence to sequence learning (Sutskever et al., 2014). Unlike sequence to sequence learning, our method optimizes a non-differentiable metric, which is the accuracy of the child network. It is therefore similar to the work on BLEU optimization in Neural Machine Translation (Shen et al., 2016; Ranzato et al., 2015). Unlike these approaches, our method learns directly from the reward signal without any supervised bootstrapping.\nAlso related to our work is the idea of learning to learn or meta-learning (Thrun & Pratt, 2012), a general framework of using information learned in one task to improve a future task. More closely related is the idea of using a neural network to learn the gradient descent updates for another network (Andrychowicz et al., 2016) and the idea of using reinforcement learning to find update policies for another network (Li & Malik, 2016)."}, {"heading": "3 METHODS", "text": "In the following section, we will first describe a simple method of using a recurrent network to generate convolutional architectures. We will show how the recurrent network can be trained with a policy gradient method to maximize the expected accuracy of the sampled architectures. We will present several improvements of our core approach such as forming skip connections to increase model complexity and using a parameter server approach to speed up training. In the last part of the section, we will focus on generating recurrent architectures, which is another key contribution of our paper."}, {"heading": "3.1 GENERATING MODEL DESCRIPTIONS WITH A CONTROLLER RECURRENT NEURAL NETWORK", "text": "In Neural Architecture Search, we use a controller to generate architectural hyperparameters of neural networks. To be flexible, the controller is implemented as a recurrent neural network. Let\u2019s suppose we would like to predict feedforward neural networks with only convolutional layers, we can use the controller to generate their hyperparameters as a sequence of tokens:\nIn our experiments, the process of generating an architecture stops if the number of layers exceeds a certain value. This value follows a schedule where we increase it as training progresses. Once the controller RNN finishes generating an architecture, a neural network with this architecture is built and trained. At convergence, the accuracy of the network on a held-out validation set is recorded. The parameters of the controller RNN, \u03b8c, are then optimized in order to maximize the expected validation accuracy of the proposed architectures. In the next section, we will describe a policy gradient method which we use to update parameters \u03b8c so that the controller RNN generates better architectures over time."}, {"heading": "3.2 TRAINING WITH REINFORCE", "text": "The list of tokens that the controller predicts can be viewed as a list of actions a1:T to design an architecture for a child network. At convergence, this child network will achieve an accuracy R on a held-out dataset. We can use this accuracy R as the reward signal and use reinforcement learning to train the controller. More concretely, to find the optimal architecture, we ask our controller to maximize its expected reward, represented by J(\u03b8c):\nJ(\u03b8c) = EP (a1:T ;\u03b8c)[R]\nSince the reward signalR is non-differentiable, we need to use a policy gradient method to iteratively update \u03b8c. In this work, we use the REINFORCE rule from (Williams, 1992):\n5\u03b8cJ(\u03b8c) = T\u2211 t=1 EP (a1:T ;\u03b8c) [ 5\u03b8c logP (at|a(t\u22121):1; \u03b8c)R ] An empirical approximation of the above quantity is:\n1\nm m\u2211 k=1 T\u2211 t=1 5\u03b8c logP (at|a(t\u22121):1; \u03b8c)Rk\nWhere m is the number of different architectures that the controller samples in one batch and T is the number of hyperparameters our controller has to predict to design a neural network architecture.\nThe validation accuracy that the k-th neural network architecture achieves after being trained on a training dataset is Rk.\nThe above update is an unbiased estimate for our gradient, but has a very high variance. In order to reduce the variance of this estimate we employ a baseline function:\n1\nm m\u2211 k=1 T\u2211 t=1 5\u03b8c logP (at|a(t\u22121):1; \u03b8c)(Rk \u2212 b)\nAs long as the baseline function b does not depend on the on the current action, then this is still an unbiased gradient estimate. In this work, our baseline b is an exponential moving average of the previous architecture accuracies.\nAccelerate Training with Parallelism and Asynchronous Updates: In Neural Architecture Search, each gradient update to the controller parameters \u03b8c corresponds to training one child network to convergence. As training a child network can take hours, we use distributed training and asynchronous parameter updates in order to speed up the learning process of the controller (Dean et al., 2012). We use a parameter-server scheme where we have a parameter server of S shards, that store the shared parameters for K controller replicas. Each controller replica samples m different child architectures that are trained in parallel. The controller then collects gradients according to the results of that minibatch of m architectures at convergence and sends them to the parameter server in order to update the weights across all controller replicas. In our implementation, convergence of each child network is reached when its training exceeds a certain number of epochs. This scheme of parallelism is summarized in Figure 3."}, {"heading": "3.3 INCREASE ARCHITECTURE COMPLEXITY WITH SKIP CONNECTIONS AND OTHER LAYER TYPES", "text": "In Section 3.1, the search space does not have skip connections, or branching layers used in modern architectures such as GoogleNet (Szegedy et al., 2015), and Residual Net (He et al., 2016a). In this section we introduce a method that allows our controller to propose skip connections or branching layers, thereby widening the search space.\nTo enable the controller to predict such connections, we use a set-selection type attention (Neelakantan et al., 2015) which was built upon the attention mechanism (Bahdanau et al., 2015; Vinyals et al., 2015). At layer N , we add an anchor point which has N \u2212 1 content-based sigmoids to indicate the previous layers that need to be connected. Each sigmoid is a function of the current hiddenstate of the controller and the previous hiddenstates of the previous N \u2212 1 anchor points:\nP(Layer j is an input to layer i) = sigmoid(vTtanh(Wprev \u2217 hj +Wcurr \u2217 hi)), where hj represents the hiddenstate of the controller at anchor point for the j-th layer, where j ranges from 0 to N \u2212 1. We then sample from these sigmoids to decide what previous layers to be\nused as inputs to the current layer. The matrices Wprev , Wcurr and v are trainable parameters. As these connections are also defined by probability distributions, the REINFORCE method still applies without any significant modifications. Figure 4 shows how the controller uses skip connections to decide what layers it wants as inputs to the current layer.\nIn our framework, if one layer has many input layers then all input layers are concatenated in the depth dimension. Skip connections can cause \u201dcompilation failures\u201d where one layer is not compatible with another layer, or one layer may not have any input or output. To circumvent these issues, we employ three simple techniques. First, if a layer is not connected to any input layer then the image is used as the input layer. Second, at the final layer we take all layer outputs that have not been connected and concatenate them before sending this final hiddenstate to the classifier. Lastly, if input layers to be concatenated have different sizes, we pad the small layers with zeros so that the concatenated layers have the same sizes.\nFinally, in Section 3.1, we do not predict the learning rate and we also assume that the architectures consist of only convolutional layers, which is also quite restrictive. It is possible to add the learning rate as one of the predictions. Additionally, it is also possible to predict pooling, local contrast normalization (Jarrett et al., 2009; Krizhevsky et al., 2012), and batchnorm (Ioffe & Szegedy, 2015) in the architectures. To be able to add more types of layers, we need to add an additional step in the controller RNN to predict the layer type, then other hyperparameters associated with it."}, {"heading": "3.4 GENERATE RECURRENT CELL ARCHITECTURES", "text": "In this section, we will modify the above method to generate recurrent cells. At every time step t, the controller needs to find a functional form for ht that takes xt and ht\u22121 as inputs. The simplest way is to have ht = tanh(W1\u2217xt+W2\u2217ht\u22121), which is the formulation of a basic recurrent cell. A more complicated formulation is the widely-used LSTM recurrent cell (Hochreiter & Schmidhuber, 1997).\nThe computations for basic RNN and LSTM cells can be generalized as a tree of steps that take xt and ht\u22121 as inputs and produce ht as final output. The controller RNN needs to label each node in the tree with a combination method (addition, elementwise multiplication, etc.) and an activation function (tanh, sigmoid, etc.) to merge two inputs and produce one output. Two outputs are then fed as inputs to the next node in the tree. To allow the controller RNN to select these methods and functions, we index the nodes in the tree in an order so that the controller RNN can visit each node one by one and label the needed hyperparameters.\nInspired by the construction of the LSTM cell (Hochreiter & Schmidhuber, 1997), we also need cell variables ct\u22121 and ct to represent the memory states. To incorporate these variables, we need the controller RNN to predict what nodes in the tree to connect these two variables to. These predictions can be done in the last two blocks of the controller RNN.\nTo make this process more clear, we show an example in Figure 5, for a tree structure that has two leaf nodes and one internal node. The leaf nodes are indexed by 0 and 1, and the internal node is indexed by 2. The controller RNN needs to first predict 3 blocks, each block specifying a combina-\ntion method and an activation function for each tree index. After that it needs to predict the last 2 blocks that specify how to connect ct and ct\u22121 to temporary variables inside the tree. Specifically, according to the predictions of the controller RNN in this example, the following computation steps will occur:\n\u2022 The controller predicts Add and Tanh for tree index 0, this means we need to compute a0 = tanh(W1 \u2217 xt +W2 \u2217 ht\u22121).\n\u2022 The controller predicts ElemMult and ReLU for tree index 1, this means we need to compute a1 = ReLU ( (W3 \u2217 xt) (W4 \u2217 ht\u22121) ) .\n\u2022 The controller predicts 0 for the second element of the \u201dCell Index\u201d, Add and ReLU for elements in \u201dCell Inject\u201d, which means we need to compute anew0 = ReLU(a0 + ct\u22121). Notice that we don\u2019t have any learnable parameters for the internal nodes of the tree.\n\u2022 The controller predicts ElemMult and Sigmoid for tree index 2, this means we need to compute a2 = sigmoid(anew0 a1). Since the maximum index in the tree is 2, ht is set to a2.\n\u2022 The controller RNN predicts 1 for the first element of the \u201dCell Index\u201d, this means that we should set ct to the output of the tree at index 1 before the activation, i.e., ct = (W3 \u2217xt) (W4 \u2217 ht\u22121).\nIn the above example, the tree has two leaf nodes, thus it is called a \u201dbase 2\u201d architecture. In our experiments, we use a base number of 8 to make sure that the cell is expressive."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": "We apply our method to an image classification task with CIFAR-10 and a language modeling task with Penn Treebank, two of the most benchmarked datasets in deep learning. On CIFAR-10, our goal is to find a good convolutional architecture whereas on Penn Treebank our goal is to find a good recurrent cell. On each dataset, we have a separate held-out validation dataset to compute the reward signal. The reported performance on the test set is computed only once for the network that achieves the best result on the held-out validation dataset. More details about our experimental procedures and results are as follows."}, {"heading": "4.1 LEARNING CONVOLUTIONAL ARCHITECTURES FOR CIFAR-10", "text": "Dataset: In these experiments we use the CIFAR-10 dataset with data preprocessing and augmentation procedures that are in line with other previous results. We first preprocess the data by whitening all the images. Additionally, we upsample each image then choose a random 32x32 crop of this upsampled image. Finally, we use random horizontal flips on this 32x32 cropped image.\nSearch space: Our search space consists of convolutional architectures, with rectified linear units as non-linearities (Nair & Hinton, 2010), batch normalization (Ioffe & Szegedy, 2015) and skip\nconnections between layers (Section 3.3). For every convolutional layer, the controller RNN has to select a filter height in [1, 3, 5, 7], a filter width in [1, 3, 5, 7], and a number of filters in [24, 36, 48, 64]. For strides, we perform two sets of experiments, one where we fix the strides to be 1, and one where we allow the controller to predict the strides in [1, 2, 3].\nTraining details: The controller RNN is a two-layer LSTM with 35 hidden units on each layer. It is trained with the ADAM optimizer (Kingma & Ba, 2015) with a learning rate of 0.0006. The weights of the controller are initialized uniformly between -0.08 and 0.08. For the distributed training, we set the number of parameter server shards S to 20, the number of controller replicas K to 100 and the number of child replicas m to 8, which means there are 800 networks being trained on 800 GPUs concurrently at any time.\nOnce the controller RNN samples an architecture, a child model is constructed and trained for 50 epochs. The reward used for updating the controller is the maximum validation accuracy of the last 5 epochs cubed. The validation set has 5,000 examples randomly sampled from the training set, the remaining 45,000 examples are used for training. The settings for training the CIFAR-10 child models are the same with those used in (Huang et al., 2016a). We use the Momentum Optimizer with a learning rate of 0.1, weight decay of 1e-4, momentum of 0.9 and used Nesterov Momentum (Sutskever et al., 2013).\nDuring the training of the controller, we use a schedule of increasing number of layers in the child networks as training progresses. On CIFAR-10, we ask the controller to increase the depth by 2 for the child models every 1,600 samples, starting at 6 layers.\nResults: After training the controller for 12,800 iterations, we find the architecture that achieves the best validation accuracy. We then run a small grid search over learning rate, weight decay, batchnorm epsilon and what epoch to decay the learning rate. The best model from this grid search is then run until convergence and we then compute the test accuracy of such model and summarize the results in Table 1. As can be seen from the table, Neural Architecture Search can design several promising architectures that perform as well as some of the best models on this dataset.\nFirst, if we ask the controller to not predict stride or pooling, it can design a 15-layer architecture that achieves 5.50% error rate on the test set. This architecture has a good balance between accuracy and depth. In fact, it is the shallowest and perhaps the most inexpensive architecture among the top performing networks in this table. This architecture is shown in Appendix A, Figure 7. A notable feature of this architecture is that it has many rectangular filters and it prefers larger filters at the top layers. Like residual networks (He et al., 2016a), the architecture also has many one-step skip connections. This architecture is a local optimum in the sense that if we perturb it, its performance becomes worse. For example, if we densely connect all layers with skip connections, its performance becomes slightly worse: 5.56%. If we remove all skip connections, its performance drops to 7.97%.\nIn the second set of experiments, we ask the controller to predict strides in addition to other hyperparameters. As stated earlier, this is more challenging because the search space is larger. In this case, it finds a 20-layer architecture that achieves 6.01% error rate on the test set, which is not much worse than the first set of experiments.\nFinally, if we allow the controller to include 2 pooling layers at layer 13 and layer 24 of the architectures, the controller can design a 39-layer network that achieves 4.47% which is very close to the best human-invented architecture that achieves 3.74%. To limit the search space complexity we have our model predict 13 layers where each layer prediction is a fully connected block of 3 layers. Additionally, we change the number of filters our model can predict from [24, 36, 48, 64] to [6, 12, 24, 36]. Our result can be improved to 3.84% by adding 32 more filters to each layer of our architecture. Additionally this model with 32 filters added is 1.2x as fast as the DenseNet model that achieves 3.74%, while having a minor degradation in performance."}, {"heading": "4.2 LEARNING RECURRENT CELLS FOR PENN TREEBANK", "text": "Dataset: We apply Neural Architecture Search to the Penn Treebank dataset, a well-known benchmark for language modeling. On this task, LSTM architectures tend to excel (Zaremba et al., 2014; Gal, 2015), and improving them is difficult (Jozefowicz et al., 2015). As PTB is a small dataset, regularization methods are needed to avoid overfitting. First, we make use of the embedding dropout and recurrent dropout techniques proposed in (Zaremba et al., 2014) and (Gal, 2015). We also try to combine them with the shared Input/Output embedding method by Press & Wolf (2016). Results with this method are marked with \u201dshared embeddings.\u201d\nSearch space: Following Section 3.4, our controller sequentially predicts a combination method then an activation function for each node in the tree. For each node in the tree, the controller RNN needs to select a combination method in [add, elem mult] and an activation method in [identity, tanh, sigmoid, relu]. The number of input pairs to the RNN cell is called the \u201dbase number\u201d and set to 8 in our experiments. When the base number is 8, the search space is has approximately 6 \u00d7 1016 architectures, which is much larger than 15,000, the number of architectures that we allow our controller to evaluate.\nTraining details: The controller and its training are almost identical to the CIFAR-10 experiments except for a few modifications: 1) the learning rate for the controller RNN is 0.0005, slightly smaller than that of the controller RNN in CIFAR-10, 2) in the distributed training, we set S to 20, K to 400 and m to 1, which means there are 400 networks being trained on 400 CPUs concurrently at any time, 3) during asynchronous training we only do parameter updates to the parameter-server once 10 gradients from replicas have been accumulated.\nIn our experiments, every child model is constructed and trained for 35 epochs. Every child model has two layers, with the number of hidden units adjusted so that total number of learnable parameters approximately match the \u201dmedium\u201d baselines (Zaremba et al., 2014; Gal, 2015). In these experiments we only have the controller predict the RNN cell structure and fix all other hyperparameters. The reward function is c(validation perplexity)2 where c is a constant, usually set at 80.\nAfter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity.\nResults: In Table 2, we provide a comprehensive list of architectures and their performance on the PTB dataset. As can be seen from the table, the models found by Neural Architecture Search outperform other state-of-the-art models on this dataset, and one of our best models achieves a gain of almost 3.6 perplexity. Not only is our cell is better, the model that achieves 64 perplexity is also more than two times faster because the previous best network requires running a cell 10 times per time step (Zilly et al., 2016).\nThe newly discovered cell is visualized in Figure 8 in Appendix A. The visualization reveals that the new cell has many similarities to the LSTM cell in the first few steps, such as it likes to compute W1 \u2217 ht\u22121 +W2 \u2217 xt several times and send them to different components in the cell.\nTransfer Learning Results: To understand whether the cell can generalize to a different task, we apply it to the character language modeling task on the same dataset. We use an experimental setup that is similar to (Ha et al., 2016), but use variational dropout by Gal (2015). We also run our own LSTM with our setup to get a fair LSTM baseline. The results on the test set of our method and state-of-art methods are reported in Table 3. The results confirm that the cell does indeed generalize, and is better than the LSTM cell.\nControl Experiment 1 \u2013 Adding more functions in the search space: To test the robustness of Neural Architecture Search, we add max to the list of combination functions and sin to the list of activation functions and rerun our experiments. The results show that even with a bigger search\nspace, the model can achieve somewhat comparable performance. The best architecture with max and sin is shown in Figure 8 in Appendix A.\nControl Experiment 2 \u2013 Comparison against Random Search: Instead of policy gradient, one can use random search to find the best network. Although this baseline seems simple, it is often very hard to surpass (Bergstra & Bengio, 2012). We report the perplexity improvements using policy gradient against random search as training progresses in Figure 6. The results show that not only the best model using policy gradient is better than the best model using random search, but also the average of top models is also much better."}, {"heading": "5 CONCLUSION", "text": "In this paper we introduce Neural Architecture Search, an idea of using a recurrent neural network to compose neural network architectures. By using recurrent network as the controller, our method is flexible so that it can search variable-length architecture space. Our method has strong empirical performance on very challenging benchmarks and presents a new research direction for automatically finding good neural network architectures. The code for running the models found by the controller on CIFAR-10 and PTB will be released at https://github.com/tensorflow/models ."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Greg Corrado, Jeff Dean, David Ha, Lukasz Kaiser and the Google Brain team for their help with the project."}, {"heading": "A APPENDIX", "text": ""}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In NAACL,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Learning to learn by gradient descent by gradient descent", "author": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W Hoffman", "David Pfau", "Tom Schaul", "Nando de Freitas"], "venue": "arXiv preprint arXiv:1606.04474,", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra and Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Algorithms for hyper-parameter optimization", "author": ["James Bergstra", "R\u00e9mi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl"], "venue": "In NIPS,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "The inference of regular LISP programs from examples", "author": ["Alan W. Biermann"], "venue": "IEEE transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Biermann.,? \\Q1978\\E", "shortCiteRegEx": "Biermann.", "year": 1978}, {"title": "Language modeling with sum-product networks", "author": ["Wei-Chen Cheng", "Stanley Kok", "Hoai Vu Pham", "Hai Leong Chieu", "Kian Ming Adam Chai"], "venue": "In INTERSPEECH,", "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "Histograms of oriented gradients for human detection", "author": ["Navneet Dalal", "Bill Triggs"], "venue": "In CVPR,", "citeRegEx": "Dalal and Triggs.,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs.", "year": 2005}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V. Le"], "venue": "In NIPS,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Neuroevolution: from architectures to learning", "author": ["Dario Floreano", "Peter D\u00fcrr", "Claudio Mattiussi"], "venue": "Evolutionary Intelligence,", "citeRegEx": "Floreano et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Floreano et al\\.", "year": 2008}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal"], "venue": "arXiv preprint arXiv:1512.05287,", "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E. Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Juergen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "Yann Lecun"], "venue": "In ICCV,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In ICML,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Brenden M. Lake", "Ruslan Salakhutdinov", "Joshua B. Tenenbaum"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning to optimize", "author": ["Ke Li", "Jitendra Malik"], "venue": "arXiv preprint arXiv:1606.01885,", "citeRegEx": "Li and Malik.,? \\Q2016\\E", "shortCiteRegEx": "Li and Malik.", "year": 2016}, {"title": "Learning programs: A hierarchical Bayesian approach", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein"], "venue": "In ICML,", "citeRegEx": "Liang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2010}, {"title": "Object recognition from local scale-invariant features", "author": ["David G. Lowe"], "venue": "In CVPR,", "citeRegEx": "Lowe.,? \\Q1999\\E", "shortCiteRegEx": "Lowe.", "year": 1999}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "In SLT,", "citeRegEx": "Mikolov and Zweig.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Zweig.", "year": 2012}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "In ICLR,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1312.6026,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Using the output embedding to improve language models", "author": ["Ofir Press", "Lior Wolf"], "venue": "arXiv preprint arXiv:1608.05859,", "citeRegEx": "Press and Wolf.,? \\Q2016\\E", "shortCiteRegEx": "Press and Wolf.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1511.06732,", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Neural programmer-interpreters", "author": ["Scott Reed", "Nando de Freitas"], "venue": "In ICLR,", "citeRegEx": "Reed and Freitas.,? \\Q2015\\E", "shortCiteRegEx": "Reed and Freitas.", "year": 2015}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams"], "venue": "In NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Mostofa Patwary", "Mostofa Ali", "Ryan P. Adams"], "venue": "In ICML,", "citeRegEx": "Snoek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2015}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "A hypercube-based encoding for evolving large-scale neural networks", "author": ["Kenneth O. Stanley", "David B. D\u2019Ambrosio", "Jason Gauci"], "venue": "Artificial Life,", "citeRegEx": "Stanley et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Stanley et al\\.", "year": 2009}, {"title": "A methodology for LISP program construction from examples", "author": ["Phillip D. Summers"], "venue": "Journal of the ACM,", "citeRegEx": "Summers.,? \\Q1977\\E", "shortCiteRegEx": "Summers.", "year": 1977}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Learning to learn", "author": ["Sebastian Thrun", "Lorien Pratt"], "venue": "Springer Science & Business Media,", "citeRegEx": "Thrun and Pratt.,? \\Q2012\\E", "shortCiteRegEx": "Thrun and Pratt.", "year": 2012}, {"title": "Modeling systems with internal state using evolino", "author": ["Daan Wierstra", "Faustino J Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In GECCO,", "citeRegEx": "Wierstra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wierstra et al\\.", "year": 2005}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "In Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "In BMVC,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent highway networks", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474,", "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "The last few years have seen much success of deep neural networks in many challenging applications, such as speech recognition (Hinton et al., 2012), image recognition (LeCun et al.", "startOffset": 127, "endOffset": 148}, {"referenceID": 25, "context": ", 2012), image recognition (LeCun et al., 1998; Krizhevsky et al., 2012) and machine translation (Sutskever et al.", "startOffset": 27, "endOffset": 72}, {"referenceID": 22, "context": ", 2012), image recognition (LeCun et al., 1998; Krizhevsky et al., 2012) and machine translation (Sutskever et al.", "startOffset": 27, "endOffset": 72}, {"referenceID": 45, "context": ", 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 32, "endOffset": 96}, {"referenceID": 2, "context": ", 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 32, "endOffset": 96}, {"referenceID": 50, "context": ", 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 32, "endOffset": 96}, {"referenceID": 28, "context": ", from SIFT (Lowe, 1999), and HOG (Dalal & Triggs, 2005), to AlexNet (Krizhevsky et al.", "startOffset": 12, "endOffset": 24}, {"referenceID": 22, "context": ", from SIFT (Lowe, 1999), and HOG (Dalal & Triggs, 2005), to AlexNet (Krizhevsky et al., 2012), VGGNet (Simonyan & Zisserman, 2014), GoogleNet (Szegedy et al.", "startOffset": 69, "endOffset": 94}, {"referenceID": 46, "context": ", 2012), VGGNet (Simonyan & Zisserman, 2014), GoogleNet (Szegedy et al., 2015), and ResNet (He et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 4, "context": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015).", "startOffset": 111, "endOffset": 185}, {"referenceID": 39, "context": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015).", "startOffset": 111, "endOffset": 185}, {"referenceID": 39, "context": "In practice, these methods often work better if they are supplied with a good initial model (Bergstra & Bengio, 2012; Snoek et al., 2012; 2015).", "startOffset": 92, "endOffset": 143}, {"referenceID": 43, "context": "Neural Architecture Search has some parallels to program synthesis and inductive programming, the idea of searching a program from examples (Summers, 1977; Biermann, 1978).", "startOffset": 140, "endOffset": 171}, {"referenceID": 5, "context": "Neural Architecture Search has some parallels to program synthesis and inductive programming, the idea of searching a program from examples (Summers, 1977; Biermann, 1978).", "startOffset": 140, "endOffset": 171}, {"referenceID": 27, "context": "In machine learning, probabilistic program induction has been used successfully in many settings, such as learning to solve simple Q&A (Liang et al., 2010; Neelakantan et al., 2015; Andreas et al., 2016), sort a list of numbers (Reed & de Freitas, 2015), and learning with very few examples (Lake et al.", "startOffset": 135, "endOffset": 203}, {"referenceID": 32, "context": "In machine learning, probabilistic program induction has been used successfully in many settings, such as learning to solve simple Q&A (Liang et al., 2010; Neelakantan et al., 2015; Andreas et al., 2016), sort a list of numbers (Reed & de Freitas, 2015), and learning with very few examples (Lake et al.", "startOffset": 135, "endOffset": 203}, {"referenceID": 0, "context": "In machine learning, probabilistic program induction has been used successfully in many settings, such as learning to solve simple Q&A (Liang et al., 2010; Neelakantan et al., 2015; Andreas et al., 2016), sort a list of numbers (Reed & de Freitas, 2015), and learning with very few examples (Lake et al.", "startOffset": 135, "endOffset": 203}, {"referenceID": 23, "context": ", 2016), sort a list of numbers (Reed & de Freitas, 2015), and learning with very few examples (Lake et al., 2015).", "startOffset": 95, "endOffset": 114}, {"referenceID": 45, "context": "This idea is borrowed from the decoder in end-to-end sequence to sequence learning (Sutskever et al., 2014).", "startOffset": 83, "endOffset": 107}, {"referenceID": 37, "context": "It is therefore similar to the work on BLEU optimization in Neural Machine Translation (Shen et al., 2016; Ranzato et al., 2015).", "startOffset": 87, "endOffset": 128}, {"referenceID": 35, "context": "It is therefore similar to the work on BLEU optimization in Neural Machine Translation (Shen et al., 2016; Ranzato et al., 2015).", "startOffset": 87, "endOffset": 128}, {"referenceID": 1, "context": "More closely related is the idea of using a neural network to learn the gradient descent updates for another network (Andrychowicz et al., 2016) and the idea of using reinforcement learning to find update policies for another network (Li & Malik, 2016).", "startOffset": 117, "endOffset": 144}, {"referenceID": 2, "context": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Despite their success, these methods are still limited in that they only search models from a fixed-length space. In other words, it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a network. In practice, these methods often work better if they are supplied with a good initial model (Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Modern neuro-evolution algorithms, e.g., Wierstra et al. (2005); Floreano et al.", "startOffset": 112, "endOffset": 658}, {"referenceID": 2, "context": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Despite their success, these methods are still limited in that they only search models from a fixed-length space. In other words, it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a network. In practice, these methods often work better if they are supplied with a good initial model (Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Modern neuro-evolution algorithms, e.g., Wierstra et al. (2005); Floreano et al. (2008); Stanley et al.", "startOffset": 112, "endOffset": 682}, {"referenceID": 2, "context": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Despite their success, these methods are still limited in that they only search models from a fixed-length space. In other words, it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a network. In practice, these methods often work better if they are supplied with a good initial model (Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Modern neuro-evolution algorithms, e.g., Wierstra et al. (2005); Floreano et al. (2008); Stanley et al. (2009), on the other hand, are much more flexible for composing novel models, yet they are usually less practical at a large scale.", "startOffset": 112, "endOffset": 705}, {"referenceID": 49, "context": "In this work, we use the REINFORCE rule from (Williams, 1992):", "startOffset": 45, "endOffset": 61}, {"referenceID": 8, "context": "As training a child network can take hours, we use distributed training and asynchronous parameter updates in order to speed up the learning process of the controller (Dean et al., 2012).", "startOffset": 167, "endOffset": 186}, {"referenceID": 46, "context": "1, the search space does not have skip connections, or branching layers used in modern architectures such as GoogleNet (Szegedy et al., 2015), and Residual Net (He et al.", "startOffset": 119, "endOffset": 141}, {"referenceID": 32, "context": "To enable the controller to predict such connections, we use a set-selection type attention (Neelakantan et al., 2015) which was built upon the attention mechanism (Bahdanau et al.", "startOffset": 92, "endOffset": 118}, {"referenceID": 2, "context": ", 2015) which was built upon the attention mechanism (Bahdanau et al., 2015; Vinyals et al., 2015).", "startOffset": 53, "endOffset": 98}, {"referenceID": 18, "context": "Additionally, it is also possible to predict pooling, local contrast normalization (Jarrett et al., 2009; Krizhevsky et al., 2012), and batchnorm (Ioffe & Szegedy, 2015) in the architectures.", "startOffset": 83, "endOffset": 130}, {"referenceID": 22, "context": "Additionally, it is also possible to predict pooling, local contrast normalization (Jarrett et al., 2009; Krizhevsky et al., 2012), and batchnorm (Ioffe & Szegedy, 2015) in the architectures.", "startOffset": 83, "endOffset": 130}, {"referenceID": 44, "context": "9 and used Nesterov Momentum (Sutskever et al., 2013).", "startOffset": 29, "endOffset": 53}, {"referenceID": 41, "context": "81 All-CNN (Springenberg et al., 2014) 7.", "startOffset": 11, "endOffset": 38}, {"referenceID": 40, "context": "72 Scalable Bayesian Optimization (Snoek et al., 2015) 6.", "startOffset": 34, "endOffset": 54}, {"referenceID": 24, "context": "37 FractalNet (Larsson et al., 2016) 21 38.", "startOffset": 14, "endOffset": 36}, {"referenceID": 11, "context": "60 ResNet (He et al., 2016a) 110 1.7M 6.61 ResNet (reported by Huang et al. (2016b)) 110 1.", "startOffset": 11, "endOffset": 84}, {"referenceID": 11, "context": "60 ResNet (He et al., 2016a) 110 1.7M 6.61 ResNet (reported by Huang et al. (2016b)) 110 1.7M 6.41 ResNet with Stochastic Depth (Huang et al., 2016b) 110 1.7M 5.23 1202 10.2M 4.91 Wide ResNet (Zagoruyko & Komodakis, 2016) 16 11.0M 4.81 28 36.5M 4.17 ResNet (pre-activation) (He et al., 2016b) 164 1.7M 5.46 1001 10.2M 4.62 DenseNet (L = 40, k = 12) Huang et al. (2016a) 40 1.", "startOffset": 11, "endOffset": 370}, {"referenceID": 11, "context": "60 ResNet (He et al., 2016a) 110 1.7M 6.61 ResNet (reported by Huang et al. (2016b)) 110 1.7M 6.41 ResNet with Stochastic Depth (Huang et al., 2016b) 110 1.7M 5.23 1202 10.2M 4.91 Wide ResNet (Zagoruyko & Komodakis, 2016) 16 11.0M 4.81 28 36.5M 4.17 ResNet (pre-activation) (He et al., 2016b) 164 1.7M 5.46 1001 10.2M 4.62 DenseNet (L = 40, k = 12) Huang et al. (2016a) 40 1.0M 5.24 DenseNet(L = 100, k = 12) Huang et al. (2016a) 100 7.", "startOffset": 11, "endOffset": 430}, {"referenceID": 11, "context": "60 ResNet (He et al., 2016a) 110 1.7M 6.61 ResNet (reported by Huang et al. (2016b)) 110 1.7M 6.41 ResNet with Stochastic Depth (Huang et al., 2016b) 110 1.7M 5.23 1202 10.2M 4.91 Wide ResNet (Zagoruyko & Komodakis, 2016) 16 11.0M 4.81 28 36.5M 4.17 ResNet (pre-activation) (He et al., 2016b) 164 1.7M 5.46 1001 10.2M 4.62 DenseNet (L = 40, k = 12) Huang et al. (2016a) 40 1.0M 5.24 DenseNet(L = 100, k = 12) Huang et al. (2016a) 100 7.0M 4.10 DenseNet (L = 100, k = 24) Huang et al. (2016a) 100 27.", "startOffset": 11, "endOffset": 492}, {"referenceID": 52, "context": "On this task, LSTM architectures tend to excel (Zaremba et al., 2014; Gal, 2015), and improving them is difficult (Jozefowicz et al.", "startOffset": 47, "endOffset": 80}, {"referenceID": 10, "context": "On this task, LSTM architectures tend to excel (Zaremba et al., 2014; Gal, 2015), and improving them is difficult (Jozefowicz et al.", "startOffset": 47, "endOffset": 80}, {"referenceID": 19, "context": ", 2014; Gal, 2015), and improving them is difficult (Jozefowicz et al., 2015).", "startOffset": 52, "endOffset": 77}, {"referenceID": 52, "context": "First, we make use of the embedding dropout and recurrent dropout techniques proposed in (Zaremba et al., 2014) and (Gal, 2015).", "startOffset": 89, "endOffset": 111}, {"referenceID": 10, "context": ", 2014) and (Gal, 2015).", "startOffset": 12, "endOffset": 23}, {"referenceID": 10, "context": ", 2014; Gal, 2015), and improving them is difficult (Jozefowicz et al., 2015). As PTB is a small dataset, regularization methods are needed to avoid overfitting. First, we make use of the embedding dropout and recurrent dropout techniques proposed in (Zaremba et al., 2014) and (Gal, 2015). We also try to combine them with the shared Input/Output embedding method by Press & Wolf (2016). Results with this method are marked with \u201dshared embeddings.", "startOffset": 8, "endOffset": 388}, {"referenceID": 52, "context": "Every child model has two layers, with the number of hidden units adjusted so that total number of learnable parameters approximately match the \u201dmedium\u201d baselines (Zaremba et al., 2014; Gal, 2015).", "startOffset": 163, "endOffset": 196}, {"referenceID": 10, "context": "Every child model has two layers, with the number of hidden units adjusted so that total number of learnable parameters approximately match the \u201dmedium\u201d baselines (Zaremba et al., 2014; Gal, 2015).", "startOffset": 163, "endOffset": 196}, {"referenceID": 53, "context": "Not only is our cell is better, the model that achieves 64 perplexity is also more than two times faster because the previous best network requires running a cell 10 times per time step (Zilly et al., 2016).", "startOffset": 186, "endOffset": 206}, {"referenceID": 29, "context": "0 Pascanu et al. (2013) - Deep RNN 6M 107.", "startOffset": 2, "endOffset": 24}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.", "startOffset": 2, "endOffset": 22}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.", "startOffset": 2, "endOffset": 69}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.", "startOffset": 2, "endOffset": 116}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.", "startOffset": 2, "endOffset": 151}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.", "startOffset": 2, "endOffset": 207}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.", "startOffset": 2, "endOffset": 267}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.", "startOffset": 2, "endOffset": 322}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.4 Kim et al. (2015) - CharCNN 19M 78.", "startOffset": 2, "endOffset": 388}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.4 Kim et al. (2015) - CharCNN 19M 78.9 Press & Wolf (2016) - Variational LSTM, shared embeddings 24M 73.", "startOffset": 2, "endOffset": 427}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.4 Kim et al. (2015) - CharCNN 19M 78.9 Press & Wolf (2016) - Variational LSTM, shared embeddings 24M 73.2 Merity et al. (2016) - Zoneout + Variational LSTM (medium) 20M 80.", "startOffset": 2, "endOffset": 495}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.4 Kim et al. (2015) - CharCNN 19M 78.9 Press & Wolf (2016) - Variational LSTM, shared embeddings 24M 73.2 Merity et al. (2016) - Zoneout + Variational LSTM (medium) 20M 80.6 Merity et al. (2016) - Pointer Sentinel-LSTM (medium) 21M 70.", "startOffset": 2, "endOffset": 563}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.4 Kim et al. (2015) - CharCNN 19M 78.9 Press & Wolf (2016) - Variational LSTM, shared embeddings 24M 73.2 Merity et al. (2016) - Zoneout + Variational LSTM (medium) 20M 80.6 Merity et al. (2016) - Pointer Sentinel-LSTM (medium) 21M 70.9 Zilly et al. (2016) - Variational RHN, shared embeddings 24M 66.", "startOffset": 2, "endOffset": 625}, {"referenceID": 29, "context": "Parameter numbers with \u2021 are estimates with reference to Merity et al. (2016).", "startOffset": 57, "endOffset": 78}, {"referenceID": 10, "context": ", 2016), but use variational dropout by Gal (2015). We also run our own LSTM with our setup to get a fair LSTM baseline.", "startOffset": 40, "endOffset": 51}], "year": 2016, "abstractText": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.84, which is only 0.1 percent worse and 1.2x faster than the current state-of-the-art model. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-ofthe-art.", "creator": "LaTeX with hyperref package"}}}