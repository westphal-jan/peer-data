{"id": "1611.01874", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Neural Machine Translation with Reconstruction", "abstract": "Although rest - without - to Neural Machine Translation (NMT) has achieved resilience progress present where behind same still, on afflicted 15 set major drawback: translations surge by NMT electrical often lack of requirement. It but there publicly occurring given NMT gradually meant denied readers some which referring both mistakenly dealing except call. To alleviate to situation, but agree time romantic ead - dispenser - reconstructor formulated for NMT. The reconstructor, incorporated apart the NMT comparison, managing will reconstruct the input source sentence went instead into layered between through output close request, to adequate that the data entered. is either one 1960s to well target being they much is possible. Experiments show that the proposed framework rise stamina into adequacy from NMT shortfall much achieves determined translation substantial made officials - of - the - paintings NMT for epidemiological MT systems.", "histories": [["v1", "Mon, 7 Nov 2016 02:03:55 GMT  (1421kb,D)", "http://arxiv.org/abs/1611.01874v1", null], ["v2", "Mon, 21 Nov 2016 09:47:22 GMT  (1321kb,D)", "http://arxiv.org/abs/1611.01874v2", "Accepted by AAAI 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhaopeng tu", "yang liu", "lifeng shang", "xiaohua liu", "hang li"], "accepted": true, "id": "1611.01874"}, "pdf": {"name": "1611.01874.pdf", "metadata": {"source": "META", "title": "Neural Machine Translation with Reconstruction", "authors": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li"], "emails": ["tu.zhaopeng@huawei.com", "shang.lifeng@huawei.com", "liuxiaohua3@huawei.com", "hangli.hl@huawei.com", "liuyang2011@tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "Past several years have observed a significant progress in Neural Machine Translation (NMT) (Kalchbrenner and Blunsom 2013; Cho et al. 2014; Sutskever et al. 2014; Bahdanau et al. 2015). Particularly, NMT has significantly enhanced the performance of translation between a language pair involving rich morphology prediction and/or significant word reordering (Luong and Manning 2015; Bentivogli et al. 2016). Long Short-Term Memory (Hochreiter and Schmidhuber 1997) enables NMT to conduct longdistance reordering, which is a significant challenge for Statistical Machine Translation (SMT) (Brown et al. 1993; Koehn et al. 2003).\nUnlike SMT which employs a number of components, NMT adopts an end-to-end encoder-decoder framework to model the entire translation process. The role of encoder is to summarize the source sentence into a sequence of latent vectors, and the decoder acts as a language model to generate a target sentence word by word by selectively leveraging the information from the latent vectors at each step. In learning, NMT essentially estimates the likelihood of a target sentence given a source sentence.\nHowever, conventional NMT faces two main problems: 1 Translations generated by NMT systems often lack of ad-\nequacy. When generating target words, the decoder often\nCopyright \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nrepeatedly selects some parts of the source sentence while ignoring other parts, which leads to over-translation and under-translation (Tu et al. 2016b). This is mainly due to that NMT does not have a mechanism to ensure that the information in the source side is completely transformed to the target side.\n2 Likelihood objective is suboptimal in decoding. NMT utilizes a beam search to find a translation that maximizes the likelihood. However, we observe that likelihood favors short translations, and thus fails to distinguish good translation candidates from bad ones in a large decoding space (e.g., beam size = 100). The main reason is that likelihood only captures unidirectional dependency from source to target, which does not correlate well with translation adequacy (Li and Jurafsky 2016; Shen et al. 2016).\nWhile previous work partially solves the above problems, in this work we propose a novel encoder-decoderreconstructor model for NMT, aiming at alleviating these problems in a unified framework. As shown in Figure 1, given a Chinese sentence \u201cduoge jichang beipo guanbi .\u201d, the standard encoder-decoder translates it into an English sentence and assigns a likelihood score. Then, the newly added reconstructor reconstructs the translation back to the source\nar X\niv :1\n61 1.\n01 87\n4v 1\n[ cs\n.C L\n] 7\nN ov\n2 01\n6\nsentence and calculates the corresponding reconstruction score. Linear interpolation of the two scores produces an overall score of the translation.\nAs seen, the added reconstructor imposes a constraint that an NMT model should be able to reconstruct the input source sentence from the target-side hidden layers, which encourages decoder to embed complete information of the source side. The reconstruction score serves as an auxiliary objective to measure the adequacy of translation. The combined objective consisting of likelihood and reconstruction, which measures both fluency and adequacy of translations, is used in both training and testing.\nExperimental results show that the proposed approach consistently improves the translation performance when increasing the decoding space. Our model achieves a significant improvement of 2.3 BLEU points over a strong attention-based NMT system, and of 4.5 BLEU points over a state-of-the-art SMT system, trained on the same data."}, {"heading": "Background", "text": ""}, {"heading": "Encoder-Decoder based NMT", "text": "Given a source sentence x = x1, . . . xj , . . . xJ and a target sentence y = y1, . . . yi, . . . yI , end-to-end NMT directly models the translation probability word by word:\nP (y|x) = I\u220f i=1 P (yi|y<i,x; \u03b8) (1)\nwhere \u03b8 is the model parameters and y<i = y1, . . . , yi\u22121 is partial translation. Prediction of the i-th target word is generally made in an encoder-decoder framework:\nP (yi|y<i,x; \u03b8) \u221d exp { f(yi\u22121, si, ci; \u03b8) } (2)\nwhere si is the i-th hidden target state computed by the decoder Recurrent Neural Network (RNN), ci is the i-th source representation for generating the i-th target word, and f(\u00b7) is an activation function in the decoder. Current NMT models differ in their ways of calculating ci from the hidden states from the encoder. Please refer to (Sutskever et al. 2014; Bahdanau et al. 2015) for more details. The parameters of NMT model are trained to maximize the likelihood of a set of training examples {[xn,yn]}Nn=1:\nL(\u03b8) = argmax \u03b8 N\u2211 n=1 logP (yn|xn; \u03b8) (3)\nWhen generating each target word, the decoder adaptively selects partial information (i.e., ci) from the encoder. This actually adopts a greedy way to select the most useful information for each generated word. There is, however, no mechanism to guarantee that the decoder conveys complete information from the source sentence to the target sentence.\nIn addition, we find that the performance of NMT decreases as the decoding space increases, as shown in Table 1. This is because likelihood favors short but inadequate translation candidates, which are newly added together with good candidates in larger decoding spaces. Normalizing likelihood by translation length faces the same problem."}, {"heading": "Beam Likelihood + Normalization", "text": "It is important to introduce an auxiliary objective to measure the adequacy of translation, which complements likelihood."}, {"heading": "Reconstruction in Auto-Encoder", "text": "Reconstruction is a standard concept in auto-encoder, which is usually realized by a feed forward network (Bourlard and Kamp 1988; Vincent et al. 2010; Socher et al. 2011). The model consists of an encoding function to compute a representation from an input, and a decoding function to reconstruct the input from the representation. The parameters involved in the two functions are trained to maximize the reconstruction score, which measures the similarity between the original input and reconstructed input.\nReconstruction examines whether the reconstructed input is faithful to the original input, which is essentially similar to the consideration of adequacy in translation. It is natural to integrate reconstruction into NMT to enhance adequacy of translation. The basic idea of our approach is to reconstruct the source sentence from the latent representations of decoder, and use the reconstruction score as the adequacy measure. Analogous to auto-encoder, our approach also learns a latent representation of source sentence on the target side. Our approach can be viewed as a supervised auto-encoder in the sense that the latent representation is not only used to reconstruct the source sentence, but also used to generate the target sentence."}, {"heading": "Approach", "text": ""}, {"heading": "Architecture", "text": "We prepose a novel encoder-decoder-reconstructor framework. More specifically, we base our approach on top of attention-based NMT (Bahdanau et al. 2015; Luong et al. 2015), which will be used as baseline in the experiments later. We note that the proposed approach is generally applicable to any other type of NMT architectures, such as the sequence-to-sequence model (Sutskever et al. 2014). The model architecture, shown in Figure 2, consists of two components: \u2022 Standard encoder-decoder reads the input sentence and\noutputs its translation along with the likelihood score, as shown in the background section.\n\u2022 Added reconstructor reads the hidden state sequence from the decoder and outputs a score of exactly reconstructing the input sentence, which we will describe below.\nReconstructor As shown in Figure 2, the reconstructor reconstructs the input. Here we use the hidden layer at the target side as the representation of the translation, since it plays a key role in generation of the translation. We aim at encouraging it to embed complete source information, and in the meantime to reduce the complexity of model and make the training easy.\nSpecifically, the reconstructor reconstructs the source sentence word by word, which is conditioned on the inverse context vector c\u0302j for each input word xj . The inverse context vector c\u0302j is computed as a weighted sum of hidden layers s at the target-side:\nc\u0302j = I\u2211 i=1 \u03b1\u0302j,i \u00b7 si (4)\nThe weight \u03b1\u0302j,i of each hidden layer sj is computed by an added inverse attention model, which has its own parameters independent from the original attention model. The reconstruction probability is calculated by\nR(x|s) = J\u220f j=1 R(xj |x<j , s)\n= J\u220f j=1 gr(xj\u22121, h\u0302j , c\u0302j) (5)\nwhere h\u0302j is the hidden state in the reconstructor, and computed by\nh\u0302j = fr(xj\u22121, h\u0302j\u22121, c\u0302j) (6)\nHere gr(\u00b7) and fr(\u00b7) are softmax function and activation function for the reconstructor, respectively. The source words x share the same word embeddings with the encoder."}, {"heading": "Training", "text": "Formally, we train both the encoder-decoder P (y|x; \u03b8) and the reconstructor R(x|s; \u03b3) on a set of training examples {[xn,yn]}Nn=1, where s is the state sequence in the decoder after generating y, and \u03b8 and \u03b3 are model parameters in the encoder-decoder and reconstructor respectively. The new training objective is:\nJ(\u03b8, \u03b3) = argmax \u03b8,\u03b3 N\u2211 n=1 { logP (yn|xn; \u03b8)\ufe38 \ufe37\ufe37 \ufe38\nlikelihood\n+\u03bb logR(xn|sn; \u03b3)\ufe38 \ufe37\ufe37 \ufe38 reconstruction\n} (7)\nwhere \u03bb is a hyper-parameter that balances the preference between likelihood and reconstruction.\nNote that the objective consists of two parts: likelihood measures translation fluency, and reconstruction measures translation adequacy. It is clear that the combined objective is more consistent with the goal of enhancing overall translation quality, and can more effectively guide the parameter training for making better translation."}, {"heading": "Testing", "text": "Once a model is trained, we use a beam search to find a translation that approximately maximizes both the likelihood and reconstruction score. As shown in Figure 3, given an input sentence, a two-phase scheme is used:\n1 The standard encoder-decoder produces a set of translation candidates, each of which is a triple consisting of a translation candidate, its corresponding hidden layer at the target-side s, and its likelihood score P .\n2 For each translation candidate, the reconstructor reads its corresponding hidden layer at the target-side and outputs an auxiliary reconstruction score R. Linear interpolation\nof likelihood P and reconstruction score R produces an overall score, which is used to select the final translation.1\nIn testing, reconstruction works as a reranking technique to select a better translation from the k-best candidates generated by the decoder.\nExperiments"}, {"heading": "Setup", "text": "We carry out experiments on Chinese-English translation. The training dataset consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively. We choose the NIST 2002 (MT02) dataset as validation set, and the NIST 2005 (MT05), 2006 (MT06) and 2008 (MT08) datasets as test sets. We use the case-insensitive 4-gram NIST BLEU score (Papineni et al. 2002) as evaluation metric, and signtest (Collins et al. 2005) for statistical significance test.\nWe compare our method with state-of-the-art SMT and NMT models: \u2022 MOSES (Koehn et al. 2007): an open source phrase-\nbased translation system with default configuration and a 4-gram language model trained on the target portion of training data.\n\u2022 RNNSEARCH: our re-implemented attention-based NMT system, which incorporates dropout (Hinton et al. 2012) on the output layer and improves the attention model by feeding the lastly generated word. For training RNNSEARCH, we limit the source and target vocabularies to the most frequent 30K words in Chinese and English. We train each model with the sentences of length up to 80 words in the training data. We shuffle mini-batches as we proceed and the mini-batch size is 80. The word embedding dimension is 620 and the hidden layer dimension is 1000. We train for 15 epochs using Adadelta (Zeiler 2012).\nFor our model, we set the hyper-parameter \u03bb = 1. The parameters of our model (i.e., encoder and decoder, except those related to reconstructor) are initialized by the RNNSEARCH model trained on a parallel corpus. We further train our model for 10 epochs."}, {"heading": "Correlation between Reconstruction and Adequacy", "text": "In the first experiment, we investigate the validity of our assumption that reconstruction score correlates well with translation adequacy, which is the underlying assumption of the approach. We conduct a subjective evaluation: two human evaluators are asked to evaluate the translations of 200 source sentences randomly sampled from the test sets. We\n1Interpolation weight \u03bb in testing is the same as in training.\ncalculate Pearson Correlation between the reconstruction scores and the corresponding adequacy and fluency scores on the samples, as shown in Table 2. Two evaluators produce similar results: reconstruction score is more related to translation adequacy than fluency."}, {"heading": "Effect of Reconstruction on Translation", "text": "In this experiment, we investigate the effect of reconstruction on translation performance over time, which is measured in BLEU scores on the validation set. For reconstruction, we use the reconstructor to stochastically generate a source sentence for each translation,2 and calculate the BLEU score of the reconstructed input under the reference of the original input. Generally, as shown in Figure 4, the BLEU score of translation goes up with the improvement of reconstruction over time. The translation performance reaches a peak at iteration 110K, when the model achieves a balance between likelihood and reconstruction score. Therefore, we use the trained model at iteration 110K in the following experiments."}, {"heading": "Effect of Reconstruction in Large Decoding Space", "text": ""}, {"heading": "Beam Likelihood +Reconstruction", "text": "2Note that it is different from the standard procedure, which calculates the probability of exactly reconstructing the original input.\nCan our approach cope with the limitation of likelihood in large decoding spaces? To answer this question, we investigate the effect of reconstruction on different beam sizes k, as shown in Table 3. Our approach can indeed solve the problem: increasing the size of decoding space generally leads to improving the BLEU score. We attribute this to the ability of the combined objective to measure both fluency and adequacy of translation candidates. There is a significant gap between k = 10 and k = 100. However, keeping increasing k does not result in significant improvements of translation accuracy but greatly decreases decoding efficiency. Therefore, in the following experiments we set the max value of k to 100, and use normalized likelihood for k = 100 if we don\u2019t use reconstruction in testing."}, {"heading": "Main Results", "text": "Table 4 shows the translation performances on test sets measured in BLEU score. RNNSEARCH significantly outperforms Moses by 2.2 BLEU points on average, indicating that it is a strong baseline NMT system. This is mainly due to the introduction of two advanced techniques. Increasing beam size leads to decreasing translation performances on test sets, which is consistent with the result on the validation set. We compare our methods with \u201cRNNSEARCH (Beam=10)\u201d in the following analysis, since it yields the best performance in the baseline systems.\nFirst, the introduction of reconstruction significantly improves the performance over baseline by 1.1 BLEU points with beam size k = 10. Most importantly, we obtain a further improvement of 1.2 BLEU points when expanding the decoding space. Second, our approach also consistently improves the quality (in terms of Oracle score, see the last column) of k-best translation candidates over the baseline system on various beam sizes. This confirms our claim that the combined objective contributes to parameter training for generating better translation candidates."}, {"heading": "Analysis", "text": "We conduct extensive analyses to better understand our model in terms of contribution of reconstruction from training and testing, alleviating typical translation problems, and building the ability of handling long sentences.\nContribution Analysis The contribution of reconstruction is of two-fold: (1) enabling parameter training for generating better translation candidates, and (2) enabling better reranking of generated candidates in testing. Table 5 lists\nthe improvements from the two contribution sources. When applied only in training, reconstruction improves translation performance by generating fluent and adequate translation candidates. On top of that, reconstruction-based reranking further improves the performance. The improvements are more significant when decoding spaces increase."}, {"heading": "Model Under-Tran. Over-Tran.", "text": "Problem Analysis We then conduct a subjective evaluation to investigate the benefit of incorporating reconstruction on the randomly selected 200 sentences. Table 6 shows the results of subjective evaluation on translation. RNNSEARCH suffers from serious under-translation and over-translation problems, which is consistent with the finding in other work (Tu et al. 2016b). Incorporating reconstruction significantly alleviates these problems, and reduces 11.0% and 38.5% of under-translation and over-translation errors respectively. The main reason is that both undertranslation and over-translation lead to lower reconstruction scores, and thus are penalized by the reconstruction objective. As a result, the corresponding candidate is less likely to be selected as the final translation.\nLength Analysis Following Bahdanau et al. (2015), we group sentences of similar lengths together and compute the BLEU score for each group, as shown in Figure 5. Clearly the proposed approach outperforms all the other systems in all length segments. Specifically, RNNSEARCH outperforms Moses on all sentence segments, while its performance degrades faster than its competitors, which is consistent with the finding in (Bentivogli et al. 2016). This is\nmainly due to that RNNSEARCH seriously suffers from inadequate translations on long sentences (Tu et al. 2016b). Our model explicitly encourages the decoder to incorporate source information as much as possible, and thus the improvements are more significant on long sentences."}, {"heading": "Comparison with Previous Work", "text": "We re-implement the methods of Tu et al. (2016b; 2016a) on top of RNNSEARCH. For the coverage mechanism (Tu et al. 2016b), we use the neural network based coverage, and the coverage dimension is 100. For the context gates (Tu et al. 2016a), we apply them on both source and target sides. Table 7 lists the comparison results. Coverage mechanism and context gates significantly improve translation performance individually, and combining them achieves a further improvement. This is consistent with the results in (Tu et al. 2016b; 2016a). Our model consistently improves the translation performance when further combined with the models."}, {"heading": "Related Work", "text": "Our work is inspired by research on improving NMT by:\nEnhancing Translation Adequacy Recently, several work shows that NMT favors fluent but inadequate translations (Tu et al. 2016b; 2016a). While all the work is towards enhancing adequacy of NMT, our approach is complimentary: the above work is still under the standard encoderdecoder framework, while we propose a novel encoderdecoder-reconstructor framework. Experiments show that combining those models together can further improve the translation performance.\nImproving Beam Search Standard NMT models exploit a simple beam search algorithm to generate the translation word by word. Several researchers rescore word candidates with additional features, such as language model probability (Gulcehre et al. 2015) and SMT features (He et al. 2016; Stahlberg et al. 2016). In contrast, Li and Jurafsky (2016) rescore translation candidates on sentence-level with the mutual information between source and target sides. In the above work, NMT is treated as a black-box and its weighted outputs are combined with other features only in testing. In this work, we move forward further by incorporating reconstruction score into the objective of training, which leads to creation of better translation candidates.\nCapturing Bidirectional Dependency Standard NMT models only capture the unidirectional dependency from source to target with the likelihood objective. It has been shown that combination of two directional models outperforms each model alone (Liang et al. 2006; Cheng et al. 2016a; Cheng et al. 2016b). Among them, Cheng et al. (2016b) reconstruct the monolingual corpora with two separate source-to-target and target-to-source NMT models. Closely related to Cheng et al. (2016b), our approach aims at enhancing adequacy of unidirectional (i.e., source-to-target) NMT via an auxiliary target-to-source objective on parallel corpora, while theirs focuses on learning bidirectional NMT models via auto-encoders on monolingual corpora."}, {"heading": "Conclusion", "text": "We propose a novel encoder-decoder-reconstructor framework for NMT, in which the newly added reconstructor introduces an auxiliary score to measure the adequacy of translation candidates. The advantage of the proposed approach is of two-fold. First, it improves parameter training for producing better translation candidates. Second, it consistently improves translation performance when the decoding space increases, while conventional NMT fails to do so. Experimental results show that the two advantages can indeed help our approach to consistently improve translation performance.\nThere is still a significant gap between de facto translation and oracle of k-best translation candidates, especially when the decoding space increases. We plan to narrow the gap with rich features, which can better measure the quality of translation candidates. It is also necessary to validate the effectiveness of our approach on more language pairs and other NMT architectures."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study", "author": ["L. Bentivogli", "A. Bisazza", "M. Cettolo", "M. Federico"], "venue": "EMNLP 2016.", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological Cybernetics 59(4-5):291\u2013294.", "citeRegEx": "Bourlard and Kamp,? 1988", "shortCiteRegEx": "Bourlard and Kamp", "year": 1988}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.E. Brown", "S.A.D. Pietra", "V.J.D. Pietra", "R.L. Mercer"], "venue": "Computational Linguistics 19(2):263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Y. Cheng", "S. Shen", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "IJCAI 2016.", "citeRegEx": "Cheng et al\\.,? 2016a", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Semi-Supervised Learning for Neural Machine Translation", "author": ["Y. Cheng", "W. Xu", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "ACL 2016.", "citeRegEx": "Cheng et al\\.,? 2016b", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1"], "venue": "ACL 2005.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "On Using Monolingual Corpora in Neural Machine Translation", "author": ["C. Gulcehre", "O. Firat", "K. Xu", "K. Cho", "L. Barrault", "H.-C. Lin", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv.", "citeRegEx": "Gulcehre et al\\.,? 2015", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Improved neural machine translation with smt features", "author": ["W. He", "Z. He", "H. Wu", "H. Wang"], "venue": "AAAI 2016.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP 2013.", "citeRegEx": "Kalchbrenner and Blunsom,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Statistical phrasebased translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "NAACL 2003.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "ACL 2007.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Mutual information and diverse decoding improve neural machine translation", "author": ["J. Li", "D. Jurafsky"], "venue": "NAACL 2016.", "citeRegEx": "Li and Jurafsky,? 2016", "shortCiteRegEx": "Li and Jurafsky", "year": 2016}, {"title": "Alignment by agreement", "author": ["P. Liang", "B. Taskar", "D. Klein"], "venue": "NAACL 2006. Luong, M.-T., and Manning, C. D. 2015. Stanford neural machine translation systems for spoken language domains. In IWSLT 2015.", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.-T. Luong", "H. Pham", "C.D. Manning"], "venue": "EMNLP 2015.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "ACL 2002.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Minimum Risk Training for Neural Machine Translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "ACL 2016.", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "EMNLP 2011.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Syntactically Guided Neural Machine Translation", "author": ["F. Stahlberg", "E. Hasler", "A. Waite", "B. Byrne"], "venue": "arxiv.org.", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS 2014.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Context Gates for Neural Machine Translation", "author": ["Z. Tu", "Y. Liu", "Z. Lu", "X. Liu", "H. Li"], "venue": "arXiv.", "citeRegEx": "Tu et al\\.,? 2016a", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Modeling Coverage for Neural Machine Translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": "ACL 2016.", "citeRegEx": "Tu et al\\.,? 2016b", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "Journal of Machine Learning Research 11(Dec):3371\u20133408.", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv.", "citeRegEx": "Zeiler,? 2012", "shortCiteRegEx": "Zeiler", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "Past several years have observed a significant progress in Neural Machine Translation (NMT) (Kalchbrenner and Blunsom 2013; Cho et al. 2014; Sutskever et al. 2014; Bahdanau et al. 2015).", "startOffset": 92, "endOffset": 185}, {"referenceID": 6, "context": "Past several years have observed a significant progress in Neural Machine Translation (NMT) (Kalchbrenner and Blunsom 2013; Cho et al. 2014; Sutskever et al. 2014; Bahdanau et al. 2015).", "startOffset": 92, "endOffset": 185}, {"referenceID": 22, "context": "Past several years have observed a significant progress in Neural Machine Translation (NMT) (Kalchbrenner and Blunsom 2013; Cho et al. 2014; Sutskever et al. 2014; Bahdanau et al. 2015).", "startOffset": 92, "endOffset": 185}, {"referenceID": 0, "context": "Past several years have observed a significant progress in Neural Machine Translation (NMT) (Kalchbrenner and Blunsom 2013; Cho et al. 2014; Sutskever et al. 2014; Bahdanau et al. 2015).", "startOffset": 92, "endOffset": 185}, {"referenceID": 1, "context": "Particularly, NMT has significantly enhanced the performance of translation between a language pair involving rich morphology prediction and/or significant word reordering (Luong and Manning 2015; Bentivogli et al. 2016).", "startOffset": 172, "endOffset": 220}, {"referenceID": 11, "context": "Long Short-Term Memory (Hochreiter and Schmidhuber 1997) enables NMT to conduct longdistance reordering, which is a significant challenge for Statistical Machine Translation (SMT) (Brown et al.", "startOffset": 23, "endOffset": 56}, {"referenceID": 3, "context": "Long Short-Term Memory (Hochreiter and Schmidhuber 1997) enables NMT to conduct longdistance reordering, which is a significant challenge for Statistical Machine Translation (SMT) (Brown et al. 1993; Koehn et al. 2003).", "startOffset": 180, "endOffset": 218}, {"referenceID": 13, "context": "Long Short-Term Memory (Hochreiter and Schmidhuber 1997) enables NMT to conduct longdistance reordering, which is a significant challenge for Statistical Machine Translation (SMT) (Brown et al. 1993; Koehn et al. 2003).", "startOffset": 180, "endOffset": 218}, {"referenceID": 24, "context": "repeatedly selects some parts of the source sentence while ignoring other parts, which leads to over-translation and under-translation (Tu et al. 2016b).", "startOffset": 135, "endOffset": 152}, {"referenceID": 15, "context": "The main reason is that likelihood only captures unidirectional dependency from source to target, which does not correlate well with translation adequacy (Li and Jurafsky 2016; Shen et al. 2016).", "startOffset": 154, "endOffset": 194}, {"referenceID": 19, "context": "The main reason is that likelihood only captures unidirectional dependency from source to target, which does not correlate well with translation adequacy (Li and Jurafsky 2016; Shen et al. 2016).", "startOffset": 154, "endOffset": 194}, {"referenceID": 22, "context": "Please refer to (Sutskever et al. 2014; Bahdanau et al. 2015) for more details.", "startOffset": 16, "endOffset": 61}, {"referenceID": 0, "context": "Please refer to (Sutskever et al. 2014; Bahdanau et al. 2015) for more details.", "startOffset": 16, "endOffset": 61}, {"referenceID": 2, "context": "Reconstruction in Auto-Encoder Reconstruction is a standard concept in auto-encoder, which is usually realized by a feed forward network (Bourlard and Kamp 1988; Vincent et al. 2010; Socher et al. 2011).", "startOffset": 137, "endOffset": 202}, {"referenceID": 25, "context": "Reconstruction in Auto-Encoder Reconstruction is a standard concept in auto-encoder, which is usually realized by a feed forward network (Bourlard and Kamp 1988; Vincent et al. 2010; Socher et al. 2011).", "startOffset": 137, "endOffset": 202}, {"referenceID": 20, "context": "Reconstruction in Auto-Encoder Reconstruction is a standard concept in auto-encoder, which is usually realized by a feed forward network (Bourlard and Kamp 1988; Vincent et al. 2010; Socher et al. 2011).", "startOffset": 137, "endOffset": 202}, {"referenceID": 0, "context": "More specifically, we base our approach on top of attention-based NMT (Bahdanau et al. 2015; Luong et al. 2015), which will be used as baseline in the experiments later.", "startOffset": 70, "endOffset": 111}, {"referenceID": 17, "context": "More specifically, we base our approach on top of attention-based NMT (Bahdanau et al. 2015; Luong et al. 2015), which will be used as baseline in the experiments later.", "startOffset": 70, "endOffset": 111}, {"referenceID": 22, "context": "We note that the proposed approach is generally applicable to any other type of NMT architectures, such as the sequence-to-sequence model (Sutskever et al. 2014).", "startOffset": 138, "endOffset": 161}, {"referenceID": 18, "context": "We use the case-insensitive 4-gram NIST BLEU score (Papineni et al. 2002) as evaluation metric, and signtest (Collins et al.", "startOffset": 51, "endOffset": 73}, {"referenceID": 7, "context": "2002) as evaluation metric, and signtest (Collins et al. 2005) for statistical significance test.", "startOffset": 41, "endOffset": 62}, {"referenceID": 14, "context": "\u2022 MOSES (Koehn et al. 2007): an open source phrasebased translation system with default configuration and a 4-gram language model trained on the target portion of training data.", "startOffset": 8, "endOffset": 27}, {"referenceID": 10, "context": "\u2022 RNNSEARCH: our re-implemented attention-based NMT system, which incorporates dropout (Hinton et al. 2012) on the output layer and improves the attention model by feeding the lastly generated word.", "startOffset": 87, "endOffset": 107}, {"referenceID": 26, "context": "We train for 15 epochs using Adadelta (Zeiler 2012).", "startOffset": 38, "endOffset": 51}, {"referenceID": 24, "context": "RNNSEARCH suffers from serious under-translation and over-translation problems, which is consistent with the finding in other work (Tu et al. 2016b).", "startOffset": 131, "endOffset": 148}, {"referenceID": 1, "context": "Specifically, RNNSEARCH outperforms Moses on all sentence segments, while its performance degrades faster than its competitors, which is consistent with the finding in (Bentivogli et al. 2016).", "startOffset": 168, "endOffset": 192}, {"referenceID": 0, "context": "Length Analysis Following Bahdanau et al. (2015), we group sentences of similar lengths together and compute the BLEU score for each group, as shown in Figure 5.", "startOffset": 26, "endOffset": 49}, {"referenceID": 24, "context": "mainly due to that RNNSEARCH seriously suffers from inadequate translations on long sentences (Tu et al. 2016b).", "startOffset": 94, "endOffset": 111}, {"referenceID": 24, "context": "\u201d denotes coverage mechanism to keep track of the attention history (Tu et al. 2016b), and \u201cCtx.", "startOffset": 68, "endOffset": 85}, {"referenceID": 23, "context": "\u201d denotes context gate to dynamically control the ratios at which source and target contexts contribute to the generation of target words (Tu et al. 2016a).", "startOffset": 138, "endOffset": 155}, {"referenceID": 24, "context": "For the coverage mechanism (Tu et al. 2016b), we use the neural network based coverage, and the coverage dimension is 100.", "startOffset": 27, "endOffset": 44}, {"referenceID": 23, "context": "For the context gates (Tu et al. 2016a), we apply them on both source and target sides.", "startOffset": 22, "endOffset": 39}, {"referenceID": 24, "context": "This is consistent with the results in (Tu et al. 2016b; 2016a).", "startOffset": 39, "endOffset": 63}, {"referenceID": 24, "context": "Enhancing Translation Adequacy Recently, several work shows that NMT favors fluent but inadequate translations (Tu et al. 2016b; 2016a).", "startOffset": 111, "endOffset": 135}, {"referenceID": 8, "context": "Several researchers rescore word candidates with additional features, such as language model probability (Gulcehre et al. 2015) and SMT features (He et al.", "startOffset": 105, "endOffset": 127}, {"referenceID": 9, "context": "2015) and SMT features (He et al. 2016; Stahlberg et al. 2016).", "startOffset": 23, "endOffset": 62}, {"referenceID": 21, "context": "2015) and SMT features (He et al. 2016; Stahlberg et al. 2016).", "startOffset": 23, "endOffset": 62}, {"referenceID": 8, "context": "Several researchers rescore word candidates with additional features, such as language model probability (Gulcehre et al. 2015) and SMT features (He et al. 2016; Stahlberg et al. 2016). In contrast, Li and Jurafsky (2016) rescore translation candidates on sentence-level with the mutual information between source and target sides.", "startOffset": 106, "endOffset": 222}, {"referenceID": 16, "context": "It has been shown that combination of two directional models outperforms each model alone (Liang et al. 2006; Cheng et al. 2016a; Cheng et al. 2016b).", "startOffset": 90, "endOffset": 149}, {"referenceID": 4, "context": "It has been shown that combination of two directional models outperforms each model alone (Liang et al. 2006; Cheng et al. 2016a; Cheng et al. 2016b).", "startOffset": 90, "endOffset": 149}, {"referenceID": 5, "context": "It has been shown that combination of two directional models outperforms each model alone (Liang et al. 2006; Cheng et al. 2016a; Cheng et al. 2016b).", "startOffset": 90, "endOffset": 149}, {"referenceID": 4, "context": "2006; Cheng et al. 2016a; Cheng et al. 2016b). Among them, Cheng et al. (2016b) reconstruct the monolingual corpora with two separate source-to-target and target-to-source NMT models.", "startOffset": 6, "endOffset": 80}, {"referenceID": 4, "context": "2006; Cheng et al. 2016a; Cheng et al. 2016b). Among them, Cheng et al. (2016b) reconstruct the monolingual corpora with two separate source-to-target and target-to-source NMT models. Closely related to Cheng et al. (2016b), our approach aims at enhancing adequacy of unidirectional (i.", "startOffset": 6, "endOffset": 224}], "year": 2016, "abstractText": "Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-theart NMT and statistical MT systems.", "creator": "TeX"}}}