{"id": "1611.01967", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Regularizing CNNs with Locally Constrained Decorrelations", "abstract": "Regularization an steps to stretching learning since it intended training generally complex smaller bringing keeping increase levels major rs-485. However, to most prevalent regularizations me does leverage having the capacity given the marketed september without help without eliminate with conditions number especially constraint. Feature decorrelation longer means introduction provided using similar made capacity the main designed hard came satiation growth eroded are even narrow both the overhead time takes. In this paper, we. although wideranging negatively fluctuating multiple although an balancing year phase decorrelation and same OrthoReg, a edited brasi specialized well locally enforces feature emoluments. As this meant, steps geographical fundamental though feature decorrelation clamp disregarding between behave outcomes screen, allowing the regularizer take only higher decorrelation farthest, various ensuring the overfitting same effectively. In subject, certainly show no the systems hard-coded making OrthoReg have higher certainty bounds why when batch negotations same schoolers being. Moreover, october opportunity goal-setting is been soloist on entire weights, it is especially suitable taking keeping convolutional neural connected, where to weight iss is stress compared to the best path setting. As a result, go are able next require the blackspots the washington - of - the - well-known CNNs without CIFAR - 10, CIFAR - 12, now SVHN.", "histories": [["v1", "Mon, 7 Nov 2016 10:15:40 GMT  (794kb,D)", "http://arxiv.org/abs/1611.01967v1", "Submitted to ICLR2017 conference"], ["v2", "Wed, 15 Mar 2017 08:18:28 GMT  (1024kb,D)", "http://arxiv.org/abs/1611.01967v2", "Accepted at ICLR2017"]], "COMMENTS": "Submitted to ICLR2017 conference", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["pau rodr\\'iguez", "jordi gonz\\`alez", "guillem cucurull", "josep m gonfaus", "xavier roca"], "accepted": true, "id": "1611.01967"}, "pdf": {"name": "1611.01967.pdf", "metadata": {"source": "CRF", "title": "REGULARIZING CNNS WITH LOCALLY CONSTRAINED DECORRELATIONS", "authors": ["Pau Rodr\u0131\u0301guez", "Jordi Gonz\u00e0lez", "Guillem Cucurull", "Josep M. Gonfaus", "Xavier Roca"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)). Deeper models, like Googlenet (Szegedy et al. (2015)) and Deep Residual Networks (Szegedy et al. (2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures.\nThere are two clearly defined regularization strategies in the literature. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al. (2013)) so as to prevent feature co-adaptation. Due to their nature, although this set of strategies have proved to be very effective, they do not leverage all the capacity of the models they regularize.\nThe second group of regularizations is those which improve the effectiveness and generality of the trained model without reducing their capacity. In this second group, the most relevant approaches decorrelate the weights or feature maps, e.g. Bengio & Bergstra (2009) introduced a new criterion so as to learn slow decorrelated features while pre-training models. In the same line Bao et al. (2013) presented \u201dincoherent training\u201d, a regularizer for reducing the decorrelation of the network activations or feature maps in the context of speech recognition. Although regularizations in the second group are promising and have already been used to reduce the overfitting in different tasks, even with the presence of Dropout (as shown by Cogswell et al. (2016)), they are seldom used in the large scale image recognition domain because of the small improvement margins they provide together with the computational overhead they introduce.\nar X\niv :1\n61 1.\n01 96\n7v 1\n[ cs\n.L G\n] 7\nN ov\n2 01\n6\nAlthough they are not directly presented as regularizers, there are other strategies to reduce the overfitting such as Batch Normalization (Ioffe & Szegedy (2015)), which decreases the overfitting by reducing their internal covariance shift. In the same line initialization strategies such as \u201dXavier\u201d (Glorot & Bengio (2010)) or \u201dHe\u201d (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al. (2013)) while normalizing the variance of the outputs as well.\nIn this work we hypothesize that regularizing negatively correlated features is an obstacle for achieving better results and we introduce OrhoReg, a novel regularization technique that addresses the performance margin issue by only regularizing positively correlated features. Moreover, OrthoReg is computationally efficient since it only regularizes the feature weights, which makes it very suitable for the latest CNN models. We verify our hypothesis through a series of experiments: first using MNIST as a proof of concept, secondly we regularize wide residual networks on Cifar10, Cifar100, and SVHN (Netzer et al. (2011)) achieving the best error rates in the dataset to the best of our knowledge."}, {"heading": "2 DEALING WITH WEIGHT REDUNDANCIES", "text": "Deep Neural Networks (DNN) are very expressive models which can usually have millions of parameters. However, with limited data, they tend to overfit. There is an abundant number of techniques in order to deal with this problem, from L1 and L2 regularizations (Nowlan & Hinton (1992)), early-stopping, Dropout or DropConnect. Models presenting high levels of overfitting usually have a lot of redundancy in their features, capturing similar patterns with slight differences which usually correspond to noise in the training data. A particular case where this is evident is in AlexNet (Krizhevsky et al. (2012)), which presents very similar convolution filters and even \u201ddead\u201d ones, as it was remarked by Zeiler & Fergus (2014).\nIn fact, given a set of parameters \u03b8I,j connecting a set of inputs I = {i1, i2, . . . , in} to a neuron hj , two neurons {hj , hk}, j 6= k will be positively correlated and thus fire always together if \u03b8I,j = \u03b8I,k and negatively correlated if \u03b8I,j = \u2212\u03b8I,k. In other words, two neurons with the same or slightly different weights will produce very similar outputs. In order to reduce the redundancy present in the network parameters, one should maximize the amount of information encoded by each neuron. From an information theory point of view, this means one should not be able to predict the output of a neuron given the output of the rest of the neurons of the layer. However, this measure requires batch statistics and huge joint probability tables and it would have a high computational cost.\nIn this paper, we will focus on the weights correlation rather than activation independence since it still is a problem in many neural network models and it can be addressed without introducing too much overhead, showing that models generalize better when different feature detectors are enforced to be dissimilar. See Table 1 in order to see how regularizing the weights can be orders of magnitude faster than acting on the feature maps for deep CNNs.\nIn order to find a good target to optimize so as to reduce the correlation between weights, it is first required to find a metric to measure it. In this paper, we propose to use the cosine similarity between feature detectors to express how strong is their relationship. Note that the cosine similarity\nis equivalent to the Pearson correlation for mean-centered normalized vectors, but we will use the term correlation for the sake of clarity."}, {"heading": "2.1 ORTHOGONAL WEIGHT REGULARIZATION", "text": "This section introduces the orthogonal weight regularization, a novel regularization technique that aims to reduce feature detector correlation enforcing local orthogonality between all pairs of weight vectors. In order to keep the magnitudes of the detectors unaffected, we have chosen the cosine similarity between the vector pairs in order to solely focus on the vectors angle \u03b2 \u2208 [\u2212\u03c0, \u03c0]. Then, given any pair of feature vectors of the same size \u03b81, \u03b82 the cosine of their relative angle is:\ncos(\u03b81, \u03b82) = \u3008\u03b81, \u03b82\u3009 ||\u03b81||||\u03b82||\n(1)\nWhere \u3008\u03b81, \u03b82\u3009 denotes the inner product between \u03b81 and \u03b82. We then square the cosine similarity in order to define a regularization cost function for steepest descent that has its local minima when vectors are orthogonal:\nC(\u03b8) = 1\n2 n\u2211 i=1 n\u2211 j=1,j 6=i cos2(\u03b8i, \u03b8j) = 1 2 n\u2211 i=1 n\u2211 j=1,j 6=i ( \u3008\u03b8i, \u03b8j\u3009 ||\u03b8i||||\u03b8j || )2 (2)\nWhere \u03b8i are the weights connecting the output of the layer l \u2212 1 to the neuron i of the layer l, which has n hidden units. Interestingly, minimizing this cost function relates to the minimization of the Frobenius norm of the cross-covariance matrix without the diagonal. This cost will be added to the global cost of the model J(\u03b8;X, y), where X are the inputs and y are the labels or targets, obtaining J\u0303(\u03b8;X, y) = J(\u03b8;X, y) + \u03b3C(\u03b8). Note that \u03b3 is an hyperparameter that weights the relative contribution of the regularization term. We can now define the gradient with respect to the parameters:\n\u03b4\n\u03b4\u03b8(i,j) C(\u03b8) = n\u2211 k=1,k 6=i \u03b8(k,j)\u3008\u03b8i, \u03b8k\u3009 \u3008\u03b8i, \u03b8i\u3009\u3008\u03b8k, \u03b8k\u3009 \u2212 \u03b8(i,j)\u3008\u03b8i, \u03b8k\u30092 \u3008\u03b8i, \u03b8i\u30092\u3008\u03b8k, \u03b8k\u3009 (3)\nThe second term is introduced by the magnitude normalization. As magnitudes are not relevant for the vector angle problem, this equation can be simplified just by assuming normalized feature detectors:\n\u03b4\n\u03b4\u03b8(i,j) C(\u03b8) = n\u2211 k=1,k 6=i \u03b8(k,j)\u3008\u03b8i, \u03b8k\u3009 (4)\nWe then add eq. 4 to the backpropagation gradient:\n\u2206\u03b8(i,j) = \u2212\u03b1 ( \u2207J\u03b8(i,j) + \u03b3 n\u2211 k=1,k 6=i \u03b8(k,j)\u3008\u03b8i, \u03b8k\u3009 )\n(5)\nWhere \u03b1 is the global learning rate coefficient, J any target loss function for the backpropagation algorithm.\nAlthough this update can be done sequentially for each feature-detector pair, it can be vectorized to speedup computations. Let \u0398 be a matrix where each row is a feature detector \u03b8(I,j) corresponding to the normalized weights connecting the whole input I of the layer to the neuron j. Then, \u0398\u0398t contains the inner product of each pair of vectors i and j in each position i, j. Subsequently, we subtract the diagonal so as to ignore the angle from each feature with respect to itself and multiply by \u0398 to compute the final value corresponding to the sum in eq. 5:\nAlgorithm 1 Orthogonal Regularization Step.\nRequire: Layer parameter matrices \u0398l, regularization coefficient \u03b3, global learning rate \u03b1. 1: for each layer l to regularize do 2: \u03b71 = norm rows(\u0398l) . Keep norm of the rows of \u0398l. 3: \u0398l1 = div rows(\u0398\nl, \u03b71) . Keep a \u0398l1 with normalized rows. 4: innerProdMat = \u0398l1transpose(\u0398 l 1) 5: \u2207\u0398l1 = \u03b3(innerProdMat\u2212 diag(innerProdMat))\u0398l1 . Second term in eq. 6 6: \u2206\u0398l = \u2212\u03b1(\u2207J\u0398l + \u03b3\u2207\u0398l1) . Complete eq. 6. 7: end for\n(a) Global loss plot (eq. 2) (b) Local loss plot (eq. 7)\n(c) Direction of gradients for the loss in (a). (d) Direction of gradients for loss in (b).\nWhere the second term is\u2207C\u0398. Algorithm 1 summarizes the steps in order to apply OrthoReg."}, {"heading": "2.2 NEGATIVE CORRELATIONS", "text": "Note that the presented algorithm, based on the cosine similarity, penalizes any kind of correlation between all pairs of feature detectors, i.e. the positive and the negative correlations, see Figure 1a. However, negative correlations are related to inhibitory connections, competitive learning, and self-organization. In fact, there is evidence that negative correlations can help a neural population to increase the signal-to-noise ratio (Chelaru & Dragoi (2016)) in the V1. In order to find out the advantages of keeping negative correlations, we propose to use an exponential so as to squash the gradients for angles greater than \u03c02 (orthogonal):\nC(\u03b8) = n\u2211 i=1 n\u2211 j=1,j 6=i log(1 + e\u03bb(cos(\u03b8i,\u03b8j)\u22121)) = log(1 + e\u03bb(\u3008\u03b8i,\u03b8j\u3009\u22121)), ||\u03b8i|| = ||\u03b8j || = 1 (7)\nWhere \u03bb is a coefficient that controls the minimum angle-of-influence of the regularizer, i.e. the minimum angle between two features so that there exists a gradient pushing them apart, see Figure 1b. We empirically found that the regularizer worked well for \u03bb = 10, see Figure 2b. Note that when \u03bb ' 10 the loss and the gradients approximate to zero when vectors are at more than \u03c02 (orthogonal). As a result of incorporating the squashing function on the cosine similarity, negatively correlated features will not be regularized. This is different from all previous approaches and the loss presented in eq. 2, where all pairs of vectors influence each other. Thus, from now on, the loss in eq. 2 is named as global loss and the loss in eq. 7 is named as local loss.\nThe derivative of eq. 7 is:\n\u03b4\n\u03b4\u03b8(i,j) C(\u03b8) = n\u2211 k=1,k 6=i \u03bb e\u03bb\u3008\u03b8i,\u03b8k\u3009\u03b8(k,j) e\u03bb\u3008\u03b8i,\u03b8k\u3009 + e\u03bb (8)\nThen, given the element-wise exponential operator exp, we define the following expression in order to simplify the formulas:\n\u0398\u0302 = exp(\u03bb(\u0398\u0398t)) (9)\nand thus, the \u2206 in vectorial form can be formulated as:\n\u2207C\u0398 = \u03bb (\u0398\u0302\u2212 diag(\u0398\u0302))\u0398\n\u0398\u0302\u2212 diag(\u0398\u0302) + e\u03bb (10)\nIn order to provide a visual example, we have created a 2D toy dataset and used the previous equations for positive and negative \u03b3 values, see Figure 2. As expected, it can be seen that the angle\nbetween all pairs of adjacent features becomes more uniform after regularization. Note that Figure 2b shows that regularization with the global loss (eq. 2) results in less uniform angles than using the local loss as shown in 2c (which corresponds to the local loss presented in eq. 7) because vectors in opposite quadrants still influence each other. This is why in Figure 2d, it can be seen that the mean nearest neighbor angle using the global loss (b) is more unstable than the local loss (c). As a proof of concept, we also performed gradient ascent, which minimizes the angle between the vectors. Thus, in Figures 2e and 2f, it can be seen that the locality introduced by the local loss reaches a stable configuration where features with angle \u03c02 are too far to attract each other.\nThe effects of global and local regularizations on Alexnet, VGG-16 and a 50-layer ResNet are shown on Figure 3. As it can be seen, OrthoReg reaches higher decorrelation bounds. Lower decorrelation peaks are still observed when the input dimensionality of the layers is smaller than the output since all vectors cannot be orthogonal at the same time. In this case, local regularization largely outperforms global regularization since it removes interferences caused by negatively correlated features. This suggests why increasing fully connected layers\u2019 size has not improved networks performance."}, {"heading": "3 EXPERIMENTS", "text": "In this section we provide a set of experiments that verify that (i) training with the proposed regularization increases the performance of naive unregularized models, (ii) negatively correlated features are useful, and (iii) the proposed regularization improves the performance of state-of-the-art models."}, {"heading": "3.1 VERIFICATION EXPERIMENTS", "text": "As a sanity check, we first train a three-hidden-layer Multi-Layer Perceptron (MLP) with ReLU non-liniarities on the MNIST dataset (LeCun et al. (1998)). Our code is based in the train-a-digit-classifier example included in torch/demos1, which uses an upsampled version of the dataset (32\u00d732). The only pre-processing applied to the data is a global standardization. The model is trained with SGD and a batch size of 200 during 200 epochs. No momentum neither weight decay was applied. By default, the magnitude of the weights of this experiments is recovered after each regularization step in order to prove the regularization only affects their angle.\nSensitivity to hyperparameters. We train a three-hidden-layer MLP with 1024 hidden units, and different \u03b3 and \u03bb values so as to verify how they affect the performance of the model. Figure 4a shows that the model effectively achieves the best error rate for the highest gamma value (\u03b3 = 1), thus proving the advantages of the regularization. On Figure 4b, we verify that higher regularization rates produce more general models. Figure 5a depicts the sensitivity of the model to \u03bb. As expected, the best value is found when lambda corresponds to Orthogonality (\u03bb ' 10).\n1https://github.com/torch/demos\nNegative Correlations. Figure 5b highlights the difference between regularizing with the global or the local regularizer. Although both regularizations reach better error rates than the unregularized counterpart, the local regularization is better than the global. This confirms the hypothesis that negative correlations are useful and thus, performance decreases when we reduce them.\nCompatibility with initialization and dropout. To demonstrate the proposed regularization can help even when other regularizations are present, we trained a CNN with (i) dropout (c32-c64-l512-d0.5-l10)2 or (ii) LSUV initialization (Mishkin & Matas (2016)). In Table 2, we show that best results are obtained when orthogonal regularization is present. The results are consistent with the hypothesis that OrthoReg, as well as Dropout and LSUV, focuses on reducing the model redundancy. Thus, when one of them is present, the margin of improvement for the others is reduced.\n2cxx = convolution with xx filters. lxx = fully-connected with xx units. dxx = dropout with prob xx."}, {"heading": "3.2 REGULARIZATION ON CIFAR-10 AND CIFAR-100", "text": "We show that the proposed OrthoReg can help to improve the performance of state-of-the-art models such as deep residual networks (He et al. (2015a)). In order to show the regularization is suitable for deep CNNs, we successfuly regularize a 110-layer ResNet3 on CIFAR-10, decreasing its error from 6.55% to 6.29% without data augmentation.\nIn order to compare with the most recent state-of-the-art, we train a wide residual network (Zagoruyko & Komodakis (May 2016)) on Cifar-10 and Cifar-100. The experiment is based on a torch implementation of the 28-layer and 10th width factor wide deep residual model, for which the average error rate on CIFAR-10 is 4.50% and 20.04% on CIFAR-100 after 200 epochs, see figure 6.4\nThe regularization coefficient \u03b3 was chosen using grid search although similar values were found for all the experiments, specially if regularization gradients are normalized before adding them to the weights. The regularization was equally applied to all the convolution layers of the (wide) ResNet. We found that, although the regularized models were already using weight decay, dropout, and batch normalization, best error rates were always achieved with OrthoReg.\nTable 3 compares the performance of the regularized models with other state-of-the-art results. As it can be seen the regularized model surpasses the state of the art."}, {"heading": "3.3 REGULARIZATION ON SVHN", "text": "For SVHN we follow the procedure depicted in Zagoruyko & Komodakis (May 2016), training a wide residual network of depth=28, width=4, and dropout. Results are shown in Table 4. As it can be seen, we reduce the error rate from 1.64% to 1.54%, which is the lower value reported on this dataset to the best of our knowledge.\n3https://github.com/gcr/torch-residual-networks 4https://github.com/szagoruyko/wide-residual-networks"}, {"heading": "4 DISCUSSION", "text": "Regularization by feature decorrelation can reduce Neural Networks overfitting even in the presence of another kind of regularizations. However, especially when the number of feature detectors is higher than the input dimensionality, its decorrelation capacity is limited due to the effects of negatively correlated features. We showed that imposing locality constraints in feature decorrelation removes interferences between negatively correlated features, allowing regularizers to reach higher decorrelation bounds, and reducing the overfitting more effectively. In particular, we show that the models regularized with the constrained regularization present lower overfitting even when batch normalization and dropout present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of 110-layer ResNets and wide ResNets on CIFAR-10, CIFAR-100, and SVHN improving their performance."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work has been supported by the Spanish project TIN2015-65464-R (MINECO/FEDER), the 2016FI B 01163, and the COST Action IC1307 iV&L Net (European Network on Integrating Vision and Language), supported by COST (European Cooperation in Science and Technology)."}], "references": [{"title": "Incoherent training of deep neural networks to de-correlate bottleneck features for speech recognition", "author": ["Yebo Bao", "Hui Jiang", "Lirong Dai", "Cong Liu"], "venue": "IEEE ICASSP,", "citeRegEx": "Bao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bao et al\\.", "year": 2013}, {"title": "Slow, decorrelated features for pretraining complex cell-like networks", "author": ["Yoshua Bengio", "James S Bergstra"], "venue": "In NIPS, pp", "citeRegEx": "Bengio and Bergstra.,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Bergstra.", "year": 2009}, {"title": "Negative correlations in visual cortical networks", "author": ["Mircea I Chelaru", "Valentin Dragoi"], "venue": "Cerebral Cortex,", "citeRegEx": "Chelaru and Dragoi.,? \\Q2016\\E", "shortCiteRegEx": "Chelaru and Dragoi.", "year": 2016}, {"title": "Fast and accurate deep network learning by exponential linear units (ELUs)", "author": ["Djork-Arn Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": null, "citeRegEx": "Clevert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2016}, {"title": "Reducing overfitting in deep networks by decorrelating representations", "author": ["Michael Cogswell", "Faruk Ahmed", "Ross Girshick", "Larry Zitnick", "Dhruv Batra"], "venue": null, "citeRegEx": "Cogswell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cogswell et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Fractional max-pooling", "author": ["Benjamin Graham"], "venue": "arXiv preprint arXiv:1412.6071,", "citeRegEx": "Graham.,? \\Q2014\\E", "shortCiteRegEx": "Graham.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML, pp", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "All you need is a good init", "author": ["Dmytro Mishkin", "Jiri Matas"], "venue": null, "citeRegEx": "Mishkin and Matas.,? \\Q2016\\E", "shortCiteRegEx": "Mishkin and Matas.", "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": null, "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Simplifying neural networks by soft weight-sharing", "author": ["Steven J. Nowlan", "Geoffrey E. Hinton"], "venue": "Neural computation,", "citeRegEx": "Nowlan and Hinton.,? \\Q1992\\E", "shortCiteRegEx": "Nowlan and Hinton.", "year": 1992}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "venue": null, "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Jost T. Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "In ICLR (workshop track),", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Training very deep networks", "author": ["Rupesh K. Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In NIPS, pp", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR, pp", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew D Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus"], "venue": "In ECCV, pp", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)).", "startOffset": 141, "endOffset": 166}, {"referenceID": 7, "context": "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)). Deeper models, like Googlenet (Szegedy et al. (2015)) and Deep Residual Networks (Szegedy et al.", "startOffset": 141, "endOffset": 221}, {"referenceID": 7, "context": "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)). Deeper models, like Googlenet (Szegedy et al. (2015)) and Deep Residual Networks (Szegedy et al. (2015); He et al.", "startOffset": 141, "endOffset": 272}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)).", "startOffset": 8, "endOffset": 190}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. There are two clearly defined regularization strategies in the literature. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al.", "startOffset": 8, "endOffset": 653}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. There are two clearly defined regularization strategies in the literature. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al.", "startOffset": 8, "endOffset": 733}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. There are two clearly defined regularization strategies in the literature. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al. (2013)) so as to prevent feature co-adaptation.", "startOffset": 8, "endOffset": 790}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. There are two clearly defined regularization strategies in the literature. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al. (2013)) so as to prevent feature co-adaptation. Due to their nature, although this set of strategies have proved to be very effective, they do not leverage all the capacity of the models they regularize. The second group of regularizations is those which improve the effectiveness and generality of the trained model without reducing their capacity. In this second group, the most relevant approaches decorrelate the weights or feature maps, e.g. Bengio & Bergstra (2009) introduced a new criterion so as to learn slow decorrelated features while pre-training models.", "startOffset": 8, "endOffset": 1255}, {"referenceID": 0, "context": "In the same line Bao et al. (2013) presented \u201dincoherent training\u201d, a regularizer for reducing the decorrelation of the network activations or feature maps in the context of speech recognition.", "startOffset": 17, "endOffset": 35}, {"referenceID": 0, "context": "In the same line Bao et al. (2013) presented \u201dincoherent training\u201d, a regularizer for reducing the decorrelation of the network activations or feature maps in the context of speech recognition. Although regularizations in the second group are promising and have already been used to reduce the overfitting in different tasks, even with the presence of Dropout (as shown by Cogswell et al. (2016)), they are seldom used in the large scale image recognition domain because of the small improvement margins they provide together with the computational overhead they introduce.", "startOffset": 17, "endOffset": 396}, {"referenceID": 4, "context": "Count of the Flops for the models used in this paper: the 3hidden-layer MLP and the 110-layer ResNet we use later in the experiments section when not regularized, using DeCov (Cogswell et al. (2016)) and using OrthoReg.", "startOffset": 176, "endOffset": 199}, {"referenceID": 7, "context": "In the same line initialization strategies such as \u201dXavier\u201d (Glorot & Bengio (2010)) or \u201dHe\u201d (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks.", "startOffset": 94, "endOffset": 112}, {"referenceID": 7, "context": "In the same line initialization strategies such as \u201dXavier\u201d (Glorot & Bengio (2010)) or \u201dHe\u201d (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al.", "startOffset": 94, "endOffset": 474}, {"referenceID": 7, "context": "In the same line initialization strategies such as \u201dXavier\u201d (Glorot & Bengio (2010)) or \u201dHe\u201d (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al. (2013)) while normalizing the variance of the outputs as well.", "startOffset": 94, "endOffset": 588}, {"referenceID": 7, "context": "In the same line initialization strategies such as \u201dXavier\u201d (Glorot & Bengio (2010)) or \u201dHe\u201d (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al. (2013)) while normalizing the variance of the outputs as well. In this work we hypothesize that regularizing negatively correlated features is an obstacle for achieving better results and we introduce OrhoReg, a novel regularization technique that addresses the performance margin issue by only regularizing positively correlated features. Moreover, OrthoReg is computationally efficient since it only regularizes the feature weights, which makes it very suitable for the latest CNN models. We verify our hypothesis through a series of experiments: first using MNIST as a proof of concept, secondly we regularize wide residual networks on Cifar10, Cifar100, and SVHN (Netzer et al. (2011)) achieving the best error rates in the dataset to the best of our knowledge.", "startOffset": 94, "endOffset": 1270}, {"referenceID": 11, "context": "A particular case where this is evident is in AlexNet (Krizhevsky et al. (2012)), which presents very similar convolution filters and even \u201ddead\u201d ones, as it was remarked by Zeiler & Fergus (2014).", "startOffset": 55, "endOffset": 80}, {"referenceID": 11, "context": "A particular case where this is evident is in AlexNet (Krizhevsky et al. (2012)), which presents very similar convolution filters and even \u201ddead\u201d ones, as it was remarked by Zeiler & Fergus (2014). In fact, given a set of parameters \u03b8I,j connecting a set of inputs I = {i1, i2, .", "startOffset": 55, "endOffset": 197}, {"referenceID": 12, "context": "1 VERIFICATION EXPERIMENTS As a sanity check, we first train a three-hidden-layer Multi-Layer Perceptron (MLP) with ReLU non-liniarities on the MNIST dataset (LeCun et al. (1998)).", "startOffset": 159, "endOffset": 179}, {"referenceID": 7, "context": "We show that the proposed OrthoReg can help to improve the performance of state-of-the-art models such as deep residual networks (He et al. (2015a)).", "startOffset": 130, "endOffset": 148}, {"referenceID": 14, "context": "57 YES Highway Network (Srivastava et al. (2015)) 7.", "startOffset": 24, "endOffset": 49}, {"referenceID": 14, "context": "24 YES All-CNN (Springenberg et al. (2015)) 7.", "startOffset": 16, "endOffset": 43}, {"referenceID": 5, "context": "71 NO 110-Layer ResNet (He et al. (2015a)) 6.", "startOffset": 24, "endOffset": 42}, {"referenceID": 3, "context": "4 NO ELU-Network (Clevert et al. (2016)) 6.", "startOffset": 18, "endOffset": 40}, {"referenceID": 3, "context": "4 NO ELU-Network (Clevert et al. (2016)) 6.55 24.28 NO OrthoReg on 110-Layer ResNet* 6.29\u00b1 0.19 28.33\u00b1 0.5 NO LSUV (Mishkin & Matas (2016)) 5.", "startOffset": 18, "endOffset": 139}, {"referenceID": 3, "context": "4 NO ELU-Network (Clevert et al. (2016)) 6.55 24.28 NO OrthoReg on 110-Layer ResNet* 6.29\u00b1 0.19 28.33\u00b1 0.5 NO LSUV (Mishkin & Matas (2016)) 5.84 YES Fract. Max-Pooling (Graham (2014)) 4.", "startOffset": 18, "endOffset": 183}, {"referenceID": 9, "context": "92 Stochastic Depth ResNet (Huang et al. (2016)) 1.", "startOffset": 28, "endOffset": 48}], "year": 2017, "abstractText": "Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated features, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively. In particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN.", "creator": "LaTeX with hyperref package"}}}