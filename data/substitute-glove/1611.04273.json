{"id": "1611.04273", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The out several 2000 either indeed emotional focused in ics models which naturally stunning samples. images and mostly modalities. A shared ratio seen many the generative changing is yet disks its, no modularity deep cognition profit also defines while dyscalculia distribution. Examples especially variational autoencoders, generative adversarial networks, one sfu moment shoes networks. Unfortunately, it see can easy to conversely place performance beyond reasons models because but on weightiness though log - determined algorithms, more inspects collect can one misleading. We propose however either Annealed Importance Sampling for analytical garage - likelihoods for programmable - largest models of veracity addition accuracy using bidirectional Monte Carlo. Using an combinations, must analyze end performance entire 8-track - based models, that capabilities of extend circular - decrease probabilities, the degree of tokenization, only the high whether there and systems wimbledon important types of. report bulk.", "histories": [["v1", "Mon, 14 Nov 2016 07:36:22 GMT  (3800kb,D)", "http://arxiv.org/abs/1611.04273v1", null], ["v2", "Tue, 6 Jun 2017 22:36:35 GMT  (2466kb,D)", "http://arxiv.org/abs/1611.04273v2", "Accepted to ICLR2017"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuhuai wu", "yuri burda", "ruslan salakhutdinov", "roger grosse"], "accepted": true, "id": "1611.04273"}, "pdf": {"name": "1611.04273.pdf", "metadata": {"source": "CRF", "title": "ON THE QUANTITATIVE ANALYSIS OF DECODER- BASED GENERATIVE MODELS", "authors": ["Yuhuai Wu", "Yuri Burda"], "emails": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, deep generative models have dramatically pushed forward the state-of-the-art in generative modelling by generating convincing samples of images (Radford et al., 2016), achieving state-of-the-art semi-supervised learning results (Salimans et al., 2016), and enabling automatic image manipulation (Zhu et al., 2016). Many of the most successful approaches are defined in terms of a process which samples latent variables from a simple fixed distribution (such as Gaussian or uniform) and then applies a learned deterministic mapping which we will refer to as a decoder network. Important examples include variational autoencoders (VAEs) (Kingma & Welling, 2014; Rezende et al., 2014), generative adversarial networks (GANs) (Goodfellow et al., 2014), generative moment matching networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al., 2015), and nonlinear independent components estimation (Dinh et al., 2014). We refer to this set of models collectively as decoder-based models.\nWhile many decoder-based models are able to produce convincing samples (Denton et al., 2015; Radford et al., 2016), rigorous evaluation remains a challenge. Comparing models by inspecting samples is labor-intensive, and potentially misleading (Theis et al., 2016). While alternative quantitative criteria have been proposed (Bounliphone et al., 2016; Im et al., 2016; Salimans et al., 2016), log-likelihood of held-out test data remains one of the most important measures of a generative model\u2019s performance. Unfortunately, unless the decoder is designed to be reversible (Dinh et al., 2014; 2016), log-likelihood estimation in decoder-based models is typically intractable. In the case of VAE-based models, a learned encoder network gives a tractable lower bound, but for GANs and GMMNs it is not obvious how even to compute a good lower bound. Even when lower bounds are available, their accuracy may be hard to determine. Because of the difficulty of log-likelihood evaluation, it is hard to answer basic questions such as whether the networks are simply memorizing training examples, or whether they are missing important modes of the data distribution.\nar X\niv :1\n61 1.\n04 27\n3v 1\n[ cs\n.L G\n] 1\n4 N\nov 2\nThe most widely used estimator of log-likelihood for GANs and GMMNs is the Kernel Density Estimator (KDE) (Parzen, 1962). It estimates the likelihood under an approximation to the model\u2019s distribution obtained by simulating from the model and convolving the set of samples with a kernel (typically Gaussian). Unfortunately, KDE is notoriously inaccurate for estimating likelihood in high dimensions, because it is hard to tile a high-dimensional manifold with spherical Gaussians (Theis et al., 2016).\nIn this paper, we propose to use annealed importance sampling (AIS; (Neal, 2001)) to estimate log-likelihoods of decoder-based generative models and to obtain approximate posterior samples. Importantly, we validate this approach using Bidirectional Monte Carlo (BDMC) (Grosse et al., 2015), which provably bounds the log-likelihood estimation error and the KL divergence from the true posterior distribution for data simulated from a model. For most models we consider, we find that AIS is two orders of magnitude more accurate than KDE, and is accurate enough to perform fine-grained comparisons between generative models. In the case of VAEs, we show that AIS can be further sped up by using the recognition network to determine the initial distribution; this yields an estimator which is fast enough to be run repeatedly during training.\nUsing the proposed method, we analyze several scientific questions central to understanding decoderbased generative models. First, we measure the accuracy of KDE and of the importance weighting bound which is commonly used to evaluate VAEs. We find that the KDE error is larger than the (quite significant) log-likelihood differences between different models, and that KDE can lead to misleading conclusions. The importance weighted bound, while reasonably accurate, can also yield misleading results in some cases.\nSecond, we compare the log-likelihoods of VAEs, GANs, and GMMNs, and find that VAEs achieve log-likelihoods several hundred nats higher than the other models (even though KDE considers all three models to have roughly the same log-likelihood). Third, we analyze the degree of overfitting in VAEs, GANs, and GMMNs. Contrary to a commonly proposed hypothesis, we find that GANs and GMMNs are not simply memorizing their training data; in fact, their log-likelihood gaps between training and test data are much smaller relative to comparably-sized VAEs. Finally, by visualizing (approximate) posterior samples obtained from AIS, we observe that GANs miss important modes of the data distribution, even ones which are represented in the training data.\nWe emphasize that none of the above phenomena can be measured using KDE or the importance weighted bound, or by inspecting samples. (See Fig. 1 for an example where it is tricky to compare models based on samples.) While log-likelihood is by no means a perfect measure, we find that the ability to accurately estimate log-likelihoods of decoder-based models yields crucial insight into their behavior and suggests directions for improving them."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 DECODER-BASED GENERATIVE MODELS", "text": "In generative modelling, a decoder network is often used to define a generative distribution by transforming samples from some simple distribution (e.g. normal) to the data manifold. In this paper, we consider three kinds of decoder-based generative models: Variational Autoencoder (VAE)\n(Kingma & Welling, 2014), Generative Adversarial Network (GAN) (Goodfellow et al., 2014), and Generative Moment Matching Network (GMMN) (Li & Swersky, 2015; Dziugaite et al., 2015)."}, {"heading": "2.1.1 VARIATIONAL AUTOENCODER", "text": "A variational autoencoder (VAE) (Kingma & Welling, 2014) is a probabilistic directed graphical model. It is defined by a joint distribution over a set of latent random variables z and the observed variables x: p(x, z) = p(x|z)p(z). The prior over the latent random variables, p(z), is usually chosen to be a standard Gaussian distribution. The data likelihood p(x|z) is usually a Gaussian or Bernoulli distribution whose parameters depend on z through a deep neural network, known as the decoder network. It also uses an approximate inference model called an encoder or recognition network, that serves as a variational approximation q(z|x) to the posterior p(z|x). The decoder network and the encoder networks are jointly trained to maximize the evidence lower bound (ELBO):\nlog p(x) \u2265 Eq(z|x)[log p(x|z)]\u2212KL(q(z|x)||p(z)) (1)\nIn addition, the reparametrization trick is used to reduce the variance of the gradient estimate."}, {"heading": "2.1.2 GENERATIVE ADVERSARIAL NETWORK (GAN)", "text": "A generative adversarial network (GAN) (Goodfellow et al., 2014) is a generative model trained by a game between a decoder network and a discriminator network. It defines the generative model by sampling the latent variable z from some simple prior distribution p(z) (e.g., Gaussian) followed through the decoder network. The discriminator network D(\u00b7) outputs a probability of a given sample coming from the data distribution. Its task is to distinguish samples from the generator distribution from real data. The decoder network, on the other hand, tries to produce samples as realistic as possible, in order to fool the discriminator into accepting its outputs as being real. The competition between the two networks results in the following minimax problem:\nmin G max D Ex\u223cpdata [logD(x)] + Ez\u223cp(z)[log(1\u2212D(G(z))] (2)\nUnlike VAE, the objective is not explicitly related to the log-likelihood of the data. Moreover, the generative distribution is a deterministic mapping, i.e., p(x|z) is a Dirac delta distribution, parametrized by the deterministic decoder. This can make data likelihood ill-defined, as the probability density of any particular point x can be either infinite, or exactly zero."}, {"heading": "2.1.3 GENERATIVE MOMENT MATCHING NETWORK (GMMN)", "text": "Generative moment matching networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al., 2015) adopt maximum mean discrepancy (MMD) as the training objective, a moment matching criterion where kernel mean embedding techniques are used to avoid unnecessary assumptions of the distributions. It has the same issue as GAN in that the log-likelihood is undefined."}, {"heading": "2.2 ANNEALED IMPORTANCE SAMPLING", "text": "Annealed importance sampling (AIS; (Neal, 2001)) is a Monte Carlo algorithm commonly used to estimate (ratios of) normalizing constants. We mainly consider the application of AIS to Bayesian posterior inference over latent variables z given some fixed observation x. Given a distribution p(x, z) where the posterior distribution p(z|x) is intractable, we ask how we can obtain the marginal likelihood p(x). One simple approach is likelihood weighting, which is a form of importance sampling when the proposal distribution is the prior distribution over z:\np(x) = \u2211 z p(x, z) p(z) p(z) = Ez\u223cp(z)[p(x|z)] (3)\nHowever, the prior over z can be drastically different than the true posterior p(z|x), especially in high dimension. This is problematic because the samples we obtain from the prior is unrepresentative of the true posterior samples, causing inaccurate estimate of the likelihood. AIS instead solves this problem by sequentially bridging the gap between the prior distribution and the true posterior. It considers a set of intermediate distributions p1, ..., pT , where pt = ftZt , constructed from the initial\ndistribution p1(z), usually chosen to be the prior p(z), and the target distribution, pT (z) = p(x,z) p(x) , where p(x) can be seen as the normalizing constant of the unnormalized distribution fT (z) = p(x, z). It is also equipped with a sequence of reversible MCMC transition operators T1, ..., TT , where Tt leaves pt invariant. Then we use AIS to produces a (nonnegative) unbiased estimate of p(x) as follows: first, we sample a random initial state z1 from p1 and set the initial weight w1 = 1. For every stage t \u2265 2 we update the weight w and sample the state zt according to\nwt \u2190 wt\u22121 ft(zt\u22121)\nft\u22121(zt\u22121) zt \u223c Tt(z|zt\u22121) (4)\nTypically, the intermediate distributions are simply defined to be geometric averages ft(z) = f1(z) 1\u2212\u03b2tfT (z) \u03b2t , where the \u03b2t are monotonically increasing parameters with \u03b21 = 0 and \u03b2T = 1. In our case, intermediate distributions have the form\nft(z) = p(z) p(x|z)\u03b2t . (5)"}, {"heading": "2.3 BIDIRECTIONAL MONTE CARLO", "text": "AIS provides a nonnegative unbiased estimate p\u0302(x) of p(x). However, it is often more meaningful to estimate p(x) in the log space, i.e. log p(x), because of underflow problem of dealing with many products of probability measure. In general, we note that logarithm of a nonnegative unbiased estimate is a stochastic lower bound of the log estimand (Grosse et al., 2015). In particular, log p\u0302(x) is a stochastic lower bound on log p(x), satisfying E[p\u0302(x)] \u2264 p(x) and Pr(p\u0302(x) > p(x) + b) < e\u2212b. Grosse et al. (2015) pointed out that if AIS is run in reverse starting from an exact posterior sample, it yields an unbiased estimate of 1/p(x), which (by the above argument) can be seen as a stochastic upper bound on log p(x). The combination of lower and upper bounds from forward and reverse AIS is known as bidirectional Monte Carlo (BDMC). In many cases, the combination of bounds can pinpoint the true value quite precisely. While posterior sampling is just as hard as log-likelihood estimation (Jerrum et al., 1986), in the case of log-likelihood estimation for simulated data, one has available a single exact posterior sample: the parameters and/or latent variables which generated the data. Because this trick is only applicable to simulated data, BDMC is most useful for measuring the accuracy of a log-likelihood estimator on simulated data.\nGrosse et al. (2016) observed that BDMC can also be used to validate posterior inference algorithms, as the gap between upper and lower bounds is itself a bound on the KL divergence of approximate samples from the true posterior distribution."}, {"heading": "3 METHODOLOGY", "text": "For a given generative distribution p(x, z) = p(z)p(x|z), our task is to measure the log-likelihood of test examples log p(xtest). We first discuss how we define the generative distribution for decoderbased networks. For VAE, the generative distribution is defined in the standard way, where p(z) is a standard normal distribution and p(x|z) is a normal distribution parametrized by mean \u00b5\u03b8(z) and \u03c3\u03b8(z), predicted by the generator given the latent code. However, the observation distribution for GANs and GMMNs is typically taken to be a delta function, so that the model\u2019s distribution covers only a submanifold of the space of observables. In order for the likelihood to be well-defined, we follow the same assumption made when evaluating using Kernel Density Estimator (Parzen, 1962): we assume a Gaussian observation model with a fixed variance hyperparameter \u03c32. We will refer to the distribution defined by this Gaussian observation model as p\u03c3 .\nObserve that the KDE estimate is given by\np\u0302\u03c3(x) = 1\nK K\u2211 k=1 p\u03c3(x|z(k)), (6)\nwhere {z(k)}Kk=1 are samples from the prior p(z). This is equivalent to likelihood weighting for the distribution p\u03c3, which is an instance of simple importance sampling (SIS). Because SIS is an unbiased estimator of the likelihood, log p\u0302\u03c3(x) is a stochastic lower bound on log p\u03c3(x) (Grosse et al., 2015). Unfortunately, SIS can result in very poor estimates when the evidence has low prior\nprobability (i.e. the posterior is very dissimilar to the prior). This suggests that AIS might be able to yield much more accurate log-likelihood estimates under p\u03c3. We note that KDE can be viewed as a special case of AIS where the number of intermediate distributions is set to 0.\nWe now describe specifically how we carry out evaluation using AIS. In most of our experiments, we choose the initial distribution of AIS to be p(z), the same prior distribution used in training decoderbased models. If the model provides an encoder network (e.g., VAE), we can take the approximated distribution predicted by the encoder q(z|x) as the initial distribution of the AIS chain. For continuous data, we define the unnormalized density of target distribution to be the joint generative distribution with the Gaussian noise model, p\u03c3(x, z) = p\u03c3(x|z)p(z). For the small subset of experiments done on the binary data, we define the observation model to be a Bernoulli model with mean predicted by the decoder. Our intermediate distributions are geometric averages of the prior and posterior, as in Eqn. 5. Since all of our experiments are done using continuous latent space, we use Hamiltonian Monte Carlo (Neal, 2010) as the transition operator for sampling latent samples along annealing."}, {"heading": "4 RELATED WORK", "text": "AIS is known to be a powerful technique of estimating the partition function of the model. One influential example was the use of AIS to evaluate deep belief networks (Salakhutdinov & Murray, 2008). Although we used the same technique, the problem we consider is completely different. First of all, the model they consider is undirected graphical models, whereas decoder-based models are directed graphical models. Secondly, their model has a well-defined probabilistic density function in terms of energy function, whereas we need to consider different probabilistic model for one in which the the likelihood is ill-defined. In addition, we validate our estimates using BDMC.\nTheis et al. (2016) give an in-depth analysis of issues that might come up in evaluating generative models. They also point out that a model that completely fails at modelling the proportion of modes of the distribution might still achieve a high likelihood score. Salimans et al. (2016) propose an image-quality measure which they find to be highly correlated with human visual judgement. They propose to feed the samples x of the model to the \u201cinception\u201d model to obtain a conditional label distribution p(y|x), and evaluate the score defined by expExKL(p(y|x)||p(y)), which is motivated by having a low entropy of p(y|x) but a large entropy of p(y). However, the measure is largely based on visual quality of the sample, and we argue that the visual quality can be a misleading way to evaluate a model."}, {"heading": "5 EXPERIMENTS", "text": ""}, {"heading": "5.1 DATASETS", "text": "All of our experiments were performed on the MNIST dataset of images of handwritten digits (LeCun et al., 1998). For consistency with prior work on evaluating decoder-based models, most of our experiments used the continuous inputs. We dequantized the data following Uria et al. (2013), by adding a uniform noise of 1256 to the data and rescaling it to be in [0, 1]\nD after dequantization. We use the standard split of MNIST into 60,000 training and 10,000 test examples, and used 50,000 images from the training set for training, and remaining 10,000 images for validation. In addition, some of our experiments used the binarized MNIST dataset with a Bernoulli observation model (Salakhutdinov & Murray, 2008)."}, {"heading": "5.2 MODELS", "text": "For most of our experiments, we considered two decoder architectures: a small one with 10 latent dimensions, and a larger one with 50 latent dimensions. We use standard Normal distribution as prior for training all of our models. All layers were fully connected, and the number of units in each layer was 10\u201364\u2013256\u2013256-1024\u2013784 for the smaller architecture and 50\u20131024\u20131024\u20131024\u2013784 for the larger one. We trained both architectures using the VAE, GAN, and GMMN objectives, resulting in six networks which we refer to as VAE-10, VAE-50, etc. In general, the larger architecture performed substantially better on both the training and test sets, but we analyze the smaller architecture as well because it better highlights some of the differences between the training criteria. Additional architectural details are given in Appendix A.1.\nIn order to enable a direct comparison between training criteria, all models used a spherical Gaussian observation model with fixed variance. This is consistent with previous protocols for evaluating GANs and GMMNs. However, we note that this observation model is a nontrivial constraint on the VAEs, which could instead be trained with a more flexible diagonal Gaussian observation model where the variances depend on the latent state. Such observation models can easily achieve much higher log-likelihood scores, for instance by noticing that boundary pixels are always close to 0. (E.g., we trained a VAE with the more general observation model which achieved a log-likelihood of at least 2200 nats on continuous MNIST.) Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model."}, {"heading": "5.3 VALIDATION OF LOG-LIKELIHOOD ESTIMATES", "text": "Before we analyze the performance of the trained networks, we must first determine the accuracy of the log-likelihood estimators. In this section, we validate the accuracy of our AIS-based estimates using BDMC. We then analyze the error in the KDE and IWAE estimates and highlight some cases where these measures miss important phenomena."}, {"heading": "5.3.1 VALIDATION OF AIS", "text": "We used AIS to estimate log-likelihoods for all models under consideration. Except where otherwise specified, all AIS estimates were obtained using 16 independent chains, 10,000 intermediate distributions of the form in Eqn. 5, and a transition operator consisting of one proposed HMC trajectory with 10 leapfrog steps.1 Following Ranzato et al. (2010), the HMC stepsize was tuned to achieve an acceptance rate of 0.65 (as recommended by Neal (2010)).\nFor all six models, we evaluated the accuracy of this estimation procedure using BDMC on data sampled from the model\u2019s distribution on 1000 simulated examples. The gap between the loglikelihood estimates produced by forward AIS (which gives a lower bound) and reverse AIS (which gives an upper bound) bounds the error of the AIS estimates on simulated data. We refer to this gap as the BDMC gap. For five of the six networks under consideration, we found the BDMC gap to be less than 1 nat. For the remaining model (GAN-50), the gap was about 10 nats. Both gaps are much smaller than our measured log-likelihood differences between models. If these gaps are representative of the true error in the estimates on the real data, then this indicates AIS is accurate enough to make fine-grained comparisons between models and to benchmark other log-likelihood estimators. (The BDMC gap is not guaranteed to hold for the real data, although Grosse et al. (2016) found the behavior of AIS to match closely between real and simulated data.)"}, {"heading": "5.3.2 HOW ACCURATE IS KERNEL DENSITY ESTIMATION?", "text": "Kernel density estimation (KDE) (Parzen, 1962) is widely used to evaluate decoder-based models (Goodfellow et al., 2014; Li & Swersky, 2015), and a variant was proposed in the setting of evaluating Boltzmann machines (Bengio et al., 2013). Papers reporting KDE estimates often caution that the\n1We used the HMC implementation from http://deeplearning.net/tutorial/ deeplearning.pdf\nKDE is not meant to be applied in high-dimensional spaces and that the results might therefore be inaccurate. Nevertheless, KDE remains the standard protocol for evaluating decoder-based models. We analyzed the accuracy of the KDE estimates by comparing against AIS. Both estimates are stochastic lower bounds on the true log-likelihood (see Section 3), so larger values are guaranteed (with high probability) to be more accurate.\nFor each estimator, we varied one parameter influencing the computational budget; for AIS, this was the number of intermediate distributions (chosen from {100, 500, 1000, 2000, 10000}), and for KDE, it was the number of samples (chosen from {10000, 100000, 500000, 1000000, 2000000}). Using GMMN-10 for illustration, we plot both log-likelihood estimates 100 simulated examples as a function of evaluation time in Fig. 2(b). We also plot the upper bound of likelihood given by running AIS in reverse direction. We see that the BDMC gap approaches to zero, validating the accuracy of AIS. We also see that the AIS estimator achieves much more accurate estimates during similar evaluation time. Furthermore, the KDE estimates appear to level off, suggesting one cannot obtain accurate results even using orders of magnitude more samples.\nThe KDE estimation error also impacts the estimate of the observation noise \u03c3, since a large value of \u03c3 is needed for the samples to cover the full distribution. We compared the log-likelihoods estimated by AIS and KDE with varying choices of \u03c3 on 100 training and validation examples of MNIST. We used 1 million simulated samples for KDE evaluation, which takes almost the same time as running AIS estimation. In Fig. 2(a), we show the log-likelihood of GAN-50 estimated by KDE and AIS as a function of \u03c3. Because the accuracy of KDE declines sharply for small \u03c3 values, it creates a strong bias towards large \u03c3."}, {"heading": "5.3.3 HOW ACCURATE IS THE IWAE BOUND?", "text": "In principle, one could estimate VAE likelihoods using the VAE objective function (which is a lower bound on the true log-likelihood). However, it is more common to use importance weighting, where the proposal distribution is computed by the recognition network. This is provably more accurate than the VAE bound (Burda et al., 2016). Because the importance weighted estimate corresponds to the objective function used by the Importance Weighted Autoencoder (IWAE) (Burda et al., 2016), we will refer to it as the IWAE bound.\nOn continuous MNIST, the IWAE bound underestimated the true log-likelihoods by at least 33.2 nats on the training set and 187.4 nats on the test set. While this is considerably more accurate than KDE, the error is still significant. Interestingly, this result also suggests that the recognition network overfits the training data.\nSince VAE and IWAE results have customarily been reported on binarized MNIST, we additionally trained an IWAE in this setting. The training details are given in Appendix A.2. We also evaluate AIS with the initial distribution defined by encoders of VAEs, denoted as AIS+encoder. We find that the IWAE bound underestimates the true value by at least 1 nat, which is a large difference by the standards of binarized MNIST. (E.g., it represents about half of the gap between a state-of-the-art permutation-invariant model (Tran et al., 2016) and one which exploits structure (van den Oord et al., 2016).) The AIS and IWAE estimates are compared in terms of evaluation time in Fig. 2 (c)."}, {"heading": "5.4 SCIENTIFIC FINDINGS", "text": "Having validated the accuracy of AIS, we now use it to analyze the effectiveness of various training criteria. We also highlight phenomena which would not be observable using existing log-likelihood estimators or by inspecting samples. For all experiments in this section, we used 10,000 intermediate distributions for AIS, 1 million simulated samples for KDE, and 200,000 importance samples for the IWAE bound. (These settings resulted in similar computation time for all three estimators.)"}, {"heading": "5.4.1 MODEL LIKELIHOOD COMPARISON", "text": "We evaluated the trained models using AIS and KDE on 1000 test examples of MNIST; results are shown in Table 2. We find that for all three training criteria, the larger architectures consistently outperformed the smaller ones. We also find that for both the 10- and 50-dimensional architectures, the VAEs achieved substantially higher log-likelihoods than GANs or GMMNs. It is not surprising that the VAEs achieved higher likelihood, because they were trained using a likelihood-based objective while the GANs and GMMNs were not. However, it is interesting that the difference in log-likelihoods was so large; in the rest of this section, we attempt to analyze what exactly is causing this large difference.\nWe note that the KDE errors were of the same order of magnitude as the differences between models, indicating that it cannot be used reliably to compare log-likelihoods. Furthermore, KDE did not identify the correct ordering of models; for instance, it estimated a lower log-likelihood for VAE50 than for VAE-10, even though its true log-likelihood was almost 300 nats higher. KDE also underestimated by an order of magnitude the log-likelihood improvements that resulted from using the larger architectures. (E.g., it estimated a 15 nat difference between GMMN-10 and GMMN-50, even though the true difference was 247 nats as estimated by AIS.)\nThese differences are also hard to observe simply by looking at samples; for instance, we were unable to visually distinguish the quality of samples for GAN-10 and GAN-50 (see Fig. 1), even though their log-likelihoods differed by almost 300 nats on both the training and test sets."}, {"heading": "5.4.2 MEASURING THE DEGREE OF OVERFITTING", "text": "One question that arises in evaluation of decoder-based generative models is whether they memorize parts of the training dataset. One cannot test this by looking only at model samples. The commonly reported nearest-neighbors from the training set can be misleading (Theis et al., 2016), and interpolation in the latent space between different samples can be visually appealing, but does not provide a quantitative measure of the degree of generalization.\nTo analyze the degree of overfitting, Fig. 3 shows training curves for three networks as measured by AIS, KDE, and the IWAE bound. We observe that GAN-50\u2019s training and test log-likelihoods are nearly identical throughout training, disconfirming the hypothesis that it was memorizing training data. Both GAN-50 and GMMN-50 overfit less than VAE-50.\nWe also observed two phenomena which could not be measured using existing techniques. First, in the case of VAE-50, the IWAE lower bound starts to decline after 200 epochs, while the AIS estimates hold steady, suggesting it is the recognition network rather than the generative network which is overfitting most. Second, the GMMN-50 training and validation error continue to improve at 10,000 epochs, even though KDE erroneously indicates that performance has leveled off."}, {"heading": "5.4.3 HOW APPROPRIATE IS THE OBSERVATION MODEL?", "text": "Appendix B addresses the questions of whether the spherical Gaussian observation model is a good fit and whether the log-likelihood differences could be an artifact of the observation model. We find that all of the models can be substantially improved by accounting for non-Gaussianity, but that this effect is insufficient to explain the gap between the VAEs and the other models."}, {"heading": "5.4.4 ARE THE NETWORKS MISSING MODES?", "text": "It was previously observed that one of the potential failure modes of Boltzmann machines is to fail to generate one or more modes of a distribution or to drastically misallocate probability mass between modes (Salakhutdinov & Murray, 2008). Here we analyze this for decoder-based models.\nFirst, we ask a coarse-grained version of this question: do the networks allocate probability mass correctly between the 10 digit classes, and if not, can this explain the difference in log-likelihood scores? In Fig. 1, we see that GAN-50\u2019s distribution of digit classes was heavily skewed: out of 100 samples, it generated 37 images of 1\u2019s, but only a single 2. This appears to be a large effect, but it does not explain the magnitude of the log-likelihood difference from VAEs. In particular, if the allocation of digit classes were off by a factor of 10, this effect by itself could cost at most log 10 \u2248 2.3 nats of log-likelihood. Since VAE-50 outperformed GAN-50 by 364 nats, this effect cannot explain the difference.\nHowever, MNIST has many factors of variability beyond simply the 10 digit classes. In order to determine whether any of the models missed more fine-grained modes, we visualized posterior samples for each model conditioned on training and test images. In particular, for each image x under consideration, we used AIS to approximately sample z from the posterior distribution p(z|x), and then ran the decoder on z. While these samples are approximate, Grosse et al. (2016) point out that the BDMC gap also bounds the KL divergence of approximate samples from the true posterior. With the exception of GAN-50, our BDMC gaps were on the order of 1 nat, suggesting our approximate posterior samples are fairly representative. The results are shown in Fig. 4. Further posterior visualizations for digit class 2 (the most difficult for the models we considered) are shown in Appendix C.\nBoth VAEs\u2019 posterior samples match the observations almost perfectly. (We observed a few poorly reconstructed examples on the test set, but not on the training set.) The GANs and GMMNs fail to reconstruct some of the examples on both the training and validation sets, suggesting that they failed to learn some modes of the distribution."}, {"heading": "ACKNOWLEDGMENTS", "text": "We like to thank Yujia Li for providing his original GMMN model and codebase, and thank Jimmy Ba for advice on training GANs. Ruslan Salakhutdinov is supported in part by Disney and ONR grant N000141310721. We also thank the developers of Lasagne (Battenberg et al., 2014) and Theano (Al-Rfou et al., 2016)."}, {"heading": "A NETWORK ARCHITECTURES/TRAINING", "text": "A.1 MODELS ON CONTINUOUS MNIST\nThe decoders have all fully connected layers, and the number of units in each layer was 10\u201364\u2013256\u2013 256-1024\u2013784 for the smaller architecture and 50\u20131024\u20131024\u20131024\u2013784 for the larger one. Other architecture details are summarized as follows.\n\u2022 For GAN-10, we used a discriminator with the architecture 784-512-256-1, where each layer used dropout with parameter 0.5. For GAN-50, we used a discriminator with architecture 784-4096-4096-4096-4096-1. All hidden layers used dropout with parameter 0.8. All hidden layers in both networks used the tanh activation function, and the output layers used the logistic function.\n\u2022 The larger model uses an encoder of an architecture 784-1024-1024-1024-100. We add dropout layer between each hidden layer, with a dropout rate of 0.2. The smaller model uses an encoder of an architecture 784-256-64-20. Generator\u2019s hidden layers use tanh activation function, and the output layer uses sigmoid unit. Encoder\u2019s hidden layers use tanh activation function, and the output layer uses linear activation.\n\u2022 GMMN: The hidden layers use ReLU activation function, and the output layer uses sigomid unit.\nFor training GAN/VAE, we use our own implementation. We use Adam for optimization, and perform grid search of learning rate from {0.001, 0.0001, 0.00001}. For training GMMN, we take the implementation from https://github.com/yujiali/gmmn.git. Following the implementation, we use SGD with momentum for optimization, and perform grid search of learning rate from {0.1, 0.5, 1, 2}, with momentum 0.9.\nA.2 MODELS ON BINARIZED MNIST\nIts decoder has the architecture 50-200-200-784 with all tanh hidden layers and sigmoid output layer, and its encoder is symmetric in architecture, with linear output layer. We take the implementation at https://github.com/yburda/iwae.git for training the IWAE model.The IWAE bound was computed with 50 samples during training. We keep all the hyperparameter choices the same as in the implementation.\nB HOW PROBLEMATIC IS THE GAUSSIAN OBSERVATION MODEL?\nIn this section, we consider whether the difference in log-likelihood between models could be an artifact of the Gaussian noise model (which we know to be a poor fit). In principle, the Gaussian noise assumption could be unfair to the GANs and GMMNs, because the VAE training uses the correct observation model, while the GAN and GMMN objectives do not have any particular observation model built in.\nTo determine the size of this effect, we evaluated the models under a different regime where, instead of choosing a fixed value of the observation noise \u03c3 on a validation set, \u03c3 was tuned independently for each example.2 This is not a proper generative model, but it can be viewed as an upper bound on the log-likelihood that would be achievable with a heavy-tailed and radially symmetric noise model.3 Results are shown in Table 3. We see that adapting \u03c3 for each example results in a log-likelihood improvement between 30 and 100 nats for all of the networks. In general, the examples which show the largest performance jump are images of 1\u2019s (which prefer smaller \u03c3) and 2\u2019s (which prefer larger \u03c3). This is a significant effect, and suggests that one could significantly improve the log-likelihood scores by picking a better observation model. However, this effect is smaller in magnitude than the differences between VAE and GAN/GMMN log-likelihoods, so it fails to explain the likelihood difference.\n2We pick the best variance parameter among {0.005, 0.01, 0.015, 0.02, 0.025} for each training/validation examples when evaluating GAN-50 and GMMN-50 and {0.015, 0.02, 0.025, 0.03, 0.035} when evaluating GAN-10 and GMMN-10.\n3In particular, heavy-tailed radially symmetric distributions can be viewed as Gaussian scale mixtures (Wainwright & Simoncelli, 1999). I.e., one has a prior distribution on \u03c3 (possibly learned) and integrates it out for each test example. Clearly the probability under such a mixture cannot exceed the maximum value with respect to \u03c3."}, {"heading": "C POSTERIOR VISUALIZATION OF DIGIT \u201c2\"", "text": "According to the log-likelihood evaluation, we find digit \u201c2\" is the hardest digit for modelling. In this section we investigate the quality of modelling \u201c2\" of each model. We randomly sampled a fixed set of 100 samples of digit \u201c2\" from training data and compare whether model capture this mode. We show the plots of \u201c2\" for GAN-10, GAN-50, VAE-10 and true data in the following figures for illustration. We see that GAN-10 fails at capturing many instances of digit \u201c2\" in the training data! We see instead of generating \u201c2\", it tries to generate digit \u201c1\", \u201c7\" \u201c9\", \u201c4\", \u201c8\" from reconstruction. GAN-50 does much better, its reconstruction are all digit \u201c2\" and there is only some style difference from the true data. VAE-10 totally dominates this competition, where it perfectly reconstructs all the samples of digit \u201c2\". We emphasize if directly sampling from each model, samples look visually indistinguishable (see Fig. 1), but we can clearly see differences in posterior samples."}], "references": [{"title": "Theano: A python framework for fast computation of mathematical expressions, 2016", "author": ["Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi"], "venue": null, "citeRegEx": "Al.Rfou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2016}, {"title": "Bounding the test log-likelihood of generative models", "author": ["Y. Bengio", "L. Yao", "K. Cho"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A test of relative similarity for model selection in generative models", "author": ["Wacha Bounliphone", "Eugene Belilovsky", "Matthew B. Blaschko", "Ioannis Antonoglou", "Arthur Gretton"], "venue": "In ICLR", "citeRegEx": "Bounliphone et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bounliphone et al\\.", "year": 2016}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "In ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E. Denton", "S. Chintala", "A. Szlam", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Nice: Non-linear independent components estimation", "author": ["Laurent Dinh", "David Krueger", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1410.8516,", "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "Density estimation using real nvp", "author": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "venue": null, "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Training generative neural networks via Maximum Mean Discrepancy optimization", "author": ["Gintare Karolina Dziugaite", "Daniel M. Roy", "Zoubin Ghahramani"], "venue": "In UAI", "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Measuring the reliability of MCMC inference with bidirectional Monte Carlo", "author": ["Roger Grosse", "Siddharth Ancha", "Daniel M. Roy"], "venue": "In NIPS,", "citeRegEx": "Grosse et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2016}, {"title": "Sandwiching the marginal likelihood using bidirectional monte carlo", "author": ["Roger B. Grosse", "Zoubin Ghahramani", "Ryan P. Adams"], "venue": "arXiv preprint arXiv:1511.02543,", "citeRegEx": "Grosse et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2015}, {"title": "Generating images with recurrent adversarial networks", "author": ["Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1602.05110,", "citeRegEx": "Im et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Im et al\\.", "year": 2016}, {"title": "Random generation of combinatorial structures from a uniform distribution", "author": ["Mark R. Jerrum", "Leslie G. Valiant", "Vijay V. Vazirani"], "venue": "Theoretical Computer Science,", "citeRegEx": "Jerrum et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Jerrum et al\\.", "year": 1986}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky"], "venue": "In In ICML", "citeRegEx": "Li and Swersky.,? \\Q2015\\E", "shortCiteRegEx": "Li and Swersky.", "year": 2015}, {"title": "Annealed importance sampling", "author": ["Radford M. Neal"], "venue": "Statistics and Computing,", "citeRegEx": "Neal.,? \\Q2001\\E", "shortCiteRegEx": "Neal.", "year": 2001}, {"title": "MCMC using Hamiltonian dynamics", "author": ["Radford M. Neal"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "Neal.,? \\Q2010\\E", "shortCiteRegEx": "Neal.", "year": 2010}, {"title": "On estimation of a probability density function and mode", "author": ["Emanuel Parzen"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Parzen.,? \\Q1962\\E", "shortCiteRegEx": "Parzen.", "year": 1962}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "In ICLR,", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Factored 3-way restricted Boltzmann machines for modeling natural images", "author": ["Marc\u2019Aurelio Ranzato", "Alex Krizhevsky", "Geoffrey E Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2010}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo J. Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "On the quantitative analysis of Deep Belief Networks", "author": ["Ruslan Salakhutdinov", "Iain Murray"], "venue": "Proceedings of the 25th Annual International Conference on Machine Learning (ICML", "citeRegEx": "Salakhutdinov and Murray.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray.", "year": 2008}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In NIPS,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "A\u00e4ron van den Oord", "Matthias Bethge"], "venue": "In ICLR,", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "The variational Gaussian process", "author": ["Dustin Tran", "Rajesh Ranganath", "David M. Blei"], "venue": "In ICLR,", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "RNADE: The real-valued neural autoregressive density-estimator", "author": ["Benigno Uria", "Iain Murray", "Hugo Larochelle"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "Pixel recurrent neural networks", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "In ICML,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Scale mixtures of Gaussians and the statistics of natural images", "author": ["Martin J. Wainwright", "Eero P. Simoncelli"], "venue": "In NIPS,", "citeRegEx": "Wainwright and Simoncelli.,? \\Q1999\\E", "shortCiteRegEx": "Wainwright and Simoncelli.", "year": 1999}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["Jun-Yan Zhu", "Philipp Kr\u00e4henb\u00fchl", "Eli Shechtman", "Alexei A. Efros"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "In recent years, deep generative models have dramatically pushed forward the state-of-the-art in generative modelling by generating convincing samples of images (Radford et al., 2016), achieving state-of-the-art semi-supervised learning results (Salimans et al.", "startOffset": 161, "endOffset": 183}, {"referenceID": 23, "context": ", 2016), achieving state-of-the-art semi-supervised learning results (Salimans et al., 2016), and enabling automatic image manipulation (Zhu et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 29, "context": ", 2016), and enabling automatic image manipulation (Zhu et al., 2016).", "startOffset": 51, "endOffset": 69}, {"referenceID": 21, "context": "Important examples include variational autoencoders (VAEs) (Kingma & Welling, 2014; Rezende et al., 2014), generative adversarial networks (GANs) (Goodfellow et al.", "startOffset": 59, "endOffset": 105}, {"referenceID": 8, "context": ", 2014), generative adversarial networks (GANs) (Goodfellow et al., 2014), generative moment matching networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 7, "context": ", 2014), generative moment matching networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al., 2015), and nonlinear independent components estimation (Dinh et al.", "startOffset": 53, "endOffset": 97}, {"referenceID": 5, "context": ", 2015), and nonlinear independent components estimation (Dinh et al., 2014).", "startOffset": 57, "endOffset": 76}, {"referenceID": 4, "context": "While many decoder-based models are able to produce convincing samples (Denton et al., 2015; Radford et al., 2016), rigorous evaluation remains a challenge.", "startOffset": 71, "endOffset": 114}, {"referenceID": 19, "context": "While many decoder-based models are able to produce convincing samples (Denton et al., 2015; Radford et al., 2016), rigorous evaluation remains a challenge.", "startOffset": 71, "endOffset": 114}, {"referenceID": 24, "context": "Comparing models by inspecting samples is labor-intensive, and potentially misleading (Theis et al., 2016).", "startOffset": 86, "endOffset": 106}, {"referenceID": 2, "context": "While alternative quantitative criteria have been proposed (Bounliphone et al., 2016; Im et al., 2016; Salimans et al., 2016), log-likelihood of held-out test data remains one of the most important measures of a generative model\u2019s performance.", "startOffset": 59, "endOffset": 125}, {"referenceID": 11, "context": "While alternative quantitative criteria have been proposed (Bounliphone et al., 2016; Im et al., 2016; Salimans et al., 2016), log-likelihood of held-out test data remains one of the most important measures of a generative model\u2019s performance.", "startOffset": 59, "endOffset": 125}, {"referenceID": 23, "context": "While alternative quantitative criteria have been proposed (Bounliphone et al., 2016; Im et al., 2016; Salimans et al., 2016), log-likelihood of held-out test data remains one of the most important measures of a generative model\u2019s performance.", "startOffset": 59, "endOffset": 125}, {"referenceID": 5, "context": "Unfortunately, unless the decoder is designed to be reversible (Dinh et al., 2014; 2016), log-likelihood estimation in decoder-based models is typically intractable.", "startOffset": 63, "endOffset": 88}, {"referenceID": 18, "context": "The most widely used estimator of log-likelihood for GANs and GMMNs is the Kernel Density Estimator (KDE) (Parzen, 1962).", "startOffset": 106, "endOffset": 120}, {"referenceID": 24, "context": "Unfortunately, KDE is notoriously inaccurate for estimating likelihood in high dimensions, because it is hard to tile a high-dimensional manifold with spherical Gaussians (Theis et al., 2016).", "startOffset": 171, "endOffset": 191}, {"referenceID": 16, "context": "In this paper, we propose to use annealed importance sampling (AIS; (Neal, 2001)) to estimate log-likelihoods of decoder-based generative models and to obtain approximate posterior samples.", "startOffset": 68, "endOffset": 80}, {"referenceID": 10, "context": "Importantly, we validate this approach using Bidirectional Monte Carlo (BDMC) (Grosse et al., 2015), which provably bounds the log-likelihood estimation error and the KL divergence from the true posterior distribution for data simulated from a model.", "startOffset": 78, "endOffset": 99}, {"referenceID": 8, "context": "(Kingma & Welling, 2014), Generative Adversarial Network (GAN) (Goodfellow et al., 2014), and Generative Moment Matching Network (GMMN) (Li & Swersky, 2015; Dziugaite et al.", "startOffset": 63, "endOffset": 88}, {"referenceID": 7, "context": ", 2014), and Generative Moment Matching Network (GMMN) (Li & Swersky, 2015; Dziugaite et al., 2015).", "startOffset": 55, "endOffset": 99}, {"referenceID": 8, "context": "A generative adversarial network (GAN) (Goodfellow et al., 2014) is a generative model trained by a game between a decoder network and a discriminator network.", "startOffset": 39, "endOffset": 64}, {"referenceID": 7, "context": "Generative moment matching networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al., 2015) adopt maximum mean discrepancy (MMD) as the training objective, a moment matching criterion where kernel mean embedding techniques are used to avoid unnecessary assumptions of the distributions.", "startOffset": 44, "endOffset": 88}, {"referenceID": 16, "context": "Annealed importance sampling (AIS; (Neal, 2001)) is a Monte Carlo algorithm commonly used to estimate (ratios of) normalizing constants.", "startOffset": 35, "endOffset": 47}, {"referenceID": 10, "context": "In general, we note that logarithm of a nonnegative unbiased estimate is a stochastic lower bound of the log estimand (Grosse et al., 2015).", "startOffset": 118, "endOffset": 139}, {"referenceID": 12, "context": "While posterior sampling is just as hard as log-likelihood estimation (Jerrum et al., 1986), in the case of log-likelihood estimation for simulated data, one has available a single exact posterior sample: the parameters and/or latent variables which generated the data.", "startOffset": 70, "endOffset": 91}, {"referenceID": 9, "context": "In general, we note that logarithm of a nonnegative unbiased estimate is a stochastic lower bound of the log estimand (Grosse et al., 2015). In particular, log p\u0302(x) is a stochastic lower bound on log p(x), satisfying E[p\u0302(x)] \u2264 p(x) and Pr(p\u0302(x) > p(x) + b) < e\u2212b. Grosse et al. (2015) pointed out that if AIS is run in reverse starting from an exact posterior sample, it yields an unbiased estimate of 1/p(x), which (by the above argument) can be seen as a stochastic upper bound on log p(x).", "startOffset": 119, "endOffset": 287}, {"referenceID": 9, "context": "In general, we note that logarithm of a nonnegative unbiased estimate is a stochastic lower bound of the log estimand (Grosse et al., 2015). In particular, log p\u0302(x) is a stochastic lower bound on log p(x), satisfying E[p\u0302(x)] \u2264 p(x) and Pr(p\u0302(x) > p(x) + b) < e\u2212b. Grosse et al. (2015) pointed out that if AIS is run in reverse starting from an exact posterior sample, it yields an unbiased estimate of 1/p(x), which (by the above argument) can be seen as a stochastic upper bound on log p(x). The combination of lower and upper bounds from forward and reverse AIS is known as bidirectional Monte Carlo (BDMC). In many cases, the combination of bounds can pinpoint the true value quite precisely. While posterior sampling is just as hard as log-likelihood estimation (Jerrum et al., 1986), in the case of log-likelihood estimation for simulated data, one has available a single exact posterior sample: the parameters and/or latent variables which generated the data. Because this trick is only applicable to simulated data, BDMC is most useful for measuring the accuracy of a log-likelihood estimator on simulated data. Grosse et al. (2016) observed that BDMC can also be used to validate posterior inference algorithms, as the gap between upper and lower bounds is itself a bound on the KL divergence of approximate samples from the true posterior distribution.", "startOffset": 119, "endOffset": 1142}, {"referenceID": 18, "context": "In order for the likelihood to be well-defined, we follow the same assumption made when evaluating using Kernel Density Estimator (Parzen, 1962): we assume a Gaussian observation model with a fixed variance hyperparameter \u03c3.", "startOffset": 130, "endOffset": 144}, {"referenceID": 10, "context": "Because SIS is an unbiased estimator of the likelihood, log p\u0302\u03c3(x) is a stochastic lower bound on log p\u03c3(x) (Grosse et al., 2015).", "startOffset": 108, "endOffset": 129}, {"referenceID": 17, "context": "Since all of our experiments are done using continuous latent space, we use Hamiltonian Monte Carlo (Neal, 2010) as the transition operator for sampling latent samples along annealing.", "startOffset": 100, "endOffset": 112}, {"referenceID": 23, "context": "Theis et al. (2016) give an in-depth analysis of issues that might come up in evaluating generative models.", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "Salimans et al. (2016) propose an image-quality measure which they find to be highly correlated with human visual judgement.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "1 DATASETS All of our experiments were performed on the MNIST dataset of images of handwritten digits (LeCun et al., 1998).", "startOffset": 102, "endOffset": 122}, {"referenceID": 14, "context": "1 DATASETS All of our experiments were performed on the MNIST dataset of images of handwritten digits (LeCun et al., 1998). For consistency with prior work on evaluating decoder-based models, most of our experiments used the continuous inputs. We dequantized the data following Uria et al. (2013), by adding a uniform noise of 1 256 to the data and rescaling it to be in [0, 1] D after dequantization.", "startOffset": 103, "endOffset": 297}, {"referenceID": 16, "context": "1 Following Ranzato et al. (2010), the HMC stepsize was tuned to achieve an acceptance rate of 0.", "startOffset": 12, "endOffset": 34}, {"referenceID": 14, "context": "65 (as recommended by Neal (2010)).", "startOffset": 22, "endOffset": 34}, {"referenceID": 9, "context": "(The BDMC gap is not guaranteed to hold for the real data, although Grosse et al. (2016) found the behavior of AIS to match closely between real and simulated data.", "startOffset": 68, "endOffset": 89}, {"referenceID": 18, "context": "Kernel density estimation (KDE) (Parzen, 1962) is widely used to evaluate decoder-based models (Goodfellow et al.", "startOffset": 32, "endOffset": 46}, {"referenceID": 8, "context": "Kernel density estimation (KDE) (Parzen, 1962) is widely used to evaluate decoder-based models (Goodfellow et al., 2014; Li & Swersky, 2015), and a variant was proposed in the setting of evaluating Boltzmann machines (Bengio et al.", "startOffset": 95, "endOffset": 140}, {"referenceID": 1, "context": ", 2014; Li & Swersky, 2015), and a variant was proposed in the setting of evaluating Boltzmann machines (Bengio et al., 2013).", "startOffset": 104, "endOffset": 125}, {"referenceID": 3, "context": "This is provably more accurate than the VAE bound (Burda et al., 2016).", "startOffset": 50, "endOffset": 70}, {"referenceID": 3, "context": "Because the importance weighted estimate corresponds to the objective function used by the Importance Weighted Autoencoder (IWAE) (Burda et al., 2016), we will refer to it as the IWAE bound.", "startOffset": 130, "endOffset": 150}, {"referenceID": 25, "context": ", it represents about half of the gap between a state-of-the-art permutation-invariant model (Tran et al., 2016) and one which exploits structure (van den Oord et al.", "startOffset": 93, "endOffset": 112}, {"referenceID": 24, "context": "The commonly reported nearest-neighbors from the training set can be misleading (Theis et al., 2016), and interpolation in the latent space between different samples can be visually appealing, but does not provide a quantitative measure of the degree of generalization.", "startOffset": 80, "endOffset": 100}, {"referenceID": 9, "context": "While these samples are approximate, Grosse et al. (2016) point out that the BDMC gap also bounds the KL divergence of approximate samples from the true posterior.", "startOffset": 37, "endOffset": 58}], "year": 2016, "abstractText": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "creator": "LaTeX with hyperref package"}}}