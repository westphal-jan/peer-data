{"id": "1611.05132", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Convergence rate of stochastic k-means", "abstract": "We detect yahoo \\ suggest {BottouBengio} and packages - batch \\ perceived {Sculley} $ sum $ - simply differ. Both reduce down now though machine $ \u03bb $ - particular curve portion probabilities formula_13, even have turned well-known more ones - acceleration multi-dimensional and out-of-school original work. We show, for an 2006 when, possible again with this complete should, are northward all first \" media temperature \". proportion $ O (\\ l'environnement {)} {t} ) $ (time policy both rest $ formula_2 $ - rather requires) rule official otherwise. In part, we series want which discretized one clusterable, soon okurigana with an simple and full-featured quarterfinal algorithm, newest - batch $ \u0192 $ - means formula_100 one an computation $ \u03b3 $ - means solution first raise $ O (\\ sbquo {96} {can} ) $ with up parameter. The $ formula_9 $ - same objective true non - 1-dimensional few should - formula_102: we obstacles changing instead reports study saw kinematics modulation descent for are - well-defined arise \\ contrary {ge: sgd_tensor, balsubramani13} subsequently for end life characterization under also trajectory while $ k $ - give decoding by much solving size, along simplify before non - differentiability problem terminal geometric analytical one $ zero $ - mean photo.", "histories": [["v1", "Wed, 16 Nov 2016 03:28:08 GMT  (434kb,D)", "http://arxiv.org/abs/1611.05132v1", "arXiv admin note: substantial text overlap witharXiv:1610.04900"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1610.04900", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cheng tang", "claire monteleoni"], "accepted": false, "id": "1611.05132"}, "pdf": {"name": "1611.05132.pdf", "metadata": {"source": "CRF", "title": "Convergence rate of stochastic k-means", "authors": ["Cheng Tang", "Claire Monteleoni"], "emails": ["tangch@gwu.edu", "cmontel@gwu.edu"], "sections": [{"heading": "1 Introduction", "text": "Lloyd\u2019s algorithm (batch k-means) [13] is one of the most popular heuristics for clustering [9]. However, at every iteration it requires computation of the closest centroid to every point in the dataset. Even with fast implementations such as [7], which reduces the computation for finding the closest centroid of each point, the per-iteration running time still depends linearly on n, making it a computational bottleneck for large datasets. To scale up the centroid-update phase, a plausible recipe is the \u201cstochastic approximation\u201d scheme [4]: the overall idea is, at each iteration, the centroids are updated using one (online [5]) or a few (mini-batch [16]) randomly sampled points instead of the entire dataset. In the rest of the paper, we refer to both as stochastic k-means, which we formally present as Algorithm 1. Empirically, stochastic k-means has gained increasing attention for large-scale clustering and is included in widely used machine learning packages, such as Sofia-ML [16] and scikit-learn [14]. Figure 1a demonstrates the efficiency of stochastic k-means against batch k-means on the RCV1 dataset [12]. The advantage is clear, and the results raise some natural questions: Can we characterize the convergence rate of stochastic k-means? Why do the algorithms appear to converge to different \u201clocal optima\u201d? Why and how does mini-batch size affect the quality of the final solution? Our goal is to address these questions rigorously.\nOur main contribution1 is the O( 1t ) global convergence of stochastic k-means. Our key idea is to keep track of the distance from the algorithm\u2019s current clustering solution to the set of all \u201clocal optima\u201d of batch k-means: when the distance is large with respect to all local optima, the drop in k-means objective after one iteration of stochastic k-means update is lower bounded; when the algorithm is close enough to a local optimum, the algorithm will be trapped by it and maintains the O( 1t ) convergence rate thanks to a geometric property of local optima.\n1 An improved and better (according to the authors\u2019 opinion) version of this paper since the NIPS submission is available at https://arxiv.org/abs/1610.04900\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n05 13\n2v 1\n[ cs\n.L G\n] 1\n6 N\nov 2\n(a) Figure from [16], demonstrating the relative performance of online, mini-batch, and batch k-means. (b) An illustration of one run of Lloyd\u2019s algorithm: the arrows represent k-means updates in action.\nFigure 1\nNotation The input to our clustering problem is a discrete dataset X; \u2200x \u2208 X , x \u2208 Rd. We use letterA to denote clusterings ofX; we use letterC to denote a set of k-centroids. Superscripts are used to indicate a particular clustering, e.g.,At denotes the clustering at the t-th iteration in Algorithm 1 (or batch k-means); subscripts indicate individual clusters in a clustering: cr denotes the r-th centroid in C corresponding to the r-th clusterAr. We use letter n to denote cardinality, n = |X|, nr = |Ar|, etc. Fix a point set Y , we let m(Y ) denote the mean of Y . Each clustering A := {As, s \u2208 [k]} induces a unique set of centroids m(A) := {m(As), s \u2208 [k]}. Fix C = {cr, r \u2208 [k]}, we let V (cr) denote the Voronoi cell belonging to centroid cr, i.e., {x \u2208 Rd, \u2016x\u2212 cr\u2016 \u2264 \u2016x\u2212 cs\u2016,\u2200s 6= r}, and we use V (C) to denote the Voronoi diagram induced by a set of centroids C. Fix C with k-centroids, we denote its k-means cost with respect to a k-clustering A by \u03c6(C,A) := \u2211k r=1 \u2211 x\u2208Ar \u2016x\u2212 cr\u2016\n2 (or simply \u03c6(C) when A is induced by the Voronoi diagram of C). We let \u03c6\u2217 := \u03c6(C\u2217), \u03c6t := \u03c6(Ct), and let \u03c6\u2217r (\u03c6 t r) denote the k-means cost of cluster A \u2217 r (A t r). \u03c6\nopt and \u03c6optr are similarly defined for the cost of an optimal k-means clustering. We use \u03c0 : [k]\u2192 [k] to denote permutations between two sets of the same cardinality.\n2 A framework for tracking batch k-means in the solution space\nFirst, we develop insights for batch k-means to facilitate our analysis of stochastic k-means.2 Batch k-means initializes the position of k centroids C0 via a seeding algorithm. Then \u2200t \u2265 1,\n1. Obtain At by assigning each x to its closest centroid (Voronoi cell). 2. For all Atr that is not empty, obtain c t r := m(A t r).\nThe algorithm alternates between two solution spaces: the continuous space of all k-centroids, which we denote by {C}, and the finite set of all k-clusterings, which we denote by {A}. One problem with k-means is it may produce degenerate solutions: if the solution Ct has k centroids, it is possible that data points are mapped to only k\u2032 < k centroids. To handle degenerate cases, starting with |C0| = k, we consider an enlarged clustering space {A}[k], which is the union of all k\u2032-clusterings with 1 \u2264 k\u2032 \u2264 k. Our key idea is that {C} can be partitioned into equivalence classes by the clustering they induce on X , and the algorithm stops if and only if two consecutive iterations stay within the same equivalence class in {C}. Specifically, for any C, let v(C) := V (C) \u2229X \u2208 {A}[k] denote the clustering they induce on X via their Voronoi diagram. Then C1, C2 are in the same equivalence class in {C} if v(C1) = v(C2) = A. In lieu of this construction, the algorithm goes from {C} to {A}[k] via mapping v : {C} \u2192 {A}[k]; it goes from {A}[k] to {C} via the mean operation m : {A}[k] \u2192 {C}. Figure 1b illustrates how batch k-means alternates between two solution spaces until convergence. We use the pre-image v\u22121(A) \u2208 {C} to denote the equivalence class induced by clustering A. When batch k-means visits a clustering At, if m(At) /\u2208 v\u22121(At), the algorithms jumps to another clustering At+1. If m(At) \u2208 v\u22121(At), the algorithm stops because At+1 = At and m(At+1) = m(At). We thus formalize the idea of \u201clocal optima\u201d of batch k-means as below. Definition 1 (Stationary clusterings). We call A\u2217 a stationary clustering of X , if m(A\u2217) \u2208 Cl(v\u22121(A\u2217)). We let {A\u2217}[k] \u2282 {A}[k] denote the set of all stationary clusterings of X with number of clusters k\u2032 \u2208 [k].\n2Due to space limit, we move the detailed construction of the framework to Appendix A, where more formal definitions and omitted discussions of issues such as degenerate cases can be found.\nAlgorithm 1 Stochastic k-means\nInput: dataset X , number of clusters k, mini-batch size m, learning rate \u03b7tr, r \u2208 [k], convergence_criterion Seeding: Apply seeding algorithm T on X and obtain seeds C0 = {c01, . . . , c0k}; repeat\nAt iteration t (t \u2265 1), obtain sample St \u2282 X of size m uniformly at random with replacement; set count n\u0302tr \u2190 0 and set Str \u2190 \u2205, \u2200r \u2208 [k] for s \u2208 St do\nFind I(s) s.t. cI(s) = C(s) StI(s) \u2190 S t I(s) \u222a s; n\u0302 t I(s) \u2190 n\u0302 t I(s) + 1\nend for for ct\u22121r \u2208 Ct\u22121 do\nif n\u0302tr 6= 0 then ctr \u2190 (1\u2212 \u03b7tr)ct\u22121r + \u03b7tr c\u0302tr with c\u0302tr := \u2211 s\u2208Str s n\u0302tr end if\nend for until convergence_criterion is satisfied\nThe operator Cl(\u00b7) denotes the \u201cclosure\u201d of an equivalence class v\u22121(A\u2217), which includes its \u201cboundary points\u201d, a set of centroids that induces ambiguous clusterings (this happens when there is a data point on the bisector of two centroids in a solution; see Appendix A for details). For each A\u2217, we define a matching centroidal solution C\u2217.\nDefinition 2 (Stationary points). For a stationary clustering A\u2217 with k\u2032 clusters, we define C\u2217 = {c\u2217r , r \u2208 [k\u2032]} to be a stationary point corresponding to A\u2217, so that \u2200A\u2217r \u2208 A\u2217, c\u2217r := m(A\u2217r). We let {C\u2217}[k] denote the corresponding set of all stationary points of X with k\u2032 \u2208 [k].\nAs preluded in the introduction, defining a distance measure on {C} is important to our subsequent analysis; For C \u2032 and C, we let \u2206(C \u2032, C) := min\u03c0:[k]\u2192[k] \u2211 r nr\u2016c\u2032\u03c0(r) \u2212 cr\u2016\n2, where nr = |Ar|. This distance is asymmetric and non-negative, and evaluates to zero if and only if two sets of centroids coincide. In addition, if C\u2217 is a stationary point, then for any solution C, \u2206(C,C\u2217) upper bounds the difference of k-means objective, \u03c6(C)\u2212 \u03c6(C\u2217) (Lemma 11).\n2.1 Stochastic k-means\nAlgorithm 1 and stochastic k-means [5, 16] are equivalent up to the choice of learning rate and sampling scheme (the proof of equivalence is in Appendix A). In [5, 16], the per-cluster learning rate is chosen as \u03b7tr := n\u0302tr\u2211 i\u2264t n\u0302 i r ; in our analysis, we choose a flat learning rate \u03b7t = c \u2032 to+t for all clusters, where c\u2032, to > 0 are some fixed constants (empirically, no obvious differences are observed; see Section 3.1 for more discussion and Section 4 for empirical comparison). Unlike a usual gradientbased algorithm on a continuous domain, the discrete nature of the k-means problem causes major differences when stochastic approximation is applied.\nFirst, for batch k-means, at every iteration t, Ct is chosen as the means, m(At), of the current k-clustering At. Since {A}[k] is finite and m(A) is unique for a fixed A, the set of \u201clegal moves\u201d of batch k-means is finite, while {C} is continuous. In stochastic k-means, however, Ct can be any member of {C} due to both stochasticity and the learning rate. As such, its effective solution space on {C} is continuous and thus infinite. Our framework enables us to impose a finite structure on {C} by mapping points in {C} to points in {A}[k]. Second, in stochastic k-means, centroids are usually updated asynchronously, especially when the mini-batch size is small compared to k; in the extreme case of online k-means, centroids are updated one at a time. Since the positions of centroids implicitly determine the clustering assignment, asynchronous k-means updates will lead to a different solution path than fully synchronized ones, even if we ignore the noise introduced by sampling and choose the learning rate to be 1. Due to asynchronicity, it is also hard to detect when stochastic k-means produces a degenerate solution, since centroids may fail to be updated for a long time due to sampling. In practice, implementation such as\nscikit-learn re-locate a centroid randomly if it fails to be updated for too long. In Algorithm 1, we do not allow re-location of centroids and our analysis subsumes the degenerate cases.\nWhen analyzing Algorithm 1, we let \u2126 denote the sample space of all outcomes (C0, C1, . . . , Ct, . . .) and Ft the natural filtration of the stochastic process up to t (\u2126 is also used in our statements of Theorems 1 and 2 as the Big-Omega). We let ptr(m) := Pr{ct\u22121r is updated at t with mini-batch size m}."}, {"heading": "3 Main result", "text": "With two weak assumptions on the properties of stationary points of a datasetX , our first result shows stochastic k-means has an overall O( 1t ) convergence rate to a stationary point of batch k-means, regardless of initialization.\n(A0) We assume all stationary points are non-boundary points, i.e., \u2200A\u2217 \u2208 {A\u2217}[k], m(A\u2217) \u2208 v\u22121(A\u2217) (By Lemma 5, under this assumption, \u2203rmin > 0 such that all stationary points are (rmin, 0)-stable). (A1) \u2200t > 0, we assume there is an upper bound B on\nmax{E[ \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 c\u0302t+1r \u20162 + \u03c6t|Ft], \u2211 r n\u2217r\u3008ct\u22121r \u2212 c\u2217r , c\u0302tr \u2212 E[c\u0302tr|Ft\u22121]\u3009, \u2211 r n\u2217r\u2016c\u0302tr \u2212 c\u2217r\u20162}\nwhere n\u2217r , c \u2217 r are the cardinality and centroid of the r-th cluster in a stationary clustering A \u2217.\nTheorem 1. Assume (A0)(A1) holds. Fix any \u03b4 > 0, if we run Algorithm 1 with learning rate \u03b7t = c \u2032\nt+to such that for all t \u2265 1,\nc\u2032 > 1 2p+min(m)(1\u2212 \u221a 1\u2212 rmin\u03c6opt2\u03c6t ) and to = \u2126\n{ [\n(c\u2032)2\n\u03c1(m)2(2c\u2032 \u2212 1) ln\n1 \u03b4 ] 2 2c\u2032\u22121\n}\nwhere p+min(m) := min r\u2208[k];\nptr(m)>0\nptr(m), \u03c1(m) := 1\u2212 (1\u2212 p\u2217min)m, and p\u2217min := min A\u2217\u2217\u2208{A\u2217}[k] min r n\u2217\u2217r n\nThen starting from any initial set of k-centroids C0, Let G denote the event {\u2203T, \u2203A\u2217\u2217 \u2208 {A\u2217}[k] s.t. At = A\u2217\u2217,\u2200t \u2265 T}. Then Pr(G) \u2265 1 \u2212 \u03b4, and there exists events parametrized by A\u2217\u2217, denoted by Go(A\u2217\u2217), such that Pr{\u222aA\u2217\u2217\u2208{A\u2217}[k]Go(A\u2217\u2217)} \u2265 1\u2212 \u03b4. And for any Go(A\u2217\u2217), we have \u2200t \u2265 1, E{\u03c6t \u2212 \u03c6\u2217\u2217|Go(A\u2217\u2217)} = O( 1t ), where \u03c6 \u2217\u2217 := \u03c6(m(A\u2217\u2217)).\nThis result guarantees the expectedO( 1t ) convergence of Algorithm 1 towards a stationary point under a general condition. However, it does not guarantee Algorithm 1 converges to the same stationary clustering as its batch counterpart, even with the same initialization. Moreover, it is even possible that the final solution becomes degenerate.\nAlgorithm parameters in the convergence bound The exact convergence bound of Theorem 1, which we hide in the Big-O notation, reveals dependence of the convergence on the algorithm\u2019s parameters. When c\u2032 is sufficiently large, the exact bound we obtain is likely dominated by a2maxB \u03b2\u22121 ( t0+2 t0+1 )\u03b2+1 1t0+t+1 where \u03b2 = 2c \u2032p+min(m)(1\u2212 \u221a \u03c6\u0303t \u03c6t ) and amax := c\u2032\n\u03c1(m) . The bound becomes tighter when c\u2032 decreases. Since the 1t -order convergence requires a sufficiently large c\n\u2032, our analysis suggests we should choose c\u2032 to be neither too large nor too small. The bound also becomes tighter when p+min(m) and \u03c1(m) becomes closer to 1. Both depend on m; less obviously, they also depend on the number of non-degenerate clusters. Since ptr = 1\u2212(1\u2212 ntr n )\nm, which equals zero if and only if ntr = 0, p + min(m) = 1\u2212 (1\u2212minr,t;ptr>0 p t r(1)) m < 1\u2212 (1\u2212 1k\u2032 ) m, where k\u2032 is the smallest number of non-degenerate centroids in a run. The similar holds for \u03c1(m). This suggests the convergence rate may be slower with larger k\u2032, which depends on the initial number of clusters k, and smaller m. For experimental results of the effect of algorithm parameters on convergence, see Section 4. A detailed explanation on the exact bound is included in Appendix B. The general convergence result in Theorem 1 is applicable to any seeding algorithm, including random sampling, which is probably the\nAlgorithm 2 Buckshot seeding [17] {\u03bdi, i \u2208 [mo]} \u2190 sample mo points from X uniformly at random with replacement {S1, . . . , Sk} \u2190run Single-Linkage on {\u03bdi, i \u2208 [mo]} until there are only k connected components left C0 = {\u03bd\u2217r , r \u2208 [k]} \u2190 take the mean of the points in each connected component Sr, r \u2208 [k]\nmost scalable seeding heuristic one can use. However, it does not provide performance guarantee with respect to the k-means objective. We next show that stochastic k-means initialized by Algorithm 2 converges to a global optimum of k-means objective at rate O( 1t ), under additional geometric assumptions on the dataset. The major advantage of Algorithm 2 over other choices of initialization, such as the k-means++ [1] or random sampling, is that its running time is independent of the data size while providing seeding guarantee. When using it with stochastic k-means, both the seeding and the update phases are independent of the data size, making the overall algorithm highly scalable.\nLet (Aopt, Copt) denote an optimal k-means clustering of X . We assume, similar to [11], that the means of each pair of clusters are well-separated and that the points from the two clusters are separated by a \u201cmargin\u201d, that is, \u2200x \u2208 Aoptr \u222aAopts , the distance from x to the bisector of coptr and copts is lower bounded. Formally, let x\u0304 denote the projection of x onto the line joining c opt r , c opt s , the margin between the two clusters is defined as \u2206rs := minx\u2208Aoptr \u222aAopts |\u2016x\u0304\u2212 c opt r \u2016 \u2212 \u2016x\u0304\u2212 copts \u2016|. In addition, we require that the size of the smallest cluster is not too small compared to the data size. The geometric assumptions are formally defined as below. (B1) Mean separation: \u2200r \u2208 [k], s 6= r, \u2016m(Aoptr )\u2212m(Aopts )\u2016 \u2265 f(\u03b1) \u221a \u03c6opt( 1\u221a\nnoptr + 1\u221a nopts ),\nwith f(\u03b1) > max{642, 5\u03b1+5256\u03b1 ,maxr\u2208[k],s 6=r noptr nopts } for some \u03b1 \u2208 (0, 1) that is sufficiently small. (B2) Existence of margin: \u2200r \u2208 [k], s 6= r, \u2206rs \u2265 \u03b3\u2016m(Aoptr )\u2212m(Aopts )\u2016, for some \u03b3 > 8 \u221a 2\u221a f .\n(B3) Cluster balance: pmin \u2265 \u03b3162f(\u03b1) + \u221a \u03b1, where pmin := minr\u2208[k] noptr n .\nTheorem 2. Assume (B1)(B2) (B3) hold for an optimal k-means clustering Aopt. Fix any \u03be > 0, if f(\u03b1) in addition satisfies f(\u03b1) \u2265 5 \u221a 1\n2wmin ln( 2\u03bepmin ln 2k \u03be ), and we choose mo in Algorithm 2 such\nthat log 2k\u03be pmin < mo < \u03be 2 exp{2( f(\u03b1) 4 \u2212 1)\n2w2min}. And if we run Algorithm 1 initialized by Algorithm 2 and choosemini-batch size m and learning rate of the form \u03b7t = c \u2032\nt+to so that\nm > 1 and c\u2032 > 1\n2[1\u2212 \u221a \u03b1\u2212 (1\u2212 \u221a \u03b1)m]\nand\nto = \u2126\n{ [\n(c\u2032)2\n\u03c1(m)2(2c\u2032(\u03c1(m)\u2212 \u221a \u03b1)\u2212 1)\nln 1\n\u03b4 ]\n2 2c\u2032(\u03c1(m)\u2212 \u221a \u03b1)\u22121 } where \u03c1(m) := 1\u2212 [1\u2212 (pmin \u2212 \u03b3162f(\u03b1) )] m. Then \u2200t \u2265 1, there exists event Gt \u2282 \u2126 s.t.\nPr{Gt} \u2265 (1\u2212 \u03b4)(1\u2212 \u03be) and E[\u03c6t|Gt]\u2212 \u03c6opt \u2264 E[\u2206(Ct, Copt)|Gt] = O( 1\nt )\nInterestingly, we cannot provide performance guarantee for online k-means (m = 1) in Theorem 2. Our intuition is, instead of allowing stochastic k-means to converge to any stationary point as in Theorem 1, it studies convergence to a fixed optimal solution; a largerm provides more stability to the algorithm and prevents it from straying away from the target solution. The proposed clustering scheme is reminiscent to the Buckshot algorithm [6], widely used in the domain of document clustering. Readers may wonder how can the algorithm approach \u03c6opt for an NP-hard problem. The reason is that our geometric assumptions softens the problem. In this case, Algorithm 1 converges to the same (optimal) solution as its batch counterpart, provided the same initialization."}, {"heading": "3.1 Related work and proof outline", "text": "Our major source of inspiration comes from recent advances in non-convex stochastic optimization for unsupervised learning problems [8, 3]. [8] studies the convergence of stochastic gradient descent (SGD) for the tensor decomposition problem, which amounts to finding a local optimum of a nonconvex objective function composed exclusively of saddle points and local optima. Inspired by their analysis framework, we divide our analysis of Algorithm 1 into two phases, that of global convergence and local convergence, indicated by the distance from the current solution to stationary points, \u2206(Ct, C\u2217). We use \u2206t := \u2206(Ct, C\u2217) as a shorthand.\nSignificant decrease in k-means objective when \u2206t is large In the global convergence phase, the algorithm is not close to any stationary point, i.e., \u2200C\u2217 \u2208 {C\u2217}[k], \u2206(Ct, C\u2217) is lower bounded. We first prove a black-box result showing that on non-stationary points, stochastic k-means decreases the k-means objective in expectation at every iteration.\nLemma 1. Suppose \u2200r \u2208 [k], \u03b7tr \u2264 \u03b7tmax < 1 w.p. 1. Then, E[\u03c6t+1 \u2212 \u03c6t|Ft] \u2264 \u22122 minr,t;pt+1r (m)>0[\u03b7 t+1 r p t+1 r (m)]\u03c6 t(1\u2212 \u221a \u03c6\u0303t \u03c6t ) + (\u03b7 t+1 max) 2E[ \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 c\u0302 t+1 r \u20162 + \u03c6t|Ft],\nwhere \u03c6\u0303t := \u2211 r \u2211 x\u2208At+1r \u2016x\u2212m(A t+1 r )\u20162.\nBy Lemma 1, the term minr,t;pt+1r (m)>0[\u03b7 t+1 r p t+1 r (m)]\u03c6\nt(1\u2212 \u221a \u03c6\u0303t\n\u03c6t ) lower bounds the per iteration\ndrop in k-means objective. Since minr;pt+1r (m)>0 p t+1 r (m) \u2265 1n , \u03c6 t \u2265 \u03c6opt, and 1\u2212 \u03c6\u0303t\u03c6t = \u03c6t\u2212\u03c6\u0303t \u03c6t = \u2206(Ct,Ct+1) \u03c6t \u2265 0 (the second equality is by Lemma 12), the drop is always lower bounded by zero. We show that if \u2206(Ct, C\u2217) is bounded away from zero (for any stationary point C\u2217), then so is \u2206(Ct, Ct+1): the rough idea is, in case Ct+1 is a non-stationary point, Ct and Ct+1 must belong to different equivalence classes, as discussed in Section 2, and their distance must be lower bounded by Lemma 4; otherwise, Ct+1 is a stationary point, and by our assumption their distance is lower\nbounded. Thus, \u2206(C t,Ct+1) \u03c6t is lower bounded by a positive constant, and so is 1\u2212\n\u221a \u03c6\u0303t\n\u03c6t . Since we chose \u03b7t := \u0398( 1t ), the expected per iteration drop of cost is of order \u2126( 1 t ), which forms a divergent series; after a sufficient number of iterations the expected drop can be arbitrarily large. We conclude that \u2206(Ct, C\u2217) cannot be bounded away from zero asymptotically, since the k-means cost of any clustering is positive. This result is presented in Lemma 2.\nBefore proceeding, note the drop increases with minr,t;pt+1r (m)>0[\u03b7 t+1 r p t+1 r (m)]. This means if we can set a cluster-dependent learning rate that adapts to pt+1r (m), the drop could be larger than choosing a flat rate, as we do in our analysis. The learning rate in [16, 5], where \u03b7tr := n\u0302tr\u2211 i\u2264t n\u0302 i r\n, is intuitively making this adaptation: in the case when the clustering assignment does not change between iterations, it can be seen that E\u03b7tr \u2248 1tptr(m) , so the effective learning rate \u03b7 t rp t r(m) is balanced for different clusters and is roughly 1t . However, in practice, the clustering assignment changes when the centroids are updated, and it is hard to analyze this adaptive learning rate due to its random dependence on all previous sampling outcomes.\nLemma 2. Assume (A1) holds. If we run Algorithm 1 on X with \u03b7t = at+to , a \u2265 1 2p+min(m) , with p+min(m) as defined in Theorem 1, and to > 1, and choose any initial set of k centroids C 0. Then for any \u03b4 > 0, \u2203t s.t. \u2206(Ct, C\u2217) \u2264 \u03b4 with C\u2217 := m(A\u2217) for some A\u2217 \u2208 {A\u2217}[k].\nLemma 2 suggests that, starting from any initial point C0 in {C}, the algorithm always approaches a stationary point asymptotically, ending its global convergence phase after a finite number of iterations. We next examine its local behavior around stationary points.\nLocal convergence to stationary points when \u2206t is small To obtain local convergence result, we first define \u201clocal attractors\u201d and \u201cbasin of attraction\u201d for batch k-means; the natural candidate for the former is the set of stationary points; a basin of attraction is a subset of the solution space so that once batch k-means enters it, it will not escape.\nDefinition 3. We call C\u2217 a (b0, \u03b1)-stable stationary point of batch k-means if it is a stationary point and for any clustering C such that \u2206(C,C\u2217) \u2264 b\u2032\u03c6\u2217, b\u2032 \u2264 b0, the Voronoi partition induced by\n{cr}, denoted by {Ar}, satisfies maxr |A\u03c0(r)4A\u2217r |\nn\u2217r \u2264 b5b+4(1+\u03c6(C)/\u03c6\u2217) , where \u03c0 is the permutation\nachieving \u2206(C,C\u2217), with b \u2264 \u03b1b\u2032 for some \u03b1 \u2208 [0, 1).\nWe can view b0 as the radius of the basin and \u03b1 the degree of the \u201cslope\u201d leading to the attractor. A key lemma to our analysis characterizes the iteration-wise local convergence property of batch k-means around stable stationary points, whose convergence rate depends on the slope \u03b1. Lemma 3. Let C\u2217 be a (b0, \u03b1)-stable stationary point. For any C such that \u2206(C,C\u2217) \u2264 b\u2032\u03c6\u2217, b\u2032 \u2264 b0, apply one step of batch k-means update on C results in a new solution C1 such that \u2206(C1, C\u2217) \u2264 \u03b1b\u2032\u03c6\u2217.\nLemma 3 resembles the standard iteration-wise convergence statement in SGD analysis, typically via convexity or smoothness of a function [15]. Here, we have neither at our dispense (we do not even have a gradient). Instead, our analysis relies on the geometric property of Voronoi diagram and the mean operation used in a k-means iteration, similar to those in recent works on batch k-means [11, 2, 17]. Although this lemma applies only to batch k-means, our hope is that stochastic k-means has similar iteration-wise convergence behavior in expectation even in the presence of noise.\nThe difficulty here is, due to non-convexity, the convergence result only holds within a local basin of attraction: if the algorithm\u2019s solution is driven off the current basin of attraction by stochastic noise at any iteration, it may converge to a different attractor, causing trouble to our analysis. To deal with this, we exploit probability tools developed in [3]. [3] studies the convergence of stochastic PCA algorithms, where the objective function is the non-convex Rayleigh quotient, which has a plateau-like component. The tools developed there were used to show that stochastic PCA gradually escapes the plateau. Here, we adapted their analysis to show Algorithm 1 stays within a basin of attraction with high probability, and converges to the attractor at rate O( 1t ). We define a nested sequence of events \u2126i \u2282 \u2126: \u2126 \u2283 \u21261 \u2283 \u00b7 \u00b7 \u00b7 \u2283 \u2126i \u2283 . . . where \u2126i := {\u2206t \u2264 b0\u03c6\u2217,\u2200t < i}. Then if Algorithm 1 is within the basin of attraction of a stable stationary point at time t, the event that it escapes this local basin of attraction is contained in the event \u222at\u2265i+1\u2126t\u22121 \\ \u2126t. We upper bound the probability of this bad event (Proposition 1) using techniques that derive tight concentration inequality via moment generating functions from [3], which in turn implies a lower bound on the probability of \u2126t, t \u2265 i. Then conditioning on \u2126t and adapting Lemma 3 from batch to stochastic k-means proves the expected local convergence rate of O( 1t ). Theorem 3. Assume (A1) holds. Let C\u2217 be a (b0, \u03b1)-stable stationary point, and let \u2206i = b\u03c6\u2217 for some b \u2264 12b0 at some iteration i in Algorithm 1. Let a t := maxr p t r(m) mins pts(m) . Suppose we set c\u2032 and m sufficiently large so that\n\u03b2 := 2c\u2032min r,t ptr(m)(1\u2212max t at \u221a \u03b1)) > 1\nFix any 0 < \u03b4 \u2264 1e , and let amax := c\u2032 minr,t ptr(m) . If in addition,\nt0 \u2265 max{( 16a2maxB\n\u2206i )\n1 \u03b2\u22121 , [\n48a2maxB 2\n(\u03b2 \u2212 1)(\u2206i)2 ln\n1 \u03b4 ] 2 \u03b2\u22121 , (ln 1 \u03b4 ) 2 \u03b2\u22121 }\nThen for all t > i,\nPr(\u2126t) \u2265 1\u2212 \u03b4 and E[\u2206t|\u2126t] \u2264 t0 + i+ 1\nt0 + t \u2206i +\na2maxB\n\u03b2 \u2212 1 ( t0 + i+ 2 t0 + i+ 1 )\u03b2+1\n1\nt0 + t+ 1\nNote how, in contrast to Theorem 1, the local convergence result does not allow degeneracy here, by implicitly requiring that minr,t ptr(m) > 0. This is reasonable, since a degenerate set of k centroids cannot converge to a fixed stationary point with k centroids.\nBuilding on the ideas introduced above, we present the key ingredients of our main theorems.\nProof idea of Theorem 1 To prove Theorem 1, we first show that all stationary points satisfying (A0) must be (rmin, 0)-stable for some rmin > 0 (Lemma 5). Then we apply results from both the global and local convergence analysis of Algorithm 1. We define the global convergence phase as when \u2206(Ct, C\u2217\u2217) > 12rmin\u03c6 \u2217, \u2200C\u2217\u2217 \u2208 {C\u2217}[k]. As discussed, since \u2206(Ct, C\u2217\u2217) is bounded away\nfrom zero, the per-iteration drop of k-means cost is of order \u2126(\u03b7t), thus we get the expected O( 1t ) convergence rate. Lemma 2 suggests that this phase will eventually end and at some iteration T , \u2206(CT , C\u2217\u2217) \u2264 12rmin\u03c6\n\u2217\u2217 must hold for some stationary point C\u2217\u2217. The first time when this happens signals the beginning of the local convergence phase: at this point, CT is in the basin of attraction of C\u2217\u2217 since C\u2217\u2217 is (rmin, 0)-stable. Thus, applying Theorem 3 implies that in this phase the algorithm converges to C\u2217\u2217 locally at rate O( 1t ). Hence, the overall global convergence rate is O( 1 t ).\nProof idea of Theorem 2 Here we only apply the local convergence result. The key step is to show that our clusterability assumption on the dataset, (B1) and (B2), implies that its optimal k-means solution, Copt, is a (b0, \u03b1)-stable stationary point with a sufficiently large b0 (Proposition 2). Then we adapt results from [17] to show that the seeds obtained from Algorithm 2 are within the basin of attraction of Copt with high probability (Lemmas 13). Using the other geometric assumption, (B3), we apply Theorem 3 to show an O( 1t ) convergence to C opt with high probability."}, {"heading": "4 Experiments", "text": "To verify the O( 1t ) global convergence rate of Theorem 1, we run stochastic k-means with varying learning rate, mini-batch size, and k on RCV1 [12]. The dataset is relatively large in size: it has manually categorized 804414 newswire stories with 103 topics, where each story is a 47236-dimensional sparse vector; it was used in [16] for empirical evaluation of mini-batch k-means. We used Python and its scikit-learn package [14] for our experiments, which has stochastic k-means implemented. We disabled centroid relocation and modified their source code to allow a user-defined learning rate (their learning rate is fixed as \u03b7tr := n\u0302tr\u2211 i\u2264t n\u0302 i r , as in [5, 16], which we refer to as BBS-rate).\nFigure 2 shows the convergence in k-means cost of stochastic k-means algorithm over 100 iterations for varying m and k; fix each pair (m, k), we initialize Algorithm 1 with a same set of k random centroids and run stochastic k-means with varying learning rate parameters (c\u2032, to), and we average the performance of each learning rate setup over 5 runs to obtain the original convergence plot. Figure 2b is an example of a convergence plot before transformation. The dashed black line in each log-log figure is \u03c6 0\u2212\u03c6min\nt , a function of order \u0398( 1 t ). To compare the performance of stochastic k-means with\nthis baseline, we first transform the original \u03c6t vs t plot to that of \u03c6t \u2212 \u03c6min vs t. By Theorem 1, E[\u03c6t \u2212 \u03c6\u2217\u2217|G(A\u2217\u2217)] = O( 1t ), so we expect the slope of the log-log plot of \u03c6\nt \u2212 \u03c6\u2217\u2217 vs t to be at least as large as that of \u0398( 1t ). Although we do not know the exact cost of the stationary point, since the algorithm has reached a stable phase over 100 iterations, as illustrated by Figure 2b, we simply use \u03c6min as an estimate of \u03c6\u2217\u2217. Most log-log convergence graphs fluctuate around a line with a slope at least as steep as that of \u0398( 1t ), and do not seem to be sensitive to the choice of learning rate in our experiment. Note in some plots, such as Figure 2a, the initial phase is flat. This is because we\nforce the plot to start at \u03c60 \u2212 \u03c6min instead of their true intercept on the y-axis. BBS-rate exhibits similar behavior to our flat learning rates. Our experiment suggests the convergence rate of stochastic k-means may be sensitive to the ratio mk ; for largerm or smaller k, faster and more stable convergence is observed."}, {"heading": "5 Appendix A: complete version of Section 2", "text": "To facilitate our analysis of mini-batch k-means, we build a framework to track the solution path produced by batch k-means; it alternates between two solutions spaces: the space of all k-centroids, which we denote by {C}, and the space of all k-clusterings, which we denote by {A}. Note the latter is a finite set. Throughout the paper, we use \u03c0 : [k]\u2192 [k] to denote permutations between two sets of the same cardinality.\nDegenerate cases One problem with batch k-means algorithm is it may produce degenerate solutions: if the solution Ct has k centroids, it is possible that data points are mapped to only k\u2032 < k centroids. To handle degenerate cases, starting with |C0| = k, we will consider the enlarged clustering space {A}[k], the union of all k\u2032-clusterings, which we denote by {A}k\u2032 , with 1 \u2264 k\u2032 \u2264 k.\nPartitioning {C} via {A} Our first observation is that most part of {C} (with exception discussed below) can be partitioned into equivalence classes by the clustering they induce on X; for any C, let v(C) := V (C) \u2229X \u2208 {A}[k] to formally denote the clustering they induce via their Voronoi diagram. For most points in {C}, there is only one such clustering so v(C) is uniquely determined. We define v\u22121(A) as the set of points inducing a unique k\u2032-clustering A, with k\u2032 \u2264 k. Then we let C1, C2 be in the same partition in {C} if v(C1), v(C2) are both unique and v(C1) = v(C2).\nAmbiguous cases However, it is not always clear which partition C belongs to: if \u2203x \u2208 X such that \u2016x\u2212 c1(x)\u2016 = \u2016x\u2212 c2(x)\u2016, where c1(x), c2(x) denote the centroids in C that are closest and second closest to x, x can be clustered into either the cluster of centroid c1(x) or that of c2(x). Centroids with this property can induce two or more clusterings due to ambiguity of Voronoi partition. Intuitively, they are at the boundary of v\u22121(Ai), for some clusterings Ai. We formalize v\u22121(A) and boundary points as below.\nDefinition 4 (members of v\u22121(A)). Fix a clustering A = {A1, . . . , Ak}, we define C \u2208 v\u22121(A) if it contains a matching set of centroids and there exists a permutation of [k] s.t. \u2200x \u2208 Ar,\u2200r \u2208 [k],\n\u2016x\u2212 c\u03c0(r)\u2016 < \u2016x\u2212 c\u03c0(s)\u2016, \u2200s 6= r\nWe say C is a boundary point of v\u22121(A), if \u2200r \u2208 [k],\u2200x \u2208 Ar ,\n\u2016x\u2212 c\u03c0(r)\u2016 \u2264 \u2016x\u2212 c\u03c0(s)\u2016, \u2200s 6= r\nwith equality attained for at least one data point x. Let B(v\u22121(A)) denote the set of all boundary points of v\u22121(A). Let Cl(v\u22121(A)) := v\u22121(A) \u222aB(v\u22121(A)) denote the closure of v\u22121(A).\nNote that in the case C has k\u2032 > k centroids, C \u2208 v\u22121(A) implies all centroids in C \\ {c\u03c0(1), . . . , c\u03c0(k)} are degenerate.\nRepresenting k-means updates For now, let C\u2217 denote a \u201clocal minimum\u201d of batch k-means and suppose C\u2217 is not a boundary point. Let A\u2217 := v\u22121(C\u2217). One run of batch k-means can be represented as\nC0 v(C0)\u2192 A1 m(A 1)\u2192 C1 \u2192 . . . m(A t\u22121)\u2192 Ct\u22121 v(C t\u22121)\u2192 At . . .\u2192 A\u2217 \u2192 C\u2217\nFigure 1b illustrates how the algorithm alternates between two solution spaces. When batch k-means visits a clustering At, if m(At) /\u2208 Cl(v\u22121(At)), the algorithms jumps to another clustering At+1. Otherwise, if m(At) \u2208 v\u22121(At), the algorithm stops because At+1 = At and m(At+1) = m(At). In the special case where m(At) is a boundary stationary point, since the algorithm arbitrarily breaks the tie, then it will continue to operate if the new clustering is chosen such that At+1 6= At, or stops if At+1 = At. In practice, it is unlikely to encounter a boundary point due to perturbations in the computing system, and regardless, a sufficient condition for batch k-means to converge at the t-th iteration is m(At) /\u2208 Cl(v\u22121(At)). This motivates us to formalize \u201clocal optima\u201d of batch k-means as below.\nDefinition 5 (Stationary clusterings). We call A\u2217 a stationary clustering of X , if m(A\u2217) \u2208 Cl(v\u22121(A\u2217)). We let {A\u2217}[k] \u2282 {A}[k] denote the set of all stationary clusterings of X with number of clusters k\u2032 \u2208 [k]. Definition 6 (Stationary points). For a stationary clustering A\u2217 with k\u2032 clusters, we define C\u2217 = {c\u2217r , r \u2208 [k\u2032]} to be a stationary point corresponding to A\u2217, so that \u2200A\u2217r \u2208 A\u2217, c\u2217r := m(A\u2217r). We let {C\u2217}[k] denote the corresponding set of all stationary points of X with k\u2032 \u2208 [k].\nNote that the correspondence between A\u2217 and C\u2217 is one to one. And by our definition, stationary points cannot be degenerate.\nDistance function from C \u2032 to C Fix a clustering A with its induced k-centroids C := m(A), and another set of k\u2032-centroids C\u2032 (k\u2032 \u2265 k) with its induced clustering A\u2032, if |A\u2032| = |A| = k (this means if k\u2032 > k, then C\u2032 has at least one degenerate centroid), then we can pair the subset of non-degenerate k centroids in C\u2032 with\nthose in C, and ignore the degenerate centroids. Under this condition, we define their centroidal distance as \u2206(C\u2032, C) := min\u03c0:[k]\u2192[k] \u2211 r nr\u2016c \u2032 \u03c0(r) \u2212 cr\u20162. Sometimes we use \u2206t := \u2206(Ct, C\u2217) as a shorthand, when we want to measure the distance between Ct from Algorithm 1 and a fixed stationary point C\u2217. This distance is asymmetric and non-negative, and evaluates to zero if and only if two sets of centroids coincide. Using this distance function, we have a sufficient test on whether a clustering A is stationary.\nNote that the permutation \u03c0\u2217 := arg min\u03c0:[k]\u2192[k] \u2211 r nr\u2016c \u2032 \u03c0(r) \u2212 cr\u20162 may not be unique, unless \u2206(C\u2032, C) is small. In the rest of our paper when we refer to such a permutation that defines \u2206(C\u2032, C) in our proofs, it can be any permutation that is a minimizer. The following lemma shows \u2206 distance can be used as a test to determine whether two sets of centroids induces the same clustering (belong to the same partition).\nLemma 4. Fix a clustering A = {A1, . . . , Ak}, let C := m(A) and C\u2032 \u2208 v\u22121(A), then \u2203\u03b4 > 0 such that the following statement holds:\n\u2206(C\u2032, C) < \u03b4 =\u21d2 C \u2208 v\u22121(A) (1)\nProof. Since C = m(A), |C| = |A| = k. Since C\u2032 \u2208 v\u22121(A), by Definition 4, |C\u2032| = k\u2032 \u2265 k, and there is a permutation \u03c0o of [k] and some subset {c\u2032\u03c0o(1), . . . , c \u2032 \u03c0o(k) } \u2282 C\u2032 such that \u2200x \u2208 Ar,\u2200r \u2208 [k],\n\u2016x\u2212 c\u2032\u03c0o(r)\u2016 < \u2016x\u2212 c \u2032 \u03c0o(s)\u2016,\u2200s 6= r\nNote also the prerequisite for defining a distance \u2206 from C\u2032 to C is satisfied, and we can choose \u03b4 > 0 s.t. \u2200x \u2208 Ar , \u2200r \u2208 [k], s 6= r,\n\u2016x\u2212 c\u2032\u03c0o(r)\u2016 \u2264 \u2016x\u2212 c \u2032 \u03c0o(s)\u2016 \u2212 2\n\u221a \u03b4\nand we require the equality is attained by at least one triple (x, r, s). Let \u03c0\u2217 be a permutation satisfying\n\u03c0\u2217 = arg min \u03c0 \u2211 r\u2208[k] nr\u2016c\u2032\u03c0(r) \u2212 cr\u20162\nLet \u03c0\u2032 := \u03c0\u22121\u2217 \u25e6 \u03c0o. We have \u2200r, s 6= r,\n\u2016x\u2212 c\u03c0\u2032(s)\u2016 \u2212 \u2016x\u2212 c\u03c0\u2032(r)\u2016 \u2265 \u2016x\u2212 c\u2032\u03c0o(s)\u2016 \u2212 \u2016c \u2032 \u03c0o(s) \u2212 c\u03c0\u2032(s)\u2016\n\u2212(\u2016x\u2212 c\u2032\u03c0o(r)\u2016+ \u2016c \u2032 \u03c0o(r) \u2212 c\u03c0\u2032(r)\u2016) > \u2016x\u2212 c \u2032 \u03c0o(s)\u2016 \u2212 \u2016x\u2212 c \u2032 \u03c0o(r)\u2016 \u2212 2\n\u221a \u03b4 \u2265 0\nwhere the second inequality is by the fact that\nmax r \u2016c\u2032\u03c0o(r) \u2212 c\u03c0\u2032(r)\u2016 2 \u2264 \u2206(C\u2032, C) < \u03b4\nSince \u03c0\u2032 is the composition of two permutations of [k], it is also a permutation of [k], and \u2200r, s 6= r, \u2016x \u2212 c\u03c0\u2032(r)\u2016 < \u2016x\u2212 c\u03c0\u2032(s)\u2016, so C \u2208 v\u22121(A) by Definition 4.\nRemark: For any two consecutive iterations of batch k-means, Ct, Ct+1, if we let At+1 be A, then Ct satisfies the condition for C\u2032 and Ct+1 for C. Applying Lemma 4, we can conclude that when \u2206(Ct, Ct+1) is sufficiently small, batch k-means converges. Lemma 4 also implies \u2203\u03b4 > 0, such that the contrapositive of statement (1) holds. This will be used in the proof of Lemma 2.\nEquivalence of Algorithm 1 to stochastic k-means Here, we formally show that Algorithm 1 with specific instantiation of sample size m and learning rates \u03b7tr is equivalent to online k-means [5] and mini-batch k-means [16].\nClaim 1. In Algorithm 1, if we set a counter for N\u0302 tr := \u2211t i=1 n\u0302 i r and if we set the learning rate \u03b7 t r :=\nn\u0302tr N\u0302tr\n, then provided the same random sampling scheme is used,\n1. When mini-batch size m = 1, the update of Algorithm 1 is equivalent to that described in [Section 3.3, [5]].\n2. When m > 1, the update of Algorithm 1 is equivalent to that described from line 3 to line 14 in [Algorithm 1, [16]] with mini-batch size m.\nProof. For the first claim, we first re-define the variables used in [Section 3.3, [5]]. We substitute index k in [5] with r used in Algorithm 1. For any iteration t, we define the equivalence of definitions: s \u2190 xi, ctr \u2190 wk, n\u0302tr \u2190 \u2206nk, N\u0302 tr \u2190 nk. According to the update rule in [5], \u2206nk = 1 if the sampled point xi is assigned to cluster with center wk. Therefore, the update of the k-th centroid according to online k-means in [5] is:\nwk \u2190 wk + 1\nnk (xi \u2212 wk)1{\u2206nk=1}\nUsing the re-defined variables, at iteration t, this is equivalent to\nctr = c t\u22121 r +\n1\nN\u0302 tr (s\u2212 ct\u22121r )1{n\u0302tr=1}\nNow the update defined by Algorithm 1 with m = 1 and \u03b7tr = n\u0302tr N\u0302tr is:\nctr = c t\u22121 r + \u03b7 t r(c\u0302 t r \u2212 ct\u22121r )1{n\u0302tr 6=0}\n= ct\u22121r + n\u0302tr\nN\u0302 tr (s\u2212 ct\u22121r )1{n\u0302tr=1} = c t\u22121 r +\n1\nN\u0302 tr (s\u2212 ct\u22121r )1{n\u0302tr=1}\nsince n\u0302tr can only take value from {0, 1}. This completes the first claim.\nFor the second claim, consider line 4 to line 14 in [Algorithm 1, [16]]. We substitute their index of time i with t in Algorithm 1. We define the equivalence of definitions: m\u2190 b, St \u2190M , s\u2190 x, ct\u22121I(s) \u2190 d[x], c t\u22121 r \u2190 c.\nAt iteration t, we let v[ct\u22121r ]t denote the value of counter v[c] upon completion of the loop from line 9 to line 14 for each center c, then N\u0302 tr \u2190 v[ct\u22121r ]t. Since according to Lemma 15, from line 9 to line 14, the updated centroid ctr after iteration t is\nctr = 1\nv[ct\u22121r ]t \u2211 s\u2208\u222ati=1S i r s = 1 N\u0302 tr \u2211 s\u2208\u222ati=1S i r s\nThis implies\nctr \u2212 ct\u22121r = 1\nN\u0302 tr \u2211 s\u2208\u222ati=1S i r s\u2212 ct\u22121r\n= 1 N\u0302 tr [ \u2211 s\u2208Str s+ \u2211\ns\u2032\u2208\u222at\u22121i=1S i r\ns\u2032]\u2212 ct\u22121r\n= 1 N\u0302 tr [ \u2211 s\u2208Str s+ N\u0302 t\u22121r c t\u22121 r ]\u2212 ct\u22121r\n= \u2212 n\u0302 t r\nN\u0302 tr ct\u22121r +\nn\u0302tr\nN\u0302 tr\n\u2211 s\u2208Str s\nn\u0302tr = \u2212\u03b7trct\u22121r + \u03b7tr c\u0302tr\nHence, the updates in Algorithm 1 and line 4 to line 14 in [Algorithm 1, [16]] are equivalent."}, {"heading": "6 Appendix B: proofs of main theorems", "text": "One subtlety we need to point out before the proofs is that, in Algorithm 1, the learning rate \u03b7tr as well as the update rule: ctr \u2190 (1\u2212 \u03b7tr)ct\u22121r + \u03b7tr c\u0302tr is only defined for a cluster r that is \u201csampled\u201d at the t-th iteration. However, even if the cluster is not \u201csampled\u201d, i.e., ctr = ct\u22121r , the same update rule with c\u0302tr = ct\u22121r and and the same learning rate still holds for this case. So in our analysis, we equivalently treat each cluster r as updated with the same learning rate \u03b7t = c \u2032\nto+t , and\ndifferentiates between a sampled and not-sampled cluster only through the definition of c\u0302tr ."}, {"heading": "6.1 Proofs leading to Theorem 1", "text": "Proof of Lemma 1. For simplicity, we denote E[\u00b7|Ft] by Et[\u00b7] (the same notation is also used as a shorthand to E[\u00b7|\u2126t] in the proof of Theorem 3; we abuse the notation here).\nEt[\u03c6 t+1] = Et[ k\u2211 r=1 \u2211 x\u2208At+2r \u2016x\u2212 ct+1r \u20162]\n\u2264 Et[ \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 ct+1r \u20162] = Et[ \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 (1\u2212 \u03b7t+1r )ctr \u2212 \u03b7t+1r c\u0302t+1r \u20162]\n= Et[ \u2211 r \u2211 x\u2208At+1r (1\u2212 \u03b7t+1r )2\u2016x\u2212 ctr\u20162 + (\u03b7t+1r )2\u2016x\u2212 c\u0302t+1r \u20162\n+2\u03b7t+1r (1\u2212 \u03b7t+1r )\u3008x\u2212 ctr, x\u2212 c\u0302t+1r \u3009]\nwhere the inequality is due to the optimality of clustering At+2 for centroids Ct+1. Since Et[c\u0302t+1r ] = (1\u2212 pt+1r )ctr + pt+1r m(At+1r ), we have\n\u3008x\u2212 ctr, x\u2212 c\u0302t+1r \u3009 = (1\u2212 pt+1r )\u2016x\u2212 ctr\u20162 + pt+1r \u3008x\u2212 ctr, x\u2212m(At+1r )\u3009 Plug this into the previous inequality, we get\nEt[\u03c6 t+1] \u2264 \u2211 r (1\u2212 2\u03b7t+1r )\u03c6tr + (\u03b7t+1r )2\u03c6tr + (\u03b7t+1r )2 \u2211\nx\u2208At+1r\n\u2016x\u2212 c\u0302t+1r \u20162\n+2\u03b7t+1r {(1\u2212 pt+1r ) \u2211\nx\u2208At+1r\n\u2016x\u2212 ctr\u20162 + pt+1r \u2211\nx\u2208At+1r\n\u3008x\u2212 ctr, x\u2212m(At+1r )\u3009}\n= \u03c6t \u2212 2 \u2211 r \u03b7t+1r p t+1 r \u03c6 t r + 2 \u2211 r \u03b7t+1r p t+1 r \u2211 x\u2208At+1r \u3008x\u2212 ctr, x\u2212m(At+1r )\u3009}\n+(\u03b7t+1r ) 2\u03c6tr + (\u03b7 t+1 r )\n2 \u2211\nx\u2208At+1r\n\u2016x\u2212 c\u0302t+1r \u20162\nNote by Cauchy-Schwarz,\u2211 x\u2208At+1r \u3008x\u2212 ctr, x\u2212m(At+1r )\u3009 \u2264 \u221a \u2211 x\u2208At+1r \u2016x\u2212 ctr\u20162 \u2211 x\u2208At+1r \u2016x\u2212m(At+1r )\u20162\n\u2264 \u2211\nx\u2208At+1r\n\u2016x\u2212 ctr\u20162 = \u03c6tr\nNow consider the two sums, \u22122 \u2211 r \u03b7t+1r p t+1 r \u03c6 t r + 2 \u2211 r \u03b7t+1r p t+1 r \u2211 x\u2208At+1r \u3008x\u2212 ctr, x\u2212m(At+1r )\u3009\nFor each r, if pt+1r = 0, both the left and right term are zero; if pt+1r > 0, we know the term is negative. Hence \u22122 \u2211 r \u03b7t+1r p t+1 r \u03c6 t r + 2 \u2211 r \u03b7t+1r p t+1 r \u2211 x\u2208At+1r \u3008x\u2212 ctr, x\u2212m(At+1r )\u3009\n\u2264 \u22122 min r,t;pt+1r >0 \u03b7t+1r p t+1 r \u2211 r;pt+1r >0 (\u03c6tr \u2212 \u2211 x\u2208At+1r \u3008x\u2212 ctr, x\u2212m(At+1r )\u3009)\nOur key observation is that pt+1r = 0 if and only if cluster At+1r is empty, i.e., degenerate. Since the degenerate clusters do not contribute to the k-means cost, we have \u2211 r;pt+1r >0 \u03c6tr = \u03c6 t. Moreover, applying CauchySchwarz again, \u2211 r;pt+1r >0 \u221a \u2211 x\u2208At+1r \u2016x\u2212 ctr\u20162 \u221a \u2211 x\u2208At+1r \u2016x\u2212m(At+1r )\u20162 \u2264 \u221a \u03c6t\u03c6\u0303t\nTherefore,\nEt[\u03c6 t+1] \u2264 \u03c6t \u2212 2 min\nr,t;pt+1r >0\n\u03b7t+1r p t+1 r (\u03c6 t \u2212 \u221a \u03c6t\u03c6\u0303t) + (\u03b7t+1max) 2(Et \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 c\u0302t+1r \u20162 + \u03c6t)\n= \u03c6t \u2212 2 min r,t;pt+1r >0 \u03b7t+1r p t+1 r \u03c6 t(1\u2212\n\u221a \u03c6\u0303t\n\u03c6t ) + (\u03b7t+1max) 2(Et \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 c\u0302t+1r \u20162 + \u03c6t)\nProof of Lemma 2. First note that since {A\u2217}[k] includes all stationary clusterings of 1 \u2264 k\u2032 \u2264 k number of clusters. At any t, Ct must have k\u2032 \u2208 [k] non-degenerate centroids, so there exists C\u2217 = m(A\u2217) with A\u2217 \u2208 {A\u2217}k\u2032 \u2208 {A\u2217}[k] such that \u2206(Ct, C\u2217) is well defined. For a contradiction, suppose \u2203\u03b4 > 0 such that \u2200t, \u2206(Ct, C\u2217) > \u03b4, for all A\u2217 \u2208 {A\u2217}[k]. At any t \u2265 1, let k\u2032 denote the number of non-degenerate clusters in Ct, then\nCase 1: If At+1 \u2208 {A\u2217}k\u2032 , then \u2203C\u2217 = m(At+1). Therefore,\n\u2206(Ct,m(At+1)) = \u2206(Ct, C\u2217) > \u03b4\nby our assumption.\nCase 2: If At+1 /\u2208 {A\u2217}k\u2032 , then Ct+1 = m(At+1) /\u2208 Cl(v\u22121(At+1)). Then by the contrapositive of statement (1) in Lemma 4,\n\u2203\u03b4\u2032 > 0 s.t. \u2200t,\u2206(Ct, Ct+1) \u2265 \u03b4\u2032\nCombining the two cases, \u2206(Ct,m(At+1)) > min{\u03b4, \u03b4\u2032} > 0. By Lemma 1 and assumption (A1), for any t,\nE[\u03c6t+1 \u2212 \u03c6t|Ft] \u2264 \u2212 2aminr\u2208[k],t;ptr(m)>0 p\nt r(m)\nt+ to \u03c6t(1\u2212\n\u221a \u03c6\u0303t\n\u03c6t ) + (\na\nt+ to )2B\nSince \u03c6t \u2212 \u03c6\u0303t = \u2211 r\u2208[k\u2032] \u2211 x\u2208At+1r \u2016x \u2212 Ct\u20162 \u2212 \u2016x \u2212 m(At+1r )\u20162 = \u2211 r \u2016c t r \u2212 m(At+1r )\u20162nt+1r =\n\u2206(Ct,m(At+1)) \u2265 min{\u03b4, \u03b4\u2032}, we get 1\u2212 \u221a \u03c6\u0303t\n\u03c6t \u2265 1\u2212\n\u221a 1\u2212 min{\u03b4,\u03b4\u2032}\n\u03c6t > 0. Since min{\u03b4, \u03b4\u2032} is a constant\nand \u03c6t is upper bounded, the previous term is bounded away from zero for all t. For convenience, let\u2019s denote by\nmin r\u2208[k],t;ptr(m)>0\nptr(m)(1\u2212\n\u221a \u03c6\u0303t\n\u03c6t ) :=\nh \u03c6t > 0\nThen \u2200t \u2265 1,\nE[\u03c6t+1]\u2212 E[\u03c6t] \u2264 \u2212 2ah t+ to + a2B (t+ to)2\nfor some positive constants B, a, h. Summing up all inequalities, E[\u03c6t+1]\u2212E[\u03c60] \u2264 \u22122ah ln t+to+1 to\n+ a 2B\nto\u22121 .\nSince t is unbounded and ln t+to+1 to\nincreases with t while a 2B\nto\u22121 is a constant, \u2203T such that for all t \u2265 T , E\u03c6t \u2212 \u03c60 \u2264 \u2212\u03c60, which means E[\u03c6t] \u2264 0, for all t large enough. This implies the k-means cost of some clusterings is negative, which is impossible. So we have a contradiction.\nLemma 5. If \u2200C\u2217 \u2208 {C\u2217}[k], C\u2217 is a non-boundary stationary point, that is, C\u2217 := m(A\u2217) \u2208 v\u22121(A\u2217). Then \u2203rmin > 0 such that \u2200C\u2217 \u2208 {C\u2217}[k], C\u2217 is a (rmin, 0)-stable stationary point.\nProof. Fix any k in the range of [k] (we abuse the notation with the same k here). For any C such that \u2206(C,C\u2217) exists (i.e., |C| = k\u2032 \u2265 k = |C\u2217|), we first show \u2203r\u2217 > 0, such that the following statement holds:\n\u2206(C,C\u2217) < r\u2217\u03c6\u2217 =\u21d2 C \u2208 v\u22121(A\u2217)\nThus, we proceed by argument analogous to that of Lemma 4. By Definition 4, there is a permutation \u03c0o of [k] such that \u2200x \u2208 Ar, \u2200r \u2208 [k] and \u2200s 6= r,\n\u2016x\u2212 c\u2217\u03c0o(r)\u2016 < \u2016x\u2212 c \u2217 \u03c0o(s)\u2016\nWe choose r\u2217 > 0 so that \u2200x \u2208 Ar, \u2200r \u2208 [k], \u2200s 6= r,\n\u2016x\u2212 c\u2217\u03c0o(r)\u2016 \u2264 \u2016x\u2212 c \u2217 \u03c0o(s)\u2016 \u2212 2\n\u221a r\u2217\u03c6\u2217, \u2200r \u2208 [k], s 6= r\nwith equality holds for at least one triple of (x, r, s). Let \u03c0\u2217 be a permutation satisfying\n\u03c0\u2217 = arg min \u03c0 \u2211 r\u2208[k] n\u2217r\u2016c\u03c0(r) \u2212 c\u2217r\u20162\nLet \u03c0\u2032 := \u03c0\u2217 \u25e6 \u03c0o. We have \u2200(x, r, s) triples,\n\u2016x\u2212 c\u03c0\u2032(s)\u2016 \u2212 \u2016x\u2212 c\u03c0\u2032(r)\u2016 \u2265 \u2016x\u2212 c\u2217\u03c0o(s)\u2016 \u2212 \u2016c \u2217 \u03c0o(s) \u2212 c\u03c0\u2032(s)\u2016\n\u2212(\u2016x\u2212 c\u2217\u03c0o(r)\u2016+ \u2016c \u2217 \u03c0o(r) \u2212 c\u03c0\u2032(r)\u2016) > \u2016x\u2212 c \u2217 \u03c0o(s)\u2016 \u2212 \u2016x\u2212 c \u2217 \u03c0o(r)\u2016 \u2212 2\n\u221a r\u2217\u03c6\u2217 \u2265 0\nwhere the second inequality is by the fact that\nmax r \u2016c\u03c0\u2217(r) \u2212 c\u2217r\u20162 \u2264 \u2206(C,C\u2217) < r\u2217\u03c6\u2217 =\u21d2 max r \u2016c\u03c0\u2217(r) \u2212 c\u2217r\u2016 <\n\u221a r\u2217\u03c6\u2217\nSince \u03c0\u2032 is the composition of two permutations of [k], it is also a permutation of [k], and \u2200r, s 6= r, \u2016x \u2212 c\u03c0\u2032(r)\u2016 < \u2016x\u2212 c\u03c0\u2032(s)\u2016, so C \u2208 v\u22121(A\u2217) by Definition 4. Since by our definition, r\u2217 is unique for each C\u2217. Since {C\u2217}[k] is finite, taking the minimum over all such r\u2217, i.e., rmin := minC\u2217\u2208{C\u2217}[k] r\n\u2217 completes the proof.\nRemark: r\u2217\u03c6\u2217 is intuitively the \u201cradius\u201d or basin of attraction of a stationary point C\u2217. rmin is smallest such radius and is only determined by the geometry of the dataset.\nProof setup of Theorem 1 The goal of the proof is to show that first, (with high probability) the algorithm converges to some stationary clustering, A\u2217\u2217 \u2208 {A\u2217}[k]. We call this event G; formally,"}, {"heading": "G := {\u2203T \u2265 1,\u2203A\u2217\u2217 \u2208 {A\u2217}[k], s.t. At = A\u2217\u2217, \u2200t \u2265 T}", "text": "Second, conditioning on G, we want to establish that the expected convergence rate of the algorithm to any stationary clustering A\u2217\u2217, which we formulate as\nE[\u03c6t \u2212 \u03c6(A\u2217\u2217)|Go(A\u2217\u2217)]\nis of order O( 1 t ), where Go(A\u2217\u2217) \u2282 G is defined in the subsequent proof.\nTo prove the two claims, we consider an eventGo \u2282 G \u2282 \u2126, which we partition into disjoint sets F (T\u2212, A\u2217\u2217)\u2229 H(T+, A\u2217\u2217), where by \u201cdisjoint\u201d we mean disjoint w.r.t. different (T,A\u2217\u2217), for T \u2265 1 and A\u2217\u2217 \u2208 {A\u2217}[k]. Formally,\nF (T\u2212, A\u2217\u2217) := {\u2200t < T,\u2206(Ct, C\u2217\u2217) > 1 2 rmin\u03c6 \u2217\u2217, \u2200A\u2217\u2217 \u2208 {A\u2217}t[k]}\n\u2229{\u2206(CT , C\u2217\u2217) \u2264 1 2 rmin\u03c6 \u2217\u2217}\nwhere we denote byC\u2217\u2217 := m(A\u2217\u2217), and we use {A\u2217}t[k] to denote the subset of {A\u2217}[k] such that \u2206(Ct, C\u2217\u2217) exists, and H(T+, A\u2217\u2217) := {\u2206(Ct, C\u2217\u2217) \u2264 rmin\u03c6\u2217\u2217, \u2200t \u2265 T} Under the event F (T\u2212, A\u2217\u2217), time T is the first time the algorithm \u201chits\u201d (hitting time) a stationary clustering, and this clustering isA\u2217\u2217. Note this event only depends on information of the sample space prior to and including time T , and we use T\u2212 to emphasize this.\nSimilarly, the event H(T+, A\u2217\u2217) only depends on information post and including T . Thus, for a fixed pair (T,A\u2217\u2217), F (T\u2212, A\u2217\u2217) \u2229 H(T+, A\u2217\u2217) 6= \u2205; for t 6= T , the two events are complementary to each other, and for t = T , they intersect since {\u2206T \u2264 1\n2 rmin\u03c6 \u2217\u2217} implies {\u2206T \u2264 rmin\u03c6\u2217\u2217}. Moreover, fixing T , F (T\u2212, A\u2217\u2217) are disjoint for different A\u2217\u2217; fixing A\u2217\u2217, F (T\u2212, A\u2217\u2217) are disjoint for different T . Thus, the events F (T\u2212, A\u2217\u2217) \u2229H(T+, A\u2217\u2217) are disjoint for different pairs of (T,A\u2217\u2217).\nProof of Theorem 1. Now we define Go to be\nGo := \u222aT\u22651 \u222aA\u2217\u2217\u2208{A\u2217}[k] F (T \u2212, A\u2217\u2217) \u2229H(T+, A\u2217\u2217)\nBy Lemma 2\nPr{\u222aT\u22651 \u222aA\u2217\u2217\u2208{A\u2217}[k] F (T \u2212, A\u2217\u2217)} = 1 (2)\nConditioning on any F (T\u2212, A\u2217\u2217), provided\nc\u2032 > 1\n2 min r\u2208[k],t; ptr(m)>0\nptr(m)(1\u2212 \u221a 1\u2212 rmin\u03c6opt \u03c6t ) \u2265 1\n2 min r\u2208[k],t; ptr(m)>0\nptr(m)(1\u2212 \u221a 1\u2212 rmin\u03c6\u2217\u2217 \u03c6t )\nWe apply Lemma 6 to get \u2200t \u2264 T ,\nE{\u03c6t \u2212 \u03c6(A\u2217\u2217)|F (T\u2212, A\u2217\u2217)} = O(1 t )\nNow let\u2019s consider the case t \u2265 T . Since A\u2217\u2217 is (rmin, 0)-stable, we can apply Theorem 3; specifically, for each parameters in the statement of Theorem 3, \u03b1 = 0, ptr(m) = 1 \u2212 ( n\u2212n\u2217\u2217r n )m, at = maxr p t r(m) minr ptr(m) , and amax = c\u2032\nminr,t ptr(m) . Thus,\n\u03b2 := 2c\u2032(1\u2212max t at \u221a \u03b1) = 2c\u2032 > 1\nis satisfied by setting c\u2032 \u2265 1 2 . Fix any 0 < \u03b4 < 1 e , the condition on to in Theorem 3 is satisfied by setting\nto \u2265 max  ( 16(c\u2032)2B [1\u2212(1\u2212p\u2217 min )m]2\u2206(CT ,C\u2217\u2217) ) 1 \u03b2\u22121 ,\n[ 48(c\u2032)2B2\n(\u03b2\u22121)[1\u2212(1\u2212p\u2217 min\n)m]2\u2206(CT ,C\u2217\u2217)2 ln 1 \u03b4\n] 2\n\u03b2\u22121 ,(ln 1 \u03b4\n) 2 \u03b2\u22121  Since T is the hitting time of the event {\u2206(CT , C\u2217\u2217) \u2264 1\n2 rmin\u03c6 \u2217\u2217}, \u2206(CT , C\u2217\u2217) \u2248 1 2 rmin\u03c6 \u2217\u2217, and we treat it as a constant. So the conditions by our requirements on c\u2032 and to. Let \u03b2\u2032 = 2c\u2032, amax := c \u2032\n\u03c1(m) . Thus we can\napply Theorem 3 to get\nPr{H(T+, A\u2217\u2217)|F (T\u2212, A\u2217\u2217)} \u2265 1\u2212 \u03b4 (3)\nand \u2200t \u2265 T , E{\u03c6t \u2212 \u03c6(A\u2217\u2217)|H(T+, A\u2217\u2217)} \u2264 E{\u2206(Ct, C\u2217\u2217)|H(T+, A\u2217\u2217)}\n\u2264 ( to + T + 1 to + t )\u03b2 \u2032 \u2206(CT , C\u2217\u2217) +\na2maxB\n\u03b2\u2032 \u2212 1 ( t0 + T + 2 t0 + T + 1 )\u03b2 \u2032+1 1 t0 + t+ 1\n\u2264 ( to + T + 1 to + t )\u03b2 \u2032 rmin\u03c6(A \u2217\u2217) + a2maxB \u03b2\u2032 \u2212 1 ( t0 + T + 2 t0 + T + 1 )\u03b2 \u2032+1 1 t0 + t+ 1 = O( 1 t ) (4)\nwhere the first inequality is by Lemma 11. Now observe\nPr{G} \u2265 Pr{Go} = Pr{\u222aT\u22651 \u222aA\u2217\u2217\u2208{A\u2217}[k] F (T \u2212, A\u2217\u2217) \u2229H(T+, A\u2217\u2217)} = \u2211\nT\u22651,A\u2217\u2217\u2208{A\u2217}[k]\nPr{F (T\u2212, A\u2217\u2217) \u2229H(T+, A\u2217\u2217)}\nwhere the second equality holds because the events F (T\u2212, A\u2217\u2217) \u2229H(T+, A\u2217\u2217) are disjoint with respect to different (T,A\u2217\u2217). Since \u2211\nT\u22651,A\u2217\u2217\u2208{A\u2217}[k]\nPr{F (T\u2212, A\u2217\u2217) \u2229H(T+, A\u2217\u2217)}\n= \u2211\nT\u22651,A\u2217\u2217\u2208{A\u2217}[k]\nPr{H(T+, A\u2217\u2217)|F (T\u2212, A\u2217\u2217)}Pr{F (T\u2212, A\u2217\u2217)}\n\u2265 (1\u2212 \u03b4) \u2211 T,A\u2217\u2217 Pr{F (T,A\u2217\u2217)} = (1\u2212 \u03b4)Pr{\u222aT,A\u2217\u2217F (T,A\u2217\u2217)} = (1\u2212 \u03b4)\nwhere the inequality is by Inequality (3), and the last equality is due to Equality (2). Therefore, Pr{G} \u2265 1\u2212 \u03b4, which completes the proof of the first statement.\nLet Go(A\u2217\u2217) := \u222aT\u22651F (T\u2212, A\u2217\u2217) \u2229 H(T+, A\u2217\u2217), i.e., Go(A\u2217\u2217) denotes the event where the algorithm converges to stationary clustering A\u2217\u2217, and\nPr{\u222aA\u2217\u2217\u2208{A\u2217}[k]Go(A \u2217\u2217)}\n= Pr{\u222aT\u22651,A\u2217\u2217\u2208{A\u2217}[k]F (T \u2212, A\u2217\u2217) \u2229H(T+, A\u2217\u2217)} \u2265 1\u2212 \u03b4\nwhich proves the second statement. Finally, combining inequalities (6) and (4), we have \u2200T \u2265 1 and \u2200t \u2265 1,\nE{\u03c6t \u2212 \u03c6(A\u2217\u2217)|F (T\u2212, A\u2217\u2217) \u2229H(T+, A\u2217\u2217)} = O(1 t )\nSince the quantity \u03c6t \u2212 \u03c6(A\u2217\u2217) is independent of information about T , we reach the conclusion\nE{\u03c6t \u2212 \u03c6(A\u2217\u2217)|Go(A\u2217\u2217)} = O( 1\nt )\nLemma 6. Suppose the assumptions and settings in Theorem 1 hold, conditioning on any F (T\u2212, A\u2217\u2217), we have \u22001 \u2264 t < T ,\nE{\u03c6t \u2212 \u03c6(A\u2217\u2217)|F (T\u2212, A\u2217\u2217)} = O(1 t )\nProof. First observe that conditioning on the event F (T\u2212, A\u2217\u2217), \u2206(Ct, C\u2217\u2217) > 1 2 rmin\u03c6 \u2217\u2217, \u2200t < T . Now we are in a setup similar to that in the proof Lemma 2, and the argument therein will lead us to the conclusion that\n\u03c6t \u2212 \u03c6\u0303t > min{1 2 rmin, rmin}\u03c6\u2217\u2217 = 1 2 rmin\u03c6 \u2217\u2217\nBy Lemma 1, we have conditioning on F (T\u2212, A\u2217\u2217),\nEt[\u03c6 t|F (T\u2212, A\u2217\u2217)] \u2264 \u03c6t\u22121{1\u2212\n2c\u2032minr\u2208[k],t;ptr(m)>0 p t r(m)\nto + t\u2212 1 (1\u2212\n\u221a \u03c6\u0303t\n\u03c6t )}\n+ (c\u2032)2B\n(to + t\u2212 1)2 (5)\nwhere B is the bound in (A1). Since by choosing\nc\u2032 > 1\n2 min r\u2208[k],t; ptr(m)>0\nptr(m)(1\u2212 \u221a 1\u2212 rmin\u03c6opt \u03c6t ) \u2265 1\n2 min r\u2208[k],t; ptr(m)>0\nptr(m)(1\u2212 \u221a 1\u2212 rmin\u03c6\u2217\u2217 \u03c6t )\nwe have\n\u03b2 = 2c\u2032 min r\u2208[k],t;ptr(m)>0 ptr(m)(1\u2212\n\u221a \u03c6\u0303t\n\u03c6t ) > 1\nThen subtracting \u03c6(A\u2217\u2217) from both sides of Inequality (5) and applying Lemma 17 with a := \u03b2 > 1, we conclude that \u22001 \u2264 t < T ,\nE[\u03c6t \u2212 \u03c6(A\u2217\u2217)|F (T\u2212, A\u2217\u2217)] \u2264 to + 1 to + t [\u03c60 \u2212 \u03c6(A\u2217\u2217)]\n+ (c\u2032)2B\n\u03b2 \u2212 1 ( to + 2 to + 1 )\u03b2+1\n1\nto + t+ 1 = O(\n1 t ) (6)\nThe overall exact convergence bound There are two convergence bound corresponding to the two phases of analysis, global and local convergence, in Theorem 1. For the first global convergence phase from t = 1 to T for some random T , the bound is\n( to + 1 to + t )\u03b2 [\u03c60 \u2212 \u03c6(A\u2217\u2217)] + (c\n\u2032)2B\n\u03b2 \u2212 1 ( to + 2 to + 1 )\u03b2+1\n1\nto + t+ 1 (by Lemma 6)\nwhere \u03b2 = 2c\u2032p+min(m)(1\u2212 \u221a \u03c6\u0303t \u03c6t ). For the second, local convergence phase, \u2200t \u2265 T ,\n( to + T + 1 to + t )\u03b2 \u2032 rmin\u03c6(A \u2217\u2217) + a2maxB \u03b2\u2032 \u2212 1 ( t0 + T + 2 t0 + T + 1 )\u03b2 \u2032+1 1 t0 + t+ 1 (by Theorem 3)\nwhere \u03b2\u2032 = 2c\u2032, amax := c \u2032 \u03c1(m) . So the overall bound is the maximum of the two bounds. When c\u2032 is sufficiently large, the first term of both bounds are likely dominated by the second term, so the overall bound is likely dominated by a 2 maxB\n\u03b2\u22121 ( t0+2 t0+1 )\u03b2+1 1 t0+t+1 ."}, {"heading": "6.2 Proofs leading to Theorem 3", "text": "In the analysis of this section, we use Et[\u00b7] as a shorthand notation for E[\u00b7|\u2126t], where \u2126t is as defined in the main paper.\nProof of Theorem 3. By Proposition 1, for any t > 1, Pr(\u2126t) \u2265 1 \u2212 Pr(\u222at>i\u2126t\u22121 \\ \u2126t) \u2265 1 \u2212 \u03b4. This proves the first statement. By Lemma 7,\nEt[\u2206 t|Ft\u22121] \u2264 \u2206t\u22121(1\u2212\n\u03b2\nto + t ) + [ amax to + t ]2B + 2amax to + t Et{ \u2211 r n\u2217r\u3008ct\u22121r \u2212 c\u2217r , \u03betr\u3009|Ft\u22121}\n= \u2206t\u22121(1\u2212 \u03b2 to + t ) + [ amax to + t ]2B\nwhere the last term vanishes since Et{\u03betr|Ft\u22121} = 0, \u2200r \u2208 [k]. Therefore,\nEt[\u2206 t] \u2264 Et\u22121[\u2206t\u22121](1\u2212\n\u03b2\nto + t ) +\na2maxB\n(t+ to)2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 Ei[\u2206i]\u03a0t\u03c4=i+1(1\u2212\n\u03b2\nto + \u03c4 ) + t\u2211 \u03c4=i+1 a2maxB (\u03c4 + to)2\nBy Lemma 17, for \u03b2 > 1,\nEt[\u2206 t] \u2264 ( to + i+ 1\nto + t )\u03b2\u2206i +\na2maxB\n\u03b2 \u2212 1 ( t0 + i+ 2 t0 + i+ 1 )\u03b2+1\n1\nt0 + t+ 1\nLemma 7. Suppose the conditions in Theorem 3 hold. If c to+i \u2264 \u03b7ipir(m) \u2264 cato+i , \u2200r \u2208 [k], with a \u2264 1\u221a \u03b1 , then\n\u2206i \u2264 \u2206i\u22121(1\u2212 \u03b2 to + i ) + [ amax to + i ]2 \u2211 r n\u2217r\u2016c\u0302ir \u2212 c\u2217r\u20162 + 2amax to + i \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , \u03beir\u3009\nwhere \u03beir := c\u0302 i r \u2212 E[c\u0302ir|Fi\u22121].\nProof. Let \u2206ir := n\u2217r\u2016cir \u2212 c\u2217r\u20162, so \u2206i = \u2211 r \u2206 i r . By the update rule of Algorithm 1,\n\u2206ir = n \u2217 r\u2016(1\u2212 \u03b7i)(ci\u22121r \u2212 c\u2217r) + \u03b7i(c\u0302ir \u2212 c\u2217r)\u20162\n\u2264 n\u2217r{(1\u2212 2\u03b7i)\u2016ci\u22121r \u2212 c\u2217r\u20162 + 2\u03b7i\u3008ci\u22121r \u2212 c\u2217r , c\u0302ir \u2212 c\u2217r\u3009 +(\u03b7i)2[\u2016ci\u22121r \u2212 c\u2217r\u20162 + \u2016c\u0302ir \u2212 c\u2217r\u20162]}\nLet \u03beir = c\u0302ir \u2212 E[c\u0302ir|Fi\u22121], where E[c\u0302ir|Fi\u22121] = (1\u2212 pir)ci\u22121r + pirm(Air), and pir := pir(m).\nSince\n\u3008ci\u22121r \u2212 c\u2217r , c\u0302ir \u2212 c\u2217r\u3009 = \u3008ci\u22121r \u2212 c\u2217r , E[c\u0302ir|Fi\u22121] + \u03beir \u2212 c\u2217r\u3009 \u2264 (1\u2212 pir)\u2016ci\u22121r \u2212 c\u2217r\u20162 + pir\u2016m(Air)\u2212 c\u2217r\u2016\u2016ci\u22121r \u2212 c\u2217r\u2016+ \u3008ci\u22121r \u2212 c\u2217r , \u03beir\u3009\nwe have\n\u2206ir \u2264 n\u2217r{\u22122\u03b7i[\u2016ci\u22121r \u2212 c\u2217r\u20162 \u2212 (1\u2212 pir)\u2016ci\u22121r \u2212 c\u2217r\u20162 \u2212 pir\u2016ci\u22121r \u2212 c\u2217r\u2016\u2016m(Air)\u2212 c\u2217r\u2016] +\u2016ci\u22121r \u2212 c\u2217r\u20162 + 2\u03b7i\u3008\u03beir, ci\u22121r \u2212 c\u2217r\u3009+ (\u03b7i)2[\u2016ci\u22121r \u2212 c\u2217r\u20162 + \u2016c\u0302ir \u2212 c\u2217r\u20162]}\n\u2264 n\u2217r{\u2212 2c to + i \u2016ci\u22121r \u2212 c\u2217r\u20162 + 2ca to + i \u2016ci\u22121r \u2212 c\u2217r\u2016\u2016m(Air)\u2212 c\u2217r\u2016\n+\u2016ci\u22121r \u2212 c\u2217r\u20162 + 2\u03b7i\u3008\u03beir, ci\u22121r \u2212 c\u2217r\u3009+ (\u03b7i)2[\u2016ci\u22121r \u2212 c\u2217r\u20162 + \u2016c\u0302ir \u2212 c\u2217r\u20162}\nNote \u2211 r n\u2217r\u2016cir \u2212 c\u2217r\u2016\u2016m(Air)\u2212 c\u2217r\u2016 \u2264 \u221a ( \u2211 r n\u2217r\u2016ci\u22121r \u2212 c\u2217r\u20162)( \u2211 r n\u2217r\u2016m(Air)\u2212 c\u2217r\u20162)\n= \u221a \u2206i\u22121\u2206(m(Ai), C\u2217) \u2264 \u221a \u03b1\u2206i\u22121\nwhere the first inequality is by Cauchy-Schwartz and the last inequality is by applying Lemma 3. Finally, summing over \u2206ir , we get\n\u2206i = \u2211 r \u2206ir \u2264 \u2206i\u22121[1\u2212 2c to + i (1\u2212 a \u221a \u03b1)]\n+[ ca (to + i)pir ]2 \u2211 r n\u2217r\u2016c\u0302ir \u2212 c\u2217r\u20162 + 2ca (to + i)pir \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , \u03beir\u3009\n\u2264 \u2206i\u22121(1\u2212 \u03b2 to + i ) + [ amax to + i ]2 \u2211 r n\u2217r\u2016c\u0302ir \u2212 c\u2217r\u20162 + 2amax to + i \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , \u03beir\u3009\nThe second inequality is by definition of \u03b2 := 2c\u2032minr,t ptr(m)(1 \u2212 maxt at \u221a \u03b1)), and the fact that c \u2265 c\u2032minr,t p t r(m) and a \u2264 maxt at.\nLemma 8. Suppose the conditions in Theorem 3 hold. For any \u03bb > 0, Ei{exp{\u03bb\u2206i}|Fi\u22121} \u2264 exp { \u03bb{(1\u2212 \u03b2\nt0 + i )\u2206i\u22121 +\na2maxB (t0 + i)2 + \u03bba2maxB 2 2(t0 + i)2 } }\nProof. By Lemma 7, we have\nEi{exp(\u03bb\u2206i)|Fi\u22121} \u2264 exp\u03bb[\u2206i\u22121(1\u2212 \u03b2\nto + i ) +\na2maxB\n(to + i)2 ]\nEi{exp\u03bb 2amax to + i \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , \u03beir\u3009|Fi\u22121}\nSince 2\u03bbamax i+t0 \u2211 r n \u2217 r\u3008\u03beir, ci\u22121r \u2212 c\u2217r\u3009 \u2264 2\u03bbamaxi+t0 B and Ei{ 2\u03bbamax i+t0 \u2211 r n \u2217 r\u3008\u03beir, ci\u22121r \u2212 c\u2217r\u3009|Fi\u22121} = 0, by Hoeffding\u2019s lemma\nEi { exp{2\u03bbamax\ni+ t0 \u2211 r n\u2217r\u3008\u03beir, ci\u22121r \u2212 c\u2217r\u3009|Fi\u22121}\n} \u2264 exp{\u03bb 2 maxa 2 maxB 2\n2(i+ t0)2 }\nCombining this with the previous bound completes the proof.\nLemma 9 (adapted from [3]). For any \u03bb > 0, Ei{e\u03bb\u2206 i\u22121 } \u2264 Ei\u22121{e\u03bb\u2206 i\u22121 }\nProof. By our partitioning of the sample space, \u2126i\u22121 = \u2126i \u222a (\u2126i\u22121 \\ \u2126i), and for any \u03c9 \u2208 \u2126i and \u03c9\u2032 \u2208 \u2126i\u22121 \\ \u2126i, \u2206i\u22121(\u03c9) \u2264 \u22060 < \u2206i\u22121(\u03c9\u2032). Taking expectation over \u2126i and \u2126i\u22121, we get Ei{e\u03bb\u2206 i\u22121 } \u2264 Ei\u22121{e\u03bb\u2206 i\u22121 }.\nProposition 1. Suppose the conditions in Theorem 3 hold. Fix any 0 < \u03b4 \u2264 1 e . If\nt0 \u2265 max{( 16a2maxB\n\u22060 )\n1 \u03b2\u22121 , [\n48a2maxB 2\n(\u03b2 \u2212 1)(\u22060)2 ln 1 \u03b4 ] 2 \u03b2\u22121 , (ln 1 \u03b4 ) 2 \u03b2\u22121 }\nthen P (\u222ai\u22651\u2126i\u22121 \\ \u2126i) \u2264 \u03b4 (here we used \u22060 instead of \u2206i and treat the starting time, the i-th iteration in Theorem 3 as the zeroth iteration for cleaner presentation).\nProof. By Lemma 8,\nEi{e\u03bb\u2206 i } \u2264 Ei{e\u03bb{(1\u2212 \u03b2 to+i )\u2206i\u22121} exp{ \u03bba\n2 maxB (to + i)2 + \u03bb2a2maxB 2 2(to + i)2 }\n\u2264 Ei\u22121{e\u03bb (1)\u2206i\u22121r } exp{ \u03bba\n2 maxB (to + i)2 + \u03bb2a2maxB 2 2(to + i)2 }\nwhere \u03bb(1) = \u03bb(1 \u2212 \u03b2 to+i ), and the second inequality is by Lemma 9. Similarly, the following recurrence relation holds for k = 0, . . . , i:\nEi\u2212k{e\u03bb (k)\u2206i\u2212k} \u2264 Ei\u2212(k+1){e\u03bb (k+1)\u2206i\u2212k\u22121} exp{ \u03bb (k)a2maxB\n(to + i\u2212 k)2 +\n(\u03bb(k))2a2maxB 2 2(to + i\u2212 k)2 }\nwhere \u03bb(0) := \u03bb, and for k \u2265 1, \u03bb(k) := \u03a0kt=1(1\u2212 2to+(i\u2212t+1) )\u03bb (0).\nSince (see, e.g., [3]) \u2200\u03b2 > 0, k \u2265 1,\n\u03a0kt=1(1\u2212 \u03b2 to + (i\u2212 t+ 1) ) \u2264 ( to + i\u2212 k + 1 to + i )\u03b2\nwe have, for 0 \u2264 k \u2264 i (this inequality is also satisfied when k = 0 by definition of \u03bb(0)),\n\u03bb(k) (t0 + i\u2212 k)2 \u2264 ( to + i\u2212 k + 1 (to + i)(to + i\u2212 k) )2\u03bb \u2264 (1 + 2 to + 1 t2o )\n\u03bb\n(to + i)\u03b2\nRepeatedly applying the relation, we get\nEi{e\u03bb\u2206 i } \u2264 e\u03bb (i)\u22060 exp{ i\u22121\u2211 k=0 ( \u03bb(k)a2maxB (to + i\u2212 k)2 + (\u03bb(k))2a2maxB 2 2(to + i\u2212 k)2 )}\n\u2264 exp{\u03bb( to + 1 to + i )\u03b2\u22060 + (\u03bbB + \u03bb2B2 2 ) (1 + 2 to + 1 t2o )a2max (to + i)\u03b2\u22121 }\nThen by Markov\u2019s inequality, for any \u03bbi > 0,\nPr(\u03c9 \u2208 \u2126i\u22121 \\ \u2126i) \u2264 Pri(\u2206i > 2\u22060) = Pri(e\u03bbi\u2206 i > e\u03bbi2\u2206 0 ) \u2264 Eie \u03bbi\u2206\ni r\ne\u03bbi2\u2206 0 r\nCombining this with the upper bound on Eie\u03bbi\u2206 i , we get\nPr(\u03c9 \u2208 \u2126i\u22121 \\ \u2126i) \u2264 exp { \u2212\u03bbi{\u22060[2\u2212 ( to + 1\nto + i )\u03b2 ]\u2212 (B + \u03bbiB\n2 2 ) (1 + 2 to + 1 t2o )a2max (to + i)\u03b2\u22121 }\n}\n\u2264 exp { \u2212\u03bbi{\u22060 \u2212 (B + \u03bbiB 2\n2 ) 4a2max (to + i)\u03b2\u22121\n} }\nsince i \u2265 1, to \u2265 1. We choose \u03bbi = 1\u2206 ln (i+1)2 \u03b4 with \u2206 = \u2206 0 2 .\nCase 1: B > \u03bbiB 2\n2 . We get \u22060 \u2212 (B + \u03bbiB\n2\n2 ) 4a2max (to+i)\u03b2\u22121 \u2265 \u2206, since to \u2265 ( 16a 2 maxB \u22060 ) 1 \u03b2\u22121 .\nCase 2: B \u2264 \u03bbiB 2\n2 . We get\n\u22060 \u2212 (B + \u03bbiB 2\n2 ) 4a2max (to + i)\u03b2\u22121 \u2265 \u22060 \u2212 \u03bbiB2 4a2max (to + i)\u03b2\u22121\n= 2\u2206\u2212 1 \u2206 ln (1 + i)2 \u03b4 4a2maxB 2 (to + i)\u03b2\u22121 \u2265 2\u2206\u2212 1 \u2206 ln (to + i) 2 \u03b4 4a2maxB 2 (to + i)\u03b2\u22121\nNow we show 1\n\u2206 ln\n(to + i) 2\n\u03b4\n4a2maxB 2\n(to + i)\u03b2\u22121 \u2264 \u2206\nSince to + i \u2265 to \u2265 max{[ 48a 2 maxB 2\n(\u03b2\u22121)(\u22060)2 ln 1 \u03b4 ]\n2 \u03b2\u22121 , (ln 1\n\u03b4 )\n2 \u03b2\u22121 }, we can apply Lemma 16 with C :=\nmax{ 16a 2 maxB 2 (\u22060)2 , \u03b2\u22121 3 }, t := to + i \u2265 ( 3C\u03b2\u22121 ln 1 \u03b4 ) 2 \u03b2\u22121 , and get\n4a2maxB 2\n\u22062 ln\n(to + i) 2\n\u03b4 = (ln\n(to + i) 2 \u03b4 ) 16a2maxB 2 (\u22060r)2 \u2264 2C ln t+ C ln 1 \u03b4 < t\u03b2\u22121 = (to + i) \u03b2\u22121\nThat is, 1 \u2206\nln (to+i) 2\n\u03b4\n4a2maxB 2\n(to+i)\u03b2\u22121 \u2264 \u2206. Thus, for both cases, Pr(\u03c9 \u2208 \u2126i\u22121 \\ \u2126i) \u2264 e\u2212 1 \u2206\n(ln (1+i)2\n\u03b4 )\u2206 \u2264 \u03b4\n(i+1)2 . Finally, we have \u2211\u221e i=1 Pr(\u03c9 \u2208 \u2126i\u22121 \\ \u2126i) \u2264 \u03b4."}, {"heading": "7 Appendix C: Proofs leading to Theorem 2", "text": ""}, {"heading": "7.1 Existence of stable stationary point under geometric assumptions on the dataset", "text": "Lemma 10 (Theorem 5.4 of [11]). Suppose the datasetX admits a stationary clusteringA\u2217 satisfying (B1) and (B2). If \u2200r \u2208 [k], s 6= r, \u2206tr + \u2206ts \u2264 \u2206rs16 . Then for any s 6= r, |A \u2217 r \u2229Ats| \u2264 b 2 f , where b \u2265 maxr,s \u2206 t r+\u2206 t s \u2206rs .\nThe proof is almost verbatim to Theorem 5.4 of [11]; we include it here for completeness.\nProof. Let x \u2208 A\u2217r \u2229Ats. Split x into its projection on the line joining c\u2217r and c\u2217s , and its orthogonal component. This implies\nx = 1 2 (c\u2217r + c \u2217 s) + \u03bb(c \u2217 r \u2212 c\u2217s) + u\nwith u \u22a5 c\u2217r \u2212 c\u2217s . Note \u03bb measures the degree of departure of the projected point to the mid-point of c\u2217r and c\u2217s . Thus, by our definition of the margin, we have\n\u2016x\u0304\u2212 1 2 (c\u2217r + c \u2217 s)\u2016 = \u2016\u03bb(c\u2217r \u2212 c\u2217s)\u2016 \u2265 1 2 \u2206rs (7)\nSince the projection of x on the line joining ctr, cts is closer to s, we have\nx(cts \u2212 ctr) \u2265 1\n2 (cts \u2212 ctr)(cts + ctr)\nSo 1\n2 (c\u2217r + c \u2217 s)(c t s \u2212 ctr) + \u03bb(c\u2217r \u2212 c\u2217s)(cts \u2212 ctr)\n+u(cts \u2212 ctr) \u2265 1\n2 (cts \u2212 ctr)(cts + ctr) (8)\nSince u \u22a5 c\u2217r \u2212 c\u2217s , let \u2206 = \u2206ts + \u2206tr . We have\nu(cts \u2212 ctr) = u(cts \u2212 c\u2217s \u2212 (ctr \u2212 c\u2217r)) \u2264 \u2016u\u2016\u2206\nRearranging Inequality (8), we have\n1 2 (c\u2217r + c \u2217 s \u2212 cts \u2212 ctr)(cts \u2212 ctr)\n+\u03bb(c\u2217r \u2212 c\u2217s)(cts \u2212 ctr) + u(cts \u2212 ctr) \u2265 0\n\u2261 \u2206 2\n2 +\n\u2206 2 \u2016c\u2217r \u2212 c\u2217s\u2016 \u2212 \u03bb\u2016c\u2217r \u2212 c\u2217s\u20162 +\u03bb\u2206\u2016c\u2217r \u2212 c\u2217s\u2016+ \u2016u\u2016\u2206 \u2265 0\nTherefore,\n\u2016x\u2212 c\u2217r\u2016 = \u2016( 1 2 \u2212 \u03bb)(c\u2217s \u2212 c\u2217r) + u\u2016 \u2265 \u2016u\u2016\n\u2265 \u03bb \u2206 \u2016c\u2217r \u2212 c\u2217s\u20162 \u2212 \u2206 2\n\u22121 2 \u2016c\u2217r \u2212 c\u2217s\u2016 \u2212 \u03bb\u2016c\u2217r \u2212 c\u2217s\u2016 \u2265 \u2206rs\u2016c\u2217r \u2212 c\u2217s\u2016 64\u2206\nwhere the last inequality is by our assumption that \u2206 \u2264 \u2206rs 16 , and \u03bb \u2265 \u2206rs 2\u2016c\u2217r\u2212c\u2217s\u2016 by (7). Therefore, for all s 6= r\n|A\u2217r \u2229Ats| \u22062rs\u2016c\u2217r \u2212 c\u2217s\u20162\nf\u22062 \u2264 \u2211 A\u2217r\u2229Ats \u2016x\u2212 c\u2217r\u20162\nSo |A\u2217r \u2229 Ats| \u2264 \u2211 A\u2217r\u2229Ats \u2016x \u2212 c\u2217r\u20162 f(\u2206tr+\u2206 t s) 2 \u22062rs\u2016c\u2217r\u2212c\u2217s\u20162 \u2264 fb 2\nf2\u03c6\u2217( 1 n\u2217r\n) ( \u2211 A\u2217r\u2229Ats \u2016x \u2212 c\u2217r\u20162). That is, |A\u2217r\u2229A t s| n\u2217r \u2264\nb2 f\u03c6\u2217 \u2211 A\u2217r\u2229Ats \u2016x \u2212 c\u2217r\u20162. Similarly, for all s 6= r, |A\u2217s\u2229A t r| n\u2217r \u2264 b 2 f\u03c6\u2217 \u2211 A\u2217s\u2229Atr \u2016x \u2212 c\u2217s\u20162 Summing over all s 6= r, |Ar4A \u2217 r |\nn\u2217r = \u03c1out + \u03c1in \u2264 b\n2\nf\u03c6\u2217 \u03c6 \u2217 = b\n2 f .\nLemma 11. Fix a stationary point C\u2217 with k centroids, and any other set of k\u2032-centroids, C, with k\u2032 \u2265 k so that C has exactly k non-degenerate centroids. We have\n\u03c6(C)\u2212 \u03c6\u2217 \u2264 min \u03c0 \u2211 r n\u2217r\u2016c\u03c0(r) \u2212 c\u2217r\u20162 = \u2206(C,C\u2217)\nProof. Since degenerate centroids do not contribute to k-means cost, in the following we only consider the sets of non-degenerate centroids {cs, s \u2208 [k]} \u2282 C and {c\u2217r , r \u2208 [k]} \u2282 C\u2217. We have for any permutation \u03c0,\n\u03c6(C)\u2212 \u03c6\u2217 = \u2211 s \u2211 x\u2208As \u2016x\u2212 cs\u20162 \u2212 \u2211 r \u2211 x\u2208A\u2217r \u2016x\u2212 c\u2217r\u20162\n\u2264 \u2211 r \u2211 x\u2208A\u2217r \u2016x\u2212 c\u03c0(r)\u20162 \u2212 \u2211 r \u2211 x\u2208A\u2217r \u2016x\u2212 c\u2217r\u20162 = \u2211 r n\u2217r\u2016c\u03c0(r) \u2212 c\u2217r\u20162\nwhere the last inequality is by optimality of clustering assignment based on Voronoi diagram, and the second inequality is by applying the centroidal property in Lemma 12 to each centroid in C\u2217. Since the inequality holds for any \u03c0, it must holds for min\u03c0 \u2211 r n \u2217 r\u2016c\u03c0(r) \u2212 c\u2217r\u20162, which completes the proof.\nLemma 12 (Centroidal property, Lemma 2.1 of [10]). For any point set Y and any point c in Rd, \u03c6(c, Y ) = \u03c6(m(Y ), Y ) + |Y |\u2016m(Y )\u2212 c\u20162. Proposition 2. Assume (B1) (B2) hold for a stationary clusteringA\u2217 with corresponding k centroids denoted by C\u2217. Then, for any C such that \u2206(C,C\u2217) \u2264 b\u03c6\u2217 for some b \u2264 \u03b3 2f(\u03b1)2\n162 , we have maxr\u2208[k] |Ar4A\u2217r | n\u2217r \u2264 b \u03b3f(\u03b1)3 .\nAnd C\u2217 is a ( \u03b3 2f(\u03b1)2\n162 , \u03b1) -stable stationary point.\nProof. The condition implies for all r \u2208 [k], \u2016cr \u2212 c\u2217r\u2016 \u2264 \u221a b\u03c6\u2217\nn\u2217r . Then for all r 6= s,\n\u2016cr \u2212 c\u2217r\u2016+ \u2016cs \u2212 c\u2217s\u2016 \u2264 \u221a b \u221a \u03c6\u2217(\n1\u221a n\u2217r + 1\u221a n\u2217s ) =\n\u221a b\n\u03b3f(\u03b1) \u03b3f(\u03b1)\n\u221a \u03c6\u2217(\n1\u221a n\u2217r + 1\u221a n\u2217s )\n\u2264 \u221a b\n\u03b3f(\u03b1) \u2206rs \u2264\n1\n16 \u2206rs\nwhere the second inequality is by assumptions (B1) and (B2), and the last inequality by our assumption on b. Thus, we may apply Lemma 10 to get |Ar4A \u2217 r |\nn\u2217r \u2264 b \u03b32f3(\u03b1) for all r, proving the first statement. Now by Lemma\n11, \u03c6(C) \u2264 (b+ 1)\u03c6\u2217, so we get \u03b1b\n5\u03b1b+ 4(1 + \u03c6(C) \u03c6\u2217 )\n\u2265 \u03b1b 5\u03b1b+ 4(2 + b)\n\u2265 \u03b1b 5\u03b1\u03b32f2(\u03b1)/162 + 4(2 + \u03b32f2(\u03b1)/162) \u2265 b \u03b32f3(\u03b1)\n\u2265 |Ar4A \u2217 r |\nn\u2217r\nwhere the last inequality holds since f(\u03b1) \u2265 5\u03b1+5 162\u03b1\nand \u03b3 \u2265 8 \u221a\n2\u221a f by (B1) and (B2), respectively. This proves\nthe second statement since C\u2217 is then ( \u03b3 2f(\u03b1)2\n162 , \u03b1)-stable by Definition 3."}, {"heading": "7.2 Proofs regarding seeding guarantee", "text": "Lemma 13 (Theorem 4 of [17]). Assume (B1) and (B2) hold for a stationary clustering A\u2217 with C\u2217 := m(A\u2217). If we obtain seeds from Algorithm 2, then\n\u2206(C0, C\u2217) \u2264 1 2\n\u03b32f(\u03b1)2\n162 \u03c6\u2217\nwith probability at least 1\u2212mo exp(\u22122( f(\u03b1)4 \u2212 1) 2w2min)\u2212 k exp(\u2212mopmin).\nProof. First note that assumption (B1) satisfies center-separability assumption in Definition 1 of [17]. Therefore, applying Theorem 4 of [17] with \u00b5r = c\u2217r and \u03bdr = c0r , we get \u2200r \u2208 [k], \u2016c0r \u2212 c\u2217r\u2016 \u2264 \u221a f(\u03b1)\n2 \u221a \u03c6\u2217r n\u2217r with\nprobability at least 1\u2212mo exp(\u22122( f(\u03b1)4 \u2212 1) 2w2min)\u2212 k exp(\u2212mopmin). Summing over all r, the previous event implies \u2211 r n \u2217 r\u2016c0r \u2212 c\u2217r\u20162 \u2264 f(\u03b1)4 \u03c6 \u2217 \u2264 1 2 \u03b32f(\u03b1)2 162 \u03c6\u2217, where the second inequality is by \u03b3 \u2265 8 \u221a 2\u221a f .\nLemma 14. Assume the conditions Lemma 13 hold. For any \u03be > 0, if in addition,\nf(\u03b1) \u2265 5\n\u221a 1\n2wmin ln(\n2\n\u03bepmin ln\n2k\n\u03be )\nIf we obtain seeds from Algorithm 2 choosing\nln 2k \u03be pmin < mo < \u03be 2 exp{2(f(\u03b1) 4 \u2212 1)2w2min}\nThen \u2206(C0, C\u2217) \u2264 1 2 \u03b32f(\u03b1)2 162 \u03c6\u2217 with probability at least 1\u2212 \u03be.\nProof. By Lemma 13, a sufficient condition for the success probability to be at least 1\u2212 \u03be is:\nmo exp(\u22122( f(\u03b1)\n4 \u2212 1)2w2min) \u2264\n\u03be\n2\nand k exp(\u2212mopmin) \u2264 \u03be 2 This translates to requiring\n1\npmin ln\n2k\n\u03be \u2264 mo \u2264\n\u03be 2 exp(2( f(\u03b1) 4 \u2212 1)2w2min)\nNote for this inequality to be possible, we also need 1 pmin ln 2k \u03be \u2264 \u03be 2 exp(2( f(\u03b1) 4 \u2212 1)2w2min), imposing a constraint on f(\u03b1). Taking logarithm on both sides and rearrange, we get\n( f(\u03b1) 4 \u2212 1)2 \u2265 1 2wmin ln( 2 \u03bepmin ln 2k \u03be )\nThat is, f(\u03b1) \u2265 5 \u221a\n1 2wmin ln( 2 \u03bepmin ln 2k \u03be ).\nProof of Theorem 2. We first show clusterability of the dataset implies stability of the optimal solution: since Copt, which is necessarily a stationary point, satisfies (B1)(B2), Copt is a ( \u03b3f(\u03b1) 2\n162 , \u03b1)-stable stationary point\nby Proposition 2. Let b0 := \u03b3f(\u03b1) 2\n162 , and we denote event F := {\u2206(C0, Copt) \u2264 1 2 b0\u03c6 opt}. Since f(\u03b1) \u2265 5 \u221a\n1 2wmin ln( 2 \u03bepmin ln 2k \u03be\n), and log 2k \u03be\npmin < mo < \u03be 2 exp{2( f(\u03b1) 4 \u2212 1)2w2min}, we can apply Lemma 14 to get\nPr{F} \u2265 1\u2212 \u03be\nConditioning on F , we can invoke Theorem 3, with c\u2032, to sufficiently large; both depends on ptr(m) as defined in Theorem 3. Since Copt is (b0, \u03b1)-stable, conditioning on F\nmax r Atr4Aoptr noptr \u2264 b0 \u03b3f(\u03b1)3 \u2264 \u03b3 162f(\u03b1)\nSo\nmin r,t ptr(1)|F \u2265 pmin \u2212max r Atr4Aoptr noptr \u2265 pmin \u2212 \u03b3 162f(\u03b1)\nAnd min r,t ptr(m)|F \u2265 1\u2212 [1\u2212 (pmin \u2212 \u03b3 162f(\u03b1) )]m where pmin \u2212 \u03b3162f(\u03b1) > \u221a \u03b1 by (B3). To apply Theorem 3, it is sufficient to have\n\u03b2 := 2c\u2032min r,t ptr(m)(1\u2212max t at \u221a \u03b1)) \u2265 2c\u2032(min r,t ptr(m)\u2212 \u221a \u03b1)\n\u2265 2c\u2032(1\u2212 \u221a \u03b1\u2212 [1\u2212 (pmin \u2212\n\u03b3\n162f(\u03b1) )]m) > 2c\u2032(1\u2212\n\u221a \u03b1\u2212 [1\u2212 \u221a \u03b1]m) > 1\nby our requirement that m > 1, and c\u2032 > 1 2[1\u2212 \u221a \u03b1\u2212(1\u2212 \u221a \u03b1)m] .\nt0 \u2265 max  ( 16(c\u2032)2B (1\u2212[1\u2212(pmin\u2212 \u03b3 162f(\u03b1) )]m)2\u22060 ) 1 \u03b2\u22121 , [ 48(c\u2032)2B2\n(1\u2212[1\u2212(pmin\u2212 \u03b3 162f(\u03b1) )]m)2(\u03b2\u22121)(\u22060)2\nln 1 \u03b4\n] 2\n\u03b2\u22121 ,(ln 1 \u03b4\n) 2 \u03b2\u22121  \u2265 max{(16a 2 maxB\n\u22060 )\n1 \u03b2\u22121 , [\n48a2maxB 2\n(\u03b2 \u2212 1)(\u22060)2 ln 1 \u03b4 ] 2 \u03b2\u22121 , (ln 1 \u03b4 ) 2 \u03b2\u22121 }\nwith amax := c \u2032 minr,t ptr(m) . Both are satisfied by our requirement on c\u2032 and to, so applying Theorem 3 we get \u2200t \u2265 1,\nE{\u2206t|\u2126t, F} = E{\u2206t|\u2126t} = O( 1\nt )\nwhere \u2126t := {\u2206t\u22121 \u2264 b0\u03c6opt} and Pr{\u2126t|F} \u2265 1\u2212 \u03b4. So\nPr{\u2126t, F} = Pr{\u2126t|F}Pr{F} \u2265 (1\u2212 \u03b4)(1\u2212 \u03be)\nFinally, using Lemma 11, and letting Gt := \u2126t \u2229 F , we get the desired result."}, {"heading": "8 Appendix D: technical lemmas", "text": "Lemma 15. Letwt, gt denote vectors of dimension Rd at time t. If we choosew0 arbitrarily, and for t = 1 . . . T , we repeatdly apply the following update\nwt = (1\u2212 1\nt )wt\u22121 +\n1 t gt\nThen\nwT = 1\nT T\u2211 t=1 gt\nProof. We prove by induction on T . For T = 1, w1 = (1\u2212 1)w0 + g1 = 11 \u22111 t=1 gt. So the claim holds for T = 1.\nSuppose the claim holds for T , then for T + 1, by the update rule\nwT+1 = (1\u2212 1\nT + 1 )wT +\n1\nT + 1 gT+1\n= (1\u2212 1 T + 1 ) 1 T T\u2211 t=1 gt + 1 T + 1 gT+1\n= T\nT + 1\n1\nT T\u2211 t=1 gt + 1 T + 1 gT+1\n= 1\nT + 1 T+1\u2211 t=1 gt\nSo the claim holds for any T \u2265 1.\nLemma 16 (technical lemma). For \u03b2 \u2208 (1, 2]. If C \u2265 \u03b2\u22121 3 , \u03b4 \u2264 1 e , and t \u2265 ( 3C \u03b2\u22121 ln 1 \u03b4 ) 2 \u03b2\u22121 , then t\u03b2\u22121 \u2212 2C ln t\u2212 C ln 1 \u03b4 > 0.\nProof. Let f(t) := t\u03b2\u22121\u22122C ln t\u2212C ln 1 \u03b4 . Taking derivative, we get f \u2032(t) = (\u03b2\u22121)t\u03b2\u22122\u2212 2C t \u2265 0 when t \u2265 ( 2C \u03b2\u22121 ) 1 \u03b2\u22121 . Since ln 1 \u03b4 3C \u03b2\u22121 \u2265 3C \u03b2\u22121 \u2265 1, (ln 1 \u03b4 3C \u03b2\u22121 ) 2 \u03b2\u22121 \u2265 ( 2C \u03b2\u22121 ) 1 \u03b2\u22121 , it suffices to show f((ln 1 \u03b4 3C \u03b2\u22121 ) 2 \u03b2\u22121 ) >\n0 for our statement to hold. f((ln 1 \u03b4 3C \u03b2\u22121 )\n2 \u03b2\u22121 ) = (ln 1\n\u03b4 3C \u03b2\u22121 ) 2 \u2212 2C ln{(ln 1 \u03b4 3C \u03b2\u22121 )\n2 \u03b2\u22121 } \u2212 C ln 1\n\u03b4 =\n(ln 1 \u03b4 )2 9C\n2\n(\u03b2\u22121)2 \u2212 4C \u03b2\u22121 ln(ln 1 \u03b4 3C \u03b2\u22121 ) \u2212 C ln 1 \u03b4 = 4C \u03b2\u22121 [\n3 2 C\n\u03b2\u22121 ln 1 \u03b4 \u2212 ln( 3C \u03b2\u22121 ln 1 \u03b4 )] + C ln 1 \u03b4 [ 3C (\u03b2\u22121)2 \u2212 1] > 0,\nwhere the first term is greater than zero because x\u2212 ln(2x) > 0 for x > 0, and the second term is greater than zero by our assumption on C.\nLemma 17 (Lemma D1 of [3]). Consider a nonnegative sequence (ut : t \u2265 to), such that for some constants a, b > 0 and for all t > to \u2265 0, ut \u2264 (1\u2212 at )ut\u22121 + b t2 . Then, if a > 1,\nut \u2264 ( to + 1\nt+ 1 )auto +\nb a\u2212 1(1 + 1 to + 1 )a+1 1 t+ 1"}], "references": [{"title": "k-means++: the advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop", "author": ["Pranjal Awasthi", "Or Sheffet"], "venue": "APPROX 2012, and 16th International Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "The fast convergence of incremental PCA. In Advances in Neural Information Processing Systems", "author": ["Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Online learning and stochastic approximations", "author": ["L\u00e9on Bottou"], "venue": "On-line learning in neural networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Convergence properties of the k-means algorithms", "author": ["L\u00e9on Bottou", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems 7, [NIPS Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Scatter/gather: A cluster-based approach to browsing large document collections", "author": ["Douglas R. Cutting", "Jan O. Pedersen", "David R. Karger", "John W. Tukey"], "venue": "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Using the triangle inequality to accelerate k-means", "author": ["Charles Elkan"], "venue": "In Machine Learning, Proceedings of the Twentieth International Conference (ICML", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Escaping from saddle points - online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Data clustering: 50 years beyond k-means", "author": ["Anil K. Jain"], "venue": "Pattern Recognition Letters,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu"], "venue": "Comput. Geom.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["Amit Kumar", "Ravindran Kannan"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1982}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Web-scale k-means clustering", "author": ["D. Sculley"], "venue": "In Proceedings of the 19th International Conference on World Wide Web, WWW", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "We analyze online [5] and mini-batch [16] k-means variants.", "startOffset": 18, "endOffset": 21}, {"referenceID": 15, "context": "We analyze online [5] and mini-batch [16] k-means variants.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "The k-means objective is non-convex and non-differentiable: we exploit ideas from recent work on stochastic gradient descent for non-convex problems [8, 3] by providing a novel characterization of the trajectory of k-means algorithm on its solution space, and circumvent the non-differentiability problem via geometric insights about k-means update.", "startOffset": 149, "endOffset": 155}, {"referenceID": 2, "context": "The k-means objective is non-convex and non-differentiable: we exploit ideas from recent work on stochastic gradient descent for non-convex problems [8, 3] by providing a novel characterization of the trajectory of k-means algorithm on its solution space, and circumvent the non-differentiability problem via geometric insights about k-means update.", "startOffset": 149, "endOffset": 155}, {"referenceID": 12, "context": "Lloyd\u2019s algorithm (batch k-means) [13] is one of the most popular heuristics for clustering [9].", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "Lloyd\u2019s algorithm (batch k-means) [13] is one of the most popular heuristics for clustering [9].", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "Even with fast implementations such as [7], which reduces the computation for finding the closest centroid of each point, the per-iteration running time still depends linearly on n, making it a computational bottleneck for large datasets.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "To scale up the centroid-update phase, a plausible recipe is the \u201cstochastic approximation\u201d scheme [4]: the overall idea is, at each iteration, the centroids are updated using one (online [5]) or a few (mini-batch [16]) randomly sampled points instead of the entire dataset.", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "To scale up the centroid-update phase, a plausible recipe is the \u201cstochastic approximation\u201d scheme [4]: the overall idea is, at each iteration, the centroids are updated using one (online [5]) or a few (mini-batch [16]) randomly sampled points instead of the entire dataset.", "startOffset": 188, "endOffset": 191}, {"referenceID": 15, "context": "To scale up the centroid-update phase, a plausible recipe is the \u201cstochastic approximation\u201d scheme [4]: the overall idea is, at each iteration, the centroids are updated using one (online [5]) or a few (mini-batch [16]) randomly sampled points instead of the entire dataset.", "startOffset": 214, "endOffset": 218}, {"referenceID": 15, "context": "Empirically, stochastic k-means has gained increasing attention for large-scale clustering and is included in widely used machine learning packages, such as Sofia-ML [16] and scikit-learn [14].", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "Empirically, stochastic k-means has gained increasing attention for large-scale clustering and is included in widely used machine learning packages, such as Sofia-ML [16] and scikit-learn [14].", "startOffset": 188, "endOffset": 192}, {"referenceID": 11, "context": "Figure 1a demonstrates the efficiency of stochastic k-means against batch k-means on the RCV1 dataset [12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "(a) Figure from [16], demonstrating the relative performance of online, mini-batch, and batch k-means.", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "Algorithm 1 and stochastic k-means [5, 16] are equivalent up to the choice of learning rate and sampling scheme (the proof of equivalence is in Appendix A).", "startOffset": 35, "endOffset": 42}, {"referenceID": 15, "context": "Algorithm 1 and stochastic k-means [5, 16] are equivalent up to the choice of learning rate and sampling scheme (the proof of equivalence is in Appendix A).", "startOffset": 35, "endOffset": 42}, {"referenceID": 4, "context": "In [5, 16], the per-cluster learning rate is chosen as \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r ; in our analysis, we choose a flat learning rate \u03b7 = c \u2032 to+t for all clusters, where c\u2032, to > 0 are some fixed constants (empirically, no obvious differences are observed; see Section 3.", "startOffset": 3, "endOffset": 10}, {"referenceID": 15, "context": "In [5, 16], the per-cluster learning rate is chosen as \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r ; in our analysis, we choose a flat learning rate \u03b7 = c \u2032 to+t for all clusters, where c\u2032, to > 0 are some fixed constants (empirically, no obvious differences are observed; see Section 3.", "startOffset": 3, "endOffset": 10}, {"referenceID": 0, "context": "The major advantage of Algorithm 2 over other choices of initialization, such as the k-means++ [1] or random sampling, is that its running time is independent of the data size while providing seeding guarantee.", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "We assume, similar to [11], that the means of each pair of clusters are well-separated and that the points from the two clusters are separated by a \u201cmargin\u201d, that is, \u2200x \u2208 A r \u222aA s , the distance from x to the bisector of c r and c s is lower bounded.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "The proposed clustering scheme is reminiscent to the Buckshot algorithm [6], widely used in the domain of document clustering.", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "Our major source of inspiration comes from recent advances in non-convex stochastic optimization for unsupervised learning problems [8, 3].", "startOffset": 132, "endOffset": 138}, {"referenceID": 2, "context": "Our major source of inspiration comes from recent advances in non-convex stochastic optimization for unsupervised learning problems [8, 3].", "startOffset": 132, "endOffset": 138}, {"referenceID": 7, "context": "[8] studies the convergence of stochastic gradient descent (SGD) for the tensor decomposition problem, which amounts to finding a local optimum of a nonconvex objective function composed exclusively of saddle points and local optima.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "The learning rate in [16, 5], where \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r , is intuitively making this adaptation: in the case when the clustering assignment does not change between iterations, it can be seen that E\u03b7 r \u2248 1 tpr(m) , so the effective learning rate \u03b7 t rp t r(m) is balanced for different clusters and is roughly 1t .", "startOffset": 21, "endOffset": 28}, {"referenceID": 4, "context": "The learning rate in [16, 5], where \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r , is intuitively making this adaptation: in the case when the clustering assignment does not change between iterations, it can be seen that E\u03b7 r \u2248 1 tpr(m) , so the effective learning rate \u03b7 t rp t r(m) is balanced for different clusters and is roughly 1t .", "startOffset": 21, "endOffset": 28}, {"referenceID": 14, "context": "Lemma 3 resembles the standard iteration-wise convergence statement in SGD analysis, typically via convexity or smoothness of a function [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "Instead, our analysis relies on the geometric property of Voronoi diagram and the mean operation used in a k-means iteration, similar to those in recent works on batch k-means [11, 2, 17].", "startOffset": 176, "endOffset": 187}, {"referenceID": 1, "context": "Instead, our analysis relies on the geometric property of Voronoi diagram and the mean operation used in a k-means iteration, similar to those in recent works on batch k-means [11, 2, 17].", "startOffset": 176, "endOffset": 187}, {"referenceID": 2, "context": "To deal with this, we exploit probability tools developed in [3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "[3] studies the convergence of stochastic PCA algorithms, where the objective function is the non-convex Rayleigh quotient, which has a plateau-like component.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "We upper bound the probability of this bad event (Proposition 1) using techniques that derive tight concentration inequality via moment generating functions from [3], which in turn implies a lower bound on the probability of \u03a9t, t \u2265 i.", "startOffset": 162, "endOffset": 165}, {"referenceID": 11, "context": "To verify the O( 1t ) global convergence rate of Theorem 1, we run stochastic k-means with varying learning rate, mini-batch size, and k on RCV1 [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "The dataset is relatively large in size: it has manually categorized 804414 newswire stories with 103 topics, where each story is a 47236-dimensional sparse vector; it was used in [16] for empirical evaluation of mini-batch k-means.", "startOffset": 180, "endOffset": 184}, {"referenceID": 13, "context": "We used Python and its scikit-learn package [14] for our experiments, which has stochastic k-means implemented.", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "We disabled centroid relocation and modified their source code to allow a user-defined learning rate (their learning rate is fixed as \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r , as in [5, 16], which we refer to as BBS-rate).", "startOffset": 167, "endOffset": 174}, {"referenceID": 15, "context": "We disabled centroid relocation and modified their source code to allow a user-defined learning rate (their learning rate is fixed as \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r , as in [5, 16], which we refer to as BBS-rate).", "startOffset": 167, "endOffset": 174}, {"referenceID": 0, "context": "[1] David Arthur and Sergei Vassilvitskii.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Pranjal Awasthi and Or Sheffet.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] L\u00e9on Bottou.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] L\u00e9on Bottou and Yoshua Bengio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Douglas R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Charles Elkan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Anil K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Tapas Kanungo, David M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Amit Kumar and Ravindran Kannan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] David D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Equivalence of Algorithm 1 to stochastic k-means Here, we formally show that Algorithm 1 with specific instantiation of sample size m and learning rates \u03b7 r is equivalent to online k-means [5] and mini-batch k-means [16].", "startOffset": 189, "endOffset": 192}, {"referenceID": 15, "context": "Equivalence of Algorithm 1 to stochastic k-means Here, we formally show that Algorithm 1 with specific instantiation of sample size m and learning rates \u03b7 r is equivalent to online k-means [5] and mini-batch k-means [16].", "startOffset": 216, "endOffset": 220}, {"referenceID": 4, "context": "3, [5]].", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "When m > 1, the update of Algorithm 1 is equivalent to that described from line 3 to line 14 in [Algorithm 1, [16]] with mini-batch size m.", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "3, [5]].", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "We substitute index k in [5] with r used in Algorithm 1.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "According to the update rule in [5], \u2206nk = 1 if the sampled point xi is assigned to cluster with center wk.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "Therefore, the update of the k-th centroid according to online k-means in [5] is: wk \u2190 wk + 1 nk (xi \u2212 wk)1{\u2206nk=1}", "startOffset": 74, "endOffset": 77}, {"referenceID": 15, "context": "For the second claim, consider line 4 to line 14 in [Algorithm 1, [16]].", "startOffset": 66, "endOffset": 70}, {"referenceID": 15, "context": "Hence, the updates in Algorithm 1 and line 4 to line 14 in [Algorithm 1, [16]] are equivalent.", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "Lemma 9 (adapted from [3]).", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": ", [3]) \u2200\u03b2 > 0, k \u2265 1, \u03a0t=1(1\u2212 \u03b2 to + (i\u2212 t+ 1) ) \u2264 ( to + i\u2212 k + 1 to + i )", "startOffset": 2, "endOffset": 5}, {"referenceID": 10, "context": "4 of [11]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "4 of [11]; we include it here for completeness.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "1 of [10]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "Lemma 17 (Lemma D1 of [3]).", "startOffset": 22, "endOffset": 25}], "year": 2016, "abstractText": "We analyze online [5] and mini-batch [16] k-means variants. Both scale up the widely used k-means algorithm via stochastic approximation, and have become popular for large-scale clustering and unsupervised feature learning. We show, for the first time, that starting with any initial solution, they converge to a \u201clocal optimum\u201d at rateO( 1t ) (in terms of the k-means objective) under general conditions. In addition, we show if the dataset is clusterable, when initialized with a simple and scalable seeding algorithm, mini-batch k-means converges to an optimal k-means solution at rate O( 1t ) with high probability. The k-means objective is non-convex and non-differentiable: we exploit ideas from recent work on stochastic gradient descent for non-convex problems [8, 3] by providing a novel characterization of the trajectory of k-means algorithm on its solution space, and circumvent the non-differentiability problem via geometric insights about k-means update.", "creator": "LaTeX with hyperref package"}}}