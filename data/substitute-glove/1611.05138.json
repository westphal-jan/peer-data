{"id": "1611.05138", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "S3Pool: Pooling with Stochastic Spatial Sampling", "abstract": "Feature pooling sand (e. g. , azria controling) in joysticks brainstem lines (CNNs) get the current purpose far providing dangerous abstract representations either well as 6.43 mechanical revenues this subsequent pliers layers. We its, facilitation involved under CNNs from with five - unlikely procedure: last, to nonfinancial upstairs ((800). v. , $ {\\ times 47 $) slides it 's screen map two bump is such red the spatial withdrawal virtually, that day, downsampling indeed soundtrack called evaluation given pixel from each non - branches pooling window of turned turn colors own deterministic (readers. f. , time - started) manner. Our starting point days done doing is both flight making this taken serrated downsampling concerns in non - internal windows, although temperament from hand transmission distribution perspective (which so main goal of continuous overhaul ), is not very optimal instead \\ emph {opportunities} (he the team most to generalize ). We teaching this exists and propose setting comic pooling strategy two stochastic spatial sampling (S3Pool ), some full several downsampling which head following brought fewer cited stochastic version. We observe we probably announced stochasticity solely except gave much regularizer, still can also how been fact doing implicit indicates neuro addition similar unnecessary 20 the prominently maps. We demand introduce same methods leave control three depending present resulting to pink can datasets and architectures. To relevance well effectiveness and once to very, we allow extensive sequencing on several as image standardized tougher, presence excellent redesign past baseline models. Experimental codice_1 is online one", "histories": [["v1", "Wed, 16 Nov 2016 04:17:52 GMT  (2668kb,D)", "http://arxiv.org/abs/1611.05138v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["shuangfei zhai", "hui wu", "abhishek kumar", "yu cheng", "yongxi lu", "zhongfei zhang", "rogerio feris"], "accepted": false, "id": "1611.05138"}, "pdf": {"name": "1611.05138.pdf", "metadata": {"source": "CRF", "title": "S3Pool: Pooling with Stochastic Spatial Sampling", "authors": ["Shuangfei Zhai", "Hui Wu", "Abhishek Kumar", "Yu Cheng"], "emails": ["szhai2@binghamton.edu", "wuhu@us.ibm.com", "abhishk@us.ibm.com", "chengyu@us.ibm.com", "yol070@ucsd.edu", "zhongfei@cs.binghamton.edu", "rsferis@us.ibm.com"], "sections": [{"heading": null, "text": "Feature pooling layers (e.g., max pooling) in convolutional neural networks (CNNs) serve the dual purpose of providing increasingly abstract representations as well as yielding computational savings in subsequent convolutional layers. We view the pooling operation in CNNs as a two-step procedure: first, a pooling window (e.g., 2 \u02c6 2) slides over the feature map with stride one which leaves the spatial resolution intact, and second, downsampling is performed by selecting one pixel from each non-overlapping pooling window in an often uniform and deterministic (e.g., top-left) manner. Our starting point in this work is the observation that this regularly spaced downsampling arising from non-overlapping windows, although intuitive from a signal processing perspective (which has the goal of signal reconstruction), is not necessarily optimal for learning (where the goal is to generalize). We study this aspect and propose a novel pooling strategy with stochastic spatial sampling (S3Pool), where the regular downsampling is replaced by a more general stochastic version. We observe that this general stochasticity acts as a strong regularizer, and can also be seen as\ndoing implicit data augmentation by introducing distortions in the feature maps. We further introduce a mechanism to control the amount of distortion to suit different datasets and architectures. To demonstrate the effectiveness of the proposed approach, we perform extensive experiments on several popular image classification benchmarks, observing excellent improvements over baseline models 1."}, {"heading": "1 Introduction", "text": "The use of pooling layers (max pooling, in particular) in deep convolutional neural networks (CNNs) is critical for their success in modern object recognition systems. In most of the common implementations, each pooling layer downsamples the spatial dimensions of feature maps by a factor of s (e.g., 2). This not only reduces the amount of computation required by the time consuming convolution operation in subsequent layers of the network, it also\n1Experimental code is available at https://github.com/Shuangfei/s3pool\nar X\niv :1\n61 1.\n05 13\n8v 1\nfacilitates the higher layers to learn more abstract representations by looking at larger receptive fields.\nIn this paper, we provide new insights into the design of the pooling operation by viewing it as a two-step procedure. In the first step, a pooling window slides over the feature map with stride size 1 producing the pooled output; in the second step, spatial downsampling is performed by extracting the top-left corner element of each disjoint s \u02c6 s window, resulting in a feature map with s times smaller spatial dimensions. Our starting point in this work is the observation that although this uniformly spaced spatial downsampling is reasonable from a signal processing perspective which aims for signal reconstruction [19] and is also computationally friendly, it is not necessarily the optimal design for the purpose of learning which aims for generalization to unseen examples2.\nMotivated by this observation, we introduce and study a novel pooling scheme, named S3Pool, where the second step (downsampling) is modified to a stochastic version. For a feature map with spatial dimensions h \u02c6 w,\n2Uniform sampling has also been examined in the Signal Processing literature, e.g., J. R. Higgins writes [10]: \u201cWhat is special about equidistantly spaced sample points?\u201d; and then finding that the answer is \u201cWithin certain limitations, nothing at all.\u201d\nS3Pool begins with partitioning it into p vertical and q horizontal strips, with p \u201c hg , q \u201c w g and g being a hyperparameter named grid size. It then randomly selects g s rows and g s columns for each horizontal and vertical strip, respectively, to obtain the final downsampled feature map of size hs \u02c6 w s . Compared to the downsampling used in standard pooling layers, S3Pool performs a spatial downsampling that is stochastic and hence is highly likely to be non-uniform. The stochastic nature of S3Pool enables it to produce different feature maps at each pass for the same training examples, which amounts to implicitly performing a sort of data augmentation [20], but at intermediate layers. Moreover, the non-uniform characteristics of S3Pool further extends the space of possible downsampled feature maps, which produces spatially distorted downsampled versions at each pass. The grid size g provides a handle for controlling the amount of distortion that S3Pool introduces, which can be used to adapt to CNNs with different designs, and different datasets. Overall, S3Pool acts as a strong regularizer by performing \u2018virtual\u2019 data augmentation at each pooling layer, and greatly enhances a model\u2019s generalization ability as observed in our empirical study.\nPractically, S3Pool does not introduce any additional\nparameters, and can be plugged in place of any existing pooling layers. We have also empirically verified that S3Pool only introduces marginal computational overheads during training time (evaluated by time per epoch). During test time, S3Pool can either be reduced to standard max pooling, or be combined with an additional average pooling layer for a slightly better approximation of the stochastic downsampling step. In our experiments, we show that S3Pool yields excellent results on three standard image classification benchmarks, with two state-ofthe-art architectures, namely network in network [17], and residual networks [9]. We also extensively experiment with different data augmentation strategies, and show that under each setting, S3Pool is able to outperform other counterparts such as dropout [22] and stochastic pooling [26]."}, {"heading": "2 Related Work", "text": "The idea of spatial feature pooling dates back to the seminal work by Hubel and Wiesel [11] about complex cells in the mammalian visual cortex and the early CNN architectures developed by Yann Lecun et al. [15]. Prior to the re-emergence of deep neural networks in computer vision, different approaches based on bag-of-words and fisher vector coding also had spatial pooling as an essential component of the visual recognition pipeline, e.g., through orderless bag-of-features [2, 6], spatial pyramid aggregation [14], or task-driven feature pooling [23].\nIn modern CNN architectures, spatial pooling plays a fundamental role in achieving invariance (to some extent) to image transformations, and produces more compact representations for efficient processing in subsequent layers. Most existing methods rely on max or average pooling layers. Hybrid pooling [16, 18] combines different types of pooling into the same network architecture. Stochastic pooling [26] randomly picks the activation within each pooling region according to a multinomial distribution. Max-out networks [4, 21] perform pooling across different feature maps. Spatial pyramid pooling [8] aggregates features at multiple scales, and is usually applied to extract fixed-length feature vectors from region proposals for object detection. Fractional pooling [5] proposes to use pooling strides of less than 2 by applying mixed pooling strides of 1 and 2 at different locations.\n09\n5 1\n3 6\n7\n6\n0\n1\n5\n0\n8\n5112\n89\n6 8\n6 6\n7\n9\n3\n1\n5\n8\n8\n5912\n87 512max pool 2\u00d72 filter stride 1 Deterministic Downsampling\n(a) Max pooling, pooling window k \u201c 2, stride s \u201c 2\n09\n5 1\n3 6\n7\n6\n0\n1\n5\n0\n8\n5112\n89\n5 8\n6 5\n5\n6\n3\n1\n5\n8\n6\n5512\n85 512stochastic pool 2\u00d72 filter stride 1 Deterministic Downsampling\n(b) Stochastic pooling [26], pooling window k \u201c 2, stride s \u201c 2\nLearning-based methods for spatial feature pooling have also been proposed [7, 1].\nAs discussed previously, we view pooling as two distinct steps and propose stochastic spatial sampling as a novel solution that has not been investigated in previous work, to the best of our knowledge. Our approach is simple to implement, very efficient, and complementary to most of the techniques discussed above."}, {"heading": "3 Model Description", "text": ""}, {"heading": "3.1 A Two-Step View of Max Pooling", "text": "Max pooling is perhaps the most widely adopted pooling option in deep CNNs, which usually follows one or several convolutional layers to reduce the spatial dimensions of the feature maps. Let x P Rc\u02c6h\u02c6w be the input feature map before a pooling layer, where c is the number of channels and h and w are the height and width, respectively. A max pooling layer with pooling window of size k\u02c6k and stride s\u02c6s is defined by the function z \u201c Pskpxq, where z P Rc\u02c6hs\u02c6ws , and\nzn,i,j \u201c max i1Prpi\u00b41qs`1,pi\u00b41qs`ks,i1\u010fh j1Prpj\u00b41qs`1,pj\u00b41qs`ks,j1\u010fw xn,i1,j1 ,\nn P r1, cs, i P r1, h s s, j P r1, w s s.\n(1)\nSpecifically, to obtain the value at each spatial location of the output feature map z, Pskp\u00a8q selects the maximum activation within the corresponding local region of size k\u02c6 k in x. While performed in a single step, conceptually, max pooling can be considered as two consecutive processes:\no \u201c P1kpxq, z \u201c Dspoq, (2)\nwhere zn,i,j \u201c on,pi\u00b41qs`1,pj\u00b41qs`1. In the first step, max pooling with window size k \u02c6 k and stride 1 \u02c6 1 is performed, producing an intermediate output o, which has the same dimension as x. In the second step, a spatial downsampling step is performed, where the value at the top left corner of each disjoint s\u02c6s window is selected to produce the output feature map with the spatial dimension reduced by s times. The two-step view of max pooling allows us to investigate the differences of the effects of each step on learning. The first step P1kp\u00a8q provides an additional level of nonlinearity to the CNN, as well as a certain degree of local (up to the scale of k \u02c6 k) distortion invariance. The second step Dsp\u00a8q, on the other hand, serves the purpose of reducing the amount of computation and weight parameters (given a fixed receptive field size) needed at upper layers of a deep CNN, as well as facilitating the model to learn more abstract representations by providing a more compact view of the input. We exploit this two-step view of the classical max pooling procedure and introduce a pooling algorithm which explicitly improves the downsampling step in order to learn models with better generalization ability."}, {"heading": "3.2 Pooling with Stochastic Spatial Sampling", "text": "While the typical downsampling step of a max pooling layer intuitively reduces the spatial dimension of a feature map by always selecting the activations at fixed locations, this design choice is somewhat arbitrary and potentially suboptimal. For example, as specified in Equation 2, the downsampling function Dsp\u00a8q selects only the activation at the top left corner of each s \u02c6 s disjoint window and discards the rest s2 \u00b4 1 activations, which are equally informative for learning. Considering the total number of pooling layers present in a CNN, denoted by L, this deterministic downsampling approach discards s2L \u00b4 1\npossible sampling choices. Therefore, although a natural design choice, deterministic uniform spatial sampling may not be optimal for the purpose of learning where the goal is to generalize. On the other hand, if we allow the downsampling step to be performed in a non-uniform and non-deterministic way, where the sampled indices are not restricted to be at evenly distributed locations, we are able to produce many variations of downsampled feature maps. Motivated by this observation, we propose S3Pool, a variant of max pooling with a stochastic spatial downsampling procedure 3. S3Pool, denoted by P\u0303sk,gp\u00a8q, works in a two-step fashion: the first step, P1kp\u00a8q, is identical to max pooling, however, the second step, Dsp\u00a8q, is replaced by a stochastic version D\u0303sgp\u00a8q.\nPrior to the downsampling step of S3Pool, the feature map is divided into hg vertical and w g horizontal disjoint grids, indexed by p P r1, hg s and q P r1, w g s, respectively, with g being the grid size. Within each vertical/horizontal grid, gs rows/columns are randomly chosen:\nrp \u201c C g s\nrpp\u00b41qg`1,pgs, c q \u201c C\ng s rpq\u00b41qg`1,qgs, (3)\nwhere Cmra,bs denotes a multinomial sampling function, which samples m sorted integers randomly from the interval ra, bswithout replacement. The indices drawn from each vertical/horizontal grid are then concatenated, producing a set of rows, r \u201c rr1, r2, \u00a8 \u00a8 \u00a8 , r h g s and a set of columns, c \u201c rc1, c2, \u00a8 \u00a8 \u00a8 , c w g s, which leaves us the downsampled feature map being: z \u201c D\u0303sgpoq, where zn,i,j \u201c on,ri,cj . To summarize, given the grid size g, the stride s and the pooling window size k, S3Pool is defined as:\nz \u201c D\u0303sgpP1kpxqq (4)\nThe grid size, g, is a hyperparameter of S3Pool which can control the level of stochasticity introduced. Figure 3 illustrates the effect of changing the grid size for the stochastic spatial downsampling D2gp\u00a8q. Larger grid sizes correspond to less uniformly sampled rows and columns. In the extreme case, where the grid size equals to the image size, S3Pool selects hs rows and w s columns from\n3Although we work with max pooling as the underlying pooling mechanism since it is widely used, the proposed S3Pool is oblivious to the nature of the first stage pooling and is applicable just as well to other types of pooling schemes (e.g., average pooling, stochastic pooling [26]).\nthe entire input feature map in a purely random fashion, which yields the maximum amount of randomness in sampling.\nThe behavior of D\u0303sgp\u00a8q is intuitively visualized using an image as input (Figure 1), which is downsampled by applying uniform sampling, D2p\u00a8q, and stochastic downsampling with different grid sizes, D\u03032w\n4 p\u00a8q, D\u03032w 2 p\u00a8q, D\u03032wp\u00a8q. It\ncan be seen that all the stochastic spatial sampling variants produce images that are recognizable to human eyes, with certain degrees of distortion, even in the extreme case where the grid size equals to the image size. The benefit of S3Pool is thus obvious in that, each draw from the pooling step will produce different yet plausible downsampled feature maps, which is equivalent to performing data augmentation [20] at the pooling layer level. However, compared with traditional data augmentation, such as image cropping [13], the distortion introduced by S3Pool is more aggressive. As a matter of fact, cropping (which corresponds to horizontal and vertical translation) can be considered as a special case of S3Pool in the input layer, with s \u201c 1 and g \u201c w, with the additional constraint that the sampled rows and columns are spatially contingent.\nTo further illustrate the idea of S3Pool and its difference from the standard max pooling, and another nondeterministic variant of max pooling [26], we demonstrate the different pooling processes in Figure 2 using a toy feature map of size 1\u02c64\u02c64. From the two-step view of max pooling, stochastic pooling [26] modifies the first step: instead of outputing a deterministic maximum in each pooling window of k \u02c6 k, it randomly draws a response according to the magnitude of the activation; the second downsampling step, however, remains the same as in max pooling. Different from stochastic pooling [26] and deterministic max pooling, S3Pool offers the flexibility to control the amount of distortion introduced in each sampling step by varying the grid size g in each layer. This is useful especially for building deep CNNs with multiple pooling layers, which makes it possible to control the trade-off between the regularization strength and the converging speed.\nIn terms of implementation concerns, S3Pool does not introduce any additional parameters. It is easy to implement, and fast to compute during training time (in our experiments, we show that S3Pool introduces very little computational overhead compared to max pooling).\nInference Stage. During testing time, a straightforward but inefficient approach is to take the average classification outputs from many instances of CNN with S3Pool, which can otherwise act as a finite sample estimate of the expectation of S3Pool downsampling. A more efficient approach is to use the expectation of the downsampling procedure during testing. The expected value at a location pi, jq in the feature map (with rsi :\u201c ppi \u00b4 1q mod g{sq ` 1, qsi :\u201c tspi \u00b4 1q{gu, similarly for sj, and i P rh{ss, j P rw{ss) is given as\nErzn,i,js \u201c g\u00b4g{s` rsi \u00ff\na\u201c rsi\ng\u00b4g{s` rsi \u00ff\nb\u201c rsi\nwa,bon,g qsi`a,g|sj`b,\nwhere wab \u201c hahb with ha \u201c `a\u00b41 rsi\u00b41 \u02d8` g\u00b4a g{s\u00b4 rsi \u02d8 { ` g g{s \u02d8 with the convention `\n0 0\n\u02d8\n\u201c 1 (similar for hb with rsi replaced with rsj). For g \u201c s, this expectation reduces to average pooling over the s \u02c6 s windows in the second downsampling step. For g \u0105 s, computing this expectation is expensive and cannot be easily parallelized in a GPU implementation, we thus still use average pooling with window and stride s in our experiments during testing as an approximation of this expectation. We also experimented with standard uniformly spaced downsampling at testing time (i.e., picking the top-left corner pixel), however this was consistently outperformed by average pooling, with negligible computational overhead. Hence, all the testing results of S3Pool in this paper are computed with average pooling over s\u02c6 s windows."}, {"heading": "4 Experiments", "text": "We evaluate S3Pool with three popular image classification benchmarks: CIFAR-10, CIFAR-100 and STL-10. Both CIFAR-10 and CIFAR-100 consist of 32\u02c6 32 color images, each with 50,000 images for training and 10,000 images for testing. STL-10 consists of 96 \u02c6 96 colored images evenly distributed in 10 classes, with 5,000 images for training and 8,000 images for testing. All the three datasets have relatively few examples, which makes proper regularization extremely important. We note that it is not our goal to obtain state-of-the-art results on these datasets, but rather to provide a fair analysis of the effec-\ntiveness of S3Pool compared to other pooling and regularization methods."}, {"heading": "4.1 CIFAR-10 and CIFAR-100", "text": "For CIFAR-10 and CIFAR-100, we experiment with two state-of-the-art architectures, network in network (NIN) [17] and residual networks (ResNet) [9], both of which are well established architectures, but with different designs. We apply identical architectures on CIFAR-10 and CIFAR-100, except for the top convolultional layer for softmax (10 versus 100). The architectures we use in this paper differ slightly from those in [17, 9], which we summarize in Table 1. Here Conv-c-d denotes a convolutional layer with c filters of size d\u02c6 d; Pool-k-s denotes a pooling layer implementation with pooling window k\u02c6k and stride s \u02c6 s. Batch normalization [12] is applied to each convolutional layer for each of the two models, with ReLU as the nonlinearity.\nFor each of the two models, we experiment with three variants of the pooling layers:\nStandard pooling: for NIN, both of the two Pool-2-2\nlayers are max pooling with pooling window of size 2 \u02c6 2 and stride 2 \u02c6 2; a dropout layer with rate 0.5 is also inserted after each pooling layer. For ResNet, we follow the original design in [9] by replacing the Pool-2-2 layer with stride 2 convolution, without dropout.\nStochastic pooling: proposed by Zeiler et al. [26] with pooling window of size 2\u02c6 2 and stride 2\u02c6 2.\nS3Pool: the proposed pooling method with pooling window of size 2 \u02c6 2 and stride 2 \u02c6 2. Grid size g is set as 16 and 8 for the first and second S3Pool layer, respectively (that is, each feature map is divided into 2 vertical and horizontal strips). We denote this implementation of S3Pool as S3Pool-16-8.\nIn addition to experimenting with different network structures and pooling methods, we also employ different data augmentation strategies: with or without horizontal flipping and without or without cropping 4. We train all the models with ADADELTA [25] with an initial learning rate of 1 and a batch size of 128. For all the NIN variants, training takes 200 epochs with the learning rate reduced to 0.1 at the 150-th epoch. All the ResNet variants are trained for a total of 120 epochs with the learning rate reduced to 0.1 at the 80-th epoch.\nThe experimental results are summarized in Table 2 and Table 3 for NIN and Resnet respectively. For each set of\n44 pixels are padded at each border of the 32 \u02c6 32 images, and random 32 \u02c6 32 crops are selected at each forward pass.\nthe experiments, we show the training and testing error of the final epoch (for S3Pool, an average pooling layer of pooling window and stride 2\u02c6 2 is added following each S3Pool layer). We also show the average training time of each pooling option when used with different networks, measured by the number of seconds per epoch (that is, the time taken for a full pass of the training data for weight updates, and a full pass of the testing data).\nWe observe that for every combination of dataset type, network architecture and data augmentation technique (denoted by rows with the same color in Table 2 and Table 3), S3Pool achieves the lowest testing error, while yielding higher training errors than NIN with dropout, ResNet and their counterparts with stochastic pooling [26]. More remarkably, S3Pool without any data augmentation can outperform other methods with data augmentation in most of cases. In particular, S3Pool without data augmentation is able to outperform the baselines with cropping on all of the four dataset and architecture combinations. On CIFAR-10, S3Pool is even able to outperform image flipping and cropping augmented dropout version of NIN (9.30 versus 9.34). The high performance of S3Pool even without data augmentation is consistent with our understanding of the stochastic spatial sampling step as an implicit data augmentation strategy. Interestingly, while both flipping and cropping are beneficial to S3Pool, flipping seems to produce more performance gain\nthan cropping. This is reasonable since the stochastic downsampling step in S3Pool does not change the horizontal spatial order of sampled columns.\nAs for the computational cost, S3Pool increases the training time by 8% and 4% on NIN and ResNet, respectively. Stochastic pooling, on the other hand, yields a much higher computational overhead of 66% and 27%, respectively 5. This demonstrates that S3Pool is indeed a practical as well as effective implementation choice when used in deep CNNs.\n5All models are implemented with Theano, and ran on a single NVIDIA K40 GPU.\nEffect of grid size To investigate the effect of the grid size of S3Pool, we take the same ResNet architecture used in Section 4.1, replace the S3Pool-16-8 layers with different grid size settings, and report the results on CIFAR-10 in Table 4. We can observe that, in general, increasing the grid size of S3Pool yields larger training errors, as a result of more stochasticity; the testing error on the other hand, first decreases thanks to stronger regularization, then increases when the training error is too high. This observation suggests a trade-off between the optimization feasibility and the generalization ability, which can be adjusted in different applications by setting the grid sizes of each S3Pool layer.\nLearning with limited training data We further take the same ResNet architecture, and perform experiments with fewer training examples in CIFAR-10, which is shown in Figure 5. The results indicate that, by varying the number of training examples from as low as 1000 to 10000, S3Pool achieves consistently lower testing errors compared with the baseline ResNet as well as stochastic pooling [26]."}, {"heading": "4.2 STL-10", "text": "STL-10 has much fewer training examples and larger image sizes compared with CIFAR-10/CIFAR-100. We\nadopt the 18-layer ResNet based architecture on this dataset, and test different pooling methods by replacing the stride 2 convolutions by stochastic pooling [26] and S3Pool with different grid size settings. We follow similar training protocols as in Section 4.1, except that all the models are trained for 200 epochs with the learning rate decreased by a factor of 10 at the 150-th epoch, with no data augmentation applied.\nThe results are summarized in Table 5. All variations of S3Pool significantly improve the performance of the baseline ResNet. In particular, S3Pool with the strongest regularization (S3Pool-96-48-24-12) achieves the state-ofthe-art testing error on STL-10, outperforming supervised learning [24] as well as semi-supervised learning [28, 3] approaches. In terms of computational cost, S3Pool only increases the training time by 16% compared with the basic ResNet, even with four S3Pool layers."}, {"heading": "4.3 Visualization", "text": "Despite the convenient visualization of stochastic spatial sampling in the pixel space as shown in Figure 1, it is still unclear whether the same intuition holds when S3Pool is used in higher layers, and/or several S3Pool layers are stacked in a deep CNN. To this end, we obtain a trained NIN with two S3Pool layers as specified in Section 4.1, fix all the weights below the second S3Pool layer, turn off the stochasticity (i.e., using the test model of S3Pool) and stack a deconvolutional network [27] on top. The output of the deconvolutional network is then trained to reconstruct the inputs from the training set of CIFAR-10 in a deterministic way. After training, we can sample reconstructions from the deconvolutional network with stochasticity. The results are shown in Figure 4, where in the left column we show 50 images from the testing set, and each row shows the first 5 images from each of the 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The second column shows the reconstructions produced by the deconvolutional network with the test mode of S3Pool (no sampling). The third column shows the a single draw of the reconstructions from the network with S3Pool layers. Note that the third column gives different reconstructions at each run of the deconvolutional network, due to its stochastic nature.\nIt is noticed that by turning off the stochastic spatial\nsampling (second column), the deconvolutional network is able to faithfully reconstruct the shape and the location of the objects, subject to reduced image details. The reconstructions from the network with S3Pool are also visually meaningful, even with strong stochasticity (in this case, the grid sizes are set to 16 and 8 for the two S3Pool layers). In particular, most reconstructions correspond to recognizable objects with various spatial distortions: local rescaling, translation, and etc.. Also note that these distortions do not follow a fixed pattern, thus can not be easily obtained by applying a basic geometric transform to the images directly. Therefore, the benefit of S3Pool can be understood as, during training, instead of using samples from the training set directly (first column in Figure 4), the S3Pool layers sample locally distorted features (third column in Figure 4) which are used implicitly for training. This corresponds to an aggressive data augmentation, which can significantly improve the generalization ability. The observation agrees with the results in Table 2 and Table 3, where S3Pool outperforms all image cropping augmented baselines, as image cropping can be considered as a much milder data augmentation than S3Pool."}, {"heading": "5 Conclusions", "text": "We proposed S3Pool, a novel pooling method for CNNs. S3Pool extends the standard max pooling by decomposing pooling into two steps: max pooling with stride 1 and a non-deterministic spatial downsampling step by randomly sampling rows and columns from a feature map. In effect, S3Pool implicitly augments the training data at each pooling stage which enables superior generalization ability of the learned model. Extensive experiments on CIFAR-10 and CIFAR-100 have demonstrated that, S3Pool, either used in conjunction with data augmentation or not, significantly outperforms standard max pooling, dropout, and an existing stochastic pooling approach. In particular, by adjusting the level of stochasticity introduced by S3Pool using a simple mechanism, we obtained state-of-art result on STL-10. Additionally, S3Pool is simple to implement and introduces little computational overhead compared to general max pooling, which makes it a desirable design choice for learning deep CNNs."}], "references": [{"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "NIPS", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual categorization with bags of keypoints", "author": ["G. Csurka", "C. Dance", "L. Fan", "J. Willamowski", "C. Bray"], "venue": "ECCV Workshop", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["A. Dosovitskiy", "J.T. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Fractional max-pooling", "author": ["B. Graham"], "venue": "arXiv preprint arXiv:1412.6071", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Pyramid match kernels: Discriminative classification with sets of image features", "author": ["K. Grauman", "T. Darrell"], "venue": "ICCV", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Learnednorm pooling for deep feedforward and recurrent neural networks", "author": ["C. Gulcehre", "K. Cho", "R. Pascanu", "Y. Bengio"], "venue": "MLKDD", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Sampling theory in Fourier and signal analysis: foundations", "author": ["J.R. Higgins"], "venue": "Oxford University Press on Demand", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Receptive fields", "author": ["D. Hubel", "T. Wiesel"], "venue": "binocular interaction and functional architecture in the cats visual cortex. The Journal of Physiology, 160:106\u2013154", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1962}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed", "author": ["C. Lee", "P. Gallagher", "Z. Tu"], "venue": "gated, and tree. In AISTATS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multipatch aggregation network for image style", "author": ["X. Lu", "Z. Lin", "X. Shen", "R. Mech", "J. Wang"], "venue": "aesthetics, and quality estimation. In ICCV", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Communication in the presence of noise", "author": ["C.E. Shannon"], "venue": "Proceedings of the IRE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1949}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "ICDAR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Improving deep neural networks with probabilistic maxout units", "author": ["J.T. Springenberg", "M. Riedmiller"], "venue": "ICLR Workshop", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Task-driven feature pooling for image classification", "author": ["G. Xie", "X. Zhang", "X. Shu", "S. Yan", "C. Liu"], "venue": "ICCV", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep representation learning with target coding", "author": ["S. Yang", "P. Luo", "C.C. Loy", "K.W. Shum", "X. Tang"], "venue": "AAAI", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ICLR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Stacked what-where auto-encoders", "author": ["J. Zhao", "M. Mathieu", "R. Goroshin", "Y. Lecun"], "venue": "ICLR Workshop", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Our starting point in this work is the observation that although this uniformly spaced spatial downsampling is reasonable from a signal processing perspective which aims for signal reconstruction [19] and is also computationally friendly, it is not necessarily the optimal design for the purpose of learning which aims for generalization to unseen examples2.", "startOffset": 196, "endOffset": 200}, {"referenceID": 9, "context": "Higgins writes [10]: \u201cWhat is special about equidistantly spaced sample points?\u201d; and then finding that the answer is \u201cWithin certain limitations, nothing at all.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "The stochastic nature of S3Pool enables it to produce different feature maps at each pass for the same training examples, which amounts to implicitly performing a sort of data augmentation [20], but at intermediate layers.", "startOffset": 189, "endOffset": 193}, {"referenceID": 16, "context": "In our experiments, we show that S3Pool yields excellent results on three standard image classification benchmarks, with two state-ofthe-art architectures, namely network in network [17], and residual networks [9].", "startOffset": 182, "endOffset": 186}, {"referenceID": 8, "context": "In our experiments, we show that S3Pool yields excellent results on three standard image classification benchmarks, with two state-ofthe-art architectures, namely network in network [17], and residual networks [9].", "startOffset": 210, "endOffset": 213}, {"referenceID": 21, "context": "We also extensively experiment with different data augmentation strategies, and show that under each setting, S3Pool is able to outperform other counterparts such as dropout [22] and stochastic pooling [26].", "startOffset": 174, "endOffset": 178}, {"referenceID": 25, "context": "We also extensively experiment with different data augmentation strategies, and show that under each setting, S3Pool is able to outperform other counterparts such as dropout [22] and stochastic pooling [26].", "startOffset": 202, "endOffset": 206}, {"referenceID": 10, "context": "The idea of spatial feature pooling dates back to the seminal work by Hubel and Wiesel [11] about complex cells in the mammalian visual cortex and the early CNN architectures developed by Yann Lecun et al.", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": ", through orderless bag-of-features [2, 6], spatial pyramid aggregation [14], or task-driven feature pooling [23].", "startOffset": 36, "endOffset": 42}, {"referenceID": 5, "context": ", through orderless bag-of-features [2, 6], spatial pyramid aggregation [14], or task-driven feature pooling [23].", "startOffset": 36, "endOffset": 42}, {"referenceID": 13, "context": ", through orderless bag-of-features [2, 6], spatial pyramid aggregation [14], or task-driven feature pooling [23].", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": ", through orderless bag-of-features [2, 6], spatial pyramid aggregation [14], or task-driven feature pooling [23].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "Hybrid pooling [16, 18] combines different types of pooling into the same network architecture.", "startOffset": 15, "endOffset": 23}, {"referenceID": 17, "context": "Hybrid pooling [16, 18] combines different types of pooling into the same network architecture.", "startOffset": 15, "endOffset": 23}, {"referenceID": 25, "context": "Stochastic pooling [26] randomly picks the activation within each pooling region according to a multinomial distribution.", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "Max-out networks [4, 21] perform pooling across different feature maps.", "startOffset": 17, "endOffset": 24}, {"referenceID": 20, "context": "Max-out networks [4, 21] perform pooling across different feature maps.", "startOffset": 17, "endOffset": 24}, {"referenceID": 7, "context": "Spatial pyramid pooling [8] aggregates features at multiple scales, and is usually applied to extract fixed-length feature vectors from region proposals for object detection.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "Fractional pooling [5] proposes to use pooling strides of less than 2 by applying mixed pooling strides of 1 and 2 at different locations.", "startOffset": 19, "endOffset": 22}, {"referenceID": 25, "context": "(b) Stochastic pooling [26], pooling window k \u201c 2, stride s \u201c 2", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": "Stochastic pooling [26] adapts the first step by choosing the activation with a stochastic procedure (b).", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "Learning-based methods for spatial feature pooling have also been proposed [7, 1].", "startOffset": 75, "endOffset": 81}, {"referenceID": 0, "context": "Learning-based methods for spatial feature pooling have also been proposed [7, 1].", "startOffset": 75, "endOffset": 81}, {"referenceID": 25, "context": ", average pooling, stochastic pooling [26]).", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "The benefit of S3Pool is thus obvious in that, each draw from the pooling step will produce different yet plausible downsampled feature maps, which is equivalent to performing data augmentation [20] at the pooling layer level.", "startOffset": 194, "endOffset": 198}, {"referenceID": 12, "context": "However, compared with traditional data augmentation, such as image cropping [13], the distortion introduced by S3Pool is more aggressive.", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "To further illustrate the idea of S3Pool and its difference from the standard max pooling, and another nondeterministic variant of max pooling [26], we demonstrate the different pooling processes in Figure 2 using a toy feature map of size 1\u02c64\u02c64.", "startOffset": 143, "endOffset": 147}, {"referenceID": 25, "context": "From the two-step view of max pooling, stochastic pooling [26] modifies the first step: instead of outputing a deterministic maximum in each pooling window of k \u02c6 k, it randomly draws a response according to the magnitude of the activation; the second downsampling step, however, remains the same as in max pooling.", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "Different from stochastic pooling [26] and deterministic max pooling, S3Pool offers the flexibility to control the amount of distortion introduced in each sampling step by varying the grid size g in each layer.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "For CIFAR-10 and CIFAR-100, we experiment with two state-of-the-art architectures, network in network (NIN) [17] and residual networks (ResNet) [9], both of which are well established architectures, but with different designs.", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "For CIFAR-10 and CIFAR-100, we experiment with two state-of-the-art architectures, network in network (NIN) [17] and residual networks (ResNet) [9], both of which are well established architectures, but with different designs.", "startOffset": 144, "endOffset": 147}, {"referenceID": 16, "context": "The architectures we use in this paper differ slightly from those in [17, 9], which we summarize in Table 1.", "startOffset": 69, "endOffset": 76}, {"referenceID": 8, "context": "The architectures we use in this paper differ slightly from those in [17, 9], which we summarize in Table 1.", "startOffset": 69, "endOffset": 76}, {"referenceID": 11, "context": "Batch normalization [12] is applied to each convolutional layer for each of the two models, with ReLU as the nonlinearity.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Table 2: Control experiments with NIN [17] on CIFAR-10 and CIFAR-100 (best seen in color).", "startOffset": 38, "endOffset": 42}, {"referenceID": 25, "context": "[26] N N 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] N Y 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Y N 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Y Y 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "For ResNet, we follow the original design in [9] by replacing the Pool-2-2 layer with stride 2 convolution, without dropout.", "startOffset": 45, "endOffset": 48}, {"referenceID": 25, "context": "[26] with pooling window of size 2\u02c6 2 and stride 2\u02c6 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "We train all the models with ADADELTA [25] with an initial learning rate of 1 and a batch size of 128.", "startOffset": 38, "endOffset": 42}, {"referenceID": 25, "context": "We observe that for every combination of dataset type, network architecture and data augmentation technique (denoted by rows with the same color in Table 2 and Table 3), S3Pool achieves the lowest testing error, while yielding higher training errors than NIN with dropout, ResNet and their counterparts with stochastic pooling [26].", "startOffset": 327, "endOffset": 331}, {"referenceID": 16, "context": "Table 3: Control experiments with ResNet [17] on CIFAR-10 and CIFAR-100 (best seen in color).", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "[26] N N 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] N Y 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Y N 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Y Y 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "The results indicate that, by varying the number of training examples from as low as 1000 to 10000, S3Pool achieves consistently lower testing errors compared with the baseline ResNet as well as stochastic pooling [26].", "startOffset": 214, "endOffset": 218}, {"referenceID": 25, "context": "[26] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] 25.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] 27.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[24] 26.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "85 adopt the 18-layer ResNet based architecture on this dataset, and test different pooling methods by replacing the stride 2 convolutions by stochastic pooling [26] and S3Pool with different grid size settings.", "startOffset": 161, "endOffset": 165}, {"referenceID": 23, "context": "In particular, S3Pool with the strongest regularization (S3Pool-96-48-24-12) achieves the state-ofthe-art testing error on STL-10, outperforming supervised learning [24] as well as semi-supervised learning [28, 3] approaches.", "startOffset": 165, "endOffset": 169}, {"referenceID": 27, "context": "In particular, S3Pool with the strongest regularization (S3Pool-96-48-24-12) achieves the state-ofthe-art testing error on STL-10, outperforming supervised learning [24] as well as semi-supervised learning [28, 3] approaches.", "startOffset": 206, "endOffset": 213}, {"referenceID": 2, "context": "In particular, S3Pool with the strongest regularization (S3Pool-96-48-24-12) achieves the state-ofthe-art testing error on STL-10, outperforming supervised learning [24] as well as semi-supervised learning [28, 3] approaches.", "startOffset": 206, "endOffset": 213}, {"referenceID": 26, "context": ", using the test model of S3Pool) and stack a deconvolutional network [27] on top.", "startOffset": 70, "endOffset": 74}], "year": 2016, "abstractText": "Feature pooling layers (e.g., max pooling) in convolutional neural networks (CNNs) serve the dual purpose of providing increasingly abstract representations as well as yielding computational savings in subsequent convolutional layers. We view the pooling operation in CNNs as a two-step procedure: first, a pooling window (e.g., 2 \u02c6 2) slides over the feature map with stride one which leaves the spatial resolution intact, and second, downsampling is performed by selecting one pixel from each non-overlapping pooling window in an often uniform and deterministic (e.g., top-left) manner. Our starting point in this work is the observation that this regularly spaced downsampling arising from non-overlapping windows, although intuitive from a signal processing perspective (which has the goal of signal reconstruction), is not necessarily optimal for learning (where the goal is to generalize). We study this aspect and propose a novel pooling strategy with stochastic spatial sampling (S3Pool), where the regular downsampling is replaced by a more general stochastic version. We observe that this general stochasticity acts as a strong regularizer, and can also be seen as doing implicit data augmentation by introducing distortions in the feature maps. We further introduce a mechanism to control the amount of distortion to suit different datasets and architectures. To demonstrate the effectiveness of the proposed approach, we perform extensive experiments on several popular image classification benchmarks, observing excellent improvements over baseline models 1.", "creator": "LaTeX with hyperref package"}}}