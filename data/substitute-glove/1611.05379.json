{"id": "1611.05379", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "PCT and Beyond: Towards a Computational Framework for `Intelligent' Communicative Systems", "abstract": "Recent because having endured reduction beyond in taken particular incentives of ` intelligent ' gansu technology involved as 3d. Honda ' urgent Asimo humanoid robotic, iRobot ' s Roomba robot fuel affordable taking Google ' nasdaq100tr backplane cars and fired entire inspires present end also policy, these organizational critical lightning some stock about, utopian worldwide of sensitive robot assistants forms the coming robot ghost! However, about referred next be way to turn set autonomous systems go the level of programs required for there on imperfect followed tasks deals study - animated orientation - there need rather rather communicative behaviour such other speech various language. Of better the field now Artificial Intelligence (AI) brought made passion strides today and areas, of has moved extended, illustrate high - level favour - analysis paradigms allowed morality architectures old operations are 747-400 in much physical environments. What instance all missing, actually, it once distinguishing method of innovative socialization behaviour that fiance standard - same conjunction require followed order came provide a among coherent critical could similar integration. This judicial script while enduring thus well a requires pop continued the norms for Perceptual Control Theory (PCT ). In this, it is observations although PCT has hitherto differ to reflected perceptual complicated also entire relatively straightforward series of transformations between performer instead vulnerability, out has irrelevant during potential than driven generative model - firm solutions that have emerged followed providing wide unusual or visual using auditory scene findings. Starting years sixth adopted, came reference of arguments for prior which cannot if shows nothing except principles trying be integrated into PCT, no another new strengthen PCT towards a somewhat parameter collection for a putting - turned communicative assistant. It is concluded that, if behaviour is the instead of evident, entered perception, also simulation however susceptible.", "histories": [["v1", "Wed, 16 Nov 2016 17:32:10 GMT  (767kb,D)", "http://arxiv.org/abs/1611.05379v1", "To appear in A. McElhone &amp; W. Mansell (Eds.), Living Control Systems IV: Perceptual Control Theory and the Future of the Life and Social Sciences, Benchmark Publications Inc"]], "COMMENTS": "To appear in A. McElhone &amp; W. Mansell (Eds.), Living Control Systems IV: Perceptual Control Theory and the Future of the Life and Social Sciences, Benchmark Publications Inc", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.HC cs.RO", "authors": ["prof roger k moore"], "accepted": false, "id": "1611.05379"}, "pdf": {"name": "1611.05379.pdf", "metadata": {"source": "CRF", "title": "PCT and Beyond: Towards a Computational Framework for \u2018Intelligent\u2019 Communicative Systems", "authors": ["Roger K. Moore"], "emails": ["r.k.moore@sheffield.ac.uk"], "sections": [{"heading": null, "text": "Keywords: perceptual control theory, artificial intelligence, cognitive systems, robotics, communicative agents\nar X\niv :1\n61 1.\n05 37\n9v 1\n[ cs\n.A I]"}, {"heading": "1 Introduction", "text": "The past few decades have seen an enormous growth in the level of interest being shown in so-called \u2018intelligent\u2019 machines [1]. Funding agencies and large corporations worldwide have been investing heavily in autonomous systems - especially robotics - on the premise that significant economic benefits may be derived from automating hitherto people-intensive activities. Ranging from production-line robots to homecare assistants, and from robotic surgeons to driverless cars, future intelligent systems are expected to transform our lives in much the same way that the invention of the steam engine accelerated the industrial revolution during the 18th and 19th centuries [2,3,4,5,6].\nHowever, in order to deliver the expected benefits, future autonomous systems will indeed need to be intelligent - they must integrate seamlessly into real-world environments, act appropriately in complex physical and temporal situations, solve difficult logistical problems, interact effectively with human users/operators using accepted social conventions (such as speech and language), be robust in the face of unpredictable disturbances and interruptions, operate independently within an accepted ethical framework, and be at least partially responsible for their own behaviours [7]. This is a challenging wish list that goes well beyond the bounds of traditional fields such as Artificial Intelligence (AI). Indeed, the requirements for intelligent systems/robots are so demanding that insights need to be integrated from a wide array of disciplines ranging from engineering and computer science to psychology, cognitive neuroscience and linguistics.\nIn practice, a robot is a complex physical entity often consisting of a large number of moving parts, an electrical power system, an array of electronic components including on-board and off-board processors, wired and wireless communication links, various computer operating systems and a range of software modules for managing the overall system. This means that it is not only necessary to develop comprehensive tools and techniques for designing, building and programming such devices to meet particular application requirements, but it is very likely that approaches will also need to be based on a deeper understanding of how existing intelligent systems - living organisms - solve the challenges listed above [8,9].\nAt present, the sheer complexity of such systems, coupled with the high cost of developing bespoke hardware platforms, inevitably means that components are integrated without a great deal of thought being given to more general principles of intelligent behaviour. Off-the-shelf solutions for locomotion, navigation, manipulation and interaction are often combined with independent modules for input/output modalities such as vision, speech and gesture without too much consideration of potential synergies. As a result, behaviours may be programmed in an ad-hoc manner using heuristic (rather than principled) approaches to the\nnecessary algorithms1. What is required is an overarching theory of intelligent behaviour that informs system-level design decisions in order to provide a more coherent approach to system integration, especially for communicative agents."}, {"heading": "1.1 Good Old-Fashioned Artificial Intelligence", "text": "The term Artificial Intelligence (AI) was coined in 1955 and, in its early years, was mainly concerned with mathematical logic and automatic theorem proving (on the assumption that \u2018intelligence\u2019 was founded on processes of rational thought) [10]. Activities such as playing chess were seen as the epitome of human intellectual achievement, and thus became the focus of early research. However, it was soon realised that physically manipulating actual chess pieces could be more challenging than playing the game itself. AI thus moved on to fulfil an important role in expanding our understanding of how living systems in general, and human beings in particular, interact physically with the world through developments in areas such as bio-inspired robotics and autonomous systems.\nUntil the mid-1980s, the main paradigm for AI-based robotics was the socalled \u2018deliberative\u2019 architecture in which symbolic representations of the world were manipulated in a hierarchical rule-based framework involving goals and subgoals. The process operated using a Sense \u2192 Plan \u2192 Act cycle (see Fig. 1), very much in tune with the \u2018behaviourist\u2019 Stimulus \u2192 Response (S-R) framework that was dominating the field of psychology at the time [11].\nWhen applied to robotics, the deliberative AI paradigm essentially took a high-level approach in which functions such as navigation were regarded as problem-solving activities that required the manipulation and search of appropriate symbolic data structures. However, it soon became apparent that this perspective - subsequently termed Good Old-Fashioned Artificial Intelligence (GOFAI) [12] - suffered from severe limitations, particularly with respect to an over-reliance on accurate world models and an inability to respond quickly to changing situations and context.\n1 Of course it\u2019s not that such theory doesn\u2019t exist. Rather, the demands of contemporary intelligent systems are such that it is often necessary to take a pragmatic approach to system implementation."}, {"heading": "1.2 Behaviour-Based Robotics", "text": "By the 1980s, the mounting difficulties faced by attempting to deploy GOFAI in real-world robots had led to a new \u2018reactive\u2019 (as opposed to \u2018deliberative\u2019) approach based on low-level representation-free processes. Hailing the establishment of what was to be known as \u2018New AI\u2019, Rodney Brooks introduced a novel \u2018subsumption\u2019 architecture in which the emphasis was on real-time behaviour using simple computations embedded in a layered structure [13], [14] (see Fig. 2). The basic idea was that the different layers should operate more or less independently (and in parallel), with the higher levels relying on successful operation of the lower levels. If necessary, the higher levels could change (that is, \u2018subsume\u2019) the behaviour of the lower levels. Overall, the emphasis was on the grounding of behaviour in the real-world, with the hypothesis that complex interactions should arise as an emergent property of an array of simple processes (as had been elegantly proposed by Braitenberg in the 1980s [15]).\nThe subsumption approach initiated a trend towards what became known as \u2018behaviour-based robotics\u2019 [16], and the emphasis shifted from high-level abstract problem solving to low-level grounded intelligence.\nBehaviour-based robotics was sufficiently successful in solving practical realworld problems that, in 1990, Rodney Brooks founded the company iRobot\u00ae to develop and market the Roomba\u00ae vacuum cleaning robot. More than 10 million Roomba robots have been sold worldwide, making it the most successful mass-market robot to date. However, the subsumption approach has been criticised for being difficult to scale up to more complex robots. Also, the emphasis on representation-free processing, whilst being popular with a subset of psychologists, is ultimately a significant restriction2."}, {"heading": "1.3 Artificial Cognitive Systems", "text": "Recent work in robotics has taken a more pragmatic view by mixing and matching the best ideas from GOFAI and behaviour-based robotics with some of the new perspectives emerging from the field of cognitive neuroscience. Known as \u2018artificial cognitive systems\u2019 [17,18], the most significant influence has been the emphasis on enaction, embodiment and situatedness in which the relationship between a goal-driven robot and its physical real-world context is paramount [19,20]. Low-level interactions are managed locally and high-level representations are given \u2018meaning\u2019, not by hand-crafted rules, but by virtue of their grounding in real-world interactions. Interaction is facilitated by the \u2018affordances\u2019 [21,22] provided by a robot\u2019s environment.\nA particular example of a contemporary architecture for an artificial cognitive system is Distributed Adaptive Control (DAC) [23,24]. The DAC architecture is based on the hypothesis that the living brain maintains a stable relationship with its environment by continuously solving the How, Why, What, Where, When (H4W) problem. Implementation uses an artificial neural structure organised into soma, reactive, adaptive and contextual layers, and columns which represent exosensing (defined as the sensation and perception of the world), endosensing (detecting and signalling states derived from the physical self) and action (the interface to the world) - see Fig. 3.\nUnlike their GOFAI predecessors, modern cognitive approaches to intelligent systems (such as DAC) view action and perception as being synergistic (rather than separate) processes [25], and that skills should be acquired through the robot\u2019s own active exploration of the world (rather then being pre-programmed by the system designer) [26,27]. Such approaches very much reflect contemporary models of living systems, particularly the discovery in the 1990s of so-called \u2018mirror neurons\u2019 - neural structures which appear to provide a vital link between sensory and motor behaviour and thereby a mechanism for action understanding, imitation and learning [28,29,30].\nA recent book by Murray Shanahan provides an excellent overview of the contemporary perspective in artificial cognitive systems [31].\n2 In reality, the proposal that internal models were unnecessary was a stance adopted by Brooks to make a strong point about the inadequacies of GOFAI. In practice, the subsumption architecture incorporates such models (for example, the building of maps is mentioned in Fig. 2)."}, {"heading": "1.4 Agent Based Modelling", "text": "One aspect of behaviour that is common across alternative approaches to modelling and building intelligent systems is intentionality. It is clearly the case that living systems appear to be goal-directed and purposeful in their endeavours [32], and this has had a major influence on AI and robotics. In particular, one area in which intentionality plays a key role is the field of Agent-Based Modelling (ABM) [33,34]. ABM is a well-established methodology for simulating the actions and interactions of multiple agents: for example, predicting the behaviour of crowds, optimising a supply chain or managing a workforce. Various modelling paradigms are employed, such as cellular automata [35] or dedicated multi-agent programming environments [36]. However, for \u2018intelligent\u2019 agents, ABM simulations are often constructed using a Beliefs Desires Intentions (BDI) architecture [37,38] on the premise that such internal structures are required to adequately condition the behaviour of individual agents - see Fig. 4.\nBDI is a powerful approach to modelling agents, and it has been applied successfully to robotics [39,40]. However, BDI does not specify how to recog-\nnise/interpret behaviour under conditions of ambiguity or uncertainty - a crucial feature of intelligent systems."}, {"heading": "1.5 Contemporary Intelligent Systems", "text": "Over the past ten years, the field of robotics and autonomous systems has grown in stature and achievement. Robots such as Boston Dynamics\u2019 Big Dog and Honda\u2019s Asimo have successfully demonstrated that modern-day computing and electronics are finally fast enough to permit real-time control of complex behaviours such as running or climbing on uneven surfaces. Robotic quadcopters have been shown flying through moving hoops [41] and playing tennis [42], robot hands have been programmed to catch thrown objects [43] and NASA\u2019s Robonaut R2 has been assisting on the International Space Station since 2011 [44]. Also, recent years have seen tremendous growth in the field of Artificial Neural Networks (ANNs), particularly with the success of deep learning as a mechanism to optimise the parameters of such multilayered networks on massive amounts of training data [45,46].\nNevertheless, despite this tremendous progress, there is still a long way to go before artificial intelligent systems will be able to demonstrate the flexibility, robustness and autonomy exhibited by even the simplest living organism [47,48]. Many contemporary robots rely on teleoperation by human operators to overcome weaknesses in their overall design, and attempts to create humanoid robots are fraught with difficulties ranging from the risk of repulsing human users due to the uncanny valley effect [49,50], to the more general uncertainties associated with human-robot interaction [51,52,53] - especially if it involves communicative agents and spoken language [54].\nFor example, probably the most well known communicative agent is Siri, Apple\u2019s voice-based personal assistant for the iPhone. Released in 2011, Siri suddenly brought spoken language technology to the attention of the non-specialist user. Voice dictation software for document creation on PCs had been available since the 1990s, culminating with the release of Dragon System\u2019s Naturally Speaking and IBM\u2019s ViaVoice products in 1997. The big difference was that Siri combined automatic speech recognition and speech synthesis with natural language processing and dialogue management in order to facilitate a more conversational interaction between users and smart devices, and competitors such as Google Now and Microsoft\u2019s Cortana soon followed.\nIn reality, the practical value of contemporary communicative agents is somewhat in doubt (as evidenced by the preponderance of videos on YouTube which depict humorous rather than practical interactions). This has been confirmed by a recent survey that discovered only 13% of respondants use their voicebased personal agent daily, whereas 46% had tried it once and then abandoned it [55]. One reason for the lack of usability is that contemporary communicative agents are founded on a classic stimulus\u2192response architecture (as illustrated in Fig. 5); a user speaks, their utterance is processed, a response is formulated and the system speaks back. Such an approach completely overlooks the reality of languaging as an emergent property of the dynamic coupling between intentional\nagents that serves to facilitate distributed sense-making through cooperative behaviours [57,58,59,60,61]. Furthermore, the contemporary view is that language is based on the co-evolution of two key traits: ostensive-inferential communication and recursive mind-reading [62].\nThese are very sophisticated notions but, as yet, there is no clear practical framework for implementing such concepts. As a result, the field is still open for new ideas, and some of them may not be so new. For example, what might Perceptual Control Theory (PCT) - established over 50 years ago [63,64] (and the topic of this volume) - have to contribute to future \u2018intelligent\u2019 communicative systems?"}, {"heading": "2 Whither Perceptual Control Theory?", "text": "Although it is more than 40 years since the publication of Bill Powers\u2019 seminal book \u2018Behavior: The Control of Perception\u2019 (B:CP) [65], Perceptual Control Theory may still have something important to say about computational models of \u2018intelligent\u2019 communicative behaviour. Over the years, the main thrust\nof PCT has been to question the traditional behaviourist stimulus\u2192response stance that still prevails in some sections of the psychology field [66,67]. PCT aims to provide a more parsimonious model which highlights the critical importance of acknowledging the existence of internal preferred states (so called \u2018reference variables\u2019) arranged in a control hierarchy, together with the power of negative feedback to achieve and maintain those preferred states in the face of unpredictable disturbances. Crucially, PCT focuses attention on the control of perceptual inputs rather than behavioural outputs [68].\nOf course, negative feedback control is endemic in the field of robotics (particularly at the lowest levels of motor control), not least because many robotocists are trained control engineers. However, mainstream control engineering tends to concentrate on the regulation of required behaviour (low-level outputs) in a single control loop, and the emphasis is on modelling the dynamics of the system under control in order to calibrate the parameters of the control process. Also, hierarchical control structures are much less common. PCT thus offers a broader perspective that is often overlooked, as well as introducing the proposal that living organisms control perceptions rather than behaviours."}, {"heading": "2.1 Classic Automatic Control", "text": "As a reminder, PCT was developed in the context of existing knowledge of the classic theory of automatic control - the study of systems that are capable of self-regulation, usually through the mechanism of negative feedback [69]. A classic single-input single-output closed loop negative feedback control system from automatic control theory is illustrated in Fig. 6. The plant (or controlled system) g2 is the process controlled by the feedback control system, and the behaviour of the plant may be subject to arbitrary disturbances d. The feedforward (control) elements g1 generate control signals u that are applied to the plant which produces the controlled output c. The reference input r specifies the desired output of the plant. The feedback elements h map the controlled output c to the feedback signal b, and this is compared to the external reference input r and the resulting error signal e generates a control action via g1. With appropriate functions for g1 , g2 and h, the system should compensate for any disturbances and stabilise with b \u2192 r without having to measure d. The process is termed negative feedback because the comparator involves subtraction.\nIt can be shown that the input-output relations of the classic control system shown in Fig. 6 are given (in the frequency domain) by the equation\nC =\n( G2G1\n1 + HG2G1\n) R, (1)\nwhere G2G11+HG2G1 is the closed-loop transfer function and HG2G1 is the loop gain. If G2G1 1 and H \u2248 1, then C \u2248 R. Clearly the tracking behaviour of a classic negative-feedback control system depends on the feedforward (control) elements g1. If the control element is slow to respond to a disturbance, then stabilisation may take too long; this is referred\nto as an overdamped system. On the other hand, if the loop gain is too high, then the system may overshoot and even oscillate; this is referred to as an underdamped system. In practice, g1 is often implemented using a Proportional, Integral, Derivative (PID) controller which takes the following idealised form:\nuPID = KP e + KI \u222b e (t) dt + KD de\ndt , (2)\nwhere the three constants - KP , KI and KD - are used to optimise the stability of the control process (referred to as a critically damped system). KP is known as the \u2018loop gain\u2019 and it determines the speed of response of the system to error. If KP is too high, then the system may respond too quickly and overshoot (or even become unstable); if it is too low, then the system may respond too late to counteract any disturbance. The effect of KD is to slow the rate of change of the controller output and thus minimise overshoot arising from a high KP . KI is intended to reduce any residual steady-state error. There are various methods for estimating these constants and, in practice, KI and KD can often be set to zero."}, {"heading": "2.2 Hierarchical Perceptual Control", "text": "Contemporary automatic control systems tend to be based on a single control loop, but the dynamic behaviour of the system to be controlled is often very complex. PCT, on the other hand, decomposes a system into a hierarchical/layered structure with a multiplicity of control loops at each level; this is referred to as Hierarchical Perceptual Control Theory (HPCT) [65]. Inspired by classic automatic control theory, the basic perceptual control unit (feedback control loop) in PCT is illustrated in Fig. 7.\nNote the similarity between Fig. 7 and Fig. 6. The main difference is that the PCT structure illustrated in Fig. 7 is specifically aimed at modelling the behaviour of a living system, and this means that the reference variable is intrinsic (rather than extrinsic). In all other respects the two control structures are effectively identical3.\n3 Note that PCT practitioners refer to the feedback signal (p in Fig. 7) as the controlled variable thereby emphasising that it is the perceptual signal that is controlled in a negative feedback control system, not the plant.\nAs mentioned, HPCT derives its power from the hierarchical structure of multiple control loops in which the output from one loop provides the reference signal for another loop. Powers [65] proposed specific orders of control (starting at the lowest first-order level) as follows: intensity \u21d4 sensation \u21d4 configuration \u21d4 transitions \u21d4 sequence \u21d4 relationships \u21d4 program \u21d4 principles \u21d4 system concepts.\nPowers also generalised the basic perceptual control unit to include the storage and retrieval of information in memory. In particular, he proposed that all reference signals constitute recordings of past perceptual signals that are retrieved by address signals from the higher level - see Fig. 8. Implementation of this feature required the addition of two switches - a memory switch and a perceptual switch. Powers noted that each of the four possible combinations of switch settings lead to interesting outcomes. If both switches are vertical (a+d), then the loop is in \u201cconventional control mode\u201d. If the perceptual switch is vertical and the memory switch is non-vertical (a+c), then information can be acquired without action taking place; Powers called this \u201cpassive observation mode\u201d. If the memory switch is vertical and the perceptual switch is non-vertical (b+d), then control takes place with no perceptual awareness at higher levels; Powers referred to this as \u201cautomatic mode\u201d. Finally, if both switches are nonvertical (b+c), then past perceptions (retrieved from memory) are redirected back up the hierarchy; Powers designated this as \u201cimagination mode\u201d, and identified it as a potential mechanism for visualisation and planning.\nOn the surface, the HPCT architecture looks remarkably similar to subsumption (as described in section 1.2). However, a key distinction is that HPCT is capable of optimising many variables simultaneously within a negative feedback control framework, whereas subsumption is essentially an event-driven stimulus\u2192response architecture [70].\nFinally, Powers identified three types of learning within HPCT: the storage of information in memory, problem-solving and reorganisation. The latter is particularly interesting since it involves altering the structure and properties of the control systems themselves. Powers viewed reorganisation as a kind of meta control system that had the objective of reducing the intrinsic error of the overall system to zero."}, {"heading": "2.3 The Way Forward?", "text": "Perceptual control theory (and HPCT) appears to offer a number of interesting features for modelling the behaviour of living systems [71], and should thus be a serious contender for implementing control in artificial systems such as robots (as envisaged by Powers [72,73,74,75]). A few attempts have been made in this area by PCT practitioners [76,77,78]. However, since a large proportion of robotocists are control engineers who take negative feedback systems as a given, they have not seen the particular benefits of using (H)PCT.\nOn the other hand, (H)PCT has started to have some influence in the field of spoken language processing [79]. It was established many years ago that human speech is optimised by speakers for listeners [80], and this has been theorised\nas resulting from the operation of negative feedback processes [81]. Hence, there is an obvious link with PCT. However, mainstream speech technology systems have tended to ignore speaker-listener dependencies and such behaviour is simply modelled as stochastic variation [82].\nIn an attempt to capture such dependencies, and inspired by classic automatic control, (H)PCT and recent discoveries in cognitive neuroscience (such as mirror neurons - discussed in Section 1.3), the PREdictive SENsory Control and Emulation (PRESENCE) architecture [83] provides some novel technical solutions in this area. For example, a PRESENCE-based text-to-speech synthesiser - C2H - has been developed that is capable of adjusting its pronunciation while it is speaking as a function of its perceived communicative success, the latter being judged using a perceptual feedback path in which the \u2018controlled variable\u2019 was an estimate of the intelligibility [84,85].\nPRESENCE not only provides a novel architecture for generating spoken language, it also shows how the principles of feedback control can be applied to the recognition and interpretation of spoke language [86]. Indeed, PRESENCE emphasises that in order to manage its own behaviour (its perceptions), a social agent also needs to be able to interpret the world and the behaviour of other in-\ntelligent systems within it. Hence, much greater emphasis is placed on modelling perception than is the norm in mainstream PCT. PRESENCE thus provides an interesting insight into a potentially more general-purpose PCT-inspired architecture that is not specific to spoken language processing and which could be applied in engineered solutions. However, the founding principles of such an architecture have not been fully elucidated, so what follows is a step in that direction."}, {"heading": "3 Towards \u2018Intelligent\u2019 Communicative Systems", "text": "The insightful catchphrase for (H)PCT is \u201cBehaviour is the control of perception\u201d - but what is perception, and how are controlled variables estimated? In\nB:CP [65], Powers himself does not say a great deal about perceptual mechanisms; he acknowledges the potentially arbitrary relationship that exists between internal perceptions and external reality, but he seems to assume some form of straightforward (neural) transformation/mapping from first-order intensities to invariant higher-order perceptions that is carried out by an \u2018input function\u2019 (albeit mediated by memory - see Section 2.2). Of course such an assumption is common in the pattern recognition/machine learning literature. However, in practical fields such as visual or auditory scene analysis (and even in the latest theories of brain function [87]), the notion of \u2018perception as transformation\u2019 has been complemented by the emergence of more powerful (and more successful) generative model-based solutions [88,89,90]. Hence, there is a potentially valuable opportunity to map these findings back into (H)PCT.\nNot only is this an interesting route to take, but it also turns out to be a crucial step towards a deeper understanding of how one intelligent system (such as an autonomous social robot) might interact/communicate with another (such as a human being) [91]. What follows, therefore, is a fundamental analysis - starting from first principles - that intersects with (H)PCT and then extends it in important and interesting ways."}, {"heading": "3.1 Actions and Consequences", "text": "Consider a world that obeys the ordinary Laws of Physics. The world W has a set of possible states S, and its state s[t] at time t is some function of its state s[t\u22121] at time t\u22121. The world can thus be viewed as a form of dynamical system (probably non-linear, almost certainly stochastic) that evolves from state to state as time progresses. These state transitions can be expressed as a transform . . .\nfW : s[t\u2212 1]\u2192 s[t], (3)\nwhere fW is some function that transforms the state of the world at time t\u2212 1 to the state of the world at time t.\nThis means that the evolution of events in the world constitutes a continuous cycle of cause-and-effect. Events follows a time course in which it can be said that actions (i.e. the sequence of events in the past) lead to consequences (i.e. the sequence of events in the future) which constitute further actions, leading to further consequences, and so on . . .\nConsequences = fW (Actions) . (4)\nThis straightforward scenario (as described by equations 3 and 4) can be expressed diagrammatically as shown in Fig. 10.\nOf course, while the transform fW might be relatively simple (since it is based on the Laws of Physics), the state-space S of possible actions and consequences could be immense depending on the complexity of the world W . This means that it is impossible to model everything that happens in the world. However, in practice, some parts of the world might have very little influence on other parts. So it is possible to consider a subset of the world w that has a minimal dependency on the rest of the world w\u0304."}, {"heading": "3.2 An Agent Manipulating the World", "text": "Now consider the presence of an intentional agent a (natural or artificial) that seeks to effect a change in the world4. In this case the agent\u2019s intentions are converted into actions which are, in turn, transformed into consequences . . .\nConsequences = fw (ga(Intentions)) , (5)\nwhere g is some function that transforms the agent\u2019s intentions into actions (a process known in robotics as \u2018action selection\u2019 ) - see Fig. 11.\nThe situation illustrated in Fig. 11 corresponds to an open-loop S-R configuration, and this means that the accuracy with which an agent can achieve its intended consequences is critically dependent on having precise information about both f and g. Mathematically, the best method for achieving the required consequences is for the agent to employ an inverse transform in which g is replaced by f\u22121 - see Fig. 125.\n4 The reason why the agent wishes to change the state of the world is addressed later (in Section 3.7). 5 Note that, in the situation where f represents the plant operating a robot\u2019s actuators, the estimation of f\u22121 is commonly referred to as \u2018inverse kinematics\u2019 .\nIt is possible to discuss at length how information about the transforms g, f or f\u22121 could be acquired. For example, model parameters could be computed using complex mathematical tools for system estimation or by employing machine learning techniques on extensive quantities of training data using a process known as \u2018expectation maximisation\u2019 (EM) [92]. Whichever approach is taken, the final outcome would inevitably be sensitive to any inaccuracies in calibrating the relevant model parameters as well as being unable to tolerate unforeseen noise and/or disturbances present in the agent or in the world.\nControl theory, and thereby PCT, of course provides an alternative closedloop solution that is not dependent on knowing f (or f\u22121). An agent simply needs to be able to judge whether its objectives are being met - that is, whether the consequences of its actions match its intentions. An agent thus needs to be able to choose actions that minimise the difference between its intentions and the perceived consequences of its actions - see Fig. 13. In PCT terminology, the agent\u2019s intentions correspond to the reference signal and the perceived consequences correspond to the controlled variable; actions are selected in order to minimise the difference between the two.\nIf the perceptual signal, and hence the error signal, is in the same parameter space as the control signal (see Fig. 6), then it is possible - in principle - to jump to the optimum solution in one step. However, in practice it takes time to correct an error (since physical actions cannot take place instantaneously). So the process typically iterates towards a solution (by information flowing around the loop). If, on the other hand, the perceptual signal and the control signal are not in the same parameter space, then there is no direct mapping between error and control action (unless such a mapping has been learnt in advance). So, again, an iterative procedure is required6.\nThis means that, although closed-loop control does not require information about f (or f\u22121), it does need to know about g - the relationship between the error gradient in perceptual space and the appropriate control action. Even a simple thermostat needs to be configured correctly such that a decrease in the temperature of a room leads to an increase in the level of heating (and vice versa); in this case, the uni-dimensional perceptual error gradient determines the polarity of the action of the controller. In general, perceptions and actions may lie in (different) high-dimensional spaces7 and, either the relationship between possible behaviours and the perceptual error gradient is (somehow) known in advance, or it has to be discovered (learnt) by active exploration; for example, using \u2018reinforcement learning\u2019 [93] or the process referred to in PCT as \u2018reorganisation\u2019 - see Section 2.2.\nA further complication is that the error space may not be continuous. In this case there is no gradient and hence no indication of which action to select in order to arrive at the correct solution. For example, imagine a person who wishes to illuminate one section of a large room being faced with a set of unmarked light switches laid out in no obvious arrangement. The only available strategy is to try each switch in a random fashion until the appropriate light illuminates8. Of course, once a mapping - g - has been discovered, it may be stored in memory and used to guide future behaviour.\nIn many situations, negative feedback control is able to employ an optimisation technique known as gradient descent in which the difference between the intentions (the reference signal) and the perceived consequences (the feedback signal) is a continuous variable that can be reduced monotonically to zero. In this case, the dynamics of the optimisation is dependent on the appropriate choice of parameters in the controller (for example, KP , KI and KD in a classic PID controller - as described in Section 2.1); an inappropriate choice of controller\n6 For example, a thermostat measures temperature, whereas the control signal for a heating/cooling system would be expressed in terms of power output. This means that a-priori there is no information on how to convert a difference between desired and actual temperature (the error signal) to an optimum power setting (the control signal). Hence the power output has to be adjusted iteratively until the desired temperature is reached. 7 In robotics, the space of possible actions is characterised by the Degrees-of-Freedom (DoF) in the system, and this usually corresponds to the number of joints or actuators/motors. 8 This is a common experience for an academic teaching in an unfamiliar lecture room.\nparameters (such as the wrong sign for KP ) can even send the system in the opposite direction (that is, positive feedback) with a consequent failure to converge. In classic control systems, the process of optimising control parameters is often performed manually. In PCT, it is performed by reorganisation (see Section 2.2) which is, itself, a negative feedback control process (and which is interestingly similar to the \u2018actor-critic\u2019 approach to reinforcement learning [94]).\nThis discussion makes it clear that, in general, negative feedback control may be viewed as an iterative search9 process and, in the example of the discrete light switches with no prior information, only a random (as opposed to directed) search is feasible. The realisation that control can be regarded as a type of search is a key result, and it emphasises that the topology of the search space and the controller\u2019s information about that topology is key to the effectiveness and outcome of the optimisation.\nReturning to the agent a attempting to manipulate the world w, negative feedback control can thus be viewed as a search over possible actions to find those which give rise to the best match between intentions and perceived consequences. This can be expressed as\nA\u0302ctions = arg min Actions\n(Intentions\u2212 Perceived Consequences) , (6)\nwhere A\u0302ctions represents an estimate of the actions required to minimise the difference between intentions and perceived consequences - see Fig. 14.\nThe structure illustrated in Fig. 14 is similar to the classic PCT arrangement, but enhanced by the identification of iterative search as the basis of a negativefeedback controlled optimisation process. However, this configuration will only function correctly if (i) the agent can observe the consequences of its actions\n9 Note that \u2018search\u2019 is a term used by computer scientists to describe a process that engineers would refer to as \u2018optimisation\u2019. In both cases, a system iterates/converges towards an optimum solution by minimising/maximising some \u2018objective function\u2019 - in this case, minimising an error signal. The value of using the term \u2018search\u2019 is that it covers both continuous and discrete forms of optimisation.\nand (ii) the search space is concave10. If the consequences of an agent\u2019s actions are hidden from observation in space (because objects are obscured11) or in time (because feedback delays are too high or the intended consequences are in the future), the loop can still function, but only if the agent is able to estimate the consequences of possible actions. Likewise, if the search space is not concave but has many local minima, then an iterative search can avoid getting stuck in the nearest one by exploring the whole space in advance. In other words, in both of these cases, an agent could gain benefit by being able to predict the consequences of possible actions.\nIn other words, if an agent cannot directly observe the consequences of its actions, or the search space has many local optima, then it needs to (i) estimate the relationship between possible actions and possible consequences (fw), (ii) perform a search over hypothetical actions and then (iii) execute those actions that are found to minimise the estimated error. In this case . . .\nA\u0302ctions = arg min A\u0303ctions\n( Intentions\u2212 f\u0302w(A\u0303ctions) ) , (7)\nwhere f\u0302w is the estimate of fw and A\u0303ctions is the set of possible actions - see Fig. 15.\nWhat is interesting in this arrangement is that the estimated transform f\u0302w can be interpreted as a form of mental simulation (or predictor) which emulates the consequences of possible actions prior to action selection [95]. In other words,\nsearching over f\u0302w(A\u0303ctions) is equivalent to planning (and corresponds to Powers\u2019 imagination mode in HPCT, i.e. the b+c setting in Fig. 8). It also shows that the ability to perform \u201cwhat if\u201d simulations is not a property of a particular\n10 That is, the search space has only one global optimum. 11 For example, attempting to achieve some objective in the dark, reaching behind\nanother object or, of particular significance here, manipulating the internal states of another agent (as will be discussed in Section 3.4).\nlevel of control, but is a generic property that can be instantiated in any control loop at any level (as proposed by Powers).\nThe ability to emulate the consequences of possible actions is important because, not only is it a link between PCT and AI planning, but it also offers the possibility of finding global (rather than local) solutions12. From an ecological perspective, this could be critical in terms of saving living systems vital time and/or energy in real world situations."}, {"heading": "3.3 An Agent Interpreting the World", "text": "Having established an extended PCT-style framework for an agent attempting to manipulate the physical world (with search as the underlying mechanism supporting negative feedback control), it is now possible to turn to the complementary situation in which an agent a is attempting to interpret the world w. In this case, interpretation is defined as an agent deriving potentially hidden actions/causes of events by observing their visible effects/consequences13.\nA\u0302ctions = ha (fw(Actions)) , (8)\nwhere h is some perceptual function that transforms observed effects into estimated causes - see Fig. 16.\nGiven that consequences are caused by actions via the transform fw, it is possible (in principle) to compute the actions directly from the observed consequences using the inverse transform f\u22121w - see Fig. 17.\nOf course, the accuracy of this process depends on the fidelity of the inverse transform. In practice, f\u22121w is not known and very hard to estimate. A more\n12 As an example, the fastest route between two points on a map may not be the shortest, nor does each move forward necessarily take you nearer to the final destination. This is the difference between local and global optimisation. 13 For example, given an observation of an object suddenly accelerating along a flat surface, it is possible to infer (using the laws of physics) that a hidden force must have acted upon that object.\ntractable solution is thus not to use an inverse model, but to construct a socalled \u2018forward model\u201914 (that is, an estimate of fw) and to compare its output with the observed signals. Mathematically, this is the optimum way to estimate hidden variables15 given uncertainty in both the observations and the underlying process. Such a configuration is known as a \u2018maximum likelihood (or Bayesian) classifier\u2019. Also, it is a standard result in the field of statistical estimation that the parameters of forward/generative models are typically much easier to derive using reliable techniques for maximum likelihood (ML) or maximum a-posteriori (MAP) estimation, and performance degrades gracefully when there is missing data.\nThe process of interpretation using such an arrangement thus proceeds by searching over possible actions/causes to find the best match between the predicted and the observed consequences . . .\nA\u0302ctions = arg min Actions\n( Consequences\u2212 f\u0302w(Actions) ) . (9)\nThis process is illustrated in Fig. 18, and what is immediately apparent is that, just as the manipulation case, the process of interpretation may also be construed as a negative-feedback control loop (in this case, a \u2018search\u2019 over possible explanations). This is a significant outcome that is not explicit in (H)PCT; hence it provides a potentially valuable link between PCT and mainstream approaches to machine perception.\nIn fact, the architecture illustrated in Fig. 18 is a familiar16 model-based recognition framework in which the recognition/interpretation/inference of the (hidden) cause of observed behaviour is viewed as a search over possible outputs from a forward model that is capable of generating that behaviour [96,97]. This is an established and powerful approach in machine perception and scene analysis\n14 Also known as a \u2018generative model\u2019 . 15 A \u2018hidden variable\u2019 is a measurement which cannot be made directly (e.g. by obser-\nvation). 16 . . . in the field of machine learning.\n(known as analysis-by-synthesis17), and it has been applied very successfully in a range of practical scenarios [98,88,99]."}, {"heading": "3.4 An Agent Communicating its Intentions to Another Agent", "text": "The foregoing establishes a remarkably symmetric framework for manipulating and interpreting the world in the presence of uncertainty and unknown disturbances; both employ PCT-style negative-feedback control loops that perform a search over the potential outputs of forward models. This section (and the next) extends the arguments to the case where the world contains more than one agent: a world in which a sending agent s is attempting to change the mental state of a receiving agent r (that is, communicating its intentions without being able to directly observe whether those intentions have been perceived18) and the receiving agent is attempting to interpret the sending agent (that is, estimating the sending agent\u2019s intentions based only on observations of its actions).\nSo, again starting from first principles - for the sending agent s\nActionss = gs (Intentionss) , (10)\nwhere gs is the transform from intentions to behaviour, and for the receiving agent r\nInterpretationsr = hr (Actionss) , (11)\nwhere hr is the transform from observed behaviour to interpretations - see Fig. 19.\nHence, for agent s attempting to communicate its intentions to agent r, the arguments put forward in Section 3.2 suggest that, if there is no direct feedback\n17 The term \u2018analysis-by-synthesis\u2019 refers to an established process in the field of signal processing in which the value of an unknown variable is estimated from observation data (\u2018analysis\u2019) using a model of how the observations might have been generated (\u2018synthesis\u2019). 18 It might seem perverse to rule out feedback. However, the aim here is to consider the general case, where feedback may be present or absent.\nfrom agent r, then agent s needs to compute appropriate behaviour (actions) based on\n\u0302Actionss = arg min \u02dcActionss\n( Intentionss \u2212 h\u0302r( \u02dcActionss) ) , (12)\nwhich is a negative-feedback control loop performing a search over possible behaviours by agent s and their interpretations by agent r as estimated by agent s - see Fig. 20. This process can be viewed as synthesis-by-analysis."}, {"heading": "3.5 An Agent Interpreting the Behaviour of Another Agent", "text": "For agent r attempting to interpret the intentions of agent s, the arguments put forward in Section 3.3 suggest that agent r needs to compare the observed actions of agent s with the output of a forward model for agent s\n\u0302Intentionss = arg min Intentionss (Actionss \u2212 g\u0302s(Intentionss)) . (13)\nwhich is a negative-feedback control loop performing a search over the possible intentions of agent s and their realisations by agent s as estimated by agent r - see Fig. 21. As in Fig. 18, this process can be viewed as analysis-by-synthesis.\nInterestingly, this configuration is exactly how contemporary approaches to automatic speech recognition are formulated (using a probabilistic forward generative model known as a \u2018Hidden Markov Model\u2019 - HMM) [90]). In fact, the analysis-by-synthesis approach to speech recognition is not only reminiscent of the Motor Theory of speech perception [100], but is also supported by recent neuroimaging data [101,102]. Hence, there is ample evidence that such a configuration is appropriate for modelling perceptual inference.\n3.6 Using Self to Model Other\nLooking at the arrangements outlined in Sections 3.4 and 3.5, we arrive at an important result; both require one agent to have a model of (some aspect of) the other. The sending agent s selects its actions by searching over possible interpretations by the receiving agent r using an estimate of the receiving agent\u2019s\ntransform from observations to interpretation (h\u0302r). The receiving agent r infers the intentions of the sending agent s by searching over possible interpretations using as estimate of the sending agent\u2019s transform from intentions to actions (g\u0302s) - see Fig. 22.\nThe configuration shown in Fig. 22 leads to an interesting question: where\ndo the transforms h\u0302r and g\u0302s come from? More precisely, how might their parameters be estimated? Obviously they could be derived using a variety of different learning procedures (including PCT-style reorganisation). However, one intriguing possibility is that, because of the similarity between agents19, each agent could approximate these functions using information recruited from their own structures20. In other words, h\u0302r \u2190 [ hs (which can be searched using gs rather than g\u0302r) and g\u0302s \u2190[ gr (which can be searched using hr rather than h\u0302s) - see Fig. 23.\nThis arrangement, in which both agents exploit sensorimotor knowledge of themselves to model each other, can be thought of as synthesis-by-analysis-bysynthesis for the sending agent and analysis-by-synthesis-by-analysis for the receiving agent. Combining both into a single communicative agent gives rise to a structure where perception and production are construed as parallel recursive control feedback processes (both of which employ search as the underlying mechanism for optimisation), and in which the intentions of self and the intentions of other are linked to the behaviour of self and the observations of other, respectively - see Fig. 24."}, {"heading": "3.7 A Needs-Driven Communicative Agent", "text": "The preceding arguments have provided interesting answers to two key questions: (i) how can an agent optimise its behaviour in order to to communicate\n19 This situation is particularly relevant to the high degree of similarity that is found in living systems between conspecifics (members of the same species). 20 The idea of recruiting information about self in order to model other is a core component of the PRESENCE architecture (Section 2.3) which, in turn, is founded on the principle of mirror structures in the brain (Section 1.3).\nits intentions and (ii) how can an agent infer the intentions of another agent by observing their behaviour? However, thus far it has been assumed that intentionality is a key driver of communicative interaction. Hence, an obvious question is - where do the intentions come from? Of course, HPCT provides a clear answer; the reference signals at one level in a hierarchy are set by the level above and mediated by memory (as illustrated in Fig. 8). So this would suggest that intentions should be the output from some higher level.\nIn B:CP [65], Powers doesn\u2019t use the term \u2018intention\u2019 as such, rather he talks about purposeful behaviour (at all levels of the HPCT hierarchy). However, by invoking intentionality as a manifestation of purposeful goal-driven behaviour, it is possible to make a direct link between HPCT and contemporary architectures such as DAC (Section 1.3) and BDI (Section 1.4)21. In particular, the BDI approach to agent-based modelling makes it clear that intentionality is not only derived from an agent\u2019s longer-term goals and desires, but that it is also\n21 The main difference between HPCT and these architectures (including the one developed here) is that HPCT provides a rich decomposition of purposeful behaviour into the necessary levels of detailed control, whereas the other schemes tend to collapse such levels into a single abstraction.\nconditioned by the beliefs that an agent holds. More directly, the DAC architecture emphasises that behaviours are ultimately driven by a motivational system based on an agent\u2019s fundamental needs (such as energy for survival). This appears to be similar to the important role that intrinsic control systems play in the development of the perceptual hierarchy in HPCT [65].\nPutting all this together, it is possible to formulate a generic (and remarkably symmetric) architecture for a needs-driven communicative agent that is both a sender and a receiver22 - see Fig. 25. In this framework it is proposed that a communicative agent\u2019s behaviour is conditioned on appropriate motivational and deliberative states (reference signals): Needs \u2192 Desires \u2192 Intentions. Likewise, the intentions, desires and needs of another agent are inferred via a parallel interpretive structure: Perception \u2192 Interpretation \u2192 Comprehension23."}, {"heading": "4 Discussion", "text": "The foregoing section has established a putative framework for a needs-driven communicative agent in which the data structures for needs, desires and intentions represent the informational belief states (controlled variables) of the agent. As such, these representational structures are analogous to the memory layers in HPCT (as shown in Fig. 8). However, a key feature of the configuration proposed here (for a communicative agent) is that these structures explicitly contain information relating to both self and other, and this allows the behaviour of self to be conditioned on the inferred informational state of other. In other words, it provides a basis for modelling interpersonal stances such as empathy [104] and Theory of Mind (ToM) [105]24.\nAnother interesting aspect of the architecture illustrated in Fig. 25 is that, like DAC and PRESENCE, it is founded on a motivational system based on needs (for example, as described by Maslow in his famous \u2018Hierarchy of Needs\u2019 [107]). This might seem to be an unnecessary embellishment - a feature that is more relevant to modelling a living system than to designing an artificial agent. However, invoking such a needs-based framework answers a fundamental question - why would an agent (natural or artificial) do anything [108]? Also, it is envisaged that it would be useful to partition needs into those that pertain to physical health (such as survival and safety needs) and those that pertain to psychological health (such as personal and social needs), with the relevant feedback signals being related to physiological and mental well-being. This not\n22 A forerunner of this framework has been referred to as MBDIAC (Mutual Beliefs Desires Intentions Actions & Consequences) [103]. 23 From a PCT perspective, these are all perceptions. 24 Interestingly, Marken - a long-standing PCT practitioner - has recently posited the\nvalue of PCT in testing the validity of the inferences that observers make about the intentional states of others [106]. This might be usefully carried over into the proposed framework as a strategy an agent might employ in order to establish a desired cooperative/competitive relationship with another agent.\nF ig . 2 5 . Illu stra tio n o f th e p ro p o sed a rch itectu re fo r a n eed s-d riv en co m m u n ica tiv e a g en t.\nonly ties in closely with Powers\u2019 observations concerning the importance of optimising intrinsic variables that are related to the health of the organism, but it also links with his view that the control of such variables provides the basis for reorganisation (i.e. structural learning) [65].\nIn practice, the motivational framework can be decomposed into two elements: (i) a needs structure that provides the incentives for behaviour and (ii) the amount of effort that will be devoted to meeting those needs. The previous paragraph (and the architecture shown in Fig. 25) addresses the first of these. The second relates to the enthusiasm of an agent, i.e. how much it cares about meeting its needs. In a conventional PCT-style negative-feedback control loop, such behaviour would correspond to the loop gain; a high loop gain implying a high degree of physical effort/enthusiasm and vice versa. In the framework outlined here, such behaviour would relate to the depth and quality of the search(es) that pervade the overall structure - deep search(es) implying a high degree of mental effort/enthusiasm and vice versa.\nInterestingly, the proposed framework also provides a practical architecture that supports appraisal theories of emotion [109,110] based on the classic dimensions of pleasure and arousal [111]. For example, the comparators that pervade the structure each constitute mini-appraisal units that generate measures of positive/negative affect - the error signals [112]. Likewise, the effort that an agent devotes to achieving its goals and intentions (as discussed in the previous paragraph) can be be interpreted as a measure of arousal. Tying this in with Powers\u2019 notion of intrinsic variables driving reorganisation, these emergent properties of the proposed framework can be interpreted as (i) emotion driving behaviour and (ii) the perception of emotion driving learning25.\nFinally, the core components in the proposed framework have been portrayed in Fig. 25 as a three-layered control structure. However, this is just a necessary simplification in order to establish the basic principles. Also, the emphasis has been on a communicative agent that seeks to influence the internal states of another agent (as opposed to an agent that seeks to manipulate the physical behaviour of another agent). In practice, the structures outlined here would be decomposed into a richer hierarchy of parallel control loops - much as envisaged in HPCT. The main difference would be that the perceptual apparatus in the proposed framework would be more structured than in HPCT, with explicit parameter sharing between behavioural and perceptual components (rather than the switching arrangement illustrated in Fig. 8). Also, whilst Powers\u2019 mechanism for reorganisation should be capable of optimising the structure of a decomposed architecture for any given problem, in practice no-one has yet succeeded in developing a practical solution. Since some of the solutions presented here are related to more contemporary approaches to machine learning, it is envisaged that they might offer a way forward in understanding how to configure an HPCT structure automatically.\n25 This overlaps nicely with Powers\u2019 description of the role of emotion [65]."}, {"heading": "5 Summary and Conclusion", "text": "This chapter has shown how a practical framework for modelling and implementing \u2018intelligent\u2019 systems can be developed from the establishment of a set of fundamental principles which both intersects with and extends Perceptual Control Theory. It has been argued that PCT has hitherto placed less emphasis on the transformation of perceptual functions through mental simulation, and has treated the mapping from sensation to perception as a relatively straightforward series of transformations (albeit mediated by memory), thereby overlooking the potential of powerful generative model-based solutions that have emerged in practical fields such as visual or auditory scene analysis. Starting from first principles, and considering how an intelligent agent might interact with the world and with other agents it might contain, it has not only been shown how these ideas might be integrated into PCT, but also how PCT might be extended towards a remarkably symmetric architecture for a needs-driven communicative agent.\nThe arguments presented in Section 3 first led to the realisation that the optimisation which takes place within a negative feedback control loop is equivalent to a search process, and this led to a reformulation of intentional behaviour as a search over possible actions in order to find those which minimise the difference between an agent\u2019s intentions and the perceived consequences. These principles were then extended to cover the situation in which the consequences of actions might be hidden from direct observation (for example, when one agent is manipulating the mental state of another agent), and this led to the emergence of simulation as a key mechanism for maintaining a control loop structure by using an estimate/prediction of possible action consequences. It was then revealed how searching a simulation facilitates off-line planning which has the important benefit of being able to compute global (rather than local) solutions.\nThese principles were then extended to an agent attempting to interpret behaviour, and it was shown how the perceptual process itself can be configured using a familiar PCT-style negative feedback control loop (also involving simulation) which turns out to be equivalent to established techniques for model-based recognition. It was then shown how the arguments could be extended to cover agent-to-agent communication, and this led to the crucial realisation that the parameters of the models used by one agent for simulating another agent could be derived from the models used to manage their own behaviour - that is, using self to model other. This novel arrangement was then embedded within a BDI (beliefs, desires, intentions) structure to create a new framework for a needs-based communicative agent.\nOne of the encouraging outcomes of this analysis is that the new perspective not only relies on a central role being played by PCT-style negative-feedback control in action selection and interpretation, but it also links with recent discoveries in cognitive neuroscience (such as the sensorimotor overlap attributed to mirror neurons) and aligns in interesting ways with neurally-inspired architectures such as DAC (Section 1.3). Also, since it has been shown that negative feedback control (as an optimisation process) may be viewed as a type of search, this opens\nup a potentially important link with a range of model-based techniques that are already established in some areas of contemporary AI and Cognitive Systems (particularly in the field of spoken language processing). This means that the development of the approach can benefit from progress being made in these other fields, as well as extending the scope and influence of PCT to provide a more parsimonious computational framework for \u2018intelligent\u2019 systems (whether natural or artificial).\nBased on the framework established thus far, the critical next steps are to formulate practical mechanisms for learning local/global structures and to investigate the implications of simulations of self and other using small scale agents (such as robots) in tasks which involve real-world interaction and communication with other agents. It is too soon to extend the principles to complex scenarios (such as full blown language-based interaction). Rather, it is necessary to scaleup the complexity in a careful and controlled manner in order to maintain a firm scientific and mathematical foundation for the entire enterprise. Such work is already underway, and early results are encouraging.\nFinally, the arguments presented herein lead to the following overall conclusion with respect to modelling intelligent communicative agents - if behaviour is the control of perception (the central tenet of Perceptual Control Theory), then perception (at least for communicative agents) can be said to be the simulation of behaviour."}, {"heading": "6 Acknowledgements", "text": "The author would like to thank colleagues in the Sheffield Centre for Robotics (SCentRo) and the Bristol Robotics Laboratory (BRL) for discussions relating to the content of this chapter. This work was partially supported by the European Commission [grant numbers EU-FP6-507422, EU-FP6-034434, EU-FP7-231868, FP7-ICT-2013-10-611971] and the UK Engineering and Physical Sciences Research Council [grant number EP/I013512/1]."}], "references": [{"title": "The Age of Intelligent Machines", "author": ["R. Kurzweil"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1990}, {"title": "Towards Personal Service Robots for the Elderly", "author": ["N. Roy", "G. Baltus", "D. Fox", "F. Gemperle", "J. Goetz", "T. Hirsch", "D. Margaritis", "M. Montemerlo", "J. Pineau", "J. Schulte", "S. Thrun"], "venue": "In Workshop on Interactive Robots and Entertainment (WIRE", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Interactive robots as social partners and peer tutors for children: A field trial", "author": ["T. Kanda", "T. Hirano", "D. Eaton", "H. Ishiguro"], "venue": "Human-Computer Interaction,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Designing robots for long-term social interaction", "author": ["R. Gockley", "A. Bruce", "J. Forlizzi", "M.P. Michalowski", "A. Mundell"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Love and Sex with Robots, The Evolution of Human-Robot Relationships. Harper-Collins", "author": ["D. Levy"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "The clinical use of robots for individuals with Autism Spectrum Disorders: a critical review", "author": ["J.J. Diehl", "L.M.Schmitt", "M. Villano", "C.R. Crowell"], "venue": "Research in Autism Spectrum Disorders,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Robotics: A Very Short Introduction", "author": ["A. Winfield"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Biologically inspired robots", "author": ["F. Delcomyn"], "venue": "Bioinspiration and Robotics: Walking and Climbing Robots", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Bio-Inspired Artificial Intelligence: Theories, Methods, and Technologies", "author": ["D. Floreano", "C. Mattiussi"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Perception and the Representative Design of Psychological Experiments", "author": ["E. Brunswik"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1956}, {"title": "Artificial Intelligence: The Very Idea", "author": ["J. Haugeland"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1985}, {"title": "A robust layered control system for a mobile robot", "author": ["R. Brooks"], "venue": "IEEE Journal of Robotics and Automation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1986}, {"title": "Intelligence without representation", "author": ["R.A. Brooks"], "venue": "Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}, {"title": "Vehicles: Experiments in Synthetic Psychology", "author": ["V. Braitenberg"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1984}, {"title": "Behavior-Based Robotics", "author": ["R.C. Arkin"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "A survey of artificial cognitive systems: implications for the autonomous development of mental capabilities in computational agents", "author": ["D. Vernon", "G. Metta", "G. Sandini"], "venue": "IEEE Transactions on Evolutionary Computation, Special Issue on Autonomous Mental Development,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Philosophical foundations of enactive AI", "author": ["D. Vernon", "D. Furlong"], "venue": "In M. Lungarella et. al. (Ed.), 50 years of AI Festschrift,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Enaction as a conceptual framework for developmental cognitive robotics", "author": ["D. Vernon"], "venue": "Journal of Behavioral Robotics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Embodiment in cognitive systems: on the mutual dependence of cognition and robotics", "author": ["D. Vernon", "G. Metta", "G. Sandini"], "venue": "Embodied Cognitive Systems. Institution of Engineering and Technology (IET), UK", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "The theory of affordances", "author": ["J.J. Gibson"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1977}, {"title": "Synergy-based affordance learning for robotic grasping", "author": ["T. Geng", "J. Wilson", "M. Sheldon", "M. Lee", "M. Hulse"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Distributive adaptive control: a paradigm for designing autonomous agents", "author": ["R. Pfeifer", "P. Verschure"], "venue": "First European Conference on Artificial Life (pp. 21-30)", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1992}, {"title": "Distributed adaptive control: A theory of the mind, brain, body nexus", "author": ["Verschure", "P.F.M. J"], "venue": "Biologically Inspired Cognitive Architectures,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Environmentally mediated synergy between perception and behaviour in mobile", "author": ["Verschure", "P.F.M. J", "T. Voegtlin", "R.J. Douglas"], "venue": "PCT and Beyond", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "The Robot in the Crib: A developmental analysis of imitation skills in infants and robots", "author": ["Y. Demiris", "A. Meltzoff"], "venue": "Infant and Child Development,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Developmental Robotics: from babies to robots", "author": ["A. Cangelosi", "M. Schlesinger"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Premotor cortex and the recognition of motor actions", "author": ["G. Rizzolatti", "L. Fadiga", "V. Gallese", "L. Fogassi"], "venue": "Cognitive Brain Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1996}, {"title": "The mirror-neuron system", "author": ["G. Rizzolatti", "L. Craighero"], "venue": "Annual Review of Neuroscience,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Mirror neurons and imitation: a computationally guided review", "author": ["E. Oztop", "M. Kawato", "M. Arbib"], "venue": "Neural Networks,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "Embodiment and the Inner Life", "author": ["M. Shanahan"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Obsessed with goals: Functions and mechanisms of teleological interpretation of actions in humans", "author": ["G. Csibra", "G. Gergely"], "venue": "Acta Psychologica,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Agent-Based Models. Sage Publications, Inc", "author": ["N. Gilbert"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "A micro-level simulation for the prediction of intention and behavior", "author": ["J. Richetin", "A. Sengupta", "M. Perugini", "I. Adjali", "R. Hurling", "D. Greetham", "M. Spence"], "venue": "Cognitive Systems Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Statistical mechanics of cellular automata", "author": ["S. Wolfram"], "venue": "Reviews of Modern Physics,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1983}, {"title": "NetLogo: Design and Implementation of a MultiAgent Modeling Environment", "author": ["S. Tisue", "U. Wilensky"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "BDI Agents: from theory to practice. Melbourne: Australian Artificial Intelligence Institute", "author": ["A. Rao", "M. Georgoff"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "Reasoning About Rational Agents", "author": ["M. Wooldridge"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2000}, {"title": "robo-CAMAL: A BDI motivational robot", "author": ["D. Davis", "J. Gwatkin"], "venue": "Journal of Behavioral Robotics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Natural language programming of complex robotic BDI agents", "author": ["N. Lincoln", "S. Veres"], "venue": "Journal of Intelligent and Robotic Systems,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Learning multiple layers of representation", "author": ["G.E. Hinton"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Understanding Intelligence", "author": ["R. Pfeifer", "C. Scheier"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1999}, {"title": "Towards truly human-level intelligence in artificial applications", "author": ["M. De Kamps"], "venue": "Cognitive Systems Research,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Bukimi no tani (the uncanny valley)", "author": ["M. Mori"], "venue": "R. K. Moore: PCT and Beyond", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1970}, {"title": "A Bayesian explanation of the Uncanny Valley effect and related psychological phenomena", "author": ["R.K. Moore"], "venue": "Nature Scientific Reports,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2012}, {"title": "Building robot systems to interact with people in real environments", "author": ["B.A. Maxwell"], "venue": "Autonomous Robots,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2007}, {"title": "First steps toward natural human-like HRI", "author": ["M. Scheutz", "P. Schermerhorn", "J. Kramer", "D. Anderson"], "venue": "Autonomous Robots,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "Towards safety in human robot interaction", "author": ["G. Herrmann", "C. Melhuish"], "venue": "International Journal of Social Robotics,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2010}, {"title": "From talking and listening robots to intelligent communicative machines", "author": ["R.K. Moore"], "venue": "In J. Markowitz (Ed.), Robots That Talk and Listen", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}, {"title": "Awareness and Usage of Speech Technology", "author": ["Liao", "S.-H"], "venue": "Masters thesis,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2015}, {"title": "The Tree of Knowledge: The Biological Roots of Human Understanding", "author": ["H.R. Maturana", "F.J. Varela"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1987}, {"title": "Voice, (inter-)subjectivity, and real time recurrent interaction", "author": ["F. Cummins"], "venue": "Frontiers in Psychology,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2014}, {"title": "Language as an interaction system", "author": ["M.H. Bickhard"], "venue": "New Ideas in Psychology,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2007}, {"title": "Dialog as interpersonal synergy", "author": ["R. Fusaroli", "J. Raczaszek-Leonardi", "K. Tyl\u00e9n"], "venue": "New Ideas in Psychology,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2014}, {"title": "Speaking Our Minds: Why human communication is different, and how language evolved to make it special", "author": ["T. Scott-Phillips"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2015}, {"title": "A general feedback theory of human behavior: part I", "author": ["W.T. Powers", "R.K. Clark", "R.L. McFarland"], "venue": "Perceptual and Motor Skills,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 1960}, {"title": "A general feedback theory of human behavior: part II", "author": ["W.T. Powers", "R.K. Clark", "R.L. McFarland"], "venue": "Perception and Motor Skills,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 1960}, {"title": "Behavior: The Control of Perception. NY: Aldine: Hawthorne", "author": ["W.T. Powers"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1973}, {"title": "Control of perception should be operationalised as a fundamental property of the nervous system", "author": ["W. Mansell"], "venue": "Topics in Cognitive Science,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2011}, {"title": "Living Control Systems III: The Fact of Control. Benchmark Publications", "author": ["W.T. Powers"], "venue": null, "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2008}, {"title": "Feedback and Control Systems", "author": ["J.J. DiStefano III", "A.R. Stubberud", "I.J. Williams"], "venue": "Schaums Outline Series (2nd ed.)", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 1990}, {"title": "Subsumption vs Perceptual-Control Architectures in Behaviour-Based Robotics", "author": ["L. Dalchow"], "venue": null, "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2012}, {"title": "Perceptual Control Theory and its application", "author": ["M.M. Taylor"], "venue": "Int. J. Human-Computer Studies,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 1999}, {"title": "The nature of robots - part 1: defining behavior", "author": ["W.T. Powers"], "venue": null, "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1979}, {"title": "The nature of robots - part 2: simulated control systems", "author": ["W.T. Powers"], "venue": null, "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1979}, {"title": "The nature of robots - part 3: a closer look at human behavior. Byte", "author": ["W.T. Powers"], "venue": "PCT and Beyond", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1979}, {"title": "The nature of robots - part 4: looking for controlled variables", "author": ["W.T. Powers"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 1979}, {"title": "Control of a multi-legged robot based on hierarchical perceptual control theory", "author": ["J.R. Kennaway"], "venue": "J. Perceptual Control Theory,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 1999}, {"title": "A Simple and Robust Hierarchical Control System for a Walking Robot. Unpublished", "author": ["J.R. Kennaway"], "venue": null, "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2004}, {"title": "Spoken language processing: piecing together the puzzle", "author": ["R.K. Moore"], "venue": "Speech Communication,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2007}, {"title": "Explaining phonetic variation: a sketch of the H&H theory", "author": ["B. Lindblom"], "venue": null, "citeRegEx": "81", "shortCiteRegEx": "81", "year": 1990}, {"title": "The Voice in the Machine", "author": ["R. Pieraccini"], "venue": null, "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2012}, {"title": "PRESENCE: A human-inspired architecture for speechbased human-machine interaction", "author": ["R.K. Moore"], "venue": "IEEE Trans. Computers,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2007}, {"title": "Reactive speech synthesis: actively managing phonetic contrast along an H&H continuum. 17th International Congress of Phonetics Sciences (ICPhS)", "author": ["R.K. Moore", "M. Nicolao"], "venue": null, "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2011}, {"title": "C2H: A computational model of H&H-based phonetic contrast in synthetic speech. INTERSPEECH", "author": ["M. Nicolao", "J. Latorre", "R.K. Moore"], "venue": null, "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2012}, {"title": "Cognitive approaches to spoken language technology", "author": ["R.K. Moore"], "venue": "Speech Technology: Theory and Applications (pp. 89-103)", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2010}, {"title": "Predictive coding under the free-energy principle", "author": ["K. Friston", "S. Kiebel"], "venue": "Phil. Trans. R. Soc. B,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2009}, {"title": "Model-based recognition of 3D objects from single images", "author": ["I. Weiss", "M. Ray"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2001}, {"title": "Speaker-independent model-based single channel speech", "author": ["M.H. Radfar", "R.M. Dansereau", "A. Sayadiyan"], "venue": "separation. Neurocomputing,", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2008}, {"title": "The application of hidden Markov models in speech recognition", "author": ["M. Gales", "S.J. Young"], "venue": "Foundations and Trends in Signal Processing,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2007}, {"title": "Interpreting intentional behaviour", "author": ["R.K. Moore"], "venue": "Dagstuhl Seminar 13451 on Computational Audio Analysis (Vol", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2014}, {"title": "What is the expectation maximization algorithm", "author": ["C.B. Do", "S. Batzoglou"], "venue": "Nature Biotechnology,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "93", "shortCiteRegEx": "93", "year": 1998}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 1983}, {"title": "The emulation theory of representation: motor control, imagery, and perception", "author": ["R. Grush"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2004}, {"title": "The case for motor involvement in perceiving conspecifics", "author": ["M. Wilson", "G. Knoblich"], "venue": "Psychological Bulletin,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2005}, {"title": "Forward models and their implications for production, comprehension, and dialogue", "author": ["M.J. Pickering", "S. Garrod"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2013}, {"title": "Model-based recognition in robot vision", "author": ["R.T. Chin", "C.R. Dyer"], "venue": "ACM Computing Surveys,", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 1986}, {"title": "Hierarchical, Attentive, Multiple Models for Execution and Recognition (HAMMER)", "author": ["Y. Demiris", "B. Khadhouri"], "venue": "IEEE International Conference on Robotics and Automation (ICRA)", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2005}, {"title": "Perception of the speech code", "author": ["A. Liberman", "F.S. Cooper", "D.P. Shankweiler", "M. Studdert-Kennedy"], "venue": "Psychological Review,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 1967}, {"title": "Echoes of the spoken past: how auditory cortex hears context during speech perception", "author": ["J.I. Skipper"], "venue": "Phil Trans R Soc B,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2014}, {"title": "Infants\u2019 brain responses to speech suggest Analysis by Synthesis", "author": ["P.K. Kuhl", "R.R. Ramirez", "A. Bosseler", "Lin", "J.-F. L", "T. Imada"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2014}, {"title": "Spoken language Processing: Time to Look Outside", "author": ["R.K. Moore"], "venue": "In 2nd International Conference on Statistical Language and Speech Processing (SLSP", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2014}, {"title": "Understanding others: imitation, language, empathy", "author": ["M. Iacoboni"], "venue": "Perspectives on Imitation: From Mirror Neurons to Memes (Vol", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2005}, {"title": "Does the chimpanzee have a theory of mind", "author": ["D. Premack", "G. Woodruff"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 1978}, {"title": "Making inferences about intention: perceptual control theory as a \u2018theory of mind\u2019 for psychologists", "author": ["R.S. Marken"], "venue": "Psychological Reports: Measures & Statistics,", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2013}, {"title": "A theory of human motivation", "author": ["A.H. Maslow"], "venue": "Psychological Review,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 1943}, {"title": "What is Intrinsic Motivation? A Typology of Computational Approaches", "author": ["Oudeyer", "P.-Y", "F. Kaplan"], "venue": "Frontiers in Neurorobotics,", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 2007}, {"title": "Appraisal Processes in Emotion: Theory, Methods, Research", "author": ["K.R. Scherer", "A. Schorr", "T. Johnstone"], "venue": null, "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2001}, {"title": "Computational models of emotion. A Blueprint for Affective Computing-A", "author": ["S. Marsella", "J. Gratch", "P. Petta"], "venue": "Sourcebook and Manual,", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2010}, {"title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament. Current Psychology: Developmental", "author": ["A. Mehrabian"], "venue": "Learning, Personality,", "citeRegEx": "111", "shortCiteRegEx": "111", "year": 1996}, {"title": "On the Self-Regulation of Behavior", "author": ["C.S. Carver", "M.F. Scheier"], "venue": null, "citeRegEx": "112", "shortCiteRegEx": "112", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "The past few decades have seen an enormous growth in the level of interest being shown in so-called \u2018intelligent\u2019 machines [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "Ranging from production-line robots to homecare assistants, and from robotic surgeons to driverless cars, future intelligent systems are expected to transform our lives in much the same way that the invention of the steam engine accelerated the industrial revolution during the 18 and 19 centuries [2,3,4,5,6].", "startOffset": 298, "endOffset": 309}, {"referenceID": 2, "context": "Ranging from production-line robots to homecare assistants, and from robotic surgeons to driverless cars, future intelligent systems are expected to transform our lives in much the same way that the invention of the steam engine accelerated the industrial revolution during the 18 and 19 centuries [2,3,4,5,6].", "startOffset": 298, "endOffset": 309}, {"referenceID": 3, "context": "Ranging from production-line robots to homecare assistants, and from robotic surgeons to driverless cars, future intelligent systems are expected to transform our lives in much the same way that the invention of the steam engine accelerated the industrial revolution during the 18 and 19 centuries [2,3,4,5,6].", "startOffset": 298, "endOffset": 309}, {"referenceID": 4, "context": "Ranging from production-line robots to homecare assistants, and from robotic surgeons to driverless cars, future intelligent systems are expected to transform our lives in much the same way that the invention of the steam engine accelerated the industrial revolution during the 18 and 19 centuries [2,3,4,5,6].", "startOffset": 298, "endOffset": 309}, {"referenceID": 5, "context": "Ranging from production-line robots to homecare assistants, and from robotic surgeons to driverless cars, future intelligent systems are expected to transform our lives in much the same way that the invention of the steam engine accelerated the industrial revolution during the 18 and 19 centuries [2,3,4,5,6].", "startOffset": 298, "endOffset": 309}, {"referenceID": 6, "context": "However, in order to deliver the expected benefits, future autonomous systems will indeed need to be intelligent - they must integrate seamlessly into real-world environments, act appropriately in complex physical and temporal situations, solve difficult logistical problems, interact effectively with human users/operators using accepted social conventions (such as speech and language), be robust in the face of unpredictable disturbances and interruptions, operate independently within an accepted ethical framework, and be at least partially responsible for their own behaviours [7].", "startOffset": 583, "endOffset": 586}, {"referenceID": 7, "context": "This means that it is not only necessary to develop comprehensive tools and techniques for designing, building and programming such devices to meet particular application requirements, but it is very likely that approaches will also need to be based on a deeper understanding of how existing intelligent systems - living organisms - solve the challenges listed above [8,9].", "startOffset": 367, "endOffset": 372}, {"referenceID": 8, "context": "This means that it is not only necessary to develop comprehensive tools and techniques for designing, building and programming such devices to meet particular application requirements, but it is very likely that approaches will also need to be based on a deeper understanding of how existing intelligent systems - living organisms - solve the challenges listed above [8,9].", "startOffset": 367, "endOffset": 372}, {"referenceID": 9, "context": "The term Artificial Intelligence (AI) was coined in 1955 and, in its early years, was mainly concerned with mathematical logic and automatic theorem proving (on the assumption that \u2018intelligence\u2019 was founded on processes of rational thought) [10].", "startOffset": 242, "endOffset": 246}, {"referenceID": 10, "context": "1), very much in tune with the \u2018behaviourist\u2019 Stimulus \u2192 Response (S-R) framework that was dominating the field of psychology at the time [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "However, it soon became apparent that this perspective - subsequently termed Good Old-Fashioned Artificial Intelligence (GOFAI) [12] - suffered from severe limitations, particularly with respect to an over-reliance on accurate world models and an inability to respond quickly to changing situations and context.", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "Hailing the establishment of what was to be known as \u2018New AI\u2019, Rodney Brooks introduced a novel \u2018subsumption\u2019 architecture in which the emphasis was on real-time behaviour using simple computations embedded in a layered structure [13], [14] (see Fig.", "startOffset": 230, "endOffset": 234}, {"referenceID": 13, "context": "Hailing the establishment of what was to be known as \u2018New AI\u2019, Rodney Brooks introduced a novel \u2018subsumption\u2019 architecture in which the emphasis was on real-time behaviour using simple computations embedded in a layered structure [13], [14] (see Fig.", "startOffset": 236, "endOffset": 240}, {"referenceID": 14, "context": "Overall, the emphasis was on the grounding of behaviour in the real-world, with the hypothesis that complex interactions should arise as an emergent property of an array of simple processes (as had been elegantly proposed by Braitenberg in the 1980s [15]).", "startOffset": 250, "endOffset": 254}, {"referenceID": 15, "context": "The subsumption approach initiated a trend towards what became known as \u2018behaviour-based robotics\u2019 [16], and the emphasis shifted from high-level abstract problem solving to low-level grounded intelligence.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "Known as \u2018artificial cognitive systems\u2019 [17,18], the most significant influence has been the emphasis on enaction, embodiment and situatedness in which the relationship between a goal-driven robot and its physical real-world context is paramount [19,20].", "startOffset": 40, "endOffset": 47}, {"referenceID": 17, "context": "Known as \u2018artificial cognitive systems\u2019 [17,18], the most significant influence has been the emphasis on enaction, embodiment and situatedness in which the relationship between a goal-driven robot and its physical real-world context is paramount [19,20].", "startOffset": 40, "endOffset": 47}, {"referenceID": 18, "context": "Known as \u2018artificial cognitive systems\u2019 [17,18], the most significant influence has been the emphasis on enaction, embodiment and situatedness in which the relationship between a goal-driven robot and its physical real-world context is paramount [19,20].", "startOffset": 246, "endOffset": 253}, {"referenceID": 19, "context": "Known as \u2018artificial cognitive systems\u2019 [17,18], the most significant influence has been the emphasis on enaction, embodiment and situatedness in which the relationship between a goal-driven robot and its physical real-world context is paramount [19,20].", "startOffset": 246, "endOffset": 253}, {"referenceID": 20, "context": "Interaction is facilitated by the \u2018affordances\u2019 [21,22] provided by a robot\u2019s environment.", "startOffset": 48, "endOffset": 55}, {"referenceID": 21, "context": "Interaction is facilitated by the \u2018affordances\u2019 [21,22] provided by a robot\u2019s environment.", "startOffset": 48, "endOffset": 55}, {"referenceID": 22, "context": "A particular example of a contemporary architecture for an artificial cognitive system is Distributed Adaptive Control (DAC) [23,24].", "startOffset": 125, "endOffset": 132}, {"referenceID": 23, "context": "A particular example of a contemporary architecture for an artificial cognitive system is Distributed Adaptive Control (DAC) [23,24].", "startOffset": 125, "endOffset": 132}, {"referenceID": 24, "context": "Unlike their GOFAI predecessors, modern cognitive approaches to intelligent systems (such as DAC) view action and perception as being synergistic (rather than separate) processes [25], and that skills should be acquired through the robot\u2019s own active exploration of the world (rather then being pre-programmed by the system designer) [26,27].", "startOffset": 179, "endOffset": 183}, {"referenceID": 25, "context": "Unlike their GOFAI predecessors, modern cognitive approaches to intelligent systems (such as DAC) view action and perception as being synergistic (rather than separate) processes [25], and that skills should be acquired through the robot\u2019s own active exploration of the world (rather then being pre-programmed by the system designer) [26,27].", "startOffset": 334, "endOffset": 341}, {"referenceID": 26, "context": "Unlike their GOFAI predecessors, modern cognitive approaches to intelligent systems (such as DAC) view action and perception as being synergistic (rather than separate) processes [25], and that skills should be acquired through the robot\u2019s own active exploration of the world (rather then being pre-programmed by the system designer) [26,27].", "startOffset": 334, "endOffset": 341}, {"referenceID": 27, "context": "Such approaches very much reflect contemporary models of living systems, particularly the discovery in the 1990s of so-called \u2018mirror neurons\u2019 - neural structures which appear to provide a vital link between sensory and motor behaviour and thereby a mechanism for action understanding, imitation and learning [28,29,30].", "startOffset": 309, "endOffset": 319}, {"referenceID": 28, "context": "Such approaches very much reflect contemporary models of living systems, particularly the discovery in the 1990s of so-called \u2018mirror neurons\u2019 - neural structures which appear to provide a vital link between sensory and motor behaviour and thereby a mechanism for action understanding, imitation and learning [28,29,30].", "startOffset": 309, "endOffset": 319}, {"referenceID": 29, "context": "Such approaches very much reflect contemporary models of living systems, particularly the discovery in the 1990s of so-called \u2018mirror neurons\u2019 - neural structures which appear to provide a vital link between sensory and motor behaviour and thereby a mechanism for action understanding, imitation and learning [28,29,30].", "startOffset": 309, "endOffset": 319}, {"referenceID": 30, "context": "A recent book by Murray Shanahan provides an excellent overview of the contemporary perspective in artificial cognitive systems [31].", "startOffset": 128, "endOffset": 132}, {"referenceID": 23, "context": "1 in [24]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 31, "context": "It is clearly the case that living systems appear to be goal-directed and purposeful in their endeavours [32], and this has had a major influence on AI and robotics.", "startOffset": 105, "endOffset": 109}, {"referenceID": 32, "context": "In particular, one area in which intentionality plays a key role is the field of Agent-Based Modelling (ABM) [33,34].", "startOffset": 109, "endOffset": 116}, {"referenceID": 33, "context": "In particular, one area in which intentionality plays a key role is the field of Agent-Based Modelling (ABM) [33,34].", "startOffset": 109, "endOffset": 116}, {"referenceID": 34, "context": "Various modelling paradigms are employed, such as cellular automata [35] or dedicated multi-agent programming environments [36].", "startOffset": 68, "endOffset": 72}, {"referenceID": 35, "context": "Various modelling paradigms are employed, such as cellular automata [35] or dedicated multi-agent programming environments [36].", "startOffset": 123, "endOffset": 127}, {"referenceID": 36, "context": "However, for \u2018intelligent\u2019 agents, ABM simulations are often constructed using a Beliefs Desires Intentions (BDI) architecture [37,38] on the premise that such internal structures are required to adequately condition the behaviour of individual agents - see Fig.", "startOffset": 127, "endOffset": 134}, {"referenceID": 37, "context": "However, for \u2018intelligent\u2019 agents, ABM simulations are often constructed using a Beliefs Desires Intentions (BDI) architecture [37,38] on the premise that such internal structures are required to adequately condition the behaviour of individual agents - see Fig.", "startOffset": 127, "endOffset": 134}, {"referenceID": 38, "context": "BDI is a powerful approach to modelling agents, and it has been applied successfully to robotics [39,40].", "startOffset": 97, "endOffset": 104}, {"referenceID": 39, "context": "BDI is a powerful approach to modelling agents, and it has been applied successfully to robotics [39,40].", "startOffset": 97, "endOffset": 104}, {"referenceID": 40, "context": "Also, recent years have seen tremendous growth in the field of Artificial Neural Networks (ANNs), particularly with the success of deep learning as a mechanism to optimise the parameters of such multilayered networks on massive amounts of training data [45,46].", "startOffset": 253, "endOffset": 260}, {"referenceID": 41, "context": "Also, recent years have seen tremendous growth in the field of Artificial Neural Networks (ANNs), particularly with the success of deep learning as a mechanism to optimise the parameters of such multilayered networks on massive amounts of training data [45,46].", "startOffset": 253, "endOffset": 260}, {"referenceID": 42, "context": "Nevertheless, despite this tremendous progress, there is still a long way to go before artificial intelligent systems will be able to demonstrate the flexibility, robustness and autonomy exhibited by even the simplest living organism [47,48].", "startOffset": 234, "endOffset": 241}, {"referenceID": 43, "context": "Nevertheless, despite this tremendous progress, there is still a long way to go before artificial intelligent systems will be able to demonstrate the flexibility, robustness and autonomy exhibited by even the simplest living organism [47,48].", "startOffset": 234, "endOffset": 241}, {"referenceID": 44, "context": "Many contemporary robots rely on teleoperation by human operators to overcome weaknesses in their overall design, and attempts to create humanoid robots are fraught with difficulties ranging from the risk of repulsing human users due to the uncanny valley effect [49,50], to the more general uncertainties associated with human-robot interaction [51,52,53] - especially if it involves communicative agents and spoken language [54].", "startOffset": 263, "endOffset": 270}, {"referenceID": 45, "context": "Many contemporary robots rely on teleoperation by human operators to overcome weaknesses in their overall design, and attempts to create humanoid robots are fraught with difficulties ranging from the risk of repulsing human users due to the uncanny valley effect [49,50], to the more general uncertainties associated with human-robot interaction [51,52,53] - especially if it involves communicative agents and spoken language [54].", "startOffset": 263, "endOffset": 270}, {"referenceID": 46, "context": "Many contemporary robots rely on teleoperation by human operators to overcome weaknesses in their overall design, and attempts to create humanoid robots are fraught with difficulties ranging from the risk of repulsing human users due to the uncanny valley effect [49,50], to the more general uncertainties associated with human-robot interaction [51,52,53] - especially if it involves communicative agents and spoken language [54].", "startOffset": 346, "endOffset": 356}, {"referenceID": 47, "context": "Many contemporary robots rely on teleoperation by human operators to overcome weaknesses in their overall design, and attempts to create humanoid robots are fraught with difficulties ranging from the risk of repulsing human users due to the uncanny valley effect [49,50], to the more general uncertainties associated with human-robot interaction [51,52,53] - especially if it involves communicative agents and spoken language [54].", "startOffset": 346, "endOffset": 356}, {"referenceID": 48, "context": "Many contemporary robots rely on teleoperation by human operators to overcome weaknesses in their overall design, and attempts to create humanoid robots are fraught with difficulties ranging from the risk of repulsing human users due to the uncanny valley effect [49,50], to the more general uncertainties associated with human-robot interaction [51,52,53] - especially if it involves communicative agents and spoken language [54].", "startOffset": 346, "endOffset": 356}, {"referenceID": 49, "context": "Many contemporary robots rely on teleoperation by human operators to overcome weaknesses in their overall design, and attempts to create humanoid robots are fraught with difficulties ranging from the risk of repulsing human users due to the uncanny valley effect [49,50], to the more general uncertainties associated with human-robot interaction [51,52,53] - especially if it involves communicative agents and spoken language [54].", "startOffset": 426, "endOffset": 430}, {"referenceID": 50, "context": "This has been confirmed by a recent survey that discovered only 13% of respondants use their voicebased personal agent daily, whereas 46% had tried it once and then abandoned it [55].", "startOffset": 178, "endOffset": 182}, {"referenceID": 51, "context": "agents that serves to facilitate distributed sense-making through cooperative behaviours [57,58,59,60,61].", "startOffset": 89, "endOffset": 105}, {"referenceID": 52, "context": "agents that serves to facilitate distributed sense-making through cooperative behaviours [57,58,59,60,61].", "startOffset": 89, "endOffset": 105}, {"referenceID": 53, "context": "agents that serves to facilitate distributed sense-making through cooperative behaviours [57,58,59,60,61].", "startOffset": 89, "endOffset": 105}, {"referenceID": 54, "context": "agents that serves to facilitate distributed sense-making through cooperative behaviours [57,58,59,60,61].", "startOffset": 89, "endOffset": 105}, {"referenceID": 55, "context": "Furthermore, the contemporary view is that language is based on the co-evolution of two key traits: ostensive-inferential communication and recursive mind-reading [62].", "startOffset": 163, "endOffset": 167}, {"referenceID": 56, "context": "For example, what might Perceptual Control Theory (PCT) - established over 50 years ago [63,64] (and the topic of this volume) - have to contribute to future \u2018intelligent\u2019 communicative systems?", "startOffset": 88, "endOffset": 95}, {"referenceID": 57, "context": "For example, what might Perceptual Control Theory (PCT) - established over 50 years ago [63,64] (and the topic of this volume) - have to contribute to future \u2018intelligent\u2019 communicative systems?", "startOffset": 88, "endOffset": 95}, {"referenceID": 58, "context": "Although it is more than 40 years since the publication of Bill Powers\u2019 seminal book \u2018Behavior: The Control of Perception\u2019 (B:CP) [65], Perceptual Control Theory may still have something important to say about computational models of \u2018intelligent\u2019 communicative behaviour.", "startOffset": 130, "endOffset": 134}, {"referenceID": 59, "context": "of PCT has been to question the traditional behaviourist stimulus\u2192response stance that still prevails in some sections of the psychology field [66,67].", "startOffset": 143, "endOffset": 150}, {"referenceID": 60, "context": "Crucially, PCT focuses attention on the control of perceptual inputs rather than behavioural outputs [68].", "startOffset": 101, "endOffset": 105}, {"referenceID": 61, "context": "As a reminder, PCT was developed in the context of existing knowledge of the classic theory of automatic control - the study of systems that are capable of self-regulation, usually through the mechanism of negative feedback [69].", "startOffset": 224, "endOffset": 228}, {"referenceID": 61, "context": "2-6 on page 16 of [69]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 58, "context": "PCT, on the other hand, decomposes a system into a hierarchical/layered structure with a multiplicity of control loops at each level; this is referred to as Hierarchical Perceptual Control Theory (HPCT) [65].", "startOffset": 203, "endOffset": 207}, {"referenceID": 58, "context": "2 on page 61 of [65]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 58, "context": "Powers [65] proposed specific orders of control (starting at the lowest first-order level) as follows: intensity \u21d4 sensation \u21d4 configuration \u21d4 transitions \u21d4 sequence \u21d4 relationships \u21d4 program \u21d4 principles \u21d4 system concepts.", "startOffset": 7, "endOffset": 11}, {"referenceID": 62, "context": "However, a key distinction is that HPCT is capable of optimising many variables simultaneously within a negative feedback control framework, whereas subsumption is essentially an event-driven stimulus\u2192response architecture [70].", "startOffset": 223, "endOffset": 227}, {"referenceID": 63, "context": "Perceptual control theory (and HPCT) appears to offer a number of interesting features for modelling the behaviour of living systems [71], and should thus be a serious contender for implementing control in artificial systems such as robots (as envisaged by Powers [72,73,74,75]).", "startOffset": 133, "endOffset": 137}, {"referenceID": 64, "context": "Perceptual control theory (and HPCT) appears to offer a number of interesting features for modelling the behaviour of living systems [71], and should thus be a serious contender for implementing control in artificial systems such as robots (as envisaged by Powers [72,73,74,75]).", "startOffset": 264, "endOffset": 277}, {"referenceID": 65, "context": "Perceptual control theory (and HPCT) appears to offer a number of interesting features for modelling the behaviour of living systems [71], and should thus be a serious contender for implementing control in artificial systems such as robots (as envisaged by Powers [72,73,74,75]).", "startOffset": 264, "endOffset": 277}, {"referenceID": 66, "context": "Perceptual control theory (and HPCT) appears to offer a number of interesting features for modelling the behaviour of living systems [71], and should thus be a serious contender for implementing control in artificial systems such as robots (as envisaged by Powers [72,73,74,75]).", "startOffset": 264, "endOffset": 277}, {"referenceID": 67, "context": "Perceptual control theory (and HPCT) appears to offer a number of interesting features for modelling the behaviour of living systems [71], and should thus be a serious contender for implementing control in artificial systems such as robots (as envisaged by Powers [72,73,74,75]).", "startOffset": 264, "endOffset": 277}, {"referenceID": 68, "context": "A few attempts have been made in this area by PCT practitioners [76,77,78].", "startOffset": 64, "endOffset": 74}, {"referenceID": 69, "context": "A few attempts have been made in this area by PCT practitioners [76,77,78].", "startOffset": 64, "endOffset": 74}, {"referenceID": 70, "context": "On the other hand, (H)PCT has started to have some influence in the field of spoken language processing [79].", "startOffset": 104, "endOffset": 108}, {"referenceID": 58, "context": "2 on page 221 of [65]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 71, "context": "as resulting from the operation of negative feedback processes [81].", "startOffset": 63, "endOffset": 67}, {"referenceID": 72, "context": "However, mainstream speech technology systems have tended to ignore speaker-listener dependencies and such behaviour is simply modelled as stochastic variation [82].", "startOffset": 160, "endOffset": 164}, {"referenceID": 73, "context": "3), the PREdictive SENsory Control and Emulation (PRESENCE) architecture [83] provides some novel technical solutions in this area.", "startOffset": 73, "endOffset": 77}, {"referenceID": 74, "context": "For example, a PRESENCE-based text-to-speech synthesiser - C2H - has been developed that is capable of adjusting its pronunciation while it is speaking as a function of its perceived communicative success, the latter being judged using a perceptual feedback path in which the \u2018controlled variable\u2019 was an estimate of the intelligibility [84,85].", "startOffset": 337, "endOffset": 344}, {"referenceID": 75, "context": "For example, a PRESENCE-based text-to-speech synthesiser - C2H - has been developed that is capable of adjusting its pronunciation while it is speaking as a function of its perceived communicative success, the latter being judged using a perceptual feedback path in which the \u2018controlled variable\u2019 was an estimate of the intelligibility [84,85].", "startOffset": 337, "endOffset": 344}, {"referenceID": 76, "context": "PRESENCE not only provides a novel architecture for generating spoken language, it also shows how the principles of feedback control can be applied to the recognition and interpretation of spoke language [86].", "startOffset": 204, "endOffset": 208}, {"referenceID": 70, "context": "Illustration of the PCT-inspired PREdictive SENsorimotor Control and Emulation (PRESENCE) architecture for spoken language processing [79,83]).", "startOffset": 134, "endOffset": 141}, {"referenceID": 73, "context": "Illustration of the PCT-inspired PREdictive SENsorimotor Control and Emulation (PRESENCE) architecture for spoken language processing [79,83]).", "startOffset": 134, "endOffset": 141}, {"referenceID": 58, "context": "B:CP [65], Powers himself does not say a great deal about perceptual mechanisms; he acknowledges the potentially arbitrary relationship that exists between internal perceptions and external reality, but he seems to assume some form of straightforward (neural) transformation/mapping from first-order intensities to invariant higher-order perceptions that is carried out by an \u2018input function\u2019 (albeit mediated by memory - see Section 2.", "startOffset": 5, "endOffset": 9}, {"referenceID": 77, "context": "However, in practical fields such as visual or auditory scene analysis (and even in the latest theories of brain function [87]), the notion of \u2018perception as transformation\u2019 has been complemented by the emergence of more powerful (and more successful) generative model-based solutions [88,89,90].", "startOffset": 122, "endOffset": 126}, {"referenceID": 78, "context": "However, in practical fields such as visual or auditory scene analysis (and even in the latest theories of brain function [87]), the notion of \u2018perception as transformation\u2019 has been complemented by the emergence of more powerful (and more successful) generative model-based solutions [88,89,90].", "startOffset": 285, "endOffset": 295}, {"referenceID": 79, "context": "However, in practical fields such as visual or auditory scene analysis (and even in the latest theories of brain function [87]), the notion of \u2018perception as transformation\u2019 has been complemented by the emergence of more powerful (and more successful) generative model-based solutions [88,89,90].", "startOffset": 285, "endOffset": 295}, {"referenceID": 80, "context": "However, in practical fields such as visual or auditory scene analysis (and even in the latest theories of brain function [87]), the notion of \u2018perception as transformation\u2019 has been complemented by the emergence of more powerful (and more successful) generative model-based solutions [88,89,90].", "startOffset": 285, "endOffset": 295}, {"referenceID": 81, "context": "Not only is this an interesting route to take, but it also turns out to be a crucial step towards a deeper understanding of how one intelligent system (such as an autonomous social robot) might interact/communicate with another (such as a human being) [91].", "startOffset": 252, "endOffset": 256}, {"referenceID": 82, "context": "For example, model parameters could be computed using complex mathematical tools for system estimation or by employing machine learning techniques on extensive quantities of training data using a process known as \u2018expectation maximisation\u2019 (EM) [92].", "startOffset": 245, "endOffset": 249}, {"referenceID": 83, "context": "In general, perceptions and actions may lie in (different) high-dimensional spaces and, either the relationship between possible behaviours and the perceptual error gradient is (somehow) known in advance, or it has to be discovered (learnt) by active exploration; for example, using \u2018reinforcement learning\u2019 [93] or the process referred to in PCT as \u2018reorganisation\u2019 - see Section 2.", "startOffset": 308, "endOffset": 312}, {"referenceID": 84, "context": "2) which is, itself, a negative feedback control process (and which is interestingly similar to the \u2018actor-critic\u2019 approach to reinforcement learning [94]).", "startOffset": 150, "endOffset": 154}, {"referenceID": 85, "context": "What is interesting in this arrangement is that the estimated transform f\u0302w can be interpreted as a form of mental simulation (or predictor) which emulates the consequences of possible actions prior to action selection [95].", "startOffset": 219, "endOffset": 223}, {"referenceID": 86, "context": "18 is a familiar model-based recognition framework in which the recognition/interpretation/inference of the (hidden) cause of observed behaviour is viewed as a search over possible outputs from a forward model that is capable of generating that behaviour [96,97].", "startOffset": 255, "endOffset": 262}, {"referenceID": 87, "context": "18 is a familiar model-based recognition framework in which the recognition/interpretation/inference of the (hidden) cause of observed behaviour is viewed as a search over possible outputs from a forward model that is capable of generating that behaviour [96,97].", "startOffset": 255, "endOffset": 262}, {"referenceID": 88, "context": "(known as analysis-by-synthesis), and it has been applied very successfully in a range of practical scenarios [98,88,99].", "startOffset": 110, "endOffset": 120}, {"referenceID": 78, "context": "(known as analysis-by-synthesis), and it has been applied very successfully in a range of practical scenarios [98,88,99].", "startOffset": 110, "endOffset": 120}, {"referenceID": 89, "context": "(known as analysis-by-synthesis), and it has been applied very successfully in a range of practical scenarios [98,88,99].", "startOffset": 110, "endOffset": 120}, {"referenceID": 80, "context": "Interestingly, this configuration is exactly how contemporary approaches to automatic speech recognition are formulated (using a probabilistic forward generative model known as a \u2018Hidden Markov Model\u2019 - HMM) [90]).", "startOffset": 208, "endOffset": 212}, {"referenceID": 90, "context": "In fact, the analysis-by-synthesis approach to speech recognition is not only reminiscent of the Motor Theory of speech perception [100], but is also supported by recent neuroimaging data [101,102].", "startOffset": 131, "endOffset": 136}, {"referenceID": 91, "context": "In fact, the analysis-by-synthesis approach to speech recognition is not only reminiscent of the Motor Theory of speech perception [100], but is also supported by recent neuroimaging data [101,102].", "startOffset": 188, "endOffset": 197}, {"referenceID": 92, "context": "In fact, the analysis-by-synthesis approach to speech recognition is not only reminiscent of the Motor Theory of speech perception [100], but is also supported by recent neuroimaging data [101,102].", "startOffset": 188, "endOffset": 197}, {"referenceID": 58, "context": "In B:CP [65], Powers doesn\u2019t use the term \u2018intention\u2019 as such, rather he talks about purposeful behaviour (at all levels of the HPCT hierarchy).", "startOffset": 8, "endOffset": 12}, {"referenceID": 58, "context": "This appears to be similar to the important role that intrinsic control systems play in the development of the perceptual hierarchy in HPCT [65].", "startOffset": 140, "endOffset": 144}, {"referenceID": 94, "context": "In other words, it provides a basis for modelling interpersonal stances such as empathy [104] and Theory of Mind (ToM) [105].", "startOffset": 88, "endOffset": 93}, {"referenceID": 95, "context": "In other words, it provides a basis for modelling interpersonal stances such as empathy [104] and Theory of Mind (ToM) [105].", "startOffset": 119, "endOffset": 124}, {"referenceID": 97, "context": "25 is that, like DAC and PRESENCE, it is founded on a motivational system based on needs (for example, as described by Maslow in his famous \u2018Hierarchy of Needs\u2019 [107]).", "startOffset": 161, "endOffset": 166}, {"referenceID": 98, "context": "However, invoking such a needs-based framework answers a fundamental question - why would an agent (natural or artificial) do anything [108]? Also, it is envisaged that it would be useful to partition needs into those that pertain to physical health (such as survival and safety needs) and those that pertain to psychological health (such as personal and social needs), with the relevant feedback signals being related to physiological and mental well-being.", "startOffset": 135, "endOffset": 140}, {"referenceID": 93, "context": "22 A forerunner of this framework has been referred to as MBDIAC (Mutual Beliefs Desires Intentions Actions & Consequences) [103].", "startOffset": 124, "endOffset": 129}, {"referenceID": 96, "context": "24 Interestingly, Marken - a long-standing PCT practitioner - has recently posited the value of PCT in testing the validity of the inferences that observers make about the intentional states of others [106].", "startOffset": 201, "endOffset": 206}, {"referenceID": 58, "context": "structural learning) [65].", "startOffset": 21, "endOffset": 25}, {"referenceID": 99, "context": "Interestingly, the proposed framework also provides a practical architecture that supports appraisal theories of emotion [109,110] based on the classic dimensions of pleasure and arousal [111].", "startOffset": 121, "endOffset": 130}, {"referenceID": 100, "context": "Interestingly, the proposed framework also provides a practical architecture that supports appraisal theories of emotion [109,110] based on the classic dimensions of pleasure and arousal [111].", "startOffset": 121, "endOffset": 130}, {"referenceID": 101, "context": "Interestingly, the proposed framework also provides a practical architecture that supports appraisal theories of emotion [109,110] based on the classic dimensions of pleasure and arousal [111].", "startOffset": 187, "endOffset": 192}, {"referenceID": 102, "context": "For example, the comparators that pervade the structure each constitute mini-appraisal units that generate measures of positive/negative affect - the error signals [112].", "startOffset": 164, "endOffset": 169}, {"referenceID": 58, "context": "25 This overlaps nicely with Powers\u2019 description of the role of emotion [65].", "startOffset": 72, "endOffset": 76}], "year": 2016, "abstractText": "Recent years have witnessed increasing interest in the potential benefits of \u2018intelligent\u2019 autonomous machines such as robots. Honda\u2019s Asimo humanoid robot, iRobot\u2019s Roomba robot vacuum cleaner and Google\u2019s driverless cars have fired the imagination of the general public, and social media buzz with speculation about a utopian world of helpful robot assistants or the coming robot apocalypse! However, there is a long way to go before autonomous systems reach the level of capabilities required for even the simplest of tasks involving human-robot interaction especially if it involves communicative behaviour such as speech and language. Of course the field of Artificial Intelligence (AI) has made great strides in these areas, and has moved on from abstract high-level rule-based paradigms to embodied architectures whose operations are grounded in real physical environments. What is still missing, however, is an overarching theory of intelligent communicative behaviour that informs system-level design decisions in order to provide a more coherent approach to system integration. This chapter introduces the beginnings of such a framework inspired by the principles of Perceptual Control Theory (PCT). In particular, it is observed that PCT has hitherto tended to view perceptual processes as a relatively straightforward series of transformations from sensation to perception, and has overlooked the potential of powerful generative model-based solutions that have emerged in practical fields such as visual or auditory scene analysis. Starting from first principles, a sequence of arguments is presented which not only shows how these ideas might be integrated into PCT, but which also extend PCT towards a remarkably symmetric architecture for a needs-driven communicative agent. It is concluded that, if behaviour is the control of perception (the central tenet of PCT), then perception (at least for communicative agents) is the simulation of behaviour.", "creator": "LaTeX with hyperref package"}}}