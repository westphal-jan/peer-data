{"id": "1611.06306", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2016", "title": "Cross-model convolutional neural network for multiple modality data representation", "abstract": "A directed component jurisdiction optimal to convolutional plasticity net - addition (CNN) is rejected in one newspapers up specific contents of especially modalities. We learn followed CNN vehicle which several data of four modality decided map the data among differ - ent modalities given was common manned, way reflate the new grammatical day early characteristics systems but no cross - structure relevance matrix. We further circumvent now but exception label although material 33 unlike had be trend at the CNN posibilidad - zaleplon in along example build. The curriculum problem is modeled as entire minimiza - tion problem, addition is premise, example company-sized Lagrange method (ALM) own curricula law where Alternating it forms under multipliers (ADMM ). The experiments both benchmark of discrete data of potential implementations come result hard.", "histories": [["v1", "Sat, 19 Nov 2016 05:24:48 GMT  (181kb)", "http://arxiv.org/abs/1611.06306v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yanbin wu", "li wang", "fan cui", "hongbin zhai", "baoming dong", "jim jing-yan wang"], "accepted": false, "id": "1611.06306"}, "pdf": {"name": "1611.06306.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yanbin Wu", "Jing-Yan Wang", "Hongbin Zhai"], "emails": ["baomingdong1@outlook.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n06 30\n6v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\nKeywords Cross-model learning \u00b7 Convolutional neural network \u00b7 Crossmodel relevance regularization \u00b7 Augmented Lagrange method\nYanbin Wu, Hongbin Zhai College of Management Science & Engineering, Hebei University of Economics and Business, Shijiazhuang 050061, China\nLi Wang State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing 100101, China\nFan Cui University Party and Government Office, China University of Mining and Technology (Beijing), Beijing 100083, China\nBaoming Dong College of Business Administration, Hebei University of Economics and Business, Shijiazhuang 050061, China Baoming Dong is the corresponding author. E-mail: baomingdong1@outlook.com\nJim Jing-Yan Wang New York University Abu Dhabi, Abu Dhabi, United Arab Emirates\n1 Introduction\n1.1 Background\nIn the machine learning community, the convolutional neural network (CNN) has been a popular method to represent data of sequence [18,19,32,16,7,30, 40,21]. Given a sequence of instances, CNN uses a sliding window to split the sequence into a group of short sub-sequences. Then it uses a bank of filters to filter the instances in these sub-sequences, and finally performs a maxpooling to the outputs of different sub-sequences. The outputs corresponding to the filters in the filter bank are used as the new representations of the sequence for the problem of classification and retrieval. Recently, CNN model has been extensively explored to represent different types of sequence data, such as image, text, video, and protein. For example, in the natural language problems, each sentence is treated as a sequence of words, and each word is represented by a word embedding vector. The sequence of word embedding vectors can be represented further by a CNN model for the problem of sematic classification [11,31,9]. Moreover, in computer vision applications, a video is also composed of a sequence of image frames, and we can also extract visual feature vector from each frame. In this case, we can also use d CNN model to represent the sequence of video for the problem of scenes classification or objective classification [15,6,8,25].\nMeanwhile, with the rapid development of internet technology, the social network is becoming more and more popular. In the social network, a great amount of data is being generated. The modalities of the data is usually of different types. For example, on a webpage of a Facebook profile, there are text, image, video, etc [37]. The diversity of the modalities of the data make the problem of classification and retrieval even more complex and difficult. To process and understand the multiple modality data, it is necessary to develop cross-model data representation, classification, and retrieval methods. However, up to now, all the existing CNN methods are limited to single modality data. When one CNN is applied to one modality data, other modality data are ignored. It is possible to learn independent CNN models for different modalities, but the sematic similarity/relevance of data points of different modalities are ignored. However, in current internet data analysis, retrieval, and understanding applications, the cross modality sematic relevance plays critical role. For example, in the multimedia retrieval application, the users usually use a textual description as query to retrieve image and/or video data. In this application, it is necessary to explore cross modality information.\n1.2 Relevant works\nTo handle the multiple modality data, many cross-model data representation methods have been proposed. For example,\n\u2013 Wang et al. [38] proposed a cross-model joint feature selection and subspace learning method to represent data of different modalities. The matrices of different modalities are projected to a common subspace by subspace learning, and in this common subspace the similarity between data points of different modalities can be measured. Moreover, the projection matrices are also regularized by the \u21132 norm penalties to select the relevant and discriminative features. To bridge the data points of different modalities, a multi-modal graph regularization term is used to regularize the data in the common space, and it can preserves the inter-modality and intra-modality similarity. The joint learning problem is solved by an iterative algorithm. \u2013 Masci et al. [28] proposed a hashing method to represent data of different modalities and used it to map the data points of these modalities to a common space, so that they can be compared in this common space. The proposed method also use the intra- and inter-modality similarity to regularize the learning of the hashing parameters. \u2013 Li et al. [20] proposed a ranking-based hashing to map the data of different modalities to a common space so that the similarity between different modalities can be measured by Hamming distance. The hashing function is not a traditional sign or threshold function, by a max-pooling function based on rank correlation measures. This hashing function is a natural probabilistic approximation. The hashing parameters are also learned by the regularization of the intra- and inter-similarity of different modalities.\n1.3 Our contributions\nAlthough there are some works on cross-model representation, but they are all limited to the subspace learning methods. For example, the method of Wang et al. [38] is a subspace learning method, and the method of Li et al. [20] is a subspace learning-based hashing method. It is still unknown how the CNN model performs for the problem of cross-model representation problem. To fill this gap, in this paper, we propose the first cross-model CNN learning method. We learn a CNN model for each modality, and then use the CNN models to map the data points of different modalities to a common space. In this common space, we further propose to learn a linear classifier to predict the class label from the CNN representations. Moreover, we also propose to use the cross-modal relevance matrix to regularize the learning of the CNN representations. If two data points of either the same modality or different modalities are relevant in the sematic concept, their CNN representations are imposed to be close to each other, and vice versa. The parameters of the classifier and the CNN models are also regularized by \u21132 norms to reduce the complexity. The objective function is a joint framework of these considered problem, and we develop an algorithm of augmented Lagrangian method (ALM) to minimize the objective function. The experiments over multi-modality data sets for retrieval problem show the proposed method outperforms better than existing cross-modal data representation methods.\nThe rest parts of this paper is organized as follows. In section 2, we introduce the proposed method of cross model CNN. In section 3, the proposed method is evaluated. In section 4, the conclusions of this paper are given.\n2 Proposed method\n2.1 Problem modeling\nIn many multiple modality data processing applications, each data point is presented as a sequence of instances, X = {x1, \u00b7 \u00b7 \u00b7 ,x|X |}, where x\u03c4 \u2208 R d is the d-dimensional input vector of the \u03c4 -th instances. For example, for the modality of text, each text is given as a sequence of words, and each work can be presented as a embedding vector. For the modality of video, each video is a sequence of frames, and we can extract a visual feature vector for each frame. To represent the data point, we first use a sliding window to explore the context information for each instance. The size of the sliding window is denoted as h, and the window covers the neighboring h instances. The window slides from the beginning of X to its end, and generates a sequence of sub-sequences of instances,\n(x1, \u00b7 \u00b7 \u00b7 ,xh), \u00b7 \u00b7 \u00b7 , (x\u03c4 , \u00b7 \u00b7 \u00b7 ,x\u03c4+h\u22121), \u00b7 \u00b7 \u00b7 , (x|X |\u2212h+1, \u00b7 \u00b7 \u00b7 ,x|X |). (1)\nIn the \u03c4 -th sequence, the \u03c4 -th instance to the \u03c4+h\u22121 is included as (x\u03c4 , \u00b7 \u00b7 \u00b7 ,x\u03c4+h\u22121). In this way, the contextual instances of each instance in the sequence are explored and used as its new presentation. We further concatenate them to a d\u00d7 h-dimensional longer vector,\ny\u03c4 = [x \u22a4 \u03c4 , \u00b7 \u00b7 \u00b7 ,x \u22a4 \u03c4+h\u22121] \u22a4 \u2208 Rdh. (2)\nIn this way, X is represented as a sequences of |X | \u2212 h+ 1 windows,\nY = {y1, \u00b7 \u00b7 \u00b7 ,y|X |\u2212h+1}. (3)\nThen we perform convolution operation to this sequence of window vectors with a filter w \u2208 Rdh. The convolution operation is composed of a sequence of pairs of filtering and nonlinear transformation actions. For a window representation, y\u03c4 , the output of the filtering action is the dot-product between w and y\u03c4 , w\n\u22a4y\u03c4 , and the online transformation function is denoted as \u03c3(\u00b7), and it is defined as a tanh function in our paper, \u03c3(x) = tanh(x). Thus the output of the filtering-nonlinear transformation action is \u03c3(w\u22a4y\u03c4 ), and the outputs of the convolution is given as follows,\n{\u03c3(w\u22a4y1), \u00b7 \u00b7 \u00b7 , \u03c3(w \u22a4y|X |\u2212h+1)}. (4)\nThen the max-pooling operation is performed to the sequence of the outputs of the convolution operation to select the maximum output,\nz = |X |\u2212h+1 max \u03c4=1 \u03c3(w\u22a4y\u03c4 ). (5)\nActually, we will use a filter bank of multiple filters for the convolution operation, denoted as W = {w1, \u00b7 \u00b7 \u00b7 ,wu}, where u is the number of filters in the filter bank. The outputs generated by the k-th filter is denoted as zk, zk = max |X |\u2212h+1 \u03c4=1 \u03c3(w \u22a4 k y\u03c4 ), and the outputs with regard to the filters in W are concatenated to form a vector of the convolutional outputs,\nz = [z1, \u00b7 \u00b7 \u00b7 , zu] \u22a4 \u2208 Ru. (6)\nThis vector is a new representation of the data point X . Moreover, to predict its binary label \u03b7 \u2208 {+1,\u22121}, we further apply a linear classification function f(\u00b7) to this representation,\n\u03b7 \u2190 f(z;v) = v\u22a4z =\nu \u2211\nk=1\nvkzk, (7)\nwhere v = [z1, \u00b7 \u00b7 \u00b7 , zu] \u2208 R u is the parameter of the classification function.\nPlease note that for different modalities, we use different filter banks, but the same classifier. In this way, the sequences of different modalities are mapped to a common CNN space, and in this space, a common classifier can be applied. To conduct the cross-model retrieval and classification, we propose to use the CNN model to map the data points of different modalities to a common CNN representation space. The filter bank for the j-th modality is denoted as Wj = {w j 1, \u00b7 \u00b7 \u00b7 ,w j u}, where w j k is the k-th filter of Wj . Please note that the sizes of the filter bank of different modalities are the same, u. The classifier parameter is denoted as v. The learning algorithm is to learn both the filter bank, Wj , for each modality and the cross-model classifier parameter, v. To this end, we use a training data set to learn the parameters. We assume we have a training set of m modalities, and the training subset of the j-th modality is denoted as Dj , j = 1, \u00b7 \u00b7 \u00b7 ,m. Dj = {X j 1 , \u00b7 \u00b7 \u00b7 ,X j |Dj| } is composed of |Dj | data points, and X j i is its i-th data point. We use the proposed CNN model to represent the data point X ji to a vector z j i . The following problems are considered in the training process.\n\u2013 To approximate the class labels from the CNN representations correctly for the training data points, we propose to minimize the loss function for all the modalities. The loss function for the classification over the i-th data point of the j-th modality, X ji , is given as the squared loss as follows,\n\u2113(\u03b7ji ,v \u22a4z j i ) = \u2016\u03b7 j i \u2212 v \u22a4z j i\u2016 2 2, (8)\nwhere \u03b7ji \u2208 {+1,\u22121} is the ground truth binary label of X j i . To approximate the ground truth label as correctly as possible, we propose to minimize the loss function over all the training data points of different modalities,\nmin\nm \u2211\nj=1\n|Dj | \u2211\ni=1\n\u2113(\u03b7ji ,v \u22a4z j i ). (9)\n\u2013 To prevent the problem of over-fitting to the training data, we propose to regularize the parameters of classifiers and the filters. The regularization term is designed as their squared \u21132 norms. The regularization terms are also minimized to seek a solution as simple as possible,\nmin\n\n\u2016v\u201622 +\nm \u2211\nj=1\nu \u2211\nk=1\n\u2016wjk\u2016 2 2\n\n . (10)\n\u2013 To bridge the information of different modalities, we propose that the CNN representations of data points of different modalities should share the same data space, and we use a sematic relevance matrix to regularize the representations in this space. The relevance between two data points X ji of\nthe j-th modality, and X j \u2032\ni\u2032 of the j \u2032-th modality is given as a binary value,\nS jj\u2032\nii\u2032 \u2208 {1, 0},\nS jj\u2032\nii\u2032 =\n\n \n \n1, if X ji and X j\u2032 i\u2032 are semantically relevant,\n\u22121, if X ji and X j\u2032 i\u2032 are semantically irrelevant, and\n0, if their relevance is unknown.\n(11)\nThis sematic similarity describes the relevance between two data points from either the same modality or different modalities. Naturally, we hope the dissimilarity between the two CNN representations of X ji and X j\u2032 i\u2032 , z j i and zj \u2032\ni\u2032 , are as small as possible if S jj\u2032 ii\u2032 = 1. For the X j i and X j\u2032 i\u2032 pair with\nS jj\u2032\nii\u2032 = \u22121, we hope their dissimilarity can be as big as possible. The dissim-\nilarity between zji and z j\u2032 i\u2032 is given as the squared \u21132 norm distance between them, \u2016zji \u2212 z j\u2032 i\u2032 \u2016 2 2, and the following minimization problem is argued,\nmin\nm \u2211\nj,j\u2032=1\n\n\n|Dj| \u2211\ni\n|Dj | \u2211\ni\u2032=1\nS jj\u2032\nii\u2032 \u2016z j i \u2212 z\nj\u2032 i\u2032 \u2016 2 2\n\n . (12)\nThis minimization is a cross-model matching regularization term. It not only requires that the data points from the same modality should be regularized by the sematic relevance, but also apply the sematic regularization to the data from different modalities.\nSummarizing the problems above, we obtain the optimization problem for the cross-model CNN learning,\nmin v,Wj,j=1,\u00b7\u00b7\u00b7 ,m\n\n\n\nm \u2211\nj=1\n|Dj | \u2211\ni=1\n\u2113(\u03b7ji ,v \u22a4z j i ) + \u03bb1\n\n\u2016v\u201622 +\nm \u2211\nj=1\nu \u2211\nk=1\n\u2016wjk\u2016 2 2\n\n\n+\u03bb2\nm \u2211\nj,j\u2032=1\n\n\n|Dj| \u2211\ni\n|Dj\u2032 | \u2211\ni\u2032=1\nS jj\u2032\nii\u2032 \u2016z j i \u2212 z\nj\u2032 i\u2032 \u2016 2 2\n\n\n\n\n\n,\n(13)\nwhere \u03bb1 and \u03bb2 are tradeoff parameters of the last two regularization terms. They control the influences of the regularization of these two terms over the final solutions. Their values are decided by linear search in our experiments. Please note that the CNN representations are functions of the filter banks according to (5). In (5), a nonlinear function and a max-pooling operation is coupled. Direct optimization of the filters by solving (13) is difficult. Instead of solving the filters directly, we explicitly introduce the CNN representation vectors to the optimization problem as independent variables, and put a constraint to impose the relation between the CNN representations and the CNN functions, [zji ]k = max |X j i |\u2212h+1 \u03c4=1 \u03c3(w j k \u22a4 [yji ]\u03c4 ), where [z j i ]k is the k-th element of the vector zji . The reformed optimization problem is given as follows,\nmin v, ( Wj ,z j\ni,i=1,\u00b7\u00b7\u00b7 ,|Dj |\n)\nj=1,\u00b7\u00b7\u00b7 ,m\n\n\n\nm \u2211\nj=1\n|Dj| \u2211\ni=1\n\u2113(\u03b7ji ,v \u22a4z j i )\n+\u03bb1\nm \u2211\nj=1\n(\n\u2016v\u201622 + u \u2211\nk=1\n\u2016wjk\u2016 2 2\n)\n+ \u03bb2\nm \u2211\nj,j\u2032=1\n\n\n|Dj | \u2211\ni\n|Dj\u2032 | \u2211\ni\u2032=1\nS jj\u2032\nii\u2032 \u2016z j i \u2212 z\nj\u2032 i\u2032 \u2016 2 2\n\n\n\n\n\n,\nsubject to [zji ]k = |X j i |\u2212h+1 max \u03c4=1 \u03c3(wjk \u22a4 [yji ]\u03c4 ), \u2200 i, j, k.\n(14) This optimization problem is a constrained minimization problem. It is interesting to note that in the objective function of this problem, we only have variables of the classifier parameter, the CNN filters, and the CNN representation vectors, the labels of the training data, and the cross-model similarity matrix as the input. The input sequences are not included in the objective function, but only appear in the constraints.\n2.2 Problem optimization\nTo solve the problem in (14), we use the ALM method [39]. The augmented Lagrangian function of the problem of (14) is given as follows,\nL(Wj ,v, z j i ;\u03b1 k ik) =\nm \u2211\nj=1\n|Dj | \u2211\ni=1\n\u2113(\u03b7ji ,v \u22a4z j i )\n+ \u03bb1\n\n\u2016v\u201622 +\nm \u2211\nj=1\nu \u2211\nk=1\n\u2016wjk\u2016 2 2\n\n+ \u03bb2\nm \u2211\nj,j\u2032=1\n\n\n|Dj | \u2211\ni\n|Dj\u2032 | \u2211\ni\u2032=1\nS jj\u2032\nii\u2032 \u2016z j i \u2212 z\nj\u2032 i\u2032 \u2016 2 2\n\n\n+\nm \u2211\nj=1\n|Dj | \u2211\ni=1\nu \u2211\nk=1\n\u03b1 j ik\n(\n[zji ]k \u2212 |X j i |\u2212h+1 max \u03c4=1 \u03c3(wjk \u22a4 [yji ]\u03c4 )\n)\n+ \u03b2\n2\nm \u2211\nj=1\n|Dj | \u2211\ni=1\nu \u2211\nk=1\n(\n[zji ]k \u2212 |X j i |\u2212h+1 max \u03c4=1 \u03c3(wjk \u22a4 [yji ]\u03c4 )\n)2\n,\n(15)\nwhere \u03b1jik is the Lagrangemultiplier of the constraint [z j i ]k = max\n|X j i |\u2212h+1\n\u03c4=1 \u03c3(w j k \u22a4 [yji ]\u03c4 ),\nand \u03b2 is its positive penalty parameter. Alternating direction method of multipliers (ADMM) [1] is used to solve the problem respect to the filters in Wj , the classifier parameters, v, the CNN representations, zji , and the Lagrange multipliers, \u03b1jik jointly. The ADMM algorithm update these variables sequentially, and the updating steps are given in Algorithm 1.\n\u2013 Algorithm 1: ADMM algorithm to update the variables of CNN model and the Lagrange multipliers. \u2013 Inputs: The sequence of instances of m modalities and the corresponding labels, Dj = {(X j 1 , \u03b7 j 1 ), \u00b7 \u00b7 \u00b7 , (X j|Dj|, \u03b7 j |Dj | )}, j = 1, \u00b7 \u00b7 \u00b7 ,m. The sematic\nsimilarity matrix of cross-modalities, S. The tradeoff parameters \u03bb1 and \u03bb2.\n\u2013 Initialize v0, W0j , (z j i ) 0, and (\u03b1jik) 0, for j = 1, \u00b7 \u00b7 \u00b7 ,m, i = 1, \u00b7 \u00b7 \u00b7 , |Dj |, and\nk = 1, \u00b7 \u00b7 \u00b7 , u. \u2013 While not converged do\n1. vt+1 = argmin v L(Wtj ,v, (z j i ) t; (\u03b1kik) t), j = 1, \u00b7 \u00b7 \u00b7 ,m. 2. (zji ) t+1 = argmin z j i L(Wtj ,v t+1, z j i ; (\u03b1 k ik) t), j = 1, \u00b7 \u00b7 \u00b7 ,m, i = 1, \u00b7 \u00b7 \u00b7 , |Dj |. 3. Wt+1j = argminWjL(Wj ,v t+1, (zji ) t+1; (\u03b1kik) t), j = 1, \u00b7 \u00b7 \u00b7 ,m. 4. (\u03b1jik) t+1 = (\u03b1jik) t + \u03b2 ( [zji ] t+1 k \u2212max |X j i |\u2212h+1 \u03c4=1 \u03c3((w t+1 k ) \u22a4[yji ]\u03c4 ) )"}, {"heading": "2.2.1 Update Step for v", "text": "The first sub-optimization problem is the minimization of L(Wtj ,v, (z j i ) t; (\u03b1kik) t) with respect to v. It has a quadratic form with regard to v. The Lagrange function is reduced to the following function by removing the terms irrelevant to v as follows,\nh(v) =\nm \u2211\nj=1\n|Dj| \u2211\ni=1\n\u2113(\u03b7ji ,v \u22a4z j i ) + \u03bb1\u2016v\u2016 2 2\n=\nm \u2211\nj=1\n|Dj| \u2211\ni=1\n\u2016\u03b7ji \u2212 v \u22a4z j i\u2016 2 2 + \u03bb1\u2016v\u2016 2 2.\n(16)\nWe solve it by setting the derivative of h(v) with regard to v to zero,\n\u2202h(v)\n\u2202v = 2\nm \u2211\nj=1\n|Dj| \u2211\ni=1\nz j i (z j i \u22a4 v\u2212 \u03b7ji ) + 2\u03bb1v = 0,\nv\u2217 =\n\n\nm \u2211\nj=1\n|Dj | \u2211\ni=1\nz j iz j i \u22a4 + \u03bb1I\n\n\n\u22121 \n\nm \u2211\nj=1\n|Dj | \u2211\ni=1\nz j i\u03b7 j i\n\n .\n(17)\n2.2.2 Update Step for z j i\nThe second sub-optimization problem is the minimization of the Lagrange function with regard to zji . We remove the irrelevant terms with regard to z j i and have the objective function as follows,\ng(zji ) = m \u2211\nj=1\n|Dj| \u2211\ni=1\n\u2113(\u03b7ji ,v \u22a4z j i ) + \u03bb2\nm \u2211\nj=1\nm \u2211\nj\u2032=1\n\n\n|Dj| \u2211\ni\n|Dj\u2032 | \u2211\ni\u2032=1\nS jj\u2032\nii\u2032 \u2016z j i \u2212 z\nj\u2032 i\u2032 \u2016 2 2\n\n\n+\nm \u2211\nj=1\n|Dj| \u2211\ni=1\nu \u2211\nk=1\n\u03b1 j ik\n(\n[zji ]k \u2212 |X j i |\u2212h+1 max \u03c4=1 \u03c3(wjk \u22a4 [yji ]\u03c4 )\n)\n+ \u03b2\n2\nm \u2211\nj=1\n|Dj| \u2211\ni=1\nu \u2211\nk=1\n(\n[zji ]k \u2212 |X j i |\u2212h+1 max \u03c4=1 \u03c3(wjk \u22a4 [yji ]\u03c4 )\n)2\n=\nm \u2211\nj=1\n|Dj| \u2211\ni=1\n\u2225 \u2225 \u2225\u03b7 j i \u2212 v \u22a4z j i \u2225 \u2225 \u2225 2\n2\n+ \u03bb2\nm \u2211\nj=1\nm \u2211\nj\u2032=1\n\n\n|Dj | \u2211\ni\n|Dj\u2032 | \u2211\ni\u2032=1\nS jj\u2032\nii\u2032\n\u2225 \u2225 \u2225z j i \u2212 z j\u2032 i\u2032 \u2225 \u2225 \u2225 2\n2\n\n\n+\nm \u2211\nj=1\n|Dj| \u2211\ni=1\n\u03b1 j i\n\u22a4 (\nz j i \u2212 z j i\n)\n+ \u03b2\n2\nm \u2211\nj=1\n|Dj | \u2211\ni=1\n\u2225 \u2225 \u2225z j i \u2212 z j i \u2225 \u2225 \u2225 2\n2\n.\n(18)\nwhere \u03b1ji = [\u03b1 j i1, \u00b7 \u00b7 \u00b7 , \u03b1 j iu] \u22a4 \u2208 Ru, and\nz j i =\n[\n|X j i |\u2212h+1 max \u03c4=1 \u03c3(w\u22a41 [y j i ]\u03c4 ), \u00b7 \u00b7 \u00b7 , |X j i |\u2212h+1 max \u03c4=1 \u03c3(w\u22a4u [y j i ]\u03c4 )\n]\u22a4\n\u2208 Ru. (19)\nWe defined a matrix by combining all the CNN representation vectors as the columns,\nZ = [\nz11, \u00b7 \u00b7 \u00b7 , z 1 |D1| , \u00b7 \u00b7 \u00b7 , zm1 , \u00b7 \u00b7 \u00b7 , z m |Dm|\n]\n\u2208 Ru\u00d7\u03b8 (20)\nwhere its ( \u2211j\u22121\nj\u2032=1 |Dj\u2032 |+ i)-th column is the CNN representation vector of the\ni-th data point of the j-th modality, zji , and \u03b8 = \u2211|Dj|\nj=1 is the total number of data points from all the data sets of different modalities. Similarly, we also define a label vector, a baseline CNN representation matrix, and a vector of Lagrange multipliers,\nZ = [\nz11, \u00b7 \u00b7 \u00b7 , z 1 |D1| , \u00b7 \u00b7 \u00b7 , zm1 , \u00b7 \u00b7 \u00b7 , z m |Dm|\n]\n\u2208 Ru\u00d7\u03b8\n\u03b7 = [\u03b711 , \u00b7 \u00b7 \u00b7 , \u03b7 1 |D1| , \u00b7 \u00b7 \u00b7 , \u03b7m1 , \u00b7 \u00b7 \u00b7 , \u03b7 m |Dm| ] \u2208 {+1,\u22121}\u03b8, and A = [\u03b111, \u00b7 \u00b7 \u00b7 ,\u03b1 1 |D1| , \u00b7 \u00b7 \u00b7 ,\u03b1m1 , \u00b7 \u00b7 \u00b7 ,\u03b1 m |Dm| ] \u2208 Ru\u00d7\u03b8.\n(21)\nWe further define a matrix of \u03b8\u00d7 \u03b8 of relevance matrix for all the data points of different modalities,\nS =\n\n            \nS1111 \u00b7 \u00b7 \u00b7 S 11 1|D1| S1m11 \u00b7 \u00b7 \u00b7 S 1m 1|Dm|\n... . . .\n... \u00b7 \u00b7 \u00b7 ...\n. . . ...\nS11|D1|1 \u00b7 \u00b7 \u00b7 S 11 |D1||D1| S1m|D1|1 \u00b7 \u00b7 \u00b7 S 1m |D1||Dm|\n... . . . ... Sm111 \u00b7 \u00b7 \u00b7 S m1 1|D1| Smm11 \u00b7 \u00b7 \u00b7 S mm 1|Dm|\n... . . .\n... \u00b7 \u00b7 \u00b7 ...\n. . . ...\nSm1|Dm|1 \u00b7 \u00b7 \u00b7 S m1 |Dm||D1|\nSmm|Dm|1 \u00b7 \u00b7 \u00b7 S mm |Dm||Dm|\n\n             \u2208 R\u03b8\u00d7\u03b8. (22)\nThe objective function of (18) can be rewritten by matrix form as follows,\ng(Z) = \u2225 \u2225\u03b7 \u2212 v\u22a4Z \u2225 \u2225 2\n2 + 2\u03bb2Tr\n( Z ( diag(1\u22a4S)\u2212 S ) Z\u22a4 )\n+ Tr ( A\u22a4 ( Z \u2212 Z )) + \u03b2\n2\n\u2225 \u2225Z \u2212 Z \u2225 \u2225 2\n2\n= [ \u03b7 \u22a4 \u03b7 \u2212 2Tr(Z\u22a4v\u03b7) + Tr(Z\u22a4vv\u22a4Z) ]\n+ 2\u03bb2Tr ( Z ( diag(1\u22a4S)\u2212 S ) Z\u22a4 ) + Tr ( ( Z \u2212 Z )\u22a4 A )\n+ \u03b2\n2\n[\nTr(Z\u22a4Z)\u2212 2Tr(Z\u22a4Z) + Tr(Z \u22a4 Z)\n]\n= Tr(Z\u22a4 ( vv\u22a4 + I ) Z) + 2\u03bb2Tr ( Z ( diag(1\u22a4S)\u2212 S ) Z\u22a4 ) \u2212 Tr ( Z\u22a4 ( 2v\u03b7 \u2212 A+ \u03b2Z )) + constant,\n(23)\nwhere 1 \u2208 R\u03b8 is a vector of ones of \u03b8 dimensions, diag(x) is a diagonal matrix with the diagonal elements as the elements of vector x, and constant is a constant irrelevant to Z. To minimize the Lagrange function with regard to\nZ, we use the method of gradient descent. The matrix Z is descended to the direction of gradient. The gradient function of g(Z) is calculated as follows,\n\u2207g(Z) = 2 ( vv\u22a4 + I ) Z + 4\u03bb2Z ( diag(1\u22a4S)\u2212 S ) \u2212 ( 2v\u03b7 \u2212A+ \u03b2Z ) .\n(24) The updating rule is as follows,\nZnew \u2190 Zold \u2212 \u03c8\u2207g(Zold), (25)\nwhere \u03c8 is the descent step size, and \u03c8 = 1 t for the t-th iteration. The updating process is repeated until convergence."}, {"heading": "2.2.3 Update step for Wj", "text": "The third sub-optimization problem is the minimization of the Lagrange function with regard to the filters of CNN models. To update the filters, we only consider the terms of the Lagrange function relevant to the filters, and the following reduced Lagrange function is obtained,\nJ(Wj) = \u03bb1\n\n\nm \u2211\nj=1\nu \u2211\nk=1\n\u2016wjk\u2016 2 2\n\n\n+\nm \u2211\nj=1\n|Dj | \u2211\ni=1\nu \u2211\nk=1\n\u03b1 j ik\n(\n[zji ]k \u2212 |X j i |\u2212h+1 max \u03c4=1 \u03c3(wjk \u22a4 [yji ]\u03c4 )\n)\n+ \u03b2\n2\nm \u2211\nj=1\n|Dj | \u2211\ni=1\nu \u2211\nk=1\n(\n[zji ]k \u2212 |X j i |\u2212h+1 max \u03c4=1 \u03c3(wjk \u22a4 [yji ]\u03c4 )\n)2\n.\n(26)\nSince the obtained function is a combination of functions of independent filters, we propose to update the filter sequentially and independently. When one filter is considered, the others are fixed. The individual objective containing one single filter wjk is given as follows,\nJ(wjk) = \u03bb1\u2016w j k\u2016 2 2 +\n|Dj| \u2211\ni=1\n\u03b1 j ik\n(\n[zji ]k \u2212 |X j i |\u2212h+1 max \u03c4=1 \u03c3(wjk \u22a4 [yji ]\u03c4 )\n)\n+ \u03b2\n2\n|Dj | \u2211\ni=1\n(\n[zji ]k \u2212 |X j i |\u2212h+1 max \u03c4=1 \u03c3(wjk \u22a4 [yji ]\u03c4 )\n)2\n.\n(27)\nDirectly optimization of this problem is difficult, because it is composed of a maximization problem to seek the output of the CNN model. Instead of optimizing it directly, we develop an iterative algorithm to solve the problem alternately. We define an indicator to indicate which window gives the maximum response of \u03c3(wjk \u22a4 [yji ]\u03c4 ) among \u03c4 = 1, \u00b7 \u00b7 \u00b7 , |X j i | \u2212 h+ 1,\n\u03c4 j ik = argmax\n|X j i |\u2212h+1\n\u03c4=1 \u03c3(w j k \u22a4 [yji ]\u03c4 ). (28)\nIn each iteration, we first update the indicators using the previous filter, and then fix indicators to update the filter. When the indicators are fixed, the problem in (27) is transformed to\nJ(wjk) = \u03bb1\u2016w j k\u2016 2 2 +\n|Dj| \u2211\ni=1\n\u03b1 j ik\n( [zji ]k \u2212 \u03c3 ( w j k \u22a4 [yji ]\u03c4 j\nik\n))\n+ \u03b2\n2\n|Dj | \u2211\ni=1\n( [zji ]k \u2212 \u03c3 ( w j k \u22a4 [yji ]\u03c4 j\nik\n))2\n.\n(29)\nTo minimize this objective, we also use the gradient descent method, and the gradient function is given as follows,\n\u2207J(wjk) = 2\u03bb1w j k \u2212\n|Dj| \u2211\ni=1\n\u03b1 j ik\u2207\u03c3\n(\nw j k \u22a4 [yji ]\u03c4 j\nik\n)\n[yji ]\u03c4 j ik\n\u2212 \u03b2\n|Dj| \u2211\ni=1\n( [zji ]k \u2212 \u03c3 ( w j k \u22a4 [yji ]\u03c4 j\nik\n)) \u2207\u03c3 (\nw j k \u22a4 [yji ]\u03c4 j\nik\n)\n[yji ]\u03c4 j ik .\n(30)\nwhere \u2207\u03c3(\u00b7) is the gradient function of the nonlinear function \u03c3(\u00b7). The updating rule for wjk is as follows,\nw j k new = wjk old \u2212 \u03c8\u2207J(wjk old ), (31)\nwhere \u03c8 is the descent step size, and \u03c8 = 1 t for the t-th iteration."}, {"heading": "3 Experiments", "text": "In this section, we evaluate the proposed cross-model CNN method (CMCNN) experimentally. It is compare to some other cross-model representation methods.\n3.1 Benchmark data sets\nIn the experiments, we use two multiple modality data sets to evaluate the proposed method. The data sets are described as follows.\nWiki. The first data set we used is the Wiki data set of two modalities, which are image and text. There are in total 2,866 documents in this data set. For each document, an image and a text is contained. To represent each image, we split the image to small patches and use a window to extract features from neighboring patches. The visual features of SIFT are extracted from each patch. The text is represented by the embeddings of the words. The number\nof classes of this data set is 10. A text/image is considered to be relevant to another text/image if they are in the same class.\nNUS-WIDE. The second data set we use is the NUS-WIDE data of two modalities, which are also image and text. In this data set, there are 72,219 documenters of image-text pairs, belonging to 21 different classes. Similarly, the images are also presented by sequences of image patches, and the text is treated as sequence of words.\n3.2 Experimental setting\nTo conduct the experiment of retrieval, we use the ten-fold cross-validation. Each data set is split to ten folds. Each fold is used as a query set, while the rest nine folds are used as training set and database set. The proposed method is applied to the training set to train the parameters, and then the parameters are used to represent both the data points of training and query set to CNN representations. The CNN representations of queries and database data points are compared by \u21132 norm distance to rank the database data points. The top ranked data points are returned as the retrieval results.\nThe retrieval performance is measured by the performance of Precision at top k (Prec@k), mean Average Precision (mAP), and the Break Even Point of Recall-Precision curve (BEPRP).The definition of these performance measures are given as follows.\nPrec@k = #{database items ranked at top k & relevant to the query}\nk ,\nReca@k = #{database items ranked at top k & relevant to thequery}\n#{database items relevant to thequery} ,\nmAP = 1\nQ\nQ \u2211\nq=1\n1\nD\nD \u2211\nk=1\n(Prec@k(q)\u00d7 \u03b4@k(q)) , and\nBEPRP = Prec@k\u2217, with Prec@k\u2217 = Reca@k\u2217, k\u2217 \u2208 {1, \u00b7 \u00b7 \u00b7 , D}, (32) where Reca@k is the Recall at top k, Q is the total number of queries, D is the number of database items. Prec@k(q) is the Prec@k of the q-th query, and \u03b4@k(q) = 1 if the k-th database item is relevant to the q-th query, and 0 otherwise.\n3.3 Experimental results\nWe compare the proposed method to the stat-of-the-art cross-model representation methods, including the cross-model Joint Feature Selection and Subspace Learning (JFSSL) [38], the cross-modal Linear Subspace Ranking Hashing (LSRH) [20], and the Multimodal Similarity-Preserving Hashing (MSPH)\n[28]. The comparison results are reported in Figure 1. According to the results reported in the figure, it is clearly that the proposed method CMCNN outperforms the compared cross-model representation methods significantly over all the three benchmark data sets. It is also noted that the outperforming of the method is also significant even measured by different performance measures. This is a further evidence of the advantage of the proposed method. Please note that because the compare methods, JFSSL, LSRH, and MSPH cannot take sequence data as input directly, thus we use vector quantization (VQ) method to transfer the sequences to histogram vectors, which are used as inputs of these methods. Thus the performance of the compared methods heavily relies on the quality of the histograms. However, the proposed CMCNN is a convolutional method which takes sequence data as input naturally. It has been shown that CNN is a good model to represent sequence data. This is a possible reason for the good performance of CMCNN."}, {"heading": "4 Conclusions", "text": "In this paper, we propose to learn convolutional models to solve the problem of cross-model data representation and retrieval problems. We propose to map the data of different modalities to a common space, and in this commons space, classification and retrieval can be performed. Also in the common space, data of different modalities can be compared, and we use a cross-model relevance regularization to compare the dissimilarities of the data points. To map the sequences of different modalities, we propose to use the CNN model with multiple filters and max-pooling operations. Different modalities have different filter banks but can map the sequences to a common space. A linear classifier is\napplied to the outputs of the cross-model CNN to predict the class labels, and we also regularize the outputs of the CNN by a cross-model relevance term. The experiments over the benchmark data sets show its advantage over the existing cross-model data representation. In the future, we will also consider using the proposed method to other applications, such as integrated circuit design [41,42], software engineering [14,13,12], network measurement [3,4,2], commuter vision [44,43,5,35,27], medical imaging [23,29,17,34,22,33,10], etc. We will also consider to use some other loss function to learn the parameters of the CNN and the classifier to optimize the multivariate performance measures [36,24,21,26]."}, {"heading": "Acknowledgements", "text": "This work was supported by the Natural Science Foundation of Hebei Province (D2015207008), Talent Training Project of Hebei Province (A201400215) and Young Prominent Talent Project of Hebei Province Higher School (BJ2014021).\nStatement of conflicts of interests\nThe authors of this manuscript state that there is no conflicts of interests between this manuscript and other published works."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Abstract A novel data representation method of convolutional neural network (CNN) is proposed in this paper to represent data of different modalities. We learn a CNN model for the data of each modality to map the data of different modalities to a common space, and regularize the new representations in the common space by a cross-model relevance matrix. We further impose that the class label of data points can also be predicted from the CNN representations in the common space. The learning problem is modeled as a minimization problem, which is solved by an augmented Lagrange method (ALM) with updating rules of Alternating direction method of multipliers (ADMM). The experiments over benchmark of sequence data of multiple modalities show its advantage.", "creator": "LaTeX with hyperref package"}}}