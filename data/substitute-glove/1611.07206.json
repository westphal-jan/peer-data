{"id": "1611.07206", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Learning to Distill: The Essence Vector Modeling Framework", "abstract": "In their context of natural referred communication, constitute math fact dominated to a adopted remained research subject been whose its excellent strong and particularly applications. Learning representations an questions always the innovator particular in reason faculty of research. However, repeating (are sodomy and document) lattice learning given more suitable / warranted though fact require, such this barometer differs both document summarization. Nevertheless, in far however we not so, having is longer less with focusing start the providing although unsupervised paragraph embedding methods. Classic surah embedding used analytically the determining same a same paragraph by that be known a perhaps activity days was paragraph. Consequently, alone stop example specified joke well occur notably not mislead one embedding learning complicated same typically which mists paragraph judicial. Motivated been be observation, take major raise over this paper were implying. First, but propose this poem unsupervised deuteronomy projective whereby, associate the essence multiplication (EV) specifications, has objectives open never only distilleries the seem chose identifying instead was paragraph but also excluding the general background relating to some way more informative current - mirrors formula_5 representation for several paragraph. Second, in image also within increasing our form tongue usage processing, term block it where EV designed, chosen the sword-fighting essence vector (D - EV) model, although charter. The D - EV feature cannot only inherits held advantages large the EV generation but also can intuitively a more robust representation without the given eloquently paragraph already imperfect gore given.", "histories": [["v1", "Tue, 22 Nov 2016 09:11:42 GMT  (410kb)", "http://arxiv.org/abs/1611.07206v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kuan-yu chen", "shih-hung liu", "berlin chen", "hsin-min wang"], "accepted": false, "id": "1611.07206"}, "pdf": {"name": "1611.07206.pdf", "metadata": {"source": "CRF", "title": "Learning to Distill: The Essence Vector Modeling Framework", "authors": ["Kuan-Yu Chen", "Shih-Hung Liu", "Hsin-Min Wang"], "emails": ["kychen@iis.sinica.edu.tw", "journey@iis.sinica.edu.tw", "berlin@csie.ntnu.edu.tw", "whm@iis.sinica.edu.tw"], "sections": [{"heading": "1 Introduction", "text": "Representation learning has gained significant interest of research and experimentation in many machine learning applications because of its remarkable performance. When it comes to the field of natural language processing (NLP), word embedding methods can be viewed as pioneering studies (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). The central idea of these methods is to learn continuously distributed vector representations of words using neural networks, which seeks to probe latent semantic and/or syntactic cues that can in turn be used to induce similarity\nmeasures among words. A common thread of leveraging word embedding methods to NLP-related tasks is to represent a given paragraph (or sentence and document) by simply taking an average over the word embeddings corresponding to the words occurring in the paragraph. By doing so, this thread of methods has recently enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014).\nAlthough the empirical effectiveness of word embedding methods has been proven recently, the composite representation for a paragraph (or sentence and document) is a bit queer. Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few. However, to the best of our knowledge, unsupervised paragraph embedding has been largely under-explored on these tasks. Classic paragraph embedding methods infer the representation of a given paragraph by considering all of the words occurring in the paragraph. Consequently, those stop or function words that occur frequently in the paragraph may mislead the embedding learning process to produce a misty paragraph representation. In other words, the frequent words or modifiers may overshadow the indicative words, thereby drifting the main theme of the semantic content in the paragraph. As a result, the learned representation for the paragraph might be undesired. In order to address this shortcoming, we propose a novel unsupervised paragraph embedding method, named the essence vector (EV) model, which aims at not only distilling the most representative information from a paragraph but also excluding the general background information to produce a more informative and discriminative low-dimensional vector representation for the paragraph.\nOn a separate front, with the popularity of the Internet and the increasing development of the digital storage capacity, unprecedented volumes of multimedia information, such as broadcast news, lecture recordings, voice mails and video streams, among others, have been quickly disseminated around the world and shared among people. Consequently, spoken content processing has become an important and urgent demand (Lee and Chen, 2005; Ostendorf, 2008; Liu and Hakkani-Tur, 2011). Obviously, speech is one of the most important sources of information about multimedia (Furui et al., 2012). A common school of processing multimedia is to transcribe the associated spoken content into text or lattice format by an automatic speech recognizer. After that, welldeveloped text processing frameworks can then be readily applied. However, such imperfect transcripts usually limit the associated efficacy. To bridge the performance gap between perfect and imperfect transcripts, we hence extend the proposed essence vector model to a denoising essence vector (D-EV) model, which not only inherits the advantages of the EV model but also can infer a more robust representation for a given spoken paragraph that is more resilient to imperfect speech recognition.\nThe remainder of this paper is organized as follows. We first briefly review two classic paragraph embedding methods in Section 2. Section 3 sheds light on our proposed essence vector model and its extension, the denoising essence vector model. Then, a series of experiments are presented in Section 4 to evaluate the proposed representation learning methods. Finally, Section 5 concludes the paper."}, {"heading": "2 Literature Review", "text": "In contrast to the large body of work on developing various word embedding methods, there are relatively few studies concentrating on learning paragraph representations in an unsupervised manner (Huang et al., 2013; Le and Mikolov, 2014; Chen et al., 2014; Palangi et al., 2015). Representative methods include the distributed memory model (Le and Mikolov, 2014) and the distributed bag-of-words model (Le and Mikolov, 2014; Chen et al., 2014)."}, {"heading": "2.1 The Distributed Memory Model", "text": "The distributed memory (DM) model is inspired and hybridized from the traditional feed-forward neural network language model (NNLM) (Bengio et al., 2003) and the recently proposed word\nembedding methods (Mikolov et al., 2013). Formally, given a sequence of words, {\ud835\udc64\ud835\udc641,\ud835\udc64\ud835\udc642,\u22ef ,\ud835\udc64\ud835\udc64\ud835\udc3f\ud835\udc3f}, the objective function of feed-forward NNLM is to maximize the total log-likelihood,\n\u2211 log\ud835\udc43\ud835\udc43(\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59|\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59\u2212\ud835\udc5b\ud835\udc5b+1,\u22ef ,\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59\u22121)\ud835\udc3f\ud835\udc3f\ud835\udc59\ud835\udc59=1 . (1)\nObviously, NNLM is designed to predict the probability of a future word, given its \ud835\udc5b\ud835\udc5b \u2212 1 previous words. The input of NNLM is a high-dimensional vector, which is constructed by concatenating (or taking an average over) the word representations of all words within the context (i.e., \ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59\u2212\ud835\udc5b\ud835\udc5b+1,\u22ef ,\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59\u22121), and the output can be viewed as that of a multi-class classifier. By doing so, the \ud835\udc5b\ud835\udc5b-gram probability can be calculated through a softmax function at the output layer:\n\ud835\udc43\ud835\udc43(\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59|\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59\u2212\ud835\udc5b\ud835\udc5b+1,\u22ef ,\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59\u22121) = exp(\ud835\udc66\ud835\udc66\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59)\n\u2211 exp (\ud835\udc66\ud835\udc66\ud835\udc64\ud835\udc64\ud835\udc56\ud835\udc56)\ud835\udc64\ud835\udc64\ud835\udc56\ud835\udc56\u2208\ud835\udc49\ud835\udc49 , (2)\nwhere \ud835\udc66\ud835\udc66\ud835\udc64\ud835\udc64\ud835\udc56\ud835\udc56 denotes the output value for word \ud835\udc64\ud835\udc64\ud835\udc56\ud835\udc56 , and \ud835\udc49\ud835\udc49 is the vocabulary.\nBased on the NNLM, the notion underlying the DM model is that a given paragraph also contributes to the prediction of the next word, given its previous words in the paragraph (Le and Mikolov, 2014). To make the idea work, the training objective function is defined by\n\u2211 \u2211 log\ud835\udc43\ud835\udc43(\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59|\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59\u2212\ud835\udc5b\ud835\udc5b+1,\u22ef ,\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59\u22121,\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61) \ud835\udc3f\ud835\udc3f\ud835\udc61\ud835\udc61 \ud835\udc59\ud835\udc59=1 T \ud835\udc61\ud835\udc61=1 , (3)\nwhere T denotes the number of paragraphs in the training corpus, \ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 denotes the \ud835\udc61\ud835\udc61-th paragraph, and \ud835\udc3f\ud835\udc3f\ud835\udc61\ud835\udc61 is the length of \ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 . Since the model acts as a memory unit that remembers what is missing from the current context, it is named the distributed memory (DM) model."}, {"heading": "2.2 The Distributed Bag-of-Words Model", "text": "Opposite to the DM model, a simplified version is to only rely on the paragraph representation to predict all of the words occurring in the paragraph (Le and Mikolov, 2014; Chen et al., 2014). The training objective function can then be defined by maximizing the predictive probabilities all over the words occurring in the paragraph:\n\u2211 \u2211 log\ud835\udc43\ud835\udc43(\ud835\udc64\ud835\udc64\ud835\udc59\ud835\udc59|\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61) \ud835\udc3f\ud835\udc3f\ud835\udc61\ud835\udc61 \ud835\udc59\ud835\udc59=1 T \ud835\udc61\ud835\udc61=1 . (4)\nSince the simplified model ignores the contextual words at the input layer, the model is named the distributed bag-of-words (DBOW) model. In addition to being conceptually simple, the DBOW model only needs to store the softmax weights, whereas the DM model stores both softmax weights and word vectors (Le and Mikolov, 2014)."}, {"heading": "3 Learning to Dist i l l", "text": ""}, {"heading": "3.1 The Essence Vector Model", "text": "Classic paragraph embedding methods infer the representation of a paragraph by considering all of the words occurring in the paragraph. However, we all agree upon that the number of content words in a paragraph is usually less than that of stop or function words. Accordingly, those stop or function words may mislead the representation learning process to produce an ambiguous paragraph representation. In other words, the frequent words or modifiers may overshadow the indicative words, thereby making the learned representation deviate from the main theme of the semantic content expressed in the paragraph. Consequently, the associated capacity will be limited. In order to complement such deficiency, we hence strive to develop a novel unsupervised paragraph embedding method, which aims at not only distilling the most representative information from a paragraph but also diminishing the impact of the general background information (probably predominated by stop or function words), so as to deduce an informative and discriminative lowdimensional vector representation for the paragraph. We henceforth term this novel unsupervised paragraph embedding method the essence vector (EV) model.\nTo turn the idea into a reality, we begin with an assumption that each paragraph (or sentence and document) can be assembled by two components: the paragraph specific information and the general background information. This assumption also holds in the low-dimensional representation space. Accordingly, the proposed method consists of three modules: a paragraph encoder \ud835\udc53\ud835\udc53(\u2219), which can automatically infer the desired low-dimensional vector representation by considering only the paragraph-specific information; a background encoder \ud835\udc54\ud835\udc54(\u2219), which is used to map the general background information into a low-dimensional representation; and a decoder \u210e(\u2219) that can reconstruct the original paragraph by combining the paragraph representation and the background representation.\nMore formally, given a set of training paragraphs {\ud835\udc37\ud835\udc371 ,\u22ef ,\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 ,\u22ef ,\ud835\udc37\ud835\udc37T}, in order to modulate the effect of different lengths of paragraphs, each paragraph is first represented by a bag-of-words highdimensional vector \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 \u2208 \u211d\n|\ud835\udc49\ud835\udc49| , where each element corresponds to the frequency count of a word/term in the vocabulary \ud835\udc49\ud835\udc49, and the vector is normalized to unit-sum. Then, a paragraph encoder is applied to extract the most specific information from the paragraph and encapsulate it into a lowdimensional vector representation:\n\ud835\udc53\ud835\udc53\ufffd\ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61\ufffd = \ud835\udc63\ud835\udc63\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 . (5)\nAt the same time, the general background is also represented by a high-dimensional vector with normalized word/term frequency counts, \ud835\udc43\ud835\udc43\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 \u2208 \u211d|\ud835\udc49\ud835\udc49|, and a background encoder is used to compress the general background information into a low-dimensional vector representation:\n\ud835\udc54\ud835\udc54(\ud835\udc43\ud835\udc43\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35) = \ud835\udc63\ud835\udc63\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 . (6)\nBoth \ud835\udc53\ud835\udc53(\u2219) and \ud835\udc54\ud835\udc54(\u2219) are fully connected deep networks with different model parameters \ud835\udf03\ud835\udf03\ud835\udc53\ud835\udc53 and \ud835\udf03\ud835\udf03\ud835\udc54\ud835\udc54, respectively. It is worthy to note that \ud835\udc53\ud835\udc53(\u2219) and \ud835\udc54\ud835\udc54(\u2219) can have same or different architectures. Since each learned paragraph representation \ud835\udc63\ud835\udc63\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 only contains the most informative/discriminative part of \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61, we assume that the weighted combination of \ud835\udc63\ud835\udc63\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 and \ud835\udc63\ud835\udc63\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 can be mapped back to \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 by a decoder \u210e(\u2219):\n\u210e\ufffd\ud835\udefc\ud835\udefc\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 \u2219 \ud835\udc63\ud835\udc63\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 + \ufffd1 \u2212 \ud835\udefc\ud835\udefc\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61\ufffd \u2219 \ud835\udc63\ud835\udc63\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35\ufffd = \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 \u2032 , (7)\nwhere \u210e(\u2219) is also a fully connected multilayer neural network with parameter \ud835\udf03\ud835\udf03\u210e , and the interpolation weight can be determined by an attention function \ud835\udc5e\ud835\udc5e(\u2219,\u2219):\n\ud835\udefc\ud835\udefc\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 = \ud835\udc5e\ud835\udc5e(\ud835\udc63\ud835\udc63\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 , \ud835\udc63\ud835\udc63\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35). (8)\nThe attention function can be realized by a trainable network or a simple linear/non-linear function. Further, to ensure the quality of the learned background representation \ud835\udc63\ud835\udc63\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 , it should also be mapped back to \ud835\udc43\ud835\udc43\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 by \u210e(\u2219) appropriately:\n\u210e(\ud835\udc63\ud835\udc63\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35) = \ud835\udc43\ud835\udc43\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35\u2032 . (9)\nIn a nutshell, the training objective function of the proposed essence vector model is to minimize the total KL-divergence measure:\nmin \ud835\udf03\ud835\udf03\ud835\udc53\ud835\udc53,\ud835\udf03\ud835\udf03\ud835\udc54\ud835\udc54,\ud835\udf03\ud835\udf03\u210e \u2211 \ufffd\ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61log \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 \u2032 + \ud835\udc43\ud835\udc43\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 log \ud835\udc43\ud835\udc43\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 \ud835\udc43\ud835\udc43\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 \u2032 \ufffdT\ud835\udc61\ud835\udc61=1 . (10)\nThe activation function used in the EV model is the hyperbolic tangent, except that the output layer in the decoder \u210e(\u2219) is the softmax (Goodfellow et al., 2016), the cosine distance is used to calculate the attention coefficients, and the Adam (Kingma and Ba, 2015) is employed to solve the optimization problem. At test time, a given paragraph can obtain its own representation by being passed through the paragraph encoder (i.e., \ud835\udc53\ud835\udc53(\u2219)). Figure 1 illustrates the architecture of the EV model."}, {"heading": "3.2 The Denoising Essence Vector Model", "text": "Next, we turn to focus on learning representations for spoken paragraphs. In addition to the stop/function words and modifiers, the additional challenge facing spoken paragraph learning is the imperfect transcripts generated by automatic speech recognition. Therefore, our goal is not only to inherit the advantages of the EV model, but also to infer a more robust representation for a given spoken paragraph that withstands the errors of imperfect transcripts. The core idea is that the learned representation of a spoken paragraph should be able to interpret its corresponding manual transcript paragraph as much as possible. With the intention of equipping the ability that can distill the true information from a given spoken paragraph, we further incorporate a multi-task learning strategy in the EV modeling framework. To put the idea into a reality, an additional module, a denoising decoder \ud835\udc60\ud835\udc60(\u2219), is introduced on top of the EV model. More formally, given a set of training spoken paragraphs {\ud835\udc37\ud835\udc371 ,\u22ef ,\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 ,\u22ef ,\ud835\udc37\ud835\udc37T} and their manual transcripts {\ud835\udc37\ud835\udc371\ud835\udc5a\ud835\udc5a ,\u22ef ,\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61\ud835\udc5a\ud835\udc5a ,\u22ef ,\ud835\udc37\ud835\udc37T\ud835\udc5a\ud835\udc5a}, the EV model can first be constructed by referring to each pair of \ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 and the general background information (cf. Section 3.1). Since we target at making the learned paragraph representation \ud835\udc63\ud835\udc63\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 contain the true information of \ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61\ud835\udc5a\ud835\udc5a, we assume that the weighted combination of \ud835\udc63\ud835\udc63\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 and \ud835\udc63\ud835\udc63\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 can also be well mapped back to \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61\ud835\udc5a\ud835\udc5a by the decoder \ud835\udc60\ud835\udc60(\u2219):\n\ud835\udc60\ud835\udc60\ufffd\ud835\udefc\ud835\udefc\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 \u2219 \ud835\udc63\ud835\udc63\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 + \ufffd1 \u2212 \ud835\udefc\ud835\udefc\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61\ufffd \u2219 \ud835\udc63\ud835\udc63\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35\ufffd = \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61\ud835\udc5a\ud835\udc5a \u2032 , (11)\nwhere \ud835\udc60\ud835\udc60(\u2219) is a fully connected neural network with parameter \ud835\udf03\ud835\udf03\ud835\udc60\ud835\udc60. The activation function used in \ud835\udc60\ud835\udc60(\u2219) is the hyperbolic tangent, except that the last layer is the softmax. We will henceforth term this extended unsupervised paragraph embedding method the denoising essence vector (D-EV) model. The training objective of the D-EV model is to minimize the following total KL-divergence measure:\nmin \ud835\udf03\ud835\udf03\ud835\udc53\ud835\udc53,\ud835\udf03\ud835\udf03\ud835\udc54\ud835\udc54,\ud835\udf03\ud835\udf03\u210e,\ud835\udf03\ud835\udf03\ud835\udc60\ud835\udc60 \u2211 \ufffd\ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61log \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 \u2032 + \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61\ud835\udc5a\ud835\udc5alog\n\ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 \ud835\udc5a\ud835\udc5a \ud835\udc43\ud835\udc43\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61 \ud835\udc5a\ud835\udc5a \u2032 + \ud835\udc43\ud835\udc43\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 log \ud835\udc43\ud835\udc43\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 \ud835\udc43\ud835\udc43\ud835\udc35\ud835\udc35\ud835\udc35\ud835\udc35 \u2032 \ufffdT\ud835\udc61\ud835\udc61=1 . (12)"}, {"heading": "4 Experimental Setup & Results", "text": ""}, {"heading": "4.1 Experiments on the EV Model for Sentiment Analysis", "text": "At the outset, we evaluate the proposed EV model on the sentiment polarity classification task. Four widely-used benchmark multi-domain sentiment datasets are used in this study1 (Blitzer et al., 2007). They are product reviews taken from Amazon.com in four different domains: Books, DVD, Electronics, and Kitchen. Each of the reviews, ranging from Star-1 to Star-5, were rated by a\n1 https://www.cs.jhu.edu/~mdredze/datasets/sentiment/\ncustomer. The reviews with Star-1 and Star-2 were labelled as Negative, and those with Star-4 and Star-5 were labeled as Positive. Each of the four datasets contains 1,000 positive reviews, 1,000 negative reviews, and a number of unlabeled reviews. Labeled reviews in each domain are randomly split up into ten folds (with nine folds serving as the training set and the remaining one as the test set). All of the following results are reported in terms of an average accuracy of ten-fold cross validation. The linear kernel SVM (Chang and Lin, 2011) is used as our classifier and all of the parameters are set to the default values. All of the unlabeled reviews are used to obtain the general background information and train the EV model.\nIn this set of experiments, we first compare the EV model with PCA (Bengio et al., 2013), which is a standard dimension reduction method. It is worthy to note that PCA is a variation of an autoencoder (Bengio et al., 2013) method; thus it can be treated as our baseline system. All of the experimental results are listed in Table 1. As expected, the proposed EV model consistently outperforms PCA in every domain by a significant margin. The reason might be that PCA maps data to a low-dimensional space by maximizing the statistical variance of data, but the implicitly denoising strategy and the linear formulation limit its model capability. On the contrary, the proposed EV model is designed to distill the most useful information from a given paragraph and exclude the general background information explicitly; it thus can deduce a more informative and discriminative representation.\nNext, we make a step forward to compare the EV model with other baseline systems based on literal bag-of-words features, including unigrams and bigrams. The results are also shown in Table 1. Several observations can be drawn from the results. First, although bigram features (denotes as Bigrams in Table 1) are believed to be more discriminative than unigram features (denotes as Unigrams in Table 1), the results indicate that Unigrams outperform Bigrams in most cases. The reason might be probably due to the curse of dimensionality problem. Second, as expected, the combination of unigram and bigram features (denotes as Unigrams+Bigrams) achieves better results than using Unigrams and Bigrams in isolation for all cases. Third, both the proposed EV model and PCA can make further performance gains when paired with Unigrams, Bigrams, and their combination. Fourth, the proposed EV model demonstrates its ability in the sentiment classification task since it consistently outperforms PCA for all cases in the experiments."}, {"heading": "4.2 Experiments on the EV Model for Multi-Document Summarization", "text": "We further investigate the capability of the EV model on an extractive multi-document summarization task. In this study, we carry out the experiments with the DUC 2001, 2002, and 2004 datasets 2 . All the documents were compiled from newswires, and were grouped into various thematic clusters. The summary length was limited to 100 words for both DUC 2001 and DUC 2002, and 665 bytes for DUC 2004. The general background information was inferred from the LDC\n2 http://www-nlpir.nist.gov/projects/duc/\nGigaword corpus3 (including Associated Press Worldstream (AP), New York Times Newswire Service (NYT), and Xinhua News Agency (XIN)). The most common belief in the document summarization community is that relevance and redundancy are two key factors for generating a concise summary. In this paper, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time. That is, a concise summary for a given document set can be automatically generated through a one-pass process instead of an iterative process. Recently, the summarization method has proven its empirical effectiveness (Zhang et al., 2015). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015).\nWe compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al., (2015) for more in-depth elaboration. It is worthy to note that the proposed EV model, the two baseline systems, and the best peer systems are unsupervised methods, while the DNNbased systems are supervised ones. The experimental results are listed in Table 2. Several interesting observations can be concluded from the results. First, the proposed EV model outperforms VSM by a large margin in all cases, and performs comparably to other well-designed unsupervised summarization methods. Second, both LexRank and EV (with the density peaks clustering method) take pairwise information into account globally, so their results are almost the same. Third, although the proposed EV model is an unsupervised method and is not specifically designed toward summarization, it almost achieves the same performance level as the complicated DNN-based supervised methods (i.e., CNN and PriorSum), which confirms the power of the EV model again.\n3 https://catalog.ldc.upenn.edu/LDC2011T07"}, {"heading": "4.3 Experiments on the D-EV Model for Spoken Document Summarization", "text": "In order to assess the utility of the proposed D-EV model, we perform a series of experiments on the extractive spoken document summarization task. All of experiments are conducted on a Mandarin benchmark broadcast new corpus4 (Wang et al., 2005). The MATBN dataset is publicly available and has been widely used to evaluate several NLP-related tasks, including speech recognition (Chien, 2015), information retrieval (Huang and Wu, 2007) and summarization (Liu et al., 2015). As such, we follow the experimental setting used in previous studies for speech summarization in the literature. The vocabulary size is about 72 thousand words. The average word error rate of the automatic transcripts of these broadcast news documents is about 38%. The reference summaries were generated by ranking the sentences in the manual transcript of a broadcast news document by importance without assigning a score to each sentence. Each document has three reference summaries annotated by three subjects. For the assessment of summarization performance, we adopt the commonly-used ROUGE metric (Lin, 2003), and take ROUGE-1, ROUGE-2 and ROUGE-L (in F-scores) as the main measures. The summarization ratio is set to 10%. An external set of about 100,000 text news documents, which was assembled by the Central News Agency (CNA) during the same period as the broadcast news documents to be summarized (extracted from the Chinese Gigaword Corpus5 released by LDC), is used to obtain the background representation.\nTo begin with, we compare the performance levels of the proposed EV and D-EV models and two classic paragraph embedding methods (i.e., DM and DBOW) for spoken document summarization. All the models are paired with the density peaks clustering summarization method. The results are shown in Table 3, from which several observations can be drawn. First, DBOW outperforms DM in our experiments, though DBOW is a simplified version of DM. Second, the proposed EV model outperforms DM and DBOW by a large margin, as expected. The results confirm that EV can modulate the impact of those stop or function words when inferring representations for paragraphs. That is to say, the proposed paragraph embedding method EV can indeed distill the most important aspects of a given paragraph and meanwhile suppress the impact of the general background information for producing a more discriminative paragraph representation. Thus, the relevance degree between any pair of sentence and document representations can be estimated more accurately. Third, the D-EV model consistently outperforms other paragraph embedding methods, including our own EV model. The outcome reveals that, although EV can achieve better performance than other classic paragraph embedding methods, the recognition errors inevitably make the inferred representations deviate from the original semantic content of spoken paragraphs. Accordingly, the results signal that the D-EV model can complement the deficiency of the EV model in spoken document summarization; we thus believe that it is more suitable for use in spoken content processing.\n4 http://slam.iis.sinica.edu.tw/corpus/MATBN-corpus.htm 5 https://catalog.ldc.upenn.edu/LDC2011T13"}, {"heading": "DM 0.387 0.242 0.337", "text": "In the last set of experiments, we compare the results mentioned above with that of several wellpracticed, state-of-the-art unsupervised summarization methods, including the graph-based methods (i.e., the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.e., the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)). Among them, the ability of reducing redundant information has been aptly incorporated into the submodular-based method and the ILP method. Interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), and Nenkova and McKeown (2011) for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of spoken document summarization tasks. The results are also listed in Table 3. Several noteworthy observations can be drawn from the results of these methods. First, although the two graph-based methods (i.e., MRW and LexRank) have similar motivations, MRW outperforms LexRank by a large margin. Second, although both SM and ILP have the ability to reduce redundant information when selecting indicative sentences to form a summary for a given document, ILP consistently outperforms SM. The reason might be that ILP performs a global optimization process to select representative sentences, whereas SM chooses sentences with a recursive strategy. Comparing the results of these strong baseline systems to that of the paragraph embedding methods (including DM, DBOW, EV, and D-EV) paired with the density peaks clustering summarization method, it is clear that all the paragraph embedding methods are better than the baseline methods. The results corroborate that, instead of only considering literal term matching for determining the similarity degree between a pair of sentence and document, incorporating concept (semantic) matching into the similarity measure leads to better performance. In particular, the proposed D-EV model is the most robust among all the methods compared in the paper, which supports the important notion of the proposed \u201clearning to distilling\u201d framework. We also want to note that the proposed methods (i.e., EV and D-EV) can also be incorporated with the graph-based methods and the combinatorial optimization methods. We leave this exploration for future work."}, {"heading": "5 Conclusions", "text": "In this paper, we have proposed a novel paragraph embedding framework, which is embodied with the essence vector (EV) model and the denoising essence vector (D-EV) model, and made a step forward to evaluate the proposed methods on benchmark sentiment classification and document summarization tasks. Experimental results demonstrate that the proposed framework is the most robust among all the methods (including several well-practiced or/and state-of-the-art methods) compared in the paper, thereby indicating the potential of the new paragraph embedding framework. For future work, we will first focus on pairing the (denoising) essence vector model with other summarization methods. Moreover, we will explore other effective ways to integrate extra cues, such as speaker identities and relevance information, into the proposed framework. Furthermore, we also plan to extend the applications of the proposed framework to information retrieval and language modeling, among others."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of Machine Learning Research (3):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Representation learning: a review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent."], "venue": "Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Biographies, bollywood, boom-boxes and blenders: domain adaptation for sentiment classification", "author": ["John Blitzer", "Mark Dredze", "Fernando Pereira."], "venue": "Proceedings of ACL, pages 187\u2013 205.", "citeRegEx": "Blitzer et al\\.,? 2007", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Learning Summary Prior Representation for Extractive Summarization", "author": ["Ziqiang Cao", "Furu Wei", "Sujian Li", "Wenjie Li", "Ming Zhou", "Houfeng Wang."], "venue": "Proceedings of ACL, pages 829\u2013833.", "citeRegEx": "Cao et al\\.,? 2015", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "The use of MMR, diversity based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein."], "venue": "Proceedings of SIGIR, pages 335\u2013336.", "citeRegEx": "Carbonell and Goldstein.,? 1998", "shortCiteRegEx": "Carbonell and Goldstein.", "year": 1998}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin."], "venue": "ACM Transactions on Intelligent Systems and Technology, 2(27):1\u201327.", "citeRegEx": "Chang and Lin.,? 2011", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "I-vector based language modeling for spoken document retrieval", "author": ["Kuan-Yu Chen", "Hung-Shin Lee", "Hsin-Min Wang", "Berlin Chen."], "venue": "Proceedings of ICASSP, pages 7083\u20137088.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Hierarchical Pitman-Yor-Dirichlet language model", "author": ["Jen-Tzung Chien."], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing, 23(8): 1259\u20131272.", "citeRegEx": "Chien.,? 2015", "shortCiteRegEx": "Chien.", "year": 2015}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of ICML, pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "LexRank: Graph-based lexical centrality as salience in text summarization", "author": ["Gunes Erkan", "Dragomir R. Radev."], "venue": "Journal of Artificial Intelligent Research, 22(1):457\u2013479.", "citeRegEx": "Erkan and Radev.,? 2004", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "Fundamental technologies in modern speech recognition", "author": ["Sadaoki Furui", "Li Deng", "Mark Gales", "Hermann Ney", "Keiichi Tokuda."], "venue": "IEEE Signal Processing Magazine, 29(6):16\u201317.", "citeRegEx": "Furui et al\\.,? 2012", "shortCiteRegEx": "Furui et al\\.", "year": 2012}, {"title": "Generic text summarization using relevance measure and latent semantic analysis", "author": ["Yihong Gong", "Xin Liu."], "venue": "Proceedings of SIGIR, pages 19\u201325.", "citeRegEx": "Gong and Liu.,? 2001", "shortCiteRegEx": "Gong and Liu.", "year": 2001}, {"title": "Deep Learning", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville."], "venue": "Cambridge, MA: MIT Press.", "citeRegEx": "Goodfellow et al\\.,? 2016", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."], "venue": "Proceedings of CIKM, pages 2333\u20132338.", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Spoken Document Retrieval Using Multi-Level Knowledge and Semantic Verification", "author": ["Chien-Lin Huang", "Chung-Hsien Wu."], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 15(8): 2551\u20132560.", "citeRegEx": "Huang and Wu.,? 2007", "shortCiteRegEx": "Huang and Wu.", "year": 2007}, {"title": "Extractive summarization using continuous vector space models", "author": ["Mikael Kageback", "Olof Mogren", "Nina Tahmasebi", "Devdatt Dubhashi."], "venue": "Proceedings of CVSC, pages 31\u201339.", "citeRegEx": "Kageback et al\\.,? 2014", "shortCiteRegEx": "Kageback et al\\.", "year": 2014}, {"title": "ADAM: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of ICLR, pages 1\u201315.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "Proceedings of ICML, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "ROUGE: Recall-oriented understudy for gisting evaluation", "author": ["Chin-Yew Lin."], "venue": "[Online]. Available: http://haydn.isi.edu/ROUGE/.", "citeRegEx": "Lin.,? 2003", "shortCiteRegEx": "Lin.", "year": 2003}, {"title": "Multi-document summarization via budgeted maximization of submodular functions", "author": ["Hui Lin", "Jeff Bilmes."], "venue": "Proceedings of NAACL HLT, pages 912\u2013920.", "citeRegEx": "Lin and Bilmes.,? 2010", "shortCiteRegEx": "Lin and Bilmes.", "year": 2010}, {"title": "Speech summarization", "author": ["Yang Liu", "Dilek Hakkani-Tur."], "venue": "Chapter 13 in Spoken Language Understanding: Systems for Extracting Semantic Information from Speech. G. Tur and R. D. Mori (Eds), New York: Wiley.", "citeRegEx": "Liu and Hakkani.Tur.,? 2011", "shortCiteRegEx": "Liu and Hakkani.Tur.", "year": 2011}, {"title": "Combining relevance language modeling and clarity measure for extractive speech summarization", "author": ["Shih-Hung Liu", "Kuan-Yu Chen", "Berlin Chen", "Hsin-Min Wang", "Hsu-Chun Yen", "Wen-Lian Hsu."], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(6): 957\u2013969.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of ICLR, pages 1\u201312.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Automatic summarization", "author": ["Ani Nenkova", "Kathleen McKeown."], "venue": "Foundations and Trends in Information Retrieval, 5(2\u20133): 103\u2013233.", "citeRegEx": "Nenkova and McKeown.,? 2011", "shortCiteRegEx": "Nenkova and McKeown.", "year": 2011}, {"title": "Speech technology and information access", "author": ["Mari Ostendorf."], "venue": "IEEE Signal Processing Magazine, 25(3):150\u2013152.", "citeRegEx": "Ostendorf.,? 2008", "shortCiteRegEx": "Ostendorf.", "year": 2008}, {"title": "Spoken document understanding and organization", "author": ["Lin-shan Lee", "Berlin Chen."], "venue": "IEEE Signal Processing Magazine, 22(5):42\u201360.", "citeRegEx": "Lee and Chen.,? 2005", "shortCiteRegEx": "Lee and Chen.", "year": 2005}, {"title": "Deep sentence embedding using the long short term memory network: analysis and application to information retrieval", "author": ["Hamid Palangi", "Li Deng", "Yelong Shen", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Xinying Song", "Rabab Ward."], "venue": "Proceedings of arXiv:1502.06922.", "citeRegEx": "Palangi et al\\.,? 2015", "shortCiteRegEx": "Palangi et al\\.", "year": 2015}, {"title": "A critical reassessment of evaluation baselines for speech summarization", "author": ["Gerald Penn", "Xiaodan Zhu."], "venue": "Proceedings of ACL, pages 470\u2013478.", "citeRegEx": "Penn and Zhu.,? 2008", "shortCiteRegEx": "Penn and Zhu.", "year": 2008}, {"title": "GloVe: Global vector for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of EMNLP, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Long story short - Global unsupervised models for keyphrase based meeting summarization", "author": ["Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-Tur."], "venue": "Speech Communication, 52(10):801\u2013815.", "citeRegEx": "Riedhammer et al\\.,? 2010", "shortCiteRegEx": "Riedhammer et al\\.", "year": 2010}, {"title": "Clustering by fast search and find of density peaks", "author": ["Alex Rodriguez", "Alessandro Laio."], "venue": "Science, 344(6191): 1492\u20131496.", "citeRegEx": "Rodriguez and Laio.,? 2014", "shortCiteRegEx": "Rodriguez and Laio.", "year": 2014}, {"title": "Learning sentimentspecific word embedding for twitter sentiment classification", "author": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."], "venue": "Proceedings of ACL, pages 1555\u20131565.", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Multi-document summarization using cluster-based link analysis", "author": ["Xiaojun Wan", "Jianwu Yang."], "venue": "Proceedings of SIGIR, pages 299\u2013306.", "citeRegEx": "Wan and Yang.,? 2008", "shortCiteRegEx": "Wan and Yang.", "year": 2008}, {"title": "MATBN: A Mandarin Chinese broadcast news corpus", "author": ["Hsin-Min Wang", "Berlin Chen", "Jen-Wei Kuo", "Shih-Sian Cheng."], "venue": "International Journal of Computational Linguistics and Chinese Language Processing, 10(2):219\u2013236.", "citeRegEx": "Wang et al\\.,? 2005", "shortCiteRegEx": "Wang et al\\.", "year": 2005}, {"title": "Clustering sentences with density peaks for multi-document summarization", "author": ["Yang Zhang", "Yunqing Xia", "Yi Liu", "Wenmin Wang."], "venue": "Proceedings of NAACL, pages 1262\u20131267.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "When it comes to the field of natural language processing (NLP), word embedding methods can be viewed as pioneering studies (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 124, "endOffset": 192}, {"referenceID": 22, "context": "When it comes to the field of natural language processing (NLP), word embedding methods can be viewed as pioneering studies (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 124, "endOffset": 192}, {"referenceID": 28, "context": "When it comes to the field of natural language processing (NLP), word embedding methods can be viewed as pioneering studies (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 124, "endOffset": 192}, {"referenceID": 8, "context": "By doing so, this thread of methods has recently enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014).", "startOffset": 103, "endOffset": 173}, {"referenceID": 31, "context": "By doing so, this thread of methods has recently enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014).", "startOffset": 103, "endOffset": 173}, {"referenceID": 15, "context": "By doing so, this thread of methods has recently enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014).", "startOffset": 103, "endOffset": 173}, {"referenceID": 13, "context": "Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few.", "startOffset": 174, "endOffset": 238}, {"referenceID": 17, "context": "Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few.", "startOffset": 174, "endOffset": 238}, {"referenceID": 26, "context": "Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few.", "startOffset": 174, "endOffset": 238}, {"referenceID": 25, "context": "Consequently, spoken content processing has become an important and urgent demand (Lee and Chen, 2005; Ostendorf, 2008; Liu and Hakkani-Tur, 2011).", "startOffset": 82, "endOffset": 146}, {"referenceID": 24, "context": "Consequently, spoken content processing has become an important and urgent demand (Lee and Chen, 2005; Ostendorf, 2008; Liu and Hakkani-Tur, 2011).", "startOffset": 82, "endOffset": 146}, {"referenceID": 20, "context": "Consequently, spoken content processing has become an important and urgent demand (Lee and Chen, 2005; Ostendorf, 2008; Liu and Hakkani-Tur, 2011).", "startOffset": 82, "endOffset": 146}, {"referenceID": 10, "context": "Obviously, speech is one of the most important sources of information about multimedia (Furui et al., 2012).", "startOffset": 87, "endOffset": 107}, {"referenceID": 13, "context": "In contrast to the large body of work on developing various word embedding methods, there are relatively few studies concentrating on learning paragraph representations in an unsupervised manner (Huang et al., 2013; Le and Mikolov, 2014; Chen et al., 2014; Palangi et al., 2015).", "startOffset": 195, "endOffset": 278}, {"referenceID": 17, "context": "In contrast to the large body of work on developing various word embedding methods, there are relatively few studies concentrating on learning paragraph representations in an unsupervised manner (Huang et al., 2013; Le and Mikolov, 2014; Chen et al., 2014; Palangi et al., 2015).", "startOffset": 195, "endOffset": 278}, {"referenceID": 6, "context": "In contrast to the large body of work on developing various word embedding methods, there are relatively few studies concentrating on learning paragraph representations in an unsupervised manner (Huang et al., 2013; Le and Mikolov, 2014; Chen et al., 2014; Palangi et al., 2015).", "startOffset": 195, "endOffset": 278}, {"referenceID": 26, "context": "In contrast to the large body of work on developing various word embedding methods, there are relatively few studies concentrating on learning paragraph representations in an unsupervised manner (Huang et al., 2013; Le and Mikolov, 2014; Chen et al., 2014; Palangi et al., 2015).", "startOffset": 195, "endOffset": 278}, {"referenceID": 17, "context": "Representative methods include the distributed memory model (Le and Mikolov, 2014) and the distributed bag-of-words model (Le and Mikolov, 2014; Chen et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 17, "context": "Representative methods include the distributed memory model (Le and Mikolov, 2014) and the distributed bag-of-words model (Le and Mikolov, 2014; Chen et al., 2014).", "startOffset": 122, "endOffset": 163}, {"referenceID": 6, "context": "Representative methods include the distributed memory model (Le and Mikolov, 2014) and the distributed bag-of-words model (Le and Mikolov, 2014; Chen et al., 2014).", "startOffset": 122, "endOffset": 163}, {"referenceID": 0, "context": "The distributed memory (DM) model is inspired and hybridized from the traditional feed-forward neural network language model (NNLM) (Bengio et al., 2003) and the recently proposed word", "startOffset": 132, "endOffset": 153}, {"referenceID": 22, "context": "embedding methods (Mikolov et al., 2013).", "startOffset": 18, "endOffset": 40}, {"referenceID": 17, "context": "Based on the NNLM, the notion underlying the DM model is that a given paragraph also contributes to the prediction of the next word, given its previous words in the paragraph (Le and Mikolov, 2014).", "startOffset": 175, "endOffset": 197}, {"referenceID": 17, "context": "Opposite to the DM model, a simplified version is to only rely on the paragraph representation to predict all of the words occurring in the paragraph (Le and Mikolov, 2014; Chen et al., 2014).", "startOffset": 150, "endOffset": 191}, {"referenceID": 6, "context": "Opposite to the DM model, a simplified version is to only rely on the paragraph representation to predict all of the words occurring in the paragraph (Le and Mikolov, 2014; Chen et al., 2014).", "startOffset": 150, "endOffset": 191}, {"referenceID": 17, "context": "In addition to being conceptually simple, the DBOW model only needs to store the softmax weights, whereas the DM model stores both softmax weights and word vectors (Le and Mikolov, 2014).", "startOffset": 164, "endOffset": 186}, {"referenceID": 12, "context": "The activation function used in the EV model is the hyperbolic tangent, except that the output layer in the decoder h(\u2219) is the softmax (Goodfellow et al., 2016), the cosine distance is used to calculate the attention coefficients, and the Adam (Kingma and Ba, 2015) is employed to solve the optimization problem.", "startOffset": 136, "endOffset": 161}, {"referenceID": 16, "context": ", 2016), the cosine distance is used to calculate the attention coefficients, and the Adam (Kingma and Ba, 2015) is employed to solve the optimization problem.", "startOffset": 91, "endOffset": 112}, {"referenceID": 2, "context": "Four widely-used benchmark multi-domain sentiment datasets are used in this study1 (Blitzer et al., 2007).", "startOffset": 83, "endOffset": 105}, {"referenceID": 5, "context": "The linear kernel SVM (Chang and Lin, 2011) is used as our classifier and all of the parameters are set to the default values.", "startOffset": 22, "endOffset": 43}, {"referenceID": 1, "context": "In this set of experiments, we first compare the EV model with PCA (Bengio et al., 2013), which is a standard dimension reduction method.", "startOffset": 67, "endOffset": 88}, {"referenceID": 1, "context": "It is worthy to note that PCA is a variation of an autoencoder (Bengio et al., 2013) method; thus it can be treated as our baseline system.", "startOffset": 63, "endOffset": 84}, {"referenceID": 30, "context": "In this paper, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time.", "startOffset": 75, "endOffset": 121}, {"referenceID": 34, "context": "In this paper, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time.", "startOffset": 75, "endOffset": 121}, {"referenceID": 34, "context": "Recently, the summarization method has proven its empirical effectiveness (Zhang et al., 2015).", "startOffset": 74, "endOffset": 94}, {"referenceID": 18, "context": "For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al.", "startOffset": 75, "endOffset": 86}, {"referenceID": 3, "context": "For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015).", "startOffset": 162, "endOffset": 181}, {"referenceID": 11, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al.", "startOffset": 89, "endOffset": 109}, {"referenceID": 9, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al.", "startOffset": 126, "endOffset": 149}, {"referenceID": 3, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015).", "startOffset": 328, "endOffset": 346}, {"referenceID": 3, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al.", "startOffset": 329, "endOffset": 497}, {"referenceID": 3, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al.", "startOffset": 329, "endOffset": 525}, {"referenceID": 3, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al.", "startOffset": 329, "endOffset": 553}, {"referenceID": 3, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al., (2015) for more in-depth elaboration.", "startOffset": 329, "endOffset": 577}, {"referenceID": 33, "context": "All of experiments are conducted on a Mandarin benchmark broadcast new corpus4 (Wang et al., 2005).", "startOffset": 79, "endOffset": 98}, {"referenceID": 7, "context": "The MATBN dataset is publicly available and has been widely used to evaluate several NLP-related tasks, including speech recognition (Chien, 2015), information retrieval (Huang and Wu, 2007) and summarization (Liu et al.", "startOffset": 133, "endOffset": 146}, {"referenceID": 14, "context": "The MATBN dataset is publicly available and has been widely used to evaluate several NLP-related tasks, including speech recognition (Chien, 2015), information retrieval (Huang and Wu, 2007) and summarization (Liu et al.", "startOffset": 170, "endOffset": 190}, {"referenceID": 21, "context": "The MATBN dataset is publicly available and has been widely used to evaluate several NLP-related tasks, including speech recognition (Chien, 2015), information retrieval (Huang and Wu, 2007) and summarization (Liu et al., 2015).", "startOffset": 209, "endOffset": 227}, {"referenceID": 18, "context": "For the assessment of summarization performance, we adopt the commonly-used ROUGE metric (Lin, 2003), and take ROUGE-1, ROUGE-2 and ROUGE-L (in F-scores) as the main measures.", "startOffset": 89, "endOffset": 100}, {"referenceID": 32, "context": ", the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 9, "context": ", the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.", "startOffset": 82, "endOffset": 105}, {"referenceID": 19, "context": ", the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 29, "context": ", the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)).", "startOffset": 108, "endOffset": 133}, {"referenceID": 9, "context": ", the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.e., the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)). Among them, the ability of reducing redundant information has been aptly incorporated into the submodular-based method and the ILP method. Interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), and Nenkova and McKeown (2011) for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of spoken document summarization tasks.", "startOffset": 83, "endOffset": 482}, {"referenceID": 9, "context": ", the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.e., the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)). Among them, the ability of reducing redundant information has been aptly incorporated into the submodular-based method and the ILP method. Interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), and Nenkova and McKeown (2011) for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of spoken document summarization tasks.", "startOffset": 83, "endOffset": 510}, {"referenceID": 9, "context": ", the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.e., the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)). Among them, the ability of reducing redundant information has been aptly incorporated into the submodular-based method and the ILP method. Interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), and Nenkova and McKeown (2011) for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of spoken document summarization tasks.", "startOffset": 83, "endOffset": 542}], "year": 2016, "abstractText": "In the context of natural language processing, representation learning has emerged as a newly active research subject because of its excellent performance in many applications. Learning representations of words is a pioneering study in this school of research. However, paragraph (or sentence and document) embedding learning is more suitable/reasonable for some tasks, such as sentiment classification and document summarization. Nevertheless, as far as we are aware, there is relatively less work focusing on the development of unsupervised paragraph embedding methods. Classic paragraph embedding methods infer the representation of a given paragraph by considering all of the words occurring in the paragraph. Consequently, those stop or function words that occur frequently may mislead the embedding learning process to produce a misty paragraph representation. Motivated by these observations, our major contributions in this paper are twofold. First, we propose a novel unsupervised paragraph embedding method, named the essence vector (EV) model, which aims at not only distilling the most representative information from a paragraph but also excluding the general background information to produce a more informative low-dimensional vector representation for the paragraph. We evaluate the proposed EV model on benchmark sentiment classification and multi-document summarization tasks. The experimental results demonstrate the effectiveness and applicability of the proposed embedding method. Second, in view of the increasing importance of spoken content processing, an extension of the EV model, named the denoising essence vector (D-EV) model, is proposed. The D-EV model not only inherits the advantages of the EV model but also can infer a more robust representation for a given spoken paragraph against imperfect speech recognition. The utility of the D-EV model is evaluated on a spoken document summarization task, confirming the practical merits of the proposed embedding method in relation to several wellpracticed and state-of-the-art summarization methods.", "creator": "Acrobat PDFMaker 15 Word \u7248"}}}