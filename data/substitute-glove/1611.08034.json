{"id": "1611.08034", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling", "abstract": "Recurrent processes networks (RNNs) likely positive help visual for nahuatl behavioral. However, large provide thus RNNs using back - propagation turn he often shock in overfitting. One reason three means because instead hamiltonian hilbert (used for with training each) does might help good estimates much example uncertainty. This cover institutionalizes recently decisive 1998 stochastic gradient Markov Chain Monte Carlo (all essential even cut training sets) while learning loads persists in RNNs. It offset while 86-run Bayesian learning iteratively, needed curve voltage during specialist (enhancing refining far the model - parameter space) include ideal averaged turn similar. Extensive chemical on various RNN versions and coming called broad range one applications demonstrate first inherent form the request unconventional far second-order front-end.", "histories": [["v1", "Wed, 23 Nov 2016 23:40:50 GMT  (1012kb,D)", "http://arxiv.org/abs/1611.08034v1", null], ["v2", "Mon, 24 Apr 2017 15:32:49 GMT  (663kb,D)", "http://arxiv.org/abs/1611.08034v2", "Accepted to ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhe gan", "chunyuan li", "changyou chen", "yunchen pu", "qinliang su", "lawrence carin"], "accepted": true, "id": "1611.08034"}, "pdf": {"name": "1611.08034.pdf", "metadata": {"source": "CRF", "title": "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling", "authors": ["Zhe Gan", "Chunyuan Li", "Changyou Chen", "Yunchen Pu", "Qinliang Su", "Lawrence Carin"], "emails": ["lcarin}@duke.edu"], "sections": [{"heading": "1 Introduction", "text": "Language modeling is a fundamental task, used for example to predict the next word or character in a text sequence given the context. Recently, recurrent neural networks (RNNs) have shown promising performance on this task (Mikolov et al., 2010; Sutskever et al., 2011). RNNs with Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997) have emerged as a popular architecture, due to their representational power and effectiveness at capturing long-term dependencies.\nRNNs are usually trained via back-propagation through time (Werbos, 1990), using stochastic optimization methods such as stochastic gradient de-\nscent (SGD) (Robbins and Monro, 1951); stochastic methods of this type are particularly important for training with large data sets. However, this approach often provides a maximum a posteriori (MAP) estimate of model parameters. The MAP solution is a single point estimate, ignoring weight uncertainty (Blundell et al., 2015; Herna\u0301ndezLobato and Adams, 2015). Natural language often exhibits significant variability, and hence such a point estimate may make over-confident predictions on test data.\nTo alleviate this overfitting problem in RNNs, good regularization is known as a key factor to successful applications. In the neural network literature, Bayesian learning has been proposed as a principled method to impose regularization and incorporate model uncertainty (MacKay, 1992; Neal, 1995), by imposing prior distributions on model parameters. Due to the intractability of posterior distributions in neural networks, Hamiltonian Monte Carlo (HMC) (Neal, 1995) has been used to provide sample-based approximations to the true posterior. Despite the elegant theoretical property of asymptotic convergence to the true posterior, HMC and other conventional Markov Chain Monte Carlo methods are not scalable to large training sets.\nThis paper seeks to scale up Bayesian learning of RNNs to meet the challenge of the increasing amount of \u201cbig\u201d sequential data in natural language processing, by leveraging recent advances in stochastic gradient Markov Chain Monte Carlo (SG-MCMC) algorithms (Welling and Teh, 2011; Chen et al., 2014; Ding et al., 2014; Li et al., 2016a). Specifically, instead of training a single network, SG-MCMC is employed to train an ensemble of networks, where each network has its parameters drawn from a shared posterior distribution. This is implemented by adding additional gradient noise during training and utilizing model\nar X\niv :1\n61 1.\n08 03\n4v 1\n[ cs\n.C L\n] 2\n3 N\nov 2\n01 6\naveraging when testing. This simple procedure has the following favorable properties for training neural networks: (i) The injected noise encourages model-parameter trajectories when training to better explore the parameter space. This procedure was also empirically found effective in (Neelakantan et al., 2016). (ii) Model averaging when testing alleviates overfitting and hence improves generalization, transferring uncertainty in the learned model parameters to subsequent prediction. (iii) In theory, both asymptotic and non-asymptotic consistency properties of SG-MCMC methods in posterior estimation have been recently established to guarantee convergence (Chen et al., 2015a; Teh et al., 2016). (iv) SG-MCMC is scalable; it shares the same level of computational cost as SGD in training, by only requiring the evaluation of gradients on a small mini-batch. To the authors\u2019 knowledge, RNN training using SG-MCMC has not been investigated previously, and is a contribution of this paper. We also perform extensive experiments on several natural language processing tasks, demonstrating the effectiveness of SG-MCMC for RNNs, including character/word-level language modeling, image captioning and sentence classification."}, {"heading": "2 Related Work", "text": "Several scalable Bayesian learning methods have been proposed recently for neural networks. These come in two broad categories: stochastic variational inference (Graves, 2011; Blundell et al., 2015; Herna\u0301ndez-Lobato and Adams, 2015) and SG-MCMC methods (Korattikara et al., 2015;\nLi et al., 2016a). While prior work focuses on feed-forward neural networks, there has been little if any research reported for RNNs using SGMCMC.\nDropout (Hinton et al., 2012; Srivastava et al., 2014) is a commonly used regularization method for training neural networks. Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b). Among them, naive dropout (Zaremba et al., 2014) can impose weight uncertainty only on encoding weights (those that connect input to hidden units) and decoding weights (those that connect hidden units to output), but not the recurrent weights (those that connect consecutive hidden states). It has been concluded that noise added in the recurrent connections leads to model instabilities, hence disrupting the RNN\u2019s ability to model sequences.\nDropout has been recently shown to be a variational approximation technique in Bayesian learning (Gal and Ghahramani, 2016a; Kingma et al., 2015). Based on this, (Gal and Ghahramani, 2016b) proposed a new variant of dropout that can be successfully applied to recurrent layers, where the same dropout masks are shared along time for encoding, decoding and recurrent weights, respectively. Alternatively, we focus on SG-MCMC, which can be viewed as the Bayesian interpretation of dropout from the perspective of posterior sampling (Li et al., 2016b); this also allows imposition of model uncertainty on recurrent layers, boosting performance. A comparison of naive dropout and SG-MCMC is illustrated in Fig. 1."}, {"heading": "3 Recurrent Neural Networks", "text": ""}, {"heading": "3.1 RNN as Bayesian Predictive Models", "text": "Consider data D = {D1, \u00b7 \u00b7 \u00b7 ,DN}, where Dn , (Xn,Yn), with input Xn and output Yn. Our goal is to learn model parameters \u03b8 to best characterize the relationship from Xn to Yn, with corresponding data likelihood p(D|\u03b8) =\u220fN n=1 p(Dn|\u03b8). In Bayesian statistics, one sets a prior on \u03b8 via distribution p(\u03b8). The posterior p(\u03b8|D) \u221d p(\u03b8)p(D|\u03b8) reflects the belief concerning the model parameter distribution after observing the data. Given a test input X\u0303 (with missing output Y\u0303), the uncertainty learned in training\nis transferred to prediction, yielding the posterior predictive distribution:\np(Y\u0303|X\u0303,D)= \u222b \u03b8 p(Y\u0303|X\u0303,\u03b8)p(\u03b8|D)d\u03b8 . (1)\nWhen the input is a sequence, RNNs may be used to parameterize the input-output relationship. Specifically, consider input sequence X = {x1, . . . ,xT }, where xt is the input data vector at time t. There is a corresponding hidden state vector ht at each time t, obtained by recursively applying the transition function ht = H(ht\u22121,xt) (specified in Section 3.2; see Fig. 1). The ouput Y differs depending on the application: a sequence {y1, . . . ,yT } in language modeling or a discrete label in sentence classification. In RNNs the corresponding decoding function is p(y|h), described in Section 3.3."}, {"heading": "3.2 RNN Architectures", "text": "The transition function H(\u00b7) can be implemented with a gated activation function, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or a Gated Recurrent Unit (GRU) (Cho et al., 2014). Both LSTM and GRU have been proposed to address the issue of learning long-term sequential dependencies.\nLong Short-Term Memory The LSTM architecture addresses the problem of learning longterm dependencies by introducing a memory cell, that is able to preserve the state over long periods of time. Specifically, each LSTM unit has a cell containing a state ct at time t. This cell can be viewed as a memory unit. Reading or writing the cell is controlled through sigmoid gates: input gate it, forget gate ft, and output gate ot. The hidden units ht are updated as follows:\nit = \u03c3(Wixt +Uiht\u22121 + bi) ,\nft = \u03c3(Wfxt +Ufht\u22121 + bf ) ,\not = \u03c3(Woxt +Uoht\u22121 + bo) ,\nc\u0303t = tanh(Wcxt +Ucht\u22121 + bc) ,\nct = ft ct\u22121 + it c\u0303t , ht = ot tanh(ct) ,\nwhere \u03c3(\u00b7) denotes the logistic sigmoid function, and represents the element-wise matrix multiplication operator. W{i,f,o,c} are encoding weights, and U{i,f,o,c} are recurrent weights, as shown in Fig. 1. b{i,f,o,c} are bias terms.\nVariants Similar to the LSTM unit, the GRU also has gating units that modulate the flow of information inside the hidden unit. It has been shown that a GRU can achieve similar performance to an LSTM in sequence modeling (Chung et al., 2014). We specify the GRU in the Supplementary Material.\nThe LSTM can be extended to the bidirectional LSTM and multilayer LSTM. A bidirectional LSTM consists of two LSTMs that are run in parallel: one on the input sequence and the other on the reverse of the input sequence. At each time step, the hidden state of the bidirectional LSTM is the concatenation of the forward and backward hidden states. In multilayer LSTMs, the hidden state of an LSTM unit in layer ` is used as input to the LSTM unit in layer `+1 at the same time step."}, {"heading": "3.3 Applications", "text": "The proposed Bayesian framework can be applied to any RNN model; we focus on the following basic tasks to demonstrate the ideas.\nLanguage Modeling In word-level language modeling, the input to the network is a sequence of words, and the network is trained to predict the next word in the sequence with a softmax classifier. Specifically, for a length-T sequence, denote yt = xt+1 for t = 1, . . . , T \u2212 1. x1 and yT are always set to a special START and END token, respectively. At each time t, there is a decoding function p(yt|ht) = softmax(Vht) to compute the distribution over words, where V are the decoding weights (the number of rows of V corresponds to the number of words/characters). We also extend this basic language model to consider other applications: (i) a character-level language model can be specified in a similar manner by replacing words with characters (Karpathy et al., 2016). (ii) Image captioning can be considered as a conditional language modeling problem, in which we learn a generative language model of the caption conditioned on an image (Vinyals et al., 2015).\nSentence Classification Sentence classification aims to assign a semantic category label y to a whole sentence X. This is usually implemented through applying the decoding function once at the end of sequence: p(y|hT ) = softmax(VhT ), where the final hidden state of a RNN hT is often considered as the summary of the sentence (here\nthe number of rows of V corresponds to the number of classes)."}, {"heading": "4 Scalable Learning with SG-MCMC", "text": ""}, {"heading": "4.1 The Pitfall of Stochastic Optimization", "text": "Typically there is no closed-form solution for (1), and traditional MCMC methods scale poorly for large N . To ease the computational burden, stochastic optimization is often employed to find the MAP solution. This is equivalent to minimizing an objective of regularized loss function U(\u03b8) that corresponds to a (non-convex) model of interest: \u03b8MAP = argminU(\u03b8), U(\u03b8) = \u2212 log p(\u03b8|D). The expectation in (1) is approximated as:\np(Y\u0303|X\u0303,D)= p(Y\u0303|X\u0303,\u03b8MAP) . (2)\nThough simple and effective, this procedure largely loses the benefit of the Bayesian approach, because the uncertainty on weights is ignored. To more accurately approximate (1), we employ SGMCMC."}, {"heading": "4.2 Large-scale Bayesian Learning", "text": "In a Bayesian model, the above regularized loss function corresponds to the potential energy defined as the negative log-posterior:\nU(\u03b8) , \u2212 log p(\u03b8)\u2212 N\u2211 n=1 log p(Dn|\u03b8). (3)\nIn optimization,E = \u2212 \u2211N\nn=1 log p(Dn|\u03b8) is typically referred to as the loss function, and R \u221d \u2212 log p(\u03b8) as a regularizer.\nFor large N , stochastic approximations are often employed:\nU\u0303t(\u03b8),\u2212 log p(\u03b8)\u2212 N\nM M\u2211 m=1 log p(Dim |\u03b8), (4)\nwhere Sm = {i1, \u00b7 \u00b7 \u00b7 , iM} is a random subset of the set {1, 2, \u00b7 \u00b7 \u00b7 , N}, with M N . The gradient on this mini-batch is denoted as f\u0303t = \u2207U\u0303t(\u03b8), which is an unbiased estimate of the true gradient. The evaluation of (4) is cheap even when N is large, allowing one to efficiently collect a sufficient number of samples in large-scale Bayesian learning, {\u03b8s}Ss=1, where S is the number of samples (this will be specified later). These samples are used to construct a sample-based estimation to the expectation in (1):\nThe finite-time estimation errors of SG-MCMC methods are bounded (Chen et al., 2015a), which guarantees (5) is an unbiased estimate of (1) asymptotically under appropriate decreasing stepsizes."}, {"heading": "4.3 SG-MCMC Algorithms", "text": "SG-MCMC and stochastic optimization are two parallel lines of work, designed for different purposes; their relationship has recently been revealed in the context of deep learning. The most basic SG-MCMC algorithm has been applied to Langevin dynamics, and is termed SGLD (Welling and Teh, 2011). To help convergence, a momentum term has been introduced in SGHMC (Chen et al., 2014), a \u201cthermostat\u201d has been devised in SGNHT (Ding et al., 2014; Gan et al., 2015) and preconditioners have been employed in pSGLD (Li et al., 2016a). These SG-MCMC algorithms often share similar characteristics with their counterpart approaches from the optimization literature such as the momentum SGD, Santa (Chen et al., 2016) and RMSprop/Adagrad (Tieleman and Hinton, 2012; Duchi et al., 2011). The interrelationships between SG-MCMC and optimizationbased approaches are summarized in Table 1.\nSGLD Stochastic Gradient Langevin Dynamics (SGLD) (Welling and Teh, 2011) draws posterior samples, with updates\n\u03b8t = \u03b8t\u22121 \u2212 \u03b7tf\u0303t\u22121 + \u221a 2\u03b7t\u03bet , (6)\nwhere \u03b7t is the learning rate, and \u03bet \u223c N (0, Ip) is a standard Gaussian random vector. SGLD is the SG-MCMC analog to SGD, whose parameter updates are given by:\n\u03b8t = \u03b8t\u22121 \u2212 \u03b7tf\u0303t\u22121 . (7)\nAlgorithm 1: pSGLD Input: Default hyperparameter settings:\n\u03b7t = 1\u00d710\u22123, \u03bb = 10\u22128, \u03b21 = 0.99. Initialize: v0 \u2190 0, \u03b81 \u223c N (0, I) ; for t = 1, 2, . . . , T do\n% Estimate gradient from minibatch St f\u0303t = \u2207U\u0303t(\u03b8); % Preconditioning vt \u2190 \u03b21vt\u22121 + (1\u2212 \u03b21)f\u0303t f\u0303t;\nG\u22121t \u2190 diag ( 1 ( \u03bb1+ v 1 2 t )) ;\n% Parameter update \u03bet \u223c N (0, \u03b7tG\u22121t ); \u03b8t+1\u2190 \u03b8t + \u03b7t2 G \u22121 t f\u0303t+ \u03bet;\nend\nSGD is guaranteed to converge to a local minimum under mild conditions (Bottou, 2010). The additional Gaussian term in SGLD helps the learning trajectory to explore the parameter space to approximate posterior samples, instead of obtaining a local minimum.\npSGLD Preconditioned SGLD (pSGLD) (Li et al., 2016a) was proposed recently to improve the mixing of SGLD. It utilizes magnitudes of recent gradients to construct a diagonal preconditioner to approximate the Fisher information matrix, and thus adjusts to the local geometry of parameter space by equalizing the gradients so that a constant stepsize is adequate for all dimensions. This is important for RNNs, whose parameter space often exhibits pathological curvature and saddle points (Pascanu et al., 2013), resulting in slow mixing. There are multiple choices of preconditioners; similar ideas in optimization include Adagrad (Duchi et al., 2011), Adam (Kingma and Ba, 2015) and RMSprop (Tieleman and Hinton, 2012). An efficient version of pSGLD, adopting RMSprop as the preconditioner G, is summarized in Algorithm 1, where denotes element-wise matrix division. When the preconditioner is fixed as the identity matrix, the method reduces to SGLD."}, {"heading": "4.4 Understanding SG-MCMC", "text": "To further understand SG-MCMC, we show its close connection to dropout/dropConnect (Srivastava et al., 2014; Wan et al., 2013). These methods improve the generalization ability of deep models, by randomly adding binary/Gaussian noise to the local units or global weights. For neural networks\nwith the nonlinear function q(\u00b7) and consecutive layers h1 and h2, dropout and dropConnect are denoted as:\nDropout: h2 = \u03be0 q(\u03b8h1), DropConnect: h2 = q((\u03be0 \u03b8)h1),\nwhere the injected noise \u03be0 can be binary-valued with dropping rate p or its equivalent Gaussian form (Wang and Manning, 2013):\nBinary noise: \u03be0 \u223c Ber(p),\nGaussian noise: \u03be0 \u223c N (1, p\n1\u2212 p ).\nNote that \u03be0 is defined as a vector for dropout, and a matrix for dropConnect. By combining dropConnect and Gaussian noise from the above, we have the update rule (Li et al., 2016b):\n\u03b8t+1 = \u03be0 \u03b8t \u2212 \u03b7\n2 f\u0303t = \u03b8t \u2212\n\u03b7 2 f\u0303t + \u03be \u2032 0 , (8)\nwhere \u03be\u20320 \u223c N ( 0, p(1\u2212p)diag(\u03b8 2 t ) )\n; (8) shows that dropout/ dropConnect and SGLD in (6) share the same form of update rule, with the distinction being that the level of injected noise is different. In practice, the noise injected by SGLD may not be enough. A better way that we find to improve the performance is to jointly apply SGLD and dropout. This method can be interpreted as using SGLD to sample the posterior distribution of a mixture of RNNs, with mixture probability controlled by the dropout rate."}, {"heading": "5 Experiments", "text": "We present results on several tasks, including character/ word-level language modeling, image captioning, and sentence classification. The hyperparameter setting, the initialization of model parameters and model specifications on each dataset are all provided in the Supplementary Material.\nWe do not perform any dataset-specific tuning other than early stopping on validation sets. When dropout is utilized, the dropout rate is empirically set to 0.5. All experiments are implemented in Theano (Theano Development Team, 2016), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory."}, {"heading": "5.1 Language Modeling", "text": "We first test character-level and word-level language modeling. The setup for each task is as follows.\n\u2022 Following (Karpathy et al., 2016), we test character-level language modeling on the War and Peace (WP) novel. The training/validation/test sets contain 260/32/33 batches, in which there are 100 characters. The vocabulary size is 87, and we consider a 2-hidden-layer RNN of dimension 128.\n\u2022 The Penn Treebank (PTB) corpus (Marcus et al., 1993) is used for word-level language modeling. The dataset adopts the standard split (929K training words, 73K validation words, and 82K test words) and has a vocabulary of size 10K. We train LSTMs of three sizes; these are denoted the small/medium/large LSTM. All LSTMs have two layers and are unrolled for 20 steps. The small, medium and large LSTM has 200, 650 and 1500 units per layer, respectively.\nWe study the effects of different types of architecture (LSTM/GRU/Vanilla RNN) on the WP dataset, and effects of different learning algorithms on the PTB dataset. The comparison of test cross-entropy loss on WP is shown in Table 2. We observe that pSGLD consistently outperforms RMSprop. Table 3 summarizes the test set performance on PTB1. It is clear that our sampling-based method consistently outperforms the optimization counterpart, where the performance gain mainly comes from adding gradient noise and model averaging. When compared with dropout, SGLD performs better on the small LSTM model, but slightly worse on the medium and large LSTM model. This may imply that dropout is suitable to regularizing large networks, while SGLD exhibits better regularization ability on small networks, partially due to the fact that dropout may inject a higher level of noise during training than SGLD. In order to inject a higher level of noise into SGLD, we empirically apply SGLD and dropout jointly, and found that this provided the best performace on the medium and large LSTM model.\nWe study three strategies to do model averaging, i.e., forward collection, backward collection and thinned collection. Given samples (\u03b81, \u00b7 \u00b7 \u00b7 ,\u03b8K) and the number of samples S used for averaging, forward collection refers to using (\u03b81, \u00b7 \u00b7 \u00b7 ,\u03b8S) for the evaluation of a test function, backward collection refers to using (\u03b8K\u2212S+1, \u00b7 \u00b7 \u00b7 ,\u03b8K), while\n1The results reported here do not match (Zaremba et al., 2014) due to the differences of experimental setup. However, we provide a fair comparison to all methods.\nthinned collection chooses samples from \u03b81 to \u03b8K with interval K/S. Fig. 2 plots the effects of these strategies, where Fig. 2(a) plots the perplexity of every single sample, Fig. 2(b) plots the perplexities using the three schemes. It can be seen that only after 20 samples is a converged perplexity achieved in the thinned collection, while it requires 30 samples for forward collection or 60 samples for backward collection. This is unsurprising, because thinned collection provides a better way to select samples. Nevertheless, averaging of samples provides significantly lower perplexity than using single samples. Note that the overfitting problem in Fig. 2(a) is also alleviated by model averaging.\nTo better illustrate the benefit of model averaging, we visualize in Fig. 3 the probabilities of each word in a randomly chosen test sentence. The first 3 rows are the results predicted by 3 distinctive model samples, respectively; the bottom row is the result after averaging. Their corresponding perplexities for the test sentence are also shown on the right of each row. The 3 individual samples provide reasonable probabilities. For example, the consecutive words \u201cNew York\u201d, \u201cstock exchange\u201d and \u201cdid not\u201d are assigned with a higher probability. After averaging, we can see a much lower perplexity, as the samples can complement each other. For example, though the second sample can\nTable 4: Performance on Flickr8k & Flickr30k datasets in terms of BLEU-1,2,3,4, METEOR, CIDEr, ROUGE-L and perplexity.\nMethods B-1 B-2 B-3 B-4 METEOR CIDEr ROUGE-L Perp. Results on Flickr8k RMSprop 0.640 0.427 0.288 0.197 0.205 0.476 0.500 16.64 RMSprop + Dropout 0.647 0.444 0.305 0.209 0.208 0.514 0.510 15.72 pSGLD 0.669 0.463 0.321 0.224 0.214 0.535 0.522 14.29 pSGLD + Dropout 0.656 0.450 0.309 0.211 0.209 0.512 0.512 14.26 Results on Flickr30k RMSprop 0.644 0.422 0.279 0.184 0.180 0.372 0.476 17.80 RMSprop + Dropout 0.656 0.435 0.295 0.200 0.185 0.396 0.481 18.05 pSGLD 0.657 0.438 0.300 0.206 0.192 0.421 0.490 15.61 pSGLD + Dropout 0.666 0.448 0.308 0.209 0.189 0.419 0.487 17.05\n25.55the new york stock exchange did not fall apart 22.24the new york stock exchange did not fall apart 29.83the new york stock exchange did not fall apart\n21.98the new york stock exchange did not fall apart 0\n0.2\n0.4\n0.6\n0.8\n1\nFigure 3: Predictive probabilities obtained by 3 samples and their average. Colors indicate normalized probability of each word. Best viewed in color.\nyield the lowest single-model perplexity, its prediction on word \u201cYork\u201d is still benefited from the other two via averaging."}, {"heading": "5.2 Image Caption Generation", "text": "We next consider the problem of image caption generation, which is a conditional RNN model, where image features are extracted by residual network (He et al., 2016), and then fed into the RNN to generate the caption. We present results on two benchmark datasets, Flickr8k (Hodosh et al., 2013) and Flickr30k (Young et al., 2014). These datasets contain 8,000 and 31,000 images, respectively. Each image is annotated with 5 sentences. A single-layer LSTM is employed with the number of hidden units set to 512.\nThe widely used BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004), and CIDEr-D (Vedantam et al., 2015) metrics are used to evaluate the performance. All the metrics are computed by using the code released by the COCO evaluation server (Chen et al., 2015b).\nTable 4 presents results for pSGLD/RMSprop with or without dropout. Consistent with the results in the basic language modeling, pSGLD\nyields improved performance compared to RMSprop. For example, pSGLD provides 2.77 BLEU-4 score improvement over RMSprop on the Flickr8k dataset. By comparing pSGLD with RMSprop with dropout, we conclude that pSGLD exhibits better regularization ability than dropout on these two datasets.\nApart from modeling weight uncertainty, different samples from our algorithm may capture different aspects of the input image. An example with two images is shown in Fig. 4, where 2 randomly chosen model samples are considered for each image. For each model sample, the top 3 generated captions are presented. We use the beam search approach (Vinyals et al., 2015) to generate captions, with a beam of size 5. In Fig. 4, the two samples for the first image mainly differ in the color and activity of the dog, e.g., \u201ctan\u201d or \u201cyellow\u201d, \u201cplaying\u201d or \u201crunning\u201d, whereas for the second image, the two samples reflect different understanding of the image content.\nTable 5: Sentence classification errors on five benchmark datasets.\nMethods MR CR SUBJ MPQA TREC RMSprop 21.26\u00b11.45 22.70\u00b12.20 8.13\u00b11.19 10.60\u00b11.28 8.58\u00b10.79 RMSprop + Dropout 20.33\u00b10.67 20.70\u00b12.22 7.24\u00b10.86 10.66\u00b10.74 7.82\u00b10.66 RMSprop + Gal\u2019s Dropout 19.66\u00b10.60 20.21\u00b12.34 7.52\u00b11.17 10.59\u00b11.12 8.32\u00b10.52 pSGLD 20.36\u00b11.09 21.18\u00b11.90 7.43\u00b11.21 10.54\u00b10.99 7.88\u00b10.72 pSGLD + Dropout 19.48\u00b10.95 19.74\u00b12.03 6.61\u00b11.06 10.22\u00b10.89 7.32\u00b10.66\n5 10 15 #Epoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nEr ro\nr\nTrain RMSprop RMSprop + Dropout pSGLD pSGLD + Dropout\n5 10 15 #Epoch\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\n0.26\nEr ro\nr\nValidation\n5 10 15 #Epoch\n0.10\n0.15 0.20 Er ro r\nTest\nFigure 5: Learning curves on TREC dataset."}, {"heading": "5.3 Sentence Classification", "text": "We study the task of sentence classification on 5 datasets as in (Kiros et al., 2015): TREC (Li and Roth, 2002), MR (Pang and Lee, 2005), SUBJ (Pang and Lee, 2004), CR (Hu and Liu, 2004) and MPQA (Wiebe et al., 2005). A detailed description of the datasets is provided in the Supplementary Material. A single-layer bidirectional LSTM is employed with the number of hidden units set to 400. Table 5 shows the testing classification errors. 10-fold cross-validation is used for evaluation on the first 4 datasets, while TREC has a pre-defined training/test split, and we run each algorithm 10 times on TREC. In addition to (naive) dropout, we further compare pSGLD with the Gal\u2019s dropout, recently proposed in (Gal and Ghahramani, 2016b), which is shown to be applicable to recurrent layers. The combination of pSGLD and dropout consistently provides the lowest errors.\nIn the following, we focus on the analysis of TREC. Each sentence of TREC is a question, and the goal is to decide which topic type the question is most related to: location, human, numeric, abbreviation, entity or description. Fig. 5 plots the learning curves of different algorithms on the training, validation and testing sets of the TREC dataset. pSGLD and dropout have similar behavior: they explore the parameter space during learning, and thus coverge slower than RSMprop on the training dataset. However, the learned uncertainty\nalleviates overfitting and results in lower errors on the validation and testing datasets.\nTo further study the Bayesian nature of the proposed approach, in Fig. 6 we choose two testing sentences with high uncertainty (i.e., standard derivation in prediction) from the TREC dataset. Interestingly, after embedding to 2d-space with tSNE (Van der Maaten and Hinton, 2008), the two sentences correspond to points lying on the boundary of different classes. We use 20 model samples to estimate the prediction mean and standard derivation on the true type and predicted type. The classifier yields higher probability on the wrong types, associated with higher standard derivations. One can leverage the uncertainty information to make decisions: either manually make a human judgement when uncertainty is high, or automatically choose the one with lower standard derivations when both types exhibits similar prediction means. A more rigorous usage of the uncertainty information is left as future work."}, {"heading": "6 Conclusion", "text": "We propose a scalable Bayesian learning framework using SG-MCMC, to model weight uncertainty in recurrent neural networks. The learn-\ning framework is tested on several tasks, including language models, image caption generation and sentence classification. Our algorithm outperforms conventional stochastic optimization algorithms, indicating the importance of learning weight uncertainty in recurrent neural networks. Our algorithm requires little additional computational overhead in training, and multiple times of forward-passing for model averaging in testing. Future works include improving the testing efficiency for the large-scale RNNs, via learning a single neural network that approximates the model averaging result (Korattikara et al., 2015)."}, {"heading": "Acknowledgments", "text": "This research was supported in part by ARO, DARPA, DOE, NGA, ONR and NSF."}, {"heading": "A Gated Recurrent Units", "text": "Similar to the LSTM unit, the GRU (Cho et al., 2014) has gating units that modulate the flow of information inside the unit, however, without using a separate memory cell. Specifically, the GRU has two gates: the reset gate rt and the update gate zt. The hidden units ht are updated as follows:\nrt = \u03c3(Wrxt +Urht\u22121 + br) , (9)\nzt = \u03c3(Wzxt +Uzht\u22121 + bz) , (10)\nh\u0303t = tanh(Whxt +Uh(rt ht\u22121) + bh) , (11)\nht = (1\u2212 zt) ht\u22121 + zt h\u0303t , (12)\nwhere \u03c3(\u00b7) denotes the logistic sigmoid function, and represents the element-wise multiply operator. W{r,z,h} are encoding weights, and U{r,z,h} are recurrent weights. b{r,z,h} are bias terms."}, {"heading": "B Model Details", "text": "B.1 Standard Language Modeling\nFor an input sequence X = {x1, . . . ,xT }, where xt is the input data vector at time t, we define an output sequence Y = {y1, . . . ,yT } with yt = xt+1 for t = 1, . . . , T \u2212 1. x1 and yT are always set to a special START and END token, respectively. The probability p(Y|X) is defined as\np(Y|X) = T\u220f t=1 p(yt|x\u2264t) = T\u220f t=1 p(yt|ht) . (13)\nAt each time t, there is a decoding function p(yt|ht) = softmax(Vht) to compute the distribution over words, where V are the decoding weights. The hidden states are recursively updated by ht = H(ht\u22121,xt), where H is a nonlinear function such as the LSTM or GRU defined above.\nB.2 Image Captioning\nImage caption generation is considered as a conditional language modeling problem, where image features are first extracted by residual network (He et al., 2016) as a preprocessing step, and then fed into the RNN to generate the caption. Denote z as the image feature vector, using the same notation as in standard language modeling, the probablity p(Y|X, z) is defined as\np(Y|X, z) = T\u220f t=1 p(yt|x\u2264t, z) = T\u220f t=1 p(yt|ht) .\nThe only difference with a standard language model is that at the first time step, we use the image feature vector z to update h1 = H(h0,x1, z). h0 is set to a zero vector. The hidden states at other time steps are recursively updated by ht = H(ht\u22121,xt), as in standard language modeling. Note that the image feature vector z is only used to generate the first word, which works better in practice than when being used at each time step of the RNN (Vinyals et al., 2015)."}, {"heading": "C SGLD Algorithm", "text": "We list the SGLD algorithm below for clarity.\nAlgorithm 2: SGLD Input: Learning rate schedule {\u03b7t}Tt=1. Initialize: \u03b81 \u223c N (0, I) ; for t = 1, 2, . . . , T do\n% Estimate gradient from minibatch Sm f\u0303t = \u2207U\u0303t(\u03b8); % Parameter update \u03bet \u223c N (0, \u03b7tI); \u03b8t+1\u2190 \u03b8t + \u03b7t2 f\u0303t+ \u03bet;\nend"}, {"heading": "D Experimental Setup", "text": "For RNN training, orthogonal initialization is employed on all recurrent matrices (Saxe et al., 2014). Non-recurrent weights are initialized from a uniform distribution in [\u22120.01, 0.01]. All the bias terms are initialized to zero. It is observed that setting a high initial forget gate bias for LSTMs can give slightly better results (Le et al., 2015). Hence, the initial forget gate bias is set to 3 throughout the experiments. Word vectors are initialized with the publicly available word2vec vectors (Mikolov et al., 2013). These vectors have dimensionality 300 and were trained using a continuous bag-of-words architecture . Words not present in the set of pre-trained words are initialized at random. Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014).\nThe hyperparameters for the algorithm include stepsize, minibatch size, thinning interval, number of burn-in epochs and variance of the Gaussian priors. We explain some hyperparameters that are unique in pSGLD and list the specific values used in our experiments in Table 6. RMSprop employs the same hyperparameter setting as pSGLD. Throughout the experiments, the dropout rate is set to 0.5.\nThe learning rate for SGD and SGLD (used in the word-level language modeling) is set to 1. For\nthe small LSTM model, we halved the learning rate every epoch after the 4th. For the medium LSTM model, we decrease the learning rate by a factor of 1.2 every epoch after the 6th. For the large LSTM model, we decrease the learning rate by a factor of 1.15 every epoch after the 15th. This setup is also used in (Zaremba et al., 2014). However, we did not use the final hidden states of the current mini-batch as the initial hidden state of the subsequent mini-batch. For every sequence in every mini-batch, we initialize the hidden states to zero.\nVariance of Gaussian Prior The prior distributions on the weights of RNNs are Gaussian, with mean 0 and variance \u03c32. The variance of this Gaussian distribution determines the prior belief of how strongly these weights should concentrate on 0. A larger variance in the prior leads to a wider range of weight choices, thus higher uncertainty. We set \u03c32 to 1 throughout the experiments.\nBurn-in To obtain a good initialization for parameter samples from regions of higher probability, we dispose of samples at the beginning of an MCMC run, prior to collection, this is called \u201cburn-in\u201d. We provide the number of burn-in on each dataset in Table 6.\nThinning Due to the fact of high autocorrelation time between samples in SG-MCMC methods, we suggest to thin the Markov chain which leaves fewer, less correlated samples. As with conventional MCMC, these thinned samples have a lower autocorrelation time and can help maintain a higher effective sample size while reducing the computational burden."}, {"heading": "E Details of Classification Datasets", "text": "We test SG-MCMC methods on various benchmark datasets for sentence classification. Summary statistics of the datasets are in Table 7. For datasets without a standard validation set, we ran-\ndomly select 10% of the training data as the validation set.\n\u2022 TREC: This task involves classifying a question into 6 types (Li and Roth, 2002).\n\u2022 MR: Movie reviews with one sentence per review. Classification involves predicting positive/negative reviews (Pang and Lee, 2005).\n\u2022 SUBJ: Subjectivity dataset where the task is to classify a sentence as being subjective or objective (Pang and Lee, 2004).\n\u2022 CR: Customer reviews of various products. This task is to predict positive/negative reviews (Hu and Liu, 2004).\n\u2022 MPQA: Opinion polarity detection subtask of the MPQA dataset (Wiebe et al., 2005)."}], "references": [{"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": "In ACL workshop", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "On fast dropout and its applicability to recurrent networks. arXiv:1311.0701", "author": ["Bayer et al.2013] J. Bayer", "C. Osendorfer", "D. Korhammer", "N. Chen", "S. Urban", "P. van der Smagt"], "venue": null, "citeRegEx": "Bayer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bayer et al\\.", "year": 2013}, {"title": "Where to apply dropout in recurrent neural networks for handwriting recognition? In ICDAR", "author": ["Bluche et al.2015] T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": null, "citeRegEx": "Bluche et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bluche et al\\.", "year": 2015}, {"title": "Weight uncertainty in neural networks", "author": ["Blundell et al.2015] C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"], "venue": null, "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "Large-scale machine learning with stochastic gradient descent. In COMPSTAT", "author": ["L Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q2010\\E", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Stochastic gradient Hamiltonian Monte Carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "On the convergence of stochastic gradient MCMC algorithms with high-order integrators", "author": ["Chen et al.2015a] C. Chen", "N. Ding", "L. Carin"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "2015b. Microsoft coco captions: Data collection and evaluation", "author": ["Chen et al.2015b] Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Bridging the gap between", "author": ["C. Chen", "D. Carlson", "Z. Gan", "C. Li", "L. Carin"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Bayesian sampling using stochastic gradient thermostats", "author": ["Ding et al.2014] N. Ding", "Y. Fang", "R. Babbush", "C. Chen", "R.D. Skeel", "H. Neven"], "venue": null, "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization. JMLR", "author": ["Duchi et al.2011] J. Duchi", "E. Hazan", "Y. Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Ghahramani2016a] Y. Gal", "Z. Ghahramani"], "venue": null, "citeRegEx": "Gal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2016}, {"title": "2016b. A theoretically grounded application of dropout in recurrent neural networks", "author": ["Gal", "Ghahramani2016b] Y. Gal", "Z. Ghahramani"], "venue": null, "citeRegEx": "Gal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2016}, {"title": "Scalable deep poisson factor analysis for topic modeling", "author": ["Gan et al.2015] Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin"], "venue": null, "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "In NIPS", "citeRegEx": "Graves.,? \\Q2011\\E", "shortCiteRegEx": "Graves.", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["He et al.2016] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "author": ["Hern\u00e1ndez-Lobato", "Adams2015] J.M. Hern\u00e1ndezLobato", "R.P. Adams"], "venue": null, "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton et al.2012] G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "In Neural computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics. JAIR", "author": ["Hodosh et al.2013] M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": null, "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Mining and summarizing customer reviews. SIGKDD", "author": ["Hu", "Liu2004] M. Hu", "B. Liu"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Visualizing and understanding recurrent networks", "author": ["Karpathy et al.2016] A. Karpathy", "J. Johnson", "L. Fei-Fei"], "venue": "In ICLR Workshop", "citeRegEx": "Karpathy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization. In ICLR", "author": ["Kingma", "Ba2015] D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Kingma et al.2015] D. Kingma", "T. Salimans", "M. Welling"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Bayesian dark knowledge", "author": ["V. Rathod", "K. Murphy", "M. Welling"], "venue": null, "citeRegEx": "Korattikara et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Korattikara et al\\.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units. arXiv:1504.00941", "author": ["Le et al.2015] Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Preconditioned stochastic gradient Langevin dynamics for deep neural networks", "author": ["Li et al.2016a] C. Li", "C. Chen", "D. Carlson", "L. Carin"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Learning weight uncertainty with stochastic gradient MCMC for shape classification", "author": ["Li et al.2016b] C. Li", "A. Stevens", "C. Chen", "Y. Pu", "Z. Gan", "L. Carin"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In ACL workshop", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "A practical Bayesian framework for backpropagation networks", "author": ["D.J.C. MacKay"], "venue": null, "citeRegEx": "MacKay.,? \\Q1992\\E", "shortCiteRegEx": "MacKay.", "year": 1992}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Mikolov et al.2010] T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013] T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Rnndrop: A novel dropout for rnns in asr. ASRU", "author": ["Moon et al.2015] T. Moon", "H. Choi", "H. Lee", "I. Song"], "venue": null, "citeRegEx": "Moon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moon et al\\.", "year": 2015}, {"title": "Bayesian learning for neural networks", "author": ["R.M. Neal"], "venue": "PhD thesis,", "citeRegEx": "Neal.,? \\Q1995\\E", "shortCiteRegEx": "Neal.", "year": 1995}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["L. Vilnis", "Q. Le", "I. Sutskever", "L. Kaiser", "K. Kurach", "J. Martens"], "venue": "In ICLR workshop", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Regularization and nonlinearities for neural language models: when are they needed? arXiv:1301.5650", "author": ["Pachitariu", "Sahani2013] M. Pachitariu", "M. Sahani"], "venue": null, "citeRegEx": "Pachitariu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pachitariu et al\\.", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Lee2004] B. Pang", "L. Lee"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating", "author": ["Pang", "Lee2005] B. Pang", "L. Lee"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu et al.2013] R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham et al.2014] V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "ICFHR", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "A stochastic approximation method. In The annals of mathematical statistics", "author": ["Robbins", "Monro1951] H. Robbins", "S. Monro"], "venue": null, "citeRegEx": "Robbins et al\\.,? \\Q1951\\E", "shortCiteRegEx": "Robbins et al\\.", "year": 1951}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In ICLR", "author": ["Saxe et al.2014] A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": null, "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Recurrent dropout without memory loss", "author": ["A. Severyn", "E. Barth"], "venue": null, "citeRegEx": "Semeniuta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Semeniuta et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["J. Martens", "G.E. Hinton"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Consistency and fluctuations for stochastic gradient Langevin dynamics. JMLR", "author": ["Teh et al.2016] Y.W. Teh", "A.H. Thi\u00e9ry", "S.J. Vollmer"], "venue": null, "citeRegEx": "Teh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. Coursera: Neural Networks for Machine Learning", "author": ["Tieleman", "Hinton2012] T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Visualizing data using tSNE", "author": ["Van der Maaten", "Hinton2008] L. Van der Maaten", "G.E. Hinton"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Cider: Consensus-based image description evaluation", "author": ["C Lawrence Zitnick", "Devi Parikh"], "venue": null, "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals et al.2015] O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Regularization of neural networks using DropConnect", "author": ["Wan et al.2013] L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": null, "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Fast Dropout training", "author": ["Wang", "Manning2013] S. Wang", "C. Manning"], "venue": "In ICML", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["Welling", "Teh2011] M. Welling", "Y.W. Teh"], "venue": null, "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P. Werbos"], "venue": "In Proceedings of the IEEE", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Annotating expressions of opinions and emotions in language. Language resources and evaluation", "author": ["Wiebe et al.2005] J. Wiebe", "T. Wilson", "C. Cardie"], "venue": null, "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL", "author": ["Young et al.2014] P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Zaremba et al.2014] W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Experimental Setup For RNN training, orthogonal initialization is employed on all recurrent matrices (Saxe et", "author": ["D end"], "venue": null, "citeRegEx": "end,? \\Q2014\\E", "shortCiteRegEx": "end", "year": 2014}], "referenceMentions": [{"referenceID": 33, "context": "Recently, recurrent neural networks (RNNs) have shown promising performance on this task (Mikolov et al., 2010; Sutskever et al., 2011).", "startOffset": 89, "endOffset": 135}, {"referenceID": 48, "context": "Recently, recurrent neural networks (RNNs) have shown promising performance on this task (Mikolov et al., 2010; Sutskever et al., 2011).", "startOffset": 89, "endOffset": 135}, {"referenceID": 58, "context": "RNNs are usually trained via back-propagation through time (Werbos, 1990), using stochastic optimization methods such as stochastic gradient descent (SGD) (Robbins and Monro, 1951); stochastic methods of this type are particularly important for training with large data sets.", "startOffset": 59, "endOffset": 73}, {"referenceID": 3, "context": "The MAP solution is a single point estimate, ignoring weight uncertainty (Blundell et al., 2015; Hern\u00e1ndezLobato and Adams, 2015).", "startOffset": 73, "endOffset": 129}, {"referenceID": 31, "context": "In the neural network literature, Bayesian learning has been proposed as a principled method to impose regularization and incorporate model uncertainty (MacKay, 1992; Neal, 1995), by imposing prior distributions on model parameters.", "startOffset": 152, "endOffset": 178}, {"referenceID": 36, "context": "In the neural network literature, Bayesian learning has been proposed as a principled method to impose regularization and incorporate model uncertainty (MacKay, 1992; Neal, 1995), by imposing prior distributions on model parameters.", "startOffset": 152, "endOffset": 178}, {"referenceID": 36, "context": "Due to the intractability of posterior distributions in neural networks, Hamiltonian Monte Carlo (HMC) (Neal, 1995) has been used to provide sample-based approximations to the true posterior.", "startOffset": 103, "endOffset": 115}, {"referenceID": 5, "context": "This paper seeks to scale up Bayesian learning of RNNs to meet the challenge of the increasing amount of \u201cbig\u201d sequential data in natural language processing, by leveraging recent advances in stochastic gradient Markov Chain Monte Carlo (SG-MCMC) algorithms (Welling and Teh, 2011; Chen et al., 2014; Ding et al., 2014; Li et al., 2016a).", "startOffset": 258, "endOffset": 337}, {"referenceID": 11, "context": "This paper seeks to scale up Bayesian learning of RNNs to meet the challenge of the increasing amount of \u201cbig\u201d sequential data in natural language processing, by leveraging recent advances in stochastic gradient Markov Chain Monte Carlo (SG-MCMC) algorithms (Welling and Teh, 2011; Chen et al., 2014; Ding et al., 2014; Li et al., 2016a).", "startOffset": 258, "endOffset": 337}, {"referenceID": 37, "context": "This procedure was also empirically found effective in (Neelakantan et al., 2016).", "startOffset": 55, "endOffset": 81}, {"referenceID": 50, "context": "(iii) In theory, both asymptotic and non-asymptotic consistency properties of SG-MCMC methods in posterior estimation have been recently established to guarantee convergence (Chen et al., 2015a; Teh et al., 2016).", "startOffset": 174, "endOffset": 212}, {"referenceID": 16, "context": "These come in two broad categories: stochastic variational inference (Graves, 2011; Blundell et al., 2015; Hern\u00e1ndez-Lobato and Adams, 2015) and SG-MCMC methods (Korattikara et al.", "startOffset": 69, "endOffset": 140}, {"referenceID": 3, "context": "These come in two broad categories: stochastic variational inference (Graves, 2011; Blundell et al., 2015; Hern\u00e1ndez-Lobato and Adams, 2015) and SG-MCMC methods (Korattikara et al.", "startOffset": 69, "endOffset": 140}, {"referenceID": 26, "context": ", 2015; Hern\u00e1ndez-Lobato and Adams, 2015) and SG-MCMC methods (Korattikara et al., 2015; Li et al., 2016a).", "startOffset": 62, "endOffset": 106}, {"referenceID": 19, "context": "Dropout (Hinton et al., 2012; Srivastava et al., 2014) is a commonly used regularization method for training neural networks.", "startOffset": 8, "endOffset": 54}, {"referenceID": 47, "context": "Dropout (Hinton et al., 2012; Srivastava et al., 2014) is a commonly used regularization method for training neural networks.", "startOffset": 8, "endOffset": 54}, {"referenceID": 1, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 43, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 61, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 2, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 35, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 46, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 61, "context": "Among them, naive dropout (Zaremba et al., 2014) can impose weight uncertainty only on encoding weights (those that connect input to hidden units) and decoding weights (those that connect hidden units to output), but not the recurrent weights (those that connect consecutive hidden states).", "startOffset": 26, "endOffset": 48}, {"referenceID": 24, "context": "Dropout has been recently shown to be a variational approximation technique in Bayesian learning (Gal and Ghahramani, 2016a; Kingma et al., 2015).", "startOffset": 97, "endOffset": 145}, {"referenceID": 9, "context": "The transition function H(\u00b7) can be implemented with a gated activation function, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or a Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 187, "endOffset": 205}, {"referenceID": 10, "context": "It has been shown that a GRU can achieve similar performance to an LSTM in sequence modeling (Chung et al., 2014).", "startOffset": 93, "endOffset": 113}, {"referenceID": 23, "context": "We also extend this basic language model to consider other applications: (i) a character-level language model can be specified in a similar manner by replacing words with characters (Karpathy et al., 2016).", "startOffset": 182, "endOffset": 205}, {"referenceID": 54, "context": "(ii) Image captioning can be considered as a conditional language modeling problem, in which we learn a generative language model of the caption conditioned on an image (Vinyals et al., 2015).", "startOffset": 169, "endOffset": 191}, {"referenceID": 5, "context": "To help convergence, a momentum term has been introduced in SGHMC (Chen et al., 2014), a \u201cthermostat\u201d has been devised in SGNHT (Ding et al.", "startOffset": 66, "endOffset": 85}, {"referenceID": 11, "context": ", 2014), a \u201cthermostat\u201d has been devised in SGNHT (Ding et al., 2014; Gan et al., 2015) and preconditioners have been employed in pSGLD (Li et al.", "startOffset": 50, "endOffset": 87}, {"referenceID": 15, "context": ", 2014), a \u201cthermostat\u201d has been devised in SGNHT (Ding et al., 2014; Gan et al., 2015) and preconditioners have been employed in pSGLD (Li et al.", "startOffset": 50, "endOffset": 87}, {"referenceID": 8, "context": "These SG-MCMC algorithms often share similar characteristics with their counterpart approaches from the optimization literature such as the momentum SGD, Santa (Chen et al., 2016) and RMSprop/Adagrad (Tieleman and Hinton, 2012; Duchi et al.", "startOffset": 160, "endOffset": 179}, {"referenceID": 12, "context": ", 2016) and RMSprop/Adagrad (Tieleman and Hinton, 2012; Duchi et al., 2011).", "startOffset": 28, "endOffset": 75}, {"referenceID": 4, "context": "SGD is guaranteed to converge to a local minimum under mild conditions (Bottou, 2010).", "startOffset": 71, "endOffset": 85}, {"referenceID": 42, "context": "This is important for RNNs, whose parameter space often exhibits pathological curvature and saddle points (Pascanu et al., 2013), resulting in slow mixing.", "startOffset": 106, "endOffset": 128}, {"referenceID": 12, "context": "There are multiple choices of preconditioners; similar ideas in optimization include Adagrad (Duchi et al., 2011), Adam (Kingma and Ba, 2015) and RMSprop (Tieleman and Hinton, 2012).", "startOffset": 93, "endOffset": 113}, {"referenceID": 47, "context": "To further understand SG-MCMC, we show its close connection to dropout/dropConnect (Srivastava et al., 2014; Wan et al., 2013).", "startOffset": 83, "endOffset": 126}, {"referenceID": 55, "context": "To further understand SG-MCMC, we show its close connection to dropout/dropConnect (Srivastava et al., 2014; Wan et al., 2013).", "startOffset": 83, "endOffset": 126}, {"referenceID": 23, "context": "\u2022 Following (Karpathy et al., 2016), we test character-level language modeling on the War and Peace (WP) novel.", "startOffset": 12, "endOffset": 35}, {"referenceID": 32, "context": "\u2022 The Penn Treebank (PTB) corpus (Marcus et al., 1993) is used for word-level language modeling.", "startOffset": 33, "endOffset": 54}, {"referenceID": 61, "context": "The results reported here do not match (Zaremba et al., 2014) due to the differences of experimental setup.", "startOffset": 39, "endOffset": 61}, {"referenceID": 17, "context": "We next consider the problem of image caption generation, which is a conditional RNN model, where image features are extracted by residual network (He et al., 2016), and then fed into the RNN to generate the caption.", "startOffset": 147, "endOffset": 164}, {"referenceID": 21, "context": "We present results on two benchmark datasets, Flickr8k (Hodosh et al., 2013) and Flickr30k (Young et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 60, "context": ", 2013) and Flickr30k (Young et al., 2014).", "startOffset": 22, "endOffset": 42}, {"referenceID": 41, "context": "The widely used BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004), and CIDEr-D (Vedantam et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 30, "context": ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004), and CIDEr-D (Vedantam et al.", "startOffset": 51, "endOffset": 62}, {"referenceID": 53, "context": ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004), and CIDEr-D (Vedantam et al., 2015) metrics are used to evaluate the performance.", "startOffset": 76, "endOffset": 99}, {"referenceID": 54, "context": "We use the beam search approach (Vinyals et al., 2015) to generate captions, with a beam of size 5.", "startOffset": 32, "endOffset": 54}, {"referenceID": 59, "context": ", 2015): TREC (Li and Roth, 2002), MR (Pang and Lee, 2005), SUBJ (Pang and Lee, 2004), CR (Hu and Liu, 2004) and MPQA (Wiebe et al., 2005).", "startOffset": 118, "endOffset": 138}, {"referenceID": 26, "context": "Future works include improving the testing efficiency for the large-scale RNNs, via learning a single neural network that approximates the model averaging result (Korattikara et al., 2015).", "startOffset": 162, "endOffset": 188}], "year": 2016, "abstractText": "Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach over stochastic optimization.", "creator": "LaTeX with hyperref package"}}}