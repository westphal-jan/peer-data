{"id": "1611.09430", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Emergence of foveal image sampling from learning to attend in visual scenes", "abstract": "We how a neural attention model with a learnable retinal sampling formula_3. The model yet agents while he characteristics data process necessary an evaluated. particular any embedded throughout a uses scene roiling themes codice_42 types long smallest different of fixations. We explore the tiling properties that emerge after the model ' s ocular precise lattice entered training. Specifically, believe making actually all formula_7 miniature when eccentricity dependent sampling lattice of is ashkenazic fluorosis, with a degree intended territory entire held rejang encampment meanwhile a low resolution periphery. Furthermore, indeed find possible except include emergent large were updrafts or starting funding gleaned to their formula_3.", "histories": [["v1", "Mon, 28 Nov 2016 23:39:20 GMT  (794kb,D)", "http://arxiv.org/abs/1611.09430v1", null], ["v2", "Sat, 21 Oct 2017 08:26:16 GMT  (792kb,D)", "http://arxiv.org/abs/1611.09430v2", "Published as a conference paper at ICLR 2017"]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["brian cheung", "eric weiss", "bruno olshausen"], "accepted": true, "id": "1611.09430"}, "pdf": {"name": "1611.09430.pdf", "metadata": {"source": "CRF", "title": "EMERGENCE OF FOVEAL IMAGE SAMPLING FROM LEARNING TO ATTEND IN VISUAL SCENES", "authors": ["Brian Cheung", "Eric Weiss", "Bruno Olshausen"], "emails": ["bcheung@berkeley.edu", "eaweiss@berkeley.edu", "baolshausen@berkeley.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "A striking design feature of the primate retina is the manner in which images are spatially sampled by retinal ganglion cells. Sample spacing and receptive fields are smallest in the fovea and then increase linearly with eccentricity, as shown in Figure 1. Thus, we have highest spatial resolution at the center of fixation and lowest resolution in the periphery, with a gradual fall-off in resolution as one proceeds from the center to periphery. The question we attempt to address here is, why is the retina designed in this manner - i.e., how is it beneficial to vision?\nThe commonly accepted explanation for this eccentricity dependent sampling is that it provides us with both high resolution and broad coverage of the visual field with a limited amount of neural resources. The human retina contains 1.5 million ganglion cells, whose axons form the sole output of the retina. These essentially constitute about 300,000 distinct samples of the image due to the multiplicity of cell types coding different aspects such as on vs. off channels (Van Essen & Anderson, 1995). If these were packed uniformly at highest resolution (120 samples/deg, the Nyquist-dictated sampling rate corresponding to the spatial-frequencies admitted by the lens), they would subtend an image area spanning just 5x5 deg2. Thus we would have high-resolution but essentially tunnel vision. Alternatively if they were spread out uniformly over the entire monocular visual field spanning roughly 150 deg2 we would have wide field of coverage but with very blurry vision, with each sample subtending 0.25 deg (which would make even the largest letters on a Snellen eye chart illegible). Thus, the primate solution makes intuitive sense as a way to achieve the best of both of these worlds. However we are still lacking a quantitative demonstration that such a sampling strategy emerges as the optimal design for subserving some set of visual tasks.\nHere, we explore what is the optimal retinal sampling lattice for an (overt) attentional system performing a simple visual search task requiring the classification of an object. We propose a learnable retinal sampling lattice to explore what properties are best suited for this task. While evolutionary pressure has tuned the retinal configurations found in the primate retina, we instead utilize gradient descent optimization for our in-silico model by constructing a fully differentiable dynamically controlled model of attention.\nOur choice of visual search task follows a paradigm widely used in the study of overt attention in humans and other primates (Geisler & Cormack, 2011). In many forms of this task, a single target is randomly located on a display among distractor objects. The goal of the subject is to find the target as rapidly as possible. Itti & Koch (2000) propose a selection mechanism based on manually\nar X\niv :1\n61 1.\n09 43\n0v 1\n[ cs\n.N E\n] 2\n8 N\nov 2\n01 6\nIll? V. A. Perry PI al. cells is shown in Fig. Q(B). In this case the slopes of the regression lines of cell body size on the logarithm of cell density for the nasal (a = 43.4, h = -6.87) and the temporal cells (u = 53.6, h = 9.1) were not significantly different (r = 1.0, df\u2018 54). Thus the differences in cell body size of PX cells in different parts of the retina is clearly related to the variation in ganglion cell density, although the differences in dendritic field size is not simply related to cell density alone. The correlation coefficient for the dendritic field size of both nasal and temporal Pa cells and the logarithm of the ganglion cell density was 0.97, i.e. most of the variance in dendritic field size can be attributed to differences in ganglion cell density. As a check on the accuracy of our ganglion cell counts we used them to estimate the total number of ganglion cells in the macaque retina which can, in turn, be compared with published estimation of the number of axons in the optic nerve. To do this we plotted onto a scaled drawing of one Nissl-stained whole-mounted retina (670mm\u2019 in area) the isodensity contours estimated from the counts along the A meridia. The isodensity contours were drawn as approximately ellipsoid but tapering in the nasal retina as has been shown in previous studies of peripheral ganglion cell counts of the primate retina.6\u2019,\u201cx The area between adjacent contours was multiplied by the mean ceil density of the two isodensity contours. This was repeated for the area between all pairs of contours and the totals for all areas were summed to give the total number of ganglion cells. This yielded an estimate of 1.4 x 10\u201d ganglion cells in the retina, in good agreement with estimates of the number of axons in the optic nerve i.e. 1.5-1.8 x IOh, I.4 x I O' and 1.2-1.3 x 10\u201d.40~so.\u201d If we neglect the naso-temporal overlap, which is small in primates,h.h\u2019 then approximately 60\u20191/ of the cells lie in the nasal retina and 40\u201c;) in the temporal retina. Our intention was to show which cell types project to the magno- and parvocellufar layers of the lateraf geniculate nucleus and to estimate the percentage of retinal ganglion cells at different eccentricities which\n0 1 2 3 L 5 6 7 8 9 10 li 12 13 I\u2018 ECCENTRICITY fTH?\ndefined low level features of real images to locate various search targets. Here the neural network must learn what features are most informative for directing attention.\nWhile neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision. An important property which distinguishes neural networks from most other neurobiological models is their ability to learn internal (latent) features directly from data.\nBut existing neural network models specify the input sampling lattice a priori. Larochelle & Hinton (2010) employ an eccentricity dependent sampling lattice mimicking the primate retina, and Mnih et al. (2014) utilize a multi scale glimpse window\u2019 that forms a piece-wise approximation of this scheme. While it seems reasonable to think that these design choices contribute to the good performance of these systems, it remains to be seen if this arrangement emerges as the optimal solution.\nWe further extend the learning paradigm of neural networks to the structural features of the glimpse mechanism of an attention model. To explore emergent properties of our learned retinal configurations, we train on artificial datasets where the factors of variation are easily controllable. Despite this departure from biology and natural stimuli, we find our model learns to create an eccentricity dependent layout where a distinct central region of high acuity emerges surrounded by a low acuity periphery. We show that the properties of this layout are highly dependent on the variations present in the task constraints. When we depart from physiology by augmenting our attention model with the ability to spatially rescale or zoom on its input, we find our model learns a more uniform layout which has properties more similar to the glimpse window proposed in Jaderberg et al. (2015); Gregor et al. (2015). These findings help us to understand the task conditions and constraints in which an eccentricity dependent sampling lattice emerges."}, {"heading": "2 RETINAL TILING IN NEURAL NETWORKS WITH ATTENTION", "text": "Attention in neural networks may be formulated in terms of a differentiable feedforward function. This allows the parameters of these models to be trained jointly with backpropagation. Most formulations of visual attention over the input image assume some structure in the kernel filters. For example, the recent attention models proposed by Jaderberg et al. (2015); Mnih et al. (2014); Gregor et al. (2015); Ba et al. (2014) assume each kernel filter lies on a rectangular grid. To create a learnable retinal sampling lattice, we relax this assumption by allowing the kernels to tile the image independently."}, {"heading": "2.1 GENERATING A GLIMPSE", "text": "We interpret a glimpse as a form of routing where a subset of the visual scene U is sampled to form a smaller output glimpse G. The routing is defined by a set of kernels k[\u2022](s), where each kernel i specifies which part of the input U [\u2022] will contribute to a particular output G[i]. A control variable s\nis used to control the routing by adjusting the position and scale of the entire array of kernels. With this in mind, many attention models can be reformulated into a generic equation written as\nG[i] = H\u2211 n W\u2211 m U [m,n]k[m,n, i](s) (1)\nwhere m and n index input pixels of U and i indexes output glimpse features. The pixels in the input image U are thus mapped to a smaller glimpse G."}, {"heading": "2.2 RETINAL GLIMPSE", "text": "The centers of each kernel filter \u00b5\u0301[i] are calculated with respect to control variables sc and sz and learnable offset \u00b5[i]. The control variables specify the position and zoom of the entire glimpse. \u00b5[i] and \u03c3[i] specify the position and spread respectively of an individual kernel k[\u2212,\u2212, i]. These parameters are learned during training with backpropagation. We describe how the control variables are computed in the next section. The kernels are thus specified as follows:\n\u00b5\u0301[i] = (sc \u2212 \u00b5[i])sz (2) \u03c3\u0301[i] = \u03c3[i]sz (3)\nk[m,n, i](s) = N (m; \u00b5\u0301x[i], \u03c3\u0301[i])N (n; \u00b5\u0301y[i], \u03c3\u0301[i]) (4)\nWe assume kernel filters factorize between the horizontal m and vertical n dimensions of the input image. This factorization is shown in equation 4, where the kernel is defined as an isotropic gaussian N . For each kernel filter, given a center \u00b5\u0301[i] and scalar variance \u03c3\u0301[i], a two dimensional gaussian is defined over the input image as shown in Figure 2. These gaussian kernel filters can be thought of as a simplified approximation to the receptive fields of retinal ganglion cells in primates (Van Essen & Anderson, 1995).\nWhile this factored formulation reduces the space of possible transformations from input to output, it can still form many different mappings from an input U to output G. Figure 3A shows one possible retinal configuration which an input image can be mapped to an outputG. The yellow circles denote the central location of a particular kernel while the size denotes the its standard deviation. Each kernel maps to one of the outputs G[i].\nTraining defines structural adjustments to individual kernels which include its position in the lattice as well as its spread (standard deviation). These adjustments are only possible during training and are fixed afterwards. Training adjustments can be considered analagous to the incremental adjustments in the layout of the retinal sampling lattice which occur over many generations, directed by evolutionary pressure in biology.\nControl defines the motor adjustments of the attention model possible during inference after training as shown in Figure 3B. Positional control sc can be considered analogous to the motor control signals\nRecurrent ht\nGlimpse Gt\nControl sc,t ; sz,t\nB. Controlling the Retinal Lattice\nFinal LayoutInitial Layout Training A. Optimizing the Retinal Lattice Time t t+1 t+2 Recurrent ht+1 Glimpse Gt+1 Control sc,t+1 ; sz,t+1 Recurrent ht+2 Glimpse Gt+2 Control sc,t+2 ; sz,t+2\nFigure 3: A: Starting from an initial lattice configuration of a uniform grid of kernels, we learn an optmized configuration from data. B: Attentional fixations generated during inference in the model, shown unrolled in time (after training).\nwhich executes saccades of the eye, whereas sz would correspond to controlling a zoom lens in the eye (which has no counterpart in biology)."}, {"heading": "3 RECURRENT NEURAL ARCHITECTURE FOR ATTENTION", "text": "A glimpse at a specific timepoint, Gt, is processed by a fully-connected recurrent network frnn().\nht = frnn(Gt, ht\u22121) (5) [sc,t; sz,t] = fcontrol(ht) (6)\nThe global center sc,t and zoom sz,t are predicted by the control network fcontrol() which is parameterized by a fully-connected neural network.\nIn this work, we investigate three variants of the proposed recurrent model:\n\u2022 Fixed Lattice: The kernel parameters \u00b5[i] and \u03c3[i] for each retinal cell are not learnable. The model can only translate the kernel filters sc,t = fcontrol(ht) and the global zoom is fixed sz,t = 1.\n\u2022 Translation Only: Unlike the fixed lattice model, \u00b5[i] and \u03c3[i] are learnable (via backpropagation).\n\u2022 Translation and Zoom: This model follows equation 6 where it can both zoom and translate the kernels.\nA summary for comparing these variants is shown in Table 1.\nPrior to training, the kernel filters are initialized as a 12x12 grid (144 kernel filters), tiling uniformly over the central region of the input image. This creates a retinal sampling lattice as shown in Figure 5 before training. Our recurrent network, frnn is a two layer traditional recurrent network with 512- 512 units. Our control network, fcontrol is a fully-connected network with 512-3 units (x,y,zoom) in each layer. Similarly, our prediction networks are fully-connected networks with 512-10 units for predicting the class. We use ReLU non-linearities for all hidden unit layers.\nAll learnable components of our model are differentiable and trained end-to-end via backpropagation through time. Similar to Gregor et al. (2015), this allows us to train the control network indirectly from signals backpropagated from the task cost. For stochastic gradient descent optimization we use Adam (Kingma & Ba, 2014) and construct our models in Theano (Bastien et al., 2012)."}, {"heading": "4 DATASETS AND TASKS", "text": ""}, {"heading": "4.1 MODIFIED CLUTTERED MNIST DATASET", "text": "Example images from of our dataset are shown in Figure 4. Handwritten digits from the original MNIST dataset LeCun & Cortes (1998) are randomly placed over a 100x100 image with varying amounts of distractors (clutter). Distractors are generated by extracting random segments of nontarget MNIST digits which are placed randomly with uniform probability over the image. In contrast to the cluttered MNIST dataset proposed in Mnih et al. (2014), the number of distractors for each image varies randomly from 0 to 20 pieces. This prevents the attention model from learning a solution which depends on the number \u2018on\u2019 pixels in a given region. In addition, we create another dataset (Dataset 2) with an additional factor of variation: the original MNIST digit is randomly resized by a factor of 0.33x to 3.0x. Examples of this dataset are shown in the second row of Figure 4."}, {"heading": "4.2 VISUAL SEARCH TASK", "text": "We define our visual search task as a recognition task in a cluttered scene. The recurrent attention model we propose must output the class c\u0302 of the single MNIST digit appearing in the image via the prediction network fpredict(). The task loss, L, is specified in equation 8. To minimize the classification error, we use cross-entropy cost:\nc\u0302t,n = fpredict(ht,n) (7)\nL = N\u2211 n T\u2211 t cnlog(c\u0302t,n) (8)\nBefore Training (Initial Layout) After 1 epochs After 10 epochs After 100 epochs\nAnalolgous to the visual search experiments performed in physiological studies, we pressure our attention model to accomplish the visual search as quickly as possible. By applying the task loss to every timepoint, the model is forced to accurately recognize and localize the target MNIST digit in as few iterations as possible. In our classification experiments, the model is given T = 4 glimpses."}, {"heading": "5 RESULTS", "text": "Figure 5 shows the layouts of the learned kernels for a Translation Only model at different stages during training. The filters are smoothly transforming from a uniform grid of kernels to an eccentricity dependent lattice. Furthermore, the kernel filters spread their individual centers to create a sampling lattice which covers the full image. This is sensible as the target MNIST digit can appear anywhere in the image with uniform probability.\nWhen we include variable sized digits as an additional factor in the dataset, the translation only model shows an even greater diversity of variances for the kernel filters. This is shown visually in the first row of Figure 6. Furthermore, the second row shows a highly dependent relationship between the sampling interval and standard deviatoin of the retinal sampling lattice and eccentricity from the center. This dependency increases when training on variable sized MNIST digits (Dataset 2). This relationship has also been observed in the primate visual system (Perry et al., 1984; Van Essen & Anderson, 1995).\nWhen the proposed attention model is able to zoom its retinal sampling lattice, a very different layout emerges. There is much less diversity in the distribution of kernel filter variances as evidenced in Figure 6. Both the sampling interval and standard deviation of the retinal sampling lattice have far less of a dependence on eccentricity. As shown in the last column of Figure 6, we also trained this model on variable sized digits and noticed no significant differences in sampling lattice configuration.\nFigure 7 shows how each model variant makes use of its retinal sampling lattice after training. The strategy each variant adopts to solve the visual search task helps explain the drastic difference in lattice configuration. The translation only variant simply translates its high acuity region to recognize and localize the target digit. The translation and zoom model both rescales and translates its sampling lattice to fit the target digit. Remarkably, Figure 7 shows that both models detect the digit early on and make minor corrective adjustments in the following iterations.\nTable 2 compares the classification performance of each model variant on the cluttered MNIST dataset with fixed sized digits (Dataset 1). There is a significant drop in performance when the retinal sampling lattice is fixed and not learnable, confirming that the model is benefitting from learning the high-acuity region. The classification performance between the Translation Only and Translation and Zoom model is competitive. This supports the hypothesis that the functionality of a high acuity region with a low resolution periphery is similar to that of zoom."}, {"heading": "6 CONCLUSION", "text": "When constrained to a glimpse window that can translate only, similar to the eye, the kernels converge to a sampling lattice similar to that found in the primate retina (Curcio & Allen, 1990; Van Essen & Anderson, 1995). This layout is composed of a high acuity region at the center surrounded by a wider region of low acuity. Van Essen & Anderson (1995) postulate that the linear relationship between eccentricity and sampling interval leads to a form of scale invariance in the primate retina. Our results from the Translation Only model with variable sized digits supports this conclusion. Additionally, we observe that zoom appears to supplant the need to learn a high acuity region for the visual search task. This implies that the high acuity region serves a purpose resembling that of a zoomable sampling lattice. The low acuity periphery is used to detect the search target and the high acuity \u2018fovea\u2019 more finely recognizes and localizes the target. These results, while obtained on an admittedly simplified domain of visual scenes, point to the possibility of using deep learning as a tool to explore the optimal sample tiling for a retinal in a data driven and task-dependent manner. Exploring how or if these results change for more challenging tasks in naturalistic visual scenes is a future goal of our research."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to acknowledge everyone at the Redwood Center for their helpful discussion and comments. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPUs used for this research."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Topography of ganglion cells in human retina", "author": ["Christine A Curcio", "Kimberly A Allen"], "venue": "Journal of Comparative Neurology,", "citeRegEx": "Curcio and Allen.,? \\Q1990\\E", "shortCiteRegEx": "Curcio and Allen.", "year": 1990}, {"title": "Models of overt attention", "author": ["Wilson S Geisler", "Lawrence Cormack"], "venue": "Oxford handbook of eye movements,", "citeRegEx": "Geisler and Cormack.,? \\Q2011\\E", "shortCiteRegEx": "Geisler and Cormack.", "year": 2011}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "A saliency-based search mechanism for overt and covert shifts of visual attention", "author": ["Laurent Itti", "Christof Koch"], "venue": "Vision research,", "citeRegEx": "Itti and Koch.,? \\Q2000\\E", "shortCiteRegEx": "Itti and Koch.", "year": 2000}, {"title": "Spatial transformer networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman"], "venue": "In Advances in Neural Information Processing Systems, pp. 2008\u20132016,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Hugo Larochelle", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Larochelle and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle and Hinton.", "year": 2010}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes.,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes.", "year": 1998}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Retinal ganglion cells that project to the dorsal lateral geniculate nucleus in the macaque", "author": ["VH Perry", "R Oehler", "A Cowey"], "venue": "monkey. Neuroscience,", "citeRegEx": "Perry et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Perry et al\\.", "year": 1984}, {"title": "Information processing strategies and pathways in the primate visual system", "author": ["David C Van Essen", "Charles H Anderson"], "venue": "An introduction to neural and electronic networks,", "citeRegEx": "Essen and Anderson.,? \\Q1995\\E", "shortCiteRegEx": "Essen and Anderson.", "year": 1995}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "6(A) and (B) Figure 1: Receptive field size (dendritic field diameter) as a function of eccentricity of Retinal Ganglion Cells from a macaque monkey (taken from Perry et al. (1984)).", "startOffset": 161, "endOffset": 181}, {"referenceID": 1, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision.", "startOffset": 102, "endOffset": 187}, {"referenceID": 8, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision.", "startOffset": 102, "endOffset": 187}, {"referenceID": 15, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision.", "startOffset": 102, "endOffset": 187}, {"referenceID": 5, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision.", "startOffset": 102, "endOffset": 187}, {"referenceID": 1, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision. An important property which distinguishes neural networks from most other neurobiological models is their ability to learn internal (latent) features directly from data. But existing neural network models specify the input sampling lattice a priori. Larochelle & Hinton (2010) employ an eccentricity dependent sampling lattice mimicking the primate retina, and Mnih et al.", "startOffset": 103, "endOffset": 577}, {"referenceID": 1, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision. An important property which distinguishes neural networks from most other neurobiological models is their ability to learn internal (latent) features directly from data. But existing neural network models specify the input sampling lattice a priori. Larochelle & Hinton (2010) employ an eccentricity dependent sampling lattice mimicking the primate retina, and Mnih et al. (2014) utilize a multi scale glimpse window\u2019 that forms a piece-wise approximation of this scheme.", "startOffset": 103, "endOffset": 680}, {"referenceID": 1, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision. An important property which distinguishes neural networks from most other neurobiological models is their ability to learn internal (latent) features directly from data. But existing neural network models specify the input sampling lattice a priori. Larochelle & Hinton (2010) employ an eccentricity dependent sampling lattice mimicking the primate retina, and Mnih et al. (2014) utilize a multi scale glimpse window\u2019 that forms a piece-wise approximation of this scheme. While it seems reasonable to think that these design choices contribute to the good performance of these systems, it remains to be seen if this arrangement emerges as the optimal solution. We further extend the learning paradigm of neural networks to the structural features of the glimpse mechanism of an attention model. To explore emergent properties of our learned retinal configurations, we train on artificial datasets where the factors of variation are easily controllable. Despite this departure from biology and natural stimuli, we find our model learns to create an eccentricity dependent layout where a distinct central region of high acuity emerges surrounded by a low acuity periphery. We show that the properties of this layout are highly dependent on the variations present in the task constraints. When we depart from physiology by augmenting our attention model with the ability to spatially rescale or zoom on its input, we find our model learns a more uniform layout which has properties more similar to the glimpse window proposed in Jaderberg et al. (2015); Gregor et al.", "startOffset": 103, "endOffset": 1850}, {"referenceID": 1, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision. An important property which distinguishes neural networks from most other neurobiological models is their ability to learn internal (latent) features directly from data. But existing neural network models specify the input sampling lattice a priori. Larochelle & Hinton (2010) employ an eccentricity dependent sampling lattice mimicking the primate retina, and Mnih et al. (2014) utilize a multi scale glimpse window\u2019 that forms a piece-wise approximation of this scheme. While it seems reasonable to think that these design choices contribute to the good performance of these systems, it remains to be seen if this arrangement emerges as the optimal solution. We further extend the learning paradigm of neural networks to the structural features of the glimpse mechanism of an attention model. To explore emergent properties of our learned retinal configurations, we train on artificial datasets where the factors of variation are easily controllable. Despite this departure from biology and natural stimuli, we find our model learns to create an eccentricity dependent layout where a distinct central region of high acuity emerges surrounded by a low acuity periphery. We show that the properties of this layout are highly dependent on the variations present in the task constraints. When we depart from physiology by augmenting our attention model with the ability to spatially rescale or zoom on its input, we find our model learns a more uniform layout which has properties more similar to the glimpse window proposed in Jaderberg et al. (2015); Gregor et al. (2015). These findings help us to understand the task conditions and constraints in which an eccentricity dependent sampling lattice emerges.", "startOffset": 103, "endOffset": 1872}, {"referenceID": 6, "context": "For example, the recent attention models proposed by Jaderberg et al. (2015); Mnih et al.", "startOffset": 53, "endOffset": 77}, {"referenceID": 6, "context": "For example, the recent attention models proposed by Jaderberg et al. (2015); Mnih et al. (2014); Gregor et al.", "startOffset": 53, "endOffset": 97}, {"referenceID": 5, "context": "(2014); Gregor et al. (2015); Ba et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2015); Ba et al. (2014) assume each kernel filter lies on a rectangular grid.", "startOffset": 8, "endOffset": 25}, {"referenceID": 2, "context": "For stochastic gradient descent optimization we use Adam (Kingma & Ba, 2014) and construct our models in Theano (Bastien et al., 2012).", "startOffset": 112, "endOffset": 134}, {"referenceID": 5, "context": "Similar to Gregor et al. (2015), this allows us to train the control network indirectly from signals backpropagated from the task cost.", "startOffset": 11, "endOffset": 32}, {"referenceID": 12, "context": "In contrast to the cluttered MNIST dataset proposed in Mnih et al. (2014), the number of distractors for each image varies randomly from 0 to 20 pieces.", "startOffset": 55, "endOffset": 74}, {"referenceID": 13, "context": "This relationship has also been observed in the primate visual system (Perry et al., 1984; Van Essen & Anderson, 1995).", "startOffset": 70, "endOffset": 118}], "year": 2016, "abstractText": "We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model\u2019s retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery. Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to their function.", "creator": "LaTeX with hyperref package"}}}