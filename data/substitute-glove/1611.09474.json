{"id": "1611.09474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Maximizing Non-Monotone DR-Submodular Functions with Cardinality Constraints", "abstract": "We difficult created problem a dividends a non - indecipherable DR - submodular characteristic instance for a cardinality stochastic. Diminishing returns (DR) submodularity is a one-dimensional within form dependence third thus for various exception over be integer fractal. This interpolation instance be. same help many machine learning given quantification benchmarking potential kinds as calculation monetary balances, revenue maximization, ors. In called meant we seek the first irreducible - again approximated optimized for non - languid inequitable maximization. We implement our modules full a 1.6 maximization problem from now real - successful cij one checked could limiting others performance.", "histories": [["v1", "Tue, 29 Nov 2016 03:40:36 GMT  (100kb,D)", "http://arxiv.org/abs/1611.09474v1", "7 pages with 2 figures"], ["v2", "Sun, 3 Sep 2017 15:35:48 GMT  (0kb,I)", "http://arxiv.org/abs/1611.09474v2", "Error description: The proposed algorithms have running time issues, in particular they are pseudo-polynomial and not fully polynomial-time"]], "COMMENTS": "7 pages with 2 figures", "reviews": [], "SUBJECTS": "cs.DS cs.AI", "authors": ["ali khodabakhsh", "evdokia nikolova"], "accepted": false, "id": "1611.09474"}, "pdf": {"name": "1611.09474.pdf", "metadata": {"source": "CRF", "title": "Maximizing Non-Monotone DR-Submodular Functions with Cardinality Constraints", "authors": ["Ali Khodabakhsh", "Evdokia Nikolova"], "emails": ["ali.kh@utexas.edu", "nikolova@austin.utexas.edu"], "sections": [{"heading": null, "text": "\u2016B\u20161\u221a k(\u2016B\u20161\u2212k) )\u22121, where\nB \u2208 RN represents the limit for each coordinate of the function and k is the parameter of cardinality constraint. This ratio is maximized when k = \u2016B\u20161 /2 and evaluates to a (1/2)-approximation. Our second algorithm is a random greedy algorithm that gives a (1/e)-approximation ratio in expectation. We implement our algorithms for a revenue maximization problem with a real-world dataset to check their efficiency and performance."}, {"heading": "1 Introduction", "text": "In this paper, we present the first polynomial time approximation algorithm for the problem of maximizing a diminishing returns submodular (DR-submodular) function under a cardinality constraint.\nSubmodular functions appear in diverse settings, such as machine learning tasks, sensor placement problems, cuts is graphs, facility location, and set covering problems [Nemhauser et al., 1978]. The inherent characteristic behind all these problems is that they all exhibit diminishing returns property. Furthermore, the diminishing returns property can appear in applications where we do not make a binary decision over a set of elements. For example, in the optimal budget allocation problem we are looking for an allocation of a given budget among media channels (like TV, newspapers, websites, etc.) in order to maximize the impact on a set of potential customers. Compared to the binary decision in submodular set function maximization, here we also\nneed to decide about the budget of each media we exploit. Alon et al. [2012] provided a (1 \u2212 1/e)-approximation algorithm for the optimal budget allocation problem. Later, Soma et al. [2014] generalized this problem and proposed a (1\u22121/e)-approximation algorithm for monotone submodular maximization over the integer lattice. We will consider such a problem in the experiments section, but our objective will be non-monotone as we are proposing the first algorithms for the non-monotone case subject to a cardinality constraint. These more general problems cannot be captured by submodular set functions.\nDR-submodularity is a generalization of submodularity to functions defined over the integer lattice, capturing the diminishing returns property. One can use available submodular maximization algorithms to maximize a DR-submodular function; but this will result in a pseudo-polynomial time approximation algorithm. We introduce a new extension function and use it to construct polynomial-time approximation algorithms. In particular, we propose two polynomial-time approximation algorithms: DR-double greedy and DR-random greedy.\nNotation: We use bold-face letters to emphasize on vectors when it is not clear by the context. The l1-norm of a vector x \u2208 Rn is defined as \u2016x\u20161 = \u2211n i=1|xi|. In this paper, N is the ground set and N is the size of N ."}, {"heading": "2 Related Work", "text": "The problem of optimizing submodular (set) functions has been well studied in the literature. Unlike the problem of minimizing submodular functions (comparable to convex minimization) which can be done efficiently [Schrijver, 2000; Iwata et al., 2001], maximizing submodular functions is NPhard, since it includes the well known NP-complete Max-Cut problem. Classic work by Nemhauser et al. [1978] showd that the greedy algorithm provides an approximation ratio of 1 \u2212 1e for maximizing a monotone increasing submodular function under a cardinality constraint. For the nonmonotone case, Buchbinder et al. [2014] provided two algorithms, namely discrete random greedy and continuous double greedy, that together yield an approximation ratio which varies in the range [0.372, 0.5]; depending on the size of ground set (n) and cardinality constraint (k). For the unconstrained case, Buchbinder et al. [2015] provided a ranar X\niv :1\n61 1.\n09 47\n4v 1\n[ cs\n.D S]\n2 9\nN ov\n2 01\n6\ndomized 1/2-approximation algorithm, improving the previous 2/5-approximation of Feige et al. [2011] and the 0.41- approximation of Gharan and Vondrak [2011] obtained by simulated annealing. A 1/2-hardness result for the unconstrained case is given in [Feige et al., 2011].\nRecently, DR-submodular maximization has received significant attention [Soma and Yoshida, 2015; Soma and Yoshida, 2016; Ene and Nguyen, 2016]. For the constrained DR-submodular maximization, Soma and Yoshida [2016] provided a (1\u2212 1e \u2212 )-approximation algorithm for cardinality constraints, polymatroid constraints, and knapsack constraints; but they assume monotonicity of the function. Ene and Nguyen [2016] introduced a generic reduction to convert the DR-submodular setting to the submodular setting, but under this reduction the cardinality constraint yields a knapsack constraint in the submodular setting. The problem of maximizing (non-monotone) submodular functions with knapsack constraints has been studied in the literature [Lee et al., 2009; Kulik et al., 2013; Chekuri et al., 2014]. Not only are the approximation ratios all less than what we obtain (in particular 1/5 \u2212 , 1/4 \u2212 , and 0.325, respectively), but also using the proposed framework, we obtain a submodular problem of size O(N + \u2211N i=1 logBi); while our proposed algorithms work in RN and have running times independent ofB. To the best of our knowledge, ours is the first direct result for nonmonotone DR-submodular maximization under a cardinality constraint."}, {"heading": "3 Problem Statement and Preliminaries", "text": "Definition 1. A set function f : 2N \u2192 R with a ground set N is submodular if:\nf(X) + f(Y ) \u2265 f(X \u2229 Y ) + f(X \u222a Y ), for all sets X,Y \u2286 N .\nAn equivalent definition of submodular (set) functions comes from the diminishing returns property, as follows: f(X \u222a {u}) \u2212 f(X) \u2265 f(Y \u222a {u}) \u2212 f(Y ) for every X \u2286 Y \u2286 N , u \u2208 N , and u 6\u2208 Y . This diminishing returns property can be generalized to functions defined over the integer lattice. Definition 2. A function f : ZN+ \u2192 R which is defined over the integer lattice is said to be DR-submodular if:\nf(x + Xu)\u2212 f(x) \u2265 f(y + Xu)\u2212 f(y), for x \u2264 y (both in ZN+ ), and u \u2208 N ; where Xu \u2208 RN is u-th unit vector with Xu(u) = 1 and other coordinates are all zero.\nIn this paper we consider the problem of maximizing a general non-negative DR-submodular function, not necessarily monotone, subject to a cardinality constraint. This optimization problem can be written as following:\nmaximize f(x) subject to 0 \u2264 x \u2264 B\n\u2016x\u20161 = k, (1)\nwhere 0 is the all zeros vector, B \u2208 RN is a coordinate-wise limit (i.e., Bi is the maximum value coordinate i can take),\nand k is a given positive integer representing the total number of elements we want to choose in our set (counting repetitive items). We will also consider the case where the constraint is \u2016x\u20161 \u2264 k instead of equality."}, {"heading": "4 Continuous Double-Greedy Algorithm", "text": "In this section we present our first polynomial time algorithm that handles both versions of cardinality constraint (inequality and equality). Before going to the algorithm, we need to introduce a new extension of DR-submodular functions to the unit hypercube. This extension is inspired by the multilinear extension for set functions [Buchbinder et al., 2014]. In the multilinear extension, each coordinate is independently sampled according to a Bernoulli distribution and the parameters of the distributions are determined by the elements of vector x. For DR-submodular functions, since coordinate i can take all integers from 0 to Bi, we use the Binomial distribution whose parameters are determined by vectors B and x. We also add a third vector m which is the deterministic part of each coordinate. This helps us to find the derivative of our extended function, which is done in lemma 1. Definition 3. Let R(B,x) be a random vector in RN where coordinate i follows the distribution:\nRi(B,x) \u223c Binomial(Bi,xi),\nindependent of the other coordinates. We define the extension of a DR-submodular function f : ZN+ \u2192 R to the unit hypercube as follows:\nF (B,x,m) \u2206 = E[f(m +R(B,x))] \u2200x \u2208 [0, 1]N .\nLemma 1. For the extension function F defined above, we have:\n\u2202\n\u2202xu F (B, x,m) = Bu \u00b7 [ F (B \u2212Xu, x,m+ Xu)\u2212 F (B \u2212Xu, x,m) ] .\nProof. Using the probability mass function of the random vector R(B, x) in definition of the extension function F implies that: F (B, x,m) = \u2211\n0\u2264y\u2264B\nf(m+ y) N\u220f i=1 ( Bi yi ) xyii (1\u2212 xi) Bi\u2212yi .\n(2) We want to differentiate with respect to the u-th coordinate of vector x (i.e., xu), but let\u2019s first consider the term related to i = u that actually includes xu (other terms are constants):\n\u2202\n\u2202xu ( Bu yu ) xyuu (1\u2212 xu)Bu\u2212yu = \u2212Bu(1\u2212 xu)Bu\u22121 yu = 0, Bux Bu\u22121 u yu = Bu,( Bu yu )[ yux yu\u22121 u (1\u2212 xu)Bu\u2212yu\u2212 (Bu \u2212 yu)xyuu (1\u2212 xu)Bu\u2212yu\u22121 ] 0 < yu < Bu.\nFor the third case (0 < yu < Bu), we can use the identities( Bu yu ) yu = Bu ( Bu\u22121 yu\u22121 ) and ( Bu yu ) (Bu \u2212 yu) = Bu ( Bu\u22121 yu ) to get:\n\u2202\n\u2202xu ( Bu yu ) xyuu (1\u2212 xu)Bu\u2212yu =\nBu [( Bu \u2212 1 yu \u2212 1 ) xyu\u22121u (1\u2212 xu)Bu\u2212yu\u2212( Bu \u2212 1 yu ) xyuu (1\u2212 xu)Bu\u2212yu\u22121 ] .\nNow using these three cases, we only need to multiply the other terms in order to obtain the derivative of the extension function. Meanwhile, we separate positive and negative terms as well:\n\u2202\n\u2202xu F (B, x,m) =[ \u2211 0\u2264y\u2264B yu\u22651 f(m+ y) N\u220f i=1 i 6=u ( Bi yi ) xyii (1\u2212 xi) Bi\u2212yi\u00d7\nBu ( Bu \u2212 1 yu \u2212 1 ) xyu\u22121u (1\u2212 xu)Bu\u2212yu ] \u2212[ \u2211\n0\u2264y\u2264B yu\u2264Bu\u22121\nf(m+ y) N\u220f i=1 i 6=u ( Bi yi ) xyii (1\u2212 xi) Bi\u2212yi\u00d7\nBu ( Bu \u2212 1 yu ) xyuu (1\u2212 xu)Bu\u2212yu\u22121 ] .\nComparing the result with equation (2), the negative term is exactly the definition of F except that Bu is replaced with Bu \u2212 1 and there is a multiplicative term Bu; so we get Bu \u00b7 F (B \u2212Xu, x,m). For the positive term we can rename yu \u2212 1 as yu, and again it gives us the definition of F , but now instead of f(m + y) we will have f(m + y + Xu), which gives us Bu \u00b7F (B\u2212Xu, x,m+Xu). This lemma also shows why we introduced a third vectorm to our extension function. It allows us to find the derivative of the extension function by calling F twice.\nNow we can present the polynomial-time approximation algorithm for problem (1). We will assume that we have an oracle access to the extension function F ; i.e., we can specify B, x, and m and retrieve the value of F (B, x,m) in constant time. This is a standard assumption in the literature because if the oracle is not available, we can always approximate the value of the extension function with sampling the function at polynomially many points while having small error.\nOur algorithm 1 is a modification of the continuous double greedy algorithm for non-monotone set functions introduced by Buchbinder et al. [2014], except that we use our new extension for DR-submodular functions and we also ensure that our final integral output satisfies the l1-norm cardinality constraint.\nThe algorithm starts with initializing two vectors x and y. One of them is set to the all zeros vector, and the other one\nAlgorithm 1 Double-Greedy DR-Submodular Maximization with Cardinality Constraint\n1: Initialize x0 = 0, y0 = 1. 2: for t \u2208 [0, 1]: 3: for every u \u2208 N define: 4: au \u2190 1Bu \u00b7 \u2202 \u2202xu\nF (B, xt,0). 5: bu \u2190 \u22121Bu \u00b7 \u2202 \u2202yu\nF (B, yt,0). 6: a\u2032u(l)\u2190 max{au \u2212 l, 0}, b\u2032u(l)\u2190 max{bu + l, 0}.\n7: dxudt (l) = a\u2032u\na\u2032u+b \u2032 u\n, dyudt (l) = \u2212b\u2032u\na\u2032u+b \u2032 u\n. 8: Let l\u2032 be a value such that \u2211\nu\u2208N Bu dxu dt (l \u2032) = k. 9: For the case of \u2016x\u20161 = k, set l\u2217 = l\u2032.\n10: For the case of \u2016x\u20161 \u2264 k, set l\u2217 = max{l\u2032, 0}. 11: Set for every u \u2208 N the derivatives: 12: dxudt \u2190 dxu dt (l\n\u2217). 13: dyudt \u2190 dyu dt (l\n\u2217). 14: return x1 = y1.\nis all ones. If the function was monotone non-decreasing, we would start from the empty set and add elements to our set one-by-one based on greedy choices. A classic result by Nemhauser et al. [1978] proves that this gives an approximation ratio of 1\u22121/e for monotone set functions; and can be extended to more settings. Once the function is non-monotone, we do not know if starting from the empty set is a good idea or not; so a natural solution is to start from both sides. Then, we check for each coordinate u \u2208 N , how much we gain if we increase the value of this coordinate at x or decrease it at y. au and bu are representatives of these marginal gains. If we were in the unconstrained scenario, we could increase xu at rate auau+bu and decrease yu at rate bu au+bu\n. Now that we have a cardinality constraint, we have to adjust these rates in order to make sure the constraint is satisfied. a\u2032u and b \u2032 u are the adjusted version of au and bu; and l\u2032 is the point where the constraint is satisfied.\nOnce the algorithm is finished, we have to convert the fractional solution x1 = y1 into an integral solution. We replicate each element xi, Bi times to get an extended vector x\u0302 of length \u2016B\u20161. We will prove that \u2016x\u0302\u20161 = k; hence applying rounding techniques like pipage rounding [Ageev and Sviridenko, 2004; Calinescu et al., 2011] will give exactly k elements of this extended vector. Then we go back to the original dimension by counting how many elements corresponding to each specific coordinate of x are selected in x\u0302, and this gives the integral solution satisfying \u2016x\u20161 = k, while preserving the approximation ratio.\nAlgorithm analysis Here are the algorithm invariants which can be easily verified by the description of the algorithm:\n\u2022 At any time t \u2208 [0, 1] we have \u2211\nu\u2208N Bu dxu dt = k (less\nthan or equal to k in the case of inequality constraint).\n\u2022 For any u \u2208 N we have dxudt \u2265 0, dyu dt \u2264 0, and dxu dt \u2212\ndyu dt = 1 at all times.\nTheorem 1. At the end of the algorithm we have x1 = y1.\nTheorem 2. At the end of the algorithm we have BTx1 = k (BTx1 \u2264 k in the case of inequality constraint).\nProof. Integrating the first invariant from 0 to 1, we will have:\u222b 1 0 \u2211 u\u2208N Bu dxu dt dt = \u2211 u\u2208N Bu \u222b 1 0 dxu dt dt =\n\u2211 u\u2208N Bu(x 1 u \u2212 x0u) = \u2211 u\u2208N Bux 1 u = \u222b 1 0 k \u00b7 dt = k.\nCorollary 1. Since we obtain x\u0302 by replicating the elements of x, B times, we have \u2016x\u0302\u20161 = k in the case of equality constraint; and \u2016x\u0302\u20161 \u2264 k in the case of inequality constraint. Theorem 3. Algorithm 1 provides a 1\n1+ 12 \u2016B\u20161\u221a\nk(\u2016B\u20161\u2212k)\napprox-\nimation for problem (1).\nProof. For the sake of the analysis, we expand x to x\u0302 as shown in Fig. 1. To obtain x\u0302, we put Bi copies of each xi into x\u0302. For example, the first element of x is repeated B1 = 3 times in x\u0302 in Fig. 1. Let T (x) be the described transformation, i.e. x\u0302 = T (x). Now we define a function g(x\u0302) and its multilinear extension G(x\u0302) as follows: When x\u0302 is a binary vector, to define g(x\u0302), we just count the number of ones corresponding to each coordinate of x, and we define g(x\u0302) equal to the value of f at the resulting vector. We also define G(x\u0302) as the multilinear extension of g to the unit hypercube. Observe that G(T (x)) = F (B, x,0), since a binomial distribution Binomial(Bi, xi) is equivalent to Bi independent Bernoulli distributions Bernoulli(xi).\nWith these definitions and extension, one can run the continuous double greedy algorithm of Buchbinder et al. [2014] on function G to get an approximation ratio of (1 + 1 2 n\u221a (n\u2212k)k\n)\u22121 where n is the dimension of x\u0302, which is \u2016B\u20161 (see [Buchbinder et al., 2014] for proof). Although this is exactly the ratio stated in this theorem, it will give us a pseudopolynomial running time; because we are running it in R\u2016B\u20161 . Here is where our speedup trick comes to play and reduces the running time.\nThe idea is to use algorithm 1 which works directly on x, but show that the result is the same as in the pseudopolynomial time algorithm.\nFact 1. If v and v\u2032 are two coordinates in x\u0302 that are related to the same coordinate u in x, and x\u0302v = x\u0302v\u2032 , then:\n\u2202G(x\u0302) \u2202x\u0302v = \u2202G(x\u0302) \u2202x\u0302v\u2032 .\nThe reason is that the order of coordinates in x\u0302 does not matter as long as they are related to the same coordinate in x. At the end, we are just counting the number of ones to determine the value of the function.\nThe previous fact establishes that in the process of running the pseudo-polynomial algorithm, all coordinates related to the same xu will evolve together (although they are not equal in the optimal solution).\nFact 2. If v is one of the coordinates in x\u0302 that is related to coordinate u in x, then:\n\u2202\n\u2202xu F (B, x, 0) = Bu\n\u2202G(x\u0302)\n\u2202x\u0302v .\nThe reason is that increasing xu by a small value is equivalent to increasing a set of Bu elements (including v) in x\u0302 all by , and by Fact 1 it is equivalent to increasing x\u0302v by Bu .\nFact 2 is the intuition behind algorithm 1. Instead of updating Bu elements of x\u0302 in the pseudo-polynomial algorithm (that we proved they will be equal at all times), we update only one element in x; but we get the same result because that single element will be copied Bu times at the end."}, {"heading": "5 Random Greedy Algorithm", "text": "In this section, we consider the other version of cardinality constraint with an inequality. In particular, we want to solve this problem:\nmaximize f(x) subject to 0 \u2264 x \u2264 B\n\u2016x\u20161 \u2264 k. (3)\nThe difference between this problem and problem (1) is that here we do not insist on choosing exactly k elements and any solution is feasible as long as it does not exceed k elements.\nNote that algorithm 1 is also able to handle this kind of constraint (See line 10 in algorithm 1), but the algorithm proposed in this section provides (in expectation) a constant factor approximation.\nAlgorithm 2 Random Greedy DR-Submodular Maximization with Cardinality Constraint\n1: Initialize x0 = 0. 2: for i = 1 to k 3: Set fu(xi\u22121) = f(xi\u22121 + Xu)\u2212 f(xi\u22121) \u2200u \u2208 N 4: Let r \u2208 ZN+ be a vector that maximizes\u2211\nu\u2208N rufu(x i\u22121) and satisfies xi\u22121 + r \u2264 B and \u2016r\u20161 = k. 5: Choose a random coordinate e where the probability of choosing coordinate i \u2208 N is ri/k. 6: xi \u2190 xi\u22121 + Xe. 7: return xk.\nThe algorithm starts with x0 \u2208 RN equal to the all zeros vector. Then at each of k steps, it increments one of the coordinates which is chosen randomly and greedily. Coordinates resulting in the largest marginal gains are considered in the random decision, and those who are far from their limits have higher probability of being chosen. After k steps, the algorithm returns xk. Remark 1. The running time of algorithm 2 is O(Nk). Line 3 takes O(N) time which should be repeated k times. Remark 2. Even though we are incrementing one coordinate of x in each iteration of the loop, the solution satisfies \u2016x\u20161 \u2264 k not \u2016x\u20161 = k. The reason is that we need to add a dummy coordinate (which does not affect the value of function) for the algorithm to run; otherwise there may not be a feasible r in step 4. We delete this dummy coordinate once the algorithm is done. See Reductions 1,2 in Buchbinder et al. [2014] for more details. Remark 3. There is no limit for the dummy coordinate. In other words, Bdummy = \u221e (More precisely, Bdummy = 2k is sufficient). Theorem 4. Let xk be the output of algorithm 2 and OPT be the optimal solution of problem (3), then:\nE[f(xk)] \u2265 1 e \u00b7 f(OPT ).\nProof. Again we consider a pseudo-polynomial time algorithm that achieves this approximation ratio, and we argue that algorithm 2 is doing the same thing but in polynomial time. Considering the expansion of figure 1, the pseudopolynomial time algorithm performs the algorithm 1 of Buchbinder et al. [2014] in R\u2016B\u20161 space. We only need to define f(x\u0302) for binary vectors in R\u2016B\u20161 . We define this by simply counting the number of ones related to each coordinate of x, and then finding the value of f at this vector. Then the algorithm achieves 1/e-approximation ratio (in expectation). Note that the optimal solution is not unique in the space of R\u2016B\u20161 . In fact, the number of optimal solutions is equal to:\nN\u220f u=1 ( Bu OPTu ) ,\nbut their value is equal to f(OPT ), which is sufficient for the analysis.\nThe pseudo-polynomial algorithm starts with an empty set S0 = \u2205 or equivalently x\u0302 = 0. Then it finds the top k coordinates that have biggest fu(x\u0302), where fu(x\u0302) is the marginal gain achieved by setting x\u0302u = 1. So in each step, it has to find fu(x\u0302) for all coordinates that are still zero. The fact below is the intuition behind algorithm 2 that reduces the running time of the pseudo-polynomial algorithm.\nFact 3. If v and v\u2032 are two coordinates in x\u0302 that are related to the same coordinate u in x, and x\u0302v = x\u0302v\u2032 = 0, then:\nfv(x\u0302) = fv\u2032(x\u0302).\nThe reason is that these two coordinates are added together in the definition of f(x\u0302), so adding 1 to any of them will lead to the same result.\nBased on this fact, we only need to compute one marginal contribution for each coordinate of x. Also in the process of choosing the top k coordinates in x\u0302, we can choose up toBu\u2212 xu coordinates related to u, because that is the number of zeros in x\u0302 related to u. ru describes the number of coordinates related to coordinate u that we choose in x\u0302. This also justifies the constraint xi\u22121 + r \u2264 B. Finally we have ru chosen coordinates while each of them is chosen with probability 1/k in the pseudo-polynomial algorithm. So in total, f(x\u0302) will have the value of f(x+Xu) with probability ru/k in the next iteration. That is exactly what we do in algorithm 2, so we have: E[f(x\u0302i)] = E[f(xi)] \u22000 \u2264 i \u2264 k, which completes the proof, because E[f(x\u0302k)] \u2265 1ef(OPT )."}, {"heading": "6 Experiments", "text": "Since this is the first polynomial-time approximation algorithm for this problem, there is no other algorithm in the literature to compare with. Hence we compare our proposed algorithms with the pseudo-polynomial time algorithms one can get by expanding the ground set to a multiset and then running available algorithms for submodular (set function) maximization. We consider an inequality (cardinality) constraint so that both our algorithms be applicable.\nExperimental setting All the experiments are performed on an Intel R\u00a9 Core i7 3.1GHz processor with 8GB of RAM. The algorithms are implemented in MATLAB [2015]. We compare the following algorithms: \u2022 Continuous Double Greedy (DG): We expand the ground\nset N to a multiset of size \u2016B\u20161, then we run algorithm 2 of Buchbinder et al. [2014]. This gives us a pseudopolynomial time algorithm.\n\u2022 Diminishing Returns Double Greedy (DR-DG, Alg. 1). \u2022 Random Greedy (RG): We expand the ground set N to a\nmultiset of size \u2016B\u20161, then we run algorithm 1 of Buchbinder et al. [2014]. This gives us a pseudo-polynomial time algorithm.\n\u2022 Diminishing Returns Random Greedy (DR-RG, Alg. 2).\nApplication: revenue maximization We consider the problem of revenue maximization on a social network G(V,E) (inspired by the model in [Hartline et al., 2008]). Each node represents a person and directed edges describe friendship ratings betweenN = 217 residents living at a residence hall located on the Australian National University campus [Kunegis, 2013]. wij = 5 means that node i has the most impact on node j (best friends); and wij = 1 is the lowest impact among friendships. There are 2672 weighted edges in total. By investing x units of cost on a user i \u2208 V , we make user i an advocate of our product with probability 1\u2212 (1\u2212 p)x where p \u2208 (0, 1) is a fixed parameter. If S \u2286 V is the set of all advocates, the revenue we get is defined as:\u2211\ni\u2208S \u2211 j\u2208V \\S wij ,\nwhich is the effect of our advocates on others (word-of-mouth effect). Since S is a random set, we define f(x) to be the expected value of this revenue:\nf(x) = ES [\u2211\ni\u2208S \u2211 j\u2208V \\S wij ] =\n\u2211 (i,j)\u2208E wij(1\u2212 (1\u2212 p)x(i))(1\u2212 p)x(j).\nIt can be easily shown that f : Z|V |+ \u2192 R is a non-monotone DR-submodular function. The goal is to maximize this revenue, while our budget for investing on users is limited (and captured by k). In our simulations, we set p = 10\u22122 and we assume a uniform limit for all residents; in particular we set Bi = 10 for all 1 \u2264 i \u2264 N .\nTable 1 compares our proposed double greedy algorithm (algorithm 1) with the pseudo-polynomial time algorithm achieved by running algorithm 2 of Buchbinder et al. [2014] in R\u2016B\u20161 space. We used \u2206t = 0.05 for updating the two vectors x, y during t \u2208 [0, 1] in both algorithms. As expected, the results are almost the same; while our algorithm is much faster. The small differences in the objective values are due to the fact that the extension function (definition 3) is obtained by random sampling. The difference diminishes as we increase the number of samples.\nTable 2 compares our proposed random greedy algorithm (algorithm 2) with the pseudo-polynomial time algorithm achieved by running algorithm 1 of [Buchbinder et al., 2014] in R\u2016B\u20161 space. As we argued earlier, as expected, the results are almost the same; while our running time is significantly smaller. In fact, the running time of the pseudo-polynomial time algorithm would be O(\u2016B\u20161 \u00d7 k) compared to O(Nk) of our proposed algorithm. Here the differences are expected due to the randomness in the algorithms.\nFigure 2 compares the number of oracle calls for the four algorithms as we change the coordinate limits (B). Again we assume Bi = B = \u2016B\u2016\u221e for all 1 \u2264 i \u2264 N . We still have N = 217 and we set k = \u2016B\u20161 /2 (where the double greedy algorithm provides the best approximation ratio). When B is equal to one, we basically have a submodular set function maximization problem (each coordinate can be zero or one); hence algorithm 1 is equivalent to DG (red and blue), and algorithm 2 is equivalent to RG (magenta and black). As we increase B, the naive pseudo-polynomial time algorithms make more oracle calls compared to our proposed algorithms."}, {"heading": "7 Conclusion", "text": "In this paper, we proposed two polynomial time approximation algorithms for non-monotone DR-submodular function maximization under a cardinality constraint. One has an approximation ratio which depends on k (from the cardinality constraint), and the other algorithm has a constant approximation ratio (in expectation). We showed the efficiency of our algorithms through an application in revenue maximization.\nConsidering set functions, minimizing supermodular functions is harder than maximizing submodular functions, although both are NP-hard. In fact, Kelner and Nikolova [2007] showed that supermodular minimization is (log n)-hard to approximate. Later, Mittal and Schulz [2013] showed that supermodular minimization is hard to approximate within any constant factor. When the supermodular function has a bounded curvature, Il\u2019ev and Sviridenko et al. [2001; 2015] have proposed approximation algorithms where the approximation ratio is a function of the curvature (or equivalently, the steepness). One interesting question is whether these definitions and algorithms can be generalized to supermodular functions defined over the integer lattice."}, {"heading": "Acknowledgement", "text": "This work was supported in part by NSF grant numbers CCF1216103, CCF-1350823, CCF-1331863, and a Google Faculty Research Award."}], "references": [{"title": "Pipage rounding: A new method of constructing algorithms with proven performance guarantee", "author": ["Alexander A Ageev", "Maxim I Sviridenko"], "venue": "Journal of Combinatorial Optimization, 8(3):307\u2013328,", "citeRegEx": "Ageev and Sviridenko. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In Proceedings of the 21st international conference on World Wide Web", "author": ["Noga Alon", "Iftah Gamzu", "Moshe Tennenholtz. Optimizing budget allocation among channels", "influencers"], "venue": "pages 381\u2013388. ACM,", "citeRegEx": "Alon et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Submodular maximization with cardinality constraints", "author": ["Buchbinder et al", "2014] Niv Buchbinder", "Moran Feldman", "Joseph Seffi Naor", "Roy Schwartz"], "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "A tight linear time (1/2)-approximation for unconstrained submodular maximization", "author": ["Niv Buchbinder", "Moran Feldman", "Joseph Seffi", "Roy Schwartz"], "venue": "SIAM Journal on Computing, 44(5):1384\u20131402,", "citeRegEx": "Buchbinder et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "SIAM Journal on Computing", "author": ["Gruia Calinescu", "Chandra Chekuri", "Martin P\u00e1l", "Jan Vondr\u00e1k. Maximizing a monotone submodular function subject to a matroid constraint"], "venue": "40(6):1740\u20131766,", "citeRegEx": "Calinescu et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "SIAM Journal on Computing", "author": ["Chandra Chekuri", "Jan Vondr\u00e1k", "Rico Zenklusen. Submodular function maximization via the multilinear relaxation", "contention resolution schemes"], "venue": "43(6):1831\u20131879,", "citeRegEx": "Chekuri et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A reduction for optimizing lattice submodular functions with diminishing returns", "author": ["Alina Ene", "Huy L Nguyen"], "venue": "arXiv preprint arXiv:1606.08362,", "citeRegEx": "Ene and Nguyen. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "SIAM Journal on Computing", "author": ["Uriel Feige", "Vahab S Mirrokni", "Jan Vondrak. Maximizing non-monotone submodular functions"], "venue": "40(4):1133\u20131153,", "citeRegEx": "Feige et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "pages 1098\u2013 1116", "author": ["Shayan Oveis Gharan", "Jan Vondr\u00e1k. Submodular maximization by simulated annealing. In Proceedings of the twenty-second annual ACMSIAM symposium on Discrete Algorithms"], "venue": "SIAM,", "citeRegEx": "Gharan and Vondr\u00e1k. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of the 17th international conference on World Wide Web", "author": ["Jason Hartline", "Vahab Mirrokni", "Mukund Sundararajan. Optimal marketing strategies over social networks"], "venue": "pages 189\u2013198. ACM,", "citeRegEx": "Hartline et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "A combinatorial strongly polynomial algorithm", "author": ["Iwata et al", "2001] Satoru Iwata", "Lisa Fleischer", "Satoru Fujishige"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "2007", "author": ["Jonathan A Kelner", "Evdokia Nikolova. On the hardness", "smoothed complexity of quasi-concave minimization. In Foundations of Computer Science"], "venue": "FOCS\u201907. 48th Annual IEEE Symposium on, pages 472\u2013482. IEEE,", "citeRegEx": "Kelner and Nikolova. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Mathematics of Operations Research", "author": ["Ariel Kulik", "Hadas Shachnai", "Tami Tamir. Approximations for monotone", "nonmonotone submodular maximization with knapsack constraints"], "venue": "38(4):729\u2013739,", "citeRegEx": "Kulik et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "KONECT \u2013 The Koblenz Network Collection", "author": ["J\u00e9r\u00f4me Kunegis"], "venue": "Proc. Int. Conf. on World Wide Web Companion, pages 1343\u20131350,", "citeRegEx": "Kunegis. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "pages 323\u2013332", "author": ["Jon Lee", "Vahab S Mirrokni", "Viswanath Nagarajan", "Maxim Sviridenko. Non-monotone submodular maximization under matroid", "knapsack constraints. In Proceedings of the forty-first annual ACM symposium on Theory of computing"], "venue": "ACM,", "citeRegEx": "Lee et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Mathematical Programming", "author": ["Shashi Mittal", "Andreas S Schulz. An fptas for optimizing a class of low-rank functions over a polytope"], "venue": "141(12):103\u2013120,", "citeRegEx": "Mittal and Schulz. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Mathematical Programming", "author": ["George L Nemhauser", "Laurence A Wolsey", "Marshall L Fisher. An analysis of approximations for maximizing submodular set functions\u00f3i"], "venue": "14(1):265\u2013294,", "citeRegEx": "Nemhauser et al.. 1978", "shortCiteRegEx": null, "year": 1978}, {"title": "Series B", "author": ["Alexander Schrijver. A combinatorial algorithm minimizing submodular functions in strongly polynomial time. Journal of Combinatorial Theory"], "venue": "80(2):346\u2013355,", "citeRegEx": "Schrijver. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Tasuku Soma", "Yuichi Yoshida. A generalization of submodular cover via the diminishing return property on the integer lattice"], "venue": "pages 847\u2013855,", "citeRegEx": "Soma and Yoshida. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 325\u2013336", "author": ["Tasuku Soma", "Yuichi Yoshida. Maximizing monotone submodular functions over the integer lattice. In International Conference on Integer Programming", "Combinatorial Optimization"], "venue": "Springer,", "citeRegEx": "Soma and Yoshida. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Optimal budget allocation: Theoretical guarantee and efficient algorithm", "author": ["Tasuku Soma", "Naonori Kakimura", "Kazuhiro Inaba", "Ken-ichi Kawarabayashi"], "venue": "ICML, pages 351\u2013359,", "citeRegEx": "Soma et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal approximation for submodular and supermodular optimization with bounded curvature", "author": ["Sviridenko et al", "2015] Maxim Sviridenko", "Jan Vondr\u00e1k", "Justin Ward"], "venue": "In Proceedings of the Twenty-Sixth Annual ACMSIAM Symposium on Discrete Algorithms,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Submodular functions appear in diverse settings, such as machine learning tasks, sensor placement problems, cuts is graphs, facility location, and set covering problems [Nemhauser et al., 1978].", "startOffset": 169, "endOffset": 193}, {"referenceID": 17, "context": "Unlike the problem of minimizing submodular functions (comparable to convex minimization) which can be done efficiently [Schrijver, 2000; Iwata et al., 2001], maximizing submodular functions is NPhard, since it includes the well known NP-complete Max-Cut problem.", "startOffset": 120, "endOffset": 157}, {"referenceID": 7, "context": "A 1/2-hardness result for the unconstrained case is given in [Feige et al., 2011].", "startOffset": 61, "endOffset": 81}, {"referenceID": 18, "context": "Recently, DR-submodular maximization has received significant attention [Soma and Yoshida, 2015; Soma and Yoshida, 2016; Ene and Nguyen, 2016].", "startOffset": 72, "endOffset": 142}, {"referenceID": 19, "context": "Recently, DR-submodular maximization has received significant attention [Soma and Yoshida, 2015; Soma and Yoshida, 2016; Ene and Nguyen, 2016].", "startOffset": 72, "endOffset": 142}, {"referenceID": 6, "context": "Recently, DR-submodular maximization has received significant attention [Soma and Yoshida, 2015; Soma and Yoshida, 2016; Ene and Nguyen, 2016].", "startOffset": 72, "endOffset": 142}, {"referenceID": 14, "context": "The problem of maximizing (non-monotone) submodular functions with knapsack constraints has been studied in the literature [Lee et al., 2009; Kulik et al., 2013; Chekuri et al., 2014].", "startOffset": 123, "endOffset": 183}, {"referenceID": 12, "context": "The problem of maximizing (non-monotone) submodular functions with knapsack constraints has been studied in the literature [Lee et al., 2009; Kulik et al., 2013; Chekuri et al., 2014].", "startOffset": 123, "endOffset": 183}, {"referenceID": 5, "context": "The problem of maximizing (non-monotone) submodular functions with knapsack constraints has been studied in the literature [Lee et al., 2009; Kulik et al., 2013; Chekuri et al., 2014].", "startOffset": 123, "endOffset": 183}, {"referenceID": 0, "context": "We will prove that \u2016x\u0302\u20161 = k; hence applying rounding techniques like pipage rounding [Ageev and Sviridenko, 2004; Calinescu et al., 2011] will give exactly k elements of this extended vector.", "startOffset": 86, "endOffset": 138}, {"referenceID": 4, "context": "We will prove that \u2016x\u0302\u20161 = k; hence applying rounding techniques like pipage rounding [Ageev and Sviridenko, 2004; Calinescu et al., 2011] will give exactly k elements of this extended vector.", "startOffset": 86, "endOffset": 138}, {"referenceID": 9, "context": "We consider the problem of revenue maximization on a social network G(V,E) (inspired by the model in [Hartline et al., 2008]).", "startOffset": 101, "endOffset": 124}, {"referenceID": 13, "context": "Each node represents a person and directed edges describe friendship ratings betweenN = 217 residents living at a residence hall located on the Australian National University campus [Kunegis, 2013].", "startOffset": 182, "endOffset": 197}], "year": 2016, "abstractText": "We consider the problem of maximizing a nonmonotone DR-submodular function subject to a cardinality constraint. Diminishing returns (DR) submodularity is a generalization of the diminishing returns property for functions defined over the integer lattice. This generalization can be used to solve many machine learning or combinatorial optimization problems such as optimal budget allocation, revenue maximization, etc. In this work we propose the first polynomial-time approximation algorithms for non-monotone constrained maximization. Our first algorithm achieves an approximation ratio of (1 + 12 \u2016B\u20161 \u221a k(\u2016B\u20161\u2212k) )\u22121, where B \u2208 R represents the limit for each coordinate of the function and k is the parameter of cardinality constraint. This ratio is maximized when k = \u2016B\u20161 /2 and evaluates to a (1/2)-approximation. Our second algorithm is a random greedy algorithm that gives a (1/e)-approximation ratio in expectation. We implement our algorithms for a revenue maximization problem with a real-world dataset to check their efficiency and performance.", "creator": "LaTeX with hyperref package"}}}