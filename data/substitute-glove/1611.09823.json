{"id": "1611.09823", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Dialogue Learning With Human-In-The-Loop", "abstract": "An part relation on integration stilted spy much this meant a bot the willingness from improve part navigate five beings and soon lessons from though needless far time makes. Most associate, highly on focusing started fixed training block include labeled identification rather over phenomena with gave dialogue partner in an aol casual. In this paper we explore this means for this reinforcement learning up turned the bot improves its whether - overheard achieve from feedback any teacher gives saw its anticipated unusual. We rebuilding a simulator nothing tests used particular of among concepts then first synthetic environment, both introduce ford that work in unfortunately regime. Finally, gives microgravity with Mechanical Turk confound the useful.", "histories": [["v1", "Tue, 29 Nov 2016 20:16:44 GMT  (360kb,D)", "https://arxiv.org/abs/1611.09823v1", null], ["v2", "Fri, 16 Dec 2016 00:22:53 GMT  (347kb,D)", "http://arxiv.org/abs/1611.09823v2", null], ["v3", "Fri, 13 Jan 2017 21:12:38 GMT  (349kb,D)", "http://arxiv.org/abs/1611.09823v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["jiwei li", "alexander h miller", "sumit chopra", "marc'aurelio ranzato", "jason weston"], "accepted": true, "id": "1611.09823"}, "pdf": {"name": "1611.09823.pdf", "metadata": {"source": "CRF", "title": "DIALOGUE LEARNING WITH HUMAN-IN-THE-LOOP", "authors": ["Jiwei Li", "Alexander H. Miller", "Sumit Chopra"], "emails": ["jiwel@fb.com", "ahm@fb.com", "spchopra@fb.com", "ranzato@fb.com", "jase@fb.com"], "sections": [{"heading": null, "text": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes. Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime. Finally, real experiments with Mechanical Turk validate the approach."}, {"heading": "1 INTRODUCTION", "text": "A good conversational agent (which we sometimes refer to as a learner or bot1) should have the ability to learn from the online feedback from a teacher: adapting its model when making mistakes and reinforcing the model when the teacher\u2019s feedback is positive. This is particularly important in the situation where the bot is initially trained in a supervised way on a fixed synthetic, domainspecific or pre-built dataset before release, but will be exposed to a different environment after release (e.g., more diverse natural language utterance usage when talking with real humans, different distributions, special cases, etc.). Most recent research has focused on training a bot from fixed training sets of labeled data but seldom on how the bot can improve through online interaction with humans. Human (rather than machine) language learning happens during communication (Bassiri, 2011; Werts et al., 1995), and not from labeled datasets, hence making this an important subject to study.\nIn this work, we explore this direction by training a bot through interaction with teachers in an online fashion. The task is formalized under the general framework of reinforcement learning via the teacher\u2019s (dialogue partner\u2019s) feedback to the dialogue actions from the bot. The dialogue takes place in the context of question-answering tasks and the bot has to, given either a short story or a set of facts, answer a set of questions from the teacher. We consider two types of feedback: explicit numerical rewards as in conventional reinforcement learning, and textual feedback which is more natural in human dialogue, following (Weston, 2016). We consider two online training scenarios: (i) where the task is built with a dialogue simulator allowing for easy analysis and repeatability of experiments; and (ii) where the teachers are real humans using Amazon Mechanical Turk.\nWe explore important issues involved in online learning such as how a bot can be most efficiently trained using a minimal amount of teacher\u2019s feedback, how a bot can harness different types of feedback signal, how to avoid pitfalls such as instability during online learing with different types of feedback via data balancing and exploration, and how to make learning with real humans feasible via data batching. Our findings indicate that it is feasible to build a pipeline that starts from a model trained with fixed data and then learns from interactions with humans to improve itself.\n1In this paper, we refer to a learner (either a human or a bot/dialogue agent which is a machine learning algorithm) as the student, and their more knowledgeable dialogue partner as the teacher.\nar X\niv :1\n61 1.\n09 82\n3v 3\n[ cs\n.A I]\n1 3\nJa n\n20 17"}, {"heading": "2 RELATED WORK", "text": "Reinforcement learning has been widely applied to dialogue, especially in slot filling to solve domain-specific tasks (Walker, 2000; Schatzmann et al., 2006; Singh et al., 2000; 2002). Efforts include Markov Decision Processes (MDPs) (Levin et al., 1997; 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP models (Young et al., 2010; 2013; Gas\u030cic et al., 2013; 2014) and policy learning (Su et al., 2016). Such a line of research focuses mainly on frames with slots to fill, where the bot will use reinforcement learning to model a state transition pattern, generating dialogue utterances to prompt the appropriate user responses to put in the desired slots. This goal is different from ours, where we study end-to-end learning systems and also consider non-reward based setups via textual feedback.\nOur work is related to the line of research that focuses on supervised learning for question answering (QA) from dialogues (Dodge et al., 2015; Weston, 2016), either given a database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short texts (Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016). In our work, the discourse includes the statements made in the past, the question and answer, and crucially the response from the teacher. The latter is what makes the setting different from the standard QA setting, i.e. we use methods that leverage this response also, not just answering questions. Further, QA works only consider fixed datasets with gold annotations, i.e. they do not consider a reinforcement learning setting.\nOur work is closely related to a recent work from Weston (2016) that learns through conducting conversations where supervision is given naturally in the response during the conversation. That work introduced the use of forward prediction that learns by predicting the teacher\u2019s feedback, in addition to using reward-based learning of correct answers. However, two important issues were not addressed: (i) it did not use a reinforcement learning setting, but instead used pre-built datasets with fixed policies given in advance; and (ii) experiments used only simulated and no real language data. Hence, models that can learn policies from real online communication were not investigated. To make the differences with our work clear, we will now detail these points further.\nThe experiments in (Weston, 2016) involve constructing pre-built fixed datasets, rather than training the learner within a simulator, as in our work. Pre-built datasets can only be made by fixing a prior in advance. They achieve this by choosing an omniscient (but deliberately imperfect) labeler that gets \u03c0acc examples always correct (the paper looked at values 50%, 10% and 1%). Again, this was not learned, and was fixed to generate the datasets. Note that the paper refers to these answers as coming from \u201cthe learner\u201d (which should be the model), but since the policy is fixed it actually does not depend on the model. In a realistic setting one does not have access to an omniscient labeler, one has to learn a policy completely from scratch, online, starting with a random policy, so their setting was not practically viable. In our work, when policy training is viewed as batch learning over iterations of the dataset, updating the policy on each iteration, (Weston, 2016) can be viewed as training only one iteration, whereas we perform multiple iterations. This is explained further in Sections 4.2 and 5.1. We show in our experiments that performance improves over the iterations, i.e. it is better than the first iteration. We show that such online learning works for both rewardbased numerical feedback and for forward prediction methods using textual feedback (under certain conditions which are detailed). This is a key contribution of our work.\nFinally, (Weston, 2016) only conducted experiments on synthetic or templated language, and not real language, especially the feedback from the teacher was scripted. While we believe that synthetic datasets are very important for developing understanding (hence we develop a simulator and conduct experiments also with synthetic data), for a new method to gain traction it must be shown to work on real data. We hence employ Mechanical Turk to collect real language data for the questions and importantly for the teacher feedback and construct experiments in this real setting."}, {"heading": "3 DATASET AND TASKS", "text": "We begin by describing the data setup we use. In our first set of experiments we build a simulator as a testbed for learning algorithms. In our second set of experiments we use Mechanical Turk to provide real human teachers giving feedback."}, {"heading": "3.1 SIMULATOR", "text": "The simulator adapts two existing fixed datasets to our online setting. Following Weston (2016), we use (i) the single supporting fact problem from the bAbI datasets (Weston et al., 2015) which consists of 1000 short stories from a simulated world interspersed with questions; and (ii) the WikiMovies dataset (Weston et al., 2015) which consists of roughly 100k (templated) questions over 75k entities based on questions with answers in the open movie database (OMDb). Each dialogue takes place between a teacher, scripted by the simulation, and a bot. The communication protocol is as follows: (1) the teacher first asks a question from the fixed set of questions existing in the dataset, (2) the bot answers the question, and finally (3) the teacher gives feedback on the bot\u2019s answer.\nWe follow the paradigm defined in (Weston, 2016) where the teacher\u2019s feedback takes the form of either textual feedback, a numerical reward, or both, depending on the task. For each dataset, there are ten tasks, which are further described in Sec. A and illustrated in Figure 5 of the appendix. We also refer the readers to (Weston, 2016) for more detailed descriptions and the motivation behind these tasks. In the main text of this paper we only consider Task 6 (\u201cpartial feedback\u201d): the teacher replies with positive textual feedback (6 possible templates) when the bot answers correctly, and positive reward is given only 50% of the time. When the bot is wrong, the teacher gives textual feedback containing the answer. Descriptions and experiments on the other tasks are detailed in the appendix. Example dialogues are given in Figure 1.\nThe difference between our simulation and the original fixed tasks of Weston (2016) is that models are trained on-the-fly. After receiving feedback and/or rewards, we update the model (policy) and then deploy it to collect teacher\u2019s feedback in the next episode or batch. This means the model\u2019s policy affects the data which is used to train it, which was not the case in the previous work."}, {"heading": "3.2 MECHANICAL TURK EXPERIMENTS", "text": "Finally, we extended WikiMovies using Mechanical Turk so that real human teachers are giving feedback rather than using a simulation. As both the questions and feedback are templated in the simulation, they are now both replaced with natural human utterances. Rather than having a set of simulated tasks, we have only one task, and we gave instructions to the teachers that they could give feedback as they see fit. The exact instructions given to the Turkers is given in Appendix B. In general, each independent response contains feedback like (i) positive or negative sentences; or (ii) a phrase containing the answer or (iii) a hint, which are similar to setups defined in the simulator. However, some human responses cannot be so easily categorized, and the lexical variability is much larger in human responses. Some examples of the collected data are given in Figure 2."}, {"heading": "4 METHODS", "text": ""}, {"heading": "4.1 MODEL ARCHITECTURE", "text": "In our experiments, we used variants of the End-to-End Memory Network (MemN2N) model (Sukhbaatar et al., 2015) as our underlying architecture for learning from dialogue.\nThe input to MemN2N is the last utterance of the dialogue history x as well as a set of memories (context) C=c1, c2, ..., cN . The memory C encodes both short-term memory, e.g., dialogue histories between the bot and the teacher, and long-term memories, e.g., the knowledge base facts that the bot has access to. Given the input x and C, the goal is to produce an output/label a.\nIn the first step, the query x is transformed to a vector representation u0 by summing up its constituent word embeddings: u0 = Ax. The input x is a bag-of-words vector and A is the d\u00d7 V word embedding matrix where d denotes the emebbding dimension and V denotes the vocabulary size. Each memory ci is similarly transformed to a vector mi. The model will read information from the memory by comparing input representation u0 with memory vectors mi using softmax weights:\no1 = \u2211 i p1imi p 1 i = softmax(u T 0mi) (1)\nThis process selects memories relevant to the last utterance x, i.e., the memories with large values of p1i . The returned memory vector o1 is the weighted sum of memory vectors. This process can be repeated to query the memory N times (so called \u201chops\u201d) by adding on to the original input, u1 = o1 + u0, or to the previous state, un = on + un\u22121, and then using un to query the memories again.\nIn the end, uN is input to a softmax function for the final prediction:\na = softmax(uTNy1, u T Ny2, ..., u T NyL) (2)\nwhere y1, . . . , yL denote the set of candidate answers. If the answer is a word, yi is the corresponding word embedding. If the answer is a sentence, yi is the embedding for the sentence achieved in the same way that we obtain embeddings for query x and memory C.\nThe standard way MemN2N is trained is via a cross entropy criterion on known input-output pairs, which we refer to as supervised or imitation learning. As our work is in a reinforcement learning setup where our model must make predictions to learn, this procedure will not work, so we instead consider reinforcement learning algorithms which we describe next."}, {"heading": "4.2 REINFORCEMENT LEARNING", "text": "In this section, we present the algorithms we used to train MemN2N in an online fashion. Our learning setup can be cast as a particular form of Reinforcement Learning. The policy is implemented by the MemN2N model. The state is the dialogue history. The action space corresponds to the set of answers the MemN2N has to choose from to answer the teacher\u2019s question. In our setting, the policy chooses only one action for each episode. The reward is either 1 (a reward from the teacher when the bot answers correctly) or 0 otherwise. Note that in our experiments, a reward equal to 0 might mean that the answer is incorrect or that the positive reward is simply missing. The overall setup is closest to standard contextual bandits, except that the reward is binary.\nWhen working with real human dialogues, e.g. collecting data via Mechanical Turk, it is easier to set up a task whereby a bot is deployed to respond to a large batch of utterances, as opposed to a single one. The latter would be more difficult to manage and scale up since it would require some form of synchronization between the model replicas interacting with each human.\nThis is comparable to the real world situation where a teacher can either ask a student a single question and give feedback right away, or set up a test that contains many questions and grade all of them at once. Only after the learner completes all questions, it can hear feedback from the teacher.\nWe use batch size to refer to how many dialogue episodes the current model is used to collect feedback before updating its parameters. In the Reinforcement Learning literature, batch size is related to off-policy learning since the MemN2N policy is trained using episodes collected with a stale version of the model. Our experiments show that our model and base algorithms are very robust to the choice of batch size, alleviating the need for correction terms in the learning algorithm (Bottou et al., 2013).\nWe consider two strategies: (i) online batch size, whereby the target policy is updated after doing a single pass over each batch (a batch size of 1 reverts to the usual on-policy online learning); and (ii) dataset-sized batch, whereby training is continued to convergence on the batch which is the size of the dataset, and then the target policy is updated with the new model, and a new batch is drawn and the procedure iterates. These strategies can be applied to all the methods we use, described below.\nNext, we discuss the learning algorithms we considered in this work."}, {"heading": "4.2.1 REWARD-BASED IMITATION (RBI)", "text": "The simplest algorithm we first consider is the one employed in Weston (2016). RBI relies on positive rewards provided by the teacher. It is trained to imitate the correct behavior of the learner, i.e., learning to predict the correct answers (with reward 1) at training time and disregarding the other ones. This is implemented by using a MemN2N that maps a dialogue input to a prediction, i.e. using the cross entropy criterion on the positively rewarded subset of the data.\nIn order to make this work in the online setting which requires exploration to find the correct answer, we employ an -greedy strategy: the learner makes a prediction using its own model (the answer assigned the highest probability) with probability 1 \u2212 , otherwise it picks a random answer with probability . The teacher will then give a reward of +1 if the answer is correct, otherwise 0. The bot will then learn to imitate the correct answers: predicting the correct answers while ignoring the incorrect ones."}, {"heading": "4.2.2 REINFORCE", "text": "The second algorithm we use is the REINFORCE algorithm (Williams, 1992), which maximizes the expected cumulative reward of the episode, in our case the expected reward provided by the teacher. The expectation is approximated by sampling an answer from the model distribution. Let a denote the answer that the learner gives, p(a) denote the probability that current model assigns to a, r denote the teacher\u2019s reward, and J(\u03b8) denote the expectation of the reward. We have:\n\u2207J(\u03b8) \u2248 \u2207 log p(a)[r \u2212 b] (3)\nwhere b is the baseline value, which is estimated using a linear regression model that takes as input the output of the memory network after the last hop, and outputs a scalar b denoting the estimation of the future reward. The baseline model is trained by minimizing the mean squared loss between the estimated reward b and actual reward r, ||r \u2212 b||2. We refer the readers to (Ranzato et al., 2015; Zaremba & Sutskever, 2015) for more details. The baseline estimator model is independent from the policy model, and its error is not backpropagated through the policy model.\nThe major difference between RBI and REINFORCE is that (i) the learner only tries to imitate correct behavior in RBI while in REINFORCE it also leverages the incorrect behavior, and (ii) the learner explores using an -greedy strategy in RBI while in REINFORCE it uses the distribution over actions produced by the model itself."}, {"heading": "4.2.3 FORWARD PREDICTION (FP)", "text": "FP (Weston, 2016) handles the situation where a numerical reward for a bot\u2019s answer is not available, meaning that there are no +1 or 0 labels available after a student\u2019s utterance. Instead, the model assumes the teacher gives textual feedback t to the bot\u2019s answer, taking the form of a dialogue utterance, and the model tries to predict this instead. Suppose that x denotes the teacher\u2019s question andC=c1, c2, ..., cN denotes the dialogue history as before. In FP, the model first maps the teacher\u2019s initial question x and dialogue history C to a vector representation u using a memory network with multiple hops. Then the model will perform another hop of attention over all possible student\u2019s answers in A, with an additional part that incorporates the information of which candidate (i.e., a) was actually selected in the dialogue:\npa\u0302 = softmax(u T ya\u0302) o = \u2211 a\u0302\u2208A pa\u0302(ya\u0302 + \u03b2 \u00b7 1[a\u0302 = a]) (4)\nwhere ya\u0302 denotes the vector representation for the student\u2019s answer candidate a\u0302. \u03b2 is a (learned) d-dimensional vector to signify the actual action a that the student chooses. o is then combined with u to predict the teacher\u2019s feedback t using a softmax:\nu1 = o+ u t = softmax(u T 1 xr1 , u T 1 xr2 , ..., u T 1 xrN ) (5)\nwhere xri denotes the embedding for the i th response. In the online setting, the teacher will give textual feedback, and the learner needs to update its model using the feedback. It was shown in Weston (2016) that in an off-line setting this procedure can work either on its own, or in conjunction with a method that uses numerical rewards as well for improved performance. In the online setting, we consider two simple extensions:\n\u2022 -greedy exploration: with probability the student will give a random answer, and with probability 1\u2212 it will give the answer that its model assigns the largest probability. This method enables the model to explore the space of actions and to potentially discover correct answers.\n\u2022 data balancing: cluster the set of teacher responses t and then balance training across the clusters equally.2 This is a type of experience replay (Mnih et al., 2013) but sampling with an evened distribution. Balancing stops part of the distribution dominating the learning. For example, if the model is not exposed to sufficient positive and negative feedback, and one class overly dominates, the learning process degenerates to a model that always predicts the same output regardless of its input."}, {"heading": "5 EXPERIMENTS", "text": "Experiments are first conducted using our simulator, and then using Amazon Mechanical Turk with real human subjects taking the role of the teacher3."}, {"heading": "5.1 SIMULATOR", "text": "Online Experiments In our first experiments, we considered both the bAbI and WikiMovies tasks and varied batch size, random exploration rate , and type of model. Figure 3 and Figure 4 shows (Task 6) results on bAbI and WikiMovies. Other tasks yield similar conclusions and are reported in the appendix.\nOverall, we obtain the following conclusions:\n\u2022 In general RBI and FP do work in a reinforcement learning setting, but can perform better with random exploration.\n\u2022 In particular RBI can fail without exploration. RBI needs random noise for exploring labels otherwise it can get stuck predicting a subset of labels and fail.\n2In the simulated data, because the responses are templates, this can be implemented by first randomly sampling the response, and then randomly sampling a story with that response; we keep the history of all stories seen from which we sample. For real data slightly more sophisticated clustering should be used.\n3 Code and data are available at https://github.com/facebook/MemNN/tree/master/HITL.\n\u2022 REINFORCE obtains similar performance to RBI with optimal . \u2022 FP with balancing or with exploration via both outperform FP alone. \u2022 For both RBI and FP, performance is largely independent of online batch size.\nDataset Batch Size Experiments Given that larger online batch sizes appear to work well, and that this could be important in a real-world data collection setup where the same model is deployed to gather a large amount of feedback from humans, we conducted further experiments where the batch size is exactly equal to the dataset size and for each batch training is completed to convergence.\nAfter the model has been trained on the dataset, it is deployed to collect a new dataset of questions and answers, and the process is repeated. Table 1 reports test error at each iteration of training, using the bAbI Task 6 as the case study (see the appendix for results on other tasks). The following conclusions can be made for this setting:\n\u2022 RBI improves in performance as we iterate. Unlike in the online case, RBI does not need random exploration. We believe this is because the first batch, which is collected with a randomly initialized model, contains enough variety of examples with positive rewards that the model does not get stuck predicting a subset of labels.\n\u2022 FP is not stable in this setting. This is because once the model gets very good at making predictions (at the third iteration), it is not exposed to a sufficient number of negative responses anymore. From that point on, learning degenerates and performance drops as the model always predicts the same responses. At the next iteration, it will recover again since it has a more balanced training set, but then it will collapse again in an oscillating behavior.\n\u2022 FP does work if extended with balancing or random exploration with sufficiently large .\n\u2022 RBI+FP also works well and helps with the instability of FP, alleviating the need for random exploration and data balancing.\nOverall, our simulation results indicate that while a bot can be effectively trained fully online from bot-teacher interactions, collecting real dialogue data in batches (which is easier to collect and iterate experiments over) is also a viable approach. We hence pursue the latter approach in our next set of experiments.\nRelation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al., 2015). However, that work involved constructing pre-built fixed policies (and hence, datasets), rather than training the learner in a reinforcement/interactive learning using a simulator, as in our work. They achieved this by choosing an omniscient (but deliberately imperfect) labeler that gets \u03c0acc examples always correct (the paper looked at values 1%, 10% and 50%). In a realistic setting one does not have access to an omniscient labeler, one has to learn a policy completely from scratch, online, starting with a random policy, as we do here. Nevertheless, it is possible to compare our learnt policies to those results because we use the same train/valid/test splits.\nThe clearest comparison comparison is via Table 1, where the policy is learnt using batch iterations of the dataset, updating the policy on each iteration. Weston et al. (2015) can be viewed as training only one iteration, with a pre-built policy, as explained above, where 59%, 81% and 99% accuracy was obtained for RBI for \u03c0acc with 1%, 10% and 50% respectively4. While \u03c0acc of 50% is good enough to solve the task, lower values are not. In this work a random policy begins with 74% accuracy on the first iteration, but importantly on each iteration the policy is updated and improves, with values of 87%, 90% on iterations 2 and 3 respectively, and 98% on iteration 6. This is a key differentiator to the work of (Weston et al., 2015) where such improvement was not shown. We show that such online learning works for both reward-based numerical feedback and for forward prediction methods using textual feedback (as long as balancing or random exploration is performed sufficiently). The final performance outperforms most values of \u03c0acc from Weston et al. (2015) unless \u03c0 is so large that the task is already solved. This is a key contribution of our work.\nSimilar conclusions can be made for Figures 3 and 4. Despite our initial random policy starting at close to 0% accuracy, if random exploration \u2265 0.2 is employed then after a number of epochs the performance is better than most values of \u03c0acc from Weston et al. (2015), e.g. compare the accuracies given in the previous paragraph (59%, 81% and 99%) to Figure 3, top left."}, {"heading": "5.2 HUMAN FEEDBACK", "text": "We employed Turkers to both ask questions and then give textual feedback on the bot\u2019s answers, as described in Section 3.2. Our experimental protocol was as follows. We first trained a MemN2N using supervised (i.e., imitation) learning on a training set of 1000 questions produced by Turkers and using the known correct answers provided by the original dataset (and no textual feedback). Next, using the trained policy, we collected textual feedback for the responses of the bot for an additional 10,000 questions. Examples from the collected dataset are given in Figure 2. Given this dataset, we compare various models: RBI, FP and FP+RBI. As we know the correct answers to the additional questions, we can assign a positive reward to questions the bot got correct. We hence measure the impact of the sparseness of this reward signal, where a fraction r of additional examples have rewards. The models are tested on a test set of \u223c8,000 questions (produced by Turkers), and hyperparameters are tuned on a similarly sized validation set. Note this is a harder task than the WikiMovies task in the simulator due to the use natural language from Turkers, hence lower test performance is expected.\n4Note, this is not the same as a randomly initialized neural network policy, because due to the synthetic construction with an omniscient labeler the labels will be balanced. In our work, we learn the policy from randomly initialized weights which are updated as we learn the policy.\nResults are given in Table 2. They indicate that both RBI and FP are useful. When rewards are sparse, FP still works via the textual feedback while RBI can only use the initial 1000 examples when r = 0. As FP does not use numericalrewards at all, it is invariant to the parameter r. The combination of FP and RBI outperforms either alone.\nWe also conducted additional experiments comparing with (i) synthetic feedback and (ii) the fully supervised case which are given in Appendix C.1. They show that the results with human feedback are competitive with these approaches."}, {"heading": "6 CONCLUSION", "text": "We studied dialogue learning of end-to-end models using textual feedback and numerical rewards. Both fully online and iterative batch settings are viable approaches to policy learning, as long as possible instabilities in the learning algorithms are taken into account. Secondly, we showed for the first time that the recently introduced FP method can work in both an online setting and on real human feedback. Overall, our results indicate that it is feasible to build a practical pipeline that starts with a model trained on an initial fixed dataset, which then learns from interactions with humans in a (semi-)online fashion to improve itself. Future research should work towards doing this in a never-ending learning setup."}, {"heading": "A FURTHER SIMULATOR TASK DETAILS", "text": "The tasks in Weston (2016) were specifically: - Task 1: The teacher tells the student exactly what they should have said (supervised baseline). - Task 2: The teacher replies with positive textual feedback and reward, or negative textual feedback. - Task 3: The teacher gives textual feedback containing the answer when the bot is wrong. - Task 4: The teacher provides a hint by providing the class of the correct answer, e.g., \u201cNo it\u2019s a movie\u201d for the question \u201cwhich movie did Forest Gump star in?\u201d. - Task 5: The teacher provides a reason why the student\u2019s answer is wrong by pointing out the relevant supporting fact from the knowledge base. - Task 6: The teacher gives positive reward only 50% of the time. - Task 7: Rewards are missing and the teacher only gives natural language feedback. - Task 8: Combines Tasks 1 and 2 to see whether a learner can learn successfully from both forms of supervision at once. - Task 9: The bot asks questions of the teacher about what it has done wrong. - Task 10: The bot will receive a hint rather than the correct answer after asking for help.\nWe refer the readers to (Weston, 2016) for more detailed descriptions and the motivation behind these tasks. The difference in our system is that the model can be trained on-the-fly via the simulator: after receiving feedback and/or rewards, the model can update itself and apply its learning to the next episode. We present results on Tasks 2, 3 and 4 in this appendix\nB INSTRUCTIONS GIVEN TO TURKERS\nThese are the instructions given for the textual feedback mechanical turk task (we also constructed a separate task to collect the initial questions, not described here):\nTitle: Write brief responses to given dialogue exchanges (about 15 min)\nDescription: Write a brief response to a student\u2019s answer to a teacher\u2019s question, providing feedback to the student on their answer.\nInstructions:\nEach task consists of the following triplets:\n1. a question by the teacher 2. the correct answer(s) to the question (separated by \u201cOR\u201d) 3. a proposed answer in reply to the question from the student\nConsider the scenario where you are the teacher and have already asked the question, and received the reply from the student. Please compose a brief response giving feedback to the student about their answer. The correct answers are provided so that you know whether the student was correct or not.\nFor example, given 1) question: \u201cwhat is a color in the united states flag?\u201d; 2) correct answer: \u201cwhite, blue, red\u201d; 3) student reply: \u201cred\u201d, your response could be something like \u201cthat\u2019s right!\u201d; for 3) reply: \u201cgreen\u201d, you might say \u201cno that\u2019s not right\u201d or \u201cnope, a correct answer is actually white\u201d.\nPlease vary responses and try to minimize spelling mistakes. If the same responses are copied/pasted or overused, we\u2019ll reject the HIT.\nAvoid naming the student or addressing \u201cthe class\u201d directly.\nWe will consider bonuses for higher quality responses during review."}, {"heading": "C ADDITIONAL EXPERIMENTS", "text": "C.1 ADDITIONAL EXPERIMENTS FOR MECHANICAL TURK SETUP\nIn the experiment in Section 5.2 we conducted experiments with real human feedback. Here, we compare this to a form of synthetic feedback, mostly as a sanity check, but also to see how much improvement we can get if the signal is simpler and cleaner (as it is synthetic). We hence constructed synthetic feedback for the 10,000 responses, using either Task 2 (positive or negative feedback), Task 3 (answers provided by teacher) or a mix (Task 2+3) where we use one or the other for each example (50% chance of each). The latter makes the synthetic data have a mixed setup of responses, which more closely mimics the real data case. The results are given in Table 4. The RBI+FP combination is better using the synthetic data than the real data with Task 2+3 or Task 3, which is to be expected, but the real data is competitive, despite the difficulty of dealing with its lexical and semantic variability. The real data is better than using Task 2 synthetic data.\nFor comparison purposes, we also ran a supervised (imitation learning) MemN2N on different sized training sets of turker authored questions with gold annotated labels (so, there are no numerical rewards or textual feedback, this is a pure supervised setting). The results are given in Table 5. They indicate that RBI+FP and even FP alone get close to the performance of fully supervised learning.\nC.2 SECOND ITERATION OF FEEDBACK\nWe conducted experiments with an additional iteration of data collection for the case of binary rewards and textual feedback using the synthetic Task 2+3 mix. We selected the best model from the previous training, using RBI+FP with r = 1 which previously gave a test accuracy of 0.478 (see Table 4). Using that model as a predictor, we collected an additional 10,000 training examples.\nWe then continue to train our model using the original 1k+10k training set, plus the additional 10k. As before, we report the test accuracy varying r on the additional collected set. We also report the performance from varying , the proportion of random exploration of predictions on the new set. The results are reported in Table 6. Overall, performance is improved in the second iteration, with slightly better performance for large r and = 0.5. However, the improvement is mostly invariant to those parameters, likely because FP takes advantage of feedback from incorrect predictions in any case."}], "references": [{"title": "Interactional feedback and the impact of attitude and motivation on noticing l2 form", "author": ["Mohammad Amin Bassiri"], "venue": "English Language and Literature Studies,", "citeRegEx": "Bassiri.,? \\Q2011\\E", "shortCiteRegEx": "Bassiri.", "year": 2011}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1506.02075,", "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["Leon Bottou", "Jonas Peters", "Denis X. Quionero-Candela", "Joaquin amd Charles", "D. Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Simard", "Ed Snelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "author": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston"], "venue": "arXiv preprint arXiv:1511.06931,", "citeRegEx": "Dodge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dodge et al\\.", "year": 2015}, {"title": "Pomdp-based dialogue manager adaptation to extended domains", "author": ["Milica Ga\u0161ic", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve Young"], "venue": "In Proceedings of SIGDIAL,", "citeRegEx": "Ga\u0161ic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161ic et al\\.", "year": 2013}, {"title": "Incremental on-line adaptation of pomdp-based dialogue managers to extended domains", "author": ["Milica Ga\u0161ic", "Dongho Kim", "Pirros Tsiakoulis", "Catherine Breslin", "Matthew Henderson", "Martin Szummer", "Blaise Thomson", "Steve Young"], "venue": "In Proceedings on InterSpeech,", "citeRegEx": "Ga\u0161ic et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ga\u0161ic et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Learning dialogue strategies within the markov decision process framework", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert"], "venue": "In Automatic Speech Recognition and Understanding,", "citeRegEx": "Levin et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Levin et al\\.", "year": 1997}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert"], "venue": "IEEE Transactions on speech and audio processing,", "citeRegEx": "Levin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Are we there yet? research in commercial spoken dialog systems", "author": ["Roberto Pieraccini", "David Suendermann", "Krishna Dayanidhi", "Jackson Liscombe"], "venue": "In International Conference on Text, Speech and Dialogue,", "citeRegEx": "Pieraccini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pieraccini et al\\.", "year": 2009}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "arXiv preprint arXiv:1606.05250,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1511.06732,", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies", "author": ["Jost Schatzmann", "Karl Weilhammer", "Matt Stuttle", "Steve Young"], "venue": "The knowledge engineering review,", "citeRegEx": "Schatzmann et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2006}, {"title": "Empirical evaluation of a reinforcement learning spoken dialogue system", "author": ["Satinder Singh", "Michael Kearns", "Diane J Litman", "Marilyn A Walker"], "venue": "In AAAI/IAAI,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Optimizing dialogue management with reinforcement learning: Experiments with the njfun system", "author": ["Satinder Singh", "Diane Litman", "Michael Kearns", "Marilyn Walker"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Singh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "Continuously learning neural dialogue management", "author": ["Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "arXiv preprint arXiv:1606.02689,", "citeRegEx": "Su et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email", "author": ["Marilyn A. Walker"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Walker.,? \\Q2000\\E", "shortCiteRegEx": "Walker.", "year": 2000}, {"title": "A trainable generator for recommendations in multimodal dialog", "author": ["Marilyn A Walker", "Rashmi Prasad", "Amanda Stent"], "venue": "In INTERSPEECH,", "citeRegEx": "Walker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2003}, {"title": "Instructive feedback: Review of parameters and effects", "author": ["Margaret G Werts", "Mark Wolery", "Ariane Holcombe", "David L Gast"], "venue": "Journal of Behavioral Education,", "citeRegEx": "Werts et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Werts et al\\.", "year": 1995}, {"title": "Dialog-based language learning", "author": ["Jason Weston"], "venue": "arXiv preprint arXiv:1604.06045,", "citeRegEx": "Weston.,? \\Q2016\\E", "shortCiteRegEx": "Weston.", "year": 2016}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Simon Keizer", "Fran\u00e7ois Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu"], "venue": "Computer Speech & Language,", "citeRegEx": "Young et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Young et al\\.", "year": 2010}, {"title": "Under review as a conference paper at ICLR", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D Williams"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Young et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Human (rather than machine) language learning happens during communication (Bassiri, 2011; Werts et al., 1995), and not from labeled datasets, hence making this an important subject to study.", "startOffset": 75, "endOffset": 110}, {"referenceID": 21, "context": "Human (rather than machine) language learning happens during communication (Bassiri, 2011; Werts et al., 1995), and not from labeled datasets, hence making this an important subject to study.", "startOffset": 75, "endOffset": 110}, {"referenceID": 22, "context": "We consider two types of feedback: explicit numerical rewards as in conventional reinforcement learning, and textual feedback which is more natural in human dialogue, following (Weston, 2016).", "startOffset": 177, "endOffset": 191}, {"referenceID": 19, "context": "Reinforcement learning has been widely applied to dialogue, especially in slot filling to solve domain-specific tasks (Walker, 2000; Schatzmann et al., 2006; Singh et al., 2000; 2002).", "startOffset": 118, "endOffset": 183}, {"referenceID": 14, "context": "Reinforcement learning has been widely applied to dialogue, especially in slot filling to solve domain-specific tasks (Walker, 2000; Schatzmann et al., 2006; Singh et al., 2000; 2002).", "startOffset": 118, "endOffset": 183}, {"referenceID": 15, "context": "Reinforcement learning has been widely applied to dialogue, especially in slot filling to solve domain-specific tasks (Walker, 2000; Schatzmann et al., 2006; Singh et al., 2000; 2002).", "startOffset": 118, "endOffset": 183}, {"referenceID": 7, "context": "Efforts include Markov Decision Processes (MDPs) (Levin et al., 1997; 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP models (Young et al.", "startOffset": 49, "endOffset": 121}, {"referenceID": 20, "context": "Efforts include Markov Decision Processes (MDPs) (Levin et al., 1997; 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP models (Young et al.", "startOffset": 49, "endOffset": 121}, {"referenceID": 11, "context": "Efforts include Markov Decision Processes (MDPs) (Levin et al., 1997; 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP models (Young et al.", "startOffset": 49, "endOffset": 121}, {"referenceID": 25, "context": ", 2009), POMDP models (Young et al., 2010; 2013; Ga\u0161ic et al., 2013; 2014) and policy learning (Su et al.", "startOffset": 22, "endOffset": 74}, {"referenceID": 4, "context": ", 2009), POMDP models (Young et al., 2010; 2013; Ga\u0161ic et al., 2013; 2014) and policy learning (Su et al.", "startOffset": 22, "endOffset": 74}, {"referenceID": 17, "context": ", 2013; 2014) and policy learning (Su et al., 2016).", "startOffset": 34, "endOffset": 51}, {"referenceID": 3, "context": "Our work is related to the line of research that focuses on supervised learning for question answering (QA) from dialogues (Dodge et al., 2015; Weston, 2016), either given a database of knowledge (Bordes et al.", "startOffset": 123, "endOffset": 157}, {"referenceID": 22, "context": "Our work is related to the line of research that focuses on supervised learning for question answering (QA) from dialogues (Dodge et al., 2015; Weston, 2016), either given a database of knowledge (Bordes et al.", "startOffset": 123, "endOffset": 157}, {"referenceID": 1, "context": ", 2015; Weston, 2016), either given a database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short texts (Weston et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 9, "context": ", 2015; Weston, 2016), either given a database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short texts (Weston et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 23, "context": ", 2016) or short texts (Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016).", "startOffset": 23, "endOffset": 90}, {"referenceID": 6, "context": ", 2016) or short texts (Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016).", "startOffset": 23, "endOffset": 90}, {"referenceID": 12, "context": ", 2016) or short texts (Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016).", "startOffset": 23, "endOffset": 90}, {"referenceID": 22, "context": "The experiments in (Weston, 2016) involve constructing pre-built fixed datasets, rather than training the learner within a simulator, as in our work.", "startOffset": 19, "endOffset": 33}, {"referenceID": 22, "context": "In our work, when policy training is viewed as batch learning over iterations of the dataset, updating the policy on each iteration, (Weston, 2016) can be viewed as training only one iteration, whereas we perform multiple iterations.", "startOffset": 133, "endOffset": 147}, {"referenceID": 22, "context": "Finally, (Weston, 2016) only conducted experiments on synthetic or templated language, and not real language, especially the feedback from the teacher was scripted.", "startOffset": 9, "endOffset": 23}, {"referenceID": 1, "context": ", 2015; Weston, 2016), either given a database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short texts (Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016). In our work, the discourse includes the statements made in the past, the question and answer, and crucially the response from the teacher. The latter is what makes the setting different from the standard QA setting, i.e. we use methods that leverage this response also, not just answering questions. Further, QA works only consider fixed datasets with gold annotations, i.e. they do not consider a reinforcement learning setting. Our work is closely related to a recent work from Weston (2016) that learns through conducting conversations where supervision is given naturally in the response during the conversation.", "startOffset": 61, "endOffset": 681}, {"referenceID": 23, "context": "Following Weston (2016), we use (i) the single supporting fact problem from the bAbI datasets (Weston et al., 2015) which consists of 1000 short stories from a simulated world interspersed with questions; and (ii) the WikiMovies dataset (Weston et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 23, "context": ", 2015) which consists of 1000 short stories from a simulated world interspersed with questions; and (ii) the WikiMovies dataset (Weston et al., 2015) which consists of roughly 100k (templated) questions over 75k entities based on questions with answers in the open movie database (OMDb).", "startOffset": 129, "endOffset": 150}, {"referenceID": 22, "context": "We follow the paradigm defined in (Weston, 2016) where the teacher\u2019s feedback takes the form of either textual feedback, a numerical reward, or both, depending on the task.", "startOffset": 34, "endOffset": 48}, {"referenceID": 22, "context": "We also refer the readers to (Weston, 2016) for more detailed descriptions and the motivation behind these tasks.", "startOffset": 29, "endOffset": 43}, {"referenceID": 22, "context": "Following Weston (2016), we use (i) the single supporting fact problem from the bAbI datasets (Weston et al.", "startOffset": 10, "endOffset": 24}, {"referenceID": 22, "context": "Following Weston (2016), we use (i) the single supporting fact problem from the bAbI datasets (Weston et al., 2015) which consists of 1000 short stories from a simulated world interspersed with questions; and (ii) the WikiMovies dataset (Weston et al., 2015) which consists of roughly 100k (templated) questions over 75k entities based on questions with answers in the open movie database (OMDb). Each dialogue takes place between a teacher, scripted by the simulation, and a bot. The communication protocol is as follows: (1) the teacher first asks a question from the fixed set of questions existing in the dataset, (2) the bot answers the question, and finally (3) the teacher gives feedback on the bot\u2019s answer. We follow the paradigm defined in (Weston, 2016) where the teacher\u2019s feedback takes the form of either textual feedback, a numerical reward, or both, depending on the task. For each dataset, there are ten tasks, which are further described in Sec. A and illustrated in Figure 5 of the appendix. We also refer the readers to (Weston, 2016) for more detailed descriptions and the motivation behind these tasks. In the main text of this paper we only consider Task 6 (\u201cpartial feedback\u201d): the teacher replies with positive textual feedback (6 possible templates) when the bot answers correctly, and positive reward is given only 50% of the time. When the bot is wrong, the teacher gives textual feedback containing the answer. Descriptions and experiments on the other tasks are detailed in the appendix. Example dialogues are given in Figure 1. The difference between our simulation and the original fixed tasks of Weston (2016) is that models are trained on-the-fly.", "startOffset": 10, "endOffset": 1643}, {"referenceID": 22, "context": "We consider 10 different tasks following Weston (2016) but here describe only Task 6; other tasks are detailed in the appendix.", "startOffset": 41, "endOffset": 55}, {"referenceID": 18, "context": "1 MODEL ARCHITECTURE In our experiments, we used variants of the End-to-End Memory Network (MemN2N) model (Sukhbaatar et al., 2015) as our underlying architecture for learning from dialogue.", "startOffset": 106, "endOffset": 131}, {"referenceID": 2, "context": "Our experiments show that our model and base algorithms are very robust to the choice of batch size, alleviating the need for correction terms in the learning algorithm (Bottou et al., 2013).", "startOffset": 169, "endOffset": 190}, {"referenceID": 22, "context": "The simplest algorithm we first consider is the one employed in Weston (2016). RBI relies on positive rewards provided by the teacher.", "startOffset": 64, "endOffset": 78}, {"referenceID": 24, "context": "The second algorithm we use is the REINFORCE algorithm (Williams, 1992), which maximizes the expected cumulative reward of the episode, in our case the expected reward provided by the teacher.", "startOffset": 55, "endOffset": 71}, {"referenceID": 13, "context": "We refer the readers to (Ranzato et al., 2015; Zaremba & Sutskever, 2015) for more details.", "startOffset": 24, "endOffset": 73}, {"referenceID": 22, "context": "3 FORWARD PREDICTION (FP) FP (Weston, 2016) handles the situation where a numerical reward for a bot\u2019s answer is not available, meaning that there are no +1 or 0 labels available after a student\u2019s utterance.", "startOffset": 29, "endOffset": 43}, {"referenceID": 10, "context": "2 This is a type of experience replay (Mnih et al., 2013) but sampling with an evened distribution.", "startOffset": 38, "endOffset": 57}, {"referenceID": 21, "context": "It was shown in Weston (2016) that in an off-line setting this procedure can work either on its own, or in conjunction with a method that uses numerical rewards as well for improved performance.", "startOffset": 16, "endOffset": 30}, {"referenceID": 22, "context": "Note that supervised, rather than reinforcement learning, with gold standard labels achieves 80% accuracy on this task (Weston, 2016).", "startOffset": 119, "endOffset": 133}, {"referenceID": 23, "context": "Relation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al., 2015).", "startOffset": 135, "endOffset": 156}, {"referenceID": 23, "context": "This is a key differentiator to the work of (Weston et al., 2015) where such improvement was not shown.", "startOffset": 44, "endOffset": 65}, {"referenceID": 22, "context": "Relation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al.", "startOffset": 27, "endOffset": 41}, {"referenceID": 22, "context": "Relation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al., 2015). However, that work involved constructing pre-built fixed policies (and hence, datasets), rather than training the learner in a reinforcement/interactive learning using a simulator, as in our work. They achieved this by choosing an omniscient (but deliberately imperfect) labeler that gets \u03c0acc examples always correct (the paper looked at values 1%, 10% and 50%). In a realistic setting one does not have access to an omniscient labeler, one has to learn a policy completely from scratch, online, starting with a random policy, as we do here. Nevertheless, it is possible to compare our learnt policies to those results because we use the same train/valid/test splits. The clearest comparison comparison is via Table 1, where the policy is learnt using batch iterations of the dataset, updating the policy on each iteration. Weston et al. (2015) can be viewed as training only one iteration, with a pre-built policy, as explained above, where 59%, 81% and 99% accuracy was obtained for RBI for \u03c0acc with 1%, 10% and 50% respectively4.", "startOffset": 27, "endOffset": 1004}, {"referenceID": 22, "context": "Relation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al., 2015). However, that work involved constructing pre-built fixed policies (and hence, datasets), rather than training the learner in a reinforcement/interactive learning using a simulator, as in our work. They achieved this by choosing an omniscient (but deliberately imperfect) labeler that gets \u03c0acc examples always correct (the paper looked at values 1%, 10% and 50%). In a realistic setting one does not have access to an omniscient labeler, one has to learn a policy completely from scratch, online, starting with a random policy, as we do here. Nevertheless, it is possible to compare our learnt policies to those results because we use the same train/valid/test splits. The clearest comparison comparison is via Table 1, where the policy is learnt using batch iterations of the dataset, updating the policy on each iteration. Weston et al. (2015) can be viewed as training only one iteration, with a pre-built policy, as explained above, where 59%, 81% and 99% accuracy was obtained for RBI for \u03c0acc with 1%, 10% and 50% respectively4. While \u03c0acc of 50% is good enough to solve the task, lower values are not. In this work a random policy begins with 74% accuracy on the first iteration, but importantly on each iteration the policy is updated and improves, with values of 87%, 90% on iterations 2 and 3 respectively, and 98% on iteration 6. This is a key differentiator to the work of (Weston et al., 2015) where such improvement was not shown. We show that such online learning works for both reward-based numerical feedback and for forward prediction methods using textual feedback (as long as balancing or random exploration is performed sufficiently). The final performance outperforms most values of \u03c0acc from Weston et al. (2015) unless \u03c0 is so large that the task is already solved.", "startOffset": 27, "endOffset": 1894}, {"referenceID": 22, "context": "Relation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al., 2015). However, that work involved constructing pre-built fixed policies (and hence, datasets), rather than training the learner in a reinforcement/interactive learning using a simulator, as in our work. They achieved this by choosing an omniscient (but deliberately imperfect) labeler that gets \u03c0acc examples always correct (the paper looked at values 1%, 10% and 50%). In a realistic setting one does not have access to an omniscient labeler, one has to learn a policy completely from scratch, online, starting with a random policy, as we do here. Nevertheless, it is possible to compare our learnt policies to those results because we use the same train/valid/test splits. The clearest comparison comparison is via Table 1, where the policy is learnt using batch iterations of the dataset, updating the policy on each iteration. Weston et al. (2015) can be viewed as training only one iteration, with a pre-built policy, as explained above, where 59%, 81% and 99% accuracy was obtained for RBI for \u03c0acc with 1%, 10% and 50% respectively4. While \u03c0acc of 50% is good enough to solve the task, lower values are not. In this work a random policy begins with 74% accuracy on the first iteration, but importantly on each iteration the policy is updated and improves, with values of 87%, 90% on iterations 2 and 3 respectively, and 98% on iteration 6. This is a key differentiator to the work of (Weston et al., 2015) where such improvement was not shown. We show that such online learning works for both reward-based numerical feedback and for forward prediction methods using textual feedback (as long as balancing or random exploration is performed sufficiently). The final performance outperforms most values of \u03c0acc from Weston et al. (2015) unless \u03c0 is so large that the task is already solved. This is a key contribution of our work. Similar conclusions can be made for Figures 3 and 4. Despite our initial random policy starting at close to 0% accuracy, if random exploration \u2265 0.2 is employed then after a number of epochs the performance is better than most values of \u03c0acc from Weston et al. (2015), e.", "startOffset": 27, "endOffset": 2256}], "year": 2017, "abstractText": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes. Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime. Finally, real experiments with Mechanical Turk validate the approach.", "creator": "LaTeX with hyperref package"}}}