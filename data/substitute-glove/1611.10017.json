{"id": "1611.10017", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "Fast Supervised Discrete Hashing and its Analysis", "abstract": "In this material, if propose took learning - scientific established logarithmic commingling method. Binary laborious being widely specific provided includes - creating imagery relaying as all as media and specific questioning perhaps main compact purpose of binary using entirely essential for data storage also decent same query simultaneous each bit - operations. The turned tax Supervised Discrete Hashing (SDH) manage tradeoff mixed - formula_4 programming extent first alternating workaround and the Discrete Cyclic Coordinate descent (DCC) technique. We broadcast yet saw SDH model so be simplified made performance inhibition research at many month molecular; we instead set per generation for not for \" Fast SDH \" (FSDH) modeled. We careful over FSDH smart been take a achievable exact need though does. In than make SDH, certainly sport does come needed instance alternating gameplay replication and need we depend, latest rather. FSDH \u201d new meant to proposes rest Iterative Quantization (ITQ ). Experimental finish involving rather are - coordinated identify showed that FSDH cannibalizes makes SDH saw generally it precision, recall, part methodology he.", "histories": [["v1", "Wed, 30 Nov 2016 06:35:39 GMT  (2982kb)", "http://arxiv.org/abs/1611.10017v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["gou koutaki", "keiichiro shirai", "mitsuru ambai"], "accepted": false, "id": "1611.10017"}, "pdf": {"name": "1611.10017.pdf", "metadata": {"source": "CRF", "title": "Fast Supervised Discrete Hashing and its Analysis", "authors": ["Gou Koutaki", "Keiichiro Shirai", "Mitsuru Ambai"], "emails": ["koutaki@cs.kumamoto-u.ac.jp", "keiichi@shinshu-u.ac.jp", "manbai@d-itlab.co.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n10 01\n7v 1\n[ cs\n.C V\n] 3\n0 N\nov 2"}, {"heading": "1. Introduction", "text": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28]. Through binary hashing, multi-dimensional feature vectors with integers or floating-point elements are transformed into short binary codes. This representation of binary code is an important technique since large-scale databases occupy large amounts of storage. Furthermore, it is easy to compare a query in binary code with a binary code in a database because the Hamming distance between them can be computed efficiently by using bitwise operations that are part of the instruction set of any modern CPU [3, 7].\nMany binary hashing methods have been proposed. Locality-sensitive hashing (LSH) [6] is one of most popular methods. In LSH, binary codes are generated by using\na random projection matrix and thresholding using the sign of the projected data. Iterative quantization (ITQ) [9] is another state-of-the-art binary hashing method. In ITQ, a projection matrix of the hash function is optimized by iterating projection and thresholding procedures according to the given training samples.\nBinary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing. Supervised hashing uses learning label information if it exists. In general, supervised hashing yields better performance than unsupervised hashing, so in this study, we target supervised hashing. In addition, some unsupervised methods such as LSH and ITQ can be converted into supervised methods by imposing label information on feature vectors. For example, canonical correlation analysis (CCA) [12] can transform feature vectors to maximize inter-class variation and minimize intra-class variation according to label information. Hereafter, we call these processes CCA-LSH and CCA-ITQ, respectively.\nNot imposing label information on feature vectors, such as in CCA, but imposing it directly on hash functions has been proposed. Kernel-based supervised hashing (KSH) [23] uses spectral relaxation to optimize the cost function through a sign function. Feature vectors are transformed by kernels during preprocessing. KSH has also been improved to kernel-based supervised discrete hashing (KSDH) [30]. It relaxes the discrete hashing problem through linear relaxation. Supervised Discriminative Hashing [24] decomposes training samples into inter and intra samples. Column sampling-based discrete supervised hashing (COSDISH) [14] uses column sampling based on semantic similarity, and decomposes the problem into a sub-problem to simplify solution.\nThe optimization of binary codes leads to a mixedinteger programming problem involving integer and noninteger variables, which is an NP-hard problem in general [28]. Therefore, many methods discard the discrete constraints, or transform the problem into a relaxed problem, i.e., a linear programming problem [26]. This relaxation significantly simplifies the problem, but is known to affect classification performance [28].\n1\nRecent research has introduced a type of supervised discrete hashing (SDH) [28, 34] that directly learns binary codes without relaxation. SDH is a state-of-the-art method because of its ease of implementation, reasonable computation time for learning, and better performance over other state-of-the-art supervised hashing methods. To solve discrete problems, SDH uses a discrete cyclic coordinate descent (DCC) method, which is an approximate solver of 0-1 quadratic integer programming problems."}, {"heading": "1.1. Contributions and advantages", "text": "In this study, we first analyze the SDH model and point out that it can be simplified without performance degradation based on some preliminary experiments. We call the approximate model the fast SDH (FSDH) model. We analyze the FSDH model and provide a mathematically exact solution to it. The model simplification is validated through experiments involving several large-scale datasets.\nThe advantages of the proposed method are as follows:\n\u2022 Unlike SDH, it does not require alternating optimization or hyper-parameters, and is not initial valuedependent.\n\u2022 It is easier to implement than ITQ and is efficient in terms of computation time. FSDH can be implemented in three lines on MATLAB.\n\u2022 High bit scalability: its learning time and performance do not depend on the code length.\n\u2022 It has better precision and recall than other state-ofthe-art supervised hashing methods."}, {"heading": "1.2. Related work", "text": "As described subsequently, the SDH model poses a matrix factorization problem: F = W\u22a4B. The popular form of this problem is singular value decomposition (SVD) [8], and when W and B are unconstrained, the Householder method is used for computation. When W \u2265 0, nonnegative matrix factorization (NMF) is used [4].\nIn the case of the SDH model, B is constrained to {\u22121, 1} and W is unconstrained. In a similar problem setting, Slawski et al. proposed matrix factorization with binary components [31] and showed an application to DNA analysis for cancer research. B is constrained to {0, 1}, and indicates Unmethylated/Methylated DNA sequences. Furthermore, a similar model has been proposed in display electronics. Koutaki proposed binary continuous decomposition for multi-view displays [15]. In this model, multiple images F are decomposed into binary images B and a weight matrix W. An image projector projects binary 0- 1 patterns through digital mirror devices (DMDs), and the weight matrix corresponds to the transmittance of the LCD shutter."}, {"heading": "2. Supervised Discrete Hashing (SDH) Model", "text": "In this section, we introduce the supervised discrete hashing (SDH) model. Let xi \u2208 RM be a feature vector, and introduce a set of N (\u2265 M) training samples X = [x1, . . . ,xN ] \u2208 RM\u00d7N . Then, consider binary label information yi \u2208 {0, 1}C corresponding to xi, where C is the number of categories to classify. Setting the kth element to 1, [yi]k = 1, and the other elements to 0 indicates that the i-th vector belongs to class k. By concatenating N samples of yi horizontally, a label matrix Y = [y1, . . . ,yN ] \u2208 {0, 1}C\u00d7N is constructed."}, {"heading": "2.1. Binary code assignment to each sample", "text": "For each sample xi, an L-bit binary code bi\u2208{\u22121, 1}L is assigned. By concatenating N samples of bi horizontally, a binary matrix B = [b1, . . . ,bN ] \u2208 {\u22121, 1}L\u00d7N is constructed. The binary code bi is computed as\nbi = sgn ( P\u22a4xi ) , (1)\nwhere P\u2208RM\u00d7L (therefore P\u22a4\u2208RL\u00d7M ) is a linear transformation matrix and sgn(\u00b7) is the sign function. The major aim of SDH is to determine the matrix P from training samples X. In practice, feature vectors {xi} are transformed by preprocessing. Therefore, we denote the original feature vectors xorii and the transformed feature vectors xi."}, {"heading": "2.2. Preprocessing: Kernel transformation", "text": "The original feature vectors of training samples xorii (i = 1, . . . , N) are converted into the feature vectors xi \u2208 RM using the following kernel transformation \u03a6:\nxi = \u03a6(x ori i )\n= [ exp ( \u2212\u2016x ori i \u2212 a1\u20162\n\u03c3\n) , . . . , exp ( \u2212\u2016x ori i \u2212 am\u20162\n\u03c3\n)]\u22a4 ,\n(2) where am is an anchor vector obtained by randomly sampling the original feature vectors, am = xorirand. Then, the transformed feature vectors are bundled into the matrix form X = [x1, . . . ,xN ]."}, {"heading": "2.3. Classification model", "text": "Following binary coding by (1), we suppose that a good binary code classifies the class, and formulate the following simple linear classification model:\ny\u0302i = W \u22a4bi, (3)\nwhere W \u2208 RL\u00d7C is a weight matrix and y\u0302i is an estimated label vector. As mentioned above, its maximum index, argmink[y\u0302i]k, indicates the assigned class of xi."}, {"heading": "2.4. Optimization of SDH", "text": "The SDH problem is defined as the following minimization problem:\nmin B,W,P\n\u2016Y \u2212W\u22a4B\u20162 + \u03bb\u2016W\u20162 + \u03bd\u2016B\u2212P\u22a4X\u20162, (4)\nwhere \u2016 \u00b7 \u2016 is the Frobenius norm, and \u03bb \u2265 0 and \u03bd \u2265 0 are balance parameters. The first term includes the classification model explained in Sec. 2.3. The second term is a regularizer for W to avoid overfitting. The third term indicates the fitting errors due to binary coding.\nIn this optimization, it is sufficient to compute P, i.e., if P is obtained, B can be obtained by (1), and W can be obtained from the following simple least squares equation:\nW = ( BB\u22a4 + \u03bbI )\u22121 BY\u22a4. (5)\nHowever, due to the difficulty of optimization, the optimization problem of (4) is usually divided into three subproblems of the optimization of B,W, and P. Thus, the following alternating optimization is performed:\n(i) Initialization: B is initialized, usually randomly.\n(ii) F-Step: P is computed by the following simple least squares method:\nP = ( XX\u22a4 )\u22121 XB\u22a4. (6)\n(iii) W-Step: W is computed by (5).\n(iv) B-Step: After fixing P and W, equation (4) becomes:\nmin B\n\u2016Y\u20162 \u2212 2Tr ( YW\u22a4B ) +Tr ( B\u22a4WW\u22a4B )\n+ \u03bd ( \u2016B\u20162 \u2212 2Tr ( P\u22a4XB ) + \u2016P\u22a4X\u20162 )\n\u21d2 min B\nTr ( B\u22a4QB+ F\u22a4B ) ,\n(7)\nwhere\nQ=WW\u22a4\u2208RL\u00d7L, F=\u22122 ( WY + \u03bdP\u22a4X ) \u2208RL\u00d7N .\n(8) Note that Tr ( B\u22a4B ) = LN . The trace can be rewritten as\nmin {bi}\nN\u2211\ni=1\nb\u22a4i Qbi + f \u22a4 i bi, (9)\nwhere fi \u2208 RL is the i-th column vector of F. {bi} are actually independent of one another. Therefore, it reduces to the following 0-1 integer quadratic programming problem for each i-th sample:\n\u2200i min bi\u2208{\u22121,1}L b\u22a4i Qbi + f \u22a4 i bi. (10)\n(v) Iterate steps (ii)\u223c(iv) until convergence."}, {"heading": "3. Discussion of the SDH Model", "text": ""}, {"heading": "3.1. 0-1 integer quadratic programming problem", "text": "DCC method To solve (10), SDH uses a discrete cyclic coordinate descent (DCC) method. In this method, a onebit element of bi is optimized while fixing the other L \u2212 1 bits; the l-th bit bl is optimized as\nbl = \u2212sgn ( 2 \u2211\ni6=l Qi,lbi + fl\n) . (11)\nThen, all bits l = 1, . . . , L are optimized, and this procedure is repeated several times. In addition, the DCC method is prone to result in a local minimum because of its greediness. To improve it, Shen et al. proposed using a proximal operation of convex optimization [29].\nBranch-and-bound method In the case of a large number of bits L \u2265 32, solving (10) exactly is difficult because this problem is NP-hard. However, there exist a few efficient methods to solve the 0-1 integer quadratic programming problem. In [15], Koutaki used a branch-and-bound method to solve the problem. b is expanded into a binary tree of depth L, and the problem of (10) is divided into a sub-problem by splitting b = [b\u22a41 ,b \u22a4 2 ]\n\u22a4. At each node, the lower bound is computed and compared with the given best solution; child nodes can be excluded from the search.\nThe computation of the lower bound depends on the structure of Q,q, and b. To compute the lower bound in general, the linear relaxation method is a standard method, b \u2208 {\u22121, 1}L \u21d2 b \u2208 [\u22121, 1]L. In this case, the rough lower bound of the quadratic term in (10) can be provided by the minimum eigenvalues of Q. However, linear relaxation is useless in the SDH model because L > C in general, so the matrix Q = WW\u22a4 is rank deficient and, as a result, the minimum eigenvalue of Q becomes zero.\nEven if we can obtain an efficient algorithm, such as branch-and-bound and good lower bound, in the application of binary hashing, we still suffer from computational difficulties because code lengths L = 64, 128, or 256 bits are still too long to optimize, and they are used frequently."}, {"heading": "3.2. Alternating optimization and initial value dependence", "text": "Even if we optimize the binary optimization in (10), the resulting binary codes B are not always optimal ones because they depend on the other fixed variables W and P. In addition, alternating optimization is prone to cause a serious problem: a solution depends on the initial values, and may fall in a local minimum during the iterations, even if each\nstep of F-Step, W-Step and B-Step provides the optimal solution.\nFigure 1 shows an example of the optimization result for a simple version of the SDH model in (4) with a small number of bits (L,C,N) = (16, 10, 10). In this case, an exact solution is known and its minimum value is 0.94 (green line in Fig. 1). DCC (red lines) provides results for 10 randomized initial conditions. The full search (blue lines) provides the results of an exact full search in B-Step, where 216 = 65, 536 nodes are searched.\nIn spite of the small size of the problem, the cost function of conventional alternating solvers (DCC and full search) cannot find the exact value, and depends on initial values. Interestingly, the results of full search immediately fall into a local minimum, and are worse than those of DCC."}, {"heading": "4. Proposed Fast SDH Model", "text": "We introduce a new hashing model by approximating the SDH model, which utilizes the following assumptions:\nA1: The number of bits L of the binary code is a power of 2: L = 2l.\nA2: The number of bits is greater than the number of classes: L \u2265 C.\nA3: Single-labeling problem.\nA4: \u2016W\u22a4Y\u20162 \u226b \u03bd\u2016P\u22a4X\u20162 in (8).\nNote that assumptions A1\u223cA3 also become the limitations of the proposed model. In A4, SDH recommends that the parameter \u03bd be set to a very small value, such as \u03bd = 10\u22125 [28]. In practice, \u2016W\u22a4Y\u20162 ; 31.53 and \u03bd\u2016P\u22a4X\u20162 ; 0.013 in the CIFAR-10 dataset. Furthermore, when \u03bd = 0, almost the same results can be obtained in all datasets as shown in the experimental results in Sec. 5. We call this\napproximation using \u03bd = 0 the \u201cfast SDH (FSDH) approximation.\u201d\nUsing the FSDH approximation, we solve the following problem for each N -sample bi in B-Step:\n\u2200i min bi\u2208{\u22121,1}L b\u22a4i Qbi + f \u22a4 i bi,\nQ = WW\u22a4, F = \u22122W\u22a4Y, (12)\nwhere Q is a constant matrix and fi depends on label yi. By using the single-label assumption in A3, the number of kinds of yi is limited to C:\ny1 = [1, 0, . . . , 0] \u22a4, . . . ,yC = [0, 0, . . . , 1] \u22a4. (13)\nThus, it is sufficient to solve only C integer quadratic programming problems of (12) from N . In general, the number of samples N is larger than that of classes: N \u226b C, e.g., N = 59, 000 and C = 10. Thereby, the computational cost of B-Step becomes 5, 900 times lower. In other words, the FSDH approximation proposes the following:\nProposition 4.1 The FSDH approximation defines the SDH model to assign a binary code to each class.\nAfter obtaining the binary codes of each class B\u2032 = [b\u20321, . . . ,b \u2032 C ] \u2208 {\u22121, 1}L\u00d7C, the binary codes of all samples B can be constructed by lining up b\u2032i as\nB = [ b\u2032y1 , . . . ,b \u2032 yN ] . (14)\nAfter constructing B, the projection matrix P can be obtained by (6)."}, {"heading": "4.1. Analytical solutions of FSDH model", "text": "From Proposition 4.1, we found that it is sufficient to determine the binary code for each class. Furthermore, we can choose the optimal binary codes under the FSDH approximation as follows:\nLemma 4.2 If f(xi) is convex, the solution of\nmin {xi}\nN\u2211\ni\nf(xi) s.t. N\u2211\ni\nxi = L (15)\nis given by the mean value xi = L/N (i = 1, . . . , N).\nProof See Appendix A.\nTheorem 4.3 An analytical solution of FSDH B\u2032 is obtained as a Hadamard matrix.\nProof Using the FSDH approximation and label representations in (13), the SDH model in (4) becomes\nmin B\u2032,W\n\u2016I\u2212W\u22a4B\u2032\u20162 + \u03bb\u2016W\u20162, (16)\nwhere I \u2208 RC\u00d7C is an identity matrix. Using the solution of (16), i.e., W = ( B\u2032B\u2032\u22a4 + \u03bbI )\u22121 B\u2032, and the eigen-decomposition of B\u2032\u22a4B\u2032 = P\u22a4DP, we denote the eigenvalues as diag(D)= {\u03c3i}Ci=1 and then get \u2211C i=1 \u03c3i = Tr(D)=Tr(B\u2032\u22a4B\u2032)=LC as the trace of diagonal values. Then, equation (16) can be represented simply as\nmin B\u2032\nC\u2211\ni=1\n\u03bb\n\u03c3i + \u03bb s.t.\nC\u2211\ni=1\n\u03c3i = LC. (17)\nBy lemma 4.2, \u03c3i = L (i = 1, . . . , C). This implies that B\u2032 is an orthogonal matrix with binary elements {\u22121, 1}; in other words, B\u2032 \u2208 {\u22121, 1}L\u00d7C can be given by a submatrix of the Hadamard matrix H \u2208 {\u22121, 1}L\u00d7L.\nCorollary 4.4 The following characteristics can be obtained easily:\n\u2022 B\u2032 is independent of regularization parameter \u03bb (\u03bbinvariant).\n\u2022 The optimal weight matrix W of FSDH is given by the version of the scaled binary matrix B\u2032:W = 1\nL+\u03bb B\u2032.\n\u2022 The minimum value of (16) is given by L L+\u03bb .\nIn short, we can eliminate the W-Step, the alternating procedure, and the initial value dependence. An exact solution of the FSDH model can be obtained independent of the hyper-parameters \u03bb and \u03bd."}, {"heading": "4.2. Implementation of FSDH", "text": "Algorithm 1 and Figure 2, respectively, show the algorithm of FSDH and sample MATLAB code, which is simple and easy to implement. Figure 3 shows an example of B\u2032 and B. A Hadamard matrix of size 2k\u00d72k can be constructed recursively by Sylvester\u2019s method [33] as\nH2 = [ 1 1 1 \u22121 ] ,\nH2k =\n[ H2k\u22121 H2k\u22121\nH2k\u22121 \u2212H2k\u22121\n] (k \u2265 2).\n(18)\nFurthermore, Hadamard matrices of orders 12 and 20 were constructed by Hadamard transformation [10]. Fortunately, in applications of binary hashing, since L = 16, 32, 64, 128, 256, and 512 bits are used frequently, Sylvester\u2019s method suffices in most cases."}, {"heading": "4.3. Analysis of bias term of FSDH", "text": "We have already shown that B obtained from the Hadamard matrix minimizes two terms: \u2016Y \u2212W\u22a4B\u20162 + \u03bb\u2016W\u20162. Furthermore, we pay attention to how B affects the bias term \u2016B \u2212 P\u22a4X\u20162. In this subsection, we continue to analyze its behavior. We suppose that samples are\nAlgorithm 1 Fast Supervised Discrete Hashing (FSDH)\nInput: Pre-processed training data X and labels {yi}Ni=1: code lengthL, number of samples N , number of classes C. Output: Projection matrix P. 1: Compute Hadamard matrix H \u2208 {\u22121, 1}L\u00d7L 2: Let [b\u20321, . . . ,b \u2032 C ] be C columns of H.\n3: Construct B by bi = b\u2032yi . 4: Compute P from B and X by (6).\nsorted by label yi. Let P\u22a4 = BX\u22a4 ( XX\u22a4 )\u22121 be the bias term:\n\u2016B\u2212P\u22a4X\u20162 = \u2016B (I\u2212K) \u20162\n= Tr ( B\u22a4B ) \u2212 Tr ( BKB\u22a4 ) ,\n(19)\nwhere K=X\u22a4 ( XX\u22a4 )\u22121 X\u2208RN\u00d7N is a projection ma-\ntrix. Therefore, to reduce the bias term, it is better that Tr ( BKB\u22a4 ) has a large value. Then, using K = KK, we can rewrite it as\nTr ( BKB\u22a4 ) = Tr ( KB\u22a4BK ) , (20)\nwhere B\u22a4B is a block-diagonal matrix\nB\u22a4B = L   JN1 O\n. . . O JNC\n  , (21)\nJNk \u2208 1Nk\u00d7Nk are matricies with all elements equal to 1, and Nk is the number of samples with label yi = k. Using\nthese values, Tr ( KB\u22a4BK ) in (20) can be expressed as\nL\nN\u2211\ni=1\n[ (Ki,1 + . . .+Ki,N1) 2 + (Ki,N1+1 + . . .+Ki,N2) 2\n+ . . .+ ( Ki,NC\u22121+1 + . . .+Ki,NC )2] ,\n(22) where {Ki,j} with the same label yi = yj are summed up. Since the definition of K is K = X\u22a4 ( XX\u22a4 )\u22121 X, Kij can be regarded as the normalized correlation of xi and xj . Since samples with the same label must represent a similar feature vector, Tr ( KB\u22a4BK ) is assumed to be a large value. Figure 4 shows visualizations of matrices K and B\u22a4B for SDH and FSDH. High-correlation areas of K are partitioned by each class block. B\u22a4B of SDH includes a \u201cnegative\u201d block in the non-diagonal components, and reduces Tr ( KB\u22a4BK ) . On the other hand, the proposed FSDH shows clear blocks; the diagonal blocks take the value L and the non-diagonal blocks 0."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Datasets", "text": "We tested the proposed method on three large-scale image datasets: CIFAR-10 [16] 1, SUN-397 [36] 2, and MNIST [18] 3. The feature vectors of all datasets were normalized. A multi-labeled NUS-WIDE dataset was not included due to the limitation that the proposed method can be applied only to single-label problems.\nCIFAR-10 includes labeled subsets of 60,000 images. In this test, we used 512-dimensional GIST features [25] extracted from the images. N =59, 000 training samples and 1,000 test samples were used for evaluation. The number of classes was C = 10, and included \u201cairplane\u201d, \u201cautomobile\u201d, \u201cbird\u201d, . . ., etc.\nSUN-397 is a large-scale image dataset for scene recognition with 397 categories, and consists of 108,754 labeled images. We extracted 10 categories with C = 10 and\n1https://www.cs.toronto.edu/ kriz/cifar.html 2http://groups.csail.mit.edu/vision/SUN/ 3http://yann.lecun.com/exdb/mnist/\nN = 5, 000 training samples. A total of 500 training samples per class and 1,000 test samples were used. We used 512-dimensional GIST features extracted from the images. Since we used C = 10, we called the dataset \u201cSUN-10\u201d in this study.\nMNIST includes an image dataset of handwritten digits. The feature vectors we used were given by 28 \u00d7 28 = 784 [pix] of data that were normalized. The number of classes was C = 10, i.e., \u20180\u2019 \u223c \u20189\u2019 digits. We used N = 30, 000 training samples and 1,000 test samples for evaluation."}, {"heading": "5.2. Comparative methods and settings", "text": "The proposed method was compared with four stateof-the-art supervised hashing methods: CCA-ITQ, CCALSH, SDH, and COSDISH [14]. Unsupervised or semisupervised methods were not assessed. All methods were implemented in MATLAB R2012b and tested on an Intel i7-4770@3.4 GHz CPU with DDR3 SDRAM@32 GB.\nCCA-ITQ and LSH: ITQ and LSH are state-of-the-art binary hashing methods. They can be converted into supervised binary hashing methods by pre-processing feature vectors X using label information. Canonical correlation analysis (CCA) transformation was performed and feature vectors were normalized and set to zero mean. They generated the projection matrix P, and binary codes were assigned by (1).\nCOSDISH is a recently proposed supervised hashing method. COSDISH generates the projection matrix P, as does ITQ. The feature vectors are transformed so they\nhave zero mean and normalized through variance in preprocessing. We used open-source MATLAB code 4.\nSDH is a state-of-the-art supervised hashing method. We used \u03bb=1 and \u03bd =10\u22125 with the maximum number of iterations set to 5, anchor points M =1, 000 (SDH1000) and M = 3, 000 (SDH3000), and kernel parameter \u03c3 = 0.4 for all datasets. SDH generated the projection matrix P, and binary codes were assigned by re-projection (1). Furthermore, to show the validity of the FSDH approximation, we\n4http://cs.nju.edu.cn/lwj/\nevaluated the case where \u03bd = 0 (SDH1000,\u03bd=0). We used open-source MATLAB code 5.\nFSDH: The proposed method used the same parameters as SDH: anchor points M = 1, 000 (FSDH1000) and M = 3, 000 (FSDH3000), and kernel parameter \u03c3=0.4 for all datasets. Moreover, we used M = 5, 000 (FSDH5000). FSDH generated the projection matrix P and assigned binary codes through re-projection (1), as in SDH. Our code\n5https://github.com/bd622/DiscretHashing\nwill be made available to the public 6, and is shown in Fig. 2."}, {"heading": "5.3. Results and discussion", "text": "Precision and recall were computed by calculating the Hamming distance between the training samples and the test samples with a Hamming radius of 2. Figure 6 shows the results, in terms of precision, recall, and the mean average of precision (MAP), of the Hamming ranking for all methods and the three datasets. Code lengths of L = 16, 32, 64, 96, and 128 were evaluated.\nCIFAR-10: COSDISH shows the best MAP. FSDH5000 yielded the best precision and recall. Although COSDISH showed a satisfactory MAP, the precision was low. In SDH and FSDH, increasing the number of anchor points improved the performance. As the code length increases, SDH reduces precision. However, FSDH maintains high precision and recall. This is a significant advantage of the proposed method. In general, by increasing the code length, precision tends to decrease with such a narrow threshold of a Hamming radius of 2.\nSUN-10: In this dataset, the results for FSDH were significantly better. In particular, the recall rates of FSDH remained high in spite of long code lengths. When the SDH and FSDH had the same number of anchor points, FSDH was clearly superior. The MAP of COSDISH was comparable to that of SDH; however, the precision and recall of COSDISH were not as good as those of CIFAR-10.\nMNIST: FSDH yielded the best results in all datasets with the same trends. It retained high precision and recall even with large values of code length.\nThe graphs on the right of Fig. 6 show the precisionrecall ROC curves based on Hamming ranking. FSDH shows better performance than SDH with the same number of anchor points. In particular, the SUN dataset yielded distinct results compared with the other methods."}, {"heading": "5.3.1 Validation of FSDH approximation", "text": "Table 1 shows the comparative results of SDH1000 with \u03bd = 10\u22125 and SDH1000 with \u03bd = 0. For all datasets,\n6https://github.com/goukoutaki/FSDH\nthe results of SDH1000 and SDH1000 with \u03bd = 0, were almost identical. Table 2 shows \u2016W\u22a4Y\u20162 and \u03bd\u2016P\u22a4X\u20162 of SDH1000 for all datasets with L = 64 and after optimization. We can confirm \u2016W\u22a4Y\u20162 \u226b \u03bd\u2016P\u22a4X\u20162. This means that the FSDH approximation was appropriate for supervised hashing."}, {"heading": "5.3.2 Computation time", "text": "Table 3 shows the computation time of each method for CIFAR-10. The time for FSDH1000 was almost identical to that of CCA=ITQ. As the number of anchors increased, the computational time increased for SDH and FSDH. The computational time for SDH and COSDISH increased with the code length. The number of iterations of the DCC method depended on the code length."}, {"heading": "5.3.3 Bit scalability and larger classes", "text": "Table 4 shows the comparative results in terms of computational time and performance with a wide range of code lengths L = 32 \u223c 1024 for the CIFAR-10 dataset. N = 10, 000 training samples, 1,000 test samples, and 1,000 anchors were used. The computation time of FSDH was almost identical in terms of code length because the main computation in FSDH involved matrix multiplication and inversion ( XX\u22a4 )\u22121 of (6). In practice, the inverse matrix was not computed directly, and Cholesky decomposition was performed. On the contrary, the computation time for SDH exponentially increased and precision decreased significantly. This means that the DCC method fell into local minima in cases of large code length.\nIn general, large bits are useful for a large number of classes. Table 5 shows the results of larger classes of the SUN dataset. L = 512-bits, C = 397 classes and N=79, 400 training samples are used. FSDH achieves high precision, high MAP and lower computational time compared with SDH. When M =20, 000 is used, MAP=0.442 can be ontained by FSDH. Here, SDH was not able to finish after three days of computation in our computational environment. In the experiments, we found that a large number of anchor points can improve performance. However it requres more computation. Therefore FSDH can use a large number of anchor points in a realistic computation time compared with SDH. For reference, we refer to the results of fast supervised hashing (FSH), LSVM-b and TopRSBC+SGD which are reprinted from [20, 2, 32]. Although FSDH outperforms those methods, note that those methods use different computational environments, feature vectors and code lengths."}, {"heading": "6. Conclusion", "text": "In this paper, we simplified the SDH model to an FSDH model by approximating the bias term, and provided exact solutions for the proposed FDSH model. The FSDH approximation was validated by comparative experiments with the SDH model. FSDH is easy to implement and outperformed several state-of-the-art supervised hashing methods. In particular, in the case of large code lengths, FSDH can maintain performance without losing precision. In future work, we intend to use this idea for other hashing models.\nAppendix"}, {"heading": "A. Proof of Lemma 4.2", "text": "The optimization problem in (15) is known as the resource allocation problem [1, 5, 13]. Here we present a simple proof for the solution.\nThe constraint \u2211N\ni=1 xi = L can be regarded as a surface equation in an N-dimensional space (x1, x2, . . . , xN ). On the other hand, the gradient vector of the object function\u2211N\ni=1 f(xi) is defined as\ng := [f \u2032(x1), f \u2032(x2), . . . , f \u2032(xN )] \u22a4 (23)\nwhere f \u2032(\u00b7) is the differentiated version of f(\u00b7) and the ith element (the gradient in the i-th direction) is given by \u2202 \u2202xi \u2211 j f(xj) = \u2211 j \u2202xj \u2202xi \u2202 \u2202xj f(xj), and \u2202xj \u2202xi\nbecomes 1 if i = j or 0 if i 6= j.\nThen, the gradient along the surface is obtained as the projection of g onto the surface, and computed as the innerproduct of g and a set of vectors {n\u22a5} perpendicular to the normal vector of the surface:\nn := 1\u221a N [1, 1, . . . , 1]\u22a4 \u2208 RN , (24)\nand the projected gradient g\u22a4n\u22a5 becomes 0 at the global extermum point on the surface. This also indicates g and n are completely parallel and their inner-product becomes\n( g \u2016g\u20162 )\u22a4 n = 1 \u21d2 g\u22a4n = \u2016g\u20162. (25)\nSubstituting Eqs. (23) and (24) into (25), we get\n1\u221a N\n\u2211\ni\nf \u2032(xi) =\n\u221a\u2211\ni\nf \u2032(xi)2 (26)\nAdditionally, when we express f \u2032(xi) as \u221a f \u2032(xi)2 and 1\u221a\nN\nas 1 N\n\u221a N , we get\n1 N\n\u2211\ni\n\u221a f \u2032(xi)2 =\n\u221a 1\nN\n\u2211\ni\nf \u2032(xi)2. (27)\nThe shape of this equality actually corresponds to Jensen\u2019s inequality: \u2211 i pih(yi) \u2265 h( \u2211 i piyi) where \u2211 i pi = 1, and the equality holds if and only if {yi} i.e. {f \u2032(xi)2} are all equal:\nf \u2032(x1) 2 = f \u2032(x2) 2 = . . . = f \u2032(xN ) 2 (28)\nAdditionally, when f(\u00b7) is a convex function, f \u2032(\u00b7) becomes an injective function because f \u2032\u2032(\u00b7) \u2265 0 is a monotonically increasing function. Also, if the sign of f \u2032(\u00b7) does not change within the valid range of xi (the case considered in this paper), f \u2032(\u00b7)2 becomes injective. Hence,\nx1 = x2 = . . . = xN . (29)\nFinally, substituting this (29) into the condition \u2211N\ni=1 xi = L, we get\n\u2200i xi = L\nN . (30)"}, {"heading": "B. Complete data of the experimental results", "text": "B.1. Recall, precision and MAP\nTables 6\u223c8 show recall, precision and MAP for all datasets when L = 16, 32, 64, 96 and 128. These were computed by calculating the Hamming distance between the training samples and the test samples with a Hamming radius of 2.\nB.2. ROC curves\nFigure 6 shows precision-recall ROC curves based on Hamming distance ranking for all datasets when L = 16 \u223c 128."}, {"heading": "C. Loss comparison", "text": "We define W-loss and P-loss of the SDH model in (4) as follows:\nW-loss = \u2016Y \u2212W\u22a4B\u20162, P-loss = \u2016B\u2212P\u22a4X\u20162.\n(31)\nTables 9 \u223c 11 show each loss of SDH and FSDH after optimization for the CIFAR-10, SUN-10 and MNIST datasets. As described in sec.4.1, FSDH can minimize W-loss exactly. Therefore, for all datasets, FSDH results in a lower value of W-loss than SDH. Furthermore, as described in sec.4.3, FSDH can also reduceP-loss. For CIFAR-10, SDH results in a lower value of P-loss than FSDH. For SUN-10 and MNIST, FSDH results in a lower value of P-loss than SDH."}], "references": [{"title": "Applied dynamic programming", "author": ["R.E. Bellman", "S.E. Dreyfus"], "venue": "Princeton University Press,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1962}, {"title": "Supervised hashing with error correcting codes", "author": ["F. Cakir", "S. Sclaroff"], "venue": "ACM MM, pages 785\u2013788,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "BRIEF: binary robust independent elementary features", "author": ["M. Calonder", "V. Lepetit", "C. Strecha", "P. Fua"], "venue": "ECCV, pages 778\u2013792,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalizednnonnegative matrix approximations with Bregman divergences", "author": ["I.S. Dhillon", "S. Sra"], "venue": "NIPS, pages 283\u2013290,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "The art and theory of dynamic programming", "author": ["S.E. Dreyfus", "A.M. Law"], "venue": "Academic Press,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1977}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "Int. Conf. Very Large Data Bases (VLDB), pages 518\u2013529,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast and compact hamming distance index", "author": ["S. Gog", "R. Venturini"], "venue": "Int. ACM SIGIR Conf. Research & Devel. Info. Retriev., pages 285\u2013294,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix computations (3rd ed.)", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Iterative quantization: a procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE T. PAMI, 35(12):2916\u20132929,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "R\u00e9solution d\u0301une question relative aux d\u00e9terminants", "author": ["J. Hadamard"], "venue": "Bulletin Sci. Math., 17:240\u2013246,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1893}, {"title": "Spherical hashing: Binary code embedding with hyperspheres", "author": ["J.P. Heo", "Y. Lee", "J. He", "S.F. Chang", "S.E. Yoon"], "venue": "IEEE T. PAMI, 37(11):2304\u20132316,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Relations between two sets of variables", "author": ["H. Hotelling"], "venue": "Biometrika, pages 312\u2013377,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1936}, {"title": "Resource allocation problems: algorithms approaches", "author": ["T. Ibaraki", "N. Katoh"], "venue": "The MIT Press,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1988}, {"title": "Column sampling based discrete supervised hashing", "author": ["W.C. Kang", "W.J. Li", "Z.H. Zhou"], "venue": "AAAI, pages 1230\u20131236,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Binary continuous image decomposition for multi-view display", "author": ["G. Koutaki"], "venue": "ACM TOG, 35(4):69:1\u201369:12,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, Univ. Toronto,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "NIPS, pages 1042\u2013 1050,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, pages 2278\u20132324,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning hash functions using column generation", "author": ["X. Li", "G. Lin", "C. Shen", "A. van den Hengel", "A. Dick"], "venue": "In ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Fast supervised hashing with decision trees for high-dimensional data", "author": ["G. Lin", "C. Shen", "Q. Shi", "A. van den Hengel", "D. Suter"], "venue": "In CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1978}, {"title": "Discrete graph hashing", "author": ["W. Liu", "C. Mu", "S. Kumar", "S.-F. Chang"], "venue": "NIPS, pages 3419\u20133427,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. fu Chang"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "CVPR, pages 2074\u20132081,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Supervised discriminative hashing for compact binary codes", "author": ["V.A. Nguyen", "J. Lu", "M.N. Do"], "venue": "ACM MM, pages 989\u2013992,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "IJCV, 42(3):145\u2013175,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Theory of linear and integer programming", "author": ["A. Schrijver"], "venue": "John Wiley & Sons, Inc.,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning binary codes for maximum inner product search", "author": ["F. Shen", "W. Liu", "S. Zhang", "Y. Yang", "H. Tao Shen"], "venue": "ICCV, pages 4148\u20134156,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised discrete hashing", "author": ["F. Shen", "C. Shen", "W. Liu", "H. Tao Shen"], "venue": "CVPR, pages 37\u201345,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast optimization method for general binary code learning", "author": ["F. Shen", "X. Zhou", "Y. Yang", "J. Song", "H.T. Shen", "D. Tao"], "venue": "IEEE T. Image Process. (TIP), 25(12):5610\u20135621,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Kernel-based supervised discrete hashing for image retrieval", "author": ["X. Shi", "F. Xing", "J. Cai", "Z. Zhang", "Y. Xie", "L. Yang"], "venue": "ECCV, pages 419\u2013433,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix factorization with binary components", "author": ["M. Slawski", "M. Hein", "P. Lutsik"], "venue": "NIPS, pages 3210\u2013 3218,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Top rank supervised binary coding for visual search", "author": ["D. Song", "W. Liu", "R. Ji", "D.A. Meyer", "J.R. Smith"], "venue": "ICCV, pages 1922\u20131930,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Thoughts on inverse orthogonal matrices, simultaneous sign successions, and tessellated pavements in two or more colours, with applications to Newton\u2019s rule, ornamental tile-work, and the theory of numbers", "author": ["J. Sylvester"], "venue": "Philos. Magazine, 34:461\u2013475,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1867}, {"title": "Supervised quantization for similarity search", "author": ["X. Wang", "T. Zhang", "G.-J. Qi", "J. Tang", "J. Wang"], "venue": "CVPR, pages 2018\u20132026,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "NIPS, pages 1753\u20131760,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "SUN database: large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "CVPR, pages 3485\u20133492,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 5, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 8, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 16, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 18, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 23, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 26, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 27, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 2, "context": "Furthermore, it is easy to compare a query in binary code with a binary code in a database because the Hamming distance between them can be computed efficiently by using bitwise operations that are part of the instruction set of any modern CPU [3, 7].", "startOffset": 244, "endOffset": 250}, {"referenceID": 6, "context": "Furthermore, it is easy to compare a query in binary code with a binary code in a database because the Hamming distance between them can be computed efficiently by using bitwise operations that are part of the instruction set of any modern CPU [3, 7].", "startOffset": 244, "endOffset": 250}, {"referenceID": 5, "context": "Locality-sensitive hashing (LSH) [6] is one of most popular methods.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "Iterative quantization (ITQ) [9] is another state-of-the-art binary hashing method.", "startOffset": 29, "endOffset": 32}, {"referenceID": 16, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 21, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 20, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 26, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 10, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 34, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 11, "context": "For example, canonical correlation analysis (CCA) [12] can transform feature vectors to maximize inter-class variation and minimize intra-class variation according to label information.", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "Kernel-based supervised hashing (KSH) [23] uses spectral relaxation to optimize the cost function through a sign function.", "startOffset": 38, "endOffset": 42}, {"referenceID": 29, "context": "KSH has also been improved to kernel-based supervised discrete hashing (KSDH) [30].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "Supervised Discriminative Hashing [24] decomposes training samples into inter and intra samples.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "Column sampling-based discrete supervised hashing (COSDISH) [14] uses column sampling based on semantic similarity, and decomposes the problem into a sub-problem to simplify solution.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "The optimization of binary codes leads to a mixedinteger programming problem involving integer and noninteger variables, which is an NP-hard problem in general [28].", "startOffset": 160, "endOffset": 164}, {"referenceID": 25, "context": ", a linear programming problem [26].", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "This relaxation significantly simplifies the problem, but is known to affect classification performance [28].", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "Recent research has introduced a type of supervised discrete hashing (SDH) [28, 34] that directly learns binary codes without relaxation.", "startOffset": 75, "endOffset": 83}, {"referenceID": 33, "context": "Recent research has introduced a type of supervised discrete hashing (SDH) [28, 34] that directly learns binary codes without relaxation.", "startOffset": 75, "endOffset": 83}, {"referenceID": 7, "context": "The popular form of this problem is singular value decomposition (SVD) [8], and when W and B are unconstrained, the Householder method is used for computation.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "When W \u2265 0, nonnegative matrix factorization (NMF) is used [4].", "startOffset": 59, "endOffset": 62}, {"referenceID": 30, "context": "proposed matrix factorization with binary components [31] and showed an application to DNA analysis for cancer research.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Koutaki proposed binary continuous decomposition for multi-view displays [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "proposed using a proximal operation of convex optimization [29].", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "In [15], Koutaki used a branch-and-bound method to solve the problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "In A4, SDH recommends that the parameter \u03bd be set to a very small value, such as \u03bd = 10\u22125 [28].", "startOffset": 90, "endOffset": 94}, {"referenceID": 32, "context": "A Hadamard matrix of size 2k\u00d72k can be constructed recursively by Sylvester\u2019s method [33] as", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "Furthermore, Hadamard matrices of orders 12 and 20 were constructed by Hadamard transformation [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "We tested the proposed method on three large-scale image datasets: CIFAR-10 [16] 1, SUN-397 [36] 2, and MNIST [18] 3.", "startOffset": 76, "endOffset": 80}, {"referenceID": 35, "context": "We tested the proposed method on three large-scale image datasets: CIFAR-10 [16] 1, SUN-397 [36] 2, and MNIST [18] 3.", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "We tested the proposed method on three large-scale image datasets: CIFAR-10 [16] 1, SUN-397 [36] 2, and MNIST [18] 3.", "startOffset": 110, "endOffset": 114}, {"referenceID": 24, "context": "In this test, we used 512-dimensional GIST features [25] extracted from the images.", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "The proposed method was compared with four stateof-the-art supervised hashing methods: CCA-ITQ, CCALSH, SDH, and COSDISH [14].", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "442 3542 FSH [20] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "142 29624 LSVM-b [2] 0.", "startOffset": 17, "endOffset": 20}, {"referenceID": 31, "context": "042 Top-RSBC+SGD [32] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 19, "context": "For reference, we refer to the results of fast supervised hashing (FSH), LSVM-b and TopRSBC+SGD which are reprinted from [20, 2, 32].", "startOffset": 121, "endOffset": 132}, {"referenceID": 1, "context": "For reference, we refer to the results of fast supervised hashing (FSH), LSVM-b and TopRSBC+SGD which are reprinted from [20, 2, 32].", "startOffset": 121, "endOffset": 132}, {"referenceID": 31, "context": "For reference, we refer to the results of fast supervised hashing (FSH), LSVM-b and TopRSBC+SGD which are reprinted from [20, 2, 32].", "startOffset": 121, "endOffset": 132}, {"referenceID": 0, "context": "The optimization problem in (15) is known as the resource allocation problem [1, 5, 13].", "startOffset": 77, "endOffset": 87}, {"referenceID": 4, "context": "The optimization problem in (15) is known as the resource allocation problem [1, 5, 13].", "startOffset": 77, "endOffset": 87}, {"referenceID": 12, "context": "The optimization problem in (15) is known as the resource allocation problem [1, 5, 13].", "startOffset": 77, "endOffset": 87}], "year": 2016, "abstractText": "In this paper, we propose a learning-based supervised discrete hashing method. Binary hashing is widely used for large-scale image retrieval as well as video and document searches because the compact representation of binary code is essential for data storage and reasonable for query searches using bit-operations. The recently proposed Supervised Discrete Hashing (SDH) efficiently solves mixed-integer programming problems by alternating optimization and the Discrete Cyclic Coordinate descent (DCC) method. We show that the SDH model can be simplified without performance degradation based on some preliminary experiments; we call the approximate model for this the \u201cFast SDH\u201d (FSDH) model. We analyze the FSDH model and provide a mathematically exact solution for it. In contrast to SDH, our model does not require an alternating optimization algorithm and does not depend on initial values. FSDH is also easier to implement than Iterative Quantization (ITQ). Experimental results involving a large-scale database showed that FSDH outperforms conventional SDH in terms of precision, recall, and computation time.", "creator": "LaTeX with hyperref package"}}}