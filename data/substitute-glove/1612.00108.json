{"id": "1612.00108", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "When to Reset Your Keys: Optimal Timing of Security Updates via Learning", "abstract": "Cybersecurity is increasingly because as advanced and prolonged bomb. As appear killed how indeed designed that verifiably a system (few a critical research, e. /. , a mode account) criticized, it true crucial well the defender way keep computer-based its agency monetary to pushed a balance between next treatment the being compromised and the cost of policy news. Moreover, these judicial there all their come made by limited and begin harmonic occur must the radio-controlled unique a effective armed. In unlike take groups attacks, some one optimal explain policy to specifying reports has both devices 2001 cybersecurity. Examples include key horizontal, password change, application similar patches, or electronic rifle thoughtful. However, rigor studies same optimal timing smaller rare. Further, existing explore typically tended. a recent - definition bomber typical that as known to the milan, for makes often might the similar in practice. In the work, happen will in further effort rapidly achieving numerical exact of measures updates between during though of found stealthy attacks. We consider called variant of seen supporter FlipIt game simplified turned asymmetric feedback and found launching did distribution, being alternative a commission components to consecutive security clash. The defender ' on no fact then envisioned as a time associative harem problem four hence made. We inferred of hopes bound network physical changing those accomplish low gesture compares with probabilities sweeps response strategies though let five has variously over threatened although reinvested mostly known.", "histories": [["v1", "Thu, 1 Dec 2016 01:43:24 GMT  (410kb)", "https://arxiv.org/abs/1612.00108v1", "9 pages, 2 figures; accepted by the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17), San Francisco, CA, USA, Feb. 2017"], ["v2", "Fri, 2 Dec 2016 18:26:18 GMT  (484kb)", "http://arxiv.org/abs/1612.00108v2", "9 pages, 2 figures; accepted by the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17), San Francisco, CA, USA, Feb. 2017"]], "COMMENTS": "9 pages, 2 figures; accepted by the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17), San Francisco, CA, USA, Feb. 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CR", "authors": ["zizhan zheng", "ness b shroff", "prasant mohapatra"], "accepted": true, "id": "1612.00108"}, "pdf": {"name": "1612.00108.pdf", "metadata": {"source": "CRF", "title": "When to Reset Your Keys: Optimal Timing of Security Updates via Learning", "authors": ["Zizhan Zheng", "Ness B. Shroff", "Prasant Mohapatra"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 2.\n00 10\n8v 2\n[ cs\n.L G\n] 2\nD ec\n2 01"}, {"heading": "Introduction", "text": "Malicious attacks are constantly evolving to inflict increasing levels of damage on the nation\u2019s infrastructure systems, cooperate IT systems, and our digital lives. For example, the Advanced Persistent Threat (APT) has become a major concern to cybersecurity in the past few years. APT attacks exhibit two distinguishing behavior patterns (van Dijk et al. 2013) that make them extremely difficult to defend using traditional techniques. First, these attacks are often funded well and persistent. They attack a target system (or a critical resource) periodically with the goal to compromise it completely e.g., by stealing full cryptography keys. Second, the attacks can be highly adaptive. In particular, they often act covertly, e.g., by operating in\nCopyright \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\na \u201clow-and-slow\u201d fashion (Bowers et al. 2014), to avoid immediate detection and obtain long-term advantages.\nFrom the defender\u2019s perspective, an effective way to thwart continuous and stealthy attacks is to update its security measures periodically to strike a balance between the risk of being compromised and the cost of updates. The primary challenge, however, is that such decisions must often be made with limited and delayed feedback because of the covert nature of the attacker. In addition to thwarting targeted attacks, such an optimal timing problem with incomplete information is crucial in various cybersecurity scenarios, e.g., key rotation (van Dijk et al. 2013), password changes (Tan and Xia 2016), application of patches (Beattie et al. 2002), and virtual machine refreshing (Juels et al. 2016). For example, Facebook receives approximately 600,000 \u201ccompromised logins\u201d from impostors every day (Barnett 2011). An efficient approach to stop these attacks is to ask users to update their passwords when the risk of attack is high.\nAlthough time-related tactical security choices have been studied since the cold war era (Blackwell 1949), rigorous study of timing decisions in the face of continuous and stealthy attacks is relatively new. In 2012, in response to an APT attack on it, the RSA lab proposed the FlitIt game, which was one of the first models to study timing decisions under stealthy takeovers. The FlipIt game model abstracts out details about concrete attack and defense operations by focusing on the stealthy and persistent nature of players. The basic model considers two players, each of whom can \u201cflip\u201d the state of a system periodically at any time with a cost. A player only learns the system state when she moves herself. The payoff of a player is defined as the fraction of time when the resource is under its control less the total cost incurred.\nThe FlipIt game captures the stealthy behavior of players in an elegant way by allowing various types of feedback structures. In the basic model where neither player gets any feedback during the game and each move flips the state of the resource instantaneously, it is known that periodic strategies with random starting phases form a pair of best response strategies (van Dijk et al. 2013). As a variant of the basic model, an asymmetric setting is studied in (Laszka, Johnson, and Grossklags 2013) where the defender gets no feedback during the game while the attacker obtains immediate feedback after each defense but incurs a\nrandom attack time to take over the resource. In this setting, it is shown in (Laszka, Johnson, and Grossklags 2013) that periodic defense and immediate attack (or no attack) form a pair of best response strategies. However, little is known beyond these two cases. In particular, designing adaptive defense strategies with partial feedback remains an open problem.\nAlthough the FlipIt game provides a proper framework to understand the strategic behavior of stealthy takeover, it relies on detailed prior knowledge about the attacker. In particular, it requires parameters such as the amount of time needed to compromise a resource and the unit cost of each attack (or their distributions) to be fixed and known to the defender so that the equilibrium solution can be derived. These parameters limit the scope of the attack model, which, however, can be hard to verify before the game starts. To address this fundamental limitation, we propose to study online learning algorithms that make minimum assumptions about the attacker and learn an optimal defense strategy from the limited feedback obtained during the game. Given the advances in big data analytics and their applications in cybersecurity, it is feasible for the defender to obtain partial feedback even under stealthy attacks. Such a learning approach makes it possible to derive adaptive and robust defense strategies against unknown attacks where the type of the attacker is derived from a fixed but unknown distribution, as well as the more challenging dynamic attacks where the type of the attacker can arbitrarily vary over time.\nIn this work, we make a first effort towards achieving optimal timing of security updates in the face of unknown stealthy attacks. We consider a variant of FlipIt game with asymmetric feedback similar to (Laszka, Johnson, and Grossklags 2013), but with two key differences. First, we consider repeated unknown attacks with attacker\u2019s type sampled from an unknown distribution. Second, we assume that the defender obtains limited feedback about potential attacks at the end of each period. The defender\u2019s goal is to minimize the long-term cumulative loss. Our objective is to derive an adaptive defense policy that has a low regret compared with the optimal periodic defense policy when the attack time distribution is known. A key observation is that the set of defense periods that the defender can choose from are dependent in the sense that the loss from one defense period may reveal the potential loss from other periods, especially shorter ones. Moreover, two defense policies played for the same number of rounds may span different lengths of time, which has to be taken into account when comparing the policies. In this paper, we model the defender\u2019s problem as a time associate stochastic bandit problem with dependent arms, where each arm corresponds to one possible defense period. We derive optimal defense strategies for both the finite-armed bandit setting where the defense periods can only take a finite set of values, and the continuum-armed bandit setting where the defense periods can take any values from a non-empty interval.\nOur main contributions can be summarized as follows.\n\u2022 We propose a stochastic time associative bandit model for optimal timing of security updates in the face of un-\nknown attacks. Our model captures both the limited feedback about stealthy attacks and the dependence between different defense options.\n\u2022 We derive upper confidence bound (UCB) based policies for time associative bandits with dependent arms. Our policies achieve a regret of O (log(T (K + 1)) +K) for the finite arm case, where T is the number of rounds played and K is the number of arms, and a regret of O(T 2/3) for the continuous arm setting.\nOur learning model and algorithms are built upon the assumption that the defender can learn from frequent system compromises. This is reasonable for many online systems such as large online social networks and content providers and large public clouds, in which many customers are subject to similar attacks. In this setting, even if a single user is compromised occasionally, the system administrator can pool data collected from multiple users to obtain a reliable estimate quickly. For example, given the large number of attacks towards its users, Facebook can collect data from thousands of incidents of similar compromises in a short time. Our online learning algorithms can be used by Facebook to alert users to update their passwords when necessary."}, {"heading": "Related Work", "text": "Time-related tactical security choices have been studied since the cold war era (Blackwell 1949). However, the study of timing decisions in the face of continuous and stealthy attacks is relatively new. In particular, the FlipIt game (van Dijk et al. 2013) and its variants (Laszka, Johnson, and Grossklags 2013; Laszka et al. 2014) are among the few models that study this problem in a rigorous way. However, all of these models assume that the parameters about the attacker are known to the defender at the beginning of the game. A gradient-based Bayesian learning algorithm was recently proposed in (Tan and Xia 2016) for a setting similar to ours, where the failure time was assumed to follow a Weibull distribution with one unknown parameter. In contrast, we consider a general attack time distribution.\nMulti-armed bandit problems have been extensively studied for both the stochastic setting and the adversarial setting (Bubeck and Cesa-Bianchi 2012). Many variants of bandit models have been considered including bandits with side observations (Caron et al. 2012; Buccapatnam, Eryilmaz, and Shroff 2014). In the context of cybersecurity, bandit models have been applied to anomaly detection (Liu, Zhao, and Swami 2013) and stackelberg security games (Balcan et al. 2015). However, the only previous work that studies the time associative bandit model is (Gyo\u0308rgy et al. 2007), where the arms are assumed to be mutually independent. In contrast, we propose to model the optimal timing problem in cybersecurity as a time associative bandit problem with dependent arms and study algorithms that can exploit side-observations to improve performance."}, {"heading": "Model", "text": "We consider the following variant of the FlipIt game (van Dijk et al. 2013) with two players, a de-\nfender and an attacker, and a security sensitive resource to protect. The attacker is persistent in compromising the resource. In response, the defender updates its security measures, e.g., keys, passwords, etc., from time to time to thwart the attacker. Assume a continuous time horizon. At any time instance, either the defender or the attacker can make a move to take over the resource at some cost. At time \u03c4 , the resource is under the control of the player that makes the last move before \u03c4 . Let \u03c4t, t = 1, 2, ... denote the time instance of the t-th defense action, and xt = \u03c4t+1 \u2212 \u03c4t the t-th defense period. We assume \u03c41 = 0 without loss of generality. Each defense action (move) incurs a fixed cost CD, which is known to the defender. Let X denote the set of all possible defense periods. We assume that X \u2286 [xmin, xmax] with xmin > 0. See Figure 1 for an example.\nWe further make the following assumptions about the game: (1) The attack in round t takes a random time at to succeed, which is i.i.d. sampled from a distribution Fa that is initially unknown to the defender. In contrast, the defender recaptures the resource immediately once it makes a move, which is a reasonable assumption as it is usually much more time consuming to compromise a resource than updating its security measures. We may also interpret at as the awareness time that the attacker takes to discover a new vulnerability in the system. We assume that at is out of the control of the attacker but its distribution is known to the attacker. The attacker does not know the value of at until it successfully compromises the system in round t. (2) Whenever the defender makes a move, this fact is learned by the attacker immediately. On the other hand, the defender has delayed and incomplete feedback in the following sense. First, the defender only gets feedback at the end of each round. Second, at time \u03c4t+1, the defender learns the value of at if at < xt, that is only when an attack is observed. Thus, the game has asymmetric feedback, a common scenario in cybersecurity. (3) The attacker is myopic and does not have a move cost. Therefore, it always attacks immediately right after a security update. Our model and solutions can be extended to the case when the attacker is myopic but with a hidden move cost (see the Appendix).\nUnder the above assumptions, in each round t, the resource is fully protected if at \u2265 xt and is compromised for a duration of xt \u2212 at otherwise. The loss to the defender in round t is then defined as:\nl(xt, at) = f [(xt \u2212 at) +] + cd (1)\nwhere f(\u00b7) models the loss from attack and cd models the cost of each defense action. We assume that f(\u00b7) \u2208 [0, 1], f(0) = 0, and f(\u00b7) is increasing. For instance, we can consider (1) a binary loss function where l(xt, at) = 1 + cd if xt > at and l(xt, at) = cd otherwise; or (2) a linear loss function l(xt, at) = (xt\u2212at) +\nxmax + cd (where the xmax factor\nis introduced to normalize the loss value). The defender\u2019s objective is to minimize the long-term average loss defined as follows:\n\u03bbu = lim sup T\u2192\u221e\nE( \u2211T t=1 l(x u t , at))\n\u2211T t=1 x u t\n(2)\nwhere u denotes any defense policy and xut is the t-th defense period chosen by policy u. Let l(x) = Ea1(l(x, a1)) denote the expected loss of defense period x, and let \u03bb(x) = l(x) x denote the time average loss of a periodic policy with period x. We make two observations: (1) defending all the time is not necessarily a good option as it may incur a very high defense cost; (2) it can be shown that the periodic defense policy with period x\u2217 = minx\u2208X \u03bb(x) minimizes the long-term time average loss (Puterman 1994; Gyo\u0308rgy et al. 2007). However, this optimal policy cannot be found when the distribution of at is unknown. Let \u03bb\u2217 = \u03bb(x\u2217) denote the optimal loss. To find an optimal defense policy when the distribution of at is unknown, we adopt the time associative bandit model (Gyo\u0308rgy et al. 2007) by considering each defense period as an arm. For a defense policy {xt}, the (pseudo) regret for the first T rounds with respect to the optimal periodic policy can be defined as:\nRT = max x\u2208X E\n[\nT \u2211\nt=1\nl(xt, at)\u2212 \u03bb(x)xt\n]\n(3)\n=\nT \u2211\nt=1\nl(xt)\u2212 \u03bb \u2217\nT \u2211\nt=1\nxt (4)\nOur objective is to find a defense policy with low regret. Note that any learning algorithm with limT\u2192\u221e RTT = 0 minimizes the long-term loss as T \u2192 \u221e. We also note that even if l(xt, at) as a function of xt (for a fixed at) has a simple structure, the mean loss function l(xt) , Eat(l(xt, at)) may have a complicated form depending on the distribution of at. Therefore, previous works on linear and convex bandits cannot be directly applied to our problem. On the other hand, we observe that the defender may obtain sideobservations during the game, which can be utilized to design more efficient learning algorithms.\nSide observations: As we discussed before, the defender learns the value of at if at < xt (hence its loss as well) at the end of each round. From this feedback, the defender may get side observations in the following sense. Consider any round t. If at < xt, then the defender learns the value of at; therefore, it learns l(xi, at) for any xi \u2208 X if it has played xi instead of xt. On the other hand, if at \u2265 xt, the defender only learns the value of l(xi, at) = cd for any xi \u2264 xt, but not the value of l(xj , at) for xj > xt. This implies that playing an arm that corresponds to a longer defense period provides\nmore side observations about other arms. Our learning algorithm incorporates these side-observations to minimize the expected regret. Indeed, our algorithm and its regret bound apply to any loss function l(xt, at) where playing one period provides side-observations to all shorter periods.\nMultiple resources: Our model can be readily extended to consider multiple resources (nodes) subject to i.i.d. attacks, which can be used to model multiple users subject to independent attacks in an online system such as Facebook. In this case, samples from multiple nodes can be pooled together when choosing the next defense period for a node. Consider a system with N nodes that are subject to i.i.d. attacks with unknown attack times sampled from Fa. Let \u03c4st denote the time instance of the t-th security update on node s and xst = \u03c4s(t+1) \u2212 \u03c4st the t-th defense period for node s. Note that xst can be different for different s. Let ast denote the attack time in the t-th attack towards node s. Let l(xst, ast) denote the loss to the defender in round t over node s. When the nodes are subject to i.i.d. attacks, there is an optimal defense period x\u2217 for all nodes with minimum time average loss \u03bb\u2217, similar to the single node setting. The i.i.d. assumption may hold in practice because (1) some parameters such as the attack time may be out of the control of the attacker and can be approximated as i.i.d. random variables during the time horizon of the game; (2) an adversarial attacker may choose to avoid correlated attacks to make its behavior more unpredictable. Assume that the game is played for Ts rounds over node s, and let T = \u2211\ns Ts. Then the regret over the T rounds of play across all the nodes can be defined as RT = \u2211N s=1 \u2211Ts t=1 l(xst)\u2212 \u03bb \u2217 \u2211N s=1 \u2211Ts t=1 xst. Note that, when choosing xst, feedback from all the nodes received before \u03c4st can be used. Our online learning algorithms can be directly applied to this setting."}, {"heading": "Optimal Timing Algorithms", "text": "In this section, we present our learning algorithms for the optimal timing problem for both discrete and continuous defense periods."}, {"heading": "Discrete Defense Periods", "text": "We first consider the finite-armed setting where the set of defense periods is finite, denoted by X = {x1, ..., xK}. Let it denote the index of the arm played in round t, i.e., xi(t) is the defense period chosen for round t. Let ni(t) = \u2211t\ns=1 I(is = i) denote the number of plays of arm i during the first t rounds. Let li,t = 1ni(t) \u2211t s=1 I(is = i)l(xi(s), as) denote the average loss from arm i during the first t rounds,\nand \u03bbi,t = li,t xi the time average loss of arm i. To simplify the notation, we omit the subscript t in li,t and \u03bbi,t when it is clear from the context, and let li , l(xi) and \u03bbi , \u03bb(xi). Let \u2206i = li \u2212 xi\u03bb\u2217 denote the relative loss of playing arm i. Note that \u2206i\u2217 = 0 for an optimal arm i\u2217 and \u2206i \u2208 [0, 1] by our assumption about l. Then RT = \u2211 i6=i\u2217 \u2206iE(ni(T )). Let \u2206min , mini:\u2206i>0 \u2206i and \u2206max , maxi \u2206i. To derive an optimal defense policy, we consider the following variant of the improved upper confidence\nAlgorithm 1 Improved UCB algorithm for time-associative bandits with side observations\nInput: A set of periods X , the number of rounds T . Initialization: Set \u2206\u03030 = 1, X0 = X . for m = 0, 1, 2, ..., do\nx(1) = min{xi \u2208 Xm}; x(2) = max{xi \u2208 Xm}. Arm selection: If |Xm| = 1, play the single period in Xm until T . Else play the longest period in Xm until round min(nm, T ), where nm = \u2308 2\u03b3m log(T (K+1)\u2206\u0303 2 m)\n\u2206\u03032m\n\u2309\nand\n\u03b3m = ( 1 + x(2) x(1)\n)2\n; update li, \u03bbi for all xi \u2208 Xm.\nArm elimination: \u03bbm = minxi\u2208Xm ( \u03bbi + cm/xi )\nwhere cm = \u221a\nlog(T (K+1)\u2206\u03032m) 2nm\n. To getXm+1, delate all the periods xi \u2208 Xm such that li\u2212xi\u03bbm \u2265 minxj\u2208Xm lj\u2212xj\u03bbm+2 ( 1 + xj x(1) ) cm.\nReset: \u2206\u0303m+1 = \u2206\u0303m2 .\nbound based policy proposed in (Auer and Ortner 2010) for stochastic bandits. We modify the improved UCB policy to address the time associative regret while taking the dependence between arms into account.\nThe algorithm proceeds in multiple stages, where each stage involves multiple rounds (see Algorithm 1). In each stage m, as in the improved UCB policy, our policy estimates \u2206i by a value \u2206\u0303m, and maintains a set of active arms Xm. \u2206\u03030 is initialized to 1 and is halved in each stage. X0 initially contains all the arms. At the end of each stage m, a subset of arms are deleted from Xm according to their observed losses in previous rounds. Compared with the improved UCB policy for stochastic bandits, our policy has several key differences. First, in the arm selection phase, each active arm is played nm \u2212 nm\u22121 times in stage m in the improved UCB policy, where nm is a function of \u2206\u0303m and is chosen so that any suboptimal arm i is eliminated as soon as \u2206\u0303m < \u2206i2 with high probability. In contrast, only the longest period in Xm is played nm \u2212 nm\u22121 times in our policy, which provides side observations to all the shorter periods as we discussed above. For any arm xi \u2208 Xm, li is defined as if i is played in all the previous nm rounds. In addition, the definition of nm in our policy is different from the improved UCB policy. In particular, nm depends on the ratio of the maximum active period to the minimum active period, which is needed to bound the time associative regret. Second, in the arm elimination phase, we compare the relative losses of arms instead of average losses as in the improved UCB, since average loss alone does not take the length of a defense period into account. In particular, we estimate the relative loss of arm i by li,nm \u2212 xi\u03bbm, where \u03bbm is an estimate of \u03bb\u2217 defined by\n\u03bbm = min xi\u2208Xm\n( \u03bbi,nm + cm/xi )\n(5)\nwhere cm = \u221a\nlog(T (K+1)\u2206\u03032m) 2nm . The value of cm is chosen\nso that for all i, \u03bbi,nm is in the cm/xi-vicinity of \u03bbi with high probability.\nTo establish the regret bound of the algorithm, we need the following lemmas.\nLemma 1. (Chernoff-Hoeffding Bound (Hoeffding 1963)) Let X1, X2, ..., Xn be a sequence of independent random variables with support [a, b] and E(Xt) = \u00b5 for all Xt. Let Xn = 1 n \u2211n t=1 Xt. Then for any \u01eb > 0, we have\nP{Xn \u2265 \u00b5+ \u01eb} \u2264 e \u2212 2n\u01eb\n2\n(b\u2212a)2 ,\nP{Xn \u2264 \u00b5\u2212 \u01eb} \u2264 e \u2212 2n\u01eb\n2\n(b\u2212a)2 .\nLemma 2. Consider any stage m where there is an optimal arm i\u2217 \u2208 Xm. If li \u2264 li + cm for all xi \u2208 Xm and l\u2217 \u2265 li\u2217 \u2212 cm, then we must have \u03bb\u2217 \u2264 \u03bbm \u2264 \u03bb\u2217 + 2cm/xi\u2217 .\nProof. To see this, let xj \u2208 Xm be the arm that minimizes \u03bbj . Then we have \u03bb\u2217 \u2264 \u03bbj \u2264 \u03bbj + cm/xj = \u03bbm, and \u03bbm \u2264 \u03bbi\u2217 + cm/xi\u2217 \u2264 \u03bb\u2217 + 2cm/xi\u2217 .\nWe now show the following bound on the expected regret of Algorithm 1.\nTheorem 1. The expected regret of Algorithm 1 is at most 48\u03b3 log ( T (K+1) \u22062max\n4\n)\n\u2206min +\n\u2211\ni:\u2206i>0\n(\n\u2206i + 48 \u2206i\n)\n, where \u03b3 =\n(1 + xmaxxmin ) 2.\nProof. Without loss of generality, we assume that the optimal arm is unique and has index K , and sort the set of arms such that \u22061 \u2265 \u22062 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u2206K = 0. For any suboptimal arm i, let mi = min{m : \u2206\u0303m < 12\u2206i} denote the first stage in which \u2206\u0303m < 12\u2206i. We have 2\nmi = 1 \u2206\u0303mi \u2264 4\u2206i < 1\n\u2206\u0303mi+1 = 2mi+1. Note that m1 \u2264 m2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 mK\u22121.\nWe consider the following events similar to (Perchet and Rigollet 2013). Let Ai denote the event that the optimal arm has not been eliminated before stage mi, and Bi the event that every arm j \u2208 {1, 2, ..., i} has been eliminated in stage mj or before. We have A1 \u2287 A2 \u00b7 \u00b7 \u00b7 \u2287 AK\u22121 and B1 \u2287 B2 \u00b7 \u00b7 \u00b7 \u2287 BK\u22121. Let Ci = Ai \u2229 Bi for i \u2208 {1, 2, ...,K \u2212 1}. Under the event Ci, let Ui denote the contribution to the regret from arms {1, 2, ..., i} and Vi the contribution to the regret from arms {i + 1, ...,K \u2212 1}. We observe that Vi \u2264 T\u2206i+1. Let C0\ndenote the sample space. We then have\nRT = K\u22121 \u2211\ni=1\nP(Ci\u22121\\Ci)(Ui + Vi)\n\u2264 K\u22121 \u2211\ni=1\nUiP(Ci\u22121\\Ci) + K\u22121 \u2211\ni=1\nT\u2206iP(Ci\u22121\\Ci)\n\u2264 K\u22121 \u2211\ni=1\nUiP(Ci\u22121\\Ci) + K\u22121 \u2211\ni=1\nT\u2206iP(B c i \u2229Bi\u22121 \u2229Ai)\n+\nK\u22121 \u2211\ni=1\nT\u2206iP(A c i \u2229Ci\u22121) (6)\nWe bound each of the three terms in (6) as follows:\nFirst term in (6): Under the event Ci, each suboptimal arm j \u2208 {1, 2, ..., i} is eliminated on or before round nmj = \u2308\n2\u03b3mj log(T (K+1)\u2206\u0303 2 mj )\n\u2206\u03032mj\n\u2309\n. Among these arms, let j1, j2, ..., jk\ndenote the sequence of suboptimal arms played where xj1 > ... > xjk , and arm ji is eliminated in stage m \u2032 ji \u2264 mji . Let B , 2\u03b3 log ( T (K + 1) \u22062max\n4\n)\n. We then have\nUi \u2264 \u2206j1nm\u2032j1 + k \u2211\ni=2\n\u2206ji(nm\u2032ji \u2212 nm\u2032 ji\u22121 )\n\u2264\u2206j1nm\u2032j1 + k \u2211\ni=2\n\u2206ji\n(\n1 + 2\u03b3 log(T (K + 1)\u2206\u03032mji )\n\u2206\u03032mji\n\u2212 2\u03b3 log(T (K + 1)\u2206\u03032mji\u22121 )\n\u2206\u03032mji\u22121\n)\n\u2264\u2206j1nm\u2032j1 + k \u2211\ni=2\n\u2206ji + 4B\nk \u2211\ni=2\n\u2206\u0303mji\n(\n1\n\u2206\u03032mji \u2212 1 \u2206\u03032mji\u22121\n)\n\u2264\u2206j1nm\u2032j1 + k \u2211\ni=2\n\u2206ji + 4B\nk \u2211\ni=2 1.5\u2206\u0303mji\u22121 (\u2206\u0303mji\u22121 \u2212 \u2206\u0303mji ) \u2206\u0303mji \u2206\u0303 2 mji\u22121\n=\u2206j1nm\u2032j1 +\nk \u2211\ni=2\n\u2206ji + 6B k \u2211\ni=2\n(\n1\n\u2206\u0303mji \u2212 1 \u2206\u0303mji\u22121\n)\n\u2264 k \u2211\ni=1\n\u2206ji + 6B 1\n\u2206\u0303mjk\n\u2264 k \u2211\ni=1\n\u2206ji + 24B 1\n\u2206jk\nTherefore, \u2211K\u22121 i=1 UiP(Ci\u22121\\Ci) \u2264 \u2211K\u22121 i=1 \u2206i + 24B 1 \u2206min .\nSecond term in (6): Under the event Bci \u2229Bi\u22121 \u2229Ai, the optimal arm is not eliminated by mi, neither does arm i. We first note that if li \u2265 li \u2212 cmi and lK \u2264 lK + cmi hold, then arm i will be eliminated in round mi. Indeed, from the definitions of cm and nm, we have cmi \u2264 \u2206\u0303mi 2 \u221a\n\u03b3mi = \u2206\u0303mi+1\u221a \u03b3mi < \u2206i 4 \u221a \u03b3mi . Then from\nLemma 2, we have\nli \u2212 xi\u03bbmi \u2265li \u2212 xi(\u03bb\u2217 + 2cmi/xK) \u2265li \u2212 xi(\u03bb\u2217 + 2cmi/xK)\u2212 cmi =lK \u2212 xK\u03bb\u2217 +\u2206i \u2212 2\nxi xK cmi \u2212 cmi\n>lK \u2212 xK\u03bb\u2217 + 4\u221a\u03b3micmi \u2212 2 xi xK cmi \u2212 cmi \u2265lK \u2212 xK\u03bb\u2217 + 2 (\n1 + xK x(1)\n)\ncmi\n\u2265lK \u2212 xK\u03bbmi + 2 ( 1 + xK x(1) ) cmi\nwhere x(1) is the minimum active period in stage mi. It follows that arm i is eliminated in stage mi as claimed.\nIt follows that P(Bci \u2229 Bi\u22121 \u2229 Ai) \u2264 P(li < li \u2212 cmi ) + P(li\u2217 > li\u2217 + cmi) \u2264 1T (K+1)\u2206\u03032mi + 1 T (K+1)\u2206\u03032mi \u2264 1 T \u2206\u03032mi by the Chernoff-Hoeffding bound. Therefore, the second term in (6) can be bounded by \u2211\ni T\u2206i\n1 T \u2206\u03032mi \u2264\u2211 i 16 \u2206i .\nThird term in (6): Under the event Aci \u2229 Ci\u22121, every arm j \u2208 {1, 2, ...i\u22121} has been eliminated by stage mj and the optimal arm is eliminated by some arm k \u2265 i in some stage m\u2217 where mi\u22121 < m\u2217 \u2264 mi. We first claim that if lk \u2265 lk\u2212cm\u2217 and lK \u2264 lK+cm\u2217 hold, then the optimal arm is not eliminated by arm k in stage m\u2217. To see this, assume that the optimal arm is eliminated, which happens only when lK \u2212xK\u03bbm\u2217 \u2265 lk\u2212xk\u03bbm\u2217 +2 (\n1 + xk x(1)\n)\ncm\u2217 .\nFrom Lemma 2, we have:\nlk \u2212 xk\u03bb\u2217 \u2264lk + cm\u2217 \u2212 xk(\u03bbm\u2217 \u2212 2cm\u2217/xK) \u2264lk \u2212 xk\u03bbm\u2217 + cm\u2217 + 2\nxk xK cm\u2217\n\u2264lk \u2212 xk\u03bbm\u2217 + 2 ( 1 + xk x(1) ) cm\u2217 \u2212 cm\u2217\n\u2264lK \u2212 xK\u03bbm\u2217 \u2212 cm\u2217 \u2264lK \u2212 xK\u03bb\u2217\nwhich contradicts the fact that k is suboptimal. It follows that the probability that the optimal arm is eliminated by a fixed arm k \u2265 i in a fixed stage m\u2217 \u2264 mi is bounded by P(lk < lk \u2212 cm\u2217) + P(lK > lK + cm\u2217 ) \u2264 1T \u2206\u03032m\u2217 by the Chernoff-Hoeffding bound. Therefore, the third term in (6) is bounded by\nK\u22121 \u2211\ni=1\nmi \u2211\nm\u2217=mi\u22121+1\nK\u22121 \u2211\nk=i\n1\nT \u2206\u03032m\u2217 T\u2206i\n=\nmaxi mi \u2211\nm\u2217=0\n\u2211\nk:mk\u2265m\u2217\n1\nT \u2206\u03032m\u2217 T max h:mh\u2265m\u2217 \u2206h\n\u2264 maxi mi \u2211\nm\u2217=0\n\u2211\nk:mk\u2265m\u2217\n1\n\u2206\u03032m\u2217 4\u2206\u0303m\u2217\n= K\u22121 \u2211\ni=1\nmi \u2211\nm\u2217=0\n4\n\u2206\u0303m\u2217 \u2264\nK\u22121 \u2211\ni=1\n4 \u00b7 2mi+1 \u2264 K\u22121 \u2211\ni=1\n32\n\u2206i\nPutting all the three cases together, we get the desired regret bound.\nRemark 1. Our algorithm achieves a regret where the coefficient of the log(T ) term is independent of K , the\nnumber of arms. This is obtained by utilizing the side observations among arms. In contrast, a direct application of the UCB based policy for time-associative bandits in (Gyo\u0308rgy et al. 2007) to our problem leads to a regret of O (\n\u2211K i=1 \u03b3 log(T (K+1)) \u22062\ni\n)\n, where the log(T ) term has a co-\nefficient that is linear of K .\nRemark 2. In our numerical study, we also consider a variant of Algorithm 1 where in arm elimination phase, we delate all the periods xi \u2208 Xm such that li \u2212 xi\u03bbm \u2265 minxj\u2208Xm lj \u2212xj\u03bbm+4cm. By using a smaller confidence interval, this variant eliminates suboptimal arms more aggressively than Algorithm 1. Although we are not able to prove a regret bound for this variant, it exhibits even better performance than Algorithm 1 in our numerical study."}, {"heading": "Continuous Defense Periods", "text": "We next consider the case where the defense periods can take any real value in X = [xmin, xmax]. Note that in this case, the bound given in Theorem 1 can be very poor due to the large K and small \u2206i. Built upon Algorithm 1, we propose a new policy with a regret that is independent of K and \u2206i under the following assumption. Let l(x) = Ea1(l(x, a1)) denote the expected loss when a period x is played. We assume that l(x) is Lipschitz continuous: there exists a constant L \u2265 0 such that for any x1, x2 \u2208 X , |l(x1) \u2212 l(x2)| \u2264 L|x1 \u2212 x2|. For instance, when the attack time follows a uniform distribution in [a1, a2], and f(\u00b7) is binary, we have |l(x1)\u2212 l(x2)| \u2264 1a2\u2212a1 |x1\u2212x2|, and we can take L = 1a2\u2212a1 .\nOur algorithm is inspired by UCB for continuous bandits (UCBC) (Auer, Ortner, and Szepesva\u0301ri 2007). We first divide X into n subintervals of equal length, where n is a parameter to be determined (see Algorithm 2). Let xk , xmin + k xmax\u2212xmin n denote the longest period in the kth interval. We then apply Algorithm 1 to the set of arms I , {x1, ..., xn}.\nAlgorithm 2 Improved UCB based optimal timing with continuous periods\nInput: A set of periods X = [xmin, xmax], the number of rounds T , the number of subintervals n. Initialization: For k = 1, 2, ..., n, xk = xmin + k xmax\u2212xminn . Apply Algorithm 1 to I = {x1, x2..., xn}.\nDefine I1 , [xmin, x1] and Ik , (xk\u22121, xk] for 1 < k \u2264 n. For any x \u2208 X , let \u2206(x) , l(x)\u2212x\u03bb\u2217 denote the relative loss of x. Among xk \u2208 I , assume xk\u2217 has the minimum \u03bb(xk). Let \u2206\u2032(x) , l(x)\u2212 x\u03bb(xk\u2217 ) denote the relative loss of arm x with respect to xk\u2217 . It is clear that \u2206\u2032(x) \u2264 \u2206(x). We further have the following property about \u2206 and \u2206\u2032.\nLemma 3. \u2206(xk\u2217 ) \u2264 L\u2032n\u22121 and \u2206(x) \u2212\u2206\u2032(x) \u2264 L\u2032n\u22121, where L\u2032 = Lxmax(xmax\u2212xmin)xmin .\nFrom the lemma, we can establish the following performance bound for Algorithm 2.\nTheorem 2. The expected regret of the variant of the improved UCB policy for continuous bandits described in Algorithm 2 is at most 3L\u2032n\u22121T + 48\u03b3 log(T (n+1))L\u2032n\u22121 + 48n2 L\u2032 + n\u2206max. By taking n = T 1/3, we have RT \u2264 O(T 2/3).\nProofs of Lemma 3 and Theorem 2 are provided in the Appendix."}, {"heading": "Numerical Results", "text": "In this section, we demonstrate the advantages of our learning algorithms through numerical study. We use the following synthetic dataset. We assume that the attack time at follows an i.i.d. Weibull Distribution with CDF F (a) = 1 \u2212 e\u2212(a/\u03bb) b\nfor a \u2265 0 and F (a) = 0 for a < 0. This model has been used in reliability engineering (Mazzuchi and Soyer 1996) and cybersecurity (Tan and Xia 2016) to model failure times. Note that when b = 1, the Weibull Distribution becomes the exponential distribution. By setting b > 1, the model indicates that the failure rate increases with time. We set b = 2 in experiments. In each trial, \u03bb is chosen from the interval [1, 20] uniformly at random. We consider a 19 arm setting with xi evenly distributed in [1, 10] with a step size of 0.5. We consider both the binary loss function and the linear loss function mentioned in the model section. In both cases, we fix the defense cost to cd = 0.1. With these parameter settings, we observe that the best arm varies over the feasible defense periods when we vary \u03bb.\nWe focus on the case where side observations are available (without attack cost) and compare our algorithms with the UCB based time-associative bandit algorithm in (Gyo\u0308rgy et al. 2007) (TUCB) that do not consider side observations. We further consider a variant of TUCB that uses the TUCB policy to choose the arm to play in each round and obtains side-observations after each play (TUCB-side). This algorithm can be considered as the application of the UCB-N policy and the UCB-MaxN policy in (Caron et al. 2012) to the time associative bandit model (UCB-N and UCB-MaxN give the same policy under the dependence structure we consider.) For our algorithms, we evaluate both Algorithm 1 (TUCB-improved-side) and its variant discussed above (TUCB-improved-side-2). The results are averaged over 100 independent trials and are given in Figure 2. We note that the linear loss setting represents the harder case since it introduces smaller variances across arms. We observe that for both loss functions, our algorithms can significantly reduce\nlong-term regrets compared to TUCB and TUCB-side by carefully incorporating side observations. Moreover, TUCBimproved-side-2 achieves the best performance among the four algorithms."}, {"heading": "Acknowledgments", "text": "This research was supported in part by a grant from the Army Research Office AROW911NF-15-1-0277, and an Army Research Office MURI W911NF-12-1-0385."}, {"heading": "Appendix", "text": ""}, {"heading": "Proof of Lemma 3", "text": "Assume \u03bb\u2217 = \u03bb(x\u2217) for x\u2217 \u2208 Ij . By the definition of k\u2217, we have \u03bb(xk\u2217 ) = l(xk\u2217 ) xk\u2217 \u2264 l(xj)xj = \u03bb(xj). It follows that\n\u2206(xk\u2217 ) = l(xk\u2217)\u2212 xk\u2217\u03bb \u2217\n\u2264 l(xj) xk\u2217\nxj \u2212 xk\u2217\u03bb\n\u2217\n= xk\u2217\nxj (l(xj)\u2212 xj\u03bb\n\u2217)\n\u2264 xmax xmin \u2206(xj).\nMoreover,\n\u2206(xj) = l(xj)\u2212 xj\u03bb \u2217\n= l(xj)\u2212 xj l(x\u2217)\nx\u2217\n\u2264 l(xj)\u2212 l(x \u2217) \u2264 L|xj \u2212 x \u2217|\n\u2264 L xmax \u2212 xmin\nn\nTherefore, \u2206(xk\u2217 ) \u2264 Lxmaxxmin xmax\u2212xmin n . Similarly, \u2206(x) \u2212 \u2206\u2032(x) = x(\u03bb(xk\u2217 ) \u2212 \u03bb(x\u2217)) \u2264 x(\u03bb(xj) \u2212 \u03bb(x\u2217)) = x( l(xj) xj \u2212 l(x \u2217) x\u2217 ) \u2264 x x\u2217 (l(xj)\u2212 l(x \u2217)) \u2264 xmaxxmin L xmax\u2212xmin n ."}, {"heading": "Proof of Theorem 2", "text": "We split the set of intervals into two parts. Define S , {k \u2208 I : \u2206(xk) > 2L\u2032n\u22121}. We have RT = \u2211n\nk=1 \u2206(xk)E(nk(T )) where nk(T ) denotes the number of times that xk is played. Let RT = RT,1 + RT,2, where RT,1 = \u2211\nk 6\u2208S \u2206(xk)E(nk(T )) and RT,2 = \u2211\nk\u2208S \u2206(xk)E(nk(T )). It is easy to see that R1 \u2264 2L\u2032n\u22121T . On the other hand, we have\nRT,2 = \u2211\nk\u2208S\n\u2206(xk)E(nk(T ))\n\u2264 \u2211\nk\u2208S\n(\u2206\u2032(xk) + L \u2032n\u22121)E(nk(T ))\n\u2264 \u2211\nk\u2208S\n\u2206\u2032(xk)E(nk(T )) + L \u2032n\u22121T\nBy Theorem 1, we have \u2211 k\u2208S \u2206 \u2032(xk)E(nk(T )) \u2264 48\u03b3 log(T (n+1)) mink\u2208S \u2206\u2032(xk) + \u2211 k\u2208S ( \u2206\u2032(xk) + 48 \u2206\u2032(xk) ) . By Lemma 3, \u2206\u2032(xk) \u2265 \u2206(xk) \u2212 L\u2032n\u22121 \u2265 L\u2032n\u22121 for any k \u2208 S. It follows that, RT \u2264 3L\u2032n\u22121T + 48\u03b3 log(T (n+1)) L\u2032n\u22121 + 48n2 L\u2032 + n\u2206max. By taking n = T 1/3, we have RT \u2264 O(T 2/3)."}, {"heading": "Costly Attacks", "text": "Our model and solutions can be extended to a myopic attacker with a hidden attack cost ca. After observing the defender\u2019s move in the beginning of round t, the attacker attacks immediately if Eat [(xt \u2212 at)\n+] > ca, and does not attack in that round otherwise That is, it attacks only if its expected benefit is larger than the attack cost. Equivalently, we can assume there is a period x0 such that the attacker attacks only when xt > x0. We can further distinguish the following two cases.\n\u2022 Fixed Attack Cost: In this case, there is a fixed x0 that is unknown to the defender such that there is no attack in round t if xt \u2264 x0 is played (thus the defender only suffers from the defense cost). The defender\u2019s loss in round t is defined as:\nl(xt, at) =\n{\ncd if xt \u2264 x0, f [(xt \u2212 at)+] + cd if xt > x0.\n(7)\nNote that by setting x0 \u2264 xmin, this case reduces to the setting when there is no attack cost.\n\u2022 Random Attack Cost: In this case, x0 is i.i.d. sampled in each round from an unknown distribution. The defender\u2019s loss in round t is same as (7).\nWhen there is a fixed attack cost, playing a longer period xi provides side-observation to a shorter arm xj only when xj > x0. Since x0 is unknown, it is insufficient to only play the longest period in each round as we did before. Therefore, we modify Algorithm 1 by maintaining a set of periods Ym that the defender knows to be longer than x0 by stage m. We set Y0 = \u2205. Since x0 is fixed, whenever an attack is observed when playing a period xi, we know xj > x0 for all xj \u2265 xi. In each stage m, for each active period in Ym,\nthe algorithm plays the longest one only as before to exploit side-observations. However, each active period not in Ym requires further exploration. Therefore, they are also played the same number of times in each stage. But whenever an attack is observed on xi, all xj \u2265 xi are added to Ym and they don\u2019t need to be played separately any further. We can prove the following regret bound for the modified algorithm.\nTheorem 3. The expected regret of the above algo-\nrithm is at most \u2211 i:xi\u2264x0 Bi \u2206i + \u2211 i:xi>x0 min\n(\nBi \u2206i , \u2206ipi\n)\n+\n48\u03b3 log\n(\nT (K+1) \u22062max\n4\n)\n\u2206min + \u2211 i:\u2206i>0\n(\n\u2206i + 48 \u2206i\n)\n, where Bi =\n32\u03b3 log ( T (K + 1) \u22062i 4 ) , and pi , P(l(xi, at) > 0).\nProof. We adopt a similar argument as in the proof of Theorem 1. In particular, the last two terms in (6) can be bounded using the same argument. The only difference is in the first term. For every period xi < x0, we bound its regret\nup to stage mi by \u2206inmi \u2264 \u2206i\n\u2308\n2\u03b3 log(T (K+1)\u2206\u03032mi )\n\u2206\u03032mi\n\u2309\n<\n\u2206i\n(\n1 + 32\u03b3 log(T (K+1)\n\u22062 i\n4 )\n\u22062 i\n)\n, which gives the first term\n(and part of the last term) in the regret. Next consider a period xj > x0. Let pj , P(l(xj , at) > 0) denote the probability that an attack is observed when playing xj . From the algorithm, the expected number of rounds until xj is added to Ym is bounded by min( 1pj , nmj ), which gives the second term in the regret (and part of the last term). After xj is added to Ym, it is played only when it becomes the longest active period in Y , which gives the third term in the regret using the same argument for the first term of (6) in Theorem 1.\nWhen x0 is i.i.d. sampled from an unknown distribution, playing longer periods do not provide deterministic sideobservations to any shorter arms. This can be addressed by playing every active arm in Xm until round nm = \u2308\n2\u03b3 log(T (K+1)\u2206\u03032m)\n\u2206\u03032m\n\u2309\nin each stage m. The algorithm applies\nto any time associative stochastic bandit problem with arbitrary lt(xt, at). By applying a similar argument as in the proof of Theorem 1, the algorithm achieves a regret bound\nof \u2211K\ni=1\n32\u03b3 log\n(\nT (K+1) \u22062\ni 4\n)\n\u2206i + \u2211 i:\u2206i>0\n(\n\u2206i + 48 \u2206i\n)\n."}], "references": [{"title": "and Ortner", "author": ["P. Auer"], "venue": "R.", "citeRegEx": "Auer and Ortner 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Improved Rates for the Stochastic Continuum-Armed Bandit Problem", "author": ["Ortner Auer", "P. Szepesv\u00e1ri 2007] Auer", "R. Ortner", "C. Szepesv\u00e1ri"], "venue": "In Proc. of COLT", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Commitment Without Regrets: Online Learning in Stackelberg Security Games", "author": ["Balcan"], "venue": "In Proc. of EC", "citeRegEx": "Balcan,? \\Q2015\\E", "shortCiteRegEx": "Balcan", "year": 2015}, {"title": "E", "author": ["Barnett"], "venue": "2011. Hackers go after Facebook sites 600,000 times every day. The Telegraph, Oct.29,", "citeRegEx": "Barnett 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Timing the Application of Security Patches for Optimal Uptime", "author": ["Beattie"], "venue": "In Proc. of USENIX LISA", "citeRegEx": "Beattie,? \\Q2002\\E", "shortCiteRegEx": "Beattie", "year": 2002}, {"title": "R", "author": ["K.D. Bowers", "M.E.V. Dijk", "A. Juels", "A.M. Oprea", "Rivest"], "venue": "L.; and Triandopoulos, N.", "citeRegEx": "Bowers et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and CesaBianchi", "author": ["S. Bubeck"], "venue": "N.", "citeRegEx": "Bubeck and Cesa.Bianchi 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "N", "author": ["S. Buccapatnam", "A. Eryilmaz", "Shroff"], "venue": "B.", "citeRegEx": "Buccapatnam. Eryilmaz. and Shroff 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Leveraging Side Observations in Stochastic Bandits", "author": ["Caron"], "venue": "In Proc. of UAI", "citeRegEx": "Caron,? \\Q2012\\E", "shortCiteRegEx": "Caron", "year": 2012}, {"title": "Continuous Time Associative Bandit Problems", "author": ["Gy\u00f6rgy"], "venue": "In Proc. of IJCAI", "citeRegEx": "Gy\u00f6rgy,? \\Q2007\\E", "shortCiteRegEx": "Gy\u00f6rgy", "year": 2007}, {"title": "R", "author": ["A. Juels", "M.E.V. Dijk", "A.M. Oprea", "Rivest"], "venue": "L.", "citeRegEx": "Juels et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Flipthem: Modeling targeted attacks with flipit for multiple resources", "author": ["Laszka"], "venue": "In Proc. of GameSec", "citeRegEx": "Laszka,? \\Q2014\\E", "shortCiteRegEx": "Laszka", "year": 2014}, {"title": "Mitigating Covert Compromises: A Game-Theoretic Model of Targeted and NonTargeted Covert Attacks", "author": ["Johnson Laszka", "A. Grossklags 2013] Laszka", "B. Johnson", "J. Grossklags"], "venue": "In Proc. of WINE", "citeRegEx": "Laszka et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Laszka et al\\.", "year": 2013}, {"title": "Dynamic Probing for Intrusion Detection under Resource Constraints", "author": ["Zhao Liu", "K. Swami 2013] Liu", "Q. Zhao", "A. Swami"], "venue": "In Proc. of ICC", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "and Soyer", "author": ["T.A. Mazzuchi"], "venue": "R.", "citeRegEx": "Mazzuchi and Soyer 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "and Rigollet", "author": ["V. Perchet"], "venue": "P.", "citeRegEx": "Perchet and Rigollet 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "M", "author": ["Puterman"], "venue": "L.", "citeRegEx": "Puterman 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "C", "author": ["Y. Tan", "Xia"], "venue": "H.", "citeRegEx": "Tan and Xia 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "FlipIt: The Game of \u201cStealthy Takeover", "author": ["van Dijk"], "venue": "Journal of Cryptology", "citeRegEx": "Dijk,? \\Q2013\\E", "shortCiteRegEx": "Dijk", "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "Cybersecurity is increasingly threatened by advanced and persistent attacks. As these attacks are often designed to disable a system (or a critical resource, e.g., a user account) repeatedly, it is crucial for the defender to keep updating its security measures to strike a balance between the risk of being compromised and the cost of security updates. Moreover, these decisions often need to be made with limited and delayed feedback due to the stealthy nature of advanced attacks. In addition to targeted attacks, such an optimal timing policy under incomplete information has broad applications in cybersecurity. Examples include key rotation, password change, application of patches, and virtual machine refreshing. However, rigorous studies of optimal timing are rare. Further, existing solutions typically rely on a pre-defined attack model that is known to the defender, which is often not the case in practice. In this work, we make an initial effort towards achieving optimal timing of security updates in the face of unknown stealthy attacks. We consider a variant of the influential FlipIt game model with asymmetric feedback and unknown attack time distribution, which provides a general model to consecutive security updates. The defender\u2019s problem is then modeled as a time associative bandit problem with dependent arms. We derive upper confidence bound based learning policies that achieve low regret compared with optimal periodic defense strategies that can only be derived when attack time distributions are known. Introduction Malicious attacks are constantly evolving to inflict increasing levels of damage on the nation\u2019s infrastructure systems, cooperate IT systems, and our digital lives. For example, the Advanced Persistent Threat (APT) has become a major concern to cybersecurity in the past few years. APT attacks exhibit two distinguishing behavior patterns (van Dijk et al. 2013) that make them extremely difficult to defend using traditional techniques. First, these attacks are often funded well and persistent. They attack a target system (or a critical resource) periodically with the goal to compromise it completely e.g., by stealing full cryptography keys. Second, the attacks can be highly adaptive. In particular, they often act covertly, e.g., by operating in Copyright \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. a \u201clow-and-slow\u201d fashion (Bowers et al. 2014), to avoid immediate detection and obtain long-term advantages. From the defender\u2019s perspective, an effective way to thwart continuous and stealthy attacks is to update its security measures periodically to strike a balance between the risk of being compromised and the cost of updates. The primary challenge, however, is that such decisions must often be made with limited and delayed feedback because of the covert nature of the attacker. In addition to thwarting targeted attacks, such an optimal timing problem with incomplete information is crucial in various cybersecurity scenarios, e.g., key rotation (van Dijk et al. 2013), password changes (Tan and Xia 2016), application of patches (Beattie et al. 2002), and virtual machine refreshing (Juels et al. 2016). For example, Facebook receives approximately 600,000 \u201ccompromised logins\u201d from impostors every day (Barnett 2011). An efficient approach to stop these attacks is to ask users to update their passwords when the risk of attack is high. Although time-related tactical security choices have been studied since the cold war era (Blackwell 1949), rigorous study of timing decisions in the face of continuous and stealthy attacks is relatively new. In 2012, in response to an APT attack on it, the RSA lab proposed the FlitIt game, which was one of the first models to study timing decisions under stealthy takeovers. The FlipIt game model abstracts out details about concrete attack and defense operations by focusing on the stealthy and persistent nature of players. The basic model considers two players, each of whom can \u201cflip\u201d the state of a system periodically at any time with a cost. A player only learns the system state when she moves herself. The payoff of a player is defined as the fraction of time when the resource is under its control less the total cost incurred. The FlipIt game captures the stealthy behavior of players in an elegant way by allowing various types of feedback structures. In the basic model where neither player gets any feedback during the game and each move flips the state of the resource instantaneously, it is known that periodic strategies with random starting phases form a pair of best response strategies (van Dijk et al. 2013). As a variant of the basic model, an asymmetric setting is studied in (Laszka, Johnson, and Grossklags 2013) where the defender gets no feedback during the game while the attacker obtains immediate feedback after each defense but incurs a random attack time to take over the resource. In this setting, it is shown in (Laszka, Johnson, and Grossklags 2013) that periodic defense and immediate attack (or no attack) form a pair of best response strategies. However, little is known beyond these two cases. In particular, designing adaptive defense strategies with partial feedback remains an open problem. Although the FlipIt game provides a proper framework to understand the strategic behavior of stealthy takeover, it relies on detailed prior knowledge about the attacker. In particular, it requires parameters such as the amount of time needed to compromise a resource and the unit cost of each attack (or their distributions) to be fixed and known to the defender so that the equilibrium solution can be derived. These parameters limit the scope of the attack model, which, however, can be hard to verify before the game starts. To address this fundamental limitation, we propose to study online learning algorithms that make minimum assumptions about the attacker and learn an optimal defense strategy from the limited feedback obtained during the game. Given the advances in big data analytics and their applications in cybersecurity, it is feasible for the defender to obtain partial feedback even under stealthy attacks. Such a learning approach makes it possible to derive adaptive and robust defense strategies against unknown attacks where the type of the attacker is derived from a fixed but unknown distribution, as well as the more challenging dynamic attacks where the type of the attacker can arbitrarily vary over time. In this work, we make a first effort towards achieving optimal timing of security updates in the face of unknown stealthy attacks. We consider a variant of FlipIt game with asymmetric feedback similar to (Laszka, Johnson, and Grossklags 2013), but with two key differences. First, we consider repeated unknown attacks with attacker\u2019s type sampled from an unknown distribution. Second, we assume that the defender obtains limited feedback about potential attacks at the end of each period. The defender\u2019s goal is to minimize the long-term cumulative loss. Our objective is to derive an adaptive defense policy that has a low regret compared with the optimal periodic defense policy when the attack time distribution is known. A key observation is that the set of defense periods that the defender can choose from are dependent in the sense that the loss from one defense period may reveal the potential loss from other periods, especially shorter ones. Moreover, two defense policies played for the same number of rounds may span different lengths of time, which has to be taken into account when comparing the policies. In this paper, we model the defender\u2019s problem as a time associate stochastic bandit problem with dependent arms, where each arm corresponds to one possible defense period. We derive optimal defense strategies for both the finite-armed bandit setting where the defense periods can only take a finite set of values, and the continuum-armed bandit setting where the defense periods can take any values from a non-empty interval. Our main contributions can be summarized as follows. \u2022 We propose a stochastic time associative bandit model for optimal timing of security updates in the face of unknown attacks. Our model captures both the limited feedback about stealthy attacks and the dependence between different defense options. \u2022 We derive upper confidence bound (UCB) based policies for time associative bandits with dependent arms. Our policies achieve a regret of O (log(T (K + 1)) +K) for the finite arm case, where T is the number of rounds played and K is the number of arms, and a regret of O(T ) for the continuous arm setting. Our learning model and algorithms are built upon the assumption that the defender can learn from frequent system compromises. This is reasonable for many online systems such as large online social networks and content providers and large public clouds, in which many customers are subject to similar attacks. In this setting, even if a single user is compromised occasionally, the system administrator can pool data collected from multiple users to obtain a reliable estimate quickly. For example, given the large number of attacks towards its users, Facebook can collect data from thousands of incidents of similar compromises in a short time. Our online learning algorithms can be used by Facebook to alert users to update their passwords when necessary. Related Work Time-related tactical security choices have been studied since the cold war era (Blackwell 1949). However, the study of timing decisions in the face of continuous and stealthy attacks is relatively new. In particular, the FlipIt game (van Dijk et al. 2013) and its variants (Laszka, Johnson, and Grossklags 2013; Laszka et al. 2014) are among the few models that study this problem in a rigorous way. However, all of these models assume that the parameters about the attacker are known to the defender at the beginning of the game. A gradient-based Bayesian learning algorithm was recently proposed in (Tan and Xia 2016) for a setting similar to ours, where the failure time was assumed to follow a Weibull distribution with one unknown parameter. In contrast, we consider a general attack time distribution. Multi-armed bandit problems have been extensively studied for both the stochastic setting and the adversarial setting (Bubeck and Cesa-Bianchi 2012). Many variants of bandit models have been considered including bandits with side observations (Caron et al. 2012; Buccapatnam, Eryilmaz, and Shroff 2014). In the context of cybersecurity, bandit models have been applied to anomaly detection (Liu, Zhao, and Swami 2013) and stackelberg security games (Balcan et al. 2015). However, the only previous work that studies the time associative bandit model is (Gy\u00f6rgy et al. 2007), where the arms are assumed to be mutually independent. In contrast, we propose to model the optimal timing problem in cybersecurity as a time associative bandit problem with dependent arms and study algorithms that can exploit side-observations to improve performance. Model We consider the following variant of the FlipIt game (van Dijk et al. 2013) with two players, a de-", "creator": "gnuplot 5.0 patchlevel 3"}}}