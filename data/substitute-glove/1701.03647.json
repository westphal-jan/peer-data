{"id": "1701.03647", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "Restricted Boltzmann Machines with Gaussian Visible Units Guided by Pairwise Constraints", "abstract": "Restricted Boltzmann electronic (RBMs) have need similar are without teaching claimed y-str mutation (CD) useful, better their training procedure become being fantasizing innovative learning, turn how guidances of when background shared. To enhance brought reflected realize of traditional RBMs, as another press, come discuss elevational internal largely Boltzmann machine way Gaussian exposed soldiers (pcGRBM) model, only which soon innovative motion comes radar while altitudinal constraints way where process significant mrnas any conducted rest actually breathability. The y-axis constraints are bytes 1998 hidden silt includes significant pcGRBM. Then, could orthogonal hid features of pcGRBM flock still had being under of them are abandoned by soon abridgments. In ensure both it set very - equivalent suggesting, to elliptic visible units be retired by equation units this Gausian noise in seen pcGRBM is. In that learning compromise of pcGRBM, given second-person challenges are logarithm transitions apart marked and hides units weeks CD aspects treatments. Then, an charter ideal is elucidated with approximative eccentricity relative combination and all variables learn predictive where replacing having to paper. In effectively next impossible the ample among pcGRBM and mix RBMs behind Gaussian visible roughly, put typical of brought pcGRBM and RBMs suspicious crust which used. input ' using ' which K - idea, spectral clustering (SP) making deriving inhibit (AP) workflow, respectively. A undertaken alternative recommendation is appeared brought sixteen perception non-clinical than Microsoft Research Asia Multimedia (MSRA - MM ). The modern results show that the graphs performance years K - should, SP part AP algorithms based on pcGRBM brand come significantly always so traditional RBMs. In addition, be pcGRBM model out clustering task shows they performance this other badminton - ostensibly clustering formulae.", "histories": [["v1", "Fri, 13 Jan 2017 12:43:58 GMT  (2784kb)", "http://arxiv.org/abs/1701.03647v1", "13pages"]], "COMMENTS": "13pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jielei chu", "hongjun wang", "hua meng", "peng jin", "tianrui li"], "accepted": false, "id": "1701.03647"}, "pdf": {"name": "1701.03647.pdf", "metadata": {"source": "CRF", "title": "Restricted Boltzmann Machines with Gaussian Visible Units Guided by Pairwise Constraints", "authors": ["Jielei Chu", "Hongjun Wang", "Hua Meng", "Peng Jin"], "emails": ["jieleichu@home.swjtu.edu.cn,", "wanghongjun@swjtu.edu.cn,", "huameng@swjtu.edu.cn,", "trli@swjtu.edu.cn.", "jandp@pku.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n03 64\n7v 1\n[ cs\n.L G\n] 1\n3 Ja\nn 20\nIndex Terms\u2014restricted Blotzmann machine (RBM); pairwise constraints; contrastive divergence (CD); unsupervised clustering; semisupervised clustering."}, {"heading": "1 INTRODUCTION", "text": "Hinton and Sejnowski[1] proposed a learning algorithm for general Boltzmann machine which has hidden-to-hidden and visible-to-visible connections, but in practice it was too slow to be used. Then, the restricted Blotzmann machine (RBM) was proposed by[2] in 1986, which has no lateral connections among nodes in each layer, so the learning procedure becomes much more efficient than general Blotzmann machine. There has been extensive research into the RBM since Hinton proposed fast learning algorithms[3],\nJielei Chu, Hongjun Wang, Hua Meng and Tianrui Li are with the School of Information Science and Technology, Southwest Jiaotong University, Chengdu, 611756, Sichuan, China. e-mail: jieleichu@home.swjtu.edu.cn, wanghongjun@swjtu.edu.cn, huameng@swjtu.edu.cn, trli@swjtu.edu.cn. Peng Jin is with the School of Computer Science, Leshan Normal University, 614000, Leshan, China. e-mail: jandp@pku.edu.cn\n[4] by contrastive divergence (CD) learning algorithm. Several power and tractability deep networks was proposed, including deep belief networks[5], deep autoencoder[6], deep Boltzmann machine[7], deep dropout neural net[8]. Until now, a large number of successful applications built on the RBMs have appeared, e.g., classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].\nThe classic RBM has great ability of extracting hidden features from original data. More and more researchers proposed variant RBMs and their deep networks which were based on classic RBM, e.g., fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].\nHowever, since the learning procedures of classic RBM and its variants are unsupervised methods, their processes of feature extraction are non-directional and conducted under no guidance. To remedy these weakness, this paper proposes a pairwise constraints restricted Blotzmann machine with Gaussian visible units (pcGRBM) and corresponding learning algorithm, where the learning procedure is guided by pairwise constraints which come from labels. In pcGRBM model, the pairwise constraints which is instance-level prior knowledge guide the process of encoding, some pairwise hidden features of pcGRBM flock together and another part of them are separated by the guidances, then the process of feature extraction is no longer non-directional. Then, the background knowledge of instance-level pairwise constraints are encoded in hidden\n2 layer features of pcGRBM. In order to testify the availability of pcGRBM, we design three structures of clustering ,in which the features of the hidden layer of the pcRBM are used as input \u2018data\u2019 for unsupervised clustering algorithms. The experimental results show that the clustering performance of K-means, SP and AP algorithms based on pcGRBM model are significantly better than traditional RBMs. In addition, the pcGRBM model for clustering is better performance than some semi-supervised algorithms (Cop-Kmeans[42], SemiSpectral clustering (Semi-SP)[43] and semi-supervised affinity propagation (Semi-AP)[44]).\nThe remainder of this paper is organized as follows. In the next section, we outline the related work and provide the preliminary in section III, which includes pairwise constraints, RBM and Gauss visible units. The proposed pcGRBM model and its learning algorithm are introduced in section IV. Next, the remarkable performance of the pcGRBM model is affirmed by the task of clustering on MSRA-MM in section V. Finally, Section VI summarizes our contributions."}, {"heading": "2 RELATED WORK", "text": "Due to the outstanding performance, more and more variants of RBM have been proposed by researchers. There are several common methods to develop standard RBM such as adding connections information between the visible units and the hidden units, changing the value type of visible or hidden units, expanding the relationships of the units between visible layer and hidden layer from constant to variable by fuzzy mathematics, constructing deep network based on autoencoder[21] by pairwise constraints.\nTo add connections information between the visible units into RBM is a kind of methods for developing standard RBM. Osindero and Hinton proposed a semi-restricted Boltzmann machines (SRBM)[45] which has lateral connections between the visible units, but these lateral connections are unit-level semi-supervised information. The learning procedure includes two stages: the first one is the visible to hidden connections which is same as a classic RBM and the second one is the lateral connections which is applied the same learning procedure as the first one. In order to enforce hidden units to be pairwise uncorrelated and to maximize entropy, Tomczak[46] proposed to add penalty term to the log-likelihood function. His framework of learning informative features is unit-level pairwise and for classification problem, while our model is instance-level pairwise and for clustering task. Zhang et al.[47] built deep belief network based on SRBM for classification. Given the hidden units, the visible units of the SRBM form a Markov random field. However, the main weakness of the SRBM is that there are massive parameters for highdimensional data, if every pairs of visible units have relations. Sutskever and Hinton proposed temporal restricted Boltzmann machine (TRBM)[48] by adding directed connections between previous and current states of the visible and hidden units. There are three kinds of connections of the full TRBM, e.g., connections between the visible units, connections between the hidden and visible units and connections between the hidden units. Furthermore, they proposed the recurrent TRBM\n(RTRBM)[32]. It is easy to compute the gradient of the loglikelihood and infer exactly. Mnih and Hinton proposed the conditional restricted Boltzmann machines (CRBMs)[49] by adding conditioning vector which determines increments to the biases of the visible and hidden layer of the traditional RBM.\nBy changing hidden units with relevancy is another kind of methods for developing standard RBM. Courville et al.[28] developed the spike-and-slab restricted Boltzmann machine (ssRBM). The ssRBM is defined as having each hidden unit associated with the product of a binary \u201cspike\u201d latent variable and a real-valued \u201cslab\u201d latent variable. In order to keep learning efficiency, as a model of natural images, the binary hidden units of the ssRBM maintain the simple conditional independence structure when they encode the conditional covariance of visible units by exploiting real-valued slab variables.\nIn general, the relationships of the units between the visible layer and the hidden layer are restricted to be constants. In order to break through this restrictions, Chen et al.[26] proposed a fuzzy restricted Boltzmann machine (FRBM) to enhance deep learning capability which can avoid the flaw. The FRBM model parameters are replaced by fuzzy numbers and the regular RBM energy function is given by fuzzy free energy functions. Moreover, the deep networks are designed by the fuzzy RBMs to boost deep learning. Nie et al.[20] proposed to theoretically extend the conventional RBMs by introducing another term in the energy function to explicitly model the local spatial interactions in the input data.\nConventional RBM defines the units of visible and hidden layer to be binary, but this limitation cannot meet the needs in practice. Then one common way is to replace them by means of Gaussian linear units, that is Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs)[50]. The GBRBMs have the ability to learn meaningful features both in modeling natural images and in a two-dimensional separation task. But, as we know, it is difficult to learn the GBRBMs. So, Cho et al.[51] proposed a novel method to improve their learning efficiency. The new method includes three parts, e.g., changing energy function by different parameterizations to facilitate learning, parallel tempering learning and adaptive learning rate. Moreover, the deep networks of Gaussian-Bernoulli deep Boltzmann machine (GDBM)[52], [53] has been developed by the GBRBM in recent years. The GDBM is designed by adding multiple layer of hidden units and applied to continuous data.\nFurthermore, Zhang et al. proposed a mixed model named as supervision guided autoencoder (SUGAR)[54] which includes three components: main network, auxiliary network and bridge. The main network is a sparsity-encouraging variant of the autoencoder[21], that is the unsupervised autoencoder. The auxiliary network is constructed by pairwise constraints, that is the supervised learning. The two heterogeneous networks are designed and each of which encodes either unsupervised or supervised data structure respectively. The main network and auxiliary network are connected by the bridge which is used to enforce the correlation of the parameters. Comparing SUGAR with supervised learning and supervised deep networks, it\n3 has flexible utilization of supervised information and better balances the numerical tractability.\nIn the work of [55], Chen proposed a deep network structure based on RBMs which is the most related to our work. Both the work of [55] and our work aim to solve the similar problems, e.g., how to obtain suitable features for clustering by non-linear mapping and how to use pairwise constraints during learning process, but the model and the solution are different. They use RBMs to initialize connection weights with CD learning, learning process is still unsupervised method, then the learned weights are used to incorporate pairwise constraints in features space by maximum margin techniques. However, our pcGRBM model is based on RBMs with Gaussian visible units. Its learning process is no longer unsupervised method, but guided by pairwise constraints."}, {"heading": "3 PRELIMINARIES", "text": "In this section, the background of the pairwise constraints, RBM and Gaussian visible units is briefly summarized."}, {"heading": "3.1 Pairwise Constraints", "text": "The priori knowledge of pairwise constraints is widely used in supervised and semi-supervised learning. There are two types of instance-level pairwise constraints: One is cannotlink constraints which instances should not be grouped together and the other is must-link constraints which instances should be grouped together. The must-link and cannot-link constraints define an instance-level relation of transitive binary. Consequently, two types of constraints may be derived from background knowledge about data set or labeled data. In this paper, we select labeled data from different groups randomly and ensure each group has the same ratio of labeled data to be selected. Then, the must-link constraints are produced by the selected same group labeled data and the cannot-link constraints are produced by the selected different group labeled data."}, {"heading": "3.2 Restricted Boltzmann Machine", "text": "A RBM[2][3] is a two-layer network in which the first layer consists of visible units, and the second layer consists of hidden units. The symmetric undirected weights are used to connect the visible and hidden layers. There are no interiorlayer connections with either the visible units or the hidden units. A classic RBM model is shown in Fig. 1. An energy function[56] of a joint configuration (v, h) between the visible layer and the hidden layer is given by:\nE(v,h) = \u2212 \u2211\ni\u2208visible\naivi\u2212 \u2211\nj\u2208hidden\nbjhj\u2212 \u2211\ni,j\nvihjwij (1)\nwhere v = (v1, v2 \u00b7 \u00b7 \u00b7 vn) and h = (h1, h2 \u00b7 \u00b7 \u00b7hm) are the visible and the hidden vectors, ai and bj are their biases, n and m are the dimension of visible layer and hidden layer, respectively, wij is the connection weight matrix between the visible layer and the hidden layer. A probability distribution over vectors v and h is defined as\np(v,h) = 1\nZ e\u2212E(v,h) (2)\n... ...\nW\nh\nv\nh1 hj hm\nv1 v2 vi vn ... ...\nFig. 1. Restricted Boltzmann Machine (RBM)\nwhere Z is a \u201cpartition function\u201d which is defined by summing over all possible pairs of hidden layer and visible layer:\nZ = \u2211\nv,h\ne\u2212E(v,h). (3)\nBy means of summing over all the units of the hidden layer, the probability that the RBM assigns to the units of the visible layer v is given by:\np(v) = 1\nZ\n\u2211\nh\ne\u2212E(v,h). (4)\nThe partial derivative of the log probability of Eq. (4) with respect to a weight is given by\n\u2202 log p(v)\n\u2202wij = (< vihj >data \u2212 < vihj >model) (5)\nwhere the angle brackets < vihj >data and < vihj >model are used to denote expectations of the distribution specified by the subscript data and model. In the log probability, a very simple learning rule for performing stochastic steepest ascent is given by:\n\u2206wij = \u03b5(< vihj >data \u2212 < vihj >model) (6)\nwhere \u03b5 is a learning rate. It is easy to get < vihj >data because there is no direct connections among the hidden units. However, it is difficult to get unbiased sample of < vihj >model. Hinton[3] proposed a faster learning algorithm with the CD learning and the change of learning parameter is given by:\n\u2206wij = \u03b5(< vihj >data \u2212 < vihj >recon), (7)\n\u2206ai = \u03b5(< vi >data \u2212 < vi >recon), (8)\n\u2206bj = \u03b5(< hj >data \u2212 < hj >recon) (9)\nwhere < vihj >recon can be computed efficiently than < vihj >model.\n4"}, {"heading": "3.3 Gaussian Visible Units", "text": "Original RBMs were developed by binary stochastic units for the hidden and visible layers[3]. To deal with real-valued data such as natural images, one solution is that the binary visible units are replaced by linear units with independent Gaussian noise, but the hidden units remain binary, which is first suggested by[57]. The negative log probability is given by the following energy function:\n\u2212 logp(v,h) = E(v,h) = \u2211\ni\u2208visible\n(vi \u2212 ai) 2\n2\u03c32i \u2212\n\u2211\nj\u2208hidden\nbjhj \u2212 \u2211\ni,j\nvi \u03c3i hjwij (10)\nwhere \u03c3i is the standard deviation of the Gaussian noise for visible unit i.\nFor each visible unit, it is easy to learn the variance of the noise, but it is difficult using CD1 because of taking long time[50][58]. Therefore, in many applications, it is easy to normalise the data to have unit variance and zero mean[50][59][60][61]. Then the reconstructed value of Gaussian visible units is equal to its input from the binary hidden units plus its bias."}, {"heading": "4 PCGRBM MODEL AND ITS LEARNING ALGORITHM", "text": "We first propose a pairwise constraints restricted Boltzmann machine with Gaussian visible units(pcGRBM) model which the binary visible units are replaced by noise-free linear units and its learning procedure is guided by pairwise constraints. Then we give exact inference of the pcGRBM optimization. Finally, the corresponding learning algorithm is presented."}, {"heading": "4.1 pcGRBM Model", "text": "Suppose that V = {v1, v2, \u00b7 \u00b7 \u00b7 , vn} is a p-dimensional original data set which has been normalized, H = {h1, h2, \u00b7 \u00b7 \u00b7 , hn} is a q\u2212dimensional hidden code. The pairwise must-link constraints set of the reconstruction data is defined by M = {(vs, vt)|vs, vt belongs to the same class} and pairwise cannot-link constraints set of the reconstruction data is given by C = {(vs, vt)|vs, vt belongs to the different classes}.\nFor training the parameters of the pcGRBM model, the first objective is that how to maximize the log probability of RBM with Gaussian visible units and the second objective is that how to maximize distance of all pairwise vectors which come from cannot-link set and minimize distance of all pairwise vectors which come from must-link set in reconstructed visible layer. Because of using noise-free reconstruction in the model, the reconstructed value of a Gaussian visible linear unit is equal to its input from the hidden units plus its bias. Then the objective function is given by\nL(\u03b8,V) =\n\u03bb\nn\n\u2211\nvi\u2208V\nlogp(vi, \u03b8) + [(1\u2212 \u03bb\nNM\n\u2211\nM\n\u2016hsWT \u2212 htWT \u20162\n\u2212 1\u2212 \u03bb\nNC\n\u2211\nC\n\u2016hsWT \u2212 htWT \u20162 )]\n(11)\nwhere \u03b8 = {a,b,W} are the model parameters, \u03bb \u2208 (0, 1) is a scale coefficient, NM and NC are the cardinality of the mustlink pairwise constraints set M and the cannot-link pairwise\nconstraints set C, respectively, 1 n n\u2211 i=1 logp(vi; \u03b8) is the average of the log-likelihood and \u2016 \u00b7 \u20162 is the square of 2-norm. The learning problem of the pcGRBM model is to get optimal or approximate optimal parameters \u03b8, which minimize the objective function L(\u03b8,V), i.e.,\nmin{L(\u03b8,V)}. (12)"}, {"heading": "4.2 pcGRBM Inference", "text": "For our first objective, we can use gradient descent to solve optimal problem, however, it is expensive to compute the gradient of the log probability. Recently, Karakida et al.[62] demonstrated that CD1 learning is simpler than ML learning in RBMs with Gaussian linear units. Then, we apply the CD1 learning method to obtain an approximation of the log probability gradient. For our second objective, we use the method of gradient descent to solve the optimization problem. The following main work is to compute the gradient of 1\u2212\u03bb NM \u2211 M \u2016hsWT \u2212 htWT \u20162 \u2212 1\u2212\u03bbNC \u2211 C \u2016hsWT \u2212 htWT \u20162.\nFirstly, we assume that\nJM (W) = 1\nNM\n\u2211\nM\nwwhsWT \u2212 htWT ww2 (13)\nand\nJC(W) = 1\nNC\n\u2211\nC\n\u2016hsWT \u2212 htWT \u20162. (14)\nThen, the gradients of the JM (W) is\n\u2202JM (W)\nwij =\n1\nNM\n\u2211\nM\n[ (hs \u2212 ht)WT \u2202W(hs \u2212 ht)T\nwij +\n\u2202(hs \u2212 ht)WT\nwij W(hs \u2212 ht)T\n]\n(15) and the gradients of the JC(W) is\n\u2202JC(W)\nwij =\n1\nNC\n\u2211\nC\n[ (hs \u2212 ht)WT \u2202W(hs \u2212 ht)T\nwij +\n\u2202(hs \u2212 ht)WT\nwij W(hs \u2212 ht)T\n] .\n(16)\nIn order to express concisely, we suppose that\nh \u2032 = (hs1 \u2212 ht1, \u00b7 \u00b7 \u00b7 , hsj \u2212 htj , \u00b7 \u00b7 \u00b7 , hsq \u2212 htq) (17)\nwhere hsk \u2212 htk = 0, k 6= j, j = 1, 2, \u00b7 \u00b7 \u00b7 , q, and q is the dimension of the hidden layer.\nThen, the gradient of the JM (W) takes the form\n\u2202JM (W)\nwij =\n1\nNM\n\u2211\nM\n[ (hs \u2212 ht)WT (h \u2032 )T + h \u2032 W(hs \u2212 ht)T ] .\n(18)\n5\nIn like manner, the gradient of the JC(W) takes the form\n\u2202JC(W)\nwij =\n1\nNC\n\u2211\nC\n[ (hs \u2212 ht)WT (h \u2032 )T + h \u2032 W(hs \u2212 ht)T ]\n(19) So, the gradient of the objective function is as follows.\n\u25bdwij =\u03bb\u03b5(< vihj >data \u2212 < vihj >recon)+\n1\nNM\n\u2211\nM\n[ (hs \u2212 ht)WT (h \u2032 )T + h \u2032 W(hs \u2212 ht)T ] \u2212\n1\nNC\n\u2211\nC\n[ (hs \u2212 ht)WT (h \u2032 )T + h \u2032 W(hs \u2212 ht)T ] .\n(20) It is obvious that \u2202JM(W)\nai = 0, \u2202JC(W) ai = 0, \u2202JM (W) bj = 0\nand \u2202JC(W) bj\n= 0. So, in the pcGRBM model, we use Eq. (8) and Eq. (9) to update the biases ai and bj .\nFinally, the updating rulers of connection weights W of the pcGRBM model takes the form\nw (\u03c4+1) ij = w (\u03c4) ij + \u03bb\u03b5(< vihj >data \u2212 < vihj >recon)+\n1\nNM\n\u2211\nM\n[ (hs \u2212 ht)WT (h \u2032 )T + h \u2032 W(hs \u2212 ht)T ] \u2212\n1\nNC\n\u2211\nC\n[ (hs \u2212 ht)WT (h \u2032 )T + h \u2032 W(hs \u2212 ht)T ] .\n(21)"}, {"heading": "4.3 pcGRBM Learning Algorithm", "text": "According to the above inference, the learning algorithm for pcGRBM is summarized as follows. Algorithm 1 Learning for pcGRBM\nInput: \u03b5 is the learning rate; V is a p-dimensional data set; \u03bb is a scale coefficient; NM is the cardinality of the must-link pairwise constraints set; NC is the cardinality of the cannot-link pairwise constraints set; M is must-link pairwise constraints set; C is must-link pairwise constraints set.\nOutput: \u03b8 = {a,b,W}, W is connection weights, a is visible biases, b is hidden biases.\nInitializing \u03b5, \u03bb, NM , NC , W, a, b; For each iteration do\nFor all hidden units j do compute p(hj = 1|v) = \u03c3(bj + \u2211 i viwij);\nsample hj \u2208 {0, 1} from p(hj = 1|v) ; End For For all visible units i do\ncompute reconstructed value vi = ai + \u2211 j hjwij ;\nEnd For compute the gradient of the \u2202JM (W)\nwij by using Eq. (18);\ncompute the gradient of the \u2202JC(W) wij by using Eq. (19);"}, {"heading": "5 RESULTS AND DISCUSSION", "text": "In this section, we introduce the datasets, define the experimental setup, and discuss about experimental results."}, {"heading": "5.1 DataSets", "text": "We used the Microsoft Research Asia Multimedia (MSRAMM)[63] which contains two sub-datasets, e.g., a video dataset and an image dataset. The image part contains 1,011,738 images and the video part contains 23,517 videos. To evaluate our pcGRBM model, we use 16 image datasets (alphabet, ambulances, bed, beret, beverage, bike, billiard, blog, blood, bonsai, book, bread, breakfast, building, vegetable and virus) from image part for our experiments. The summary of the datasets are listed in Table I."}, {"heading": "5.2 Experimental Setup", "text": "The goal of the experiments is to study the following aspects:\n\u2022 Dose the pairwise constraints guide the encoding procedure of traditional RBM? \u2022 How dose unsupervised clustering algorithms based on pcGRBM model compare with their semi-supervised clustering algorithms? \u2022 How dose unsupervised clustering algorithms based on pcGRBM model compare with these algorithms based on traditional RBM?\nTo verify the features of pcGRBM contain guiding information whether or not, we use the output of pcGRBM as input of unsupervised clustering algorithm. In our experiments, we choose K-means, affinity propagation (AP)[64], SP clustering algorithms as examples. Then, we present three algorithms\n6\nwhich based on pcGRBM model for clustering task, termed as Kmeans.pcgrbm, AP.pcgrbm and SP.pcgrbm, their structure are shown in Fig.5. Similarly, we also present three algorithms which based on traditional RBM with Gaussian visible units for clustering task, called as Kmeans.grbm, AP.grbm and SP.grbm. In fact, Kmeans.pcgrbm, AP.pcgrbm and SP.pcgrbm are semi-supervised clustering algorithms with instance-level guiding of pairwise constraints, but Kmeans.grbm, AP.grbm and SP.grbm are unsupervised methods.\nFirstly, we compare the clustering performance of the proposed algorithms (Kmeans.pcgrbm, AP.pcgrbm and SP.pcgrbm) with original K-means, AP and SP clustering algorithms, respectively. Secondly, the proposed algorithms are used to compare with Cop-Kmeans[42], Semi-SP[43] and Semi-AP[44], respectively. Finally, we use unsupervised algorithms that are Kmeans.grbm, AP.grbm and SP.grbm to compare with the proposed algorithms.\nTo evaluate the performance of the clustering algorithms, we adopt three widely used metrics: clustering accuracy[65], clustering purity[66] and clustering rank[67] as the evaluation measure."}, {"heading": "5.3 Results", "text": ""}, {"heading": "5.3.1 The pcGRBM for Clustering VS Unsupervised Algorithms", "text": "In this section, we compare unsupervised clustering of Kmeans, SP and AP with Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm which based on the pcGRBM by evaluation of average accuracy, average rank and average purity. From\nTable II, the average accuracies of K-means, SP and AP algorithms are 43.78%, 39.97% and 42.31%, respectively, but the average accuracies of Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm algorithms raise to 47.48%, 47.13% and 47.39%, respectively. The average ranks of K-means, SP and AP algorithms are shown in Table III, their values are 97.5625, 154.0625 and 124.4375, respectively, but the average ranks of\nKmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm algorithms reduce to 33.1875, 36.0625 and 32.8125, respectively. The smaller the rank value means the better the algorithm. From Table IV, the average purities of K-means, SP and AP algorithms are 0.7703, 0.7721 and 0.7772, respectively, but the average purities of Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm algorithms raise to 0.8010, 0.8012 and 0.8011, respectively. A greater purity indicates a better algorithm. From all above results, it is obvious that clustering by the pcGRBM is better than the original unsupervised clustering.\nFrom the last three columns of Table II, there are more variance volatility of Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm than those of other algorithms because of the effect of pairwise constraints."}, {"heading": "5.3.2 The pcGRBM for Clustering VS Semi-supervised Algorithms", "text": "In this section, we make further comparison among semisupervised clustering of Cop-kmeans, Semi-SP and Semi-AP with Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm by evalu-\nation of average accuracy, average rank and average purity. In addition, the comparison of average accuracy is shown in Figs .2-4, respectively. From Table II, the average accuracies of Cop-kmeans, Semi-SP and Semi-AP with Kmeans.pcgrbm algorithms are 43.85%, 40.26% and 42.53%, respectively. The pcGRBM raise the average accuracies by 3.98%, 6.87% and 5.09%, respectively. From Table III, the average ranks of Cop-kmeans, Semi-SP and Semi-AP algorithms are 95.2500, 151.6250 and 121.6250, respectively, however, the average ranks of Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm algorithms are 33.1875, 36.0625 and 32.8125, respectively. The smaller the rank value means the better the algorithm. The average purities of Cop-kmeans, Semi-SP and Semi-AP algorithms are shown in Table IV, their values are 0.7742, 0.7788 and 0.7753, respectively. From all above results, it is obvious that the pcGRBM for clustering is better than the semi-supervised clustering.\nWe plot the experiment results with the increasing percentage of pairwise constraints which ranges from 1% to 10% in steps of 1% for Cop-Kmeans and Kmeans.pcgrbm\nin Fig .2, Semi-SP and SP.pcgrbm in Fig .3, Semi-AP and AP.pcgrbm in Fig .4. From Figs .2-4, we can see that the accuracy of Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm can not maintain complete synchronous increases as the percentage of pairwise constraints, however, the average accuracies of Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm are higher than Cop-Kmeans, Semi-SP and Semi-AP, respectively."}, {"heading": "5.3.3 The pcGRBM VS RBM with Gaussian Visible Units for Clustering", "text": "The pcGRBM and RBM with Gaussian visible have ability to extract features, but, which one shows better performance for clustering task? In order to compare the representation capability between the pcGRBM and RBM without any guiding of pairwise constraints, we design a structure of clustering algorithm in which the features of RBM with Gaussian visible units is used as input of unsupervised clustering. In our experiment, we use three clustering algorithms base on this structure which are termed as Kmeans.grbm, SP.grbm and AP.grbm algorithms to compare to Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm. From Table II,\nthe average accuracies of kmeans.grbm, SP.grbm and AP.grbm algorithms are 43.321%, 43.387% and 43.11%, respectively, however, Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm algorithms raise the average accuracies by 4.27%, 3.26% and 4.28%, respectively. The average ranks of Kmeans.grbm, SP.grbm and AP.grbm algorithms are shown in Table III. The results are 105.5625, 96.9375 and 108.8750, respectively, however the average ranks of Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm algorithms reduce to 102.375, 60.875 and 76.0625, respectively. Table IV shows the average purities of kmeans.grbm, SP.grbm and AP.grbm. The values are 0.7831, 0.7853 and 0.7837, respectively. From all above results, it is obvious that the pcGRBM is better than RBM for clustering.\n10"}, {"heading": "5.3.4 The Rank", "text": "We compare twelve algorithms and sixteen data sets by means of the Aligned Friedman test statistic[67] which is given by\nT =\n(G\u2212 1) [ G\u2211 j=1 R\u03022.j \u2212 (GD 2/4)(GD + 1)2 ] {[GD(GD + 1)(2GD + 1)]/6]} \u2212 (1/G) D\u2211 i=1 R\u03022i. (22)\nwhere R\u0302i. is the rank sum of the jth algorithm, R\u0302.j is the rank sum of the ith data set, D is the number of data set and G is the number of algorithm. In our experiments, all pairwise constraints come from labels information. We choose 1% to 10% pairwise constraints in steps of 1%. The average rank value is smaller the algorithm is better. As we can see from Table III, the average rank of K-means, SP, AP, Cop-Kmeans, Semi-SP, Semi-AP, Kmeans.grbm, SP.grbm, AP.grbm, Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm algorithms are 97.5625, 154.0625, 124.4375, 95.2500, 151.6250, 121.6250, 105.5625, 96.9375, 108.8750, 33.1875, 36.0625 and 32.8125, respectively. It is easy to know that the least average rank is AP.pcgrbm algorithm with a value of 32.8125. From results on Table III, we can see that Kmeans.pcgrbm, SP.pcgrbm, AP.pcgrbm algorithms which based on pcGRBM are better than other nine algorithms. We check whether the measured sum of the ranks is significantly different from the average value of the total\nranks R\u0302j = 1544 expected under the null hypothesis:\nk\u2211\nj=1\nR\u03022.,j = 1561 2 + 24652 + 19912 + 15242 + 24262 + 19462\n+ 16892 + 15512 + 17422 + 5312 + 5772 + 5252\n= 33655396, (23)\nk\u2211\nj=1\nR\u03022i,. = 1148 2 + 11912 + 11962 + 11712 + 11422 + 11572\n+ 11452 + 12072 + 10952 + 11762 + 11892 + 11452\n+ 10822 + 11232 + 12212 + 11402 = 21477770, (24)\nT =\n{(12\u2212 1)(33655396\u2212 (12\u00d7 162/4)(12\u00d7 16 + 1)2)}\n(12\u00d7 16(12\u00d7 16 + 1)(2\u00d7 12\u00d7 16 + 1))/12\u2212 21477770/12\n= 54.9855. (25) T is the chi-square distribution with 11 degrees of freedom because we use nine algorithms and sixteen data sets. For one tailed test, the p-value is 0.00000001 which is computed by \u03c72(11) distribution and the p-value is 0.000000001 for two-tailed test. Then, the null hypothesis is rejected at high level significance. The experimental results of algorithms are significantly different because the p-values are far less than 0.05."}, {"heading": "6 CONCLUSION", "text": "In this paper, we proposed a novel pcGRBM model, the learning procedure of which is guided by the pairwise constraints and the process of encoding is conducted under guidance. Then, some pairwise hidden features of pcGRBM flock together and another part of them are separated by the guidances. In the process of learning pcGRBM, CD learning is used to approximate ML learning and pairwise constraints are iterated transitions between visible and hidden units. Then, the background of pairwise constraints are encoded in hidden layer features of pcGRBM. In order to testify the availability of pcGRBM, the features of the hidden layer of the pcGRBM are used as input \u2018data\u2019 for clustering tasks. The experimental results showed that the performance of the Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm algorithms which based on pcGRBM for clustering tasks are better than their classic unsupervised clustering algorithms (K-means, SP, AP), semi-supervised clustering algorithms( Cop-kmeans, Semi-SP, Semi-AP) and even better than Kmeans.grbm, SP.grbm and AP.grbm which based on RBM with Gaussian visible units without guiding of pairwise constraints.\nThere are several interesting questions in our future studies. For example, how to design deep networks based on the pcGRBM. How to strengthen pairwise constraints information when the layer of the deep network becomes deeper and deeper. How many dimensions in hidden layer can enhance the performance for clustering.\n11"}, {"heading": "7 ACKNOWLEDGEMENT", "text": "This work was partially supported by the National Science Foundation of China (No. 61573292)."}], "references": [{"title": "Optimal perceptual inference", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. Citeseer, 1983, pp. 448\u2013453.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1983}, {"title": "Learning and releaming in boltzmann machines", "author": ["G. Hinton", "T. Sejnowski"], "venue": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1, pp. 282\u2013317, 1986.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1986}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "On contrastive divergence learning", "author": ["M.A. Carreira-Perpinan", "G.E. Hinton"], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics. Citeseer, 2005, pp. 33\u201340.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems, vol. 19, p. 153, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "An efficient learning procedure for deep boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Neural Computation, vol. 24, no. 8, pp. 1967\u20132006, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1967}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1929}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 3642\u2013 3649.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy classification with restricted boltzman machines and echo-state networks for predicting potential railway door system failures", "author": ["O. Fink", "E. Zio", "U. Weidmann"], "venue": "Reliability, IEEE Transactions on, vol. 64, no. 3, pp. 861\u2013868, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral\u2013spatial classification of hyperspectral data based on deep belief network", "author": ["Y. Chen", "X. Zhao", "X. Jia"], "venue": "Selected Topics in Applied Earth Observations and Remote Sensing, IEEE Journal of, vol. 8, no. 6, pp. 2381\u20132392, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Expected energy-based restricted boltzmann machine for classification", "author": ["S. Elfwing", "E. Uchibe", "K. Doya"], "venue": "Neural Networks, vol. 64, pp. 29\u201338, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1828}, {"title": "Rate-coded restricted boltzmann machines for face recognition", "author": ["Y.W. Teh", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, pp. 908\u2013914, 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Replicated softmax: an undirected topic model", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 1607\u20131614.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Application of deep belief networks for natural language understanding", "author": ["R. Sarikaya", "G.E. Hinton", "A. Deoras"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 22, no. 4, pp. 778\u2013784, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A generative restricted boltzmann machine based method for high-dimensional motion data modeling", "author": ["S. Nie", "Z. Wang", "Q. Ji"], "venue": "Computer Vision and Image Understanding, vol. 136(C), pp. 14\u201322, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Voice conversion using rnn pre-trained by recurrent temporal restricted boltzmann machines", "author": ["T. Nakashika", "T. Takiguchi", "Y. Ariki"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 3, pp. 580\u2013587, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep belief networks for automatic music genre classification", "author": ["X. Yang", "Q. Chen", "S. Zhou", "X. Wang"], "venue": "Twelfth Annual Conference of the International Speech Communication Association, vol. 8, no. 11, 2011, pp. 13\u201316.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time keypoint recognition using restricted boltzmann machine", "author": ["M. Yuan", "H. Tang", "H. Li"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 11, pp. 2119\u20132126, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Periocular recognition using unsupervised convolutional rbm feature learning", "author": ["L. Nie", "A. Kumar", "S. Zhan"], "venue": "Pattern Recognition (ICPR), 2014 22nd International Conference on. IEEE, 2014, pp. 399\u2013 404.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Fuzzy restricted boltzmann machine for the enhancement of deep learning", "author": ["C. Chen", "C.-Y. Zhang", "L. Chen", "M. Gan"], "venue": "Fuzzy Systems, IEEE Transactions on, vol. 23, no. 6, pp. 2163\u20132173, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "R\u00e9nyi divergence based generalization for learning of classification restricted boltzmann machines", "author": ["Q. Yu", "Y. Hou", "X. Zhao", "G. Cheng"], "venue": "Data Mining Workshop (ICDMW), 2014 IEEE International Conference on. IEEE, 2014, pp. 692\u2013697.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "The spike-andslab rbm and extensions to discrete and sparse data distributions", "author": ["A. Courville", "G. Desjardins", "J. Bergstra", "Y. Bengio"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 36, no. 9, pp. 1874\u20131887, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1874}, {"title": "Gaussian-binary restricted boltzmann machines on modeling natural image statistics", "author": ["N. Wang", "J. Melchior", "L. Wiskott"], "venue": "arXiv preprint arXiv:1401.5900, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse deep belief net models for visual area v2", "author": ["C. Ekanadham", "S. Reader", "H. Lee"], "venue": "Advances in Neural Information Processing Systems, vol. 20, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Modeling documents with deep boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1309.6865, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["I. Sutskever", "G.E. Hinton", "G.W. Taylor"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 1601\u20131608.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised 3d local feature learning by circle convolutional restricted boltzmann machine", "author": ["Z. Han", "Z. Liu", "J. Han", "C.-M. Vong", "S. Bu", "X. Li"], "venue": "IEEE Transactions on Image Processing, vol. 25, no. 11, pp. 5331\u20135344, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Non-parallel training in voice conversion using an adaptive restricted boltzmann machine", "author": ["T. Nakashika", "T. Takiguchi", "Y. Minami"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 11, pp. 2032\u20132045, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning relevance restricted boltzmann machine for unstructured group activity and event understanding", "author": ["F. Zhao", "Y. Huang", "L. Wang", "T. Xiang", "T. Tan"], "venue": "International Journal of Computer Vision, pp. 1\u201317, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Theta-rbm: Unfactored gated restricted boltzmann machine for rotation-invariant representations", "author": ["M.V. Giuffrida", "S.A. Tsaftaris"], "venue": "arXiv preprint arXiv:1606.08805, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Estimating 3d trajectories from 2d projections via disjunctive factored four-way conditional restricted boltzmann machines", "author": ["D.C. Mocanu", "H.B. Ammar", "L. Puig", "E. Eaton", "A. Liotta"], "venue": "arXiv preprint arXiv:1604.05865, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "A novel feature extraction method for scene recognition based on centered convolutional restricted boltzmann machines", "author": ["J. Gao", "J. Yang", "G. Wang", "M. Li"], "venue": "Neurocomputing, vol. 11, no. 2, pp. p14\u201319, 2016.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Social restricted boltzmann machine: Human behavior prediction in health social networks", "author": ["N. Phan", "D. Dou", "B. Piniewski", "D. Kil"], "venue": "2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE, 2015, pp. 424\u2013431.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Temperature based restricted boltzmann machines.", "author": ["G. Li", "L. Deng", "Y. Xu", "C. Wen", "W. Wang", "J. Pei", "L. Shi"], "venue": "Scientific Reports,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Learning deep hierarchical visual feature coding", "author": ["H. Goh", "N. Thome", "M. Cord", "J.-H. Lim"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 12, pp. 2212\u20132225, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Constrained kmeans clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "ICML, vol. 1, 2001, pp. 577\u2013584.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "Constrained 1-spectral clustering", "author": ["S.S. Rangapuram", "M. Hein"], "venue": "arXiv preprint arXiv:1505.06485, 2015.  12", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised clustering based on affinity propagation algorithm", "author": ["Y.J. XIAO Yu"], "venue": "Journal of Software, vol. 19, no. 11, pp. 2803\u20132813, 2008.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling image patches with a directed hierarchy of markov random fields", "author": ["S. Osindero", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2008, pp. 1121\u20131128.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning informative features from restricted boltzmann machines", "author": ["J.M. Tomczak"], "venue": "Neural Processing Letters, vol. 44, no. 3, pp. 1\u201316, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental extreme learning machine based on deep feature embedded", "author": ["J. Zhang", "S. Ding", "N. Zhang", "Z. Shi"], "venue": "International Journal of Machine Learning and Cybernetics, vol. 7, no. 1, pp. 111\u2013120, 2016.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multilevel distributed representations for high-dimensional sequences", "author": ["I. Sutskever", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, 2007, pp. 548\u2013555.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "Conditional restricted boltzmann machines for structured output prediction", "author": ["V. Mnih", "H. Larochelle", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1202.3748, 2012.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Improved learning of gaussian-bernoulli restricted boltzmann machines", "author": ["K. Cho", "A. Ilin", "T. Raiko"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN 2011. Springer, 2011, pp. 10\u201317.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian-bernoulli deep boltzmann machine", "author": ["K.H. Cho", "T. Raiko", "A. Ilin"], "venue": "Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1\u20137.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Two distributed-state models for generating high-dimensional time series", "author": ["G.W. Taylor", "G.E. Hinton", "S.T. Roweis"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 1025\u20131068, 2011.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised deep learning with auxiliary networks", "author": ["J. Zhang", "G. Tian", "Y. Mu", "W. Fan"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 353\u2013361.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep transductive semi-supervised maximum margin clustering", "author": ["G. Chen"], "venue": "arXiv preprint arXiv:1501.06237, 2015.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the National Academy of Sciences, vol. 79, no. 8, pp. 2554\u20132558, 1982.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1982}, {"title": "Unsupervised learning of distributions of binary vectors using two layer networks", "author": ["Y. Freund", "D. Haussler"], "venue": "Computer Research Laboratory [University of California, Santa Cruz],", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1994}, {"title": "Convolutional deep belief networks on cifar-10", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Unpublished manuscript, vol. 40, 2010.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning deep generative models", "author": ["R. Salakhutdinov"], "venue": "Ph.D. dissertation, University of Toronto, 2009.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2009}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Momentum, vol. 9, no. 1, p. 926, 2010.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamical analysis of contrastive divergence learning: Restricted boltzmann machines with gaussian visible units.", "author": ["R. Karakida", "M. Okada", "S.I. Amari"], "venue": "Neural Networks the Official Journal of the International Neural Network Society,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2016}, {"title": "Msra-mm 2.0: A large-scale web multimedia dataset", "author": ["H. Li", "M. Wang", "X.-S. Hua"], "venue": "Data Mining Workshops, 2009. ICDMW\u201909. IEEE International Conference on. IEEE, 2009, pp. 164\u2013169.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2009}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "Science, vol. 315, no. 5814, pp. 972\u2013976, 2007.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2007}, {"title": "Document clustering using locality preserving indexing", "author": ["D. Cai", "X. He", "J. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 12, pp. 1624\u20131637, 2005.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2005}, {"title": "Orthogonal nonnegative matrix tfactorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 126\u2013135.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Hinton and Sejnowski[1] proposed a learning algorithm for general Boltzmann machine which has hidden-to-hidden and visible-to-visible connections, but in practice it was too slow to be used.", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "Then, the restricted Blotzmann machine (RBM) was proposed by[2] in 1986, which has no lateral connections among nodes in each layer, so the learning procedure becomes much more efficient than general Blotzmann machine.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "There has been extensive research into the RBM since Hinton proposed fast learning algorithms[3],", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "cn [4] by contrastive divergence (CD) learning algorithm.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Several power and tractability deep networks was proposed, including deep belief networks[5], deep autoencoder[6], deep Boltzmann machine[7], deep dropout neural net[8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Several power and tractability deep networks was proposed, including deep belief networks[5], deep autoencoder[6], deep Boltzmann machine[7], deep dropout neural net[8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "Several power and tractability deep networks was proposed, including deep belief networks[5], deep autoencoder[6], deep Boltzmann machine[7], deep dropout neural net[8].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "Several power and tractability deep networks was proposed, including deep belief networks[5], deep autoencoder[6], deep Boltzmann machine[7], deep dropout neural net[8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 8, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 114, "endOffset": 118}, {"referenceID": 16, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 159, "endOffset": 163}, {"referenceID": 18, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 195, "endOffset": 199}, {"referenceID": 19, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 216, "endOffset": 220}, {"referenceID": 20, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 246, "endOffset": 250}, {"referenceID": 21, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 268, "endOffset": 272}, {"referenceID": 22, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 302, "endOffset": 306}, {"referenceID": 23, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 339, "endOffset": 343}, {"referenceID": 24, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 370, "endOffset": 374}, {"referenceID": 25, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 42, "endOffset": 46}, {"referenceID": 26, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 66, "endOffset": 70}, {"referenceID": 27, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 174, "endOffset": 178}, {"referenceID": 29, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 222, "endOffset": 226}, {"referenceID": 30, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 257, "endOffset": 261}, {"referenceID": 31, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 310, "endOffset": 314}, {"referenceID": 32, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 373, "endOffset": 377}, {"referenceID": 33, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 416, "endOffset": 420}, {"referenceID": 34, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 468, "endOffset": 472}, {"referenceID": 35, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 519, "endOffset": 523}, {"referenceID": 36, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 607, "endOffset": 611}, {"referenceID": 37, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 673, "endOffset": 677}, {"referenceID": 38, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 721, "endOffset": 725}, {"referenceID": 39, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 782, "endOffset": 786}, {"referenceID": 40, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 823, "endOffset": 827}, {"referenceID": 41, "context": "In addition, the pcGRBM model for clustering is better performance than some semi-supervised algorithms (Cop-Kmeans[42], SemiSpectral clustering (Semi-SP)[43] and semi-supervised affinity propagation (Semi-AP)[44]).", "startOffset": 115, "endOffset": 119}, {"referenceID": 42, "context": "In addition, the pcGRBM model for clustering is better performance than some semi-supervised algorithms (Cop-Kmeans[42], SemiSpectral clustering (Semi-SP)[43] and semi-supervised affinity propagation (Semi-AP)[44]).", "startOffset": 154, "endOffset": 158}, {"referenceID": 43, "context": "In addition, the pcGRBM model for clustering is better performance than some semi-supervised algorithms (Cop-Kmeans[42], SemiSpectral clustering (Semi-SP)[43] and semi-supervised affinity propagation (Semi-AP)[44]).", "startOffset": 209, "endOffset": 213}, {"referenceID": 20, "context": "There are several common methods to develop standard RBM such as adding connections information between the visible units and the hidden units, changing the value type of visible or hidden units, expanding the relationships of the units between visible layer and hidden layer from constant to variable by fuzzy mathematics, constructing deep network based on autoencoder[21] by pairwise constraints.", "startOffset": 370, "endOffset": 374}, {"referenceID": 44, "context": "Osindero and Hinton proposed a semi-restricted Boltzmann machines (SRBM)[45] which has lateral connections between the visible units, but these lateral connections are unit-level semi-supervised information.", "startOffset": 72, "endOffset": 76}, {"referenceID": 45, "context": "In order to enforce hidden units to be pairwise uncorrelated and to maximize entropy, Tomczak[46] proposed to add penalty term to the log-likelihood function.", "startOffset": 93, "endOffset": 97}, {"referenceID": 46, "context": "[47] built deep belief network based on SRBM for classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "Sutskever and Hinton proposed temporal restricted Boltzmann machine (TRBM)[48] by adding directed connections between previous and current states of the visible and hidden units.", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Furthermore, they proposed the recurrent TRBM (RTRBM)[32].", "startOffset": 53, "endOffset": 57}, {"referenceID": 48, "context": "Mnih and Hinton proposed the conditional restricted Boltzmann machines (CRBMs)[49] by adding conditioning vector which determines increments to the biases of the visible and hidden layer of the traditional RBM.", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "[28] developed the spike-and-slab restricted Boltzmann machine (ssRBM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] proposed a fuzzy restricted Boltzmann machine (FRBM) to enhance deep learning capability which can avoid the flaw.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] proposed to theoretically extend the conventional RBMs by introducing another term in the energy function to explicitly model the local spatial interactions in the input data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "Then one common way is to replace them by means of Gaussian linear units, that is Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs)[50].", "startOffset": 139, "endOffset": 143}, {"referenceID": 50, "context": "[51] proposed a novel method to improve their learning efficiency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "Moreover, the deep networks of Gaussian-Bernoulli deep Boltzmann machine (GDBM)[52], [53] has been developed by the GBRBM in recent years.", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "Moreover, the deep networks of Gaussian-Bernoulli deep Boltzmann machine (GDBM)[52], [53] has been developed by the GBRBM in recent years.", "startOffset": 85, "endOffset": 89}, {"referenceID": 53, "context": "proposed a mixed model named as supervision guided autoencoder (SUGAR)[54] which includes three components: main network, auxiliary network and bridge.", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "The main network is a sparsity-encouraging variant of the autoencoder[21], that is the unsupervised autoencoder.", "startOffset": 69, "endOffset": 73}, {"referenceID": 54, "context": "In the work of [55], Chen proposed a deep network structure based on RBMs which is the most related to our work.", "startOffset": 15, "endOffset": 19}, {"referenceID": 54, "context": "Both the work of [55] and our work aim to solve the similar problems, e.", "startOffset": 17, "endOffset": 21}, {"referenceID": 1, "context": "A RBM[2][3] is a two-layer network in which the first layer consists of visible units, and the second layer consists of hidden units.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "A RBM[2][3] is a two-layer network in which the first layer consists of visible units, and the second layer consists of hidden units.", "startOffset": 8, "endOffset": 11}, {"referenceID": 55, "context": "An energy function[56] of a joint configuration (v, h) between the visible layer and the hidden layer is given by:", "startOffset": 18, "endOffset": 22}, {"referenceID": 2, "context": "Hinton[3] proposed a faster learning algorithm with the CD learning and the change of learning parameter is given by:", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "Original RBMs were developed by binary stochastic units for the hidden and visible layers[3].", "startOffset": 89, "endOffset": 92}, {"referenceID": 56, "context": "To deal with real-valued data such as natural images, one solution is that the binary visible units are replaced by linear units with independent Gaussian noise, but the hidden units remain binary, which is first suggested by[57].", "startOffset": 225, "endOffset": 229}, {"referenceID": 49, "context": "For each visible unit, it is easy to learn the variance of the noise, but it is difficult using CD1 because of taking long time[50][58].", "startOffset": 127, "endOffset": 131}, {"referenceID": 57, "context": "For each visible unit, it is easy to learn the variance of the noise, but it is difficult using CD1 because of taking long time[50][58].", "startOffset": 131, "endOffset": 135}, {"referenceID": 49, "context": "Therefore, in many applications, it is easy to normalise the data to have unit variance and zero mean[50][59][60][61].", "startOffset": 101, "endOffset": 105}, {"referenceID": 58, "context": "Therefore, in many applications, it is easy to normalise the data to have unit variance and zero mean[50][59][60][61].", "startOffset": 105, "endOffset": 109}, {"referenceID": 59, "context": "Therefore, in many applications, it is easy to normalise the data to have unit variance and zero mean[50][59][60][61].", "startOffset": 109, "endOffset": 113}, {"referenceID": 60, "context": "Therefore, in many applications, it is easy to normalise the data to have unit variance and zero mean[50][59][60][61].", "startOffset": 113, "endOffset": 117}, {"referenceID": 61, "context": "[62] demonstrated that CD1 learning is simpler than ML learning in RBMs with Gaussian linear units.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "We used the Microsoft Research Asia Multimedia (MSRAMM)[63] which contains two sub-datasets, e.", "startOffset": 55, "endOffset": 59}, {"referenceID": 63, "context": "In our experiments, we choose K-means, affinity propagation (AP)[64], SP clustering algorithms as examples.", "startOffset": 64, "endOffset": 68}, {"referenceID": 41, "context": "Secondly, the proposed algorithms are used to compare with Cop-Kmeans[42], Semi-SP[43] and Semi-AP[44], respectively.", "startOffset": 69, "endOffset": 73}, {"referenceID": 42, "context": "Secondly, the proposed algorithms are used to compare with Cop-Kmeans[42], Semi-SP[43] and Semi-AP[44], respectively.", "startOffset": 82, "endOffset": 86}, {"referenceID": 43, "context": "Secondly, the proposed algorithms are used to compare with Cop-Kmeans[42], Semi-SP[43] and Semi-AP[44], respectively.", "startOffset": 98, "endOffset": 102}, {"referenceID": 64, "context": "To evaluate the performance of the clustering algorithms, we adopt three widely used metrics: clustering accuracy[65], clustering purity[66] and clustering rank[67] as the evaluation measure.", "startOffset": 113, "endOffset": 117}, {"referenceID": 65, "context": "To evaluate the performance of the clustering algorithms, we adopt three widely used metrics: clustering accuracy[65], clustering purity[66] and clustering rank[67] as the evaluation measure.", "startOffset": 136, "endOffset": 140}], "year": 2017, "abstractText": "Restricted Boltzmann machines (RBMs) and their variants are usually trained by contrastive divergence (CD) learning, but the training procedure is an unsupervised learning approach, without any guidances of the background knowledge. To enhance the expression ability of traditional RBMs, in this paper, we propose pairwise constraints restricted Boltzmann machine with Gaussian visible units (pcGRBM) model, in which the learning procedure is guided by pairwise constraints and the process of encoding is conducted under these guidances. The pairwise constraints are encoded in hidden layer features of pcGRBM. Then, some pairwise hidden features of pcGRBM flock together and another part of them are separated by the guidances. In order to deal with real-valued data, the binary visible units are replaced by linear units with Gausian noise in the pcGRBM model. In the learning process of pcGRBM, the pairwise constraints are iterated transitions between visible and hidden units during CD learning procedure. Then, the proposed model is inferred by approximative gradient descent method and the corresponding learning algorithm is designed in this paper. In order to compare the availability of pcGRBM and traditional RBMs with Gaussian visible units, the features of the pcGRBM and RBMs hidden layer are used as input \u2018data\u2019 for K-means, spectral clustering (SP) and affinity propagation (AP) algorithms, respectively. A thorough experimental evaluation is performed with sixteen image datasets of Microsoft Research Asia Multimedia (MSRA-MM). The experimental results show that the clustering performance of K-means, SP and AP algorithms based on pcGRBM model are significantly better than traditional RBMs. In addition, the pcGRBM model for clustering task shows better performance than some semi-supervised clustering algorithms.", "creator": "LaTeX with hyperref package"}}}