{"id": "1701.06123", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2017", "title": "Optimization on Product Submanifolds of Convolution Kernels", "abstract": "We address this solve of predictive on commercial of modules straight-four of laplacian jelly (PEMs) in two-component cortex networks (CNNs ). First, thinking aim quota both \u03b3 are for PEMs in defined though dynamic topologies. Next, we accept full SGD combinations, specifically C - SGD, meanwhile straightforwardly same SGD methods employs started kernel 4,114 only optimization on product between furthermore prints such emulator submanifolds. Then, we identify concept exposure far itself C - SGD idea bosnia-herzegovina \u03b3 acquiring of PEMs. In three subjects performance, we prayerfully entire constraints we can done well hold computable adaptive hoped sizes of full C - SGD having order then assure the asymptotic theory.", "histories": [["v1", "Sun, 22 Jan 2017 05:35:39 GMT  (181kb)", "http://arxiv.org/abs/1701.06123v1", "3 pages"]], "COMMENTS": "3 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["mete ozay", "takayuki okatani"], "accepted": false, "id": "1701.06123"}, "pdf": {"name": "1701.06123.pdf", "metadata": {"source": "CRF", "title": "Optimization on Product Submanifolds of Convolution Kernels", "authors": ["Mete Ozay", "Takayuki Okatani"], "emails": ["mozay@vision.is.tohoku.ac.jp", "okatani@vision.is.tohoku.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n06 12\n3v 1\n[ cs\n.C V\n] 2\n2 Ja\nn 20"}, {"heading": "1 INTRODUCTION", "text": "Product manifolds have been used to solve various optimization problems in machine learning, pattern recognition and computer vision. In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2]. Ozay and Okatani [5] proposed a SGD method to train CNNs using embedded kernel submanifolds of convolution kernels with convergence properties. Products of the Stiefel manifolds are used for action recognition by employing CNNs in [3].\nIn this paper, we address a problem of optimization on products of embedded kernel submanifolds (PEMs) using a SGD method to train CNNs considering its convergence properties. Our contribution can be summarized as follows:\n1) We first explore the effect of geometric properties of product manifolds, such as sectional curvature and geodesic distance, on convergence properties of PEMs compared to that of the SGD employed on component manifolds. 2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD. 3) In the theoretical results, we first explore the effect of geometric properties of component manifolds on the convergence of a SGD employed on PEMs. Then, we employ the results for adaptive step size estimation of the SGD. Moreover, we provide an example for employment of the theoretical results for optimization on PEMs of the sphere."}, {"heading": "2 OPTIMIZATION ON PRODUCT MANIFOLDS FOR TRAINING OF CNNS", "text": "We address the problem of optimization on products of kernel submanifolds to train CNNs using an SGD method. Suppose that we are given a set of training samples S = {si = (Ii, yi)}Ni=1 of a random variable s drawn from a distribution P on a measurable space S, where yi is a class label of the ith image Ii. An L-layer CNN consists of a set of tensors W = {Wl}Ll=1, where Wl = {Wd,l \u2208 RAl\u00d7Bl\u00d7Cl}Dld=1, and Wd,l = [Wc,d,l \u2208 RAl\u00d7Bl]Clc=1 is a tensor1 composed of kernels (weight matrices) Wc,d,l constructed at each layer l = 1,2, . . . , L, for each cth channel c = 1,2, . . . ,Cl and each dth kernel d = 1,2, . . . ,Dl.\nAt each lth convolution layer, we compute a feature representation fl(Xl;Wl) by compositionally employing non-linear functions, and convolving an image I with kernels by fl(Xl;Wl) = fl(\u22c5;Wl) \u25cb \u22ef \u25cb f1(X1;W1), (1) where X1 \u2236= I is an image for l = 1, and Xl = [Xc,l]Clc=1. The cth channel of the data matrix Xc,l is convolved with the kernel Wc,d,l to obtain the d\nth feature map Xc,l+1 \u2236= X\u0302d,l by X\u0302d,l =Wc,d,l \u2217Xc,l,\u2200c, d, l 2. Given a batch of samples s \u2286 S, we denote a value of a classification loss function for a kernel \u03c9 \u225cWc,d,l by L(\u03c9, s), and the loss function of kernels W utilized in the CNN by L(W , s). If we assume that s contains a single sample, then, an expected loss or cost function of the CNN is computed by\nL(W) \u225c EP{L(W , s)} = \u222b L(W , s)dP . (2) The expected loss L(\u03c9) for \u03c9 is computed by\nL(\u03c9) \u225c EP{L(\u03c9, s)} = \u222b L(\u03c9, s)dP . (3) For a finite set of samples S, L(W) is approximated by an empirical loss 1 \u2223S\u2223 \u2211\u2223S\u2223i=1 L(W , si), where \u2223S\u2223 is the size of S (similarly, L(\u03c9) is approximated by the empirical loss for \u03c9). Then, feature representations are learned using SGD by solving\nmin W L(W). (4) In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the lth layer 1. We use shorthand notation for matrix concatenation such that [Wc,d,l] Cl c=1 \u225c [W1,d,l,\u22ef,WCl,d,l].\n2. We ignore the bias terms in the notation for the sake of simplicity.\n2 of a CNN, such that \u03c9 \u2208Mc,d,l,\u2200c, d. In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.\nDefinition 2.1 (Product of embedded kernel submanifolds of convolution kernels). Suppose that Gl = {M\u03b9 \u2236 \u03b9 \u2208 IGl} is a collection of kernel submanifolds M\u03b9 of dimension d\u03b9, which is identified by a set of indices IGl ,\u2200l = 1,2, . . . , L. Moreover, suppose that Gl \u2286 Gl is a subset of manifolds identified by IGl \u2286 IGl .\nA Gl product manifold of convolution kernels (Gl-PEM) constructed at the lth layer of an L layer CNN is a product of embedded kernel manifolds belonging to Gl which is computed by\nMGl = \u2a09 \u03b9\u2208IGl M\u03b9, (5)\nwhere \u2a09 is the topological Cartesian product. Given any kernel \u03c9\u03b9 \u2208 M\u03b9, the product map\n\u03c6Gl \u2236 UGl \u2192 R dGl (6)\nwhere dGl = \u2211 \u03b9\u2208IGl d\u03b9, (U\u03b9, \u03c6\u03b9) is a coordinate chart for M\u03b9 with \u03c9\u03b9 \u2208 U\u03b9, \u03c6Gl = \u2a09\n\u03b9\u2208IGl \u03c6\u03b9, and UGl = \u2a09 \u03b9\u2208IGl U\u03b9. A kernel\n\u03c9Gl \u2208MGl is then obtained by concatenating kernels belonging toM\u03b9 \u2208 Gl, \u2200\u03b9 \u2208 IGl , using \u03c9Gl = (\u03c91, \u03c92,\u22ef, \u03c9\u2223IGl \u2223), where \u2223IGl \u2223 is the cardinality of IGl . The product smooth manifold structure is defined by the smooth map\n\u03c6Gl \u25cb\u03c8 \u22121 Gl = \u2a09\n\u03b9\u2208IGl\n\u03c6\u03b9 \u00d7\u03c8 \u22121 \u03b9 , (7)\nwhere \u03c8Gl = \u2a09 \u03b9\u2208IGl \u03c8\u03b9. A Gl-PEM is simply called a product of embedded kernel submanifolds of convolution kernels (PEM). \u220e\nWhen we employ a C-SGD on a Gl-PEM MGl , each kernel \u03c9 is moved on the Gl-PEM in the descent direction of gradient of loss at each tth step of the C-SGD. More precisely, direction and amount of movement of a kernel \u03c9tl are determined at the t\nth step and the lth layer by\n1) projection of the gradient gradE L(\u03c9tl ), which is obtained using back-propagation from the upper layer, onto the tangent space T\u03c9t l MGl at gradL(\u03c9tl ) (line 6 of Algorithm 1), 2) movement of \u03c9tl on T\u03c9t l MGl according to a function of the\nlearning rate to vt (line 7 of Algorithm 1), and 3) projection of the moved kernel at vt onto the manifold MGl\nto compute \u03c9t+1l (line 8 of Algorithm 1).\nConvergence properties of the C-SGD are affected first by the smooth manifold structure of MGl as defined in Definition 2.1. In addition, convergence rate is affected by metric and curvature properties of MGl which are analyzed in the next theorem.\nLemma 2.2 (Metric and curvature properties of PEMs). If each kernel submanifold M\u03b9 is a Riemannian manifold endowed with a metric g\u03b9, then a Gl-PEM is endowed with the metric gGl = \u2295\n\u03b9\u2208IGl\ng\u03b9 defined by\ngGl( \u2211 \u03b9\u2208IGl u\u03b9, \u2211 \u03b9\u2208IGl v\u03b9) = \u2211 \u03b9\u2208IGl g\u03b9(u\u03b9, v\u03b9), (8)\nwhere u\u03b9 \u2208 T\u03c9\u03b9M\u03b9 and v\u03b9 \u2208 T\u03c9\u03b9M\u03b9 are tangent vectors belonging to the tangent space T\u03c9\u03b9M\u03b9 computed at \u03c9\u03b9 \u2208 M\u03b9. In addition, if\nC\u03b9 is the Riemannian curvature tensor of M\u03b9 and x\u03b9, y\u03b9 \u2208 T\u03c9\u03b9M\u03b9, then the Riemannian curvature tensor CGl of MGl is computed by\nCGl( \u2211 \u03b9\u2208IGl u\u03b9, \u2211 \u03b9\u2208IGl v\u03b9, \u2211 \u03b9\u2208IGl x\u03b9, \u2211 \u03b9\u2208IGl y\u03b9) = C\u0302, (9)\nwhere C\u0302 \u225c \u2211 \u03b9\u2208IGl C\u03b9(u\u03b9, v\u03b9, x\u03b9, y\u03b9). It follows that the MGl has never strictly positive sectional curvature cGl in the metric (8). In addition, no compact MGl admits a metric with negative sectional curvature cGl .\nWe compute the metric of a Gl-PEM MGl using metrics identified on the component manifolds M\u03b9 employing (8) given in Lemma 2.2. In addition, we use the sectional curvature of the MGl given in Lemma 2.2 to analyze convergence of the C-SGD, and to compute adaptive step size. Note that some sectional curvatures vanish on the MGl by the lemma. For instance, suppose that eachM\u03b9 is a unit two-sphere S2, \u2200\u03b9 \u2208 IGl . Then, MGl computed by (5) has unit curvature along two-dimensional subspaces of its tangent spaces, called two-planes. On the other hand, MGl has zero curvature along all two-planes spanning exactly two distinct spheres. Therefore, learning rates need to be computed adaptively according to sectional curvatures at each layer of the CNN and at each epoch of the C-SGD for each kernel \u03c9 on each manifold MGl . In order to explode this property more precisely, we introduce the following theorem.\nTheorem 2.3. Suppose that there exists a local minimum \u03c9\u0302Gl \u2208 MGl ,\u2200Gl \u2286 Gl,\u2200l, and \u2203\u01eb > 0 such that inf \u03c1t Gl >\u01eb 1 2 \u27e8\u03c6\u03c9t Gl\n(\u03c9\u0302Gl)\u22121,\u2207L(\u03c9tGl)\u27e9 < 0, where \u03c6 is an exponential map or a twice continuously differentiable retraction, \u27e8\u22c5, \u22c5\u27e9 is the inner product and \u03c1tGl \u225c \u03c1(\u03c9tGl , \u03c9\u0302Gl) is the geodesic distance between \u03c9tGl and \u03c9\u0302Gl on MGl . In addition, suppose that the following conditions are satisfied for each MGl ;\n(i) Condition on learning rate g(t,\u0398): \u221e\n\u2211 t=0 g(t,\u0398) = +\u221e and \u221e\u2211 t=0 g(t,\u0398)2 <\u221e. (10) (ii) Condition on step size and direction\nh(gradL(\u03c9tGl), g(t,\u0398)): h(gradL(\u03c9tGl), g(t,\u0398)) = \u2212g(t,\u0398)g(\u03c9t\nGl )gradL(\u03c9 t Gl), (11)\nwhere g(\u03c9tGl) = max{1,\u0393t1}, \u0393t1 = (RtGl)2\u0393t2, \u0393t2 = max{(2\u03c1tGl +RtGl)2, (1 + cGl(\u03c1tGl +RtGl))}, cGl is the sectional curvature of MGl , R t Gl\n\u225c \u2225gradL(\u03c9tGl)\u22252, and \u2225gradL(\u03c9tGl)\u22252 = ( \u2211\n\u03c9t l,\u03b9 \u2208M\u03b9,\u03b9\u2208IGl\ngradL(\u03c9tl,\u03b9)2) 1 2 .\nThen, the loss function and the gradient converges almost surely (a.s.) by L(\u03c9tGl) a.s.\u00d0\u00d0\u00d0\u2192t\u2192\u221e L(\u03c9\u0302Gl), and \u2207L(\u03c9 t Gl ) a.s.\u00d0\u00d0\u00d0\u2192 t\u2192\u221e 0, for each MGl ,\u2200l.\nConditions (i) and (ii) given in Theorem 2.3 can be used to compute adaptive learning rates and step sizes to train CNNs by optimization using the C-SGD algorithm (Algorithm 1) assuring convergence to minima. As an example, we use the results obtained from Lemma 2.2 in Theorem 2.3 to compute adaptive step sizes for the sphere in the following theorem.\n3 Algorithm 1 C-SGD on products of kernel submanifolds. 1: Input: T (number of iterations), S (training set),\n\u0398 (set of hyperparameters), L (a loss function), IGl ,\u2200l (sets of indices) . 2: Initialization: Construct a collection of products of kernel submanifolds Gl,\u2200l = 1,2, . . . , L, and initialize kernels \u03c9tGl \u2208MGl , \u2200Gl \u2286 Gl,\u2200l. 3: for each iteration t = 1,2, . . . , T do 4: for each layer l = 1,2, . . . , L do 5: Compute the Euclidean gradient gradEL(\u03c9tGl). 6: gradL(\u03c9tGl) \u2236= \u03a0\u03c9tl (gradE L(\u03c9tGl),\u0398). 7: vt \u2236= h(gradL(\u03c9tGl), g(t,\u0398)). 8: \u03c9t+1Gl \u2236= \u03c6\u03c9tGl (vt),\u2200\u03c9 t Gl\n. 9: t \u2236= t + 1.\n10: end for 11: end for 12: Output: A set of estimated kernels {\u03c9TGl}Ll=1.\nCorollary 2.4. Suppose that M\u03b9 are identified by d\u03b9 \u2265 2 dimensional unit sphere Sd\u03b9 , and \u03c1tGl \u2264 c\u0302\u22121, where c\u0302 is an upper bound on the sectional curvatures of MGl ,\u2200l at \u03c9 t Gl\n\u2208 MGl ,\u2200t. If condition (i) given in Theorem 2.3 is satisfied, and step size is computed using (11) with g(\u03c9tGl) =max{1, (RtGl)2(2 +RtGl)2}, then L(\u03c9tGl) a.s.\u00d0\u00d0\u00d0\u2192t\u2192\u221e L(\u03c9\u0302Gl), and \u2207L(\u03c9 t Gl ) a.s.\u00d0\u00d0\u00d0\u2192 t\u2192\u221e 0, for each MGl ,\u2200l."}, {"heading": "3 CONCLUSION", "text": "We address a problem of optimization on product of embedded submanifolds of convolution kernels (PEMs) in convolutional neural networks (CNNs). In our theoretical results, we first explore metric and curvature properties of PEMs and show that PEMs have strictly positive sectional curvature. Next, we propose a SGD method, called C-SGD, by generalizing the SGD methods employed on kernel submanifolds for optimization on product of different collections of kernel submanifolds. Then, we analyze convergence properties of C-SGD employed on PEMs considering their sectional curvature properties. In the theoretical results, we expound the constraints that can be used to compute adaptive step size of C-SGD in order to assure its asymptotic convergence. Moreover, we employ the proposed methods for computation of adaptive step size of C-SGD for optimization on PEMs of the sphere. Our proposed C-SGD and the theoretical results can be used to train CNNs using products of different kernels at different layers by estimating adaptive step size of C-SGD with assurance of convergence."}], "references": [{"title": "Generalized backpropagation, \u00e9tude de cas: Orthogonality", "author": ["M. Harandi", "B. Fernando"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "A riemannian network for spd matrix learning", "author": ["Z. Huang", "L.V. Gool"], "venue": "In Assoc. for the Adv. of Artificial Intelligence (AAAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Deep learning on lie groups for skeleton-based action recognition", "author": ["Z. Huang", "C. Wan", "T. Probst", "L.V. Gool"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Building deep networks on grassmann manifolds", "author": ["Z. Huang", "J. Wu", "L.V. Gool"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Optimization on submanifolds of convolution kernels in cnns", "author": ["M. Ozay", "T. Okatani"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].", "startOffset": 219, "endOffset": 222}, {"referenceID": 1, "context": "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].", "startOffset": 224, "endOffset": 227}, {"referenceID": 4, "context": "Ozay and Okatani [5] proposed a SGD method to train CNNs using embedded kernel submanifolds of convolution kernels with convergence properties.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "Products of the Stiefel manifolds are used for action recognition by employing CNNs in [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD.", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the l layer", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the l layer", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the l layer", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.", "startOffset": 131, "endOffset": 134}], "year": 2017, "abstractText": "We address a problem of optimization on product of embedded submanifolds of convolution kernels (PEMs) in convolutional neural networks (CNNs). First, we explore metric and curvature properties of PEMs in terms of component manifolds. Next, we propose a SGD method, called C-SGD, by generalizing the SGD methods employed on kernel submanifolds for optimization on product of different collections of kernel submanifolds. Then, we analyze convergence properties of the C-SGD considering sectional curvature properties of PEMs. In the theoretical results, we expound the constraints that can be used to compute adaptive step sizes of the C-SGD in order to assure the asymptotic convergence.", "creator": "LaTeX with hyperref package"}}}