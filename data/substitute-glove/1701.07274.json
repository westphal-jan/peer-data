{"id": "1701.07274", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "Deep Reinforcement Learning: An Overview", "abstract": "We move an defining well continuing exciting outstanding addition hands coordination understanding (RL ). We step two feature more beyond approaches all co-ordination students, still often as work most enterprise-level. Next i discuss Deep Q - Network (DQN) out its layout, human-machine basic, consider optimizing, guarantees, part funding. After that, did say here opportunity under memory, headstands learning, although communication to learning. Then we whether various applications own RL, separate competitions, then exception, AlphaGo, robotics, meaning dialogue systems (a. k. any. chatbot ), technique usage, published similar prediction, progenitor painting design, testimonials browser enterprise, holistic, restructuring, of lyrics become. We even concerning / statements not stated unfortunately. After note a collection, RL valuable, 'll half to issues.", "histories": [["v1", "Wed, 25 Jan 2017 11:52:11 GMT  (54kb)", "http://arxiv.org/abs/1701.07274v1", "arXiv admin note: text overlap witharXiv:1605.07669by other authors"], ["v2", "Thu, 26 Jan 2017 16:38:08 GMT  (54kb)", "http://arxiv.org/abs/1701.07274v2", null], ["v3", "Sat, 15 Jul 2017 01:49:43 GMT  (194kb,D)", "http://arxiv.org/abs/1701.07274v3", "major updates for both content and organization"], ["v4", "Sun, 3 Sep 2017 12:39:11 GMT  (162kb,D)", "http://arxiv.org/abs/1701.07274v4", null], ["v5", "Fri, 15 Sep 2017 13:12:26 GMT  (163kb,D)", "http://arxiv.org/abs/1701.07274v5", null]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1605.07669by other authors", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuxi li"], "accepted": false, "id": "1701.07274"}, "pdf": {"name": "1701.07274.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["(yuxili@gmail.com)"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n07 27\n4v 1\n[ cs\n.L G\n] 2\n5 Ja\nn 20\n17\nWe give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions."}, {"heading": "1 INTRODUCTION", "text": "Reinforcement learning (RL) is usually about sequential decision making, solving problems in a wide range of fields in science, engineering and arts (Sutton and Barto, 2017).\nThe integration of reinforcement learning and neural networks dated back to 1990s (Tesauro, 1994; Bertsekas and Tsitsiklis, 1996; Schmidhuber, 2015). With recent exciting achievements of deep learning (LeCun et al., 2015; Goodfellow et al., 2016), benefiting from big data, powerful computation and new algorithmic techniques, we have been witnessing the renaissance of reinforcement learning (Krakovsky, 2016), especially, the combination of reinforcement learning and deep neural networks, i.e., deep reinforcement learning (deep RL).\nWe have been witnessing breakthroughs, like deep Q-network (Mnih et al., 2015), AlphaGo (Silver et al., 2016) and differentiable neural computer (Graves et al., 2016); and novel architectures and applications, like asynchronous methods (Mnih et al., 2016), dueling network architectures (Wang et al., 2016a), value iteration networks (Tamar et al., 2016), dual learning for machine translation (He et al., 2016a), spoken dialogue systems (Su et al., 2016b), information extraction (Narasimhan et al., 2016), guided policy search (Levine et al., 2016a), generative adversarial imitation learning (Ho and Ermon, 2016), unsupervised reinforcement and auxiliary learning (Jaderberg et al., 2017), and neural architecture design (Zoph and Le, 2017), etc. In this overview, we mainly focus on work in recent couple of years, and by no means complete.1\nWe refer readers to materials for further study: reinforcement learning (Sutton and Barto, 2017; Szepesva\u0301ri, 2010; Bertsekas, 2012; Powell, 2011; Bertsekas and Tsitsiklis, 1996; Puterman, 2005; Littman, 2015; Kaelbling et al., 1996); deep learning (LeCun et al., 2015; Goodfellow et al., 2016; Bengio, 2009; Deng and Dong, 2014); machine learning (Jordan and Mitchell, 2015; Hastie et al., 2009; Bishop, 2011; Murphy, 2012; James et al., 2013); practical machine learning advices (Domingos, 2012; Zinkevich, 2017); artificial intelligence (Russell and Norvig, 2009), deep learning in neural networks (Schmidhuber, 2015); natural language processing (NLP) (Hirschberg and Manning, 2015; Deng and Liu, 2017); robotics (Kober et al., 2013); transfer learning (Taylor and Stone, 2009; Pan and Yang, 2010; Weiss et al., 2016); semi-supervised learning (Zhu and Goldberg, 2009); Bayesian RL (Ghavamzadeh et al., 2015); spoken dialogue systems (Hinton et al., 2012; He and Deng, 2013; Young et al., 2013); AI safety (Amodei et al., 2016; Garc\u0131\u0300a and Ferna\u0300ndez, 2015), Monte Carlo tree search (MCTS) (Browne et al., 2012;\n1We consider this overview as incomplete, for time and timing reasons, in the sense that we may not discuss in depth all relevant work, and we will see fast and enormous growth in this field in the next couple of years. Yet we decide to make this overview public available, hoping it would be helpful for some people in the community and we would appreciate feedbacks for us to make improvements.\nGelly et al., 2012); multi-agent RL (Shoham et al., 2003; Busoniu et al., 2008); game theory (Leyton-Brown and Shoham, 2008), etc. We list RL resources in Section 23. See lists of RL applications at: goo.gl/KoXIQC, and goo.gl/1Q1lzg.\nThe outline of this overview follows: background of deep learning and reinforcement learning, as well as introduction of testbeds in Section 2; Deep Q-Network (DQN) and its extensions in Section 3; asynchronous methods in Section 4; policy optimization in Section 5; reward in Section 6; planning in Section 7; attention and memory, in particular differentiable neural computer (DNC), in Section 8; unsupervised learning in Section 9; learning to learn in Section 10; games, including board games, video games and imperfect information games, in Section 11; AlphaGo in Section 12; robotics in Section 13; spoken dialogue systems (a.k.a. chatbot) in Section 14; machine translation in Section 15; text sequence prediction in Section 16; neural architecture design in Section 17; personalized web services in Section 18; healthcare in Section 19; finance in Section 20; music generation in Section 21; a to-do list of topics/papers not reviewed yet in Section 22; and discussions in Section 24.2\nIn particular, we list a collection of RL resources including books, online courses, tutorials, conferences, journals and workshops, and blogs in Section 23. If picking a single RL resource, it is Professor Sutton\u2019s RL book (Sutton and Barto, 2017), 2nd edition in progress. It covers RL fundamentals and reflects new progress, e.g., in deep Q-network, AlphaGo, policy gradient methods, as well as in psychology and neuroscience. A single pick for deep learning is Goodfellow et al. (2016)."}, {"heading": "2 BACKGROUND", "text": "In this section, we briefly introduce concepts and fundamentals in deep learning (Goodfellow et al., 2016) and reinforcement learning (Sutton and Barto, 2017)."}, {"heading": "2.1 DEEP LEARNING", "text": "Deep learning is in contrast to \u201dshallow\u201d learning. For many machine learning algorithms, e.g., linear regression, logistic regression, support vector machines (SVMs), decision trees, and boosting, we have input layer and output layer, and the inputs may be transformed with manual feature engineering before training. In deep learning, between input and output layers, we have one or more hidden layers. At each layer except input layer, we compute the input to each unit, as the weighted sum of units from the previous layer; then we usually use nonlinear transformation, or activation function, such as logistic, tanh, or more popular recently, rectified linear unit (ReLU), to apply to the input of a unit, to obtain a new representation of the input from previous layer. We have weights on links between units from layer to layer. After computations flow forward from input to output, at output layer and each hidden layer, we can compute error derivatives backward, and backpropagate gradients towards the input layer, so that weights can be updated to optimize some loss function.\nA feedforward deep neural network or multilayer perceptron (MLP) is to map a set of input values to output values with a mathematical function formed by composing many simpler functions at each layer. A convolutional neural network (CNN) is a feedforward deep network, with convolutional layers, pooling layers and fully connected layers. CNNs are designed to process data with multiple arrays, e.g., colour image, language, audio spectrogram, and video, benefit from the properties of such signals: local connections, shared weights, pooling and the use of many layers, and are inspired by simple cells and complex cells in visual neuroscience (LeCun et al., 2015). A recurrent neural network (RNN) is often used to process sequential inputs like speech and language, element\n2We discuss how/why we organize the overview from Section 3 to Section 21 in the current way: starting with RL fundamentals: value function/control, policy, reward, and planning (model in to-do list); next attention and memory, unsupervised learning, and learning to learn, which, together with transfer/semi-supervised/oneshot learning, etc, would be critical mechanisms for RL; then various applications.\nWe basically make a flat organization of topics. Otherwise, there may be multiple ways to categorize the topics reviewed. For example, we can combine spoken dialogue systems, machine translation and text sequence prediction as a single section about language models. Another way is to combine these topics, together with learning to learn, neural architecture design and music generation as a section about sequence modelling. Dueling architecture, Value Iteration Netwroks, and differentiable neural computer (DNC) are novel neural networks architectures for RL.\nby element, with hidden units to store history of past elements. A RNN can be seen as a multilayer network with all layers sharing the same weights, when being unfolded in time of forward computation. It is hard for RNN to store information for very long time and the gradient may vanish. Long short term memory networks (LSTM) and gated recurrent unit (GRU) were proposed to address such issues, with gating mechanisms to manipulate information through recurrent cells. Gradient backpropagation or its variants can be used for training all above deep neural networks.\nDropout is a regularization strategy to train an ensemble of sub-networks by removing non-output units randomly from the original network. Batch normalization performs the normalization for each training mini-batch, to accelerate training by reducing internal covariate shift, i.e., the change of parameters of previous layers will change each layer\u2019s inputs distribution.\nDeep neural networks learn representations automatically from raw inputs to recover the compositional hierarchies in many natural signals, i.e., higher-level features are composed of lower-level ones, e.g., in images, the hierarch of objects, parts, motifs, and local combinations of edges. Distributed representation is a central idea in deep learning, which implies that many features may represent each input, and each feature may represent many inputs. The exponential advantages of deep, distributed representations combat the exponential challenges of the curse of dimensionality. The notion of end-to-end training refers to that a learning model uses raw inputs without manual feature engineering to generate outputs, e.g., AlexNet (Krizhevsky et al., 2012) with raw pixels for image classification, Seq2Seq (Sutskever et al., 2014) with raw sentences for machine translation, and DQN (Mnih et al., 2015) with raw pixels and score to play games."}, {"heading": "2.2 REINFORCEMENT LEARNING", "text": "Reinforcement learning usually solves sequential decision making problems. An RL agent interacts with an environment over time. At each time step t, the agent receives a state st and selects an action at from some action space A, following a policy \u03c0(at|st), which is the agent\u2019s behavior, i.e., a mapping from state st to actions at, receives a scalar reward rt, and transitions to the next state st+1, according to the environment dynamics, or model, for reward function R(s, a) and state transition probability P (st+1|st, at) respectively. In an episodic problem, this process continues until the agent reaches a terminal state and then it restarts. The return Rt = \u2211\u221e k=0 \u03b3 krt+k is the discounted, accumulated reward with the discount factor \u03b3 \u2208 (0, 1]. The agent aims to maximize the expectation of such long term return from each state.\nA value function is a prediction of the expected, accumulative, discounted, future reward, measuring how good is each state, or state-action pair. The action value Q\u03c0(s, a) = E[Rt|st = s, at = a] is the expected return for selecting action a in state s and then following policy \u03c0. An optimal action value function Q\u2217(s, a) is the maximum action value achievable by any policy for state s and action a. We can define state value V \u03c0(s) and optimal state value V \u2217(s) similarly.\nTemporal difference (TD) learning is a central idea in RL. It learns value function V (s) directly from experience with TD error, with bootstrapping, in a model-free, online, and fully incremental way. The update rule is V (st) \u2190 V (st)+\u03b1[rt+\u03b3V (st+1)\u2212V (st)], where \u03b1 is a learning rate, and rt + \u03b3V (st+1)\u2212 V (st) is called TD error. Similarly, Q-learning learns action value function, with the update rule, Q(st, at) \u2190 Q(st, at) + \u03b1[r + \u03b3maxat+1 Q(st+1, at+1)\u2212Q(st, at)]. Q-learning is an off-policy control method. In contrast, SARSA, representing state, action, reward, (next) state, (next) action, is an on-policy control method, with the update rule, Q(st, at) \u2190 Q(st, at) + \u03b1[r + \u03b3Q(st+1, at+1)\u2212Q(st, at)]. SARSA refines the policy greedily with respect to action values. TDlearning, Q-learning and SARSA converge under certain conditions. From optimal action value function, we can derive an optimal policy.\nThe above algorithms are referred to as TD(0) and Q(0), with one-step return. We have multi-step return variants or Monte-Carlo approach in the forward view. The eligibility trace from the backward view provides an online, incremental implementation, resulting in TD(\u03bb) and Q(\u03bb) algorithms, where \u03bb \u2208 [0, 1]. When \u03bb = 1, it is the same as a Monte Carlo approach.\nWe discuss the tabular cases above, where a value function or a policy is stored in a tabular form. Function approximation is a way for generalization when the state and/or action spaces are large or continuous. Linear function approximation used to be a popular choice, esp. before the work of Deep Q-Network (Mnih et al., 2015).\nIn contrast to value-based methods like TD learning and Q-learning, policy-based methods optimize the policy \u03c0(a|s; \u03b8) (with function approximation) directly, and update the parameters \u03b8 by gradient ascent on E[Rt]. REINFORCE is a policy gradient method, updating \u03b8 in the direction of \u2207\u03b8 log \u03c0(at|st; \u03b8)Rt. Usually a baseline bt(st) is subtracted from the return to reduce the variance of gradient estimate, yet keeping its unbiasedness, to yield the gradient direction \u2207\u03b8 log \u03c0(at|st; \u03b8)(Rt \u2212 bt(st)). Using V (st) as the baseline bt(st), we have the advantage function A(at, st) = Q(at, st)\u2212 V (st), since Rt is an estimate of Q(at, st). In actor-critic algorithms, the critic updates action-value function parameters, and the actor updates policy parameters, in the direction suggested by the critic.\nWe obtain deep reinforcement learning (deep RL) methods when we use deep neural networks to approximate any of the following component of reinforcement learning: value function, V (s; \u03b8) or Q(s, a; \u03b8), policy \u03c0(a|s; \u03b8), and model (state transition and reward). Here, the parameters \u03b8 are the weights in deep neural networks. When we use \u201dshallow\u201d models, like linear function, decision trees, tile coding and so on as the function approximator, we obtain \u201dshallow\u201d RL, and the parameters \u03b8 are the weight parameters in these models. Note, a shallow model, e.g., decision trees, may be non-linear. The distinct difference between deep RL and \u201dshallow\u201d RL is what function approximator is used. This is similar to the difference between deep learning and \u201dshallow\u201d learning. We usually utilize stochastic gradient descent to update weight parameters in deep RL. When off-policy, function approximation, in particular, non-linear function approximation, and bootstrapping are combined together, instability and divergence may occur (Tsitsiklis and Van Roy, 1997). However, recent work like Deep Q-Network (Mnih et al., 2015) and AlphaGo (Silver et al., 2016) stabilized the learning and achieved outstanding results.\nWe explain some terms in RL parlance. The prediction problem, or policy evaluation, is to compute the state or action value function for a policy. The control problem is to find the optimal policy. Planning constructs a value function or a policy with a model. On-policy methods evaluate or improve the behavioural policy, e.g., SARSA fits the action-value function to the current policy, i.e., SARSA evaluates the policy based on samples from the same policy, then refines the policy greedily with respect to action values. In off-policy methods, an agent learns an optimal value function/policy, maybe following an unrelated behavioural policy, e.g., Q-learning attempts to find action values for the optimal policy directly, not necessarily fitting to the policy generating the data, i.e., the policy Q-learning obtains is usually different from the policy that generates the samples. The notion of on-policy and off-policy can be understood as same-policy and different-policy.The exploration-exploitation dilemma is about the agent needs to exploit the currently best action to obtain rewards, yet it has to explore the environment to find better actions. In model-free methods, the agent learns with trail-and-error from experience explicitly; the model (state transition function) is not known or learned from experience. RL methods that use models are model-based methods. In online mode, training algorithms are executed on data acquired in sequence. In batch mode, models are trained on the entire data set. With bootstrapping, an estimate of state or action value is updated from subsequent estimates."}, {"heading": "2.3 TESTBEDS", "text": "The Arcade Learning Environment (ALE) (Bellemare et al., 2013) is a framework composed of Atari 2600 games to develop and evaluate AI agents.\nDeepMind released a first-person 3D game platform DeepMind Lab (Beattie et al., 2016). Deepmind and Blizzard will collaborate to release the Starcraft II AI research environment (goo.gl/Ptiwfg).\nOpenAI Gym (https://gym.openai.com) is a toolkit for the development of RL algorithms, consisting of environments, e.g., Atari games and simulated robots, and a site for the comparison and reproduction of results.\nOpenAI Universe (https://universe.openai.com) is used to turn any program into a Gym environment. Universe has already integrated many environments, including Atari games, flash games, browser tasks like Mini World of Bits and real-world browser tasks. Recently, GTA V was added to Universe for self-driving vehicle simulation.\nFAIR TorchCraft (Synnaeve et al., 2016) is a library for Real-Time Strategy (RTS) games such as StarCraft: Brood War.\nViZDoom is a Doom-based AI research platform for visual RL (Kempka et al., 2016).\nTORCS is a car racing simulator (Bernhard Wymann et al., 2014).\nMuJoCo, Multi-Joint dynamics with Contact, is a physics engine. See http://www.mujoco.org.\nDuan et al. (2016) presented a benchmark for continuous control tasks. The open source is available at: https://github.com/openai/rllab.\nNogueira and Cho (2016) presented WebNav Challenge for Wikipedia links navigation."}, {"heading": "3 DEEP Q-NETWORK", "text": "Mnih et al. (2015) introduced Deep Q-Network (DQN) and ignited the field of deep RL. Before DQN, it is well known that RL is unstable or even divergent when action-value Q function is approximated with a nonlinear function like neural networks. DQN made several important contributions: 1) stabilize the training of Q action value function approximation with deep neural networks (CNN) using experience replay (Lin, 1992) and target network; 2) designing an end-to-end RL approach, with only the pixels and the game score as inputs, so that only minimal domain knowledge is required; 3) training a flexible network with the same algorithm, network architecture and hyperparameters to perform well on many different tasks, i.e., 49 Atari games (Bellemare et al., 2013), and outperforming previous algorithms and performing comparably to a human professional tester.\nSee Chapter 16 in Sutton and Barto (2017) for a Sutton-style description of Deep Q-Network. See Deepmind\u2019s description of DQN at goo.gl/IWco9h. We present DQN pseudo code below.\nInput: the pixels and the game score Output: Q action value function (from which we obtain policy and select action) Initialize replay memory D Initialize action-value function Q with random weight \u03b8 Initialize target action-value function Q\u0302 with weights \u03b8\u2212 = \u03b8 for episode = 1 to M do\nInitialize sequence s1 = {x1} and preprocessed sequence \u03c61 = \u03c6(s1) for t = 1 to T do\nFollowing \u01eb-greedy policy, select at =\n{\na random action with probability \u01eb argmaxa Q(\u03c6(st), a; \u03b8) otherwise\nExecute action ai in emulator and observe reward rt and image xt+1 Set st+1 = st, at, xt+1 and preprocess \u03c6t+1 = \u03c6(st+1) Store transition (\u03c6t, at, rt, \u03c6t+1) in D // experience replay Sample random minibatch of transitions (\u03c6j , aj , rj , \u03c6j+1) from D\nSet yj =\n{\nrj if episode terminates at step j + 1 rj + \u03b3maxa\u2032 Q\u0302(\u03c6j+1, a\n\u2032; \u03b8\u2212) otherwise Perform a gradient descent step on (yj \u2212Q(\u03c6j , aj ; \u03b8))2 w.r.t. the network parameter \u03b8 // periodic update of target network\nEvery C steps reset Q\u0302 = Q, i.e., set \u03b8\u2212 = \u03b8 end\nend Algorithm 1: Deep Q-Nework (DQN), adapted from Mnih et al. (2015)"}, {"heading": "3.1 DOUBLE DQN", "text": "van Hasselt et al. (2016a) proposed Double DQN (D-DQN) to tackle the overestimate problem in Q-learning. In standard Q-learning, as well as in DQN, the parameters are updated as follows:\n\u03b8t+1 = \u03b8t + \u03b1(y Q t \u2212Q(st, at; \u03b8t))\u2207\u03b8tQ(st, at; \u03b8t),\nwhere y Q t = rt+1 + \u03b3max\na Q(st+1, a; \u03b8t),\nso that the max operator uses the same values to both select and evaluate an action. As a consequence, it is more likely to select overestimated values, and results in overoptimistic value estimates. van Hasselt et al. (2016a) proposed to evaluate the greedy policy according to the online network, but to use the target network to estimate its value. This can be achieved with a minor change to the DQN algorithm, replacing yQt with\ny D\u2212DQN t = rt+1 + \u03b3Q(st+1,max\na Q(st+1, at; \u03b8t); \u03b8\n\u2212 t ),\nwhere \u03b8t is the parameter for online network and \u03b8 \u2212 t is the parameter for target network. For reference, yQt can be written as\ny Q t = rt+1 + \u03b3Q(st+1,max\na Q(st+1, at; \u03b8t); \u03b8t).\nD-DQN found better policies than DQN on Atari games."}, {"heading": "3.2 PRIORITIZED EXPERIENCE REPLAY", "text": "In DQN, experience transitions are uniformly sampled from the replay memory, regardless of the significance of experiences. Schaul et al. (2016) proposed to prioritize experience replay, so that important experience transitions can be replayed more frequently, to learn more efficiently. The importance of experience transitions are measured by TD errors. The authors designed a stochastic prioritization based on the TD errors, using importance sampling to avoid the bias in the update distribution. The authors used prioritized experience replay in DQN and D-DQN, and improved their performance on Atari games."}, {"heading": "3.3 DUELING ARCHITECTURE", "text": "Wang et al. (2016b) proposed the dueling network architecture to estimate state value function V (s) and associated advantage function A(s, a), and then combine them to estimate action value function Q(s, a), to converge faster than Q-learning. In DQN, a CNN layer is followed by a fully connected (FC) layer. In dueling architecture, a CNN layer is followed by two streams of FC layers, to estimate value function and advantage function separately; then the two streams are combined to estimate action function. Usually we use the following to combine V (s) and A(s, a) to obtain Q(s, a),\nQ(s, a; \u03b8, \u03b1, \u03b2) = V (s; \u03b8, \u03b2) + (\nA(s, a; \u03b8, \u03b1) \u2212max a\u2032\nA(s, a\u2032; \u03b8, \u03b1) )\nwhere \u03b1 and \u03b2 are parameters of the two streams of FC layers. Wang et al. (2016b) proposed to replace max operator with average as following for better stability,\nQ(s, a; \u03b8, \u03b1, \u03b2) = V (s; \u03b8, \u03b2) + ( A(s, a; \u03b8, \u03b1) \u2212 a\n|A| A(s, a\u2032; \u03b8, \u03b1)\n)\nDueling architecture implemented with D-DQN and prioritized experience replay improved previous work, DQN and D-DQN with prioritized experience replay, on Atari games."}, {"heading": "3.4 MORE EXTENSIONS", "text": "Mnih et al. (2016) proposed asynchronous methods for RL methods, in particular, the asynchronous advantage actor-critic (A3C) algorithm, as discussed in Section 4. Osband et al. (2016) designed better exploration strategy to improve DQN. O\u2019Donoghue et al. (2017) proposed policy gradient and Q-learning (PGQ), as discussed in Section 5.6. He et al. (2017) proposed to accelerate DQN by optimality tightening, a constrained optimization approach, to propagate reward faster, and to improve accuracy over DQN. Babaeizadeh et al. (2017) proposed a hybrid CPU/GPU implementation of A3C. Liang et al. (2016) attempted to understand the success of DQN and reproduced results with shallow RL."}, {"heading": "4 ASYNCHRONOUS METHODS", "text": "Mnih et al. (2016) proposed asynchronous methods for four RL methods, Q-learning, SARSA, nstep Q-learning and advantage actor-critic, and the asynchronous advantage actor-critic (A3C) algorithm performs the best. Parallel actors employ different exploration policies to stabilize training, so that experience replay is not utilized. Different from most deep learning algorithms, asynchronous methods can run on a single multi-core CPU. For Atari games, A3C ran much faster yet performed better than or comparably with DQN, Gorila, D-DQN, Dueling D-DQN, and Prioritized D-DQN. A3C also succeeded on continuous motor control problems: TORCS car racing games and MujoCo physics manipulation and locomotion, and Labyrinth, a navigating task in random 3D mazes using visual inputs, in which an agent will face a new maze in each new episode, so that it needs to learn a general strategy to explore random mazes.\nWe present pseudo code for asynchronous advantage actor-critic for each actor-learner thread. A3C maintains a policy \u03c0(at|st; \u03b8) and an estimate of the value function V (st; \u03b8v), being updated with n-step returns in the forward view, after every tmax actions or reaching a terminal state, similar to using minibatches. The gradient update can be seen as \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)A(st, at; \u03b8, \u03b8v), where A(st, at; \u03b8, \u03b8v) = \u2211k\u22121 i=0 \u03b3 irt+i + \u03b3 kV (st+k; \u03b8v)\u2212 V (st; \u03b8v) is an estimate of the advantage function, with k upbounded by tmax.\nGlobal shared parameter vectors \u03b8 and \u03b8v, thread-specific parameter vectors \u03b8\u2032 and \u03b8\u2032v Global shared counter T = 0, Tmax Initialize step counter t \u2190 1 for T \u2264 Tmax do\nReset gradients, d\u03b8 \u2190 0 and d\u03b8v \u2190 0 Synchronize thread-specific parameters \u03b8\u2032 = \u03b8 and \u03b8\u2032v = \u03b8v Set tstart = t, get state st for st not terminal and t\u2212 tstart \u2264 tmax do\nTake at according to policy \u03c0(at|st; \u03b8\u2032) Receive reward rt and new state st+1 t \u2190 t+ 1, T \u2190 T + 1\nend\nR =\n{\n0 for terminal st V (st, \u03b8 \u2032 v) otherwise\nfor i \u2208 {t\u2212 1, ..., tstart} do R \u2190 ri + \u03b3R accumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190 d\u03b8 +\u2207\u03b8\u2032 log \u03c0(ai|si; \u03b8\u2032)(R\u2212 V (si; \u03b8\u2032v)) accumulate gradients wrt \u03b8\u2032v: d\u03b8v \u2190 d\u03b8v +\u2207\u03b8\u2032v(R \u2212 V (si; \u03b8 \u2032 v)) 2 end Update asynchronously \u03b8 using d\u03b8, and \u03b8v using d\u03b8v\nend Algorithm 2: A3C, each actor-learner thread, based on Mnih et al. (2016)"}, {"heading": "5 POLICY OPTIMIZATION", "text": "Policies are usually stochastic. However, Silver et al. (2014) introduced the deterministic policy gradient (DPG) for efficient estimation of policy gradient. Lillicrap et al. (2016) extended DPG with deep neural networks. We also introduce several recent work, including Guided Policy Search (Levine et al., 2016a), Trust Region Policy Optimization (Schulman et al., 2015), benchmark results (Duan et al., 2016) and policy gradient and Q-learning (O\u2019Donoghue et al., 2017)."}, {"heading": "5.1 DETERMINISTIC POLICY GRADIENT", "text": "Silver et al. (2014) introduced the deterministic policy gradient (DPG) algorithm for RL problems with continuous action spaces. The deterministic policy gradient is the expected gradient of the action-value function, which integrates over the state space; whereas in the stochastic case, the policy gradient integrates over both state and action spaces. Consequently, the deterministic policy gradient can be estimated more efficiently than the stochastic policy gradient. The authors intro-\nduced an off-policy actor-critic algorithm to learn a deterministic target policy from an exploratory behaviour policy, and to ensure unbiased policy gradient with the compatible function approximation for deterministic policy gradients. Empirical results showed its superior to stochastic policy gradients, in particular in high dimensional tasks, on several problems: a high-dimensional bandit; standard benchmark RL tasks of mountain car and pendulum and 2D puddle world with low dimensional action spaces; and controlling an octopus arm with a high-dimensional action space. The experiments were conducted with tile-coding and linear function approximators."}, {"heading": "5.2 DEEP DETERMINISTIC POLICY GRADIENT", "text": "Lillicrap et al. (2016) proposed an actor-critic, model-free, deep deterministic policy gradient (DDPG) algorithm in continuous action spaces, by extending DQN (Mnih et al., 2015) and DPG (Silver et al., 2014). With actor-critic as in DPG, DDPG avoids the optimization of action at every time step to obtain a greedy policy as in Q-learning, which will make it infeasible in complex action spaces with large, unconstrained function approximators like deep neural networks. To make the learning stable and robust, similar to DQN, DDPQ deploy experience replay and an idea similar to target network, \u201dsoft\u201d target, which, rather than copying the weights directly as in DQN, updates the soft target network weights \u03b8\u2032 slowly to track the learned networks weights \u03b8: \u03b8\u2032 \u2190 \u03c4\u03b8 + (1 \u2212 \u03c4)\u03b8\u2032, with \u03c4 \u226a 1. The authors adapted batch normalization to handle the issue that the different components of the observation with different physical units. As an off-policy algorithm, DDPG learns an actor policy from experiences from an exploration policy by adding noise sampled from a noise process to the actor policy. More than 20 simulated physics tasks of varying difficulty in the MuJoCo environment were solved with the same learning algorithm, network architecture and hyper-parameters, and obtained policies with performance competitive with those found by a planning algorithm with full access to the underlying physical model and its derivatives. DDPG can solve problems with 20 times fewer steps of experience than DQN, although it still needs a large number of training episodes to find solutions, as in most model-free RL methods. It is end-to-end, with raw pixels as input. DDPQ paper also contains links to videos for illustration."}, {"heading": "5.3 GUIDED POLICY SEARCH", "text": "Levine et al. (2016a) proposed to train the perception and control systems jointly end-to-end, to map raw image observations directly to torques at the robot\u2019s motors. The authors introduced guided policy search (GPS) to train policies represented as CNN, by transforming policy search into supervised learning to achieve data efficiency, with training data provided by a trajectory-centric RL method operating under unknown dynamics. GPS alternates between trajectory-centric RL and supervised learning, to obtain the training data coming from the policy\u2019s own state distribution, to address the issue that supervised learning usually does not achieve good, long-horizon performance. GPS utilizes pre-training to reduce the amount of experience data to train visuomotor policies. Good performance was achieved on a range of real-world manipulation tasks requiring localization, visual tracking, and handling complex contact dynamics, and simulated comparisons with previous policy search methods. As the authors mentioned, \u201dthis is the first method that can train deep visuomotor policies for complex, high-dimensional manipulation skills with direct torque control\u201d."}, {"heading": "5.4 TRUST REGION POLICY OPTIMIZATION", "text": "Schulman et al. (2015) introduced an iterative procedure to monotonically improve policies, and proposed a practical algorithm, Trust Region Policy Optimization (TRPO), by making several approximations. The authors also unified policy iteration and policy gradient with analysis. In the experiments, TRPO methods performed well on simulated robotic tasks of swimming, hopping, and walking, as well as playing Atari games in an end-to-end manner directly from raw images."}, {"heading": "5.5 BENCHMARK RESULTS", "text": "Duan et al. (2016) presented a benchmark for continuous control tasks, including classic tasks like cart-pole, tasks with very large state and action spaces such as 3D humanoid locomotion and tasks with partial observations, and tasks with hierarchical structure, implemented various algorithms, including batch algorithms: REINFORCE, Truncated Natural Policy Gradient (TNPG), Reward-\nWeighted Regression (RWR), Relative Entropy Policy Search (REPS), Trust Region Policy Optimization (TRPO), Cross Entropy Method (CEM), Covariance Matrix Adaption Evolution Strategy (CMA-ES); online algorithms: Deep Deterministic Policy Gradient (DDPG); and recurrent variants of batch algorithms. The open source is available at: https://github.com/ rllab/rllab.\nDuan et al. (2016) compared various algorithms, and showed that DDPG, TRPO, and Truncated Natural Policy Gradient (TNPG) (Schulman et al., 2015) are effective in training deep neural network policies, yet better algorithms are called for hierarchical tasks."}, {"heading": "5.6 COMBINING POLICY GRADIENT AND Q-LEARNING", "text": "O\u2019Donoghue et al. (2017) proposed to combine policy gradient with off-policy Q-learning (PGQ), to benefit from experience replay. Usually actor-critic methods are on-policy. The authors also showed that action value fitting techniques and actor-critic methods are equivalent, and interpreted regularized policy gradient techniques as advantage function learning algorithms. Empirically, the authors showed that PGQ outperformed DQN and A3C on Atari games."}, {"heading": "6 REWARD", "text": "Inverse reinforcement learning (IRL) is the problem of determining a reward function given observations of optimal behaviour (Ng and Russell, 2000). In imitation learning, or apprenticeship learning, an agent learns to perform a task from expert demonstrations, with samples of trajectories from the expert, without reinforcement signal, without additional data from the expert while training; two main approaches for imitation learning are behavioral cloning and inverse reinforcement learning; behavioral cloning is formulated as a supervised learning problem to map state-action pairs from expert trajectories to policy (Ho and Ermon, 2016)."}, {"heading": "6.1 GENERATIVE ADVERSARIAL NETWORKS", "text": "Goodfellow et al. (2014) proposed generative adversarial nets (GANs) to estimate generative models via an adversarial process by training two models simultaneously, a generative model G to capture the data distribution, and a discriminative model D to estimate the probability that a sample comes from the training data but not the generative model G.\nGoodfellow et al. (2014) modelled G and D with multilayer perceptrons: G(z : \u03b8g) and D(x : \u03b8d), where \u03b8g and \u03b8d are parameters, x are data points, and z are input noise variables. Define a prior on input noise variable pz(z). G is a differentiable function and D(x) outputs a scalar as the probability that x comes from the training data rather than pg, the generative distribution we want to learn.\nD will be trained to maximize the probability of assigning labels correctly to samples from both training data and G. Simultaneously, G will be trained to minimize such classification accuracy, log(1\u2212D(G(z))). As a result, D and G form the two-player minimax game as follows:\nmin G max D Ex\u223cpdata(x)[logD(x)] + Ez\u223cpz(z)[log(1\u2212D(G(z)))]\nGoodfellow et al. (2014) showed that as G and D are given enough capacity, generative adversarial nets can recover the data generating distribution, and provided a training algorithm with backpropagation by minibatch stochastic gradient descent.\nGenerative adversarial networks have received much attention. See Goodfellow (2017) for Ian Goodfellow\u2019s summary of his NIPS 2016 Tutorial."}, {"heading": "6.2 GENERATIVE ADVERSARIAL IMITATION LEARNING", "text": "With IRL, an agent learns a reward function first, then from which derives an optimal policy. Many IRL algorithms have high time complexity, with an RL problem in the inner loop.\nHo and Ermon (2016) proposed generative adversarial imitation learning algorithm to learn policies directly from data, bypassing the intermediate IRL step. Generative adversarial training was de-\nployed to fit the discriminator, the distribution of states and actions that defines expert behavior, and the generator, the policy.\nGenerative adversarial imitation learning finds a policy \u03c0\u03b8 so that a discriminator DR can not distinguish states following the expert policy \u03c0E and states following the imitator policy \u03c0\u03b8 , hence forcing DR to take 0.5 in all cases and \u03c0\u03b8 not distinguishable from \u03c0E in the equillibrium. Such a game is formulated as:\nmax \u03c0\u03b8 min DR \u2212E\u03c0\u03b8 [logDR(s)]\u2212 E\u03c0E [log(1\u2212DR(s))]\nThe authors represented both \u03c0\u03b8 and DR as deep neural networks, and found an optimal solution by repeatedly performing gradient updates on each of them. DR can be trained with supervised learning with a data set formed from traces from a current \u03c0\u03b8 and expert traces. For a fixed DR, an optimal \u03c0\u03b8 is sought. Hence it is a policy optimization problem, with \u2212 logDR(s) as the reward. The authors trained \u03c0\u03b8 by trust region policy optimization (Schulman et al., 2015).\nFinn et al. (2016) established a connection between GANs, IRL, and energy-based models. Pfau and Vinyals (2016) established the connection between GANs and actor-critic algorithms."}, {"heading": "7 PLANNING", "text": "Planning constructs a value function or a policy usually with a model. Tamar et al. (2016) introduced Value Iteration Networks (VIN), a fully differentiable CNN planning module to approximate the value iteration algorithm, to learn to plan, e.g, policies in RL. In contrast to conventional planning, VIN is model-free, where reward and transition probability are part of the neural network to be learned, so that it avoids issues with system identification. VIN can be trained end-to-end with backpropagation. VIN can generalize in a diverse set of tasks: simple gridworlds, Mars Rover Navigation, continuous control and WebNav Challenge for Wikipedia links navigation (Nogueira and Cho, 2016). One merit of Value Iteration Network, as well as Dueling Network(Wang et al., 2016b), is that they design novel deep neural networks architectures for reinforcement learning problems. See a blog about VIN at goo.gl/Dr8gKL."}, {"heading": "8 ATTENTION AND MEMORY", "text": "Attention and memory are two important mechanisms, which work together in many cases.\nMnih et al. (2014) introduced the recurrent attention model (RAM) to focus on selected sequence of regions or locations from an image or video for image classification and object detection. The authors used RL methods, in particular, REINFORCE algorithm, to train the model, to overcome the issue that the model is non-differentiable, and experimented on an image classification task and a dynamic visual control problem. Xu et al. (2015) integrated attention to image captioning, trained the hard version attention with the REINFORCE algorithm, and showed the effectiveness of attention on Flickr8k, Flickr30k, and MS COCO datasets. The attention mechanism is also deployed in NLP, e.g., in Bahdanau et al. (2015; 2017), and with external memory, in differentiable neural computer (Graves et al., 2016).\nGraves et al. (2016) proposed differentiable neural computer (DNC), in which, a neural network can read from and write to an external memory, so that DNC can solve complex, structured problems, which a neural network without read-write memory can not solve. DNC minimizes memory allocation interference and enables long-term storage. Similar to a conventional computer, in a DNC, the neural network is the controller and the external memory is the random-access memory; and a DNC represents and manipulates complex data structures with the memory. Differently, a DNC learns such representation and manipulation end-to-end with gradient descent from data in a goal-directed manner. When trained with supervised learning, a DNC can solve synthetic question answering problems, for reasoning and inference in natural language; it can solve the shortest path finding problem between two stops in transportation networks and the relationship inference problem in a family tree. When trained with reinforcement learning, a DNC can solve a moving blocks puzzle with changing goals specified by symbol sequences. DNC outperformed normal neural network like\nLSTM or DNC\u2019s precursor Neural Turing Machine (Graves et al., 2014); with harder problems, an LSTM may simply fail. Although these experiments are relatively small-scale, we expect to see further improvements and applications of DNC.\nSee Deepmind\u2019s description of DNC at goo.gl/58mgoX. See more work on attention and/or memory, e.g., Ba et al. (2014; 2016); Chen et al. (2016a); Danihelka et al. (2016); Eslami et al. (2016); Gregor et al. (2015); Jaderberg et al. (2015); Kaiser and Bengio (2016); Kadlec et al. (2016); Oquab et al. (2015); Weston et al. (2015); Sukhbaatar et al. (2015); Yang et al. (2015); Zagoruyko and Komodakis (2017); Zaremba and Sutskever (2015). See goo.gl/ArW2nE and goo.gl/UukROv for blogs about attention and memory."}, {"heading": "9 UNSUPERVISED LEARNING", "text": "Jaderberg et al. (2017) proposed UNsupervised REinforcement and Auxiliary Learning (UNREAL) to improve learning efficiency by maximizing pseudo-reward functions, besides the usual cumulative reward, while sharing a common representation. UNREAL benefits from learning from the abundant possible training signals, especially when the extrinsic reward signals are rarely observed. UNREAL is composed of RNN-LSTM base agent, pixel control, reward prediction, and value function replay. The base agent is trained on-policy with A3C. Experiences of observations, rewards and actions are stored in a reply buffer, for being used by auxiliary tasks. The auxiliary policies use the base CNN and LSTM, together with a deconvolutional network, to maximize changes in pixel intensity of different regions of the input images. The reward prediction module predicts short-term extrinsic reward in next frame by observing the last three frames, to tackle the issue of reward sparsity. Value function replay further trains the value function. UNREAL improved A3C\u2019s performance on Atari games, and performed well on 3D Labyrinth game. See Deepmind\u2019s description of UNREAL at goo.gl/zhqBGy.\nWe discuss robotics navigation with similar unsupervised auxiliary learning in Section 13, and generative adversarial networks (GANs), a recent unsupervised learning framework, in Section 6. See Sutton et al. (2011) for Horde, a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction."}, {"heading": "10 LEARNING TO LEARN", "text": "Learning to learn is related to transfer learning, multi-task learning or representation learning, and is a core ingredient to achieve strong AI (Lake et al., 2016). Learning to learn is also related to meta learning or one-shot learning.\nDuan et al. (2017) and Wang et al. (2016a) proposed to learn a flexible RNN model to handle a family of RL tasks, to improve sample efficiency, learn new tasks in a few samples, and benefit from prior knowledge. The agent is modelled with RNN, with inputs of observations, rewards, actions and termination flags; the weights of RNN are trained with RL,TRPO in Duan et al. (2017) and A3C in Wang et al. (2016a), and achieve similar performance for various problems to specific RL algorithms. Duan et al. (2017) experimented with multi-arm bandits, tabular MDPs and visual navigation, and discussed that for larger problems, better RL algorithms are needed to train RNN. Wang et al. (2016a) experimented with bandits with independent arms, bandits with dependant arms, restless arms and MDPs. A future work is to improve scalability.\nLi and Malik (2017) proposed to automate unconstrained continuous optimization algorithms with guided policy search (Levine et al., 2016a) by representing a particular optimization algorithm as a policy, and convergence rate as reward."}, {"heading": "11 GAMES", "text": "Games provide excellent testbeds for RL/AI algorithms. We discuss Deep Q-Network (DQN) in Section 3 and its extensions, all of which experimented with Atari games. We discuss Mnih et al. (2016) in Section 4, Jaderberg et al. (2017) in Section 9, and Mirowski et al. (2017) in Section 13, and they used Labyrinth as the testbed.\nBackgammon and Go are perfect information games. We discuss briefly Backgammon in Section 11.1 about board games. We talk about video games like Doom in Section 11.2. We put poker, a board game, under Section 11.3 about imperfect information games, where game theory is concerned. Video games like Labyrinth and Doom are usually imperfect information games, whereas game theory is not (yet) used in these work to approach the problem. We single out AlphaGo (Silver et al., 2016) in Section 12, for its significance."}, {"heading": "11.1 BOARD GAMES", "text": "Board games, e.g., backgammon, Go, chess, checker and othello, are classical testbeds for RL/AI algorithms. Tesauro (1994) approached backgammon by using neural networks to approximate value function learned with TD learning, and achieved human level performance."}, {"heading": "11.2 VIDEO GAMES", "text": "Wu and Tian (2017) deployed A3C with CNN to train an agent in a partially observable 3D environment, Doom, from recent four raw frames and game variables, to predict next action and value function, following the curriculum learning (Bengio et al., 2009) approach of starting with simple tasks and gradually transition to harder ones. It is nontrivial to apply A3C to such 3D games directly, partly due to sparse and long term reward. The authors won the champion in Track 1 of ViZDoom Competition by a large margin, and plan the following future work: a map from an unknown environment, localization, a global plan to act, and visualization of the reasoning process.\nDosovitskiy and Koltun (2017) approached the problem of sensorimotor control in immersive environments with supervised learning, and won the Full Deathmatch track of the Visual Doom AI Competition. We list it here since it is usually an RL problem, yet it was solved with supervised learning. Lample and Chaplot (2016) also discussed how to tackle Doom.\nUsunier et al. (2016) studied StarCraft and Tessler et al. (2017) studied Minecraft."}, {"heading": "11.3 IMPERFECT INFORMATION GAMES", "text": "Heinrich and Silver (2016) proposed Neural Fictitious Self-Play (NFSP) to combine fictitious selfplay with deep RL to learn approximate Nash equilibria for games of imperfect information in a scalable end-to-end approach without prior domain knowledge. NFSP was evaluated on twoplayer zero-sum games. In Leduc poker, NFSP approached a Nash equilibrium, while common RL methods diverged. In Limit Texas Hold\u2019em, a real-world scale imperfect-information game, NFSP performed similarly from scratch to state-of-the-art, superhuman algorithms which are based on significant domain expertise.\nHeads-up Limit Hold\u2019em Poker was essentially solved (Bowling et al., 2015) with counterfactual regret minimization (CFR), which is an iterative method to approximate a Nash equilibrium of an extensive-form game with repeated self-play between two regret-minimizing algorithms.\nRecently, significant progress has been made for Heads-up No-Limit Hold\u2019em Poker (Moravc\u030c\u0131\u0301k et al., 2017), the DeepStack computer program defeated professional poker players for the first time. DeepStack utilized the recursive reasoning of CFR to handle information asymmetry, focusing computation on specific situations arising when making decisions and use of value functions trained automatically, with little domain knowledge or human expert games, without abstraction and offline computation of complete strategies as before (Sandholm, 2015).\nImperfect information games, or game theory in general, have many applications, e.g., security and medical decision support (Sandholm, 2015). It is interesting to see more progress of deep RL in such applications, and the full version of Texas Hold\u2019em."}, {"heading": "12 ALPHAGO", "text": "AlphaGo (Silver et al., 2016), a computer Go program, won the human European Go champion, 5 games to 0, in October 2015, and became the first computer Go program to won a human professional Go player without handicaps on a full-sized 19 \u00d7 19 board. Soon after that in March 2016,\nAlphaGo defeated Lee Sedol, an 18-time world champion Go player, 4 games to 1, making headline news worldwide. This set a landmark in AI. The challenge of solving Go comes from not only the gigantic search space of about 250150, an astronomical number, but also the hardness of position evaluation, which was successfully used in solving many other games, like backgammon and chess."}, {"heading": "12.1 TRAINING PIPELINE AND MCTS", "text": "We discuss briefly how AlphaGo works based on Silver et al. (2016) and Sutton and Barto (2017). See Chapter 16 in Sutton and Barto (2017) for a detailed and intuitive description of AlphaGo. See Deepmind\u2019s description of AlphaGo at goo.gl/lZoQ1d.\nAlphaGo was built with techniques of deep CNN, supervised learning, reinforcement learning, and Monte Carlo tree search (MCTS) (Browne et al., 2012; Gelly et al., 2012). AlphaGo is composed of two phases: neural network training pipeline and MCTS. The training pipeline phase includes training a supervised learning (SL) policy network from expert moves, a fast rollout policy, an RL policy network, and an RL value network.\nThe SL policy network has convolutional layers, ReLU nonlinearities, and an output softmax layer representing probability distribution over legal moves. The inputs to the CNN are 19 \u00d7 19 \u00d7 48 image stacks, where 19 is the dimension of a Go board and 48 is the number of features. Stateaction pairs are sampled from expert moves to train the network with stochastic gradient ascent to maximize the likelihood of the move selected in a given state. The fast rollout policy uses a linear softmax with small pattern features.\nThe RL policy network improves SL policy network, with the same network architecture, and the weights of SL policy network as initial weight, and policy gradient for training. The reward function is +1 for winning and -1 for losing in the terminal states, and 0 otherwise. Games are played between the current policy network and a random, previous iteration of the policy network, to stabilize the learning and to avoid overfitting. Weights are updated by stochastic gradient ascent to maximize the expected outcome.\nThe RL value network still has the same network architecture as SL policy network, except the output is a single scalar predicting the value of a position. The value network is learned in a Monte Carlo policy evaluation approach. To tackle the overfitting problem caused by strongly correlated successive positions in games, data are generated by self-play between the RL policy network and itself until game termination. The weights are trained by regression on state-outcome pairs, using stochastic gradient descent to minimize the mean squared error between the prediction and the corresponding outcome.\nIn MCTS phase, AlphaGo selects moves by lookahead search. It builds a partial game tree starting from the current state, in the following stages: 1) select a promising node to explore further, 2) expand a leaf node guided by the SL policy network and collected statistics, 3) evaluate a leaf node with a mixture of the RL value network and the rollout policy, 4) backup evaluations to update the action values. A move is then selected."}, {"heading": "12.2 DISCUSSIONS", "text": "The Deepmind team integrated several existing techniques together to engineered AlphaGo and it has achieved tremendous results. However, the RL policy network and RL value network are not strong/accurate enough, so that the RL value network, together with the SL policy network and the rollout network, assist MCTS to search for the move. This might explain the one game loss against Lee Sedol. Moverover, AlphaGo still requires manually defined features with human knowledge, so it is not entirely an end-to-end solution yet; in contrast, DQN requires only raw pixels and scores as inputs. Such a room for improvements would inspire intellectual inquisition for better computer Go programs, potentially with deep RL only, without MCTS, like TD-Gammon (Sutton and Barto, 2017). This would be based on a novel RL algorithm, a novel deep neural network architecture, and powerful computation. New RL algorithms are called for, for data efficiency, and possibly for better knowledge representation and reasoning. New deep neural network architectures are called for, for the sophistication to represent complex scenarios in Go and the elegance for learning in a reasonable time, so that an optimal policy and/or an optimal value function can be directly approximated to\nmake decisions without the help of MCTS to choose moves. Admittedly, such endeavour would be illusive at large currently.\nBeing more practical, we expect more applications/extensions of techniques in Silver et al. (2016) in solving problems requiring titanic search spaces, like classical AI problems, e.g., planning, scheduling, and constraint satisfaction, etc."}, {"heading": "13 ROBOTICS", "text": "As we discuss in Section 5, Schulman et al. (2015) proposed Trust Region Policy Optimization (TRPO), and experimented with simulated robotic tasks, and Levine et al. (2016a) proposed Guided Policy Search (GPS) to handle physical robots.\nMirowski et al. (2017) obtained the navigation ability by solving an RL problem maximizing cumulative reward and jointly considering un/self-supervised tasks to improve data efficiency and task performance. The authors addressed the sparse reward issues by augmenting the loss with two auxiliary tasks, 1) unsupervised reconstruction of a low-dimensional depth map for representation learning to aid obstacle avoidance and short-term trajectory planning; 2) self-supervised loop closure classification task within a local trajectory. The authors incorporated a stacked LSTM to use memory at different time scales for dynamic elements in the environments. The proposed agent learn to navigate in complex 3D mazes end-to-end from raw sensory input, and performed similarly to human level, even when start/goal locations change frequently.\nIn this approach, navigation is a by-product of the goal-directed RL optimization problem, in contrast to conventional approaches such as Simultaneous Localisation and Mapping (SLAM), where explicit position inference and mapping are used for navigation. This may have the chance to replace the popular SLAM, which usually requires manual processing.\nSee more recent robotics papers, e.g., Chebotar et al. (2016); Finn and Levine (2016); Gu et al. (2016a); Levine et al. (2016b); Yahya et al. (2016); Zhu et al. (2016). See Kober et al. (2013) for a survey of RL in robotics. See Science Robotics."}, {"heading": "14 SPOKEN DIALOGUE SYSTEMS", "text": "In spoken dialogue systems, conversational agent, or simply, chatbot, human and computer interacts with natural speech. There are usually two categories: chat-oriented and task-oriented systems; the former aims to converse with users in contextually reasonable way; the latter aims to assist users for specific goals (Su et al., 2016b).\nLi et al. (2016c) proposed to use deep RL to generate dialogues to model future reward for better informativity, coherence, and ease of answering, to attempt to address the issues in the sequence to sequence models based on Sutskever et al. (2014): the myopia and misalignment of maximizing the probability of generating a response given the previous dialogue turn, and the infinite loop of repetitive responses. The authors designed a reward function to reflect the above desirable properties, and deployed policy gradient to optimize the long term reward. It would be interesting to investigate the reward model with the approach in Su et al. (2016b) below or with inverse RL and imitation learning as discussed in Section 6, although Su et al. (2016b) mentioned that such methods are costly, and humans may not act optimally.\nSu et al. (2016b) proposed an on-line learning framework to train the dialogue policy jointly with the reward model via active learning with a Gaussian process model, to tackle the issue that it is unreliable and costly to use explicit user feedback as the reward signal. In the proposed framework, when each dialogue ends, a set of turn-level features is extracted and fed into an embedding function to obtain a fixed-dimension dialogue representation that serves as the input space of the reward model. This reward is modelled as a Gaussian process which for every input point provides an estimate of task success along with a measure of the estimate uncertainty. Based on this uncertainty, the reward model decides whether to query the user for feedback or not. It then returns a reinforcement signal to update the dialogue policy, which is trained using the GP-SARSA algorithm. GP-SARSA also deploys Gaussian process estimation to provide an on-line sample-efficient reinforcement learning algorithm capable of bootstrapping estimates of sparse value functions from minimal numbers of\nsamples (dialogues). The quality of each dialogue is defined by its cumulative reward, where each dialogue turn incurs a small negative reward (-1) and the final reward of either 0 or 20 depending on the estimate of task success are provided by the reward model. The authors showed empirically that the proposed framework reduced manual data annotations significantly and mitigated noisy user feedback in dialogue policy learning.\nLi et al. (2016d) designed a user simulator for movie booking, with both rules and collected data, for movie ticket booking and movie seeking. The source code is available at: goo.gl/jOv4AR.\nSome recent papers follow: Asri et al. (2016), Bordes and Weston (2016), Chen et al. (2016b), Dhingra et al. (2016), Fatemi et al. (2016), Li et al. (2016a), Lipton et al. (2016), Mesnil et al. (2015), Mo et al. (2016), Shah et al. (2016), Su et al. (2016a), Wen et al. (2015a), Williams and Zweig (2016), Yang et al. (2016), Zhao and Eskenazi (2016).\nSee Li Deng\u2019s recent talk at goo.gl/BqzeIZ. See conferences like SIGDIAL and INTERSPEECH. See NIPS 2016 Workshop on End-to-end Learning for Speech and Audio Processing, and NIPS 2015 Workshop on Machine Learning for Spoken Language Understanding and Interactions."}, {"heading": "15 MACHINE TRANSLATION", "text": "He et al. (2016a) proposed dual learning mechanism to tackle the data hunger issue in machine translation, inspired by the observation that the information feedback between the primal, translation from language A to language B, and the dual, translation from B to A, can help improve both translation models, with a policy gradient method, using the language model likelihood as the reward signal. Experiments showed that, with only 10% bilingual data for warm start and monolingual data, the dual learning approach performed comparably with previous neural machine translation methods with full bilingual data in English to French tasks. The dual learning mechanism may have extensions to many tasks, if the task has a dual form, e.g., speech recognition and text to speech, image caption and image generation, question answering and question generation, search and keyword extraction, etc.\nSee Sutskever et al. (2014); Bahdanau et al. (2015) for sequence to sequence neural machine translation. See Wu et al. (2016) for Google\u2019s Neural Machine Translation System."}, {"heading": "16 TEXT SEQUENCE PREDICTION", "text": "Text generation models are usually based on n-gram, feed-forward neural networks, or recurrent neural networks, trained to predict next word given the previous ground truth words as inputs; then in testing, the trained models are used to generate a sequence word by word, using the generated words as inputs. The errors will accumulate on the way, causing the exposure bias issue. Moreover, these models are trained with word level losses, e.g., cross entropy, to maximize the probability of next word; however, the models are evaluated on a different metrics like BLEU.\nRanzato et al. (2016) proposed Mixed Incremental Cross-Entropy Reinforce (MIXER) for sequence prediction, with incremental learning and a loss function combining both REINFORCE and crossentropy. MIXER is a sequence level training algorithm, aligning training and testing objective, such as BLEU, rather than predicting the next word as in previous works.\nBahdanau et al. (2017) proposed an actor-critic algorithm for sequence prediction, attempting to further improve Ranzato et al. (2016). The authors utilized a critic network to predict the value of a token, i.e., the expected score following the sequence prediction policy, defined by an actor network, trained by the predicted value of tokens. Some techniques are deployed to improve performance: SARSA rather than Monter-Carlo method to lessen the variance in estimating value functions; target network for stability; sampling prediction from a delayed actor whose weights are updated more slowly than the actor to be trained, to avoid the feedback loop when actor and critic need to be trained based on the output of each other; reward shaping to avoid the issue of sparse training signal.\nYu et al. (2017) proposed SeqGAN, sequence generative adversarial nets with policy gradient, integrating the adversarial scheme in Goodfellow et al. (2014). Li et al. (2017) proposed to improve sequence generation by considering the knowledge about the future."}, {"heading": "17 NEURAL ARCHITECTURE DESIGN", "text": "Neural networks architecture design is a notorious, nontrivial engineering issue. Neural architecture search provides a promising avenue to explore.\nZoph and Le (2017) proposed the neural architecture search to generate neural networks architectures with an RNN trained by RL, in particular, REINFORCE, searching from scratch in variablelength architecture space, to maximize the expected accuracy of the generated architectures on a validation set. In the RL formulation, a controller generates hyperparameters as a sequence of tokens, which are actions chosen from hyperparameters spaces; each gradient update to the policy parameters corresponds to training one generated network to convergence; an accuracy on a validation set is the reward signal. The neural architecture search can generate convolutional layers, with skip connections or branching layers, and recurrent cell architecture. The authors designed a parameter server approach to speed up training. Comparing with state of the art methods, the proposed approach achieved competitive results for an image classification task with CIFAR-10 dataset; and better results for a language modeling task with Penn Treebank. See also Baker et al. (2017)."}, {"heading": "18 PERSONALIZED WEB SERVICES", "text": "Li et al. (2010) formulated personalized news articles recommendation as a contextual bandit problem, to learn an algorithm to select articles sequentially for users based on contextual information of the user and articles, such as historical activities of the user and descriptive information and categories of content, and to take user-click feedback to adapt article selection policy to maximize total user clicks in the long run.\nTheocharous et al. (2015) formulated a personalized Ad recommendation systems as an RL problem to maximize life-time value (LTV) with theoretical guarantees. This is in contrast to a myopic solution with supervised learning or contextual bandit formulation, usually with the performance metric of click through rate (CTR). As the models are hard to learn, the authors deployed a modelfree approach to computes a lower-bound on the expected return of a policy to address the off-policy evaluation problem, i.e., how to evaluate a RL policy without deployment.\nLi et al. (2015) also attempted to maximize lifetime value of customers. Silver et al. (2013) proposed concurrent reinforcement learning for the customer interaction problem. See Chapter 16 in Sutton and Barto (2017) for a detailed and intuitive description of personalized web services."}, {"heading": "19 HEALTHCARE", "text": "There are many opportunities and challenges in healthcare for machine learning (Saria, 2014). Personalized medicine is getting popular in healthcare. It systematically optimizes the patient\u2019s health care, in particular, for chronic conditions and cancers using individual patient information, potentially from electronic health/medical record (EHR/EMR). Here dynamic treatment regimes (DTRs) or adaptive treatment strategies are sequential decision making problems. Some issues in DTRs are not in standard RL. Shortreed et al. (2011) tackled the missing data problem, and designed methods to quantify the evidence of the learned optimal policy. Goldberg and Kosorok (2012) proposed methods for censored data (patients may drop out during the trial) and flexible number of stages. See Chakraborty and Murphy (2014) for a recent survey, and Kosorok and Moodie (2015) for an edited book about recent progress in DTRs. Currently Q-learning is the RL method in DTRs. It is interesting to see the applications of deep RL methods in this field.\nSome recent workshops at the intersection of machine learning and healthcare are: NIPS 2016 Workshop on Machine Learning for Health (http://www.nipsml4hc.ws) and NIPS 2015 Workshop on Machine Learning in Healthcare (https://sites.google.com/site/nipsmlhc15/)."}, {"heading": "20 FINANCE", "text": "RL is a natural solution to some finance and economics problems (Hull, 2014; Luenberger, 1997), like option pricing (Longstaff and Schwartz, 2001; Tsitsiklis and Van Roy, 2001; Li et al., 2009),\nand multi-period portfolio optimization (Brandt et al., 2005), where value function based RL methods were used. Moody and Saffell (2001) proposed to utilize policy gradient to learn to trade; Deng et al. (2016) extended it with deep neural networks. Deep (reinforcement) learning would provide better solutions in some issues in risk management (Hull, 2014; Yu et al., 2009). The market efficiency hypothesis is fundamental in finance. However, there are well-known behavioral biases in human decision-making under uncertainty. A reconciliation is the adaptive markets hypothesis (Lo, 2004), which may be approached by reinforcement learning.\nIt is nontrivial for finance and economics academia to accept blackbox methods like neural networks; Heaton et al. (2016) may be regarded as an exception. However, there is a lecture in AFA 2017 annual meeting: Machine Learning and Prediction in Economics and Finance (goo.gl/7xdePd). A (obvious) factor is financial firms would probably hold state-of-the-art research/application results."}, {"heading": "21 MUSIC GENERATION", "text": "Jaques et al. (2017) proposed to combine maximum likelihood estimation with RL training, using RL to impose structure on an RNN trained on data by choosing reward functions, to attempt to ensure coherent global structure in multi-step generated sequences. A Note-RNN was trained to predict the next note in a musical sequence with a large corpus of songs. Then the Note-RNN was refined using RL to obtain RL Tuner, with a reward function considering both rules of music theory and output of another trained Note-RNN. RL Tuner produced more pleasant-sounding and subjectively pleasing melodies than alternative methods. The proposed approach has the potential for training sequence models other than music, by allowing for encoding high-level domain knowledge into the RNN."}, {"heading": "22 TO-DO LIST", "text": "We list interesting and/or important directions/papers we have not discussed in this overview as below, hoping it would provide pointers for those who may be interested in studying them further.3 This would be part of our future work.4\n\u2022 understanding deep learning, Daniely et al. (2016); Li et al. (2016b); Zhang et al. (2017)\n\u2022 exploration, e.g., Stadie et al. (2015); Bellemare et al. (2016); Kulkarni et al. (2016); Osband et al. (2016); Nachum et al. (2017)\n\u2022 model-based learning, e.g., Oh et al. (2015); Gu et al. (2016b)\n\u2022 retrace algorithm, Munos et al. (2016)\n\u2022 predictron, Silver et al. (2017)\n\u2022 hierarchical RL, e.g., Kulkarni et al. (2016); Vezhnevets et al. (2016); Tessler et al. (2017); Florensa et al. (2017)\n\u2022 transfer/multitask RL, e.g., Maurer et al. (2016); Mo et al. (2016); Parisotto et al. (2016), NIPS 2015 Transfer and Multi-Task Learning: Trends and New Perspectives Workshop\n\u2022 zero/one-shot learning, e.g., Vinyals et al. (2016); Lake et al. (2015); Johnson et al. (2016)\n\u2022 semi-supervised RL, e.g., Finn et al. (2017)\n\u2022 deep symbolic RL, Garnelo et al. (2016)\n\u2022 intrinsic motivation, e.g., Stadie et al. (2015); Kulkarni et al. (2016); Oudeyer et al. (2016)\n\u2022 hyperparameter learning, e.g. Andrychowicz et al. (2016)\n\u2022 information extraction, e.g., Narasimhan et al. (2016)\n\u2022 text games, e.g., He et al. (2016b); Narasimhan et al. (2015)\n\u2022 language tree-structure learning, e.g., Yogatama et al. (2017)\n3Some topics/papers may not contain RL yet. However, we believe these are interesting and/or important directions for RL in the sense of either theory or application.\n4It would be definitely more desirable if we could finish reviewing these before publishing this overview. One factor is we set the deadline for the first version before January 28, 2017, the Chinese Spring Festival.\n\u2022 question answering, e.g., Shen et al. (2016); Trischler et al. (2016)\n\u2022 large action space, e.g., Dulac-Arnold et al. (2016); He et al. (2016c)\n\u2022 adaptive normalization, van Hasselt et al. (2016b)\n\u2022 self-driving vehicle, e.g., Bojarski et al. (2016), NIPS 2016 Workshop on Machine Learning for Intelligent Transportation Systems\n\u2022 smart grid, e.g., Wen et al. (2015b)\n\u2022 physics experiments, e.g., Denil et al. (2016)\n\u2022 deep probabilistic programming, Tran et al. (2017)\n\u2022 deep learning games, Schuurmans and Zinkevich (2016)\n\u2022 program learning, e.g., Reed and de Freitas (2016)\n\u2022 quantum RL, e.g., Crawford et al. (2016), NIPS 2015 Workshop on Quantum Machine Learning"}, {"heading": "23 RESOURCES", "text": "We list some resources for Deep RL, which by no means are complete."}, {"heading": "23.1 BOOKS", "text": "\u2022 The definite and intuitive reinforcement learning book by Richard S. Sutton and Andrew G. Barto (Sutton and Barto, 2017)\n\u2022 Concise and theoretical, Algorithms for Reinforcement Learning by Csaba Szepesva\u0301ri (Szepesva\u0301ri, 2010)\n\u2022 A theoretical book about approximate dynamic programming by Dimitri P. Bertsekas (Bertsekas, 2012)\n\u2022 An operations research oriented book, Approximate Dynamic Programming, by Warren B. Powell (Powell, 2011)\n\u2022 Deep learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (Goodfellow et al., 2016)"}, {"heading": "23.2 COURSES", "text": "\u2022 David Silver, Reinforcement Learning, 2015, slides (goo.gl/UqaxlO), video-lectures (goo.gl/7BVRkT)\n\u2022 Sergey Levine, John Schulman and Chelsea Finn, CS 294: Deep Reinforcement Learning, Spring 2017, http://rll.berkeley.edu/deeprlcourse/\n\u2022 Charles Isbell, Michael Littman and Pushkar Kolhe, Udacity: Machine Learning: Reinforcement Learning, goo.gl/eyvLfg\n\u2022 Fei-Fei Li, Andrej Karpathy and Justin Johnson, CS231n: Convolutional Neural Networks for Visual Recognition, http://cs231n.stanford.edu\n\u2022 Richard Socher, CS224d: Deep Learning for Natural Language Processing, http://cs224d.stanford.edu\n\u2022 Nando de Freitas, Deep Learning Lectures, https://www.youtube.com/user/ProfNandoDF"}, {"heading": "23.3 TUTORIALS", "text": "\u2022 David Silver, Deep Reinforcement Learning, ICML 2016\n\u2022 Pieter Abbeel and John Schulman, Deep Reinforcement Learning Through Policy Optimization, NIPS 2016\n\u2022 Andrew Ng, Nuts and Bolts of Building Applications using Deep Learning, NIPS 2016\n\u2022 John Schulman, The Nuts and Bolts of Deep Reinforcement Learning Research, Deep Reinforcement Learning Workshop, NIPS 2016\n\u2022 John Schulman, Deep Reinforcement Learning, Deep Learning School, 2016\n\u2022 Pieter Abbeel, Deep Reinforcement Learning, Deep Learning Summer School, 2016; http://videolectures.net/deeplearning2016 abbeel deep reinforcement/\n\u2022 David Silver, Deep Reinforcement Learning, 2nd Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM), Edmonton 2015; http://videolectures.net/rldm2015 silver reinforcement learning/\n\u2022 Rich Sutton, Introduction to Reinforcement Learning with Function Approximation, https://www.microsoft.com/en-us/research/video/tutorial-introduction-to-reinforcementlearning-with-function-approximation/\n\u2022 Joelle Pineau, Introduction to Reinforcement Learning, Deep Learning Summer School, 2016; http://videolectures.net/deeplearning2016 pineau reinforcement learning/\n\u2022 Deep Learning Summer School, 2016, 2015"}, {"heading": "23.4 CONFERENCES, JOURNALS AND WORKSHOPS", "text": "\u2022 NIPS: Neural Information Processing Systems\n\u2022 ICML: International Conference on Machine Learning\n\u2022 ICLR: International Conference on Learning Representation\n\u2022 RLDM: Multidisciplinary Conference on Reinforcement Learning and Decision Making\n\u2022 AAAI, IJCAI, ACL, EMNLP, SIGDIAL, ICRA, IROS, KDD, SIGIR, CVPR, etc.\n\u2022 Science Robotics, JMLR, MLJ, AIJ, JAIR, PAMI, etc\n\u2022 Nature May 2015, Science July 2015, survey papers on machine learning/AI\n\u2022 Deep Reinforcement Learning Workshop, NIPS 2016, 2015; IJCAI 2016\n\u2022 Deep Learning Workshop, ICML 2016"}, {"heading": "23.5 BLOGS", "text": "\u2022 Andrej Karpathy, karpathy.github.io, esp. goo.gl/1hkKrb\n\u2022 Denny Britz, www.wildml.com, esp. goo.gl/MyrwDC\n\u2022 Junling Hu, Reinforcement learning explained - learning to act based on long-term payoffs\n\u2022 Li Deng, How deep reinforcement learning can help chatbots\n\u2022 Christopher Olah, colah.github.io\nIn the current information/social media age, we are overwhelmed by information, e.g., from Twitter, Google+, WeChat, arXiv, etc. The skill to efficiently select the best information becomes essential."}, {"heading": "24 DISCUSSIONS", "text": "It is both the best and the worst of times for the field of deep RL, for the same reason: it has been growing so fast and so enormously. We have been witnessing breakthroughs, exciting new methods and applications, and we expect to see much more and much faster. As a consequence, this overview is incomplete, in the sense of both depth and width. However, we attempt to summarize important achievements and discuss potential directions and applications in this amazing field.\nWe have been witnessing breakthroughs, three papers about or using Deep RL published in Nature in less than two years: deep Q-network (Mnih et al., 2015), AlphaGo (Silver et al., 2016) and differentiable neural computer (Graves et al., 2016); We have already seen many extensions to, improvements for and applications of deep Q-network (Mnih et al., 2015). The mechanisms of attention and memory (Graves et al., 2016) has been attracting much attention.\nNovel architectures and applications using deep RL were recognized in top tier conferences as best (student) papers in 2016: dueling network architectures (Wang et al., 2016a) at ICML, spoken dialogue systems (Su et al., 2016b) at ACL (student), information extraction (Narasimhan et al.,\n2016) at EMNLP, and value iteration networks (Tamar et al., 2016) at NIPS. Exciting achievements abound: asynchronous methods (Mnih et al., 2016), dual learning for machine translation (He et al., 2016a), guided policy search (Levine et al., 2016a), generative adversarial imitation learning (Ho and Ermon, 2016), unsupervised reinforcement and auxiliary learning (Jaderberg et al., 2017), and neural architecture design (Zoph and Le, 2017), etc.\nValue function is central to reinforcement learning, e.g., in deep Q-network and its many extentions. Policy optimization approaches have been gaining traction, in many, diverse applications, e.g., robotics, neural architecture design, spoken dialogue systems, machine translation, attention, and learning to learn, and this list is boundless. New learning mechanisms have emerged, e.g., using unsupervised/semi-supervised/tranfer learning to improve the quality and speed of learning, and more new mechanisms will be emerging. This is the renaissance of reinforcement learning (Krakovsky, 2016). In fact, reinforcement learning and deep learning have been making steady progress even in the AI winter.\nIt is essential to consider issues of learning models, like stability, convergence, accuracy, data efficiency, scalability, speed, simplicity, interpretability, robustness, and safety, etc. It is important to investigate comments/criticisms, e.g., from conginitive science, like intuitive physics, intuitive psychology, causal model, compositionality, learning to learn, and act in real time (Lake et al., 2016), for stronger AI. See also Peter Norvig\u2019s perspective at goo.gl/obvmVB.\nDeep learning, in this third wave of AI, will have deeper influences, as we have already seen many achievements. Reinforcement learning, as a more general learning and decision making paradigm, will deeply influence deep learning, machine learning, and artificial intelligence in general.5 It is interesting to mention that when Professor Rich Sutton started working in the University of Alberta in 2003, he named his lab RLAI: Reinforcement Learning and Artificial Intelligence.\nACKOWLEDGEMENT\nI appreciate comments from Baochun Bai, Junling Hu, Ruitong Huang, Lihong Li, Dale Schuurmans, David Silver, Rich Sutton, Csaba Szepesva\u0301ri, Yi Wan and Qing Yu. Any remaining issues and errors are my own. This document also benefits from discussions during various seminars/webinars, in particular, an AlphaGo seminar at MIT in April 2016, deep (reinforcement) learning seminars at the University of Toronto, McGill University and the University of Alberta in October 2016 as part of the North America tour of Synced (Jiqizhixin), and webinars using David Silver\u2019s slides in November and December 2016, and discussions in several WeChat groups."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions.", "creator": "LaTeX with hyperref package"}}}