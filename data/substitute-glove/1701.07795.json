{"id": "1701.07795", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2017", "title": "Match-Tensor: a Deep Relevance Model for Search", "abstract": "The licensing such Deep Neural Networks for candidates 2004 sent engine in sufficed where ability for place undertook genre industrial larger actually though different - their - 8th particular. However, could fun that styles practical relevance matching features comes BM25 own exist Deep Neural Net versions similar substantially improves the accuracy of same smaller, uncertain does they like not capture sustain local merits matching respond. We describe on fiction wind Recurrent Neural Net - scientific model although be come Match - Tensor. The concepts of created Match - Tensor model involved accounts some both as relevance purse now potential pleasantness continuously allowing they setting rich quirky four them when micro end demonstrates beyond full wording either a correspondences. On a from set - way test set consisting of social media documents, we enhance not number that Match - Tensor outperforms BM25 and among classes this DNNs never it whether it mainly federates signals thus in similar characteristics.", "histories": [["v1", "Thu, 26 Jan 2017 17:59:38 GMT  (293kb,D)", "http://arxiv.org/abs/1701.07795v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["aaron jaech", "hetunandan kamisetty", "eric ringger", "charlie clarke"], "accepted": false, "id": "1701.07795"}, "pdf": {"name": "1701.07795.pdf", "metadata": {"source": "META", "title": "Match-Tensor: a Deep Relevance Model for Search", "authors": ["Aaron Jaech", "Hetunandan Kamisetty", "Eric Ringger", "Charlie Clarke"], "emails": ["ajaech@uw.edu", "hetu@fb.com", "eringger@fb.com", "claclark@gmail.com"], "sections": [{"heading": null, "text": "The application of Deep Neural Networks for ranking in search engines may obviate the need for the extensive feature engineering common to current learning-to-rank methods. However, we show that combining simple relevance matching features like BM25 with existing Deep Neural Net models often substantially improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel deep Recurrent Neural Net-based model that we call Match-Tensor. The architecture of the Match-Tensor model simultaneously accounts for both local relevance matching and global topicality signals allowing for a rich interplay between them when computing the relevance of a document to a query. On a large held-out test set consisting of social media documents, we demonstrate not only that Match-Tensor outperforms BM25 and other classes of DNNs but also that it largely subsumes signals present in these models."}, {"heading": "1 INTRODUCTION", "text": "Machine learning models have traditionally relied on a set of manually defined features whose weights are trained in order to optimize an objective function on a set of labeled examples. The learning-to-rank models in widespread use for search may require hundreds of such features to be generated for each query/document pair [27]. In particular, the approach requires features that capture the presence of query terms in a given document, the proximity and location of those terms in the document, document quality, document type, and many other important aspects of the query, of the document, and of the relationship between them. In many rankers, the feature set includes classic information retrieval models, such as BM25 [37], and term dependence models [28]. These models in turn combine more basic query and document features in an attempt to capture salient aspects of relevance.\nGeneration of these features requires a substantial software engineering effort, and the addition of a single new feature may require the creation and testing of new and specialized code to compute it. More importantly, the scope of the feature set is limited by the imagination of its engineers. If\nthe feature set fails to capture some aspect of relevance, that aspect is lost to the ranker.\nRecently Deep Neural Networks (DNNs) have replaced this traditional machine learning paradigm in many applications by directly learning a model and its features from the inputs, obviating the need for manually specified features. Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] . These efforts share a goal of reducing the need for traditional feature engineering, and even if DNNs cannot completely eliminate feature engineering, they may capture aspects of relevance beyond its scope.\nUnfortunately, while previous efforts have demonstrated that these approaches can capture novel aspects of relevance, they can also fail to capture the most salient aspect of the search task, namely local relevance matching [14]. Indeed, some recent efforts to develop completely new DNN architectures tailored to search still show lower accuracy on TREC datasets than the classic information retrieval models, which are used as features in traditional learning-to-rank approaches [35, 36]. While this failure may possibly be due to the lack of sufficient training data, which is critical to the success of DNNs, the failure may also reflect a more fundamental gap in these architectures.\nWe develop a new deep neural net architecture tailored for the search task involving social media using a method we refer to as Match-Tensor. The model architecture is simultaneously simple and expressive and is schematically illustrated in Fig. 1: a pair of sequence models computes representations of a query and a given document accounting for both local and distributed context; these representations are used in matching each pair of query and document words along several dimensions and stored as a 3-D tensor (hence, the term Match-Tensor). A convolutional network then translates this tensor to a relevance score. Using a large volume of social media search data, we train a model of this type and analyze the performance, demonstrating that it is also substantially more accurate than other current model-classes. We also demonstrate that this approach largely subsumes other models such as BM25[37] and SSM[17, 41]: the MatchTensor model alone performs nearly as well as meta-models trained with both Match-Tensor and these features as input. ar X\niv :1\n70 1.\n07 79\n5v 1\n[ cs\n.I R\n] 2\n6 Ja\nn 20"}, {"heading": "2 BACKGROUND", "text": "Like most learning-to-rank methods, we wish to learn a function \u03a6(q, d) that computes a score reflecting the relevance of document d for query q. In the traditional feature engineering approach, we would start with a set of hand-crafted features F that capture various aspects of relevance matching, combine them in a single model M \u2013 such as logistic regression or boosted decision trees \u2013 and train the model using a learning-to-rank approach [3, 4] to predict the labels on training data:\n\u03a6(q, d) = M(F (q, d))\nThe features employed in this approach may be as simple as binary query term presence in the document or as complex as separately trained classification or ranking sub-models. Furthermore, it is standard to include classic information retrieval models in this feature set, particularly BM25 [37]. Liu [25] provides a thorough overview of traditional learningto-rank methods for search. Macdonald et al. [27] cover many of the engineering issues associated with deploying learning-to-rank in a search engine.\nThe advent of Deep Neural Networks has led to the development of an exciting alternative: In this approach, a single learning procedure is used to learn both features and a model simultaneously. Huang et al. [17] introduced the first Deep Neural Network architectures for Web search that operated on (query, title) pairs, using a so-called siamese architecture [23], in which two feed-forward networks NNQ and NND map the query q and the title of a given web document d, respectively, into fixed-length representations:\n\u03a6(q, d) = cos(NNQ(q), NND(d)),\nThe final documents are then ranked by their similarity to the query in this space computed using cosine similarity. The\napplication of convolutional neural networks, in lieu of feedforward-networks, by Shen et al. [41] marks the next notable advancement using the same siamese architecture. The local connectivity of convolutional networks can allow for more accurate models, especially when it mirrors the structure of the task at hand.\nIn parallel to these developments, there have been several advances in Deep Neural Networks, especially for modeling text. While earlier approaches to DNNs for text used convolutional networks, more recent approaches have used Recurrent Neural Networks (RNNs), especially those based on Long Short-term Memory (LSTM) units [16]. Unlike convolutional networks, the units in recurrent networks maintain an internal state that is updated from word to word as a given text is processed, allowing for the network to capture sequential relations across a query or document. A popular architecture for machine translation uses the so-called sequence-to-sequence paradigm in which the input text in the source language is \u201cencoded\u201d using an encoder network to produce a fixed-length representation (the RNN state)[42]. A \u201cdecoder\u201d then begins with this representation and emits an output in the target language. While the use of a fixedlength representation is similar to the architecture of Huang et al. [17] and Shen et al. [41], the use of RNNs such as those based on LSTMs is critical to their performance. Attentionbased schemes build on this architecture by dynamically re-weighting (i.e., focusing attention) on various elements of the source representation during the decoding process, and they have demonstrated considerable improvements over their non-attention counterparts [2].\nThe \u201crepresentation-based\u201c nature of siamese architectures has also been identified as a limitation in search [14] and has led to the development of alternate \u201cinteraction-based\u201d architectures, in which the relationships between query and\ndocument are considered earlier. In an approach called MatchPyramid, Pang et al. [36] construct an interaction matrix between query and document terms, where each entry in the matrix denotes the strength of similarity between the corresponding terms. A hierarchical convolutional model then operates on this single interaction matrix to compute the final score . A recent paper by Mitra et al. [32], that appeared while this manuscript was being prepared, uses a \u201cduet\u201d architecture in which two separate networks (one \u201crepresentation\u201d-based and the other \u201cinteraction\u201d-based) are combined to simultaneously account for local and distributed measures of similarity. The key idea in this method is to use an exact-match matrix followed by convolutional layers on the \u201cinteraction\u201d half of the network in addition to a siamese architecture. A crucial limitation of such an approach to modeling interactions is that all tokens in the query are given equal importance: the interaction model can therefore not distinguish between query terms that are important and those that are not [39].\nTo address some of these shortcomings, we developed a new architecture that we refer to as Match-Tensor. Our thesis is that no single interaction matrix can capture all aspects of the matching problem. Unlike previous interaction-based approaches, which use a single representation of the interaction matrix and cannot distinguish between different words that have the same pattern of matching, the Match-Tensor model architecture simultaneously considers similarity along several dimensions during the matching process. This approach allows for a rich interplay between different dimensions in subsequent stages of the architecture in order to determine the relevance of a document to the query."}, {"heading": "3 MATCH-TENSOR ARCHITECTURE", "text": "The central thesis of the Match-Tensor architecture is that it is vital to incorporate multiple notions of similarity, capturing both immediate and larger contexts in a given document when computing the relevance of that document to a query. This objective is achieved by a three-dimensional tensor, in which one dimension corresponds to terms in the query, one dimension for the terms in the document, and a third dimension for various match channels. Each match channel contains a distinct estimate of the match similarity between the query and document, hence the name \u201cMatch-Tensor\u201d. The tensor is computed using the output of a neural network operating on word-embeddings and is supplemented with an exact-match channel that operates directly on the tokens; a downstream neural network is then employed to determine the relevance of the document to the query using the tensor. The entire network is trained end-to-end with a discriminative objective. Thus, the manner in which these multiple notions of similarity are combined to produce the final relevance score is deferred until after all channels are computed. Fig.2 illustrates the final model in detail. In what follows, we motivate and introduce each element of the architecture."}, {"heading": "3.1 Input to the Match-Tensor Layer", "text": "To begin, a word-embedding lookup layer converts query and document terms into separate sequences of word-embeddings. The word embedding table is itself computed offline from a large corpus of social media documents using the word2vec package [30] in an unsupervised manner and are held fixed during the training of the Match-Tensor network. In our\nimplementation, word-embeddings are 256-dimensional vectors of floating point numbers. The word embeddings are then passed through a linear projection layer to a reduced l-dimensional space (here l = 40); the same linear projection matrix is applied to both the query and the document word vectors. This linear projection allows the size of the embeddings to be varied and tuned as a hyperparameter without relearning the embeddings from scratch each time. Two Recurrent Neural Networks, specifically bi-directional LSTMs (i.e, bi-LSTMs) [11, 16], then encode the query (respectively document) word-embedding sequence into a sequence of LSTM states. The bi-LSTM states capture separate representations in vector form of the query and the document, respectively, that reflects their sequential structure, looking beyond the granularity of a word to phrases of arbitrary size. During hyperparameter tuning, we allowed the models to use a linear projection layer inside the bi-LSTM recurrent connection, as defined in Sak et al. [38], but the best models did not make use of it. There is a separate linear projection after the bi-LSTM to establish the same number k of dimensions in the representation of query and document (in our case, k = 50). Thus, at the end, each token in the query and the document is represented as a k-dimensional vector."}, {"heading": "3.2 Match-Tensor Layer", "text": "For m words in the query and n words in the document, the actual match-tensor \u2013 from which the architecture inherits its name \u2013 is an m\u00d7n\u00d7k+1 tensor, where k+1 is the number of channels in the match-tensor (specifically, 51 in the detailed figure). Each of the k+1 channels is computed from a distinct representation of the query and document: all but one of the channels are computed using the element-wise product of the corresponding bi-LSTM states of the query and document (after applying the subsequent projection). Including each dimension as a separate layer instead of collapsing them into a single layer allows the model to include state-specific (approximately: term-specific) signals in the matching process and to weigh matching different terms according to their importance. While this approach captures most of the key signals, one omission of the first k layers is their inability to properly represent out-of-vocabulary tokens in the query or document, beginning in the initial word embedding lookup. To compensate for this problem, the initial embedding lookup includes an out-of-vocabulary vector, and the model appends an extra exact-match channel in the match-tensor (hence, k + 1 total channels) such that the entry at position i, j of this channel is set to \u03b1 if word i in the query is an exact match to word j in the document and zero otherwise. This exact-match channel is critical for capturing local textual match. The value of \u03b1 is learned via back-propagation along with the rest of the model."}, {"heading": "3.3 From Match-Tensor to Score", "text": "The secondary neural network begins with the match-tensor and applies a convolutional layer: the match-tensor is convolved cross the full depth (k + 1) of the tensor with three\nsets of filters, each having a width of three query words and a height of three, four, or five document words. As illustrated in Fig. 1, these 3-D convolution filters enable the model to learn interactions among the representations in ways that would be very difficult to anticipate as a feature engineer, lending expressive strength to the model architecture. The model applies a ReLU (rectified linear unit) function to the output of these convolutions and then convolves that output with a set of 1 \u00d7 1 filters. The ReLU activation function was chosen because it has been shown to be effective for convolutional neural networks in computer vision [18]. Finally, the model applies 2-D max-pooling to coalesce the peaks from the ReLU into a single fixed size vector. This is fed into a fully-connected layer (labeled as \u201clinear projection\u201d in the diagram) and through a sigmoid to produce a single probability of relevance on the output of the model."}, {"heading": "4 ADDITIONAL RELATED WORK", "text": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44]. Our work is closest to the so-called Match Pyramid models of Pang et al. [35, 36]: Match Pyramid models construct a single match matrix and then use a convolutional network on top of it (hence \u201cpyramid\u201d) to compute a relevance score. Unlike them, the Match-Tensor architecture developed in this work simultaneously considers multiple channels during the matching process allowing for a rich interplay between the different channels in determining the relevance of a document to a query. Match Pyramid models are unable to distinguish between different words having the same match pattern. Guo et al. [14] developed a neural-network based model (DRMM) that uses matching histograms and term-gating. They report that this model is more accurate than BM25 and other alternatives on standard TREC test collections (Robust-04 and ClueWeb-09-Cat-B) however Mitra et. al [32] report that a model incorporating an exact-match channel with a representation based \u201cdistributed\u201d model outperforms DRMM on a larger collection of websearch queries.\nSeveral other models that use word-embeddings have been proposed for various aspects of Search including Diaz et al. [8] who use them in query expansions, Ganguly et al. [10] who use them in smoothing language models, Nalisnick et al. [33] who propose dual embeddings and Grbovic et al. [12, 13] who use them in sponsored search. Cohen et al. [6] have also studied the utility of DNNs for several IR tasks."}, {"heading": "5 METHODOLOGY", "text": ""}, {"heading": "5.1 Data", "text": "We base our experiments on a collection of approximately 1.6 million (query,document,label) triplets, collected on a major social media site between between 2016-01-01 and 2016-06-01. Each document is a publicly viewable social media post, which might include videos, photos, and links, as well as text, but for the experiments reported in this paper, we consider only the text. Labels indicate the relevance level of the document\nwith respect to the query. For these experiments, we use three levels of relevance: \u201cVITAL\u201d, \u201cRELEVANT\u201d, and \u201cNONRELEVANT\u201d. We split this dataset (by query) into 3 parts: train, validation, and test, so that each query string appears in exactly one of the partitions. We provide details of the partitioning in table 1. The training and validation sets were used to train the models and perform hyper-parameter sweeps. The test set was used only for evaluation, at the end of this process."}, {"heading": "5.2 Implementation Details", "text": "We implemented our Match-Tensor neural net model using TensorFlow [1]. We used pre-trained 256-dimensional phrase embeddings using the word2vec package [29] on a large corpus of documents with a vocabulary size of around 2 million tokens containing unigrams and selected bigrams. Out-ofvocabulary words are mapped to a special token. Queries are truncated to a maximum of eight words in length, whereas documents are truncated to a maximum length of 200 words. Both the query and the documents are then preprocessed by lowercasing and applying a simple tokenizer to split words and remove punctuation. Since social media documents are structured into separate fields (e.g. the title, author, and body), we added special tokens for demarcating the boundaries between fields and for the start and end of a document. The embeddings for these special boundary tokens are randomly initialized and kept fixed during training. We used dropout as a regularizer on the non-recurrent connections of all bi-LSTMs. We employed the Adam optimizer for gradient descent [20] with a learning rate of 0.001 and mini-batch size of 200. Hyperparameter settings are shown in 2.\nTo investigate the importance of each of the major components in the model we also experimented with alternative choices for these components and alternate architectures, tailoring them to social media documents. While some architectures have been suggested for short text common in some social media[26, 36], studies indicate that they do not beat baseline models such as BM25[35]. In contrast, both early models [17, 41] and recent developments by Mitra et.al that has shown strong performance [31] have been designed for web-search and are not directly usable in our setting. We therefore adapted these model architectures from web search\nfor our setting. The details are discussed in the following sections."}, {"heading": "5.3 Semantic Similarity Model(SSM)", "text": "We constructed an model using the siamese network architecture based on the semantic similarity models (SSM) appearing in other work [17, 34, 41]. In this SSM model shown in detail in Figure 5, we construct a query embedding by concatenating the last output from each of the forward and backward directions of the query bi-LSTM and a document embedding by max-pooling over the output bi-LSTM states across the entire document. Max-pooling is used for the document because the documents can be much longer than the query and it is harder for the bi-LSTM to propagate the relevant information all the way to the end of the document [22]. These fixed-length document and query embeddings are then passed throw linear projections before computing a dot-product between them which is then used to compute the final score. The model parameters and hyper-parameters were optimized on the same dataset as the Match-Tensor model."}, {"heading": "5.4 Match-Tensor(Exact-only)+SSM", "text": "In recent work that appeared while this manuscript was being prepared, Mitra et al [32] show that a combination of local and distributed matching can outperform other models for web-search. Since several details of their model are specific to the structure of web documents, we constructed an model that has similar characteristics for our settings by combining a single channel exact-match only Match-Tensor component with an SSM component into a single model. This is done by concatenating the output from the last layer of MatchTensor filters with the hidden layer of the SSM comparison network as shown in Figure 6. The Match-Tensor and SSM components share parameters for the word embedding and LSTM portion of the model."}, {"heading": "5.5 Match-Tensor+SSM", "text": "We also compare the effect of using all the channels in the Match-Tensor architecture in conjunction with the SSM architecture. This model is shown in Figure 6. The only difference in architecture between this model and the previous (exactmatch only channel) model is the number of channels in the tensor layer: the former has one channel while this model has k + 1 like the Match-Tensor model."}, {"heading": "5.6 bi-LSTMs vs CNNs", "text": "We compared all three model architectures listed against similar ones that uses convolutional layers in-place of bi-LSTMs. We used a mix of width 1 and width 3 convolutional filters. Compared to the bi-LSTMs, that can incorporate information over a wide token span, the representations produced by the convolutional filters only look at trigrams (when the width is 3) but are computationally cheaper."}, {"heading": "5.7 Attention Pooling", "text": "To improve on the query-agnostic pooling schemes of SSM, we also implemented an attention pooling mechanism for the document embeddings as an alternative to max pooling. The hypothesis here is that information from the query is important in deciding in how to summarize the document. The attention pooling model learns a ReLU activated transformation from the query embedding and each output from the document bi-LSTM. Attention weights are determined by taking the dot product between these vectors and normalized using the Softmax function. The attention-pooled document embedding is the weighted combination of the biLSTM outputs. We note that our use of attention is different from that of Palangi et al. [45] where attention-based pooling was used in a query-agnostic manner. In our preliminary experiments, using attention-based pooling did not result in improved results compared to a max pooling baseline so we did not pursue it further."}, {"heading": "5.8 Ensemble models", "text": "Comparing different model architectures using absolute metrics can yield insight into the relative importance of the types of signals for the task at hand. However, one model might outperform another model without capturing the signal in the latter model. Consequently, to test if one model subsumes another, we train additional ensemble models that use the scores of both models. We measure the accuracy of the ensemble models in addition to the individual models."}, {"heading": "6 RESULTS", "text": ""}, {"heading": "6.1 Model selection", "text": "We optimized hyperparameters based on a random grid search on the validation set for each model architecture studied, selecting the one model with the best score out of 200 runs. For each model architecture we next evaluated the single best model on the test set. Table 2 reports these hyperparamters for the three main model architectures we studied. Critically, we note that the final Match-Tensor model has fewer parameters than the final SSM model."}, {"heading": "6.2 Sensitivity to Training Size", "text": "To evaluate the sensitivity of the model performance to the amount of training data, for each of the NN architectures we sub-sampled the training set, retrained models (keeping the hyper-parameters fixed), and computed the test-loss. Figure 3 shows the test loss of each model as a function of its final accuracy. Not surprisingly, each architecture we consider here benefits from the availability of large training sets, and the accuracy improves substantially as we increase the size of the training set. However, the relative comparisons between the model architectures appear to be reasonably robust to training data size."}, {"heading": "6.3 Performance of Neural Models", "text": "Figure 4 summarizes the performance of the various neural model architectures relative to a BM25 baseline. The figure reports NDCG at various levels as well as Expected Reciprocal Rank (ERR) [5], with all measures computed using the three relevance grades. Overall, the Match-Tensor model (with bi-LSTMs) is the most accurate individual model, with an 11% improvement in AUC of the ROC curve (right panel of the figure) over a BM25 baseline and smaller but consistent improvements on NDCG and ERR. We note that while the relative ordering of models appears to be robust to variations in the test set, the values of these relative improvements appear to be sensitive to the composition of the test set: relative improvements when restricted to a subset of the test-set that are \u201chard\u201d (at most half the available results are relevant) are much larger.\nThe SSM architecture had lower NDCG than the BM25 baselines. This result is consistent with [14] and others who have highlighted the limitation of models that only match semantic representations in relevance-matching tasks. MatchTensor is not only more accurate in aggregate, it is also more accurate at every cutoff: the Precision of the model is higher than the others at all values of Recall."}, {"heading": "6.4 bi-LSTMs vs CNNs for text representation", "text": "We considered the use of convolutional neural networks to compute the text representations in the first stage of each model architecture in place of bi-LSTMs. Table 3 shows that across the four model architectures under consideration, using bi-LSTMs results in more accurate models than their CNN counterparts in terms of AUC, NDCG, and ERR. For AUC in particular, the relative gain in AUC from using bi-LSTMs is between two to three percent. The fact that this increase holds for both SSM and Match-Tensor architecture variants\nsuggests that the improvements are due to bi-LSTMs \u2013 across the board \u2013 providing more accurate representations at each position. This outcome is not surprising given similar results in other language modeling tasks and is consistent with gains in NDCG observed in [34] in going from convolutional to bi-LSTM-based semantic similarity models."}, {"heading": "6.5 2-D Matching vs. SSM", "text": "As we have seen, the Match-Tensor architecture outperforms the SSM architecture, often significantly. Although both architectures are most accurate when using bi-LSTMs, the\nrelative improvement when going from SSM to Match-Tensor is substantial. This improvement holds even when using CNNs to represent the state at each query and document token: AUC goes up by 4% when using bi-LSTMs and 3% when using CNNs, suggesting that the improvement is a consequence of the underlying difference in architectures. The superiority of Match-Tensor is not surprising given that the Match-Tensor architecture has a substantially more expressive matching function. Furthermore, combining the Match-Tensor and SSM architectures gives no substantial gain in performance: as seen, small improvements in AUC are\noffset by small reduction in NDCG and ERR. The absence of a difference for this hybrid architecture would suggest that the bi-LSTM representation at each position is already capturing global context sufficiently well to make the additional explicit per-document representation non-informative for this problem."}, {"heading": "6.6 Influence of the exact-match channel", "text": "While we introduced the exact-match channel to account for out-of-vocabulary tokens where the bi-LSTM states might not be accurate, it is computed for all cases. Not surprisingly, it is an important contributor to the accuracy of the final model. However the interplay between all channels improves the accuracy of the model further: the relative NDCG at 1 goes up by 2% with the bi-LSTM channels on compared to the exact-match alone model where the relative improvement is roughly 1% i.e., roughly half. This approximate doubling in relative accuracy when moving from the single channel to the full Match-Tensor model is seen across all positions in NDCG and in ERR."}, {"heading": "6.7 Ensemble models", "text": "To determine if a deep relevance model is indeed capturing all essential relevance matching signals, we trained ensemble models: boosted trees [9] that combine as inputs the neural model\u2019s output as a feature and a BM25 feature using 5- fold cross-validation on the existing validation set. Neural Models that capture essential relevance matching signals better should show relatively small improvements when BM25 is added to the mix, compared to those that don\u2019t since a good model should already capture most of the signal in BM25. As seen in Table 4, Match-Tensor shows the smallest relative increase when BM25 is added to the mix compared to all other alternatives. An exact-match only Match-Tensor +SSM model also does better in this regard than SSM alone although again the full Match-Tensor model is substantially better by allowing for interplay among channels even without having an explicit SSM-like component. We note that despite the small relative increase, the Match-Tensor&BM25 model is more accurate than all other ensemble variants and is nearly 1% more accurate than Match-Tensor(Exact only)+SSM&BM25. Thus, the Match-Tensor model is not only the most accurate model in this list, it also largely subsumes the semantic matching signals in SSM and the relevance matching signals in BM25 as indicated by the relative small improvement in accuracy when BM25 is added to it."}, {"heading": "6.8 Model Introspection", "text": "We illustrate the strengths of the Match-Tensor model over other approaches with a few examples in Table 5. SSM is focused towards matching representations. As a result, it often misses out on relevance matching signals by finding a result about the same broad topic but different in several crucial details: as an example, for a query about a celebrity tv show, the model ranks a document about a different celebrity\u2019s TV show above a relevant result.\nUnder its bag of words model, BM25 often scores results that have completely wrong phrases but the right set of tokens above a relevant result. This is best illustrated by the query \u201clow fat high carb\u201d where the model prefers a result about \u201clow carb high fat\u201d over a relevant result. Traditional learningto-rank methods address this problem with specifically engineering proximity and ordering features. Match-Tensor, on the other hand, correctly ranks these results, learning the necessary proximity, ordering, grammatical, and other relationships directly from training examples.\nThe Match-Tensor(Exact only)+SSM model uses only exact matches between query and document terms and relies on a single representation of query and document to capture semantic signals. This results in subtler failures, often due to an over-reliance on the exact-match channel: for a query inquiring about scholarships for graduate programs, \u201dscholarship to master degree\u201d, the exact-only model prefers a document that has the exact phrase but is semantically not useful to the searcher. The full Match-Tensor model correctly prefers another result that matches the intent of the query even though the document doesn\u2019t contain an exact match."}, {"heading": "7 CONCLUDING DISCUSSION", "text": "Deep Neural Networks are a compelling development in machine learning that have substantially advanced the stateof-the-art in several disciplines[24]. While initial developments in several domains were focused on the absolute accuracy [21, 42] of these models when compared to alternatives, the focus has more recently gravitated towards the completeness of these models; indeed in several domains such as speech recognition, computer vision and machine translation, entire production systems have been completely replaced with neural networks that are trained end-to-end [15, 43].\nEarly neural network models for search focused on semantic matching signals which supplemented existing relevance matching features. By computing similarity between semantic representations of the query and document, this class of models naturally captured signals that were hard to determine using traditional models. However, this general class of models appears to miss critical relevance-matching signals [14]. In this work, we explored Match-Tensor, a new Deep Relevance model architecture for Search. By simultaneously accounting for several notions of similarity with an expressive 3-D tensor layer and by deferring the combination of these signals into a relevance score to later layers, Match-Tensor is able to achieve higher accuracies than other architectures. More interestingly, this architecture appears to largely subsume the signals in previous models: adding a SSM-like component to the model does not affect the accuracy of the final model, while the improvement when adding BM25 is small and far less than the corresponding improvements in other model architectures. While we have tailored the details of the model Match-Tensor architecture and the alternatives we studied for the search task within a specific social network, we expect that these learnings might also be adaptable to search within other domains \u2013 just as we have adapted the\nSSM style models originally developed for web-search to our setting.\nThe ability to select diverse ways of computing similarity between query and document in the form of channels in the match-tensor model layer is a general and powerful primitive. Indeed, we have only explored a few design choices within this general design space (comparing bi-LSTMs to CNNs). It is possible that increasing the diversity of these sources, using a mix of RNNs, CNNs and other notions of exact matching (by incorporating named entity linking, for example) might further strengthen the accuracy and performance of the model. These, and other explorations are deferred to future work."}, {"heading": "APPENDIX: ALTERNATIVE MODEL ARCHITECTURES WITH HYPERPARAMETERS", "text": ""}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["Mart\u0301\u0131n Abadi"], "venue": "Learning on Heterogeneous Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "From RankNet to LambdaRank to LambdaMART: An overview", "author": ["Christopher J C Burges"], "venue": "Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Learning to rank: From pairwise approach to listwise approach", "author": ["Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li"], "venue": "In 24th International Conference on Machine learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Expected reciprocal rank for graded relevance", "author": ["Olivier Chapelle", "Donald Metlzer", "Ya Zhang", "Pierre Grinspan"], "venue": "In 18th ACM Conference on Information and Knowledge Management", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Adaptability of neural networks on varying granularity IR tasks", "author": ["Daniel Cohen", "Qingyao Ai", "W Bruce Croft"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Query expansion with locally-trained word embeddings", "author": ["Fernando Diaz", "Bhaskar Mitra", "Nick Craswell"], "venue": "arXiv preprint arXiv:1605.07891", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["Jerome H Friedman"], "venue": "Annals of statistics", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Word embedding based generalized language model for information retrieval", "author": ["Debasis Ganguly", "Dwaipayan Roy", "Mandar Mitra", "Gareth JF Jones"], "venue": "In 38th International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks 18,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Search retargeting using directed query embeddings", "author": ["Mihajlo Grbovic", "Nemanja Djuric", "Vladan Radosavljevic", "Narayan Bhamidipati"], "venue": "In 24th International Conference on the World Wide Web", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Context-and contentaware embeddings for query rewriting in sponsored search", "author": ["Mihajlo Grbovic", "Nemanja Djuric", "Vladan Radosavljevic", "Fabrizio Silvestri", "Narayan Bhamidipati"], "venue": "In 38th International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "A deep relevance matching model for ad-hoc retrieval", "author": ["Jiafeng Guo", "Yixing Fan", "Qingyao Ai", "W Bruce Croft"], "venue": "In 25th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In 22nd ACM International Conference on Conference on Information & Knowledge Management", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Short text similarity with word embeddings", "author": ["Tom Kenter", "Maarten de Rijke"], "venue": "In 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Recurrent Convolutional Neural Networks for Text Classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In 29th AAAI Conference on Artificial Intelligence", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks 3361,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "Learning to rank for information retrieval", "author": ["Tie-Yan Liu"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "A deep architecture for matching short texts", "author": ["Zhengdong Lu", "Hang Li"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "The whens and hows of learning to rank for web search", "author": ["Craig Macdonald", "Rodrygo L. Santos", "Iadh Ounis"], "venue": "Information Retrieval 16,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "A markov random field model for term dependencies", "author": ["Donald Metzler", "W. Bruce Croft"], "venue": "In 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and Their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In 26th International Conference on Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Exploring session context using distributed representations of queries and reformulations", "author": ["Bhaskar Mitra"], "venue": "In 38th International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Learning to match using local and distributed representations of text for web search", "author": ["Bhaskar Mitra", "Fernando Diaz", "Nick Craswell"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Improving document ranking with dual word embeddings", "author": ["Eric Nalisnick", "Bhaskar Mitra", "Nick Craswell", "Rich Caruana"], "venue": "In 25th International Conference Companion on the World Wide Web. International World Wide Web Conferences Steering Committee,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Semantic Modelling with Long-Short-Term Memory for Information Retrieval", "author": ["Hamid Palangi", "Li Deng", "Yelong Shen", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Xinying Song", "Rabab K. Ward"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "A study of matchpyramid models on ad-hoc retrieval", "author": ["Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Xueqi Cheng"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Text Matching as Image Recognition", "author": ["Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Shengxian Wan", "Xueqi Cheng"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "author": ["Stephen E Robertson", "Steve Walker"], "venue": "In 17th Annual International ACM SIGIR Conference on Research and development in Information Retrieval", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1994}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew W Senior", "Fran\u00e7oise Beaufays"], "venue": "In Interspeech", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Gerard Salton", "Christopher Buckley"], "venue": "Information processing & management 24,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1988}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In 23rd ACM International Conference on Conference on Information and Knowledge Management", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In 23rd International Conference on the World Wide Web", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "aNMM: Ranking short answer texts with attention-based neural matching model", "author": ["Liu Yang", "Qingyao Ai", "Jiafeng Guo", "W Bruce Croft"], "venue": "In 25th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Deepintent: Learning attentions for online advertising with recurrent neural networks", "author": ["Shuangfei Zhai", "Keng-hao Chang", "Ruofei Zhang", "Zhongfei Mark Zhang"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "The learning-to-rank models in widespread use for search may require hundreds of such features to be generated for each query/document pair [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 32, "context": "In many rankers, the feature set includes classic information retrieval models, such as BM25 [37], and term dependence models [28].", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "In many rankers, the feature set includes classic information retrieval models, such as BM25 [37], and term dependence models [28].", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 15, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 26, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 28, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 35, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 36, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 38, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 12, "context": "vance, they can also fail to capture the most salient aspect of the search task, namely local relevance matching [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 30, "context": "which are used as features in traditional learning-to-rank approaches [35, 36].", "startOffset": 70, "endOffset": 78}, {"referenceID": 31, "context": "which are used as features in traditional learning-to-rank approaches [35, 36].", "startOffset": 70, "endOffset": 78}, {"referenceID": 32, "context": "We also demonstrate that this approach largely subsumes other models such as BM25[37] and SSM[17, 41]: the MatchTensor model alone performs nearly as well as meta-models trained with both Match-Tensor and these features as input.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "We also demonstrate that this approach largely subsumes other models such as BM25[37] and SSM[17, 41]: the MatchTensor model alone performs nearly as well as meta-models trained with both Match-Tensor and these features as input.", "startOffset": 93, "endOffset": 101}, {"referenceID": 36, "context": "We also demonstrate that this approach largely subsumes other models such as BM25[37] and SSM[17, 41]: the MatchTensor model alone performs nearly as well as meta-models trained with both Match-Tensor and these features as input.", "startOffset": 93, "endOffset": 101}, {"referenceID": 2, "context": "using a learning-to-rank approach [3, 4] to predict the labels on training data:", "startOffset": 34, "endOffset": 40}, {"referenceID": 3, "context": "using a learning-to-rank approach [3, 4] to predict the labels on training data:", "startOffset": 34, "endOffset": 40}, {"referenceID": 32, "context": "Furthermore, it is standard to include classic information retrieval models in this feature set, particularly BM25 [37].", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "Liu [25] provides a thorough overview of traditional learningto-rank methods for search.", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "[27] cover many of the engineering issues associated with deploying learning-to-rank in a search engine.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] introduced the first Deep Neural Network architectures for Web search that operated on (query, title) pairs, using a so-called siamese architecture [23], in which two feed-forward networks NNQ and NND map the query q and the title of a given web document d, respectively, into fixed-length representations:", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[17] introduced the first Deep Neural Network architectures for Web search that operated on (query, title) pairs, using a so-called siamese architecture [23], in which two feed-forward networks NNQ and NND map the query q and the title of a given web document d, respectively, into fixed-length representations:", "startOffset": 153, "endOffset": 157}, {"referenceID": 36, "context": "[41] marks the next notable advancement using the same siamese architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "convolutional networks, more recent approaches have used Recurrent Neural Networks (RNNs), especially those based on Long Short-term Memory (LSTM) units [16].", "startOffset": 153, "endOffset": 157}, {"referenceID": 37, "context": "A popular architecture for machine translation uses the so-called sequence-to-sequence paradigm in which the input text in the source language is \u201cencoded\u201d using an encoder network to produce a fixed-length representation (the RNN state)[42].", "startOffset": 237, "endOffset": 241}, {"referenceID": 14, "context": "[17] and Shen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[41], the use of RNNs such as those based on LSTMs is critical to their performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": ", focusing attention) on various elements of the source representation during the decoding process, and they have demonstrated considerable improvements over their non-attention counterparts [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 12, "context": "The \u201crepresentation-based\u201c nature of siamese architectures has also been identified as a limitation in search [14] and has led to the development of alternate \u201cinteraction-based\u201d architectures, in which the relationships between query and", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "[36] construct an interaction matrix between query and document terms, where each entry in the matrix denotes the strength of similarity between the corresponding terms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[32], that appeared while this manuscript was being prepared, uses a \u201cduet\u201d architecture in which two separate networks (one", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "that are not [39].", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "The word embedding table is itself computed offline from a large corpus of social media documents using the word2vec package [30] in an unsupervised manner and are held fixed during the training of the Match-Tensor network.", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "e, bi-LSTMs) [11, 16], then encode the query (respectively document) word-embedding sequence into a sequence of LSTM states.", "startOffset": 13, "endOffset": 21}, {"referenceID": 13, "context": "e, bi-LSTMs) [11, 16], then encode the query (respectively document) word-embedding sequence into a sequence of LSTM states.", "startOffset": 13, "endOffset": 21}, {"referenceID": 33, "context": "[38], but the best models did", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 15, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 26, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 28, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 35, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 36, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 38, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 30, "context": "[35, 36]: Match", "startOffset": 0, "endOffset": 8}, {"referenceID": 31, "context": "[35, 36]: Match", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[14] developed", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "al [32] report that a model incorporating an exact-match channel with a representation based \u201cdistributed\u201d model outperforms DRMM on a larger collection of websearch queries.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "[8] who use them in query expansions, Ganguly et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] who use them in smoothing language models, Nalisnick et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[33] who propose dual embeddings and Grbovic et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12, 13] who use them in sponsored search.", "startOffset": 0, "endOffset": 8}, {"referenceID": 11, "context": "[12, 13] who use them in sponsored search.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "[6] have also studied the utility of DNNs for several IR tasks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "TensorFlow [1].", "startOffset": 11, "endOffset": 14}, {"referenceID": 24, "context": "We used pre-trained 256-dimensional phrase embeddings using the word2vec package [29] on a large corpus of documents with a vocabulary size of around 2 million tokens containing unigrams and selected bigrams.", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "We employed the Adam optimizer for gradient descent [20] with a learning rate of 0.", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "While some architectures have been suggested for short text common in some social media[26, 36], studies indicate that they do not beat baseline models such as BM25[35].", "startOffset": 87, "endOffset": 95}, {"referenceID": 31, "context": "While some architectures have been suggested for short text common in some social media[26, 36], studies indicate that they do not beat baseline models such as BM25[35].", "startOffset": 87, "endOffset": 95}, {"referenceID": 30, "context": "While some architectures have been suggested for short text common in some social media[26, 36], studies indicate that they do not beat baseline models such as BM25[35].", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "In contrast, both early models [17, 41] and recent developments by Mitra et.", "startOffset": 31, "endOffset": 39}, {"referenceID": 36, "context": "In contrast, both early models [17, 41] and recent developments by Mitra et.", "startOffset": 31, "endOffset": 39}, {"referenceID": 26, "context": "al that has shown strong performance [31] have been designed for web-search and are not directly usable in our setting.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "We constructed an model using the siamese network architecture based on the semantic similarity models (SSM) appearing in other work [17, 34, 41].", "startOffset": 133, "endOffset": 145}, {"referenceID": 29, "context": "We constructed an model using the siamese network architecture based on the semantic similarity models (SSM) appearing in other work [17, 34, 41].", "startOffset": 133, "endOffset": 145}, {"referenceID": 36, "context": "We constructed an model using the siamese network architecture based on the semantic similarity models (SSM) appearing in other work [17, 34, 41].", "startOffset": 133, "endOffset": 145}, {"referenceID": 18, "context": "mation all the way to the end of the document [22].", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "prepared, Mitra et al [32] show that a combination of local and distributed matching can outperform other models for web-search.", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "[45] where attention-based pooling", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The figure reports NDCG at various levels as well as Expected Reciprocal Rank (ERR) [5], with all measures computed using the three relevance grades.", "startOffset": 84, "endOffset": 87}, {"referenceID": 12, "context": "This result is consistent with [14] and others who have highlighted the limitation of models that only match semantic representations in relevance-matching tasks.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "This outcome is not surprising given similar results in other language modeling tasks and is consistent with gains in NDCG observed in [34] in going from convolutional to bi-LSTM-based semantic similarity models.", "startOffset": 135, "endOffset": 139}, {"referenceID": 7, "context": "To determine if a deep relevance model is indeed capturing all essential relevance matching signals, we trained ensemble models: boosted trees [9] that combine as inputs the neural model\u2019s output as a feature and a BM25 feature using 5fold cross-validation on the existing validation set.", "startOffset": 143, "endOffset": 146}, {"referenceID": 17, "context": "ments in several domains were focused on the absolute accuracy [21, 42] of these models when compared to alternatives, the focus has more recently gravitated towards the completeness of these models; indeed in several domains such as speech", "startOffset": 63, "endOffset": 71}, {"referenceID": 37, "context": "ments in several domains were focused on the absolute accuracy [21, 42] of these models when compared to alternatives, the focus has more recently gravitated towards the completeness of these models; indeed in several domains such as speech", "startOffset": 63, "endOffset": 71}, {"referenceID": 12, "context": "However, this general class of models appears to miss critical relevance-matching signals [14].", "startOffset": 90, "endOffset": 94}], "year": 2017, "abstractText": "The application of Deep Neural Networks for ranking in search engines may obviate the need for the extensive feature engineering common to current learning-to-rank methods. However, we show that combining simple relevance matching features like BM25 with existing Deep Neural Net models often substantially improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel deep Recurrent Neural Net-based model that we call Match-Tensor. The architecture of the Match-Tensor model simultaneously accounts for both local relevance matching and global topicality signals allowing for a rich interplay between them when computing the relevance of a document to a query. On a large held-out test set consisting of social media documents, we demonstrate not only that Match-Tensor outperforms BM25 and other classes of DNNs but also that it largely subsumes signals present in these models.", "creator": "LaTeX with hyperref package"}}}