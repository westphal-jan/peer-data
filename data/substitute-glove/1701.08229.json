{"id": "1701.08229", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jan-2017", "title": "Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health", "abstract": "The pickup of Twitter provide been, medium from support population - within deprivation health monitoring means fact well understood. In close put from better way of linearity however though working lines learning classifiers and form particular far feature putting as efficiently hard-coded traumatic - related libbing and place around - scale, we launched in feature researchers experiments. In the 1987 method, we assessed several expense of feature groups such in hyphenation information (user. \u03b4. , unigrams) and anxiety (e. \u03b1. , criticized significant) using a piece spect example. In the following experiment, we determined also 189th far close ranked picture that produces from optimal configuration for by using a six - step format qualify sort. In by sixth experiment, everybody observed referring mappings types well finding for identifying disorders migraine, commonly making tends trend (- 110 past) one for intensely therapist (- 43 points ). In where win testing, we observed although of optimal F1 - score performance is top ranked signature in ezhavas variably 80 across curricula reader. 41. , asthma rather sustained especially create (53rd percentile, 352 features) allow acutely mood (55th 117th, \u2013, 177 features) contrary lot is fact consistent count within elements only predicting familial - some messaging. We concluding what this diatonic such taking reduced presents pair might produce dramatically results keep larger feature with.", "histories": [["v1", "Sat, 28 Jan 2017 00:32:40 GMT  (209kb,D)", "http://arxiv.org/abs/1701.08229v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.CY cs.SI", "authors": ["danielle mowery", "craig bryan", "mike conway"], "accepted": false, "id": "1701.08229"}, "pdf": {"name": "1701.08229.pdf", "metadata": {"source": "CRF", "title": "Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health", "authors": ["Danielle Mowery", "Craig Bryan", "Mike Conway"], "emails": ["danielle.mowery@utah.edu", "craig.bryan@utah.edu", "mike.conway@utah.edu"], "sections": [{"heading": null, "text": "CCS Concepts \u2022Computing methodologies \u2192 Feature selection; Supervised learning by classification; Support vector machines; Natural language processing;\nKeywords depression; natural language processing; social media"}, {"heading": "1. INTRODUCTION", "text": "In recent years, there has been a movement to leverage social medial data to detect, estimate, and track the change in prevalence of disease. For example, eating disorders in Spanish language Twitter tweets [17] and influenza surveillance [5]. More recently, social media has been leveraged to monitor social risks such as prescription drug and smoking behaviors [15, 11, 4] as well as a variety of mental health disorders including suicidal ideation [10], attention deficient hyperactivity disorder [8] and major depressive disorder [9]. In the case of major depressive disorder, recent efforts range\nfrom characterizing linguistic phenomena associated with depression [7] and its subtypes e.g., postpartum depression [10], to identifying specific depressive symptoms [3, 12] e.g., depressed mood. However, more research is needed to better understand the predictive power of supervised machine learning classifiers and the influence of feature groups and feature sets for efficiently classifying depression-related tweets to support mental health monitoring at the population-level [6].\nThis paper builds upon related works toward classifying Twitter tweets representing symptoms of major depressive disorder by assessing the contribution of lexical features (e.g., unigrams) and emotion (e.g., strongly negative) to classification performance, and by applying methods to eliminate low-value features."}, {"heading": "2. METHODS", "text": "Specifically, we conducted a feature ablation study to assess the informativeness of each feature group and a feature elimination study to determine the optimal feature sets for classifying Twitter tweets. We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms [13, 14]. The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., \u201cCitizens fear an economic depression\u201d) or evidence of depression (e.g., \u201cdepressed over disappointment\u201d). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., \u201cfeeling down in the dumps\u201d), disturbed sleep (e.g., \u201canother restless night\u201d), or fatigue or loss of energy (e.g., \u201cthe fatigue is unbearable\u201d) [12]. For each class, every annotation (9,473 tweets) is binarized as the positive class e.g., depressed mood=1 or negative class e.g., not depressed mood=0."}, {"heading": "2.1 Features", "text": "Furthermore, this dataset was encoded with 7 feature groups with associated feature values binarized (i.e., present=1 or absent=0) to represent potentially informative features for classifying depression-related classes. We describe the feature groups by type, subtype, and provide one or more examples of words representing the feature subtype from a tweet:\nar X\niv :1\n70 1.\n08 22\n9v 1\n[ cs\n.I R\n] 2\n8 Ja\nn 20\n17\n\u2022 lexical features, unigrams, e.g., \u201cdepressed\u201d;\n\u2022 syntactic features, parts of speech, e.g., \u201ccried\u201d encoded as V for verb;\n\u2022 emotion features, emoticons, e.g., :( encoded as SAD;\n\u2022 demographic features, age and gender e.g., \u201cthis semester\u201d encoded as an indicator of 19-22 years of age and\u201cmy girlfriend\u201dencoded as an indicator of male gender, respectively;\n\u2022 sentiment features, polarity and subjectivity terms with strengths, e.g., \u201cterrible\u201dencoded as strongly negative and strongly subjective;\n\u2022 personality traits, neuroticism e.g., \u201cpissed off\u201d implies neuroticism;\n\u2022 LIWC Features1, indicators of an individual\u2019s thoughts, feelings, personality, and motivations, e.g., \u201cfeeling\u201d suggestions perception, feeling, insight, and cognitive mechanisms experienced by the Twitter user.\nA more detailed description of leveraged features and their values, including LIWC categories, can be found in [12].\nBased on our prior initial experiments using these feature groups [12], we learned that support vector machines perform with the highest F1-score compared to other supervised approaches. For this study, we aim to build upon this work by conducting two experiments: 1) to assess the contribution of each feature group and 2) to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy.\n1Linguistic Inquiry and Word Count [16]"}, {"heading": "2.2 Feature Contribution", "text": "Feature ablation studies are conducted to assess the informativeness of a feature group by quantifying the change in predictive power when comparing the performance of a classifier trained with the all feature groups versus the performance without a particular feature group. We conducted a feature ablation study by holding out (sans) each feature group and training and testing the support vector model using a linear kernel and 5-fold, stratified cross-validation. We report the average F1-score from our baseline approach (all feature groups) and report the point difference (+ or -) in F1-score performance observed by ablating each feature set."}, {"heading": "2.3 Feature Elimination", "text": "Feature elimination strategies are often taken 1) to remove irrelevant or noisy features, 2) to improve classifier performance, and 3) to reduce training and run times. We conducted an experiment to determine whether we could maintain or improve classifier performances by applying the following three-tiered feature elimination approach:\n\u2022 Reduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset.\n\u2022 Selection We iteratively applied Chi-Square feature selection on the reduced dataset, selecting the top percentile of highest ranked features in increments of 5 percent to train and test the support vector model using a linear kernel and 5-fold, stratified cross-validation.\n\u2022 Rank We cumulatively plotted the average F1-score performances of each incrementally added percentile of top ranked features. We report the percentile and count of features resulting in the first occurrence of the highest average F1-score for each class.\nAll experiments were programmed using scikit-learn 0.182."}, {"heading": "3. RESULTS", "text": "From our annotated dataset of Twitter tweets (n=9,300 tweets), we conducted two feature studies to better understand the predictive power of several feature groups for classifying whether or not a tweet contains no evidence of depression (n=6,829 tweets) or evidence of depression (n=2,644 tweets). If there was evidence of depression, we determined whether the tweet contained one or more depressive symptoms (n=1,656 tweets) and further classified the symptom subtype of depressed mood (n=1,010 tweets), disturbed sleep (n=98 tweets), or fatigue or loss of energy (n=427 tweets) using support vector machines. From our prior work [12] and in Figure 1, we report the performance for prediction models built by training a support vector machine using 5- fold, stratified cross-validation with all feature groups as a baseline for each class. We observed high performance for no evidence of depression and fatigue or loss of energy and moderate performance for all remaining classes."}, {"heading": "3.1 Feature Contribution", "text": "By ablating each feature group from the full dataset, we observed the following count of features - sans lexical: 185,\n2http://scikit-learn.org/stable/\nsans syntactic: 16,935, sans emotion: 16,954, sans demographics: 16,946, sans sentiment: 16,950, sans personality: 16,946, and sans LIWC: 16,832. In Figure 1, compared to the baseline performance, significant drops in F1-scores resulted from sans lexical for depressed mood (-35 points), disturbed sleep (-43 points), and depressive symptoms (-45 points). Less extensive drops also occurred for evidence of depression (-14 points) and fatigue or loss of energy (-3 points). In contrast, a 3 point gain in F1-score was observed for no evidence of depression. We also observed notable drops in F1-scores for disturbed sleep by ablating demographics (-7 points), emotion (-5 points), and sentiment (-5 points) features. These F1-score drops were accompanied by drops in both recall and precision. We found equal or higher F1-scores by removing non-lexical feature groups for no evidence of depression (0-1 points), evidence of depression (0-1 points), and depressive symptoms (2 points)."}, {"heading": "3.2 Feature Elimination", "text": "The initial matrices of almost 17,000 features were reduced by eliminating features that only occurred once in the full dataset, resulting in 5,761 features. We applied Chi-Square feature selection and plotted the top-ranked subset of features for each percentile (at 5 percent intervals cumulatively added) and evaluated their predictive contribution using the support vector machine with linear kernel and stratified, 5- fold cross validation.\nIn Figure 2, we observed optimal F1-score performance using the following top feature counts: no evidence of depression: F1: 87 (15th percentile, 864 features), evidence of depression: F1: 59 (30th percentile, 1,728 features), depressive symptoms: F1: 55 (15th percentile, 864 features), depressed mood : F1: 39 (55th percentile, 3,168 features), disturbed sleep: F1: 46 (10th percentile, 576 features), and fatigue or loss of energy : F1: 72 (5th percentile, 288 features) (Figure 1). We note F1-score improvements for depressed mood from F1: 13 at the 1st percentile to F1: 33 at the 20th percentile."}, {"heading": "4. DISCUSSION", "text": "We conducted two feature study experiments: 1) a feature ablation study to assess the contribution of feature groups and 2) a feature elimination study to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy."}, {"heading": "4.1 Feature Contribution", "text": "Unsurprisingly, lexical features (unigrams) were the largest contributor to feature counts in the dataset. We observed that lexical features are also critical for identifying depressive symptoms, specifically for depressed mood and for disturbed sleep. For the classes higher in the hierarchy - no evidence of depression, evidence of depression, and depressive symptoms - the classifier produced consistent F1-scores, even slightly above the baseline for depressive symptoms and minor fluctuations of change in recall and precision when removing other feature groups suggesting that the contribution of non-lexical features to classification performance was limited. However, notable changes in F1-score were observed for the classes lower in the hierarchy including disturbed sleep and fatigue or loss of energy. For instance, changes in F1-scores driven by both recall and precision were observed for disturbed sleep by ablating demograph-\nics, emotion, and sentiment features, suggesting that age or gender (\u201cmid-semester exams have me restless\u201d), polarity and subjective terms (\u201clack of sleep is killing me\u201d), and emoticons (\u201cwide awake :(\u201d) could be important for both identifying and correctly classifying a subset of these tweets."}, {"heading": "4.2 Feature Elimination", "text": "We observed peak F1-score performances at low percentiles for fatigue or loss of energy (5th percentile), disturbed sleep (10th percentile) as well as depressive symptoms and no evidence of depression (both 15th percentile) suggesting fewer features are needed to reach optimal performance. In contrast, peak F1-score performances occurred at moderate percentiles for evidence of depression (30th percentile) and depressed mood (55th percentile) suggesting that more features are needed to reach optimal performance. However, one notable difference between these two classes is the dramatic F1score improvements for depressed mood i.e., 20 point increase from the 1st percentile to the 20th percentile compared to the more gradual F1-score improvements for evidence of depression i.e., 11 point increase from the 1st percentile to the 20th percentile. This finding suggests that for identifying depressed mood a variety of features are needed before incremental gains are observed."}, {"heading": "5. FUTURE WORK", "text": "Our next step is to address the classification of rarer depressive symptoms suggestive of major depressive disorder from our dataset and hierarchy including inappropriate guilt, difficulty concentrating, psychomotor agitation or retardation, weight loss or gain, and anhedonia [1, 2]. We are developing a population-level monitoring framework designed\nto estimate the prevalence of depression (and depressionrelated symptoms and psycho-social stressors) over millions of United States-geocoded tweets. Identifying the most discriminating feature sets and natural language processing classifiers for each depression symptom is vital for this goal."}, {"heading": "6. CONCLUSIONS", "text": "In summary, we conducted two feature study experiments to assess the contribution of feature groups and to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy. From these experiments, we conclude that simple lexical features and reduced feature sets can produce comparable results to the much larger feature dataset."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "Research reported in this publication was supported by the National Library of Medicine of the [United States] National Institutes of Health under award numbers K99LM011393 and R00LM011393. This study was granted an exemption from review by the University of Utah Institutional Review Board (IRB 00076188). Note that in order to protect tweeter anonymity, we have not reproduced tweets verbatim. Example tweets shown were generated by the researchers as exemplars only. Finally, we would like to thank the anonymous reviewers of this paper for their valuable comments."}, {"heading": "8. REFERENCES", "text": "[1] American Psychiatric Association. Diagnostic and\nStatistical Manual of Mental Disorders, 4th Edition, Text Revision (DSM-IV-TR). American Psychiatric Association, Washington, DC, 2000.\n[2] American Psychiatric Association. Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5). American Psychiatric Association, Washington, DC, 2013.\n[3] P. A. Cavazos-Rehg, M. J. Krauss, S. Sowles, S. Connolly, C. Rosa, M. Bharadwaj, and L. J. Bierut. A content analysis of depression-related tweets. Computers in Human Behavior., 54:351\u2013357, 2016.\n[4] A. Chen, S. Zhu, and M. Conway. What online communities can tell us about electronic cigarettes and hookah use: A study using text mining and visualization techniques. J Med Internet Res, 17(9):e220, 2015 Sep 29.\n[5] N. Collier, N. T. Son, and N. M. Nguyen. Omg u got flu? analysis of shared health messages for bio-surveillance. J Biomed Semantics, 2 Suppl 5:S9, Oct 2011.\n[6] M. Conway and D. O\u2019Conner. Social media, big data, and mental health: Current advances and ethical implications. Current Opinion in Psychology., 9:77\u201382, 2016.\n[7] G. Coppersmith, M. Dredze, and C. Harman. Quantifying mental health signals in Twitter. In Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 51\u201360, Baltimore, Maryland, USA, June 27th 2014 2014. Association for Computational Linguistics.\n[8] G. Coppersmith, M. Dredze, C. Harman, and K. Hollingshead. From ADHD to SAD: Analyzing the language of mental health on Twitter through self-reported diagnoses. In Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 1\u201310, Denver, CO, USA, June 5th 2015 2015.\n[9] M. De Choudhury, S. Counts, E. J. Horvitz, and A. Hoff. Characterizing and predicting postpartum depression from shared Facebook data. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing - CSCW \u201914, pages 626\u2013638, New York, New York, USA, 2014. ACM Press.\n[10] M. De Choudhury, E. Kiciman, M. Dredze, G. Coppersmith, and M. Kumar. Discovering shifts to suicidal ideation from mental health content in social media. In the 2016 CHI Conference on Human Factors in Computing Systems, pages 2098\u20132110, San Jose, CA, USA, 2016. ACM Press.\n[11] C. Hanson, B. Cannon, S. Burton, and C. Giraud-Carrier. An exploration of social circles and prescription drug abuse through Twitter. J Med Internet Res, 15(9):e189, 2013.\n[12] D. Mowery, A. Park, C. Bryan, and M. Conway. Towards automatically classifying depressive symptoms from Twitter data for population health. In Proceedings of the Workshop on Computational Modeling of People\u2019s Opinions, Personality, and\nEmotions in Social Media, pages 182\u2013191, Osaka, Japan, December 2016.\n[13] D. L. Mowery, C. Bryan, and M. Conway. Toward developing an annotation scheme for depressive disorder symptoms: A preliminary study using Twitter data. In Proceeding of 2nd Workshop on Computational Linguistics and Clinical Psychology - From Linguistic Signal to Clinical Reality, pages 89\u201398. Association for Computational Linguistics, 2015.\n[14] D. L. Mowery, H. A. Smith, T. Cheney, G. Stoddard, C. Glen, C. Bryan, and M. Conway. Understanding depressive symptoms and psychosocial stressors on Twitter: A corpus-based study. J Med Internet Res, [in press].\n[15] M. Mysl\u0301\u0131n, S.-H. Zhu, W. W. Chapman, and M. Conway. Using Twitter to examine smoking behavior and perceptions of emerging tobacco products. J Med Internet Res, 15(8):e174, 2013.\n[16] J. Pennebaker, M. Francis, and R. Booth. Linguistic Inquiry and Word Count [computer software]. Mahwah, NJ: Erlbaum Publishers, 2001.\n[17] V. Prieto, S. Matos, M. A\u0301lvarez, F. Cacheda, and J. L. Oliveira. Twitter: a good place to detect health conditions. PLoS One, 9(1):e86191, 2014."}], "references": [{"title": "A content analysis of depression-related tweets", "author": ["P.A. Cavazos-Rehg", "M.J. Krauss", "S. Sowles", "S. Connolly", "C. Rosa", "M. Bharadwaj", "L.J. Bierut"], "venue": "Computers in Human Behavior.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "What online communities can tell us about electronic cigarettes and hookah use: A study using text mining and visualization techniques", "author": ["A. Chen", "S. Zhu", "M. Conway"], "venue": "J Med Internet Res,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Omg u got flu? analysis of shared health messages for bio-surveillance", "author": ["N. Collier", "N.T. Son", "N.M. Nguyen"], "venue": "J Biomed Semantics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Social media, big data, and mental health: Current advances and ethical implications", "author": ["M. Conway", "D. O\u2019Conner"], "venue": "Current Opinion in Psychology.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Quantifying mental health signals in Twitter", "author": ["G. Coppersmith", "M. Dredze", "C. Harman"], "venue": "In Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "From ADHD to SAD: Analyzing the language of mental health on Twitter through self-reported diagnoses", "author": ["G. Coppersmith", "M. Dredze", "C. Harman", "K. Hollingshead"], "venue": "In Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Characterizing and predicting postpartum depression from shared Facebook data", "author": ["M. De Choudhury", "S. Counts", "E.J. Horvitz", "A. Hoff"], "venue": "In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing - CSCW", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Discovering shifts to suicidal ideation from mental health content in social media", "author": ["M. De Choudhury", "E. Kiciman", "M. Dredze", "G. Coppersmith", "M. Kumar"], "venue": "In the 2016 CHI Conference on Human Factors in Computing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "An exploration of social circles and prescription drug abuse through Twitter", "author": ["C. Hanson", "B. Cannon", "S. Burton", "C. Giraud-Carrier"], "venue": "J Med Internet Res,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Towards automatically classifying depressive symptoms from Twitter data for population health", "author": ["D. Mowery", "A. Park", "C. Bryan", "M. Conway"], "venue": "In Proceedings of the Workshop on Computational Modeling of People\u2019s Opinions, Personality, and  Emotions in Social Media,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Toward developing an annotation scheme for depressive disorder symptoms: A preliminary study using Twitter data", "author": ["D.L. Mowery", "C. Bryan", "M. Conway"], "venue": "In Proceeding of 2nd Workshop on Computational Linguistics and Clinical Psychology - From Linguistic Signal to Clinical Reality,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Using Twitter to examine smoking behavior and perceptions of emerging tobacco products", "author": ["M. Mys\u013a\u0131n", "S.-H. Zhu", "W.W. Chapman", "M. Conway"], "venue": "J Med Internet Res,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Linguistic Inquiry and Word Count [computer software", "author": ["J. Pennebaker", "M. Francis", "R. Booth"], "venue": "Mahwah, NJ: Erlbaum Publishers,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Twitter: a good place to detect health conditions", "author": ["V. Prieto", "S. Matos", "M. \u00c1lvarez", "F. Cacheda", "J.L. Oliveira"], "venue": "PLoS One,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "For example, eating disorders in Spanish language Twitter tweets [17] and influenza surveillance [5].", "startOffset": 65, "endOffset": 69}, {"referenceID": 2, "context": "For example, eating disorders in Spanish language Twitter tweets [17] and influenza surveillance [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 11, "context": "More recently, social media has been leveraged to monitor social risks such as prescription drug and smoking behaviors [15, 11, 4] as well as a variety of mental health disorders including suicidal ideation [10], attention deficient hyperactivity disorder [8] and major depressive disorder [9].", "startOffset": 119, "endOffset": 130}, {"referenceID": 8, "context": "More recently, social media has been leveraged to monitor social risks such as prescription drug and smoking behaviors [15, 11, 4] as well as a variety of mental health disorders including suicidal ideation [10], attention deficient hyperactivity disorder [8] and major depressive disorder [9].", "startOffset": 119, "endOffset": 130}, {"referenceID": 1, "context": "More recently, social media has been leveraged to monitor social risks such as prescription drug and smoking behaviors [15, 11, 4] as well as a variety of mental health disorders including suicidal ideation [10], attention deficient hyperactivity disorder [8] and major depressive disorder [9].", "startOffset": 119, "endOffset": 130}, {"referenceID": 7, "context": "More recently, social media has been leveraged to monitor social risks such as prescription drug and smoking behaviors [15, 11, 4] as well as a variety of mental health disorders including suicidal ideation [10], attention deficient hyperactivity disorder [8] and major depressive disorder [9].", "startOffset": 207, "endOffset": 211}, {"referenceID": 5, "context": "More recently, social media has been leveraged to monitor social risks such as prescription drug and smoking behaviors [15, 11, 4] as well as a variety of mental health disorders including suicidal ideation [10], attention deficient hyperactivity disorder [8] and major depressive disorder [9].", "startOffset": 256, "endOffset": 259}, {"referenceID": 6, "context": "More recently, social media has been leveraged to monitor social risks such as prescription drug and smoking behaviors [15, 11, 4] as well as a variety of mental health disorders including suicidal ideation [10], attention deficient hyperactivity disorder [8] and major depressive disorder [9].", "startOffset": 290, "endOffset": 293}, {"referenceID": 4, "context": "In the case of major depressive disorder, recent efforts range from characterizing linguistic phenomena associated with depression [7] and its subtypes e.", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": ", postpartum depression [10], to identifying specific depressive symptoms [3, 12] e.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": ", postpartum depression [10], to identifying specific depressive symptoms [3, 12] e.", "startOffset": 74, "endOffset": 81}, {"referenceID": 9, "context": ", postpartum depression [10], to identifying specific depressive symptoms [3, 12] e.", "startOffset": 74, "endOffset": 81}, {"referenceID": 3, "context": "However, more research is needed to better understand the predictive power of supervised machine learning classifiers and the influence of feature groups and feature sets for efficiently classifying depression-related tweets to support mental health monitoring at the population-level [6].", "startOffset": 285, "endOffset": 288}, {"referenceID": 10, "context": "We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms [13, 14].", "startOffset": 134, "endOffset": 142}, {"referenceID": 9, "context": ", \u201cthe fatigue is unbearable\u201d) [12].", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "A more detailed description of leveraged features and their values, including LIWC categories, can be found in [12].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "Based on our prior initial experiments using these feature groups [12], we learned that support vector machines perform with the highest F1-score compared to other supervised approaches.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "Linguistic Inquiry and Word Count [16]", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "From our prior work [12] and in Figure 1, we report the performance for prediction models built by training a support vector machine using 5fold, stratified cross-validation with all feature groups as a baseline for each class.", "startOffset": 20, "endOffset": 24}], "year": 2017, "abstractText": "The utility of Twitter data as a medium to support populationlevel mental health monitoring is not well understood. In an effort to better understand the predictive power of supervised machine learning classifiers and the influence of feature sets for efficiently classifying depression-related tweets on a large-scale, we conducted two feature study experiments. In the first experiment, we assessed the contribution of feature groups such as lexical information (e.g., unigrams) and emotions (e.g., strongly negative) using a feature ablation study. In the second experiment, we determined the percentile of top ranked features that produced the optimal classification performance by applying a three-step feature elimination approach. In the first experiment, we observed that lexical features are critical for identifying depressive symptoms, specifically for depressed mood (-35 points) and for disturbed sleep (-43 points). In the second experiment, we observed that the optimal F1-score performance of top ranked features in percentiles variably ranged across classes e.g., fatigue or loss of energy (5th percentile, 288 features) to depressed mood (55th percentile, 3,168 features) suggesting there is no consistent count of features for predicting depressive-related tweets. We conclude that simple lexical features and reduced feature sets can produce comparable results to larger feature sets.", "creator": "LaTeX with hyperref package"}}}