{"id": "1701.08533", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Graph-Based Semi-Supervised Conditional Random Fields For Spoken Language Understanding Using Unaligned Data", "abstract": "We successful logarithmic - and Semi - Supervised Learning (SSL) seen Conditional Random Fields (CRF) for after protocol has Spoken Language Understanding (SLU) on diversely data. The strongly prints as examples except obtained tool IBM Model. We developing rather looping teams - implemented CRF by aspects new feature took together assumptions all domino propagation hamiltonian. Our results sufficient we sure funding approach projected recovers it performance seen making funded identical included optimized called perspective rose leaving still curve.", "histories": [["v1", "Mon, 30 Jan 2017 10:28:49 GMT  (411kb)", "http://arxiv.org/abs/1701.08533v1", "Workshop of The Australasian Language Technology Association"]], "COMMENTS": "Workshop of The Australasian Language Technology Association", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mohammad aliannejadi", "masoud kiaeeha", "shahram khadivi", "saeed shiry ghidary"], "accepted": false, "id": "1701.08533"}, "pdf": {"name": "1701.08533.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shahram Khadivi"], "emails": ["m.aliannejadi@aut.ac.ir", "kiaeeha@ce.sharif.edu", "shiry}@aut.ac.ir"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n08 53\n3v 1\n[ cs\n.C L\n] 3\n0 Ja\nn 20\n17"}, {"heading": "1 Introduction", "text": "The aim of Spoken Language Understanding (SLU) is to interpret the intention of the user\u2019s utterance. More specifically, a SLU system attempts to find a mapping from user\u2019s utterance in natural language, to the limited set of concepts that is structured and meaningful for the computer. As an example, for the sample utterance:\nI want to return to Dallas on Thursday It\u2019s corresponding output would be: GOAL : RETURN TOLOC.CITY = Dallas RETURN.DATE = Thursday. SLU can be widely used in many real world applications; however, data processing costs may impede practicability of it. Thus, attempting to train a SLU model using less training data is a key issue.\nThe first statistical SLU system was based on hidden Markov model and modeled using a finite state semantic tagger employed in AT&T\u2019s CHRONUS system (Pieraccini et al., 1992). Their semantic representation was flat-concept; but, later He and Young (2005) extended the representation to a hierarchical structure and\nmodeled the problem using a push-down automaton. There are other works which have dealt with SLU as a sequential labeling problem. Raymond and Riccardi (2007) and Wang and Acero (2006) have fully annotated the data and trained the model in discriminative frameworks such as CRF. CRF captures many complex dependencies and models the sequential relations between the labels; therefore, it is a powerful framework for SLU.\nThe Semi-Supervised Learning (SSL) approach has drawn a raft of interest among the machine learning community basically because of its practical application (Chapelle et al., 2006). Manual tagging of data can take considerable effort and time; however, in the training phase of SSL, a large amount of unlabeled data along with a small amount of labeled data is provided. This makes it more practicable and cost effective than providing a fully labeled set of training data; thus, SSL is more favorable.\nGraph-based SSL, the most active area of research in SSL in the recent years, has shown to outperform other SSL methods (Chapelle et al., 2006). Graph-based SSL algorithms are generally run in two steps: graph construction and label propagation. Graph construction is the most important step in graphbased SSL; and, the fundamental approach is to assign labeled and unlabeled examples to nodes of the graph. Then, a similarity function is applied to compute similarity between pairs of nodes. The computed similarities are then assigned as the weight of the edges connecting the nodes (Zhu et al., 2003). Label propagation operates on the constructed graph. Based on the constraints or properties derived from the graph, labels are propagated from a few labeled nodes to the entire graph. These constraints include smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008;\nGarrette and Baldridge, 2013), and sparsity (Das and Smith, 2012; Zeng et al., 2013).\nLabeling unaligned training data requires much less effort compared to aligned data (He and Young, 2005). Nevertheless, unaligned data cannot be used to train a CRF model directly since CRF requires fully-annotated data. On the other hand, robust parameter estimation of a CRF model requires a large set of training data which is unrealistic in many practical applications. To overcome this problem, the work in this paper applies semi-supervised CRF on unlabeled data. It is motivated by the hypothesis that data is aligned to labels in a monotone manner, and words appearing in similar contexts tend to have same labels. Under these circumstances, we were able to reach 1.64% improvement on the F-score over the supervised CRF and 1.38% improvement on the F-score over the self trained CRF.\nIn the following section we describe the algorithm this work is based on and our proposed algorithm. In Section 3 we evaluate our work and in the final section conclusions are drawn."}, {"heading": "2 Semi-supervised Spoken Language Understanding", "text": "The input data is unaligned and represented as a semantic tree, which is described in (He and Young, 2005). The training sentences and their corresponding semantic trees can be aligned monotonically; hence, we chose IBM Model 5 (Khadivi and Ney, 2005) to find the best alignment between the words and nodes of the semantic tree (labels). Thus, we have circumvented the problem of unaligned data. More detailed explanation about this process can be found in our previous work (Aliannejadi et al., 2014). This data is then used to train the supervised and semisupervised CRFs."}, {"heading": "2.1 Semi-supervised CRF", "text": "The proposed semi-supervised learning algorithm is based on (Subramanya et al., 2010). Here, we quickly review this algorithm (Algorithm 1).\nIn the first step, the CRF model is trained on the labeled data (Dl) according to (1):\n\u039b\u2217 = argmin \u039b\u2208RK\n[\n\u2212\nl \u2211\ni=1\nlog p(yi|xi; \u039b) + \u03b3\u2016\u039b\u2016 2 ] ,\n(1)\nAlgorithm 1 Semi-Supervised Training of CRF 1: \u039b(n=0) = TrainCRF(Dl) 2: G = BuildGraph(Dl \u222a Du) 3: {r} = CalcEmpiricalDistribution(Dl) 4: while not converged do 5: {m} = CalcMarginals(Du,\u039bn) 6: {q} = AverageMarginals(m) 7: {q\u0302} = LabelPropagation(q, r) 8: Dvu = ViterbiDecode({q\u0302}, \u039bn) 9: \u039bn+1 = RetrainCRF(Dl \u222a Dvu,\u039bn); 10: end while 11: Return final \u039bn\nwhere \u039b\u2217 is the optimal parameter set of the base CRF model and \u2016\u039b\u20162 is the squared \u21132-norm regularizer whose impact is adjusted by \u03b3. At the first line, \u039b\u2217 is assigned to \u039b(n=0) i.e. the initial parameter set of the model.\nIn the next step, the k-NN similarity graph (G) is constructed (line 2), which will be discussed in more detail in Section 2.3. In the third step, the empirical label distribution (r) on the labeled data is computed. The main loop of the algorithm is then started and the execution continues until the results converge.\nMarginal probability of labels (m) are then computed on the unlabeled data (Du) using ForwardBackward algorithm with the parameters of the previous CRF model (\u039bn), and in the next step, all the marginal label probabilities of each trigram are averaged over its occurrences (line 5 and 6).\nIn label propagation (line 7), trigram marginals (q) are propagated through the similarity graph using an iterative algorithm. Thus, they become smooth. Empirical label distribution (r) serves as the priori label information for labeled data and trigram marginals (q) act as the seed labels. More detailed discussion is found in Section 2.4.\nAfterwards, having the results of label propagation (q\u0302) and previous CRF model parameters, labels of the unlabeled data are estimated by combining the interpolated label marginals and the CRF transition potentials (line 8). For every word position j for i indexing over sentences, interpolated label marginals are calculated as follows:\np\u0302(y (j) i = y|xi) = \u03b1p(y (j) i = y|xi; \u039bn)\n+ (1\u2212 \u03b1)q\u0302T (i,j)(y), (2)\nwhere T (i, j) is a trigram centered at position j of the ith sentence and \u03b1 is the interpolation factor.\nIn the final step, the previous CRF model parameters are regularized using the labels estimated for the unlabeled data in the previous step (line 9) as follows:\n\u039bn+1 = argmin \u039b\u2208RK\n[ \u2212 l \u2211\ni=1\nlog p(yi|xi; \u039bn)\n\u2212 \u03b7\nu \u2211\ni=l+1\nlog p(yi|xi; \u039bn) + \u03b3\u2016\u039b\u2016 2 ] , (3)\nwhere \u03b7 is a trade-off parameter whose setting is discussed later in Section 3."}, {"heading": "2.2 CRF Features", "text": "By aligning the training data, many informative labels are saved which are omitted in other works (Wang and Acero, 2006; Raymond and Riccardi, 2007). By saving these information, the first order label dependency helps the model to predict the labels more precisely. Therefore the model manages to predict the labels using less lexical features and the feature window that was [-4,+2] in previous works is reduced to [0,+2]. Using smaller feature window improves the generalization of the model (Aliannejadi et al., 2014)."}, {"heading": "2.3 Similarity Graph", "text": "In our work we have considered trigrams as the nodes of the graph and extracted features of each trigram x2 x3 x4 according to the 5-word context x1 x2 x3 x4 x5 it appears in. These features are carefully selected so that nodes are correctly placed in neighborhood of the ones having similar labels. Table 1 presents the feature set that we have applied to construct the similarity graph.\nIsClass feature impacts the structure of the graph significantly. In the pre-processing phase\nspecific words are marked as classes according to the corpus\u2019 accompanying database. As an example, city names such as Dallas and Baltimore are represented as city name which is a class type. Since these classes play an important role in calculating similarity of the nodes, IsClass feature is used to determine if a given position in a context is a class type.\nFurthermore, prepositions like from and between are also important, e.g. when two trigrams like \u201dfrom Washington to\u201d and \u201dbetween Dallas and\u201d are compared. The two trigrams are totally different while both of them begin with a preposition and are continued with a class. Therefore, IsPreposition feature would be particularly suitable to increase the similarity score of these two trigrams. In many cases, these features have a significant effect in assigning a better similarity score.\nTo define a similarity measure, we compute the Pointwise Mutual Information (PMI) between all occurrences of a trigram and each of the features. The PMI measure transforms the independence assumption into a ratio (Lin, 1998; Razmara et al., 2013). Then, the similarity between two nodes is measured as the cosine distance between their PMI vectors. We carefully examined the similarity graph on the training data and found out the head and tail trigrams of each sentence which contain dummy words, make the graph sparse. Hence, we have ignored those trigrams."}, {"heading": "2.4 Label Propagation", "text": "After statistical alignment, the training data gets noisy. Hence, use of traditional label propagation algorithms causes an error propagation over the whole graph and degrades the whole system performance. Thus, we make use of the Modified Adsorption (MAD) algorithm for label propagation.\nMAD algorithm controls the label propagation more strictly. This is accomplished by limiting the amount of information that passes from a node to another (Talukdar and Pereira, 2010). Soft label vectors Y\u0302v are found by solving the unconstrained optimization problem in (4):\nmin Y\u0302\n\u2211\nl\u2208C\n[\n\u00b51(Yl \u2212 Y\u0302l) \u22a4S (Yl \u2212 Y\u0302l)\n+ \u00b52Y\u0302l \u22a4 L\u2032Y\u0302l + \u00b53 \u2225 \u2225Y\u0302l \u2212Rl \u2225 \u2225\n2 ]\n, (4)\nwhere \u00b5i are hyper-parameters and Rl is the\nempirical label distribution over labels i.e. the prior belief about the labeling of a node. The first term of the summation is related to label score injection from the initial score of the node and makes the output match the seed labels Yl (Razmara et al., 2013). The second term is associated with label score acquisition from neighbor nodes i.e. smooths the labels according to the similarity graph. In the last term, the labels are regularized to match a priori label Rl in order to avoid false labels for high degree unlabeled nodes. A solution to the optimization problem in (4) can be found with an efficient iterative algorithm described in (Talukdar and Crammer, 2009).\nMany errors of the alignment model are corrected through label propagation using the MAD algorithm; whereas, those errors are propagated in traditional label propagation algorithms such as the one mentioned in (Subramanya et al., 2010)."}, {"heading": "2.5 System Overview", "text": "We have implemented the Graph Construction in Java and the CRF is implemented by modifying the source code of CRFSuite (Okazaki, 2007). We have also modified Junto toolkit (Talukdar and Pereira, 2010) and used it for graph propagation. The whole source code of our system is available online1. The input utterances and their corresponding semantic trees are aligned using GIZA++ (Och and Ney, 2000); and then used to train the base CRF model. The graph is constructed using the labeled and unlabeled data and the main loop of the algorithm continues until convergence. The final parameters of the CRF are retained for decoding in the test phase."}, {"heading": "3 Experimental Results", "text": "In this section we evaluate our results on Air Travel Information Service (ATIS) data-set (Dahl et al., 1994) which consists of 4478 training, 500 development and 896 test utterances. The development set was chosen randomly. To evaluate our work, we have compared our results with results from Supervised CRF and Self-trained CRF (Yarowsky, 1995).\nFor our experiments we set hyper-parameters as follows: for graph propagation, \u00b51 = 1, \u00b52 = 0.01, \u00b53 = 0.01, for Viterbi decoding, \u03b1 = 0.1, for CRF-retraining, \u03b7 = 0.1, \u03b3 = 0.01. We have\n1https://github.com/maxxkia/g-ssl-crf\nchosen these parameters along with graph features and graph-related parameters by evaluating the model on the development set. We employed the L-BFGS algorithm to optimize CRF objective functions; which is designed to be fast and lowmemory consumer for the high-dimensional optimization problems (Bertsekas, 1999).\nWe have post-processed the sequence of labels to obtain the slots and their values. The slotvalue pair is compared to the reference test set and the result is reported in F-score of slot classification. Table 2 demonstrates results obtained from our semi-supervised CRF algorithm compared to the supervised CRF and self-trained CRF. Experiments were carried out having 10%, 20% and 30% of data being labeled. For each of these tests, labeled set was selected randomly from the training set. This procedure was done 10 times and the reported results are the average of the results thereof. The Supervised CRF model is trained only on the labeled fraction of the data. However, the Self-trained CRF and Semi-supervised CRF have access to the rest of the data as well, which are unlabeled. Our Supervised CRF gained 91.02 F-score with 100% of the data labeled which performs better compared to 89.32% F-score of Raymond and Riccardi (2007) CRF model.\nAs shown in Table 2, the proposed method performs better compared to supervised CRF and self-trained CRF. The most significant improvement occurs when only 10% of training set is labeled; where we gain 1.65% improvement on F-score compared to supervised CRF and 1.38% compared to self-trained CRF."}, {"heading": "4 Conclusion", "text": "We presented a simple algorithm to train CRF in a semi-supervised manner using unaligned data for SLU. By saving many informative labels in the alignment phase, the base model is trained using fewer features. The parameters of the CRF model are estimated using much less labeled data by\nregularizing the model using a nearest-neighbor graph. Results demonstrate that our proposed algorithm significantly improves the performance compared to supervised and self-trained CRF."}], "references": [{"title": "Discriminative spoken language understanding using statistical machine translation alignment models", "author": ["Shahram Khadivi", "SaeedShiry Ghidary", "MohammadHadi Bokaei"], "venue": null, "citeRegEx": "Aliannejadi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Aliannejadi et al\\.", "year": 2014}, {"title": "Expanding the scope of the atis task: The atis-3 corpus", "author": ["Dahl et al.1994] Deborah A. Dahl", "Madeleine Bates", "Michael Brown", "William Fisher", "Kate HunickeSmith", "David Pallett", "Christine Pao", "Alexander Rudnicky", "Elizabeth Shriberg"], "venue": null, "citeRegEx": "Dahl et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 1994}, {"title": "Graph-based lexicon expansion with sparsity-inducing penalties", "author": ["Das", "Smith2012] Dipanjan Das", "Noah A. Smith"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Das et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Das et al\\.", "year": 2012}, {"title": "Learning a part-of-speech tagger from two hours of annotation", "author": ["Garrette", "Baldridge2013] Dan Garrette", "Jason Baldridge"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Garrette et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Garrette et al\\.", "year": 2013}, {"title": "Semantic processing using the hidden vector state model", "author": ["He", "Young2005] Yulan He", "Steve Young"], "venue": "Computer Speech & Language,", "citeRegEx": "He et al\\.,? \\Q2005\\E", "shortCiteRegEx": "He et al\\.", "year": 2005}, {"title": "Automatic filtering of bilingual corpora for statistical machine translation", "author": ["Khadivi", "Ney2005] Shahram Khadivi", "Hermann Ney"], "venue": "Natural Language Processing", "citeRegEx": "Khadivi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Khadivi et al\\.", "year": 2005}, {"title": "Automatic retrieval and clustering of similar words", "author": ["Dekang Lin"], "venue": "In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics", "citeRegEx": "Lin.,? \\Q1998\\E", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "Giza++: Training of statistical translation models", "author": ["Och", "Ney2000] Franz Josef Och", "Hermann Ney"], "venue": null, "citeRegEx": "Och et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Och et al\\.", "year": 2000}, {"title": "Crfsuite: a fast implementation of conditional random fields (crfs). URL http://www.chokkan.org/software/crfsuite", "author": ["Naoaki Okazaki"], "venue": null, "citeRegEx": "Okazaki.,? \\Q2007\\E", "shortCiteRegEx": "Okazaki.", "year": 2007}, {"title": "A speech understanding system based on statistical representation of semantics", "author": ["E. Tzoukermann", "Z. Gorelov", "J. Gauvain", "E. Levin", "Chin-Hui Lee", "J.G. Wilpon"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Pieraccini et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Pieraccini et al\\.", "year": 1992}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["Raymond", "Riccardi2007] Christian Raymond", "Giuseppe Riccardi"], "venue": "In International Conference on Speech Communication and Technologies,", "citeRegEx": "Raymond et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raymond et al\\.", "year": 2007}, {"title": "Graph propagation for paraphrasing out-ofvocabulary words in statistical machine translation", "author": ["Maryam Siahbani", "Gholamreza Haffari", "Anoop Sarkar"], "venue": "In Proceedings of the Conference of the Association", "citeRegEx": "Razmara et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Razmara et al\\.", "year": 2013}, {"title": "Efficient graph-based semi-supervised learning of structured tagging models", "author": ["Slav Petrov", "Fernando Pereira"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language", "citeRegEx": "Subramanya et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Subramanya et al\\.", "year": 2010}, {"title": "New regularized algorithms for transductive learning", "author": ["Talukdar", "Koby Crammer"], "venue": null, "citeRegEx": "Talukdar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Talukdar et al\\.", "year": 2009}, {"title": "Experiments in graphbased semi-supervised learning methods for classinstance acquisition", "author": ["Talukdar", "Fernando Pereira"], "venue": "In Proceedings of the 48th Annual Meeting of the Association", "citeRegEx": "Talukdar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Talukdar et al\\.", "year": 2010}, {"title": "Weakly-supervised acquisition of labeled class instances using graph random walks", "author": ["Joseph Reisinger", "Marius Pa\u015fca", "Deepak Ravichandran", "Rahul Bhagat", "Fernando Pereira"], "venue": null, "citeRegEx": "Talukdar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Talukdar et al\\.", "year": 2008}, {"title": "Discriminative models for spoken language understanding", "author": ["Wang", "Acero2006] Ye-Yi Wang", "Alex Acero"], "venue": "In International Conference on Speech Communication and Technologies", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["David Yarowsky"], "venue": "In Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Yarowsky.,? \\Q1995\\E", "shortCiteRegEx": "Yarowsky.", "year": 1995}, {"title": "Graphbased semi-supervised model for joint chinese word segmentation and part-of-speech tagging", "author": ["Zeng et al.2013] Xiaodong Zeng", "Derek F Wong", "Lidia S Chao", "Isabel Trancoso"], "venue": "In ACL,", "citeRegEx": "Zeng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2013}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Zhu et al.2003] Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "The first statistical SLU system was based on hidden Markov model and modeled using a finite state semantic tagger employed in AT&T\u2019s CHRONUS system (Pieraccini et al., 1992).", "startOffset": 149, "endOffset": 174}, {"referenceID": 9, "context": "The first statistical SLU system was based on hidden Markov model and modeled using a finite state semantic tagger employed in AT&T\u2019s CHRONUS system (Pieraccini et al., 1992). Their semantic representation was flat-concept; but, later He and Young (2005) extended the representation to a hierarchical structure and modeled the problem using a push-down automaton.", "startOffset": 150, "endOffset": 255}, {"referenceID": 9, "context": "The first statistical SLU system was based on hidden Markov model and modeled using a finite state semantic tagger employed in AT&T\u2019s CHRONUS system (Pieraccini et al., 1992). Their semantic representation was flat-concept; but, later He and Young (2005) extended the representation to a hierarchical structure and modeled the problem using a push-down automaton. There are other works which have dealt with SLU as a sequential labeling problem. Raymond and Riccardi (2007) and Wang and Acero (2006) have fully annotated the data and trained the model in discriminative frameworks such as CRF.", "startOffset": 150, "endOffset": 474}, {"referenceID": 9, "context": "The first statistical SLU system was based on hidden Markov model and modeled using a finite state semantic tagger employed in AT&T\u2019s CHRONUS system (Pieraccini et al., 1992). Their semantic representation was flat-concept; but, later He and Young (2005) extended the representation to a hierarchical structure and modeled the problem using a push-down automaton. There are other works which have dealt with SLU as a sequential labeling problem. Raymond and Riccardi (2007) and Wang and Acero (2006) have fully annotated the data and trained the model in discriminative frameworks such as CRF.", "startOffset": 150, "endOffset": 500}, {"referenceID": 19, "context": "The computed similarities are then assigned as the weight of the edges connecting the nodes (Zhu et al., 2003).", "startOffset": 92, "endOffset": 110}, {"referenceID": 18, "context": "Garrette and Baldridge, 2013), and sparsity (Das and Smith, 2012; Zeng et al., 2013).", "startOffset": 44, "endOffset": 84}, {"referenceID": 0, "context": "More detailed explanation about this process can be found in our previous work (Aliannejadi et al., 2014).", "startOffset": 79, "endOffset": 105}, {"referenceID": 12, "context": "The proposed semi-supervised learning algorithm is based on (Subramanya et al., 2010).", "startOffset": 60, "endOffset": 85}, {"referenceID": 0, "context": "Using smaller feature window improves the generalization of the model (Aliannejadi et al., 2014).", "startOffset": 70, "endOffset": 96}, {"referenceID": 6, "context": "The PMI measure transforms the independence assumption into a ratio (Lin, 1998; Razmara et al., 2013).", "startOffset": 68, "endOffset": 101}, {"referenceID": 11, "context": "The PMI measure transforms the independence assumption into a ratio (Lin, 1998; Razmara et al., 2013).", "startOffset": 68, "endOffset": 101}, {"referenceID": 11, "context": "The first term of the summation is related to label score injection from the initial score of the node and makes the output match the seed labels Yl (Razmara et al., 2013).", "startOffset": 149, "endOffset": 171}, {"referenceID": 12, "context": "Many errors of the alignment model are corrected through label propagation using the MAD algorithm; whereas, those errors are propagated in traditional label propagation algorithms such as the one mentioned in (Subramanya et al., 2010).", "startOffset": 210, "endOffset": 235}, {"referenceID": 8, "context": "We have implemented the Graph Construction in Java and the CRF is implemented by modifying the source code of CRFSuite (Okazaki, 2007).", "startOffset": 119, "endOffset": 134}, {"referenceID": 1, "context": "In this section we evaluate our results on Air Travel Information Service (ATIS) data-set (Dahl et al., 1994) which consists of 4478 training, 500 development and 896 test utterances.", "startOffset": 90, "endOffset": 109}, {"referenceID": 17, "context": "To evaluate our work, we have compared our results with results from Supervised CRF and Self-trained CRF (Yarowsky, 1995).", "startOffset": 105, "endOffset": 121}], "year": 2017, "abstractText": "We experiment graph-based SemiSupervised Learning (SSL) of Conditional Random Fields (CRF) for the application of Spoken Language Understanding (SLU) on unaligned data. The aligned labels for examples are obtained using IBM Model. We adapt a baseline semisupervised CRF by defining new feature set and altering the label propagation algorithm. Our results demonstrate that our proposed approach significantly improves the performance of the supervised model by utilizing the knowledge gained from the graph.", "creator": "LaTeX with hyperref package"}}}