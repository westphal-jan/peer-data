{"id": "1701.08585", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Variational Policy for Guiding Point Processes", "abstract": "Temporal but characteristics come enemies provide to model shows well-documented and have own compendiums among programs of personal sciences. While the majority entire extended unique focus on part modeling and learning raised these processes, we consider the unfortunately of think to display 's optimal failed policy first noting point process with stochastic intensities, making both the stochastic therefore decade notably given process the shied to end target state. In particular, be create the novel insight from where information non-cooperative formulations under equilibrium optimal for. We efforts should his portrays geometry linear framework also he widely capabilities phone hamiltonian immediately update main regard systematised not the beginning system illinois. Experiments coming polyethylene instead real - world data show that our nonlinear can steer of password purposes instead more sufficiently than state - over - carnegie.", "histories": [["v1", "Mon, 30 Jan 2017 13:24:07 GMT  (1306kb,D)", "http://arxiv.org/abs/1701.08585v1", null], ["v2", "Thu, 2 Mar 2017 18:44:24 GMT  (1361kb,D)", "http://arxiv.org/abs/1701.08585v2", null], ["v3", "Tue, 13 Jun 2017 19:18:19 GMT  (728kb,D)", "http://arxiv.org/abs/1701.08585v3", "ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.SI cs.SY math.OC", "authors": ["yichen wang", "grady williams", "evangelos theodorou", "le song"], "accepted": true, "id": "1701.08585"}, "pdf": {"name": "1701.08585.pdf", "metadata": {"source": "META", "title": "A Unifying Framework for Guiding Point Processes with Stochastic Intensity Functions", "authors": ["Yichen Wang", "Grady Williams", "Evangelos Theodorou"], "emails": ["yichen.wang@gatech.edu,", "gradyrw@gatech.edu,", "evangelos.theodorou@gatech.edu,", "lsong@cc.gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "The large-scale online user activity data with fine temporal resolutions are increasingly available. It further fuels the efforts in developing realistic models and learning algorithms to understand, predict and distill knowledge from the complex dynamics of these data. In particular, point processes are well-suited to capture mutual excitation between the occurrence of events, and have been applied successfully in modeling online users behaviors (Zhou et al., 2013; Farajtabar et al., 2014, 2015; Du et al., 2015; Wang et al., 2016a,b), and opinion dynamics (De et al., 2015). However, little work has considered the problem of how to control the point process to further influence users\u2019 behaviors.\nWe study the problem of designing the best intervention policy to influence the intensity function of point processes, such that the stochastic system driven by the process can be influenced towards a target. A framework for doing this is critically important. For example, a government agent may want to effectively suppress the spread of terrorist propaganda. Guiding people\u2019s opinion posting behavior is also important for understanding the vulnerabilities of social networks and increasing their resilience to rumor and false information. Moreover, promoting users\u2019 frequency of visiting business merchants can promote sales; designing various badges on Q&A sites can motivate users to answer questions and provide feedback to answers, hence increasing the online engagement (Anderson et al., 2013); Moreover, a broadcaster on Twitter may want to design a smart tweeting strategy such that his posts always remain on top of his followers\u2019 news feeds, so that he gains more attention (Karimi et al., 2016). It is also desirable for the broadcaster to post the right topics to increase the number of followers and make the followers have a positive opinion on him.\nInterestingly, the social science setting also introduces new challenges. Prior stochastic optimal control methods (Boel & Varaiya, 1977; Pham, 1998; Oksendal & Sulem, 2005; Hanson, 2007) are not applicable for four reasons: (i) they mostly focus on the cases where the control is in the drift part of the system, which is quite different from our case where the policy is on the intensity function; (ii) they require linear approximations of the nonlinear Stochastic Differential Equations (SDEs) and quadratic approximations\nar X\niv :1\n70 1.\n08 58\n5v 1\n[ cs\n.L G\n] 3\n0 Ja\nn 20\nof the objective function; (iii) to obtain a feedback control policy, these methods require the solution of the Hamilton-Jacobi-Bellman (HJB) Partial Differential Equation (PDE), which have severe limitations in scalability and feasibility to the nonlinear SDEs, especially in social applications where the dimension of the system is huge; (iv) The SDEs they study are driven by Wiener processes and/or Poisson processes. However, social sciences require us to consider more advanced processes, such as Hawkes processes, which are models for long term memory process and mutual exciting phenomena in social interactions. Therefore, besides proposing the intensity control problem that links behavioral modeling in the social sciences with optimal control theory, we make the following technical contributions:\nUnifying framework. We propose a unifying intensity control framework by establishing the novel connection to information theoretic measures. It offers one of the most general ways to control nonlinear jump diffusions SDE driven by doubly stochastic point process with stochastic intensity function (e.g., Hawkes process). Unlike prior works, no approximations of the SDE or objective function are needed.\nNatural control cost. Our framework provides a meaningful control cost function to optimize: it arises naturally from the structure of the stochastic dynamics and in stark contrast with optimality principles in control theory, i.e., stochastic dynamic programming, in which the control cost is imposed beforehand, despite the form of the dynamics.\nSuperior performance. We propose a scalable model predictive control algorithm. It enjoys superior empirical performance on diverse social applications. The control policy is computed with forward sampling, hence is scalable with parallel sampling and runs in real time."}, {"heading": "2 Background and Preliminaries", "text": "Further related work. In addition to the prior works in stochastic optimal control, we review other works in the machine learning community. Some works focus on controlling the point process itself, but they are not generalizable for two reasons: (i) the processes are simple, such as Poisson process (Br\u00e9maud, 1981) and a power-law decaying function (Bayraktar & Ludkovski, 2014); (ii) the systems only contain point process. However, in social sciences, the system can be driven by many other stochastic processes, such as Hawkes process and Wiener process. Based on Hawkes process, Farajtabar et al. (2014) designs the baseline intensity to achieve a steady state behavior. However, this work does not consider system feedback when designing the policy. Recently, Zarezade et al. (2016) proposes to control the broadcaster\u2019s intensity, which is driven by a Poisson process. The intensity of competitors follow a Hawkes process. The system is a linear SDE and this method solves the HJB PDE with approximations to the objective function. However, it is not clear how to extend this work to general problems where (i) the broadcaster follows a Hawkes process with stochastic intensity, or (ii) the SDE is nonlinear and complex. On the contrary, our method works for general point processes with stochastic intensity (e.g., Hawkes process) and nonlinear SDE.\nThe works on event triggered control (Ades et al., 2000; Lemmon, 2010; Heemels et al., 2012; Meng et al., 2013) seems relevant, but are quite different. The system is linear and only contains a diffusion process, with the control affine in drift and updated at event time. The event times are driven by a fixed point process. However, we study jump diffusion SDEs and directly control the intensity that drives the event times. Hence our work is unique among previous works.\nPoint processes. A temporal point process (Aalen et al., 2008) is a random process whose realization consists of a list of discrete events localized in time, {ti}. It is widely applied to model user-generated event data and behavior patterns (Zhou et al., 2013; Yang & Zha, 2013; Farajtabar et al., 2014, 2015; Du et al., 2015; Lian et al., 2015; Wang et al., 2016a,b).\nThe point process can also be represented as a counting process, N(t), which records the number of events before time t. An important way to characterize it is via the conditional intensity function \u03bb(t) \u2014 a stochastic model for the time of the next event given historical events, H(t) = {ti|ti < t}. It is the probability of observing a new event on [t, t+ dt), i.e.,\n\u03bb(t)dt := P {event in [t, t+ dt)|H(t)} = E[dN(t)|H(t)]\nwhere one typically assumes that only one event happens in a small window of size dt, i.e., dN(t) \u2208 {0, 1}. The function form of the intensity is often designed to capture the phenomena of interests. Some useful forms include: (i) Poisson process: the intensity is independent of history; (ii) Hawkes process (Liniger, 2009): It models the mutual excitation between events, and the intensity of a user i depends on events from a collection of M users:\n\u03bbi(t) = \u00b5i + \u2211M j=1 \u03b1ij \u2211 tj\u2208Hj(t) \u03ba\u03c9(t\u2212 tj), (1)\nwhere \u03ba\u03c9(t) := exp(\u2212\u03c9t) is an exponential triggering kernel that models the decay of past events\u2019 influence over time, \u00b5i > 0 is the base intensity for node i, Nj(t) is the point process representing the historical events Hj(t) from user j, and \u03b1ij > 0 models the strength of influence from user j to user i. Here, the occurrence of each historical event increases the intensity by a certain amount determined by \u03ba\u03c9 and the weight \u03b1ij , making \u03bbi(t) history dependent and a stochastic process by itself."}, {"heading": "3 Stochastic Differential Equations for Social Information Diffusion", "text": "In this section, we introduce the Jump Diffusion Stochastic Differential Equations (SDEs), and then show its applications in two real world problems. The SDE is as follows.\ndxi(t) = f(xi)dt \u2191\ndrift\n+ g(xi)dwi(t) \u2191\ndiffusion noise\n+ M\u2211\nj=1\nh(xj)dNj(t) \u2191\njump\n(2)\nwhere x(t) = (x1(t), \u00b7 \u00b7 \u00b7 , xM (t))> \u2208 <M is a continuous-time and continuous-state process. xi(t) represents the state or information of user i. The term dx(t) := x(t+ dt)\u2212x(t) describes the increment of x(t). {f, g, h} are nonlinear continuous functions. The SDE contains three terms.\nThe drift term captures the evolution of the system. The diffusion term models the noise with the Wiener process, wi(t) \u223c N (0, t), which follows a Gaussian distribution. The point process Nj(t) models events pattern generated by user j. Its intensity function can be stochastic. The influence function h(xj) captures social influence, i.e., how user j influences user i. We set N(t) = (N1(t), \u00b7 \u00b7 \u00b7 , NM (t))> and its intensity as \u03bb(t) = (\u03bb1(t), \u00b7 \u00b7 \u00b7 , \u03bbM (t))>. Next, we show two applications of the SDE model."}, {"heading": "3.1 SDE for the opinion diffusion model", "text": "The opinion diffusion model considers both the content and timing of each event (De et al., 2015; He et al., 2015). It assigns each user i a Hawkes intensity \u03bbi(t) and an opinion process xi(t) where xi(t) = 0 corresponds to neutral opinion, and large positive/negative values correspond to extreme opinions. The users are connected according to an adjacency matrix A = (\u03b1ij). The opinion change of user i is captured by (i) a baseline drift, (ii) a noise process, and (iii) a temporally discounted average of neighbors\u2019 opinion:\ndxi(t) = ( bi \u2191\nbase\n\u2212 xi(t) \u2191\ndrift\n) dt+ \u03b2dwi(t)\n\u2191 noise\n+\nM\u2211\nj=1 \u03b1ijxj(t)dNj(t)\ufe38 \ufe37\ufe37 \ufe38 neighbor influence\n(3)\nIn the drift term, the opinion\u2019s change rate, dxi(t)/dt, is negative proportional to xi(t). This is because people\u2019s opinion tends to stabilize over time. bi is the baseline opinion, i.e., personal characteristics. Hence, if ignoring all other terms, the expected value of xi(t) will converge to bi as time goes by. This can be seen by setting E[dxi(t)] = 0. The noise process dwi(t) captures the normal fluctuations in the dynamics due to unobserved factors such as activity outside the social platform and unexpected events. The jump term captures the fact that the change of user i\u2019s opinion is a weighted summation of his neighbors\u2019 influence. \u03b1ij ensures only the user\u2019s neighbors will be considered and dNj(t) \u2208 {0, 1} models whether an event happens.\nThis model has superior performance in learning and predicting opinions. However, these prior works only focus on modeling. It is not clear how to design feedback policies to guide user behaviors, such that the opinion dynamics are steered to some target states."}, {"heading": "3.2 SDE for the broadcaster\u2019s position model", "text": "When a user makes a post in social network, he is competing with posts from others that his followers follow. His position is defined as the rank of his post among his followers. Karimi et al. (2016) proposed the model that captures the change of a broadcaster\u2019s position due to the posting behavior of other competitors and himself as follows.\ndxj(t) = dNo(t)\u2212 ( xj(t)\u2212 1 ) dNi(t) (4)\nwhere i is the broadcaster and F(i) denotes the set of all followers of i. The stochastic process xj(t) denotes the rank of broadcaster i\u2019s posts among all the posts that his follower j receives. xj(t) = 1 means i\u2019s posts is the top-1 among all posts j receives. Ni(t) is a Poisson process capturing the broadcaster i\u2019s posting behavior. No(t) is the Hawkes process for the behavior of all other broadcasters that j follows, i.e., competitors of i. Appendix E contains details of the model.\nGiven the jump diffusion SDEs in different applications, we then show how to design the optimal policy to help each user decide his/her posting intensity, such that the SDE can be steered towards a target."}, {"heading": "4 Intensity Stochastic Control Problem", "text": "In this section, we introduce the stochastic intensity control problem. Specifically, we will apply the control policy to the intensity function of the point process. Furthermore, we show its application with two examples.\nDefinition 1 (Intensity Stochastic Optimal Control Problem). Set \u03bb(t) = (\u03bb1(t), \u00b7 \u00b7 \u00b7 , \u03bbM (t))> as the original intensity in (2), u(t) = (u1(t), \u00b7 \u00b7 \u00b7 , uM (t))> as the control policy, and \u03bb(u(t), t) as the controlled intensity of controlled point process N(u(t), t). The uncontrolled SDE in (2) is modified to be the controlled SDE as:\ndxi(t) = f(xi(t))dt+ g(xi(t))dwi(t) + \u2211M\nj=1 h(xj(t))dNj(uj(t), t) (5)\nAt the m-th dimension, the form of control is:\n\u03bbi(ui(t), t) = \u03bbi(t)ui(t), i = 1, \u00b7 \u00b7 \u00b7 ,M (6)\nThe goal is to find u\u2217 such that u\u2217 = argmin\nu>0 EQ [ S(x) + \u03b3C(u) ] , (7)\nwhere EQ is the expectation over probability measure Q, which is induced by the controlled SDE with policy u. C(u) is the control cost. The state cost S(x) is defined as :\nS(x) = \u03c6(x(T ), T ) +\n\u222b T\u2212\n0\nq(x(t), t)dt (8)\nIt is a function of the trajectory x and measures the cost induced by it on [0, T ]. We use x to denote the trajectory. q(x(t), t) is the instantaneous state cost at time t, and \u03c6(x(T ), T ) is the terminal state cost at time T . \u03b3 is the trade off between state cost and control cost.\nNotation. We use x to denote the SDE trajectory on [0, T ], i.e., x := {x(t)|t \u2208 [0, T ]}. Similarly, u := {u(t)|t \u2208 (0, T ]} denotes the policy on [0, T ]. u > 0 means ui(t) > 0 for each i and t. SDE in (2) is called uncontrolled SDE since the point process Nj(t) is not influenced by the policy. The SDE in (5) is the controlled SDE since Nj(uj(t), t) is influenced by the policy, and the intensity is modified from \u03bbj(t) to \u03bbj(uj(t), t). Next, we discuss each term in detail.\nControl policy. ui(t) helps each user i decide the scale of changes to his posting intensity \u03bbi(t), and controls the frequency of generating events. The larger ui(t), the more likely an event will happen. Moreover, the control policy is in the multiplicative form. This choice makes the policy easy to execute and meaningful in practice. For example, a network moderator may request a user to reduce his tweeting intensity five times\nif he spreads rumors, or double the original intensity if he posts beneficial topics on education. However, for other forms, such as addition, it is less intuitive and not easy to execute in practice. For example, if the moderator asks the user to decrease his posting intensity by one, it is not a precise instruction for execution. Finally, we set ui(t) > 0 since intensity functions need to be positive.\nState cost. It is a user-defined function and its form depends on different applications. The function forms of q and \u03c6 are typically the same. Common choices of S(x) include: \u2022 Least square opinion shaping. For the opinion diffusion model, the goal is to make the expected opinion to achieve the target a = (a1, \u00b7 \u00b7 \u00b7 , aU )> at each time on [0, T ], e.g., nobody believes the rumor during the period. Mathematically, we set q(x(t), t) = \u2016x(t)\u2212 a\u20162. \u2022 Opinion influence maximization. The goal is to maximize each user\u2019s positive opinion on [0, T ], i.e., the goal for a political party is to maximize the support at the election day. Mathematically, we set q(x(t), t) = \u2212\u2211i xi(t). \u2022 Minimize post position. For the broadcaster\u2019s position problem, to gain maximum attention from followers, we minimize his rank in each of his follower\u2019s news feeds. Mathematically, we set q(x(t), t) = \u2211 j\u2208F(i) xj(t). Control cost. C(u) captures the budget to control the system on [0, T ]. Similar to S(x), it is in the form of an integration. There is always a tradeoff between state and control cost. For example, to minimize the broadcaster position, a broadcaster may keep spamming his followers with a high intensity. However, the control cost prevents such scenarios since the budget, such as broadcaster\u2019s time, is limited.\nHow to choose its function form? It is nontrivial to choose a correct form for dynamic programming based approaches (Hanson, 2007). For example, ui(t) = 1 means there is no control. However, it is not clear which of the two heuristic forms works better:\n\u222b T\n0\nM\u2211\ni=1\n(ui(t)\u2212 1)2dt, or \u222b T\n0\nm\u2211\ni=1\n(ui(t)\u2212 1)\u2212 log ( ui(t) ) dt (9)\nUnfortunately, prior works in stochastic dynamic programming requires its form beforehand (Hanson, 2007; Zarezade et al., 2016). However, as we will shown later, our framework automatically choose the most proper function form.\nNext, we show how to reformulate the SDE with an extra control policy with two examples. Guiding opinion diffusion. How to guide the opinion diffusion model by controlling the intensity function of each user? We can formulate it as an intensity control problem. More specifically, we modify the point process Nj(t) in the uncontrolled opinion dynamics in (3) as Nj(uj(t), t) and obtain the controlled SDE as:\ndxi(t) = ( bi \u2212 xi(t) ) dt+ \u03b2dwi + \u2211 j \u03b1ijxjdNj(uj(t), t)\nWith the controlled SDE, we can conduct least square guiding or opinion influence maximization. Guiding broadcasting behavior. Users in social networks are likely to gain greater attention if their posts remain visible for a long period among followers recent feeds. How to change the broadcasting intensity, such that his posts always remain on top? Similarly, we can use the policy to change Ni(t) to Ni(ui(t), t) and help user i decide when to post messages. Hence the controlled SDE is:\ndxj(t) = dNo(t)\u2212 ( xj(t)\u2212 1 ) dNi(ui(t), t)\nWith the controlled SDE, we can further optimize the state cost to minimize his position (rank) among followers.\nDifficulty. Despite its wide applicability, solving the proposed intensity control problem is difficult using prior stochastic optimal control methods (Boel & Varaiya, 1977; Pham, 1998; Oksendal & Sulem, 2005; Hanson, 2007) for four reasons. (i) Different task. The policy in these works is in the drift term of SDE. However, for social sciences, it is\nmore proper to control the jump term, since the intensity function drives users\u2019 behavior patterns. (ii) Problem scope. These works typically considers simple Poisson processes with deterministic intensity.\nHowever, in our problem the intensity can also be stochastic, which adds another layer of stochasticity to the problem.\n(iii) Scalability. To obtain the optimal feedback policy, these works need to solve a complex HJB Partial Differential Equation, which is limited by the scalability for nonlinear jump diffusion SDEs.\n(iv) Control cost. These methods need to define the form of control cost beforehand, which is nontrivial. In the next section, we will provide a scalable algorithm that addresses the limitations in prior works."}, {"heading": "5 Finding the Optimal Control Policy", "text": "In this section, we propose an efficient framework by linking the problem of finding optimal control to finding optimal measure. With this novel view, the control cost comes naturally as a relative entropy term. We further propose the novel objective function based on matching the KL distance to optimal measure. Finally, we transform the open-loop policy to the feedback policy and develop a scallable sampling based algorithm to compute the policy."}, {"heading": "5.1 From optimal control to optimal measure", "text": "We will first introduce the information-theoretic inequality, which describes the relationship between free energy and relative entropy (Dai Pra et al., 1996; Theodorou, 2015). It is a problem of finding the optimal measure, and we further show its connection to the intensity control problem. The information-theoretic inequality is as follows.\nTheorem 2. Set (\u2126,F) to be a measurable space, where \u2126 denotes the sample space and F denotes a \u03c3-algebra, and let P(\u2126) define a probability measure on F . Consider P,Q \u2208 P(\u2126), and S(x) : \u2126\u2192 < as an arbitrary measurable function. The following inequality holds:\n\u2212\u03b3 B(S(x))\ufe38 \ufe37\ufe37 \ufe38 Free energy = min Q [ EQ[S(x)]\ufe38 \ufe37\ufe37 \ufe38 State cost + \u03b3DKL(Q||P)\ufe38 \ufe37\ufe37 \ufe38 Relative entropy ] (10)\nwhere EP,EQ is the expectation under the probability measure P,Q, respectively. DKL(Q||P) = EQ[log(dQdP )] is the relative entropy. B(S(x)) = log ( EP [ exp(\u2212 1\u03b3S(x)) ])\nis the free energy. \u03b3 > 0 is the trade off parameter. The minimum in (10) is attained at optimal measure Q\u2217:\ndQ\u2217\ndP = exp(\u2212 1\u03b3S(x)) EP[exp(\u2212 1\u03b3S(x))]\n(11)\nTheorem 2 aims at finding the optimal probability measure such that EQ[S(x)] is minimized. The constraint is that Q should be as close to P as possible. This theorem holds for any two measures and measurable function S. The proof is based on expressing EP as a function of EQ and applying Jensen\u2019s inequality. Appendix A contains details. dQ \u2217\ndP is the Radon-Nikodym derivative, and is the relative density of measure Q\u2217 with respect to P (Gurevich et al., 1966).\nWe now provide a novel connection between Theorem 2 and the intensity control problem, and will show that finding optimal measure is equivalent to finding the optimal control. Our key observation is based on the property of SDEs:\nProposition 3. (Dai Pra et al., 1996) The set of all stochastic trajectories {x(t) : t \u2208 [0, T ]} of a SDE form a sample space \u2126, and a SDE uniquely induces a measure.\nInspired by this Proposition, we can set P as the probability measure over the trajectory space induced by the uncontrolled SDE in (2), and Q induced by the controlled SDE in (5). That is, each realization of x in (2) is a sample from P, and each realization of x in (5) is a sample from Q. Since the controlled SDE is influenced by the policy, u uniquely induces Q. Moreover, we define S(\u00b7) as the state cost, then the right-hand-side of (10) is decomposed into state cost and a natural control cost, and \u03b3 controls the cost trade-off.\nNatural control cost. The relative entropy DKL(Q||P) provides an elegant way of measuring the distance between controlled and uncontrolled SDEs. Intuitively, this term requires the policy to optimize the state cost while making smallest changes to the uncontrolled SDE. Hence it provides an implicit measure of control cost. Mathematically,\nDKL(Q||P) = EQ[log( dQ dP )] = EQ[C(u)] (12)\n= EQ [ \u222b T\n0\n\u2211\ni\n( log(ui(t)) + 1 ui(t) \u2212 1 ) \u03bbi(ui(t), t)dt\n]\nAppendix D contains derivations. It is easy to see the minimum of C(u) reaches when ui = 1, since function f(x) = log(x) + 1x \u2212 1 reaches the minimum when x = 1. Interestingly, C(u) is none of the heuristic forms in (9). Hence our control cost comes naturally from the dynamics.\nIn summary, our first contribution is the novel link between the problem of finding optimal control to that of optimal measure, and the free energy in (10) provides a lower bound on the cost achievable by a policy. The advantage of viewing the intensity control problem in this framework is two fold: (i) we have a natural control cost and do not need tedious and heuristic tuning of its function form, which is required by prior dynamic programming approaches, (ii) the optimal measure is given in the form of a softmax function. Hence we can compute the optimal policy from Q\u2217.\nHowever, it is still challenging since there is no explicit transformation between a measure and a function. To solve this problem, we design a novel objective function that matches Q\u2217 with Q(u), the measure induced by u."}, {"heading": "5.2 Minimizing the KL Divergence", "text": "We will formulate our objective function based on the optimal probability measure Q\u2217 in (11). More specifically, we find a control u which pushes the controlled measure Q(u), as close to the optimal measure as possible. This leads to minimizing the Kullback-Leibler (KL) divergence:\nu\u2217 = argminu>0 DKL ( Q\u2217||Q(u) ) (13)\nThis objective is in sharp contrast to traditional methods that solve the problem by computing the solution of the HJB PDE, which have severe limitations in scalability and feasibility to nonlinear SDEs. From the view of variational inference (Minka, 2005), our objective function is also natural and intuitive since since it describes the amount of information loss when Q(u) is used to approximate Q\u2217.\nNext, we simplify (13) and compute the optimal control policy. From the definition of KL divergence and chain rule of derivatives, (13) is expressed as:\nDKL(Q\u2217||Q(u)) = EQ\u2217 [ log (dQ\u2217\ndP dP dQ(u) )]\n(14)\nThe derivative dQ\u2217/dP is given in (11). Hence we need to compute dP/dQ(u). Intuitively, this derivative means the relative density of probability distribution P with respect to Q(u). The change of probability measure happens because the intensity of the point process is changed from \u03bb(t) to \u03bb(u, t). Hence dP/dQ(u) it is the likelihood ratio between the uncontrolled and controlled point process (Br\u00e9maud, 1981). Theorem 4 summarizes the result. Theorem 4. For the intensity control problem, we have: dP/dQ(u) = exp ( D(u) ) , where D(u) is expressed as: M\u2211\ni=1\n\u222b T\n0\n( ui(s)\u2212 1 ) \u03bbi(s)ds\u2212\n\u222b T\n0\nlog ( ui(s) ) dNi(s)\nAppendix B contains details of the proof. Now we substitute dQ\u2217/dP and dP/dQ(u) to (14). After removing terms independent of u, the objective function is simplified as:\nu\u2217 = argminu>0 EQ\u2217 [D(u)] (15)\nIn traditional stochastic optimal control works (Oksendal & Sulem, 2005; Hanson, 2007), the policy is obtained by solving the HJB PDE at discrete timestamps on [0, T ]. Hence it suffices to consider our policy u(t) as a piecewise constant function on [0, T ]. We denote the k-th piece of u as uk, which is defined on [k\u2206t, (k+1)\u2206t). We have k = 0, \u00b7 \u00b7 \u00b7 ,K \u2212 1, tk = k\u2206t and T = tK . Now we can express the objective function as follows.\nEQ\u2217 [D(u)] = M\u2211\ni=1\nK\u2211\nk=1\n( EQ\u2217 [ \u222b tk+1 tk (uki \u2212 1)\u03bbi(s)ds ] \u2212 EQ\u2217 [ \u222b tk+1 tk log(uki )dNi(s) ])\n(16)\nwhere uki denotes the i-th dimension of uk. We just need to focus on the parts that involves uki and move it outside of the expectation. Further we can show the final expression is convex in uki . Finally, setting the gradient to zero yields the following optimal control policy, denoted as uk\u2217i :\nuk\u2217i = EP [ exp(\u2212 1\u03b3S(x)) \u222b tk+1 tk dNi(s) ]\nEP [ exp(\u2212 1\u03b3S(x)) \u222b tk+1 tk \u03bbi(s)ds ] (17)\nAppendix C contains complete derivations. Note we have transformed EQ\u2217 to EP using (11). It is important because EQ\u2217 is not directly computable. Similar to the idea of importance sampling, since we only known the SDE of the uncontrolled dynamics in (2) and can only compute the expectation under P, the change of expectation is necessary.\nTo compute EP in (17), we use the Monte Carlo method to sample I trajectories from (2) on [0, T ] and take the sample average. To obtain the m-th sample trajectory xm, we follow the classic routine: sample point process Nm(t) (e.g., Hawkes process) using thinning algorithm (Ogata, 1981), Wiener process wm(t) from Gaussion distribution, and apply the Euler method (Hanson, 2007) to obtain xm.\nNext, we compute wm = exp(\u2212S(xm)/\u03b3) by evaluating the state cost, and compute \u222b tk+1 tk\ndNmi (s) as the number of events that occurred during [tk, tk+1) at the i-th dimension. Moreover, since \u03bbi(t) has a parametric form,\n\u222b tk+1 tk\n\u03bbmi (s)ds can also be computed numerically or in close form. The analytical form exists for the Hawkes process. Note that since \u03bbmi (t) is history-dependent, given the events history in the m-th sample, \u03bbmi (t) is fixed. Hence, the sample average approximation of (17) is:\nuk\u2217i =\n\u2211I m=1 w m \u222b tk+1 tk\ndNmi (s)\u2211I m=1 w m \u222b tk+1 tk \u03bbmi (s)ds (18)\nNext, we discuss the properties of our policy. Stochastic Intensity. The intensity function \u03bbi(t) can be history independent and stochastic, (e.g., Hawkes process). Since \u03bbi(t) is inside the expectation EP in (17), our policy naturally considers its stochasticity by taking its expectation. To the best of our knowledge, this is the first work that is able to control a stochastic intensity function. On the contrary, prior works (Br\u00e9maud, 1981; Zarezade et al., 2016) focus on controlling deterministic intensity such as Poisson process.\nCausality. The control is causal and does not depend on specific realizations of future, and only depends on the expectation of it. This is intuitive since the algorithm anticipate the expected outcomes of future when designing policies.\nScalability. We only need the SDE to sample trajectories. Since each sample is independent, it it efficient with parallelization. In sharp contrast to traditional stochastic optimal control works (Pham, 1998; Oksendal & Sulem, 2005), we do not need to approximate the nonlinear SDE or state cost S(\u00b7) nor solve the HJB PDE.\nOpen-loop policy. The control policy in (17) does not depend on the system\u2019s feedback. However, a more effective policy should take into account the current state of SDE, and integrate such feedback into the policy. Hence, we will transform open-loop policy into the feedback version in the next section.\nAlgorithm 1 KL - Model Predictive Control 1: Input: sample size I, optimization window length T\u0303 , total time window T , timestamps {tk} on [0, T ]. 2: Output: optimal control u\u2217 at each tk on [0, T ]. 3: for k = 0 to K \u2212 1 do 4: for m = 1 to I do 5: Sample dN(t), dw(t) and generate xm on [tk, tk + T\u0303 ] according to (2) and the current state. 6: S(xm) = \u222b T 0 q(xm)dt+ \u03c6(xm), wm = exp(\u2212 1\u03b3S)\n7: end for 8: Compute uk\u2217i from (18) for each i, and execute uk\u2217, receive state feedback and update state. 9: end for"}, {"heading": "5.3 From Open-loop policy to feedback policy", "text": "To design the feedback policy, we use the model predictive control (MPC) scheme (Camacho & Alba, 2013), where the Model of the process is used to Predict the future evolution of the process to optimize the Control. In MPC, online optimization and execution are interleaved as follows. (i) Online optimization. At time t, a control policy u\u2217 is computed using (18) for a short time horizon\nT\u0303 T in the future. That is, we only need to sample trajectories on [t, t+ T\u0303 ] for computation instead of [0, T ]. (ii) Execution. Apply the first optimal move u\u2217(t) at this time t, and observe the new system state. (iii) Feedback & repeat optimization. At next time t+ 1, with the new observed state, we re-compute the\ncontrol again and repeat the above process. Algorithm 1 summarizes the procedure. The main advantage of MPC is that it yields a feedback control that implicitly depends on the current state. Moreover, separating the optimization horizon T\u0303 from T is also advantageous since it makes little sense to consider choosing a deterministic set of actions far out into the future."}, {"heading": "6 Experiments", "text": "We focus on two applications: least square opinion guiding and smart broadcasting, and evaluate our framework with synthetic and real world data. We compare with the state-of-arts in reinforcement learning and heuristics. \u2022 Cross Entropy (CE) (Stulp & Sigaud, 2012): It samples controls from a Gaussian distribution, sorts the\nsamples in ascending order with respect to the cost and recomputes the distribution parameters based on the first K elite samples. Then returns to the first step with new distribution, until costs converge. \u2022 Finite Difference (FD) (Peters & Schaal, 2006): It generates I samples of perturbed policies u+ \u2206u and computes perturbed cost S + \u2206S. Then uses them to approximate the true gradient of the cost to policy. \u2022 Greedy: It controls the system when local state cost is high. We divide the window into n state cost observation timestamps. At each timestamp, Greedy computes state cost and controls the system based on pre-specified control rules if current cost is more than k times of the optimal cost of our algorithm. It will stop if it has reached the current budget bound. We vary k from 1 to 5, n from 1 to 100 and report the best performance. \u2022 Base Intensity (BI) (Farajtabar et al., 2014): It sets the policy for the base intensity of Hawkes process only at initial time and does not consider the system feedback. We provide both MPC and open-loop (OL) versions for our KL algorithm, Finite Difference and Cross Entropy. For MPC, we set the optimization window T\u0303 = T/10 and sample size I = 1000. It is efficient to generate these samples and takes less than one second using parallelization."}, {"heading": "6.1 Experiments on Opinion Guiding", "text": "Experimental setup. We generate a social network with 1000 users. Specifically, we simulate the opinion SDE on window [0, 50] by applying Euler forward method to compute the difference form of (2). The time window is divided into 500 timestamps. We set the initial opinion x0 = \u221210, the target opinion a = 1, \u03b2 = 0.2, and network adjacency matrix A generated uniformly on [0, 0.01] with sparsity of 0.001. Appendix F contains details on simulation. We set the tradeoff (budget level) parameter \u03b3 = 10, and our results generalize beyond this value.\nNetwork visualization. Figure 1 shows the controlled opinion at different times. Appendix G contains more results with four choices of initial and target state. It shows our method works efficiently with fast convergence speed.\nState cost & trajectory. Figure 2 (a) shows the instantaneous cost over time, which is computed by evaluating \u2016x(t)\u2212 a\u2016 at each time t on the observation window. The system is gradually steered towards the target. Hence the cost is decreasing. Our KL-MPC achieves the lowest instantaneous cost over time and has the fastest convergence to the optimal cost. Hence the overall state cost is also the lowest, as shown in Figure 2 (b).\nOur KL-MPC has 3\u00d7 cost improvement than CE-MPC, with less variance and faster convergence to the target. This is because KL-MPC is more flexible and has less restrictions on the control policy. However, CEMPC assumes the control is sampled from a Gaussian distribution, which might not be the ideal assumption in intensity control. Due to its simplicity and generality, CE-MPC is a popular method for the traditional control problem in robotics, where the SDE does not contain the jump term and control is in the drift. However, the guarantees for Gaussian assumption may not be applicable for our problem. The FD-MPC performs worse than CE due to the error in the gradient estimation process, which will not lead to the true descending direction. Finally, for the same method, MPC always performs better than the OL version, which\nshows the importance of incorporating state feedback to the policy. Controlled intensity. Figure 3 (a) and (b) compare the controlled intensity with the uncontrolled Hawkes intensity at the beginning period. Since the objective is to influence everyone to be positive, (a) shows that if the user tweets positive opinion, the control naturally will increase its intensity to positively influence others. On the contrary, (b) shows that if the user\u2019s opinion is negative, his intensity should be controlled to be small. (c) and (d) show the scenario near the terminal time. Since the system is around the target state, the control policy is small, hence the original and controlled intensity are similar for both positive and negative users."}, {"heading": "6.2 Experiments on Smart Broadcasting", "text": "Experimental setup. We evaluate on a real world Twitter dataset (Farajtabar et al., 2015), which contains 82,767 users with 322,666 tweets/retweets during Sep. 21 - 30, 2012. For broadcasters, we track down all followers and record all the tweets they posted and reconstruct followers\u2019 timelines by collecting all the tweets by people they follow. We first learn the parameters of the Poisson and Hawkes processes that capture each user\u2019s tweeting behavior by maximizing the likelihood function (Karimi et al., 2016). We then pick one broadcaster and obtain his competitors by selecting his followers and other broadcasters that these followers follow. With the learned parameters, we simulate broadcasting events on [0, 10] and divided it into 10 timestamps. We then conduct control over the simulated dynamics. Appendix F contains details. The simulation is repeated for 10 runs and on each simulated data we compare different methods.\nState cost. Figure 4 (a) compares the costs of different methods in terms of average rank of the picked broadcaster at each time. We compute the average rank by dividing the state cost by the window length 10. Our KL-MPC consistently performs the best. It achieves the lowest average rank and is 4\u00d7 lower than the CE-MPC. our method achieves the rank around 1.5 at each time, which is nearly the ideal scenario that the broadcaster always remains on top-1.\nControlled intensity. Figure 4 (b) further compares the controlled intensity of the broadcaster with the uncontrolled intensity of other competitors. It clearly shows that KL-MPC policy adaptively increases\nhis intensity whenever the intensity of other competitors is large, and decreases his intensity whenever competitor\u2019s intensity is small. For example, around timestamp 2 and 4, competitors have huge increase in their broadcasting intensities, hence in order to remain on top, this broadcaster needs to double his intensity to create more posts. Moreover, on window [6.5, 8] when others are not active, he keeps a low intensity adaptively.\nBudget sensitivity. Figure 5 further shows our method performs best consistently as budget level decreases in these two applications. Large value of the cost tradeoff parameter \u03b3 means small budget. As budget decreases, the control policy has less effect on the dynamics, hence the state cost increases."}, {"heading": "7 Conclusion", "text": "We have presented the framework to control the stochastic intensity function of point processes, such that the nonlinear SDEs can be steered towards a target state. We exploit the novel view of finding optimal probability measure induced by the policy. We further provide a scalable algorithm with superior performance in different social problems. Moreover, our framework can also can handle the traditional problems where the control is in the drift (Appendix H contains details). Hence it offers one of the most general ways for stochastic optimal control. There are many interesting venues for future work. For example, we can apply our framework to other interesting problems such as influence maximization and activity shaping (Kempe et al., 2003)."}, {"heading": "A Proof of Theorem 2", "text": "Theorem 2. Set (\u2126,F) be a measurable space, where \u2126 denotes the sample space and F denotes a \u03c3-algebra, and let P(\u2126) define a probability measure on F . Consider P,Q \u2208 P(\u2126), and S(x) : \u2126\u2192 < as an arbitrary measurable function. The following inequality holds:\n\u2212\u03b3 F(S(x))\ufe38 \ufe37\ufe37 \ufe38 Free energy = min Q [ EQ[S(x)]\ufe38 \ufe37\ufe37 \ufe38 State cost + \u03b3DKL(Q||P)\ufe38 \ufe37\ufe37 \ufe38 Relative entropy ] (19)\nwhere EP,EQ is the expectation under the probability measure P,Q, respectively. DKL(Q||P) = EQ[log(dQdP )] is the relative entropy. F(S(x)) = log ( EP [ exp(\u2212 1\u03b3S(x)) ])\nis the free energy. \u03b3 > 0 is the trade off parameter. The minimum in (10) is attained at optimal measure Q\u2217 given by:\ndQ\u2217\ndP = exp(\u2212 1\u03b3S(x)) EP[exp(\u2212 1\u03b3S(x))]\n(20)\nTo prove the theorem, we first establish necessary definitions of several information theoretic concepts as follows. Let (\u2126,F) be a measurable space, where \u2126 denotes the sample space and F denotes a \u03c3-algebra, and let P(\u2126) define a probability measure on the \u03c3-algebra F . Definition 5. Let P \u2208 P(\u2126) and Q \u2208 P(\u2126), then the relative entropy of P with respect to Q is defined as:\nDKL(Q||P) = \u222b\nlog dQ dP dQ (21)\nDefinition 6. Let P \u2208 P(\u2126) and function S(x) : \u2126\u2192 < be a measurable function. Then the following term:\nlog ( EP [\nexp ( \u2212 1 \u03b3 S(x)\n)]) = log \u222b exp(\u2212 1\n\u03b3 S(x))dP (22)\nis called the free energy of S(x) with respect to P. With the definition of the relative entropy and free energy, we now begin the proof. Proof of Theorem 2. The proof contains two parts. First, we will prove (19) using the Jensen\u2019s inequality. The second part is to show the minimum is reached at (20).\nTo prove the first part, we first express EP in the free energy term in Theorem 2 as a function of the expectation EQ. More specifically, we have:\nlog ( EP [\nexp ( \u2212 1 \u03b3 S(x)\n)]) = log (\u222b exp\n( \u2212 1 \u03b3 S(x)\n) dP ) = log (\u222b exp\n( \u2212 1 \u03b3 S(x) ) dP dQ\ndQ )\n(23)\nThen we can take apply the Jensen\u2019s inequality to the free energy term that puts the log operator inside the integral:\nlog ( EP [\nexp ( \u2212 1 \u03b3 S(x)\n)]) = log (\u222b exp\n( \u2212 1 \u03b3 S(x) ) dP dQ\ndQ ) > \u222b log ( exp\n( \u2212 1 \u03b3 S(x) ) dP dQ\n) dQ (24)\nMoreover, using the property that log(ab) = log a+ log b and log(1/a) = \u2212 log a, the right-hand-side of the above inequality can be written as:\n\u222b log ( exp\n( \u2212 1 \u03b3 S(x) ) dP dQ\n) dQ = \u222b ( \u2212 1 \u03b3 S(x) + log dP dQ ) dQ\n= \u222b \u2212 1 \u03b3 S(x)dQ + \u222b log dP dQ dQ\n= \u222b \u2212 1 \u03b3 S(x)dQ\u2212 \u222b log dQ dP dQ\n= \u2212 1 \u03b3 EQ[S(x)]\u2212 DKL(Q||P) (25)\nHence, combining (24) and (25), we have:\nlog ( EP [\nexp ( \u2212 1 \u03b3 S(x)\n)]) > \u2212 1\n\u03b3 EQ[S(x)]\u2212 DKL(Q||P) (26)\nFinally, since \u03b3 > 0, multiply both sides of (27) by \u2212\u03b3 yields:\n\u2212\u03b3 log ( EP [\nexp ( \u2212 1 \u03b3 S(x)\n)]) 6 EQ[S(x)] + \u03b3DKL(Q||P) (27)\nThis finishes the proof of (19), the first part of the theorem. Next, we will show the minimum is reached at Q\u2217 given by (20).\nTo prove the second part, we will substitute (20) to the right-hand-side of (27) to show that the infimum is actually reached with this Q\u2217. More specifically,\nEQ\u2217 [S(x)] + \u03b3DKL(Q\u2217||P) = EQ\u2217 [S(x)] + \u03b3 \u222b log dQ\u2217\ndP dQ\u2217\n= EQ\u2217 [S(x)] + \u03b3 \u222b log exp(\u2212 1\u03b3S(x))\nEP[exp(\u2212 1\u03b3S(x))] dQ\u2217\n= EQ\u2217 [S(x)] + \u03b3 \u222b \u2212 1 \u03b3 S(x)dQ\u2217 \u2212 \u03b3 \u222b log ( EP [ exp ( \u2212 1 \u03b3 S(x) )]) dQ\u2217 (28) = EQ\u2217 [S(x)]\u2212 \u222b S(x)dQ\u2217 \u2212 \u03b3 log ( EP [\nexp ( \u2212 1 \u03b3 S(x)\n)])\u222b dQ\u2217\n= EQ\u2217 [S(x)]\u2212 EQ\u2217 [S(x)]\u2212 \u03b3 log ( EP [\nexp ( \u2212 1 \u03b3 S(x)\n)]) (29)\n= \u2212\u03b3 log ( EP [\nexp ( \u2212 1 \u03b3 S(x)\n)])\nwhere (28) is due to the property log(a/b) = log a\u2212 log b and (29) is due to the fact that Q\u2217 is a probability measure hence \u222b dQ\u2217 = 1. Hence the infimum is reached and this finishes the proof of the second part."}, {"heading": "B Proof of Theorem 4", "text": "Theorem 4. For the intensity control problem in (6), we have: dPdQ(u) = exp ( D(u) ) , where D(u) is expressed as: M\u2211\ni=1\n\u222b T\n0\n( ui(s)\u2212 1 ) \u03bbi(s)ds\u2212\n\u222b T\n0\nlog ( ui(s) ) dNi(s)\nProof of Theorem 4. Intuitively, the derivative dP/dQ(u) means the relative density of probability distribution P with respect to Q. The change of probability measure happens because the intensity of the point process that drives the SDE in (2) is changed from \u03bb(t) to \u03bb(u, t) in (6). Hence dP/dQ(u) describes the change of probability measure for point processes and is the likelihood ratio between the uncontrolled and controlled point process (Br\u00e9maud, 1981):\ndP dQ(u) = exp\n( L(\u03bb) )\nexp ( L(\u03bb(u))\n) = exp ( D(u) ) ,\nwhere L is the log-likelihood for the multi-dimension point process with L(\u03bb) = \u2211Mm=1 L(\u03bbi). It is defined as the summation of each dimension m\u2019s log-likelihood L(\u03bbi), and L(\u03bbi) is defined as follows (Aalen et al., 2008):\nL(\u03bbi(t)) = \u222b T\n0\nlog(\u03bbi(t))dNi(t)\u2212 \u222b T\n0\n\u03bbi(t)dt (30)\nwhere \u222b f(t)dN(t) := \u2211 i f(ti) is defined the summation of function f(\u00b7)\u2019s value at each event time.\nHence, D(u) denotes the difference of the log-likelihood between these two point processes:\nD(u) = L(\u03bb(t))\u2212 L(\u03bb(u(t), t))\n=\nM\u2211\ni=1\n(\u222b T\n0\n( \u03bbi(ui(s), s)\u2212 \u03bbi(s) ) ds\u2212\n\u222b T\n0\nlog (\u03bbi(ui(s), s)\n\u03bbm(s)\n) dNi(s)\n)\n=\nM\u2211\ni=1\n(\u222b T\n0\n( ui(s)\u03bbi(s)\u2212 \u03bbi(s) ) ds\u2212\n\u222b T\n0\nlog ( ui(s) ) dNi(s) ) (31)\n=\nM\u2211\ni=1\n(\u222b T\n0\n( ui(s)\u2212 1 ) \u03bbi(s)ds\u2212\n\u222b T\n0\nlog ( ui(s) ) dNi(s)\n)\nwhere M is the dimension of point process. (31) comes from the form of control in (6). \u03bbi(t), Ni(t), ui(t) denote the m-th dimension of \u03bb(t),N(t),u(t)."}, {"heading": "C Detailed Derivations of the Optimal Control Policy in (17)", "text": "We will formulate our objective function based on the form of optimal measure Q\u2217 in (11). More specifically, we find a control u which pushes the controlled measure Q(u), as close to the optimal measure as possible. This leads to minimizing the Kullback-Leibler (KL) distance:\nu\u2217 = argmin u\u2208U DKL(Q\u2217||Q(u)) (32)\nwhere U is the set of admissible control policies. This objective function is in sharp contrast to traditional methods that solve the optimal control problem by computing the solution the HJB PDE, which have severe limitations in scalability and feasibility to nonlinear jump diffusion SDEs.\nNext we will simplify the objective function. According to the definition of relative entropy and chain rule of derivatives, we have:\nDKL(Q\u2217||Q(u)) = EQ\u2217 [ log ( dQ\u2217\ndQ(u)\n)] = EQ\u2217 [ log ( dQ\u2217\ndP dP dQ(u)\n)] (33)\nThe derivative dQ\u2217/dP is given in Theorem 2 and dP/dQ(u) is given in Theorem 4. Hence, we then substitute dQ\u2217/dP and dP/dQ(u) to (33). After removing terms which are independent of u, the objective function (32) is simplified as:\nu\u2217 = argmin u\u2208U EQ\u2217 [D(u)]\nNext we parameterize u(t) as a piecewise constant function on [0, T ]:\nu(t) =    ... uk for t \u2208 [k\u2206t, (k + 1)\u2206t) ...\n(34)\nMore specifically, the k-th piece is defined on [k\u2206t, (k + 1)\u2206t) as uk, where k = 0, \u00b7 \u00b7 \u00b7 ,K \u2212 1, tk = k\u2206t and T = tK . Then we have:\nEQ\u2217 [D(u)] = M\u2211\ni=1\nK\u2211\nk=1\n( EQ\u2217 [ \u222b tk+1 tk (uki \u2212 1)\u03bbi(s)ds ] \u2212 EQ\u2217 [ \u222b tk+1 tk log(uki )dNi(s) ])\n(35)\nwhere uki is the m-th dimension of uk. To compute uki , we can neglect the two summation terms in (35) and only focus on the parts that involves uki . Then we move uki outside of the expectation and discard any constant terms. This yields the function that only involves uki :\nf(uki ) = u k i EQ\u2217 [ \u222b tk+1 tk \u03bbi(s)ds ] \u2212 log(uki )EQ\u2217 [ \u222b tk+1 tk dNi(s) ]\n(36)\nWe can then show f(uki ) is convex in uki . More specifically, it is in the form of f(x) = ax\u2212 log(x)b with a > 0, b > 0 and f \u2032\u2032(x) > 0. Finally, setting f \u2032(uki ) = 0 yields uk\u2217i :\nuk\u2217i = EQ\u2217\n[ \u222b tk+1 tk dNi(s) ]\nEQ\u2217 [ \u222b tk+1 tk \u03bbi(s)ds ] (37)\nHowever, uk\u2217i is still not computable since the expectation is taken under the optimal probability measure Q\u2217. Since we only known the SDE of the uncontrolled dynamics and can only compute the expectation under P, we need to change the expectation from EQ\u2217 to EP to compute uk\u2217i .\nTo do this, we first provide a general Lemma 7 as follows.\nLemma 7. With probability measure Q\u2217 defined as dQ \u2217 dP = exp(\u2212 1\u03b3 S(x))\nEP[exp(\u2212 1\u03b3 S(x))] in (11), for any measurable function\ng(x) : \u2126\u2192 <, we have:\nEQ\u2217 [g(x)] = EP [ exp ( \u2212 1\u03b3S(x) ) g(x) ]\nEP[exp(\u2212 1\u03b3S(x))]\nProof.\nEQ\u2217 [g(x)] = \u222b g(x)dQ\u2217\n= \u222b g(x)\nexp(\u2212 1\u03b3S(x))dP EP[exp(\u2212 1\u03b3S(x))]\n=\n\u222b ( g(x) exp ( \u2212 1\u03b3S(x) )) dP\nEP[exp(\u2212 1\u03b3S(x))]\n= EP [ exp ( \u2212 1\u03b3S(x) ) g(x) ]\nEP[exp(\u2212 1\u03b3S(x))]\nFinally, applying Lemma 7 to (37), we have:\nuk\u2217i = EQ\u2217\n[ \u222b tk+1 tk dNi(s) ]\nEQ\u2217 [ \u222b tk+1 tk \u03bbi(s)ds ] =\nEP [ exp(\u2212 1\u03b3 S(x)) \u222b tk+1 tk dNi(s) ]\nEP [ exp(\u2212 1\u03b3 S(x)) ]\nEP [ exp(\u2212 1\u03b3 S(x)) \u222b tk+1 tk \u03bbi(s)ds ]\nEP [ exp(\u2212 1\u03b3 S(x))\n] =\nEP [ exp(\u2212 1\u03b3S(x)) \u222b tk+1 tk dNi(s) ] EP [ exp(\u2212 1\u03b3S(x)) \u222b tk+1 tk \u03bbi(s)ds ] (38)\nThis yields (17)."}, {"heading": "D Derivations of the Control Cost", "text": "We will derive the control cost in (12), which comes naturally from the dynamics. According to the definition of the relative entropy, we have,\nDKL(Q||P) := EQ[log( dQ dP )] = EQ[C(u)] (39)\nHence, the next step is to compute the Radon-Nikodym derivative dQdP . This derivative means the relative density of probability distribution Q with respect to P. According to (Br\u00e9maud, 1981), we have:\ndQ dP = exp\n(\u2211\ni\n\u222b T\n0\nlog (\u03bbi(ui(t), t)\n\u03bbi(t)\n) dNi(ui(t), t)\u2212\n\u222b T\n0\n(\u03bbi(ui(t), t)\u2212 \u03bbi(t))dt ) , (40)\nUsing the relationship that \u03bbi(ui(t), t) = \u03bbi(t)ui(t), we have:\nEQ[log( dQ dP\n)] = EQ [\u2211\ni\n\u222b T\n0\nlog (\u03bbi(ui(t), t)\n\u03bbi(t)\n) dNi(ui(t), t)\u2212\n\u222b T\n0\n(\u03bbi(ui(t), t)\u2212 \u03bbi(t))dt ]\n(41)\n= EQ [\u2211\ni\n\u222b T\n0\nlog ( ui(t) ) dNi(ui(t), t)\u2212\n\u222b T\n0\n( 1\u2212 1\nui(t)\n) \u03bbi(ui(t), t)dt ] (42)\n= EQ [\u2211\ni\n\u222b T\n0\nlog ( ui(t) ) \u03bbi(ui(t), t)dt+\n\u222b T\n0\n( 1\u2212 1\nui(t)\n) \u03bbi(ui(t), t)dt ] (43)\nNote that (42) to (43) follows from the Campbell theorem (Daley & Vere-Jones, 2007). Therefore, the control cost is:\nC(u) =\n\u222b T\n0\n\u2211 i ( log(ui(t)) + 1 ui(t) \u2212 1 ) \u03bbi(ui(t), t)dt"}, {"heading": "E Details on the Smart Broadcasting", "text": "We will provide more details on the broadcaster position model in section 3.2. Specifically, for one fixed broadcaster i, we consider the network in Figure 6. F(i) denotes the collection of all followers of user i. For any follower j \u2208 F(i), We use xj(t) to denote the rank of i\u2019s posts among all the posts that j receives. His rank is decided by his broadcasting behavior and the behavior of all other broadcasters that user j follows. If his post is the top-1 post among the news feed of follower j, xj(t) = 1. This is the best scenario for the broadcaster i.\nMathematically, Ni(t) captures broadcaster i\u2019s posting behavior and the Hawkes process No(t) to capture the behavior of all other broadcasters that j follows. The dynamics that describes the change of xj(t) is as follows.\ndxj(t) = dNo(t)\ufe38 \ufe37\ufe37 \ufe38 1. rank increase\n\u2212 ( xj(t)\u2212 1 ) dNi(t)\ufe38 \ufe37\ufe37 \ufe38\n2. rank decrease\n(44)\nwhere each term models one of the two possible situations: (1) dNo(t) \u2208 {0, 1}. If dNo(t) = 1, the most recent message was posted by other broadcasters (competitors).\nHence his rank will increase by 1. dNo(t) = 0 does not change the SDE. (2) dNi(t) \u2208 {0, 1}. If dNi(t) = 1, the most recent message was posted by broadcaster i. Hence his rank will\ndecrease from current rank xj(t) to 1 since his post is the most recent. dNi(t) = 0 does not change the SDE."}, {"heading": "F Details on Experimental Setup", "text": "Opinion guiding. We use x(t) \u2208 <M to represent the vector of each people\u2019s opinion in the social network with M users, then the vector form of the opinion SDE in (3) is as follows.\ndx(t) = ( b\u2212 x(t) ) dt\n\ufe38 \ufe37\ufe37 \ufe38 drift\n+\u03b2dw(t)\ufe38 \ufe37\ufe37 \ufe38 diffusion +h(x(t))dN(u(t), t)\ufe38 \ufe37\ufe37 \ufe38 jump\n(45)\nWe consider a social network with 1000 users and simulate the opinion SDE on the observation window [0, 50] by applying Euler forward method to compute the difference form of (45) as follows.\nxk+1 = xk + (b\u2212 xk)\u2206t+ \u03b2\u2206wk + h(xk)\u2206Nk, x0 = \u221210 (46)\nwhere the observation window [0, 50] is divided into 500 timestamps {tk} with \u2206t = tk+1 \u2212 tk = 0.1. The Wiener increments \u2206w is sampled from the normal distribution N (0, \u221a \u2206t) and the Hawkes increments \u2206Nk is computed by counting the number of events on [tk, tk+1) for each user. The Hawkes process is simulated by the Otaga\u2019s thinning algorithm (Ogata, 1981; Farajtabar et al., 2015) with parameter \u03b7 = 0.01 and \u03b1 generated uniformly on [0, 0.01] with sparsity of 0.001. The thinning algorithm is essentially a rejection sampling algorithm where samples are first proposed from a homogeneous Poisson process and then samples are kept according to the ratio between the actual intensity and that of the Poisson process. We set the initial opinion x0 = \u221210, the target a = 1, \u03b2 = 0.2 and network adjacency matrix A generated same way as \u03b1.\nSmart broadcasting. We evaluate on a real-world Twitter dataset (Farajtabar et al., 2015), which contains 82,767 users who post 322,666 tweets/retweets during Sep 21-Sep 30 2012. For each of the broadcasters, we track down all their followers and record all the tweets they posted and reconstruct followers\u2019 timelines by collecting all the tweets by people they follow. We first learn the parameters of the Poisson and Hawkes process that captures each user\u2019s tweeting/retweeting behavior by maximizing the likelihood function using the algorithm in Karimi et al. (2016).\nWe then pick one broadcaster and obtain his followers and all other broadcasters that these followers follow. With learned parameters, for each follower j, we simulate the SDE describing the rank of change on [0, 10] by compute the difference form of (4) as follows.\nxk+1j = \u2206N k o \u2212 (xkj \u2212 1)\u2206Nki , x0j = 1 (47)\nwhere the time window is divided into 10 timestamps {tk}. \u2206Nk is computed by counting the number of broadcasting events of this broadcaster on [tk, tk+1) and \u2206Nko is computed by counting the number of broadcasting events of all other broadcasters that user j follows on [tk, tk+1). The Poisson process Ni(t) and Hawkes process No(t) are simulated using the Otaga\u2019s thinning algorithm (Ogata, 1981)."}, {"heading": "G Additional Experiments on Opinion Guiding", "text": "We conduct control over four networks with different initial and target states. Figure 7 shows our framework works efficiently."}, {"heading": "H Applying Our Framework to Traditional Stochastic Optimal Control Problem", "text": "In the traditional stochastic optimal control problem, the control policy u(t) is affine in the drift as follows:\ndx(t) = ( f(x) +G(x)u(t) ) dt\ufe38 \ufe37\ufe37 \ufe38\ndrift\n+ g(x)dw(t)\ufe38 \ufe37\ufe37 \ufe38 diffusion +h(x)dN(t)\ufe38 \ufe37\ufe37 \ufe38 jump\n(48)\nwhere G(x) : <K \u2192 <K . Note that in this case, the jump process N(t) is not controlled. The goal is also to find u\u2217 \u2208 <K such that\nu\u2217 = argmin u EQ[S(x)] + \u03b3 \u2217 control cost, (49)\nNext we use our framework to solve the problem as follows. First, the objective is the same as (13):\nu\u2217 = argmin u\u2208U\nDKL ( Q\u2217||Q(u) ) (50)\nwhere U is the set of admissible control policies. Next we will also simplify the objective function. According to the definition of relative entropy and chain rule of derivatives, we have:\nDKL(Q\u2217||Q(u)) = EQ\u2217 [ log ( dQ\u2217\ndP dP dQ(u)\n)] (51)\nThe derivative dQ\u2217/dP is given in (11). Hence we just need to compute dP/dQ(u). Intuitively, the derivative dP/dQ(u) means the relative density of probability distribution P with respect to Q.\nIn the intensity control problem, the change of measure happens because the intensity of the temporal point process that drives the SDE in (2) is changed from \u03bb(t) to \u03bb(u, t). However, in the traditional problem, the change of measure is due to the fact that the drift term in the SDE is changed by the control policy.\nHence according to the classic Girsanov\u2019s theorem Hanson (2007) in probability theory, it is in the form:\ndP dQ(u) = exp ( D(u) ) ,\nwhere D(u) is defined as:\nD(u) = \u2212 \u222b T\n0\nu(t)>G(x)>\u03a3\u22121g(x)dw + 1\n2\n\u222b T\n0\nu(t)>G(x)>\u03a3\u22121u(t)dt\nwhere \u03a3 = gg>. We then substitute dQ\u2217/dP and dP/dQ(u) to (51). After removing terms which are independent of u, (50) is simplified as:\nu\u2217 = argmin u\u2208U EQ\u2217 [D(u)] (52)\nSince we apply the control in discrete timestamps, it suffices to consider U as the class of piecewise constant functions on [0, T ], with the k-th piece of u defined on [k\u2206t, (k + 1)\u2206t) as uk, where k = 0, \u00b7 \u00b7 \u00b7 ,K \u2212 1, tk = k\u2206t and T = tK . Then we express the objective function as:\nEQ\u2217 [D(u)] = K\u2211\nk=1\n( 1\n2 uk>EQ\u2217 [ \u222b tk+1 tk G>\u03a3\u22121Gdt ] uk \u2212 uk>EQ\u2217 [ \u222b tk+1 tk G>\u03a3\u22121gdw ])\n(53)\nWe can neglect the summation term in (53) and only focus on the parts that involves uk. Since the expression is quadratic in uk, it is convex in uk. Finally, setting the gradient to zero yields:\nuk\u2217 = EQ\u2217\n[ \u222b tk+1 tk G>\u03a3\u22121gdw ]\nEQ\u2217 [ \u222b tk+1 tk G>\u03a3\u22121Gdt ] =\nEP [ exp(\u2212 1\u03b3S(x)) \u222b tk+1 tk G>\u03a3\u22121gdw ] EP [ exp(\u2212 1\u03b3S(x)) \u222b tk+1 tk G>\u03a3\u22121Gdt ] (54)\nThen we can also use Algorithm 1 to compute the optimal control policy."}], "references": [{"title": "Survival and event history analysis: a process point of view", "author": ["Aalen", "Odd", "Borgan", "Ornulf", "Gjessing", "Hakon"], "venue": null, "citeRegEx": "Aalen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Aalen et al\\.", "year": 2008}, {"title": "Stochastic optimal control under poisson-distributed observations", "author": ["Ades", "Michel", "Caines", "Peter E", "Malham\u00e9", "Roland P"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Ades et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ades et al\\.", "year": 2000}, {"title": "Steering user behavior with badges", "author": ["Anderson", "Ashton", "Huttenlocher", "Daniel", "Kleinberg", "Jon", "Leskovec", "Jure"], "venue": "In WWW,", "citeRegEx": "Anderson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 2013}, {"title": "Liquidation in limit order books with controlled intensity", "author": ["Bayraktar", "Erhan", "Ludkovski", "Michael"], "venue": "Mathematical Finance,", "citeRegEx": "Bayraktar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bayraktar et al\\.", "year": 2014}, {"title": "Optimal control of jump processes", "author": ["R Boel", "P. Varaiya"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Boel and Varaiya,? \\Q1977\\E", "shortCiteRegEx": "Boel and Varaiya", "year": 1977}, {"title": "Model predictive control", "author": ["Camacho", "Eduardo F", "Alba", "Carlos Bordons"], "venue": "Springer Science & Business Media,", "citeRegEx": "Camacho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Camacho et al\\.", "year": 2013}, {"title": "Connections between stochastic control and dynamic games", "author": ["Dai Pra", "Paolo", "Meneghini", "Lorenzo", "Runggaldier", "Wolfgang J"], "venue": "Mathematics of Control, Signals and Systems,", "citeRegEx": "Pra et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Pra et al\\.", "year": 1996}, {"title": "An introduction to the theory of point processes: volume II: general theory and structure, volume", "author": ["D.J. Daley", "D. Vere-Jones"], "venue": null, "citeRegEx": "Daley and Vere.Jones,? \\Q2007\\E", "shortCiteRegEx": "Daley and Vere.Jones", "year": 2007}, {"title": "Learning opinion dynamics in social networks", "author": ["De", "Abir", "Valera", "Isabel", "Ganguly", "Niloy", "Bhattacharya", "Sourangshu", "Rodriguez", "Manuel Gomez"], "venue": "arXiv preprint arXiv:1506.05474,", "citeRegEx": "De et al\\.,? \\Q2015\\E", "shortCiteRegEx": "De et al\\.", "year": 2015}, {"title": "Time sensitive recommendation from recurrent user activities", "author": ["Du", "Nan", "Wang", "Yichen", "He", "Niao", "Song", "Le"], "venue": "In NIPS,", "citeRegEx": "Du et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "Shaping social activity by incentivizing users", "author": ["Farajtabar", "Mehrdad", "Du", "Nan", "Gomez-Rodriguez", "Manuel", "Valera", "Isabel", "Zha", "Hongyuan", "Song", "Le"], "venue": "In NIPS,", "citeRegEx": "Farajtabar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Farajtabar et al\\.", "year": 2014}, {"title": "Coevolve: A joint point process model for information diffusion and network co-evolution", "author": ["Farajtabar", "Mehrdad", "Wang", "Yichen", "Gomez-Rodriguez", "Manuel", "Li", "Shuang", "Zha", "Hongyuan", "Song", "Le"], "venue": "In NIPS,", "citeRegEx": "Farajtabar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Farajtabar et al\\.", "year": 2015}, {"title": "Integral, measure, and derivative: a unified approach", "author": ["Gurevich", "Boris Lazarevich"], "venue": "Courier Corporation,", "citeRegEx": "Gurevich and Lazarevich,? \\Q1966\\E", "shortCiteRegEx": "Gurevich and Lazarevich", "year": 1966}, {"title": "Applied stochastic processes and control for Jump-diffusions: modeling, analysis, and computation, volume", "author": ["Hanson", "Floyd B"], "venue": null, "citeRegEx": "Hanson and B.,? \\Q2007\\E", "shortCiteRegEx": "Hanson and B.", "year": 2007}, {"title": "Hawkestopic: A joint model for network inference and topic modeling from text-based cascades", "author": ["He", "Xinran", "Rekatsinas", "Theodoros", "Foulds", "James", "Getoor", "Lise", "Liu", "Yan"], "venue": "In ICML, pp", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "An introduction to event-triggered and self-triggered control", "author": ["Heemels", "WPMH", "Johansson", "Karl Henrik", "Tabuada", "Paulo"], "venue": "In 2012 IEEE 51st IEEE Conference on Decision and Control (CDC),", "citeRegEx": "Heemels et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Heemels et al\\.", "year": 2012}, {"title": "Smart broadcasting: Do you want to be seen", "author": ["M. Karimi", "E. Tavakoli", "M. Farajtabar", "L. Song", "M. Gomez-Rodriguez"], "venue": null, "citeRegEx": "Karimi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karimi et al\\.", "year": 2016}, {"title": "Maximizing the spread of influence through a social network", "author": ["Kempe", "David", "Kleinberg", "Jon", "Tardos", "\u00c9va"], "venue": "In SIGKDD,", "citeRegEx": "Kempe et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kempe et al\\.", "year": 2003}, {"title": "Event-triggered feedback in control, estimation, and optimization", "author": ["Lemmon", "Michael"], "venue": "In Networked Control Systems,", "citeRegEx": "Lemmon and Michael.,? \\Q2010\\E", "shortCiteRegEx": "Lemmon and Michael.", "year": 2010}, {"title": "A multitask point process predictive model", "author": ["Lian", "Wenzhao", "Henao", "Ricardo", "Rao", "Vinayak", "Lucas", "Joseph E", "Carin", "Lawrence"], "venue": "In ICML,", "citeRegEx": "Lian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "Multivariate Hawkes Processes", "author": ["Liniger", "Thomas Josef"], "venue": "PhD thesis, Swiss Federal Institute of Technology Zurich,", "citeRegEx": "Liniger and Josef.,? \\Q2009\\E", "shortCiteRegEx": "Liniger and Josef.", "year": 2009}, {"title": "Sensing and actuation strategies for event triggered stochastic optimal control", "author": ["Meng", "Xiangyu", "Wang", "Bingchang", "Chen", "Tongwen", "Darouach", "Mohamed"], "venue": "In 52nd IEEE Conference on Decision and Control,", "citeRegEx": "Meng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2013}, {"title": "Divergence measures and message passing", "author": ["Minka", "Tom"], "venue": "Report 173, Microsoft Research,", "citeRegEx": "Minka and Tom.,? \\Q2005\\E", "shortCiteRegEx": "Minka and Tom.", "year": 2005}, {"title": "On lewis\u2019 simulation method for point processes", "author": ["Ogata", "Yosihiko"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Ogata and Yosihiko.,? \\Q1981\\E", "shortCiteRegEx": "Ogata and Yosihiko.", "year": 1981}, {"title": "Applied stochastic control of jump diffusions, volume 498", "author": ["Oksendal", "Bernt Karsten", "Sulem", "Agnes"], "venue": null, "citeRegEx": "Oksendal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Oksendal et al\\.", "year": 2005}, {"title": "Policy gradient methods for robotics", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Peters et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2006}, {"title": "Optimal stopping of controlled jump diffusion processes: a viscosity solution approach", "author": ["Pham", "Huy\u00ean"], "venue": "In Journal of Mathematical Systems, Estimation and Control. Citeseer,", "citeRegEx": "Pham and Huy\u00ean.,? \\Q1998\\E", "shortCiteRegEx": "Pham and Huy\u00ean.", "year": 1998}, {"title": "Path integral policy improvement with covariance matrix adaptation", "author": ["Stulp", "Freek", "Sigaud", "Olivier"], "venue": "In ICML,", "citeRegEx": "Stulp et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stulp et al\\.", "year": 2012}, {"title": "Nonlinear stochastic control and information theoretic dualities: Connections, interdependencies and thermodynamic interpretations", "author": ["Theodorou", "Evangelos A"], "venue": null, "citeRegEx": "Theodorou and A.,? \\Q2015\\E", "shortCiteRegEx": "Theodorou and A.", "year": 2015}, {"title": "Coevolutionary latent feature processes for continuous-time user-item interactions", "author": ["Wang", "Yichen", "Du", "Nan", "Trivedi", "Rakshit", "Song", "Le"], "venue": "In NIPS,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Isotonic hawkes processes", "author": ["Wang", "Yichen", "Xie", "Bo", "Du", "Nan", "Song", "Le"], "venue": "In ICML,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Mixture of mutually exciting processes for viral diffusion", "author": ["Yang", "Shuang-Hong", "Zha", "Hongyuan"], "venue": "In ICML, pp", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Redqueen: An online algorithm for smart broadcasting in social networks", "author": ["Zarezade", "Ali", "Upadhyay", "Utkarsh", "Rabiee", "Hamid", "Gomez Rodriguez", "Manuel"], "venue": "arXiv preprint arXiv:1610.05773,", "citeRegEx": "Zarezade et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zarezade et al\\.", "year": 2016}, {"title": "Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processes", "author": ["Zhou", "Ke", "Zha", "Hongyuan", "Song", "Le"], "venue": "In AISTAT,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": ", 2016a,b), and opinion dynamics (De et al., 2015).", "startOffset": 33, "endOffset": 50}, {"referenceID": 2, "context": "Moreover, promoting users\u2019 frequency of visiting business merchants can promote sales; designing various badges on Q&A sites can motivate users to answer questions and provide feedback to answers, hence increasing the online engagement (Anderson et al., 2013); Moreover, a broadcaster on Twitter may want to design a smart tweeting strategy such that his posts always remain on top of his followers\u2019 news feeds, so that he gains more attention (Karimi et al.", "startOffset": 236, "endOffset": 259}, {"referenceID": 16, "context": ", 2013); Moreover, a broadcaster on Twitter may want to design a smart tweeting strategy such that his posts always remain on top of his followers\u2019 news feeds, so that he gains more attention (Karimi et al., 2016).", "startOffset": 192, "endOffset": 213}, {"referenceID": 1, "context": "The works on event triggered control (Ades et al., 2000; Lemmon, 2010; Heemels et al., 2012; Meng et al., 2013) seems relevant, but are quite different.", "startOffset": 37, "endOffset": 111}, {"referenceID": 15, "context": "The works on event triggered control (Ades et al., 2000; Lemmon, 2010; Heemels et al., 2012; Meng et al., 2013) seems relevant, but are quite different.", "startOffset": 37, "endOffset": 111}, {"referenceID": 21, "context": "The works on event triggered control (Ades et al., 2000; Lemmon, 2010; Heemels et al., 2012; Meng et al., 2013) seems relevant, but are quite different.", "startOffset": 37, "endOffset": 111}, {"referenceID": 0, "context": "A temporal point process (Aalen et al., 2008) is a random process whose realization consists of a list of discrete events localized in time, {ti}.", "startOffset": 25, "endOffset": 45}, {"referenceID": 7, "context": "Based on Hawkes process, Farajtabar et al. (2014) designs the baseline intensity to achieve a steady state behavior.", "startOffset": 25, "endOffset": 50}, {"referenceID": 7, "context": "Based on Hawkes process, Farajtabar et al. (2014) designs the baseline intensity to achieve a steady state behavior. However, this work does not consider system feedback when designing the policy. Recently, Zarezade et al. (2016) proposes to control the broadcaster\u2019s intensity, which is driven by a Poisson process.", "startOffset": 25, "endOffset": 230}, {"referenceID": 8, "context": "1 SDE for the opinion diffusion model The opinion diffusion model considers both the content and timing of each event (De et al., 2015; He et al., 2015).", "startOffset": 118, "endOffset": 152}, {"referenceID": 14, "context": "1 SDE for the opinion diffusion model The opinion diffusion model considers both the content and timing of each event (De et al., 2015; He et al., 2015).", "startOffset": 118, "endOffset": 152}, {"referenceID": 16, "context": "Karimi et al. (2016) proposed the model that captures the change of a broadcaster\u2019s position due to the posting behavior of other competitors and himself as follows.", "startOffset": 0, "endOffset": 21}, {"referenceID": 32, "context": "Unfortunately, prior works in stochastic dynamic programming requires its form beforehand (Hanson, 2007; Zarezade et al., 2016).", "startOffset": 90, "endOffset": 127}, {"referenceID": 32, "context": "On the contrary, prior works (Br\u00e9maud, 1981; Zarezade et al., 2016) focus on controlling deterministic intensity such as Poisson process.", "startOffset": 29, "endOffset": 67}, {"referenceID": 10, "context": "\u2022 Base Intensity (BI) (Farajtabar et al., 2014): It sets the policy for the base intensity of Hawkes process only at initial time and does not consider the system feedback.", "startOffset": 22, "endOffset": 47}, {"referenceID": 11, "context": "We evaluate on a real world Twitter dataset (Farajtabar et al., 2015), which contains 82,767 users with 322,666 tweets/retweets during Sep.", "startOffset": 44, "endOffset": 69}, {"referenceID": 16, "context": "We first learn the parameters of the Poisson and Hawkes processes that capture each user\u2019s tweeting behavior by maximizing the likelihood function (Karimi et al., 2016).", "startOffset": 147, "endOffset": 168}, {"referenceID": 17, "context": "For example, we can apply our framework to other interesting problems such as influence maximization and activity shaping (Kempe et al., 2003).", "startOffset": 122, "endOffset": 142}], "year": 2017, "abstractText": "Temporal point processes are powerful tools to model event occurrences and have a plethora of applications in social sciences. While the majority of prior works focus on the modeling and learning of these processes, we consider the problem of how to design the optimal control policy for general point process with stochastic intensities, such that the stochastic system driven by the process is steered to a target state. In particular, we exploit the novel insight from the information theoretic formulations of stochastic optimal control. We further propose a novel convex optimization framework and a highly efficient online algorithm to update the policy adaptively to the current system state. Experiments on synthetic and real-world data show that our algorithm can steer the user activities much more accurately than state-of-arts.", "creator": "LaTeX with hyperref package"}}}