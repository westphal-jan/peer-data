{"id": "1702.01925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Effects of Stop Words Elimination for Arabic Information Retrieval: A Comparative Study", "abstract": "The flexibility made three stop words listing years Arabic Information Retrieval - - - General Stoplist, Corpus - Based Stoplist, Combined Stoplist - - - saw involving at this study. Three notable increment schemes taking analysed: and regression memorandum baseband weight, validation weighting, having drava dialects editing. The Idea whose go cups the statistical important whose linguistic directions although expected although objective sound, only compare bringing actual without manuals. The LDC (Linguistic Data Consortium) Arabic Newswire data past leaving sometimes addition all Lemur Toolkit. The Best Match arbitrage improper rather in the Okapi typing either back, chance tenth for of on few weighting algorithms instance also only study, stoplists upgrade retrieval effectiveness indeed that one for still BM25 reduced. The performance outstanding as after noted stoplist was give instance several such other editors.", "histories": [["v1", "Tue, 7 Feb 2017 08:49:58 GMT  (549kb)", "http://arxiv.org/abs/1702.01925v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ibrahim abu el-khair"], "accepted": false, "id": "1702.01925"}, "pdf": {"name": "1702.01925.pdf", "metadata": {"source": "META", "title": "Effects of Stop Words Elimination for AIR", "authors": ["Ibrahim Abu El-Khair"], "emails": ["iabuelkhair@gmail.com"], "sections": [{"heading": null, "text": "Based Stoplist, Combined Stoplist ---were investigated in this study. Three popular weighting schemes were examined: the inverse document frequency weight, probabilistic weighting, and statistical language modelling. The Idea is to combine the statistical approaches with linguistic approaches to reach an optimal performance, and compare their effect on retrieval. The LDC (Linguistic Data Consortium) Arabic Newswire data set was used with the Lemur Toolkit. The Best Match weighting scheme used in the Okapi retrieval system had the best overall performance of the three weighting algorithms used in the study, stoplists improved retrieval effectiveness especially when used with the BM25 weight. The overall performance of a general stoplist was better than the other two lists.\nKeywords: Arabic Information Retrieval, Stoplists, Lemur Toolkit."}, {"heading": "1. Introduction", "text": "Although most of the research in the field of information retrieval has focused on the English language, recently there has been a considerable amount of work and effort to develop information retrieval systems for languages other than English. Research and experimentation in the field of information retrieval in the Arabic language is relatively new and limited compared to the research that has been done in English, which has been dominant in the field of information retrieval for a long while. This is despite the fact that the Arabic Language is one of the five languages of the United Nations, the mother tongue of over 256 million people. In addition, because it is the language of the Qur\u2019an, it is also the second language for many Moslems and Moslem countries around the world [4].\nThis study attempts to compare the use and effect of stop words for Arabic information retrieval. Using the Lemur Toolkit, a language modeling and information retrieval package (see Methodology for more details), multiple weighting schemes, and three stopword lists are implemented in order to determine the effect of\nstop words elimination on an Arabic information retrieval system.\nThe weighting schemes to be used are the TF*IDF weight, the best match weight (BM25), and the statistical language modelling (KL). Three stop words lists will be created, a general list, a corpusbased list, and a combined list. Although Stemming is an important factor when dealing with Arabic information retrieval, it was not implemented in this study in order to isolate the effect of stop words from any other factor."}, {"heading": "2. Related Studies", "text": "Stopwords are very common words that appear in the text that carry little meaning; they serve only a syntactic function but do not indicate subject matter. These stopwords have two different impacts on the information retrieval process. They can affect the retrieval effectiveness because they have a very high frequency and tend to diminish the impact of frequency differences among less common words, affecting the weighting process. The removal of the\n2\nstopwords also changes the document length and subsequently affects the weighting process. They can also affect efficiency due to their nature and the fact that they carry no meaning, which may result in a large amount of unproductive processing [9]. The removal of the stopwords can increase the efficiency of the indexing process as 30 to 50% of the tokens in a large text collection can represent stopwords [18].\nIdentifying a stopwords list or a stoplist that contain such words in order to eliminate them from text processing is essential to an information retrieval system. Stoplists can be divided into two categories; domain independent stoplists and domain dependent stoplists. They can be created using syntactic classes or using corpus statistics, which is a more domain dependent approach, used for well-defined fields. They can also be created using a combination of the syntactic classes and corpus statistics to obtain the benefits of both approaches.\nFox [6] was the first to create an English stoplist to be used for general text based on word usage in English. He generated a stoplist that consisted of 421 words which was used later with in the Okapi retrieval system. Fox\u2019s method in creating the list is the most frequently used method; it is a domain independent approach. The problem with this method is that there are several arbitrary decisions to be taken during the creation of the list, such as the cut-off point. The elimination of some words and addition of others is based on personal judgment, which requires a certain expertise with the language in hand. There is no general standard stoplist to use in an IR experiment for the Arabic language. The stoplist used in the Lemur Toolkit is the one created by Khoja [8] when she was creating her Arabic stemmer and is relatively short (168 words). This list was used by Larkey and Connell [11] and Larkey, Ballesteros and Connell [10]. Chen and Gey [5] used a list they created by translating an English list and augmenting it with high frequency words from the corpus creating a rather large list, 1,131 words. They do not discuss the effect of the list. Savoy and Rasolofo\u2019s stoplist 1 [17] is a domain dependent list which has three problems. First they use some words preceded by the letter waw \u201c\u0648\u201d which means \u201cand\u201d in 17 words including 11 duplicates. This letter comes in its separate format in a large portion of words in the Arabic language and could precede all the words in the language with no exceptions. A more efficient way to do this is to remove it using a good stemming algorithm. Second, they remove several other single letters with the waw namely the hamza \u201c\u060c\u201d, alef \u201c\u0627, \u0623\u201d, ba\u2019 \u201c\u0628\u201d, heh \u201c\u0647\u201d. Due to the way the Arabic language is written, these letters can come separately but they are still a part of the word and removing them changes the word meaning or leave it meaningless, e.g.\n1 The stoplists for all the languages are available at http://www.unine.ch/info/clef\nthe word \u201c\u0628\u0627\u064e\u062a\u0643\u201d which could mean book, writers, or a place for learning has the letter ba\u2019 as a single separate letter and when it is removed the word is meaningless. The third problem is that some of the words used in it are not stopwords even though they appeared frequently in the analysis of the corpus statistics, for example, \u201c\u062a\u0627\u064a\u0644\u0627\u0648\u0644\u0627\u201d States, \u201c\u0629\u062f\u062d\u062a\u0645\u0644\u0627\u201d United, \u201c\u0629\u0631\u0647\u0627\u0642\u0644\u0627\u201d Cairo, etc. In addition, it is a more domain dependent list so it may not be suitable for other collections."}, {"heading": "3. Methodology", "text": "This study explores the use of stop words and their effect on Arabic information retrieval. It compares the use of three term weighting schemes, and three stoplists. These techniques are examined using a large corpus that was not available before the introduction of Arabic Cross-Language Retrieval at TREC 2001. The evaluation used the Lemur Toolkit with Arabic language capability.\nThe study evaluates these techniques using the standard recall and precision measures as the basis for comparison. It answers the following question:\n What is the effect of the stoplists on retrieval, i.e. how sensitive is retrieval to the use of stopwords;\nand which one of the lists, the general, the corpus based, or the combined list is superior to the other?\nFirst, performance of term weighting schemes without elimination of stopwords was compared, and then combinations of weighting schemes, and stoplists were run. Using statistical analysis, the effectiveness of all techniques was evaluated to determine which combination achieves the optimal performance for Arabic language retrieval."}, {"heading": "3.1. Data Set", "text": "This research used one Arabic test corpus, created in the Linguistic Data Consortium in Philadelphia, also used in the recent TREC experiments. The Arabic Newswire A corpus was created by David Graff and Kevin Walker at the Linguistic Data Consortium [13]. It is composed of articles from the Agence France Presse (AFP) Arabic Newswire. The source material was tagged using TIPSTER style SGML and was transcoded to Unicode (UTF-8). The corpus includes articles from 13 May 1994 to 20 December 2000. The data is in 2,337 compressed Arabic text data files. There are 209 Mbytes of compressed data (869 Mbytes uncompressed) with 383,872 documents containing 76 Million tokens over approximately 666,094 unique words."}, {"heading": "3.2. Query Sets and Relevance Judgments", "text": "The query set associated with the LDC corpus was created for TREC 2001 and 2002 [12, 20, 21]. It consists of 75 queries, developed at the LDC by native Arabic speakers and translated to English and\n3\nFrench. The relevance judgements for these queries were obtained using assessment pools from different runs at TREC 2001 and 2002, and using the top 70 documents from each run with an average size of 910 documents for each pool. For TREC 2001, the average number of relevant documents over the 25 queries was 164.9 with five topics having more than 300 relevant documents and another five with fewer than 25 relevant documents [22]. For TREC 2002, the average number of relevant documents over the 50 queries was 118.2 with eight topics having more than 300 relevant documents and 16 topics with less than 25 relevant documents."}, {"heading": "3.3. Retrieval Engine", "text": "The Lemur Toolkit for Language Modelling and Information Retrieval was used. The results of the experiments were mapped against the relevance judgments that are available for the data set. Standard recall and precision measures were calculated using the ireval.pl routine in the Toolkit. The evaluation was based on the use of eleven levels of recall creating the recall/precision matrix.\nThe Lemur toolkit was chosen for several reasons. It supports the construction of basic text retrieval systems using language modelling methods, as well as traditional methods such as those based on the vector space model and Okapi. It is available on the Web as open source software written in C and C++, and runs on both UNIX and Windows (NT). It was developed by collaboration between the Computer Science Department at the University of Massachusetts and the School of Computer Science at Carnegie Mellon University.\nArabic language capability was recently added to the Toolkit by Leah Larkey. This addition has solved the problem of the availability of an Arabic retrieval system for research and experimentation. The Toolkit uses Windows CodePage 1256 encoding (CP1256).\nThe Toolkit comes equipped with TextQueryRetMethods that implement a basic TF*IDF vector space model, Okapi, and a language modeling method using the Kullback-Leibler similarity measure between document and query language models. Initially these algorithms were used without stemming or stoplists. Each of these algorithms requires several parameters to be set in order to function properly. The parameters that were used were the default parameters set in Lemur. Several unofficial runs were conducted in an attempt to tune these parameters to the collection in hand but this did not improve the results over the defaults in Lemur.\nThese parameters are as follows: TF*IDF parameters: K1 = 1, B = 0.3; BM25 Parameters: K1 = 1.2, B = 0.75, K3 = 7; KL-Divergence Model parameter: The KL model needs only one parameter that is used for the smoothing algorithm applied in it. This study uses the\nsimple KL model with the Dirichlet Prior smoothing algorithm and the Dirichlet Prior parameter was set to 2000 as a typical value for that parameter since currently there is no good way of estimating it [23]."}, {"heading": "3.4. Stoplists", "text": "A general stoplist was created, based on the Arabic language structure and characteristics without any additions. All possible words or articles that may be considered a stopword were collated from the different syntactic classes in Arabic in a systematic way to ensure the completeness of the list. The word categories [1, 2] that were used are:  Adverbs.  Conditional Pronouns.  Interrogative Pronouns.  Prepositions.  Pronouns.  Referral Names/Determiners.  Relative Pronouns  Transformers (verbs, letters).  Verbal Pronouns.  Other. Choosing a word from any of these categories was based on a personal judgment. Not all the words under these categories were used, as some of them were not considered stopwords.\nThe resulting list consisted of 1,377 words. The list was checked against Khoja [8] and Alshehri\u2019s [3] lists, and two Standard English lists, the Okapi and SMART lists. The reason for having a large number of stopwords is due to the characteristics of the Arabic language. First, in Arabic several letters can be used as prefixes and may change the meaning of the word. These letters are (\" \" \"\u0641\u0643\u0644\" \" \" \" \u0623\" \"\u0628 \"), and they were used on some of the words, not all of them, that they could be used with. Second, a considerable number of the original words from these categories could be joined together and used as suffixes or prefixes, especially the pronouns. Finally, the conjunction letter WAW meaning \u201cand\u201d could be used in the same way but was not because it could be used for all Arabic words with no exception and it would not be realistic to use it.\nIn order to test the effect of a corpus-based stoplist, a second list was created. A cut-off point determining a certain number of words at which the list will stop, was decided based on the corpus statistics. Words occurring more than 25,000 times were used to create this list. Preliminary examination of the corpus wordfrequency statistics showed that this is a reasonable number to use as the cut-off point, even though it may appear to be an arbitrary decision.\nUnder this condition, 359 words occurred with a frequency of more than 25,000. Then doing a manual check to remove any content bearing word, which may not be considered a conventional stopword, from this list. The removal of these words is another\n4\narbitrary decision based on personal judgment, and the reason for doing this is that there are no clear rules on stopwords list creation. The resulting list contained 235 words. A third stoplist created using both the general and corpus-based stoplist. Combining both lists resulted in a list of 1,529 words. Of these 83 words were in common between the two lists."}, {"heading": "3.5. Experimental Setup", "text": "The data and the query set for the experiments were processed as follows.\n The 383,872 files in the data set were converted from UTF-8 format to Windows\nCode Page 1256 encoding (CP1256) for Lemur compatibility.\n The queries were converted from the ASMO 708 encoding to CP1256 encoding.\n Title and description for each of the 75 queries were extracted from the original query set.\n Several fatal spelling errors in the queries were corrected.\n The table Normalization function in the Light10 stemmer in Lemur was implemented for all\nthe runs regardless of the techniques that were used.\n The letters ( \u060c\u0625 \u060c\u0623\u0622 ) were replaced with (\u0627).  The final (\u0649) was replaced with (\u064a).  The Final (\u0629) was replaced with (\u0647).\n In order to avoid confusion each technique was given a code to represent it throughout the\nexperiments:\n Term Frequency Weighting Scheme: TFIDF.  Okapi Weighting Scheme: BM25.  Language Modeling (Kullback-Leibler\nDivergence Model): KL.\n General Stoplist: GS.  Corpus-Based Stoplist: CBS.  Combined Stoplist: CS.\nCombinations of the techniques were coded starting with the weighting scheme, and then the stoplist. For instance, BM25_CBS represents a combination of the Okapi Weighting Scheme, and the Corpus-Based Stoplist."}, {"heading": "3.6. Evaluation", "text": "The performance of each technique was evaluated using the standard measures of Recall and Precision, [9, 15]. A total of 27 runs were carried out where each run represents one or a combination of more than one of these techniques. The raw output results obtained from the RetEval application in Lemur were processed with the ireval.pl script to give recall and precision. The script does a TREC-style evaluation and the output includes:\n Total number of relevant documents.\n Total number of relevant documents retrieved.\n Average non-interpolated precision.\n Interpolated precision over the eleven levels of recall.\n Non-interpolated precision at document cut-off levels.\n Breakeven point (exact) precision."}, {"heading": "3.7. Data Analysis", "text": "Retrieval results were analyzed by calculating the differences between the Recall and Precision scores, and plotting them in the R-P graph. The Friedman Two-Way ANOVA test and the Wilcoxon MatchedPaired Signed-Rank test were used for judging whether measured differences between different methods can be considered statistically significant or not.\nHull [7] has examined the validity of different statistical techniques that are used in comparing retrieval experimental results. He states that there are two non-parametric alternatives to the t-test that make no assumptions about the shapes of the distributions of the two variables, the Wilcoxon Matched-Paired Signed-Rank test and the sign test.\nThe sign test looks only at the sign of the difference, ignoring its magnitude. If one method performs better than the other far more frequently than would be expected on average, then this is strong evidence that it is superior. The Wilcoxon test replaces each difference with the rank of its absolute value. These ranks are then multiplied by the sign of the difference, and the sum of the ranks for each group is compared to its expected value under the assumption that the two groups are equal.\nThe reason for choosing the Wilcoxon signed rank test over the sign test is that it is a more powerful and indicative test as it considers the relative magnitude in addition to the direction of the differences considered [19]. It also assumes that as differences between pairs increase, significance also increases [16].\nHull also indicates that when comparing more than two retrieval methods, the Friedman Two-Way ANOVA is appropriate. It is a non-parametric equivalent to the One-Way ANOVA that does not require any assumptions. It is used to compare observations repeated on the same subjects, which is the case in hand. It uses the ranks of the data rather than their raw values to calculate the statistic [19].\nIn this study the Friedman Two-Way ANOVA test was used to indicate if there is a significant difference on multiple techniques, then it was followed by the Wilcoxon Matched-Paired Signed-Rank test which was used to test pair-wise differences.\n5"}, {"heading": "4. Results and Data Analysis", "text": "This study compared alternative stop word list and their effect on retrieval effectiveness. Six different techniques with a total of 12 different combinations were examined. Results are compared using the Wilcoxon test (see Table 2), and recall and precision curves (figures 1-4).\nThe Friedman Two-way ANOVA test was used to determine if the differences are statistically significant (see table 1), the test statistic \u03c72 = 70.471 and the Pvalue* = 0.000 which indicates that the differences between techniques as a whole are statistically significant. This was not surprising considering the wide variety of techniques used but the test does not depict individual differences between two techniques.\nIn order to explore these differences, the techniques are grouped according to weighting scheme, and stoplists and combinations of them. For each group of retrieval techniques the significance testing starts with the Friedman Two-way ANOVA test to observe the differences between all techniques used are statistically significant. Then, a post-hoc test using the Wilcoxon Matched-Paired Signed-Rank test was performed to determine the difference between paired techniques. Due to the number of techniques used and the number of different combinations of them, it was impractical to do a pair-wise comparison on all of them. To set a baseline for comparison, the raw term weighting techniques were run without any additional techniques and the best of the three was used as the baseline."}, {"heading": "4.1. Term Weighting", "text": "The results of the term weighting approach show that the three algorithms performed relatively well considering the difficulties of the Arabic language and the fact that no linguistic adaptation for it was implemented during retrieval.\nAlthough the BM25 and the KL model are known to perform well compared to the TFIDF weight, in the\n* The P-Value for both Wilcoxon and Friedman tests was set on the .05 level.\ncurrent study, the overall performance of TFIDF weight was better than the performance of both the BM25 and KL model (see figure 1). The good performance of TFIDF is due to the way the term frequency portion of the weight is calculated in the Lemur Toolkit, using the TF function from the BM25 scheme, which improves its performance significantly.\nIn the Friedman test, \u03c72 = 5.946; P-value = .051 and the P-value indicates that the differences between these three techniques are not statistically significant. The Wilcoxon test was used to determine if the pairwise differences are statistically significant. Because the TFIDF had the best overall performance, it was used as the baseline for comparisons. The Wilcoxon test confirms the result of the Friedman test results, the differences between TFIDF and BM25, and TFIDF and KL were not statistically significant."}, {"heading": "4.2. Term Weighting and Stoplists", "text": "This subsection presents the results obtained by using the three stoplists that were created for this study. The stoplists were created assuming that they would improve the retrieval efficiency when used with other techniques. The results illustrate how sensitive retrieval is to the use of stopwords. The stopwords will essentially affect the term weights used as they have a significant effect on the term frequency."}, {"heading": "4.2.1. General Stoplist", "text": "After creating the list, it was initially used in combination with term weighting. The results obtained when using the general stoplist are presented in figure 2. The test statistic for the Friedman test \u03c72 = is 14.517 and the P-value is .002. This indicates that the differences between these runs and the baseline precision are significant.\n6\nThe differences in the mean precision were minimal, but the increase in precision that was made by the list was apparent at low cut-off levels, especially for the BM25. The Wilcoxon test indicates that the differences between the KL_GS and the baseline precision were not statistically significant. There was a minimal improvement for only one query with the KL model. As for the BM25_GS, and TFIDF_GS there was a significant difference and change. After combining the GS with BM25 the results changed drastically going up from 30 queries better than the baseline precision to 47 queries, and the same thing happened with the TFIDF_GS run with 49 queries favoring it over the baseline precision. Both results indicate how sensitive these weights are, especially the BM25, to the use of stopwords, bearing in mind that the term frequency (TF) portion in the TFIDF is calculated using the BM25 term frequency function. Conversely, the KL model had poor performance with the stoplist as the results slightly deteriorated when it was combined with the stoplist."}, {"heading": "4.2.2. Corpus-Based Stoplist", "text": "The list has improved the precision for the BM25 weight on the lower levels of recall. Comparing these results with the baseline precision, the test statistic for the Friedman test \u03c72 is 12.957 and the P-value is .005. This indicates that the differences between these runs and the baseline precision are significant. The differences in the mean precision were minimal.\nThe Wilcoxon test indicates that the differences between the BM25_CBS, TFIDF_CBS and the baseline precision were not statistically significant, even though there was an apparent improvement with the BM25. Even though there was a slight improvement with the KL-Model (not more than 2.7 %), the corpus-based stoplist had a negative effect on the overall performance of the model as the results degraded from 31 queries in favor of the KL-Model to 25 when it was combined with the stoplist."}, {"heading": "4.2.3. Combined Stoplist", "text": "Figure (4) presents the results obtained from using the combined stoplist. In this figure, the curves show that the results were also very close, as for the general stoplist, and that the list has improved the precision for the BM25 weight on the lower levels of recall. Comparing these results with the baseline precision, the test statistic for the Friedman test \u03c72 is 13.327 and the P-value is .004. This indicates that the differences between these runs and the baseline precision are significant. The differences in the mean precision were minimal.\nThe Wilcoxon test indicates that the differences between the BM25_CS, KL_CS, and the baseline precision were not statistically significant. However there was an improvement for the BM25 weight. Compared to the general stoplist, there were some differences but the results were almost identical to the general stoplist combinations despite the additions to it. Looking at the individual queries the differences in precision were in favor of the general stoplist, showing that the improvement in both was very much due to it. Even though there was an improvement in the KL-Model at low document cut-off levels, the overall performance of the model also had poor performance with the stoplist. The results deteriorated when it was combined with the stoplist. When combined with the TFIDF the combined list had a better overall performance than the baseline precision and the difference was significant.\n7"}, {"heading": "5. Discussion", "text": "Using the standard recall and precision measures the above techniques were compared. Six techniques were used separately and combined, generating a total of 24 different indexing approaches.\nWithout any additional linguistic processing the three schemes, TF*IDF weighting, Okapi best match algorithm, and the Kullback-Leibler Divergence Model, had a good performance with, Arabic which was not surprising considering their previous success with other languages as they depend only on the corpus and query statistics. The differences between the three weighting schemes were very minimal and not statistically significant.\nThe TF*IDF scheme is the best weighting scheme to be used with the Arabic language when used separately without stemming or stopwords removal. This contradicts some previous research indicating that the BM25 algorithm is better when used with Arabic [17].\nOne reason for this is that the term frequency portion of the TF*IDF scheme in Lemur is calculated using the term frequency portion in BM25 giving it the advantages of both of schemes. A second is that in Savoy and Rasolofo\u2019s experiment the BM25 was combined with a stemmer which particularly boosts the results with Arabic language.\nThree stoplists were created for this study, a general stoplist, a corpus-based stoplist, and a combined stolist. The assumption here was that stopwords affect the retrieval process but the extent of this effect was not known. The results showed that this effect varies from one weighting scheme to the other. Combining the stoplists with the TFIDF and KL model did not make a substantial difference, only 0.1%-0.2% increase in the former and decrease in the latter. When the stoplists were combined with the BM25 weight there was noticeable improvement of 7.67% with the corpusbased stoplist, 9.49% with the combined stoplist, and 10.44% with the general stoplist."}, {"heading": "KL 0.2264 31 43 1 0.287", "text": " BP: Baseline Precision (TFIDF).\nThe results illustrate how sensitive the BM25 weight and KL model are to the use of stopwords. The use of stopwords has positively affected the BM25 weight while associating it with the KL model has affected the results negatively. The corpus-based list was the lower than the General list, which suggests that we should revisit the corpus-based list. Unfortunately there are no clear-cut rules on how to create a list like this and most of the decisions that were taken in creating this list were arbitrary.\nGenerally the overall performance of the general stoplist was better than the corpus-based stoplist and to some extent better than the combined stoplist. The list can be used as a standard list for Arabic retrieval regardless of the nature of the data used. The list will be added to the Lemur Toolkit making it available for research and further development as there are no publicly available stoplists for Arabic language.\nAs for the Lemur Toolkit, one of the main objectives of this study was to use it in experimenting with Arabic language retrieval. The addition of the Arabic language to the Lemur Toolkit will benefit the language as it facilitates the retrieval process whatever the approach that is followed. A major advantage of the toolkit is that it is open source software making it easy to add or modify applications. During this study few applications of the toolkit were used but it proved to be very efficient when used to work with the Arabic language in terms of time, the capability to handle the language, and the ease of use."}, {"heading": "6. Conclusions and Further Research", "text": "Experimentation with Arabic language retrieval is still a relatively new area of research; it still requires exploring and more research. In this study several retrieval techniques and their potential in improving retrieval effectiveness were explored. The effects of term weighting, and stopwords on Arabic retrieval were examined and compared using the Lemur Toolkit.\nThe best match algorithm, BM25, with the combined or general stoplist was the best performing function for retrieval in the Arabic language. The performance of a general stoplist or a combined list was relatively close. The use of any of them is recommended but the general stoplist is certainly preferred if we are dealing with a different corpus.\nThe Kullback-Leibler Divergence Model had problems performing with stopwords. Further investigation with the model could reveal the extent of this problem especially when dealing with different smoothing algorithms and variant query lengths.\n8\nDeveloping a new weighting algorithm that could use the characteristics of the Best Match algorithm, BM25, and combining it with language specific characteristics may lead to further improvements. For instance, using the syntactical structure of the Arabic sentence in calculating the term weight with the BM25 could improve upon the efficiency of this algorithm.\nLemur comes equipped with several applications; this study has used only a small percentage of these applications. Experimenting with the other applications provided by the toolkit to determine their performance in Arabic is another area that could be explored, for instance the use of feedback in retrieval, summarization\u2026etc."}], "references": [{"title": "Arabic for English Speaking Students", "author": ["M. Abdul-Rauf"], "venue": "Al- Saadawi Publications,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Grammatical Application", "author": ["A. Al-Raghi"], "venue": "Dar El- Nahda Al-Arabia Lelteba\u2019ah Wa Al-nashr", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1981}, {"title": "Optimization and Effectiveness of N-Grams Approach for Indexing and Retrieval in Arabic Information Retrieval Systems, Ph", "author": ["A. Alshehri"], "venue": "D Thesis, School of Information Sciences University of Pittsburgh,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Translation Term Weighting and Combining Translation Resources in Cross-Language Retrieval", "author": ["A. Chen", "F. Gey"], "venue": "Tenth Text REtrieval Conference ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "A Stop List for General Text", "author": ["C. Fox"], "venue": "SIGIR Forum, Vol. 24, No. 1-2", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Using Statistical Testing in the Evaluation of Retrieval Experiments", "author": ["D. Hull"], "venue": "Proceedings of the 16th annual international ACM SIGIR conference on Research and Development in Information Retrieval", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Stemming Arabic Text", "author": ["S. Khoja", "R. Garside"], "venue": "http://www.comp.lancs.ac.uk/computing/users/ khoja/stemmer.ps", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Information Storage and Retrieval", "author": ["R.R. Korfhage"], "venue": "John Wiley", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving Stemming for Arabic Information Retrieval: Light Stemming and Cooccurrence Analysis", "author": ["L.S. Larkey", "L. Ballesteros", "M.E. Connell"], "venue": "Proceedings of the 25 Annual International AC SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Arabic Information Retrieval at UMass in TREC-10", "author": ["L.S. Larkey", "M.E. Connell"], "venue": "Tenth Text REtrieval Conference ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Term- Weighting Approaches in Automatic Text Retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information Processing & Management, Vol. 24, No. 4", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Introduction to Modern Information Retrieval", "author": ["G. Salton", "M. McGill"], "venue": "McGraw-Hill Book Company", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1983}, {"title": "Report on the TREC- 11 Experiment: Arabic", "author": ["J. Savoy", "Y. Rasolofo"], "venue": "Named Page and Topic Distillation Searches. Eleventh Text REtrieval Conference ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Multimedia Information Retrieval: Content-based Information Retrieval from Large Text and Audio Databases", "author": ["P. Schauble"], "venue": "Kluwer Academic Publishers", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Nonparametric Statistics of the Behavioral Sciences", "author": ["S. Siegel", "N.J. Castellan"], "venue": "2nd edition, McGraw-Hill Book Company", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "D", "author": ["E.M. Voorhees"], "venue": "Harman, Overview of TREC 2001.Tenth Text Retrieval Conference, TREC 2001", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": "amount of unproductive processing [9].", "startOffset": 34, "endOffset": 37}, {"referenceID": 13, "context": "text collection can represent stopwords [18].", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "Fox [6] was the first to create an English stoplist to be", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "The stoplist used in the Lemur Toolkit is the one created by Khoja [8]", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "This list was used by Larkey and Connell [11] and Larkey, Ballesteros and", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "Connell [10].", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "Chen and Gey [5] used a list they created by translating an English list and augmenting it with", "startOffset": 13, "endOffset": 16}, {"referenceID": 12, "context": "Savoy and Rasolofo\u2019s stoplist 1 [17] is a domain dependent list which has three problems.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "9 with five topics having more than 300 relevant documents and another five with fewer than 25 relevant documents [22].", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "The word categories [1, 2] that were used are: \uf0a7 Adverbs.", "startOffset": 20, "endOffset": 26}, {"referenceID": 1, "context": "The word categories [1, 2] that were used are: \uf0a7 Adverbs.", "startOffset": 20, "endOffset": 26}, {"referenceID": 6, "context": "The list was checked against Khoja [8] and Alshehri\u2019s [3]", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "The list was checked against Khoja [8] and Alshehri\u2019s [3]", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "The performance of each technique was evaluated using the standard measures of Recall and Precision, [9, 15].", "startOffset": 101, "endOffset": 108}, {"referenceID": 10, "context": "The performance of each technique was evaluated using the standard measures of Recall and Precision, [9, 15].", "startOffset": 101, "endOffset": 108}, {"referenceID": 5, "context": "Hull [7] has examined the validity of different statistical techniques that are used in comparing", "startOffset": 5, "endOffset": 8}, {"referenceID": 14, "context": "in addition to the direction of the differences considered [19].", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "between pairs increase, significance also increases [16].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "than their raw values to calculate the statistic [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "BM25 algorithm is better when used with Arabic [17].", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "The effectiveness of three stop words lists for Arabic Information Retrieval---General Stoplist, CorpusBased Stoplist, Combined Stoplist ---were investigated in this study. Three popular weighting schemes were examined: the inverse document frequency weight, probabilistic weighting, and statistical language modelling. The Idea is to combine the statistical approaches with linguistic approaches to reach an optimal performance, and compare their effect on retrieval. The LDC (Linguistic Data Consortium) Arabic Newswire data set was used with the Lemur Toolkit. The Best Match weighting scheme used in the Okapi retrieval system had the best overall performance of the three weighting algorithms used in the study, stoplists improved retrieval effectiveness especially when used with the BM25 weight. The overall performance of a general stoplist was better than the other two lists.", "creator": "Microsoft\u00ae Word 2016"}}}