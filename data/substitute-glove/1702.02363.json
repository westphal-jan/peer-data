{"id": "1702.02363", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Automatically Annotated Turkish Corpus for Named Entity Recognition and Text Categorization using Large-Scale Gazetteers", "abstract": "Turkish Wikipedia Named - Entity Recognition many Text Categorization (TWNERTC) optimized part a included of automatically categorized then novellas imprisonment relating was Wikipedia. We created across - initial sathon following instead a explosivity ha-ha minimax which collect relevant jurisdiction work superfamily information along makes indexing personal 10, Freebase. The railway gazetteers types 13,000 300K categories with around has fine - grained entity typical laid 138 than domains. Since automated complexity directly prone if relation, wo two introduce taking it limited specifically acoustic improvements pre-clinical. Moreover, unfortunately map fine - grained unitary various because two value four cornmeal - 3/4-inch sizes: each, demarcates, cpic, cscc. Eventually, else enable over different rescaling versions and evaluate the content more omitting by comparing ground trivial from understanding annotators. We wo actually spectrophotometric publicly available to groups learning on Turkish called - entity recognition (NER) already code non-literal (TC ).", "histories": [["v1", "Wed, 8 Feb 2017 10:45:23 GMT  (108kb,D)", "https://arxiv.org/abs/1702.02363v1", "10 page, 1 figure, white paper"], ["v2", "Thu, 9 Feb 2017 08:35:12 GMT  (103kb,D)", "http://arxiv.org/abs/1702.02363v2", "10 page, 1 figure, white paper, update: added correct download link for dataset"]], "COMMENTS": "10 page, 1 figure, white paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["h bahadir sahin", "caglar tirkaz", "eray yildiz", "mustafa tolga eren", "ozan sonmez"], "accepted": false, "id": "1702.02363"}, "pdf": {"name": "1702.02363.pdf", "metadata": {"source": "CRF", "title": "Automatically Annotated Turkish Corpus for Named Entity Recognition and Text Categorization using Large-Scale Gazetteers", "authors": ["H. Bahadir Sahin", "Caglar Tirkaz", "Eray Yildiz", "Mustafa Tolga Eren", "Ozan Sonmez"], "emails": ["eray.yildiz@huawei.com", "osonmez@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Named-entity recognition (NER) is an information extraction (IE) task that aims to detect and categorize entities to pre-defined types in a text. On the other hand, the goal of text categorization (TC) is to assign correct categories to texts based on their content. Most NER and TC studies focus on English, hence accessing available English datasets is not a issue. However, the annotated datasets for Turkish\n1https://www.freebase.com/\nNER and TC are scarce. It is hard to manually construct datasets for these tasks due to excessive human effort, time and budget. In this paper, our motivation is to construct an automatically annotated dataset that would be very useful for NER and TC researches in Turkish.\nThe emergence of structured and linked semantic knowledge bases (KBs) provide an important opportunity to overcome these problems. Approaches that leverage such KBs can be found in literature (Heck et al., 2013; Gerber et al., 2013; Hoffart et al., 2011; Mendes et al., 2011). However, using the structured data from KBs is a challenging task for linking named entities and domains to raw texts due to ambiguous texts and named entities (Cucerzan, 2007).\nIn this work, we publish TWNERTC dataset in which named entities and categories of sentences have been automatically annotated. We use Turkish Wikipedia dumps as the text source2 and Freebase to construct a large-scale gazetteers to map finegrained types to entities. To overcome noisy and ambiguous data, we leverage domain information which is given by Freebase and develop domainindependent and domain-dependent methodologies. All versions of datasets can be downloaded from our project web-page3. Our main contributions are (1) the publication of Turkish corpus for coarse-grained and fine-grained NER, and TC research, (2) six different versions of corpus according to noise reduction methodology and entity types, (3) an analysis of the corpus and (4) benchmark comparisons for\n2https://dumps.wikimedia.org/ 3http://dx.doi.org/10.17632/cdcztymf4k.1\nar X\niv :1\n70 2.\n02 36\n3v 2\n[ cs\n.C L\n] 9\nF eb\n2 01\nNER and TC tasks against human annotators. To the best of our knowledge, these datasets are the largest datasets available for Turkish NER ad TC tasks.\nThe rest of the paper is organized as follows: In Section 2, we briefly investigate the literature about NER, TC and datasets which are used in these research. In Section 3, we explain the construction of large-scale gazetteers by using Freebase. In Section 4, we explain how to use the gazetteers to automatically annotate and categorize Wikipedia texts to construct datasets along with dataset statistics, and noise reduction methodologies. Our evaluation about the quality of these constructed datasets are reported in Section 5."}, {"heading": "2 Related Work", "text": "Named Entity Recognition (NER) and Text Classification (TC) are well-researched NLP tasks relevant to large amount of information retrieval and semantic applications. TC research predates to \u201960s; however, it is accepted as a research field in \u201990s with the advances in technology and learning algorithms (Sebastiani, 2002). On the contrary, the classical NER task is defined in MUC (Chinchor, 1998) and CoNLL (Tjong Kim Sang and De Meulder, 2003) conferences with coarse-grained entity types: person, location, organization and misc. In addition, few studies address the problem of finegrained NER where the challenge is to capturing more than four entity types (Pasca et al., 2006; Ekbal et al., 2010; Yogatama et al., 2015).\nAs research on NER has been pushing the limits of automated systems performing named-entity recognition, the need for annotated datasets and benchmarks is also increasing. Knowledge bases are important for NLP researches, since they provide a structured schema of topics that can be used to annotate entities with fine-grained types and/or categorize raw texts into related domains.\nSteinmetz et al. (Steinmetz et al., 2013) published benchmark evaluations that compare three datasets that use semantic information from KBs: DBpedia Spotlight (Mendes et al., 2011), KORE50 (Hoffart et al., 2011; Yosef et al., 2011) and the Wikilinks Corpus (Singh et al., 2011). These datasets are in English and constructed with the aim of evaluating the performance of NER systems. The authors\npresent the statistics of each dataset and baseline performances of various algorithms. There are other methodologies which leverages KBs to named entity extraction and linking; however, most of them are not available to public (Hoffart et al., 2011; Heck et al., 2013).\nConstructing a comprehensive dataset for TC is tougher than NER since there is no limit for the number of categories that are represented in such sets. In general, there are many TC datasets available in English for many different problems such as sentiment analysis (Liu et al., 2015) and categorizing gender (Mukherjee and Liu, 2010). The largest and the most popular dataset among them is Reuters Corpus Volume 1 (RCV1) which consists of manually categorized 800K news stories with over 100 sub-categories under three different main categories (Rose et al., 2002). This version the dataset has problems with document categories and suffers from lack of documentation about the dataset. Lewis et al. propose an improved version of this dataset with reduced categorization mistakes and provide a better documentation (Lewis et al., 2004).\nResearch on Turkish NER and TC are very limited compared to English and several other languages. The main reason is the lack of accessibility and usability of both Turkish NER and TC datasets. The most popular Turkish NER dataset is introduced by Go\u0308khan et al. (Tu\u0308r et al., 2003). This dataset contains articles from newspapers, approximately 500K words, and is manually annotated with coarse-grained entity types. Tatar and C\u0327ic\u0327ekli propose another coarse-grained NER dataset (Tatar and C\u0327ic\u0327ekli, 2011); however, it contains only 55K words which makes this dataset to less preferable than previous dataset. More recent studies focus on Turkish NER in social media texts (O\u0308nal and Karago\u0308z, 2015; Ku\u0308c\u0327u\u0308k et al., 2014; Demir and O\u0308zgur, 2014; C\u0327elikkaya et al., 2013). Due to the research focus in the field, several Twitter-based coarse-grained NER datasets are published (Ku\u0308c\u0327u\u0308k et al., 2014; Ku\u0308c\u0327u\u0308k and Steinberger, 2014; Tantug, 2015). According to our knowledge, there is no literature available regarding to fine-grained NER in Turkish.\nTurkish TC researchers tend to construct their own, case specific datasets in general (Amasyal\u0131 and Diri, 2006). The newspapers are the main text source for such studies since they are easy to ob-\ntain and classify manually (Akkus\u0327 and C\u0327ak\u0131c\u0131, 2013; Toraman et al., 2011; K\u0131l\u0131nc\u0327 et al., 2015). When the amount of annotated data is considered to train state-of-the-art learning algorithms, aforementioned Turkish datasets suffer from the lack of enough data. The main bottlenecks are requires human effort and time constraint, which limits the size and scope of the constructed datasets. In contrast, our aim is to provide larger, more comprehensive and insightful Turkish datasets for both NER and TC by using knowledge bases to create large-scale gazetteers and eliminating human factor in the annotation process.\nIn the next section, we will explain our dataset construction methodology starting with building gazetteers by using Freebase and Wikipedia. We will investigate how to build a graph crawler algorithm to crawl the knowledge in Freebase and to map its entity types and domain information into the raw texts automatically. We will also discuss noise reduction methods and propose three different versions of the dataset."}, {"heading": "3 Dataset Construction Methodology", "text": ""}, {"heading": "3.1 Constructing Gazetteers", "text": "Gazetteers, or entity dictionaries, are important sources for information extraction since they store large numbers of entities and cover vast amount of different domains. KBs use a graph structure that is used to represent thousands of films and/or millions of songs in gazetteers. Hence, such graph structures can be efficient to construct large-scale gazetteers.\nIn our work, we use Freebase as the KB since it provides both quality and quantity in terms of entity types and domains for Turkish. Freebase has 77 domains that cover many different areas from music to meteorology with approximately 50M total entities. Among them, Turkish has 300K entities, and approximately 110K of them have a link to corresponding Turkish Wikipedia page.\nBy using KBs, one can eliminate the necessity of creating semantic schema design, collecting data and manually annotating raw texts. However, such large entity lists contain ambiguous and inaccurate information that can impact the quality. For instance, both film and boat domains contain \u201cTitanic\u201d in their entity lists, and such ambiguity could create false links. Considering the number of entities,\nthe importance of disambiguating named entities becomes more important.\nOur work is inspired from Heck et al.(Heck et al., 2013) who employ a method that takes advantage of user click logs of a web search engine in order to improve precision of gazetteers while maintaining recall. Since we do not have any sources to access such user click logs, we depend on attributes, such as entity domains, types and properties, that Freebase provides in order to improve the quality of our gazetteers. Note that a named entity\u2019s domain, type and property are represented as /domain/type/property in Freebase.\nFreebase distinguishes entities that exist in more than one domain by unique machine id (mid ). For instance, Titanic is an entity in both film and boat domains with two mids depending on the domain. On the other hand, there are cases in which same mids can be associated with multiple domains. For instance the mid of Titanic in the film domain is also used in the award domain. These cases occur when domains are closely related. Note that mid is language independent. Hence, an entity has the same mid regardless of its language; however, the information related to that entity can differ, e.g., missing equivalence of entities, translation differences.\nDomains covered in Freebase have large amount of entities and related descriptive texts. However, for Turkish, we need to filter or merge some domains due to insufficient number of related raw texts. For instance, we merge \u201cAmerican football\u201d, \u201ccricket\u201d, \u201cice hockey\u201d domains under already existing sports domain.\nDuring annotation, due to ambiguous cases in Freebase, we first compute the domain and the entity type distribution of directly related (first order) relations of the selected named entity. Among the candidates, the entity type that contains the most first order relations in the knowledge graph is selected as the type of the entity. This ensures that the most informative entity type is selected for the annotated entity. For instance, while annotating the Wikipedia page of \u201cTitanic (film)\u201d the possible candidates are /film/film and /award/award winning work. Our method chooses /film/film as the domain and type of the entity since it has more information compared to its competitor.\nAfter determining the entity type of each entity,\nwe form large-scale gazetteers for Turkish which contain 300K named-entities. Each entity in the gazetteer has its Wikipedia description, determined type and 1st-order relations. Note that some entities may not have Wikipedia description due to several reasons, e.g. song names or deleted Wikipedia pages; however, we use such entities in the annotating process."}, {"heading": "4 Automatic Annotation Generation", "text": ""}, {"heading": "4.1 Fine-Grained Annotations", "text": "A knowledge graph consists of nodes and edges between nodes which are defined by the schema of a domain. Nodes are entities and edges represent relations between nodes, which are properties in Freebase. Examples of entities and relations are shown in Figure 1, for \u201cfilm\u201d and \u201cpeople\u201d domains. The central node having the most number of relations in the film domain is the \u201cfilm name\u201d. We call directly connected relations to the central node as first-order relations. Name of the director who directed the movie is \u201cfilm director\u201d relation. Since directors are people, they have also relations in \u201cperson\u201d domain. Through such relationship we can get second-order relations about a movie. We benefit this graph structure along with the raw texts to create multi-purpose, annotated and categorized corpora. Inspired by the approach of (Heck et al., 2013), we create a graph crawling algorithm that is capable of categorizing sentences into Freebase domains and\nannotate named entities within the sentences with Freebase types and properties. This set of annotations are the Fine-Grained Annotations (FGA). Our method consists 5 steps and is applicable to both English and Turkish (with slight changes for sentence processing).\n1. Select the central node from gazetteers. This node is \u201cCentral Pivot Node\u201d (CPN). It is the main entity-type of the domain, e.g. film names in film domain.\n2. Retrieve the descriptions provided by Wikipedia or Freebase. If Wikipedia has the corresponding page, fetch full texts from dump. Else if only Freebase description is available, use the description. Otherwise, return to the first step. Since several Turkish Wikipedia texts are direct copy of English version, language detection is applied on all texts and English texts are eliminated.\n3. Extract sentences from raw texts and annotate sentences by longest-string pattern matching where the first-order relations of CPN are used resulting in IOB style annotation. We use Aktas\u0327 and C\u0327ebi\u2019s sentence detection method to extract sentences from full texts (Aktas\u0327 and C\u0327ebi, 2013).\n4. Extend annotation process with second-order relations.\n5. Categorize sentences by selecting the domain of entities that is used the most.\nOne should consider that higher order relations can create a long chain of relationships. In our work, we limit this chain with the second-order relations, since further relations provide less information while increasing ambiguity and inconsistency of automated annotations."}, {"heading": "4.2 Statistics of the FGA", "text": "TWNERTC contains 300K entities in total of which 110K have Turkish descriptions (from Freebase or Wikipedia dump). There are totally 700171 annotated sentences from 49 different domains with the largest and smallest domains being people (139K sentences) and fashion (493 sentences) subsequently. TWNERTC consists of 10997037 tokens without punctuation and among these tokens approximately 2M of them are annotated. 16K of the tags are unique which results in approximately 332 unique entity type per domain on the average. Note that, even if a sentence is categorized into the film domain, it can contain entities from other domains, such as person and time. Location is the domain having the highest number of unique tags (1051) whereas physics has the least amount of unique tags (15)."}, {"heading": "4.3 Disambiguate Noisy Entity Types", "text": "We refine the gazetteers to minimize the effect of noise in the generated annotations and assigned domains. However, due to the nature of automated algorithm, we find that entity annotations have still inconsistent or missing types while categorization\nprocess is resulted with more accurate in general. In order to reduce remaining noise, we apply both domain dependent and domain independent noise reduction. Domain dependent approach finds the most common entity type of every entity according to the domain of the sentences. Then, entities are reannotated with the common entity types. Domain independent approach follows the same process without using domain information while eliminating the noisy information. Statistics about these two versions of the dataset are presented in Table 1."}, {"heading": "4.4 Transform FGA to Coarse-Grained Annotation", "text": "FGA provides fine-grained annotations with many detailed entity types and properties. However, the amount of different entity types affects the learning algorithms negatively. Moreover, it is hard to evaluate the quality of the annotations when most works in literature performs coarse-grained entity recognition. Thus, we provide a coarse-grained version of the FGA datasets. In order to transform FGA to coarse-grained annotation (CGA), we map each fine-grained entity type in each domain to a coarsegrained entity type, i.e. person, organization, location and misc. We keep the IOB notations in types while converting the type. In this process, we eliminate several domains, such as meteorology, interests and chemistry, due to the lack of types that can be mapped to a coarse-grained version. This elimination process leaves 25 unique domains in CGAbased datasets.\nIn coarse version of the dataset, there are approximately 380K sentences. Similar to FGA statis-\ntics, the people domain has the highest number of sentences (104508) while law domain has the least number of sentences (454) among all 25 domains. The number of tokens is 7326286 with punctuation. Among these tokens, 851123 of them are annotated. Unlike FGA, CGA has only 4 types for each domain. 19 of the domains contains all 4 types whereas the remaining domains might contain 2 or 3 entity types such as geography and food.\nWe also transform the post-processed datasets we introduced in previous chapter to CGA. Table 2 presents the details of created corpora with CGA. In total, we publish six datasets (one original and two post-processed with FGA, one original and two post-processed with CGA)4."}, {"heading": "5 Evaluation", "text": "To experimentally evaluate TWNERTC, five human annotators categorize and annotate test sets that are sampled from the datasets. We create six test sets (3 CGA, 3 FGA) with 10K-word for NER and one test with 2K sentences for TC. We compare annotations and domains of these sets with manually created ground truths.\nFor NER evaluation, we follow two different approaches for coarse-grained and fine-grained versions. While we evaluate the CGA versions against manually annotated ground-truths, it is an almost impossible to evaluate FGA versions. Hence, we train a fine-grained NER model to predict top-5 possible entity types for all entities and human annotators create ground-truths by using these predictions. For both cases, we extract 10K-word test sets for all three versions (original and post-processed). Note that, test sets are not identical but randomly selected sentences from the datasets. In addition, we exclude IOB tags since we prioritize evaluating entity type agreement in our evaluation results.\n4http://dx.doi.org/10.17632/cdcztymf4k.1\nFor evaluating CGA versions, given the sentence and the corresponding automatically created annotation, annotators are allowed to change any entity type with one of the five possible types, i.e. person, organization, location, misc or O (out). Finally, we merge the results of all annotators such that if at least 3 annotators agree on the same type for an entity, that type is the ground-truth. If there is no agreement, we keep the entity type as it is.\nFor evaluating FGA, we use a fine-grained entity recognizer, FIGER (Ling and Weld, 2012)5, to create ground-truths. It is not an ideal evaluation approach since FIGER is designed for English and have not been tested for other languages according to our knowledge. However, more than thousands of different entity types are available in TWNERTC, and it is impracticable to ask human annotators to construct such fine-grained ground-truths manually from scratch. Therefore, we train a Turkish finegrained model by using all remaining sentences and predicted possible types for entities in the test sets. Then, we ask human annotators to rank types of an entity from the most relevant to the least relevant. They are also allowed to suggest alternative types different than the given choices.\nWe randomly sample 2K sentences from the unmodified corpus as TC test set. We train an internal classification algorithm with the rest of sentences and get top five predicted domains for the test sentences. We present predicted categories to human annotators and ask them to rank domains from the most relevant to the less relevant. They are also allowed to suggest different domains among the 49 domains which are represented in full corpus. Finally, we rank domains of each test sentence according to the annotator agreement and form the test set such that each sentence has 5 possible domains where first domain is the most relevant domain.\n5https://github.com/xiaoling/figer"}, {"heading": "5.1 Evaluation Results for Coarse-Grained Annotations", "text": "In Table 3, we present the number of entity types exist in automatically and manually annotated sets with the number of changes that annotators have made. We define changes from type O to any other type is an addition and opposite of this action is a removal. Misc is the most added, removed and changed type by annotators. It is an acceptable outcome since this specific type covers vast amount of entities except person, location and organization. We do not consider the number of added entity types as a major problem, since our gazetteers do not have infinite information and we have future plans to improve it. However, miss-annotated types are the real danger since if the amount of such mistakes increase, performance of the learning algorithms is affected negatively. In \u201dchange\u201d type of , annotators change misc type to mainly organization and person.\nAs a conclusion, among the automatically annotated coarse-grained entity types, there are %76 matching ratio without O tags and manually added types. Additionally, we present precision, recall and F-score values in Table 4. The dataset with domaindependent post-process provides better NER performance compared to other two versions in general. Moreover, both post-processing methods improve the performance compared to the original dataset. Further, it can be observed that misc type is has the lowest F-Score among all types in all versions; however, this result is expected since misc\u2019s coverage is larger than the other three types and makes it more vulnerable to mismatches."}, {"heading": "5.2 Evaluation Results for Fine-Grained Annotations", "text": "As we discuss earlier, we train an external finegrained algorithm, FIGER, to create Turkish NER models for all versions of the TWNERTC. By using the resulted models, we take five possible predicted type for each entity that are represented in test sets, and provide them as ground-truths to five human annotators. Obviously, FIGER designed to solve finegrained NER in English; however, to evaluate the automated fine-grained types in a reasonable time, we leverage predictions of this algorithm.\nIn Table 5, we present the F1-scores of trained models on each test set. We provide these scores to give a better insight to researchers about groundtruths. Note that, strict represents the original F1score formula, while loose macro and loose mi-\ncro scores represent variations of the same formula (Ling and Weld, 2012). It can be observed that the model trained with original TWNERTC performs poorly compared to post-processed versions. Since domain independent (DI) noise reduction method ensures that an entity can have only one type, it improves the performance more than the domain dependent (DD) method.\nTable 6 presents the human annotators evaluation on automated fine-grained NER datasets, given FIGER predictions as possible ground-truths. Annotators rank the provided ground-truths and we check their ranking agreements. Eventually, top-1 agreement is hard to fulfill since our gazetteers contains thousands of entity types, and an entity may have more than ten different possible options. On the other hand, top-5 agreements provide promising results considering the amount of possible groundtruths."}, {"heading": "5.3 Evaluation Results for TC", "text": "Original TWNERTC contains 49 different domains. The number of domains causes ambiguities in categorization among sentences depending on the context understanding. Thus, we evaluate three levels of accuracy. From Table 7, we can see that automatically assigned domains have relatively low direct matches with annotators according to top-1 score. However, when we observe top-3, accuracy scores are doubled. Furthermore, error rates are less than %2 when all five ground truths are considered. Note that in top-3 and top-5, we only considered whether\nautomatically assigned domain exists or not in the ground truth list.\nAmount of the difference between from top-1 to top-3 scores is mainly caused by annotators\u2019 different understanding of the sentence context. For instance, a sentence about Lionel Messi\u2019s birth location is categorized as people in the test set. Whereas ground-truths start with soccer and followed by people and location. In addition, similar domains, such as sports and soccer, are also the reason of this difference.\nIn overall, results validates that automatically assigned domains are likely similar to what human annotators suggest to corresponding sentences. However, while automation process is limited to the context, humans can use their knowledge when suggesting domains. Hence, low accuracy in top-1 score is not a mistake of the methodology but a shortcoming in the process."}, {"heading": "6 Conclusion and Future Research Directions", "text": "We have described six, publicly available corpora for NER and TC tasks in Turkish. The data consists of Wikipedia texts which are annotated and categorized according to the entity and domain information extracted from Freebase. We explain the process to construct the datasets and introduce methodologies to eliminate noisy and incorrect data. We provide comprehensive statistics about dataset content. We analyzed subsets from these datasets and evaluate automatically created annotations and domains against manually created ground-truths. The final results show that automatic annotations and domains are quite similar to the ground-truths.\nThe obvious next step is to develop learning algorithms for NER and TC tasks to find baselines using traditional machine learning algorithms, and extending these baselines with approaches. Since TWNERTC provides a vast amount of structured data for researchers, deep learning methods can be exploited to solve fine-grained NER problem."}, {"heading": "7 Acknowledgments", "text": "This project is partially funded by 3140951 numbered TUBITAK-TEYDEB (The Scientific and Technological Research Council of Turkey \u2013 Technology and Innovation Funding Programs Directorate)."}], "references": [{"title": "Categorization of turkish news documents with morphological analysis", "author": ["Akku\u015f", "\u00c7ak\u0131c\u01312013] Burak Kerim Akku\u015f", "Ruket \u00c7ak\u0131c\u0131"], "venue": "In ACL (Student Research Workshop),", "citeRegEx": "Akku\u015f et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Akku\u015f et al\\.", "year": 2013}, {"title": "Rule-based sentence detection method (rbsdm) for turkish", "author": ["Akta\u015f", "\u00c7ebi2013] \u00d6zlem Akta\u015f", "Yal\u00e7\u0131n \u00c7ebi"], "venue": "International Journal of Language and Linguistics,", "citeRegEx": "Akta\u015f et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Akta\u015f et al\\.", "year": 2013}, {"title": "Automatic turkish text categorization in terms of author, genre and gender", "author": ["Amasyal\u0131", "Diri2006] M Fatih Amasyal\u0131", "Banu Diri"], "venue": "In Natural Language Processing and Information Systems,", "citeRegEx": "Amasyal\u0131 et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Amasyal\u0131 et al\\.", "year": 2006}, {"title": "Large-scale named entity disambiguation based on wikipedia data", "author": ["Silviu Cucerzan"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Cucerzan.,? \\Q2007\\E", "shortCiteRegEx": "Cucerzan.", "year": 2007}, {"title": "Improving named entity recognition for morphologically rich languages using word embeddings", "author": ["Demir", "\u00d6zgur2014] Hakan Demir", "Arzucan \u00d6zgur"], "venue": "In Machine Learning and Applications (ICMLA),", "citeRegEx": "Demir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Demir et al\\.", "year": 2014}, {"title": "Assessing the challenge of fine-grained named entity recognition and classification", "author": ["Ekbal et al.2010] Asif Ekbal", "Eva Sourjikova", "Anette Frank", "Simone Paolo Ponzetto"], "venue": "In proceedings of the 2010 Named Entities Workshop,", "citeRegEx": "Ekbal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ekbal et al\\.", "year": 2010}, {"title": "Toruno\u011flu, and G\u00fcl\u015fen Eryi\u011fit", "author": ["Dilara"], "venue": null, "citeRegEx": "\u00c7elikkaya and Dilara,? \\Q2013\\E", "shortCiteRegEx": "\u00c7elikkaya and Dilara", "year": 2013}, {"title": "Real-time rdf extraction from unstructured data streams", "author": ["Gerber et al.2013] Daniel Gerber", "Sebastian Hellmann", "Lorenz B\u00fchmann", "Tommaso Soru", "Ricardo Usbeck", "Axel-Cyrille Ngonga Ngomo"], "venue": "In The Semantic Web\u2013ISWC", "citeRegEx": "Gerber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gerber et al\\.", "year": 2013}, {"title": "Leveraging knowledge graphs for", "author": ["Heck et al.2013] Larry P Heck", "Dilek Hakkani-T\u00fcr", "G\u00f6khan T\u00fcr"], "venue": null, "citeRegEx": "Heck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heck et al\\.", "year": 2013}, {"title": "Robust disambiguation of named entities in text", "author": ["Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": null, "citeRegEx": "Hoffart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "Ttc-3600: A new benchmark dataset for turkish text categorization", "author": ["K\u0131l\u0131n\u00e7 et al.2015] Deniz K\u0131l\u0131n\u00e7", "Ak\u0131n \u00d6z\u00e7ift", "Fatma Bozyigit", "Pelin Y\u0131ld\u0131r\u0131m", "Fatih Y\u00fccalar", "Emin Borandag"], "venue": "Journal of Information Science,", "citeRegEx": "K\u0131l\u0131n\u00e7 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "K\u0131l\u0131n\u00e7 et al\\.", "year": 2015}, {"title": "Experiments to improve named entity recognition on turkish tweets", "author": ["K\u00fc\u00e7\u00fck", "Steinberger2014] Dilek K\u00fc\u00e7\u00fck", "Ralf Steinberger"], "venue": "arXiv preprint arXiv:1410.8668", "citeRegEx": "K\u00fc\u00e7\u00fck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "K\u00fc\u00e7\u00fck et al\\.", "year": 2014}, {"title": "Named entity recognition on turkish tweets", "author": ["K\u00fc\u00e7\u00fck et al.2014] Dilek K\u00fc\u00e7\u00fck", "Guillaume Jacquet", "Ralf Steinberger"], "venue": "In LREC,", "citeRegEx": "K\u00fc\u00e7\u00fck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "K\u00fc\u00e7\u00fck et al\\.", "year": 2014}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Lewis et al.2004] David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S Weld"], "venue": "In AAAI", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Automated rule selection for aspect extraction in opinion mining", "author": ["Liu et al.2015] Qian Liu", "Zhiqiang Gao", "Bing Liu", "Yuanlin Zhang"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Dbpedia spotlight: shedding light on the web of documents", "author": ["Max Jakob", "Andr\u00e9s Garc\u0131\u0301a-Silva", "Christian Bizer"], "venue": "In Proceedings of the 7th International Conference on Semantic Systems,", "citeRegEx": "Mendes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mendes et al\\.", "year": 2011}, {"title": "Improving gender classification of blog authors", "author": ["Mukherjee", "Liu2010] Arjun Mukherjee", "Bing Liu"], "venue": "In Proceedings of the 2010 conference on Empirical Methods in natural Language Processing,", "citeRegEx": "Mukherjee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2010}, {"title": "Named entity recognition from scratch on social media", "author": ["\u00d6nal", "Karag\u00f6z2015] Kezban Dilek \u00d6nal", "Pinar Karag\u00f6z"], "venue": null, "citeRegEx": "\u00d6nal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "\u00d6nal et al\\.", "year": 2015}, {"title": "Organizing and searching the world wide web of factsstep one: the one-million fact extraction challenge", "author": ["Pasca et al.2006] Marius Pasca", "Dekang Lin", "Jeffrey Bigham", "Andrei Lifchits", "Alpa Jain"], "venue": "In AAAI,", "citeRegEx": "Pasca et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pasca et al\\.", "year": 2006}, {"title": "The reuters corpus volume 1-from", "author": ["Rose et al.2002] Tony Rose", "Mark Stevenson", "Miles Whitehead"], "venue": null, "citeRegEx": "Rose et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Rose et al\\.", "year": 2002}, {"title": "Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1\u201347", "author": ["Fabrizio Sebastiani"], "venue": null, "citeRegEx": "Sebastiani.,? \\Q2002\\E", "shortCiteRegEx": "Sebastiani.", "year": 2002}, {"title": "Large-scale cross-document coreference using distributed inference and hierarchical models", "author": ["Singh et al.2011] Sameer Singh", "Amarnag Subramanya", "Fernando Pereira", "Andrew McCallum"], "venue": "In Proceedings of the 49th Annual Meeting of the Associa-", "citeRegEx": "Singh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2011}, {"title": "Statistical analyses of named entity disambiguation benchmarks. In NLPDBPEDIA@ ISWC", "author": ["Magnus Knuth", "Harald Sack"], "venue": null, "citeRegEx": "Steinmetz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Steinmetz et al\\.", "year": 2013}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of the seventh conference on Natural language learning", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Developing a text categorization template for turkish news portals", "author": ["Fazl\u0131 Can", "Seyit Ko\u00e7berber"], "venue": "In Innovations in Intelligent Systems and Applications (INISTA),", "citeRegEx": "Toraman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Toraman et al\\.", "year": 2011}, {"title": "A statistical information extraction system for turkish", "author": ["T\u00fcr et al.2003] G\u00f6khan T\u00fcr", "Dilek Hakkani-T\u00fcr", "Kemal Oflazer"], "venue": "Natural Language Engineering,", "citeRegEx": "T\u00fcr et al\\.,? \\Q2003\\E", "shortCiteRegEx": "T\u00fcr et al\\.", "year": 2003}, {"title": "Embedding methods for fine grained entity type classification", "author": ["Dan Gillick", "Nevena Lazic"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Yogatama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}, {"title": "Aida: An online tool for accurate disambiguation of named entities in text and tables", "author": ["Johannes Hoffart", "Ilaria Bordino", "Marc Spaniol", "Gerhard Weikum"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "Yosef et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yosef et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 8, "context": "(Heck et al., 2013; Gerber et al., 2013; Hoffart et al., 2011; Mendes et al., 2011).", "startOffset": 0, "endOffset": 83}, {"referenceID": 7, "context": "(Heck et al., 2013; Gerber et al., 2013; Hoffart et al., 2011; Mendes et al., 2011).", "startOffset": 0, "endOffset": 83}, {"referenceID": 9, "context": "(Heck et al., 2013; Gerber et al., 2013; Hoffart et al., 2011; Mendes et al., 2011).", "startOffset": 0, "endOffset": 83}, {"referenceID": 16, "context": "(Heck et al., 2013; Gerber et al., 2013; Hoffart et al., 2011; Mendes et al., 2011).", "startOffset": 0, "endOffset": 83}, {"referenceID": 21, "context": "TC research predates to \u201960s; however, it is accepted as a research field in \u201990s with the advances in technology and learning algorithms (Sebastiani, 2002).", "startOffset": 138, "endOffset": 156}, {"referenceID": 23, "context": "(Steinmetz et al., 2013) published benchmark evaluations that compare three datasets that use semantic information from KBs: DBpedia Spotlight (Mendes et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": ", 2013) published benchmark evaluations that compare three datasets that use semantic information from KBs: DBpedia Spotlight (Mendes et al., 2011), KORE50 (Hoffart et al.", "startOffset": 126, "endOffset": 147}, {"referenceID": 9, "context": ", 2011), KORE50 (Hoffart et al., 2011; Yosef et al., 2011) and the Wikilinks Corpus (Singh et al.", "startOffset": 16, "endOffset": 58}, {"referenceID": 28, "context": ", 2011), KORE50 (Hoffart et al., 2011; Yosef et al., 2011) and the Wikilinks Corpus (Singh et al.", "startOffset": 16, "endOffset": 58}, {"referenceID": 22, "context": ", 2011) and the Wikilinks Corpus (Singh et al., 2011).", "startOffset": 33, "endOffset": 53}, {"referenceID": 9, "context": "There are other methodologies which leverages KBs to named entity extraction and linking; however, most of them are not available to public (Hoffart et al., 2011; Heck et al., 2013).", "startOffset": 140, "endOffset": 181}, {"referenceID": 8, "context": "There are other methodologies which leverages KBs to named entity extraction and linking; however, most of them are not available to public (Hoffart et al., 2011; Heck et al., 2013).", "startOffset": 140, "endOffset": 181}, {"referenceID": 15, "context": "In general, there are many TC datasets available in English for many different problems such as sentiment analysis (Liu et al., 2015) and categorizing gender (Mukherjee and Liu, 2010).", "startOffset": 115, "endOffset": 133}, {"referenceID": 20, "context": "Corpus Volume 1 (RCV1) which consists of manually categorized 800K news stories with over 100 sub-categories under three different main categories (Rose et al., 2002).", "startOffset": 147, "endOffset": 166}, {"referenceID": 13, "context": "propose an improved version of this dataset with reduced categorization mistakes and provide a better documentation (Lewis et al., 2004).", "startOffset": 116, "endOffset": 136}, {"referenceID": 26, "context": "(T\u00fcr et al., 2003).", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "Due to the research focus in the field, several Twitter-based coarse-grained NER datasets are published (K\u00fc\u00e7\u00fck et al., 2014; K\u00fc\u00e7\u00fck and Steinberger, 2014; Tantug, 2015).", "startOffset": 104, "endOffset": 167}, {"referenceID": 25, "context": "tain and classify manually (Akku\u015f and \u00c7ak\u0131c\u0131, 2013; Toraman et al., 2011; K\u0131l\u0131n\u00e7 et al., 2015).", "startOffset": 27, "endOffset": 94}, {"referenceID": 10, "context": "tain and classify manually (Akku\u015f and \u00c7ak\u0131c\u0131, 2013; Toraman et al., 2011; K\u0131l\u0131n\u00e7 et al., 2015).", "startOffset": 27, "endOffset": 94}, {"referenceID": 8, "context": "(Heck et al., 2013) who employ a method that takes advantage of user click logs of a web search engine in order to improve precision of gazetteers while maintaining recall.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "Inspired by the approach of (Heck et al., 2013), we create a graph crawling algorithm that is capable of categorizing sentences into Freebase domains and annotate named entities within the sentences with", "startOffset": 28, "endOffset": 47}], "year": 2017, "abstractText": "Turkish Wikipedia Named-Entity Recognition and Text Categorization (TWNERTC) dataset is a collection of automatically categorized and annotated sentences obtained from Wikipedia. We constructed large-scale gazetteers by using a graph crawler algorithm to extract relevant entity and domain information from a semantic knowledge base, Freebase1. The constructed gazetteers contains approximately 300K entities with thousands of fine-grained entity types under 77 different domains. Since automated processes are prone to ambiguity, we also introduce two new content specific noise reduction methodologies. Moreover, we map fine-grained entity types to the equivalent four coarse-grained types, person, loc, org, misc. Eventually, we construct six different dataset versions and evaluate the quality of annotations by comparing ground truths from human annotators. We make these datasets publicly available to support studies on Turkish named-entity recognition (NER) and text categorization (TC).", "creator": "LaTeX with hyperref package"}}}