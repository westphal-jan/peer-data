{"id": "1702.03488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2017", "title": "Octopus: A Framework for Cost-Quality-Time Optimization in Crowdsourcing", "abstract": "Managing handset - tasks down crowdsourcing non-regulated uses flexible facts objectives - - the quality instance writing, 150 cost incurred and it meant completion. Previous guards least focused a reducing - addition, or require - finally envisage, tighter their given - competing conformity. As a opportunity way that got enough the Octopus, within 1987 AI agent well development manages all. importance in tailskid. Octopus is conjunction start a excruciatingly tractable, dynamic - agent simplest consisting of other components; one suggested advance in shares per ballot to stronger the rate of temporary of tasks, another that optimizes but task for improving where place next that perform leadership positive. We demonstrate both Octopus outshines existing civil - one - within - notable mechanisms having algorithms some experiments including making sources, demonstrating addition commanding better. We making launch Octopus on Amazon Mechanical Turk give plan yet ability came providing appropriate in second something - nations, mechanisms work.", "histories": [["v1", "Sun, 12 Feb 2017 04:53:25 GMT  (1478kb,D)", "https://arxiv.org/abs/1702.03488v1", null], ["v2", "Tue, 15 Aug 2017 09:14:08 GMT  (1699kb,D)", "http://arxiv.org/abs/1702.03488v2", "10 pages, to appear in HCOMP 2017"]], "reviews": [], "SUBJECTS": "cs.AI cs.HC cs.MA", "authors": ["karan goel", "shreya rajpal", "mausam"], "accepted": false, "id": "1702.03488"}, "pdf": {"name": "1702.03488.pdf", "metadata": {"source": "CRF", "title": "Octopus: A Framework for Cost-Quality-Time Optimization in Crowdsourcing", "authors": ["Karan Goel", "Shreya Rajpal"], "emails": ["kgoel93@gmail.com", "shreya.rajpal@gmail.com", "mausam@cse.iitd.ac.in"], "sections": [{"heading": "Introduction", "text": "Task control of workflows over micro-task crowdsourcing platforms, such as Amazon Mechanical Turk (AMT), has received significant attention in AI literature (Weld et al. 2015). Typically, a requester needs to balance three competing objectives \u2013 (1) total cost, owing to payments made to workers for their responses (or ballots), (2) overall quality, usually evaluated as accuracy of the final output, and (3) the total time for completing the task. These criteria are interrelated: increasing the pay per task attracts more workers to the task, thereby reducing completion time. However, it also exhausts the budget sooner, so requesters can afford fewer ballots per task, likely reducing the overall quality.\nMost prior work on crowd controllers has focused on the tradeoff between cost (or no. of ballots) and quality (Dai et al. 2013; Lin, Mausam, and Weld 2012; Bragg, Mausam, and Weld 2013; Kamar et al. 2013; Parameswaran et al. 2012). A common approach is to define a Partially Observable Markov Decision Process (POMDP) per task, which decides on whether to get another ballot or submit the best answer for that task. However, this work is time-agnostic, and assumes that pay per ballot is given as input.\nRecent work has also studied the tradeoff between cost and completion time for a batch of tasks (Gao and\n\u2217Most work was carried out when the authors were students at the Indian Institute of Technology - Delhi. Copyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nParameswaran 2014). They model the problem as a Markov Decision Process (MDP) that changes the pay per ballot, so that all ballots can be obtained by the given deadline in a cost-efficient manner. However, this work assumes that the number of ballots needed to complete the whole batch is a constant known to the requester in advance.\nThere is limited research on simultaneously addressing tradeoffs between cost, quality and latency. We know of only one work that studies this for the specific workflow for finding max of a set of items (Venetis et al. 2012). This work assumes that latency is pay-independent \u2013 an assumption wellknown to be incorrect (Faradani, Hartmann, and Ipeirotis 2011; Gao and Parameswaran 2014). Under a fixed latencyper-response assumption, they speed up the task and save cost by taking fewer responses. Our three-way optimization is for the broader case of answering a batch of tasks, and uses variable pricing to alter the latency of task completion, in line with crowdsourced marketplace dynamics.\nBuilding upon these strands of research, we present OCTOPUS, an AI agent that can balance all three objectives (cost, quality, time) in concert on real crowdsourced marketplaces, by optimizing a requester-specified, joint utility function for a batch of tasks. It achieves this by controlling both the pay per ballot and the (predicted) accuracy of each individual task.\nWe could model the whole problem as a single POMDP, however, that is unlikely to scale. An alternative could be to use multi-objective MDPs, but they are also less tractable, because they produce a pareto-optimal set of solution policies (Chatterjee, Majumdar, and Henzinger 2006). OCTOPUS uses a three-component architecture \u2013 one to set the pay per ballot (COSTSETTER), another to choose the next available task (TASKSELECTOR) and a third to control each task\u2019s quality (QUALITYMANAGER). A key technical novelty is in the careful modeling of the COSTSETTER\u2019s state space in order to circumvent intractability \u2013 the state space contains aggregate statistics regarding completion levels of all tasks, so that it can decide the next best pay to set.\nWe perform extensive experiments using both simulated and real data, as well as online experiments on AMT. Since no existing system performs direct 3-way optimization in crowdsourced marketplaces, our experiments compare against existing state-of-the art approaches that optimize 2 of the 3 objectives. We find that in most settings,\nar X\niv :1\n70 2.\n03 48\n8v 2\n[ cs\n.A I]\n1 5\nA ug\n2 01\n7\nOCTOPUS simultaneously outperforms, or is at par with multiple variants of these baselines. Our contributions are:\n1. We describe OCTOPUS, a novel framework to address cost-quality-time optimization for a batch of tasks in a crowdsourced marketplace setting. It contains three components that set the pay per ballot, select the next task and control each task\u2019s quality. A key technical novelty is the use of aggregate statistics of all tasks in the state space design for the COSTSETTER, ensuring tractability for real-time deployment.\n2. OCTOPUS consistently performs at par with or better than state-of-the-art baselines, yielding up to 37% reward improvements on real data.\n3. We deploy OCTOPUS on AMT and demonstrate that it is able to optimize utility effectively in a live, online experiment.1"}, {"heading": "Related Work", "text": "Cost-Quality Optimization. There is significant work on getting more quality out of a fixed budget. One branch of this research focuses on collective classification, which develops aggregation mechanisms to infer the best output per task, given a static set of ballots (Whitehill et al. 2009; Welinder et al. 2010; Oleson et al. 2011; Welinder and Perona 2010). The other branch studies intelligent control, which dynamically decides whether to ask for a new ballot on a task, or stop and submit the answer. These include control of binary or multiple choice tasks (Dai et al. 2013; Parameswaran et al. 2012; Kamar et al. 2013), multi-label tasks (Bragg, Mausam, and Weld 2013), and tasks beyond multiple choice answers (Lin, Mausam, and Weld 2012; Dai, Mausam, and Weld 2011). All these works design agents to control a single task and assume a constant pay per ballot. Our work closely follows the POMDP formulation laid down in Dai et al. (2013) for binary tasks. Cost-Time Optimization. Increasing pay per ballot can reduce completion times. Faradani et al. (2011) develop models to find upfront, the static price per ballot so that a desired deadline can be met. Gao & Parameswaran (2014) extend this by varying pay at discrete time-steps using an MDP. Both approaches assume a fixed number of ballots known a-priori, without dynamic quality control of tasks. There is also some work on price-independent latency reduction (Haas et al. 2015). Cost-Quality-Time Optimization. There is limited work in this area. The only paper we are aware of is Venetis et al. (2012), which addresses cost-quality-time optimization but in a restrictive setting with important distinctions from our work: (i) they look at max-finding for a set of items, while our task-type is classification; (ii) they consider latency to be pay-independent and fixed per task, while we study the more realistic setting in which changing pay directly impacts workers\u2019 desire to work on our tasks; (iii) unlike us, they don\u2019t change pay per task directly, and instead, change the number of responses sought per task to control both cost\n1Code can be found at https://github.com/ krandiash/octopus.\nand latency. Qualitatively, our work thus also highlights how workers perceive pay changes in a crowdsourcing marketplace and its overall effect on task completion. Worker Retention. Previous work deals with incentivizing workers to perform more tasks via bonuses or diversification (Rzeszotarski et al. 2013; Ipeirotis and Gabrilovich 2014; Difallah et al. 2014; Dai et al. 2015). Recently, Kobren et al. (2015) model the process of worker retention. We present empirical results that suggest worker retention plays a dominant role in determining task completion rates. Task Routing. Prior work deals with two issues; deciding which task from the batch to solve next, or which worker to route a task to. Ambati et al. (2011) rank tasks based on user preferences using a max-entropy classifier. Other work uses low-rank matrix approximations (Karger, Oh, and Shah 2014) for equal difficulty tasks. Rajpal et al. (2015) decide which worker pool to route a task to. Other papers study task routing on volunteer platforms (Bragg et al. 2014; Shahaf and Horvitz 2010).\nLike AMT, we assume no control on which worker picks a ballot job, but we select the best next task to assign to an incoming worker. Following (Mason and Watts 2010; Gao and Parameswaran 2014), we assume that worker quality is independent of the pay per ballot. We re-verify this for our data in our experiments. Decentralized Approaches. There is related work in decentralized Wald stopping problems (Teneketzis and Ho 1987) which considers how to optimize a common utility function given a set of agents who each make independent observations. However, these approaches do not scale well with the number of agents (tasks in our setting), which can be quite large. There is also work in decentralized metareasoning (Hansen and Zilberstein 2001) to decide when to stop optimizing a utility function. Metareasoning approaches typically assume that utility is monotonically increasing over time, which is not true in our setting since for instance, conflicting ballots on a single task would decrease utility."}, {"heading": "Problem Definition", "text": "A requester provides a batch of n binary tasks q \u2208 1 . . . n, each having a 0/1 response. They also provide a utility function U , which describes how to tradeoff cost, time and quality. The agent can dynamically change pay per ballot c, and choose a variable number of ballots per task to optimize the final objective. We study the setting where U is expressed as a sum of task-level utilities (U ) minus cost, i.e. U = \u2211n q=1(Uq \u2212 Cq). Here, Cq is total money spent on q. We assume that answers to all tasks are to be returned to the requester as one single batch. MDP/POMDP background. An MDP models the longterm reward optimization problem under full observability and is defined by a five tuple \u3008S,A, T,R, \u03b3\u3009. Here, S is a set of states, A a set of actions, T (s\u2032|s, a) denotes the probability of transitioning to state s\u2032 after taking action a in state s, and R(s, a) maps a state-action pair to a realvalued reward. \u03b3 is the discount factor for making infinitehorizon MDPs well formed. A POMDP extends an MDP into a partially-observable setting, where the state is not fully\nobservable and only a belief (probability distribution) over possible states can be maintained using observations from the model. A POMDP is represented as \u3008S,A, T,R,O, \u03b3\u3009 tuple, where a new function O(o|s\u2032, a) denotes the distribution over observations on taking an action a and arriving in a new state s\u2032. Lack of space precludes a long discussion of the subject \u2013 there are existing solvers for solving MDPs and POMDPs of reasonable sizes, e.g. (Smith and Simmons 2012), which we use in our work.\nTo formulate the problem optimally, we would need to define a single, centralized POMDP over a state containing answers and difficulty estimates of all individual tasks as well as the current pay per ballot and the current time. The actions will include requesting a ballot on a task q, changing the pay, and a terminal submit action. Solving this POMDP would yield the optimal policy, which would decide which task to get ballots on next, when to change pay and when to submit. Since the number of tasks in a batch can be huge, this POMDP is unlikely to scale due to a large state and action space. Naive extensions to Dai et al.\u2019s or Gao et al.\u2019s state-of-the-art models for cost-quality and cost-time optimization respectively are not possible either \u2013 Dai et al.\u2019s model is solved per task, whereas pay must be set based on progress of the whole batch of tasks; Gao et al.\u2019s model assumes a fixed number of ballots per task, and has no natural way to optimize quality by taking a variable number of ballots based on each task\u2019s difficulty."}, {"heading": "OCTOPUS for Three-Way Optimization", "text": "We propose a three-component architecture (see Figure 1). In OCTOPUS each task has its own QUALITYMANAGER that decides, based on the current pay, whether it is worth taking another ballot for this task (light edge) or not (dark edge). This information is conveyed to the TASKSELECTOR, which selects an available (light) task to route to an incoming worker. Based on the current progress of the whole batch, the COSTSETTER decides what pay per ballot to set; this action is taken at periodic intervals. We design OCTOPUS such that it can allocate tasks as and when workers arrive and works instantaneously in practice, therefore utiliz-\ning crowdsourcing marketplaces with full parallelism."}, {"heading": "QualityManager", "text": "Background. QUALITYMANAGERs follow the worker response model and POMDP formulation of Dai et al. (2013). Each worker is assumed to have an error parameter \u03b3 \u2208 (0,\u221e) (\u03b3 = 0 is error-free). An average worker has error parameter \u03b3\u0304. Each task q has an unknown true Boolean answer tq , and an associated difficulty dq \u2208 [0, 1] (dq = 0 is easy) \u2013 these are estimated using an Expectation-Maximization algorithm (Whitehill et al. 2009) as data is received. Each task has a prior difficulty distribution p(dq). A worker\u2019s ballot for q depends on their \u03b3, tq and dq .\nIn Dai et al. (2013) the POMDP for q maintains a belief state bq over (dq, tq) state tuples. For instance bq(0.5, 1) = 0.4 indicates a 40% belief that dq = 0.5 and the answer to q is 1. The POMDP has two actions; (1) request another ballot, or (2) mark q as completed. The POMDP policy \u03c0q maps every belief state to an action. Each time the POMDP receives another ballot, a Bayesian update is performed to re-estimate a new belief. For OCTOPUS, the POMDP optimizesUq\u2212Cq . We note that \u03c0q depends on the pay per ballot c: if c is smaller, the POMDP can afford more ballots. Dai et al.\u2019s original model keeps c constant, but in our case the COSTSETTER can change it, triggering a change in \u03c0q .\nWe define vq as the confidence in the most probable answer for task q; vq = max(v0q , v 1 q ) \u2208 [0.5, 1], where v0q , v1q are the current probabilities that q\u2019s answer is 0 or 1 respectively. vq can be computed from bq by summing out dq . Computation of Aggregate Statistics. We now define two batch-level statistics, which will be a key part of the state space representation for the COSTSETTER.\nWe first define a normalized estimate of task quality, \u03bdq = 2vq \u2212 1; \u03bdq normalizes vq so that it lies in [0, 1]. A high value of \u03bdq (near 1) indicates the POMDP\u2019s high confidence in its estimated answer for q, and vice versa for a low (near 0) value. We also define a related notion of batch quality, \u03bd\u0304 = 1n \u2211n q=1 \u03bdq , which is an aggregate statistic estimating the current quality for the entire batch of n tasks. Finally, we construct the batch quality histogram \u2013 a histogram built by binning tasks into equally sized bins based on their \u03bdq values. The bin width is denoted as \u2206\u03bd .\nThe COSTSETTER also needs an estimate of the number of ballots remaining. We define \u03b8q(\u03c0q) as an estimate of the expected number of ballots that will be needed (starting from the current time) until task q will be marked completed. For notational ease we write \u03b8q to denote \u03b8q(\u03c0q). Recall that \u03c0q can change with a change in c. Hence \u03b8q also depends on c.\nHow can we compute \u03b8q? We use a trajectory-tree approach (similar to (Kearns, Mansour, and Ng 1999)) called FRONTIERFINDING. We construct a binary tree of future observations, rooted at the current time step, where each edge corresponds to an observation (a worker response of 0/1). The node below any edge contains the belief state generated by updating the POMDP, using the observation associated with that edge. Each trajectory is a path from the root to a leaf, and is generated with an associated path probability (using Dai et al.\u2019s generative model assuming an average worker). A leaf is created whenever the policy takes\nthe \u2018mark as completed\u2019 action, or when the path probability drops below a threshold. \u03b8q is simply the expected length of a trajectory in this tree.\nWe also estimate the expected ballots to completion for the batch: \u03b8 = \u2211n q=1 \u03b8q . \u03b8\u2019s role is similar to that of \u03bd\u0304 \u2013 it is an aggregate statistic that describes how far the batch is from completion. It also helps in quantifying the expected cost of completion: if \u03b8 = 1000 and c = 3, we would expect to spend c \u00b7 \u03b8 = 3000 units of money to complete the batch.\nIn summary, we described the design of a per-task QUALITYMANAGER. Collectively, n of these help us in estimating two aggregated quantities, \u03bd\u0304 and \u03b8, which measure the overall quality, and degree of completion of the batch, respectively. All notation for this and future sections is summarized in Table 1."}, {"heading": "TaskSelector", "text": "The TASKSELECTOR decides which incomplete task to assign to the next incoming worker. It must have an \u2018anytime\u2019 behavior, i.e. it must increase utility U quickly. This is because the time of final submission is not in its control, and the batch might be submitted at any time by the COSTSETTER.\nTo be prepared for any contingency, the TASKSELECTOR uses a 1-step greedy policy over expected utility gain. We define each task\u2019s priority (\u03c6q) as the difference between the current utility (Uq) of q and the expected utility after receiving 1 ballot (U \u2032q) from an average worker (error rate \u03b3\u0304), given the current belief state bq of q\u2019s QUALITYMANAGER. Thus, \u03c6q = E[U \u2032 q|\u03b3\u0304,bq] \u2212 Uq . TASKSELECTOR assigns the task with the maximum \u03c6q value to the next available worker. Unfortunately, Uq (and therefore U) is neither monotonic (conflicting ballots decrease utility) nor submodular (a skilled worker could arrive after an error-prone one), so we cannot utilize prior work on adaptive submodularity (Golovin and Krause 2011; Bragg et al. 2014) to guarantee solution quality. Providing quality bounds is left for future work.\nLastly, note that the task allocation process is instantaneous, as well as completely parallelized, since we don\u2019t wait for a task to be returned before allocating another task. Given enough workers, we could get ballots on every single task in parallel. This is important, since it allows us to take full advantage of micro-task marketplaces."}, {"heading": "CostSetter", "text": "The COSTSETTER is an MDP that changes c (pay per ballot) in order to maximize U . It uses information about the completion level of each task to assess whether the batch of tasks is completing on schedule or needs to be sped up or slowed down. To influence the rate of completion of the batch, it sets c at discrete time steps \u03c4 \u2208 {0,\u2206\u03c4 , 2\u2206\u03c4 , . . . }.\nThe key challenge for the COSTSETTER is in defining the state space. Ideally, as stated earlier, each task\u2019s belief bq should be part of the state, but that would make computations intractable. Instead, we approximate by using aggregate statistics over the whole batch of tasks. We describe the state space, actions, transition functions, and rewards of this MDP below.\nState Space. The choice of the best pay per ballot c depends on its current value, the current time, and the aggregate degree of completion of the batch. We choose the state to be a 4-tuple (\u03bd\u0304, \u03b8, \u03c4, c). Both \u03bd\u0304 and \u03b8 are important for this decision; \u03bd\u0304 estimates the expected accuracy on the batch, while \u03b8 gives us an idea of how much more improvement in \u03bd\u0304 is possible (at the current c). For a fixed \u03bd\u0304 and c, a high \u03b8 would indicate the presence of several unsolved tasks and the possibility of improving \u03bd\u0304. On the other hand, a low \u03b8 would indicate that most tasks are solved and there is little improvement possible. \u03b8 therefore captures the spread of the distribution of task qualities \u03bdq , while \u03bd\u0304 is the mean of this distribution. We use this intuition later to construct the transition function for the COSTSETTER.\nAs another example consider a case where both \u03b8 and \u03bd\u0304 are high. This indicates that the QUALITYMANAGERs consider there be to scope for quality improvement despite the batch quality being high already, possibly due to very low c. If we did not have \u03b8 in the state, we would instead base our decision on the high \u03bd\u0304 value, and believe that further improvement in utility was not possible.\nAll state variables are continuous, and for tractability we discretize them. \u03b8 is discretized with a granularity \u2206\u03b8, \u03bd\u0304 with a granularity \u2206\u03bd , and \u03c4 with a granularity \u2206\u03c4 . We assume that c can take values {c1, c2 . . . , ck}. These values are defined by the requester, and in practice would respect marketplace constraints, such as minimum wages. Interestingly, the model is robust in that if a requester provides a very poor starting wage, workers will likely not pick up the task, and the model will subsequently respond by increasing the wage.\nActions and Rewards. Every state has access to two paychange actions: \u2191 and \u2193. \u2191 increases ci to ci+1 while \u2193 does the opposite. The \u2191 and \u2193 actions incur no cost to the system. However, in practice we assign a small cost to these actions to prevent frequent cyclical pay-changes in a policy. Changing c has no impact on \u03c4 and \u03bd\u0304, but it does change \u03b8, since the number of ballots remaining per task depends on the current pay per ballot (via the policy \u03c0q). We discuss how to compute this when defining the transition function.\nWe also have a no-change action, which increments \u03c4 by \u2206\u03c4 , along with asking workers for more ballots at c, which remains unchanged (for the next \u2206\u03c4 duration). This is essentially a marketplace action, where we post tasks to the marketplace with a pay per ballot equaling c. At the end of \u2206\u03c4 minutes, we would then arrive in a new state. The cost of this transition (to the nearest \u2206\u03b8 value) is just the amount paid to workers during this duration on the marketplace, equaling the number of ballots received during this time, multiplied by c.\nThe final action is a \u2018terminate\u2019 action that submits all answers to the requester. Its reward should be U based on batch quality and current time (cost is not needed, since that was already accounted in the no-change action). Unfortunately, the MDP has access to only the aggregate statistics, and not the full batch quality histogram, which is needed for computing U . We now describe a novel \u03b2-reconstruction procedure that allows us to extrapolate the full histogram from aggregate statistics, useful for computing this reward as well as the transition function. \u03b2-Reconstruction. The goal is to reconstruct an approximate batch quality histogram given the aggregate statistics, \u03bd\u0304 and \u03b8, and the current c. The procedure assumes that the histogram can be approximated with a two parameter Beta distribution, \u03b2\u03bb1,\u03bb2 . Also assume that we are provided a function \u03b8\u0303 that maps a (\u03bdq, c) pair to \u03b8q; it returns the expected number of ballots needed for a task q given its current quality and pay per ballot. Note also that \u03b8\u0303 will be a non-increasing function of \u03bd. We now show that we can find suitable \u03bb1 and \u03bb2 given \u03bd\u0304, \u03b8 and \u03b8\u0303.\nTo compute the best fit \u03bb1, \u03bb2 values, we solve two equations. The 1st equation enforces that the mean of the reconstructed distribution is \u03bd\u0304: \u03bb1\u03bb1+\u03bb2 = \u03bd\u0304. We can reparameterize \u03b2\u03bb1,\u03bb2 using \u03bb1 = \u03bb\u03bd\u0304; \u03bb2 = \u03bb(1 \u2212 \u03bd\u0304) and write it as \u03b2\u03bb(\u03bd\u0304). Our task now reduces to finding the best fit \u03bb. A 2nd equation imposes \u03b8 as an expectation over the batch quality distribution: \u03b8\u0302(\u03bb) = n \u222b 1 0 \u03b8\u0303(\u03bd, c)\u03b2\u03bb(\u03bd)d\u03bd \u2248 \u03b8.\nSince \u03b8\u0303 is computed using a POMDP policy, it will rarely be available in closed form. The integral above is approximated using a numerical algorithm. The best \u03bb is found via argmin\u03bb|\u03b8\u0302(\u03bb) \u2212 \u03b8| using a linear search, and works instantaneously in practice. Having found a suitable \u03bb value, we now bin all n tasks into the \u03b2 distribution to recover the task quality histogram, as desired.\nFinally, we describe the procedure for estimation of \u03b8\u0303(\u03bdq, c). First we calculate the corresponding vq = 0.5(\u03bdq+ 1). Intuitively, \u03b8\u0303(\u03bdq, c) assesses the number of ballots taken by the QUALITYMANAGER when its belief bq in the current\nanswer is vq and it has difficulty dq . However, we haven\u2019t reconstructed dq \u2013 we use the prior distribution p(d) as its belief on dq . To compute \u03b8\u0303(\u03bdq, c) we initiate the QUALITYMANAGER\u2019s POMDP from such a belief state, and compute \u03b8q under policy \u03c0q using FRONTIERFINDING. \u2191 / \u2193 Transitions. We now describe the transitions for \u2191 action in a given state (\u03bd\u0304, \u03b8, \u03c4, ci) to reach (\u03bd\u0304, \u03b8\u2032, \u03c4, ci+1). As we change c, the main change is in \u03b8. We quantify this change by a simple observation: regardless of pay, the number of ballots taken until this point is fixed. Suppose that at \u03c4 = 0 we compute the estimated number of ballots for the batch as \u03b80(ci). If we have taken x ballots till now, our current estimate of \u03b8 will be simply \u03b80(ci) \u2212 x. At pay ci+1, our next state\u2019s estimate should be \u03b80(ci+1) \u2212 x. Thus, when changing pay from ci to ci+1 we can simply add \u03b80(ci+1) \u2212 \u03b80(ci) to compute \u03b8\u2032. A similar analysis works for the \u2193 action. No-Change Transitions. The no-change action emulates the setting that the tasks are posted on the platform for \u2206\u03c4 time at pay c. Let the next state be (\u03bd\u0304\u2032, \u03b8\u2032, \u03c4 + \u2206\u03c4 , c). Estimation of \u03bd\u0304\u2032 and \u03b8\u2032 requires a model of task completion. Similar to Gao & Parameswaran (2014), we maintain a paydependent ballot completion model, Pr(nb|\u2206\u03c4 , c), as the probability that OCTOPUS will receive nb ballots in duration \u2206\u03c4 at pay c. However, different from their work, the probability model combines the effects of worker arrival, retention and time taken per task (and not just arrival). Thus, \u03b8 will reduce by nb with probability Pr(nb|\u2206\u03c4 , c). The cost of the transition will be \u2212c \u00b7 nb. Since nb is discretized upto \u2206\u03b8 granularity, the cost will be rounded off to the nearest bucket.\nFor updating \u03bd\u0304, we \u03b2-reconstruct the batch quality histogram using the current state. We then bin n tasks into this histogram, and simulate the TASKSELECTOR on this reconstructed batch. To do this, we first create a POMDP belief state for each reconstructed task by choosing \u03bd from the histogram to recover v, and using the prior difficulty distribution. We then select a task, simulate a ballot using an average worker (\u03b3\u0304) and compute the posterior belief. We continue until all nb ballots are used up. At the end we compute the \u03bd\u0304\u2032 based on the updated state of the batch. For robustness, we repeat this entire procedure multiple times and average the \u03bd\u0304 values from different runs. Implementation Details. We construct the whole MDP with all transitions and rewards using simulations and \u03b2reconstructions as described above. Since time can be unbounded, we keep a max time \u03c4max when defining the total state space. We also recognize that the transition from \u03bd\u0304 to \u03bd\u0304\u2032 depends on nb but not on any other part of the state. By caching a table of (\u03bd\u0304, nb, \u03bd\u0304\u2032) values once, we can save on a lot of simulations when computing the transition function of the no-change action in various states.\nWe use Value Iteration to learn the policy with (\u03bd\u0304 = 0, \u03b8 = n \u00b7 max \u03b8\u0303(\u03bd = 0, c = c1), \u03c4 = 0, c = c1) as our start state. Intuitively, we are starting at 0 quality, the maximum possible number of ballots to completion, and the lowest price.\nIn a real execution environment, it is possible that over\ntime, the COSTSETTER\u2019s aggregates diverge from the set of QUALITYMANAGER\u2019s beliefs in the batch. To improve performance, we synchronize the COSTSETTER\u2019s \u03bd\u0304 and \u03b8 to those of the real batch after every time interval. Experimentally OCTOPUS performs well even without synchronization, indicating that the learned transitions are effective.\nIn summary, we described the COSTSETTER, an MDP that keeps track of the aggregate statistics \u03bd\u0304 and \u03b8 for a batch of n tasks, and changes pay to optimize utility U . A key contribution is a novel \u03b2-reconstruction procedure that approximates the batch quality histogram for a state."}, {"heading": "Experiments", "text": "We conduct three sets of experiments: (i) experiments on simulated data; (ii) offline evaluation on real data; (iii) live experiments on Amazon MTurk (AMT). Model Parameters. For experiments, we initialize OCTOPUS with a hard deadline utility function: Uq = \u2212\u221e if \u03c4 < \u03c4max; otherwise, Uq = \u2212P for every incorrect answer and zero for a correct answer. This joint utility combines the utilities from Dai et al.\u2019s and Gao et al.\u2019s models. Here, penalty P represents how important quality is to the requester. Notice that since a POMDP doesn\u2019t know whether it is submitting the correct answer, it cannot compute the utility exactly. It uses its belief to estimate expected utility as \u2212P(1\u2212 vq). This linear utility makes the reward computations for COSTSETTER\u2019s \u2018terminate\u2019 action simple. Given the \u03bd\u0304 of the state, the reward is calculated as \u22120.5P(1\u2212 \u03bd\u0304) on termination.\nHaving a hard deadline makes the COSTSETTER state space finite, since we only consider states with \u03c4 \u2264\n\u03c4max. We use a time-independent Poisson process for the ballot completion model Pr(nb|\u2206\u03c4 , c). These parameters are illustrative \u2013 OCTOPUS can accommodate other utilities/distributions. Baselines. Our main goal is to compare OCTOPUS\u2019s performance with state-of-the-art methods. However, we know of no algorithms that performs direct three-way optimization in a crowdsourced marketplace setting.2 So we compare against state-of-the-art methods that optimize 2 of the 3 objectives, while giving them the added benefit of our TASKSELECTOR. We compare OCTOPUS to Dai et al.\u2019s and Gao et al.\u2019s models, which are related to our work. Code was provided by the authors.\nAll comparisons are on the requester\u2019s utility function, U computed against gold labels with P = 200. We normalize U so that OCTOPUS always has 1.0 utility, i.e. the performance of baselines is represented as a proportion of OCTOPUS. Data collection. We collect data on AMT at 6 pay points using a Twitter Sentiment dataset (Sheshadri and Lease 2013). Workers are asked to classify the sentiments of tweets into either positive or negative sentiment. At each price point ($0.001, $0.002, . . . , $0.006 per ballot), we post 400 tweets, and seek 20 ballots/tweet. A single HIT contains 10 tweets for a worker to solve. 40 tweets are common to all prices for a total of 2200 tweets. Each price is posted on a different weekday at the same time, and remains active for 24 hours. This ensures consistency in data collection across prices and minimizes interaction between different prices.\nFigure 2a (task completion rates vs. pay) verifies that higher pricing results in faster task completion: at 0.1 cent, only 5500 ballots are received even after 24 hours, whereas at 0.6 cents, all 8000 ballots are received within 12 hours. We estimate the Poisson parameters using this data. Worker Retention vs. Arrival. Contrary to prior work (Faradani, Hartmann, and Ipeirotis 2011; Gao and Parameswaran 2014), we observe that the increase in task completion rate with pay is predominantly due to higher worker retention, rather than a higher rate or number of worker arrivals (possibly due to the large number of HITs that we sought). Figure 3a shows that both retention and task completion rates are highly correlated, doubling as price goes from 0.1 to 0.6 cents. However, Figure 3b shows that the worker arrival rate does not rise much. To the best of our knowledge, there is no prior marketplace model that handles both retention and arrivals.\nWe also verify that worker quality is independent of pay. We run a K-S test for every pair of costs. We could not reject the null hypothesis (error rates drawn from cost-independent distributions) at p < 0.05 (Figure 2b)."}, {"heading": "Simulation Experiments", "text": "Our main aim through simulations is to assess the quality of \u03b2-reconstruction. We use a variety of parameter settings and\n2We don\u2019t compare to Venetis et al. (2012) since they run a tournament for max-finding, different from our task type. They also don\u2019t change pay directly or model latency as done by us.\nsimulate OCTOPUS with ballot arrivals simulated according to a pay-dependent Poisson process, and answers generated based on worker models. Note that this experiment is without any synchronization between the COSTSETTER state and the real batch. Figure 5a compares the \u03b8 values between the system and the batch (for one such setting3). Even after 10 time intervals, and across multiple cost changes (the sharp drops in the curve), our tracking is extremely accurate. Figure 5b shows that our \u03bd\u0304 tracking is extremely effective as well, with very little divergence from the true value. Even when the values do diverge in both cases, they are highly correlated.\nHigh quality tracking of \u03b8 and \u03bd\u0304 are essential. If we di-\n3500 tasks with p(d) = \u03b2(2.0, 2.0), worker errors sampled from \u0393(2.0, 0.5), P = 200, \u2206\u03c4 = 15 mins, \u2206\u03b8 = 10, \u2206\u03bd\u0304 = 100.\nverge too far from the real values, we would likely optimize the long-term expected reward poorly. Overall, this reaffirms the hypothesis that the COSTSETTER is able to capture the global state of the whole batch using just the aggregate statistics.\nOur other goal is to compare OCTOPUS\u2019s performance with baselines in simulation, which we do below. Comparison to DAI. Comparing OCTOPUS with DAI highlights the benefit of changing cost on real-world utility. Ideally, our method should be able to vary pay to match or exceed the utility of the static cost baselines. We run both OCTOPUS and DAI with deadlines ranging from 60 to 360 minutes. We run DAI for different static pays, ranging from 1 to 6 (measured in a tenth of a cent), allowing it to take ballots until the deadline. Statistical significance is indicated on the plots.\nFigure 4 shows the comparison. OCTOPUS simultaneously outperforms (or is at par with) all static cost baselines for every deadline, whereas each static cost baseline has a \u2018sweet spot\u2019 range of deadlines where it does best. OCTOPUS plans robustly, where despite making pricing decisions 24 times without synchronizing for the 6 hr deadline, it still outperforms DAI. Figure 4b depicts the utility averaged across all deadlines as a function of task-completion rates. As shown, OCTOPUS is robust to different task-completion rates, and continues to outperform DAI on changing them. Figure 4d & 4e explain why OCTOPUS achieves better utility scores than DAI \u2013 it maintains very high accuracy while keeping costs reasonable. Qualitatively, for every deadline, we observe that OCTOPUS tends to have an average cost that is close to the best static cost.\nComparison to GAO. Comparing with GAO allows us to delineate the effect of the QUALITYMANAGERs while optimizing for batch quality using aggregate statistics.\nFor fairness, we augment GAO\u2019s framework; fixing r ballots/task up-front, and using the same worker response model as OCTOPUS for ballot aggregation. Figure 4c demonstrates OCTOPUS\u2019s performance against GAO for different values of r. OCTOPUS consistently outperforms GAO for all values of r and across all deadlines. OCTOPUS\u2019s performance improves for longer deadlines, due to the higher quality achieved by the QUALITYMANAGER. For instance, OCTOPUS is around 100% better than GAO for the 6 hr deadline. We see that GAO-3 and OCTOPUS incur nearly the same cost (Figure 4g) but OCTOPUS is \u223c6% better in terms of accuracy (Figure 4f). Examining Figure 6b reveals that\nthis is in part due to OCTOPUS collecting \u00d7 13 more ballots than GAO-3 by changing pay/ballot more intelligently.\nIt is interesting to note that increasing r does not significantly improve baseline performance. This is due to the fact that the QUALITYMANAGER only takes more ballots on tasks that really need them. For GAO the extra cost spent on every task is not offset by a corresponding increase in quality, especially for tasks that are very easy (don\u2019t require r ballots) or too hard (unsolved even with r ballots). In Figure 4f & 4g, we notice that OCTOPUS once again has nearhighest accuracy, but spends much less than GAO variants that have higher accuracy than OCTOPUS.\nLastly, Figure 6 demonstrates that OCTOPUS has intuitive behavior \u2013 as the deadline length increases, OCTOPUS spends less pay/ballot on average since more time can be taken to finish the batch of tasks.\nThese experiments demonstrate that OCTOPUS simultaneously achieves higher utility than every variant of baseline methods, and does so without sacrificing accuracy or incurring high costs."}, {"heading": "Offline Experiments on Real Data", "text": "We run extensive offline experiments on our collected data against the state-of-the-art baselines described earlier.\nWhen comparing algorithms we sample different execution trajectories from the real data by choosing a random ballot for each task (since quality is cost-independent), while keeping ballot arrivals as per the real data. Multiple trajectories help compute statistical significance over algorithms\u2019 performances. These experiments synchronize the COST-\nSETTER\u2019s state with the real batch after every \u2206\u03c4 time. We use an uninformed uniform distribution for the difficulty prior p(d).\nFigure 7a shows OCTOPUS when compared to DAI at different static price points, with the x-axis being different \u03c4max values. Statistical significance is marked on the plots. OCTOPUS outperforms most DAI costs across all deadlines, with upto 37% increase in real utility. No single static cost is able to match OCTOPUS across all deadlines. This underscores the benefits of changing pay dynamically based on current task completion. In Figure 7d & 7e, we see that OCTOPUS achieves high accuracy at relatively low cost.\nFigure 7b compares OCTOPUS with GAO-r, with r denoting the static number of ballots per task. We find that OCTOPUS outperforms GAO for most deadlines. Further analysis reveals that our batch exhibits a bi-modal task difficulty distribution \u2013 no algorithm exceeds around 76% accuracy, while getting 70% is easy with \u223c1 ballot per task (in Figure 7f, we see a sharp jump in accuracy only when the deadline is long enough to take a lot of ballots). For shorter deadlines, where solving difficult tasks is infeasible, GAO outperforms OCTOPUS slightly, since OCTOPUS optimizes with respect to a uniform prior. For longer deadlines, estimates of task difficulties are refined by the QUALITYMANAGERs, and OCTOPUS gives large gains.\nLastly, OCTOPUS outperforms TASKSELECTOR baselines where GREEDY task selection is replaced by a random policy (RANDOM), or a single round robin followed by random selection (RANDOM-ROBIN), by large margins (Figure 7c). Note also that the baselines start to converge over time, as the advantage of doing intelligent task selection diminishes when nearly all tasks are run till completion.\nOverall, we find that OCTOPUS learns robust policies, consistently outperforming all baselines."}, {"heading": "Live Online Experiments", "text": "Lastly, we deploy OCTOPUS on AMT to test performance in a dynamic, online setting, as well as gain qualitative insight into worker behavior. In this, we keep exactly 3 HITS (of 10 tasks each) on AMT at a time. If a worker accepts a HIT, another one is posted immediately \u2013 this enables greedy task routing while ensuring full power of worker parallelism. After every \u2206\u03c4 mins, all available HITs are taken down and reposted with the new price output by OCTOPUS. OCTOPUS\u2019s policies are learned using task completion rates estimated from real data earlier. At runtime, querying OCTOPUS is instantaneous. We solve a batch of 500 tweets for 3 deadlines \u2013 1, 2, and 4 hours. We compare against GAO-1, the best baseline in offline expts in Table 2.\nFor the 1 hr deadline, OCTOPUS maintains pay at 0.5 cent/ballot, before decreasing it to 0.1 cent/ballot in the last 15 minutes. This allows OCTOPUS to receive around 1 ballot/task; more ballots stagnate U for short deadlines due to conflicting workers, and OCTOPUS prefers to save money to optimize utility. On the other hand, GAO maintains pay at 0.6 cent throughout to also ensure it receives 1 ballot/task, but is unlucky in the responses it receives. In terms of decision making, the superiority of OCTOPUS is clear, since it\nrecognizes the danger posed by disagreement on task quality estimates.\nIn the 2 hr deadline, OCTOPUS once again increases pay initially, but gets more ballot arrivals than expected. In response, after 45 minutes OCTOPUS decreases pay so that it can take a higher number of ballots for difficult tasks, getting a substantial accuracy and utility improvement over GAO.\nFor the 4 hr deadline, OCTOPUS is aggressive in trying to solve all tasks till completion. Due to the bi-modal difficulty of the tasks, workers provide several conflicting ballots on harder tasks, which the TASKSELECTOR prefers to re-route for utility gain. The overall accuracy increases marginally over the 2 hour deadline.\nQualitatively, workers respond as predicted \u2013 flocking to the tasks when pay was set at 0.5 cent or more, and staying away at very low pay. Workers respond naturally to the price changing algorithm; dropping out immediately if the pay is suddenly lowered, and coming back if it is increased once again. No worker complained about the fluctuating pay."}, {"heading": "Discussion", "text": "In this work, we focused on experiments with OCTOPUS in a fixed time deadline setting to compare with past work. Extension to the popular fixed budget setting (where \u2211 q Cq is constrained) is simple: (i) never synchronize \u03b8 so that its value in any state is simply the start value of \u03b8 (which is fixed and known) minus the ballots received, making it a proxy for cost incurred so far; (ii) modify the COSTSETTER\u2019s reward to give a \u2212\u221e reward for total cost (now computable using \u03b8) exceeding the budget. Another extension involves using a different formulation of the QUALITYMANAGER; any algorithm that defines an appropriate policy and from which we can compute \u03b8 and \u03bd\u0304 is suitable."}, {"heading": "Conclusion", "text": "We present OCTOPUS, one of the first AI agents for a 3-way optimization of total cost, work quality and completion time in crowdsourcing. The agent combines three different subagents that control quality per task, select the best next task and set pay for the whole batch. A key technical contribution is the computation of aggregate statistics of the quality and completeness of the whole batch \u2013 this is used as the state for best setting the next pay.\nOCTOPUS outperforms state-of-the-art baselines in a variety of simulated and real world settings, demonstrating the superiority of our approach. We also showcase OCTOPUS\u2019s real world applicability by deploying it directly on AMT. In the future, we hope to develop general purpose formulations\nof OCTOPUS as a plug-and-play architecture for practitioners."}, {"heading": "Acknowledgments", "text": "This work is supported by Google language understanding and knowledge discovery focused research grants, a Bloomberg award, a Microsoft Azure sponsorship, and a Visvesvaraya faculty award by Govt. of India to the third author. We thank Chris Lin, Yihan Gao and Aditya Parameswaran for sharing code, and all the AMT workers who participated in our experiments."}], "references": [{"title": "Towards task recommendation in micro-task markets", "author": ["Vogel Ambati", "V. Carbonell 2011] Ambati", "S. Vogel", "J.G. Carbonell"], "venue": "In AAAI Workshop on Human Computation", "citeRegEx": "Ambati et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ambati et al\\.", "year": 2011}, {"title": "Parallel task routing for crowdsourcing", "author": ["Bragg"], "venue": "In HCOMP", "citeRegEx": "Bragg,? \\Q2014\\E", "shortCiteRegEx": "Bragg", "year": 2014}, {"title": "Crowdsourcing multi-label classification for taxonomy creation", "author": ["Mausam Bragg", "J. Weld 2013] Bragg", "Mausam", "D.S. Weld"], "venue": "In HCOMP", "citeRegEx": "Bragg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bragg et al\\.", "year": 2013}, {"title": "Markov decision processes with multiple objectives", "author": ["Majumdar Chatterjee", "K. Henzinger 2006] Chatterjee", "R. Majumdar", "T.A. Henzinger"], "venue": null, "citeRegEx": "Chatterjee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chatterjee et al\\.", "year": 2006}, {"title": "Pomdp-based control of workflows for crowdsourcing", "author": ["Dai"], "venue": null, "citeRegEx": "Dai,? \\Q2013\\E", "shortCiteRegEx": "Dai", "year": 2013}, {"title": "And now for something completely different: Improving crowdsourcing workflows with microdiversions", "author": ["Dai"], "venue": "In CSCW. ACM", "citeRegEx": "Dai,? \\Q2015\\E", "shortCiteRegEx": "Dai", "year": 2015}, {"title": "Artificial intelligence for artificial artificial intelligence", "author": ["Mausam Dai", "P. Weld 2011] Dai", "Mausam", "D.S. Weld"], "venue": null, "citeRegEx": "Dai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2011}, {"title": "Scaling-up the crowd: Micro-task pricing schemes for worker retention and latency improvement", "author": ["Difallah"], "venue": "HCOMP", "citeRegEx": "Difallah,? \\Q2014\\E", "shortCiteRegEx": "Difallah", "year": 2014}, {"title": "What\u2019s the right price? pricing tasks for finishing on time", "author": ["Hartmann Faradani", "S. Ipeirotis 2011] Faradani", "B. Hartmann", "P.G. Ipeirotis"], "venue": "In AAAI Workshop on Human Computation", "citeRegEx": "Faradani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Faradani et al\\.", "year": 2011}, {"title": "Finish them!: Pricing algorithms for human computation. PVLDB", "author": ["Gao", "Y. Parameswaran 2014] Gao", "A.G. Parameswaran"], "venue": null, "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "author": ["Golovin", "D. Krause 2011] Golovin", "A. Krause"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Golovin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Golovin et al\\.", "year": 2011}, {"title": "Clamshell: speeding up crowds for low-latency data labeling. VLDB", "author": ["Haas"], "venue": null, "citeRegEx": "Haas,? \\Q2015\\E", "shortCiteRegEx": "Haas", "year": 2015}, {"title": "Monitoring and control of anytime", "author": ["Hansen", "E.A. Zilberstein 2001] Hansen", "S. Zilberstein"], "venue": null, "citeRegEx": "Hansen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2001}, {"title": "Quizz: Targeted crowdsourcing with a billion (potential) users", "author": ["Ipeirotis", "E. Gabrilovich"], "venue": "In WWW. ACM", "citeRegEx": "Ipeirotis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ipeirotis et al\\.", "year": 2014}, {"title": "Lifelong learning for acquiring the wisdom of the crowd", "author": ["Kamar"], "venue": "In IJCAI. Citeseer", "citeRegEx": "Kamar,? \\Q2013\\E", "shortCiteRegEx": "Kamar", "year": 2013}, {"title": "Budget-optimal task allocation for reliable crowdsourcing systems. Operations Research", "author": ["Oh Karger", "D.R. Shah 2014] Karger", "S. Oh", "D. Shah"], "venue": null, "citeRegEx": "Karger et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2014}, {"title": "Approximate planning in large pomdps via reusable trajectories. Citeseer", "author": ["Mansour Kearns", "M.J. Ng 1999] Kearns", "Y. Mansour", "A.Y. Ng"], "venue": null, "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Getting more for less: optimized crowdsourcing with dynamic tasks and goals", "author": ["Kobren"], "venue": null, "citeRegEx": "Kobren,? \\Q2015\\E", "shortCiteRegEx": "Kobren", "year": 2015}, {"title": "Crowdsourcing control: Moving beyond multiple choice", "author": ["Mausam Lin", "C.H. Weld 2012] Lin", "Mausam", "D.S. Weld"], "venue": "In UAI", "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Financial incentives and the performance of crowds", "author": ["Mason", "W. Watts 2010] Mason", "D.J. Watts"], "venue": "ACM SigKDD Explorations Newsletter", "citeRegEx": "Mason et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mason et al\\.", "year": 2010}, {"title": "Programmatic gold: Targeted and scalable quality assurance in crowdsourcing", "author": ["Oleson"], "venue": "Human computation", "citeRegEx": "Oleson,? \\Q2011\\E", "shortCiteRegEx": "Oleson", "year": 2011}, {"title": "Crowdscreen: algorithms for filtering data with humans", "author": ["Parameswaran"], "venue": null, "citeRegEx": "Parameswaran,? \\Q2012\\E", "shortCiteRegEx": "Parameswaran", "year": 2012}, {"title": "Pomdp-based worker pool selection for crowdsourcing", "author": ["Goel Rajpal", "S. Mausam 2015] Rajpal", "K. Goel", "Mausam"], "venue": "CrowdML Workshop,", "citeRegEx": "Rajpal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajpal et al\\.", "year": 2015}, {"title": "Inserting micro-breaks into crowdsourcing workflows", "author": ["Rzeszotarski"], "venue": "HCOMP", "citeRegEx": "Rzeszotarski,? \\Q2013\\E", "shortCiteRegEx": "Rzeszotarski", "year": 2013}, {"title": "Generalized task markets for human and machine computation", "author": ["Shahaf", "D. Horvitz 2010] Shahaf", "E. Horvitz"], "venue": null, "citeRegEx": "Shahaf et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shahaf et al\\.", "year": 2010}, {"title": "Square: A benchmark for research on computing crowd consensus", "author": ["Sheshadri", "A. Lease 2013] Sheshadri", "M. Lease"], "venue": "In HCOMP", "citeRegEx": "Sheshadri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sheshadri et al\\.", "year": 2013}, {"title": "Point-based pomdp algorithms: Improved analysis and implementation", "author": ["Smith", "T. Simmons 2012] Smith", "R. Simmons"], "venue": "arXiv preprint arXiv:1207.1412", "citeRegEx": "Smith et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2012}, {"title": "The decentralized wald problem. Information and Computation 73(1):23\u201344", "author": ["Teneketzis", "D. Ho 1987] Teneketzis", "Ho", "Y.-C"], "venue": null, "citeRegEx": "Teneketzis et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Teneketzis et al\\.", "year": 1987}, {"title": "Max algorithms in crowdsourcing environments", "author": ["Venetis"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "Venetis,? \\Q2012\\E", "shortCiteRegEx": "Venetis", "year": 2012}, {"title": "Artificial intelligence and collective intelligence", "author": ["J. Bragg"], "venue": "Handbook of Collective Intelligence.", "citeRegEx": "Bragg,? 2015", "shortCiteRegEx": "Bragg", "year": 2015}, {"title": "Online crowdsourcing: rating annotators and obtaining cost-effective labels", "author": ["Welinder", "P. Perona 2010] Welinder", "P. Perona"], "venue": null, "citeRegEx": "Welinder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "The multidimensional wisdom of crowds", "author": ["Welinder"], "venue": null, "citeRegEx": "Welinder,? \\Q2010\\E", "shortCiteRegEx": "Welinder", "year": 2010}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise", "author": ["Whitehill"], "venue": null, "citeRegEx": "Whitehill,? \\Q2009\\E", "shortCiteRegEx": "Whitehill", "year": 2009}], "referenceMentions": [{"referenceID": 30, "context": "One branch of this research focuses on collective classification, which develops aggregation mechanisms to infer the best output per task, given a static set of ballots (Whitehill et al. 2009; Welinder et al. 2010; Oleson et al. 2011; Welinder and Perona 2010).", "startOffset": 169, "endOffset": 260}, {"referenceID": 1, "context": "2013), multi-label tasks (Bragg, Mausam, and Weld 2013), and tasks beyond multiple choice answers (Lin, Mausam, and Weld 2012; Dai, Mausam, and Weld 2011). All these works design agents to control a single task and assume a constant pay per ballot. Our work closely follows the POMDP formulation laid down in Dai et al. (2013) for binary tasks.", "startOffset": 26, "endOffset": 327}, {"referenceID": 1, "context": "2013), multi-label tasks (Bragg, Mausam, and Weld 2013), and tasks beyond multiple choice answers (Lin, Mausam, and Weld 2012; Dai, Mausam, and Weld 2011). All these works design agents to control a single task and assume a constant pay per ballot. Our work closely follows the POMDP formulation laid down in Dai et al. (2013) for binary tasks. Cost-Time Optimization. Increasing pay per ballot can reduce completion times. Faradani et al. (2011) develop models to find upfront, the static price per ballot so that a desired deadline can be met.", "startOffset": 26, "endOffset": 447}, {"referenceID": 1, "context": "2013), multi-label tasks (Bragg, Mausam, and Weld 2013), and tasks beyond multiple choice answers (Lin, Mausam, and Weld 2012; Dai, Mausam, and Weld 2011). All these works design agents to control a single task and assume a constant pay per ballot. Our work closely follows the POMDP formulation laid down in Dai et al. (2013) for binary tasks. Cost-Time Optimization. Increasing pay per ballot can reduce completion times. Faradani et al. (2011) develop models to find upfront, the static price per ballot so that a desired deadline can be met. Gao & Parameswaran (2014) extend this by varying pay at discrete time-steps using an MDP.", "startOffset": 26, "endOffset": 572}, {"referenceID": 11, "context": "(Haas et al. 2015). Cost-Quality-Time Optimization. There is limited work in this area. The only paper we are aware of is Venetis et al. (2012), which addresses cost-quality-time optimization but", "startOffset": 1, "endOffset": 144}, {"referenceID": 4, "context": "2014; Dai et al. 2015). Recently, Kobren et al. (2015) model the process of worker retention.", "startOffset": 6, "endOffset": 55}, {"referenceID": 0, "context": "Ambati et al. (2011) rank tasks based on user preferences using a max-entropy classifier.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Ambati et al. (2011) rank tasks based on user preferences using a max-entropy classifier. Other work uses low-rank matrix approximations (Karger, Oh, and Shah 2014) for equal difficulty tasks. Rajpal et al. (2015) decide which worker pool to route a task to.", "startOffset": 0, "endOffset": 214}, {"referenceID": 4, "context": "QUALITYMANAGERs follow the worker response model and POMDP formulation of Dai et al. (2013). Each worker is assumed to have an error parameter \u03b3 \u2208 (0,\u221e) (\u03b3 = 0 is error-free).", "startOffset": 74, "endOffset": 92}, {"referenceID": 4, "context": "QUALITYMANAGERs follow the worker response model and POMDP formulation of Dai et al. (2013). Each worker is assumed to have an error parameter \u03b3 \u2208 (0,\u221e) (\u03b3 = 0 is error-free). An average worker has error parameter \u03b3\u0304. Each task q has an unknown true Boolean answer tq , and an associated difficulty dq \u2208 [0, 1] (dq = 0 is easy) \u2013 these are estimated using an Expectation-Maximization algorithm (Whitehill et al. 2009) as data is received. Each task has a prior difficulty distribution p(dq). A worker\u2019s ballot for q depends on their \u03b3, tq and dq . In Dai et al. (2013) the POMDP for q maintains a belief state bq over (dq, tq) state tuples.", "startOffset": 74, "endOffset": 569}, {"referenceID": 21, "context": "Similar to Gao & Parameswaran (2014), we maintain a paydependent ballot completion model, Pr(nb|\u2206\u03c4 , c), as the probability that OCTOPUS will receive nb ballots in duration \u2206\u03c4 at pay c.", "startOffset": 17, "endOffset": 37}, {"referenceID": 28, "context": "We don\u2019t compare to Venetis et al. (2012) since they run a tour-", "startOffset": 20, "endOffset": 42}], "year": 2017, "abstractText": "We present OCTOPUS, an AI agent to jointly balance three conflicting task objectives on a micro-crowdsourcing marketplace \u2013 the quality of work, total cost incurred, and time to completion. Previous control agents have mostly focused on cost-quality, or cost-time tradeoffs, but not on directly controlling all three in concert. A naive formulation of threeobjective optimization is intractable; OCTOPUS takes a hierarchical POMDP approach, with three different components responsible for setting the pay per task, selecting the next task, and controlling task-level quality. We demonstrate that OCTOPUS significantly outperforms existing state-of-the-art approaches on real experiments. We also deploy OCTOPUS on Amazon Mechanical Turk, showing its ability to manage tasks in a real-world, dynamic setting.", "creator": "LaTeX with hyperref package"}}}