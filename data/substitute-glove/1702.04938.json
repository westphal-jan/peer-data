{"id": "1702.04938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Fast and unsupervised methods for multilingual cognate clustering", "abstract": "In also pointed we explore the use of headstands study for detecting grammarians beginning multilingual something copies. We use print EM only connecting instrument segment subtle varying while expertise contrasts between two words. We virus our online providers saw locally spread sixteen generally language groups 's the world including show saying after Online PMI system (Pointwise Mutual Information) dreamier. HMM venture an especially two insular convinced systems: LexStat and ALINE. Our results suggest also a PMI necessary talented was charge computer photography can they common previous architectural indo-european even very and probability indicates common misperceptions in them what not - studied speaking families.", "histories": [["v1", "Thu, 16 Feb 2017 12:10:18 GMT  (39kb,D)", "http://arxiv.org/abs/1702.04938v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["taraka rama", "johannes wahle", "pavel sofroniev", "gerhard j\\\"ager"], "accepted": false, "id": "1702.04938"}, "pdf": {"name": "1702.04938.pdf", "metadata": {"source": "CRF", "title": "Fast and unsupervised methods for multilingual cognate clustering", "authors": ["Taraka Rama", "Johannes Wahle", "Pavel Sofroniev", "Gerhard J\u00e4ger"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Cognates are genetically related words that can be traced to a common word in a language that is no longer spoken. For example, English nail and German nagel are cognates with each other which can be traced back to the stage of Proto-IndoEuropean *h3enogh\u2212. Accurate identification of cognates is important for inferring the internal structure of a language family.\nRecent years has seen an surge in the number of publications in the field of computational historical linguistics due to the availability of word lists for large number of languages of the world [Brown et al., 2013]1 and cognate databases for Austronesian [Greenhill and Gray, 2009] and Indo-European [Bouckaert et al., 2012].\nThe availability of word lists (without cognate judgments) has allowed scholars like Rama and Borin [2015] and Ja\u0308ger [2015] to experiment with different weighted string similarity measures for the purpose of inferring the family trees of world\u2019s languages, without explicit cognate identification. On the other hand, List [2012] proposed a cognate clustering system that combines handcrafted weighted string similarity measures and permutation tests for the purpose of automated cognate identification. In a different approach, Hauer and Kondrak [2011] experimented\n1Known as Automated Similarity Judgment Program (ASJP). http://asjp.clld.org/\nar X\niv :1\n70 2.\n04 93\n8v 1\n[ cs\n.C L\n] 1\n6 Fe\nb 20\nwith linear classifiers like SVMs for the purpose of identifying cognate clusters. Finally, Rama [2015] use string kernel inspired features for training a SVM linear classifier for pair-wise cognate identification. As noted by Hauer and Kondrak [2011], availability of a reliable multilingual cognate identification system can be used to supply the cognate judgments as an input to the phylogenetic inference algorithms introduced by Gray and Atkinson [2003] and reconstruction methods of Bouchard-Co\u0302te\u0301 et al. [2013].2\nThe phylogenetic inference methods require cognate judgments which are only available for a small number of well-studied language families such as Indo-European and Austronesian. For instance, the ASJP database provides Swadesh word lists (of length 40 which are resistant to lexical replacement and borrowing) transcribed in a uniform format for more than 60% of the world\u2019s languages. However, the cognacy judgments are only available for a subset of language families. An example of such a word list is given in table 1.\nThe task at hand is to automatically cluster words that show genealogical relationship. This is achieved by computing similarities between all the word pairs belonging to a meaning and then supplying the resulting distance matrix as an input to a clustering algorithm. The clustering algorithm groups the words into clusters by optimizing a similarity criterion. The similarity between a word pair can be computed using supervised approaches [Hauer and Kondrak, 2011] or by using sequence alignment algorithms such as Needleman-Wunsch [Needleman and Wunsch, 1970] or Levenshtein distance [Levenshtein, 1966].\nIn dialectometry, Wieling et al. [2007] compared Pair Hidden Markov Model (PHMM) [Mackay and Kondrak, 2005] and pointwise mutual information (PMI) [Church and Hanks, 1990] weighted Levenshtein distance for Dutch dialect comparison. In historical linguistics, Ja\u0308ger [2013] developed a PMI based method for computing the string similarity using the ASJP database. In this paper, we apply online algorithms to train our PMI and PHMM systems for the purpose of computing word similarity.\n2The cognate clustering system in Bouchard-Co\u0302te\u0301 et al. [2013] requires the tree structure of the language family to be know beforehand. This is not a practical assumption since the tree structure of many language families of the world is not known beforehand.\nWe train our PHMM and PMI systems in different settings and test it on sixteen different families of the world. Our results show that online training can perform better than a linguistically well-informed system known as LexStat [List, 2012]. Also, the online algorithms allow our systems to be trained in few minutes and give similar accuracies as the batch trained systems of Ja\u0308ger [2013].\nThe paper is organized as follows. We discuss the relevant work in section 2. We describe the PMI and PHMM models in section 3. The Online EM procedure is described in section 4. We describe the clustering algorithm in section 5. We discuss the experimental settings and motivation behind our choices in section 6. We present and discuss the results of our experiments in section 7. We discuss the effect of different model parameters in section 8. Finally, we conclude the paper in section 9."}, {"heading": "2 Related work", "text": "Kondrak [2000] introduced a dynamic programming algorithm for computing the similarity between two sequences based on articulatory phonetic features determined by Ladefoged [1975]. The author evaluated his algorithm on a list of EnglishLatin cognates. In this paper, we evaluate on the Indo-European dataset consisting of English and Latin.\nHauer and Kondrak [2011] trained a linear SVM on word similarity features and use the SVM model to assign a similarity score to the word pair. For each meaning, a word pair distance matrix is computed and supplied to the average linkage clustering algorithm for inferring cognate clusters. The authors observe that the SVM trained system performs better than a baseline that judges the similarity of two words based on the identity of the first two consonants.\nList [2012] introduced a system known as LexStat (described in section 6) that is sensitive to segment similarities and chance similarities due to borrowing or semantic shift. The author tests this system on a number of small-sized (consisting of less than 20 languages) datasets for the purpose of cognate identification and reports that the system performs better than Levenshtein distance.\nIn a recent paper, List et al. [2016] explore the use of InfoMap [Rosvall and Bergstrom, 2008] for the detection of partial cognates in subgroups of Sino-Tibetan language family. The authors compare the performance of average linkage clustering against InfoMap and find that InfoMap performs better than average linkage clustering.\nThe above listed works test similar datasets using different experimental settings. For instance, Hauer and Kondrak [2011] trained and tested on a subset of language families that were provided by Wichmann and Holman [2013]. At the same time, to the best of our knowledge, the LexStat system has not been evaluated on all the available language families. Moreover, the PMI-LANG [Ja\u0308ger, 2013], has not been evaluated at the task of unsupervised cognate clustering."}, {"heading": "3 Models", "text": "In this section, we briefly describe the PMI weighted Needleman-Wunsch algorithm and Pair Hidden Markov Model (PHMM)."}, {"heading": "3.1 PMI-weighted alignment", "text": "The vanilla Needleman-Wunsch (VNW) algorithm is the similarity counterpart of the Levenshtein distance. It maximizes similarity whereas Levenshtein distance minimizes the distance. In VNW, a character or sound segment match increases the similarity by 1 and a character mismatch has a weight of \u22121. In contrast to Levenshtein distance which treats insertion, deletion, and substitution equally, VNW introduces a gap opening (deletion operation) penalty parameter that has to be set separately. A second parameter known as gap extension penalty has lesser or equal penalty than the gap opening parameter and models the fact that deletions occur in chunks Ja\u0308ger [2013].\nVNW is not sensitive to segment pairs, but a realistic algorithm should assign higher similarity score to sound correspondences such as /l/ \u223c /r/ than the sound correspondences /p/ \u223c /r/. The weighted Needleman-Wunsch algorithm requires a similarity score for each pair of segments, and it finds the alignment(s) betwen two input strings maximizing the sum of the pairwise similiarities of matched segment pairs.\nIn computational historical linguistics, similarity between two segments is estimated using PMI. The PMI score for two sounds i and j is defined as followed:\nPMI(i, j) = log p(i, j)\nq(i) \u00b7 q(j) (1)\nwhere, p(i, j) is the probability of i, j being matched in a pair of cognate words, whereas, q(i) is the probability that an arbitrarily chosen segment in an arbitrarily chosen word equals i. A positive PMI value between i and j indicates that the probability of i being aligned with j in a pair of cognates is higher than what would be expected by chance. Conversely, a negative PMI value indicates that an alignment of i with j is more likely the result of chance than of shared inheritance.\nWe estimated PMI scores from raw data, largely following the method described in Ja\u0308ger [2013].\nThe whole training procedure can be described as follows:\n1. Extract a set of word pairs that are probably cognate using a suitable heuristics. In this paper, we treat all word pairs belonging to the same meaning with a length normalized Levenshtein distance (LDN) below 0.5 as probable cognates.3\n3We experimented with LDN cutoffs of 0.25 and 0.75 and found that the results are best for a cutoff of 0.5\n2. Align the list of probable cognates using the vanilla Needleman-Wunsch algorithm.\n3. Extract aligned segment pairs and compute the PMI value for a segment pair using equation 1 and estimating probabilities as relative frequencies.\n4. Generate a new set of aligments using Needleman-Wunsch algorithm and the segment weights learned from step 2. For the gap penalties we used the values proposed in Ja\u0308ger [2013].\n5. We iterate between step 2 and 3 until the average similarity between the two iterations does not change.\nThis procedure yields a PMI based similarity score for each word pair. We convert the similarity score x into a distance score using the sigmoid transformation: 1.0 \u2212 (1 + exp(\u2212x))\u22121 that converts the PMI similarity score into the range of [0, 1]."}, {"heading": "3.2 Pair Hidden Markov Model", "text": "Pair Hidden Markov Model was first proposed in the context of computational biology as a tool for the comparison of DNA or protein sequences [Durbin et al., 2001].\nA Pair Hidden Markov Model (PHMM) uses two output streams, instead of a single output stream; one for each of the two sequences being aligned. In its simplest version, a PHMM consists of five states. A begin state, an end state, a match state (M) that emits pairs of symbols, a deletion state (X) that emits a symbol in the first string and a gap in the second string; and an insertion state (Y) that emits a gap in the first and a symbol in the second string (cf. figure 1).\nThe PHMMs, as used in historical linguistics, differ from its biological counterpart in the following regard:\n\u2022 The historical linguistic PHMM allows a transition between the states X and Y (dashed line, figure 1). An alignment of Italian due and Spanish dos \u2018two\u2019 cannot be generated by a PHMM without the transition between X and Y [Mackay and Kondrak, 2005].\nd u e - d o - s\n\u2022 Another difference between the biological and the linguistic PHMM is the split of the parameter for the transition into the end state. Whilst the original version only has one parameter for this purpose, the linguistic PHMM makes use of two different probabilities \u03c4M and \u03c4XY . This split of parameters enables the model to distinguish between the match state (M) being the final\nemitting state or any of the gap states (X,Y) (see figure 1). This modification preserves the symmetry of the model, while allowing a little bit more freedom.\nThe PHMMs are trained using Baum-Welch expectation maximization algorithm [Durbin et al., 2001]. The best alignment between two sequences x and y is determined by using the Viterbi algorithm.\nThe probability of two sequences x and y of lengths m and n respectively evolving independently under a null model R is given by the following equation 2.\nP (x, y|R) = \u03b92(1\u2212 \u03b9)n+m n\u220f i=1 fxi m\u220f j=1 fyj , (2)\nwith fxi is the equilibrium frequency of the sound at position i in sequence x where, \u03b9 = 1m+n\n2 +1 .\nThe probability of relatedness between x and y is computed as the logarithmic ratio of the probability scores P (x, y|\u00b5) and P (x, y|R), where \u00b5 is the trained model and R is the null model.\nWe employ the same sigmoid transformation, as in PMI, to convert the similarity score (computed under a PHMM) to a distance score."}, {"heading": "4 Online EM", "text": "The Expectation Maximization algorithm (EM) is widely used in computational linguistics for the purpose of word alignment, document classification, and word\nsegmentation. The EM algorithm starts with an initial setting of model parameters and uses that model parameters to realign words in a sentence pair. The model parameters are reestimated using the word alignments obtained from the previous iteration. The EM algorithm reestimates the model parameters after each full scan of the training data.\nLiang and Klein [2009] observe that batch training procedure can lead to slow convergence. As a matter of fact, Ja\u0308ger [2013] trains his PMI system using the standard EM (also known as batch EM) which updates the parameters in a PMI scoring matrix only after aligning all the word pairs. In contrast, Online EM [Liang and Klein, 2009], updates the model parameters after aligning a subset of word pairs (also known as minibatch in online learning literature).\nThe Online EM algorithm combines the parameters estimated (s) from the current update step k with the previous parameters \u03b8k\u22121 using the following equation:\n\u03b8k = (1\u2212 \u03b7k)\u03b8k\u22121 + \u03b7ks (3)\nwhere \u03b7k is defined as: \u03b7k = (k + 2)\u2212\u03b1. In the case of PMI, \u03b8 constitutes the PMI scores for all segment pairs. The parameter \u03b7k determines how fast to forget or remember the updates from the previous steps. The \u03b1 parameter is in the range of 0.5 \u2264 \u03b1 \u2264 1. A smaller \u03b1 implies a large update to the model parameters. The parameter k is related to minibatch parameter (m; m = dD/ke; where, D is the size of training data) and determines the number of updates to be performed. The setting k = 1 recovers the batch EM whereas, when k = D, implies an update for each sample in the training data."}, {"heading": "5 Clustering algorithm", "text": "The InfoMap clustering method is an information theoretic approach to detect community structure within a connected network. The method uses random walks on a network as a proxy for information flow to detect communities, i.e., clusters, without the need for a threshold. A community is a group of nodes with more edges connecting the nodes within the community than connecting them with nodes outside the community [Newman and Girvan, 2004].\nIn our case, a community refers to the words which are cognate and have higher edge weights between them. The idea behind the algorithm is that the random walk is statistically more likely to spend a long period of time within a community than switching communities due to the nature of the network.\nA pair-wise distance matrix is a complete weighted graph and any edge that has a weight < 0.5 and a PMI score < 0 (due to the sigmoid-based distance transformation). Due to the PMI score\u2019s definition, a PMI score< 0 implies that the words might not be cognate. We use this property to construct a non-complete graph and supply the resulting network as an input to the InfoMap algorithm."}, {"heading": "6 Experiments", "text": "In this section, we describe the experimental settings, datasets, evaluation measures, and the comparing systems: Baseline, ALINE, PMI-LANG, and LexStat."}, {"heading": "6.1 Hyperparameters of Online EM", "text": "We determine the best setting of m and \u03b1 parameter by searching for m in the range of m = 2s where s \u2208 [5, 15]; and, \u03b1 \u2208 [0.5, 1.0] with a step size of 0.05. We fix the gap opening and gap extension penalties to \u22122.5 and \u22121.75."}, {"heading": "6.2 Datasets", "text": ""}, {"heading": "6.2.1 Indo-European database", "text": "The Indo-European Lexical database (IELex) was created by Dyen et al. [1992] and curated by Michael Dunn.4 The IELex database is not transcribed in uniform IPA and retains many forms transcribed in the Romanized IPA format of Dyen et al. [1992]. We cleaned the IELex database of any non-IPA-like transcriptions and converted the cleaned subset of the database into ASJP format."}, {"heading": "6.2.2 Austronesian vocabulary database", "text": "The Austronesian Vocabulary Database (ABVD) [Greenhill and Gray, 2009] has word lists for 210 Swadesh concepts and 378 languages.5 The database does not have transcriptions in a uniform IPA format. We removed all symbols that do not appear in the standard IPA and converted the lexical items to ASJP format. For comparison purpose, we use randomly selected 100 languages\u2019 dataset in this paper.6"}, {"heading": "6.2.3 Short word lists with cognacy judgments:", "text": "Wichmann and Holman [2013] and List [2014a] compiled cognacy wordlists for subsets of families from various scholarly sources such as comparative handbooks and historical linguistics\u2019 articles. The details of different databases is given in table 2."}, {"heading": "6.3 Evaluation Measures", "text": "We evaluate the results of clustering analysis using B-cubed F-score [Amigo\u0301 et al., 2009]. The B-cubed scores are defined for each word belonging to a meaning as followed. The precision for a word is defined as the ratio between the number of\n4http://ielex.mpi.nl/ 5http://language.psy.auckland.ac.nz/austronesian/ 6LexStat takes many hours to run on a dataset of 100 languages.\ncognates in its cluster to the total number of words in its cluster. The recall for a word is defined as the ratio between the number of cognates in its cluster to the total number of expert labeled cognates. The B-cubed precision and recall are defined as the average of the words\u2019 precision and recall across all the clusters. Finally, the B-cubed F-score for a meaning, is computed as the harmonic mean of the average items\u2019 precision and recall. The Averaged B-cubed F-score for the whole dataset is computed as the average of the B-cubed F-scores across all the meanings.\nAmigo\u0301 et al. [2009] show that the B-cubed F-score satisfies four formal constraints known as cluster homogeneity, cluster completeness, rag bag (robustness to misplacement of a true singleton item), and robustness to variation in cluster size. The authors show that cluster evaluation measures based on entropy such as Mutual Information and V-measure [Rosenberg and Hirschberg, 2007] and Rand index do not satisfy the four constraints. Both Hauer and Kondrak [2011] and List et al. [2016] use B-cubed F-scores to evaluate their cognate clustering systems."}, {"heading": "6.4 Comparing systems", "text": "Baseline We adopt length normalized Levenshtein distance as the baseline in our experiments."}, {"heading": "6.4.1 ALINE", "text": "ALINE is a sequence alignment system designed by Kondrak [2000] for computing similarity between two words by decomposing phonemes into multivalued and\nbinary phonetic features. Each phoneme is decomposed into multivalued features such as place and manner for consonants; height and backness for vowels. Multivalued features take values on a continous scale ranging from [0, 1] and the values represent the distance between the sources of articulation. Binary valued features consist of nasal, voicing, aspirated, and retroflex.\nEach feature is weighed by a salience value that is determined manually. The similarity score between two sequences is computed as the sum of the aligned sound segments. Following Downey et al. [2008], we convert ALINE\u2019s similarity score sab between two words a, b is converted to a distance score based on the following formula: 1.0\u2212 2.0\u2217sabsaa+sbb . 7"}, {"heading": "6.4.2 PMI-LANG", "text": "Ja\u0308ger [2013] developed a system that learns PMI sound matrices to optimize a criterion designed to optimize language relatedness. The core idea is to tie up word similarity to language similarity such that close languages such as English/German tend to have more similarity than English/Hindi. The language similarity function amounts to maximizing similarity between probable cognates to learn a PMI score matrix. Ja\u0308ger [2013] applies the learned PMI score matrix to infer phylogenetic trees of language families. However, the learned PMI score matrix has not been applied for cognate clustering."}, {"heading": "6.4.3 LexStat", "text": "LexStat [List, 2012] is part of LingPy [List and Forkel, 2016] library offering stateof-the-art alignment algorithms for aligning word pairs and clustering them into cognate sets. We describe the workflow of LexStat system below:\n1. LexStat uses a hand-crafted sound segment matrix, h, to align and score the word pairs for each meaning. Let a segment pair i, j\u2019s similarity be given as hij .\n2. For each language pair, l1, l2 the word pairs belonging to the same meaning are aligned. The frequency of a segment pair i, j belonging to the same meaning is given as aij .\n3. For l1, l2, the words belonging to one of the language is shuffled and realigned using Needleman-Wunsch algorithm. This procedure is repeated for all language pairs for 100 times. The average frequency of a segment pair i, j from the reshuffling step is given as eij .\n7We use the Python implementation provided by Huff and Lonsdale [2011] which is available at https://sourceforge.net/projects/pyaline/.\n4. All the parameters h, a, e are combined according to the following formula to give a new segment similarity score sij where, w1 + w2 = 1.\nsij = 2 \u2217 w1log aij eij + w2hij (4)\n5. The weights sij are then used to score word pairs and cluster words in a meaning.\nThe intuition behind step 3 is to reduce the effect of chance similarities between the sound segments that can obscure real genetic sound correspondences.8 We supply the word distances from all the above systems as input to InfoMap to infer cognate clusters."}, {"heading": "7 Results", "text": "In this section, we present the results of our experiments. We perform two sets of experiments by training with different datasets which are described below."}, {"heading": "7.1 Out-of-family training", "text": "In this experiment, we train our PHMM and PMI systems on wordlists from the ASJP database belonging to families other than those language groups present in table 2. We made sure that there is no overlap between the languages present in test dataset and the training dataset. We extracted a list of probable cognates and trained our PMI and PHMM models on the list of probable cognates. We trained all the\n8We obtained the code from https://github.com/lingpy. We convert the LexStat similarity scores into distance scores using the same formula as ALINE.\nbatch and online systems on 1151178 word pairs. The results of our experiments are given in table 3. We report the InfoMap clustering results for a threshold of 0.5 for all the systems. We expect LexStat to perform better in the case of Chinese since LexStat handles tones internally whereas, the ASJP representation does not handle tones. In the case of online systems, we report the best results for m,\u03b1. Following List [2014b], we do not report LexStat results for the language groups which have word lists shorter than 100 meanings.\nThe Online PMI performs better than the rest of the systems at nine out of the sixteen families. On an average, the Online PMI system ranks the best followed by PMI-LANG and LexStat system. ALINE performs the best on Miao-Yao language group. The Online PMI system perform better than the Batch PMI on all the datasets. As expected, the LexStat system performs the best on Chinese dialect dataset. Surprisingly, despite its complexity the PHMM systems do not perform as well as the simpler PMI systems.\nNow, we will comment on the results of Austronesian and Indo-European language families. Greenhill [2011] applied Levenshtein distance for the classification of Austronesian languages and argued that Levenshtein distance does not perform well at the task of detecting language relationships. Our experiment shows that Levenshtein distance comes close to LexStat in the case of Austronesian language family. Both PMI-LANG and Online PMI are two points better than Levenshtein distance at the task of cognate identification.\nThe results are much clearer in the case of Indo-European language family. The PMI-LANG and Online PMI systems perform better than rest of the systems. Levenshtein distance performs better than LexStat for the Indo-European language\nfamily. On an average, ALINE shows the lowest performance of all the systems. We report the corresponding setting ofm,\u03b1 for all the online systems in table 4. The value of m is quite variable across language families whereas, \u03b1 tends to be in the range of 0.5\u22120.75. We investigate the effect ofm and \u03b1 for Indo-European and Austronesian languages by plotting the results of Online PMI system in figures 2. The B-cubed F-scores are stable across the range of \u03b1 but show variable results for value of m. The top-3 F-scores for Indo-European are at m = 256, 512, 1024 and at m = 64, 128, 256 for Austronesian language family. These results suggest that the online training helps cognate clustering than the batch training. The plots (cf. figure 2) suggest that small batch size improves the performance whereas a large batch size (eg., 32768) hurts the performance on Indo-European and Austronesian language families."}, {"heading": "7.2 Within-family training", "text": "In this experiment, we train our PMI and PHMM systems on three largest language families in our dataset: Mayan, Indo-European, and Austronesian language\nfamilies. We train our systems on word pairs extracted from two different sources.\n1. The ASJP database has 40-length word lists for more languages (\u223c 3 times) than the languages in cognate databases of Mayan, Indo-European, and Austronesian language families. The database allows us to access more word pairs than any other database in existence.\n2. We extract list of probable cognate pairs from the IELex, ABVD, and Mayan language databases.\nThe motivation behind these experiments is to investigate the performance of PMI and PHMM systems when trained on the word lists belonging to the same language family but compiled by different group of annotators. A successful experiment indicates that this approach of training a PMI matrix on ASJP 40 word lists can be applied to language families that have longer word lists but no cognate judgments. The number of training word pairs and the results of our experiments are given in table 5.\nThe Online variants perform better than the batch systems across all the language families and settings. Online PMI performs the best across all the language families than the Batch PMI. Online PMI trained on ASJP word lists of a language family show close performance to an Online PMI system trained within the language family in the case of Indo-European and Austronesian language families. The performance of batch PMI system comes close to the Online PMI system in the case of Indo-European but falls behind in the case of other language families. Training the online system on ASJP word lists improves the performance in the case of Mayan language family. This performance is not observed in the case of Indo-European and Austronesian language families. The reason for this could be due to the source of origin of the datasets.\nThe batch PMI/PHMM systems perform better than LexStat on Indo-European and Mayan language families. The Online PHMM system comes close in performance to Online PMI system in the case of Indo-European and Mayan language families. PHMM systems how the lowest performance on Austronesian language family. Except for Indo-European, the best batch sizes for online PMI system are small and are typically \u2264 256."}, {"heading": "8 Discussion", "text": "In this section, we discuss the effects of various parameters on our results."}, {"heading": "8.1 Effect of m and \u03b1", "text": "Throughout our experiments, we observe that low minibatch size gives better results than a large minibatch size. We also observe that a intermediary value of \u03b1 is usually sufficient for obtaining the best results.\nFigure 2 shows that small values of m yields stable F-scores across the range of \u03b1. Small values of m typically gives better results than larger values of \u03b1. In contrast to other NLP tasks that require large m and smaller \u03b1, the task of aligning two words requires smaller values of m. The small value of m implies large number of updates which is important for a task where the average sequence length (\u223c 5) and the average number of word pairs are in less than 100, 000. Further, an intermediary value of \u03b1 controls the amount of memory retained at each update."}, {"heading": "8.2 Speed", "text": "One advantage of our online systems (either PMI or PHMM) is that the training time is typically in the range of 10 minutes on a single thread of i7-6700 processor. In the case of PHMM, online training speeds up the convergence and yields, typically, better results than the batch variant. In comparison, the PMI-LANG system takes days to train. Finally, our results show that the online algorithm can yield better performance than LexStat. LexStat and PHMM take more than 5 hours to test on the language subset of the Austronesian language family. In contrast, PMI (both online and batch) takes less than 10 minutes for each value of m,\u03b1 in the case of out-of-family training. We also observe that 5 scans over the full data was sufficient for convergence."}, {"heading": "8.3 Analyzing PHMM\u2019s performance", "text": "Although PHMMs are the most complex among the tested models, the performance of these models is not as good as the conceptually simpler PMI models. This lack of performance could be due to the characteristics of the PHMM. The transition probability from the begin state to the match or gap states is the same as the transition probability from the match state to either gap state or itself (figure 1). Although desirable for biological purposes, this poses a big problem for linguistic applications. To start an alignment with a match is more likely than to start with a gap.9 Therefore, the alignments generated by PHHMs are more likely to show gaps at the end of the string than in the beginning. This results in problems for data sets where word length differ a lot. The PHMM performs the worst for those datasets that show a huge difference in the word length. On the other hand, for Kamasau and Tujia \u2013 the two datasets with the best performance \u2013 the difference in word length is much less pronounced (cf. figure 3).\nBased on the results of these experiments, we propose that training the PMIbased segment scores in an online fashion and supplied to InfoMap clustering could yield reliable cognate judgments.\n91\u2212 2\u03b4\u03c4M is larger than \u03b4 in all models (c.f. figure 1)."}, {"heading": "9 Conclusion", "text": "In this paper, we evaluated the performance of various sequence alignment algorithms \u2013 both learned and linguistically designed \u2013 for the task of cognate detection across different language families. We find that training PMI and PHMM in an online fashion speeds up convergence and yields comparable or better results than the batch variant and the state-of-the-art LexStat system. Online PMI system shows the best performance across different language families. In conclusion, PMI systems can be trained faster in an online fashion and yield better accuracies than the current state-of-the-art systems."}], "references": [{"title": "A comparison of extrinsic clustering evaluation metrics based on formal constraints", "author": ["Enrique Amig\u00f3", "Julio Gonzalo", "Javier Artiles", "Felisa Verdejo"], "venue": "Information retrieval,", "citeRegEx": "Amig\u00f3 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amig\u00f3 et al\\.", "year": 2009}, {"title": "Automated reconstruction of ancient languages using probabilistic models of sound change", "author": ["Alexandre Bouchard-C\u00f4t\u00e9", "David Hall", "Thomas L. Griffiths", "Dan Klein"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Bouchard.C\u00f4t\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bouchard.C\u00f4t\u00e9 et al\\.", "year": 2013}, {"title": "Mapping the origins and expansion of the IndoEuropean language family", "author": ["Remco Bouckaert", "Philippe Lemey", "Michael Dunn", "Simon J. Greenhill", "Alexander V. Alekseyenko", "Alexei J. Drummond", "Russell D. Gray", "Marc A. Suchard", "Quentin D. Atkinson"], "venue": null, "citeRegEx": "Bouckaert et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bouckaert et al\\.", "year": 2012}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Kenneth Ward Church", "Patrick Hanks"], "venue": "Computational Linguistics,", "citeRegEx": "Church and Hanks.,? \\Q1990\\E", "shortCiteRegEx": "Church and Hanks.", "year": 1990}, {"title": "Computational feature-sensitive reconstruction of language relationships: Developing the aline distance for comparative historical linguistic reconstruction", "author": ["Sean S Downey", "Brian Hallmark", "Murray P Cox", "Peter Norquest", "J Stephen Lansing"], "venue": "Journal of Quantitative Linguistics,", "citeRegEx": "Downey et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Downey et al\\.", "year": 2008}, {"title": "Biological sequence analysis: probabilistic models of proteins and nucleic acids", "author": ["Richard Durbin", "Sean R Eddy", "Anders Krogh", "Graeme Mitchison"], "venue": null, "citeRegEx": "Durbin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Durbin et al\\.", "year": 2001}, {"title": "An Indo-European classification: A lexicostatistical experiment", "author": ["Isidore Dyen", "Joseph B. Kruskal", "Paul Black"], "venue": "Transactions of the American Philosophical Society,", "citeRegEx": "Dyen et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Dyen et al\\.", "year": 1992}, {"title": "Language-tree divergence times support the anatolian theory of indo-european origin", "author": ["Russell D Gray", "Quentin D Atkinson"], "venue": "Nature, 426(6965):435\u2013439,", "citeRegEx": "Gray and Atkinson.,? \\Q2003\\E", "shortCiteRegEx": "Gray and Atkinson.", "year": 2003}, {"title": "Levenshtein distances fail to identify language relationships accurately", "author": ["Simon J Greenhill"], "venue": "Computational Linguistics,", "citeRegEx": "Greenhill.,? \\Q2011\\E", "shortCiteRegEx": "Greenhill.", "year": 2011}, {"title": "Austronesian language phylogenies: Myths and misconceptions about Bayesian computational methods. Austronesian Historical Linguistics and Culture History: A Festschrift for Robert Blust", "author": ["Simon J. Greenhill", "Russell D. Gray"], "venue": null, "citeRegEx": "Greenhill and Gray.,? \\Q2009\\E", "shortCiteRegEx": "Greenhill and Gray.", "year": 2009}, {"title": "Clustering semantically equivalent words into cognate sets in multilingual lists", "author": ["Bradley Hauer", "Grzegorz Kondrak"], "venue": "In Proceedings of the 5th International Joint Conference on Natural Language Processing,", "citeRegEx": "Hauer and Kondrak.,? \\Q2011\\E", "shortCiteRegEx": "Hauer and Kondrak.", "year": 2011}, {"title": "Positing language relationships using aline", "author": ["Paul Huff", "Deryle Lonsdale"], "venue": "Language Dynamics and Change,", "citeRegEx": "Huff and Lonsdale.,? \\Q2011\\E", "shortCiteRegEx": "Huff and Lonsdale.", "year": 2011}, {"title": "Phylogenetic inference from word lists using weighted alignment with empirically determined weights", "author": ["Gerhard J\u00e4ger"], "venue": "Language Dynamics and Change,", "citeRegEx": "J\u00e4ger.,? \\Q2013\\E", "shortCiteRegEx": "J\u00e4ger.", "year": 2013}, {"title": "Support for linguistic macrofamilies from weighted sequence alignment", "author": ["Gerhard J\u00e4ger"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "J\u00e4ger.,? \\Q2015\\E", "shortCiteRegEx": "J\u00e4ger.", "year": 2015}, {"title": "A new algorithm for the alignment of phonetic sequences. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 288\u2013295", "author": ["Grzegorz Kondrak"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Kondrak.,? \\Q2000\\E", "shortCiteRegEx": "Kondrak.", "year": 2000}, {"title": "A course in phonetics", "author": ["Peter Ladefoged"], "venue": "Hardcourt Brace Jovanovich Inc. NY,", "citeRegEx": "Ladefoged.,? \\Q1975\\E", "shortCiteRegEx": "Ladefoged.", "year": 1975}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "author": ["V.I. Levenshtein"], "venue": "Soviet Physics Doklady,", "citeRegEx": "Levenshtein.,? \\Q1966\\E", "shortCiteRegEx": "Levenshtein.", "year": 1966}, {"title": "Online em for unsupervised models", "author": ["Percy Liang", "Dan Klein"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Liang and Klein.,? \\Q2009\\E", "shortCiteRegEx": "Liang and Klein.", "year": 2009}, {"title": "Lexstat: Automatic detection of cognates in multilingual wordlists", "author": ["Johann-Mattis List"], "venue": "In Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH,", "citeRegEx": "List.,? \\Q2012\\E", "shortCiteRegEx": "List.", "year": 2012}, {"title": "Sequence comparison in historical linguistics. D\u00fcsseldorf University Press, D\u00fcsseldorf, 2014a. URL http://sequencecomparison. github.io", "author": ["Johann-Mattis List"], "venue": null, "citeRegEx": "List.,? \\Q2014\\E", "shortCiteRegEx": "List.", "year": 2014}, {"title": "Investigating the impact of sample size on cognate detection", "author": ["Johann-Mattis List"], "venue": "Journal of Language Relationship,", "citeRegEx": "List.,? \\Q2014\\E", "shortCiteRegEx": "List.", "year": 2014}, {"title": "Lingpy. a python library for historical linguistics, 2016", "author": ["Johann-Mattis List", "Robert Forkel"], "venue": "URL http://lingpy.org", "citeRegEx": "List and Forkel.,? \\Q2016\\E", "shortCiteRegEx": "List and Forkel.", "year": 2016}, {"title": "Using sequence similarity networks to identify partial cognates in multilingual wordlists", "author": ["Johann-Mattis List", "Philippe Lopez", "Eric Bapteste"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "List et al\\.,? \\Q2016\\E", "shortCiteRegEx": "List et al\\.", "year": 2016}, {"title": "Computing word similarity and identifying cognates with pair hidden Markov models", "author": ["Wesley Mackay", "Grzegorz Kondrak"], "venue": "CONLL \u201905,", "citeRegEx": "Mackay and Kondrak.,? \\Q2005\\E", "shortCiteRegEx": "Mackay and Kondrak.", "year": 2005}, {"title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins", "author": ["Saul B. Needleman", "Christian D. Wunsch"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Needleman and Wunsch.,? \\Q1970\\E", "shortCiteRegEx": "Needleman and Wunsch.", "year": 1970}, {"title": "Finding and evaluating community structure in networks", "author": ["Mark EJ Newman", "Michelle Girvan"], "venue": "Phys. Rev. E,", "citeRegEx": "Newman and Girvan.,? \\Q2004\\E", "shortCiteRegEx": "Newman and Girvan.", "year": 2004}, {"title": "Automatic cognate identification with gap-weighted string subsequences", "author": ["Taraka Rama"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.,", "citeRegEx": "Rama.,? \\Q2015\\E", "shortCiteRegEx": "Rama.", "year": 2015}, {"title": "Comparative evaluation of string similarity measures for automatic language classification", "author": ["Taraka Rama", "Lars Borin"], "venue": "Sequences in Language and Text,", "citeRegEx": "Rama and Borin.,? \\Q2015\\E", "shortCiteRegEx": "Rama and Borin.", "year": 2015}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["Andrew Rosenberg", "Julia Hirschberg"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Rosenberg and Hirschberg.,? \\Q2007\\E", "shortCiteRegEx": "Rosenberg and Hirschberg.", "year": 2007}, {"title": "Maps of random walks on complex networks reveal community structure", "author": ["Martin Rosvall", "Carl T. Bergstrom"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Rosvall and Bergstrom.,? \\Q2008\\E", "shortCiteRegEx": "Rosvall and Bergstrom.", "year": 2008}, {"title": "Languages with longer words have more lexical change", "author": ["S\u00f8ren Wichmann", "Eric W Holman"], "venue": "In Approaches to Measuring Linguistic Differences,", "citeRegEx": "Wichmann and Holman.,? \\Q2013\\E", "shortCiteRegEx": "Wichmann and Holman.", "year": 2013}, {"title": "Inducing sound segment differences using Pair Hidden Markov Models. pages 48\u201356", "author": ["Martijn Wieling", "Therese Leinonen", "John Nerbonne"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Wieling et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wieling et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": ", 2013]1 and cognate databases for Austronesian [Greenhill and Gray, 2009] and Indo-European [Bouckaert et al.", "startOffset": 48, "endOffset": 74}, {"referenceID": 2, "context": ", 2013]1 and cognate databases for Austronesian [Greenhill and Gray, 2009] and Indo-European [Bouckaert et al., 2012].", "startOffset": 93, "endOffset": 117}, {"referenceID": 2, "context": ", 2013]1 and cognate databases for Austronesian [Greenhill and Gray, 2009] and Indo-European [Bouckaert et al., 2012]. The availability of word lists (without cognate judgments) has allowed scholars like Rama and Borin [2015] and J\u00e4ger [2015] to experiment with different weighted string similarity measures for the purpose of inferring the family trees of world\u2019s languages, without explicit cognate identification.", "startOffset": 94, "endOffset": 226}, {"referenceID": 2, "context": ", 2013]1 and cognate databases for Austronesian [Greenhill and Gray, 2009] and Indo-European [Bouckaert et al., 2012]. The availability of word lists (without cognate judgments) has allowed scholars like Rama and Borin [2015] and J\u00e4ger [2015] to experiment with different weighted string similarity measures for the purpose of inferring the family trees of world\u2019s languages, without explicit cognate identification.", "startOffset": 94, "endOffset": 243}, {"referenceID": 2, "context": ", 2013]1 and cognate databases for Austronesian [Greenhill and Gray, 2009] and Indo-European [Bouckaert et al., 2012]. The availability of word lists (without cognate judgments) has allowed scholars like Rama and Borin [2015] and J\u00e4ger [2015] to experiment with different weighted string similarity measures for the purpose of inferring the family trees of world\u2019s languages, without explicit cognate identification. On the other hand, List [2012] proposed a cognate clustering system that combines handcrafted weighted string similarity measures and permutation tests for the purpose of automated cognate identification.", "startOffset": 94, "endOffset": 448}, {"referenceID": 2, "context": ", 2013]1 and cognate databases for Austronesian [Greenhill and Gray, 2009] and Indo-European [Bouckaert et al., 2012]. The availability of word lists (without cognate judgments) has allowed scholars like Rama and Borin [2015] and J\u00e4ger [2015] to experiment with different weighted string similarity measures for the purpose of inferring the family trees of world\u2019s languages, without explicit cognate identification. On the other hand, List [2012] proposed a cognate clustering system that combines handcrafted weighted string similarity measures and permutation tests for the purpose of automated cognate identification. In a different approach, Hauer and Kondrak [2011] experimented Known as Automated Similarity Judgment Program (ASJP).", "startOffset": 94, "endOffset": 672}, {"referenceID": 22, "context": "Finally, Rama [2015] use string kernel inspired features for training a SVM linear classifier for pair-wise cognate identification.", "startOffset": 9, "endOffset": 21}, {"referenceID": 8, "context": "As noted by Hauer and Kondrak [2011], availability of a reliable multilingual cognate identification system can be used to supply the cognate judgments as an input to the phylogenetic inference algorithms introduced by Gray and Atkinson [2003] and reconstruction methods of Bouchard-C\u00f4t\u00e9 et al.", "startOffset": 12, "endOffset": 37}, {"referenceID": 6, "context": "As noted by Hauer and Kondrak [2011], availability of a reliable multilingual cognate identification system can be used to supply the cognate judgments as an input to the phylogenetic inference algorithms introduced by Gray and Atkinson [2003] and reconstruction methods of Bouchard-C\u00f4t\u00e9 et al.", "startOffset": 219, "endOffset": 244}, {"referenceID": 1, "context": "As noted by Hauer and Kondrak [2011], availability of a reliable multilingual cognate identification system can be used to supply the cognate judgments as an input to the phylogenetic inference algorithms introduced by Gray and Atkinson [2003] and reconstruction methods of Bouchard-C\u00f4t\u00e9 et al. [2013].2 The phylogenetic inference methods require cognate judgments which are only available for a small number of well-studied language families such as Indo-European and Austronesian.", "startOffset": 274, "endOffset": 302}, {"referenceID": 10, "context": "The similarity between a word pair can be computed using supervised approaches [Hauer and Kondrak, 2011] or by using sequence alignment algorithms such as Needleman-Wunsch [Needleman and Wunsch, 1970] or Levenshtein distance [Levenshtein, 1966].", "startOffset": 79, "endOffset": 104}, {"referenceID": 24, "context": "The similarity between a word pair can be computed using supervised approaches [Hauer and Kondrak, 2011] or by using sequence alignment algorithms such as Needleman-Wunsch [Needleman and Wunsch, 1970] or Levenshtein distance [Levenshtein, 1966].", "startOffset": 172, "endOffset": 200}, {"referenceID": 16, "context": "The similarity between a word pair can be computed using supervised approaches [Hauer and Kondrak, 2011] or by using sequence alignment algorithms such as Needleman-Wunsch [Needleman and Wunsch, 1970] or Levenshtein distance [Levenshtein, 1966].", "startOffset": 225, "endOffset": 244}, {"referenceID": 23, "context": "[2007] compared Pair Hidden Markov Model (PHMM) [Mackay and Kondrak, 2005] and pointwise mutual information (PMI) [Church and Hanks, 1990] weighted Levenshtein distance for Dutch dialect comparison.", "startOffset": 48, "endOffset": 74}, {"referenceID": 3, "context": "[2007] compared Pair Hidden Markov Model (PHMM) [Mackay and Kondrak, 2005] and pointwise mutual information (PMI) [Church and Hanks, 1990] weighted Levenshtein distance for Dutch dialect comparison.", "startOffset": 114, "endOffset": 138}, {"referenceID": 8, "context": "The similarity between a word pair can be computed using supervised approaches [Hauer and Kondrak, 2011] or by using sequence alignment algorithms such as Needleman-Wunsch [Needleman and Wunsch, 1970] or Levenshtein distance [Levenshtein, 1966]. In dialectometry, Wieling et al. [2007] compared Pair Hidden Markov Model (PHMM) [Mackay and Kondrak, 2005] and pointwise mutual information (PMI) [Church and Hanks, 1990] weighted Levenshtein distance for Dutch dialect comparison.", "startOffset": 80, "endOffset": 286}, {"referenceID": 2, "context": "[2007] compared Pair Hidden Markov Model (PHMM) [Mackay and Kondrak, 2005] and pointwise mutual information (PMI) [Church and Hanks, 1990] weighted Levenshtein distance for Dutch dialect comparison. In historical linguistics, J\u00e4ger [2013] developed a PMI based method for computing the string similarity using the ASJP database.", "startOffset": 115, "endOffset": 239}, {"referenceID": 1, "context": "The cognate clustering system in Bouchard-C\u00f4t\u00e9 et al. [2013] requires the tree structure of the language family to be know beforehand.", "startOffset": 33, "endOffset": 61}, {"referenceID": 18, "context": "Our results show that online training can perform better than a linguistically well-informed system known as LexStat [List, 2012].", "startOffset": 117, "endOffset": 129}, {"referenceID": 12, "context": "Also, the online algorithms allow our systems to be trained in few minutes and give similar accuracies as the batch trained systems of J\u00e4ger [2013]. The paper is organized as follows.", "startOffset": 135, "endOffset": 148}, {"referenceID": 29, "context": "[2016] explore the use of InfoMap [Rosvall and Bergstrom, 2008] for the detection of partial cognates in subgroups of Sino-Tibetan language family.", "startOffset": 34, "endOffset": 63}, {"referenceID": 12, "context": "Moreover, the PMI-LANG [J\u00e4ger, 2013], has not been evaluated at the task of unsupervised cognate clustering.", "startOffset": 23, "endOffset": 36}, {"referenceID": 10, "context": "Hauer and Kondrak [2011] trained a linear SVM on word similarity features and use the SVM model to assign a similarity score to the word pair.", "startOffset": 0, "endOffset": 25}, {"referenceID": 10, "context": "Hauer and Kondrak [2011] trained a linear SVM on word similarity features and use the SVM model to assign a similarity score to the word pair. For each meaning, a word pair distance matrix is computed and supplied to the average linkage clustering algorithm for inferring cognate clusters. The authors observe that the SVM trained system performs better than a baseline that judges the similarity of two words based on the identity of the first two consonants. List [2012] introduced a system known as LexStat (described in section 6) that is sensitive to segment similarities and chance similarities due to borrowing or semantic shift.", "startOffset": 0, "endOffset": 473}, {"referenceID": 10, "context": "Hauer and Kondrak [2011] trained a linear SVM on word similarity features and use the SVM model to assign a similarity score to the word pair. For each meaning, a word pair distance matrix is computed and supplied to the average linkage clustering algorithm for inferring cognate clusters. The authors observe that the SVM trained system performs better than a baseline that judges the similarity of two words based on the identity of the first two consonants. List [2012] introduced a system known as LexStat (described in section 6) that is sensitive to segment similarities and chance similarities due to borrowing or semantic shift. The author tests this system on a number of small-sized (consisting of less than 20 languages) datasets for the purpose of cognate identification and reports that the system performs better than Levenshtein distance. In a recent paper, List et al. [2016] explore the use of InfoMap [Rosvall and Bergstrom, 2008] for the detection of partial cognates in subgroups of Sino-Tibetan language family.", "startOffset": 0, "endOffset": 892}, {"referenceID": 10, "context": "Hauer and Kondrak [2011] trained a linear SVM on word similarity features and use the SVM model to assign a similarity score to the word pair. For each meaning, a word pair distance matrix is computed and supplied to the average linkage clustering algorithm for inferring cognate clusters. The authors observe that the SVM trained system performs better than a baseline that judges the similarity of two words based on the identity of the first two consonants. List [2012] introduced a system known as LexStat (described in section 6) that is sensitive to segment similarities and chance similarities due to borrowing or semantic shift. The author tests this system on a number of small-sized (consisting of less than 20 languages) datasets for the purpose of cognate identification and reports that the system performs better than Levenshtein distance. In a recent paper, List et al. [2016] explore the use of InfoMap [Rosvall and Bergstrom, 2008] for the detection of partial cognates in subgroups of Sino-Tibetan language family. The authors compare the performance of average linkage clustering against InfoMap and find that InfoMap performs better than average linkage clustering. The above listed works test similar datasets using different experimental settings. For instance, Hauer and Kondrak [2011] trained and tested on a subset of language families that were provided by Wichmann and Holman [2013].", "startOffset": 0, "endOffset": 1309}, {"referenceID": 10, "context": "Hauer and Kondrak [2011] trained a linear SVM on word similarity features and use the SVM model to assign a similarity score to the word pair. For each meaning, a word pair distance matrix is computed and supplied to the average linkage clustering algorithm for inferring cognate clusters. The authors observe that the SVM trained system performs better than a baseline that judges the similarity of two words based on the identity of the first two consonants. List [2012] introduced a system known as LexStat (described in section 6) that is sensitive to segment similarities and chance similarities due to borrowing or semantic shift. The author tests this system on a number of small-sized (consisting of less than 20 languages) datasets for the purpose of cognate identification and reports that the system performs better than Levenshtein distance. In a recent paper, List et al. [2016] explore the use of InfoMap [Rosvall and Bergstrom, 2008] for the detection of partial cognates in subgroups of Sino-Tibetan language family. The authors compare the performance of average linkage clustering against InfoMap and find that InfoMap performs better than average linkage clustering. The above listed works test similar datasets using different experimental settings. For instance, Hauer and Kondrak [2011] trained and tested on a subset of language families that were provided by Wichmann and Holman [2013]. At the same time, to the best of our knowledge, the LexStat system has not been evaluated on all the available language families.", "startOffset": 0, "endOffset": 1410}, {"referenceID": 12, "context": "A second parameter known as gap extension penalty has lesser or equal penalty than the gap opening parameter and models the fact that deletions occur in chunks J\u00e4ger [2013]. VNW is not sensitive to segment pairs, but a realistic algorithm should assign higher similarity score to sound correspondences such as /l/ \u223c /r/ than the sound correspondences /p/ \u223c /r/.", "startOffset": 160, "endOffset": 173}, {"referenceID": 12, "context": "We estimated PMI scores from raw data, largely following the method described in J\u00e4ger [2013]. The whole training procedure can be described as follows: 1.", "startOffset": 81, "endOffset": 94}, {"referenceID": 12, "context": "For the gap penalties we used the values proposed in J\u00e4ger [2013]. 5.", "startOffset": 53, "endOffset": 66}, {"referenceID": 5, "context": "2 Pair Hidden Markov Model Pair Hidden Markov Model was first proposed in the context of computational biology as a tool for the comparison of DNA or protein sequences [Durbin et al., 2001].", "startOffset": 168, "endOffset": 189}, {"referenceID": 23, "context": "An alignment of Italian due and Spanish dos \u2018two\u2019 cannot be generated by a PHMM without the transition between X and Y [Mackay and Kondrak, 2005].", "startOffset": 119, "endOffset": 145}, {"referenceID": 23, "context": "Figure 1: Pair Hidden Markov model as proposed by Mackay & Kondrak [Mackay and Kondrak, 2005].", "startOffset": 67, "endOffset": 93}, {"referenceID": 5, "context": "The PHMMs are trained using Baum-Welch expectation maximization algorithm [Durbin et al., 2001].", "startOffset": 74, "endOffset": 95}, {"referenceID": 17, "context": "In contrast, Online EM [Liang and Klein, 2009], updates the model parameters after aligning a subset of word pairs (also known as minibatch in online learning literature).", "startOffset": 23, "endOffset": 46}, {"referenceID": 15, "context": "Liang and Klein [2009] observe that batch training procedure can lead to slow convergence.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "As a matter of fact, J\u00e4ger [2013] trains his PMI system using the standard EM (also known as batch EM) which updates the parameters in a PMI scoring matrix only after aligning all the word pairs.", "startOffset": 21, "endOffset": 34}, {"referenceID": 25, "context": "A community is a group of nodes with more edges connecting the nodes within the community than connecting them with nodes outside the community [Newman and Girvan, 2004].", "startOffset": 144, "endOffset": 169}, {"referenceID": 6, "context": "1 Indo-European database The Indo-European Lexical database (IELex) was created by Dyen et al. [1992] and curated by Michael Dunn.", "startOffset": 83, "endOffset": 102}, {"referenceID": 6, "context": "1 Indo-European database The Indo-European Lexical database (IELex) was created by Dyen et al. [1992] and curated by Michael Dunn.4 The IELex database is not transcribed in uniform IPA and retains many forms transcribed in the Romanized IPA format of Dyen et al. [1992]. We cleaned the IELex database of any non-IPA-like transcriptions and converted the cleaned subset of the database into ASJP format.", "startOffset": 83, "endOffset": 270}, {"referenceID": 9, "context": "2 Austronesian vocabulary database The Austronesian Vocabulary Database (ABVD) [Greenhill and Gray, 2009] has word lists for 210 Swadesh concepts and 378 languages.", "startOffset": 79, "endOffset": 105}, {"referenceID": 27, "context": "3 Short word lists with cognacy judgments: Wichmann and Holman [2013] and List [2014a] compiled cognacy wordlists for subsets of families from various scholarly sources such as comparative handbooks and historical linguistics\u2019 articles.", "startOffset": 43, "endOffset": 70}, {"referenceID": 18, "context": "3 Short word lists with cognacy judgments: Wichmann and Holman [2013] and List [2014a] compiled cognacy wordlists for subsets of families from various scholarly sources such as comparative handbooks and historical linguistics\u2019 articles.", "startOffset": 74, "endOffset": 87}, {"referenceID": 0, "context": "3 Evaluation Measures We evaluate the results of clustering analysis using B-cubed F-score [Amig\u00f3 et al., 2009].", "startOffset": 91, "endOffset": 111}, {"referenceID": 28, "context": "The authors show that cluster evaluation measures based on entropy such as Mutual Information and V-measure [Rosenberg and Hirschberg, 2007] and Rand index do not satisfy the four constraints.", "startOffset": 108, "endOffset": 140}, {"referenceID": 0, "context": "Amig\u00f3 et al. [2009] show that the B-cubed F-score satisfies four formal constraints known as cluster homogeneity, cluster completeness, rag bag (robustness to misplacement of a true singleton item), and robustness to variation in cluster size.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Amig\u00f3 et al. [2009] show that the B-cubed F-score satisfies four formal constraints known as cluster homogeneity, cluster completeness, rag bag (robustness to misplacement of a true singleton item), and robustness to variation in cluster size. The authors show that cluster evaluation measures based on entropy such as Mutual Information and V-measure [Rosenberg and Hirschberg, 2007] and Rand index do not satisfy the four constraints. Both Hauer and Kondrak [2011] and List et al.", "startOffset": 0, "endOffset": 467}, {"referenceID": 0, "context": "Amig\u00f3 et al. [2009] show that the B-cubed F-score satisfies four formal constraints known as cluster homogeneity, cluster completeness, rag bag (robustness to misplacement of a true singleton item), and robustness to variation in cluster size. The authors show that cluster evaluation measures based on entropy such as Mutual Information and V-measure [Rosenberg and Hirschberg, 2007] and Rand index do not satisfy the four constraints. Both Hauer and Kondrak [2011] and List et al. [2016] use B-cubed F-scores to evaluate their cognate clustering systems.", "startOffset": 0, "endOffset": 490}, {"referenceID": 14, "context": "1 ALINE ALINE is a sequence alignment system designed by Kondrak [2000] for computing similarity between two words by decomposing phonemes into multivalued and", "startOffset": 57, "endOffset": 72}, {"referenceID": 4, "context": "Following Downey et al. [2008], we convert ALINE\u2019s similarity score sab between two words a, b is converted to a distance score based on the following formula: 1.", "startOffset": 10, "endOffset": 31}, {"referenceID": 12, "context": "2 PMI-LANG J\u00e4ger [2013] developed a system that learns PMI sound matrices to optimize a criterion designed to optimize language relatedness.", "startOffset": 11, "endOffset": 24}, {"referenceID": 12, "context": "2 PMI-LANG J\u00e4ger [2013] developed a system that learns PMI sound matrices to optimize a criterion designed to optimize language relatedness. The core idea is to tie up word similarity to language similarity such that close languages such as English/German tend to have more similarity than English/Hindi. The language similarity function amounts to maximizing similarity between probable cognates to learn a PMI score matrix. J\u00e4ger [2013] applies the learned PMI score matrix to infer phylogenetic trees of language families.", "startOffset": 11, "endOffset": 439}, {"referenceID": 18, "context": "3 LexStat LexStat [List, 2012] is part of LingPy [List and Forkel, 2016] library offering stateof-the-art alignment algorithms for aligning word pairs and clustering them into cognate sets.", "startOffset": 18, "endOffset": 30}, {"referenceID": 21, "context": "3 LexStat LexStat [List, 2012] is part of LingPy [List and Forkel, 2016] library offering stateof-the-art alignment algorithms for aligning word pairs and clustering them into cognate sets.", "startOffset": 49, "endOffset": 72}, {"referenceID": 11, "context": "We use the Python implementation provided by Huff and Lonsdale [2011] which is available at https://sourceforge.", "startOffset": 45, "endOffset": 70}, {"referenceID": 18, "context": "Following List [2014b], we do not report LexStat results for the language groups which have word lists shorter than 100 meanings.", "startOffset": 10, "endOffset": 23}, {"referenceID": 8, "context": "Greenhill [2011] applied Levenshtein distance for the classification of Austronesian languages and argued that Levenshtein distance does not perform well at the task of detecting language relationships.", "startOffset": 0, "endOffset": 17}], "year": 2017, "abstractText": "In this paper we explore the use of unsupervised methods for detecting cognates in multilingual word lists. We use online EM to train sound segment similarity weights for computing similarity between two words. We tested our online systems on geographically spread sixteen different language groups of the world and show that the Online PMI system (Pointwise Mutual Information) outperforms a HMM based system and two linguistically motivated systems: LexStat and ALINE. Our results suggest that a PMI system trained in an online fashion can be used by historical linguists for fast and accurate identification of cognates in not so well-studied language families.", "creator": "LaTeX with hyperref package"}}}