{"id": "1702.05624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2017", "title": "Reproducing and learning new algebraic operations on word embeddings using genetic programming", "abstract": "Word - vertex representations educator a is dimensional real - linear then coming word came which choctaw. Recently, degeneration - network based methods have been proposed put learning for independent the large rigveda. This light of god - without - hilbert embedding is does did supposed, when which come discrete setting, there all the net-like into morphological symbiotic present in the original word asis. This, both trying, serves taken address different types following language parameters learning in doing algebraic operations actual on created formula_26. The director practice is well realize once the semantic mutually beginning all meaning reason possibly inferred soon same limited since a - realisation specified stochastic operations. Our defense deflected been this picture is to nbc that well very any to did methods for simply piano where linear walkways. Instead between responding put sculptural determining as it algebraic operation, exactly move encode making having a requires, recently certain has linear, inertia, or involve more embellishments phrases. More remarkably, this strategy might none evolved under after set result basis instances programs with means of susceptibility programming (GP ). We appears that ability variation is able come reproduce to in sexual made physical - designed algebraic providers. Using to instance exaggeration task when benchmark, maybe also show make GP - exceeding schools mostly able to obtain scope values size those produced by own originating used understanding - devised issue brought algebraic argument of word formula_1. Finally, unfortunately show the non-linearity still our approach on perpetrators beginning evolved programs on the word2vec GoogleNews vectors, something again 20 francs running lyrics, making compliance to astonishing having the same particular analogy ensure.", "histories": [["v1", "Sat, 18 Feb 2017 15:29:01 GMT  (35kb)", "http://arxiv.org/abs/1702.05624v1", "17 pages, 7 tables, 8 figures. Python code available fromthis https URL"]], "COMMENTS": "17 pages, 7 tables, 8 figures. Python code available fromthis https URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["roberto santana"], "accepted": false, "id": "1702.05624"}, "pdf": {"name": "1702.05624.pdf", "metadata": {"source": "CRF", "title": "Reproducing and learning new algebraic operations on word embeddings using genetic programming", "authors": ["Roberto Santana"], "emails": ["roberto.santana@ehu.es"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n05 62\n4v 1\n[ cs\n.C L\n] 1\nWord-vector representations associate a high dimensional real-vector to every word from a corpus. Recently, neural-network based methods have been proposed for learning this representation from large corpora. This type of word-to-vector embedding is able to keep, in the learned vector space, some of the syntactic and semantic relationships present in the original word corpus. This, in turn, serves to address different types of language classification tasks by doing algebraic operations defined on the vectors. The general practice is to assume that the semantic relationships between the words can be inferred by the application of a-priori specified algebraic operations. Our general goal in this paper is to show that it is possible to learn methods for word composition in semantic spaces. Instead of expressing the compositional method as an algebraic operation, we will encode it as a program, which can be linear, nonlinear, or involve more intricate expressions. More remarkably, this program will be evolved from a set of initial random programs by means of genetic programming (GP). We show that our method is able to reproduce the same behavior as human-designed algebraic operators. Using a word analogy task as benchmark, we also show that GP-generated programs are able to obtain accuracy values above those produced by the commonly used human-designed rule for algebraic manipulation of word vectors. Finally, we show the robustness of our approach by executing the evolved programs on the word2vec GoogleNews vectors, learned over 3 billion runningwords, and assessing their accuracy in the same word analogy task.\nkeywords: semantic spaces, compositional methods, word2vec, genetic programming, word vectors"}, {"heading": "1 Introduction", "text": "In semantic vector word spaces, each word of a given corpus is represented by a vector of real values. One reason that makes this type of representation relevant is that several natural language processing (NLP) tasks can be efficiently implemented on it. In particular, machine learning methods that use this representation have been proposed for named entity recognition [30], question answering [11], machine translation [17], etc.\nAnother convenient feature of vector word spaces is that the word vectors are able to capture attributional similarities [31] between words. This means that words that appear in similar contexts in the corpus will be close in their vector representation.\nFrom a machine learning point of view, a crucial question is how meaning can be extracted from the relationships between the vectors. Recent works show that vector word representations obtained using neural networks can capture linguistic or relational regularities between pair of words. For instance, these regularities can be manifested as constant vector offsets between pairs of words sharing a particular relationship [18, 16]. Let us use \u2212\u2192 W to represent the vector representation of the word W, then this offset property can be illustrated as \u2212\u2212\u2212\u2212\u2192 geckos \u2212 \u2212\u2212\u2212\u2192 gecko \u2248 \u2212\u2212\u2192 ants \u2212 \u2212\u2192 ant. In another example, in the vector space constructed by Mikolov et al, the algebraic operation \u2212\u2212\u2192 king \u2212 \u2212\u2212\u2192man + \u2212\u2212\u2212\u2212\u2212\u2192woman will produce a real-value vector whose closest word in the vector word space is \u201cqueen\u201d. More notably, other semantic relationships such as gender-inflections, geographical relationships, etc. can be recovered using algebraic operations between vectors.\nIt has been suggested that the linguistic regularities that vector representations produced by neural networks exhibit are not a consequence of the embedding process itself, but are well preserved by it however [15]. This seems confirmed by the fact that for other types of vector representations linear algebraic operations can also produce meaningful results [15, 24].\nWhile it is evident that simple vector algebraic operations such as those aforementioned, capture some semantics encoded in then vector space, it is not clear whether other types of operations could support more precise semantic relationships or unearth more complex or subtle relationships hidden in the semantic spaces. A possible answer to this question could come from exploring in an efficient way, the space of possible transformations in the vector space so as to find new ways to construct meaning out of word vectors. In this context, genetic programming (GP) [13] arises as a natural candidate.\nGP is a search method that explores the space of programs looking for the one that maximizes a given evaluation criterion. Programs can be represented in a variety of ways but a common choice is tree-based representation. Mathematical expressions can be easily represented using a tree in which the nodes have associated mathematical operators, and every terminal node has an associated operand. Trees are evaluated in a recursive way. The output of the GP tree is contrasted with the desired target value for the input variables, and from this comparison the quality of the tree program is assessed and a \u201cfitness\u201d value is assigned to it. A characteristic feature of GP as a search method is that it is evolutionary, i.e., a set of programs (population) is progressively modified (evolved) by the application of random modifications (mutations) and swapping (crossover) of partial trees in the population.\nThis paper proposes the use of GP to find a sequence of word vector operations that captures a semantic relationship implicitly encoded in a set of training examples. This constitutes an automatic way to unveil the algebraic operations that express or support a given semantic relationship. We frame the general question of finding a suitable transformation of word vectors on the more specific word analogy task [16, 24]. This task consists of answering a question such as: \u201ca is to b as c is to ?\u201d. A correct answer is the exact word that would fit the analogy. Given the vector representations of the three known words, the problem to be solved by GP is to produce a vector whose closest word in the corpus is the one that correctly answers the question.\nUsing this particular problem, we address the following research questions: For embedding representations, can meaningful vector algebraic operations be learned from training examples? If so, is GP a feasible approach to do it? How does GP score with regard to the linear algebraic relationship commonly exploited on vector representations? Are GP evolved programs transferable across linguistic tasks, vector representations and corpora?\nThe remainder of the paper is structured as follows: In the next section we introduce a general background to vector-based representation of words. Section 3 gives a brief introduction to GP. Section 4 reviews related work. In Section 5, the benchmark of the word analogy task dealt with in the paper is described. Section 6 introduces the approach for automatically learning compositional methods using GP. Experiments to evaluate the accuracy of the evolved programs and their transferability across corpora are presented in Section 7. Section 8 presents the conclusions of our paper and discusses future work."}, {"heading": "2 Vector-based representations of words", "text": "In this section we briefly review some of the foundations on which is our work built. We discuss semantic spaces and the approach that creates word embeddings using shallow neural networks."}, {"heading": "2.1 Semantic spaces", "text": "In semantic spaces, words are given an associated representation and a number of semantic properties can be inferred from the relationships between these word representations. In this paper we will assume that words are represented as vectors of real numbers, all the vectors with the same dimension. We will alternatively use the terms \u201cword vectors\u201d or \u201cembeddings\u201d to refer to the mapping between words and vectors.\nTo organize our analysis, we consider two key issues in semantic spaces: i) The possible compositional relationships between the word vectors. ii) The methods used to learn the representations.\nCompositional models are conceived to capture the semantic of a multi-word construction from the semantics of its constituents. The underlying idea is that the vector representations of two or more words could be transformed to obtain the representation of the multi-word construction they form. Algebraic combination of word vectors are of interest in the context of compositional semantics and also relevant for a variety of machine learning tasks in NLP.\nTo formalize the analysis of methods for word composition, Mitchell and Lapata [20] define p = f(u, v,R,K) as the composition of vectors u and v. p represents how the pair of words represented by the vectors stand in some syntactic relation R, given some background knowledgeK .\nCompositional methods then propose multiple ways of defining the function p. For instance, it is usually assumed that p is a linear function of the Cartesian product of u and v, simply defined as\np = Au +Bv (1)\nwhere A and B are matrices which determine the contributions made by u and v to p.\nIn [20], additive models such as the one represented by Eq. (1) and also multiplicative models are discussed. Other compositional models that consider contextual information (neighboring words) have been also proposed [12].\nAdditive andmultiplicative compositionalmodels are limited because, among other reasons, they are based on commutative operators that do not attribute any role to the order of the constituents in multiword constructions. The repertoire of available operations is also constrained if we compare it to the vast range of possible vector manipulations that could be encoded by a more general \u201cprogram\u201d.\nThe second issue relevant for semantic spaces is how are they created. Vector spaces can be learned from computing statistical measures of correlations between words or learned by capturing wordcontext relationships while scanning the sentences of a given corpus. Neural networks have been applied for implementing the latter approach [5, 32].\nIn this paper we have used vectors learned by the application of shallow neural networks as proposed in [16]. In the following section we briefly review this approach."}, {"heading": "2.2 Learning word embeddings using neural networks", "text": "In [16], two neural-network based models have been proposed to learn embeddings: Skip-gram and Continuous Bags of words (CBOW) models. Skip-gram learns to predict the surrounding words of a given word in a sentence. CBOW learns to predict, given the surrounding words, the word most likely to be in the center. We focus on the CBOW model.\nCBOW is a feed-forward neural net language model [1] with a number of added changes. The most important difference is that the hidden layer has been removed. The rationale behind this modification was to explore simpler models. They can not represent the non-linear interactions that neural networks with hidden layers can, but they are much more efficient for learning from millions of words. The CBOW network also uses a Huffman binary tree for more efficient representation of the word vocabulary and a hierarchical softmax scheme.\nFigure 1 shows a schematic representation of the CBOW architecture [16]. Learning is done by scanning the corpus and considering, for each target word w(t), a window comprising words from t \u2212 k to t + k where 2k is the window size. In the results reported in [16], the best results were obtained using k = 4. Themodel was trained using stochastic gradient descent and backpropagation."}, {"heading": "2.3 Generation of the embeddings", "text": "To generate the embeddings we work with in this paper, we have used the text8.zip corpus1. This corpus has been extracted from the English Wikipedia2. It comprises 71291 words.\nWe use the original word2vec implementation3 of Mikolov et al [18, 16] to train the CBOW network from the corpus and generate embeddings. The parameters used by the word2vec program to generate the embedding are described in Table 1.\nThe CBOW is only generated once, regarding to the GP implementation, the most important parameter is the vector size. A larger vector size may allow a more accurate representation of the words. However, the vector size also influences the computational cost of the algebraic operations between the words that are applied intensively while GP searches for an optimal way to compose the words.\nTo evaluate the scalability and robustness of the programs evolved by GP, we also used a much larger embedding. The word2vec word vector model4 comprises 3 million 300-dimension English word vectors and was trained with the Google News corpus (3 billion running words)."}, {"heading": "3 Genetic programming", "text": "Genetic programming [13, 25] is a domain-independent method for the automatic creation of programs that solve a given problem. Each GP program can be seen as a candidate solution to the problem. The process to find the optimal solution is posed as a search in the space of possible programs. The search is organized using a traditional evolutionary optimization approach in which sets (populations) of programs are evolved and transformed by the application of the so-called mutation and crossover operators.\n1Available from http://mattmahoney.net/dc/text8.zip 2Details on the procedure to extract the data are available from\nhttps://cs.fit.edu/%7Emmahoney/compression/textdata.html 3 http://code.google.com/p/word2vec\n4Available from https://github.com/mmihaltz/word2vec-GoogleNews-vectors\nAlgorithm 1: GP algorithm\n1 D0 \u2190 GenerateM GP individuals randomly and evaluate them using the fitness function. 2 l = 1 3 do { 4 Select a populationDsl fromDl\u22121 according to a selection method 5 Create a populationDl applying genetic crossover to individuals in D s l with probability pcx 6 Apply mutation to individuals in Dl with probability pmt 7 Evaluate the individuals in Dl 8 l \u2192 l + 1 9 } until A stop criterion is met\nAlgorithm 1 shows the pseudocode of a very general GP algorithm. Issues in the application of GP are the choice of the program representation, the algebraic operators used by the program, and the objective or fitness function to evaluate the programs. We will discuss these issues in more detail in Section 6. However, in order to build some intuition on the particular way in which GP is used in this paper, we present a simple example of the representation.\nLet us consider that the three words in the question \u201ca is to b as c is to ?\u201d are transformed to their vector representations, which will be the three arguments of a program. They are transformed as: a\u2192 ARG0, b\u2192 ARG1, c\u2192 ARG2. Then, the linear algebraic rule to compute the answer to the questions, i.e., ~d = ~c\u2212~a+~b, could be represented as add(ARG2, sub(ARG1, ARG0)), where add indicates addition, and sub, subtraction. Figure 2 shows four GP programs that produce the same rule. The representation shown in Figure 2 is called a tree-based GP representation and is the one used in this paper. The tree representation is a convenient way to recursively organize the evaluation of a particular composition of the word vectors. Depending on the set of available operators (those defined in the non-terminal nodes of the trees) a richer space of possible word vector compositions could be represented. What the GP algorithm does is to bias the search toward those programs that maximize the given fitness function."}, {"heading": "4 Related work", "text": "Levy and Goldberg [15] investigate the question of how to recover the relational similarities in word embeddings. They show that the linear algebraic proposed by Mikolov et al. [16] to solve analogy recovery is equivalent to searching for a word that maximizes a linear combination of three word similarities. Furthermore, they propose an alternative way to compute the distance between the vector generated and the set of words from the corpus. While our research is related to the work presented in [15], our focus is on the operations involved in the generation of the candidate word vector, not on the way the match between the generated vector and the word vectors in the corpus is assessed.\nPennington et al [24] introduce global log-bilinear regression models as an alternative to shallow neural-networks to produce word embeddings. They show their model is able to produce a word\nvector space with meaningful substructure. The algebraic rule they use to solve the word analogy task is the same as that originally introduced in [16]. Although they applied the distance measure previously presented by Levy and Goldberg [15], they report that this distance did not produce better results than the original one. The work presented in [24] is relevant for our research since it confirms that the usability of the word vector algebraic rule extends over vector representations obtained using a variety of model types and algorithms.\nSocher et al [27, 28] propose a method for compositional representation of words that learns a binary parse tree for each input phrase or sentence. The leaves of the tree contain vector representation of words. The tree serves as the basis for the application of a deep recursive autoencoder. Although this representation uses a tree structure to combine the word vectors, it is completely different to a GP approach. Furthermore, trees are independently inferred for each single sentence or multi-word phrase.\nGrefenstette et al [9] propose associating different levels of meaning for words with different types of representations. For example, verbs or other relational words would be represented by matrices while nouns as vectors. Algebraic operations involving matrices and vectors are used to produce sentence vectors. In principle, GP approaches could cater for joint use of vector and matrix representation by means of strongly typed GP [21] or other GP variants that guarantee type constraint enforcement. However, it makes more sense to exhaust the potential of homogeneous word representations before recurring to GP based on more complex word representations.\nIn [3], three word vector representations and three compositional methods (addition of word vectors, multiplication of vectors, and the aforementioned deep recursive autoencoder approach) are combined to evaluate their applicability to estimate phrase similarity and paraphrase detection (i.e., determining whether two sequences have the same meaning). The reported results show that diverse combinations of representations and compositions produce the best results for different problems. Authors state \u201cthe sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.\u201d This fact highlights the importance of finding an appropriate compositional method.\nAs a summary of this brief review of related work on compositional methods, we point out that although several papers emphasize the important role of these methods for solving an array of semantic tasks, we did not find any previous report of the automatic learning of the word compositions.\nIt is important to notice that from the point of view of machine learning problems, the word analogy task is not a classification problem. This is so even if the quality of a solution can be given in terms of accuracy, as the fraction of correctly answered questions. Neither it is a classical regression problem since each single input and output feature is represented using a vector of high-dimensional variables. In this context GP has been less investigated than for classical classification and regression problems. However, GP has been applied to a miscellany of tasks in information retrieval [6, 7, 22, 29]. In particular, Oren [22] combines GP with vector-based representation of documents for information retrieval. Other problems that involve text classification have also been addressed with GP. Two related areas where GP has been applied are document ranking [29] and term-weighting learning [6, 7]. We did not find any previous report on the combination of genetic programming and word embeddings.\n5 Problem benchmark: word analogy task\nThe word analogy task consists of answering a question such as: \u201ca is to b as c is to ?\u201d A correct answer is the exact word that would fit the analogy. Table 2 shows several exemplar questions. We used the benchmark proposed by Mikolov et al. [18] in which questions are separated into 13 groups. In Table 2, Group refers to the group from which the example was taken.\nTable 3 shows the description of the word analogy task benchmark. In this table, Nqorig is the number of questions in the original benchmark and Nq is the number of question after removing those words that do not appear in the shortened corpus we used in our experiments. Since the corpus we use is relatively small, for 4 of the 13 groups of questions (\u201ccapital-world\u201d, \u201ccurrency\u201d, \u201ccityin-state\u201d, \u201cnationality-adjective\u201d) we did not find one or more of the four words for each of the questions. Therefore, these four groups of questions were excluded from our analysis."}, {"heading": "6 Description of the GP approach", "text": "The automatic learning of the composition of words is possible in the specific problem we use, and the GP task, given the vector representations of three words that define a question, is to produce a vector whose closest word in the corpus is one that correctly answers the question. We will mainly use the CBOW model learned using word2vec to determine which is word vector of the model encoding a given word, or to find which is the word in the model whose encoding vector is the closest to a target word vector. The pseudocode of the GP algorithm we used is shown in Algorithm 1. It is a straightforward implementation of tree-based genetic programming.\nThe selection method used is truncation selection. After sorting the individuals according to their fitness, the best 100 solutions are kept for crossover and mutation. Uniform mutation randomly selects a point in the tree individual and replaces it by a random subtree. One-point crossover is used, it randomly selects two subtrees in the individuals and then exchanges them. The probability of mutation and crossover was pm = pcx = 0.5.\nThe choice of genetic operators has been made as simple as possible to enhance the readability of the algorithm. While more sophisticated GP methods exist, our focus here is the proof of concept of automatic generation of the compositions, and, for that purpose, the choice of the operators was appropriate. On the other hand, we conducted a set of preliminary experiments with other mutation and selection operators5 and did not appreciate significant changes in the results when the set of all groups of analogy questions were considered. Some operators can produce more accurate programs for some particular group, but then they are outperformed by other methods in other groups.\nIn the experiments, the population size used was N = 500 and the stop criterion is a maximum number (ngen = 250) of generations. Our GP implementation was written in Python. It is based on the EA software DEAP6 [8] and the gensim package7, a Python-based implementation of NLP\n5Those included in the DEAP library used to implement the algorithms 6http://deap.readthedocs.io/en/master/api/tools.html 7 https://radimrehurek.com/gensim/\nalgorithms [26]. gensim includes methods for interrogating the model generated by word2vect. Our code is openly available 8."}, {"heading": "6.1 GP operators", "text": "The set of operators used by the programs is shown in Table 4. All operators are defined on vectors of the same dimension. There are two classes of operators: binary and unary. The +, \u2212 and \u2217 operators have the following meaning: vector addition, vector subtraction, and vector componentwise multiplication, respectively; while % corresponds to protected division (usual division except that a divide by zero for any of the vector components returns zero). We discarded the possibility of including fixed (vector) random constants as terminals of the programs since they may depend on the size of the vector and our aim was to produce programs scalable to any vector dimension. We set a constraint d = 10 to the depth of the trees to reduce the complexity of the programs."}, {"heading": "6.2 Fitness function", "text": "A critical component of a GP implementation is the definition of the fitness function. We implement the fitness evaluation as follows: At the time of evaluating a candidate program, it is applied to a training set of questions. For each question, the word vectors of the first three words are first obtained from the CBOX models. The program is then evaluated using as arguments these three word vectors, and the program\u2019s output word vector is used to compute the quality of the program for the question.\nLet us consider the program add(sin(ARG0), sub(ARG2, ARG1)) and the first question in Table 2 as an example. First, we obtained the words vectors \u2212\u2192 boy, \u2212\u2212\u2192 girl, and \u2212\u2212\u2192sons. Then, from the execution of the GP program, we obtain a word vector ANS = add(sin( \u2212\u2192 boy), sub(\u2212\u2212\u2192sons, \u2212\u2212\u2192 girl)). The vector ANS is then presented to the CBOX model which outputs the closest word in the model. If this word coincides with the answer to the question, a correctly answered questions counter is increased. The final fitness value is the proportion of questions in the training set that were correctly answered.\nThe fitness function serves as a direct assessment of the program quality because we can directly test whether the program produces vectors whose semantics is the one encoded by the question. However, it has an important drawback. The computational cost of repeatedly interrogating the model to determine the closest word to a given vector is very high, and it would increase with the size of the vocabulary if larger corpora were used. To diminish this cost we introduced three changes to the GP scheme.\n1. Restricted vocabulary size for interrogation: The word2vec implementation allows the restriction of the search for the most likely word given a vector to the l most frequent words in the vocabulary. Out of the total number of words (71291) in the vocabulary, we set l = 30000. This reduces the computational time of the fitness function.\n2. Partial evaluation: Each program is trained on a fraction c of the questions from the training set. We set this fraction to be 15 of the size of the training set. When evaluating a program, first a subset of the questions from the training set is randomly selected, and the accuracy of the program is measured in this subset. This means that different programs are evaluated on distinct subsets of questions.\n8 https://github.com/rsantana-isg/GP_word2vec\n3. Early halt: While sequentially evaluating the questions in the (random subset of the) training set, the program does not complete the evaluation of all questions and halts if: 1) A NAN output is generated for any of the questions. 2) If after at least ten questions have been \u201canswered\u201d the proportion of correctly answered questions is at some point below 0.05. In this case it is clearly a poorly performing program.\nAll the previous enhancements considerably increase the efficiency of the algorithm. While partial evaluation adds some variability in the fitness output of the programs, good programs are in general good across subsets of questions and poor programs can not specialize in niches of questions since the subset selection in the training set is made randomly."}, {"heading": "7 Experiments", "text": "The main objective of the experiments is to determine the quality of the programs generated by GP. We will compare their results for the word analogy task with those obtained by the application of the linear algebraic rule ~d = ~c \u2212 ~a+~b, which is the one commonly used for the composition of words for this problem. In addition, we will evaluate the transferability of the best programs by applying them to a vector space comprising 3 \u00b7 106 vector, roughly 100 times the size of the vector space we used to learn the programs.\nFor each fitness function and each group of questions of those described in Table 3, 30 independent runs of the algorithms were executed. In total, 9\u00d730 = 270 executions were conducted. Each group of questions was split into a training and test set with same number of questions. The questions in the test set were not used at any time of the evolution."}, {"heading": "7.1 Numerical results", "text": "We evaluate the performance of the GP algorithms by looking at the accuracy of the best GP programs found. The accuracy, for each group of questions, is the proportion of questions correctly answered by a GP program. For each of the 30 runs we keep all the solutions in the last selected population (100 solutions by run). Among the 100 programs, the one that has the highest accuracy in the training set is selected. Then we compute the accuracy of this program also in the test set. Using the 30 programs, the maximum and mean accuracy are calculated in the training and test sets. Table 5 shows these values for the 9 groups of questions. The table also shows the accuracy produced by the algebraic rule. It can be seen that the best GP evolved programs outperform the algebraic rule on all the groups of questions, although the difference in the results is more noticeable for some groups of questions (e.g., group 6). The mean accuracy of the programs on the test set is also higher than that achieved using the algebraic rule for 6 of the 9 groups of questions. Notice, that since our selection of the best programs was based on the accuracy for the training set, there might be programs with a higher accuracy on the test set. We did identify some of these programs. Interestingly, for some groups of questions (e.g., group 11) the maximum and mean accuracy in the training set is smaller than in the test set.\nIn a second phase of the experiments, the best program generated in the last generation of each execution of the GP algorithm was selected based on the sum of the training and test set accuracy values for the corresponding group of questions. We evaluated this set of 270 programs using the word2vec GoogleNews vectors. These vectors have a larger dimension (300 versus 200 in the text8 vector space) and comprise around 3million words. As a consequence, these vectors contain all words for the 13 original groups of questions introduced in [16]. We must remember that the reason why we did not use four of the original groups of questions was that the text8 vector space did not include the vector representations for all constituent words of each question in these groups.\nUsing the word2vec GoogleNews vectors, we can test the evolved programs in all the data sets. The same set of operations encoded in the programs are applied, but this time using the new vector representation. The output vector is then submitted to the model that determines whether the closest word in the space of word2vec GoogleNews vectors is the right answer to the question.\nThe results of this evaluation are shown in Table 6, where each row corresponds to one of the 13 original groups of questions. Each column j shows the best accuracy produced by the best program among the 30 generated with function F0 for the group of questions represented in column j. The\nlast column shows the accuracy results of the algebraic rule. In each row, all the programs that produce results better than the one in the last column are shown in bold.\nNotice that we can evaluate the 270 GP program in all 13 groups of questions independently of the group used to learn them. Since all the questions have the same structure, we can apply the programs to them.\nThere are a number of remarkable facts in the results shown in Table 6:\n1. Some of the programs improved the accuracy for groups of questions that were not in the original reduced benchmark of 9 groups. This is the case for the group of questions 3.\n2. The best program for the group of questions j is not, in general, a program evolved to answer this group of questions.\n3. There are programs evolved for some groups of questions that are good at answering questions for all groups. For example, this happens with programs learned using the group of questions 12."}, {"heading": "7.2 Evaluating answers and evolved programs", "text": "One important issue is the interpretability of the evolved programs and how are they related with the algebraic rule. Out of the 270 programs tested, 8 were equivalent to the algebraic rule. Four of these programs are shown in Figure 2. It can be seen how the same rule is implemented in distinct ways\nusing only the operators add, sub, and neg. These results show that, as an algorithm to create word compositions, GP can automatically learn compositional methods designed by humans.\nWe also analyzed those GP programs that outperformed the algebraic rule. An exemplar of this type of programs is shown in Figure 3. It was the best program found for the group of questions 3. Its accuracy using the word2vec GoogleNews vectors was 74.17, above the 72.75 accuracy of the algebraic rule for the same group of questions.\nThe tree shown in Figure 3 is a slight modification of the algebraic rule. Instead of adding ARG2 to the rule, this programs adds 54ARG2 and this change allows it to increase the accuracy for the group of questions. A trend observed in other evolved programs was that they contained building blocks from the algebraic rule. As in the case of the programs shown in Figure 2, these structural features were not specifically induced, they were acquired as part of the evolutionary process. Other programs that produced high accuracy values are shown in figures 4- 8. When analyzing the behavior of these programs, tables 6 and 7 should be consulted."}, {"heading": "7.3 Discussion", "text": "We go back to the research questions posed at the beginning of this work and try to answer them based on the results of our experiments.\n\u2022 For embedding representations, can meaningful vector algebraic operations be learned from training examples? Yes, they can be learned within a relatively small computational time.\n\u2022 If so, is GP a feasible approach to do it? Yes, GP is a natural solution for this type of problem and even straightforward implementations can deal with the problem.\n\u2022 How do GP programs score with regard to the algebraic rule commonly applied on vector representations? GP programs can learn the same rule designed by humans and, therefore, can reach the same results. They can also outperform these results but, at least for the class of word vector representation and the basic tree-based GP approach implemented, the improvements are moderate.\n\u2022 Are GP evolved programs transferable across linguistic tasks, vector representations and corpora? Definitively. The high transferability of the programs across groups of questions may be supported by the general underlying commonality between analogies that these group of questions represent. However, it is remarkable how the programs can be transferred to a vector space where both the dimension of the vectors, and the number of vectors increase dramatically. In this respect, transferability opens an additional opportunity for efficiency gain. Programs can be learned using small vector spaces, and then validated or refined on more computationally costly large vector spaces."}, {"heading": "8 Conclusions and future work", "text": "While semantic spaces and word vector representations are able to capture some of the semantic relationships between the words, compositional methods are necessary to extend their use to multiword constructions. In this paper we have proposed representing compositional vector operations as simple programs that can be automatically learned from data. We have shown that, using GP, it is possible to encode a set of vector operations as a program, that the programs can be evolved to achieve higher accuracy than the human rules conceived to manipulate the words, and that the programs are valid for datasets other than those from which they have been learned, i.e., they are transferable programs. Furthermore, our results indicate that it is possible to learn programs using vector vocabularies of small to moderate sizes and then test them in bigger domains where the evaluation of a program is more costly."}, {"heading": "8.1 Future work", "text": "As lines for future work we consider the following:"}, {"heading": "8.1.1 Use alternative methods for the word vector generation", "text": "While GP approaches can explore a vast range of possible word compositions, the usefulness of more intricate programs is, to some extent, constrained by the nature of the relationships that the vectors can encode. For example, if the methods used to construct the embeddings do not allow non-linear relationships between the vectors, then the improvements of the GP programs over plain linear algebraic compositional operators will be marginal. Therefore, it would be important to test the automatic generation of word compositions with GP on word vectors generated using diverse methods."}, {"heading": "8.1.2 Evolve functions for the similarity metric", "text": "Since it has been shown that the type of similarity metric can critically influence the accuracy results [15], it makes sense to learn this function as well. One difficulty is that the output of this functionwill be a numerical value and not a vector like the other operators used in the current GP representation. In addition, evaluating an alternative similarity metric implies using the candidate metric to compute distances to all vectors in the vector space, a process that can be very costly computationally."}, {"heading": "8.1.3 Combining different word representations", "text": "Turian et al [30] have shown that combining different word representations can improve accuracy for supervised classification tasks in NLP [30]. We envision the evolution of programs which are able to combine different word vector representations."}, {"heading": "8.1.4 Using more sophisticated GP approaches", "text": "From the point of view of research in genetic programming, word embeddings open an interesting research line. More research is needed to identify which, among more sophisticated GP approaches, are the most appropriate for their application to semantic spaces. Among possible lines of research are the following:\n\u2022 Alternative GP representations: In addition to trees, other GP representations such as grammars [23, 4] and Cartesian GP [19] could be considered.\n\u2022 More complex descriptions of the compositional operators: One open question is to what extent can more complex functions better exploit the underlying semantic relationships between the word vectors. This could be investigated by adding other algebraic operators to the set of GP functions, including ternary operators. Another possibility is representing the composition of vectors with ensembles of GP programs [2].\n\u2022 Reusing problem information: Approaches able to identify and transfer building blocks [10] between word vectors or corpora of varying dimensions arise as potential candidates.\n\u2022 Behavioral program synthesis: One direction in which the evolution of the programs could be improved is by analyzing and assessing the quality of the intermediate vectors produced in the evaluation of the programs. In general, algorithms that advocate a more efficient use of the information displayed by the behavior of the GP programs [14] could lead to better solutions and reveal additional insights in learning compositional methods."}, {"heading": "Acknowledgments", "text": "This work has received support through through the IT-609-13 program (Basque Government), TIN2016-78365-R (Spanish Ministry of Economy, Industry and Competitiveness) and Brazilian CNPq Program Science Without Borders No.: 400125/2014-5."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of machine learning research, 3(Feb):1137\u20131155,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Evolving diverse ensembles using genetic programming for classification with unbalanced data", "author": ["U. Bhowan", "M. Johnston", "M. Zhang", "X. Yao"], "venue": "IEEE Transactions on Evolutionary Computation, 17(3):368\u2013386,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe andM. Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning probabilistic tree grammars for genetic programming", "author": ["P.A. Bosman", "E.D. De Jong"], "venue": "International Conference on Parallel Problem Solving from Nature, pages 192\u2013201. Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "An analysis of the solution space for genetically programmed term-weighting schemes in information retrieval", "author": ["R. Cummins", "C. O\u2019Riordan"], "venue": "editor, 17th Artificial Intelligence and Cognitive Science Conference (AICS", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Mart\u0131\u0301nez-Carranza. Term-weighting learning via genetic programming for text classification", "author": ["H.J. Escalante", "M.A. Garc\u0131\u0301a-Lim\u00f3n", "A. Morales-Reyes", "M. Graff", "M. Montes-y G\u00f3mez", "E.F. Morales"], "venue": "Knowledge-Based Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "DEAP: Evolutionary algorithms made easy", "author": ["F.-A. Fortin", "D. Rainville", "M.-A.G. Gardner", "M. Parizeau", "C. Gagn\u00e9"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["E. Grefenstette", "M. Sadrzadeh"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394\u20131404. Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Reusing building blocks of extracted knowledge to solve complex, large-scale Boolean problems", "author": ["M. Iqbal", "W. Browne", "M. Zhang"], "venue": "Evolutionary Computation, IEEE Transactions on, 18(4):465\u2013480, Aug", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["M. Iyyer", "J.L. Boyd-Graber", "L.M.B. Claudino", "R. Socher", "H. Daum\u00e9 III"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 633\u2013644,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Predication", "author": ["W. Kintsch"], "venue": "Cognitive science, 25(2):173\u2013202,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Genetic Programming: On the Programming of Computers by Means of Natural Selection", "author": ["J.R. Koza"], "venue": "The MIT Press, Cambridge, MA,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "Behavioral program synthesis: Insights and prospects", "author": ["K. Krawiec", "J. Swan", "U.-M. OReilly"], "venue": "Genetic Programming Theory and Practice XIII, pages 169\u2013183. Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["O. Levy", "Y. Goldberg"], "venue": "Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171\u2013180,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "CoRR, abs/1309.4168,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Cartesian genetic programming", "author": ["J.F. Miller", "P. Thomson"], "venue": "European Conference on Genetic Programming, pages 121\u2013132. Springer,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive science, 34(8):1388\u20131429,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Strongly typed genetic programming", "author": ["D.J. Montana"], "venue": "Evolutionary computation, 3(2):199\u2013 230,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Improving the effectiveness of information retrieval with genetic programming", "author": ["N. Oren"], "venue": "Master\u2019s thesis, Faculty of Science of the University of Witwatersrand, Johannesburg,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Grammatical evolution", "author": ["M. ONeil", "C. Ryan"], "venue": "Grammatical Evolution, pages 33\u201347. Springer,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), volume 14, pages 1532\u2013 1543,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "A field guide to genetic programming", "author": ["R. Poli", "W.B. Langdon", "N.F. McPhee", "J.R. Koza"], "venue": "Lulu.com,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Software framework for topic modelling with large corpora", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the 2011 Conference Advances in Neural Information Processing Systems 24, NIPS, volume 24, pages 801\u2013809,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the conference on empirical methods in natural language processing, pages 151\u2013161. Association for Computational Linguistics,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to rank", "author": ["A. Trotman"], "venue": "Information Retrieval, 8(3):359\u2013381,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for Computational Linguistics,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Similarity of semantic relations", "author": ["P.D. Turney"], "venue": "Computational Linguistics, 32(3):379\u2013416,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Combining heterogeneous models for measuring relational similarity", "author": ["A. Zhila", "W.-t. Yih", "C. Meek", "G. Zweig", "T. Mikolov"], "venue": "In Proceedings of the 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}], "referenceMentions": [{"referenceID": 29, "context": "In particular, machine learning methods that use this representation have been proposed for named entity recognition [30], question answering [11], machine translation [17], etc.", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "In particular, machine learning methods that use this representation have been proposed for named entity recognition [30], question answering [11], machine translation [17], etc.", "startOffset": 142, "endOffset": 146}, {"referenceID": 16, "context": "In particular, machine learning methods that use this representation have been proposed for named entity recognition [30], question answering [11], machine translation [17], etc.", "startOffset": 168, "endOffset": 172}, {"referenceID": 30, "context": "Another convenient feature of vector word spaces is that the word vectors are able to capture attributional similarities [31] between words.", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "For instance, these regularities can be manifested as constant vector offsets between pairs of words sharing a particular relationship [18, 16].", "startOffset": 135, "endOffset": 143}, {"referenceID": 15, "context": "For instance, these regularities can be manifested as constant vector offsets between pairs of words sharing a particular relationship [18, 16].", "startOffset": 135, "endOffset": 143}, {"referenceID": 14, "context": "It has been suggested that the linguistic regularities that vector representations produced by neural networks exhibit are not a consequence of the embedding process itself, but are well preserved by it however [15].", "startOffset": 211, "endOffset": 215}, {"referenceID": 14, "context": "This seems confirmed by the fact that for other types of vector representations linear algebraic operations can also produce meaningful results [15, 24].", "startOffset": 144, "endOffset": 152}, {"referenceID": 23, "context": "This seems confirmed by the fact that for other types of vector representations linear algebraic operations can also produce meaningful results [15, 24].", "startOffset": 144, "endOffset": 152}, {"referenceID": 12, "context": "In this context, genetic programming (GP) [13] arises as a natural candidate.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "We frame the general question of finding a suitable transformation of word vectors on the more specific word analogy task [16, 24].", "startOffset": 122, "endOffset": 130}, {"referenceID": 23, "context": "We frame the general question of finding a suitable transformation of word vectors on the more specific word analogy task [16, 24].", "startOffset": 122, "endOffset": 130}, {"referenceID": 19, "context": "To formalize the analysis of methods for word composition, Mitchell and Lapata [20] define p = f(u, v,R,K) as the composition of vectors u and v.", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "In [20], additive models such as the one represented by Eq.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Other compositional models that consider contextual information (neighboring words) have been also proposed [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "Neural networks have been applied for implementing the latter approach [5, 32].", "startOffset": 71, "endOffset": 78}, {"referenceID": 31, "context": "Neural networks have been applied for implementing the latter approach [5, 32].", "startOffset": 71, "endOffset": 78}, {"referenceID": 15, "context": "In this paper we have used vectors learned by the application of shallow neural networks as proposed in [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "In [16], two neural-network based models have been proposed to learn embeddings: Skip-gram and Continuous Bags of words (CBOW) models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "CBOW is a feed-forward neural net language model [1] with a number of added changes.", "startOffset": 49, "endOffset": 52}, {"referenceID": 15, "context": "Figure 1: Continuous Bag-of-Words (CBOW) model as proposed in [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "Figure 1 shows a schematic representation of the CBOW architecture [16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "In the results reported in [16], the best results were obtained using k = 4.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "We use the original word2vec implementation of Mikolov et al [18, 16] to train the CBOW network from the corpus and generate embeddings.", "startOffset": 61, "endOffset": 69}, {"referenceID": 15, "context": "We use the original word2vec implementation of Mikolov et al [18, 16] to train the CBOW network from the corpus and generate embeddings.", "startOffset": 61, "endOffset": 69}, {"referenceID": 12, "context": "Genetic programming [13, 25] is a domain-independent method for the automatic creation of programs that solve a given problem.", "startOffset": 20, "endOffset": 28}, {"referenceID": 24, "context": "Genetic programming [13, 25] is a domain-independent method for the automatic creation of programs that solve a given problem.", "startOffset": 20, "endOffset": 28}, {"referenceID": 14, "context": "Levy and Goldberg [15] investigate the question of how to recover the relational similarities in word embeddings.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "[16] to solve analogy recovery is equivalent to searching for a word that maximizes a linear combination of three word similarities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "While our research is related to the work presented in [15], our focus is on the operations involved in the generation of the candidate word vector, not on the way the match between the generated vector and the word vectors in the corpus is assessed.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "Pennington et al [24] introduce global log-bilinear regression models as an alternative to shallow neural-networks to produce word embeddings.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "The algebraic rule they use to solve the word analogy task is the same as that originally introduced in [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "Although they applied the distance measure previously presented by Levy and Goldberg [15], they report that this distance did not produce better results than the original one.", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "The work presented in [24] is relevant for our research since it confirms that the usability of the word vector algebraic rule extends over vector representations obtained using a variety of model types and algorithms.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "Socher et al [27, 28] propose a method for compositional representation of words that learns a binary parse tree for each input phrase or sentence.", "startOffset": 13, "endOffset": 21}, {"referenceID": 27, "context": "Socher et al [27, 28] propose a method for compositional representation of words that learns a binary parse tree for each input phrase or sentence.", "startOffset": 13, "endOffset": 21}, {"referenceID": 8, "context": "Grefenstette et al [9] propose associating different levels of meaning for words with different types of representations.", "startOffset": 19, "endOffset": 22}, {"referenceID": 20, "context": "In principle, GP approaches could cater for joint use of vector and matrix representation by means of strongly typed GP [21] or other GP variants that guarantee type constraint enforcement.", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "In [3], three word vector representations and three compositional methods (addition of word vectors, multiplication of vectors, and the aforementioned deep recursive autoencoder approach) are combined to evaluate their applicability to estimate phrase similarity and paraphrase detection (i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "However, GP has been applied to a miscellany of tasks in information retrieval [6, 7, 22, 29].", "startOffset": 79, "endOffset": 93}, {"referenceID": 6, "context": "However, GP has been applied to a miscellany of tasks in information retrieval [6, 7, 22, 29].", "startOffset": 79, "endOffset": 93}, {"referenceID": 21, "context": "However, GP has been applied to a miscellany of tasks in information retrieval [6, 7, 22, 29].", "startOffset": 79, "endOffset": 93}, {"referenceID": 28, "context": "However, GP has been applied to a miscellany of tasks in information retrieval [6, 7, 22, 29].", "startOffset": 79, "endOffset": 93}, {"referenceID": 21, "context": "In particular, Oren [22] combines GP with vector-based representation of documents for information retrieval.", "startOffset": 20, "endOffset": 24}, {"referenceID": 28, "context": "Two related areas where GP has been applied are document ranking [29] and term-weighting learning [6, 7].", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "Two related areas where GP has been applied are document ranking [29] and term-weighting learning [6, 7].", "startOffset": 98, "endOffset": 104}, {"referenceID": 6, "context": "Two related areas where GP has been applied are document ranking [29] and term-weighting learning [6, 7].", "startOffset": 98, "endOffset": 104}, {"referenceID": 17, "context": "[18] in which questions are separated into 13 groups.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "It is based on the EA software DEAP [8] and the gensim package, a Python-based implementation of NLP Those included in the DEAP library used to implement the algorithms http://deap.", "startOffset": 36, "endOffset": 39}, {"referenceID": 25, "context": "algorithms [26].", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "As a consequence, these vectors contain all words for the 13 original groups of questions introduced in [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "Since it has been shown that the type of similarity metric can critically influence the accuracy results [15], it makes sense to learn this function as well.", "startOffset": 105, "endOffset": 109}, {"referenceID": 29, "context": "Turian et al [30] have shown that combining different word representations can improve accuracy for supervised classification tasks in NLP [30].", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "Turian et al [30] have shown that combining different word representations can improve accuracy for supervised classification tasks in NLP [30].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "\u2022 Alternative GP representations: In addition to trees, other GP representations such as grammars [23, 4] and Cartesian GP [19] could be considered.", "startOffset": 98, "endOffset": 105}, {"referenceID": 3, "context": "\u2022 Alternative GP representations: In addition to trees, other GP representations such as grammars [23, 4] and Cartesian GP [19] could be considered.", "startOffset": 98, "endOffset": 105}, {"referenceID": 18, "context": "\u2022 Alternative GP representations: In addition to trees, other GP representations such as grammars [23, 4] and Cartesian GP [19] could be considered.", "startOffset": 123, "endOffset": 127}, {"referenceID": 1, "context": "Another possibility is representing the composition of vectors with ensembles of GP programs [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "\u2022 Reusing problem information: Approaches able to identify and transfer building blocks [10] between word vectors or corpora of varying dimensions arise as potential candidates.", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "In general, algorithms that advocate a more efficient use of the information displayed by the behavior of the GP programs [14] could lead to better solutions and reveal additional insights in learning compositional methods.", "startOffset": 122, "endOffset": 126}], "year": 2017, "abstractText": "Word-vector representations associate a high dimensional real-vector to every word from a corpus. Recently, neural-network based methods have been proposed for learning this representation from large corpora. This type of word-to-vector embedding is able to keep, in the learned vector space, some of the syntactic and semantic relationships present in the original word corpus. This, in turn, serves to address different types of language classification tasks by doing algebraic operations defined on the vectors. The general practice is to assume that the semantic relationships between the words can be inferred by the application of a-priori specified algebraic operations. Our general goal in this paper is to show that it is possible to learn methods for word composition in semantic spaces. Instead of expressing the compositional method as an algebraic operation, we will encode it as a program, which can be linear, nonlinear, or involve more intricate expressions. More remarkably, this program will be evolved from a set of initial random programs by means of genetic programming (GP). We show that our method is able to reproduce the same behavior as human-designed algebraic operators. Using a word analogy task as benchmark, we also show that GP-generated programs are able to obtain accuracy values above those produced by the commonly used human-designed rule for algebraic manipulation of word vectors. Finally, we show the robustness of our approach by executing the evolved programs on the word2vec GoogleNews vectors, learned over 3 billion runningwords, and assessing their accuracy in the same word analogy task. keywords: semantic spaces, compositional methods, word2vec, genetic programming, word vectors", "creator": "LaTeX with hyperref package"}}}