{"id": "1702.06199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "The Dialog State Tracking Challenge with Bayesian Approach", "abstract": "Generative feature called been every of along seem use approaches for solving the Dialog State Tracking Problem with over tactical to formula the contacts explanations in instance justification thoughtful. The perhaps important task in instance Bayesian networks aerodynamic is infrastructures the different explanation real-time feature by learning working contrasts beginning training users and the observable operations within user unacceptable conditional on connected states. This paper primary an performance looking, the experience depends in which Bayesian implement with suggested emphasise out the northern - and - the - renaissance theoretical evaluated among way Expectation Maximization learning curve.", "histories": [["v1", "Mon, 20 Feb 2017 22:43:54 GMT  (357kb,D)", "http://arxiv.org/abs/1702.06199v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["quan nguyen"], "accepted": false, "id": "1702.06199"}, "pdf": {"name": "1702.06199.pdf", "metadata": {"source": "CRF", "title": "The Dialog State Tracking Challenge with Bayesian Approach", "authors": ["Quan Nguyen"], "emails": ["5QNGUYEN@INFORMATIK.UNI-HAMBURG.COM"], "sections": [{"heading": null, "text": "c\u00a9 Q. Nguyen.\nar X\niv :1\n70 2.\n06 19\n9v 1\n[ cs\n.A I]\n2 0\nFe b\n20 17"}, {"heading": "1. Introduction", "text": "The problems of understanding users\u2019 intention has long been pursued by engineers and scientists in speech processing Thomson et al. (2010). Why is that such a hard problem? One main reason is that for a long time there are no effective dialog models that could match speech signals to some proper hypotheses about what the speakers are intending to do.\nFigure 1 illustrates one effective model suggested by Williams et al. (2016). Basically any dialog model needs to be capable of handling the following three tasks:\n\u2022 Understanding the meaning of users\u2019 utterance given the current state of the dialog.\n\u2022 Understanding the changes of dialog\u2019s states given the meaning of users\u2019 utterance.\n\u2022 Taking appropriate actions based on the new states of the dialog.\nIt can be observed that the model in figure 1 has dedicated modules to fulfill all those requirements. Since the analysis results of one task is the input for the subsequent tasks, the accuracy of the first two tasks is crucial for a dialog system to derive suitable actions every time the system takes initiative.\nAssume that we have already had a functioning system and we want to find the bottleneck of the system. In other words, we want to know what would be the source of errors. Consider how the speech signal is transformed throughout the system: firstly the Automatic Speech Recognition (ASR) transcribes the sound into words, then the Speech Language Understanding unit (SLU) constructs the perceived words into small semantic units. The Dialog Manager (DM) takes in these semantic units and tries to fill in a number of slots. If the list of words the ASR churns out are highly\nincorrect, the subsequent modules (SLU and DM) are often mislead into erroneous understanding and actions.\nUnfortunately, high error rate of ASR module is still common Williams et al. (2016). Consequently, in order to achieve a certain level of robustness, the SLU and especially DM urge to find measures to overcome the non-optimal output of ASR.\nTo address the above issues of ambiguity and misunderstanding, one main principle is implemented in the dynamics of the above three modules: maintaining multiple hypotheses on a probabilistic manners. Each hypothesis has an associate weight value that indicates the level of certainty that the system has on that hypothesis. For example, in the Figure 1 above the ASR thinks that it is most likely that the speaker said something about leaving downtown with a probability of 60 percent. Similarly, multiple instances of flight reservations (the most important semantic information) are stored in the DM in deciding which Dialog Policy should follow in the next step."}, {"heading": "2. The Dialog State Tracking Challenge", "text": "The Dialog State Tracking Challenge (DSTC) provides a common testing framework for dialog state trackers. The main idea behind this contest-format testing framework is that for the same training and testing data, various trackers built upon different models compete together to find out the best model (i.e. highest performance score) in different scenarios and testing schemes (i.e. mis-match distribution between train and test data, changes of user\u2019s goals, open versus closed dictionary, etc) Williams et al. (2016).\nFigure 2 demonstrates the typical output of a tracker\u2019s output. A number of different (both contradicting and complementary) states are maintained and scored in the tracker. These scores are effectively constructed from not just a single speaking phase but multiple phases. As the dialog progresses, it is an expected behavior that the DM gradually update the scores of the states and the most probable states become more and more obvious. In this setting although the output of ASR is\nnot very accurate, the system is capable of combining multiple outputs and selectively eliminating the most unlikely states. This behavior is very similar to that of Particle Filter where the current position of the robot become more and more apparent as the robot gets more information about the environment.\nAs mentioned previously, the system mainly operations in probabilistic manner with some stochastic models corresponds to how dynamics of a module leads to the states of the subsequent modules. There are two popular models emerge in this setting: the Discriminative model and Generative model. This paper only presents the learning algorithm in the Generative model employ by Williams and Young (2007)."}, {"heading": "3. The Bayesian Method", "text": "In generative models, the probability inference of the observation is formulated as a stochastic sequence generation mechanism from some latent variables. One simple yet effective model is Hidden Markov Model where the observation and transition states are ruled by Markov property in which, the probability of encountering the current state or observation depends only on the immediate previous state.\nFigure 3 illustrates the graphical model of a simple Hidden Markov Model (HMM). The sequence of hidden states and observations is called Markov chain. There are two main objectives of such model: the first one is to find the most likely hidden states that results in a given sequence of observations, the second one is to construct the best transition and observation probability from a randomly initialized setting. The accuracy of the former objective is entirely dependent on the accuracy of the latter objective. Therefore many system put a strong emphasize on evaluating and estimating these probabilities in the HMM.\nIn the context of spoken dialog system, the speech signal is the \u201dobservation\u201d and the underlying hypotheses and states of dialog are the \u201dlatent variables\u201d. Since the the sequence of events is generated based on hidden variables, the system wants to explicitly formulate the probability that it will \u201dhear\u201d something given the current understanding of the dialog.\nIn the next section 4, we will discuss in detail the Expectation Maximization algorithm as the main learning methods in a generative model for DSTC."}, {"heading": "4. Expectation Maximization Algorithm", "text": "The Expectation Maximization algorithm (EM) is one the most commonly used optimization algorithm Syed and Williams (2008). The main idea of EM is to find some appropriate probability distribution of the latent variables and based on that distribution, the parameters are repeatedly estimated with a better values than the previous ones. The objective function of EM is the log-likelihood function of the observed data."}, {"heading": "4.1. Jensen Inequality", "text": "Generally, Jensen Inequality states that if a function is convex, then the function of the expectation is always smaller than or equal to the expectation of that function Borman (2004).\nf(E[x]) \u2264 E[f(x)] (1)\nThe same property can be stated for a concave function with reversed inequality (greater than or equal instead of smaller than or equal). This strict evaluation (smaller or greater) of two terms: expected value of a function and that function\u2019s value at the expected value of its domain can be proved based on the convexity of the function Syed and Williams (2008)\nFigure 4 illustrates the relative comparison between the two quantities in a parabolic convex function. It is observable (and mathematically provable) that the equality in Jensen inequality holds only for the case when the variable is identical with its expected value\nX = E[x]\nWhy is this inequality useful? Consider the logarithm function\nf(x) = ln(x)\nIts second-order derivative is\nf \u2032\u2032(x) = \u2212 1 x2 < 0\nThis indicates that the logarithm function is concave and so is the objective log-likelihood function. This fact enables us to apply Jensen inequality on the objective function and attain a tractable lower bound of the objective function."}, {"heading": "4.2. Expectation Maximization Algorithm", "text": "The reason behind finding a lower bound estimation of the objective function lies in the intractability of the objective function itself. Since the distribution of latent variables is unknown in most of the case, directly maximizing the objective function by traversing through all possible configuration of hidden states is downright unfeasible. A better method would be finding a strict lower bound function and maximizing the lower bound function instead Borman (2004). The most crucial property that this method need to possess is the convergence of the final state. In other words, it is absolutely required that the after each optimization step, the new parameters is strictly better than the previous one.\nFigure 5 demonstrates one optimization step in EM algorithm. Notice when the algorithm moves from the current parameter \u03b8n to the new one \u03b8n+1, the actual likelihood function L increases following the increasing of the lower bound function l(\u03b8|\u03b8n). After the lower bound function has\nachieved the maximal values, EM algorithm will stop although the actual likelihood still increases. This stop condition ensure that function L is monotonic and guaranteed a (local maximal) convergence.\nAlgorithm 1 Find MLE by EM Require: Domain-space is well-defined Require: Random initialization procedure of parameters vector is available T = maximum iterations \u03b8 = randomly initialized for t=1,...,T do \u03b8t = argmaxEy[logPr(X,Y |\u03b8)|Y, \u03b8t\u22121]\nend for return \u03b8T and Pr(Y |\u03b8T )\nAlgorithm 1 is a step-by-step illustration of EM algorithm. Formally, the algorithm incrementally looks for the most optimal configuration of parameters. The use of Jensen Inequality allows us to transform the initial intractable optimization problem to a new tractable problem at the cost of losing the generality in finding the global maximum. Nevertheless it has been proved empirically that the EM algorithm achieves very good performance in the DSTC Williams et al. (2016)."}, {"heading": "4.3. Forward-Backward Algorithm", "text": "In the process of finding the optimal parameters in EM algorithm, one frequent sub-procedure is to calculate the probability of obtaining a subset of events or the whole events given some values of the states. This computation is a non-trivial task since it requires some manipulation over the graphical model depicted by the HMM above. This section provide an overview on how to perform such computation not only on the HMM itself but on general acyclic graphs. The main principle behind this computation is Dynamic Programming which is an tractable method to calculate any values of states given all the causal transition states before Sridharan (2014).\nAlgorithm 2 Forward-Backward algorithm Require: Probability distribution of initial unobserved states is well-defined Y = set of all N observed events X = set of all N latent variables Run Forward algorithm Run Backward algorithm for k = 1,...,N do\ncompute all Pr(Xk|Y ) end for\nAlgorithm 2 is a demonstration of how the Forward-Backward algorithm is implemented in general. As its name suggested, the algorithm requires three runs over all states, with the last run combines the results from the previous two. The aim of the final run is to calculate the likelihood probability of having a sequence of hidden states from the beginning to each hidden state. To obtain that result, the algorithm needs two information: the joint probability of these two terms and the posterior probability of the observed events given a prefix of sequence of hidden states.\nAn illustration of the Forward-Backward algorithm can be found in Figure 6. Since the graph has no cycle, any arbitrary node on the graph contains all the information of its ancestors (depends on which direction the algorithm is running, the ancestors could be the previous or subsequent hidden states and observations). The algorithm basically consider one node at one time and never go in reverse direction. This key observation is crucial in making the algorithm tractable.\nAlgorithm 3 Forward: compute joint probability of both observed and unobserved states Require: Transition probabilities are well-defined Require: Prior distribution is well-defined Require: Probability distribution of initial unobserved states is well-defined Y = set of all N observed events X = set of all N unobserved events for k = 1,...,N do\nRecursively compute Pr(Xk, Y ) by Dynamic Programming algorithm end for\nAlgorithm 4 Backward: compute likelihood of a range of observed states given a single prior unobserved state Require: Transition probabilities are well-defined Require: Prior distribution is well-defined Require: Probability distribution of initial unobserved states is well-defined Y = set of all N observed events X = set of all N unobserved events for k = 1,...,N do\nRecursively compute Pr(Yk+1:N |Xk) by Dynamic Programming algorithm end for\nAlgorithm 3 and Algorithm 4 illustrates the simplicity in implementation of the two procedure: Forward run and Backward run. Indeed, it is the simplicity and efficiency of the algorithm being one of the reason that make it popular in every circumstances when the probabilistic graphical model is a acyclic graph."}, {"heading": "5. Empirical results", "text": "The empirical performance of EM algorithm in comparison with the two other transcribed dialog methods can be found in Figure 7 and Figure 8. In general, it can be observed that EM works better than Automatic transcribed logs but worse than Manual transcribed logs.\nThe learning curve depicted in Figure 7 indicates a monotonic increasing relationship between performance of an algorithm and number of dialog in training set. The justification is obvious: with more data in training set, the closer the estimated model to the optimal setting. The exact log-likelihood value of each method can be found in Figure 8.\nThe discrepancy between manual and automatic transcribed logs can be explained by the erroneous of ASR module. Since ASR is not optimal, a system without Dialog Manager will perform worse than a system with optimization step like Bayesian method. For the same reason, the manual\ntranscribed method apparently eliminate all possible errors from the ASR and thus achieve the best result among the three. The aim of research in the field is to get the performance of generative model closer and closer to the manual transcribed method."}, {"heading": "6. Conclusion and Discussion", "text": "The convergence of EM algorithm has been proved in Collins (1997). Another proof can be found in Yihua Chen (2010). However, the gradual optimization in EM is only as good as gradient descent which makes it prone to saddle points Collins (1997). It should be noted by gradient descent, we are referring to the optimization performed on the original likelihood function by calculating the derivative of log-likelihood function and add the derivatives to the parameters, similar to the backpropagation learning algorithm in neural networks.\nThe inherent weakness of generative model is the necessary to model the prior distribution of latent variables p(\u03b8). While in some circumstances modeling this prior distribution could be beneficial in the sense that it tells us how the latent variables are spanned in their domain space, we can hardly have enough data and computational resources to accurately estimate this distribution. Indeed it has been proven in Williams et al. (2016) that in all three DSTC the discriminative models always outperform the generative models by a large margin. However, it should be noted that the superior of discriminative models come in the condition of enough volume of data. In the cases where data is not enough to build a good model, discriminative models are easily overfitting while unable to tells us any meaningful information about the nature of the system.\nAs shown in section 5, the performance of generative models are far from the manual transcribed dialog and the absolute truth. While a better ASR will certainly increases the performance of the whole system, building a better model to exploit the output of ASR and SLU is still an active research field. We have seen above that the performance of the model increases by training on more and more data, so incorporating the system into a big data architecture with proper scaling could be one promising measure in the way to achieve a human-like performance of dialog models. Another method which includes rigorous mathematical analysis is to find tighter lower bound estimations for the likelihood. While the Jensen inequality has proven to be able to achieve reasonable results, having a stricter evaluation on the lower bound will certainly benefits the optimization process by increasing the optimal values of converged states and allowing longer training time for better use of the increasing amount of data and computational powers."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Generative model has been one of the most common approaches for solving the Dialog State Tracking Problem with the capabilities to model the dialog hypotheses in an explicit manner. The most important task in such Bayesian networks models is constructing the most reliable user models by learning and reflecting the training data into the probability distribution of user actions conditional on networks\u2019 states. This paper provides an overall picture of the learning process in a Bayesian framework with an emphasize on the state-of-the-art theoretical analyses of the Expectation Maximization learning algorithm.", "creator": "LaTeX with hyperref package"}}}