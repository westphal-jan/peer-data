{"id": "1702.06235", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Learning to generate one-sentence biographies from Wikidata", "abstract": "We investigate the newest. _ - sentence Wikipedia mentions from false derived from Wikidata newscasts - rise dressage. We ship every recurrent neural network chronology - to - sequence speed with attention to boards whatever have generate description friendlies. Our version translation end novel secondary purpose that helps priority far generates imposition that multiple under functionality describe. The model criterion comes BLEU score own 43, achieve increased upon 's vanilla stages - immediately - sequence model and third 4,500 put however within gave method query advantage. Human lack recommendation suggests the model neither nearly example my as the Wikipedia usage. Manual analysis explores content consideration, viewed the component instance strengthening the ability as reasoning knowledge united both pressures such hallucinating unfounded materials.", "histories": [["v1", "Tue, 21 Feb 2017 01:30:59 GMT  (64kb,D)", "http://arxiv.org/abs/1702.06235v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["andrew chisholm", "will radford", "ben hachey"], "accepted": false, "id": "1702.06235"}, "pdf": {"name": "1702.06235.pdf", "metadata": {"source": "CRF", "title": "Learning to generate one-sentence biographies from Wikidata", "authors": ["Andrew Chisholm", "Will Radford", "Ben Hachey"], "emails": ["andy.chisholm.89@gmail.com", "wradford@hugo.ai", "bhachey@hugo.ai"], "sections": [{"heading": "1 Introduction", "text": "Despite massive effort, Wikipedia and other collaborative knowledge bases (KBs) have coverage and quality problems. Popular topics are covered in great detail, but there is a long tail of specialist topics with little or no text. Other text can be incorrect, whether by accident or vandalism. We report on the task of generating textual summaries for people, mapping slot-value facts to onesentence encyclopaedic biographies. In addition to initialising stub articles with only structured data, the resulting model could be used to improve consistency and accuracy of existing articles. Figure 1 shows a Wikidata entry for Mathias Tuomi, with fact keys and values flattened into a sequence, and the first sentence from his Wikipedia article. Some values are in the text, others are missing\n(e.g. male) or expressed differently (e.g. dates).\nWe treat this knowlege-to-text task like translation, using a recurrent neural network (RNN) sequence-to-sequence model (Sutskever et al., 2014) that learns to select and realise the most salient facts as text. This includes an attention mechanism to focus generation on specific facts, a shared vocabulary over input and output, and a multi-task autoencoding objective for the complementary extraction task. We create a reference dataset comprising more than 400,000 knowledgetext pairs, handling the 15 most frequent slots. We also describe a simple template baseline for comparison on BLEU and crowd-sourced human preference judgements over a heldout TEST set.\nOur model obtains a BLEU score of 41.0, compared to 33.1 without the autoencoder and 21.1 for the template baseline. In a crowdsourced preference evaluation, the model outperforms the baseline and is preferred 40% of the time to the Wikipedia reference. Manual analysis of content selection suggests that the model can infer knowledge but also makes mistakes, and that the autoencoding objective encourages the model to select more facts without increasing sentence length. The task formulation and models are a foundation for text completion and consistency in KBs.\nar X\niv :1\n70 2.\n06 23\n5v 1\n[ cs\n.C L\n] 2\n1 Fe\nb 20\n17"}, {"heading": "2 Background", "text": "RNN sequence-to-sequence models (Sutskever et al., 2014) have driven various recent advances in natural language understanding. While initial work focused on problems that were sequences of the same units, such as translating a sequence of words from one language to another, other work been able to use these models by coercing different structures into sequences, e.g., flattening trees for parsing (Vinyals et al., 2015), predicting span types and lengths over byte input (Gillick et al., 2016) or flattening logical forms for semantic parsing (Xiao et al., 2016).\nRNNs have also been used successfully in knowledge-to-text tasks for human-facing systems, e.g., generating conversational responses (Vinyals and Le, 2015), abstractive summarisation (Rush et al., 2015). Recurrent LSTM models have been used with some success to generate text that completely expresses a set of facts: restaurant recommendation text from dialogue acts (Wen et al., 2015), weather reports from sensor data and sports commentary from on-field events (Mei et al., 2015). Similarly, we learn an end-to-end model trained over key-value facts by flattening them into a sequence.\nChoosing the salient and consistent set of facts to include in generated output is also difficult. Recent work explores unsupervised autoencoding objectives in sequence-to-sequence models, improving both text classification as a pretraining step (Dai and Le, 2015) and translation as a multitask objective (Luong et al., 2016). Our work explores an autoencoding objective which selects content as it generates by constraining the text output sequence to be predictive of the input.\nBiographic summarisation has been extensively researched and is often approached as a sequence of subtasks (Schiffman et al., 2001). A version of the task was featured in the Document Understanding Conference in 2004 (Blair-Goldensohn et al., 2004) and other work learns policies for content selection without generating text (Duboue and McKeown, 2003; Zhang et al., 2012; Cheng et al., 2015). While pipeline components can be individually useful, integrating selection and generation allows the model to exploit the interaction between them.\nKBs have been used to investigate the interaction between structured facts and unstructured text. Generating textual templates that are filled\nby structured data is a common approach and has been used for conversational text (Han et al., 2015) and biographical text generation (Duma and Klein, 2013). Wikipedia has also been a popular resource for studying biography, including sentence harvesting and ordering (Biadsy et al., 2008), unsupervised discovery of distinct sequences of life events (Bamman and Smith, 2014) and fact extraction from text (Garera and Yarowsky, 2009). There has also been substantial work in generating from other structured KBs using template induction (Kondadadi et al., 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al., 2010).\nLebret et al. (2016) present the closest work to ours with a similar task using Wikipedia infoboxes in place of Wikidata. They condition an attentional neural language model (NLM) on local and global properties of infobox tables, including copy actions that allow wholesale insertion of values into generated text. They use 723k sentences from Wikipedia articles with 403k lower-cased words mapping to 1,740 distinct facts. They compare to a 5-gram language-model with copy actions, and find that the NLM has higher BLEU and lower perplexity than their baseline. In contrast, we utilise a deep recurrent model for input encoding, minimal slot value templating and greedy output decoding. We also explore a novel autoencoding objective that measures whether input facts can be re-created from the generated sentence.\nEvaluating generated text is challenging and no one metric seems appropriate to measure overall performance. Lebret et al. (2016) report BLEU scores (Papineni et al., 2002) which calculate the n-gram overlap between text produced by the system with respect to a human-written reference. Summarisation evaluations have concentrated on the content that is included in the summary, with semantic content typically extracted manually for comparison (Lin and Hovy, 2003; Nenkova and Passonneau, 2004). We draw from summarisation and generation to formulate a comprehensive evaluation based on automated metrics and human validation. Our final system comparison follows Kondadadi et al. (2013) in running a crowd task to collect pairwise preferences for evaluating and comparing both systems and references."}, {"heading": "3 Task and Data", "text": "We formulate the one-sentence biography generation task as shown in Figure 1. Input is a flat string representation of the structured data from the KB, comprising slot-value pairs (the subject being the topic of the KB record, e.g., Mathias Tuomi), ordered by slot frequency from most to least common. Output is a biography string describing the salient information in one sentence.\nWe validate the task and evaluation using a closely-aligned set of resources: Wikipedia and Wikidata. In addition to the KB maintenance issues discussed in the introduction, Wikipedia first sentences are of particular interest because they are clear and concise biographical summaries. These could be applied to entities outside Wikipedia for which one can obtain comparable parallel structured/textual data, e.g., movie summaries from IMDb, resume overviews from LinkedIn, product descriptions from Amazon.\nWe use snapshots of Wikidata (2015/07/13) and Wikipedia (2015/10/02) and batch process them to extract instances for learning. We select all entities that are INSTANCE OF human in Wikidata. We then use sitelinks to identify each entity\u2019s Wikipedia article text and NLTK (Bird et al., 2009) to tokenize and extract the lower-cased first sentence. This results in 1,268,515 raw knowledgetext pairs. The summary sentences can be long and the most frequent length is 21 tokens. We filter to\nonly include those between the 10th and 90th percentiles: 10 and 37 tokens. We split this collection into TRAIN, DEV and TEST collections with 80%, 10% and 10% of instances allocated respectively. Given the large variety of slots which may exist for an entity, we restrict the set of slots used to the top-15 by occurrence frequency. This criteria covers 72.8% of all facts. Table 1 shows the distribution of fact slots in the structured data and the percentage of time tokens from a fact value occur in the corresponding Wikipedia summary.\nAdditionally, some Wikidata entities remain underpopulated and do not contain sufficient facts to reconstruct a text summary. We control for this information mismatch by limiting our dataset to include only instances with at least 6 facts present. The final dataset includes 401,742 TRAIN, 50,017 DEV and 50,030 TEST instances. Of these instances, 95% contain 6 to 8 slot values while 0.1% contain the maximum of 10 slots. 51% of unique slot-value pairs expressed in TEST and DEV are not observed in TRAIN so generalisation of slot usage is required for the task. The KB facts give us an opportunity to measure the correctness of the generated text in a more precise way than text-to-text tasks. We use this for analysis in Section 7.3, driving insight into system characteristics and implications for use."}, {"heading": "3.1 Task complexity", "text": "Wikipedia first sentences exhibit a relatively narrow domain of language in comparison to other generation tasks such as translation. As such, it is not clear how complex the generation task is, and we first try to use perplexity to describe this.\nWe train both RNN models until DEV perplexity stops improving. Our basic sequence-to-sequence model (S2S) reaches perplexity of 2.82 on TRAIN and 2.92 on DEV after 15,000 batches of stochastic gradient descent. The autoencoding sequence-tosequence model (S2S+AE) takes longer to fit, but reaches a lower minimum perplexity of 2.39 on TRAIN and 2.51 on DEV after 25,000 batches.\nTo help ground perplexity numbers and understand the complexity of sentence biographies we train a benchmark language model and evaluate perplexity on DEV. Following Lebret et al. (2016), we build Kneser-Ney smoothed 5-gram language models using the KenLM toolkit (Heafield, 2011).\nTable 2 lists perplexity numbers for the benchmark LM models with different templating\nschemes on DEV. We observe decreasing perplexity for data with greater fact value templating. TITLE indicates templating of entity names only, while FULL indicates templating of all fact values by token index as described in Lebret et al. (2016). This shows that templating is an effective way to reduce the sparsity of a task, and that titles account for a large component of this.\nAlthough Lebret et al. (2016) evaluate on a different dataset, we are able to draw some comparisons given the similarity of our task. On their data, the benchmark LM baseline achieves a similar perplexity of 10.5 to ours when following their templating scheme on our dataset - suggesting both samples are of comparable complexity."}, {"heading": "4 Model", "text": "We model the task as a sequence-to-sequence learning problem. In this setting, a variable length input sequence of entity facts is encoded by a multi-layer RNN into a fixed-length distributed representation. This input representation is then fed into a separate decoder network which estimates a distribution over tokens as output. During training, parameters for both the encoder and decoder networks are optimized to maximize the likelihood of a summary sequence given an observed fact sequence.\nOur setting differs from the translation task in that the input is a sequence representation of structured data rather than natural human language. As described above in Section 3, we map Wikidata facts to a sequence of tokens that serves as input to the model as illustrated at the top of Figure 2. Experiments below demonstrate that this is sufficient for end-to-end learning in the generation task addressed here. To generate summaries, our model must both select relevant content and transform it into a well formed sentence. The decoder network includes an attention mechanism (Vinyals et al., 2015) to help facilitate accurate content selection. This allows the network to focus on different parts of the input sequence during inference."}, {"heading": "4.1 Sequence-to-sequence model (S2S)", "text": "To generate language, we seed the decoder network with the output of the encoder and a designated GO token. We then generate symbols greedily, taking the most likely output token from the decoder at each step given the preceding sequence until an EOS token is produced. This approach follows (Sutskever et al., 2014) who demonstrate a larger model with greedy sequence inference performs comparably to beam search. In contrast to translation, we might expect good performance on the summarization task where output summary sequences tend to be well structured and often formulaic. Additionally, we expect a partially-shared language across input and output. To exploit this, we use a tied embedding space, which allows both the encoder and decoder networks to share information about word meaning between fact values and output tokens.\nOur model uses a 3-layer stacked Gated Recurrent Unit RNN for both encoding and decoding, implemented using TensorFlow.1 We limit the shared vocabulary to 100,000 tokens with 256 dimensions for each token embedding and hidden layer. Less common tokens are marked as UNK, or unknown. To account for the long tail of entity names, we replace matches of title tokens with templated copy actions (e.g. TITLE0 TITLE1. . . ). These template are then filled after generation, as well as any initial unknown tokens in the output, which we fill with the first title token. We learn using minibatch Stochastic Gradient Descent with a batch size of 64 and a fixed learning rate of 0.5.\n1https://www.tensorflow.org, v0.8."}, {"heading": "4.2 S2S with autoencoding (S2S+AE)", "text": "One challenge for vanilla sequence-to-sequence models in this setting is the lack of a mechanism for constraining output sequences to only express those facts present in the data. Given a fact extraction oracle, we might compare facts expressed in the output sequence with those of the input and appropriately adjust the loss for each instance. While a forward-only model is only constrained to generate text sequences predicted by the facts, an autoencoding model is additionally constrained to generate text predictive of the input facts. In place of this ideal setting, we introduce a second sequence-to-sequence model which runs in reverse - re-encoding the text output sequence of the forward model into facts.\nThis closed-loop model is detailed in Figure 3. The resulting network is trained end-to-end to minimize both the input-to-output sequence loss L(x, y) and output-to-input reconstruction loss L(x, x\u2032). While gradients cannot propagate through the greedy forward decode step, shared parameters between the forward and backward network are fit to both tasks. To generate language at test time, the backward network does not need to be evaluated."}, {"heading": "5 Experimental methodology", "text": "The evaluation suite here includes standard baselines for comparison, automated metrics for learning, human judgement for evaluation and detailed analysis for diagnostics. While each are individually useful, their combination gives a comprehensive analysis of a complex problem space."}, {"heading": "5.1 Benchmarks", "text": "WIKI We use the first sentence from Wikipedia both as a gold standard reference for evaluating generated sentences, and as an upper bound in human preference evaluation.\nBASE Template-based systems are strong baselines, especially in human evaluation. While output may be stilted, the corresponding consistency can be an asset when consistency is important. We induce common patterns from the TRAIN set, replacing full matches of values with their slot and choosing randomly on ties. Multiple non-fact tokens are collapsed to a single symbol. A small sample of the most frequent patterns were manually examined to produce templates, roughly expressed as: TITLE, known as GIVEN NAME, (born DATE OF BIRTH in PLACE OF BIRTH; died DATE OF DEATH in PLACE OF DEATH) is an POSITION HELD and OCCUPATION from CITIZENSHIP, with some sensible back-offs where slots are not present, and rules for determiner agreement and is versus was where a death date is present. For example, ollie freckingham (born 12 november 1988) is a cricketer from the united kingdom. In total, there are 48 possible template variations."}, {"heading": "5.2 Metrics", "text": "BLEU We also report BLEU n-gram overlap with respect to the reference Wikipedia summary. With a large dev/test sets (10,000 sentences here), BLEU is a reasonable evaluation of generated content. However, it does not give an indication of wellformedness or readability. Thus we complement BLEU with a human preference evaluation.\nHuman preference We use crowd-sourced judgements to evaluate the relative quality of generated sentences and the reference Wikipedia first sentence. We obtain pairwise judgements, showing output from two different systems to crowd workers and asking each to give their binary preference. The system name mappings are anonymized and ordered pseudo-randomly. We request 3 judgements and dynamically increase this until we reach at least 70% agreement or a maximum of 5 judgements. We use CrowdFlower2 to collect judgements at the cost of 31 USD for all 6 pairwise combinations over 82\n2http://www.crowdflower.com\nrandomly selected entities. 67 workers contributed judgements to the test data task, each providing no more than 50 responses. We use the majority preference for each comparison. The CrowdFlower agreement is 80.7%, indicating that roughly 4 of 5 votes agree on average."}, {"heading": "5.3 Analysis of content selection", "text": "Finally, no system is perfect, and it can be challenging to understand the inherent difficulty of the problem space and the limitations of a system. Due to the limitations of the evaluation metrics mentioned above, we propose that manual annotation is important and still required for qualitative analysis to guide system improvement. The structured data in knowledge-to-text tasks allows us, if we can identify expressions of facts in text, cases where facts have been omitted, incorrectly mentioned, or expressed differently."}, {"heading": "6 Results", "text": ""}, {"heading": "6.1 Comparison against Wikipedia reference", "text": "Table 3 shows BLEU scores calculated over 10,000 entities sampled from DEV and TEST using the Wikipedia sentence as a single reference, using uniform weights for 1- to 4-grams, and padding sentences with fewer than 4 tokens. Scores are similar across DEV and TEST, indicating that the samples are of comparable difficulty. We evaluate significance using bootstrapped resampling with 1,000 samples. Each system result lies outside the 95% confidence intervals of other systems. BASE has reasonable scores at 21, with S2S higher at around 32, indicating that the model is at least able to generate closer text than the baseline. S2S+AE scores higher still at around 41, roughly double the baseline scores, indicating that the autoencoder is indeed able to constrain the model to generate better text."}, {"heading": "6.2 Human preference evaluation", "text": "Table 4 shows the results of our human evaluation over 82 entities sampled from TEST. For each\npair of systems, we show the percentage of entities where the crowd preferred A over B. Significant differences are annotated with \u2217 and \u2217\u2217 for p values < 0.05 and 0.01 using a one-way \u03c72 test. WIKI is uniformly preferred to any system, as is appropriate for an upper bound. The S2S model is the least-preferred with respect to WIKI. The S2S+AE model is more-preferred than the BASE and S2S models, by a larger margin for the latter. These results show that without autoencoding, the sequence-to-sequence model is less effective than a template-based system. Finally, although WIKI is more preferred than S2S+AE, the distributions are not significantly different, which we interpret as evidence that the model is able to generate good text from the human point-of-view, but autoencoding is required to do so."}, {"heading": "7 Analysis", "text": "While results presented above are encouraging and suggest that the model is performing well, they are not diagnostic in the sense that they can drive deeper insights into model strengths and weaknesses. While inspection and manual analysis is still required, we also leverage the structured factual data inherent to our task to perform quantitative as well as qualitative analysis."}, {"heading": "7.1 Fact Count", "text": "Figure 4 shows the effects of input fact count on generation performance. While more input facts give more information for the model to work with, longer inputs are also both rarer and more complex to encode. Interestingly, we observe the S2S+AE model maintains performance for more complex inputs while S2S performance declines."}, {"heading": "7.2 Example generated text", "text": "Table 5 shows some DEV entities and their summaries. The model learns interesting mappings: between numeric and string dates, and country de-\nmonyms. The model also demonstrates the ability to work around edge cases where templates fail, i.e. stripping parenthetical disambiguations (e.g. (actor)) and emitting the name Robert when the input is Bob. Output also suggests the model may perform inference across multiple facts to improve generation precision, e.g. describing an entity as english rather than british given information about both citizenship and place of birth. Unfortunately, the model can also infer unsubstantiated facts into the text (i.e. jazz drummer)."}, {"heading": "7.3 Content selection and hallucination", "text": "We randomly sample 50 entities from DEV and manually annotate the Wikipedia and system text. We note which fact slots are expressed as well as whether the expressed values are correct with respect to Wikidata. Given two sets of correctly extracted facts, we can consider one gold, one system and calculate set-based precision, recall and F1.\nWhat percentage of facts are used in the reference summaries? Firstly, to understand how Wikipedia editors select content for the first sentence of articles, we measure recall with the real facts as gold, and Wikipedia as system. Overall, the recall is 0.61 indicating that 61% of input facts are expressed in the reference summary from Wikipedia. The entity name (TITLE) is always expressed. Four slots are nearly always expressed when available: OCCUPATION (90%), DATE OF BIRTH (84%), CITIZENSHIP (81%), DATE OF DEATH (80%). Six slots are infrequently expressed in the analysis sample: PLACE OF BIRTH (33%), POSITION HELD (25%), PARTICIPANT OF (20%), POLITICAL PARTY (20%), EDUCATED AT (14%), SPORTS TEAM (9%). Two are never expressed explicitly: PLACE OF DEATH (0%), SEX OR GENDER (0%). AWARD RECEIVED and SPORT are not in the analysis sample.\nDo systems select the same facts found in the reference summaries? Table 6 shows content selection scores for systems with respect to the Wikipedia text as reference. This suggests that the autoencoding in S2S+AE helps increase fact recall without sacrificing precision. The template baseline also attains this higher recall, but at the cost of precision. For commonly expressed facts found in most person biographies, recall is over 0.95 (e.g., CITIZENSHIP, BIRTH DATE, DEATH DATE and OCCUPATION). Facts that are infrequently expressed are more difficult to select, with system F1 ranging from 0.00 to 0.50. Interestingly, macro-averaged F1 across infrequently expressed facts mirror human preference rather than BLEU results, with S2S+AE (0.26) > BASE (0.17) > S2S (0.07). However, all systems perform poorly on these facts and no reliable differences are observed.\nHow does autoencoding effect fact density? Interestingly, we observe that the autoencoding objective encourages the model to select more\nfacts (5.2 for S2S+AE vs. 4.5 for S2S), without increasing sentence length (19.1 vs. 19.7 tokens). BASE is similarly productive (5.1 facts) but wordier (21.2 tokens), while the WIKI reference produces both more facts (6.1) and longer sentences (23.7).\nDo systems hallucinate facts? To quantify the effect of hallucinated facts, we asses content selection scores of systems with respect to the input Wikidata relations (Table 7). Our best model achieves a precision of 0.93 with respect to Wikidata input. Notably, the template-driven baseline maintains a precision of 1.0 as it is constrained to emit Wikidata facts verbatim."}, {"heading": "8 Discussion and future work", "text": "Our experiments show that RNNs can generate biographic summaries from structured data, and that a secondary autoencoding objective is able to account for some of the information mismatch between input facts and target output sentences. In the future, we will explore whether results improve with explicit modelling of facts and conditioning of generation and autoencoding losses on slots. We expect this could benefit generation for diverse and noisy slot schemas like Wikipedia Infoboxes.\nAnother natural extension is to investigate the performance of the network running in reverse, from summary text back to facts. We plan to isolate the performance of the S2S+AE backward model when inferring facts and compare it to stan-\ndard relation extraction systems. Finally, similar RNN models have been applied extensively to language translation tasks. We plan to explore whether a joint model of machine translation and fact-driven generation can help populate KB entries for low-coverage languages by leveraging a shared set of facts."}, {"heading": "9 Conclusion", "text": "We present a neural model for mapping between structured and unstructured data, focusing on creating Wikipedia biographic summary sentences from Wikidata slot-value pairs. We introduce a sequence-to-sequence autoencoding RNN which improves upon base models by jointly learning to generate text and reconstruct facts. Our analysis of the task suggests evaluation in this domain is challenging. In place of a single score, we analyse statistical measures, human preference judgements and manual annotation to help characterise the task and understand system performance. In the human preference evaluation, our best model outperforms template baselines and is preferred 40% of the time to the gold standard Wikipedia reference.\nCode and data is available at https:// github.com/andychisholm/mimo."}, {"heading": "Acknowledgments", "text": "This work was supported by a Google Faculty Research Award (Chisholm) and an Australian Research Council Discovery Early Career Researcher Award (DE120102900, Hachey). Many thanks to reviewers for insightful comments and suggestions, and to Glen Pink, Kellie Webster, Art Harol and Bo Han for feedback at various stages."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["Gabor Angeli", "Percy Liang", "Dan Klein."], "venue": "Conference on Empirical Methods in Natural Language Processing, pages 502\u2013512.", "citeRegEx": "Angeli et al\\.,? 2010", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Unsupervised discovery of biographical structure from text", "author": ["David Bamman", "Noah A. Smith."], "venue": "Transactions of the Association for Computational Linguistics, 2:363\u2013376.", "citeRegEx": "Bamman and Smith.,? 2014", "shortCiteRegEx": "Bamman and Smith.", "year": 2014}, {"title": "An unsupervised approach to biography production using Wikipedia", "author": ["Fadi Biadsy", "Julia Hirschberg", "Elena Filatova."], "venue": "Annual Meeting of the Association for Computational Linguistics, pages 807\u2013815.", "citeRegEx": "Biadsy et al\\.,? 2008", "shortCiteRegEx": "Biadsy et al\\.", "year": 2008}, {"title": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Columbia University at DUC 2004", "author": ["Sasha Blair-Goldensohn", "David Evans", "Vasileios Hatzivassiloglou", "Kathleen McKeown", "Ani Nenkova", "Rebecca Passonneau", "Barry Schiffman", "Andrew Schlaikjer", "Advaith Siddharthan", "Sergey Siegelman."], "venue": "In", "citeRegEx": "Blair.Goldensohn et al\\.,? 2004", "shortCiteRegEx": "Blair.Goldensohn et al\\.", "year": 2004}, {"title": "Summarizing entity descriptions for effective and efficient human-centered entity linking", "author": ["Gong Cheng", "Danyun Xu", "Yuzhong Qu."], "venue": "International Conference on World Wide Web, pages 184\u2013 194.", "citeRegEx": "Cheng et al\\.,? 2015", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Semisupervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le."], "venue": "Annual Conference on Neural Information Processing Systems, pages 3079\u20133087.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Statistical acquisition of content selection rules for natural language generation", "author": ["Pablo Ariel Duboue", "Kathleen R McKeown."], "venue": "Conference on Empirical Methods in Natural Language Processing, pages 121\u2013128.", "citeRegEx": "Duboue and McKeown.,? 2003", "shortCiteRegEx": "Duboue and McKeown.", "year": 2003}, {"title": "Generating natural language from linked data: Unsupervised template extraction", "author": ["Daniel Duma", "Ewan Klein."], "venue": "International Conference on Computational Semantics, pages 83\u201394.", "citeRegEx": "Duma and Klein.,? 2013", "shortCiteRegEx": "Duma and Klein.", "year": 2013}, {"title": "Structural, transitive and latent models for biographic fact extraction", "author": ["Nikesh Garera", "David Yarowsky."], "venue": "Conference of the European Chapter of the Association for Computational Linguistics, pages 300\u2013308.", "citeRegEx": "Garera and Yarowsky.,? 2009", "shortCiteRegEx": "Garera and Yarowsky.", "year": 2009}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics, pages 1296\u20131306.", "citeRegEx": "Gillick et al\\.,? 2016", "shortCiteRegEx": "Gillick et al\\.", "year": 2016}, {"title": "Surface realisation from knowledge-bases", "author": ["Bikash Gyawali", "Claire Gardent."], "venue": "Annual Meeting of the Association for Computational Linguistics, pages 424\u2013434.", "citeRegEx": "Gyawali and Gardent.,? 2014", "shortCiteRegEx": "Gyawali and Gardent.", "year": 2014}, {"title": "Exploiting knowledge base to generate responses for natural language dialog listening agents", "author": ["Sangdo Han", "Jeesoo Bang", "Seonghan Ryu", "Gary Geunbae Lee."], "venue": "Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "KenLM: Faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Workshop on Statistical Machine Translation, pages 187\u2013197.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "A statistical NLG framework for aggregated planning and realization", "author": ["Ravi Kondadadi", "Blake Howald", "Frank Schilder."], "venue": "Annual Meeting of the", "citeRegEx": "Kondadadi et al\\.,? 2013", "shortCiteRegEx": "Kondadadi et al\\.", "year": 2013}, {"title": "Conceptto-text generation via discriminative reranking", "author": ["Ioannis Konstas", "Mirella Lapata."], "venue": "Annual Meeting of the Association for Computational Linguistics, pages 369\u2013378.", "citeRegEx": "Konstas and Lapata.,? 2012", "shortCiteRegEx": "Konstas and Lapata.", "year": 2012}, {"title": "Neural text generation from structured data with application to the biography domain", "author": ["R\u00e9mi Lebret", "David Grangier", "Michael Auli."], "venue": "Conference on Empirical Methods in Natural Language Processing, pages 1203\u20131213.", "citeRegEx": "Lebret et al\\.,? 2016", "shortCiteRegEx": "Lebret et al\\.", "year": 2016}, {"title": "Automatic evaluation of summaries using n-gram cooccurrence statistics", "author": ["Chin-Yew Lin", "Eduard Hovy."], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics, pages 71\u201378.", "citeRegEx": "Lin and Hovy.,? 2003", "shortCiteRegEx": "Lin and Hovy.", "year": 2003}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "What to talk about and how? Selective generation using LSTMs with coarse-to-fine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Mei et al\\.,? 2015", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "Evaluating content selection in summarization: The pyramid method", "author": ["Ani Nenkova", "Rebecca Passonneau."], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics, pages 145\u2013152.", "citeRegEx": "Nenkova and Passonneau.,? 2004", "shortCiteRegEx": "Nenkova and Passonneau.", "year": 2004}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Annual Meeting on Association for Computational Linguistics, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Expressing OWL axioms by english sentences: Dubious in theory, feasible in practice", "author": ["Richard Power", "Allan Third."], "venue": "International Conference on Computational Linguistics, pages 1006\u2013 1013.", "citeRegEx": "Power and Third.,? 2010", "shortCiteRegEx": "Power and Third.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Conference on Empirical Methods in Natural Language Processing, pages 379\u2013389.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Producing biographical summaries: Combining linguistic knowledge with corpus statistics", "author": ["Barry Schiffman", "Inderjeet Mani", "Kristian Concepcion."], "venue": "Annual Meeting of the Association for Computational Linguistics, pages 458\u2013465.", "citeRegEx": "Schiffman et al\\.,? 2001", "shortCiteRegEx": "Schiffman et al\\.", "year": 2001}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Annual Conference on Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc V. Le."], "venue": "ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Annual Conference on Neural Information Processing Systems, pages 2755\u20132763.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrk\u0161i\u0107", "PeiHao Su", "David Vandyke", "Steve Young."], "venue": "Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Sequence-based structured prediction for semantic parsing", "author": ["Chunyang Xiao", "Marc Dymetman", "Claire Gardent."], "venue": "Annual Meeting of the Association for Computational Linguistics, pages 1341\u2013 1350.", "citeRegEx": "Xiao et al\\.,? 2016", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Summarizing highly structured documents for effective search interaction", "author": ["Lanbo Zhang", "Yi Zhang", "Yunfei Chen."], "venue": "International Conference on Research and Development in Information Retrieval, pages 145\u2013154.", "citeRegEx": "Zhang et al\\.,? 2012", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "We treat this knowlege-to-text task like translation, using a recurrent neural network (RNN) sequence-to-sequence model (Sutskever et al., 2014) that learns to select and realise the most", "startOffset": 120, "endOffset": 144}, {"referenceID": 25, "context": "RNN sequence-to-sequence models (Sutskever et al., 2014) have driven various recent advances in natural language understanding.", "startOffset": 32, "endOffset": 56}, {"referenceID": 27, "context": ", flattening trees for parsing (Vinyals et al., 2015), predicting span types and lengths over byte input (Gillick et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 10, "context": ", 2015), predicting span types and lengths over byte input (Gillick et al., 2016) or flattening logical forms for semantic parsing (Xiao et al.", "startOffset": 59, "endOffset": 81}, {"referenceID": 29, "context": ", 2016) or flattening logical forms for semantic parsing (Xiao et al., 2016).", "startOffset": 57, "endOffset": 76}, {"referenceID": 26, "context": ", generating conversational responses (Vinyals and Le, 2015), abstractive summarisation (Rush et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 23, "context": ", generating conversational responses (Vinyals and Le, 2015), abstractive summarisation (Rush et al., 2015).", "startOffset": 88, "endOffset": 107}, {"referenceID": 28, "context": "that completely expresses a set of facts: restaurant recommendation text from dialogue acts (Wen et al., 2015), weather reports from sensor data and sports commentary from on-field events (Mei et al.", "startOffset": 92, "endOffset": 110}, {"referenceID": 19, "context": ", 2015), weather reports from sensor data and sports commentary from on-field events (Mei et al., 2015).", "startOffset": 85, "endOffset": 103}, {"referenceID": 6, "context": "Recent work explores unsupervised autoencoding objectives in sequence-to-sequence models, improving both text classification as a pretraining step (Dai and Le, 2015) and translation as a multitask objective (Luong et al.", "startOffset": 147, "endOffset": 165}, {"referenceID": 18, "context": "Recent work explores unsupervised autoencoding objectives in sequence-to-sequence models, improving both text classification as a pretraining step (Dai and Le, 2015) and translation as a multitask objective (Luong et al., 2016).", "startOffset": 207, "endOffset": 227}, {"referenceID": 24, "context": "Biographic summarisation has been extensively researched and is often approached as a sequence of subtasks (Schiffman et al., 2001).", "startOffset": 107, "endOffset": 131}, {"referenceID": 4, "context": "A version of the task was featured in the Document Understanding Conference in 2004 (Blair-Goldensohn et al., 2004) and other work learns policies for content selection without generating text (Duboue and McKeown, 2003; Zhang et al.", "startOffset": 84, "endOffset": 115}, {"referenceID": 7, "context": ", 2004) and other work learns policies for content selection without generating text (Duboue and McKeown, 2003; Zhang et al., 2012; Cheng et al., 2015).", "startOffset": 85, "endOffset": 151}, {"referenceID": 30, "context": ", 2004) and other work learns policies for content selection without generating text (Duboue and McKeown, 2003; Zhang et al., 2012; Cheng et al., 2015).", "startOffset": 85, "endOffset": 151}, {"referenceID": 5, "context": ", 2004) and other work learns policies for content selection without generating text (Duboue and McKeown, 2003; Zhang et al., 2012; Cheng et al., 2015).", "startOffset": 85, "endOffset": 151}, {"referenceID": 12, "context": "Generating textual templates that are filled by structured data is a common approach and has been used for conversational text (Han et al., 2015) and biographical text generation (Duma and Klein, 2013).", "startOffset": 127, "endOffset": 145}, {"referenceID": 8, "context": ", 2015) and biographical text generation (Duma and Klein, 2013).", "startOffset": 41, "endOffset": 63}, {"referenceID": 2, "context": "Wikipedia has also been a popular resource for studying biography, including sentence harvesting and ordering (Biadsy et al., 2008), unsupervised discovery of distinct sequences of life events (Bamman and Smith, 2014) and fact extraction from text (Garera and Yarowsky, 2009).", "startOffset": 110, "endOffset": 131}, {"referenceID": 1, "context": ", 2008), unsupervised discovery of distinct sequences of life events (Bamman and Smith, 2014) and fact extraction from text (Garera and Yarowsky, 2009).", "startOffset": 69, "endOffset": 93}, {"referenceID": 9, "context": ", 2008), unsupervised discovery of distinct sequences of life events (Bamman and Smith, 2014) and fact extraction from text (Garera and Yarowsky, 2009).", "startOffset": 124, "endOffset": 151}, {"referenceID": 14, "context": "There has also been substantial work in generating from other structured KBs using template induction (Kondadadi et al., 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al.", "startOffset": 102, "endOffset": 126}, {"referenceID": 22, "context": ", 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": ", 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al.", "startOffset": 82, "endOffset": 109}, {"referenceID": 15, "context": ", 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al.", "startOffset": 147, "endOffset": 173}, {"referenceID": 0, "context": ", 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al., 2010).", "startOffset": 239, "endOffset": 260}, {"referenceID": 21, "context": "(2016) report BLEU scores (Papineni et al., 2002) which calculate the n-gram overlap between text produced by the system with respect to a human-written reference.", "startOffset": 26, "endOffset": 49}, {"referenceID": 17, "context": "Summarisation evaluations have concentrated on the content that is included in the summary, with semantic content typically extracted manually for comparison (Lin and Hovy, 2003; Nenkova and Passonneau, 2004).", "startOffset": 158, "endOffset": 208}, {"referenceID": 20, "context": "Summarisation evaluations have concentrated on the content that is included in the summary, with semantic content typically extracted manually for comparison (Lin and Hovy, 2003; Nenkova and Passonneau, 2004).", "startOffset": 158, "endOffset": 208}, {"referenceID": 15, "context": "Lebret et al. (2016) report BLEU scores (Papineni et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Our final system comparison follows Kondadadi et al. (2013) in running a crowd task to collect pairwise preferences for evaluating and", "startOffset": 36, "endOffset": 60}, {"referenceID": 3, "context": "We then use sitelinks to identify each entity\u2019s Wikipedia article text and NLTK (Bird et al., 2009) to tokenize and extract the lower-cased first sentence.", "startOffset": 80, "endOffset": 99}, {"referenceID": 13, "context": "(2016), we build Kneser-Ney smoothed 5-gram language models using the KenLM toolkit (Heafield, 2011).", "startOffset": 84, "endOffset": 100}, {"referenceID": 15, "context": "Following Lebret et al. (2016), we build Kneser-Ney smoothed 5-gram language models using the KenLM toolkit (Heafield, 2011).", "startOffset": 10, "endOffset": 31}, {"referenceID": 16, "context": "TITLE indicates templating of entity names only, while FULL indicates templating of all fact values by token index as described in Lebret et al. (2016). This shows that templating is an effective way to reduce the sparsity of a task, and that titles account for a large component of this.", "startOffset": 131, "endOffset": 152}, {"referenceID": 16, "context": "Although Lebret et al. (2016) evaluate on a different dataset, we are able to draw some comparisons given the similarity of our task.", "startOffset": 9, "endOffset": 30}, {"referenceID": 27, "context": "The decoder network includes an attention mechanism (Vinyals et al., 2015) to help facilitate accurate content selection.", "startOffset": 52, "endOffset": 74}, {"referenceID": 25, "context": "This approach follows (Sutskever et al., 2014) who demonstrate a", "startOffset": 22, "endOffset": 46}], "year": 2017, "abstractText": "We investigate the generation of onesentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. The model achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the model is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.", "creator": "LaTeX with hyperref package"}}}