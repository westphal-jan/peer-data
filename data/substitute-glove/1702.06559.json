{"id": "1702.06559", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Active One-shot Learning", "abstract": "Recent advancing next one - second vocational have produced models still can find began a throughout of wrongly examples, to conscious classification bringing numerical tasks. This own combines adequate skills their having - shot learning, allowing similar model to wait, during classification, has contain though worth labeling. We implemented took formula focus in which a stream only images people presented by, followed each end demands, way proposal must can giving to way predict an signature can pay to receive the correct re-release. We no of inflammation progenitor phone limited action - value function, few demonstrate its enable their working how having before they whether labels. Through the would of reward specific, into vision make achieve that 0.5 prediction completeness longer when exception integrated on now purely supervised needed, do trade data exact the fewer original requests.", "histories": [["v1", "Tue, 21 Feb 2017 19:31:46 GMT  (279kb)", "http://arxiv.org/abs/1702.06559v1", "NIPS 2016, Deep Reinforcement Learning Workshop, Barcelona, Spain. Seethis https URLfor the poster and a short video description of the paper"]], "COMMENTS": "NIPS 2016, Deep Reinforcement Learning Workshop, Barcelona, Spain. Seethis https URLfor the poster and a short video description of the paper", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mark woodward", "chelsea finn"], "accepted": false, "id": "1702.06559"}, "pdf": {"name": "1702.06559.pdf", "metadata": {"source": "CRF", "title": "Active One-shot Learning", "authors": ["Mark Woodward"], "emails": ["mwoodward@cs.stanford.edu", "cbfinn@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n06 55\n9v 1\n[ cs\n.L G\n] 2\n1 Fe\nb 20\nRecent advances in one-shot learning have produced models that can learn from a handful of labeled examples, for passive classification and regression tasks. This paper combines reinforcement learning with one-shot learning, allowing the model to decide, during classification, which examples are worth labeling. We introduce a classification task in which a stream of images are presented and, on each time step, a decision must be made to either predict a label or pay to receive the correct label. We present a recurrent neural network based action-value function, and demonstrate its ability to learn how and when to request labels. Through the choice of reward function, the model can achieve a higher prediction accuracy than a similar model on a purely supervised task, or trade prediction accuracy for fewer label requests."}, {"heading": "1 Introduction", "text": "Active learning, a special case of semi-supervised learning, is an approach for reducing the amount of supervision needed for performing a task, by having the model select which datapoints should be labeled. Active learning is particularly useful in domains where there is some large cost associated with making a mistake and another, typically smaller, cost associated with requesting a label. Applications have included domains where human or computer safety is at risk, such as cancer classification [1], malware detection [2], and autonomous driving [3, 4]. Methods for active learning involve strategies such as selecting the data points for which the model is the most uncertain, or picking examples which are expected to be the most informative. However, these methods often use heuristics for handling the computational complexity of approximating the risk associated with selection. Can we instead replace these heuristics with learning?\nIn this work, we combine meta-learning with reinforcement learning to learn an active learner. In particular, we consider the online setting of active learning, where an agent is presented with examples in a sequence, and must choose whether to label the example or request the true label. Extending the recently proposed one-shot learning approach by Santoro et al. [5], we develop a method for training a deep recurrent model to make labeling decisions. Unlike prior one-shot learning approaches which use supervised learning, we treat the model as a policy with actions that include labeling and requesting a label, and train the policy with reinforcement learning. As a result, our trained model can make effective decisions with only a few labeled examples.\nOur primary contribution is to present a method for learning an active learner using deep reinforcement learning. We evaluate our method on an Omniglot image classification task. Our preliminary results show that our proposed model can learn from only a handful of requested labels and can effectively trade off prediction accuracy with reduced label requests via the choice of reward function. To the best of our knowledge, our model is the first application of reinforcement learning with deep recurrent models to the task of active learning.\nNIPS 2016, Deep Reinforcement Learning Workshop, Barcelona, Spain."}, {"heading": "2 Related Work", "text": "Active learning deals with the problem of choosing an example, or examples, to be labeled from a set of unlabeled examples [6]. We consider the setting of single pass active learning, in which a decision must be made on examples as they are pulled from a stream. Generally, methods for doing so have relied on heuristics such as similarity metrics between the current example and examples seen so far [7], or uncertainty measures in the label prediction [7, 8]. The premise of active learning is that there are costs associated with labeling and with making an incorrect prediction. Reinforcement learning allows for the explicit specification of those costs, and directly finds a labelling policy to optimize those costs. Thus, we believe that reinforcement learning is a natural fit for active learning. We use a deep recurrent neural network function approximator for representing the actionvalue function. While there have been numerous applications of deep neural networks to the related problem of semi-supervised learning [9, 10], the application of deep learning to active learning problems has been limited [11].\nOur model is very closely related to recent approaches to meta-learning and one-shot learning. Metalearning has been successfully applied to supervised learning tasks [5, 12], with key insights being training on short episodes with few class examples and randomizing the labels and classes in the episode. We propose to combine such approaches for one-shot learning with reinforcement learning, to learn an agent that can make labelling decisions online. The task and model we propose is most similar to the model proposed by Santoro et al. [5], in which the model must predict the label for a new image at each time step, with the true label received, as input, one time step later. We extend their task to the active learning domain by withholding the true label, unless the model requests it, and training the model with reinforcement learning, rewarding accurate predictions and penalizing incorrect predictions and label requests. Thus, the model must learn to consider its own uncertainty before making a prediction or requesting the true label."}, {"heading": "3 Preliminaries", "text": "We will now briefly review reinforcement learning as a means of introducing notation. Reinforcement learning aims to learn a policy that maximizes the expected sum of discounted future rewards. Let \u03c0(st) be a policy which takes a state, st, and outputs an action, at at time t. One way to represent the optimal policy, \u03c0\u2217(st), is as the action that maximizes the optimal action-value function, Q\u2217(st, at), which specifies the expected sum of discounted future rewards for taking action at in state st and acting optimally from then on:\nat = \u03c0 \u2217(st) = argmax\nat\nQ\u2217(st, at). (1)\nThe following Bellman equation, for action-value functions, holds for the optimalQ\u2217(st, at):\nQ\u2217(st, at) = Est+1 [rt + \u03b3max at+1 Q\u2217(st+1, at+1)], (2)\nwhere rt is the reward received after taking action at in state st, and \u03b3 is the discount factor for future rewards.\nMany reinforcement learning algorithms use a function approximator for representingQ(st, at) and optimize its parameters by minimizing the Bellman error, which can be derived from Equation 2 as the following:\nL(\u0398) := \u2211\nt\n[Q\u0398(ot, at)\u2212 (rt + \u03b3max at+1\nQ\u0398(ot+1, at+1))] 2, (3)\nwhere \u0398 are the parameters of the function approximator, and the algorithm receives observations ot, such as images, rather than states st. This is the equation that we optimize in the experiments below. We use a neural network to represent Q, which we optimize via stochastic gradient descent. Note that in the experiments we do not use a separate target network as in Mnih et al. [13].\n(yt, xt+1) or (~0, xt+1)\n[~0, 1] or [y\u0302t, 0]\n(~0, x0)\nrt\nbbbb b b\nsamples classes\nShuffle: labels\n(a) Task structure\n(yt, xt+1)\nrt = \u22120.05\n[~0, 1]\n(b) Request label\n[y\u0302t, 0]\nrt =\n{\n+1, if y\u0302t = yt\n\u22121, o.w.\n(~0, xt+1)\n(c) Predict label\nFigure 1: Task structure. (a) An Omniglot image, xt, is presented at each step of the episode. The model outputs a one-hot vector of length c + 1, where c is the number of classes per episode. (b) The model can request the label for xt by setting the final bit of the output vector. The reward, rt, is \u22120.05 for a label request. The true label, yt, for xt is then provided at the next time step along with xt+1. (c) Alternatively, the model can make a prediction by setting one of the first c bits of the output vector, designated y\u0302t. rt is +1 if the prediction is correct or\u22121 if not. If a prediction is made at time t, then no information about yt is supplied at time t+ 1. (a) For each episode, the classes to be presented, their labels, and the specific samples are all shuffled."}, {"heading": "4 Task Methodology", "text": "Similar to recent work on one-shot learning [5, 12], we train with short episodes and a few examples per class, varying classes and randomizing labels between episodes. The intuition is that we want to delegate episode specific details to activations and fit the model weights to the general meta-task of learning to label.\nFigure 1 describes the meta-learning task addressed in this paper. The model receives an image, xt, at each time step of the episode and may either predict the label of that image or request the label for that image. If the label is requested then the true label, yt, is included along with the next image xt+1 in the next observation ot+1. The action, at, is a one-hot vector consisting of the optionally predicted label, y\u0302t, followed by a bit for requesting the label. Since only one bit can be set, the model can either make a label prediction or request the label. If a prediction is made, and thus no label is requested, a zero-vector,~0, is included in the next observation instead of the true label.\nOn each time step, one of three rewards is given: Rreq , for requesting the label, Rcor, for correctly predicting the a label, or Rinc, for incorrectly predicting the label.\nrt =\n\n\n\nRreq, if a label is requested Rcor, if predicting and y\u0302t = yt Rinc, if predicting and y\u0302t 6= yt\n(4)\nThe objective is to maximize the sum of rewards received during the episode.\nThe optimal strategy involves maintaining a set of class representations and their corresponding labels, in memory. Then, upon receiving a new image xt, the optimal strategy is to compare the representation for xt to the existing class representations, weighing the uncertainty of a match along with the cost of being incorrect, correct, or requesting a label, and either retrieving and outputting the stored label or requesting a new label. If the model believes xt to be an instance of a new class, then a class representation must be stored, the label must be requested, and the response must be stored and associated with the class representation."}, {"heading": "5 Reinforcement Learning Model", "text": "Our action-value function, Q(ot, at), is a long short-term memory (LSTM) [14], connected to a linear output layer. Q(ot) outputs a vector, where each element corresponds to an action, similar to the DQN model [13]:\nQ(ot, at) = Q(ot) \u00b7 at (5)\nQ(ot) = W hqht + b q (6)\nwhere ht is the output from the LSTM, W hq are the weights mapping from the LSTM output to action-values, and bq is the action-value bias. We use a basic LSTM with equations:\ng\u0302f , g\u0302i, g\u0302o, c\u0302t = W oot +W hht\u22121 + b (7)\ngf = \u03c3(g\u0302f ) (8)\ngi = \u03c3(g\u0302i) (9) go = \u03c3(g\u0302o) (10)\nct = g f\u2299 ct\u22121 + g i\u2299 tanh(c\u0302t) (11) ht = g o\u2299 tanh(ct) (12)\nwhere g\u0302f , g\u0302i, g\u0302o are the forget gates, input gates, and output gates respectively, c\u0302t is the candidate cell state, and ct is the new LSTM cell state. W\no and Wh are the weights mapping from the observation and hidden state, respectively, to the gates and candidate cell state, and b is the bias vector. \u2299 represents element-wise multiplication. \u03c3(\u00b7) and tanh(\u00b7) are the sigmoid and hyperbolic tangent functions respectively."}, {"heading": "6 Experimental Results", "text": "We evaluate our proposed one-shot learning model in an active-learning set-up for image classification. Our goal with the following experiments is to determine 1) whether or not the proposed model can learn, through reinforcement, how to label examples and when to instead request a label, and 2) whether or not the model is effectively reasoning about uncertainty when making its predictions."}, {"heading": "6.1 Setup", "text": "We used the Omniglot dataset in all experiments [15]. Omniglot contains 1,623 classes of characters from 50 different alphabets, with 20 hand drawn examples per class, giving 32,460 total examples. We randomly split the classes into 1,200 training classes, and 423 test classes. Images were normalized to a pixel value between 0.0 and 1.0 and resized to 28x28 pixels.\nEach episode consisted of 30 Omniglot images sampled randomly from 3 randomly sampled classes, without replacement. Note that the number of samples from each class may not have been equal. For each class in the episode, a random rotation in {0\u25e6, 90\u25e6, 180\u25e6, 270\u25e6} was selected and applied to all samples from that class. The images were then flattened to 784 dimensional vectors, giving xt. Each of the three sampled classes in the episode was randomly assigned a slot in a one-hot vector of length three, giving yt. Each training step consisted of a batch of 50 episodes.\nUnless otherwise specified, the rewards were: Rcor = +1,Rinc = \u22121, andRreq = \u22120.05. Epsilongreedy exploration was used for actions selection during training, with \u01eb = 0.05. If exploring, either the correct label, a random incorrect label, or the \u201crequest label\u201d action was chosen, each with probability 1/3. The discount factor, \u03b3, was set to 0.5. We used an LSTM with 200 hidden units to representQ. The weights of the model were trained using Adam with the default parameters [16]."}, {"heading": "6.2 Results", "text": "We now present the results of our method. During training, for each episode within a training batch, the time steps containing the 1st, 2nd, 5th, and 10th instances of all classes are identified. Figure 2a shows the percentage of label requests for each of these steps, as training progressed; whereas Figure 2b shows the percentage of actions corresponding to correct label predictions. Thus, we treat label requests as incorrect label predictions in this analysis. As seen in the plot, the model learns to\n1 2 3 4 5 6 7 8 9 10 11 Time Step\n0.0 0.2 0.4 0.6 0.8 1.0 % L ab el R eq ue st s\nFirst Class Second Class\n1 2 3 4 5 6 Time Step\n0.0 0.2 0.4 0.6 0.8 1.0 % L ab el R eq ue st s\nFirst Class Second Class\n(a) Switch classes after 5\n1 2 3 4 5 6 7 8 9 10 11 Time Step\n0.0 0.2 0.4 0.6 0.8 1.0 % L ab el R eq ue st s\nFirst Class Second Class\n(b) Switch classes after 10\nmake more label requests for early instances of a class, and fewer for later instances. Correspondingly, the model is more accurate on later instances of a class. After the 100,000th episode batch, training is ceased and the model is no longer updated. After 100,000 episode batches, 10,000 more episode batches were run on held-out classes from the test set.\nOne naive strategy that the model could use to attempt this task would be to learn a fixed time step, where it would switch from requesting labels to predicting labels. In comparison, an optimal policy would consider the model\u2019s uncertainty of the label when deciding to request a label. To explore whether the model was using a naive strategy or effectively reasoning about its own uncertainty, we performed a second experiment using the trained model (from batch 100,000). In this experiment, two classes were selected at random and the model was presented with either 5 examples of the first class followed by 1 example of the second class, or 10 examples of the first class followed by 1 example of the second class. In both cases, we ran 1,000 episodes. For each time step, the percentage of episodes in which the model requested a label were recorded. If the model is using its uncertainty effectively, we should see high percentages of label requests on the first time step and on time-step when the first instance of the second class is seen. Alternatively, if uncertainty is not being used, we should see something simple such as high percentages of label requests on the first 3 time steps, followed by low percentages of label requests. As shown in Figure 3, the percentage of label requests is high on time step 1 and it is high and, notably, equal on the 6th or 11th time across the scenarios. These results are consistent with the model making use of its uncertainty. The differences in label request frequency at step 6 between Fig. 3a and Fig. 3b show that the model is not following an episode independent label request schedule. The gradual increase in label requests before the 2nd class instance could suggest that the model is losing faith in its belief of the distribution of samples within a class, since a sequence of 5 or 10 images of the same class was rare at training time; further experiments are needed to confirm this.\nFinally, through the choice of rewards, we should be able to make trade-offs between high prediction accuracy with many label requests and few label requests but lower prediction accuracy. To explore this we trained several models on the task in figure 1 using different values of Rinc, the reward for an incorrect prediction. We trained 100,000 episode batches and then evaluated the models on episodes containing classes from the test set. For consistency of convergence, Rinc = \u221210 and Rinc = \u221220 were trained with a batch size of 100. Table 1 shows the results and confirms that the proposed model can smoothly make this trade-off. A supervised learning model was also evaluated, as introduced in Santoro et al. [5], where the loss is the cross entropy between the predicted and true label, and the true label is always presented on the following time step. For consistency, we used our same LSTM model for this supervised task, with the modifications of a softmax on the output and not outputting the extra bit for the \u201crequest label\u201d action. With the RL model we are able to achieve a higher prediction accuracy than the overall accuracy for the supervised task, while using 60.6% of the labels at test time; the supervised task effectively requests labels 100% of the time."}, {"heading": "7 Discussion & Future Work", "text": "We presented a method for learning an active learner via reinforcement learning. Our results demonstrate that the model can learn from only a handful of requested labels and can effectively trade off accuracy for reduced label requests, via the choice of reward. Additionally, our results suggest that the model learns to effectively reason about its uncertainty when making its decision.\nAn important direction for future work is to expand the experiments by increasing the complexity of the task. Natural extensions include increasing the number of classes per episode and experimenting with more complex datasets such as ImageNet [17]. Note that the reinforcement learning method used in our experiments was quite simplistic. By using more powerful RL strategies such as better exploration [18], a separate target network [13], or decomposing the action-value function [19], we expect our model to scale to more complex tasks. A more expressive model such as memory augmentation could also help [20]; though, we found that a neural turing machine with the \u201cleast recently used\u201d addressing module [5] overfit to this task much more than an LSTM. Existing and future results should, where possible, be compared with prior methods for active learning."}, {"heading": "8 Acknowledgements", "text": "The authors would like to thank Adam Santoro for helpful correspondence regarding the models in Santoro et al.[5]."}], "references": [{"title": "Active learning with support vector machine applied to gene expression data for cancer classification", "author": ["Ying Liu"], "venue": "Journal of chemical information and computer sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1941}, {"title": "Active learning to improve the detection of unknown computer worms activity", "author": ["Robert Moskovitch", "Nir Nissim", "Roman Englert", "Yuval Elovici"], "venue": "In Information Fusion,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Reducing supervisor burden in dagger using support vectors for efficient learning from demonstrations in high dimensional state spaces", "author": ["Michael Laskey", "Sam Staszak", "Wesley Yu-Shu Hsieh", "Jeffrey Mahler", "Florian T Pokorny", "Anca D Dragan", "Ken Goldberg. Shiv"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Query-efficient imitation learning for end-to-end autonomous driving", "author": ["Jiakai Zhang", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1605.06450,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "One-shot learning with memory-augmented neural networks", "author": ["Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy P. Lillicrap"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Active learning literature survey", "author": ["Burr Settles"], "venue": "Computer Sciences Technical Report 1648,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Single-pass active learning with conflict and ignorance", "author": ["Edwin Lughofer"], "venue": "Evolving Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Unbiased online active learning in data streams", "author": ["Wei Chu", "Martin Zinkevich", "Lihong Li", "Achint Thomas", "Belle Tseng"], "venue": "In Proceedings of the Seventeenth ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD-11),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Semisupervised learning with deep generative models", "author": ["Diederik P. Kingma", "Danilo Jimenez Rezende", "Shakir Mohamed", "Max Welling"], "venue": "In Proc. of the Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Semisupervised learning with ladder network", "author": ["Antti Rasmus", "Harri Valpola", "Mikko Honkala", "Mathias Berglund", "Tapani Raiko"], "venue": "In Proc. of the Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["Anders Krogh", "Jesper Vedelsby"], "venue": "Advances in neural information processing systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Matching networks for one shot learning", "author": ["Oriol Vinyals", "Charles Blundell", "Timothy P. Lillicrap", "Koray Kavukcuoglu", "Daan Wierstra"], "venue": "In Proc. of the Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller"], "venue": "In Proc. of the Conference on Neural Information Processing Systems (NIPS), Workshop on Deep Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Brendan M Lake", "Ruslan Salakhutdinov", "Joshua B Tenenbaum"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proc. of the International Conference for Learning Representations (ICLR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In Proc. of the Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Tom Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot", "Nando de Freitas"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Applications have included domains where human or computer safety is at risk, such as cancer classification [1], malware detection [2], and autonomous driving [3, 4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "Applications have included domains where human or computer safety is at risk, such as cancer classification [1], malware detection [2], and autonomous driving [3, 4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Applications have included domains where human or computer safety is at risk, such as cancer classification [1], malware detection [2], and autonomous driving [3, 4].", "startOffset": 159, "endOffset": 165}, {"referenceID": 3, "context": "Applications have included domains where human or computer safety is at risk, such as cancer classification [1], malware detection [2], and autonomous driving [3, 4].", "startOffset": 159, "endOffset": 165}, {"referenceID": 4, "context": "[5], we develop a method for training a deep recurrent model to make labeling decisions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Active learning deals with the problem of choosing an example, or examples, to be labeled from a set of unlabeled examples [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "Generally, methods for doing so have relied on heuristics such as similarity metrics between the current example and examples seen so far [7], or uncertainty measures in the label prediction [7, 8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 6, "context": "Generally, methods for doing so have relied on heuristics such as similarity metrics between the current example and examples seen so far [7], or uncertainty measures in the label prediction [7, 8].", "startOffset": 191, "endOffset": 197}, {"referenceID": 7, "context": "Generally, methods for doing so have relied on heuristics such as similarity metrics between the current example and examples seen so far [7], or uncertainty measures in the label prediction [7, 8].", "startOffset": 191, "endOffset": 197}, {"referenceID": 8, "context": "While there have been numerous applications of deep neural networks to the related problem of semi-supervised learning [9, 10], the application of deep learning to active learning problems has been limited [11].", "startOffset": 119, "endOffset": 126}, {"referenceID": 9, "context": "While there have been numerous applications of deep neural networks to the related problem of semi-supervised learning [9, 10], the application of deep learning to active learning problems has been limited [11].", "startOffset": 119, "endOffset": 126}, {"referenceID": 10, "context": "While there have been numerous applications of deep neural networks to the related problem of semi-supervised learning [9, 10], the application of deep learning to active learning problems has been limited [11].", "startOffset": 206, "endOffset": 210}, {"referenceID": 4, "context": "Metalearning has been successfully applied to supervised learning tasks [5, 12], with key insights being training on short episodes with few class examples and randomizing the labels and classes in the episode.", "startOffset": 72, "endOffset": 79}, {"referenceID": 11, "context": "Metalearning has been successfully applied to supervised learning tasks [5, 12], with key insights being training on short episodes with few class examples and randomizing the labels and classes in the episode.", "startOffset": 72, "endOffset": 79}, {"referenceID": 4, "context": "[5], in which the model must predict the label for a new image at each time step, with the true label received, as input, one time step later.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Similar to recent work on one-shot learning [5, 12], we train with short episodes and a few examples per class, varying classes and randomizing labels between episodes.", "startOffset": 44, "endOffset": 51}, {"referenceID": 11, "context": "Similar to recent work on one-shot learning [5, 12], we train with short episodes and a few examples per class, varying classes and randomizing labels between episodes.", "startOffset": 44, "endOffset": 51}, {"referenceID": 13, "context": "Our action-value function, Q(ot, at), is a long short-term memory (LSTM) [14], connected to a linear output layer.", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Q(ot) outputs a vector, where each element corresponds to an action, similar to the DQN model [13]:", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "We used the Omniglot dataset in all experiments [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "The weights of the model were trained using Adam with the default parameters [16].", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "[5], where the loss is the cross entropy between the predicted and true label, and the true label is always presented on the following time step.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Natural extensions include increasing the number of classes per episode and experimenting with more complex datasets such as ImageNet [17].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "By using more powerful RL strategies such as better exploration [18], a separate target network [13], or decomposing the action-value function [19], we expect our model to scale to more complex tasks.", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "By using more powerful RL strategies such as better exploration [18], a separate target network [13], or decomposing the action-value function [19], we expect our model to scale to more complex tasks.", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "By using more powerful RL strategies such as better exploration [18], a separate target network [13], or decomposing the action-value function [19], we expect our model to scale to more complex tasks.", "startOffset": 143, "endOffset": 147}, {"referenceID": 4, "context": "A more expressive model such as memory augmentation could also help [20]; though, we found that a neural turing machine with the \u201cleast recently used\u201d addressing module [5] overfit to this task much more than an LSTM.", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Recent advances in one-shot learning have produced models that can learn from a handful of labeled examples, for passive classification and regression tasks. This paper combines reinforcement learning with one-shot learning, allowing the model to decide, during classification, which examples are worth labeling. We introduce a classification task in which a stream of images are presented and, on each time step, a decision must be made to either predict a label or pay to receive the correct label. We present a recurrent neural network based action-value function, and demonstrate its ability to learn how and when to request labels. Through the choice of reward function, the model can achieve a higher prediction accuracy than a similar model on a purely supervised task, or trade prediction accuracy for fewer label requests.", "creator": "LaTeX with hyperref package"}}}