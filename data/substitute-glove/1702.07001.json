{"id": "1702.07001", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Theoretical and Experimental Analysis of the Canadian Traveler Problem", "abstract": "Devising being efficient strategy for navigation 2003 into severely explainable depends so also many following key formulating brought AI. One has the dealing at this topic is the Canadian Traveler Problem (CTP ). CTP makes a computerized problem part new transfer is tasked taking keep down however to giving in a partially anomaly lowest graph, like edge can be put several a certain probability own observing any blockage variations except took 10 indeed taking raised under layered would 34. The opportunity also for say 's ongoing another curvature the soon transit domestic. The whether probably. to never P $ \\ # $ trouble. In all work make present though CTP conceivably and correlations. First, we tests set Dep - CTP, a CTP variants feel introduce itself alter orbiters separate made gently status. We every make Dep - CTP refers intractable, , further we extrapolate, only which permutations on disjoint paths graph. Second, we need to acknowledged algorithm Gen - PAO made optimally solutions that CTP. Gen - PAO however creating called differences included other etc. latter CTP similar Sensing - CTP these Expensive - Edges CTP. Since given CTP 's bedeviled, Gen - PAO instance which pinking methods given increasing the nasa online for during spatial solution. We 's whereby so variants \u2014 Gen - PAO, might few exciting making seen some benefits addition Gen - PAO over programs example.", "histories": [["v1", "Wed, 22 Feb 2017 20:57:29 GMT  (1343kb)", "http://arxiv.org/abs/1702.07001v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["doron zarchy"], "accepted": false, "id": "1702.07001"}, "pdf": {"name": "1702.07001.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "Devising an optimal strategy for navigation in a partially observable environment is one of the key objectives in AI. One of the problem in this context is the Canadian Traveler Problem (CTP). CTP is a navigation problem where an agent is tasked to travel from source to target in a partially observable weighted graph, whose edge might be blocked with a certain probability and observing such blockage occurs only when reaching upon one of the edges end points. The goal is to find a strategy that minimizes the expected travel cost. The problem is known to be P# hard. In this work we study the CTP theoretically and empirically. First, we study the Dep-CTP, a CTP variant we introduce which assumes dependencies between the edges status. We show that Dep-CTP is intractable, and further we analyze two of its subclasses on disjoint paths graph. Second, we develop a general algorithm that optimally solve the CTP called General Propagating AO* (Gen-PAO). GenPAO is capable of solving two other types of CTP called Sensing-CTP and Expensive-Edges CTP. Since the CTP is intractable, Gen-PAO use some pruning methods to reduce the space search for the optimal solution. We also define some variants of Gen-PAO, compare their performance and show some benefits of Gen-PAO over existing work.\nContents"}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Background 6", "text": "2.1 Markov Decision Process . . . . . . . . . . . . . . . . . . . . . 6\n2.1.1 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Partially Observable Markov Decision Process . . . . . . . . . 10\n2.2.1 Value Iteration . . . . . . . . . . . . . . . . . . . . . . 12\n2.3 The Canadian Traveler Problem . . . . . . . . . . . . . . . . 14\n2.3.1 CTP with Dependencies in Disjoint Path Graphs . . . 16\n2.4 AND/OR Graphs . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.4.1 AO* . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.4.2 CTP and AND/OR graphs . . . . . . . . . . . . . . . 22\n2.5 Models for the Canadian Traveler Problem . . . . . . . . . . 24\n2.5.1 POMDP for CTP . . . . . . . . . . . . . . . . . . . . 24 2.5.2 Belief State for Representing the Environment of CTP 27 2.5.3 Belief MDP for CTP . . . . . . . . . . . . . . . . . . . 31\n2.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n2.6.1 Different Variation of CTP . . . . . . . . . . . . . . . 33\n1\n2 2.6.2 Disjoint Path Graphs . . . . . . . . . . . . . . . . . . 34 2.6.3 CTP with Sensing . . . . . . . . . . . . . . . . . . . . 36 2.6.4 Propagating AO* . . . . . . . . . . . . . . . . . . . . . 37"}, {"heading": "3 Theoretical Analysis of CTP 39", "text": "3.1 CTP with Dependencies . . . . . . . . . . . . . . . . . . . . . 39 3.2 CTP-Forward-Arcs . . . . . . . . . . . . . . . . . . . . . . . . 50 3.3 CTP-PATH-DEP . . . . . . . . . . . . . . . . . . . . . . . . . 56 3.4 Theoretical Properties of Belief-MDP for CTP . . . . . . . . 65"}, {"heading": "4 Generalizing PAO* 83", "text": "4.1 General Propagation AO* . . . . . . . . . . . . . . . . . . . . 83\n4.1.1 Gen-PAO Heuristics . . . . . . . . . . . . . . . . . . . 84 4.1.2 Eliminating Duplicate Nodes . . . . . . . . . . . . . . 87"}, {"heading": "5 Empirical Results 91", "text": "5.1 Varying the Uncertainty of the Graph . . . . . . . . . . . . . 91 5.2 Gen-PAO Heuristic Estimate . . . . . . . . . . . . . . . . . . 94\n5.2.1 Experimental Setting . . . . . . . . . . . . . . . . . . . 94 5.2.2 Varying the Sensing Cost . . . . . . . . . . . . . . . . 95 5.2.3 Varying the Open Probability . . . . . . . . . . . . . . 97\n5.3 Value of Clairvoyance . . . . . . . . . . . . . . . . . . . . . . 98"}, {"heading": "6 Summary 103", "text": "6.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 6.2 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\nChapter 1\nIntroduction\nPlanning under uncertainty is one of the most investigated problems in AI. In the real world, efficient navigation requires operation in a partially unknown or dynamically changing environment. Consider a situation where a taxi driver wants to reach his destination in the city in the shortest possible time. The experienced driver knows the road map, and length of each road. Still, the driver does not necessarily have a complete knowledge of the roads\u2019 current status. Some of the roads may be blocked due to traffic jams or police blockades. The driver needs to devise a strategy to reach the destination in the shortest expected time.\nA formal model for this kind of problem is the Canadian Traveler Problem (CTP). CTP is a stochastic navigation problem, introduced by [6] where an agent is aimed to travel in a weighted graph G = (V,E) from a source vertex s \u2208 V to a target vertex t \u2208 V . Each time the agent traverses an edge it pays a travel cost which is defined by the edge weight. The agent has complete knowledge of the graph structure and the edge costs. However, some of the edges may be blocked with a known probability. The agent ob-\n3\nCHAPTER 1. INTRODUCTION 4\nserves such blockage only when the agent physically arrives a vertex that is incident to that edge. The goal is to find a strategy for reaching from s to t that minimizes the expected cost.\n[6] showed that finding the optimal solution for the Canadian Traveler Problem was shown to be P# hard . However some special classes of CTP such as CTP on disjoint path graphs\nand CTP on directed acyclic graphs are solvable in polynomial time\n[5, 3].\nIn this work, we explore certain variations of the CTP. The first variation introduced is CTP with Dependencies (Dep-CTP). In the original problem, the distribution over the edges is independent. Dep-CTP is a generalization of CTP where we assume that dependencies exist between the status of a particular edge with the status of other edges. Specifically, we are given a Bayesian network that defines the dependencies between the edges. The second variant is CTP with remote sensing(CTP with sensing), introduced by [3]. In CTP with sensing, an agent may perform sensing on any edge, with a given sensing cost, in order to reveal its status. The third variant is Expensive-Edge CTP, a variant of CTP in which edges cannot be blocked, but are expensive and incurs a high travel cost when traversed.\nThis work contains two different approaches for studying the CTP, by theoretical analysis and by experimental analysis. Regarding the theoretical aspect, we attempt to classify certain classes of Dep-CTP by their computational complexity using probabilistic models as belief-MDP and AND/OR graphs, and we show some general properties for CTP with sensing. Regarding the empirical aspect, we introduce the Gen-PAO algorithm, a gen-\nCHAPTER 1. INTRODUCTION 5\neralization of PAO* [1] that optimally solves the CTP, CTP with sensing and Exp-CTP. Gen-PAO uses several pruning methods to reduce the size of the state space search and running time. In addition, we explore the value of clairvoyance which represent the value of having full knowledge of the graph.\nThe remainder of this work is organized as follows: Chapter 2 contains formal definitions of the Canadian Traveler Problem and its variants. In addition it contains a background for decision and probabilistic models and reviews a number of related algorithms. Chapter 3 shows some proofs concerning the hardness of Dep-CTP and for two of its subclasses. In addition, some theoretical properties concerning the CTP with sensing are shown. Chapter 4 introduces the Gen-PAO algorithm and some of the pruning methods it uses. Chapter 5 provides empirical results, comparing the performance of Gen-PAO and some of its variants. In addition, results concerning the value of Clairvoyance are presented. Chapter 6 summarizes this work and discusses possible directions for future research. Appendix A presents some of the instances in which empirical analysis is used .\nChapter 2\nBackground"}, {"heading": "2.1 Markov Decision Process", "text": "A Markov Decision Process(MDP) is a framework for sequential stochastic decision problems with a fully observable environment. Formally, MDP is defined by the tuple < S,A, T,R >, where\n\u2022 S is a finite set of states where s \u2208 S describes the environment at a\nspecific time step.\n\u2022 A is a set of actions.\n\u2022 T : (S \u00d7 A \u00d7 S) \u2192 [0, 1] is the transition function where T (s, a, s\u2032)\nspecifying the probability of entering a state s\u2032 \u2208 S given the previous state s \u2208 S and the action a \u2208 A.\n\u2022 R : (S \u00d7 A \u00d7 S) \u2192 R is the reward function which specifying the\nreward R(s, a, s\u2032) that is received by transitioning from state s \u2208 S to state s \u2208 S\u2032 after performing action a \u2208 A in s.\n6\nCHAPTER 2. BACKGROUND 7\nAt each time step, the agent is state s \u2208 S chooses an action a \u2208 A, reaches the state s\u2032 \u2208 S with probability T(s,a,s\u2019), and obtain reward R(s,a,s\u2019).A deterministic variant to MDP defines deterministic actions, where each pair of action a and state s specifies deterministically the result state s\u2032 i.e. There exist a state s\u2032 \u2208 S in which T (s, a, s\u2032) = 1 and for every s\u0302 6= s T (s, a, s\u0302) = 0. The model assumes that the transitions are Markovian in a sense that the probability of reaching a state depends only on the previous state and the action instead of a history of earlier states. The solution of the MDP is a policy. A policy \u03c0 : S \u2192 A is a mapping from a set of states to a set of actions. At each time step, a given policy is executed, starting from an initial state s0. By having a complete policy, the agent will always know what to do next. However, the stochastic nature of the environment will lead to a different environment history. The decision making problem may be a finite horizon or an infinite horizon. A finite horizon constrains the time steps that the agent exists (or equivalently, considers the rewards after time N as zeros). In this case the utility function is usually an additive reward function: t=\u221e\u2211\nt=0\n(st, a, st+1) Where a = \u03c0(st)\nHowever, in infinite horizon the time sequence is unbounded. In infinite horizon the utility function is computed with discount rate 0 < \u03b3 < 1:\nt=\u221e\u2211\nt=0\n\u03b3t(st, a, st+1) Where a = \u03c0(st)\nThis utility is called discounted reward. Usually, the performance of an agent is the utility of sequence of states, which is measured by the sum\nCHAPTER 2. BACKGROUND 8\nof rewards for the states visited. The utility of a policy \u03c0(s) in state s is the expected cost over all possible state sequences, starting from s until the MDP terminates. The utility of a policy \u03c0(s) in finite horizon is computed using dynamic programming:\nV\u03c0,0(s) = R(s, \u03c0(s), s\u0301) V\u03c0,n(s) = \u2211\ns\u0301\u2208S\nT (s, \u03c0(s), s\u0301) \u00b7 (R(s, \u03c0(s), s\u0301) + \u03b3 \u00b7 V\u03c0,n\u22121(s\u0301))\nWhere the utility of a policy \u03c0(s) in finite horizon is given by,\nV\u03c0(s) = \u2211\ns\u0301\u2208S\nT (s, \u03c0(s), s\u0301) \u00b7 (R(s, \u03c0(s), s\u0301) + \u03b3 \u00b7 V\u03c0(s\u0301)) (2.1)\nIn an infinite horizon we usually have a terminal state. The optimal policy is a policy that yields the highest expected utility(or lowest, depends on the specification of the problem). Given a policy \u03c0, the value of the policy in state s can be computed by an algorithm called value iteration. The value iteration algorithm computes the value of every state s under policy \u03c0 using a reasoning process that goes backwards in time, from the end, in order to determine the optimal sequence of actions. Once choosing the last action, we can determine the best second-last action etc. This process continues until received a best action for all states. We compute the value of each state s under the optimal policy \u03c0\u2217 using the Bellman equations:\nV \u2217(s) = MAX\u03c0(s)\u2208A \u2211\ns\u0301\u2208S\nT (s, \u03c0(s), s\u0301) \u00b7 (R(s, \u03c0(s), s\u0301) + \u03b3 \u00b7 V \u2217(s\u0301)) (2.2)\nThis is process is iterated until it reaches equilibrium which indicates the\nCHAPTER 2. BACKGROUND 9\nconvergence of the algorithm."}, {"heading": "2.1.1 Policy Iteration", "text": "Another approach for solving MDP is policy iteration. Policy iteration is a feedback strategy obtained by iterative search in the space of policies. The algorithm is based on two steps: The first step is the evaluation where the algorithm evaluate the values of the states given a set of a action for each state is given by:\nV \u03c0ki+1(s) \u2190 \u2211\ns\u2032\nT (s, \u03c0k(s), s \u2032)[R(s, \u03c0k(s), s \u2032) + \u03b3V \u03c0ki (s \u2032)]\nthis can be done by solving a set of linear equations. After the values are computed for the given actions, the algorithm makes the second step: improvement. The algorithm considers whether it can improve the policy by choosing a new action for the state. If such action exists, the policy execute the new action.\n\u03c0k+1(s) = argmaxa\n\u2032\u2211\ns\nT (s, a, s\u2032)[R(s, a, s\u2032) + \u03b3V \u03c0k(s\u2032)]\nThe algorithm guarantees that each iteration strictly improves the value of the policy. Therefore, the policy stops when there are no available actions that improve the policy cost. The number of possible policies cannot be more than |S||A| where |S| is the number of states and |A| is the number of actions. We know that the policy improves at each iteration and the number of possible policies is |S||A|, thus the algorithm finds the optimal policy within no more than |S||A| iterations.\nCHAPTER 2. BACKGROUND 10"}, {"heading": "2.2 Partially Observable Markov Decision Process", "text": "A partially observable Markov decision process (POMDP) is a generalization of the standard MDP, such that the environment is not fully observable, and allows imperfect information about the current state of the environment. In the real world the input may not always be precise where the data may be received with a noise. In robot navigation for instance, the robot will receive its input through sensors which do not describe the environment precisely. Sonar or voice sensors most of the time will probably be a bit noisy and digital video lose information by using a discrete presentation to describe a continuous environment. The POMDP is used as a framework for theoretical decision making and reasoning under uncertainty. Such problems arise in a wide range of application domains including assisting technologies, mobile robotics and preference elicitation. Many of the real POMDP problems are naturally modeled by a continuous states and observations. For instance, in a robot navigation task, the state will correspond to the coordinates in the space and the observations may correspond to the distance measured by the sonar. A common approach to a continuous model requires of discretization and approximation the continuous component of the grid. This usually leads to an important tradeoff between complexity and accuracy with the change of the coarsens of the discretization. On discrete time POMDP, each time period the agent is in some state s, chooses an action a, and receive a reward with expected value. Performing the action, the agent makes a transition to a new state according to some state distribution and observes the environment with a given probability to each state.\nCHAPTER 2. BACKGROUND 11\nFormally, POMDP is an extension of the MDP defined by the tuple < S,A, T,\u2126, R > where: S is a finite set of state that represents the current situation in the environment. A is a set of actions where the agent choose in each state. T(Transition function) is a function that maps S \u00d7 A into a distribution over the states S\u0301. T (s\u0301|s, a) is the probability to reach s\u0301 where the agent is at state s and perform action a. R is the reward function. R maps any S\u00d7A\u00d7S\u0301 into a number which represents the reward or the penalty. The observation function \u2126(s\u2032, a, o) describes the probability of observation o given that action a was performed in state s\u2019 was reached.\nGenerally, in POMDP we do not know the current state. The only information that is given on the environment is the observations. Therefore, POMDP defines a vector of probabilities b(s) in the size of the state set, called belief state, which specify for each state s, the probability that the environment is in s.\nSimilarly to MDP, The goal of the POMDP is to construct a policy \u03c0\u2217\nwhich maximizes the expected rewards E[ \u2211T\nt=o \u03b3 tr(st, at, s \u2032 t)] where T is the\nnumber of time steps left in a finite horizon, or T = \u221e in an infinite horizon. Since the agent does not know the exact state of the environment, the reward function is given by the belief state i.e. R(b, a) = \u2211\ns\u2208S b(s)R(s, a), or\nin the case of continuous belief space the sum becomes an integral. The belief state of the environment is based on the previous belief state of the environment. Thus, the agent updates the belief b(s\u2019) after being at belief state b(s),choosing action a and receiving an observation o in the following way:\nP (b\u2032|a, b) = \u2211\no\nP (b\u2032, o|a, b) = \u2211\no\nP (b\u2032|o, a, b)P (o|a, b)\nCHAPTER 2. BACKGROUND 12\nusing the product rule we get\nP (o|a, b) = \u2211\ns\u2208S\nP (o, s\u2032|a, b) = \u2211\ns\u2208S\nP (o|s\u2032a, b)P (s\u2032|a, b)\nP (o|s\u2032a, b) = P (o|s\u2032, a) = \u2126(o, s\u2032, a)\nP (s\u2032|a, b) = \u2211\ns\nP (s\u2032, s|a, b)\n= \u2211\ns\nP (s\u2032|s, a, b)\nbeliefstatefors \ufe37 \ufe38\ufe38 \ufe37\nP (s|a, b) = \u2211\ns\nP (s\u2032|s, a)b(s)\nWhen we put it all together we get:\nP (b\u2032|a, b) = \u2211\no\nP (b\u2032|o, a, b) \u2211\ns\u2032\n\u2126(s\u2032, o, a) \u2211 P (s\u2032|s, a)b(s)\nP (b\u2032|o, a, b) = 1 where P (b\u2032|o, a, b) = forward(o, a, b)\nLet L be the number of time P (b\u2032|o, a, b) = 1 Therefore,\nP (b\u2032|a, b) = L \u00b7 \u2211\ns\u2032\n\u2126(s\u2032, o, a) \u2211\ns\nP (s\u2032|s, a)b(s)\nA generalization on the discrete POMDP is where the space of the belief state is continuous. In this case, we still assume the the actions and observation are discrete, the propagation is defined by the integral\nP (s\u2032|a, s) =\n\u222b\ns\u2208S P (s\u2032|s, a)P (s)ds"}, {"heading": "2.2.1 Value Iteration", "text": "Defining the probability update and the reward function for belief state we can can transform the POMDP into a belief state MDP by casting the\nCHAPTER 2. BACKGROUND 13\nPOMDP problem into a fully observable MDP, where the belief state of the POMDP are reduced to simple state of the MDP. The MDP here is continuous and over |S|-dimensional state space. The transformation allows applying a value function for each belief state according to the Bellman equation:\nV \u2217(b) = MAXa\u2208A[r(b, a) + \u03b3 \u2211\nb\u2208B\n\u03c4(b\u2032, a, b)V (bao)]\nThis means that the value of belief state b is the reward of taking the best action in b plus the discounted expected reward of the resulting belief state V (bao) where b a o is the unique belief state computed based on b,a,o as in equation\u2014. Solving the value iteration by dynamic programing will bring optimal solution at the limit, however, the space size over all the belief states that have to be backed up is enormous. Because exact value iteration is intractable, a lot of work has focused on approximate algorithms. One of the most promising approaches for finding an approximate solution point based value iteration (PBVI). In PBVI instead of optimizing the value function over the entire belief state, only specific reachable beliefs are considered. The belief points are selected heuristically and the values are computed only for these points. The heuristic simulate trajectories in order to find reachable beliefs.The success of PBVI depends on the selection of the belief points. In particular the belief points should cover the space as evenly as possible. The set of belief state is expanded over time in order to cover more of the reachable belief state. Adding more point increases the accuracy of the value function.\nCHAPTER 2. BACKGROUND 14\nThe key to practical implementation of a dynamic programming algorithm is a piecewise-linear and convex representation of the value function. The reward function r(b, a) as defined above is linear. The exact solution of POMDP is based on Smallwood and Sondik(1973) proof which takes advantage of the fact that the exact solution is piecewise-linear convex functions and can be represented by |S| hyperplanes in the space of beliefs. Each hyperplane is a value function V over |S| real numbers represented by V = v1, v2, ..., vk where the value of each belief state is defined as follows:\nv(b) = MAX0\u2264i\u2264k \u2211 b(s)vi(s)\nEach hyperplane correspond to a single action, and the value iteration updates can be performed directly on these hyperplanes."}, {"heading": "2.3 The Canadian Traveler Problem", "text": "In the Canadian traveler problem(CTP) [Papadimitriou and Yannakakis, 1991] a traveling agent is given a tuple (G,P,w,s,t) as input whereG = (V,E) connected weighted graph that consists initial source vertex (s \u2208 V ), and a target vertex (t \u2208 V ). The input graph G may undergo changes, that are not known to the agent, before the agent begins to act, but remains fixed subsequently. In particular, some of the edges in E may become blocked and thus untraversable. Each edge e in G has a weight, or cost, w(e), and is blocked with a probability P (e), where P (e) is known to the agent.1 The agent can perform move actions along an unblocked edge which incurs a\n1Note that it is sufficient to deal only with blocking of edges, since a blocked vertex would have all of its incident edges blocked.\nCHAPTER 2. BACKGROUND 15\ntravel cost w(e). Traditionally, the CTP was defined such that the status of an edge can only be revealed upon arriving at a node incident to that edge, i.e., only local sensing is allowed. In this paper we call this variant the basic CTP variant. The task of the agent is to travel from s to t while aiming to minimize the total travel cost Ctravel. As the exact travel cost is uncertain until the end, the task is to devise a traveling strategy which yields a small (ideally optimal) expected travel cost.\nA somewhat more general version of CTP is CTP with sensing. CTP with sensing is a tuple (G,P, SC,w, s, t), where in this variant, in addition to move actions (and local sensing), an agent situated at a vertex v can also perform a Sense action and query the status of any edge e \u2208 G. This action is denoted sense(v, e), and incurs a cost SC(v, e), or just SC(e) when the cost does not depend on v. The cost function is domain-dependent, as discussed below. The task of the agent is to travel to the goal while minimizing a total cost Ctotal = Ctravel + Csensing.\nWe further generalize CTP to allow dependencies between edges, and non-binary edge weight distributions. In this general form, CTP-Gen is a 5-tuple (G,W,SC, s, t) where G = (V,E) is a graph, W is a distribution over weights of the edges E, SC : V \u00d7 E \u2192 R+ is a sensing cost function, s, t \u2208 V are the start and goal vertices, respectively. The distribution model W is over random variables indexed by the edges in E, abusing notation we will use the edges in place of the respective random variables. The domain of these random variables are arbitrary weights or cost sets. W is usually specified as a structured distribution model over the random variables e \u2208 E. Henceforth we assume that W is specified as a Bayes network (E,A,P ) over\nCHAPTER 2. BACKGROUND 16\nthese random variables, where E is the set of random variables, A is a set of directed arcs so that (E,A) is a directed acyclic graph, and P are the conditional probability tables, one for each e \u2208 E.\nWe mostly limit ourselves to the binary case where the edges can be blocked (\u201cinfinite weight\u201d) or open (some known weight, possibly different for each edge). In these cases, and to simplify the resentation of the distribution, we use a uniform binary domain {Blocked, Unblocked} for the edges, and describe the weight of the (unblocked) edges separately, by a weight function w : E \u2192 R+. In the degenerate binary case where W is a Bayes network with no arcs (A = \u2205), i.e. all random variables are independent, the problem reduces back to the basic CTP with sensing. In this case we usually specify the distribution as a function p : E \u2192 [0, 1], the probability that each edge e \u2208 E is blocked."}, {"heading": "2.3.1 CTP with Dependencies in Disjoint Path Graphs", "text": "As CTP-Gen is extremely complicated, we focus on some special cases w.r.t. the topology of the graph G. Specifically, we examine the basic CTP with no remote sensing where G is a disjoint-path graph (w.r.t. s, t). As this case is known to be solvable in closed form in polynomial time, we generalize it to the case where edges are dependent, and edge weights are binary (blocked/unblocked) random variables. Thus we consider CTP-DEP, defined by the 5-tuple (G,W,w, s, t) where G is an undirected CTP graph, W is a Bayes network representing the edge blocking distribution model, w is a function denoting the edge weights (for unblocked edges), and s, t are the start and goal vertices respectively, as usual.\nCHAPTER 2. BACKGROUND 17\nAs we will show, finding an optimal problem for CTP-DEP is intractable even for special cases, and we will thus consider cases where W has dependencies only between edges on the same path. Thus the Bayes network W representing the distribution model has one (or more) unconnected component, for each set of edges composing a path. We call this simplified variant CTP-PATH-DEP.\nIn disjoint path graph, we index the edges such that each edges has two indexes where the first index indicates the path and second index indicates the serial location of vertex in the path. For instance ei,1, ei,2, ...ei,ki are the edges composing the i\u2019th path. Similarly to edges, we index the vertices such that the first vertex indicates the path and second index indicate the serial location of the vertex in the path. s, vi,1, vi,2, ...vi,mi, t are the vertices composing the i\u2019th path. Note that each edge ei,j can be represented by (vi,j\u22121, vi,j)"}, {"heading": "2.4 AND/OR Graphs", "text": "Many problems in artificial intelligence can be formulated as a framework for problem solving in a state space search. The AND/OR graph is a directed graph that represent a problem solving process. The solution of AND/OR is a sub-graph of the AND/OR, called solution graph, that is a derivation for the optimal solution of the original problem. In this work, we use the AND/OR graph for finding optimal solutions to probabilistic reasoning problems and CTP in particular. With a slight abuse of notation we use the same notation graph to indicate both CTP graph and AND/OR graph.\nCHAPTER 2. BACKGROUND 18\nFormally, an AND/OR graph is a tuple GAO = (NAND\u222aNOR, E, T, c, p)\ndefines as follows:\n\u2022 N = NAND \u222aNOR where NAND, NOR are finite sets of nodes.\n\u2022 T \u2282 NOR is the set of terminal leaf nodes. \u2022 E \u2282 (NAND \u00d7 NOR) \u22c3 (NOR \u00d7 T\\NAND) is a set of directed edges\nbetween the nodes.\n\u2022 c : E \u222a T \u2192 R is a cost function over the edges and terminal cost.\n\u2022 The graph associate probabilities p(n, n0) over the edges (n, n0) such\nthat, for every n \u2208 NAND \u2211 \u3008n,n\u2032\u3009\u2208E p \u3008n, n \u2032\u3009 = 1.\nThe root node of GAO is denoted by n0. A policy graph of the AND/OR graph is a subgraph H = (NH , EH) of GAO such that\n\u2022 n0 \u2208 NH\n\u2022 If n \u2208 NH\\T is AND node, all its children are in H.\n\u2022 If n \u2208 NH is OR node then only one of its children is in H.\n\u2022 Every leaf node (node with no children) in GAO is terminal.\nand,\n\u2022 If n \u2208 NH\\T is AND node, all outgoing edges are in EH .\n\u2022 If n \u2208 NH is OR node, and n \u2032 \u2208 H is a child of n, then \u3008n, n\u2032\u3009 \u2208 EH .\nDefine subpolicy of node n denoted by SHn to be a subgraph of GAO that satisfies all properties of H except that the root of SHn is an arbitrary node n instead of n0.\nCHAPTER 2. BACKGROUND 19\nThe value of each node n \u2208 NH is defined as follows:\nCH(n) =\n   c(n) n \u2208 T min \u3008n,n\u2032\u3009\u2208EH c(\u3008n, n\u2032\u3009) +CH(n \u2032) n \u2208 NOR \u2211\n\u3008n,n\u2032\u3009\u2208EH\np(\u3008n, n\u2032\u3009)(c(\u3008n, n\u2032\u3009) + CH(n \u2032)) n \u2208 NAND\nThe child of OR node with the minimal value is called called preferred son. The cost of the policy graph is defined as CH(n0). The policy graph is optimal if there are no other policy graphs with lower cost. We define a policy subgraph to be a subgraph of NH such that\n\u2022 n0 \u2208 NH\n\u2022 If n \u2208 NH\\T is AND node, all its children are in H.\n\u2022 If n \u2208 NH is OR node then only one of its children is in H.\nWe define policy subgraph Hs to be a subgraph of GAO that satisfies all properties except that the leafs are not necessarily terminal. The cost of the policy subgraph is defined as CHs(n0). The best policy subgraph is a policy subgraph with the minimal cost in the AND/OR graph."}, {"heading": "2.4.1 AO*", "text": "The AO* is an heuristic based search algorithm that performs a search in the AND/OR graph for finding the optimal policy graph. The AO* performs a search in the AND/OR graph, gradually building up a partial policy graph, assigning heuristic values to the leaves, and propagating the heuristic values up to the root. The heuristics, used to evaluate the real cost of the nodes in AND/OR graph, are admissible, and therefore, finding the optimal solution\nCHAPTER 2. BACKGROUND 20\nis guaranteed. The AO* is beneficial when solving problems with a large state space. The AO* algorithm assumes that the AND/OR graph that represents the problem is not given, however the algorithm construct the AND/OR graph by expanding it each iteration, and thereby develop the optimal policy graph subgraph each iteration. The process ends when all the leaf nodes of the partial policy graph are terminal. The AO* takes advantage of the fact that once a node is known to be in the optimal policy graph it does not required any further expansion. Thus, the algorithm maintains a boolean parameter called \u201dSOLVED\u201d for each node in the AND/OR graph which signs the algorithm if node is a part of the optimal policy, i.e. a node n is set SOLVED, performed by the operation MarkSolved(n), if n is known to be in the optimal policy subgraph. Once a node is SOLVED, it remains SOLVED. A node n is SOLVED if and only if all the nodes in the subpolicy SHn spanned from n, are solved. Hence, when a node n is set SOLVED, the subpolicy SHn spanned from this node does not require any further update or expansion. Implementing the \u201csolving\u201d process, the AO* performs MarkSolved(n), if node n satisfies one of the following:\n\u2022 n is a terminal node\n\u2022 n is an AND node and all its children are are set SOLVED.\n\u2022 n is an OR node and its preferred son is set SOLVED.\nBasically, each iteration of the AO* algorithm has two phases: expansion\nand propagation, described as follows:\n\u2022 Expansion phase:\nCHAPTER 2. BACKGROUND 21\n1. Trace down the marked edges (directed edges) from n0 and go\ndownwards until reaching a non-terminal leaf node n and expands it. (Finding the expansion nodes requires recurrence exploration through the AND/OR graph since the partial policy graph is changing each iteration.)\n2. For each child ni \u2208 n1, n2, ..., nk of n, if ni has not been generated,\nthen add it to the policy graph and assign it admissible heuristic. If ni is a terminal node then assign 0 to its heuristic value, and perform MarkSolved(ni).\n\u2022 propagation phase: In the propagation phase, the heuristic values and\nmarked edges of the expansion nodes are propagated from the leaves onward up to the root. The propagation processed as follows:\n1 If n is OR node then its heuristic value is updated by,\nh(n) = min i c(\u3008n, ni\u3009) + CH(ni) (2.3)\nThe marked edge is directed from n to the child ni which achieves the minimum in equation 2.3, and n is set SOLVED if and only if ni is set SOLVED.\n2 If n is AND node then its heuristic value is updated by,\n\u2211\ni\np(\u3008n, ni\u3009)(c(\u3008n, ni\u3009) + CH(ni)) (2.4)\nThe node n is set SOLVED if and only if all its children are set SOLVED.\nCHAPTER 2. BACKGROUND 22\nThe procedure of updating the heuristic values and marking the edges is repeated for all nodes ancestors of n.\nProperties of the AO*:\n\u2022 The heuristic values are optimistic estimations (lower bound) to the\nreal value of the state, where each update raises up the heuristic value and reduces its imprecision relatively to the real value.\n\u2022 The AO* is beneficial when it applied to a large state space. One\nreason for this is that AO* considers only states that are reachable from the initial state. Secondly, the informative heuristic function directs the focus on states that are in the course of a good policy graph(partial policy graph). As a result, the AO* may find an optimal solution by exploring a small fraction of the entire state space."}, {"heading": "2.4.2 CTP and AND/OR graphs", "text": "AND/OR graph is a natural structure for representing the state space of CTP, where the policy of CTP is represented by the policy graph. The problem solving process is a search for an optimal policy graph in a policy graph space. Here, the OR nodes represents the agent\u2019s decision in a current state out of all its available actions. Where, in basic CTP, the available actions are all the moves available from a certain vertex, while in SensingCTP, the available actions are all the available remote sensing actions, in addition to all available moves from a certain vertex(the remote sensing is defined available if it is performed on an unknown edge). The AND nodes represent the actions. Since the CTP is a stochastic problem, each action\nCHAPTER 2. BACKGROUND 23\nAlgorithm 2.1 AO* procedure main(Graph GAO, Node s)\n1: if (s is terminal) return GAO; 2: while s is not marked SOLVED do 3: Trace down the marked edges in GAO from s, until reaching to all the non terminal leaf node n1, n2, ..., nk; 4: L \u2190 {n1, n2, ..., nk}; 5: while L 6= \u03c6 do 6: ni \u2190 Expand(GAO, s) 7: remove ni form L; 8: Z \u2190 ExtractAnsectors(ni) \u222a ni; 9: Propagate(GAO ,Z)\n10: end while 11: end while\nvertex function Expand(Graph GAO, Node s)\n1: select ni \u2208 L expand it and add its children C = {c1, c2, ..., cm} to GAO;\n2: for each ci \u2208 C do 3: if ci is terminal then 4: f(ci) \u2190 0; 5: MarkSolved(ci); 6: else 7: f(ci) \u2190 h(ci); 8: end if 9: end for\n10: return ni\nprocedure Propagate(Graph GAO, NodeSet Z)\n1: while Z 6= \u03c6 do 2: select zi \u2208 Z such that zi has no children in Z; 3: if zi is AND node then 4: f(zi) \u2190 \u2211\nj\u2208successor(zi) a=(zi,j)\n[tr(zi, a, j)h(j) + c(a)];\n5: if all successors of zi are marked SOLVED then 6: MarkSolved(zi) 7: end if 8: end if\n9: if zi is OR node then 10: f(zi) \u2190 min\nzj\u2208successor(zi) a=(zi,zj)\n[tr(zi, a, zj)h(zj) + c(a)];\n11: if zj is marked SOLVED then 12: mark the edge represents the chosen action a; 13: MarkSolved(zi); 14: end if 15: end if 16: remove zi from Z; 17: end while\nCHAPTER 2. BACKGROUND 24\nmay result several possible states, which is represented by the AND node\u2019s children. The states of the environment in CTP are represented by the OR nodes. The states are the belief states of the agent in a current time step, where each belief state is represented by its form (i.e every belief state b is represented by the tuple \u3008b\u3009 = \u3008Loc(b), stb(e, b1), ..., stb(e, bn)\u3009). Henceforth, all functions,predicate and lemmas presented in section 3.4 can be applied to the states in the AND/OR graph. We call the set of states that appears in the AND/OR graph the expanded states and denote it by Z. Although the AND nodes do not represent the states(they are called \u201csemi state\u201d), they maintain heuristic values as described in AO* algorithm which is specified for propagation. Since the environment is static, once an agent observes an edge, its status is remained unchanged. A terminal state b is a state in which its location variable is the target(Loc(b) = t). A node is a terminal leaf node if the state with which it associate is terminal.\nDefinition 2.4.1. A belief state b is expanded belief state if there is an OR node in the AND/OR graph that is associated with b."}, {"heading": "2.5 Models for the Canadian Traveler Problem", "text": ""}, {"heading": "2.5.1 POMDP for CTP", "text": "In this section we show that CTP can be modeled by POMDP. Let I = (G,P,w, s\u0302, t\u0302) be an instance of of basic CTP, and I \u2032 = (G,P,w, SC, s\u0302, t\u0302) be an instance of CTP-with-sensing, where G = (V,E). Given POMDP M = (S,A, Tr,R,Z,O, s0), we show how I and I\u2019 can be modeled by M as follows:\nCHAPTER 2. BACKGROUND 25\nCHAPTER 2. BACKGROUND 26\nactions, in which agent that performs a = Sense(e), senses an edge e \u2208 E. This action can be performed from any vertex v \u2208 V .\n\u2022 The transition function Tr. Given s, s\u2032 \u2208 S, and a = move(vi, vj), we\ndefine Tr by the following: Tr(s, a, s\u2032) = 1 if it satisfies the following:\n\u2013 For all edges e \u2208 E the status of the edge in s is equal to the\nstate in s\u2019, i.e ste(s) = ste(s \u2032).\n\u2013 vi = LocS(s) and vj = LocS(s \u2032).\n\u2013 The edge e=(vi, vj) is open in s, i.e. ste(s) = Oe.\nOtherwise Tr(s,a,s\u2019)=0. If a = Sense(e) where e \u2208 E we get Tr(s, a, s\u2032) = 1 if and only if s = s\u2032 since the Sense action does not change the state of the environment.\n\u2022 The reward(cost) function R. Given s, s\u2032 \u2208 S, we define R as follows:\nIn case that a = move(e) then,\nR(s, a, s\u2032) =\n   w(e) if Tr(s, a, s\u2032) = 1\n0 otherwise\nIn case that a = Sense(e) then,\nR(s, a, s\u2032) =\n   SC(e) if Tr(s, a, s\u2032) = 1\n0 otherwise\nNotation 2.5.1. Let X be a set. Denote the power set of X by P(X).\n\u2022 The observation set Z. Let Z \u2032 = {Oe, Be} E . We define Z to be the\npower set of Z\u2019, Namely Z = P(Z \u2032) .\nCHAPTER 2. BACKGROUND 27\n\u2022 The observation function O. Given s \u2208 S ,a \u2208 A and z \u2208 Z we define\nO as follows: In case that a = move(e) where e = (vi, vj), the only observation that received are the edges incident to vertex vj, then,\nO(s, a, z) =\n   1 if z = {ste(s)|e \u2208 Inc(vj)}\n0 otherwise\nWhere in case that a = sense(e), the only observation that received is the sensed edge e, then,\nO(s, a, z) =\n   1 if z = ste(s)\n0 otherwise\n\u2022 s0 is the initial state.\nNotation 2.5.2. The optimal policy of MS is denoted by \u03c0 \u2217.\nNotation 2.5.3. Let X be a set. Denote the power set of X by P(X)."}, {"heading": "2.5.2 Belief State for Representing the Environment of CTP", "text": "A belief state, which is defined as a distribution over all possible states, is a representation of the agent\u2019s knowledge about the environment. In CTP, the belief states can be represented by the location of the agent and the status of each edge in the graph.\nDefinition 2.5.4. We say that status of edge e is:\n\u2022 \u201cknown to be blocked\u201d if e has been already sensed and found to be\nblocked.\n\u2022 \u201cknown to be open\u201d if e has been already sensed and found to be open.\nCHAPTER 2. BACKGROUND 28\n\u2022 \u201cunknown\u201d if e has not been sensed.\nDefinition 2.5.5. Define stb : E \u00d7 B \u2192 {Open,Blocked, Unkown} as follows: stb(e, b) is the edge status of e in belief state b, where\nstb(e, b) =\n   Open if edge e is known to be Open in b Blocked if edge e is known to be Blocked in b\nUnknown otherwise (if the status of edge e is Unknown in b)\nDefinition 2.5.6. Define Loc : B \u2192 V as the location of an agent in a belief state, where Loc(b) outputs the physical location of an agent that is in belief state b ,i.e. Loc(b) = LocS(s) where s is an arbitrary state s \u2208 S which satisfies b(s) > 0.\nNote that definition 2.5.6 assumes that there cannot be two state s1, s2 which satisfy b(s1) > 0, b(s2) > 0 such that LocS(s1) 6= LocS(s2) since by definition, the agent always knows its own location, and thus, for every belief state b, if exists s \u2208 S in which Loc(b) 6= LocS(s) then b(s) = 0.\nThus, we can define an alternative way for representing a belief state,\nDefinition 2.5.7. Let n = |E|. The form of b, denoted by \u3008b\u3009, is defined to be the tuple \u3008b\u3009 = \u3008Loc(b), stb(e1, b), stb(e2, b), ..., , stb(en, b)\u3009,\nDefinition 2.5.8. Let b be a belief state, we define the following sets:\n1. Unknown(b) is the set of all edges e \u2208 E in which stb(e, b) = Unknown\n2. Blocked(b) is the set of all edges e \u2208 E in which stb(e, b) = Blocked\n3. Open(b) is the set of all edges e \u2208 E in which stb(e, b) = Open\nCHAPTER 2. BACKGROUND 29\nLet b be a belief state. Then, there is a mapping from \u3008b\u3009 to b. Namely,\nfor every s \u2208 S there is a mapping from \u3008b\u3009 = \u3008Loc(b), stb(e1, b), stb(e2, b), ..., , stb(en, b)\u3009 to b(s) as follows:\nb(s) =\n   0 if G(e, s, b) = 0 \u220f\n{e\u2208Unknown(b)|st(e,s)=Blocked}\np(e) \u220f\n{e\u2208Unknown(b)|st(e,s)=Open}\n1\u2212 p(e) Otherwise\nWhere G : E \u00d7 S \u00d7B \u2192 {0, 1} is defined as follows:\nG(e, s, b) = 0 if one of the following is satisfied:\n1. st(e, s) = Open, stb(e, b) = Blocked\n2. st(e, s) = Blocked, stb(e, b) = Open\n3. Loc(b) 6= LocS(s)\notherwise G(s, b) = 1\nCorollary 2.5.9. Since there is a mapping from \u3008b\u3009 to b, we can use the form \u3008b\u3009 instead of the belief state b itself, for representing the belief state of an agent.\nDefinition 2.5.10. Let Pb(e, b) be the probability that edge e is blocked given that the agent is in belief state b. Namely Pb(e, b) = \u2211 i b(si) such that st(e, si) = Blocked.\nIn the basic variant of CTP, the probabilities associated with the edges are independent, and hence, as long as stb(e, b) = Unknwon, we have Pb(e, b) = P (e).\nCHAPTER 2. BACKGROUND 30\nCorollary 2.5.11. From definition 2.5.10 we get that the probability that edge e is blocked given the agent is in belief state b is given by:\nPb(e, b) =\n   0 if stb(e, b) = Open 1 if stb(e, b) = Blocked\nP (e) if stb(e, b) = Unknwon\nCHAPTER 2. BACKGROUND 31"}, {"heading": "2.5.3 Belief MDP for CTP", "text": "Given a POMDP M = (S,A, Tr,R,Z,O, s0) of instance I = (G,P,w, s\u0302, t\u0302) of CTP (or I \u2032 = (G,P,w, SC, s\u0302, t\u0302) of CTP-with sensing). Let B be the belief state space of M, we define a belief MDP MS = (B,A, Tr,R, b0) of I, based on M, where the states space B is over the state space S. CTP is a special case of POMDP(called Det-POMDP) where transition function Tr and reward function R can be simplified here as follows:\n\u2022 The transition function Tr. In general belief MDP, given b, b\u2032 \u2208 B,\na \u2208 A, Tr is given by:\nTr(b\u2032, a, b) = P (b\u2032|a, b) = \u2211\nz\nP (b\u2032, z|a, b) = \u2211\nz\nP (b\u2032|z, a, b)P (z|a, b)\nGiven a = move(vi, vj), we define E\u0302 to be the set of all edges incident to vj that are unknown in b and known in b \u2032 (the edges that are revealed by the local sensing), i.e.\nE\u0302 = {e \u2208 E|stb(e, b) = Unknown, stb(e, b\u2032) 6= Unknown, e \u2208 Inc(vj), b \u2208 B, b \u2032 \u2208 B}\nThen Tr(b\u2032, a, b) > 0 if and only if,\n\u2013 For all e \u2208 E\\E\u0302 stb(b, e) = stb(e, b\u2032). The status of the edges do\nnot change as well as the information about any unraveled edge that is not sensed.\n\u2013 vi = Loc(b) and vj = Loc(b \u2032).\n\u2013 The edge e=(vi, vj) is open in b, i.e. stb(e, b) = Open.The edge\nhas to be open in order to traverse it.\nCHAPTER 2. BACKGROUND 32\nIn this case,\nTr(b\u2032, a, b) = \u220f\ne\u2208E\u0302,stb(b\u2032,e)=Blocked\np(e) \u220f\ne\u2208E\u0302,stb(b\u2032,e)=Open\n1\u2212 p(e)\nGiven a = Sense(e\u2032), Tr(b\u2032, a, b) > 0 if and only if,\n\u2013 stb(b\u2032, e\u2032) 6= Unknown. The edge e\u2019 is known after the performing\nSense(e\u2019).\n\u2013 For all e \u2208 E\\e\u2032 stb(b, e) = stb(e, b\u2032). The state is not effected by\nSense action and the only information that received is the status of e\u2019.\n\u2013 Loc(b) = Loc(b\u2032). The location is not effected by Sense action.\nIn this case,\nTr(b\u2032, a, b) =\n   p(e) if stb(e, b) = Blocked\n1\u2212 p(e) otherwise\n\u2022 The reward function R. In general, the reward function is defined by\nR(b, a, b) = \u2211 b\u2032\u2208B b(b \u2032) \u2211 b\u2208B b(b)R(b, a, b \u2032). Denote the action cost of a by C(a), where C(a) = w(e) if a = Move(e) and C(a) = SC(e) if a = Sense(e). Hence,\nR(b, a, b\u2032) =\n   C(a) if and only if Tr(b, a, b\u2032) > 0 0 otherwise (2.5)\nWe define R(b, a) = \u2211\nb\u2032\u2208B R(b, a, b \u2032)Tr(b, a, b\u2032). Therefore, R(b, a) =\nC(a) if there exist b\u2032 \u2208 B such that Tr(b, a, b\u2032) > 0. OtherwiseR(b, a) =\nCHAPTER 2. BACKGROUND 33\n0. Note that in case that a = Sense(e), there always exist b\u2032 reachable from b in which Tr(b, a, b\u2032) > 0, thus R(b, a, b\u2032) = SC(e) always holds.\n\u2022 b0 is the initial belief state.\nDefinition 2.5.12. We say that action a can be performed in belief state b if there is a belief state b\u2032 such that Tr(b, a, b\u2032) > 0"}, {"heading": "2.6 Related Work", "text": ""}, {"heading": "2.6.1 Different Variation of CTP", "text": "The Canadian traveler problem is known to be p# hard [6]. In the lack of approximation solutions, different versions of special classes of graphs have been suggested where the exact solution can be found in polynomial time. [2] have investigated the case of Recoverable CTP , where each vertex is associated with a specific recovery time to reopen any blocked edge that is incident to it. When an agent finds a blocked edge e it can either traverse another edge or wait a period of time and check if e has been opened. The basic CTP is a special case of the Recoverable CTP where all the recovery times are infinitely large. There are two variation to the Recoverable CTP, deterministic and stochastic. In the deterministic variation the assumption is that the number of edges that may be blocked is bounded. In the stochastic variation, each edge is associated with a probability of being blocked while it assumes that the recovery time is not long relative to the travel time. The two cases were proved to be polynomial in the number of edges and vertexes and in the maximal number of blocked edges. [5] investigated a CTP variant where the environment is dynamic, in a sense that the status\nCHAPTER 2. BACKGROUND 34\nof each edge e \u2208 E is generated randomly with a given probability P (e) whenever the agent reaches an incident vertex of e. This variant can be modeled by MDP, where the states represent only the current location of the agent. Since MDP is solvable in polynomial time, this variant is solvable in polynomial time as well. Notice that basic CTP is much harder to solve, since the edges status is remained fixed and thus the state space is exponentially larger (in the number of edges). Nikolova et al. have shown that CTP on directed acyclic graph (DAG) can be solved in polynomial time by using a dynamic programming."}, {"heading": "2.6.2 Disjoint Path Graphs", "text": "Disjoint path graph is an undirected graph G = (V,E) with source s \u2208 V and destination t \u2208 V such that all paths p1, ..., pk in G are between s and t, and these paths are pairwise disjoint. [3] have shown that CTP on disjoint path graph is solvable in polynomial time. The proof is based on the property the the optimal policy is committing . This guarantees that whenever an agent follows a path, the optimal action is to continue the path until reaching the target unless it hits a blocked edge. The optimal policy of CTP on disjoint path is to follow the paths by their order of Di (Di is parameter associated with each path in G) Meaning, the optimal policy is to travel the path with the minimal Di till reaching the target unless the path is blocked. If the path is blocked then return to s and travel the path with second minimal Di and so on. Di is defined as,\nDi = E[BCi]\nProb(path i is traversable) +Wi,ki\nCHAPTER 2. BACKGROUND 35\nWhere BCi denotes the backtracking cost of path i which is the cost of traversing path i until hitting a blocked edge and then returning back to the s when the path is not traversable , or 0 when the path is traversable. The expected cost of BCi is\nE[BCi] = 2\nki\u22121\u2211\nj=2\nWi,j\u22121p(eij)\nj\u22121 \u220f\nm=1\n1\u2212 p(eim)\nWhere Wi,j = \u2211m=1 j w (eim) .\nAnother variation of CTP on disjoint path graphs is when the edges cannot be blocked but instead have two possible finite costs: a cheap and and expensive [5]. A simple case of this variation is when the value edges is binary, i.e., 0 or 1. In this case the optimal policy would be to explore all the edges with cost 0 of each path until it reaches an edge with cost 1 on the path, and then return to the path with the fewest unexplored edges and follow it until reaching the target. A more general case of this variation is when the edges are associated with the cost 1 or K. In this case the optimal policy has the property that once an edge with cost k has been crossed, it is optimal to continue along the same path until reaching the target. Taking advantage of the special structure of the policy induced by this property, allows to define an MDP with concise representation that decides in what order to explore the paths and how many, before committing a path. This two cases were proved to be solvable in polynomial time.\nCHAPTER 2. BACKGROUND 36"}, {"heading": "2.6.3 CTP with Sensing", "text": "The CTP with sensing is a harder problem than the basic CTP since a simple reduction can be constructed from any instance of CTP: The graph of the basic CTP is the graph of the CTP with sensing, however, the sensing cost of all edge are large enough, such that sensing an edge is never worthwhile. As such, the expected cost of the two optimal policies is equal.\nHeuristic search algorithms\nIn order to facilitate the search for solution of CTP with sensing, some heuristic based algorithm have been suggested. The algorithms do not provide an optimal solution, however, they may be much simpler. [3] have suggested the FSSN algorithm that is based on the free space assumption heuristic. The free space assumption [4] assumes that edges are traversable unless specifically known otherwise. The FSSN plans a path p from some vertex v \u2208 V to t with the shortest path under the free space assumption. The agent can either attempt to traverse P without sensing or may decide to interleave sensing actions into the movement actions, according to a sensing policy that is embedded in the algorithm.\nNumber of sensing policies to the FSSN have been suggested: Never Sense is a brute force policy that never senses any remote edge. This policy never incurs any sensing cost but it may lead to an increase travel cost.\nAlways Sense is a brute force policy that senses all the unknown edges\nin the path before it moves along it.\nValue of information a policy that decides what edges to be sensed ac-\nCHAPTER 2. BACKGROUND 37\ncording to their value of information."}, {"heading": "2.6.4 Propagating AO*", "text": "AO* harness the benefits of the heuristic search to avoid searching states that are undesirable. However, in many situations AO* examines far more states than necessary. Propagating AO*(PAO*) [1] is an extension of the AO* that takes one step forward for facilitating the search. PAO* propagates the heuristic values on a larger scale in which minimizes the expansion of non-terminal nodes. PAO* is based on a specific variation of the AO* algorithm; Ferguson et al. constructed an algorithm that solves a variation of the CTP where most of the graph (edges) is observable such that only a single unknown edge (called pinch point) can be incident to a vertex (In the original paper the pinch points are called \u201cfaces\u201d ). As such, any chance node (AND node) has at most two children that represent a traversable edge and a blocked edge. PAO* is described as follows: The expansion phase is processed exactly as the AO* where the PAO* grows the best partial policy graph by expanding the non terminal leaf nodes, and assigning heuristic values to its children. Similarly to AO*, PAO* propagates the heuristic values onward up to the root. However, PAO* propagate the heuristic values sideways and downwards to the children as well. Furthermore, the algorithm takes advantage of the fact that the AND node has only two children (traversable and blocked) such that the parent node heuristic value should never be less than the traversable child value. Thus, PAO* propagate the heuristic value of the parent to the traversable child if the heuristic value of the traversable child is higher. Similarly, the heuristic value of the parent\nCHAPTER 2. BACKGROUND 38\nshould never be greater than the value of the blocked child. Therefore, PAO* propagate the value to the non-traversable child in case that the heuristic value of the parent is higher.\nChapter 3\nTheoretical Analysis of CTP"}, {"heading": "3.1 CTP with Dependencies", "text": "Theorem 3.1.1. CTP with dependencies is at least as hard as CTP with sensing.\nProof outline: By reduction from CTP-with-sensing to CTP-with-\ndependencies.\nProof: Proof. Let I=(G,W,C,SC,s,t) be an instance of CTP-with-sensing. We\nconstruct an equivalent instance I\u2019=(G\u2019,W\u2019,C\u2019,s\u2019,t\u2019) of CTP-with-dependencies and show that there is a one-to one equivalence between I and I\u2019. Construction of I\u2019 is as follows, G\u2019 contains G entirely, and in addition, each vertex in G is attached to two-edge dead-end path, that simulate the sensing operation of I. One path for each possible sensing operation in I.\nFormally, the construction of I\u2019 is as follows:\nFirst, we construct G\u0302(V\u0302 , E\u0302) by copying the graph G(V,E) using the following functions:\n39\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 40\n\u2022 gv : V \u2192 V\u0302 is a bijection function that copies V into V\u0302 such that for\neach vi \u2208 V , gv(vi) is the copied vertex of vi.\n\u2022 ge : E \u2192 E\u0302 is a bijection function that copies E into E\u0302 such that for\neach ej \u2208 E, ge(ej) is the copied edge of ej .\nLet V\u0302 be the set of all the vertices that were copied from V, meaning V\u0302 = \u22c3\n1\u2264i\u2264n gv(vi). Let E\u0302 be the set of all the edges that were copied from E,\nmeaning E\u0302 = \u22c3\n1\u2264j\u2264m ge(ej).\nNotation 3.1.2. v\u0302i \u2208 V\u0302 denote the copied vertex f(vi).\nNotation 3.1.3. e\u0302i \u2208 E\u0302 denote the copied edge ge(ei)\nWe construct a new graph G\u2019(V\u2019,E\u2019) by extending V\u0302 and E\u0302 using the\nfollowing functions:\n\u2022 fv1 : E\u0302 \u00d7 V\u0302 \u2192 V and fv2 : E\u0302 \u00d7 V\u0302 \u2192 V are one to one functions that\ngenerates a vertex for each element in E\u0302 \u00d7 V\u0302 . Meaning, given vi \u2208 V\u0302 and ei \u2208 E\u0302, fv1(vi, ej) = vij1, fv2(vi, ej) = vij2.\n\u2022 fe1 : E\u0302 \u00d7 V\u0302 \u2192 E and fe2 : E\u0302 \u00d7 V\u0302 \u2192 E are one to one functions\nthat generates an edge for each element in E\u0302 \u00d7 V\u0302 such that given vi \u2208 V\u0302 and ei \u2208 E\u0302, fe1(vi, ej) = eij1, fe2(vi, ej) = eij2 and in addition, eij1 = (vi, vij1) and eij2 = (vij1, vij2).\nLet graph G\u2032(V \u2032, E\u2032) defined as follows:\nV \u2032 = {\nV\u0302 \u222a V \u2032ij1 \u222a V \u2032 ij2\n}\nE\u2032 = {\nE\u0302 \u222a E\u2032ij1 \u222a E \u2032 ij2\n}\n(3.1)\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 41\nwhere,\nV \u2032ij1 = \u22c3\n1\u2264i\u2264n,1\u2264j\u2264m\nv\u2032ij1\nV \u2032ij2 = \u22c3\n1\u2264i\u2264n,1\u2264j\u2264m\nv\u2032ij2\nE\u2032ij1 = \u22c3\n1\u2264i\u2264n,1\u2264j\u2264m\ne\u2032ij2\nE\u2032ij2 = \u22c3\n1\u2264i\u2264n,1\u2264j\u2264m\ne\u2032ij2\n(3.2)\nNotation 3.1.4. Given eij1 \u2208 Eij1, eij2 \u2208 Eij1, we define a two edge dead end path pij = \u3008eij1, eij2\u3009.\nNote thatG\u2032(V \u2032, E\u2032) can be viewed as \u201dattachment\u201d of paths \u22c3\n1<j0|E|Pij\nto each g(vi) \u2208 V\u0302 .\nW (X,Y ) is the Bayesian network that is associated with edges in G where X is the set of nodes and Y is the set of arcs. Similarly W \u2032(X \u2032, Y \u2032) is the Bayesian network that is associated with edges in G\u2019 where X\u2019 is the set of nodes and Y\u2019 is the set of arcs. Let x be a node in X, and x\u2019 be a node in X\u2019. We define W\u2019 by W as follows:\n\u2022 For each x \u2208 X, x\u2032 ge(ei) = 0 \u21d4 xei = 0(stei = stge(ei)). \u2022 For each e\u2032ij1 \u2208 Eij1, P (x \u2032 e\u2032ij1 = 0) = 1, i.e. all edges in Eij1 are open. \u2022 For each j, x\u2032 e\u20321j2 = 0, x\u2032 e\u20322j2 = 0, ..., x\u2032 e\u2032nj2 = 0 \u21d4 x\u2032 ge(ej) = 0, i.e. for each\nj, all edges e\u2032ij2 \u2208 E \u2032 ij2 are open if and only if xge(ej) is open.\nThe weight function C \u2032 is defined by:\n1. \u2200ei \u2208 E,C \u2032(ge(ei)) = C(ei).\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 42\n2. \u2200eij1 \u2208 E \u2032 ij1, C \u2032(eij1) = SC(ej) 2 where SC(ej) denote the sensing cost\nof ej .\n3. \u2200eij2 \u2208 E \u2032 ij2, C \u2032(eij2) = \u221e.\nThe computational time that takes to generate this reduction is polyno-\nmial, since the size of |E\u2032| = |E\u2032ij1| + |E \u2032 ij2| + |E\u0302|, where |E \u2032 ij1| = |E \u2032 ij2| = |E| \u00d7 |V | and |E\u0302| = |E|, therefore |E\u2032| = |E| + 2|E| \u00b7 |V | = |E|(1 + 2|V |). Furthermore, the size of |X \u2032| is |E\u2032| since each node in X\u2019 is associated with an edge in E\u2019 , and |Y | = |V ||E| since each node in X\u2019 that is associated with edge in E\u0302 is connected to |E| nodes.\nLet M1 = (S1, A1, Z1, T r1, O1, R1) be a POMDP that modes I, where S1, A1, Z1, T r1, O1, R1 are finite sets of states, actions, observations, transition functions, observation functions and reward functions respectively. Similarly, let M2 = (S2, A2, Z2, T r2, O2, R2) be a POMDP that models I\u2019 where Z2 is the set of observations in I\u2019, S2 is special subsets of the states set in I\u2019, and A2 is a special meta-action set in I\u2019(a set of series of action in I\u2019) which will be defined later. Tr2, O2, R2 are the transition functions, observation functions and reward functions in I\u2019. Let \u03c0\u22171 be the optimal policy of I and \u03c0 \u2032\u2217 2 be the optimal policy of I\u2019. In order to prove theorem 3.1.1, it is suffice to show that Exp(\u03c0\u22171) = Exp(\u03c0 \u2217 2). In the remainder of this proof we prove this property by showing that M1 is equivalent to M2 and that M2 actually models I\u2019.\nWe want to define the subset S2 \u2282 S \u2032 that contains all states in which\nthe agent is located in a \u201d\u2018copied\u201d\u2019 vertex. Formally,\nDefinition 3.1.5. Given S\u2032, we define S2 to be the subset of S\u2019 such that\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 43\nsi \u2208 S2 if and only if si \u2208 S \u2032 and Loc(si) \u2208 V\u0302 . Meaning S2 = {si|si \u2208 S\u2032, Loc(si) \u2208 V\u0302 }.\nLemma 3.1.6. Let V\u0303 be the location space of S2, (i.e. V\u0303 = {v|v = Loc(s2), s2 \u2208 S2}) then V\u0303 = V\u0302 .\nProof. => V\u0303 \u2282 V\u0302 . According to V\u0303 , for every si \u2208 S \u2032 Loc(si) \u2208 V\u0303 if Loc(si) \u2208 V\u0302 . <= V\u0302 \u2282 V\u0303 . V\u2019 contains all possible locations that agent can be in G\u2019,\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 44\nwhere V\u0303 is subset of V\u2019 which contains all possible location in G\u0302. Thus every element in viV\u0302 is in V\u0303\nCorollary 3.1.7. V\u0302 is the location space of S2.\nNow, we want to show that S2 and S1 are equivalent in a sense that there exist a one-to-one correspondence between S2 and S1 . In order to show that we need to make the following definitions and statements,\nDefinition 3.1.8. We define EStatusSet(S) be the set of Estatus(si) of all elements si \u2208 S, meaning EstatusSet(S) = {Estatus(si)|si \u2208 S}.\nIn fact, EstatusSet(S) is the set of all the possible status vectors of E,\nas such EstatusSet(S) = {Open,Blocked}|E|.\nLemma 3.1.9. There exist a one-to-one correspondence between V and V\u0302\nProof. Since gv is a bijection, the exist a one to one correspondence between vi \u2208 V and gv(vi) \u2208 V\u0302\nLemma 3.1.10. There exists a one-to-one correspondence between EstatusSet(S1) and EstatusSet(S2).\nProof. There exist a one to one correspondence between EstatusSet(S1) and EstatusSet(S\u2032) since for all s1 \u2208 S1, s \u2032 \u2208 S\u2032, each element Estatus(s1) \u2208 EstatusSet(S1) can be mapped into a different element Estatus(s \u2032) \u2208 EstatusSet(S\u2032). This is due to the following facts:\n1 (injective)According to definition of W\u2019, \u2200i x\u2032 ge(ei) = 0 \u21d4 xei = 0,\nin other words every edge ei \u2208 E has equal status as its copied edge ge(ei) and thus there exist a one to one correspondence between each\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 45\nset of edge status Estatus(s1) and each set of edge status ste(E\u0302, s1) (In fact for each s1 \u2208 S1 Estatus(s1) = ste(E\u0302, s1)). Since for every s\u2032 \u2208 S\u2032, ste(E\u0302, s\u2032) is a subset of EstatusSet(s\u2032), there exist a one to one mapping between EstatusSet(S1) and EstatusSet(S \u2032)\n2 (surjective) The status of edges ste(E\u2032 \u2212 E\u0302, s\u2032) is completely deter-\nmined and unique, given edges status of edges ste(E\u0302, s\u2032), i.e. ste(E\u0302, s\u2032) = {st(ge(e1), s \u2032), st(ge(e2), s \u2032), .., st(ge(en), s \u2032)}. In particular, there exist exactly one element in EstatusSet(s\u2032) \u2208 EstatusSet(S\u2032) with a given edges status ste(E\u0302, s\u2032), since each variable associated with edges in E\u2032ij2 depends completely on variables associated with edges in E\u0302 (\u2200i, j x\u2032 e\u2032ij2 = 0 \u21d4 x\u2032geej = 0) and the status of all edges in E \u2032 ij1 are predetermined to be open (\u2200i, j P (x\u2032 e\u2032 ij1 = 0) = 1).\nWe are left to show that there exist a one to one correspondence between EstatusSet(S\u2032) and EstatusSet(S2). Since the location of the agent is independent to the edges status, we can represent S2 as a cartezian product S2 = V\u0302 \u00d7 EstatusSet(S2) (According to corollary 3.1.7 V\u0302 is the location space of S2) but according to definition 3.1.5 we can represent S2 as S2 = V\u0302 \u00d7EstatusSet(S \u2032), hence EstatusSet(S\u2032) = EstatusSet(S2). Therefore, there exists a one-to-one correspondence between EstatusSet(S1) and EstatusSet(S2).\nLemma 3.1.11. There exists one-to-one correspondence between S1 and S2.\nProof. According to lemma 3.1.9, there exist a one-to-one correspondence between V and V\u0302 . According to lemma 3.1.10, there exists a one-to-one correspondence between EstatusSet(S1) and EstatusSet(S2). Since S =\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 46\nV \u00d7EstatusSet(S1) and S2 = V\u0303 \u00d7EstatusSet(S2), there exists one-to-one correspondence between S1 and S2.\nDefinition 3.1.12. Let vi, vj \u2208 V , v \u2032 ij1 \u2208 Vij1, v\u0302i, v\u0302j \u2208 V\u0302 and ej \u2208 E. We define a2 in I\u2019 to be the equivalent meta-action to action a1 \u2208 A1(a1 \u223c a2) if and only if:\na2 =\n   move(v\u0302i, v\u0302j) if a \u2208 move(vi, vj)\nmove(v\u0302i, v \u2032 ij1),move(v \u2032 ij1, v\u0302i) if a \u2208 sense(vi, ej)\nDefinition 3.1.13. We define A2 to be the set of all equivalent actions of actions in A1. Meaning A2 = {a2|a2 \u223c a1, a1 \u2208 A1}\nDefinition 3.1.14. We define the set s\u0303tei to be the following:\ns\u0303tei =\n   {Oge(ei), Oe1i2 , Oe2i2 , ..., Oeni2} if stei = Oei\n{Bge(ei), Be1i2 , Be2i2 , ..., Beni2} if stei = Bei\nDefinition 3.1.15. Let Z2 be a set of observations in I\u2019 and Z1 be a set of observations in I. We define z2 \u2208 Z2 is the equivalent observation of z1 \u2208 Z1(denoted by z1 \u223c z2) if and only if:\nz2 = {s\u0303tei1 \u2229 s\u0303tei2 \u2229 ... \u2229 s\u0303teim} if z1 = {stei1 , stei2 , ..., steim} (3.3)\nLemma 3.1.16. The cost of action in A1 is equal to the cost of the equivalent meta-action in A2.\nProof.\n1 C(move(vi, vj)) = C(move(v\u0302i, v\u0302j)) (by definition of the weight func-\ntion).\n2 C(sense(vi, ej)) = SC(ej), and C(move(v\u0302i, v \u2032 ij1)) = C(move(v \u2032 ij1, v\u0302i)) =\nSC(e\u2032j)\n2 . Therefore, C(sense(vi, ej)) = C(move(v\u0302i, v \u2032 ij1))+C(move(ij1, v\u0302i))).\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 47\nLemma 3.1.17. Given s1 \u2208 S1, a1 \u2208 A1, z1 \u2208 Z1, and s2 \u2208 S2, a2 \u2208 A2, z2 \u2208 Z2 such that s1 \u223c s2, a1 \u223c a2, z1 \u223c z2, then O1(s1, a1, z1) = O2(s2, a2, z2).\nProof.\n1 In case that a1 = move(vj , vi). Let ei1 , ei2 , ..., ein \u2208 Evi(the edges inci-\ndent to vi) and let E\u0303vi = {s\u0303tei1\u2229s\u0303tei2\u2229...\u2229s\u0303teim}. By definition of CTP, if a1 = move(vj , vi) then the agent observes z1 = stei1 , stei2 , ..., stein (the pre-known edges incident to vi which are revealed by the action). Therefore O1(s1, a1, z1) = 1 if z1 = ste(Evi , s1) and O1(s1, a1, z1) = 0 otherwise. The equivalent action of a1 is a2 = move(v\u0302i, v\u0302j), hence, taking a2, the agent directly observes ste\u0302i1 , ste\u0302i2 , ..., ste\u0302in , but in addition, according to definition 3.1 x\u2032 e\u20321i2 = 0, x\u2032 e\u20322i2 = 0, ..., x\u2032 e\u2032ni2 = 0 \u21d4 x\u2032e\u0302i = 0, hence the agent also indirectly observes edges e1i2, e2i2, ..., eni2 . Thus, the agent\u2019s overall observation is z2 = E\u0303vi . Since z1 \u223c z2, by definition 3.1.15 if z1 = ste(Evi , s1) then z2 = E\u0303vi . Hence O2(s2, a2, z2) = 1 if z1 = ste(Evi , s1) and O2(s2, a2, z2) = 0 otherwise. Thus, in this case O1(s1, a1, z1) = O2(s2, a2, z2)\n2 In case that a1 = sense(ej) the agent observes z1 = stej therefore\nO1(s1, a1, z1) = 1 if z1 = stej and O1(s1, a1, z1) = 0 otherwise. Since a1 \u223c a2, a2 = (move(v\u0302i, v \u2032 ij1),move(v \u2032 ij1, v\u0302i)) where agent observes ste\u0302j directly and observes ste1j2 , ste2j2 , ..., stenj2 indirectly(the same cause as in case 1). Thus, the agent\u2019s overall observation is z2 = s\u0303tej . Similarly to case 1, since z1 \u223c z2, by definition 3.1.15 if z1 = stej then z2 = tildeEvi . Hence O2(s2, a2, z2) = 1 if z1 = stej and O2(s2, a2, z2) = 0 otherwise. Thus, in this case O1(s1, a1, z1) = O2(s2, a2, z2) as well.\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 48\nLemma 3.1.18. Given s1a \u2208 S1, s1b \u2208 S1, a1 \u2208 A1, z1 \u2208 Z1, and s2a \u2208 S2, s2b \u2208 S2, a2 \u2208 A2, z2 \u2208 Z2 such that action a1 is taken in s1a and meta-action a2 is taken in s2a, then if s1a \u223c s2a, s1b \u223c s2b, a1 \u223c a2 then Tr1(s1a, a1, s1b) = Tr2(s2a, a2, s2b).\nProof. WLOG, let Loc(s1a) = vi, and Loc(s1b) = vj . Since s1a \u223c s2a, s1b \u223c s2b we get Loc(s1a) = v\u0302i and Loc(s1b) = v\u0302j .\n\u2022 WLOG, in case that a1 = move(vi, vj). If (v1, v2) \u2208 E we get Tr1(s1b, a, s1a) =\n1 otherwise Tr1(s1b, a, s1a) = 0. Furthermore, if (vi, vj) \u2208 E then (gv(vi), gv(vj)) \u2208 E\u0302 which incurs Tr2(s2b, a, s2a) = 1, otherwise Tr2(s2b, a, s2a) = 0. Thus, in case that a1 = move(vi, vj) we get Tr1(s1b, a, s1a) = Tr2(s2b, a, s1a).\n\u2022 WLOG, in case that a1 = sense(ej , vi). Since the sense action does\nnot change the location of the agent we get s1a = s1b. Since a1 \u223c a2 a2 = move(v\u0302i, vij1),move(vij1, v\u0302i). In this case Loc(s2a) = Loc(s2b) since the agent return to it original location v\u0302i. This incurs s2a = s2b and thus Tr1(s1b, a, s1a) = Tr2(s2b, a, s2a) = 1.\nLemma 3.1.19. Given states s1 \u2208 S1, s2 \u2208 S2, a1 \u2208 A1, a2 \u2208 A2 then if s1 \u223c s2 and a1 \u223c a2 then R1(s1, a1) = R2(s2, a2).\nProof. WLOG, let Loc(s1) = v1. Given that s1 \u223c s2 then Loc(s2) = v\u03021.\n\u2022 In case that a1 = move(vi, vj), R1(s1, a1) = C(move(vi, vj)). If a1 \u223c\na2 then a2 = move(v\u0302i, v\u0302j). Since C(move(vi, vj)) = C(move(v\u0302i, v\u0302j)) we get R1(s1, a1) = R2(s2, a2).\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 49\n\u2022 In case that a1 = sense(vi, ej), R1(s1, a1) = C(sense(vi, ej)). If a1 \u223c\na2 then a2 = move(vi, vij1),move(vij1, vi). Since C(sense(vi, ej)) = C(move(v\u0302i, v \u2032 ij1)) + C(move(v \u2032 ij1, v\u0302i))) we get R1(s1, a1) = R2(s2, a2).\nLemma 3.1.20. M1 is equivalent to M2.\nProof. We have shown that there exist a one to one correspondence between S1 and S2. By defining the set A2 which consist of equivalent action in A1, and by defining the set Z2 which consist of equivalent observations in Z1, we have shown that functions Tr1 = Tr2, O1 = O2, and R1 = R2 when generated on equivalent set of states, observation and actions.\nLemma 3.1.21. M2 models the problem of I\u2019.\nProof. Here we show that although M2 models a subproblem of I\u2019 (M2 is defined on subsets of states, actions of I\u2019), it actually models the exact problem of I\u2019. For every state s \u2208 S\u2032 \u2212 S2, Loc(s) = vij1 where vij1 \u2208 Vij1. An agent located in vij1 can only move to vi. In addition, in order that agent would be located in vij1 it has to move from vi. Thus A2 replace the two move actions in to one meta-action and thus we can reduce the state set of S\u2019 into the subset S2. Therefore, M2 models I\u2019.\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 50"}, {"heading": "3.2 CTP-Forward-Arcs", "text": "Definition 3.2.1. Let G = (V,E) be a disjoint paths graph of CTP-PATHDEP and W = (X,Y ) be its associated Bayesian network. Let xeij , xeik \u2208 X be the associated node of edges eij and eik (note that the edges are in the same path i in G). Then the arc \u2329 xei,j , xei,k \u232a \u2208 Y is Forward-Arc if j < k, i.e. if eij is closer to s than eik.\nDefinition 3.2.2. CTP-Forward-Dependency(CTP-FOR-DEP) is a special case of CTP-PATH-DEP such that all the arcs in W are Forward-Arcs.\nTheorem 3.2.3. CTP-FOR-DEP is solvable in polynomial time.\nProof outline. CTP on disjoint paths graph with independent distribution over the edges(CTP-PATH-IND) is shown to be solvable in polynomial time [Bnaya, Felner and Shimony, 2009]. We show that we can transform CTP-FOR-DEP into an instance of CTP-PATH-IND with new distribution over the edges such that the optimal policy of the new CTP-PATH-IND can be applied to CTP-FOR-DEP.\nProof. Let I = (G,W,w, s, t) be an instance of CTP-FOR-DEP. We construct a new instance I \u2032 = (G,W \u2032, w, s, t) of CTP-PATH-IND by constructing a new Bayesian network W\u2019(X\u2019,Y\u2019) of I\u2019 such that\n\u2022 Y \u2032 = {}. In other words W\u2019 is \u201carc free\u201d where each node is an\nindependent component in the BN.\n\u2022 \u2200x\u2032eik \u2208 X \u2032 P (x\u2032eik = 1) = P (xeik = 1|xei1 = 0, xei2 = 0, ..., xei(k\u22121) =\n0).\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 51\nLet M = (B,A, Tr,R) be a belief state MDP of I, where B is the set of belief states , A is the set of actions, Tr is a set of transition probabilities, R is the reward function. We construct a new belief state MDP M \u2032 = (B\u2032, A, T r\u2032, R\u2032) of I\u2019 where B\u2019 is the set of belief states, A is a set of actions which is common to the set of action in I (since it refers to the same graph G), Tr\u2019 is a set of transition probabilities, and R\u2019 is the reward function .\nDefinition 3.2.4. Let f : B \u2192 B\u2032 be a function defined as follows: Let b \u2208 B and b\u2032 \u2208 B\u2032 such that f(b) = b\u2032 then \u3008b\u3009 = \u3008b\u2032\u3009.\nNotice that f(b) is well defined since there is a one to one mapping from\n\u3008b\u3009 to b and from \u3008b\u2032\u3009 to b\u2032.\nLemma 3.2.5. Let b, b\u0302 be reachable belief states in B and let a \u2208 A be an action. Then Tr(b\u0302|a, b) = Tr(f(b\u0302)|a, f(b))\nProof. Let a = Move(e) where e = \u2329 vi(k\u22121), vi(k) \u232a and let ef = for(e)\n1 In case that Tr(b\u0302|a, b) = 0 (i.e. action a not performable in b), then\nTr\u2032(f(b\u0302)|a, f(b)) = 0. If Tr(b\u0302|a, b) = 0 then one of the following cases must satisfied:\n\u2022 Edge e is not adjacent to the location of the agent in b, i.e. e /\u2208\nInc(LocB(b)). If e /\u2208 Inc(LocB(b)) then e /\u2208 Inc(LocB(f(b)) since LocB(b) = LocB(f(b)). Thus Tr\u2032(f(b\u0302\u2032)|a, f(b)) = 0.\n\u2022 Edge e is not adjacent to location of the agent in b\u0302, i.e. e /\u2208\nInc(LocB(b\u0302)). If e /\u2208 Inc(LocB(b\u0302)) then e /\u2208 Inc(LocB(f(b\u0302)), since LocB(b\u0302) = LocB(f(b\u0302)). Thus Tr\u2032(f(b\u0302)|a, f(b)) = 0.\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 52\n\u2022 Edge e is blocked in belief state b, i.e. stb(b, e) = Be. If stb(b, e) =\nBe then stb(f(b\u0302), e) = Be since stb(b, e) = stb(f(b\u0302), e). Thus Tr\u2032(f(b\u0302)|a, f(b)) = 0.\n\u2022 There exist an edge e\u2032 6= For(e) such that stb(b, e\u2032) 6= stb(b\u0302, e\u2032).\nSince stb(b, e) = stb(f(b), e) and stb(f(b\u0302), e) = stb(b\u0302, e) , there exist an edge e\u2032 6= For(e) Since is blocked in belief state b, i.e. stb(b, e) = Be. If stb(b, e) = Be then stb(b\u0302, e) = Be, since stb(b, e) = stb(f(b\u0302), e). Thus Tr\u2032(f(b\u0302)|a, f(b)) = 0.\n2 In case that Tr(b\u0302|a, b) > 0 (i.e action a is performable in b) then edge\ne has to be open and one of the following cases must satisfied:\n\u2022 Edge ef is Open in b (i.e stb(b, ef ) = Oef ). If stb(b, ef ) = Oef\nthen the status of all edges in b must be the same as in b\u0302, i.e. (\u2200e \u2208 Estb(e, b) = stb(e, b\u0302)) since the agent does not sense any unknown edge when performing a and hence Tr(b\u0302|a, b) = 1. If stb(b, ef ) = Oef then stb(f(b), ef ) = Oef and the status of all edges in b must be the same as in b\u0302 from the same reasons as before. Hence Tr\u2032(f(b\u0302)|a, f(b)) = 1, and we have Tr(b\u0302|a, b) = Tr\u2032(f(b\u0302)|a, f(b)).\n\u2022 Edge ef is Unknown in b (i.e stb(b, ef ) = Uef ). Since W is the\nbelief network of CTP-FOR-DEP and b is reachable from b0, the status of all edge ei1, ei2, ..., eik have to be Open (In order to reach vik all edges in path i from s to vik must be traversable). Thus,\nTr(b\u0302|a, b) = P (xef = 1|xei1 = 0, xei2 = 0, ..., xeik = 0)\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 53\nIn addition stb(f(b), ef ) = Uef (since stb(f(b), ef ) = stb(b, ef )). There is no dependencies in W\u2019 (i.e. Y \u2032 = {}). Therefore,\nTr\u2032(b\u0302|a, b)) = P (x\u2032ef = 1).\nHowever, by definition of X\u2019 we have,\nP (x\u2032ef = 1) = P (xef = 1|xei1 = 0, xei2 = 0, ..., xeik = 0)\nHence,\nTr(b\u0302|a, b) = Tr\u2032(f(b\u0302)|a, f(b))\nLemma 3.2.6. Let b, b\u0302 be reachable belief states in B and let a \u2208 A be an action. Then R(b, a, b\u0302) = R\u2032(f(b), a, f(b\u0302)).\nProof. From definition 2.5 it follows that: R(b, a, b\u0302) = R\u2032(f(b), a, f(b\u0302)) if and only if Tr(b\u0302|a, b) = Tr\u2032(f(b\u0302)|a, f(b)). But we proved in lemma 3.2.5 that Tr(b\u0302|a, b) = Tr\u2032(f(b\u0302)|a, f(b)) Thus, R(b, a, b\u0302) = R\u2032(f(b), a, f(b\u0302)).\nDefinition 3.2.7. We define the predicate REACHABLE(bn, b0) to be true if and only if bn is reachable from b0 in belief-MDP M. i.e. there exist b1, ..., bn\u22121 such that \u220f\n0\u2264i\u2264n\u22121,a\u2208A Tr(bi, a, bi+1) > 0\nLemma 3.2.8. Let b \u2208 B. Then REACHABLE(b, b0) is true if and only if REACHABLE(f(b), f(b0)) is true.\nProof. Follows from definition 3.2.7 and lemma 3.2.5.\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 54\nDefinition 3.2.9. Define the set Breach \u2282 B to be the set of all belief states b \u2208 B that satisfy REACHABLE(b, b0). Namely,\nBreach = {b|b \u2208 B,REACHABLE(b, b0)}\nNext, we define an analogue set for B\u2019,\nDefinition 3.2.10. Define the set B\u2032reach \u2282 B to be the set of all belief states f(b) \u2208 B\u2032 , for b \u2208 B, that satisfy REACHABLE(f(b), f(b0)). Namely,\nB\u2032reach = {f(b)|f(b) \u2208 B \u2032, b \u2208 B,REACHABLE(f(b), f(b0))}\nLet Mr = (Breach, A, T r,R) be a belief MDP over belief state Breach and\nlet M \u2032r = (B \u2032 reach, A, T r \u2032, R\u2032) be a belief MDP over belief state B\u2032reach Lemma 3.2.11. Mr and M \u2032 r are isomorphism.\nProof. Follows from definition 3.2.7 that F is a bijection over Breach and B\u2032reach. In addition, F preserves the function Tr , Tr\u2019 lemma 3.2.5 as well as R,R\u2019 lemma 3.2.6 .\nCorollary 3.2.12. Let \u03c0\u2217 be the optimal policy of I, and \u03c0\u2032\u2217 be the optimal policy of I\u2019. Then for every reachable belief state b \u2208 Breach we have \u03c0 \u2217(b) = \u03c0\u2032\u2217(f(b)).\nProof. Since Mr and M \u2032 r are isomorphism, the problems are equivalent and their optimal solutions are equivalent.\nTherefore, we can transform any instance of CTP-FOR-DEP into an instance of CTP-PATH-IND, apply the algorithm which solves CTP-PATH-\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 55\nIND in polynomial time, and equivalent optimal solution is guaranteed (corollary 3.2.12).\nNow, we show that determining the probability P (xei,k = 1|xei,1 = 0, xei,2 = 0, ..., xei,k\u22121 = 0) for all nodes can be computed in polynomial time. We use the Bayesian theorem to get:\nP (xei,k = 1|xei,1 = 0, xei,2 = 0, ..., xei,k\u22121 = 0) = P (xei,k = 1, xei,1 = 0, xei,2 = 0, ..., xei,k = 0) \u00b7 P (xei,k = 1|xei,1 = 0, xei,2 = 0, ..., xei,k\u22121 = 0)\n(3.4)\nWe use the chain rule to get:\nP (xei,k = 1, xei,1 = 0, xei,2 = 0, ..., xei,k = 0) = P (xei,k = 1|xei,1 = 0, xei,2 = 0, ..., xei,k\u22121 = 0) \u00b7 P (xei,k = 1, xei,1 = 0, xei,2 = 0, ..., xei,k\u22121 = 0)\n(3.5)\nThe variables in the Bayesian network are topologically ordered by their order in the path and hence each probability P (xeik) can be iteratively computed given that its ancestors values have already been determined(using equations 3.4,3.5). Therefore, inferring the probability of each edge takes linear time and inferring the probability of all edges takes O(|E|2). Thus computing the optimal policy takes polynomial time.\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 56"}, {"heading": "3.3 CTP-PATH-DEP", "text": "Definition 3.3.1.\nCTP-PATH-DEP is a special case of CTP-DEP where the associated\nBayesian network has dependencies only between edges on the same path.\nTheorem 3.3.2. CTP-PATH-DEP is NP-hard.\nProof outline By reduction from 3-SAT to CTP-PATH-DEP.\nProof. Let L be a set boolean variables l1, ..., ln. Let the 3CNF formula F be a conjunction of the clauses C1, C2, ..., Ck where each clause Ci is a disjunction of three literals l\u2032i 1, l\u2032i 2, l\u2032i 3 and for each literal l\u2032i j it holds that l\u2032i j \u2208 L or \u00acl\u2032i j \u2208 L. We construct the instance I = (G,W,w, s, t) of CTPPATH-DEP from F, such that F is satisfiable if and only if the expected cost of the optimal policy is greater than some given constant. I is defined as follows: G = (V,E) is a graph consisting two disjoint paths p1, p2, where\n1. p1 = \u2329 eY , ed1, ..., ed(k\u22121), ec1 , ..., eck , el1 , ..., eln , eR \u232a (The edges are or-\ndered from the edge incident to s to the edge incident to t). Edges ec1 , ..., eck correspond to clauses C1, ..., Ck respectively, and edges el1 , ..., eln correspond to variable l1, ..., ln respectively. The correspondence will be define later in this proof.\n2. p2 consist of a single edge eL.\nw is the weight function over the edges, is defined by:\n\u2022 w(eY ) = 1\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 57\n\u2022 w(eL) = (1 + k\n2n+1 )\n\u2022 w(e) = 0 for all other edges.\nW = (X,Y ) is a Bayesian network.\nDefinition 3.3.3. For every edge e \u2208 E in path p1, we define the variable xe \u2208 X to be the variable corresponded to edge e such that xe = 0 if and only if e is Open.\nThe set of node X of W is a union of the following sets:\n\u2022 XY = xY . XY is a set that contains the single variable xY .\n\u2022 XR = xR. XR is a set that contains the single variable xR.\n\u2022 Xl = {xl1 , xl2 , ..., xln}. Xl is a set that contains all nodes that corre-\nspond to variables l1, ..., ln.\n\u2022 Xc = {xc1 , xc2 , ..., xck}. Xc is a set that contains all nodes that corre-\nspond to variables c1, ..., ck..\n\u2022 Xd = {xd1 , xd2 , ..., xdk\u22121}. Xc is a set that contains all nodes that\ncorrespond to variables d1, ..., dk\u22121..\nNamely, X = XY \u222aXR \u222aXl \u222aXc \u222aXd. The arcs in Y are defined by the followng sets:\n\u2022 YRli = {\u3008xR, xli\u3009}. An arc from node xR to node xli .\n\u2022 YRci = {\u3008xR, xci\u3009}. An arc from node xR to node xci .\n\u2022 YRdi = {\u3008xR, xdi\u3009}. An arc from node xR to node xdi .\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 58\n\u2022 Yli = { \u2329\nxl1i , xci\n\u232a , \u2329\nxl2i , xci\n\u232a , \u2329\nxl3i , xci\n\u232a\n}. A set of three arcs from each\nvariable node x l j i\n(for 1 \u2264 j \u2264 3) to clause node xci such that l j i is the\nvariable corresponding to literal l\u2032i j . For instance C5 = {l \u2032 5 1 \u2228 \u00acl\u20325 2 \u2228 \u00acl\u20325 3} then l15 = l \u2032 5 1, l\u20325 2 = \u00acl\u20325 2, and l35 = \u00acl \u2032 5 3.\n\u2022 \u22001\u2264i\u2264k (xci, xdi) \u2208 Y - Arc from each node xci to a corresponding node\nxdi\n\u2022 \u22001\u2264i\u2264k (xdi, xY ) \u2208 Y - Arc from each node xdi to node xY\nThe condition probabilities of W are as follows:\n1 P (xeR=0) = 0.5 (xR is an independent variable).\n2 For every variable node xi it holds that P (xi = 0|xR = 0) = 1, i.e. if\nxR = 0 then path p1 is always open with probability 1.\n3 Given xR = 1, W is specified as follows:\n(a) xci = 0) \u21d4 \u22273 j=1 xlij = 0 (b) xd1 = 0 \u21d4 xc1 = 0\n(c) \u2200i > 1 xd(i+1) = 0 \u21d4 xci = 0, xdi = 0 (d) xY = 1 \u21d4 \u2227 i xdi = 0\nThe reduction maps each variable of F to a variable of W such that,\n\u2022 Each boolean SAT variable li is mapped to a binary variable in the\nBayes network xli, such that xli = 0 if and only if li = T\n\u2022 Each clause Ci is mapped to binary variable xci such that xci = 0 if\nand only if Ci = T .\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 59\nLemma 3.3.4. Given xR = 1, then F is satisfiable \u21d4 xY = 0.\nProof. If xR = 1 then F is satisfiable\u21d4 C1 = T,C2 = T, ..., Cn = T \u21d4 xc1 = 0, xc2 = 0, ..., xcn = 0 in addition, xc1 = 0 \u21d4 xd1 = 0 and \u2200i > 1 xd(i+1) = 0 \u21d4 xci = 0, xdi = 0. Thus, xc1 = 0, xc2 = 0, ..., xcn = 0 \u21d4 xd1 = 0, xd2 = 0, ..., xdn = 0 \u21d4 xY = 0\nFor simplicity w(eY ) is denoted by CY and w(eL) is denoted by CL.\nNote that CL > CY .\nThe construction of the reduction is computable in polynomial time since the graph G contains O(n) vertices, O(n) edges and the Bayes network W\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 60\ncontains O(n) nodes and O(n) arcs. In addition, function g, which maps each variable in F to variable in W , is computable in polynomial time as well.\nThe optimal policy is committing in a sense that after the agent chooses a path, it keeps following this path until reaching t, unless agent hits a blocked edge. This is caused due to the fact that if agent chooses to traverse p1 first, after traversing the first edge eY , it is optimal to keep following p1 toward t since the rest of the edges in p1 are 0, and thus if p1 is traversable,\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 61\nno extra travel cost is paid. On the other hand, if p1 is not traversable then the agent pays extra CY regardless of how many edges did it traversed in p1. Therefore the decision problem of the optimal policy here is simply whether to choose p1 as a first path to try or p2.\nNotation 3.3.5. Let \u03c012 denote a committing policy that chooses p1 as a first path to try, and \u03c021 denote a committing policy that chooses p2 as a first path to try.\nLemma 3.3.6. Let C be a constant, such that 1 + k 2n+2 < C < 1 + k 2n+1 , where k is the number of models in F and n is the number of boolean-SATvariables in F. Let \u03c0\u2217 be the optimal policy of I. F is satisfiable if and only if Exp(\u03c0\u2217) > C\nProof. => Suppose that F is satisfiable. The probability that eY is open is\nP (xY = 0) = P (xY = 0|xR = 0)P (xR = 0) + P (xY = 0|xR = 1)P (xR = 1)\nby construction of W:\nP (xY = 0|xR = 0) = 1\nP (xR = 0) = P (xR = 1) = 0.5\n(3.6)\nThe probability P (F = True) = k2n since there are k sets of literals of F such that its instantiation gives F=true, and the domain size is the number\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 62\nof all possible instantiations to l1, ..., ln, which equals 2 n. Thus,\nP (xY = 0|xR = 1) = k\n2n (3.7)\n=> P (xY = 0) = 0.5 + 0.5 \u00b7 k\n2n . (3.8)\nLet PY denote the probability P (xY = 0).\nNow, we want to calculate the probability that path p1 is open given\nthat ey is open.\nP (p1 open|xY = 0) = P (p1 open, xY = 0)\nP (xY = 0) =\nP (p1 open, xY = 0|xR = 0)P (xR = 0) + P (p1 open, xY = 0|xR = 1)P (xR = 1)\nP (xY = 0)\n(3.9)\nAccording to W:\nP (p1 open, xY = 0|xR = 0) = 1 (3.10)\nif xR = 1 then p1 is blocked\nP (p1 open, xY = 0|xR = 1) = 0 (3.11)\nSetting equations 3.8,3.10,3.11 in equation 3.9 gives:\nP (p1 open|xY = 0) = 0.5\n0.5 + 0.5 \u00b7 k2n =\n1\n1 + k2n =\n0.5 PY . (3.12)\nDenote w(p1) to be the sum cost of all edge in p1 and w(p2) to be the sum\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 63\ncost of all edge in p2. The expected cost of the policy when choosing first path p1 is\nExp(\u03c012) =\nEy is open and path p1 is open \ufe37 \ufe38\ufe38 \ufe37 P (p1 open|xY = 0)P (xY = 0) \u00b7 w(p1)\n+\nEy is open and path p1 is blocked \ufe37 \ufe38\ufe38 \ufe37 P (p1 blocked|xY = 0)P (xY = 0) \u00b7 (2w(p1) + w(p2))+ Ey is blocked \ufe37 \ufe38\ufe38 \ufe37 P (xY = 1) \u00b7 w(p2)\n= 0.5\nPY \u00b7 PY \u00b7 w(eY ) + (1\u2212\n0.5 PY ) \u00b7 PY \u00b7 (2w(eY ) + w(e2)) + (1\u2212 PY ) \u00b7 w(e2)\n= 2PY CY + 0.5(CL\u2212 CY )\n(3.13)\nNote that in case that the agent traverses eY and p1 is blocked, the agent hits a blocked edge and is forced to pay another CY extra, when the agent moves backward to s. The expected cost of \u03c021 is simply CL. Since CL < 2PY CY +0.5(CL\u2212CY ), the optimal policy is \u03c021 and Exp(\u03c0 \u2217) = CL. It is given that CL = 1+ k2n+1 > C, therefore if F is satisfiable then Exp(\u03c0 \u2217) > C.\n<= Suppose that F is not satisfiable. Now, the calculation of the probability is easier because we know that the only case where eY is open is when eR = 0.\nTherefore,\nP (p1 open, xY = 0|xR = 0) = 1 (3.14)\nP (p1 open, xY = 0|xR = 1) = 0 (3.15)\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 64\nP (xY = 0) = P (xY = 0|xR = 0)P (xR = 0) + P (xY = 0|xR = 1)P (xR = 1)\n= 1 \u00b7 0.5 + 0 \u00b7 0.5 = 0.5 (3.16)\nAccording to equations 3.11,3.14,3.15,3.16\nP (p1 open|xY = 0) = 1 \u00b7 0.5\n0.5 = 1 (3.17)\n\u21d2 P (p1 blocked|xY = 0) = 0 (3.18)\nThus if eY is open then p1 is open. The expected cost of \u03c012 is\nExp(\u03c012) =\nEy is open and path p1 is open \ufe37 \ufe38\ufe38 \ufe37 P (p1 open|xY = 0)P (xY = 0) \u00b7 w(p1)\n+ P\nEy is open and path p1 is blocked \ufe37 \ufe38\ufe38 \ufe37 (p1 blocked|xY = 0)P (xY = 0) \u00b7 (2w(p1) + w(p2))+ Ey is blocked \ufe37 \ufe38\ufe38 \ufe37 P (xY = 1) \u00b7 w(p2)\n= 1 \u00b7 0.5 \u00b7 CY + 0 \u00b7 0.5 \u00b7 (2CY + CL) + 0.5 \u00b7 CL\n= 0.5 \u00b7 (CY + CL)\n(3.19)\nAgain the expected cost of \u03c021 is CL. Since CL > CY \u21d2 CL > 0.5 \u00b7 (CY + CL) the optimal policy is \u03c012 and therefore Exp(\u03c0 \u2217) = 0.5 \u00b7 (CY + CL) = 0.5(1 + 1 + k 2n+1 ) = 1 + k 2n+1 . It is given that 1 + k 2n+1 < C. and thus if F is not satisfiable then Exp(\u03c0\u2217) < C\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 65"}, {"heading": "3.4 Theoretical Properties of Belief-MDP for CTP", "text": "In the following section, we are given an instance I = (G,P,w, s, t) of CTP, where G = (V,E). We construct a belief state MDP MS = (B,A, Tr,R, b0) of I, where S is the state set of I.\nDefinition 3.4.1. Policy \u03c0 is called finite if the AO-graph for \u03c0 is acyclic(DAG).\nNotation 3.4.2. Denote the expected cost of the optimal policy of MS in belief state b as C\u2217(b); namely, C\u2217(b) \u2261 C\u03c0 \u2217 (b).\nIf the AO-graph for policy \u03c0 is acyclic then C\u03c0 is finite [Bonet, 2010]. By definition, there is a traversable edge \u3008s, t\u3009 in G. Therefore, there is a policy \u03c0 with finite cost and hence C\u03c0 \u2217 is finite [Bonet, 2010]. It should be noted that all policies referred to this section are finite.\nDefinition 3.4.3. The predicate MoreBlocked(b1, b2), defined over b1, b2, is true if and only if the following properties are satisfied:\n1. Loc(b1) = Loc(b2)\n2. For all e \u2208 E,\n\u2022 stb(e, b1) = Open if and only if stb(e, b2) = Open.\n\u2022 stb(e, b1) = Blocked if stb(e, b2) = Blocked.\n\u2022 stb(e, b1) = Unknown if stb(e, b2) = Unknown or if stb(e, b2) =\nBlocked.\nThe predicateMoreBlocked(b1, b2) indicates that \u201cb1 is at least as blocked\nas b2\u201d, meaning if the pair b1, b2 satisfiesMoreBlocked(b1, b2) thenBlocked(b1) \u2286 Blocked(b2).\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 66\nLet E = {e1, e2, e3}. We demonstrate MoreBlocked(b1, b2) by the fol-\nlowing table:\n\u3008b1\u3009 \u3008b2\u3009 MoreBlocked(b1, b2) Reason\n\u3008v1, Be1 , Oe2 , Ue3\u3009 \u3008v1, Be1 , Oe2 , Ue3 \u3009 TRUE All properties are satisfied \u3008v1, Be1 , Oe2 , Ue3\u3009 \u3008v1, Be1 , Oe2 , Be3\u3009 TRUE All properties are satisfied \u3008v1, Ue1 , Oe2 , Ue3\u3009 \u3008v1, Oe1 , Oe2 , Be3\u3009 FALSE stb(e1, b1) = Unknown and\nstb(e1, b2) = Open\nDefinition 3.4.4. Let b1, b2 \u2208 B. Define the predicate MoreOpen(b1, b2) to be true if and only if the following properties are satisfied:\n1. Loc(b1) = Loc(b2)\n2. For all e \u2208 E,\n\u2022 stb(e, b1) = Blocked if and only if stb(e, b2) = Blocked. \u2022 stb(e, b1) = Open if stb(e, b2) = Open. \u2022 stb(e, b1) = Unknown if stb(e, b2) = Unknown or if stb(e, b2) =\nOpen.\nIntuitively, MoreOpen(b1, b2) means that \u201cb2 is at least as open as b1\u201d, where the set of known open edges in b1 is contained in the set of known open edges in b2.\nWe demonstrate MoreOpen(b1, b2) by the following table:\n\u3008b1\u3009 \u3008b2\u3009 MoreOpen(b1, b2) Reason\n\u3008v1, Be1 , Oe2 , Ue3\u3009 \u3008v1, Be1 , Oe2 , Ue3 \u3009 TRUE All properties are satisfied \u3008v1, Be1 , Oe2 , Ue3\u3009 \u3008v1, Be1 , Oe2 , Oe3 \u3009 TRUE All properties are satisfied \u3008v1, Ue1 , Oe2 , Be3\u3009 \u3008v1, Be1 , Oe2 , Be3\u3009 FALSE stb(e1, b1) = Unknown and\nstb(e1, b2) = Blocked\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 67\nDefinition 3.4.5. We define the function BlockEdges : P(E) \u00d7 B \u2192 B as follows: Let b1, b2 \u2208 B such that b1 = BlockEdges(E\u0302, b2), then \u3008b1\u3009 is defined by the following(by its elements):\n1. Loc(b1) = Loc(b2).\n2. For all e \u2208 E\u0302 stb(e, b1) = Blocked.\n3. For all e /\u2208 E\u0302 stb(e, b1) = stb(e, b2)\nNote that by corollary 2.5.9, b1 can be determined from \u3008b1\u3009. The function is called BlockEdges(E\u0302, b2) since it \u201cblocks\u201d all the edges in E\u0302 (i.e. for every edge e \u2208 E\u0302 the function BlockEdges(E\u0302, b2) \u201cchanges\u201d the status of edge e in b2 to stb(e, b1) = Blocked) where all the other element in \u3008b2\u3009 are remained unchanged in \u3008b1\u3009.\nFor example, we are given belief state b such that \u3008b\u3009 = \u3008v1, Be1 , Ue2 , Oe3 , Ue4 , Ue5\u3009\nand E\u0302 = {e2, e4, e5}. Hence, if b \u2032 = BlockEdges(b, E\u0302) then \u3008b\u2032\u3009 = \u3008v1, Be1 , Be2 , Oe3 , Be4 , Be5\u3009\nProperty 3.4.6. For every belief state b and a set of edges E\u0302 \u2282 E we have MoreBlocked(E\u0302, BlockEdges(E\u0302, b)).\nProof. Follows immediately from definition 3.4.5.\nDefinition 3.4.7. We define the function OpenEdges : P(E) \u00d7 B \u2192 B as follows: Let b1, b2 \u2208 B such that b1 = OpenEdges(E\u0302, b2), then \u3008b1\u3009 is defined by the following(by its elements):\n1. Loc(b1) = Loc(b2).\n2. For all e \u2208 E\u0302 stb(e, b1) = Open.\n3. For all e /\u2208 E\u0302 stb(e, b1) = stb(e, b2)\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 68\nThe function is called OpenEdges(E\u0302, b2) since it \u201copen\u201d all the edges in E\u0302 (i.e. for every edge e \u2208 E\u0302 the function OpenEdges(E\u0302, b2) \u201cchanges\u201d the status of edge e in b2 to stb(e, b1) = Open) where all the other element in \u3008b2\u3009 are remained unchanged in \u3008b1\u3009.\nFor example, we are given belief state b such that \u3008b\u3009 = \u3008v1, Be1 , Ue2 , Oe3 , Ue4 , Ue5\u3009\nand E\u0302 = {e2, e4, e5}. Hence, if b \u2032 = OpenEdges(b, E\u0302) then \u3008b\u2032\u3009 = \u3008v1, Be1 , Oe2 , Oe3 , Oe4 , Oe5\u3009\nProperty 3.4.8. For every belief state b and a set of edges E\u0302 \u2282 E we have MoreOpen(E\u0302, OpenEdges(E\u0302, b)).\nProof. Follows immediately from definition 3.4.7.\nDefinition 3.4.9. We define the function BlockEdges\u22121 : P(E) \u00d7 B \u2192 P(B) as follows: B2 = BlockEdges \u22121(E\u0302, b1) is the set of all belief states b2 \u2208 B such that b1 = BlockEdges(E\u0302, b2). Meaning BlockEdges \u22121(E\u0302, b1) = {b2|b1 = BlockEdges(E\u0302, b2)}.\nNote that the function BlockEdges\u22121 is somehow a generalization of an inverse function in a way that for every b \u2208 B and E\u0302 \u2208 P(E) we get b = BlockEdges(BlockEdges\u22121(b, E\u0302), E\u0302) .\nFor instance, let B\u2032 = BlockEdges\u22121(E\u0302, b) where E\u0302 = {e2} and b \u2208 B\nsuch that \u3008b\u3009 = \u3008Oe1, Be2\u3009. Then B \u2032 = {b1, b2} such that,\n\u2022 \u3008b1\u3009 = \u3008Oe1, Be2\u3009\n\u2022 \u3008b2\u3009 = \u3008Oe1, Ue2\u3009\nDefinition 3.4.10. Let E\u0302 \u2208 E. The equivalence relation \u223c E\u0302 is defined as follows: The belief states b1, b2 satisfy b1 \u223cE\u0302 b2 if and only if MoreBlockedE\u0302(b1) = MoreBlocked E\u0302 (b2).\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 69\nDefinition 3.4.11. Let DiffEStatus : B \u00d7 B \u2192 P(E) be a function. E\u2032 = DiffEStatus(b, b\u2032) is defined to be the set of all edges incident to Loc(b\u2032) which are unknown in b and known in b\u2032, i.e.\nDiffEStatus(b, b\u2032) = {e \u2208 Inc(Loc(b\u2032))|stb(e, b) = Unknown, stb(e, b\u2032) 6= Unknown}\nDefinition 3.4.12. Let \u03a0 be a set of finite policies over B. We define the function SimBlocked : \u03a0 \u00d7 P(E) \u2192 \u03a0 as follows: For every pair of belief states b, b\u2032 \u2208 B such that b\u2032 = BlockEdges(b,E\u2032), the policy \u03c0\u2032 = SimBlocked(\u03c0, E\u0302) satisfies \u03c0\u2032(b) = \u03c0(b\u2032).\nDefinition 3.4.13. We define the function Next : B \u00d7 A \u2192 P(B) as follows: B\u2032 = Next(b, a) is the set of all possible belief state that can be reached from belief state b immediately after taking action a. Meaning\nNext(b, a) = {b\u2032 \u2208 B|Tr(b, \u03c0(b), b\u2032) > 0}\nDefinition 3.4.14. Let B1 \u2282 B and E\u0302 \u2282 E. Define Set\u2212BlockedE\u0302(B1) = {b2|b2 = BlockEdges(E\u0302, b1), b1 \u2208 B1}\nLemma 3.4.15. Let b1, b2 \u2208 B such that MoreBlocked(b1, b2). Let b \u2032 1, b \u2032 2 \u2208 B, E\u0302 \u2282 E such that b\u20322 = BlockEdges(E\u0302, b \u2032 1). Let a \u2208 A such that Tr(b1, a, b \u2032 1) > 0, then Tr(b2, a, b \u2032 2) > 0.\nProof. Let e = \u3008vi, vj\u3009. If a = Sense(e) then a can be performed in any belief state. However, if a = Move(e) then, for every b \u2208 B, a can be performed in b if and only if Loc(b) = vi and stb(e, b) = Open. By definition 3.4.3 we have Loc(b1) = Loc(b2). All belief state in B are consistent with a given realization(all belief states in B describes the knowledge about the same environment), and since e is known in b1(e \u2208 Inc(vi)), we get st(e, b1) =\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 70\nOpen if and only if stb(e, b2) = Open. Thus an agent in b1 can perform a = Move(e) if and only if an agent in b2 can be perform a = Move(e). But we are given that Tr(b1, a, b \u2032 1) > 0, hence a can be performed in b2 as well. Let E\u03021 = DiffEStatus(b1, b \u2032 1) and let E\u03022 = DiffEStatus(b2, b \u2032 2). We are left to show that the status of all edges in E\\E\u03022 is equal in b2 and in b\u20322 i.e. for all e \u2208 E\\E\u03022 stb(e, b2) = stb(e, b \u2032 2). By definition 3.4.3 we have UnknownEdges(b\u20322) \u2286 UnknownEdges(b \u2032 1), and thus E\u03022 \u2286 E\u03021. Thus, by definition of DiffEStatus, the status of all edges in E\\E\u03022, is equal in b2 and in b\u20322. This satisfies all conditions for having Tr(b2, a, b \u2032 2) > 0.\nLemma 3.4.16. Let b1, b2 \u2208 B such that MoreBlocked(b1, b2). Let B1 = NEXT (b1) and B2 = Set\u2212BlockedE\u0302(B1). Let b2i \u2208 B2 and B \u2032 1 \u2282 B1 such that B\u20321 = MoreBlocked \u22121(E\u0302, b2i). Then, Tr(b2, a, b2i) = \u2211\nb1i\u2208B\u20321 Tr(b1, a, b1i).\nProof. Let b\u20321 \u2208 B1. By definition of B1, Tr(b1, a, b \u2032 1) > 0, hence, by lemma 3.4.15, Tr(b2, a, b \u2032 2) > 0. Let E \u2032 = DiffEStatus(b2, b \u2032 2), we define the probability P2E\u2032 by,\nP2E\u2032 = \u220f\ne\u2208E\u2032,stb(b\u20322,e)=Blocked\np(e) \u220f\ne\u2208E\u2032,stb(b\u20322,e)=Open\n1\u2212 p(e) (3.20)\nBy definition of transition function Tr(b2, a, b \u2032 2) = P2E\u2032 . For every 0 \u2264 i \u2264 n, where n = |B\u20321|, define E \u2032\u2032 i = DiffEStatus(b1, b1i). For every i, UnknownEdges(b\u20322) \u2286 UnknownEdges(b1i), hence E \u2032 \u2286 E\u2032\u2032i . We define the probabilities P1E\u2032 , P1E\u2032\u2032i as follows,\nP1E\u2032 = \u220f\ne\u2208E\u2032,stb(b1i,e)=Blocked\np(e) \u220f\ne\u2208E\u2032,stb(b1i,e)=Open\n1\u2212 p(e)\nP1E\u2032\u2032i = \u220f\ne\u2208E\u2032\u2032i \\E \u2032,stb(b1i,e)=Blocked\np(e) \u220f\ne\u2208E\u2032\u2032i \\E \u2032,stb(b1i,e)=Open\n1\u2212 p(e)\n(3.21)\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 71\nBy definition of transition function, Tr(b1, \u03c0(b1), b1i) = P1E\u2032P1E\u2032\u2032i . Summing up Tr(b1, a, b1i) over all b1i \u2208 B \u2032 1 gives,\n\u2211\nb1i\u2208B\u20321\nTr(b1, a, b1i) = \u2211\nb1i\u2208B\u20321\nP1E\u2032P1E\u2032\u2032i\n\u2217 \ufe37\ufe38\ufe38\ufe37 = P1E\u2032 \u2211\nb1i\u2208B\u20321\nP1E\u2032\u2032i\n\u2217\u2217 \ufe37\ufe38\ufe38\ufe37 = P1E\u2032\n* P1E\u2032 is equal for all b1i \u2208 B \u2032 1. ** \u2211\nb1i\u2208B\u20321\nPE\u2032\u2032i = 1 (The sum of all marginal probabilities equals 1).\nFor all e \u2208 E\u2032 we have stb(e, b1i) = stb(e, b \u2032 2), hence P1E\u2032 = P2E\u2032 . Thus, \u2211\nb1i\u2208B\u20321\nTr(b1, a, b1i) = P1E\u2032 = P2E\u2032 = Tr(b2, a, b \u2032 2)\nTheorem 3.4.17. Let b1, b2 \u2208 B such that MoreBlocked(b1, b2). Then C\u2217(b2) \u2265 C \u2217(b1).\nProof outline: We prove that for every finite policy \u03c0 there is a finite\npolicy \u03c0\u2032 such that C\u03c0(b2) = C \u03c0\u2032(b1).\nProof. By induction. Let E\u0302 be the subset of all edges e \u2208 E such that e is unknown in b1 and blocked in b2. i.e.\nE\u0302 = {e|e \u2208 E, stb(e, b1) = Unknown, stb(e, b2) = Blocked}\nLet B1 = NEXT (b1) andB2 = Set\u2212BlockedE\u0302(B1) whereB2 = {b21, b22, ..., b2n}. Let \u03c0 be a finite policy and let \u03c0\u2032 be a policy defined as follows: For every belief state b \u2208 B \u03c0\u2032(b) = \u03c0(BlockEdges(E\u0302, b)). Meaning, \u03c0\u2032 maps every belief state b to an action a by simulating \u03c0 on belief state b\u2032 = BlockEdges(E\u0302, b) and output a = \u03c0(b\u2032). Clearly, an agent acting according to \u03c0\u2032 will never traverse any edge in E\u0302. We show by induction that\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 72\nC\u03c0(b2) = C \u03c0\u2032(b1) as follows,\n\u2022 Base case: If b1, b2 are terminal states then C \u03c0(b2) = C \u03c0\u2032(b1) \u2261 0 (by\ndefinition of terminal states).\n\u2022 Assume by induction that for every b\u20321 \u2208 B1 and b \u2032 2 \u2208 B2 we have\nC\u03c0(b\u20322) = C \u03c0\u2032(b\u20321). By definition of \u03c0 \u2032 we have \u03c0\u2032(b1) = \u03c0(b2). Let a = \u03c0\u2032(b1). Since \u03c0 \u2032(b1) = \u03c0(b2) we have a = \u03c0(b2) Hence, according to bellman equations,\nC\u03c0 \u2032 (b1) = R(b1, a) + \u2211\nb\u20321\u2208B1 Tr(b1, a, b\n\u2032 1)C \u03c0\u2032(b\u20321)\nC\u03c0(b2) = R(b2, a) + \u2211\nb\u20322\u2208B2 Tr(b2, a, b\n\u2032 2)C \u03c0(b\u20322)\n(3.21)\nIn order to show that C\u03c0(b2) = C \u03c0\u2032(b1) we show the equivalence in the right sides of the equations above. Given a = Sense(e) then,\n\u2013 R(b1, a) = R(b2, a) = SC(e) (action Sense is always performable). \u2013 \u2211\nb\u20321\u2208B1 Tr(b1, a, b\n\u2032 1)C \u03c0\u2032(b\u20321) = \u2211\nb\u20322\u2208B2 Tr(b2, a, b\n\u2032 2)C \u03c0(b\u20322). Let b1B \u2208\nB1, b2B \u2208 B2 be the belief states that are reached immediately after the agent has sensed e in b1, b2 respectively, and e was found to be blocked. By definition of transition function, Tr(b1, a, b1B) = Tr(b2, a, b2B) = p(e). By assumption of induction, C \u03c0(b1B) = C\u03c0 \u2032 (b2B). Hence, Tr(b1, a, b1B)C \u03c0(b1B) = Tr(b2, a, b2B)C \u03c0\u2032(b2B). Similarly, let b1O \u2208 B1, b2O \u2208 B2 be the belief states that are reached immediately after the agent has sensed e in b1, b2 respectively, and e was found to be open. Then Tr(b1, a, b1O) = Tr(b2, a, b2O) = 1\u2212p(e). By assumption of induction, C \u03c0(b1O) =\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 73\nC\u03c0 \u2032 (b2O). Hence, Tr(b1, a, b1O)C \u03c0(b1O) = Tr(b2, a, b2O)C \u03c0\u2032(b2O). Thus,\n\u2211\nb\u20321\u2208B1\nTr(b1, a, b \u2032 1)C \u03c0\u2032(b\u20321) = \u2211\nb\u20322\u2208B2\nTr(b2, a, b \u2032 2)C \u03c0(b\u20322) = p(e)VB+(1\u2212p(e))VO\nWhere VB denotes C \u03c0\u2032(b1B) and VO denotes C \u03c0\u2032(b1O)(recall that C \u03c0\u2032(b1B), C \u03c0\u2032(b2B) as well as C\u03c0 \u2032 (b1O), C \u03c0\u2032(b2O) are interchangeable). Given a = move(e), where e = \u3008vi, vj\u3009, then,\n\u2013 R(b1, a) = R(b2, a). By definition of reward function, for every\nb \u2208 B R(b, a) > 0 if and only if Tr(b, a) > 0 if and only if Loc(b) = vi and stb(e, b) = Open. Thus, R(b1, a) = R(b2, a) > 0 if and only if Loc(b1) = Loc(b2) and stb(e, b1) = stb(e, b2) = Open. From definition 3.4.3 it follows that Loc(b1) = Loc(b2). In addition, stb(e, b1) = Open if and only if stb(e, b2) = Open, due to the following:\n1. All belief states reachable from b0, and in particular the belief\nstates b1, b2, are referred to the same unknown given environment s. Hence, stb(e, b1) = Open only if stb(e, b2) = Open or stb(e, b2) = Unknown and similarly stb(e, b2) = Open only if stb(e, b1) = Open or stb(e, b1) = Unknown.\n2. stb(e, b1) 6= Unknown, stb(e, b2) 6= Unknown. Since Loc(b1) =\nLoc(b2) we have e \u2208 Inc(Loc(b1)) if and only if e \u2208 Inc(Loc(b2)). By definition, an agent located in vertex v, knows the status of all edges incident to v. Thus stb(e, b1) 6= Unknown, stb(e, b2) 6= Unknown.\n\u2013 \u2211\nb\u20321\u2208B1 Tr(b1, a, b\n\u2032 1)C \u03c0\u2032(b\u20321) = \u2211\nb\u20322\u2208B2 Tr(b2, a, b\n\u2032 2)C \u03c0(b\u20322).\nLet B\u20321 = {B11, B12, ..., B1n} be a partition of B by the equiv-\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 74\nalence relation \u2261 E\u0302 such that without loss of generality B1i = MoreBlocked\u22121(E\u0302, b2i) for every 0 \u2264 i \u2264 n. Let Vi = C \u03c0\u2032(b2i) for every 0 \u2264 i \u2264 n. Then, by assumption of induction, for every b1i \u2208 B1i, we have C \u03c0(b2i) = C \u03c0\u2032(b1i), hence Vi = C \u03c0\u2032(b1i) as well. According to lemma 3.4.16, summing up Tr(b1, a, b1i) over all b1i \u2208 B1i gives,\n\u2211\nb1i\u2208B1i\nTr(b1, a, b1i) = Tr(b2, a, b2i) (3.22)\nHence,\nTr(b2, a, b2i)C \u03c0(b2i)\n= Tr(b2, a, b2i)Vi = Vi \u2211\nb1i\u2208B1i\nTr(b1, a, b1i) (by equation 3.22)\n= \u2211\nb1i\u2208B1i\nTr(b1, a, b1i)Vi (Vi is constant for every b1i \u2208 B1i)\n= \u2211\nb1i\u2208B1i\nTr(b1, a, b1i)C \u03c0\u2032(b1i)\n(3.23)\nThus, summing up over all transition functions gives,\n\u2211\nb\u20321\u2208B1\nTr(b1, a, b1\u2032)C \u03c0\u2032(b1\u2032)\n= \u2211\nB1i\u2208B\u20321\n\u2211\nb1i\u2208B1i\nTr(b1, a, b1i)C \u03c0\u2032(b1i) (B \u2032 1 is a partition of B1)\n= \u2211\nb2i\u2208B2i\nTr(b2, a, b2i)C \u03c0(b2i) (by equation 3.23)\n= \u2211\nb2\u2032\u2208B2\nTr(b2, a, b2\u2032)C \u03c0(b2\u2032) (without loss of generality b2i = b2\u2032)\n(3.24)\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 75\nThis completes the induction proof.\nWe have shown that for every finite policy \u03c0 we can define a finite policy \u03c0\u2032 which satisfies C\u03c0(b2) = C \u03c0\u2032(b1). Since the optimal policy is also finite, the equation C\u2217(b2) = C \u03c0\u2032(b1) holds. Thus, in general C \u2217(b2) \u2265 C \u2217(b1).\nIn figure 3.4, we demonstrate the \u201csimulation\u201d of policy \u03c0\u2032 presented in theorem 3.4.17. Here, we are given a graph G=(V,E), where V = {s, v1, v2, v3, t} (abusing notation, we denote one vertex as s, and one as t), E = {(s, v1), (v1, v2), (v1, v3), (v2, v3), (v1, t), (v2, t), (v3, t)}, where w, which is noted with each edge in the figure, represents the edge weight. In addition, two belief states b1, b2 \u2208 B are given with the following forms: \u3008b1\u3009 = \u2329 s,O(s,v1), B(v1,v2), O(v1,v3), O(v2,v3), O(v1,t), O(v2,t), U(v3,t) \u232a \u3008b2\u3009 = \u2329 s,O(s,v1), B(v1,v2), O(v1 ,v3), O(v2,v3), O(v1,t), O(v2,t), B(v3,t) \u232a . On the upper left of the figure, the edges status are based on b1 and on the upper right the edges status are based on b2 , where the green lines represent open edges, black lines represent blocked edges, and red lines represent unknown edges. Notice that b1, b2 satisfy MoreBlocked(b1, b2) and thus b2 = BlockEdges(b1). The lower figures represent the execution of policy \u03c0\u2032 on b1 where \u03c0 \u2032(b1) = \u03c0(b2)) and \u03c0 \u2217 on b2. We see the equivalence between the policies(the same sequence of actions). Notice that agent acting according to \u03c0\u2032(as shown by the doted line), does not perform the action move(v1, t) although it is optimal, since \u03c0 \u2032 treats all edges in Blocked(b2) as blocked in b1(edge (v1, t) in this figure)\nCorollary 3.4.18. Suppose that MoreBlocked(b1, b2) is true for b1, b2 \u2208 B, then if h(b1) is an admissible heuristic(optimistic) of b1, then h(b1) is an admissible heuristic of b2 as well.\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 76\nProof. Follows from theorem 3.4.17\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 77\nIn the following statement, we use theorem 3.4.17 to show that ifMoreOpen(b1, b2)\nthen C\u2217(b2) is a lower bound of C \u2217(b1).\nLemma 3.4.19. Let b, bopen \u2208 B such that MoreOpen(b, bopen). Then, C\u2217(b) \u2265 C\u2217(bopen).\nProof. Let b, bopen, bblocked \u2208 B such that b, bopen, bblocked differs only by the status of edge e\u0302, where e\u0302 is Unknown in b, Open in bopen and Blocked in bblocked. We prove that C \u2217(b) \u2265 C\u2217(bopen).\n\u2022 By the law of total probability we can express C\u2217(b1) as follows:\nC\u2217(b1) = P (e\u0302)C \u2217(bblocked) + (1\u2212 P (e\u0302))C \u2217(bopen) (3.25)\nFrom lemma 3.4.3 we get that C\u2217(b1) \u2264 C \u2217(bblocked). Thus, there is R \u2265 0 such that,\nC\u2217(bblocked) = C \u2217(b1) +R\nWe can express equation 3.25 as follows:\nC\u2217(b1) = p(e\u0302)(C \u2217(b1) +R) + (1\u2212 p(e\u0302))C \u2217(bopen)\nSubstracting p(e\u0302)C\u2217(b1) from both sides and then dividing both sides by (1\u2212 p(e\u0302)), we get:\nC\u2217(b1) = R p(e\u0302)\n1\u2212 p(e\u0302) + C\u2217(bopen)\nSince R( p(e\u0302)1\u2212p(e\u0302)) \u2265 0 we get\nC\u2217(b1) \u2265 C \u2217(bopen)\nTrivially, it can be shown by induction that C\u2217(b) \u2265 C\u2217(bopen), for\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 78\nany set of edges E\u0302 such that edges in E\u0302 are unknown in b and open in bopen.\nCorollary 3.4.20. Suppose that b1, b2 satisfy MoreOpen(b1, b2), then if h(b2) is an admissible heuristic(optimistic) of b2, then h(b2) is an admissible heuristic of b1 as well.\nProof. Follows immediately from lemma 3.4.19\nIn the rest of the section we provide some new definitions and lemmas in order to prove another lower bound to the cost of optimal policy on belief state b1 by a cost of the optimal policy on another belief state b2 where, in contrast to the previous lemmas, the locations of the agents in b1 and in b2 are different.\nDefinition 3.4.21. Let b1, b2 \u2208 B. We define the predicate DiffLoc(b1, b2) to be true if and only if Loc(b1) 6= Loc(b2) and for every edge e \u2208 E stb(e, b1) = stb(e, b2).\nDefinition 3.4.22. Define the set DB to be the set of all pair b1, b2 \u2208 B such that DiffLoc(b1, b2). Meaning DB = {< b1, b2 > |b1, b2 \u2208 B,DiffLoc(b1, b2)}. We call DB the DiffLoc of B.\nDefinition 3.4.23. Define the set Openb \u2286 E to be the set of all edges that are known to be open in belief state b. Meaning Eb = {e|e \u2208 E, b \u2208 B, stb(e, b) = Open}\nDefinition 3.4.24. Let P be the set of all paths in G and let DB be the DiffLoc of B. We define the function shortestPath : DB \u2192 P such that for < b1, b2 >\u2208 DB p = shortestPath(< b1, b2 >) is the shortest path\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 79\nbetween v1 = Loc(b1) and v2 = Loc(b2) in graph G \u2032 = (V,Eb1).(Note that G\u2032 = (V,Eb1 is a subgraph of G=(V,E) since Eb1 \u2286 E)\nNote that Eb1 = Eb2 for every < b1, b2 >\u2208 DB , since the status of edges\nspecified by b1 and b2 are equal.\nDefinition 3.4.25. Let P be the set of all paths in G. We define a path cost function CP : P \u2192 R as follows: Let p be a path, then CP (p) = \u2211 e\u2208p c(e).\nDefinition 3.4.26. We define the set KEb \u2286 E to be the set of all known edges in belief state b. Meaning KEb = {e|e \u2208 E, b \u2208 B, stb(e, b) = Open \u2228 stb(e, b) = Blocked}. KEb is called the knowledge in b.\nLemma 3.4.27. The value of information in the Canadian Traveler Problem is never less than zero.\nProof. Let b1, b2 \u2208 B such that b2 is reached from b1 immediately after performing SENSE(e) (Suppose hypothetically that an agent in b1 is allowed to perform action SENSE(e) once, on any edge e \u2208 E, with no cost) and we get KEb1 \u2286 KEb2 . Hence, this lemma is true if and only if C \u2217(b1) \u2265 C \u2217(b2) (by definition of value of information). SinceKEb1 \u2286 KEb2 , we can simulate any policy of b1 on b2 by \u201cignoring\u201d the information received from SENSE(e) in b2 and in particular the optimal policy \u03c0 \u2217. Therefore, we can define the policy \u03c0\u2217b1 such that \u03c0 \u2217 b1 (b\u20322) = \u03c0 \u2217(b\u20321) for every belief state b \u2032 1 reachable from b1 and belief state b \u2032 2 = b1\u2032e=ste where ste = SENSE(e). Since b1 and b2 are referred to the same physical environment, the execution of \u03c0\u2217 on b1 will be equal to the execution of \u03c0\u2217b1 on b2(will produce the same sequence of actions). Hence, C \u03c0\u2217 b1 (b2) = C \u2217(b1), and in general C \u2217(b1) \u2265 C \u2217(b2).\nLemma 3.4.28. Let b1, b2 \u2208 B such that < b1, b2 >\u2208 DB, then C \u2217(b1) + Cp(shortestPath(< b1, b2 >)) \u2265 C \u2217(b2).\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 80\nProof outline: In the next lemma we show that C\u2217(b1)+Cp(shortestPath(<\nb1, b2 >)) \u2265 C\u2217(b2). This gives us a lower bound to C \u2217(b1) since C \u2217(b1) \u2265 C\u2217(b2)\u2212 Cp(shortestPath(< b1, b2 >)). We show this by defining a policy \u03c0\u0302 such that when executing \u03c0\u0302 on b2 we have the following: An agent under \u03c0\u0302(b2) moves through the shortest path(under assumption that all unknown edges in b2 are blocked) to the location referred by b1 (Loc(b1)), reaching belief state b\u2019, and then under \u03c0\u0302(b\u2032) the agent is followed by the execution of the optimal policy \u03c0\u2217.\nProof. Assume(by negation) that\nC\u2217(b1) + Cp(shortestPath(< b1, b2 >)) < C \u2217(b2). (3.26)\nLet v1 = Loc(b1) and v2 = Loc(b2). We define a new policy \u03c0\u0302 such that executing it on b2 gives the following:\n1. An agent under \u03c0\u0302(b\u2032) traverses the path p = shortestPath(< b1, b2 >)\nstraightforward from v2 to v1(which is always possible since all edges in p are open). Let b\u2032 be the belief state that the agent reaches when arriving v1.\n2. Immediately after reaching b\u2032, the agent under \u03c0\u0302(b\u2032) acts according to\nthe optimal policy until reaching t. Meaning, for any belief state b\u201d reachable from b\u2032 \u03c0\u0302(b\u2032\u2032) = \u03c0\u2217(b\u2032\u2032).\nClearly,\nC \u03c0\u0302(b2) =\nCost of traversing p \ufe37 \ufe38\ufe38 \ufe37 Cp(shortestPath(< b1, b2 >))+\nExp. cost of \u03c0\u2217 on b\u2032\n\ufe37 \ufe38\ufe38 \ufe37 C\u2217(b\u2032) (3.27)\nWe claim that KEb1 \u2286 KEb\u2032 . This result from:\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 81\n1. KEb1 = KEb2 . The knowledge in b1 and b2 is equal by definition of\nelement pairs < b1, b2 >\u2208 DB .\n2. KEb2 \u2286 KEb\u2032 . An agent in b2 that follows the shortest path p may\nobtain information if a vertex in path p(a vertex that is incident to two edges in p) is incident to an edge that has not been sensed yet.\nSince an agent A1 in b1 and an agent A2 in b \u2032 are at the same physical state s \u2208 S, and the knowledge of A1 about s is a subseteq of the knowledge of A2 about s (KEb1 \u2286 KEb\u2032) we get C \u2217(b\u2032) \u2264 C\u2217(b1)(followed by the lemma of value of information).\nThus,\nC\u2217(b\u2032)+Cp(shortestPath(< b1, b2 >)) \u2264 C \u2217(b1)+Cp(shortestPath(< b1, b2 >))\n(3.28)\nby equation 3.27 we get,\nC \u03c0\u0302(b2) \u2264 C \u2217(b1) + Cp(shortestPath(< b1, b2 >)) (3.29)\nFollowing assumption 3.26 we get,\nC \u03c0\u0302(b2) \u2264 C \u2217(b2) (3.30)\nwhich is a contradiction to the optimality of policy \u03c0\u2217. Hence,\nC\u2217(b1) + Cp(shortestPath(< b1, b2 >)) \u2265 C \u2217(b2) (3.31)\nCorollary 3.4.29. Let b1, b2 \u2208 B such that < b1, b2 >\u2208 DB, then if h(b1) is a lower bound to C \u2217(b1), then h(b1) is a lower bound to C \u2217(b2) +\nCHAPTER 3. THEORETICAL ANALYSIS OF CTP 82\nC(shortestPath < b1, b2 >) as well.\nProof. Follows immediately from lemma 3.4.28\nWe want to define two relations which will be used in the next section:\nChapter 4\nGeneralizing PAO*"}, {"heading": "4.1 General Propagation AO*", "text": "In many cases, PAO* lowers dramatically the running time by reducing the state space, however, it assumes that each vertex is connected to at most one unknown edge, such that each AND node in the AND/OR graph has at most two successors. We present the generalized propagation AO* algorithm (Gen-PAO in short), a generalization of PAO*, which does not assume any preknown knowledge of the graph (except the edges incident to s which are always defined as Open). Gen-PAO solves the Sensing-CTP as well. Each sensing action is associated with a sensing AND node, where each sense node has only two children nodes for the two possible statuses of the sensed edge (Open/Blocked). This variant is extremely harder than the basic CTP since the agent can sense any unknown edge in any state and hence, the branching factor of the OR nodes is significantly larger.\n83\nCHAPTER 4. GENERALIZING PAO* 84"}, {"heading": "4.1.1 Gen-PAO Heuristics", "text": "Similarly to AO* and PAO*, each iteration of Gen-PAO is based on two phases: Expansion and Propagation. Gen-PAO differs from AO* and PAO* only in the Propagation phase (i.e, the Main and Expand method as presented in algorithm 2.1 are part of Gen-PAO as well). However, in the propagation phase, Gen-PAO propagates the heuristic values not only upwards to the ancestors as AO*, but to the entire state space, incorporating three novel heuristics: HBlocked, HOpen, and HDiffLoc (line 13). The heuristic HBlocked is based on the predicate MoreBlocked (definition 3.4.3), HOpen is based on the predicate MoreOpen (definition 3.4.4), and HDiffLoc is based on the predicate Diffloc(definition 3.4.21). Let Z be the set of belief states that expanded by Gen-PAO and let b \u2208 Z. The heuristics are defined as follows:\n\u2022 HBlocked(b): If there is a belief state b\u2032 \u2208 Z that satisfies MoreBlocked(b, b\u2032)\nand h(b) > h(b\u2032) then h(b\u2032) \u2190 h(b).\n\u2022 HOpen(b): If there is a belief state b\u2032 \u2208 Z that satisfies MoreOpen(b\u2032, b)\nand h(b\u2032) < h(b) then h(b\u2032) \u2190 h(b).\n\u2022 HDiffLoc(b): If there is a belief state b\u2032 \u2208 Z that satisfies DiffLoc(b, b\u2032)\nand h(b\u2032) < h(b) \u2212 CSP (b, b\u2032) then h(b\u2032) \u2190 h(b) \u2212 CSP (b, b\u2032), where CSP is the cost of the shortest path from b to b\u2019.\nBelief states of which values are updated due to propagation from b are called propagated belief state of b. Notice that HBlocked(b),HOpen(b), and HDiffLoc(b) always raise up the heuristic value of the propagated belief states of b. However, due to corollaries 3.4.18, 3.4.20, and 3.4.29 respectively, heuristics HBlocked(b),HOpen(b), and HDiffLoc(b) are admissible, and thus they are upper bounded by V \u2217(b).\nCHAPTER 4. GENERALIZING PAO* 85\nFigure 4.1 illustrate an update of belief state b2 by the three heuristic methods (the the new heuristic values of b2 are notified in parenthesis)\nThe heuristic methods are invoked when a value of belief state is updated(procedure Propagate, line 13). The heuristics methods are ineffective on a major part of the expanded states (i.e. most of the expanded states b\u2032 \u2208 Z do not satisfy the predicates MoreBlocked(b, b\u2032),MoreOpen(b, b\u2032), and DiffLoc(b, b\u2032), for a given expanded state b, and their values are not updated by their compatible heuristic methods). In order to reduce the number of expanded states that are checked for update, we use two data structures: BlockedStructue and OpenStucture. For defining these structures we define new equivalence relations: The equivalence relations \u2261o and \u2261b on belief states b1, b2 are defined as follows:\nDefinition 4.1.1. b1 \u2261o b2 if and only if:\n1. Loc(b1) = Loc(b2)\n2. Open(b1) = Open(b2)\nSimilarly,\nCHAPTER 4. GENERALIZING PAO* 86\nAlgorithm 4.1 Gen-PAO procedure Propagate(Graph GAO, NodeSet Z)\n1: while Z 6= \u03c6 do 2: select zi \u2208 Z such that zi has no children in Z; 3: if zi is AND node then 4: f(zi) \u2190 \u2211\nzj\u2208successor(zi) a=(zi,zj)\n[tr(zi, a, zj)h(zj) + c(a)];\n5: if all successors of zi are marked SOLVED then 6: MarkSolved(zi) 7: end if 8: end if\n9: if zi is OR node then 10: f(zi) \u2190 min\nzj\u2208successor(zi) a=(zi,zj)\n[tr(zi, a, zj)h(zj) + c(a)];\n11: MarkAction(zi, zj); 12: if SOLVED(zj) then 13: MarkSolved(zi); 14: end if 15: invokeHeuristics(Graph GAO, Node zi); 16: end if 17: remove zi from Z; 18: end while\nCHAPTER 4. GENERALIZING PAO* 87\nDefinition 4.1.2. b1 \u2261b b2 if and only if:\n1. Loc(b1) = Loc(b2)\n2. Blocked(b1) = Blocked(b2)\nEach of these structures is a hash table that contains the entire expanded state space Z (more precisely the hash table refers to Z), where the entires of each table divide Z into equivalence classes called \u201cbuckets\u201d. Namely, the set of buckets {mo1, ...,mon}, in BlockedStructue, partitions Z in to equivalence classes by the relation \u2261o, while the set of buckets {mb1, ...,mbn}, in OpenStucture, partitions Z into equivalence classes by the relation \u2261b. By definition above, HBlocked(b), never updates the heuristic value h(b\u2032) if b and b\u2019 do not share the same bucket of BlockedStructue, and similarly, HOpen(b) never update a value of b\u2019 if b and b\u2019 do not share the same bucket of OpenStructue. Procedure propBlocked (Algorithm 4.2) implements the heuristicHBlocked. classBlocked(b) (line 1) returns the set ZB of all expanded nodes whose belief states are in the same bucket of BlockedStructue as the belief state of node z. Similarly, procedure propOpen is implementation of the heuristic HOpen. classOpen(b) (line 1) returns the set of all expanded nodes ZO whose belief states that are at the same bucket of OpenStructue as the belief state of node z."}, {"heading": "4.1.2 Eliminating Duplicate Nodes", "text": "In most cases Gen-PAO expands the same node more than once. This may lead to a large expense of memory and run time when it is generated on large graphs. Taking this into consideration, we introduce the Gen-PAO-EDN (short for Gen-PAO Eliminating Duplicate Nodes) algorithm, a variation of\nCHAPTER 4. GENERALIZING PAO* 88\nAlgorithm 4.2 Heuristic methods for Gen-PAO procedure propBlocked(NodeSet Z, Node z)\n1: ZB \u2190 classBlocked(Z); 2: for all (zi \u2208 ZB|zi 6= z) do 3: if MoreBlocked(z, zi) and h(z) > h(zi) then 4: h(zi) \u2190 h(z); 5: end if 6: end for\nprocedure propOpen(NodeSet Z,Node s)\n1: ZO \u2190 stateStructureOpen(Z) 2: for all (zi \u2208 ZO|zi 6= z) do 3: if MoreOpen(zi, z) and h(z) > h(zi) then 4: zi \u2190 z; 5: end if 6: end for\nGen-PAO, that maintains a single OR node for every state, by eliminating all duplicate OR nodes (more precisely, preventing the expansion of duplicate nodes) which shares the same state into one OR node. There are two key differences between Gen-PAO-EDN an Gen-PAO:\n\u2022 Gen-PAO maintains one representative OR node for each state. When\nGen-PAO-EDN expands an AND node, it creates a new OR node only if its associated state is not represented by any OR node in the AND/OR graph. Otherwise, if a representative OR node to this state already exists, then the expanded AND node becomes an additional parent of the representative OR node.\n\u2022 The AND/OR graph may contain cycles(not a tree as in AO* and\nGen-PAO). A special type of cycle, called strongly connected (defined below), induces loops in the propagation phase if the cycle is a subgraph of the partial solution.\nDefinition 4.1.3. Let AO be an AND/OR graph, A1, ...An \u2208 AO be AND nodes, and O1, ..., On \u2208 AO be OR nodes. A cycle O1\u2212 >\nCHAPTER 4. GENERALIZING PAO* 89\nA1\u2212 > O2\u2212 > A2\u2212 > ...\u2212 > On\u2212 > An\u2212 > O1 is strongly connected if for every 1 \u2264 i \u2264 n Ai is the preferred son of Oi.\nIf the propagate method enters a strongly connected circle C (which occurs when the propagation goes upwards to the ancestors), the heuristic values are re-updated every iteration, where each update raises up a bit the values of the nodes in . In some point on of the following eventually happens:\n1. The value of one of the AND nodes in C is raised up to a level that\nit ceased to be the preferred successor of its OR parent. Namely, in some point in the process of update, there is an AND node n, with a sibling n\u2032, such that h(n\u2032) < h(n). Then n\u2032 becomes the preferred son. Hence, the cycle is no longer a strong connected and the loop ends.\n2. The propagation process in C raises up the values of the nodes\nin C until the values are converged to a certain finite limit.\nClearly, if the values of nodes in C are not converged then case 1 must hold. In case 2, the propagation may enter into an endless loop if the values are not converged in any finite iteration. In order to overcome this, each time the value of a node n \u2208 Z is updated, we check the delta \u2206(n) = h(n)\u2212hprev(n), and stop the loop if \u2206(n) < \u01eb, where \u01eb is a small positive constant which is chosen before the run, and hprev(n) is the value of n before the update. It should be noted that \u01eb is defined to be so small, that it does not change the propagation process(i.e if case 2 holds then case 1 does not hold even if the loop would have never been stopped). As we proposed a unifying approach to Gen-PAO, we now propose\nCHAPTER 4. GENERALIZING PAO* 90\na unifying approach to the AO*. The algorithm AO-EDN is an improvement of the AO* algorithm in which unifies the OR nodes that associate with the same state. In fact AO-EDN is the same algorithm as Gen-PAO-EDN despite that it does not include heuristic HBlocked and HOpen in the propagation phase.\nChapter 5\nEmpirical Results\nIn order to evaluate our scheme we implemented alternative algorithms for the Gen-PAO and compared them by their execution time and by the size of their generated AND/OR graph (defined as the number of its nodes). Note that although the size of the AND/OR graph and the run time of GenPAO-EDN may decrease as a result of the heuristic propagation and nodes unification, still the algorithms described in this section requires a time exponential in the number of unknown edges, which makes this approach prohibitive for graphs with large sets of unknown edges."}, {"heading": "5.1 Varying the Uncertainty of the Graph", "text": "In the first two experiment we explored how the uncertainty of the graph affect the performance of AO* (Section 2.4), GenPAO (Section 4.1), and AO-EDN (Section 4.1.2). The performance of each algorithm was measured for different graph sizes where each graph had different number of unknown edges. To ensure that the experiments could be performed within a reasonable time frame, the parameters were chosen so that a single run takes no more than few minutes.\n91\nCHAPTER 5. EMPIRICAL RESULTS 92\nFigure 5.1 compares the performance of the algorithms above on instances of basic-CTP. Figure 5.1a and figure 5.1b show respectively the change in the execution time and in the size of AND/OR graph as the number of unknown edges ascend from 2 to 12. This comparison indicates that Gen-PAO has a significant advantage in execution time over AO* since the embedded heuristics in Gen-PAO lowers dramatically the size of the AND/OR graph. Moreover, Gen-PAO has a slight advantage in execution time over AO-EDN although the size of the AND/OR graph generated by AO-EDN is smaller than the graph generated by Gen-PAO. The increased execution time of AO-EDN is incurred by the overhead of the iterative propagation in the redundancy elimination process (Section 4.1.2) in which depends on the value of default edge (default edge cost was chosen to be 100).\nCHAPTER 5. EMPIRICAL RESULTS 93\nFigure 5.2 shows the comparison between AO-EDN and Gen-PAO on instances of Sense-CTP (the sensing cost was fixed to 0.5 for all edges). AO* was discarded from this comparison due to an extremely large execution time. In contrast to previous comparison, here AO-EDN outperforms Gen-PAO in AND/OR graph size (figure 5.2b) as well as in execution time (figure 5.2a). The elimination of redundancy nodes provides an advantage despite the overhead, since the number of expansions saved by the unification increases considerably as the number of unknown edges ascends. The plot does not contain more than 7 unknown edges since Gen-PAO consumes all the RAM on larger graphs.\nIt should be mentioned that since the performances of Gen-PAO-EDN (Section 4.1.2) and Gen-PAO are almost identical on instances of basic-CTP and Sense-CTP, the performance of Gen-PAO-EDN is not presented.\nCHAPTER 5. EMPIRICAL RESULTS 94"}, {"heading": "5.2 Gen-PAO Heuristic Estimate", "text": ""}, {"heading": "5.2.1 Experimental Setting", "text": "We now define a variant of the Canadian Traveler Problem called Expensive Edges CTP (Exp-CTP in short). Exp-CTP is defined as CTP, except that each edge e \u2208 E can be expensive/cheap instead of blocked/unblocked. Formally, Expensive-Edge-CTP is a 6 tuple I = (G,P,w, s, t,DC) where G = (V,E) is a graph, P and w are respectively the probability and cost functions over the edges, s, t \u2208 V are the start and goal vertices, and DC is a positive real number. P (e) denote the probability that e is cheap and 1 \u2212 P (e) denote the probability that e is expensive. An agent can traverse edge e \u2208 E whether its cheap or expensive. However, if the agent traverses e and e is cheap then it pays w(e), and if e is expensive then it pays DC, where DC (short for Detour cost) is a fixed cost which is higher than any edge cost(except the cost of the default edge \u3008s, t\u3009). In fact Exp-CTP can be defined as a subclass of CTP as well, where every unknown edge \u3008vi, vj\u3009 in G, has a parallel path lij = \u3008\u3008vi, vk\u3009 , \u3008vk, vj\u3009\u3009 called detour path such that the path cost of lij is DC and lij is always traversable. Namely,\n\u2022 w(\u3008vi, vk\u3009) = DC and w(\u3008vk, vj\u3009) = 0\n\u2022 P (\u3008vi, vk\u3009) = 1 and P (\u3008vk, vj\u3009) = 1\nTo evaluate the performance of Gen-PAO heuristics we implemented four alternative algorithms for Gen-PAO-EDN, where on each algorithm, different heuristic was embedded in the propagation phase. Since the heuristics has almost no impact when Gen-PAO-EDN is applied on instances of basic-CTP and sense-CTP, the algorithms were executed on instances of Exp-CTP. The implemented algorithms are as follows:\nCHAPTER 5. EMPIRICAL RESULTS 95\n\u2022 PAO-Blocked - Gen-PAO-EDN which propagates the heuristic values\naccording to HBlocked (Section 4.1).\n\u2022 PAO-Open - propagates the heuristic values according to HOpen (Sec-\ntion 4.1).\n\u2022 PAO-All - propagates the heuristic values according to HOpen and\nHBlocked.\n\u2022 PAO-None - basic propagation with no heuristic included."}, {"heading": "5.2.2 Varying the Sensing Cost", "text": "In order to learn the effect of the sensing cost on the algorithms performance we conducted several runs using different fixed sensing cost(the sensing cost was equal for of all edges) on a graph that consists 8 vertices and 13 edges (10 edges are unknown). In all experiments, the probability of all unknown edges was fixed to 0.5. Figure 5.3a shows the change in the size of AND/OR graph as the sensing cost ascends from 0.1 to 1.1. This result indicates that the size of AND/OR graph (generated by all variants of Gen-PAO) decreases, as the sensing cost increases . We believe that this can be attributed to the increased number of expanded states in the AND/OR graph incurred by the low sensing cost, in which makes the sensing action worthwhile. In particular, there exists a limit m, such that every sensing cost below m makes the Sense actions always preferable over the Move actions. This causes many expansions of Sense nodes and expansion of new belief state (that are not reachable without preforming Sense) which results in a large AND/OR graph. The comparison of the algorithms shows that PAONone generates a relatively small AND/OR graph for low sensing cost, while PAO-Blocked and PAO-All has advantage on high sensing cost. This is also\nCHAPTER 5. EMPIRICAL RESULTS 96\ntrue for larger graphs that contain larger sets of unknown edges. We believe that this effect can be explained by the fact that on low levels of sensing cost, it is worthwhile to sense unknown edges, in which improves the estimate accuracy of the heuristic values(on low cost). The high accuracy level of the heuristic estimate leads to low rates of pruning since the heuristics HBlocked and HOpen are based the gap between the real and estimated value, which is small in this case. Thus, a large AND/OR graph was obtained. A comparison of the run time (figure 5.3b) shows that the run time extends as the size of the AND/OR graph increases. The reason for this positive correlation is obvious: the increased size of the graph leads to larger computation time required for expanding the states, as well as for propagating the heuristic values to a larger set of states.\nCHAPTER 5. EMPIRICAL RESULTS 97"}, {"heading": "5.2.3 Varying the Open Probability", "text": "In this experiment we investigated the effect of distribution over the edges on the performance of variants of Gen-PAO-EDN (Section 4.1.2). In order to perform simple experiment that analyzes this effect, we configured the graph such that all unknown edges was open with the same value of fixed probability, called open probability, which is given as an input. Figure 5.4 illustrates the performance of different heuristics on a graph that consists 19 unknown edges for DC=7 and DC=9 . Figures 5.4a and 5.4c show the change in the size of the AND/OR graph size as the open probability ascends from 0.1 to 0.9. These results indicates that for all algorithms there exists a certain value of open probability p (p=0.5 on figure 5.4a and p=0.3 on figure 5.4c) such that for any value of open probability p\u2032 (called low open probability) smaller than p the size of the AND/OR graph increases as p\u2032 rises, while for any value of open probability p\u2032\u2032 larger than p (called high open probability) the size of the AND/OR graph decreases as the p\u2032\u2032 rises. We call p\u2032 low open probability and p\u2032\u2032 high open probability\nThis can be explained by the following reasons(referred to AND/OR\ngraphs generated by all algorithms):\n\u2022 On high open probability most of the decision nodes (OR nodes) de-\ncides correctly their best action node when first expanded without changing their decision afterwards, and thus, relatively large portion of the expanded states is also a part of the optimal policy graph and the AND/OR graph is relatively small. However, as the open probability lowers, the AND/OR graph size increases since the heuristic estimates are less accurate and more alternative actions are considered for the optimal policy. This leads to an excessive expansion of nodes and a larger AND/OR graph.\nCHAPTER 5. EMPIRICAL RESULTS 98\n\u2022 On low open probability, as the open probability lowers, the graph\nbecomes \u201cmore blocked\u201d, and the default path becomes preferable. In such cases, all variants of Gen-PAO-EDN tend to prune action nodes that are not associated with the default path (sensing or traversing edges that are not in the default path) and, as a result, a smaller AND/OR graph is obtained.\nThe comparison between the heuristic of Gen-PAO-ELN shows advantage of PAO-Blocked and PAO-All on low open probabilities. This is due to the high pruning rate incurred by HBlocked on low open probability, where the gap between the heuristic values the real values are high. Again HBlocked is effective since the chances that heuristic value of different belief state will be updated are high (see conditions of HBlocked in section 4.1).\nFigures 5.4b and 5.4d show the time spent by the four algorithms. As in previous experiments, there is a tight correlation between the execution time and the AND/OR graph size. The size of the AND/OR graphs generated by PAO-Blocked and PAO-All are smaller then PAO-None on all levels of open probability, however, the advantage on runtime of PAO-Blocked and PAO-All occurs on low open probability."}, {"heading": "5.3 Value of Clairvoyance", "text": "In order to get some general indication of the total value of information, we checked the ratio (see Papadimitriou 1991), denoted by RV , on instances of basic CTP and Exp-CTP. RV is defined as C \u2217\nAS where C\u2217 is the expected cost\nof the optimal policy and AS is the expected cost of the optimal policy given that the graph is fully observable (can be also described as the expected cost of the policy Always Sense when the sensing cost is 0 (see [Bnaya,Felner and\nCHAPTER 5. EMPIRICAL RESULTS 99\nShimony]). Formally, Let l1, l2, ..., ln be the paths in the graph ordered by their path cost, Pi be the probability that path li is traversable, and Ci be\nCHAPTER 5. EMPIRICAL RESULTS 100\nthe path cost of path li then AS can be described as follows:\nAS = n\u2211\ni=1\ni\u22121\u220f\nj=1\n(1\u2212 Pj)PiCi\nWe performed experiments on instance of basic CTP for different values of open probabilities and values of the default edge. Results for graph 7V11E (figure 5.5) shows that RV is relatively high on low values of the default edge (where default edge cost is 20). This can be explained by the fact that AS is relatively low since the agent would not traverse the default edge if there exists an open path to the target (in addition to the default edge) however C\u2217 is almost high as the cost of default edge since it is usually worthwhile to traverse the default edge when MaxEdge is low (note that C\u2217 is always lower than MaxEdge). In addition, RV is high on low open probabilities (i.e. on p \u2208 [0.1, 0.3]) , since the the graph \u201ctends\u201d to be blocked and the default edge is preferable over the \u201ccheap\u201d paths. Tough on extremely high cost of the default edge (not illustrated in the figure), i.e. on MaxEdge > 300 , RV is low even on low open probability (around 1.3), since the agent takes the default edge only if there is no open path other then the default edge.\nAn analogue experiment was performed on instanfce of Exp-CTP for the same graph as used on previous experiment. RV was measured for different values of DC and open probabilities while default edge cost remained fixed (default edge cost is 200). Figure 5.6 shows that the result is qualitatively similar to the results of the previous experiment, however lower value of RV were obtained in all domain. The reason for this similarity is the same as in the previous experiment, despite that now, the agent prefers to traverse the detour path instead of the default edge. RV is lower than in previous experiment since the paths cost, on average, is higher (it is sometimes required\nCHAPTER 5. EMPIRICAL RESULTS 101\nto pay DC several times) and thus AS is higher.\nCHAPTER 5. EMPIRICAL RESULTS 102\nChapter 6\nSummary"}, {"heading": "6.1 Contributions", "text": "In this thesis we explored the Canadian traveler problem theoretically and empirically. In the context of theoretical analysis the following theorems has been proved:\n\u2022 Correlated-CTP is at least as hard as Sensing-CTP.\n\u2022 CTP-PATH-DEP is NP-hard.\n\u2022 CTP-FOR-DEP is solvable in polynomial time.\n\u2022 Properties of Belief MDP for CTP.\nThe main aspect of the practical analysis is the framework of Gen-PAO, where its main contributions are:\n\u2022 Gen-PAO extends the PAO* algorithm such that it is not restricted\nto special types of graphs.\n\u2022 Gen-PAO optimally solves instances Exp-CTP and sensing CTP in\naddition to basic CTP.\n103\nCHAPTER 6. SUMMARY 104\n\u2022 Two heuristics HBlocked and HOpen have been proposed. HBlocked\nand HOpen can be plugged in Gen-PAO and in some cases reduces the size of the AND/OR graph and the execution time.\nIn addition, we analyzed the parameter RV for instances of Exp-CTP and basic CTP and showed its general behivior."}, {"heading": "6.2 Future work", "text": "There is a lot remained to be done in theoretical analysis of the CTP, and in particular classifying other subclasses of the CTP. On the practical aspect, Gen-PAO can be further modified to solve other type of CTP such as Correlated CTP and multi-agent CTP. Moreover, we believe that Gen-PAO can be further enhanced by aiming it to other type of POMDP problems. It might be worth consideration to improve the performance of Gen-PAO by implementing heuristics that specialize in specific type of graphs."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Devising an optimal strategy for navigation in a partially observable environment is one of the key objectives in AI. One of the problem in this context is the Canadian Traveler Problem (CTP). CTP is a navigation problem where an agent is tasked to travel from source to target in a partially observable weighted graph, whose edge might be blocked with a certain probability and observing such blockage occurs only when reaching upon one of the edges end points. The goal is to find a strategy that minimizes the expected travel cost. The problem is known to be P# hard. In this work we study the CTP theoretically and empirically. First, we study the Dep-CTP, a CTP variant we introduce which assumes dependencies between the edges status. We show that Dep-CTP is intractable, and further we analyze two of its subclasses on disjoint paths graph. Second, we develop a general algorithm that optimally solve the CTP called General Propagating AO* (Gen-PAO). GenPAO is capable of solving two other types of CTP called Sensing-CTP and Expensive-Edges CTP. Since the CTP is intractable, Gen-PAO use some pruning methods to reduce the space search for the optimal solution. We also define some variants of Gen-PAO, compare their performance and show some benefits of Gen-PAO over existing work.", "creator": "LaTeX with hyperref package"}}}