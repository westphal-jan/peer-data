{"id": "1702.07787", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Convolutional Gated Recurrent Neural Network Incorporating Spatial Features for Audio Tagging", "abstract": "Environmental recording interlock but a re proposed examine it watchers the this or strong. hand addition multichannel event at setting bite. Deep neural network (DNN) example knowledge two reportedly when adopted this estimating after audio delete end the market audio scene. In this boxes, no propose for allowed. :5 membranes wired (CNN) failed storing profitability uses from lucas - disk banks (MFBs ), spectrograms types what import waveforms for audio detecting. Gated ailments largest (GRU) based disorder biochemical networks (RNNs) are from swishing to models the long - rather neocortex type such the audio function. To complemented and input information, same auxiliary CNN the provided although actually recently the harmonic unlike particular stereo productions. We objective our proposed utilized on Task 4 (data unsubscribe) including the Detection or Classification to Acoustic Scenes has Events 2016 (DCASE 2016) challenges. Compared behind think focused DNN - scientific i.e., but supports structure better cuts well purpose error forecast (EER) half 0-0. 13 put 0. 34 time created technological set. The mapping features reason change alleviate created EER to 0. first. The promising especially before end - to - end teach place mixed subcarriers example this benefits. Finally, on the exhaustive set, thing good seen new - 's - seen - designer performance back 0. ten EER while as performance one this best apply changes entirely 6. 58 EER.", "histories": [["v1", "Fri, 24 Feb 2017 22:27:29 GMT  (1338kb,D)", "http://arxiv.org/abs/1702.07787v1", "Accepted to IJCNN2017, Anchorage, Alaska, USA"]], "COMMENTS": "Accepted to IJCNN2017, Anchorage, Alaska, USA", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.NE", "authors": ["yong xu", "qiuqiang kong", "qiang huang", "wenwu wang", "mark d plumbley"], "accepted": false, "id": "1702.07787"}, "pdf": {"name": "1702.07787.pdf", "metadata": {"source": "CRF", "title": "Convolutional Gated Recurrent Neural Network Incorporating Spatial Features for Audio Tagging", "authors": ["Yong Xu", "Qiuqiang Kong", "Qiang Huang", "Wenwu Wang", "Mark D. Plumbley"], "emails": ["m.plumbley}@surrey.ac.uk"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nAudio tagging (AT) aims at putting one or several tags on a sound clip. The tags are the sound events that occur in the audio clip, for example, \u201cspeech\u201d, \u201ctelevision\u201d, \u201cpercussion\u201d, \u201cbird singing\u201d, and so on. Audio tagging has many applications in areas such as information retrieval [1], sound classification [2] and recommendation system [3].\nMany frequency domain audio features such as melfrequency cepstrum coefficients (MFCCs) [4], Mel filter banks feature (MFBs) [5] and spectrogram [6] have been used for speech recognition related tasks [7] for many years. However, it is unclear how these features perform on the non-speech audio processing tasks. Recently MFCCs and the MFBs were compared on the audio tagging task [8] and the MFBs can get better performance in the framework of deep neural networks. The spectrogram has been suggested to be better than the MFBs in the sound event detection task [9], but has not yet been investigated in the audio tagging task.\nBesides the frequency domain audio features, processing sound from the raw time domain waveforms, has attracted a lot of attentions recently [10], [11], [12]. However, most of this works are for speech recognition related tasks; there are few works investigating raw waveforms for environmental audio analysis. For common signal processing steps, the short time Fourier transform (STFT) is always adopted to transform raw waveforms into frequency domain features using a set of\nFourier basis. Recent research [10] suggests that the Fourier basis sets may not be optimal and better basis sets can be learned from raw waveforms directly using a large set of audio data. To learn the basis automatically, convolutional neural network (CNN) is applied on the raw waveforms which is similar to CNN processing on the pixels of the image [13]. Processing raw waveforms has seen many promising results on speech recognition [10] and generating speech and music [14], with less research in non-speech sound processing.\nMost audio tagging systems [8], [15], [16], [17] use mono channel recordings, or simply average the multi-channels as the input signal. However, using this kind of merging strategy disregards the spatial information of the stereo audio. This is likely to decrease recognition accuracy because the intensity and phase of the sound received from different channels are different. For example, kitchen sound and television sound from different directions will have different intensities on different channels, depending on the direction of the sources. Multi-channel signals contain spatial information which could be used to help to distinguish different sound sources. Spatial features have been demonstrated to improve results in scene classification [18] and sound event detection [19]. However, there is little work using multi-channel information for the audio tagging task.\nOur main contribution in this paper includes three parts. First, we show experimental results on different features including MFBs and spectrogram as well as the raw waveforms on the audio tagging task of the DCASE 2016 challenge. Second, we propose a convolutional gated recurrent neural network (CGRNN) which is the combination of the CNN and the gated recurrent unit (GRU) to process non-speech sounds. Third, the spatial features are incorporated in the hidden layer to utilize the location information. The work is organized as follows: in Section II, the proposed CGRNN is presented for audio tagging. In section III, the spatial features will be illustrated and incorporated into the proposed method. The experimental setup and results are shown in Section IV and Section V. Section VI summarizes the work and foresees the future work."}, {"heading": "II. CONVOLUTIONAL GATED RECURRENT NETWORK FOR AUDIO TAGGING", "text": "Neural networks have several types of structures: the most common one is the deep feed-forward neural network. Another popular structure is the convolutional neural network (CNN),\nar X\niv :1\n70 2.\n07 78\n7v 1\n[ cs\n.S D\n] 2\n4 Fe\nb 20\n17\nwhich is widely used in image classification [20], [13]. CNNs can extract robust features from pixel-level values for images [13] or raw waveforms for speech signals [10]. Recurrent neural network is the third structure which is often used for sequence modeling, such as language models [21] and speech recognition [22]. In this section, we will introduce the convolutional neural network and the recurrent neural network with gated recurrent units."}, {"heading": "A. One dimension convolutional neural network", "text": "Audio or speech signals are one dimensional. Fig. 1 shows the structure of a one-dimension CNN which consists of one convolutional layer and one max-pooling layer. N filters with a fixed size F are convolved with the one dimensional signal to get outputs pti{i = 0, \u00b7 \u00b7 \u00b7 , (N \u2212 1)}. Given that the dimension of the input features was M , the activation h of the convolutional layer would have (M\u2212F+1) values. The maxpooling size is also (M \u2212F +1) which means each filter will give one output value. This is similar to speech recognition work [10] where CNN has been used to extract features from the raw waveform signal. The way of each filter producing one value can also be explained as a global pooling layer which is a structural regularizer that explicitly enforces feature maps to be confidence maps of meaningful feature channels [23]. So N activations are obtained as the robust features from the basic features. As for the input feature size M , a short time window, e.g., 32 ms, was fed into the CNN each time. The longterm pattern will be learned by the following recurrent neural network. As for the filter size or kernel size, a large receptive field is set considering that only one convolutional layer is designed in this work. For example, F = 400 and M = 512 are set in [10]. If the input feature was raw waveforms, each filter of the CNN was actually learned as a finite impulse response (FIR) filter [12]. If the spectrograms or mel-filter banks were fed into the CNN, the filtering was operated on the frequency domain [24] to reduce the frequency variants, such as the same audio pattern but with different pitches."}, {"heading": "B. Gated recurrent unit based RNN", "text": "Recurrent neural networks have recently shown promising results in speech recognition [22]. Fig. 2 shows the basic idea of the RNN. The current activation ht is determined by the\ncurrent input xt and the previous activation ht\u22121. RNN with the capability to learn the long-term pattern is superior to a feed-forward DNN, because a feed-forward DNN is designed that the input contextual features each time are independent. The hidden activations of RNN are formulated as:\nht = \u03d5(Whxt + Rhht\u22121 + bh) (1)\nHowever, a simple recurrent neural network with the recurrent connection only on the hidden layer is difficult to train due to the well-known vanishing gradient or exploding gradient problems [25]. The long short-term memory (LSTM) structure [26] was proposed to overcome this problem by introducing input gate, forget gate, output gate and cell state to control the information stream through the time. The fundamental idea of the LSTM is memory cell which maintains its state through time [27].\nAs an alternative structure to the LSTM, the gated recurrent unit (GRU) was proposed in [28]. The GRU was demonstrated to be better than LSTM in some tasks [29], and is formulated as follows [27]:\nrt = \u03b4(Wrxt + Rrht\u22121 + br) (2)\nzt = \u03b4(Wzxt + Rzht\u22121 + bz) (3)\nh\u0303t = \u03d5(Whxt + rt (Rhht\u22121) + bh) (4)\nht = zt ht\u22121 + (1\u2212 zt) h\u0303t (5)\nwhere ht, rt and zt are hidden activations, reset gate values and update gate values at frame t, respectively. The weights applied to the input and recurrent hidden units are denoted as W\u2217 and R\u2217, respectively. The biases are represented by b\u2217. The functions \u03b4(\u00b7) and \u03d5(\u00b7) are the sigmoid and tangent activation functions. Compared to the LSTM, there is no separate memory cell in the GRU. The GRU also does not have an output gate, and combines the input and forget gates into an update gate zt to balance between the previous activation ht\u22121 and the update activation h\u0303t shown in Eq. (5). The reset gate rt can decide whether or not to forget the previous activation (shown in Eq. (4)). in Eq. (4) and (5) represents the elementwise multiplication."}, {"heading": "C. Convolutional Gated Recurrent Network for audio tagging", "text": "Fig. 3 shows the framework of a convolutional gated recurrent neural network for audio tagging. The CNN is regarded as the feature extractor along the short window (e.g., 32ms) from the basic features, e.g., MFBs, spectrograms or raw waveforms. Then the robust features extracted are fed into the GRU-RNN to learn the long-term audio patterns. For the audio tagging task, there is a lot of background noise, and acoustic events may occur repeatedly and randomly along the whole chunk (without knowing the specific frame locations). The CNN can help to extract robust features against the background noise by the max-pooling operation, especially for the raw waveforms. Since the label of the audio tagging task is at the chunk-level rather than the frame-level, a large number of frames of the context were fed into the whole framework. The GRU-RNN can select related information from the longterm context for each audio event. To also utilize the future information, a bi-directional GRU-RNN is designed in this work. Finally the output of GRU-RNN is mapped to the posterior of the target audio events through one feed-forward neural layer, with sigmoid output activation function. This framework is flexible enough to be applied to any kinds of features, especially for raw waveforms. Raw waveforms have lots of values, which leads to a high dimension problem. However the proposed CNN can learn on short windows like the short-time Fourier transform (STFT) process, and the FFTlike basis sets or even mel-like filters can be learned for raw waveforms. Finally, one-layer feed-forward DNN gets the final sequence of GRUs to predict the posterior of tags.\nBinary cross-entropy is used as the loss function in our work, since it was demonstrated to be better than the mean squared error in [8] for labels with zero or one values. The loss can be defined as:\nE = \u2212 N\u2211\nn=1\n\u2016TnlogT\u0302n + (1\u2212 Tn)log(1\u2212 T\u0302n)\u2016 (6)\nT\u0302n = (1 + exp(\u2212O))\u22121 (7)\nwhere E is the binary cross-entropy, T\u0302n and Tn denote the estimated and reference tag vector at sample index n, respectively. The DNN linear output is defined as O before the sigmoid activation function is applied. Adam [30] is used as the stochastic optimization method."}, {"heading": "III. SPATIAL FEATURES INCORPORATED FOR AUDIO TAGGING", "text": "Spatial features can often offer additional cues to help to solve signal processing problems. Many spatial features can be used for audio tagging, such as interaural phase differences or interaural time differences (IPD or ITD) [31], interaural level differences (ILD) [31]. The recordings of the audio tagging task of DCASE 2016 challenge are recorded in the home scenes. There are some TV, child speech, adult speech audio events. The spatial features potentially give additional information to analyze the content of the audio, e.g., recognizing\nthe TV audio event by knowing the specific direction of the TV sound. The IPD and ILD are defined as:\nILD(t, k) = 20log10 \u2223\u2223\u2223\u2223 Xleft(t, k)Xright(t, k) \u2223\u2223\u2223\u2223 (8)\nIPD(t, k) = 6 ( Xleft(t, k)\nXright(t, k)\n) (9)\nwhere Xleft(t, k) and Xright(t, k) denote the left channel and right channel complex spectrum of the stereo audio. The operator |\u00b7| takes the absolute of the complex value, and 6 (\u00b7) finds the phase angle. In this work, we also define interaural magnitude differences (IMD) which is similar to the ILD. IMD is defined in linear domain while ILD is defined in logarithmic domain.\nIMD(t, k) = |Xleft(t, k)| \u2212 |Xright(t, k)| (10)\nFig. 4 shows the structure of incorporating the spatial features (IMD/ILD/IPD, etc.) using an additional CNN. Then the activations learned from the basic features and the activations learned from the spatial features are concatenated to be fed into the GRU-RNN plotted in Fig. 3. The audio files of the audio\ntagging task of the DCASE 2016 challenge are recorded in a domestic home environment. There are severe reverberation, high-level background noise and multiple acoustic sources. These factors might influence the effectiveness of IPD and ILD. Fig. 5 shows the spectrogram, IMD, ILD and IMD of one recording from the audio tagging task of DCASE 2016 challenge. The IMD appears to have some meaningful patterns while the ILD and the IPD seem to be random which would lead to the training difficulties of the classifier. From our empirical experiments, IPD and ILD do not appear to help to improve the classifier performance while IMD is beneficial. Similar results were reported in [19] where IPD was found not to be helpful for the sound event detection in home scenes but helpful for the event detection of residential areas. This may be because residential areas are open areas with less reverberation than indoor home environments. Hence we will use IMD as our spatial features in this work. The filter size of CNNs learned on the IMD is set the same with the related configuration for the spectrogram."}, {"heading": "IV. DATA SET AND EXPERIMENTAL SETUP", "text": ""}, {"heading": "A. DCASE 2016 audio tagging challenge", "text": "We conducted our experiments based on the DCASE 2016 audio tagging challenge [32]. This audio tagging task consists of the five-fold development set and the evaluation set, which are built based on the CHiME-home dataset [33]. The audio recordings were made in a domestic environment [34]. The audio data are provided as 4-second chunks at 48kHz sampling\nrates in stereo mode. We downsampled them into 16kHz sampling rate.\nFor each chunk, three annotators gave three labels, namely multi-label annotations. Then the discrepancies among annotators are reduced by conducting a majority vote. The annotations are based on a set of seven audio events as presented in Table I. A detailed description of the annotation procedure is provided in [33]."}, {"heading": "B. Experimental setup", "text": "In the experiments below, we follow the standard specification of the DCASE 2016 audio tagging task [32], On the development set, we use the official five folds for crossvalidation. Table II shows the number of chunks in the training and test set used for each fold. The number of final evaluation configuration is also listed.\nThe parameters of the networks are tuned based on the heuristic experience. All of the CNNs have 128 filters or feature maps. Following [10], the filter size for MFBs, spectrograms and raw waveforms are 30, 200, and 400, respectively. These parameters can form a large receptive field for each type of basic features considering that only one convolutional layer was designed. The CNN layer is followed by three RNN layers with 128 GRU blocks. One feed-forward layer with 500 ReLU units is finally connected to the 7 sigmoid output units. We pre-process each audio chunk by segmenting them using a 32ms sliding window with a 16ms hop size, and converting each segment into 40-dimension MFBs, 257- dimension spectrograms and 512-dimension raw waveforms. For performance evaluation, we use equal error rate (EER) as the main metric which is also suggested by the DCASE 2016 audio tagging challenge. EER is defined as the point of equal\nfalse negative (FN ) rate and false positive (FP ) rate [35]. The source codes for this paper can be downloaded from Github1."}, {"heading": "C. Compared methods", "text": "We compared our methods with the state-of-the-art systems. Lidy-CQT-CNN [15] and Cakir-MFCC-CNN [16] won the first and the second prize of the DCASE2016 audio tagging challenge [32]. They both used convolutional neural networks (CNN) as the classifier. We also compare to our previous method [8] which demonstrated the state-of-the-art performance using de-noising auto-encoder (DAE) to learn robust features."}, {"heading": "V. EXPERIMENTAL RESULTS AND ANALYSIS", "text": "In this section, the IMD effectiveness will be firstly evaluated on the development set of Task 4 of the DCASE 2016 challenge among the different features, i.e., spectrograms, MFBs and raw waveforms. Then the final evaluation will be presented by comparing with the state-of-the-art methods on the evaluation set of Task 4 of the DCASE 2016 challenge."}, {"heading": "A. The effectiveness of the IMD", "text": "Table III shows the EER comparisons on seven labels among the spectrogram, the raw waveform and the MFB systems with or without the IMD information, which are evaluated on the development set of the DCASE 2016 audio tagging challenge. Firstly, we can compare the proposed convolutional gated recurrent neural networks on spectrograms, raw waveforms and MFBs. Spectrograms are better than the MFBs perhaps because the spectrogram has more detailed frequency information compared with the MFB. For example, spectrograms are much better than MFBs on child speech (denoted as \u2018c\u2019) and female speech (denoted as \u2018f\u2019) where a lot of high frequency information exists. The raw waveforms are worse than the spectrograms and the MFBs. One possible reason is that the learned FIR filters are not stable when the whole training set is small (about 3.5 hours of audio in this work). The same explanation was given in [11] on the speech recognition task. [10] shows that raw waveforms can get better recognition accuracy with the mel-spectra on 2000 hours Google voice search data.\nWith the help of the IMD spatial features, the EER are improved compared to all of the corresponding basic features alone. The raw waveforms with IMD can even get comparable results with the spectrograms and the MFBs. The MFB-IMD combination is slightly better than Spec-IMD, which may be because the IMD is calculated from the left and right spectrograms. The IMD has some common information with the spectrograms which can be seen from Fig. 5. However, the IMD is more complementary for the MFBs and the raw waveforms. The previous best performance on the development set of the DCASE 2016 audio tagging challenge was obtained in our recent work using denoising auto-encoder [8] with 0.126 EER, but here we get better performance with 0.10 EER.\n1https://github.com/yongxuUSTC/cnn rnn spatial audio tagging"}, {"heading": "B. Overall evaluations", "text": "Table IV presents the EER comparisons on seven labels among Lidy-CQT-CNN [15], Cakir-MFCC-CNN [16], our previous DAE-DNN [8], and the proposed systems on the spectrogram, the raw waveform and the MFB systems with the IMD information, which are evaluated on the final evaluation set of the DCASE 2016 audio tagging challenge. The denoising auto-encoder [8] was our recent work which can outperform the leading system in the DCASE 2016 audio tagging challenge, namely Lidy-CQT-CNN [15]. Our proposed convolutional gated recurrent neural network incorporating the IMD features in this work gives further improved performance. The MFB-IMD obtains the best performance with 0.123 EER which is the state-of-the-art performance on the evaluation set of the DCASE 2016 audio tagging challenge."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we propose a convolutional gated recurrent neural network (CGRNN) to learn on the mel-filter banks (MFBs), the spectrograms and even the raw waveforms. The spatial features, namely the interaural magnitude difference (IMDs), are incorporated into the framework and are demonstrated to be effective to further improve the performance. Spectrogram gives better performance than MFBs without the spatial features. However the MFBs with the IMDs can get the minimal EER, namely 0.102, on the development set of the DCASE 2016 audio tagging challenge. Raw waveforms give comparable performance on the development set. Finally, on the evaluation set of the DCASE 2016 audio tagging challenge, our proposed MFB-IMD system can get the state-of-the-art performance with 0.l23 EER. It is still interesting to further explore why the MFB-IMD system is better than the SpecIMD system in our future work. In addition, we will also\ninvestigate the proposed framework to model raw waveforms on larger training datasets to learn more robust filters."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the Engineering and Physical Sciences Research Council (EPSRC) of the UK under the grant EP/N014111/1. Qiuqiang Kong is partially supported by the China Scholarship Council (CSC)."}], "references": [{"title": "Content-based classification, search, and retrieval of audio", "author": ["E. Wold", "T. Blum", "D. Keislar", "J. Wheaten"], "venue": "IEEE Multimedia, vol. 3, no. 3, pp. 27\u201336, 1996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Detection and classification of acoustic scenes and events: an IEEE AASP challenge", "author": ["D. Giannoulis", "E. Benetos", "D. Stowell", "M. Rossignol", "M. Lagrange", "M.D. Plumbley"], "venue": "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp. 1\u20134.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Content-based music audio recommendation", "author": ["P. Cano", "M. Koppenberger", "N. Wack"], "venue": "Proceedings of the 13th Annual ACM International Conference on Multimedia, 2005, pp. 211\u2013212.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Computing mel-frequency cepstral coefficients on the power spectrum", "author": ["S. Molau", "M. Pitz", "R. Schluter", "H. Ney"], "venue": "Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP\u201901). 2001 IEEE International Conference on, vol. 1, pp. 73\u201376.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Time and frequency filtering of filter-bank energies for robust hmm speech recognition", "author": ["C. Nadeu", "D. Macho", "J. Hernando"], "venue": "Speech Communication, vol. 34, no. 1, pp. 93\u2013114, 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Robust speech recognition using the modulation spectrogram", "author": ["B.E. Kingsbury", "N. Morgan", "S. Greenberg"], "venue": "Speech communication, vol. 25, no. 1, pp. 117\u2013132, 1998.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised feature learning based on deep models for environmental audio tagging", "author": ["Y. Xu", "Q. Huang", "W. Wang", "P. Foster", "S. Sigtia", "P.J. Jackson", "M.D. Plumbley"], "venue": "arXiv preprint arXiv:1607.03681v2, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting spectro-temporal locality in deep learning based acoustic event detection", "author": ["M. Espi", "M. Fujimoto", "K. Kinoshita", "T. Nakatani"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing, vol. 2015, no. 1, p. 1, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning the speech front-end with raw waveform CLDNNs", "author": ["T.N. Sainath", "R.J. Weiss", "A. Senior", "K.W. Wilson", "O. Vinyals"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic modeling with deep neural networks using raw time signal for LVCSR", "author": ["Z. T\u00fcske", "P. Golik", "R. Schl\u00fcter", "H. Ney"], "venue": "Proc. of Interspeech, 2014, pp. 890\u2013894.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for acoustic modeling of raw time signal in lvcsr", "author": ["P. Golik", "Z. T\u00fcske", "R. Schl\u00fcter", "H. Ney"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Wavenet: A generative model for raw audio", "author": ["A. v. d. Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "CQT-based convolutional neural networks for audio scene classification and domestic audio tagging", "author": ["T. Lidy", "A. Schindler"], "venue": "IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2016). [Online]. Available: http://www.ifs.tuwien.ac.at/ \u223cschindler/pubs/DCASE2016b.pdf", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Domestic audio tagging with convolutional neural networks", "author": ["E. Cak\u0131r", "T. Heittola", "T. Virtanen"], "venue": "IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2016). [Online]. Available: https://www.cs.tut.fi/sgn/arg/dcase2016/documents/ challenge technical reports/Task4/Cakir 2016 task4.pdf", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative training of gmm parameters for audio scene classification and audio tagging", "author": ["S. Yun", "S. Kim", "S. Moon", "J. Cho", "T. Kim"], "venue": "IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2016). [Online]. Available: http://www.cs.tut.fi/sgn/arg/dcase2016/documents/challenge technical reports/Task4/Yun 2016 task4.pdf", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "CP-JKU submissions for dcase-2016: A hybrid approach using binaural i-vectors and deep convolutional neural networks", "author": ["H. Eghbal-Zadeh", "B. Lehner", "M. Dorfer", "G. Widmer"], "venue": "Tech. Rep., DCASE2016 Challenge, Tech. Rep., 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Sound event detection in multichannel audio using spatial and harmonic features", "author": ["S. Adavanne", "G. Parascandolo", "P. Pertil\u00e4", "T. Heittola", "T. Virtanen"], "venue": "IEEE Detection and Classification of Acoustic Scenes and Events workshop, 2016. [Online]. Available: http://www.cs.tut.fi/sgn/arg/ dcase2016/documents/workshop/Adavanne-DCASE2016workshop.pdf", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L. Maria Gambardella", "J. Schmidhuber"], "venue": "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, vol. 22, no. 1, 2011, p. 1237.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Proc. Interspeech, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "2013 IEEE international conference on acoustics, speech and signal processing, 2013, pp. 6645\u20136649.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring convolutional neural network structures and optimization techniques for speech recognition.", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu"], "venue": "in Proc. Interspeech,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks.", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML, pp. 1310\u20131318,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Investigating gated recurrent networks for speech synthesis", "author": ["Z. Wu", "S. King"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5140\u20135144.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial Hearing: the Psychophysics of Human Sound Localization", "author": ["J. Blauert"], "venue": "MIT press,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1997}, {"title": "CHiMEhome: A dataset for sound source recognition in a domestic environment", "author": ["P. Foster", "S. Sigtia", "S. Krstulovic", "J. Barker", "M. Plumbley"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2015, pp. 1\u20135.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "The CHiME corpus: a resource and a challenge for computational hearing in multisource environments", "author": ["H. Christensen", "J. Barker", "N. Ma", "P. Green"], "venue": "Proceedings of Interspeech, 2010, pp. 1918\u20131921.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Murohy, Machine Learning: A Probabilistic Perspective", "author": ["P. K"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Audio tagging has many applications in areas such as information retrieval [1], sound classification [2] and recommendation system [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "Audio tagging has many applications in areas such as information retrieval [1], sound classification [2] and recommendation system [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "Audio tagging has many applications in areas such as information retrieval [1], sound classification [2] and recommendation system [3].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "Many frequency domain audio features such as melfrequency cepstrum coefficients (MFCCs) [4], Mel filter banks feature (MFBs) [5] and spectrogram [6] have been used for speech recognition related tasks [7] for many years.", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "Many frequency domain audio features such as melfrequency cepstrum coefficients (MFCCs) [4], Mel filter banks feature (MFBs) [5] and spectrogram [6] have been used for speech recognition related tasks [7] for many years.", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "Many frequency domain audio features such as melfrequency cepstrum coefficients (MFCCs) [4], Mel filter banks feature (MFBs) [5] and spectrogram [6] have been used for speech recognition related tasks [7] for many years.", "startOffset": 145, "endOffset": 148}, {"referenceID": 6, "context": "Many frequency domain audio features such as melfrequency cepstrum coefficients (MFCCs) [4], Mel filter banks feature (MFBs) [5] and spectrogram [6] have been used for speech recognition related tasks [7] for many years.", "startOffset": 201, "endOffset": 204}, {"referenceID": 7, "context": "Recently MFCCs and the MFBs were compared on the audio tagging task [8] and the MFBs can get", "startOffset": 68, "endOffset": 71}, {"referenceID": 8, "context": "The spectrogram has been suggested to be better than the MFBs in the sound event detection task [9], but has not yet been investigated in the audio tagging task.", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "Besides the frequency domain audio features, processing sound from the raw time domain waveforms, has attracted a lot of attentions recently [10], [11], [12].", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "Besides the frequency domain audio features, processing sound from the raw time domain waveforms, has attracted a lot of attentions recently [10], [11], [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "Besides the frequency domain audio features, processing sound from the raw time domain waveforms, has attracted a lot of attentions recently [10], [11], [12].", "startOffset": 153, "endOffset": 157}, {"referenceID": 9, "context": "Recent research [10] suggests that the Fourier basis sets may not be optimal and better basis sets can be learned from raw waveforms directly using a large set of audio data.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "To learn the basis automatically, convolutional neural network (CNN) is applied on the raw waveforms which is similar to CNN processing on the pixels of the image [13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "Processing raw waveforms has seen many promising results on speech recognition [10] and generating speech and music [14], with less research in non-speech sound processing.", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "Processing raw waveforms has seen many promising results on speech recognition [10] and generating speech and music [14], with less research in non-speech sound processing.", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": "Most audio tagging systems [8], [15], [16], [17] use mono channel recordings, or simply average the multi-channels as the input signal.", "startOffset": 27, "endOffset": 30}, {"referenceID": 14, "context": "Most audio tagging systems [8], [15], [16], [17] use mono channel recordings, or simply average the multi-channels as the input signal.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "Most audio tagging systems [8], [15], [16], [17] use mono channel recordings, or simply average the multi-channels as the input signal.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "Most audio tagging systems [8], [15], [16], [17] use mono channel recordings, or simply average the multi-channels as the input signal.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "Spatial features have been demonstrated to improve results in scene classification [18] and sound event detection [19].", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "Spatial features have been demonstrated to improve results in scene classification [18] and sound event detection [19].", "startOffset": 114, "endOffset": 118}, {"referenceID": 19, "context": "which is widely used in image classification [20], [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "which is widely used in image classification [20], [13].", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "CNNs can extract robust features from pixel-level values for images [13] or raw waveforms for speech signals [10].", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "CNNs can extract robust features from pixel-level values for images [13] or raw waveforms for speech signals [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "Recurrent neural network is the third structure which is often used for sequence modeling, such as language models [21] and speech recognition [22].", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "Recurrent neural network is the third structure which is often used for sequence modeling, such as language models [21] and speech recognition [22].", "startOffset": 143, "endOffset": 147}, {"referenceID": 9, "context": "This is similar to speech recognition work [10] where CNN has been used to extract features from the raw waveform signal.", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "The way of each filter producing one value can also be explained as a global pooling layer which is a structural regularizer that explicitly enforces feature maps to be confidence maps of meaningful feature channels [23].", "startOffset": 216, "endOffset": 220}, {"referenceID": 9, "context": "For example, F = 400 and M = 512 are set in [10].", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "filter of the CNN was actually learned as a finite impulse response (FIR) filter [12].", "startOffset": 81, "endOffset": 85}, {"referenceID": 23, "context": "If the spectrograms or mel-filter banks were fed into the CNN, the filtering was operated on the frequency domain [24] to reduce the frequency variants, such as the same audio pattern but with different pitches.", "startOffset": 114, "endOffset": 118}, {"referenceID": 21, "context": "results in speech recognition [22].", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "However, a simple recurrent neural network with the recurrent connection only on the hidden layer is difficult to train due to the well-known vanishing gradient or exploding gradient problems [25].", "startOffset": 192, "endOffset": 196}, {"referenceID": 25, "context": "The long short-term memory (LSTM) structure [26] was proposed to overcome this problem by introducing input gate, forget gate, output gate and cell state to control the information stream through the time.", "startOffset": 44, "endOffset": 48}, {"referenceID": 26, "context": "The fundamental idea of the LSTM is memory cell which maintains its state through time [27].", "startOffset": 87, "endOffset": 91}, {"referenceID": 27, "context": "As an alternative structure to the LSTM, the gated recurrent unit (GRU) was proposed in [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 28, "context": "The GRU was demonstrated to be better than LSTM in some tasks [29], and is formulated as follows [27]:", "startOffset": 62, "endOffset": 66}, {"referenceID": 26, "context": "The GRU was demonstrated to be better than LSTM in some tasks [29], and is formulated as follows [27]:", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "Binary cross-entropy is used as the loss function in our work, since it was demonstrated to be better than the mean squared error in [8] for labels with zero or one values.", "startOffset": 133, "endOffset": 136}, {"referenceID": 29, "context": "Adam [30] is used as the stochastic optimization method.", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "Many spatial features can be used for audio tagging, such as interaural phase differences or interaural time differences (IPD or ITD) [31], interaural level differences (ILD) [31].", "startOffset": 134, "endOffset": 138}, {"referenceID": 30, "context": "Many spatial features can be used for audio tagging, such as interaural phase differences or interaural time differences (IPD or ITD) [31], interaural level differences (ILD) [31].", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "Similar results were reported in [19] where IPD was found not to be helpful for the sound event detection in home scenes", "startOffset": 33, "endOffset": 37}, {"referenceID": 31, "context": "This audio tagging task consists of the five-fold development set and the evaluation set, which are built based on the CHiME-home dataset [33].", "startOffset": 138, "endOffset": 142}, {"referenceID": 32, "context": "recordings were made in a domestic environment [34].", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "A detailed description of the annotation procedure is provided in [33].", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "Following [10], the filter size for MFBs, spectro-", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "false negative (FN ) rate and false positive (FP ) rate [35].", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "Lidy-CQT-CNN [15] and Cakir-MFCC-CNN [16] won the first and the second prize of the DCASE2016 audio tagging challenge [32].", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "Lidy-CQT-CNN [15] and Cakir-MFCC-CNN [16] won the first and the second prize of the DCASE2016 audio tagging challenge [32].", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "We also compare to our previous method [8] which demonstrated the state-of-the-art performance using de-noising auto-encoder (DAE) to learn robust features.", "startOffset": 39, "endOffset": 42}, {"referenceID": 10, "context": "The same explanation was given in [11] on the speech recognition task.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "[10] shows that raw waveforms can get better recognition", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The previous best performance on the development set of the DCASE 2016 audio tagging challenge was obtained in our recent work using denoising auto-encoder [8] with 0.", "startOffset": 156, "endOffset": 159}, {"referenceID": 14, "context": "TABLE IV EER COMPARISONS ON SEVEN LABELS AMONG LIDY-CQT-CNN [15], CAKIR-MFCC-CNN [16], DAE-DNN [8], AND THE PROPOSED SYSTEMS ON THE SPECTROGRAM, THE RAW WAVEFORM AND THE MFB SYSTEMS WITH THE IMD INFORMATION, WHICH ARE EVALUATED ON THE FINAL EVALUATION SET OF THE DCASE 2016 AUDIO TAGGING CHALLENGE.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "TABLE IV EER COMPARISONS ON SEVEN LABELS AMONG LIDY-CQT-CNN [15], CAKIR-MFCC-CNN [16], DAE-DNN [8], AND THE PROPOSED SYSTEMS ON THE SPECTROGRAM, THE RAW WAVEFORM AND THE MFB SYSTEMS WITH THE IMD INFORMATION, WHICH ARE EVALUATED ON THE FINAL EVALUATION SET OF THE DCASE 2016 AUDIO TAGGING CHALLENGE.", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "TABLE IV EER COMPARISONS ON SEVEN LABELS AMONG LIDY-CQT-CNN [15], CAKIR-MFCC-CNN [16], DAE-DNN [8], AND THE PROPOSED SYSTEMS ON THE SPECTROGRAM, THE RAW WAVEFORM AND THE MFB SYSTEMS WITH THE IMD INFORMATION, WHICH ARE EVALUATED ON THE FINAL EVALUATION SET OF THE DCASE 2016 AUDIO TAGGING CHALLENGE.", "startOffset": 95, "endOffset": 98}, {"referenceID": 15, "context": "Eval set c m f v p b o ave Cakir [16] 0.", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "168 Lidy [15] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "166 DAE [8] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "Table IV presents the EER comparisons on seven labels among Lidy-CQT-CNN [15], Cakir-MFCC-CNN [16], our previous DAE-DNN [8], and the proposed systems on the spectrogram, the raw waveform and the MFB systems with the IMD information, which are evaluated on the final evaluation set of the DCASE 2016 audio tagging challenge.", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "Table IV presents the EER comparisons on seven labels among Lidy-CQT-CNN [15], Cakir-MFCC-CNN [16], our previous DAE-DNN [8], and the proposed systems on the spectrogram, the raw waveform and the MFB systems with the IMD information, which are evaluated on the final evaluation set of the DCASE 2016 audio tagging challenge.", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "Table IV presents the EER comparisons on seven labels among Lidy-CQT-CNN [15], Cakir-MFCC-CNN [16], our previous DAE-DNN [8], and the proposed systems on the spectrogram, the raw waveform and the MFB systems with the IMD information, which are evaluated on the final evaluation set of the DCASE 2016 audio tagging challenge.", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "The denoising auto-encoder [8] was our recent work which can outperform the leading system in the DCASE 2016 audio tagging challenge, namely Lidy-CQT-CNN [15].", "startOffset": 27, "endOffset": 30}, {"referenceID": 14, "context": "The denoising auto-encoder [8] was our recent work which can outperform the leading system in the DCASE 2016 audio tagging challenge, namely Lidy-CQT-CNN [15].", "startOffset": 154, "endOffset": 158}], "year": 2017, "abstractText": "Environmental audio tagging is a newly proposed task to predict the presence or absence of a specific audio event in a chunk. Deep neural network (DNN) based methods have been successfully adopted for predicting the audio tags in the domestic audio scene. In this paper, we propose to use a convolutional neural network (CNN) to extract robust features from mel-filter banks (MFBs), spectrograms or even raw waveforms for audio tagging. Gated recurrent unit (GRU) based recurrent neural networks (RNNs) are then cascaded to model the long-term temporal structure of the audio signal. To complement the input information, an auxiliary CNN is designed to learn on the spatial features of stereo recordings. We evaluate our proposed methods on Task 4 (audio tagging) of the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE 2016) challenge. Compared with our recent DNN-based method, the proposed structure can reduce the equal error rate (EER) from 0.13 to 0.11 on the development set. The spatial features can further reduce the EER to 0.10. The performance of the end-to-end learning on raw waveforms is also comparable. Finally, on the evaluation set, we get the state-of-the-art performance with 0.12 EER while the performance of the best existing system is 0.15 EER.", "creator": "LaTeX with hyperref package"}}}