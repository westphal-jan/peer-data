{"id": "1702.08074", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2017", "title": "Learning Control for Air Hockey Striking using Deep Reinforcement Learning", "abstract": "We consider the task according learning control intervention be with module negotiation striking next puck first an combat hockey start. The control continuous is every direct command to also monster ' 9 motors. We employ a introducing supposed deep structural learning structured came why similar preciseness skills similar brought the puck estimation following order to score. We propose similar improvements taken own standard learning scheme addition must the few Q - teacher encoding economically during only whether otherwise fail. Our improvements works integrating subsequently context broken the learning fraud, taking accounting for given changing availability of substances while within unfortunately manchester buffer. Finally make to our simulation testing there preventing coming have assure the successful experiences means this coordinated, the the improvement years algorithm stability previously not the allow removal.", "histories": [["v1", "Sun, 26 Feb 2017 19:59:59 GMT  (1091kb,D)", "https://arxiv.org/abs/1702.08074v1", null], ["v2", "Tue, 25 Apr 2017 10:52:33 GMT  (1224kb,D)", "http://arxiv.org/abs/1702.08074v2", "Corrected typos Graphs added in results section"]], "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["ayal taitler", "nahum shimkin"], "accepted": false, "id": "1702.08074"}, "pdf": {"name": "1702.08074.pdf", "metadata": {"source": "CRF", "title": "Learning Control for Air Hockey Striking using Deep Reinforcement Learning", "authors": ["Ayal Taitler"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The problem of learning a skill, a mapping between states and actions to reach a goal in a continuous world, lies at the heart of every interaction of an autonomous system with its environment. In this paper, we consider the problem of a robot learning how to strike effectively the puck in a game of air hockey. Air hockey is a fast competitive game where two players play against each other on a low-friction table. Players are required to develop and perfect skills as blocking and striking in order to play and win. Classical approaches for striking the puck involve a multi stage process of planning and execution. First, planning a strategy based on the goal and skill, e.g., calculating the best point of collision to achieve the goal, then planning a path and trajectory and finally executing the low level motoric control [15]. Each part requires full knowledge of the mechanical and physical models, which might be complex. We propose doing the planning and the control simultaneously with learning, which offer an off model way to learn from the final result. The result will be given in a form of reward at the end of each trial, and will direct the learning to the correct policy.\nSuch problems include policy gradients [26] where a mapping between states and actions is learned with gradient ascent optimization on the accumulated reward, with or without keeping track of the value function. Another popular approach is Learning from Demonstration (LfD) [2, 18] sometimes refereed as imitation learning [13] and apprenticeship learning [1]. In LfD a human expert (or a programmed agent) is recorded and the learning agent learns on the\nar X\niv :1\n70 2.\n08 07\n4v 2\n[ cs\n.L G\n] 2\n5 A\nrecorded data in a supervised fashion. Sometimes this process is used as an initialization for a second reinforcement learning stage for improvement. Paper [3] used imitation learning to learn primitive behaviors for a humanoid robot in air hockey.\nExploration in such an environment is also an interesting issue. -greedy exploration which is the most common one, is not highly efficient in such systems, since a dynamical system functions as a low pass filter [8] and once in a while using a random action might have little affect on the output of the system. We combined several types of explorations including -greedy and local exploration, along with prior knowledge based exploration that we proposed.\nWe propose an algorithm suitable for learning complex policies in dynamic physical environments. The algorithm combined -greedy exploration with a temporally correlated noise [9] for local exploration, which proved to be essential for effective learning. We further propose two novel contributions. We suggest a more relaxed approach to LfD which does not have the same limitations as standard LfD and can be learned from experience as regular RL. We also manage to overcome the instability of the learning due to the non-stationarity of the observed data, by expending the target update period.\nWe compare our results with other deep reinforcement learning algorithms and achieve significant improvements, we are able to reach near optimal results, and keep them without suffering from a drop in the score function and the policies obtained."}, {"heading": "2 Related Work", "text": "Research on learning in autonomous systems was conducted in several directions. Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].\nSince the groundbreaking results shown by Deep Q-Learning for learning to play games on the Atari 2600 arcade environment, there has been extensive research on deep reinforcement learning. Deep Q-learning in particular seeks to approximate the Q-values [25] using deep networks, such as deep convolutional neural networks. There has also been work on better target estimation [24], improving the learning by prioritizing the experience replay buffer to maximize learning [19] and preforming better gradient updates with parallel batch approaches [10, 14]. Some work on adaptation to the continuous control domain has been done also by [9]. Policy gradients methods were traditionally used [7,17,26], but struggled as the number of parameters increased. Adaptation to the deep neural network framework has also been done in recent years [20, 21]. Several benchmarks such as [5] have made comparisons between continuous control algorithms. In This paper we focus on the online DQN based approach, and extend it in the domain of continuous state optimal control for striking in air hockey."}, {"heading": "3 Deep Q-Networks", "text": "We consider a standard reinforcement learning setup consisting of an agent interacting with the environment in discrete time steps. At each step the agent receives an observation st \u2208 Rn which represents the current physical state of the system, takes a action at \u2208 A which it applies to the environment, receives a scalar reward rt = r(st, at), and observes a new state st+1 which the environment transitions to. It is assumed that the next state is according to a stochastic transition model P (st+1|st, at). The action set A is assumed to be discrete.\nThe goal of the agent is to maximize the sum of rewards gained from interaction with the environment. Our problem is a finite horizon problem in which the game terminates if the agent reached some predefined time T. We define the future return at time t as Rt = \u2211T t\u2032=t rt\u2032 , where T is the time at which the game terminates. The goal is to learn a policy which maximizes the expected return E [ R0 ] from the initial state.\nThe action-value function Q\u2217(s, a) is used in many reinforcement learning algorithms. It describes the expected return after taking an action a in state s and thereafter following an optimal policy. The optimal state-action value function Q\u2217 obeys the equality known as the Bellman\u2019s equation.\nQ\u2217 ( st, at ) = Est+1 [ rt + max a\u2032 Q\u2217 ( st+1, a \u2032) \u2223\u2223\u2223st, at] (1)\nFor learning purposes it is common to approximate the value of Q\u2217 ( s, a )\nby using a function approximator, such as a neural network. We refer to the neural network function approximator with weights \u03b8 as a Q-network. A neural network representing the Q-function can be trained by considering the loss function:\nL ( \u03b8 ) = Est,at,rt,st+1\u223cD [( y(\u03b8)\u2212Q ( st, at; \u03b8 ))2] (2)\nwhere y(\u03b8) = r ( st, at ) st+1 terminal\ny(\u03b8) = r ( st, at ) + max a Q ( st+1, a; \u03b8 ) st+1 not terminal\n(3)\nDuring training time each transition of state, action, reward and next state < st, at, rt, st+1 > is stored in an experience replay buffer D from which samples are drawn uniformly in order to reduce time correlations to train the network. y(\u03b8) is called the target and typically also a function of \u03b8. The max{\u00b7} operator in the target makes it hard to calculation derivatives in respect to the weights, so the target is kept constant and the derivatives are calculated only according to Q(st, at; \u03b8). This loss function has the tendency to oscillate and diverge. In order to keep the target stationary and prevent oscillations, the DQN algorithm make use of another network, called a target network with parameters \u03b8\u0302\u2212. The target network is the same as the on-line network except that its parameters are copied every C updates from the on-line network, so that \u03b8\u0302\u2212 are kept fixed during all other updates. The training of the network in this case is according to the following sequence of loss functions\nLi ( \u03b8i ) = Est,at,rt,st+1\u223cD [( yi(\u03b8\u0302 \u2212 i )\u2212Q ( st, at; \u03b8i ))2] (4)\nThe target used by DQN is then\nyi(\u03b8\u0302 \u2212 i ) = r ( st, at ) + max a Q ( st+1, a; \u03b8\u0302 \u2212 i ) (5)\nand the on-line network weights can be trained by stochastic gradient descent (SGD) and backpropagation\n\u03b8i+1 = \u03b8i + \u03b1\u2207\u03b8iLi(\u03b8i) (6)\nwhere \u03b1 is the learning rate. An improvement on that has been proposed in the double DQN algorithm, the decoupling of the estimation of the next value and the selection of the action, and decrease the problem of value overestimation, the following target has been used\nyi(\u03b8\u0302 \u2212 i ) = r ( st, at ) +Q ( st+1, at+1; \u03b8\u0302 \u2212 i ) at+1 = argmax a Q ( st+1, a; \u03b8i ) (7)\nIn our work unless specified otherwise all learning updates have been done according to the double DQN learning rule.\nTo explore the environment the systems typically explore via the -greedy heuristic. Given a state, a deep Q-network (DQN) predicts a value for each action. The agent chooses the action with the highest value with probability 1\u2212 and a random action with probability ."}, {"heading": "4 Striking in Air Hockey", "text": "We next introduce the striking problem and our learning approach.\n4.1 The Striking Problem\nThe striking problem deals in general with interception of a moving puck and striking it in a controlled manner. We specialize here to the case where the puck is stationary. We wish to learn the control policy for striking the puck such that after the impact, the puck trajectory will have some desired properties. We focus on learning to strike the puck directly to the opponent\u2019s goal. We also considered some other different modes of striking the puck, Such as hitting the wall first. These are not presented here, but the same learning scheme fits them as well. We refer to these modes as skills, which a high level agent can choose from in full air hockey game. The learning goal is to be able to learn these skills with the following desired properties\n\u2022 the puck\u2019s velocity should be maximal after the impact with the agent.\n\u2022 the puck\u2019s end position at the opponent\u2019s side should be the center of the goal.\n\u2022 the puck\u2019s direction should be according to the selected skill.\nThe agent is a planar robot with 2 degrees of freedom, X and Y (gantry like robot). We used a second order kinematics for the agent and puck. The state vector of the problem is st \u2208 R8, which includes all the position and velocities of the agent and the puck in both axes, i.e., st = [ mx, mV x, my, mV y, px, pV x, py, pV y ]T . Here m\u2217 stands for the agent\u2019s state variables and p\u2217 stands for the puck\u2019s state variables. The actions are at \u2208 R2, and include the accelerations in both axes for the agent.\nThe striking problem can be described as the following discrete time optimal planning problem:\nminimize ak \u03c6(sT , T )\nsubject to sk+1 = f(sk, ak)\ns (i) k \u2208\n[ S\n(i) min, S (i) max\n] , i = 1, . . . , 8\na (j) k \u2208\n[ A\n(j) min, A (j) max\n] , j = 1, 2\ns0 = s(0)\n(8)\nHere the objective function \u03c6(sT , T ) represents the value of the final state sT (in terms of velocity and accuracy), and the final time T which we desire to be small. The function f(\u00b7) is the physical model dynamics. S\n(i) min, S (i) max and A (j) min, A (j) max are the constraints on the state\n(table boundaries and velocities) and action spaces (accelerations/torques) respectively. s0 is the initial state. We assume that f(\u00b7), the collision models and the table state constraints are hidden from the learning algorithm, The best known collision model is non-linear and hard to work with [16]. Solving analytically such a problem when these function are known is a challenging problem, when they are unknown it is practically impossible with analytic tools. In the\nsimulations specific models were specified as explained in Section 5.\nIn order to fit the problem as stated in 4.1 to the DQN learning scheme, where the outputs are discrete Q values associated with discrete actions, we discretized the action space by sampling a 2D grid with n actions in each dimension (each dimension represents an axis in joint frame). Thus, we have n2 actions. We make sure to include the marginal and the zero action, so our class of policies we search in will include the Bang-Zero-Bang profile which is associated with time optimal problems. Each action is associated with an output of the neural network, where each output represents the Q-values of each action under the input state supplied to the network, e.g., if state s is supplied to the network, output i is the Q-value of Q(s, ai; \u03b8). Thus, for every given state we have n2 Q-values from the network, associated with the n2 actions.\n4.2 Reward Definition\nThe learning is episodic and in this problem the agent receives success indication only upon reaching a terminal state and finishing an episode. The terminal states are states in which the mallet collide with one of the walls (table boundaries violation), and the states in which the mallet strikes the puck (the agent does not perform any actions beyond this point). Any other state including the states in which an episode terminates due to reaching the maximal allowed steps, are not defined as terminal states. At the terminal state of each episode the agent receive the following reward\nRterminal = rc + rv + rd (9)\nRterminal is consists of three components. The first is rc, which is a fixed reward indicating a puck striking. The second component is a reward which encourages the agent to strike the puck with maximum velocity, and given by\nrv = sign (V ) \u00b7 V 2 (10)\nwhere V is the projection of the velocity on the direction of a desired location xg on the goal line of the opponent. The last component is a reward for striking accuracy, which indicates how close the puck reached xg.\nrd = { c |x\u2212 xg| \u2264 w c \u00b7 e\u2212d\u00b7(|x\u2212xg |\u2212w) |x\u2212 xg| > w\n(11)\nwhere x is the actual point the puck reaches on the opponent\u2019s side on the goal line, c is a scaling factor for the reward, w is the width of the window around the target point which receives the highest reward and d is a decay rate around the desired target location. Naturally, if the episode terminates without striking the puck Rterminal is zero. In order to encourage the agent to reach a terminal state in minimum time, the agent receives a negative small reward \u2212rtime for each time step of the simulation until termination. The accumulative reward for the entire episode then is Rtotal = Rterminal \u2212 n \u00b7 rtime, where n is the number of time steps for that episode.\n4.3 Exploration\nThe problem of Exploration is a major one, especially in the continuous domain. We address the issue from two angles, completely random exploration and local exploration.\n4.3.1 Completely Random Exploration\nWe use -greedy exploration (see Section 3) in order to allow experimenting with arbitrary actions. In physical systems with inertia it is not efficient since the system acts as a low pass filter, but it does give the agent some sampling of actions it would not try under normal conditions.\n4.3.2 Local Exploration\nThe main type of exploration is what we refer to as local exploration. Similarly to what was done in [9], we added a noise sampled from a noise process N to our currently learned policy.Since the agent can apply only actions from a discrete set of actions A, we projected the outcome on the agent\u2019s action set:\nat = PA{argmax a\nQ ( st, a; \u03b8 ) +Nt} (12)\nWe used forN an Ornstein\u2013Uhlenbeck process [23] to generate temporally correlated exploration noise for exploring efficiently. The noise parameters should be chosen in such a way that after the projection the exploration will be effective. Small noise might not change the action after the projection, but large noise might result in straying too far from the greedy policy. Thus, the parameters of the noise should be in proportion to the actions range and the aggregation.\n4.4 Prior Knowledge from Experience\nIn a complex environment, learning from scratch has been shown to be a hard task. Searching in a continuous high dimensional spaces with local exploration might prove futile. In many learning problems prior knowledge and understanding are present and can be used to improve the learning performance. A common way of inserting priors into the learning process uses LfD. For that purpose, multiple samples of expert performance should be collected, which is not always feasible or applicable.\nIn many cases the prior knowledge can be translated to some reasonable actions, although usually not an optimal policy. Examples for that can be seen in almost every planning problem. In games, the rules give us some guidance to what to do, e.g., in soccer, \u201cKick the ball to the goal\u201d, so for an agent to spend time on learning the fact that it has to kick the ball is a waste. In skydiving, the skydivers are told to move their hands to the sides in order to rotate, they are not required to search every possible pose to learn how to rotate. Furthermore, the basic rotating procedure taught to new skydivers is not the correct way to do it, it is taught as a basic technique, an initialization for them to modify and find the correct way.\nWe propose showing the agent a translation of the prior knowledge as a teacher policy. In some episodes instead of letting the agent to act according to the greedy policy, it does what the teacher policy suggests. The samples collected in those episodes are stored in the experience replay buffer as any other samples, allowing the learning algorithm to call upon that experience from the replay buffer and learn from it in within the standard framework.\nFor the problem of air hockey, we used a policy encapsulating some crude knowledge we have of the problem. We just instruct the agent to move in the direction of the puck, regardless of the task at hand (aiming to the right\\left\\middle), since this knowledge was simple, and robust enough. The guidance policy we constructed has the following form:\nVnext = Ppuck \u2212 Pagent \u2016Ppuck \u2212 Pagent\u2016 \u00b7MaxV elocity a = Vnext\u2212Vagent \u2206t\n\u2016Vnext\u2212Vagent\u2206t \u2016 \u00b7MaxForce\n(13)\nwhere Pobject is the x, y position vector of the object, and MaxV elocity, MaForce are physical constraints of the mechanics. The agent acts by the projection of the policy on its action space PA{\u00b7} This policy will result with an impact between the agent and puck, but by no account will be considered as a \u201cgood\u201d strike since there is no reason the puck will move in the goal\u2019s\ndirection (except in the special case when the puck lays on the line between the agent and the goal). The guidance policy is shown (the agent acts by it) and stored in the replay buffer with probability p.\n4.5 Non-Uniform Target Network Update Periods\nThe deep reinforcement learning problem is different from the supervised learning problem in a fundamental way, as the data which the network uses during the learning changes over time. At the beginning, the experience replay buffer is empty, the agent starts to act and fills the buffer, when the buffer reaches its maximal capacity new transitions overwrite the older ones. It is obvious that the data is changing over time, first changing in size and then changing in distribution. As the agent learns and gets better, the data in the buffer reflects that and the samples are more and more of good states which maximize the reward.\nRecall that the value the neural network tries to minimize is the loss function stated in (2). In order to stabilize the oscillations a target network with fixed weights over constant update periods were introduced. That led to the stationarity of the target value. The choosing of update period length became of the parameters that had to be set. Small update period result with instability since the target network changes too fast and oscillates, large update periods may be too stationary and the bootstrap process might not work properly. Thus, a period that is somewhere in the middle is chosen so the updates are stable.\nIn may domains such as in the air hockey and also in some of Atari games, DQN still suffers from a fall in the score. We argue that this fall is not only due to value overestimation (it happens for Double DQN updates as well), but also for issues with the target value. Choosing a middle value for the update period may result in slow learning in the beginning and fall in the score later in the learning due to oscillations.\nIn many domains such as in the air hockey and also in some of Atari games, DQN still suffers from a drop in the score as the learning process progresses (see, e.g., Fig. 2). We argue that this drop is not only due to value overestimation (it happens for Double DQN updates as well), but also for issues with the target value. Choosing a middle value for the update period may result in slow learning in the beginning and a drop in the score later in the learning due to oscillations.\nWe show that by adjusting the update period over time, we manage to stabilize the learning and prevent completely the drop in the score. We start with a small update period since the replay buffer D is empty and we want to learn quickly, we then keep expanding the period as the buffer gets larger, and we need more sampling to cover it. As the agent gets better and the distribution stabilizes, we also expand the update period in order to filter oscillations and keep the agent in the vicinity of the good learned policy. The expansion of the update period is done at each target weights update according to\nC = C \u00b7 Cr, Cr \u2265 1 (14)\nwhere Cr is the expansion rate. When Cr = 1 the updates are uniform as in the standard DQN.\nAt the beginning every sample contains new information that should affect the learning. As the learning progresses and the optimal policy hasn\u2019t been obtained yet, the samples in the replay buffer are diverse allowing the agent to learn from good samples and bad samples as well. At later stages when the agent has already learned a good policy, and the distribution of samples in the replay buffer resembles that policy. The network at the point if learning continuous,\nmight suffer from what is known as the catastrophic forgetting [6] of neural networks. Freezing the target network before that stage, stabilize the learning and allows the network fine tune its performances, even though the distribution in the replay buffer is undiverse. The target network contains then the knowledge gained in the past from bad examples. At that stage of the learning the update period should be large for that purpose. This is achieved by gradually increasing the update period from an initial small period at the beginning during the learning.\n4.6 Guided-DQN\nPutting the above-discussed features together produces the guided-DQN algorithm we used in the air hockey problem. The algorithm is given in algorithm 1.\nAlgorithm 1: Guided Deep Q-Network\ninput : Guidance policy \u03c0(s), Expansion rate Cr 1 Initialize replay memory D 2 Initialize states-actions value function Q with random weights \u03b8 3 Initialize target states-actions value function Q\u0302 with weights \u03b8\u0302\u2212 = \u03b8 4 for episode=1,M do 5 Observe initial state from environment s0 6 Initialize random process n for action exploration 7 with probability p decide if this episode is guided or not 8 while t<N and st is not terminal do 9 if guided episode then\n10 Select at = PA{argmax a\n\u03c0 ( st ) }"}, {"heading": "11 else", "text": "12 With probability select random action at otherwise select\nat = PA{argmax a Q(st, a; \u03b8) + nt}\n13 Execute action at in environment and observe reward rt, next state xt+1 and if terminal dt+1 14 Set new state st+1 = st, xt+1 15 Store transition < st, at, rt, st+1 > in D 16 Sample random mini-batch of transition < sj , aj , rj , sj+1 > from D\n17 set yt = rt +Q ( sj+1, argmax a Q ( sj+1, a; \u03b8 ) ; \u03b8\u0302\u2212 ) 18 Perform gradient descent step on ( yj \u2212Q(sj , aj ; \u03b8) )2 with respect to the network parameters \u03b8 19 Every C step reset Q\u0302 = Q, and set C = C \u00b7 Cr 20 return Q\nAs an input the algorithm gets the guidance policy, which encapsulated the prior knowledge we have on the problem, and the expansion rate Cr. At each episode, with probability p the entire episode will be executed with the guidance policy \u03c0(s), or with probability 1\u2212 p according to the greedy policy, with the addition of time correlated exploration noise. In either case, a guided episode or a greedy episode, at each step the algorithm stores the transitions in the replay buffer, and preforms a learning step on the Q network. Samples from the replay buffer are selected randomly with uniform probability. The projection operator PA projects the continuous actions onto the agent\u2019s discrete set, by choosing the action with the lowest euclidean distance. Every C updates the target Q network is updated with the weights of the on-line Q network, and C is expanded with a factor of Cr so the next time the target network\ngets updated, it will be after a longer period than the previous update.\nThe learning rule is a Double DQN learning rule. Note that if the algorithm is not provided with a guidance policy (equivalent to setting p to zero), Cr = 1, and the temporal correlated process is N \u2261 0, the GDQN algorithm reduces to the standard Double DQN algorithm."}, {"heading": "5 Experiments", "text": "The simulation was fashioned after the robotic system in Fig. 1.\nIn the robotic system the algorithm would learn on the real unknown physical models, but for the purpose of simulation we used simulation models for the agent dynamics and collision models. The simulation models are hidden from the learning algorithm and exist solely for the purpose of simulating the system for learning. For the agent dynamics we used a discrete time second order dynamics\nXm Vx,m Ym Vy,m  k+1 =  1 T 0 0 0 1 0 0 0 0 1 T 0 0 0 1   Xm Vx,m Ym Vy,m  k +  0 0 T 0 0 0 0 T [axay ] k\n(15)\nunder the following constraints\n|ax,y| < Maximum force |V {x, y},m| < Maximum velocity |Xm, Ym| < Table boundaries\nThese constraints represent the physical constraints present in the mechanical system, where the velocity has a maximum value, the torques are bounded and we are not allowing the mallet to move outside of the table boundaries.\nWe used in the simulations an ideal impact model between the mallet and puck in the sense that we neglected the friction between the bodies during the impact and we assume the impact is\ninstantaneous with energy loss according to a restitution coefficient e. The forces, accelerations, velocities and space (the field\u2019s boundaries) are constrained to reflect the physical constraints in the robotic system.\nThe list of parameters (learning rate, probabilities, etc.) used throughout the simulations is given in table 1.\nThe learning environment is a custom built simulation based on OpenAI gym [4]. The simulation is modeled after an air hockey robotic system with two motors and track, one for each axis. The simulation includes visually the table, the mallet and the puck.\nWe simulated each attempt to strike the puck as an independent episode comprised of discrete time steps. At the beginning of each episode the puck is placed at a random position on the table at the agent\u2019s half court with zero velocity and the agent starts from a home position (a fixed position near the middle of the goal) with zero velocity. Each episode terminates upon reaching a terminal state or upon passing the maximum number of steps defined for an episode. The maximum steps number is 150 steps and the terminal states are the states where the agent collides with the puck (\u201cgood\u201d states) or with one of the walls (\u201cbad\u201d states). The environment returns a reward as described in Section 4.2. No reward is given upon hitting a wall beyond the timely reward.\nThe dynamic model of the puck and agent is a second order model as described in Section 4.1. T is the sampling time of the system and was set to 0.05 [sec] in the simulation. The puck\u2019s rotation was neglected, thus the collision models (puck-agent, puck-wall) are ideal with inbound and outbound angles the same. Energy loss in the collisions was modeled with restitution coefficient of e = 0.99.\nThe controller is a non-linear neural controller, a fully connected Multi-Layer Perceptron with 4 layers (3 hidden layers and an output layer), the first two hidden layers are of 100 units each, the third hidden layer is of 40 units and the output layer is of 25 units. All activations are the linear rectifier f(x) = max(0, x). The controller is a map between states st (the inputs to the controller) and discretized Q-values. We choose 5 actions in each axis, yielding 25 output actions\\Q-values (see Section 4.1). We used the RMSProp algorithm with mini-batches of size 64.\nIn all the simulation experiments we measured the score for random initial positions of the puck, it will always be shown in graph with the caption random. In addition we measured\nthe performances for additional 3 fix representing states of the puck, fixed positions in the left side, the middle and the right side of the table. In addition we estimated the average value of all the states and present it as well. The graphs matching these measures will be shown with appropriate captions. We present in this paper the results for the \u201cdirect hit\u201d.\n5.1 Results\nFirst we show the performance of the standard Double DQN in Fig. 2 for different target network update periods. We choose a fast period a intermediate period and a slow period calculated such that each state in the buffer will be visited 8 times on average before being thrown away from the buffer.\nIt can be seen that the DDQN with fast updates (DDQN200) rises the fastest but also drops quickly, the same behavior can be observed for the intermediate updates (DDQN1000) but the rise is slower and the drop happens less sharply. The score value the network drops to, \u2212150, is exactly the value of the time penalty for a complete episode, i.e., the agent doesn\u2019t reach a terminal state. When investigating the policies obtained it can be seen that the agent\u2019s action oscillated between two opposite actions which affectively cause it to stand still. For the slow updates (DDQN5000) the case is different, the network seems mostly indifferent to the updates, and at the end it manages to rise a little. The average value for all three runs oscillates and in general suffers from severe underestimation.\nIn Fig. 3 we compare the results of three algorithms, the DDQN algorithm with the intermediate update period (the best of the three shown before), Deep-mind\u2019s Deep Deterministic Policy Gradients (DDPG) algorithm, and our Guided-DQN algorithm.\nThe DDPG algorithm manages to learn a suboptimal policy, but oscillates strongly around it. It can be seen in the fix positions graphs of the puck, although in the random graphs it looks pretty stable on the suboptimal policy. DDQN was discussed before, and our GDQN as can be seen clearly, learns the optimal policy and reaches the maximum score possible for each of them. In the random puck position the score also reaches an optimal policy in a very stable manner. Note that the score doesn\u2019t drop at all, and even the rise at the beginning is faster than the\nother two algorithms, it even faster than the rise of the DDQN with the fast updates shown in Fig. 2, due to the fast updates at the beginning and the guidance of the teacher policy. The average values of DDQN and DDPG are oscillating and suffering from underestimation and overestimation respectively, where GDQN\u2019s average value is extremely stable and does not suffer from over or under estimation.\nThe learned control polices and the trajectories are shown in Fig. 4 for a puck stationed in the left side of the table. The profile in the X-Y plane of the table is shown in Fig. 5. The agent is doing a curve in order to hit the puck from the left so it will go to the middle of the goal. The motion is visually very similar to an S-curve, in the X axis the agent performs a saturated action, compatible with a Bang-Bang profile, and in the Y axis something that effectively is like a Band-Zero-Bang."}, {"heading": "6 Conclusions", "text": "We addressed the application of striking a stationary puck with a physical mechanism. We showed that the standard DQN algorithm did not lead to satisfactory results. Therefore we proposed two novel improvements to this algorithm.\n1. using prior knowledge during the learning to direct the algorithm to interesting region of the state and action spaces.\n2. using non-uniform target update periods with expanding rate in order to stabilize the learning.\nWe also augmented the commonly used -greedy exploration mechanism with a local exploration with temporally correlated random process to better suite the physical environment.\nThe modified algorithm is shown to learn near optimal performance in the motion planning and control problem of air hockey striking. In particular, it solves completely the problem of score drop that was observed in Double DQN."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and autonomous systems 57,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "A framework for learning from observation using primitives", "author": ["D.C. Bentivegna", "C.G. Atkeson"], "venue": "In Robot Soccer World Cup", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Catastrophic forgetting in connectionist networks", "author": ["R.M. French"], "venue": "Trends in cognitive sciences 3,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Truncated natural policy gradient", "author": ["S.M. Kakade"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "The International Journal of Robotics Research", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": "Nature 518,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Learning table tennis with a mixture of motor primitives", "author": ["K. Muelling", "J. Kober", "J. Peters"], "venue": "In 2010 10th IEEE-RAS International Conference on Humanoid Robots (2010),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A. De Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S Petersen"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Hierarchical processing architecture for an air-hockey robot system", "author": ["A. Namiki", "S. Matsushita", "T. Ozeki", "K. Nonami"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Control of planar rigid body sliding with impacts and friction", "author": ["C.B. Partridge", "M.W. Spong"], "venue": "The International Journal of Robotics Research 19,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Reinforcement learning by reward-weighted regression for operational space control", "author": ["J. Peters", "S. Schaal"], "venue": "In Proceedings of the 24th international conference on Machine learning (2007),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Learning from demonstration. Advances in neural information processing systems", "author": ["S. Schaal"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "On the theory of the brownian motion", "author": ["G.E. Uhlenbeck", "L.S. Ornstein"], "venue": "Physical review 36,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1930}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. Van Hasselt", "A. Guez", "D. Silver"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning 8,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1992}], "referenceMentions": [{"referenceID": 13, "context": ", calculating the best point of collision to achieve the goal, then planning a path and trajectory and finally executing the low level motoric control [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 21, "context": "Such problems include policy gradients [26] where a mapping between states and actions is learned with gradient ascent optimization on the accumulated reward, with or without keeping track of the value function.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "Another popular approach is Learning from Demonstration (LfD) [2, 18] sometimes refereed as imitation learning [13] and apprenticeship learning [1].", "startOffset": 62, "endOffset": 69}, {"referenceID": 16, "context": "Another popular approach is Learning from Demonstration (LfD) [2, 18] sometimes refereed as imitation learning [13] and apprenticeship learning [1].", "startOffset": 62, "endOffset": 69}, {"referenceID": 11, "context": "Another popular approach is Learning from Demonstration (LfD) [2, 18] sometimes refereed as imitation learning [13] and apprenticeship learning [1].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "Another popular approach is Learning from Demonstration (LfD) [2, 18] sometimes refereed as imitation learning [13] and apprenticeship learning [1].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Paper [3] used imitation learning to learn primitive behaviors for a humanoid robot in air hockey.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "-greedy exploration which is the most common one, is not highly efficient in such systems, since a dynamical system functions as a low pass filter [8] and once in a while using a random action might have little affect on the output of the system.", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "The algorithm combined -greedy exploration with a temporally correlated noise [9] for local exploration, which proved to be essential for effective learning.", "startOffset": 78, "endOffset": 81}, {"referenceID": 9, "context": "Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].", "startOffset": 76, "endOffset": 92}, {"referenceID": 10, "context": "Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].", "startOffset": 76, "endOffset": 92}, {"referenceID": 18, "context": "Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].", "startOffset": 76, "endOffset": 92}, {"referenceID": 20, "context": "Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].", "startOffset": 76, "endOffset": 92}, {"referenceID": 7, "context": "Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].", "startOffset": 175, "endOffset": 178}, {"referenceID": 20, "context": "There has also been work on better target estimation [24], improving the learning by prioritizing the experience replay buffer to maximize learning [19] and preforming better gradient updates with parallel batch approaches [10, 14].", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "There has also been work on better target estimation [24], improving the learning by prioritizing the experience replay buffer to maximize learning [19] and preforming better gradient updates with parallel batch approaches [10, 14].", "startOffset": 223, "endOffset": 231}, {"referenceID": 12, "context": "There has also been work on better target estimation [24], improving the learning by prioritizing the experience replay buffer to maximize learning [19] and preforming better gradient updates with parallel batch approaches [10, 14].", "startOffset": 223, "endOffset": 231}, {"referenceID": 7, "context": "Some work on adaptation to the continuous control domain has been done also by [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "Policy gradients methods were traditionally used [7,17,26], but struggled as the number of parameters increased.", "startOffset": 49, "endOffset": 58}, {"referenceID": 15, "context": "Policy gradients methods were traditionally used [7,17,26], but struggled as the number of parameters increased.", "startOffset": 49, "endOffset": 58}, {"referenceID": 21, "context": "Policy gradients methods were traditionally used [7,17,26], but struggled as the number of parameters increased.", "startOffset": 49, "endOffset": 58}, {"referenceID": 17, "context": "Adaptation to the deep neural network framework has also been done in recent years [20, 21].", "startOffset": 83, "endOffset": 91}, {"referenceID": 3, "context": "Several benchmarks such as [5] have made comparisons between continuous control algorithms.", "startOffset": 27, "endOffset": 30}, {"referenceID": 14, "context": "We assume that f(\u00b7), the collision models and the table state constraints are hidden from the learning algorithm, The best known collision model is non-linear and hard to work with [16].", "startOffset": 181, "endOffset": 185}, {"referenceID": 7, "context": "Similarly to what was done in [9], we added a noise sampled from a noise process N to our currently learned policy.", "startOffset": 30, "endOffset": 33}, {"referenceID": 19, "context": "We used forN an Ornstein\u2013Uhlenbeck process [23] to generate temporally correlated exploration noise for exploring efficiently.", "startOffset": 43, "endOffset": 47}, {"referenceID": 4, "context": "might suffer from what is known as the catastrophic forgetting [6] of neural networks.", "startOffset": 63, "endOffset": 66}], "year": 2017, "abstractText": "We consider the task of learning control policies for a robotic mechanism striking a puck in an air hockey game. The control signal is a direct command to the robot\u2019s motors. We employ a model free deep reinforcement learning framework to learn the motoric skills of striking the puck accurately in order to score. We propose certain improvements to the standard learning scheme which make the deep Q-learning algorithm feasible when it might otherwise fail. Our improvements include integrating prior knowledge into the learning scheme, and accounting for the changing distribution of samples in the experience replay buffer. Finally we present our simulation results for aimed striking which demonstrate the successful learning of this task, and the improvement in algorithm stability due to the proposed modifications.", "creator": "LaTeX with hyperref package"}}}