{"id": "1703.00048", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Provably Optimal Algorithms for Generalized Linear Contextual Bandits", "abstract": "Contextual poachers already which used until Internet services under daily comment soon industry. Generalized linear suv (sufficient arithmetic same nature) well shown stronger performance each discrete features moved many applications. However, most theoretical analyses taken perspectives bandits always once are went linear miscreants. In way work, we require it upper confident vessel based method will cyclic formula_3 cross-cultural dacoits, addition implies an $ \\ background_notes {O} (\\ sqrt {dT} ) $ apologize over $ T $ hour half $ d $ dimensional typical formula_12. This regret soccer the minimax declines airplane, up with non-zero terms, on improvement on an best 1998 nevertheless although puts $ \\ sqrt {nunn} $ this, expectation in addition entire arms is interest. A another component second without detailed \u201d to establish a though, sharp formula_8 - exact expecting plane for maximum - anticipate compared, algebraic planar concepts, into may be particular leading stock. We as examine that rendering upper economy bound workaround, which part develop in practice, and ever meant to but correlation desire for rest that subject.", "histories": [["v1", "Tue, 28 Feb 2017 20:39:44 GMT  (39kb)", "http://arxiv.org/abs/1703.00048v1", "15 pages"], ["v2", "Sun, 18 Jun 2017 04:07:45 GMT  (39kb)", "http://arxiv.org/abs/1703.00048v2", "Published at ICML 2017"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["lihong li", "yu lu", "dengyong zhou"], "accepted": true, "id": "1703.00048"}, "pdf": {"name": "1703.00048.pdf", "metadata": {"source": "META", "title": "Provable Optimal Algorithms for Generalized Linear Contextual Bandits", "authors": ["Lihong Li", "Yu Lu", "Dengyong Zhou"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n00 04\n8v 1\n[ cs\n.L G\n] 2\n8 Fe\nb 20\n17\nservices from news recommendation to advertising. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contex-\ntual bandits, which achieves an O\u0303( \u221a dT ) regret over T rounds with d dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on\nthe best previous result by a \u221a d factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximumlikelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for the certain cases."}, {"heading": "1. Introduction", "text": "Contextual bandit problems are originally motivated by applications in clinical trials (Woodroofe, 1979). When a standard treatment and a new treatment are available for a certain disease, the doctor needs to decide, in a sequetial manner, which of them to use based on the patient\u2019s profiles such as age, general physical status or medicine history. With the development of modern technologies, contextual bandit problems have more applications, especially in web-based recommendation, advertising and search (Agarwal et al., 2009; Li et al., 2010; 2012). In the problem of personalized news recommendation, the website must recommend news articles that are most interesting to users that visit the website. The problem is especially challenging for breaking news, as little data are available\n1Microsoft Research, Redmond, WA 98052 2Department of Statistics, Yale University, New Haven, CT, USA.\nto make good prediction about user interest. A trade-off naturally occurs in this kind of sequential decision making problems. One needs to balance exploitation\u2014choosing actions that performed well in the past\u2014and exploration\u2014 choosing actions that may potentially give better outcomes.\nIn this paper, we study the following stochastic, Karmed contextual bandit problem. Suppose at each of the T rounds, an agent is presented with a set of K actions, each of which is associated with a context (a d-dimensional feature vector). By choosing an action based on the rewards obtained from previous rounds and on the contexts, the agent will receive a stochastic reward generated from some unknown distribution conditioned on the context and the chosen action. The goal of the agent is to maximize the expected cumulative rewards over T rounds. The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector. The linear model is theoretically convenient to work on. However, in practice, we usually have binary rewards (click or not, treatment working or not). Logistic regression model based algorithms have been shown to have substantial improvements over linear models (Li et al., 2012). Hence, we consider generalized linear models (GLM) in the contextual bandit setting, in which linear, logistic and probit regression serve as three important special cases.\nThe celebrated work of Lai & Robbins (1985) first introduces the upper confidence bound (UCB) approach to efficient exploration. Later, the idea of confidence bound has been successfully applied to many stochastic bandits problems, from K-arm bandits problems (Auer et al., 2002a; Bubeck & Cesa-Bianchi, 2012) to linear bandits (Auer, 2003; Abbasi-Yadkori et al., 2011). UCB-type algorithms are both efficient and provable optimal in K-arm bandits andK-armed linear bandits. However, most study are limited to the linear case. While some UCB-type algorithms using GLMs perform well empirically (Li et al., 2012), there is little theoretical study of them. A natural question arises: can we find an efficient algorithm to achieve the optimal convergence rate for generalized linear bandits?\nOur Contributions In this paper, we propose a GLM version of the UCB algorithm called SupCB-GLM that\nachieves a regret overT rounds of order O\u0303( \u221a dT ). This rate improves the state-of-the-art results of Filippi et al. (2010)\nby a \u221a d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem\u2019s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits. A key observation in proving this result is that the \u21132 confidence ball of the unknown parameter is insufficient to calculate a sharp upper confidence bound, yet what we need is the confidence interval in all directions. Thus, we prove a finite sample normality type confidence bound for the maximum likelihood estimator of GLM. To our best knowledge, this is the first non-asymptotic normality type result for the GLM and might be of its own theoretical value. We also analyze a simple version of UCB algorithm called UCB-GLM that is widely used in practice. We prove it also achieves the optimal regret bound under a reasonable assumption. These results shed light on explaining the good empirical performance of GLM bandits in practice.\nRelated Work The study of GLM bandits problem goes back at least to Sarkar (1991), who considered discounted regrets rather than cumulative regerts. They prove that a myopic rule without exploration is asymptotically optimal. Recently, Filippi et al. (2010) study the same stochastic GLM bandit problem considered here. They propose the GLM-UCB algorithm, similar to our Algorithm 1, which\nachieves a regret of O\u0303(d \u221a T ) after T rounds. However, as we believe the optimal regret for stochastic GLM bandits should be the same as linear case when the number of arms\nis small, their rates misses a \u221a d term than the optimal rates.\nAnother line of research focuses on using EXP-type algorithms, which can be applied to almost any model classes (Auer et al., 2002b). These algorithms, which choose actions using a carefully randomized policy, use importance sampling to reduce a bandit problem to its fullinformation analogue. Later variants of the EXP4 algorithm (Beygelzimer et al., 2010; Agarwal et al., 2014) give\nan O\u0303( \u221a dKT ) regret that is near-optimal with respect to\nT . However, these regret bounds have a \u221a K dependence. Moreover, these algorithms can be expensive to run: they either have a computational complexity exponential in d for our GLM case, or need to make a large number of calls to a nontrivial optimization oracle.\nOrganization The rest of this paper is organized as follows. Section 2 introduceS the generalized linear bandit problem. Section 3 gives a brief review of the statistical properties of generalized linear model, and gives a sharp\nnon-asymptotic normality-type result for GLM parameter estimation which can be of independent value. With this tool, Section 4 presents our algorithms and the main theoretical results. Section 5 concludes the paper with further discussions, including several open problems. All proofs are given in the supplementary materials.\nNotations For a vector x \u2208 Rd, we use \u2016x\u2016 to denote its \u21132- norm and x\n\u2032 its transpose. Denote the d-dimensional unit ball centered at the origin byBd := {x \u2208 Rd : \u2016x\u2016 \u2264 1}. The weighted \u21132-norm associated with a positivedefinite matrix A is defined by \u2016x\u2016A := \u221a x\u2032Ax. The minimum and maximum singular values of a matrix A are written as \u03bbmin(A) and \u2016A\u2016, respectively. The trace of a matrixA is tr (A). For two symmetric matricesA andB of the same dimensions, A B means that A\u2212B is positive semi-definite. For a real-valued function f , we use f\u0307 and f\u0308 to denote its first and second derivatives. Finally, the set {1, 2, . . . , n} is abbreviated as [n]."}, {"heading": "2. Problem Setting", "text": "We considered the stochastic K-armed contextual bandit problem. Let T be the number of total rounds. At round t, the agent observes a context consisting of a set of K feature vectors, {xt,a | a \u2208 [K]} \u2282 Rd, which is drawn IID from an unknown distribution \u03bd, with \u2016xt,a\u2016 \u2264 1. Each feature vector xt,a is associated with an unknown stochastic reward yt,a \u2208 [0, 1]. The agent selects one action, denoted at, and observes the corresponding reward yt,at . Finally, we make a regularity assumption about the distribution \u03bd: there exists a constant \u03c30 > 0 such that \u03bbmin(E[ 1 K \u2211 a\u2208[K] xt,ax \u2032 t,a]) \u2265 \u03c320 for all t.\nIn this paper, we consider the generalized linear model, or GLM, in which there is an unknown parameter \u03b8\u2217 \u2208 Rd and a fixed, strictly increasing link function \u00b5 : R \u2192 R such that E[Y | X ] = \u00b5(X \u2032\u03b8\u2217), where X is the chosen action\u2019s feature and Y the corresponding reward. One can verify that linear and logistic models are special cases of GLMwith \u00b5(x) = x and \u00b5(x) = 1/(1+e\u2212x), respectively.\nThe agent\u2019s goal is to maximize the cumulative expected rewards over T rounds. Suppose the agent takes action at at round t. Then the agent\u2019s strategy can be evaluated by comparing its expected reward to the best expected reward. To do so, define the optimal action at round t by a\u2217t = argmaxa\u2208[K] \u00b5(x \u2032 t,a\u03b8\n\u2217). Then, the agent\u2019s total regret of following strategy \u03c0 can be expressed as follows\nRT (\u03c0) :=\nT \u2211\nt=1\n(\n\u00b5(x\u2032t,a\u2217t \u03b8 \u2217)\u2212 \u00b5(x\u2032t,at\u03b8\u2217)\n)\n.\nNote that RT (\u03c0) is in general a random variable due to the possible randomness in \u03c0. Denote by Xt = xt,at , Yt =\nyt,at , and our model can be written as\nYt = \u00b5(X \u2032 t\u03b8 \u2217) + \u01ebt , (1)\nwhere {\u01ebt, t \u2208 [T ]} are independent zero-mean noise. Here, Xt is a random variable because the agent chooses current action based on previous rewards. Formally, we assume there is an increasing sequence of sigma fields {Fn} such that \u01ebt is Ft-measurable with E [ \u01ebt | Ft\u22121 ] = 0. An example of Fn will be the sigma-field generated by {X1, Y1, . . . , Xn, Yn}. Also, we assume the noise \u01ebt is sub-Gaussian with parameter \u03c3, where \u03c3 is some positive, universal constant; that is, for all t,\nE [ e\u03bb\u01ebt | Ft\u22121 ] \u2264 e\u03bb2\u03c32/2. (2)\nIn practice, when we have bounded reward Yt \u2208 [0, 1], the noise \u01ebt is also bounded and hence satisfies (2) with some appropriate \u03c3 value. In addition to the boundedness assumption on the rewards and feature vectors, we also need the following assumption on the link function \u00b5.\nAssumption 1. \u03ba := inf{\u2016x\u2016\u22641, \u2016\u03b8\u2212\u03b8\u2217\u2016\u22641} \u00b5\u0307(x \u2032\u03b8) > 0.\nAs we shall see in Section 3, the asymptotic normality of maximum-likelihood estimates implies the necessity of this assumption. Note that this assumption is weaker than Assumption 1 in Filippi et al. (2010), as it only requires to control the local behavior of \u00b5\u0307(x\u2032\u03b8) near \u03b8\u2217.\nAssumption 2. \u00b5 is twice differentiable. Its first and second order derivatives are upper-bounded by L\u00b5 and M\u00b5, respectively.\nIt can be verified that Assumption 2 holds for the logistic link function, where we may choose L\u00b5 = M\u00b5 = 1/4."}, {"heading": "3. Generalized Linear Models", "text": "To motivate the algorithms proposed in this paper, we first briefly review the classical likelihood theory of generalized linear models. In the canonical generalized linear model (McCullagh & Nelder, 1989), the conditional distribution of Y given X is from the exponential family, and its density, parameterized by \u03b8 \u2208 \u0398, can be written as\nP(Y | X ) = exp\n{\nY X \u2032\u03b8\u2217 \u2212m(X \u2032\u03b8\u2217)\ng(\u03b7) + h(Y, \u03b7)\n}\n. (3)\nHere, \u03b7 \u2208 R+ is a known scale parameter; m, g and h are three normalization functions mapping from R to R. The exponential family (3) is a very broad family of distributions including the Gaussian, binomial, Poisson, gamma and inverse-Gaussian distributions. It follows from standard properties of exponential families (Brown, 1986) that m is infinitely differentiable satisfying m\u0307(X \u2032\u03b8\u2217) = E[Y | X ] = \u00b5(X \u2032\u03b8\u2217) and m\u0308(X \u2032\u03b8\u2217) = V(Y | X). It can be checked that the data generated from (3) automatically satisfies the sub-Gaussian condition (2).\nSuppose we have independent samples of Y1, Y2, . . . , Yn condition onX1, X2, . . . , Xn. The log-likelihood function of \u03b8 under model (3) is\nlog \u2113(\u03b8) =\nn \u2211\nt=1\n[ YtX \u2032 t\u03b8 \u2212m(X \u2032t\u03b8) v(\u03b7) + c(Yt, \u03b7) ]\n= 1\nv(\u03b7)\nn \u2211\nt=1\n[YtX \u2032 t\u03b8 \u2212m(X \u2032t\u03b8)] + constant .\nConsequently, the maximum likelihood estimate (MLE) may be defined by\n\u03b8\u0302n \u2208 argmax \u03b8\u2208\u0398\nn \u2211\nt=1\n[YtX \u2032 t\u03b8 \u2212m(X \u2032t\u03b8)] .\nFrom classical likelihood theory (Lehmann & Casella, 1998), we know that when the sample size n goes to infinity, theMLE \u03b8\u0302n is asymptotically normal, that is, \u03b8\u0302n\u2212\u03b8\u2217 \u2192 N (0, I\u22121\u03b8\u2217 ), where I\u03b8 = \u2211n t=1 \u00b5\u0307(X \u2032 t\u03b8)XtX \u2032 t is the Fisher InformationMatrix. Note that if \u00b5\u0307(X \u2032t\u03b8 \u2217) \u2192 0, the asymptotic variance of x\u2032\u03b8\u0302 can go to infinity for some x. This suggests the necessity of Assumption 1.\nAs we will see later, the normality result is crucial in our regret analysis of GLM bandits. However, to the best of our knowledge, there is no non-asymptotic normality results of the MLE for GLM. In the following, we present a finite-sample version of the classical asymptotic normality results, which can be of independent interest. Theorem 1. Define Vn = \u2211n t=1 XtX \u2032 t, and let \u03b4 > 0 be given. Furthermore, assume that\n\u03bbmin(Vn) \u2265 512M2\u00b5\u03c3 2\n\u03ba4\n(\nd2 + log 1\n\u03b4\n)\n. (4)\nThen, with probability at least 1 \u2212 3\u03b4, the maximumlikelihood estimator satisfies, for any x \u2208 Rd, that\n|x\u2032(\u03b8\u0302n \u2212 \u03b8\u2217)| \u2264 3\u03c3\n\u03ba\n\u221a\nlog(1/\u03b4) \u2016x\u2016V \u22121n . (5)\nThis theorem characterizes the behavior of MLE on every direction. It implies that x\u2032(\u03b8\u0302n \u2212 \u03b8\u2217) has a sub-Gaussian tail bound for any x \u2208 Rd. It also provides a rigorous justification of the asymptotic upper confidence bound derived heuristically by Filippi et al. (2010, Section 4.2).\nThe proof of the theorem is given in the appendix. It consists of two main steps, as is typical for proving normalitytype results of MLEs (Van der Vaart, 2000). We first show the n\u22121/2-consistency of \u03b8\u0302 to \u03b8\u2217. Then, by using a secondorder Taylor expansion or Newton-step, we can prove the desired normality of \u03b8\u0302.\nThe condition (4) on \u03bbmin(Vn) is necessary for the consistency of estimating linear models (Lai & Wei,\n1982; Bickel et al., 2009) and generalized linear models (Fahrmeir & Kaufmann, 1985; Chen et al., 1999). It can be satisfied under mild conditions such as the proposition below, which will be useful for our analysis.\nProposition 1. Define Vn = \u2211n t=1 XtX \u2032 t, where Xt is drawn iid from some distribution \u03bd with support in the unit ball, Bd. Furthermore, let \u03a3 := E[XtX \u2032 t] be the second moment matrix, and B and \u03b4 > 0 be two positive constants. Then, there exist positive, universal constants C1 and C2 such that \u03bbmin(Vn) \u2265 B with probability at least 1\u2212 \u03b4, as long as\nn \u2265 (\nC1 \u221a d+ C2 \u221a log(1/\u03b4)\n\u03bbmin(\u03a3)\n)2\n+ 2B\n\u03bbmin(\u03a3) .\nProof Sketch. We give a proof sketch here, and the full proof is found in the appendix. In the following, for simplicity, we will drop the subscript n when there is no ambiguity. Therefore,Vn is denotedV and so on. We will need a technical lemma, which is an existing result in randommatrix theory. The version we presented here is adapted from Equation (5.23) of Theorem 5.39 from Vershynin (2012).\nLemma 1. Let A \u2208 Rn\u00d7d be a matrix whose rows Ai are independent sub-Gaussian isotropic random vectors in R d with parameter \u03c3, namely, E exp(x\u2032(Ai \u2212 EAi)) \u2264 exp(\u03c32 \u2016x\u20162 /2) for any x \u2208 Rd. Then, there exist positive, universal constants C1 and C2 such that, for every t \u2265 0, the following holds with probability at least 1 \u2212 2 exp(\u2212C2t2), where \u03b5 = \u03c32(C1 \u221a d/n + t/ \u221a n): \u2225 \u2225\n1 nA \u2032A\u2212 Id \u2225 \u2225 \u2264 max{\u03b5, \u03b52}.\nLet X be a random vector drawn from the distribution \u03bd. Define Z := \u03a3\u22121/2X . Then Z is isotropic, namely, E[ZZ \u2032] = Id. Define U = \u2211n t=1 ZtZ \u2032 t = \u03a3\n\u22121/2V \u03a3\u22121/2. From Lemma 1, we have that, for any t, with probability at least 1 \u2212 2 exp(\u2212C2t2), \u03bbmin(U) \u2265 n \u2212 C1\u03c32 \u221a nd \u2212\n\u03c32t \u221a n, where \u03c3 is the sub-Gaussian parameter ofZ , which is upper-bounded by \u2225 \u2225\u03a3\u22121/2 \u2225\n\u2225 = \u03bb \u22121/2 min (\u03a3) (see, e.g.,\nVershynin (2012)). We thus can rewrite the above inequality (which holds with probability 1\u2212 \u03b4 as\n\u03bbmin(U) \u2265 n\u2212 \u03bb\u22121min(\u03a3) ( C1\u03c3 2 \u221a nd+ t \u221a n ) .\nThis implies the following lower bound:\n\u03bbmin(V ) \u2265 \u03bbmin(\u03a3)n\u2212 C1 \u221a nd\u2212 C2 \u221a n log(1/\u03b4) .\nFinally, simple calculations show that the last expression above is no less than B as long as n is no smaller than the expression stated in the proposition, finishing the proof."}, {"heading": "4. Algorithms and Main Results", "text": "In this section, we are going to present two algorithms. While the first algorithm is computationally more efficient, the second algorithm has a provable optimal regret bound."}, {"heading": "4.1. Algorithm UCB-GLM", "text": "The idea of upper confidence bounds (UCB) is highly effective in dealing with the exploration and exploitation trade-off in many parametric bandit problems, including K-arm bandits (Auer et al., 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008). For the generalized linear model considered here, since \u00b5 is a strictly increasing function, our goal is equivalent to choosing a \u2208 [K] to maximize x\u2032t,a\u03b8 \u2217 at round t. Suppose \u03b8\u0302t is our current estimator of \u03b8 \u2217 after round t. An exploitation action is to take the action that maximizes the estimated mean value, while an exploration action is to choose the one that has the largest variance. Thus, to balance exploitation and exploration, we can simply choose the action that maximizes the sum of estimated mean and variance, which can be interpreted as an upper confidence bound of x\u2032t,a\u03b8\u0302t. This leads to the algorithm UCB-GLM (Algorithm 1).\nAlgorithm 1 UCB-GLM\nInput: the total rounds T , tuning parameter \u03c4 and \u03b1. Initialization: randomly choose at \u2208 [K] for t \u2208 [\u03c4 ], set V\u03c4+1 = \u2211\u03c4 i=1 XtX \u2032 t\nFor t = \u03c4 + 1, \u03c4 + 2, . . . , T do\n1. Calculate the maximum-likelihood estimator \u03b8\u0302t by solving the equation\nt\u22121 \u2211\ni=1\n(Yi \u2212 \u00b5(X \u2032i\u03b8))Xi = 0 (6)\n2. Choose at = argmaxa\u2208[K]\n( X \u2032t,a\u03b8\u0302t + \u03b1 \u2016Xt,a\u2016V \u22121t )\n3. Observe Yt, let Xt \u2190 Xt,at , Vt+1 \u2190 Vt +XtX \u2032t End For\nUCB-GLM take two parameters. At the initialization stage, we randomly choose actions to ensure a unique solution of (6). The choice of \u03c4 in the theorem statement follows from Proposition 1 with B = 1. It should be noted that the IID assumption about contextual (i.e., the distribution \u03bd) is only needed to ensure V\u03c4+1 is invertable (similar to the first phase in the algorithm of Filippi et al. (2010)); the rest of our analysis does not depend on this stochastic assumption. The same may be achieved by using regularization (see, e.g., Abbasi-Yadkori et al. (2011)). Another tuning parameter \u03b1 is used to control the amount of exploration. The larger the \u03b1 is, the more exploration will be used.\nAs mentioned earlier, the feature vectors Xt depend\non the previous rewards. Consequently, the rewards {Yi, i \u2208 [t]} may not be independent given {Xi, i \u2208 [t]}. We instead use results on self-normalized martingales (Abbasi-Yadkori et al., 2011), together with a finitetime normality result like Theorem 1, to prove the next theorem.\nTheorem 2. Fix any \u03b4 > 0. There exists a universal constant C > 0, such that if we run UCB-GLM with\n\u03b1 = \u03c3\u03ba\n\u221a\nd 2 log(1 + 2T/d) + log(1/\u03b4) and \u03c4 = C\u03c3 \u22122 0 (d+\nlog(1/\u03b4)), then, with probability at least 1 \u2212 2\u03b4, the regret of the algorithm is upper bounded by\nRT \u2264 \u03c4 + 2L\u00b5\u03c3d\n\u03ba log\n(\nT\nd\u03b4\n)\u221a T .\nThe theorem shows an O\u0303(d \u221a T ) regret bound that is independent of K . Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008). By choosing \u03b4 = 1/T and using the fact that RT \u2264 T , this highprobability result implies a bound on the expected regret:\nE[RT ] = O\u0303(d \u221a T ). Our result improves the previous\nregret bound of Filippi et al. (2010) by a \u221a logT factor. Moreover, the algorithm proposed in Filippi et al. (2010) involves a projection step, which is computationally more expensive comparing to UCB-GLM. Finally, this algorithm works well in practice. We give a heuristic argument for its strong performance in Section 5, under a specific condition that sometimes are satisfied.\nProof of Theorem 2. We first bound the one-step regret. To do so, fix t and let X\u2217t = xt,a\u2217t and \u2206t = \u03b8\u0302t \u2212 \u03b8\u2217, where a\u2217t \u2208 argmaxa\u2208[K] \u00b5(x\u2032t,a\u03b8\u2217) is an optimal action at round t. The selection of at in UCB-GLM implies\n\u3008X\u2217t , \u03b8\u0302t\u3009+ \u03b1 \u2016X\u2217t \u2016V \u22121t \u2264 \u3008Xt, \u03b8\u0302t\u3009+ \u03b1 \u2016Xt\u2016V \u22121t . (7) Then, we have\n\u3008X\u2217t , \u03b8\u2217\u3009 \u2212 \u3008Xt, \u03b8\u2217\u3009 = \u3008X\u2217t \u2212Xt, \u03b8\u0302t\u3009 \u2212 \u3008X\u2217t \u2212Xt, \u03b8\u0302t \u2212 \u03b8\u2217\u3009 \u2264 \u03b1 \u2016Xt\u2016V \u22121t \u2212 \u03b1 \u2016X \u2217 t \u2016V \u22121t \u2212 \u3008X \u2217 t \u2212Xt,\u2206t\u3009\n\u2264 \u03b1 ( \u2016Xt\u2016V \u22121t \u2212 \u2016X \u2217 t \u2016V \u22121t ) + \u2016X\u2217t \u2212Xt\u2016V \u22121t \u2016\u2206t\u2016Vt , where the last inequality is due to Cauchy-Schwartz inequality. We have the following two lemmas to bound \u2016\u2206t\u2016Vt and \u2016Xt\u2016V \u22121t , respectively. Their proofs are deferred to the appendix.\nLemma 2. Let {Xt}\u221et=1 be a sequence in Rd satisfying \u2016Xt\u2016 \u2264 1. Define X0 = 0 and Vt = \u2211t\u22121 s=0 XsX \u2032 s. Suppose there is an integerm such that \u03bbmin(Vm+1) \u2265 1, then for all n > 0,\nm+n \u2211\nt=m+1\n\u2016Xt\u2016V \u22121t \u2264 \u221a 2nd log ( n+m\nd\n)\n.\nLemma 3. Suppose \u03bbmin(V\u03c4+1) \u2265 1. For any \u03b4 \u2208 [1/T, 1), define event\nE\u2206 := { \u2016\u2206t\u2016Vt \u2264 \u03c3\n\u03ba\n\u221a\nd 2 log(1 + 2t/d) + log(1/\u03b4)\n}\n.\nThen, event E\u2206 holds for all t \u2265 \u03c4 with probability at least 1\u2212 \u03b4.\nWe now choose \u03b1 = \u03c3\u03ba\n\u221a\nd 2 log(1 + 2T/d) + log(1/\u03b4). If\nevent E\u2206 holds for all t \u2265 \u03c4 , then,\n\u3008X\u2217t , \u03b8\u2217\u3009 \u2212 \u3008Xt, \u03b8\u2217\u3009 \u2264 \u03b1 (\n\u2016Xt\u2016V \u22121t \u2212 \u2016X \u2217 t \u2016V \u22121t + \u2016X \u2217 t \u2212Xt\u2016V \u22121t\n)\n\u2264 2\u03b1 \u2016Xt\u2016V \u22121t .\nCombining the above with Lemma 2 yields\nT \u2211\nt=\u03c4+1\n( \u3008X\u2217t , \u03b8\u2217\u3009 \u2212 \u3008Xt, \u03b8\u2217\u3009 )\n\u2264 2\u03b1 \u221a 2Td log ( T\nd\n)\n\u2264 2d\u03c3 \u03ba log\n(\nT\nd\u03b4\n)\u221a T . (8)\nNote that \u00b5 is an increasing Lipschitz function with Lipschitz constant L\u00b5 and the \u00b5 function is bounded between 0 and 1. The regret of algorithm UCB-GLM can be upper bounded as\nRT =\n\u03c4 \u2211\nt=1\n( \u00b5 (\u3008X\u2217t , \u03b8\u2217\u3009)\u2212 \u00b5 (\u3008Xt, \u03b8\u2217\u3009) )\n+ T \u2211\nt=\u03c4+1\n( \u00b5 (\u3008X\u2217t , \u03b8\u2217\u3009)\u2212 \u00b5 (\u3008Xt, \u03b8\u2217\u3009) )\n\u2264 \u03c4 + L\u00b5 T \u2211\nt=\u03c4+1\n( \u3008X\u2217t , \u03b8\u2217\u3009 \u2212 \u3008Xt, \u03b8\u2217\u3009 ) .\nThe proof can be finished by applying (8) and the specified value of \u03c4 to the bound above."}, {"heading": "4.2. Algorithm SupCB-GLM", "text": "While the algorithm UCB-GLM performs sufficiently well in practice (Li et al., 2012), it is unclear whether it can achieve the optimal rates of O( \u221a dT logK), when K is small. As mentioned in Section 4.1, the key technical difficulty in analyzing UCB-GLM is the dependence between samples. Inspired by a technique developed by Auer (2003) to create independent samples for linear contextual bandits, we propose another algorithm SupCB-GLM (Algorithm 3), which uses algorithm CB-GLM (Algorithm 2) as a subroutine.\nAlgorithm 2 CB-GLM\nInput: parameter \u03b1, index set \u03a8(t), and candidate set A.\n1. Let \u03b8\u0302t be the solution of \u2211 i\u2208\u03a8(t) [Yi \u2212 \u00b5(X \u2032i\u03b8)]Xi = 0\n2. Vt = \u2211 i\u2208\u03a8(t) XiX \u2032 i\n3. For a \u2208 A, do\nwt,a = \u03b1 \u2016xt,a\u2016V \u22121t , mt,a = \u3008xt,a, \u03b8\u0302t\u3009\nEnd For\nThis algorithm also relies on the idea of confidence bound to do exploration. At round t, the algorithm screens the candidate actions based on the value of w (s) t,a through S stages until an action is chosen. At stage s, we set the confidence level at stage s to be 2\u2212s. If w (s) t,a > 2 \u2212s for some a, we need to do more exploration on xt,a and thus we choose this action. Otherwise, the actions are filtered in step 2d such that the actions passed to the next stage are close enough to the optimal action. Since all the widths are smaller than 2\u2212s, if m (s) t,a < m (s) t,j \u2212 2 \u00b7 2\u2212s for some j \u2208 As, the action a can not be the optimal action. The filter process terminates when we have already got accurate estimate of all x\u2032t,a\u03b8 \u2217 up to the 1/ \u221a T level and we do not need to do exploration. Thus in step 2c we just choose the action that maximizes the estimated mean value.\nOur algorithm is different from the algorithm SupLinRel in Auer (2003) that we directly maximize the mean, rather than the upper confidence bound, in steps c and d. This modification leads to a simpler algorithm and a cleaner regret analysis. Also, we would like to point out that, unlike SpectralEliminator (Valko et al., 2014), the algorithm can easily handle a changing action set.\nThe following result, adapted fromAuer (2003, lemma 14), shows how the algorithm SupCB-GLM will give us independent samples. For the sake of completeness, we also present its proof here.\nLemma 4. For all s \u2208 [S] and t \u2208 [T ], given {xi,ai , i \u2208 \u03a8s(t)}, the rewards {yi,ai , i \u2208 \u03a8s(t)} are independent random variables.\nProof of Lemma 4. Since a trial t can only be added to \u03a8s(t) in step 2b of algorithm SupCB-GLM, the event {t \u2208 \u03a8s} only depends on the results of trials \u03c4 \u2208 \u222a\u03c3<s\u03a8\u03c3(t) and on w\n(s) t,a . From the definition of w (s) t,a , we know it only\ndepends on the feature vectors xi,ai , i \u2208 \u03a8s(t) and on xt,i. This implies the lemma.\nWith Lemma 4, we are able to apply the non-asymptotic\nAlgorithm 3 SupCB-GLM\nInput: tuning parameter \u03b1, \u03c4 , the number of trials T .\nInitialization:\nfor t \u2208 [\u03c4 ], randomly choose at \u2208 [K]. Set S = \u230alog2 T \u230b, F = {a1, \u00b7 \u00b7 \u00b7 , a\u03c4} and \u03a80 = \u03a81 =\n\u00b7 \u00b7 \u00b7 = \u03a8S = \u2205. For t = \u03c4 + 1, \u03c4 + 2, \u00b7 \u00b7 \u00b7 , T do\n1. Initialize A1 = [K] and s = 1.\n2. While at =Null\na. Run CB-GLM with \u03b1 and \u03a8s \u222a F to calculate m\n(s) t,a and w (s) t,a for all a \u2208 As.\nb. If w (s) t,a > 2 \u2212s for some a \u2208 As,\nset at = a, update\u03a8s = \u03a8s \u222a {t}\nc. Else if w (s) t,a \u2264 1/ \u221a T for all a \u2208 As,\nset at = argmax a\u2208As\nm (s) t,a, update\u03a80 = \u03a80 \u222a {t}\nd. Else if w (s) t,a \u2264 2\u2212s for all a \u2208 As,\nAs+1 = {a \u2208 As,m(s)t,a \u2265 max j\u2208As m (s) t,j \u2212 2 \u00b7 2\u2212s},\ns \u2190 s+ 1 ."}, {"heading": "End For", "text": "normality result (5) and thus to prove our regret bound of Algorithm SupCB-GLM.\nTheorem 3. For any 0 < \u03b4 < 1, if we run the SupCB-GLM algorithm with \u03c4 = \u221a dT and \u03b1 = 3\u03c3\u03ba \u221a\n2 log(TK/\u03b4) for"}, {"heading": "T \u2265 T0 rounds, where", "text": "T0 = \u2126\n(\n\u03c32 \u03ba4 max\n{\nd3, log(TK/\u03b4)\nd\n})\n, (9)\nthe regret of the algorithm is bounded as\nRT \u2264 45(\u03c3L\u00b5/\u03ba) \u221a logT log(TK/\u03b4) log(T/d) \u221a dT ,\nwith probability at least 1\u2212 \u03b4. With \u03b4 = 1/T , we obtain\nE[RT ] = O ( (log T )1.5 \u221a dT logK ) .\nThe theorem demonstrates an O\u0303( \u221a dT logK) regret bound for the algorithm SupCB-GLM. It has been proved in\nChu et al. (2011, Theorem 2) that \u221a dT is the minimax lower bound of the expected regret forK-armed linear bandits, a special of the GLM bandits considered here. Therefore, the regret of our SupCB-GLM algorithm is optimal up\nto logarithm terms of T and K . To the best of our knowledge, this is the first algorithm which achieves the (near)optimal rate of GLM bandits.\nIt is worthwhile to compare Theorem 3 with the result in Theorem 2. When K = o(2d) is small, the rate of SupCBGLM is faster, and we improve the previous rates by a\u221a d factor. Here, we give a briefly illustration of how we\nget rid of the extra \u221a d factor. Both in Theorem 2 and in Filippi et al. (2010), |x\u2032(\u03b8\u0302n \u2212 \u03b8\u2217)| is upper bounded by using the Cauchy-Schwartz inequality,\n|x\u2032(\u03b8\u0302n \u2212 \u03b8\u2217)| \u2264 \u2016x\u2016V \u22121n \u2225 \u2225 \u2225 \u03b8\u0302n \u2212 \u03b8\u2217 \u2225 \u2225 \u2225\nVn . (10)\nLemma 3 in the supplementary material establishes that\n\u2225 \u2225 \u2225 \u03b8\u0302n \u2212 \u03b8\u2217 \u2225 \u2225 \u2225\nVn \u2264 C2\n\u221a\nd log(T/\u03b4).\nThis will lead to an extra \u221a d factor compared to (5). By using Cauchy-Schwartz (10), we only make use of the fact that \u03b8\u0302n is close to \u03b8 \u2217 in the \u21132 sense. However, (5) tells us that actually \u03b8\u0302n is close to \u03b8 \u2217 in every direction. This is\nthe reason why we are able to remove the extra \u221a d factor to achieve a near-optimal regret. It also explains why the bound in Theorem 2 is tight when K is large. As K goes large, it is likely there is a direction x for which (10) is tight.\nProof of Theorem 3. To facilitate our proof, we first present two technical lemmas. Lemma 4, Theorem 1, Theorem 5.39 of Vershynin (2012) together with a union bound yield Lemma 5. The proof of Lemma 6 is deferred to the appendix. Lemma 5. Fix \u03b4 > 0. Choose in SupCB-GLM \u03c4 = \u221a dT and \u03b1 = 3\u03c3\u03ba \u221a\n2 log(TK/\u03b4). Suppose T satisfies condition (9). Define the following event:\nEX := {|m(s)t,a\u2212x\u2032t,a\u03b8\u2217| \u2264 w(s)t,a , \u2200t \u2208 [\u03c4+1, T ], s \u2208 [S], a \u2208 [K]} . (11) Then, event EX holds with probability at least 1\u2212 \u03b4.\nProof of Lemma 5. By Lemma 4, we have independent samples now. Then to apply Theorem 1, the key is to lower bound the minimum eigenvalue of Vt. Note that we randomly select the feature vectors at the first \u03c4 = \u221a dT rounds, that is, they are independent. Moreover, the feature vectors are bounded. Thus, X1, X2, . . . , X\u03c4 are independent sub-Gaussian with parameter 1. By Proposition 1, we have\n\u03bbmin(Vt) \u2265 \u03bbmin(V\u03c4 ) \u2265 c \u221a dT\nfor some constant c with probability at least 1 \u2212 exp(\u2212 \u221a dT ). By Theorem 1 and union bound, we have the desired result under condition (9).\nLemma 6. Suppose that event EX holds, and that in round t, the action at is chosen at stage st. Then, a \u2217 t \u2208 As for all s \u2264 st. Furthermore, we have\n\u00b5(x\u2032t,a\u2217t \u03b8 \u2217)\u2212 \u00b5(x\u2032t,at\u03b8\u2217)\n\u2264 { (8L\u00b5)/2 st if at is selected in step 2b\n(2L\u00b5)/ \u221a T if at is selected in step 2c .\nDefine Vs,t = \u2211\nt\u2208\u03a8s(T ) XtX \u2032 t, then by Lemma 2,\n\u2211\nt\u2208\u03a8s(T )\nw (s) t,at =\n\u2211\nt\u2208\u03a8s(T )\n\u03b1(\u03b4)\u2016xt,at\u2016V \u22121s,t\n\u2264 \u03b1(\u03b4) \u221a 2d log(T/d)|\u03a8s(n)| .\nOn the other hand, by the step 2b of SupCB-GLM,\n\u2211\nt\u2208\u03a8s(T )\nw (s) t,at \u2265 2\u2212s|\u03a8s(T )|.\nCombining the above two inequalities gives us\n|\u03a8s(T )| \u2264 2s\u03b1(\u03b4) \u221a 2d log (T/d)|\u03a8s(T )|. (12)\nLet \u03a80 be the collection of trials such that at is chosen in step 2c. Since we have chose S = log2 T , each t \u2208 [\u03c4 + 1, T ]must be in one of \u03a8s and hence, {\u03c4, \u03c4 + 1, . . . , T } = \u03a80 \u222a ( \u222aSs=1\u03a8s(T ) ) . If we set \u03c4 = \u221a dT , we have\nRT =\n\u03c4 \u2211\nt=1\n(\n\u00b5(x\u2032t,a\u2217t \u03b8 \u2217)\u2212 \u00b5(x\u2032t,at\u03b8\u2217)\n)\n+\nT \u2211\nt=\u03c4+1\n(\n\u00b5(x\u2032t,a\u2217t \u03b8 \u2217)\u2212 \u00b5(x\u2032t,at\u03b8\u2217)\n)\n\u2264 \u03c4 + \u2211\nt\u2208\u03a80\n(\n\u00b5(x\u2032t,a\u2217t \u03b8 \u2217)\u2212 \u00b5(x\u2032t,at\u03b8\u2217)\n)\n+\nS \u2211\ns=1\n\u2211\nt\u2208\u03a8s(T )\n(\n\u00b5(x\u2032t,a\u2217t \u03b8 \u2217)\u2212 \u00b5(x\u2032t,at\u03b8\u2217)\n)\n\u2264 \u221a dT + T \u00b7 2L\u00b5\u221a\nT +\nS \u2211\ns=1\nL\u00b5 \u00b7 23\u2212s \u00b7 |\u03a8s(T )|\n\u2264 \u221a dT + 2L\u00b5 \u221a T + 8L\u00b5\u03b1(\u03b4) S \u2211\ns=1\n\u221a\n2d log T\nd |\u03a8s(T )|\n\u2264 \u221a dT + 2L\u00b5 \u221a T + 8L\u00b5\u03b1(\u03b4) \u221a 2d log(T/d) \u221a ST \u2264 45(\u03c3L\u00b5/\u03ba) \u221a logT log(TK/\u03b4) log(T/d) \u221a dT ,\nwith probability at least 1 \u2212 \u03b4. Here, the first inequality is due to the assumption that 0 \u2264 \u00b5 \u2264 1. The second inequality is Lemma 6. The third inequality is the inequality (12) and the fourth inequality is implied by Cauchy-Schwartz. This completes the proof of the high-probability result."}, {"heading": "5. Discussions", "text": "In this paper, we propose two algorithms forK-armed bandits with generalized linear models. While the first algorithm, UCB-GLM, achieves the optimal rate for the case of infinite number of actions, the second algorithm SupCBGLM is provable optimal for the case of finite number actions at each round. However, it remains open whether UCB-GLM can achieve the optimal rate for small K ."}, {"heading": "5.1. A better regret bound for UCB-GLM", "text": "A key quantity in determine the regret of algorithm UCBGLM are the minimum eigenvalue of Vt. If we make an addition assumption on the minimum eigenvalue of Vt, we will be able to prove an O( \u221a dT ) regret bound for UCB-"}, {"heading": "GLM.", "text": "Theorem 4. We run algorithm UCB-GLM with \u03c4 = 8\u03c32\n\u03ba2 d logT and \u03b1 \u2264 L\u00b5\u03c3/\u03ba. For any \u03b4 \u2208 [1/T, 1), suppose there is an universal constant c such that\nT \u2211\nt=\u03c4+1\n\u03bb \u22121/2 min (Vt) \u2264 c\n\u221a T . (13)\nholds with probability at least 1\u2212 \u03b4, and\nT = \u2126\n(\n\u03c3R\n\u03baL\u00b5 d log2 T\n)\n. (14)\nThen, the regret of the algorithm is bounded by\nRT \u2264 CL\u00b5\u03c3\n\u03ba\n\u221a\ndT log(T/\u03b4)\nwith probability at least 1 \u2212 2\u03b4, where C is an universal constant.\nThis theorem provides some insights of why UCB-GLM performs well in practice. Although the condition in (13) is hard to check and may be violated in some cases, for example, in K-armed bandits, we provide a heuristic argument to justify this assumption in a range of problems. When t is large enough, our estimator \u03b8\u0302t is very close to \u03b8 \u2217. If we assume there is a positive gap between \u3008xt,a\u2217t , \u03b8\u2217\u3009 and \u3008xt,a, \u03b8\u2217\u3009 for all a 6= a\u2217t , we will have at = a\u2217t after, for example, \u221a T steps. Since {xt,a, a \u2208 [K]} are independent for t \u2208 [T ], {xt,a\u2217t } are also independent samples. Then Vt/twill be well-approximated by the covariancematrix of xt,a\u2217t , which we denote by \u03a30. In many problem in practice, especially when features are dense, it is unlikely the feature vector xt,a\u2217t lies in a low-dimensional subspace of R d. It implies that \u03a30 has full rank, and that we will have \u03bbmin(Vt) = \u0398(t \u00b7 \u03bbmin(\u03a30)) when t is large enough. It follows that,\nT \u2211\nt=\u03c4+1\n1 \u221a\n\u03bbmin(Vt) =\nT \u2211\nt=\u03c4+1\n\u0398(t\u22121/2) = O( \u221a T ).\nIt should be cautioned that, since we do not know the distribution of our feature vectors, we cannot assume the above gap exists. It is therefore challenging to make the above arguments rigorous. In fact, when studying the ARIMA model in time series, Lai & Wei (1982, Example 1) provide an example such that \u03bbmin(Vt) = O(log t)."}, {"heading": "5.2. Open Questions", "text": "Computational efficient algorithms. While UCB-GLM and SupCB-GLM enjoy good theoretical properties, they can be expensive in some application domains. First, they require inverting a d \u00d7 d matrix in every step, a costly operation when d is large. Second, at step t, the MLE is computed using \u0398(t) samples, meaning that the per-step complexity grows at least linearly with t for a straightforward implementation of the algorithms. It is therefore interesting to investigate more scalable alternatives. It is possible to use a first-order, iterative optimization procedure to amortize the cost, analogous to the approach taken by Agarwal et al. (2014).\nK-dependent lower bound. Currently, all the lower bound results on (generalized) linear bandits have no dependence on the number of actionsK . The minimax lower bound will be of particularly interest because all current lower bound results assume that K \u2264 d. Although it will at most be a logarithm dependence on K , it is still a theoretically interesting question.\nRandomized algorithms with optimal regret rate. As opposed to the deterministic, UCB-style algorithms studied in this paper, randomized algorithms like EXP4 (Auer et al., 2002b) and Thompson Sampling (Thompson, 1933) have advantages in certain situations, for example, when reward observations are delayed (Chapelle & Li, 2012). Recently developed techniques for analyzing Bayes regret in BLM bandits (Russo & Van Roy, 2014) may be useful to analyze the cumulative regret considered here."}, {"heading": "A. Proof of Theorem 1", "text": "In the following, for simplicity, we will drop the subscript n when there is no ambiguity. Therefore, Vn is denoted V and so on.\nTo prove normality-type results of the maximum likelihood estimator \u03b8\u0302, typically we first show the n\u22121/2-consistency of \u03b8\u0302 to \u03b8\u2217. Then, by using a second-order Taylor expansion or Newton-step, we can prove the desired normality of \u03b8\u0302. More details can be found in standard textbooks such as Van der Vaart (2000).\nSince m is twice differentiable with m\u0308 \u2265 0, the maximum-likelihood estimation can be written as the solution to the following equation\nn \u2211\ni=1\n(Yi \u2212 \u00b5(X \u2032i\u03b8))Xi = 0 . (15)\nDefine G(\u03b8) := \u2211n i=1 (\u00b5(X \u2032 i\u03b8)\u2212 \u00b5(X \u2032i\u03b8\u2217))Xi, and we have\nG(\u03b8\u2217) = 0 and G(\u03b8\u0302) =\nn \u2211\ni=1\n\u01ebiXi , (16)\nwhere the noise \u01ebi is defined in (1). For convenience, define Z := G(\u03b8\u0302) = \u2211n i=1 \u01ebiXi.\nStep 1: Consistency of \u03b8\u0302. We first prove the consistency of \u03b8\u0302. For any \u03b81, \u03b82 \u2208 Rd, mean value theorem implies that there exists some \u03b8\u0304 = v\u03b81 + (1\u2212 v)\u03b82 with 0 < v < 1, such that\nG(\u03b81)\u2212G(\u03b82) = [ n \u2211\ni=1\n\u00b5\u0307(X \u2032i \u03b8\u0304)XiX \u2032 i\n]\n(\u03b81 \u2212 \u03b82) := F (\u03b8\u0304)(\u03b81 \u2212 \u03b82) (17)\nSince \u00b5\u0307 > 0 and \u03bbmin(V ) > 0, we have\n(\u03b81 \u2212 \u03b82)\u2032(G(\u03b81)\u2212G(\u03b82)) \u2265 (\u03b81 \u2212 \u03b82)\u2032(\u03baV )(\u03b81 \u2212 \u03b82) > 0\nfor any \u03b81 6= \u03b82. Hence, G(\u03b8) is an injection from Rd to Rd, and so G\u22121 is a well-defined function. Consequently, (15) has a unique solution \u03b8\u0302 = G\u22121(Z).\nLet us consider an \u03b7-neighborhood of \u03b8\u2217, B\u03b7 := {\u03b8 : \u2016\u03b8 \u2212 \u03b8\u2217\u2016 \u2264 \u03b7}, where \u03b7 > 0 is a constant that will be specified later. Note that B\u03b7 is a convex set, thus \u03b8\u0304 \u2208 B\u03b7 as long as \u03b81, \u03b82 \u2208 B\u03b7 . Define \u03ba\u03b7 := inf\u03b8\u2208B\u03b7 \u00b5\u0307(x\u2032\u03b8) > 0. From (17), for any \u03b8 \u2208 B\u03b7,\n\u2016G(\u03b8)\u20162V \u22121 = \u2016G(\u03b8) \u2212G(\u03b8\u2217)\u2016 2 V \u22121\n= (\u03b8 \u2212 \u03b8\u2217)\u2032F (\u03b8\u0304)V \u22121F (\u03b8\u0304)(\u03b8 \u2212 \u03b8\u2217) \u2265 \u03ba2\u03b7\u03bbmin(V ) \u2016\u03b8 \u2212 \u03b8\u2217\u20162 ,\nwhere the last inequality is due to the fact that F (\u03b8\u0304) \u03ba\u03b7V . On the other hand, Lemma A of Chen et al. (1999) implies that\n{\n\u03b8 : \u2016G(\u03b8)\u2016V \u22121 \u2264 \u03ba\u03b7\u03b7 \u221a \u03bbmin(V ) } \u2282 B\u03b7 .\nNow it remains to upper bound \u2016Z\u2016V \u22121 = \u2225 \u2225 \u2225 G(\u03b8\u0302) \u2225 \u2225 \u2225\nV \u22121 to ensure \u03b8\u0302 \u2208 B\u03b7 . To do so, we need the following technical\nlemma, whose proof is deferred to Section C.\nLemma 7. Recall \u03c3 which is the constant in (2). For any \u03b4 > 0, define the following event:\nEG := { \u2016Z\u2016V \u22121 \u2264 4\u03c3 \u221a d+ log(1/\u03b4) } .\nThen, EG holds with probability at least 1\u2212 \u03b4.\nSuppose EG holds for the rest of the proof. Then, \u03b7 \u2265 4\u03c3\u03ba\u03b7 \u221a d+log(1/\u03b4) \u03bbmin(V ) implies \u2225 \u2225 \u2225 \u03b8\u0302t \u2212 \u03b8 \u2225 \u2225 \u2225 \u2264 \u03b7. Since \u03ba = \u03ba1, we have \u03ba\u03b7 \u2265 \u03ba as long as \u03b7 \u2264 1. Thus, we have\n\u2225 \u2225 \u2225 \u03b8\u0302 \u2212 \u03b8 \u2225 \u2225 \u2225 \u2264 4\u03c3\n\u03ba\n\u221a\nd+ log(1/\u03b4)\n\u03bbmin(V ) \u2264 1 , (18)\nwhen \u03bbmin(V ) \u2265 16\u03c32 [d+ log(1/\u03b4)] /\u03ba2.\nStep 2: Normality of \u03b8\u0302. Now, we are ready to precede to prove the normality result. The following assumes EG holds (which is high-probability event, according to Lemma 7).\nDefine\u2206 := \u03b8\u0302 \u2212 \u03b8\u2217. It follows from (17) that there exists a v \u2208 [0, 1] such that\nZ = G(\u03b8\u0302)\u2212G(\u03b8\u2217) = (H + E)\u2206 ,\nwhere \u03b8\u0303 := v\u03b8\u2217 + (1\u2212 v)\u03b8\u0302, H := F (\u03b8\u2217) =\u2211ni=1 \u00b5\u0307(X \u2032i\u03b8\u2217)XiX \u2032i and E := F (\u03b8\u0303)\u2212 F (\u03b8\u2217). Intuitively, when \u03b8\u0302 and \u03b8\u2217 are close, elements in E are small. By the mean value theorem,\nE =\nn \u2211\ni=1\n( \u00b5\u0307(X \u2032i \u03b8\u0303)\u2212 \u00b5\u0307(X \u2032i\u03b8\u2217) ) XiX \u2032 i =\nn \u2211\ni=1\n\u00b5\u0308(ri)X \u2032 i\u2206XiX \u2032 i\nfor some ri \u2208 R. Since \u00b5\u0308 \u2264 M\u00b5 and v \u2208 [0, 1], for any x \u2208 Rd \\ {0}, we have\nx\u2032H\u22121/2EH\u22121/2x = (1 \u2212 v) n \u2211\ni=1\n\u00b5\u0308(ri)X \u2032 i\u2206 \u2225 \u2225 \u2225 x\u2032H\u22121/2Xi \u2225 \u2225 \u2225 2\n\u2264 n \u2211\ni=1\nM\u00b5 \u2016Xi\u2016 \u2016\u2206\u2016 \u2225 \u2225 \u2225 x\u2032H\u22121/2Xi \u2225 \u2225 \u2225 2\n\u2264 M\u00b5 \u2016\u2206\u2016 ( x\u2032H\u22121/2 ( n \u2211\ni=1\nXiX \u2032 i\n)\nH\u22121/2x\n)\n\u2264 M\u00b5 \u03ba \u2016\u2206\u2016 \u2016x\u20162 ,\nwhere we have used the assumption that \u2016Xi\u2016 \u2264 1 for the second inequality. Therefore,\n\u2225 \u2225 \u2225 H\u22121/2EH\u22121/2 \u2225 \u2225 \u2225 \u2264 M\u00b5 \u03ba \u2016\u2206\u2016 \u2264 4M\u00b5\u03c3 \u03ba2\n\u221a\nd+ log(1/\u03b4)\n\u03bbmin(V ) . (19)\nWhen \u03bbmin(V ) \u2265 64M2\u00b5\u03c32(d+ log(1/\u03b4))/\u03ba4, we have \u2225\n\u2225 \u2225 H\u22121/2EH\u22121/2\n\u2225 \u2225 \u2225 \u2264 1/2 . (20)\nNow we are ready to prove the theorem. For any x \u2208 Rd,\nx\u2032(\u03b8\u0302 \u2212 \u03b8\u2217) = x\u2032(H + E)\u22121Z = x\u2032H\u22121Z \u2212 x\u2032H\u22121E(H + E)\u22121Z . (21)\nNote that the matrix (H + E) is nonsingular, so its inversion exists.\nFor the first term, {\u01ebi} are sub-Gaussian random variables with sub-Gaussian parameter \u03c3. Define\nD := [X1, X2, . . . , Xn] \u2032 \u2208 Rn\u00d7d\nto be the design matrix. Hoeffding inequality gives\nP{|x\u2032H\u22121Z| \u2265 t} \u2264 2 exp { \u2212 t 2\n2\u03c32 \u2016x\u2032H\u22121D\u2032\u20162\n}\n. (22)\nSinceH \u03baV = \u03baD\u2032D, we have\n\u2225 \u2225x\u2032H\u22121D\u2032 \u2225 \u2225 2 = x\u2032H\u22121D\u2032DH\u22121x \u2264 1\n\u03ba2 x\u2032V \u22121x =\n1\n\u03ba2 \u2016x\u20162V \u22121 ,\nso (22) implies\nP{|x\u2032H\u22121Z| \u2265 t} \u2264 2 exp { \u2212 t 2\u03ba2\n2\u03c32 \u2016x\u20162V \u22121\n}\n.\nLet the right-hand side be 2\u03b4 and solve for t, we obtain that with probability at least 1\u2212 2\u03b4,\n|x\u2032H\u22121Z| \u2264 \u221a 2\u03c3\n\u03ba\n\u221a\nlog(1/\u03b4) \u2016x\u2016V \u22121 . (23)\nFor the second term,\n|x\u2032H\u22121E(H + E)\u22121Z| \u2264 \u2016x\u2016H\u22121 \u2225 \u2225 \u2225 H\u22121/2E(H + E)\u22121Z \u2225 \u2225 \u2225\n\u2264 \u2016x\u2016H\u22121 \u2225 \u2225 \u2225 H\u22121/2E(H + E)\u22121H1/2 \u2225 \u2225 \u2225 \u2016Z\u2016H\u22121\n\u2264 1 \u03ba \u2016x\u2016V \u22121\n\u2225 \u2225 \u2225 H\u22121/2E(H + E)\u22121H1/2 \u2225 \u2225\n\u2225 \u2016Z\u2016V \u22121 , (24)\nwhere the last inequality is due to the fact that H \u03baV . Since (H + E)\u22121 = H\u22121 \u2212H\u22121E(H + E)\u22121, we have \u2225\n\u2225 \u2225 H\u22121/2E(H + E)\u22121H1/2\n\u2225 \u2225 \u2225 = \u2225 \u2225 \u2225 H\u22121/2E ( H\u22121 \u2212H\u22121E(H + E)\u22121 ) H1/2 \u2225 \u2225 \u2225\n= \u2225 \u2225 \u2225 H\u22121/2EH\u22121/2 +H\u22121/2EH\u22121E(H + E)\u22121H1/2 \u2225 \u2225 \u2225\n\u2264 \u2225 \u2225 \u2225 H\u22121/2EH\u22121/2 \u2225 \u2225 \u2225 + \u2225 \u2225 \u2225 H\u22121/2EH\u22121/2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 H\u22121/2E(H + E)\u22121H1/2 \u2225 \u2225 \u2225 .\nBy solving this inequality, we get\n\u2225 \u2225 \u2225 H\u22121/2E(H + E)\u22121H1/2 \u2225 \u2225 \u2225 \u2264\n\u2225 \u2225H\u22121/2EH\u22121/2 \u2225 \u2225\n1\u2212 \u2225 \u2225H\u22121/2EH\u22121/2 \u2225 \u2225\n\u2264 2 \u2225 \u2225 \u2225 H\u22121/2EH\u22121/2 \u2225 \u2225 \u2225 \u2264 8M\u00b5\u03c3\n\u03ba2\n\u221a\nd+ log(1/\u03b4)\n\u03bbmin(V ) ,\nwhere we have used (20) and (19) in the second and third inequalities, respectively. Combining it with (24) and the bound in EG, we have\n|x\u2032H\u22121E(H + E)\u22121Z| \u2264 32M\u00b5\u03c3 2 \u03ba3 d+ log(1/\u03b4) \u221a \u03bbmin(V ) \u2016x\u2016V \u22121 . (25)\nFrom (21), (23) and (25), one can see that (5) holds as long as the lower bound (4) for \u03bbmin(V ) holds. Finally, an application of a union bound on two small-probability events (given in Lemma 7 and (23), respectively) asserts that (5) holds with probability at least 1\u2212 3\u03b4."}, {"heading": "B. Proof of Proposition 1", "text": "In the following, for simplicity, we will drop the subscript n when there is no ambiguity. Therefore, Vn is denoted V and so on.\nLet X be a random vector drawn from the distribution \u03bd. Define Z := \u03a3\u22121/2X . Then Z is isotropic, namely, E[ZZ \u2032] = Id. Define U = \u2211n t=1 ZtZ \u2032 t = \u03a3\n\u22121/2V\u03a3\u22121/2. From Lemma 1, we have that, for any t, with probability at least 1\u2212 2 exp(\u2212C2t2),\n\u03bbmin(U) \u2265 n\u2212 C1\u03c32 \u221a nd\u2212 \u03c32t\u221an .\nwhere \u03c3 is the sub-Gaussian parameter of Z , which is upper-bounded by \u2225 \u2225\u03a3\u22121/2 \u2225 \u2225 = \u03bb \u22121/2 min (\u03a3) (see, e.g., Vershynin (2012)). We thus can rewrite the above inequality (which holds with probability 1\u2212 \u03b4 as\n\u03bbmin(U) \u2265 n\u2212 \u03bb\u22121min(\u03a3) ( C1\u03c3 2 \u221a nd+ t \u221a n ) .\nWe now bound the minimum eigenvalue of V , as follows:\n\u03bbmin(V ) = min x\u2208Bd\nx\u2032V x\n= min x\u2208Bd\nx\u2032\u03a31/2U\u03a31/2x\n\u2265 \u03bbmin(U) min x\u2208Bd x\u2032\u03a3x\n= \u03bbmin(U)\u03bbmin(\u03a3) \u2265 \u03bbmin(\u03a3) ( n\u2212 \u03bb\u22121min(\u03a3)(C1\u03c32 \u221a nd+ t \u221a n) ) = \u03bbmin(\u03a3)n\u2212 C1 \u221a nd\u2212 C2 \u221a n log(1/\u03b4) .\nFinally, it can be verified (Lemma 9) that the last expression above is no less than B as long as\nn \u2265 (\nC1 \u221a d+ C2 \u221a log(1/\u03b4)\n\u03bbmin(\u03a3)\n)2\n+ 2B\n\u03bbmin(\u03a3) ,\nfinishing the proof."}, {"heading": "C. Technical Lemmas and Proofs", "text": ""}, {"heading": "C.1. Proof of Lemma 7", "text": "Noting that\n\u2016Z\u2016V \u22121 = \u2016V \u22121/2Z\u20162 = sup \u2016a\u20162\u22641 \u3008a, V \u22121/2Z\u3009,\nlet B\u0302 be a 1/2-net of the unit ball Bd. Then |B\u0302| \u2264 6d (Pollard, 1990, Lemma 4.1), and for any x \u2208 Bd, there is a x\u0302 \u2208 B\u0302 such that \u2016x\u2212 x\u0302\u2016 \u2264 1/2. Consequently,\n\u3008x, V \u22121/2Z\u3009 = \u3008x\u0302, V \u22121/2Z\u3009+ \u3008x\u2212 x\u0302, V \u22121/2Z\u3009\n= \u3008x\u0302, V \u22121/2Z\u3009+ \u2016x\u2212 x\u0302\u2016 \u3008 x\u2212 x\u0302\u2016x\u2212 x\u0302\u2016 , V \u22121/2Z\u3009\n\u2264 \u3008x\u0302, V \u22121/2Z\u3009+ 1 2 sup z\u2208Bd \u3008z, V \u22121/2Z\u3009.\nTaking supremum on both sides, we get\nsup x\u2208Bd \u3008x, V \u22121/2Z\u3009 \u2264 2max x\u0302\u2208B\u0302 \u3008x\u0302, V \u22121/2Z\u3009 .\nThen a union bound argument implies\nP {\u2016Z\u2016V \u22121 > t} \u2264 P {\nmax x\u0302\u2208B\u0302\n\u3008x\u0302, V \u22121/2Z\u3009 > t/2 }\n\u2264 \u2211\nx\u0302\u2208B\u0302\nP\n{ \u3008x\u0302, V \u22121/2Z\u3009 > t/2 }\n\u2264 \u2211\nx\u0302\u2208B\u0302\nexp\n{\n\u2212 t 2\n8\u03c32 \u2225 \u2225x\u0302\u2032V \u22121/2X \u2032 \u2225 \u2225 2\n}\n\u2264 exp { \u2212t2/(8\u03c32) + d log 6 } ,\nwhere we have used Hoeffding\u2019s inequality for the third inequality and |B\u0302| \u2264 6d for the last inequality. A choice of t = 4\u03c3 \u221a d+ log(1/\u03b4) completes the proof."}, {"heading": "C.2. Proof of Lemma 2", "text": "By Abbasi-Yadkori et al. (2011, Lemma 11), we have\nm+n \u2211\nt=m+1\n\u2016Xt\u20162V \u22121t \u2264 2 log detVm+n+1 detVm+1 \u2264 2d log ( tr (Vm+1) + n d ) \u2212 2 log detVm+1 .\nNote that tr (Vm+1) = \u2211m t=1 tr (XtX \u2032 t) = \u2211m t=1 \u2016Xt\u20162 \u2264 m and that det Vm+1 = \u220fd i=1 \u03bbi \u2265 \u03bbdmin(Vm+1) \u2265 1, where {\u03bbi} are the eigenvalues of Vm+1. Applying Cauchy-Schwartz inequality yields m+n \u2211\nt=m+1\n\u2016Xt\u2016V \u22121t \u2264\n\u221a \u221a \u221a \u221an m+n \u2211\nt=m+1\n\u2016Xt\u20162V \u22121t \u2264 \u221a 2nd log ( n+m\nd\n)\n."}, {"heading": "C.3. Proof of Lemma 3", "text": "Define Gt(\u03b8) = \u2211t\u22121 i=1(\u00b5(X \u2032 i\u03b8) \u2212 \u00b5(X \u2032i\u03b8\u2217))Xi and Zt = \u2211t\u22121 i=1 \u01ebiXi. Following the same argument as in the proof of Theorem 1, we have Gt(\u03b8\u0302t) = Zt and \u2016Gt(\u03b8)\u20162V \u22121t \u2265 \u03ba 2\u2016\u03b8 \u2212 \u03b8\u2217\u20162Vt (26) for any \u03b8 \u2208 {\u03b8 : \u2016\u03b8 \u2212 \u03b8\u2217\u2016 \u2264 1}. Combining (26) with the following lemma and the equality Zt = Gt(\u03b8\u0302t) completes the proof. Lemma 8. Suppose there is an integerm such that \u03bbmin(Vm) \u2265 1, then for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, for all t > m,\n\u2016Zt\u20162V \u22121t \u2264 4\u03c3 2\n(\nd 2 log(1 + 2t/d) + log(1/\u03b4)\n)\n.\nProof. For convenience, fix t such that t > m, and denote Vt and Zt by V and Z , respectively. Furthermore, define V\u0304 := V + \u03bbI and let 1 be the vector of all 1s. It is easy to observe that\n\u2016Z\u20162V \u22121 = \u2016Z\u2016 2 V\u0304 \u22121 + Z \u2032(V \u22121 \u2212 V\u0304 \u22121)Z . (27) We start with bounding the second term. The ShermanMorrison formula gives\nV\u0304 \u22121 = V \u22121 \u2212 \u03bbV \u22122\n1 + \u03bb1\u2032V \u221211 .\nSince 1\u2032V \u221211 \u2265 0, the above implies that 0 \u2264 Z \u2032(V \u22121 \u2212 V\u0304 \u22121)Z\n\u2264 \u03bbZ \u2032V \u22122Z \u2264 \u03bb \u2225 \u2225V \u22121 \u2225\n\u2225 \u2016Z\u20162V \u22121\n= \u03bb\n\u03bbmin(V ) \u2016Z\u20162V \u22121 .\nSince \u03bbmin(V ) \u2265 \u03bbmin(Vm) \u2265 1, we now have\n0 \u2264 Z \u2032(V \u22121 \u2212 V\u0304 \u22121)Z \u2264 \u03bb \u2016Z\u20162V \u22121 .\nThe above inequality together with (27) implies that\n\u2016Z\u20162V \u22121 \u2264 (1\u2212 \u03bb)\u22121 \u2016Z\u2016 2 V\u0304 \u22121 .\nThe proof can be finished by applying Theorem 1 and Lemma 10 from Abbasi-Yadkori et al. (2011) to bound \u2016Z\u20162V\u0304 \u22121 , using \u03bb = 1/2."}, {"heading": "C.4. Proof of Lemma 6", "text": "We will prove the first part of the lemma by induction. It is easy to check the lemma holds for s = 1. Suppose we have a\u2217t \u2208 As and we want to prove a\u2217t \u2208 As+1. Since the algorithm proceeds to stage s+ 1, we know from step 2b that\n|m(s)t,a \u2212 x\u2032t,a\u03b8\u2217| \u2264 w(s)t,a \u2264 2\u2212s\nfor all a \u2208 As. Specially, it holds for a = a\u2217t because a\u2217t \u2208 As by our induction step. Then the optimality of a\u2217t implies\nm (s) t,a\u2217t \u2265 x\u2032t,a\u2217t \u03b8 \u2217 \u2212 2\u2212s \u2265 x\u2032t,a\u03b8\u2217 \u2212 2\u2212s \u2265 m(s)t,a \u2212 2 \u00b7 2\u2212s\nfor all a \u2208 As. Thus we have a\u2217t \u2208 As+1 according to step 2d. Suppose at is selected at stage st in step 2b. If st = 1, obviously the lemma holds because 0 \u2264 \u00b5(x) \u2264 1 for all x. If st > 1, since we have proved a \u2217 t \u2208 Ast , again step 2b at stage st \u2212 1 implies\n|m(st\u22121)t,a \u2212 x\u2032t,a\u03b8\u2217| \u2264 2\u2212st+1\nfor a = at and a = a \u2217 t . Step 2d at stage st \u2212 1 implies\nm (st\u22121) t,a\u2217t \u2212m(st\u22121)t,at \u2264 2 \u00b7 2\u2212st+1 .\nCombining above two inequalities, we get\nx\u2032t,at\u03b8 \u2217 \u2265 m(st\u22121)t,at \u2212 2\u2212st+1 \u2265 m (st\u22121) t,a\u2217t \u2212 3 \u00b7 2\u2212st+1 \u2265 x\u2032t,a\u2217t \u03b8 \u2217 \u2212 4 \u00b7 2\u2212st+1 .\nWhen at is selected in step 2c, sincem (st) t,at \u2265 m (st) t,a\u2217t , we have\nx\u2032t,at\u03b8 \u2217 \u2265 m(st)t,at \u2212 1/ \u221a T \u2265 m(st)t,a\u2217t \u2212 1/ \u221a T \u2265 x\u2032t,a\u2217t \u03b8 \u2217 \u2212 2/ \u221a T .\nUsing the fact that \u00b5(x1)\u2212 \u00b5(x2) \u2264 L\u00b5(x1 \u2212 x2) for x1 \u2265 x2, we will get the desired result."}, {"heading": "C.5. Proof of Lemma 9", "text": "Lemma 9. Let a and b be two positive constants. Ifm \u2265 a2 + 2b, thenm\u2212 a\u221am\u2212 b \u2265 0.\nProof. Supposem \u2265 a2 + 2b, then\nm\u2212 a\u221am\u2212 b = a2 + b\u2212 a \u221a a2 + 2b\n\u2265 a2 + b\u2212 a \u221a a2 + 2b+ b2/a2 = a2 + b\u2212 a \u221a (a+ b/a)2 = a2 + b\u2212 a(a+ b/a) = 0 ."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Abbasi-Yadkori", "Yasin", "P\u00e1l", "D\u00e1vid", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Agarwal", "Alekh", "Hsu", "Daniel", "Kale", "Satyen", "Langford", "John", "Li", "Lihong", "Schapire", "Robert E"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML),", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Online models for content optimization", "author": ["Agarwal", "Deepak", "Chen", "Bee-Chung", "Elango", "Pradheep", "Motgi", "Nitin", "Park", "Seung-Taek", "Ramakrishnan", "Raghu", "Roy", "Scott", "Zachariah", "Joe"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Agarwal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2009}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Auer", "Peter"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Auer and Peter.,? \\Q2003\\E", "shortCiteRegEx": "Auer and Peter.", "year": 2003}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Fischer", "Paul"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Beygelzimer", "Alina", "Langford", "John", "Li", "Lihong", "Reyzin", "Lev", "Schapire", "Robert E"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Beygelzimer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2010}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["Bickel", "Peter J", "Ritov", "Ya\u2019acov", "Tsybakov", "Alexandre B"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory, volume 9 of Lecture Notes-Monograph Series", "author": ["Brown", "Lawrence D"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "Brown and D.,? \\Q1986\\E", "shortCiteRegEx": "Brown and D.", "year": 1986}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["Bubeck", "S\u00e9bastien", "Cesa-Bianchi", "Nicolo"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "An empirical evaluation of Thompson sampling", "author": ["Chapelle", "Olivier", "Li", "Lihong"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Chapelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2012}, {"title": "Strong consistency of maximum quasi-likelihood estimators in generalized linear models with fixed and adaptive designs", "author": ["Chen", "Kani", "Hu", "Inchi", "Ying", "Zhiliang"], "venue": "The Annals of Statistics,", "citeRegEx": "Chen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1999}, {"title": "Contextual bandits with linear payoff functions", "author": ["Chu", "Wei", "Li", "Lihong", "Reyzin", "Lev", "Schapire", "Robert E"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Chu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2011}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Dani", "Varsha", "Hayes", "Thomas P", "Kakade", "ShamM"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Consistency and asymptotic normality of the maximum likelihood estimator in generalized linear models", "author": ["Fahrmeir", "Ludwig", "Kaufmann", "Heinz"], "venue": "The Annals of Statistics,", "citeRegEx": "Fahrmeir et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Fahrmeir et al\\.", "year": 1985}, {"title": "Parametric bandits: The generalized linear case", "author": ["Filippi", "Sarah", "Cappe", "Olivier", "Garivier", "Aur\u00e9lien", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Lai", "Tze Leung", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "Least squares estimates in stochastic regression models with applications to identification and control of dynamic systems", "author": ["Lai", "Tze Leung", "Wei", "Ching Zong"], "venue": "The Annals of Statistics,", "citeRegEx": "Lai et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1982}, {"title": "Theory of Point Estimation, volume 31 of Springer Texts in Statistics", "author": ["Lehmann", "Erich Leo", "Casella", "George"], "venue": "Springer Science & Business Media,", "citeRegEx": "Lehmann et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 1998}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Li", "Lihong", "Chu", "Wei", "Langford", "John", "Schapire", "Robert E"], "venue": "In Proceedings of the 19th International Conference on World Wide Web (WWW),", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "An unbiased offline evaluation of contextual bandit algorithms with generalized linear models", "author": ["Li", "Lihong", "Chu", "Wei", "Langford", "John", "Moon", "Taesup", "Wang", "Xuanhui"], "venue": "JMLR Workshop and Conference Proceedings,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Generalized Linear Models, volume 37", "author": ["McCullagh", "Peter", "Nelder", "John A"], "venue": "CRC press,", "citeRegEx": "McCullagh et al\\.,? \\Q1989\\E", "shortCiteRegEx": "McCullagh et al\\.", "year": 1989}, {"title": "Empirical processes: Theory and applications. In NSF-CBMS regional conference series in probability and statistics, pp. i\u201386", "author": ["Pollard", "David"], "venue": "JSTOR,", "citeRegEx": "Pollard and David.,? \\Q1990\\E", "shortCiteRegEx": "Pollard and David.", "year": 1990}, {"title": "Linearly parameterized bandits", "author": ["Rusmevichientong", "Paat", "Tsitsiklis", "John N"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Rusmevichientong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong et al\\.", "year": 2010}, {"title": "Learning to optimize via posterior sampling", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2014}, {"title": "One-armed bandit problems with covariates", "author": ["Sarkar", "Jyotirmoy"], "venue": "The Annals of Statistics,", "citeRegEx": "Sarkar and Jyotirmoy.,? \\Q1991\\E", "shortCiteRegEx": "Sarkar and Jyotirmoy.", "year": 1991}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["Thompson", "William R"], "venue": null, "citeRegEx": "Thompson and R.,? \\Q1933\\E", "shortCiteRegEx": "Thompson and R.", "year": 1933}, {"title": "Spectral bandits for smooth graph functions", "author": ["Valko", "Michal", "Munos", "R\u00e9mi", "Kveton", "Branislav", "Koc\u00e1k", "Tom\u00e1\u0161"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML),", "citeRegEx": "Valko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Valko et al\\.", "year": 2014}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Vershynin", "Roman"], "venue": "Compressed Sensing: Theory and Applications,", "citeRegEx": "Vershynin and Roman.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2012}, {"title": "A one-armed bandit problem with a concomitant variable", "author": ["Woodroofe", "Michael"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Woodroofe and Michael.,? \\Q1979\\E", "shortCiteRegEx": "Woodroofe and Michael.", "year": 1979}, {"title": "\u03b8\u2217\u2016 , where the last inequality is due to the fact that F (\u03b8\u0304", "author": ["\u2265 \u03ba\u03b7\u03bbmin(V ) \u2016\u03b8"], "venue": "\u03ba\u03b7V . On the other hand, Lemma A of Chen et al", "citeRegEx": "\u2212,? \\Q1999\\E", "shortCiteRegEx": "\u2212", "year": 1999}, {"title": "Z\u3009, let B\u0302 be a 1/2-net of the unit ball B. Then |B\u0302| \u2264 6 (Pollard, 1990, Lemma 4.1), and for any x \u2208 B, there is a x\u0302 \u2208 B\u0302 such that \u2016x\u2212 x\u0302", "author": ["V \u3008a"], "venue": null, "citeRegEx": ".a,? \\Q1990\\E", "shortCiteRegEx": ".a", "year": 1990}], "referenceMentions": [{"referenceID": 2, "context": "With the development of modern technologies, contextual bandit problems have more applications, especially in web-based recommendation, advertising and search (Agarwal et al., 2009; Li et al., 2010; 2012).", "startOffset": 159, "endOffset": 204}, {"referenceID": 19, "context": "With the development of modern technologies, contextual bandit problems have more applications, especially in web-based recommendation, advertising and search (Agarwal et al., 2009; Li et al., 2010; 2012).", "startOffset": 159, "endOffset": 204}, {"referenceID": 13, "context": "The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector.", "startOffset": 76, "endOffset": 191}, {"referenceID": 12, "context": "The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector.", "startOffset": 76, "endOffset": 191}, {"referenceID": 0, "context": "The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector.", "startOffset": 76, "endOffset": 191}, {"referenceID": 20, "context": "Logistic regression model based algorithms have been shown to have substantial improvements over linear models (Li et al., 2012).", "startOffset": 111, "endOffset": 128}, {"referenceID": 0, "context": ", 2002a; Bubeck & Cesa-Bianchi, 2012) to linear bandits (Auer, 2003; Abbasi-Yadkori et al., 2011).", "startOffset": 56, "endOffset": 97}, {"referenceID": 20, "context": "While some UCB-type algorithms using GLMs perform well empirically (Li et al., 2012), there is little theoretical study of them.", "startOffset": 67, "endOffset": 84}, {"referenceID": 0, "context": ", 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector. The linear model is theoretically convenient to work on. However, in practice, we usually have binary rewards (click or not, treatment working or not). Logistic regression model based algorithms have been shown to have substantial improvements over linear models (Li et al., 2012). Hence, we consider generalized linear models (GLM) in the contextual bandit setting, in which linear, logistic and probit regression serve as three important special cases. The celebrated work of Lai & Robbins (1985) first introduces the upper confidence bound (UCB) approach to efficient exploration.", "startOffset": 8, "endOffset": 640}, {"referenceID": 6, "context": "Later variants of the EXP4 algorithm (Beygelzimer et al., 2010; Agarwal et al., 2014) give an \u00d5( \u221a dKT ) regret that is near-optimal with respect to T .", "startOffset": 37, "endOffset": 85}, {"referenceID": 1, "context": "Later variants of the EXP4 algorithm (Beygelzimer et al., 2010; Agarwal et al., 2014) give an \u00d5( \u221a dKT ) regret that is near-optimal with respect to T .", "startOffset": 37, "endOffset": 85}, {"referenceID": 10, "context": "This rate improves the state-of-the-art results of Filippi et al. (2010) by a \u221a d factor, assuming the number of arms is fixed.", "startOffset": 51, "endOffset": 73}, {"referenceID": 10, "context": "This rate improves the state-of-the-art results of Filippi et al. (2010) by a \u221a d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem\u2019s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits.", "startOffset": 51, "endOffset": 313}, {"referenceID": 10, "context": "This rate improves the state-of-the-art results of Filippi et al. (2010) by a \u221a d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem\u2019s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits. A key observation in proving this result is that the l2 confidence ball of the unknown parameter is insufficient to calculate a sharp upper confidence bound, yet what we need is the confidence interval in all directions. Thus, we prove a finite sample normality type confidence bound for the maximum likelihood estimator of GLM. To our best knowledge, this is the first non-asymptotic normality type result for the GLM and might be of its own theoretical value. We also analyze a simple version of UCB algorithm called UCB-GLM that is widely used in practice. We prove it also achieves the optimal regret bound under a reasonable assumption. These results shed light on explaining the good empirical performance of GLM bandits in practice. Related Work The study of GLM bandits problem goes back at least to Sarkar (1991), who considered discounted regrets rather than cumulative regerts.", "startOffset": 51, "endOffset": 1229}, {"referenceID": 10, "context": "This rate improves the state-of-the-art results of Filippi et al. (2010) by a \u221a d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem\u2019s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits. A key observation in proving this result is that the l2 confidence ball of the unknown parameter is insufficient to calculate a sharp upper confidence bound, yet what we need is the confidence interval in all directions. Thus, we prove a finite sample normality type confidence bound for the maximum likelihood estimator of GLM. To our best knowledge, this is the first non-asymptotic normality type result for the GLM and might be of its own theoretical value. We also analyze a simple version of UCB algorithm called UCB-GLM that is widely used in practice. We prove it also achieves the optimal regret bound under a reasonable assumption. These results shed light on explaining the good empirical performance of GLM bandits in practice. Related Work The study of GLM bandits problem goes back at least to Sarkar (1991), who considered discounted regrets rather than cumulative regerts. They prove that a myopic rule without exploration is asymptotically optimal. Recently, Filippi et al. (2010) study the same stochastic GLM bandit problem considered here.", "startOffset": 51, "endOffset": 1405}, {"referenceID": 15, "context": "Note that this assumption is weaker than Assumption 1 in Filippi et al. (2010), as it only requires to control the local behavior of \u03bc\u0307(x\u03b8) near \u03b8.", "startOffset": 57, "endOffset": 79}, {"referenceID": 11, "context": ", 2009) and generalized linear models (Fahrmeir & Kaufmann, 1985; Chen et al., 1999).", "startOffset": 38, "endOffset": 84}, {"referenceID": 30, "context": "We give a proof sketch here, and the full proof is found in the appendix. In the following, for simplicity, we will drop the subscript n when there is no ambiguity. Therefore,Vn is denotedV and so on. We will need a technical lemma, which is an existing result in randommatrix theory. The version we presented here is adapted from Equation (5.23) of Theorem 5.39 from Vershynin (2012). Lemma 1.", "startOffset": 7, "endOffset": 385}, {"referenceID": 30, "context": "\u2225 = \u03bb \u22121/2 min (\u03a3) (see, e.g., Vershynin (2012)).", "startOffset": 6, "endOffset": 48}, {"referenceID": 0, "context": ", 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008).", "startOffset": 28, "endOffset": 106}, {"referenceID": 12, "context": ", 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008).", "startOffset": 28, "endOffset": 106}, {"referenceID": 13, "context": ", 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008).", "startOffset": 28, "endOffset": 106}, {"referenceID": 14, "context": ", the distribution \u03bd) is only needed to ensure V\u03c4+1 is invertable (similar to the first phase in the algorithm of Filippi et al. (2010)); the rest of our analysis does not depend on this stochastic assumption.", "startOffset": 114, "endOffset": 136}, {"referenceID": 0, "context": ", Abbasi-Yadkori et al. (2011)).", "startOffset": 2, "endOffset": 31}, {"referenceID": 0, "context": "We instead use results on self-normalized martingales (Abbasi-Yadkori et al., 2011), together with a finitetime normality result like Theorem 1, to prove the next theorem.", "startOffset": 54, "endOffset": 83}, {"referenceID": 13, "context": "Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008).", "startOffset": 125, "endOffset": 144}, {"referenceID": 13, "context": "Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008). By choosing \u03b4 = 1/T and using the fact that RT \u2264 T , this highprobability result implies a bound on the expected regret: E[RT ] = \u00d5(d \u221a T ). Our result improves the previous regret bound of Filippi et al. (2010) by a \u221a logT factor.", "startOffset": 126, "endOffset": 358}, {"referenceID": 13, "context": "Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008). By choosing \u03b4 = 1/T and using the fact that RT \u2264 T , this highprobability result implies a bound on the expected regret: E[RT ] = \u00d5(d \u221a T ). Our result improves the previous regret bound of Filippi et al. (2010) by a \u221a logT factor. Moreover, the algorithm proposed in Filippi et al. (2010) involves a projection step, which is computationally more expensive comparing to UCB-GLM.", "startOffset": 126, "endOffset": 436}, {"referenceID": 20, "context": "Algorithm SupCB-GLM While the algorithm UCB-GLM performs sufficiently well in practice (Li et al., 2012), it is unclear whether it can achieve the optimal rates of O( \u221a dT logK), when K is small.", "startOffset": 87, "endOffset": 104}, {"referenceID": 19, "context": "Algorithm SupCB-GLM While the algorithm UCB-GLM performs sufficiently well in practice (Li et al., 2012), it is unclear whether it can achieve the optimal rates of O( \u221a dT logK), when K is small. As mentioned in Section 4.1, the key technical difficulty in analyzing UCB-GLM is the dependence between samples. Inspired by a technique developed by Auer (2003) to create independent samples for linear contextual bandits, we propose another algorithm SupCB-GLM (Algorithm 3), which uses algorithm CB-GLM (Algorithm 2) as a subroutine.", "startOffset": 88, "endOffset": 359}, {"referenceID": 27, "context": "Also, we would like to point out that, unlike SpectralEliminator (Valko et al., 2014), the algorithm can easily handle a changing action set.", "startOffset": 65, "endOffset": 85}, {"referenceID": 29, "context": "If w (s) t,a > 2 \u2212s for some a, we need to do more exploration on xt,a and thus we choose this action. Otherwise, the actions are filtered in step 2d such that the actions passed to the next stage are close enough to the optimal action. Since all the widths are smaller than 2, if m (s) t,a < m (s) t,j \u2212 2 \u00b7 2 for some j \u2208 As, the action a can not be the optimal action. The filter process terminates when we have already got accurate estimate of all xt,a\u03b8 \u2217 up to the 1/ \u221a T level and we do not need to do exploration. Thus in step 2c we just choose the action that maximizes the estimated mean value. Our algorithm is different from the algorithm SupLinRel in Auer (2003) that we directly maximize the mean, rather than the upper confidence bound, in steps c and d.", "startOffset": 17, "endOffset": 675}, {"referenceID": 15, "context": "Both in Theorem 2 and in Filippi et al. (2010), |x(\u03b8\u0302n \u2212 \u03b8\u2217)| is upper bounded by using the Cauchy-Schwartz inequality, |x(\u03b8\u0302n \u2212 \u03b8)| \u2264 \u2016x\u2016V \u22121 n \u2225", "startOffset": 25, "endOffset": 47}, {"referenceID": 31, "context": "This will lead to an extra \u221a d factor compared to (5). By using Cauchy-Schwartz (10), we only make use of the fact that \u03b8\u0302n is close to \u03b8 \u2217 in the l2 sense. However, (5) tells us that actually \u03b8\u0302n is close to \u03b8 \u2217 in every direction. This is the reason why we are able to remove the extra \u221a d factor to achieve a near-optimal regret. It also explains why the bound in Theorem 2 is tight when K is large. As K goes large, it is likely there is a direction x for which (10) is tight. Proof of Theorem 3. To facilitate our proof, we first present two technical lemmas. Lemma 4, Theorem 1, Theorem 5.39 of Vershynin (2012) together with a union bound yield Lemma 5.", "startOffset": 11, "endOffset": 618}], "year": 2017, "abstractText": "Contextual bandits are widely used in Internet services from news recommendation to advertising. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an \u00d5( \u221a dT ) regret over T rounds with d dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a \u221a d factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximumlikelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for the certain cases.", "creator": "LaTeX with hyperref package"}}}