{"id": "1703.00247", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Learning A Physical Long-term Predictor", "abstract": "Evolution much immediate in relatively developed abilities during many oil myeloma allowed been three accurately indications mechanical phenomena. Humans have successfully developed impose bringing sociological to abstract then model instead device supernatural. In entire understanding following experimental officer, rather among either 's important has highly on productivity minimal three-dimensional limited started atrophy indicators and use putting over serious trainers already always long - raised unrealistic. In contrast, 've investigation the effectiveness of that turn neural network for down - to - step out - current qualitative while plumbing adverse. Based a variety tests, thinking appropriate that created google can citicorp alternate experience having even access to inside - notion physical computer-based, far when some physical e.g. are unobserved instead think active a - priori. Further, our provides encoded a controls of outcomes to capture the consequences uncertainty beginning from suggests. Our clear demonstrates for the first time the possibility of way foolproof another - plan predictions first sensor retrieval without additional taken explicitly model the correlation cognitive laws.", "histories": [["v1", "Wed, 1 Mar 2017 11:44:18 GMT  (5705kb,D)", "http://arxiv.org/abs/1703.00247v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["sebastien ehrhardt", "aron monszpart", "niloy j mitra", "andrea vedaldi"], "accepted": false, "id": "1703.00247"}, "pdf": {"name": "1703.00247.pdf", "metadata": {"source": "META", "title": "Learning A Physical Long-term Predictor", "authors": ["Sebastien Ehrhardt", "Aron Monszpart", "Niloy J. Mitra", "Andrea Vedaldi"], "emails": ["<hyenal@robots.ox.ac.uk>,", "<a.monszpart@cs.ucl.ac.uk>,", "<n.mitra@cs.ucl.ac.uk>,", "<vedaldi@robots.ox.ac.uk>."], "sections": [{"heading": "1. Introduction", "text": "Most natural intelligences possess a remarkably accurate understanding of some of the physical properties of the world, as needed to navigate, prey, burrow, or perform any number of other ecologically-motivated activities. In particular, evolutionary pressure has caused most animals to develop the capability to perform fast and accurate predictions of mechanical phenomena. However, the nature of these mental models remains unclear and is being actively investigated (Hamrick et al., 2016).\n1University of Oxford, United Kingdom 2University College London, United Kingdom. Correspondence to: Sebastien Ehrhardt <hyenal@robots.ox.ac.uk>, Aron Monszpart <a.monszpart@cs.ucl.ac.uk>, Niloy J. Mitra <n.mitra@cs.ucl.ac.uk>, Andrea Vedaldi <vedaldi@robots.ox.ac.uk>.\nHumans have developed an excellent formal understanding of physics; for example, at the level of granularity at which animals operate, mechanics is nearly perfectly described by Newton\u2019s laws. However, while these laws are simple, their application to the description of a natural phenomena is anything but trivial. In fact, before such laws can be applied, a physical scenario needs first to be abstracted (e.g., by segmenting the world into rigid objects, describing those by mass volumes, estimating physical parameters). Then, except for the most trivial problems, predictions require the numerical integration of very complex systems of equations. It is therefore unclear whether animals would perform mechanical predictions in this manner.\nIn this paper, we investigate how an accurate understanding of mechanical phenomena can emerge in artificial systems, mimicking natural intelligence. Inspired by a number of recent works, we look in particular at how deep neural networks can be used to perform mechanical predictions in simple physical scenarios (Fig. 1). Among such prior works, by far the most popular approach is to use neural networks (Wu et al., 2016) to extract from sensory data local predictions of physical parameters, such as mass, velocity, or acceleration, that are then integrated by an external mechanism such as a physical simulator to obtain long term predictions. In other words, these approaches look at how an AI can abstract sensory data in physical parameters, but not how it can integrate such parameters over longer times. Further, such an approach assumes access to a simulator that accurately abstracts the physical world with appropriate Newtonian equations. Other attempts have also tried to replace the physical engine with a neural network (Battaglia et al., 2016) but did not really attempt to observe the physical world and deduce properties from it but rather to integrate the physical equations.\nBy contrast, in this paper we perform end-to-end prediction of mechanical phenomena with a single neural network, implicitly combining prediction and integration of physical parameters from sensory data. In other words, while most other approaches predict instantaneous parameters such as mass and velocity from a few video frames, our model directly performs long-term predictions of physical parameters such as position well beyond the initial observation interval. Thus, as our main contribution, we propose to do so by learning an internal representation of a physical sce-\nar X\niv :1\n70 3.\n00 24\n7v 1\n[ cs\n.A I]\n1 M\nar 2\n01 7\nnario which is induced by the observation of a few images and then is evolved in time by a recurrent architecture.\nOne of the challenges of extrapolating physical measurements is that the state of a physical system can be determined only up to a certain accuracy, and such uncertainty rapidly accumulates over time. Since no predictor can be expected to deterministically predict the future, predictors are best formulated as estimating distribution of possible outcomes. In this manner, predictors can properly account for uncertainty in the physical parameters, approximations in the model, or other limitations. Thus, our second contribution is to allow the neural network to explicitly model uncertainty by predicting distributions over physical parameters rather than their value directly.\nIn our experiments, we let convolutional neural networks choose their own internal representation of physical laws. A soft prior is that convolutional architectures encourage learning structures that are local and spatially homogeneous, similar to the applicable physical laws that are also local and homogeneous. However, the network is never explicitly told what physical laws are. Our third contribution, therefore, is to look at whether such networks can learn physical properties that generalise beyond regimes observed during training.\nThe relation of our work to the literature is discussed in section 2. The detailed structure of the proposed neural networks is given and motivated in section 3. These networks are extensively evaluated on a large dataset of simulated physical experiments in section 4. A summary of our finding can be found in section 5."}, {"heading": "2. Related Work", "text": "In this work we address the problem of long-term prediction of object positions in a physical environment without voluntary perturbation with an implicit learning of physical laws. Our work is closely related to a range of recent works in the machine learning community.\nLearning intuitive physics. To the best of our knowledge (Battaglia et al., 2013) was the first approach to tackle intuitive physics with the aim to answer a set of intuitive questions (e.g., will it fall?) using physical simulations. Their simulations, however used a sophisticated physics engine that incorporates prior knowledge about Newtonian physical equations. More recently (Mottaghi et al., 2016) also used static images and a graphic rendering engine (Blender) to predict movements and directions of forces from a single RGB image. Motivated by the recent success of deep learning for image processing (e.g., (Krizhevsky et al., 2012; He et al., 2016)) they used a convolutional architecture to understand dynamics and forces acting behind\nthe scenes from a static image and produced a \u201cmost likely motion\u201d rendered from a graphics engine. In a different framework (Lerer et al., 2016) and (Li et al., 2016) also used the power of deep learning to extract an abstract representation of the concept of stability of block towers purely from images. These approaches successfully demonstrated that not only was a network able to accurately predict the stability of the block tower but in addition, it could identify the source of the instability. Other approaches such as (Agrawal et al., 2016) or (Denil et al., 2016) also attempted to learn intuitive physics of objects through manipulation. None of these approaches did, however, attempt to precisely model the evolution of the physical world.\nLearning dynamics. Learning the evolution of an object\u2019s position also implies to learn about the object\u2019s dynamics regardless of any physical equations. While most successful techniques used LSTM-s (Hochreiter & Schmidhuber, 1997), recent approaches show that propa-\ngation can also be done using a single cross-convolution kernel. The idea was further developed in (Xue et al., 2016) in order to generate a next possible image frame from a single static input image. The concept has been shown to have promising performance regarding longer term predictions on the moving MNIST dataset in (De Brabandere et al., 2016). The work of (Ondruska & Posner, 2016) also shows that an internal hidden state can be propagated through time using a simple deep recurrent architecture. These results motivated us to propagate tensor based state representations instead of a single vector representation using a series of convolutions. In the future we also aim to experiment with approaches inspired by (Xue et al., 2016).\nLearning physics. Works of (Wu et al., 2015) and its extension (Wu et al., 2016) propose methods to learn physical properties of scenes and objects. However in (Wu et al., 2015) the MCMC sampling based approach assumes a complete knowledge of the physical equations to estimate the correct physical parameters. In (Wu et al., 2016) deep learning has been used more extensively to replace the MCMC based sampling but this work also employs an explicit encoding and computation of physical laws to regress the output of their tracker. (Stewart & Ermon, 2016) also used physical laws to predict the movement of a pillow from unlabelled data though their approach was only applied to a fixed number of frames.\nIn another related approach (Fragkiadaki et al., 2015) attempted to build an internal representation of the physical world. Using a billiard board with an external simulator they built a network which observing four frames and an applied force, was able to predict the 20 next object velocities. Generalization in this work was made using an LSTM in the intermediate representations. The process can be interpreted as iterative since frame generation is made to provide new inputs to the network. This can also be seen as a regularization process to avoid the internal representation of dynamics to decay over time which is different to our approach in which we try to build a stronger internal representation that will attempt to avoid such decay.\nOther research attempted to abstract the physics engine enforcing the laws of physics as neural network models. (Battaglia et al., 2016) and (Chang et al., 2016) were able to produce accurate estimations of the next state of the world. Although the results look plausible and promising, long term predictions are still an issue in such frameworks. Note, that their process is an iterative one as opposed to ours, which propagates an internal state of the world through time.\nApproximate physics with realistic output. Other approaches also focused on learning the production of realistic future scenarios ((Tompson et al., 2016) and (Jeong\net al., 2015)), or inferring collision parameters from monocular videos (Monszpart et al., 2016). In these approaches the authors used physics based losses to produce visually plausible yet erroneous results. They however show promising results and constructed new losses taking into account additional physical parameters other than velocity."}, {"heading": "3. Mechanics Networks", "text": "In this section, we introduce a number of neural network models that can predict the behaviour of a simple mechanical system. We start by describing the physical setup and then we introduce the proposed network architectures."}, {"heading": "3.1. Physical setup", "text": "The physical setup (Fig. 1) consists of a small object sliding down an inclined plane. For notational simplicity, we identify the 3D Euclidean space with the underlying vector space R3 and denote as p = (px, py, pz) \u2208 R3 the coordinates of points as well as of vectors. The plane, which for simplicity passes through the origin, has equation \u03c0 = {p \u2208 R3 : \u3008n,p\u3009 = 0}, where n is the plane unit normal vector. In addition to the normal n, capturing the inclination, the plane has also a Coulomb friction coefficient \u03c1, which in the simple case is homogeneous, but which can also be a spatially varying quantity \u03c1 : \u03c0 \u2192 R+.\nWe set the camera to be located above the plane, at height h > 0, centered at point (0, 0, h), and looking downwards along the Z-direction (0, 0,\u22121). The camera axes are aligned to the world axes and the camera projection model is orthographic; in this setting, a world point p simply projects to pixel (px, py) in the image. Note that h does not have an influence on the generated image and can be dropped.\nThe sliding object is a cube with center of mass q(t) = (qx(t), qy(t), qz(t)) at time t, which projects to pixel y(t) = \u03b1(qx(t), qy(t)) +\u03b2 in the image. Here we consider Hi\u00d7Wi = 128\u00d7128 images with pixels of size \u03b1 = 1 and offset \u03b2 = (\u221264,\u221264). Initially, the cube is placed at rest on top of the plane at a random location (q0x, q 0 y), and then starts to slide under the effect of gravity. The cube motion is also affected by friction.\nAn experiment instance is a tuple \u03b1 = (q0x, q 0 y,n, \u03c1), consisting of the values of the initial object position, the plane normal, and the friction coefficient or distribution. The inclination n is arbitrary (within limits), such that the object can slide in any direction. These parameters, as well as many other constant parameters described in section 4.1, are passed to a physical simulator and rendered to simulate the experiment, resulting in a sequence of color images X\u03b1T = (x\u03b1(0),x\u03b1(1), . . . ,x\u03b1(T \u22121)). The simulator\nalso produces the ground-truth center of mass projections Y\u03b1T = (y\u03b1(0), . . . ,y\u03b1(T \u2212 1)). Note that, for the purpose of learning predictors, physics needs not to be specified further; while in fact a complete set of parameters are required to run the physical simulation, the predictor learns automatically to extract the required information from the observed images."}, {"heading": "3.2. Neural network architectures", "text": "We focus on long-term predictors \u03a6 : XT0 7\u2192 YT that take as input the first T0 = 4 framesXT0 of a video sequenceXT and produce as output a long-term estimate YT of the location of the object\u2019s center of mass at times t = 0, 1, . . . , T , where T T0.\nOur method comprises three building blocks (Fig. 2): a feature extractor, a propagation network and an estimation network. The core of our model is the internal representation of the physics, initialized by the feature extractor, updated by the propagation module, and decoded by the estimation module. We compare two representation types: a vector representation, in which each frame is encoded as C-dimensional vector (or 1 \u00d7 1 \u00d7 C tensor), and a H \u00d7W \u00d7 C tensor representation. The importance of this difference is that the vector representation is spatially concentrated, whereas the tensor representation is spatially\ndistributed.\nNext, the three modules are discussed in detail.\n(i) Feature extraction network. The predictor YT = \u03a6(XT0) starts by extracting information from T0 video frames. Similarly to (Fragkiadaki et al., 2015), the RGB channels of the images are concatenated in a single Hi \u00d7 Wi \u00d7 3T0 tensor and this is processed by a convolutional neural network \u03c6init, obtaining a \u03c6init(x0, . . .xT0\u22121) \u2208 RH\u00d7W\u00d7C tensor output. These features serve as the internal physical representation of our network that is propagated through time. Inspired by (Fragkiadaki et al., 2015), we start from the VGG16 network pre-trained on ImageNet (Simonyan & Zisserman, 2015). The network is cut and the last layer adapted as needed. In particular, starting from a (Hi,Wi) = (128, 128, 3) image, the vector representation uses the (H,W,C) = (1, 1, 128) dimensional output of layer fc6, and the tensor representation uses the (8, 8, 512) dimensional output of conv5 instead. All feature extraction layers are frozen in training, except the new layer fc6 and conv1, whose shape changes.\n(ii) Propagation network. The internal representation initialized by the feature extractor is evolved through time by the propagation network F : RH\u00d7W\u00d7C \u2192 RH\u00d7W\u00d7C . Formally, the internal state St is initialized as S0 = \u03c6init(XT0) and updated by iteratively applying F as St+1 = F (St) = F\nt(\u03c6init(XT0)) for t \u2265 1 (note that there is an index shift between state and time, so that St predicts the objct position at time t + T0 \u2212 1). For the vector representation, the propagation network is an LSTM with 128 hidden units. Since there are no more observations after T0, the LSTM input at time t is set to the internal state of the LSTM at the previous time. This is similar in approach to (Cho et al., 2014), although our output is directly fed to the network without re-embedding. For the distributed representation, we use a simple chain of two convolution layers (with 256 and 512 filters respectively, of size 3 \u00d7 3, stride 1, and padding 1) interleaved by a ReLU layer. When using discrete probability map, the representation St is normalised channel-wise in L2 norm after each update in order to avoid the decay of intermediate propagation layers.\n(iii) Estimation network. In the simplest instance, the network predictor estimates directly the values Y\u0302T = (y\u0302(0), . . . , y\u0302(T \u2212 1)) of the object\u2019s center of mass y\u0302(t) \u2208 R2 during the sequence. The learning loss is simply the average squared distance between predicted and ground-truth locations:\nL(Y\u0302T ,YT ) = 1\nT T\u22121\u2211 t=0 \u2016y\u0302(t)\u2212 y(t)\u20162.\nAs discussed above, however, it is preferable to predict the uncertainty of the estimate as well. While in some cases\nthis cannot improve accuracy directly (i.e in the bivariate gaussian case), it is interesting to see if a network is able to develop an internal sense of prediction errors. Further, probabilistic modelling may help the network discount difficult-to-predict points during training, which may otherwise work as outliers negatively affecting training.\nWe propose to do so in two ways. In the first approach, we predict the mean and variance Y\u0302T = (\u00b5(t),\u03a3(t); t = 0, . . . , T \u2212 1) of a bivariate Gaussian distributionN (\u00b7;\u00b5,\u03a3). The loss is the negative log-likelihood of the measured object locations:\nLnrm(Y\u0302T ,YT ) = \u2212 1\nT T\u22121\u2211 t=0 logN (y(t);\u00b5(t),\u03a3(t)).\nIn practice, the neural network estimates the two dimensional vector \u00b5(t) as well as a three dimensional vector \u03bb1(t), \u03bb2(t), \u03b8(t) with the first two being the eigenvalues of \u03a3(t), and the third entry being the angle of the rotation matrix in the decomposition \u03a3(t) =\nR(\u2212\u03b8(t)) [ \u03bb1(t) 0\n0 \u03bb2(t)\n] R(\u03b8(t)). In order to ensure nu-\nmerical stability, eigenvalues are constrained to be in the range [0.01 . . . 100] by setting them as the output of a scaled and translated sigmoid \u03bbi(t) = \u03c3\u03bb,\u03b1(\u03b2i(t)), where \u03c3\u03bb,\u03b1(z) = \u03bb/(1 + exp(\u2212z)) + \u03b1. For more details regarding the training procedure of this model please see section 4.3.\nIn the second approach, we predict discrete probability maps YT = (p(0), . . . , p(T \u2212 1)), where p(t) \u2208 RHi\u00d7Wi and p(t)ij is the probability that the object\u2019s center of mass is contained in a 1 \u00d7 1 square centered at location (j \u2212 Wi/2, i \u2212 Hi/2). Similar to the Gaussian loss, we use the negative log-likelihood of the ground-truth observations as loss:\nLheat(Y\u0302T ,YT ) = 2 log \u03b4 \u2212 1\nT T\u22121\u2211 t=0 log p(t)by(t)e,\nwhere b\u00b7e is the rounding operator and \u03b4 = 1 is the sampling step.1 The probability maps p(t) sum to one and are obtained by applying the softmax operator to a tensor A(t) \u2208 RHi\u00d7Wi estimated by the neural network:\np(t)ij = eA(t)ij\u2211 mn e A(t)mn .\nAll the output predictions at time t + T0 \u2212 1 are extracted from the internal state St by a single layer L(St). The layer L is linear and fully-connected layer for L and Lnrm, and a deconvolutional layer similar to (Long et al., 2015) in the\n1The correction 2 log \u03b4 results in the log-likelihood value of the piecewise-constant continuous distribution corresponding to the discrete one and makes likelihood values comparable for different step sizes \u03b4, as well as comparable to the Gaussian loglikelihood.\ncase of Lheat. Outputs at times t = 0, 1, . . . , T0 are all predicted from S0 using an analogous but independentlytrained fully-connected layer L\u2032. Overall, the output of the predictor is given by:\n\u03a6(XT0) = (L\u2032(S0)1, . . . , L\u2032(S0)T0 , L(S1), . . . , L(ST\u2212T0\u22121))."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Data generation", "text": "Experiments consider three variants of the physical setup described in section 3.1, called Scenarios S0, S1, and S2. Different scenarios sample experiments \u03b1 = (q0x, q 0 y,n, \u03c1) of increasing difficulty. The parameters of each scenario are summarised in Table 1 and described next. The plane normal n was obtained by rotating the Z axis around the X and Y axis by random angles \u03b8x, \u03b8y (Scenario S0 uses a fixed inclination). For Scenarios S1 and S2, the Coulomb friction coefficient \u03c1 of the plane is homogeneous and sampled uniformly at random. For Scenario S2, the plane is split in 10 \u00d7 10 patches, each with a random friction coefficient sampled independently. The friction upper bound was chosen so that the object always slides along the slope.\nThe plane was rendered as a black object so that no static visual cues allow deducing any of the physical parameters except the initial position of the cube; instead the predictor has to approximate physics as needed by observing the motion of the object during the first T0 = 4 frames of each experiment.\nEach experiment was run for at most 240 frames, or terminated early if the object left the field of view. In order to observe enough movement in each recorded sequence, the first 30 video frames of each video were removed, and the rest of the video was sub-sampled by a factor of 3. In practice, most experiments consist of 40-50 images.\nThe dataset contains 12,500 experiments for each Scenario, 70% of which are used for training, 15% for validation, and 15% for test.\nImplementation details. The object\u2019s starting position is initialized randomly using rejection sampling in such a way that (q0x, q 0 y) falls in the slope quadrant that contains the largest visible h coordinate. This procedure generates samples that have most of their trajectory visible to the camera.\nRendering and physical simulation use Blender 2.77\u2019s OpenGL renderer and the Blender Game Engine (relying on Bullet 2 as physics engine) respectively. The object is a cube of side 0.13 Blender units with mass = 1. The simulation parameters are: max physics steps = 5, physics substeps = 3, max logic steps = 5, FPS = 120. Rendering used white environment lighting (energy = 0.7) and no other light sources. The object color was set to Lambertian red (RGB: 0.8, 0.04, 0.04) with no specular component. The slope is completely black, covering the whole field of view. The output images were stored as 128\u00d7 128 color JPG files. See Fig. 1 for an example initial setting from Scenario S2."}, {"heading": "4.2. Baseline predictors", "text": "Least squares fit. We compare the performance of our methods to two simple least squares baselines: Linear and Quadratic. In both cases we fit two least squares polynomials to the estimated screen-space coordinates of the first T = 10 frames. The polynomials are of first and second degree(s), respectively. We estimate the object\u2019s position in this case by using the maximum location of the red channel of the input image. Note, that being able to observe the first 10 frames is a large advantage compared to the networks, which only see the first T0 = 4 frames.\nPhysics simulator. The SimNet baseline is used to evaluate the long term prediction ability of a neural network that has access to an explicit physics simulator, in a manner analogous to the work of (Wu et al., 2015). Similarly to the other networks, SimNet observes the first T0 images and aims to regress the physical parameters necessary to predict the trajectory of the object using the physics engine.\nThe simulator is assumed to have access to a perfect model of the underlying physical laws. The regression architecture constitutes of the vector based feature extraction network described in Section 3.2 with an extra fully-connected layer on top to regress physical parameters. The network is trained with an L2 loss to infer the current slope rotation angles and friction coefficient (\u03b8x, \u03b8y, \u03c1), the object\u2019s position at the observed frames (t0, . . . , T0 \u2212 1), and its final velocity at frame T0 \u2212 1.\nWe input the regressed parameters to the same physics simulation system that generated the dataset (Section 4.1) and run the simulation to predict the following object positions T0 . . . T . Note that, since the simulator used by the network is the same as the one used to generate the data, this network is given a significant advantage over the other models."}, {"heading": "4.3. MechaNets", "text": "Experiments consider four different variants of the mechanical prediction networks (MechaNet1 to 4 for short). MechaNet1 and MechaNet2 are trained to optimise the L; MechaNet1 uses the LSTM propagation network and the spatially concentrated internal representation, whereas MechaNet2 uses the simpler convolutional propagation network but the distributed representation. MechaNet3 and MechaNet4 are similar to MechaNet2, but they use probabilist predictors, using the Gaussian and probability map outputs, respectively. The four variants are summarized in Table 2.\nImplementation details. Network weights are initialized by sampling from a Gaussian distribution. Training uses a batch size of 50 using the first 10 to 40 frames of each video sequence using RMSProp (Tieleman & Hinton, 2012). Training is halted when there is no improvement of the L2 loss after 40 consecutive epochs; 1,000 epochs were found to be usually sufficient for convergence.\nSince during the initial phases of training the network is very uncertain, the model using the Gaussian loglikelihood loss Lnrm was found to get stuck on solutions with very high variance \u03a3(t); to solve this issue, the regularizer \u03bb \u2211 t det \u03a3(t) was added to the loss, setting \u03bb = 10 for the first few epochs and then lowering it to \u03bb = 0 when the value of the determinant stablized under 100 on average (this variance is comparable to the image size).\nIn all our experiments we used Tensorflow (Abadi et al., 2015) r0.12 on a single NVIDIA Titan X GPU."}, {"heading": "4.4. Results", "text": "Long term predictions. Table 2 and Fig. 3 compare the baseline predictors and the four MechaNets on the task of long term prediction of the object trajectory. We call this \u201clong term\u201d in the sense that all methods observe only the first T0 = 4 frames of a video (except the linear and quadratic extrapolators which observe the first 10 frames instead), to then extrapolate the trajectory to 40 time steps.\nAll networks can be used to perform arbitrary long predictions; the table, in particular, reports the the average L2 prediction errors at time Ttest = 20 and 40. However, models in this table are only shown the first Ttrain = 20 frames of each video during training.\nConsider first the prediction results for Ttest = Ttrain = 20. All networks perform considerably better than the linear and quadratic extrapolators in all scenarios, with error rates 5-40\u00d7 smaller. As expected, Scenario S1 and S2 are harder than Scenario S0, which uses a fixed slope and homogeneous friction, but the network prediction errors are still small, in the order of 1-2 pixels. All networks perform similarly well, particularly in Scenarios S1, with a slight advantage for the LSTM-based propagation networks. SimNet is very competitive, as may be expected given that it uses the ground-truth physics engine for integration. However, in Scenario S2 this method does not work well since the variable friction distribution is not observable from the first T0 = 4 frames of a video; MechaNet2 and MechaNet4, which can better learn such effects, can account for such uncertainty and significantly outperform SimNet.\nResults are different for predictions at time Ttest = 40 Ttrain. All networks still outperform the extrapolators, but\n0 20 40 60 80 100 120 0\n20\n40\n60\n80\n100\n120\nPrediction at time 0\n0 20 40 60 80 100 120\nPrediction at time 9\n0 20 40 60 80 100 120\nPrediction at time 19\n0 20 40 60 80 100 120\nPrediction at time 29\n0 20 40 60 80 100 120\nPrediction at time 39\nFigure 4: Uncertainty prediction using probability maps. The figure shows the output of MechaNet4 (40) on one example sequence in Scenario S1.\nin Scenarios S0 and S1 SimNet performs better than the other networks: by having access to the physics simulator, generalization is not an issue. On the other hand, this experiment shows that the deep networks have a limited capacity to generalize physics beyond the regimes observed during training. Among such networks, the ones modelling uncertainty (MechaNet3 and MechaNet4) are able to generalize better. Scenario S2 still breaks the assumptions made by SimNet, and the other networks outperform it.\nGeneralization. The issue of generalization is explored in detail in Table 3, focusing on MechaNet4 that exhibits the best generalization capabilities. The table reports prediction errors at T = 10, 20, 30, 40 for networks trained with video sequence of length T = 10, 20, 30, 40 respectively. Recall that predictors always observe only the first T0 = 4 frames of each sequence; the only change is to allow the training loss to assess the predictors\u2019 performance on progressively longer videos during training.\nAs expected, training on longer sequences dramatically improves the accuracy of longer term predictions, but also the shorter term ones. Training on the full sequences, in particular, performs \u223c 20% better than SimNet. This confirms that, while deep networks are able to learn physical rules accurately for the range of physical experiences observed during training, they do not necessarily learn rules that generalize as readily as conventional physical laws.\nPredicting uncertainty. MechaNet3 and MechaNet4 predict a posterior distribution of possible object locations, using a Gaussian and a probability map model respectively. Table 2 shows that the latter model has significantly lower perplexity, suggesting that the Gaussian model is somewhat too constrained. Qualitatively, Fig. 3 and 4 show that both models make very reasonable predictions of uncertainty, with the uncertain area growing over time as expected."}, {"heading": "5. Conclusions", "text": "In this paper we explored the possibility of using a single neural network for long-term prediction of mechanical phenomena. We considered in particular the problem of predicting the long-term motion of a cuboid sliding down a slope of unknown inclination and heterogeneous friction. Differently from many other approaches, we use the network not to predict some physical quantities to be integrated by a simulator, but to directly predict the complete trajectory of the object end-to-end.\nOur results, obtained from extensive synthetic simulation, indicate that deep neural networks can successfully predict long-term trajectories without requiring explicit modeling of the underlying physics. They can also reliably estimate a distribution over such predictions to account for uncertainty in the data. Remarkably, these models are competitive with alternative predictors that have access to the ground-truth physical simulator, and outperform them when some of the physical parameters are not observable or known a-priori. However, neural networks exhibit a limited capability to perform predictions outside the physical regimes observed during training. In other words, the internal representation of physics learned by such model is not as general as standard physical laws.\nSeveral future directions remain to be explored. Given the accuracy of mechanical simulators, synthetic experiments are sufficient to assess the capability of networks to learn mechanical phenomena. However, the obvious next phase will be to test the framework on video footage obtained from real-world data in order to assess the ability to do so from visual data affected by real nuisance factors. The other important generalization is to consider more complex physical phenomena, including multiple sliding objects with possible interactions, rolling motion, and sliding over non-flat surfaces."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Abadi"], "venue": null, "citeRegEx": "Abadi,? \\Q2015\\E", "shortCiteRegEx": "Abadi", "year": 2015}, {"title": "Learning to Poke by Poking: Experiential Learning of Intuitive Physics", "author": ["Agrawal", "Pulkit", "Nair", "Ashvin V", "Abbeel", "Pieter", "Malik", "Jitendra", "Levine", "Sergey"], "venue": "In Proc. NIPS,", "citeRegEx": "Agrawal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Interaction networks for learning about objects, relations and physics", "author": ["Battaglia", "Peter", "Pascanu", "Razvan", "Lai", "Matthew", "Rezende", "Danilo Jimenez"], "venue": "In Proc. NIPS,", "citeRegEx": "Battaglia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2016}, {"title": "Simulation as an engine of physical scene understanding", "author": ["Battaglia", "Peter W", "Hamrick", "Jessica B", "Tenenbaum", "Joshua B"], "venue": "PNAS, 110(45):18327\u201318332,", "citeRegEx": "Battaglia et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2013}, {"title": "A compositional object-based approach to learning physical dynamics", "author": ["Chang", "Michael B", "Ullman", "Tomer", "Torralba", "Antonio", "Tenenbaum", "Joshua B"], "venue": "arXiv preprint arXiv:1612.00341,", "citeRegEx": "Chang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "G\u00fcl\u00e7ehre", "\u00c7alar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In Proc. EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Dynamic filter networks", "author": ["De Brabandere", "Bert", "Jia", "Xu", "Tuytelaars", "Tinne", "Van Gool", "Luc"], "venue": "In Proc. NIPS,", "citeRegEx": "Brabandere et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brabandere et al\\.", "year": 2016}, {"title": "Learning to perform physics experiments via deep reinforcement learning", "author": ["Denil", "Misha", "Agrawal", "Pulkit", "Kulkarni", "Tejas D", "Erez", "Tom", "Battaglia", "Peter", "de Freitas", "Nando"], "venue": "Deep Reinforcement Learning Workshop,", "citeRegEx": "Denil et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2016}, {"title": "Learning visual predictive models of physics for playing billiards", "author": ["Fragkiadaki", "Katerina", "Agrawal", "Pulkit", "Levine", "Sergey", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1511.07404,", "citeRegEx": "Fragkiadaki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fragkiadaki et al\\.", "year": 2015}, {"title": "Inferring mass in complex scenes by mental", "author": ["J.B. Hamrick", "P.W. Battaglia", "T.L. Griffiths", "J.B. Tenenbaum"], "venue": "simulation. Cognition,", "citeRegEx": "Hamrick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hamrick et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In IEEE CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Data-driven fluid simulations using regression forests", "author": ["Jeong", "SoHyeon", "Solenthaler", "Barbara", "Pollefeys", "Marc", "Gross", "Markus"], "venue": "ACM Trans. on Graphics (TOG),", "citeRegEx": "Jeong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jeong et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Proc. NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning physical intuition of block towers by example", "author": ["Lerer", "Adam", "Gross", "Sam", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1603.01312,", "citeRegEx": "Lerer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lerer et al\\.", "year": 2016}, {"title": "Visual stability prediction and its application to manipulation", "author": ["Li", "Wenbin", "Leonardis", "Ale\u0161", "Fritz", "Mario"], "venue": "arXiv preprint arXiv:1609.04861,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "In IEEE CVPR,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "SMASH: Physics-guided Reconstruction of Collisions from Videos", "author": ["Monszpart", "Aron", "Thuerey", "Nils", "Mitra", "Niloy"], "venue": "ACM Trans. on Graphics (TOG),", "citeRegEx": "Monszpart et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Monszpart et al\\.", "year": 2016}, {"title": "Newtonian scene understanding: Unfolding the dynamics of objects in static images", "author": ["Mottaghi", "Roozbeh", "Bagherinezhad", "Hessam", "Rastegari", "Mohammad", "Farhadi", "Ali"], "venue": "In IEEE CVPR,", "citeRegEx": "Mottaghi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mottaghi et al\\.", "year": 2016}, {"title": "Deep tracking: Seeing beyond seeing using recurrent neural networks", "author": ["Ondruska", "Peter", "Posner", "Ingmar"], "venue": "In Proc. AAAI,", "citeRegEx": "Ondruska et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ondruska et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Label-free supervision of neural networks with physics and domain knowledge", "author": ["Stewart", "Russell", "Ermon", "Stefano"], "venue": null, "citeRegEx": "Stewart et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 2016}, {"title": "Lecture 6.5\u2014RMSProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Accelerating Eulerian Fluid Simulation With Convolutional Networks", "author": ["J. Tompson", "K. Schlachter", "P. Sprechmann", "K. Perlin"], "venue": "ArXiv e-print", "citeRegEx": "Tompson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tompson et al\\.", "year": 2016}, {"title": "Galileo: Perceiving physical object properties by integrating a physics engine with deep learning", "author": ["Wu", "Jiajun", "Yildirim", "Ilker", "Lim", "Joseph J", "Freeman", "Bill", "Tenenbaum", "Josh"], "venue": "In Proc. NIPS, pp", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Physics 101: Learning physical object properties from unlabeled videos", "author": ["Wu", "Jiajun", "Lim", "Joseph J", "Zhang", "Hongyi", "Tenenbaum", "Joshua B", "Freeman", "William T"], "venue": "In Proc. BMVC,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["Xue", "Tianfan", "Wu", "Jiajun", "Bouman", "Katherine L", "Freeman", "William T"], "venue": "In Proc. NIPS,", "citeRegEx": "Xue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "However, the nature of these mental models remains unclear and is being actively investigated (Hamrick et al., 2016).", "startOffset": 94, "endOffset": 116}, {"referenceID": 25, "context": "Among such prior works, by far the most popular approach is to use neural networks (Wu et al., 2016) to extract from sensory data local predictions of physical parameters, such as mass, velocity, or acceleration, that are then integrated by an external mechanism such as a physical simulator to obtain long term predictions.", "startOffset": 83, "endOffset": 100}, {"referenceID": 2, "context": "Other attempts have also tried to replace the physical engine with a neural network (Battaglia et al., 2016) but did not really attempt to observe the physical world and deduce properties from it but rather to integrate the physical equations.", "startOffset": 84, "endOffset": 108}, {"referenceID": 3, "context": "To the best of our knowledge (Battaglia et al., 2013) was the first approach to tackle intuitive physics with the aim to answer a set of intuitive questions (e.", "startOffset": 29, "endOffset": 53}, {"referenceID": 18, "context": "More recently (Mottaghi et al., 2016) also used static images and a graphic rendering engine (Blender) to predict movements and directions of forces from a single RGB image.", "startOffset": 14, "endOffset": 37}, {"referenceID": 13, "context": ", (Krizhevsky et al., 2012; He et al., 2016)) they used a convolutional architecture to understand dynamics and forces acting behind Camera", "startOffset": 2, "endOffset": 44}, {"referenceID": 10, "context": ", (Krizhevsky et al., 2012; He et al., 2016)) they used a convolutional architecture to understand dynamics and forces acting behind Camera", "startOffset": 2, "endOffset": 44}, {"referenceID": 14, "context": "In a different framework (Lerer et al., 2016) and (Li et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 15, "context": ", 2016) and (Li et al., 2016) also used the power of deep learning to extract an abstract representation of the concept of stability of block towers purely from images.", "startOffset": 12, "endOffset": 29}, {"referenceID": 1, "context": "Other approaches such as (Agrawal et al., 2016) or (Denil et al.", "startOffset": 25, "endOffset": 47}, {"referenceID": 7, "context": ", 2016) or (Denil et al., 2016) also attempted to learn intuitive physics of objects through manipulation.", "startOffset": 11, "endOffset": 31}, {"referenceID": 26, "context": "The idea was further developed in (Xue et al., 2016) in order to generate a next possible image frame from a single static input image.", "startOffset": 34, "endOffset": 52}, {"referenceID": 26, "context": "In the future we also aim to experiment with approaches inspired by (Xue et al., 2016).", "startOffset": 68, "endOffset": 86}, {"referenceID": 24, "context": "Works of (Wu et al., 2015) and its extension (Wu et al.", "startOffset": 9, "endOffset": 26}, {"referenceID": 25, "context": ", 2015) and its extension (Wu et al., 2016) propose methods to learn physical properties of scenes and objects.", "startOffset": 26, "endOffset": 43}, {"referenceID": 24, "context": "However in (Wu et al., 2015) the MCMC sampling based approach assumes a complete knowledge of the physical equations to estimate the correct physical parameters.", "startOffset": 11, "endOffset": 28}, {"referenceID": 25, "context": "In (Wu et al., 2016) deep learning has been used more extensively to replace the MCMC based sampling but this work also employs an explicit encoding and computation of physical laws to regress the output of their tracker.", "startOffset": 3, "endOffset": 20}, {"referenceID": 8, "context": "In another related approach (Fragkiadaki et al., 2015) attempted to build an internal representation of the physical world.", "startOffset": 28, "endOffset": 54}, {"referenceID": 2, "context": "(Battaglia et al., 2016) and (Chang et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 4, "context": ", 2016) and (Chang et al., 2016) were able to produce accurate estimations of the next state of the world.", "startOffset": 12, "endOffset": 32}, {"referenceID": 23, "context": "Other approaches also focused on learning the production of realistic future scenarios ((Tompson et al., 2016) and (Jeong et al.", "startOffset": 88, "endOffset": 110}, {"referenceID": 12, "context": ", 2016) and (Jeong et al., 2015)), or inferring collision parameters from monocular videos (Monszpart et al.", "startOffset": 12, "endOffset": 32}, {"referenceID": 17, "context": ", 2015)), or inferring collision parameters from monocular videos (Monszpart et al., 2016).", "startOffset": 66, "endOffset": 90}, {"referenceID": 8, "context": "Similarly to (Fragkiadaki et al., 2015), the RGB channels of the images are concatenated in a single Hi \u00d7 Wi \u00d7 3T0 tensor and this is processed by a convolutional neural network \u03c6init, obtaining a \u03c6init(x0, .", "startOffset": 13, "endOffset": 39}, {"referenceID": 8, "context": "Inspired by (Fragkiadaki et al., 2015), we start from the VGG16 network pre-trained on ImageNet (Simonyan & Zisserman, 2015).", "startOffset": 12, "endOffset": 38}, {"referenceID": 5, "context": "This is similar in approach to (Cho et al., 2014), although our output is directly fed to the network without re-embedding.", "startOffset": 31, "endOffset": 49}, {"referenceID": 16, "context": "The layer L is linear and fully-connected layer for L and Lnrm, and a deconvolutional layer similar to (Long et al., 2015) in the", "startOffset": 103, "endOffset": 122}, {"referenceID": 24, "context": "The SimNet baseline is used to evaluate the long term prediction ability of a neural network that has access to an explicit physics simulator, in a manner analogous to the work of (Wu et al., 2015).", "startOffset": 180, "endOffset": 197}], "year": 2017, "abstractText": "Evolution has resulted in highly developed abilities in many natural intelligences to quickly and accurately predict mechanical phenomena. Humans have successfully developed laws of physics to abstract and model such mechanical phenomena. In the context of artificial intelligence, a recent line of work has focused on estimating physical parameters based on sensory data and use them in physical simulators to make long-term predictions. In contrast, we investigate the effectiveness of a single neural network for end-to-end long-term prediction of mechanical phenomena. Based on extensive evaluation, we demonstrate that such networks can outperform alternate approaches having even access to ground-truth physical simulators, especially when some physical parameters are unobserved or not known a-priori. Further, our network outputs a distribution of outcomes to capture the inherent uncertainty in the data. Our approach demonstrates for the first time the possibility of making actionable long-term predictions from sensor data without requiring to explicitly model the underlying physical laws.", "creator": "LaTeX with hyperref package"}}}