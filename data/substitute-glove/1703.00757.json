{"id": "1703.00757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Predicting Rankings of Software Verification Competitions", "abstract": "Software verification paralympics, although first the monthly SV - COMP, necessary software verification designing past belief to their wrongheadedness few efficiency. Typically, also argument beyond though competition particular up (although definition - suggest) ranking especially the tools. For as requires, for as building underwriting derivations, this would clear socially although have an very among but (therefore) performance of rigorous rudimentary on a given ensures our beforehand, come. e. , prior well sure pulling take rudimentary to although coordinating.", "histories": [["v1", "Thu, 2 Mar 2017 12:28:12 GMT  (143kb,D)", "http://arxiv.org/abs/1703.00757v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SE", "authors": ["mike czech", "eyke h\\\"ullermeier", "marie-christine jakobs", "heike wehrheim"], "accepted": false, "id": "1703.00757"}, "pdf": {"name": "1703.00757.pdf", "metadata": {"source": "CRF", "title": "Predicting Rankings of So\u0081ware Verification Competitions\u2217", "authors": ["Mike Czech", "Eyke H\u00fcllermeier", "Marie-Christine Jakobs", "Heike Wehrheim"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we present a machine learning approach to predicting rankings of tools on veri cation tasks. e method builds upon so-called label ranking algorithms, which we complement with appropriate kernels providing a similarity measure for veri cation tasks. Our kernels employ a graph representation for so ware source code that mixes elements of control ow and program dependence graphs with abstract syntax trees. Using data sets from SV-COMP, we demonstrate our rank prediction technique to generalize well and achieve a rather high predictive accuracy. In particular, our method outperforms a recently proposed feature-based approach of Demyanova et al. (when applied to rank predictions).\nCCS CONCEPTS \u2022Computingmethodologies\u2192Ranking; Support vectormachines; Cross-validation; \u2022So ware and its engineering \u2192So ware veri cation; Formal so ware veri cation;\nKEYWORDS So ware veri cation, machine learning, ranking.\nACM Reference format: Mike Czech, Eyke Hu\u0308llermeier, Marie-Christine Jakobs, Heike Wehrheim . 2017. Predicting Rankings of So ware Veri cation Competitions."}, {"heading": "1 INTRODUCTION", "text": "e annual holding of so ware veri cation competitions has recently stimulated the development of veri cation tools, in particular the tuning of tools towards performance and precision. e participating candidate tools typically employ a large range of different techniques, from static analysis, abstract interpretation and automata-based techniques to SAT or SMT solving. In the area of automatic veri cation, the most prominent competition today is\n\u2217 is work was partially supported by the German Research Foundation (DFG) within the Collaborative Research Centre \u201cOn- e-Fly Computing\u201d (SFB 901).\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). \u00a9 2017 Copyright held by the owner/author(s).\nthe Competition on So ware Veri cation SV-COMP [1]. Over the years, the SV-COMP community has collected a large number of benchmark veri cation tasks, i.e., so ware source code together with properties to be proven, and is constantly continuing to do so. Veri cation tasks are bundled in categories, and the outcome of SV-COMP are rankings (overall and per category) computed by means of a scoring schema.\nRankings of veri cation tools on veri cation tasks in so ware competitions o er an a-posteriori insight into the particular usefulness of a tool on a veri cation task. However, for programmers wanting to select a tool for veri cation of their program, or for building portfolio solvers, a prediction on a likely ranking without actually running (all or some) tools is needed. In this paper, we propose such a method.\nOur method for rank prediction builds upon machine learning techniques, more precisely on so-called label ranking algorithms [9] using support vector machines [3] as base learners. As training data, we take the SV-COMP results of 2015 and the only recently published results of 2017. e key ingredient of our approach is the de nition of a number of kernels [20], which act as similarity measures on veri cation tasks. So far, two other machine learning methods for selecting tools or algorithms for veri cation have been proposed [6, 23], both of them being based on feature vectors: while Tulsian et al. [23] only employ structural features of programs (like the number of arrays, loops, recursive functions), Demyanova et al. [6] uses a number of data- ow analyses to also determine more sophisticated features (e.g., certain loop pa erns). us, both approaches try to explicitly capture aspects of source code that make veri cation hard (for some or all tools). With our kernels, we take a di erent approach: we supply the learning algorithm with a representation of source code that enables the learner itself to identify the distinguishing pa erns. We believe that our kernels are thus more readily usable for other program analysis tasks, for which a machine learning method might be considered (e.g., program classication or program analysis). In that case, we could use exactly the same kernel and just exchange the training data. Still, our experiments show that the prediction accuracy for rankings of so ware veri cation competitions is higher than that of Demyanova et al. [6] (when using their feature vectors for predicting rankings, not just predicting winners)1.\nMore speci cally, our kernels are constructed on graph representations of source code. We have experimented with di erent (weighted) combinations of control ow graphs (CFGs), program dependence graphs (PDGs), and abstract syntax trees (ASTs). In these, concrete inscriptions on nodes (like x:=y+1) are rst of all replaced by abstract labels (e.g., Assign). Such labelled graphs are used within our speci c adaptation of the Weisfeiler-Lehman graph\n1A comparison with Tulsian et al. [23] is di cult due to non-reproducability of their results.\nar X\niv :1\n70 3.\n00 75\n7v 1\n[ cs\n.L G\n] 2\nM ar\n2 01\n7\nkernel framework [21] that compares graphs not only according to their labels (and how o en they occur) but also according to associations between labels (via edges in the graph). is is achieved by iteratively comparing larger and larger subtrees of nodes, where the maximum depth of subtrees to be considered is a parameter to the framework. e choice of Weisfeiler-Lehman kernels is motivated by their be er scalability compared to other graph kernels, such as random walk or shortest path kernels (see [21]). We prove our kernels to be positive de nite, which is the key property for kernels to be usable for machine learning. e ranking is nally computed by a method for rank prediction via pairwise comparison [9], using support vector machines as base learners.\nWe have implemented our technique and carried out experimental (cross-validation) studies using data from SV-COMP 2015 and 2017. e experiments show that our technique can predict rankings with a rather high accuracy, using Spearman\u2019s rank correlation [22] to compare predicted with true rankings. To see how our technique compares to existing approaches, we have also used the feature vectors of Demyanova et al. [6] for rank prediction. It turns out that, for three di erent data sets (containing general safety, termination and memory safety veri cation tasks), our technique outperforms the technique of [6] in almost all instantiations (choosing a depth for subtrees and a combination of CFG, PDG and AST) . Summarizing, this paper makes the following contributions:\n\u2022 We propose a technique for the prediction of rankings in so ware veri cation competitions (instead of just predicting winners); \u2022 we present an expressive representation of source code ready for use in other machine learning approaches to program analysis; \u2022 we experimentally demonstrate our technique\u2014despite being more general and more widely applicable\u2014to compare favorably with existing approaches on the speci c task of predicting rankings of so ware veri cation tools.\nAll data of 2015 and so ware is available at h ps://github.com/ zenscr/PyPRSVT."}, {"heading": "2 REPRESENTING VERIFICATION TASKS", "text": "Our objective is to predict rankings of so ware veri cation competitions via machine learning. To this end, the learning algorithm has to be supplied with training data, which, in our case, is readily available from the SV-COMP website. We start with explaining what kind of data our rank prediction technique is supplied with, and how this data is represented.\ne purpose of SV-COMP is to compare veri cation tools with respect to their e ectiveness and e ciency. To this end, the tools are supplied with veri cation tasks.\nDe nition 2.1. A veri cation task (P ,\u03c6) consists of a program P (for SV-COMP wri en in C) and a property (also called speci cation) \u03c6 (typically wri en as assertion into the program).\nFigure 1 shows our running example PSUM of a veri cation task (computing n times 2 via addition). In a veri cation run, a veri cation tool is run on a veri cation task in order to determine whether the program ful lls the speci cation. e outcome of such"}, {"heading": "1 int i; 6 i = 0;", "text": ""}, {"heading": "2 int n; 7 while (i <= n)", "text": ""}, {"heading": "3 int sn; 8 sn = sn + 2;", "text": ""}, {"heading": "4 n = input(); 9 i = i + 1;", "text": ""}, {"heading": "5 sn = 0; 10 assert (sn == n*2 || sn == 0);", "text": "Figure 1: e veri cation task PSUM\na veri cation run is a pair (TIME, ANSWER),2 where TIME is the time in seconds from the start of the veri cation run to its end, and ANSWER is of the following form:\nTRUE when the veri cation tool has concluded that P satises \u03c6, FALSE when the veri cation tool has concluded that P violates \u03c6, and\nUNKNOWN when no conclusive result was achieved. In SV-COMP, veri cation tasks are bundled into categories (e.g., memory safety, termination, concurrency). Ranking of tools is rst of all being carried out per category (but then extended to meta categories). e ranking within a category is done via a scoring schema which gives positive and negative points to outcomes, e.g., negative points when the tool incorrectly concluded the property to be valid for the program. When the scores of two tools are the same, the runtimes (of successful runs) determine the ordering. e data from SV-COMP available for learning rankings thus consists of veri cation tasks in di erent categories, outcomes of tools on these tasks and scores assigned to these outcomes as well as the nal ranking.\ne purpose of the machine learning algorithm is to learn from these observations how tools will perform on speci c veri cation tasks. Our machine learning technique is based on kernel methods (see e.g. [20]). In general, a kernel can be interpreted as a similarity measure on data instances (in our case veri cation tasks), with the idea that similar results (in our case rankings) are produced for similar instances. While kernel-based learning algorithms are completely generic, the kernel function itself is application-speci c and, to achieve strong performance, needs to be designed in an appropriate way. In other words, a key question is how to de ne kernels suitable for the application at hand.\ne simplest way of de ning a kernel is via the inner product of feature vectors, i.e., vectorial representations of data objects. In the two approaches existing so far [6, 23], corresponding features of programs, such as the number of loops, conditionals, pointer variables, or arrays in a program, are de ned in an explicit way. Obviously, this approach requires su cient domain knowledge to identify those features that are important for the prediction problem at hand. Our approach essentially di ers in that features are speci ed in a more indirect way, namely by systematically extracting (a typically large number of) generic features from a suitable representation of the veri cation task. Selecting the useful features and combining them appropriately is then basically le to the learner.\n2In addition, witnesses are part of the outcomes. Witnesses have only been part of the scoring scheme of 2017, and are thus for reasons of harmonisation of 2015 and 2017 not considered here.\nBut how to represent the veri cation tasks in a proper way? e rst idea is to use the source code itself (i.e., strings). However, the source code of two programs might look very di erent although the underlying program is actually the same (di erent variable names, while instead of for loops, etc.). What we need is a representation that abstracts from issues like variable names but still represents the structure of programs, in particular dependencies between elements of the program. ese considerations (and some experiments comparing di erent representations) have led to a graph representation of programs combining concepts of three existing program representations:\nControl ow graphs: CFGs record the control ow in programs and thus the overall structure with loops, conditionals etc.; these are needed, for example, to see loops in programs. Program dependence graphs: PDGs [10] represent dependencies between elements in programs. We distinguish control and data dependencies. is information is important, for example, to detect whether a loop boundary"}, {"heading": "1 int i = 0; 1 int i = 0;", "text": ""}, {"heading": "2 int n = abs(input()); 2 int n = abs(input());", "text": ""}, {"heading": "3 while (i < n) 3 while (i < n)", "text": ""}, {"heading": "4 i++; 4 i++;", "text": ""}, {"heading": "5 assert (i == n); 5 assert (i != n);", "text": "Unlike CFGs and PDGs but (partly) alike ASTs, we abstract from concrete names occuring in programs. Nodes in the graph will thus not be labelled with statements or variables as occuring in the program, but with abstract identi ers. We let Lab be the set of all\nsuch labels. Table 1 lists some identi ers and their meaning. e following de nition formalizes this graph representation.3\nDe nition 2.2. Let P be a veri cation task. e graph representation of P is a graph G = (N ,E, s, t , \u03c1,\u03c4 ,\u03b7) with\n\u2022 N a set of nodes (basically, we build an AST for every statement in P , and use the nodes of these ASTs), \u2022 E a set of edges, with s : E \u2192 N denoting the start and t : E \u2192 N the end node of an edge, \u2022 \u03c1 : N \u2192 Lab a labelling function for nodes, \u2022 \u03c4 : E \u2192 {CD,DD, SD,CF} a labelling function for edges\nre ecting the type of dependence: CD (control dependency) and DD (data dependency) origin in PDGs, SD (syntactical dependence) is the \u201cconsists-of\u201d relationship of ASTs and CF (control ow) the usual control ow in programs, \u2022 \u03bd : E \u2192 {T , F } a function labelling control dependence edges according to the valuation of the conditional they arise from. All other edges are labelled true.\nWe let GV denote the set of all veri cation task graphs.\nFigure 2 depicts the graph representation of the veri cation task PSUM . e rectangle nodes represent the statements in the program and act as root nodes of small ASTs. For instance, the rectangle labelled Assert at the bo om, middle represents the assertion in line 10. e gray ovals represent the AST parts below the root nodes. We de ne the depth of nodes n, d(n), as the distance of a node to its root node. As an example, the depth of the Assert-node itself is 0, the depth of both ==-nodes is 2.\nis graph representation allows us to see the key structural properties of a veri cation task, e.g., that the loop (condition) in our example program depends on an assignment where the righthand-side is an input (which makes veri cation more complicated). With respect to semantical properties, our graph representation (as well as all feature-based approaches relying on static analyses of programs) is less adequate. To see this, consider the two programs in Figure 3. ey only di er in the assertion at line 5, which from its syntax is the same on both sides: a simple boolean expression on two variables of exactly the same type and dependencies. However, veri cation of the le program is di cult for veri cation tools which cannot generate loop invariants. Veri cation of the program on the right, however, is easy as it is incorrect (which can e.g. be detected by a bounded unrolling of the loop). Here, we clearly see the limits of any learning approach based on structural properties of programs."}, {"heading": "3 PREDICTING RANKINGS", "text": "is section starts with a short description of the necessary background in machine learning. More speci cally, we explain the problem of label ranking as well as the method of ranking by pairwise comparison for solving this problem. In the second part, we recall binary classi cation with support vector machines and introduce our kernel functions on veri cation tasks.\n3Actually, it only partly does, because a full formalization would require de nitions of CFGs, PDGs and ASTs which \u2013 due to lack of space \u2013 cannot all be given here."}, {"heading": "3.1 Label Ranking", "text": "Consider a nite set of K alternatives identi ed by class labels Y = {y1, . . . ,yK }; in our case, the alternatives correspond to the veri cation tools to be compared. We are interested in total order relations on Y, that is, complete, transitive, and antisymmetric relations, where yi yj indicates that yi precedes yj in the order. Formally, a total order can be identi ed with a permutation \u03c0 of the set [K] = {1, . . . ,K}, such that \u03c0 (i) is the position of yi in the order. We denote the class of permutations of [K] (the symmetric group of order K) by SK . By abuse of terminology, though justi ed in light of the above one-to-one correspondence, we refer to elements \u03c0 \u2208 SK as both permutations and rankings.\nIn the se ing of label ranking (see e.g. [25]), preferences on Y are \u201ccontextualized\u201d by instances x \u2208 X, where X is an underlying instance space; in our case, instances are programs to be veri ed. us, each instance x is associated with a ranking x of the label set Y or, equivalently, a permutation \u03c0x \u2208 SK . More speci cally, since label rankings do not necessarily depend on instances in a deterministic way, each instance x is associated with a probability distribution P(\u00b7 | x) on SK . us, for each \u03c0 \u2208 SK , P(\u03c0 | x) denotes the probability to observe the ranking \u03c0 in the context speci ed by x .\ne goal in label ranking is to learn a \u201clabel ranker\u201d, that is, a model\nM : X \u2192 SK that predicts a ranking \u03c0\u0302 for each instance x given as an input. More speci cally, seeking a model with optimal prediction performance, the goal is to nd a risk (expected loss) minimizer\nM\u2217 \u2208 argmin M\u2208M \u222b X\u00d7SK D(M(x),\u03c0 )d P ,\nwhere M is the underlying model class, P is the joint measure P(x ,\u03c0 ) = P(x)P(\u03c0 | x) on X \u00d7 SK and D is a loss function on SK . A common example of such a loss is D(\u03c0 , \u03c0\u0302 ) = 1 \u2212 S(\u03c0 , \u03c0\u0302 ), where S(\u03c0 , \u03c0\u0302 ) is the Spearman rank correlation [22]:\nS(\u03c0 , \u03c0\u0302 ) = 1 \u2212 6\n\u2211K i=1(\u03c0 (i) \u2212 \u03c0\u0302 (i))2\nK(K2 \u2212 1) \u2208 [\u22121, 1]\nAs training dataD, a label ranker uses a set of instances xn (n \u2208 [N ]), together with information about the associated rankings \u03c0n ."}, {"heading": "3.2 Ranking by Pairwise Comparison", "text": "Ranking by pairwise comparison (RPC) is a meta-learning technique that reduces a label ranking task to a set of binary classi cation problems [12]. More speci cally, the idea is to train a separate model (base learner) Mi, j for each pair of labels (yi ,yj ) \u2208 Y, 1 \u2264 i < j \u2264 K ; thus, a total number of K(K \u2212 1)/2 models is needed (see Figure 4 for an illustration).\nFor training, the original data D is rst turned into binary classication data sets Di, j , 1 \u2264 i < j \u2264 K . To this end, each preference information of the form yi x yj (extracted from full or partial information about a ranking \u03c0x ) is turned into a positive (classi cation) example (x , 1) for the learnerMi, j ; likewise, each preference yj x yi is turned into a negative example (x , 0). us, Mi, j trained on Di, j is intended to learn the mapping that outputs 1 if yi x yj and 0 if yj x yi . is mapping can be realized by any\nbinary classi er. Instead of a {0, 1}-valued classi er, one can of course also employ a scoring classi er. For example, the output of a probabilistic classi er would be a number in the unit interval [0, 1] that can be interpreted as a probability of the preference yi x yj . In our approach, we use support vector machines as base learners [19, 24] .\nAt classi cation time, a query x0 \u2208 X is submi ed to the complete ensemble of binary learners. us, a collection of predicted pairwise preference degreesMi, j (x), 1 \u2264 i, j \u2264 K , is obtained. e problem, then, is to turn these pairwise preferences into a ranking of the label set Y. To this end, di erent ranking procedures can be used. e simplest approach is to extend the (weighted) voting procedure that is o en applied in pairwise classi cation [7]: For each label yi , a score\nSi = \u2211\n1\u2264j,i\u2264K Mi, j (x0)\nis derived (whereMi, j (x0) = 1 \u2212Mj,i (x0) for i > j), and then the labels are sorted according to these scores. Despite its simplicity, this ranking procedure has several appealing properties. Apart from its computational e ciency, it turned out to be relatively robust in practice and, moreover, it possesses some provable optimality properties in the case where Spearman\u2019s rank correlation is used as an underlying accuracy measure [11]."}, {"heading": "3.3 Support Vector Machines", "text": "As already said, support vector machines (SVMs) are used as base learners in RPC. SVMs are so-called \u201clarge margin\u201d classi ers [19]. ey separate positive from negative training instances in Rm by means of a linear hyperplane that maximizes the minimum distance of any of the training instances from the hyperplane (decision boundary). Formally, a hyperplane {x |w>x + b = 0} in Rm is characterized by the normal vector w \u2208 Rm and the bias term b \u2208 R. en, encoding the two classes by \u00b11, the margin of a\ntraining example (xi ,yi ) \u2208 Rm \u00d7 {\u22121,+1} is given by yi (w>xi +b); thus, a positive margin indicates that xi is on the right side of the decision boundary, and hence classi ed correctly, whereas a negative margin corresponds to a mistake on the training data.\ne \u201cso margin\u201d version allows for adding a slack variable \u03bei \u2265 0 and de nes the margin asyi (w>xi +b)+\u03bei for each example xi ; this is necessary in the case of data that is not linearly separable. Obviously, the values of the slack variables should be kept small, i.e., the problem comes down to nding a reasonable balance between a large (so ) margin and a small amount of slack. is problem can be formalized in terms of a constrained quadratic optimization problem:\n(w\u2217,b\u2217) = argmin w,b,\u03be { 1 2 \u2016w \u20162 +C N\u2211 i=1 \u03bei } subject to the constraints\nyi (w>xi + b) \u2265 1 \u2212 \u03bei , \u03bei \u2265 0 , (1)\nwhere C is a parameter that controls the penalization of errors on the training data (indicated by a non-zero \u03bei ). Instead of solving this problem directly, it is o en more convenient to solve its dual.\nAt prediction time, a new instance x0 \u2208 Rm is classi ed positive or negative depending on whether it lies above or below the hyperplane (w\u2217,b\u2217). Instead of only returning a binary decision, the distance from the hyperplane is o en reported as kind of measure of certainty (with the idea that the closer an instance to the decision boundary, the less certain the prediction). As a disadvantage of this measure, note that the distance is not normalized and therefore di cult to interpret and compare. So-called Pla scaling is a post-processing step, in which distances are mapped to [0, 1] via a logistic transformation; thus, each instance is assigned a (pseudo-)probability of belonging to the positive class [14].\nIn the dual formulation of the above optimization problem, training instances xi ,x j never occur in isolation but always in the form of inner products \u3008xi ,x j \u3009. is allows for the \u201ckernelization\u201d of SVMs, simply be replacing such inner products by values k(xi ,x j ) of a so-called kernel function k(\u00b7).\nDe nition 3.1. A function k : X \u00d7 X \u2192 R is a positive semide nite kernel i k is symmetric, i.e., k(x ,x \u2032) = k(x \u2032,x), and\nN\u2211 i=1 N\u2211 j=1 cic jk(xi ,x j ) \u2265 0\nfor arbitrary N , arbitrary instances x1, . . . ,xN \u2208 X and arbitrary c1, . . . , cN \u2208 R.\nIf k(\u00b7) is a proper kernel function, one can guarantee the existence of an induced feature space F (which is a Hilbert space) and a feature map \u03d5 : X \u2192 F such that \u3008\u03d5(x),\u03d5(x \u2032)\u3009 = k(xi ,x j ). us, the computation of inner products in the (typically very highdimensional) space F can be replaced by the evaluations of the kernel, which in turn allows a linear model to be t in F without ever accessing that space or computing the image\u03d5(xi ) of a training instance xi\u2014this is called the \u201ckernel trick\u201d. e learning algorithm only requires access to the Gram matrix, i.e., the value of the kernel\nAlgorithm 1 relabel (Graph relabelling) Input:\nG = (N ,E, s, t , \u03c1,\u03c4 ,\u03bd ) graph z : \u03a3\u2217 \u2192 \u03a3 injective compression function \u03b7 : N \u2192 2E neighbour function m iteration bound\nOutput: relabelled graph G\n1: for i = 1 tom do 2: for n \u2208 N do 3: Au\u0434(n) := \u2329 z ( \u03c1(s(e)) \u2295 \u03c4 (e) \u2295 \u03bd (e) ) | e \u2208 \u03b7(n) \u232a 4: Au\u0434(n) := sort(Au\u0434(n)) 5: str (n) := concat(Au\u0434(n)) 6: str (n) := \u03c1(n) \u2295 str (n) 7: \u03c1(n) := z(str (n)) 8: return G\nfor each pair of training instances:\nG = \u00a9\u00ab k(x1,x1) k(x1,x2) . . . k(x1,xN ) k(x2,x1) k(x2,x2) . . . k(x2,xN ) ... ... . . . ... k(xN ,x1) k(xN ,x2) . . . k(xN ,xN ) \u00aa\u00ae\u00ae\u00ae\u00ae\u00ac Note that the instance space X, on which the kernel is de ned, is not necessarily an Euclidean space. Instead, X can be any space or set of objects. In particular, this allows SVMs to be trained on structured (non-vectorial) objects. In general, a kernel function can be interpreted as a kind of similarity measure on X, i.e., the more similar instances xi ,x j , the larger k(xi ,x j ). Next, we address the question of how to de ne appropriate kernel functions on veri cation tasks."}, {"heading": "3.4 Graph Kernels for Veri cation Tasks", "text": "Veri cation tasks are represented by speci c graphs, whence our kernel needs to operate on graphs. A number of graph kernels already exist, for instance based on comparisons of shortest paths or random walks of graphs. However, most of these graph kernels do not scale well to large graphs [21]. As our graphs are representations of programs with several thousands lines of code, and hence very large, we have chosen to proceed from our own kernel development based on Weisfeiler-Lehman subtree kernels [21], which are known to scale be er.\nWeisfeiler-Lehman kernels are extensions of the Weisfeiler-Lehman test of isomorphism between two discretely labelled, undirected graphs [26]. is test basically compares graphs according to their node labels. For taking edges into account, node labels are extended with information about neighbouring nodes in three steps:\nAugmentation: Concatenate label of node n with labels of its neighbours, Sorting: Sort this sequence according to prede ned order on labels, Compression: Compress thus obtained sequences into new labels.\nese steps are repeated until the node label sets of the two graphs di er or until a prede ned bound on the number of iterations is\nexhausted. is bound is used to regulate the depth of subtrees considered. Note that this is a test only, not a proof of isomorphism.\nFor making this Weisfeiler-Lehman test act as a kernel for veri - cation tasks, we made three adaptations to the graph relabelling, giving rise to Algorithm 1:\n(1) extension to directed multigraphs, (2) customization to speci c neighbours of nodes, and (3) integration of edge labels.\nIn Algorithm 1, we use the notation \u3008. . . | . . .\u3009 for list comprehensions, de ning a sequence of values. Moreover, z is the compression function compressing sequences of labels into new labels (which thus needs to be injective). In our case, we use numbers as labels, i.e., \u03a3 = N with the usual ordering \u2264. To this end, we rst map all node identi ers and edge labels toN. Every newly arising sequence then simply gets a new number assigned. e neighbour function \u03b7 is used to customize kernels by selectively choosing the neighbours to be considered during augmentation. ereby, we can specialize our kernels to just control ow or just data dependence edges, for example. e functions sort and concat sort sequences of labels (in ascending order) and concatenate sequences, respectively.\nis lets us nally de ne our kernels for veri cation tasks.\nDe nition 3.2. Let Gi = (Ni ,Ei , si , ti , \u03c1i ,\u03c4i ,\u03bdi ), i = 1, 2 be graph representations of veri cation tasks, z : \u03a3\u2217 \u2192 \u03a3 a compression function,m \u2208 N an iteration bound, d \u2208 N a depth for subtrees and \u03b7i : Ni \u2192 2Ei neighbour functions. e veri cation graph kernel k (d,m) \u03b71,\u03b72,z : GV \u00d7 GV \u2192 R is de ned as\nk (d,m) \u03b71,\u03b72,z (G1,G2) = m\u2211 i=1 kd ( relabel(G1, z,\u03b71,m), relabel(G2, z,\u03b72,m) ) with\nkd (G,G \u2032) = \u2211 n\u2208N \u2211 n\u2032\u2208N \u2032 kd\u03b4 (n,n \u2032) and\nkd\u03b4 (n,n \u2032) = { \u03b4 (\u03c11(n), \u03c12(n\u2032)) if d(n) \u2264 d \u2227 d(n\u2032) \u2264 d 0 else ,\nwhere \u03b4 is a Dirac kernel de ned as \u03b4 (u,w) = 1 if u equals w and 0 otherwise.\nIntuitively, the kernels count the number of equally labelled nodes in all iterations, where the iteration bound steers to what extent subtrees of root ASTs nodes are considered, the neighbour function controls what edges are taken into account, and the depth d xes whether a node is considered at all. For the la er, remember that the depth of a node is its distance to its top-level AST node. By incorporating the depth, we have the option to consider or ignore details of expressions. We can show the following result (for the proof, we refer to [5]):\nTheorem 3.3. e kernel k(d,m)\u03b71,\u03b72,z is positive semi-de nite.\nOur kernels can now be used in a support vector machine within the ranking by pairwise composition approach outlined above."}, {"heading": "4 IMPLEMENTATION AND EXPERIMENTAL EVALUATION", "text": "In our experiments, we studied the performance of our method for rank prediction in the SV-COMP 2015 and 2017. To this end,\nwe compared six variants of our kernel with respect to prediction accuracy, each of which focuses on di erent aspects of a program. Such kind of customization of kernels becomes possible thanks to the two neighbouring functions \u03b71 and \u03b72. In our case, neighbours are chosen according to the type of edge connecting them. We de ne \u03b7` , ` \u2208 {CD,DD, SD,CF} to be \u03b7`(n) = {e | \u03c4 (e) = `\u2227s(e) = n}, and let \u03b7L(n) = \u22c3 `\u2208L \u03b7`(n) for a node n. For our kernels, we always use the same neighbouring function on both graphs. Hence, we will just use the edge labels employed in neighbouring functions as indizes for kernels.\nOur experiments include kernels\n\u2022 k(d,m){CF } (control- ow), \u2022 k(d,m){CD } (control dependencies), \u2022 k(d,m){DD} (data dependencies), \u2022 k(d,m){CD,DD} (control and data dependencies), and \u2022 k(d,m){CF,CD,DD} (control- ow, data- and controldependencies).\nIn addition, we included an equally weighted linear combination k (d,m) l in of some of our kernels, which is de ned as\nk (d,m) l in (G1,G2) = 1 3k (d,m) {CF } (G1,G2) + 1 3k (d,m) {CD} (G1,G2) + 1 3k (d,m) {DD} (G1,G2)\n(one can easily check that this is again a valid kernel, see e.g. [5]). To get an insight on how the prediction accuracy performs compared to state-of-the-art approaches, we also included the accuracy achieved by using the feature vectors from Demyanova et al. [6].\nIn addition, we constructed a default predictor for comparison: the default predictor takes all rankings of the data set used for learning, determines the ranking which minimizes the distance (wrt. Spearman rank correlation) to these rankings and always predicts this default ranking without any learning.\nWe constructed the following data sets for our experiments: SAFETY, TERMINATION, and MEMSAFETY. Each data set consists of several veri cation tasks taken from the SV-COMP 2015 and 2017 benchmark sets. To provide a comprehensive analysis under varying conditions, each data set represents a di erent property type (safety, termination, and memory safety). In case of SV-COMP 2015, SAFETY is a data set of 483 veri cation tasks originating from the SV-COMP categories Loops, BitVectors, Floats, Simple, ControlFlowInteger, and HeapManipulation. For 2017, our SAFETY set consists of 637 veri cation tasks out of the categories ReachSafety-Bitvectors, ReachSafety-ControlFlow, ReachSafety-Heap, ReachSafety-Floats and ReachSafety-Loops. e set of considered tools in SAFETY consists of the tools, which participated in all these categories (6 tools for 2015 and 11 tools for 2017). TERMINATION is a data set of veri cation tasks taken from the category Termination, 393 tasks in 2015 and 507 for 2017. In this data set, we consider tools that participated in this category and successfully proved or disproved termination on at least one veri cation task (5 tools both for 2015 and 2017). MEMSAFETY is the data set of veri cation tasks consisting of tasks from the category MemorySafety, 205 for 2015 and 181 for 2017. Again, we considered only tools that report at least one correct outcome (9 tools in 2015 and 11 tools in 2017).\nFor the computation of our veri cation graphs, we used the con gurable so ware analysis framework CPAChecker [2]: To\nobtain control- ow and AST information, we used the integrated C parser. In case of data dependencies, we utilized the integrated reaching de nition analysis as is described in [10]. For the sake of simplicity, we ignored complex dependencies introduced by pointers. Also according to [10], we computed control dependencies. Eventually, we built another extension of the CPAChecker that combines all the collected information into one graph using the JGraphT library4. To solve our label ranking problem, we integrated the RPC approach and our kernel framework into the scikit-learn library5. ere, we also employed the implementation of support vector machines (with Pla scaling) o ered by scikit-learn. Finally, we integrated the feature vectors of [6] through the tool Verifolio6. All the code and data (of 2015) is available via GitHub7.\nTo examine the prediction accuracy for each con guration, we performed a 10-fold cross-validation. A k-fold cross-validation is a commonly used technique for model assessment. First, the data is divided into k subsets of equal size. en, one subset is used as test set, whereas the learning algorithm trains a model on the remaining k \u2212 1 subsets. is procedure is repeated exactly k times, each time using one of the folds for testing, and the overall performance is obtained as the average of the k test performances thus produced. A er each step of the cross-validation, we compared the actual true rankings on the test sets to the corresponding predicted rankings (with RPC and SVMs) using the Spearman rank correlation. e overall accuracy is then the average over all the accuracies encountered in each step.\nIn Table 2 (SV-COMP 2015) and Table 3 (SV-COMP 2017), we report the average prediction accuracies (and standard deviations) in terms of the Spearman rank correlation; note that an average accuracy of 0 would be obtained by guessing rankings at random, while +1 stands for predictions that perfectly coincide with the true ranking (and \u22121 for completely reversing that ranking). As can be seen, our approach shows a rather strong predictive performance. Depending on the veri cation task, di erent kernels achieve the best results, though the di erences in performance are statistically non-signi cant. More importantly, our approach signi cantly outperforms the one of [6] as well as the default predictor on all tasks. is applies to the data of 2015 as well as 2017.\nTable 4 (SV-COMP 2015) and Table 5 (SV-COMP 2017) show the average training and testing times during the 10-fold cross validation (using the precomputed Gram matrix), i.e., the time in seconds for the training with 9 folds of the input data and the time for computing the rankings (testing) for the remaining fold. As expected, training a model is more time-consuming than using it for prediction. Moreover, like for accuracy, there are no signi - cant di erences between the kernels. Interestingly, the kernels are sometimes even faster than the simple feature representation of [6]."}, {"heading": "5 CONCLUSION", "text": "In the recent years, machine learning has a racted increasing attention in so ware engineering and related elds, where it has been used, for example, in program construction and analysis. In 4h p://jgrapht.org 5h p://scikit-learn.org 6h p://forsyte.at/so ware/verifolio/ 7h ps://github.com/zenscr/PyPRSVT\nthis paper, we have proposed a method for predicting rankings of veri cation tools on given programs. Our rank prediction technique builds on existing methods for label ranking via pairwise comparison. To this end, we have developed an expressive representation of source code, capturing various forms of dependencies between program elements. Instead of explicitly extracting features of programs tailored towards veri cation, we have constructed a kernel that compares programs according to their elements and the connections between them. Due to its generic nature, we speculate that this kernel will also be useful for other sorts of learning problems on programs\u2014a conjecture we shall verify in future work.\nOur approach can be seen as a tool for algorithm selection, a problem that has also been tackled by other authors [6, 23, 27]. Other applications of machine learning include the learning of programs from examples ([13, 15]) and the prediction of properties of programs (e.g., types for program variables [16] or malware in Android apps [17]). Just like our approach, the la er also uses Weisfeiler-Lehman subtree kernels (on CFGs only). A machine learning approach to so ware veri cation itself has recently been proposed in [4]. However, to the best of our knowledge, the use of machine learning for predicting rankings of tools (algorithms) has never been tried so far.\nFor future work, we are planning to generalize our methodology by exploiting properties (features) of veri cation tools, which are only identi ed by their name so far. Recently, a generalization of label ranking called dyad ranking has been proposed, in which not only the instances but also the alternatives to be ranked can be described in terms of properties [18]. As an important advantage of this approach, note that it in principle allows for ranking alternatives with very few or even no training information so far. is becomes possible by generalizing via the feature descriptions (alternatives with similar properties are expected to perform similarly and hence to occupy similar ranks). In our case, this would mean, for example, that predictions can be made for a new veri cation tool that has never been tried so far\u2014provided, of course, meaningful descriptions of such tools are available. Developing corresponding representations is one of the challenges we will address next."}], "references": [{"title": "So\u0089ware Veri\u0080cation and Veri\u0080able Witnesses - (Report on SV-COMP 2015)", "author": ["Dirk Beyer"], "venue": "In TACAS 2015 (LNCS), Christel Baier and Cesare Tinelli (Eds.),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "CPAchecker: A Tool for Con\u0080gurable So\u0089ware Veri\u0080cation", "author": ["Dirk Beyer", "M. Erkan Keremoglu"], "venue": "In Computer Aided Veri\u0080cation ,CAV (Lecture Notes in Computer Science), Ganesh Gopalakrishnan and Shaz Qadeer (Eds.),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "A Training Algorithm for Optimal Margin Classi\u0080ers", "author": ["Bernhard E. Boser", "Isabelle Guyon", "Vladimir Vapnik"], "venue": "In ACM Conference on Computational Learning \u008aeory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "PAC learning-based veri\u0080cation and model synthesis", "author": ["Yu-Fang Chen", "Chiao Hsieh", "Ondrej Leng\u00e1l", "Tsung-Ju Lii", "Ming-Hsien Tsai", "Bow- Yaw Wang", "Farn Wang"], "venue": "In International Conference on So\u0087ware Engineering,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Predicting Rankings of So\u0087ware Veri\u0080cation Tools using Kernels for Structured Data", "author": ["Mike Czech"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Round Robin Classi\u0080cation", "author": ["J. F\u00fcrnkranz"], "venue": "Journal of Machine Learning Research", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Preference Learning and Ranking by Pairwise Comparison", "author": ["Johannes F\u00fcrnkranz", "Eyke H\u00fcllermeier"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "\u008ce Use of Program Dependence Graphs in So\u0089ware Engineering", "author": ["Susan Horwitz", "\u008comas W. Reps"], "venue": "In International Conference on So\u0087ware Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "On predictive accuracy and risk minimization in pairwise label ranking", "author": ["E. H\u00fcllermeier", "J. F\u00fcrnkranz"], "venue": "J. Comput. System Sci", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Label Ranking by Learning Pairwise Preferences", "author": ["E. H\u00fcllermeier", "J. F\u00fcrnkranz", "W. Cheng", "K. Brinker"], "venue": "Arti\u0080cial Intelligence", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Programming by Demonstration: a Machine Learning Approach", "author": ["Tessa Lau"], "venue": "Ph.D. Dissertation. University of Washington", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classi\u0080ers", "author": ["John Pla"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Learning programs from noisy data", "author": ["Veselin Raychev", "Pavol Bielik", "Martin T. Vechev", "Andreas Krause"], "venue": "In Symposium on Principles of Programming Languages, POPL, Rastislav Bod\u0131\u0301k and Rupak Majumdar (Eds.). ACM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Predicting Program Properties from \u201dBig Code", "author": ["Veselin Raychev", "Martin T. Vechev", "Andreas Krause"], "venue": "In Symposium on Principles of Programming  Languages,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "A Machine Learning Approach to Android Malware Detection", "author": ["Justin Sahs", "Latifur Khan"], "venue": "In European Intelligence and Security Informatics Conference,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Dyad Ranking using a Bilinear Placke\u008a- Luce Model. In Proceedings ECML/PKDD\u20132015, European Conference on Machine Learning and Knowledge Discovery in Databases", "author": ["D. Sch\u00e4fer", "E. H\u00fcllermeier"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "AJ. Smola"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Kernel Methods for Pa\u0088ern Analysis", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Weisfeiler-Lehman Graph Kernels", "author": ["Nino Shervashidze", "Pascal Schweitzer", "Erik Jan van Leeuwen", "Kurt Mehlhorn", "Karsten M. Borgwardt"], "venue": "Journal of Machine Learning Research", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "\u008ce proof and measurement of association between two things", "author": ["Charles Spearman"], "venue": "American Journal of Psychology", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1904}, {"title": "MUX: algorithm selection for so\u0089ware model checkers", "author": ["Varun Tulsian", "Aditya Kanade", "Rahul Kumar", "Akash Lal", "Aditya V. Nori"], "venue": "In Conference on Mining So\u0087ware Repositories,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Statistical Learning \u008aeory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Label Ranking Algorithms: A Survey", "author": ["Shankar Vembu", "\u008comas G\u00e4rtner"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "A reduction of a graph to a canonical form and an algebra arising during this reduction", "author": ["Boris Weisfeiler", "A.A. Lehman"], "venue": "Nauchno Technicheskaya Informatsia", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1968}, {"title": "SATzilla: Portfolio-based Algorithm Selection for SAT", "author": ["Lin Xu", "Frank Hu\u008aer", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "CoRR abs/1111.2249", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "the Competition on So\u0089ware Veri\u0080cation SV-COMP [1].", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "Our method for rank prediction builds upon machine learning techniques, more precisely on so-called label ranking algorithms [9] using support vector machines [3] as base learners.", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "Our method for rank prediction builds upon machine learning techniques, more precisely on so-called label ranking algorithms [9] using support vector machines [3] as base learners.", "startOffset": 159, "endOffset": 162}, {"referenceID": 17, "context": "\u008ce key ingredient of our approach is the de\u0080nition of a number of kernels [20], which act as similarity measures on veri\u0080cation tasks.", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "So far, two other machine learning methods for selecting tools or algorithms for veri\u0080cation have been proposed [6, 23], both of them being based on feature vectors: while Tulsian et al.", "startOffset": 112, "endOffset": 119}, {"referenceID": 20, "context": "[23] only employ structural features of programs (like the number of arrays, loops, recursive functions), Demyanova et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] is di\u0081cult due to non-reproducability of their results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "kernel framework [21] that compares graphs not only according to their labels (and how o\u0089en they occur) but also according to associations between labels (via edges in the graph).", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "\u008ce choice of Weisfeiler-Lehman kernels is motivated by their be\u008aer scalability compared to other graph kernels, such as random walk or shortest path kernels (see [21]).", "startOffset": 162, "endOffset": 166}, {"referenceID": 6, "context": "\u008ce ranking is \u0080nally computed by a method for rank prediction via pairwise comparison [9], using support vector machines as base learners.", "startOffset": 86, "endOffset": 89}, {"referenceID": 19, "context": "\u008ce experiments show that our technique can predict rankings with a rather high accuracy, using Spearman\u2019s rank correlation [22] to compare predicted with true rankings.", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "[20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In the two approaches existing so far [6, 23], corresponding features of programs, such as the number of loops, conditionals, pointer variables, or arrays in a program, are de\u0080ned in an explicit way.", "startOffset": 38, "endOffset": 45}, {"referenceID": 7, "context": "Ref variable reference Int Literal Small integer literal in [0,10]", "startOffset": 60, "endOffset": 66}, {"referenceID": 7, "context": "Function Call function call Int Literal Medium integer literal in [10,100]", "startOffset": 66, "endOffset": 74}, {"referenceID": 7, "context": "Program dependence graphs: PDGs [10] represent dependencies between elements in programs.", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "[25]), preferences on Y are \u201ccontextualized\u201d by instances x \u2208 X, where X is an underlying instance space; in our case, instances are programs to be veri\u0080ed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "A common example of such a loss is D(\u03c0 , \u03c0\u0302 ) = 1 \u2212 S(\u03c0 , \u03c0\u0302 ), where S(\u03c0 , \u03c0\u0302 ) is the Spearman rank correlation [22]:", "startOffset": 114, "endOffset": 118}, {"referenceID": 9, "context": "Ranking by pairwise comparison (RPC) is a meta-learning technique that reduces a label ranking task to a set of binary classi\u0080cation problems [12].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "For example, the output of a probabilistic classi\u0080er would be a number in the unit interval [0, 1] that can be interpreted as a probability of the preference yi x yj .", "startOffset": 92, "endOffset": 98}, {"referenceID": 16, "context": "In our approach, we use support vector machines as base learners [19, 24] .", "startOffset": 65, "endOffset": 73}, {"referenceID": 21, "context": "In our approach, we use support vector machines as base learners [19, 24] .", "startOffset": 65, "endOffset": 73}, {"referenceID": 5, "context": "\u008ce simplest approach is to extend the (weighted) voting procedure that is o\u0089en applied in pairwise classi\u0080cation [7]: For each label yi , a score", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "Apart from its computational e\u0081ciency, it turned out to be relatively robust in practice and, moreover, it possesses some provable optimality properties in the case where Spearman\u2019s rank correlation is used as an underlying accuracy measure [11].", "startOffset": 241, "endOffset": 245}, {"referenceID": 16, "context": "SVMs are so-called \u201clarge margin\u201d classi\u0080ers [19].", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "So-called Pla\u008a scaling is a post-processing step, in which distances are mapped to [0, 1] via a logistic transformation; thus, each instance is assigned a (pseudo-)probability of belonging to the positive class [14].", "startOffset": 83, "endOffset": 89}, {"referenceID": 11, "context": "So-called Pla\u008a scaling is a post-processing step, in which distances are mapped to [0, 1] via a logistic transformation; thus, each instance is assigned a (pseudo-)probability of belonging to the positive class [14].", "startOffset": 211, "endOffset": 215}, {"referenceID": 18, "context": "However, most of these graph kernels do not scale well to large graphs [21].", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "As our graphs are representations of programs with several thousands lines of code, and hence very large, we have chosen to proceed from our own kernel development based on Weisfeiler-Lehman subtree kernels [21], which are known to scale be\u008aer.", "startOffset": 207, "endOffset": 211}, {"referenceID": 23, "context": "Weisfeiler-Lehman kernels are extensions of the Weisfeiler-Lehman test of isomorphism between two discretely labelled, undirected graphs [26].", "startOffset": 137, "endOffset": 141}, {"referenceID": 4, "context": "We can show the following result (for the proof, we refer to [5]):", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "[5]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "For the computation of our veri\u0080cation graphs, we used the con\u0080gurable so\u0089ware analysis framework CPAChecker [2]: To", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "In case of data dependencies, we utilized the integrated reaching de\u0080nition analysis as is described in [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "Also according to [10], we computed control dependencies.", "startOffset": 18, "endOffset": 22}, {"referenceID": 20, "context": "Our approach can be seen as a tool for algorithm selection, a problem that has also been tackled by other authors [6, 23, 27].", "startOffset": 114, "endOffset": 125}, {"referenceID": 24, "context": "Our approach can be seen as a tool for algorithm selection, a problem that has also been tackled by other authors [6, 23, 27].", "startOffset": 114, "endOffset": 125}, {"referenceID": 10, "context": "Other applications of machine learning include the learning of programs from examples ([13, 15]) and the prediction of properties of programs (e.", "startOffset": 87, "endOffset": 95}, {"referenceID": 12, "context": "Other applications of machine learning include the learning of programs from examples ([13, 15]) and the prediction of properties of programs (e.", "startOffset": 87, "endOffset": 95}, {"referenceID": 13, "context": ", types for program variables [16] or malware in Android apps [17]).", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": ", types for program variables [16] or malware in Android apps [17]).", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "A machine learning approach to so\u0089ware veri\u0080cation itself has recently been proposed in [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 15, "context": "Recently, a generalization of label ranking called dyad ranking has been proposed, in which not only the instances but also the alternatives to be ranked can be described in terms of properties [18].", "startOffset": 194, "endOffset": 198}], "year": 2017, "abstractText": "So\u0089ware veri\u0080cation competitions, such as the annual SV-COMP, evaluate so\u0089ware veri\u0080cation tools with respect to their e\u0082ectivity and e\u0081ciency. Typically, the outcome of a competition is a (possibly category-speci\u0080c) ranking of the tools. For many applications, such as building portfolio solvers, it would be desirable to have an idea of the (relative) performance of veri\u0080cation tools on a given veri\u0080cation task beforehand, i.e., prior to actually running all tools on the task. In this paper, we present a machine learning approach to predicting rankings of tools on veri\u0080cation tasks. \u008ce method builds upon so-called label ranking algorithms, which we complement with appropriate kernels providing a similarity measure for veri\u0080cation tasks. Our kernels employ a graph representation for so\u0089ware source code that mixes elements of control \u0083ow and program dependence graphs with abstract syntax trees. Using data sets from SV-COMP, we demonstrate our rank prediction technique to generalize well and achieve a rather high predictive accuracy. In particular, our method outperforms a recently proposed feature-based approach of Demyanova et al. (when applied to rank predictions).", "creator": "LaTeX with hyperref package"}}}