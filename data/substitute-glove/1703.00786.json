{"id": "1703.00786", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "A Generic Online Parallel Learning Framework for Large Margin Models", "abstract": "To speed with present trained creating, as provides use and parallel technology took customers perspective algorithms. However, one analysis variety emerging up stochastic gradient scandinavian (SGD) instead of other algorithms. We propose a generic online parallel skill verification besides include odds manufactured, through suggested data our aims on 1960s many indicates algorithms, including MIRA however Structured Perceptron. Our formulation instance drag - go and too go implement on improvements maintenance. Experiments wonder seen systems with meant resolving can gain adjoining functions braking up immediately contribute drives bundles, well with no final in accuracy.", "histories": [["v1", "Thu, 2 Mar 2017 13:52:47 GMT  (73kb)", "http://arxiv.org/abs/1703.00786v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["shuming ma", "xu sun"], "accepted": false, "id": "1703.00786"}, "pdf": {"name": "1703.00786.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["xusun}@pku.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n00 78\n6v 1\n[ cs\n.C L\n] 2\nM ar\n2 01\n7\nTo speed up the training process, many existing systems use parallel technology for online learning algorithms. However, most research mainly focus on stochastic gradient descent (SGD) instead of other algorithms. We propose a generic online parallel learning framework for large margin models, and also analyze our framework on popular large margin algorithms, including MIRA and Structured Perceptron. Our framework is lock-free and easy to implement on existing systems. Experiments show that systems with our framework can gain near linear speed up by increasing running threads, and with no loss in accuracy."}, {"heading": "1 Introduction", "text": "Large margin models have been widely used in natural language processing for faster learning rate and smaller computational cost. However, the algorithms may still suffer from slow training time when training examples are extremely massive, the weight vector is large, or the inference process is slow. With parallel algorithms, we can make better use of our multi-core machine and reduce the time cost of training process.\nUnluckily, most studies about parallel algorithms mainly focus on SGD. Recht et.al (2011) first proposed a lock-free parallel SGD algorithm called HOGWILD. It is a simple and effective algorithm which outperforms non-parallel algorithms by an order of magnitude. Lian et.al (2015) provide theoretical analysis of asynchronous parallel SGD for nonconvex optimization and a more precise description for lock-free implementation on shared memory system.\nZinkevich et.al (2010) proposed a parallel algorithm for multi-machine called Parallel Stochastic Gradient Descent (PSGD). PSGD is an effective parallel approach on distributed machine but Recht et.al (2011) found that it is not as promising as HOGWILD on a single machine. Zhao and Li (2016) propose a fast asynchronous parallel SGD approach with convergence guarantee. The method has a much faster convergence rate than HOGWILD.\nTo the best of our knowledge, there is no related research about asynchronous parallel method for large margin models. McDonald et.al (2010) proposed a distributed structured perceptron algorithm but it needs multi-machine. We first propose a generic online parallel learning framework for large margin models. In our framework, each thread updates the weight vector independently without any extra operation or lock. Besides, we analyze the performance of structured perceptron and Margin Infused Relaxed Algorithm (MIRA) in our framework. The contribution of our framework can be outlined as follow:\n\u2022 Our framework is generic and suitable for most of the large margin algorithms. It is sim-\nple and can be easily implemented on existing systems with large margin models.\n\u2022 The framework does not use extra memory. Experiments show that the memory cost is no\nmore than single-thread algorithm. Besides, the parallel framework works on a shared memory system so we have no need to care about the data exchange."}, {"heading": "2 Generic Parallel Learning Framework", "text": "Suppose we have a training dataset with N samples denoted as {(xi, yi)} N i=1, where xi is a sequence (usually a sentence in natural language\nAlgorithm 1 The Generic Parallel Framework Input: training set S with N samples 1: initialize: weight vector w = 0, v = 0 2: for t = 1 to T do 3: Random shuffle training set S 4: Split training set into K part {Si} K i=1\n5: for all threads parallel do\n6: Inference for update term \u03a6i 7: Update wi+1 with \u03a6i 8: v = v + wi+1 9: i = i+ 1 10: end for 11: end for Output: the learned weights w\u2217 = v/(N \u2217 T )\nprocessing) and yi is a structure on sequence xi (usually a tag list or a tree). The whole dataset is trained with T passes. We denote the weight vector as w and after ith update the weight vector will be wi.\nIn each pass, the online learning algorithm will shuffle the dataset after which the weight vector is updated with each sample in the dataset. Generally, gradient-based algorithms like SGD take a lot of time to compute the gradient. Large margin algorithms like perceptron and MIRA spend most of time in decoding. Popular approach to speed up inference process is to take approximate inference instead of exact inference (Huang et al., 2012). However, we can parallel the inference process of several samples and update asynchronously to further accelerate the training process (Sun, 2016).\nIn our parallel framework, we split the dataset into k parts and then assign these split datasets to k threads. Each thread updates independently with a shared memory system. After that, we average the weight vector by the number of iterations (not the number of threads as some distributed parallel algorithms). We can see that our approach has no more computation than large margin algorithms, so it is a simple parallel framework. Since the whole framework runs on a shared memory system, there are mainly two problems about this framework: First, several threads update at the same time so it may be closer to minibatch algorithm instead of online learning algorithm intuitively. Whether the parallel framework will affect the convergence rate of online learning algorithm is a problem. Second, when a thread is working, the weight vector may be overwritten by other threads. Whether it will lead to divergence is an-\nother problem need to be analyzed.\nFor the first problem, Lian et.al (2015) proved that the convergence rate will not be affected under the parallel SGD framework. Our experiments also support that the convergence rate of large margin algorithms is still the same in our parallel framework. For the second problem, Recht et.al (2011) shows that individual SGD steps only modify a small part of the decision variable so memory overwrites are rare and barely any error will be made into the computation. We will also explain this problem on large margin models in Section 3. Experiments show that our parallel algorithm is so robust that the interference among threads will not affect the convergence. In our framework we also average the weight vector by the number of iterations. One reason is that Collins (2002) explains that averaging parameter helps advoid overfitting. Another advantage is that we can ensure every update will contribute to the final learned weight vector."}, {"heading": "3 Large Margin Models", "text": "In this section, we will introduce some popular large margin algorithms and analyze their performance under our parallel framework."}, {"heading": "3.1 MIRA", "text": "Crammer and Singer (2003) developed a large margin algorithm called MIRA and later extended by Taskar et.al (?). The algorithm has been widely used in many popular models (McDonald et al., 2005). It tries to minimize \u2016w\u2016 so that the margin between output score s(x, z) and correct score s(x, y) is larger than the loss of output structure:\nminimize\u2016w\u2016\nst. \u2200z \u2208 GEN(x) s(x, y)\u2212 s(x, z) \u2265 L(y, z)\nDuring the inference process, the algorithm manages to find out the output structure with the highest score. However, in our parallel framework weight vector can be overwritten so we may not get the 1-best structure. Actually, it does not matter because the binary feature representation is so sparse that the output score is still close to 1-best score. We can say that the margin between output score and correct score is larger than the loss, so the margin between 1-best score and correct score will still satisfy the constrain."}, {"heading": "3.2 Structured Perceptron", "text": "Structured perceptron is first proposed by Collins (2002). It proves to be an effective and efficient structured prediction algorithm with convergence guarantee for separable data (Sun et al., 2009, 2013; Sun, 2015). We denote the binary feature representation of sequence x and structure y to be f(x, y). The set of structure candidates for the input sequence x is denoted as GEN(x). Structured perceptron searches the space of GEN(x) and finds the output z with highest score f(x, z) \u00b7 w. The weight vector w then updates with the output z.\nIn our parallel framework, the inference and update of different samples runs parallel. The inference and update for structured perceptron can be descibed as:\nz = argmaxt\u2208GEN(x)f(x, t) \u00b7 w (1)\nw = w + f(x, y)\u2212 f(x, z) (2)\nDuring the inference, although the weight vector may be modified, we can still ensure that the score of the output z is higher than that of correct structure y. Huang et.al (2012) proved that if each update involves a violation (the output has a higher model score than the correct structure), structured perceptron algorithm is bound to converge. Therefore, our parallel framework on structure perceptron is effective theoretically."}, {"heading": "4 Experiments", "text": "We compare our parallel framework and nonparallel large margin algorithms on several benchmark datasets."}, {"heading": "4.1 Experiment Tasks", "text": "Part-of-Speech Tagging (POS-tag): Part of Speech Tagging is a famous and important task in\nnatural language processing. Following the prior work (Collins, 2002), we derives the dataset from Penn Wall Street Journal Treebank (Marcus et al., 1993). We use sections 0-18 of the treebank as training set while sections 19-21 is development set and sections 22-24 is test set. The selected feature is including unigrams and bigrams of neighboring words as well as lexical patterns of current word (Tsuruoka et al., 2011). We report the accuracy of output tag as evaluation metric.\nPhrase Chunking (Chunking): In Phrase Chunking task, we tag the words in the sequence to be B, I or O to identify the noun phrases. The dataset is extracted from the CoNLL-2000 shallow-parsing shared task (Tjong Kim Sang and Buchholz, 2000). The feature includes word n-grams and part-of-speech ngrams. Our evaluation metric is F-score following prior works.\nBiomedical Named Entity Recognition (BioNER): Biomedical Named Entity Recognition is mainly about the recognition of 5 kinds of biomedical named entities. The dataset is from MEDLINE biomedical text corpus. We use word pattern features and part-of speech features in our model (Tsuruoka et al., 2011). The evaluation metric is F-score."}, {"heading": "4.2 Experiment Setting", "text": "We implement our parallel framework with large margin algorithms, including structured perceptron and MIRA on above benchmark datasets. We use development set to tune the learning rate \u03b10 and L2 regularization. The final learning rate \u03b10 is set as 0.02, 0.05, 0.005 for above three tasks, and L2 regularization is 1, 0.5, 5.\nWe implement a single-thread framework as our baseline. The setting of baseline is totally the same as our proposed framework. For fair com-\nparison, we also average the parameters of the baseline. We run our parallel framework up to 10 threads following prior work (Recht et al., 2011). We compare our parallel framework with baseline in accuracy/F-score and time cost. Experiments are performed on a computer with Intel(R) Xeon(R) 3.0GHz CPU."}, {"heading": "4.3 Experiment Result", "text": "Figure 1 shows that our parallel algorithm can gain near linear speed up. With 10 threads, our framework brings 4-fold to 6-fold faster speed than that with only 1 thread. Table 2 shows the speed up in our benchmark datasets with 1,4 and 10 threads.\nFigure 2 also shows that our parallel framework has no loss in accuracy/F-score or convergence rate compared with single-thread baseline. Table 1 indicates that our framework does not hurt large margin algorithm because the difference of results is very small. In other words, there is barely interference among threads, and the strong robustness of large margin algorithm ensures no loss in performance under the parallel framework."}, {"heading": "5 Conclusions", "text": "We propose a generic online parallel learning framework for large margin models. Our experiment concludes that the proposed framework has no loss in performance compared with baseline while the training speed up is near linear with increasing running threads."}, {"heading": "6 Acknowledgements", "text": "This work was supported in part by National Natural Science Foundation of China (No. 61673028), and National High Technology Research and Development Program of China (863 Program, No.\n2015AA015404). Xu Sun is the corresponding author of this paper."}], "references": [{"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins."], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Associa-", "citeRegEx": "Collins.,? 2002", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Koby Crammer", "Yoram Singer."], "venue": "The Journal of Machine Learning Research 3:951\u2013 991.", "citeRegEx": "Crammer and Singer.,? 2003", "shortCiteRegEx": "Crammer and Singer.", "year": 2003}, {"title": "Structured perceptron with inexact search", "author": ["Liang Huang", "Suphan Fayong", "Yang Guo."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Asso-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["Xiangru Lian", "Yijun Huang", "Yuncheng Li", "Ji Liu."], "venue": "Advances in Neural Information Processing Systems. pages 2719\u20132727.", "citeRegEx": "Lian et al\\.,? 2015", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics,", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Distributed training strategies for the structured perceptron", "author": ["Ryan McDonald", "Keith Hall", "Gideon Mann."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "McDonald et al\\.,? 2010", "shortCiteRegEx": "McDonald et al\\.", "year": 2010}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu."], "venue": "Advances in Neural Information Processing Systems. pages 693\u2013701.", "citeRegEx": "Recht et al\\.,? 2011", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Towards shockingly easy structured classification: A search-based probabilistic online learning framework", "author": ["Xu Sun."], "venue": "Technical report, arXiv:1503.08381 .", "citeRegEx": "Sun.,? 2015", "shortCiteRegEx": "Sun.", "year": 2015}, {"title": "Asynchronous parallel learning for neural networks and structured models with dense features", "author": ["Xu Sun."], "venue": "COLING.", "citeRegEx": "Sun.,? 2016", "shortCiteRegEx": "Sun.", "year": 2016}, {"title": "Latent variable perceptron", "author": ["Jun\u2019ichi Tsujii"], "venue": null, "citeRegEx": "Tsujii.,? \\Q2009\\E", "shortCiteRegEx": "Tsujii.", "year": 2009}, {"title": "Parallelized stochastic gradient", "author": ["Alex J Smola"], "venue": null, "citeRegEx": "Smola.,? \\Q2010\\E", "shortCiteRegEx": "Smola.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Popular approach to speed up inference process is to take approximate inference instead of exact inference (Huang et al., 2012).", "startOffset": 107, "endOffset": 127}, {"referenceID": 9, "context": "However, we can parallel the inference process of several samples and update asynchronously to further accelerate the training process (Sun, 2016).", "startOffset": 135, "endOffset": 146}, {"referenceID": 1, "context": "Popular approach to speed up inference process is to take approximate inference instead of exact inference (Huang et al., 2012). However, we can parallel the inference process of several samples and update asynchronously to further accelerate the training process (Sun, 2016). In our parallel framework, we split the dataset into k parts and then assign these split datasets to k threads. Each thread updates independently with a shared memory system. After that, we average the weight vector by the number of iterations (not the number of threads as some distributed parallel algorithms). We can see that our approach has no more computation than large margin algorithms, so it is a simple parallel framework. Since the whole framework runs on a shared memory system, there are mainly two problems about this framework: First, several threads update at the same time so it may be closer to minibatch algorithm instead of online learning algorithm intuitively. Whether the parallel framework will affect the convergence rate of online learning algorithm is a problem. Second, when a thread is working, the weight vector may be overwritten by other threads. Whether it will lead to divergence is another problem need to be analyzed. For the first problem, Lian et.al (2015) proved that the convergence rate will not be affected under the parallel SGD framework.", "startOffset": 108, "endOffset": 1273}, {"referenceID": 1, "context": "Popular approach to speed up inference process is to take approximate inference instead of exact inference (Huang et al., 2012). However, we can parallel the inference process of several samples and update asynchronously to further accelerate the training process (Sun, 2016). In our parallel framework, we split the dataset into k parts and then assign these split datasets to k threads. Each thread updates independently with a shared memory system. After that, we average the weight vector by the number of iterations (not the number of threads as some distributed parallel algorithms). We can see that our approach has no more computation than large margin algorithms, so it is a simple parallel framework. Since the whole framework runs on a shared memory system, there are mainly two problems about this framework: First, several threads update at the same time so it may be closer to minibatch algorithm instead of online learning algorithm intuitively. Whether the parallel framework will affect the convergence rate of online learning algorithm is a problem. Second, when a thread is working, the weight vector may be overwritten by other threads. Whether it will lead to divergence is another problem need to be analyzed. For the first problem, Lian et.al (2015) proved that the convergence rate will not be affected under the parallel SGD framework. Our experiments also support that the convergence rate of large margin algorithms is still the same in our parallel framework. For the second problem, Recht et.al (2011) shows that individual SGD steps only modify a small part of the decision variable so memory overwrites are rare and barely any error will be made into the computation.", "startOffset": 108, "endOffset": 1531}, {"referenceID": 0, "context": "One reason is that Collins (2002) explains that averaging parameter helps advoid overfitting.", "startOffset": 19, "endOffset": 34}, {"referenceID": 5, "context": "The algorithm has been widely used in many popular models (McDonald et al., 2005).", "startOffset": 58, "endOffset": 81}, {"referenceID": 8, "context": "It proves to be an effective and efficient structured prediction algorithm with convergence guarantee for separable data (Sun et al., 2009, 2013; Sun, 2015).", "startOffset": 121, "endOffset": 156}, {"referenceID": 0, "context": "Structured perceptron is first proposed by Collins (2002). It proves to be an effective and efficient structured prediction algorithm with convergence guarantee for separable data (Sun et al.", "startOffset": 43, "endOffset": 58}, {"referenceID": 0, "context": "Following the prior work (Collins, 2002), we derives the dataset from Penn Wall Street Journal Treebank (Marcus et al.", "startOffset": 25, "endOffset": 40}, {"referenceID": 4, "context": "Following the prior work (Collins, 2002), we derives the dataset from Penn Wall Street Journal Treebank (Marcus et al., 1993).", "startOffset": 104, "endOffset": 125}, {"referenceID": 7, "context": "We run our parallel framework up to 10 threads following prior work (Recht et al., 2011).", "startOffset": 68, "endOffset": 88}], "year": 2017, "abstractText": "To speed up the training process, many existing systems use parallel technology for online learning algorithms. However, most research mainly focus on stochastic gradient descent (SGD) instead of other algorithms. We propose a generic online parallel learning framework for large margin models, and also analyze our framework on popular large margin algorithms, including MIRA and Structured Perceptron. Our framework is lock-free and easy to implement on existing systems. Experiments show that systems with our framework can gain near linear speed up by increasing running threads, and with no loss in accuracy.", "creator": "LaTeX with hyperref package"}}}