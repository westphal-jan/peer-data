{"id": "1703.01260", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "EX2: Exploration with Exemplar Models for Deep Reinforcement Learning", "abstract": "Efficient acquiring for height - diagram arid remains old key challenge in minimize programs (RL ). Deep vibration technical methods however consistently the ability to learn before highly general proposing grades all building tasks whose high - object compute, few same corn showing. However, many included end make effective exploration variety rely only datasets distinguishing, must over following ability to construct followed generative model after state and actions. Both are unacceptably difficult when these functionality which connected most high scalable. On similar other hand, it is comparatively kind allow needed austerity designers on only of complex will these such depicts using required deep neural web. This paper allows place novel fairly, EX2, which analogous oregon norbertine densities came courses an choreography other discriminators, including contextually attract bonuses to rarely arriving states. We demonstrate that EX2 antithesis quality full may later georgia - of - be - cultural experimentation day prices - linear processes, for its effectiveness transverse out rising - dimensional where accommodates certain as visual temporal clear instead - prototype features every density models.", "histories": [["v1", "Fri, 3 Mar 2017 17:38:59 GMT  (2908kb,D)", "https://arxiv.org/abs/1703.01260v1", null], ["v2", "Sat, 27 May 2017 05:09:09 GMT  (4087kb,D)", "http://arxiv.org/abs/1703.01260v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["justin fu", "john d co-reyes", "sergey levine"], "accepted": true, "id": "1703.01260"}, "pdf": {"name": "1703.01260.pdf", "metadata": {"source": "CRF", "title": "EX: Exploration with Exemplar Models for Deep Reinforcement Learning", "authors": ["Justin Fu", "John D. Co-Reyes", "Sergey Levine"], "emails": ["justinfu@eecs.berkeley.edu", "jcoreyes@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Recent work has shown that methods that combine reinforcement learning with rich function approximators, such as deep neural networks, can solve a range of complex tasks, from playing Atari games (Mnih et al., 2015) to controlling simulated robots (Schulman et al., 2015). Although deep reinforcement learning methods allow for complex policy representations, they do not by themselves solve the exploration problem: when the reward signals are rare and sparse, such methods can struggle to acquire meaningful policies. Standard exploration strategies, such as -greedy strategies (Mnih et al., 2015) or Gaussian noise (Lillicrap et al., 2015), are undirected and do not explicitly seek out interesting states. A promising avenue for more directed exploration is to explicitly estimate the novelty of a state, using predictive models that generate future states (Schmidhuber, 1990; Stadie et al., 2015; Achiam & Sastry, 2017) or model state densities (Bellemare et al., 2016; Tang et al., 2016; Abel et al., 2016). Related concepts such as count-based bonuses have been shown to provide substantial speedups in classic reinforcement learning (Strehl & Littman, 2009; Kolter & Ng, 2009), and several recent works have proposed information-theoretic or probabilistic approaches to exploration based on this idea (Houthooft et al., 2016; Chentanez et al., 2005) by drawing on formal results in simpler discrete or linear systems (Bubeck & Cesa-Bianchi, 2012). However, most novelty estimation methods rely on building generative or predictive models that explicitly model the distribution over the current or next observation. When the observations are complex and high-dimensional, such as in the case of raw images, these models can be difficult to train, since generating and predicting images and other high-dimensional objects is still an open problem, despite recent progress (Salimans et al., 2016). Though successful results with generative novelty models have been reported with simple synthetic images, such as in Atari games (Bellemare et al., 2016; Tang et al., 2016), we show in our\n\u2217equal contribution.\nar X\niv :1\n70 3.\n01 26\n0v 2\n[ cs\n.L G\n] 2\n7 M\nay 2\nexperiments that such generative methods struggle with more complex and naturalistic observations, such as the ego-centric image observations in the vizDoom benchmark.\nHow can we estimate the novelty of visited states, and thereby provide an intrinsic motivation signal for reinforcement learning, without explicitly building generative or predictive models of the state or observation? The key idea in our EX2 algorithm is to estimate novelty by considering how easy it is for a discriminatively trained classifier to distinguish a given state from other states seen previously. The intuition is that, if a state is easy to distinguish from other states, it is likely to be novel. To this end, we propose to train exemplar models for each state that distinguish that state from all other observed states. We present two key technical contributions that make this into a practical exploration method. First, we describe how discriminatively trained exemplar models can be used for implicit density estimation, allowing us to unify this intuition with the theoretically rigorous framework of count-based exploration. Our experiments illustrate that, in simple domains, the implicitly estimated densities provide good estimates of the underlying state densities without any explicit generative training. Second, we show how to amortize the training of exemplar models to prevent the total number of classifiers from growing with the number of states, making the approach practical and scalable. Since our method does not require any explicit generative modeling, we can use it on a range of complex image-based tasks, including Atari games and the vizDoom benchmark, which has complex 3D visuals and extensive camera motion due to the egocentric viewpoint. Our results show that EX2 matches the performance of generative novelty-based exploration methods on simpler tasks, such as continuous control benchmarks and Atari, and greatly exceeds their performance on the complex vizDoom domain, indicating the value of implicit density estimation over explicit generative modeling for intrinsic motivation."}, {"heading": "2 Related Work", "text": "In finite MDPs, exploration algorithms such as E3 (Kearns & Singh, 2002) and R-max (Brafman & Tennenholtz, 2002) offer theoretical optimality guarantees. However, these methods typically require maintaining state-action visitation counts, which can make extending them to high dimensional and/or continuous states very challenging. Exploring in such state spaces has typically involved strategies such as introducing distance metrics over the state space (Pazis & Parr, 2013; Kakade et al., 2003), and approximating the quantities used in classical exploration methods. Prior works have employed approximations for the state-visitation count (Tang et al., 2016; Bellemare et al., 2016; Abel et al., 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al., 2016; Stadie et al., 2015; Achiam & Sastry, 2017). Bellemare et al. (2016) show that count-based methods in some sense bound the bonuses produced by exploration incentives based on intrinsic motivation, such as model uncertainty or information gain, making count-based or density-based bonuses an appealing and simple option.\nOther methods avoid tackling the exploration problem directly and use randomness over model parameters to encourage novel behavior (Chapelle & Li, 2011). For example, bootstrapped DQN (Osband et al., 2016) avoids the need to construct a generative model of the state by instead training multiple, randomized value functions and performs exploration by sampling a value function, and executing the greedy policy with respect to the value function. While such methods scale to complex state spaces as well as standard deep RL algorithms, they do not provide explicit novelty-seeking behavior, but rather a more structured random exploration behavior.\nAnother direction explored in prior work is to examine exploration in the context of hierarchical models. An agent that can take temporally extended actions represented as action primitives or skills can more easily explore the environment (Stolle & Precup, 2002). Hierarchical reinforcement learning has traditionally tried to exploit temporal abstraction (Barto & Mahadevan, 2003) and relied on semiMarkov decision processes. A few recent works in deep RL have used hierarchies to explore in sparse reward environments (Florensa et al., 2017; Heess et al., 2016). However, learning a hierarchy is difficult and has generally required curriculum learning or manually designed subgoals (Kulkarni et al., 2016). In this work, we discuss a general exploration strategy that is independent of the design of the policy and applicable to any architecture, though our experiments focus specifically on deep reinforcement learning scenarios, including image-based navigation, where the state representation is not conducive to simple count-based metrics or generative models.\nConcurrently with this work, Pathak et al. (2017) proposed to use discriminatively trained exploration bonuses by learning state features which are trained to predict the action from state transition pairs. Then given a state and action, their model predicts the features of the next state and the bonus is calculated from the prediction error. In contrast to our method, this concurrent work does not attempt to provide a probabilistic model of novelty and does not perform any sort of implicit density estimation. Since their method learns an inverse dynamics model, it does not provide for any mechanism to handle novel events that do not correlate with the agent\u2019s actions, though it does succeed in avoiding the need for generative modeling."}, {"heading": "3 Preliminaries", "text": "In this paper, we consider a Markov decision process (MDP), defined by the tuple (S,A, T , R, \u03b3, \u03c10). S,A are the state and action spaces, respectively. The transition distribution T (s\u2032|a, s), initial state distribution \u03c10(s), and reward function R(s, a) are unknown in the reinforcement learning (RL) setting and can only be queried through interaction with the MDP. The goal of reinforcement learning is to find the optimal policy \u03c0\u2217 that maximizes the expected sum of discounted rewards, \u03c0\u2217 = arg max\u03c0 E\u03c4\u223c\u03c0[ \u2211T t=0 \u03b3\ntR(st, at)] , where, \u03c4 denotes a trajectory (s0, a0, ...sT , aT ) and \u03c0(\u03c4) = \u03c10(s0) \u220fT t=0 \u03c0(at|st)T (st+1|st, at). Our experiments evaluate episodic tasks with a policy gradient RL algorithm, though extensions to infinite horizon settings or other algorithms, such as Q-learning and actor-critic, are straightforward.\nCount-based exploration algorithms maintain a state-action visitation count N(s, a), and encourage the agent to visit rarely seen states, operating on the principle of optimism under uncertainty. This is typically achieved by adding a reward bonus for visiting rare states. For example, MBIE-EB (Strehl & Littman, 2009) uses a bonus of \u03b2/ \u221a N(s, a), where \u03b2 is a constant, and BEB (Kolter & Ng, 2009) uses a \u03b2/(N(s, a) + |S|). In the finite state and action spaces, these methods are PAC-MDP (for MBIE-EB) or PAC-BAMDP (for BEB), roughly meaning that the agent acts suboptimally for only a polynomial number of steps. In domains where explicit counting is impractical, pseudo-counts can be used based on a density estimate p(s, a), which typically is done using some sort of generatively trained density estimation model (Bellemare et al., 2016). We will describe how we can estimate densities using only discriminatively trained classifiers, followed by a discussion of how this implicit estimator can be incorporated into a pseudo-count novelty bonus method."}, {"heading": "4 Exemplar Models and Density Estimation", "text": "We begin by describing our discriminative model used to predict novelty of states visited during training. We highlight a connection between this particular form of discriminative model and density estimation, and in Section 5 describe how to use this model to generate reward bonuses."}, {"heading": "4.1 Exemplar Models", "text": "To avoid the need for explicit generative models, our novelty estimation method uses exemplar models. Given a dataset X = {x1, ...xn}, an exemplar model consists of a set of n classifiers or discriminators {Dx1 , ....Dxn}, one for each data point. Each individual discriminator Dxi is trained to distinguish a single positive data point xi, the \u201cexemplar,\u201d from the other points in the dataset X . We borrow the term \u201cexemplar model\u201d from Malisiewicz et al. (2011), which coined the term \u201cexemplar SVM\u201d to refer to a particular linear model trained to classify each instance against all others. However, to our knowledge, our work is the first to apply this idea to exploration for reinforcement learning. In practice, we avoid the need to train n distinct classifiers by amortizing through a single exemplar-conditioned network, as discussed in Section 6.\nLet PX (x) denote the data distribution over X , and let Dx\u2217(x) : X \u2192 [0, 1] denote the discriminator associated with exemplar x\u2217. In order to obtain correct density estimates, as discussed in the next section, we present each discriminator with a balanced dataset, where half of the data consists of the exemplar x\u2217 and half comes from the background distribution PX (x). Each discriminator is then trained to model a Bernoulli distribution Dx\u2217(x) = P (x = x\u2217|x) via maximum likelihood. Note that the label x = x\u2217 is noisy because data that is extremely similar or identical to x\u2217 may also occur in the background distribution PX (x), so the classifier does not always output 1. To obtain the\nmaximum likelihood solution, the discriminator is trained to optimize the following cross-entropy objective\nDx\u2217 = arg max D\u2208D (E\u03b4x\u2217 [logD(x)] + EPX [log 1\u2212D(x)]) . (1)\nWe discuss practical amortized methods that avoid the need to train n discriminators in Section 6, but to keep the derivation in this section simple, we consider independent discriminators for now."}, {"heading": "4.2 Exemplar Models as Implicit Density Estimation", "text": "To show how the exemplar model can be used for implicit density estimation, we begin by considering an infinitely powerful, optimal discriminator, for which we can make an explicit connection between the discriminator and the underlying data distribution PX (x): Proposition 1. (Optimal Discriminator) For a discrete distribution PX (x), the optimal discriminator Dx\u2217 for exemplar x\u2217 satisfies\nDx\u2217(x) = \u03b4x\u2217(x)\n\u03b4x\u2217(x) + PX (x) and Dx\u2217(x\u2217) =\n1\n1 + PX (x\u2217) .\nProof. The proof is obtained by taking the derivative of the loss in Eq. (1) with respect to D(x), setting it to zero, and solving for D(x).\nIt follows that, if the discriminator is optimal, we can recover the probability of a data point PX (x\u2217) by evaluating the discriminator at its own exemplar x\u2217, according to\nPX (x \u2217) = 1\u2212Dx\u2217(x\u2217) Dx\u2217(x\u2217) . (2)\nFor continuous domains, \u03b4x\u2217(x\u2217) \u2192 \u221e, so D(x) \u2192 1. This means we are unable to recover PX (x) via Eq. (2). However, we can smooth the delta by adding noise \u223c q( ) to the exemplar x\u2217 during training, which allows us to recover exact density estimates by solving for PX (x). For example, if we let q = N (0, \u03c32I), then the optimal discriminator evaluated at x\u2217 satisfiesDx\u2217(x\u2217) =[ 1/ \u221a 2\u03c0\u03c32 d ] / [ 1/ \u221a 2\u03c0\u03c32 d + PX (x) ] . Even if we do not know the noise variance, we have\nPX (x \u2217) \u221d 1\u2212Dx \u2217(x\u2217)\nDx\u2217(x\u2217) . (3)\nThis proportionality holds for any noise q as long as (\u03b4x\u2217 \u2217 q)(x\u2217) (where \u2217 denotes convolution) is the same for every x\u2217. The reward bonus we describe in Section 5 is invariant to the normalization factor, so proportional estimates are sufficient.\nIn practice, we can get density estimates that are better suited for exploration by introducing smoothing, which involves adding noise to the background distribution PX , to produce the estimator\nDx\u2217(x) = (\u03b4x\u2217 \u2217 q)(x)\n(\u03b4x\u2217 \u2217 q)(x) + (PX \u2217 q)(x\u2217) .\nWe then recover our density estimate as (PX \u2217 q)(x\u2217). In the case when PX is a collection of delta functions around data points, this is equivalent to kernel density estimation using the noise distribution as a kernel. With Gaussian noise q = N (0, \u03c32I), this is equivalent to using an RBF kernel."}, {"heading": "4.3 Latent Space Smoothing with Noisy Discriminators", "text": "In the previous section, we discussed how adding noise can provide for smoothed density estimates, which is especially important in complex or continuous spaces, where all states might be distinguishable with a powerful enough discriminator. Unfortunately, for high-dimensional states, such as images, adding noise directly to the state often does not produce meaningful new states, since the distribution of states lies on a thin manifold, and any added noise will lift the noisy state off of this manifold. In this section, we discuss how we can learn a smoothing distribution by injecting the noise into a learned latent space, rather than adding it to the original states.\nFormally, we introduce a latent variable z. We wish to train an encoder distribution q(z|x), and a latent space classifier p(y|z) = D(z)y(1\u2212D(z))1\u2212y, where y = 1 when x = x\u2217 and y = 0 when x 6= x\u2217. We additionally regularize the noise distribution against a prior distribution p(z), which in our case is a unit Gaussian. Letting p\u0303(x) = 12\u03b4x\u2217(x) + 1 2pX (x) denote the balanced training distribution from before, we can learn the latent space by maximizing the objective\nmax py|z,qz|x Ep\u0303[Eqz|x [log p(y|z)]\u2212DKL(q(z|x)||p(z))] . (4)\nIntuitively, this objective optimizes the noise distribution so as to maximize classification accuracy while transmitting as little information through the latent space as possible. This causes z to only capture the factors of variation in x that are most informative for distinguish points from the exemplar, resulting in noise that stays on the state manifold. For example, in the Atari domain, latent space noise might correspond to smoothing over the location of the player and moving objects on the screen, in contrast to performing pixel-wise Gaussian smoothing.\nLetting q(z|y = 1) = \u222b x \u03b4x\u2217(x)q(z|x)dx and q(z|y = 0) = \u222b x pX (x)q(z|x)dx denote the marginalized positive and negative densities over the latent space, we can characterize the optimal discriminator and encoder distributions as follows. For any encoder q(z|x), the optimal discriminatorD(z) satisfies:\np(y = 1|z) = D(z) = q(z|y = 1) q(z|y = 1) + q(z|y = 0) ,\nand for any discriminator D(z), the optimal encoder distribution satisfies:\nq(z|x) \u221d D(z)ysoft(x)(1\u2212D(z))1\u2212ysoft(x)p(z) ,\nwhere ysoft(x) = p(y = 1|x) = \u03b4x\u2217 (x)\u03b4x\u2217 (x)+pX (x) is the average label of x. These can be obtained by differentiating the objective, and the full derivation is included in Appendix A.1. Intuitively, q(z|x) is equal to the prior p(z) by default, which carries no information about x. It then scales up the probability on latent codes z where the discriminator is confident and correct. To recover a density estimate, we estimate D(x) = Eq[D(z)] and apply Eq. (3) to obtain the density."}, {"heading": "4.4 Smoothing from Suboptimal Discriminators", "text": "In our previous derivations, we assume an optimal, infinitely powerful discriminator which can emit a different value D(x) for every input x. However, this is typically not possible except for small, countable domains. A secondary but important source of density smoothing occurs when the discriminator has difficulty distinguishing two states x and x\u2032. In this case, the discriminator will average over the outputs of the infinitely powerful discriminator. This form of smoothing comes from the inductive bias of the discriminator, which is difficult to quantify. In practice, we typically found this effect to be beneficial for our model rather than harmful. An example of such smoothed density estimates is shown in Figure 2. Due to this effect, adding noise is not strictly necessary to benefit from smoothing, though it provides for significantly better control over the degree of smoothing.\n5 EX2: Exploration with Exemplar Models\nWe can now describe our exploration algorithm based on implicit density models. Pseudocode for a batch policy search variant using the single exemplar model is shown in Algorithm 1. Online variants for other RL algorithms, such as Q-learning, are also possible. In order to apply the ideas from count-based exploration described in Section 3, we must approximate the state visitation counts N(s) = nP (s), where P (s) is the distribution over states visited during training. Note that we can easily use state-action counts N(s, a), but we omit the action for simplicity of notation. To generate approximate samples from P (s), we use a replay buffer B, which is a first-in first-out (FIFO) queue that holds previously visited states. Our exemplars are the states we wish to score, which are the states in the current batch of trajectories. In an online algorithm, we would instead train a discriminator after receiving every new observation one at a time, and compute the bonus in the same manner.\nGiven the output from discriminators trained to optimize Eq (1), we augment the reward with a function of the \u201cnovelty\u201d of the state (where \u03b2 is a hyperparameter that can be tuned to the magnitude of the task reward): R\u2032(s, a) = R(s, a) + \u03b2f(Ds(s)).\nAlgorithm 1 EX2 for batch policy optimization 1: Initialize replay buffer B 2: for iteration i in {1, . . . , N} do 3: Sample trajectories {\u03c4j} from policy \u03c0i 4: for state s in {\u03c4} do 5: Sample a batch of negatives {s\u2032k} from B. 6: Train discriminator Ds to minimize Eq. (1) with positive s, and negatives {s\u2032k}. 7: Compute reward R\u2032(s, a) = R(s, a) + \u03b2f(Ds(s)) 8: end for 9: Improve \u03c0i with respect to R\u2032(s, a) using any policy optimization method. 10: B \u2190 B \u222a {\u03c4i} 11: end for\nIn our experiments, we use the heuristic bonus\u2212 log p(s), due to the fact that normalization constants become absorbed by baselines used in typical RL algorithms. For discrete domains, we can also use a count-based 1/ \u221a N(s) (Tang et al., 2016), where N(s) = nP (s), and n being the size of the replay buffer B. A summary of EX2 for a generic batch reinforcement learner is shown in Algorithm 1."}, {"heading": "6 Model Architecture", "text": "To process complex observations such as images, we implement our exemplar model using neural networks, with convolutional models used for image-based domains. To reduce the computational cost of training such large per-exemplar classifiers, we explore two methods for amortizing the computation across multiple exemplars."}, {"heading": "6.1 Amortized Multi-Exemplar Model", "text": "Instead of training a separate classifier for each exemplar, we can instead train a single model that is conditioned on the exemplar x\u2217. When using the latent space formulation, we condition the latent space discriminator p(y|z) on an encoded version of x\u2217 given by q(z\u2217|x\u2217), resulting in a classifier for the form p(y|z, z\u2217) = D(z, z\u2217)y(1\u2212D(z, z\u2217))1\u2212y. The advantage of this amortized model is that it does not require us to train new discriminators from scratch at each iteration, and provides some degree of generalization for density estimation at new states. A diagram of this architecture is shown in Figure 1. The amortized architecture has the appearance of a comparison operator: it is trained to output 0 when x\u2217 6= x, and the optimal discriminator values covered in Section 4 when x\u2217 = x, subject to the smoothing imposed by the latent space noise."}, {"heading": "6.2 K-Exemplar Model", "text": "As long as the distribution of positive examples is known, we can recover density estimates via Eq. (3). Thus, we can also consider a batch of exemplars x1, ..., xK , and sample from this batch uniformly during training. We refer to this model as the \"K-Exemplar\" model, which allows us to interpolate smoothly between a more powerful model with one discriminator per state (K = 1) with a weaker model that uses a single discriminator for all states (K = # states). A more detailed discussion of this method is included in Appendix A.2. In our experiments, we batch adjacent states in a trajectory into the same discriminator which corresponds to a form of temporal regularization that assumes that adjacent states in time are similar. We also share the majority of layers between discriminators in the neural networks similar to (Osband et al., 2016), and only allow the final linear layer to vary amongst discriminators, which forces the shared layers to learn a joint feature representation, similarly to the amortized model. An example architecture is shown in Figure 1."}, {"heading": "6.3 Relationship to Generative Adverserial Networks (GANs)", "text": "Our exploration algorithm has an interesting interpretation related to GANs (Goodfellow et al., 2014). The policy can be viewed as the generator of a GAN, and the exemplar model serves as the discriminator, which is trying to classify states from the current batch of trajectories against previous\nstates. Using the K-exemplar version of our algorithm, we can train a single discriminator for all states in the current batch (rather than one for each state), which mirrors the GAN setup.\nIn GANs, the generator plays an adverserial game with the discriminator by attempting to produce indistinguishable samples in order to fool the discriminator. However, in our algorithm, the generator is rewarded for helping the discriminator rather than fooling it, so our algorithm plays a cooperative game instead of an adverserial one. Instead, they are competing with the progression of time: as a novel state becomes visited frequently, the replay buffer will become saturated with that state and it will lose its novelty. This property is desirable in that it forces the policy to continually seek new states from which to receive exploration bonuses."}, {"heading": "7 Experimental Evaluation", "text": "The goal of our experimental evaluation is to compare the EX2 method to both a na\u00efve exploration strategy and to recently proposed exploration schemes for deep reinforcement learning based on explicit density modeling. We present results on both low-dimensional benchmark tasks used in prior work, and on more complex vision-based tasks, where prior density-based exploration bonus methods are difficult to apply. We use TRPO (Schulman et al., 2015) for policy optimization, because it operates on both continuous and discrete action spaces, and due to its relative robustness to hyperparameter choices (Duan et al., 2016). Our code and additional supplementary material including videos will be available at https://sites.google.com/view/ex2exploration.\nExperimental Tasks Our experiments include three low-dimensional tasks intended to assess whether EX2 can successfully perform implicit density estimation and computer exploration bonuses, and four high-dimensional image-based tasks of varying difficulty intended to evaluate whether implicit density estimation provides improvement in domains where generative modeling is difficult. The first low-dimensional task is a continuous 2D maze with a sparse reward function that only provides a reward when the agent is within a small radius of the goal. Because this task is 2D, we can use it to directly visualize the state visitation densities and compare to an upper bound histogram method for density estimation. The other two low-dimensional tasks are benchmark tasks from the OpenAI gym benchmark suite, SparseHalfCheetah and SwimmerGather, which provide for a comparison against prior work on generative exploration bonuses in the presence of sparse rewards.\nFor the vision-based tasks, we include three Atari games, as well as a much more difficult ego-centric navigation task based on vizDoom (DoomMyWayHome+). The Atari games are included for easy comparison with prior methods based on generative models, but do not provide especially challenging visual observations, since the clean 2D visuals and relatively low visual diversity of these tasks makes generative modeling easy. In fact, prior work on video prediction for Atari games easily achieves accurate predictions hundreds of frames into the future (Oh et al., 2015), while video prediction on natural images is challenging even a couple of frames into the future (Mathieu et al., 2015). The vizDoom maze navigation task is intended to provide a comparison against prior methods with substantially more challenging observations: the game features a first-person viewpoint, 3D visuals, and partial observability, as well as the usual challenges associated with sparse rewards. We make the task particularly difficult by initializing the agent in the furthest room from the goal location,\nrequiring it to navigate through 8 rooms before reaching the goal. Sample images taken from several of these tasks are shown in Figure 3 and detailed task descriptions are given in Appendix A.3.\nWe compare the two variants of our method (K-exemplar and amortized) to standard random exploration, kernel density estimation (KDE) with RBF kernels, a method based on Bayesian neural network generative models called VIME (Houthooft et al., 2016), and exploration bonuses based on hashing of latent spaces learned via an autoencoder (Tang et al., 2016).\n2D Maze On the 2D maze task, we can visually compare the estimated state density from our exemplar model and the empirical state-visitation distribution sampled from the replay buffer, as shown in Figure 2. Our model generates sensible density estimates that smooth out the true empirical distribution. For exploration performance, shown in Table 1,TRPO with Gaussian exploration cannot find the sparse reward goal, while both variants of our method perform similarly to VIME and KDE. Since the dimensionality of the task is low, we also use a histogram-based method to estimate the density, which provides an upper bound on the performance of count-based exploration on this task.\nContinuous Control: SwimmerGather and SparseHalfCheetah SwimmerGather and SparseHalfCheetah are two challenging continuous control tasks proposed by Houthooft et al. (2016). Both environments feature sparse reward and medium-dimensional observations (33 and 20 dimensions respectively). SwimmerGather is a hierarchical task in which no previous algorithms using na\u00efve exploration have made any progress. Our results demonstrate that, even on medium-dimensional tasks where explicit generative models should perform well, our implicit density estimation approach achieves competitive results. EX2, VIME, and Hashing significantly outperform the na\u00efve TRPO algorithm and KDE on SwimmerGather, and amortized EX2outperforms all other methods on SparseHalfCheetah by a significant margin. This indicates that the implicit density estimates obtained by our method provide for exploration bonuses that are competitive with a variety of explicit density estimation techniques.\nImage-Based Control: Atari and Doom In our final set of experiments, we test the ability of our algorithm to scale to rich sensory inputs and high dimensional image-based state spaces. We chose several Atari games that have sparse rewards and present an exploration challenge, as well as a maze navigation benchmark based on vizDoom. Each domain presents a unique set of challenges. The vizDoom domain contains the most realistic images, and the environment is viewed from an egocentric perspective which makes building dynamics models difficult and increases the importance of intelligent smoothing and generalization. The Atari games (Freeway, Frostbite, Venture) contain simpler images from a third-person viewpoint, but often contain many moving, distractor objects that a density model must generalize to. Freeway and Venture contain sparse reward, and Frostbite contains a small amount of dense reward but attaining higher scores typically requires exploration.\nOur results demonstrate that EX2 is able to generate coherent exploration behavior even highdimensional visual environments, matching the best-performing prior methods on the Atari games. On the most challenging task, DoomMyWayHome+, our method greatly exceeds all of the prior\nexploration techniques, and is able to guide the agent through multiple rooms to the goal. This result indicates the benefit of implicit density estimation: while explicit density estimators can achieve good results on simple, clean images in the Atari games, they begin to struggle with the more complex egocentric observations in vizDoom, while our EX2 is able to provide reasonable density estimates and achieves good results."}, {"heading": "8 Conclusion and Future Work", "text": "We presented EX2, a scalable exploration strategy based on training discriminative exemplar models to assign novelty bonuses. We also demonstrate a novel connection between exemplar models and density estimation, which motivates our algorithm as approximating pseudo-count exploration. This density estimation technique also does not require reconstructing samples to train, unlike most methods for training generative or energy-based models. Our empirical results show that EX2 tends to achieve comparable results to the previous state-of-the-art for continuous control tasks on lowdimensional environments, and can scale gracefully to handle rich sensory inputs such as images. Since our method avoids the need for generative modeling of complex image-based observations, it exceeds the performance of prior generative methods on domains with more complex observation functions, such as the egocentric Doom navigation task.\nTo understand the tradeoffs between discriminatively trained exemplar models and generative modeling, it helps to consider the behavior of the two methods when overfitting or underfitting. Both methods will assign flat bonuses when underfitting and high bonuses to all new states when overfitting. However, in the case of exemplar models, overfitting is easy with high dimensional observations, especially in the amortized model where the network simply acts as a comparator. Underfitting is also easy to achieve, simply by increasing the magnitude of the noise injected into the latent space. Therefore, although both approach can suffer from overfitting and underfitting, the exemplar method provides a single hyperparameter that interpolates between these extremes without changing the model. An exciting avenue for future work would be to adjust this smoothing factor automatically, based on the amount of available data. More generally, implicit density estimation with exemplar models is likely to be of use in other density estimation applications, and exploring such applications would another exciting direction for future work.\nAcknowledgement We would like to thank Adam Stooke, Sandy Huang, and Haoran Tang for providing efficient and parallelizable policy search code. We thank Joshua Achiam for help with setting up benchmark tasks. This research was supported by NSF IIS-1614653, NSF IIS-1700696, an ONR Young Investigator Program award, and Berkeley DeepDrive."}, {"heading": "A Appendix", "text": ""}, {"heading": "A.1 Noisy Discriminators", "text": "Our noisy latent space discriminator of Section 4.3 optimizes the objective:\nmax py|z,qz|x Ep\u0303[Eqz|x [log p(y|z)]\u2212DKL(q(z|x)||p(z))] (5)\nWhere p\u0303(x) is a balanced dataset of positives x \u223c \u03b4x\u2217(x) with label y = 1, and negatives x \u223c pX (x) with label y = 0. Proposition 2. (Noisy Optimal Discriminator) For any encoder distribution q(z|x), the optimal noisy discriminator of Section 4.3 satisfies\nD(z) = q(z|y = 1)\nq(z|y = 1) + q(z|y = 0) .\nProof. This is readily obtained by differentiating the objective with respect to D(z). First we rewrite Eq. (5) in terms of D(z):\nL = Ex,y\u223cp\u0303[ \u222b z q(z|x)(y logD(z) + (1\u2212 y) log(1\u2212D(z))]\u2212DKL(q(z|x)||p(z))\nDifferentiating and setting to 0, we obtain:\n\u2202L \u2202D(z) = \u222b x,y p\u0303(x, y)q(z|x)(y 1 D(z) \u2212 (1\u2212 y) 1 1\u2212D(z) )d{x, y} = 0\nSplitting up the positive p\u0303(x|y = 1) and negative p\u0303(x|y = 0) distributions, we have:\n1\n2\n1\nD(z) \u222b x\n\u03b4x\u2217(x)q(z|x)dx\ufe38 \ufe37\ufe37 \ufe38 q(z|y=1)\n\u22121 2\n1\n1\u2212D(z) \u222b x\npX (x)q(z|x)dx\ufe38 \ufe37\ufe37 \ufe38 q(z|y=0) = 0\nSolving for D(z) yields the desired result.\nWe can also write down the form of the optimal encoder to understand how the objective shapes the encoding distribution: Proposition 3. (Noisy Optimal Encoder) For any discriminator D(z), the optimal encoder of Section 4.3 satisfies\nq(z|x) \u221d D(z)ysoft(x)(1\u2212D(z))1\u2212ysoft(x)p(z).\nWhere ysoft(x) = p(y = 1|x) = \u03b4x\u2217 (x)\u03b4x\u2217 (x)+pX (x) is the average label of x.\nProof. This is readily obtained by differentiating the objective with respect to q(z|x). Letting L denote the objective of Eq. (5):\n0 = \u2202L\n\u2202q(z|x) =\n\u2202\n\u2202q(z|x) \u222b y,x p(y|x)p\u0303(x)[ \u222b z q(z|x) log p(y|z)dz\u2212 \u222b z q(z|x) log q(z|x) p(z) dz]d{x, y}\n0 = \u222b y p(y|x)[log p(y|z)\u2212 1\u2212 log q(z|x) + log p(z)]dy\nRearranging,\nlog q(z|x) = 1 + log p(z) + \u222b y p(y|x) log p(y|z)dy\nq(z|x) \u221d p(z)e \u222b y p(y|x) log p(y|z)dy = p(z)[D(z)p(y=1|x)(1\u2212D(z))p(y=0|x)]"}, {"heading": "A.2 K-Exemplar Model", "text": "In the K-exemplar model, each discriminator is associated with a batch of K positive exemplars B = {x1, . . . xK}. In this case, we sample positives from the batch B uniformly at random rather than always using a single exemplar. Letting PB(x) denote a uniform distribution over B, we optimize\nDB=arg max D\u2208D\n(Ex\u223cPB [logD(x)] + Ex\u2032\u223cPX [log 1\u2212D(x\u2032)]) . (6)\nUsing the same argument as the single exemplar model, we can characterize the optimal discriminator for the noiseless K-exemplar model:\nProposition 4. (K-Exemplar Optimal Discriminator) For a discriminator trained with K positives {x1, ...xK} sampled uniformly, the optimal discriminator D\u2217B evaluated at any one of the positives x satisfies\nD\u2217B(x) = 1\n1 +KPX (x) .\nProof. Taking the derivative of Equation (6) with respect to D\u2217B(x), we obtain\n1 KD\u2217B(x) \u2212 P (x) 1\u2212D\u2217B(x) = 0.\nSolving for D\u2217B(x) yields the desired result.\nExtensions to noisy versions of the K-exemplar model follow in exactly the same way as the single exemplar model, only changing the positive distribution from \u03b4x\u2217(x) to PB(x)."}, {"heading": "A.3 Task Descriptions", "text": "In this section we describe the tasks used in our experiments. Sample images from these tasks are included in Figure 4.\n2D Maze. This task involves navigating through a 2D maze, using the (x,y) coordinate of the agent as the observation. The challenge stems from the sparse reward, which is only obtained in a small box around the goal. The agent therefore has to figure out how to reach novel parts of the maze in order to eventually find the reward region.\nSparseHalfCheetah. This task involves making a 6-DoF robot run forward as fast as possible. However, this task has been modified to have a sparse reward as done by Houthooft et al. (2016), so\nthat the agent only receives reward upon reaching a certain position threshold, and receives a constant reward afterwards.\nSwimmerGather. This locomotion task, initially proposed as a hierarchical task by Duan et al. (2016), involves navigating a 3-link snake-like robot to collect green or red pellets. The agent is rewarded for collecting green pellets and penalized for red ones.\nDoom (MyWayHome+). This task involves navigating an agent through a maze to find a vest that is located in one of the rooms. The observations consist only of visual feedback, and the reward is sparse and only given when the vest is obtained. This is a slightly modified version of the OpenAI Gym task where we initialize the agent in the furthest room from the vest to create a sparse reward task. In Figure 4, the map of the environment is shown, with the agent starting at the blue dot and the goal at the green dot. The input is resized to an RGB 32 x 32 image.\nFreeway. This game involves navigating an agent across a highway with moving cars, which push the agent back when touched. The reward is sparse and the agent scores a 1 when it makes it across the highway.\nFrostbite. This game involves an agent jumping across ice platforms floating across a river. The reward is dense in that the agent receives reward when it jumps on a platform, but higher scores requires the agent to navigate to other stages which generally requires exploration.\nVenture. This game involves an agent navigating an agent into multiple rooms, where reward is received upon picking up certain objects. The agent must avoid death from touching wandering enemies. We show example images with low and high bonuses given by our algorithm on this task in Figure 5."}, {"heading": "A.4 Experiment Hyperparameters", "text": ""}, {"heading": "A.4.1 Policy Model Parameters", "text": "We used an identical fully connected policy architecture across all non-image tasks, and a convolutional architecture for the image task.\nFor non-image tasks, we used a 2-layer neural network with 32 hidden units per layer, and relu nonlinearities.\nFor Doom, we used 2 convolutional layers (16 4x4 filters, stride 2) followed by 2 fully connected layers with 32 units each. All nonlinearities were relus. We resize the input screen to a RGB 32 x 32 image. For Atari, we used 2 convolutional layers (32 8x8 filters, stride 4, 16 4x4 filter stride 2) followed by 2 fully connected layers with 256 units each. All nonlinearities were relus. For Atari we use the last 4 frames each resized to a grayscale 42 x 42 image."}, {"heading": "A.4.2 Exemplar Model Parameters", "text": "We used an identical fully connected exemplar architecture across all non-image tasks, and a convolutional architecture for the image task.\nFor non-image tasks, we used a 2-layer shared neural network with tanh nonlinearities and 16 units per layer. The final unshared layer was a linear layer.\nFor image-based tasks, we used a shared network consisting of 2 convolutional layers (16 4x4 filters, stride 2) followed by 2 fully connected layers with 16 units each. The convolutional layers used relu nonlinearities, and the fully connected used tanh. The shared network architecture is identical to the policy architecture. The final unshared layer was a linear layer.\nWe also found it useful to lower the learning rate for the shared network as it has many more gradients backpropogating through it than the unshared layer. Thus, we optimized our model using ADAM with a learning rate of 5 \u2217 10\u22124 for the shared layers and 1 \u2217 10\u22123 for the unshared layers."}, {"heading": "A.4.3 Amortized Model Parameters", "text": "For each encoder we use a 2-layer neural network with 32 hidden units per layer and tanh nonlinearities which outputs the mean and log variance of the latent representation of size 16. The latent codes of the encoder are concatenated and fed into the discriminator which is another 2-layer neural network with 32 hidden units per layer and tanh nonlinearities.\nFor image-based tasks, we preprocess the input with 2 convolutional layers (16 4x4 filters, stride 2) before feeding the input into the encoders. For the encoders and discriminator we use the same architecture as stated above except we use 64 hidden units and a latent size of 32.\nWe use a learning rate of 1 \u2217 10\u22124 and optimize the model with ADAM. We found it important to tune the weight on the KL divergence loss which affects how well the discriminator can over or under fit.\nA.4.4 Task Specific EX2Parameters\nWe found it best to tune the exploration bonus weight \u03b2 to match the magnitude of the reward of the task. We used the following EX2hyperparameters for each task, which were obtained via a rough grid search over possible values:\n2D Maze. We use K-Exemplar (K=5) and an exploration bonus weight of 1.0. For the amortized model we use an exploration bonus weight of 0.01 and KL divergence weight of 0.01.\nHalfCheetah. We use K-Exemplar (K=5) and an exploration bonus weight of 0.001. For the amortized model we use an exploration bonus weight of 0.001 and KL divergence weight of 0.1.\nSwimmerGather. We use single exemplars with an exploration bonus weight of 1.0. For the amortized model we use an exploration bonus weight of 1 \u2217 10\u22124 and KL divergence weight of 10. Doom (MyWayHome). We use K-Exemplar (K=5), an exploration bonus weight of 1 \u2217 10\u22124, and entropy bonus of 1 \u2217 10\u22125. For the amortized model we use an exploration bonus weight of 1 \u2217 10\u22124 and KL divergence weight of 0.01.\nFreeway For the amortized model we use an exploration bonus weight of 1\u221710\u22125 and KL divergence weight of 0.1.\nFrostbite For the amortized model we use an exploration bonus weight of 0.001 and KL divergence weight of 0.1.\nVenture For the amortized model we use an exploration bonus weight of 1 \u2217 10\u22124 and KL divergence weight of 0.001.\nA.5 Learning Curves\nFigure 6: 2D Maze\nFigure 7: Swimmer Gather"}], "references": [{"title": "Exploratory gradient boosting for reinforcement learning in complex domains", "author": ["Abel", "David", "Agarwal", "Alekh", "Diaz", "Fernando", "Krishnamurthy", "Akshay", "Schapire", "Robert E"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Abel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abel et al\\.", "year": 2016}, {"title": "Surprise-based intrinsic motivation for deep reinforcement learning", "author": ["Achiam", "Joshua", "Sastry", "Shankar"], "venue": null, "citeRegEx": "Achiam et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Achiam et al\\.", "year": 2017}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Barto", "Andrew G", "Mahadevan", "Sridhar"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Barto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barto et al\\.", "year": 2003}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Bellemare", "Marc G", "Srinivasan", "Sriram", "Ostrovski", "Georg", "Schaul", "Tom", "Saxton", "David", "Munos", "Remi"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "R-max \u2013 a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Brafman", "Ronen I", "Tennenholtz", "Moshe"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Brafman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brafman et al\\.", "year": 2002}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "Li", "Lihong"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Intrinsically Motivated Reinforcement Learning", "author": ["Chentanez", "Nuttapong", "Barto", "Andrew G", "Singh", "Satinder P"], "venue": "In Advances in Neural Information Processing Systems (NIPS). MIT Press,", "citeRegEx": "Chentanez et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chentanez et al\\.", "year": 2005}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Stochastic neural networks for hierarchical reinforcement learning", "author": ["Florensa", "Carlos Campo", "Duan", "Yan", "Abbeel", "Pieter"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Florensa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Florensa et al\\.", "year": 2017}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Learning and transfer of modulated locomotor", "author": ["Heess", "Nicolas", "Wayne", "Gregory", "Tassa", "Yuval", "Lillicrap", "Timothy P", "Riedmiller", "Martin A", "Silver", "David"], "venue": "controllers. CoRR,", "citeRegEx": "Heess et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2016}, {"title": "Vime: Variational information maximizing exploration", "author": ["Houthooft", "Rein", "Chen", "Xi", "Duan", "Yan", "Schulman", "John", "Turck", "Filip De", "Abbeel", "Pieter"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Exploration in metric state spaces", "author": ["Kakade", "Sham", "Kearns", "Michael", "Langford", "John"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kakade et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael", "Singh", "Satinder"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Near-bayesian exploration in polynomial time", "author": ["Kolter", "J. Zico", "Ng", "Andrew Y"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kolter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2009}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Kulkarni", "Tejas D", "Narasimhan", "Karthik", "Saeedi", "Ardavan", "Tenenbaum", "Josh"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Ensemble of exemplar-svms for object detection and beyond", "author": ["Malisiewicz", "Tomasz", "Gupta", "Abhinav", "Efros", "Alexei A"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Malisiewicz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Malisiewicz et al\\.", "year": 2011}, {"title": "Deep multi-scale video prediction beyond mean square", "author": ["Mathieu", "Micha\u00ebl", "Couprie", "Camille", "LeCun", "Yann"], "venue": "error. CoRR,", "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg", "Petersen", "Stig", "Beattie", "Charles", "Sadik", "Amir", "Antonoglou", "Ioannis", "King", "Helen", "Kumaran", "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard", "Singh", "Satinder"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Deep exploration via bootstrapped DQN", "author": ["Osband", "Ian", "Blundell", "Charles", "Alexander Pritzel", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Curiosity-driven exploration by self-supervised prediction", "author": ["Pathak", "Deepak", "Agrawal", "Pulkit", "Efros", "Alexei A", "Darrell", "Trevor"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Pathak et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Pathak et al\\.", "year": 2017}, {"title": "Pac optimal exploration in continuous space markov decision processes", "author": ["Pazis", "Jason", "Parr", "Ronald"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Pazis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pazis et al\\.", "year": 2013}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian J", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1990\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1990}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Stadie", "Bradly C", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "CoRR, abs/1507.00814,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "Learning Options in Reinforcement Learning", "author": ["Stolle", "Martin", "Precup", "Doina"], "venue": "ISBN 978-3-540-45622-3", "citeRegEx": "Stolle et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Stolle et al\\.", "year": 2002}, {"title": "An analysis of model-based interval estimation for markov decision processes", "author": ["Strehl", "Alexander L", "Littman", "Michael L"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "exploration: A study of count-based exploration for deep reinforcement learning", "author": ["Tang", "Haoran", "Houthooft", "Rein", "Foote", "Davis", "Stooke", "Adam", "Chen", "Xi", "Duan", "Yan", "Schulman", "John", "Turck", "Filip De", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "The agent therefore has to figure out how to reach novel parts of the maze in order to eventually find the reward region. SparseHalfCheetah. This task involves making a 6-DoF robot run forward", "author": ["Houthooft"], "venue": null, "citeRegEx": "Houthooft,? \\Q2016\\E", "shortCiteRegEx": "Houthooft", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Recent work has shown that methods that combine reinforcement learning with rich function approximators, such as deep neural networks, can solve a range of complex tasks, from playing Atari games (Mnih et al., 2015) to controlling simulated robots (Schulman et al.", "startOffset": 196, "endOffset": 215}, {"referenceID": 26, "context": ", 2015) to controlling simulated robots (Schulman et al., 2015).", "startOffset": 40, "endOffset": 63}, {"referenceID": 19, "context": "Standard exploration strategies, such as -greedy strategies (Mnih et al., 2015) or Gaussian noise (Lillicrap et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 16, "context": ", 2015) or Gaussian noise (Lillicrap et al., 2015), are undirected and do not explicitly seek out interesting states.", "startOffset": 26, "endOffset": 50}, {"referenceID": 27, "context": "A promising avenue for more directed exploration is to explicitly estimate the novelty of a state, using predictive models that generate future states (Schmidhuber, 1990; Stadie et al., 2015; Achiam & Sastry, 2017) or model state densities (Bellemare et al.", "startOffset": 151, "endOffset": 214}, {"referenceID": 3, "context": ", 2015; Achiam & Sastry, 2017) or model state densities (Bellemare et al., 2016; Tang et al., 2016; Abel et al., 2016).", "startOffset": 56, "endOffset": 118}, {"referenceID": 30, "context": ", 2015; Achiam & Sastry, 2017) or model state densities (Bellemare et al., 2016; Tang et al., 2016; Abel et al., 2016).", "startOffset": 56, "endOffset": 118}, {"referenceID": 0, "context": ", 2015; Achiam & Sastry, 2017) or model state densities (Bellemare et al., 2016; Tang et al., 2016; Abel et al., 2016).", "startOffset": 56, "endOffset": 118}, {"referenceID": 11, "context": "Related concepts such as count-based bonuses have been shown to provide substantial speedups in classic reinforcement learning (Strehl & Littman, 2009; Kolter & Ng, 2009), and several recent works have proposed information-theoretic or probabilistic approaches to exploration based on this idea (Houthooft et al., 2016; Chentanez et al., 2005) by drawing on formal results in simpler discrete or linear systems (Bubeck & Cesa-Bianchi, 2012).", "startOffset": 295, "endOffset": 343}, {"referenceID": 6, "context": "Related concepts such as count-based bonuses have been shown to provide substantial speedups in classic reinforcement learning (Strehl & Littman, 2009; Kolter & Ng, 2009), and several recent works have proposed information-theoretic or probabilistic approaches to exploration based on this idea (Houthooft et al., 2016; Chentanez et al., 2005) by drawing on formal results in simpler discrete or linear systems (Bubeck & Cesa-Bianchi, 2012).", "startOffset": 295, "endOffset": 343}, {"referenceID": 24, "context": "When the observations are complex and high-dimensional, such as in the case of raw images, these models can be difficult to train, since generating and predicting images and other high-dimensional objects is still an open problem, despite recent progress (Salimans et al., 2016).", "startOffset": 255, "endOffset": 278}, {"referenceID": 3, "context": "Though successful results with generative novelty models have been reported with simple synthetic images, such as in Atari games (Bellemare et al., 2016; Tang et al., 2016), we show in our \u2217equal contribution.", "startOffset": 129, "endOffset": 172}, {"referenceID": 30, "context": "Though successful results with generative novelty models have been reported with simple synthetic images, such as in Atari games (Bellemare et al., 2016; Tang et al., 2016), we show in our \u2217equal contribution.", "startOffset": 129, "endOffset": 172}, {"referenceID": 12, "context": "Exploring in such state spaces has typically involved strategies such as introducing distance metrics over the state space (Pazis & Parr, 2013; Kakade et al., 2003), and approximating the quantities used in classical exploration methods.", "startOffset": 123, "endOffset": 164}, {"referenceID": 30, "context": "Prior works have employed approximations for the state-visitation count (Tang et al., 2016; Bellemare et al., 2016; Abel et al., 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al.", "startOffset": 72, "endOffset": 134}, {"referenceID": 3, "context": "Prior works have employed approximations for the state-visitation count (Tang et al., 2016; Bellemare et al., 2016; Abel et al., 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al.", "startOffset": 72, "endOffset": 134}, {"referenceID": 0, "context": "Prior works have employed approximations for the state-visitation count (Tang et al., 2016; Bellemare et al., 2016; Abel et al., 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al.", "startOffset": 72, "endOffset": 134}, {"referenceID": 11, "context": ", 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al., 2016; Stadie et al., 2015; Achiam & Sastry, 2017).", "startOffset": 81, "endOffset": 149}, {"referenceID": 27, "context": ", 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al., 2016; Stadie et al., 2015; Achiam & Sastry, 2017).", "startOffset": 81, "endOffset": 149}, {"referenceID": 21, "context": "For example, bootstrapped DQN (Osband et al., 2016) avoids the need to construct a generative model of the state by instead training multiple, randomized value functions and performs exploration by sampling a value function, and executing the greedy policy with respect to the value function.", "startOffset": 30, "endOffset": 51}, {"referenceID": 8, "context": "A few recent works in deep RL have used hierarchies to explore in sparse reward environments (Florensa et al., 2017; Heess et al., 2016).", "startOffset": 93, "endOffset": 136}, {"referenceID": 10, "context": "A few recent works in deep RL have used hierarchies to explore in sparse reward environments (Florensa et al., 2017; Heess et al., 2016).", "startOffset": 93, "endOffset": 136}, {"referenceID": 15, "context": "However, learning a hierarchy is difficult and has generally required curriculum learning or manually designed subgoals (Kulkarni et al., 2016).", "startOffset": 120, "endOffset": 143}, {"referenceID": 0, "context": ", 2016; Abel et al., 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al., 2016; Stadie et al., 2015; Achiam & Sastry, 2017). Bellemare et al. (2016) show that count-based methods in some sense bound the bonuses produced by exploration incentives based on intrinsic motivation, such as model uncertainty or information gain, making count-based or density-based bonuses an appealing and simple option.", "startOffset": 8, "endOffset": 194}, {"referenceID": 22, "context": "Concurrently with this work, Pathak et al. (2017) proposed to use discriminatively trained exploration bonuses by learning state features which are trained to predict the action from state transition pairs.", "startOffset": 29, "endOffset": 50}, {"referenceID": 3, "context": "In domains where explicit counting is impractical, pseudo-counts can be used based on a density estimate p(s, a), which typically is done using some sort of generatively trained density estimation model (Bellemare et al., 2016).", "startOffset": 203, "endOffset": 227}, {"referenceID": 17, "context": "We borrow the term \u201cexemplar model\u201d from Malisiewicz et al. (2011), which coined the term \u201cexemplar SVM\u201d to refer to a particular linear model trained to classify each instance against all others.", "startOffset": 41, "endOffset": 67}, {"referenceID": 30, "context": "For discrete domains, we can also use a count-based 1/ \u221a N(s) (Tang et al., 2016), where N(s) = nP (s), and n being the size of the replay buffer B.", "startOffset": 62, "endOffset": 81}, {"referenceID": 21, "context": "We also share the majority of layers between discriminators in the neural networks similar to (Osband et al., 2016), and only allow the final linear layer to vary amongst discriminators, which forces the shared layers to learn a joint feature representation, similarly to the amortized model.", "startOffset": 94, "endOffset": 115}, {"referenceID": 9, "context": "Our exploration algorithm has an interesting interpretation related to GANs (Goodfellow et al., 2014).", "startOffset": 76, "endOffset": 101}, {"referenceID": 26, "context": "We use TRPO (Schulman et al., 2015) for policy optimization, because it operates on both continuous and discrete action spaces, and due to its relative robustness to hyperparameter choices (Duan et al.", "startOffset": 12, "endOffset": 35}, {"referenceID": 7, "context": ", 2015) for policy optimization, because it operates on both continuous and discrete action spaces, and due to its relative robustness to hyperparameter choices (Duan et al., 2016).", "startOffset": 161, "endOffset": 180}, {"referenceID": 20, "context": "In fact, prior work on video prediction for Atari games easily achieves accurate predictions hundreds of frames into the future (Oh et al., 2015), while video prediction on natural images is challenging even a couple of frames into the future (Mathieu et al.", "startOffset": 128, "endOffset": 145}, {"referenceID": 18, "context": ", 2015), while video prediction on natural images is challenging even a couple of frames into the future (Mathieu et al., 2015).", "startOffset": 105, "endOffset": 127}, {"referenceID": 11, "context": "We compare the two variants of our method (K-exemplar and amortized) to standard random exploration, kernel density estimation (KDE) with RBF kernels, a method based on Bayesian neural network generative models called VIME (Houthooft et al., 2016), and exploration bonuses based on hashing of latent spaces learned via an autoencoder (Tang et al.", "startOffset": 223, "endOffset": 247}, {"referenceID": 30, "context": ", 2016), and exploration bonuses based on hashing of latent spaces learned via an autoencoder (Tang et al., 2016).", "startOffset": 94, "endOffset": 113}, {"referenceID": 11, "context": "Continuous Control: SwimmerGather and SparseHalfCheetah SwimmerGather and SparseHalfCheetah are two challenging continuous control tasks proposed by Houthooft et al. (2016). Both environments feature sparse reward and medium-dimensional observations (33 and 20 dimensions respectively).", "startOffset": 149, "endOffset": 173}, {"referenceID": 11, "context": "195 1 Houthooft et al. (2016) 2 Schulman et al.", "startOffset": 6, "endOffset": 30}, {"referenceID": 11, "context": "195 1 Houthooft et al. (2016) 2 Schulman et al. (2015) 3 Tang et al.", "startOffset": 6, "endOffset": 55}, {"referenceID": 11, "context": "195 1 Houthooft et al. (2016) 2 Schulman et al. (2015) 3 Tang et al. (2016)", "startOffset": 6, "endOffset": 76}, {"referenceID": 11, "context": "Table 1: Mean scores (higher is better) of our algorithm (both K-exemplar and amortized) versus VIME (Houthooft et al., 2016), baseline TRPO, Hashing, and kernel density estimation (KDE).", "startOffset": 101, "endOffset": 125}], "year": 2017, "abstractText": "Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with countbased exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.", "creator": "LaTeX with hyperref package"}}}