{"id": "1703.01720", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Sound-Word2Vec: Learning Word Representations Grounded in Sounds", "abstract": "Sound there enhance ones the schools modalities whether concerned how me perceive next world around says. Thus, so no coming to incorporate tracking from or modalities into speakers put help machines activate better with animal. While services including been chiefly incorporating visual cues into language isogonal, both plan the emphasizes word finite we respect plasticity 80-day appears also - explored. In this such, do amend a on equivalently payments, rather - word2vec more niece originated embeddings by fracturing them 2003 sound - - with specifically, two trouble fatal concepts, begins all paper usually moves though our analytic mars as done well similar rustling mix. We demonstrate that this government generalisations complete better than similar - only word representations, on two purely textual creating that require discourse probably otherworldly real-world - - version keystrokes to alan - singing evidence. Finally, thought analyze bus neighbors to progress that unique maps armies earlier sound - w2v while equivalent given speak - only reductive.", "histories": [["v1", "Mon, 6 Mar 2017 04:30:12 GMT  (43kb)", "https://arxiv.org/abs/1703.01720v1", "5 pages"], ["v2", "Fri, 28 Apr 2017 06:35:16 GMT  (59kb)", "http://arxiv.org/abs/1703.01720v2", "5 pages; 3 tables"], ["v3", "Thu, 10 Aug 2017 04:26:57 GMT  (227kb,D)", "http://arxiv.org/abs/1703.01720v3", "Accepted at EMNLP 2017. Contains 6 pages; 3 tables; 1 figure"], ["v4", "Tue, 29 Aug 2017 15:54:31 GMT  (227kb,D)", "http://arxiv.org/abs/1703.01720v4", "Accepted at EMNLP 2017. Contains 6 pages; 3 tables; 1 figure"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.SD", "authors": ["ashwin k vijayakumar", "ramakrishna vedantam", "devi parikh"], "accepted": true, "id": "1703.01720"}, "pdf": {"name": "1703.01720.pdf", "metadata": {"source": "CRF", "title": "Sound-Word2Vec: Learning Word Representations Grounded in Sounds", "authors": ["Ashwin Vijayakumar", "Ramakrishna Vedantam", "Devi Parikh", "Georgia Tech", "Virgina Tech"], "emails": ["ashwinkv@gatech.edu,", "parikh@gatech.edu,", "vrama1@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "Sound and vision are the dominant perceptual signals, while language helps us communicate complex experiences via rich abstractions. For example, a novel can stimulate us to mentally construct the image of the scene despite having never physically perceived it. Indeed, language has evolved to contain numerous constructs that help depict visual concepts. For example, we can easily form the picture of a white, furry cat with blue eyes via. a description of the cat in terms of its visual attributes (Lampert et al., 2009; Parikh and Grauman, 2011).\nNeed for Onomatopoeia. However, how would one describe the auditory instantiation of cats? While a first thought might be to use audio descriptors like loud, shrill, husky etc. as mid-level constructs or \u201cattributes\u201d, arguably, it is difficult to precisely convey and comprehend sound through such language. Indeed, Wake and Asahi (1998) find that humans first communicate sounds using \u201conomatopoeia\u201d \u2013 words that are suggestive of the phonetics of sounds while having no explicit meaning e.g. meow, tic-toc. When asked for further explanation of sounds, humans provide descriptions of potential sound sources or impressions created by the sound (pleasant, annoying, etc.)\nNeed for Grounding in Sound. While onomatopoeic words exist for commonly found concepts, a vast majority of concepts are not as perceptually striking or sufficiently frequent for us to come up with dedicated words describing their sounds. Even worse, some sounds, say, musical instruments, might be difficult to mimic using speech. Thus, for a large number of concepts there seems to be a gap between sound and its counterpart in language (Sundaram and Narayanan, 2006). This becomes problematic in specific situations where we want to talk about the heavy tail of concepts and their sounds, or while describing a particular sound we want to create as an effect (say in movies). To alleviate this, a common literary strategy is to provide metaphors to more relatable exemplars. For example, when we say, \u201cHe thundered angrily\u201d, we compare the person\u2019s angry speech to the sound of thunder to convey the seriousness of the situation. However, without this grounding in sound, thunder and anger both appear to be seemingly unrelated concepts in terms of semantics. ar X iv :1 70 3. 01 72 0v 4\n[ cs\n.C L\n] 2\n9 A\nug 2\n01 7\nContributions. In this work, we learn embeddings to bridge the gap between sound and its counterpart in language. We follow a retrofitting strategy, capturing similarity in sounds associated with words, while using distributional semantics (from word2vec) to provide smoothness to the embeddings. Note that we are not interested in capturing phonetic similarity, but the grounding in sound of the concept associated with the word (say \u201crustling\u201d of leaves and paper.) We demonstrate the effectiveness of our embeddings on three downstream tasks that require reasoning about related aural cues: 1. Text-based sound retrieval \u2013 Given a textual query describing the sound and a database containing sounds and associated textual tags, we retrieve sound samples by matching text (Sec. 5.1) 2. Foley Sound Discovery \u2013 Given a short phrase that outlines the technique of producing Foley sounds1, we discover other relevant words (objects or actions) which can produce similar sound effects (Sec. 5.2) 3. Aurally-relevant word relatedness assessment on AMEN and ASLex (Kiela and Clark, 2015) (Sec. 5.3)\nWe also qualitatively compare with word2vec to highlight the unique notions of word relatedness captured by imposing auditory grounding."}, {"heading": "2 Related Work", "text": "Audio and Word Embeddings. Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and van Miltenburg (2015) show preliminary results on using sound to learn distributional representations, Kiela and Clark (2015) build on ideas from Bruni et al. (2014) to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective. Further, they propose various fusion strategies to combine knowledge from both the\n1Foley sounds are sound effects (typically ambient sounds) that are added to movies in the post-production stage to make actions or situations appear more realistic. These sounds are generally created using easily available proxy objects that mimic the sound of the true situation being depicted. For example, sound of breaking celery sticks is used to create the effect of breaking bones.\nmodalities. Instead, we \u201cspecialize\u201d embeddings to exclusively respect relationships defined by sounds, while initializing with word2vec embeddings for smoothness. Similar to previous findings (Melamud et al., 2016), we observe that our specialized embeddings outperform language-only as well as other multi-modal embeddings in the downstream tasks of interest. In an orthogonal and interesting direction, other recent works (Chung et al., 2016; He et al., 2016; Settle and Livescu, 2016) learn word representations based on similarity in their pronunciation and not the sounds associated with them. In other words, phonetically similar words that have near identical pronunciations are brought closer in the embedding space (e.g., flower and flour). Sundaram and Narayanan (2006) study the applicability of onomatopoeia to obtain semantically meaningful representations of audio. Using a novel word-similarity metric and principal component analysis, they find representations for sounds and cluster them in this derived space to reason about similarities. In contrast, we are interested in learning word representations that respect aural-similarity. More importantly, our approach learns word representations for in a data-driven manner without having to first map the sound or its tags to corresponding onomatopoeic words.\nMultimodal Learning with Surrogate Supervision. Kottur et al. (2016) and Owens et al. (2016) use a surrogate modality to induce supervision to learn representations for a desired modality. While the former learns word embeddings grounded in cartoon images, the latter learns visual features grounded in sound. In contrast, we use sound as the surrogate modality to supervise representation learning for words."}, {"heading": "3 Datasets", "text": "Freesound. We use the freesound database (Font et al., 2013), also used in prior work (Kiela and Clark, 2015; Lopopolo and van Miltenburg, 2015) to learn the proposed sound-word2vec embeddings. Freesound is a freely available, collaborative dataset consisting of user uploaded sounds permitting reuse. All uploaded sounds have human descriptions in the form of tags and captions in natural language. The tags contain a broad set of relevant topics for a sound (e.g., ambience, electronic, birds, city, reverb) and\ncaptions describing the content of the sound, in addition to details pertaining to audio quality. For the text-based sound retrieval task, we use a subset of 234,120 sounds from this database and divide it into training (80%), validation (10%) and testing splits (10%). Further, for foley sound discovery, we aggregate descriptions of foley sound production provided by sound engineers (epicsound, accessed 23-Jan-2017; Singer, accessed 23-Jan-2017) to create a list of 30 foley sound pairs, which forms our ground truth for the task. For example, the description to produce a foley \u201cdriving on gravel\u201d sound is to record the \u201ccrunching sound of plastic or polyethene bags\u201d.\nAMEN and ASLex. AMEN and ASLex (Kiela and Clark, 2015) are subsets of the standard MEN (Bruni et al., 2014) and SimLex (Hill et al., 2015) word similarity datasets consisting of word-pairs that \u201ccan be associated with a distinctive associated sound\u201d. We evaluate on this dataset for completeness to benchmark our approach against previous work. However, we are primarily interested in the slightly different problem of relating words with similar auditory instantions that may or may not be semantically related as opposed to relating semantically similar words that can be associated with some common auditory signal."}, {"heading": "4 Approach", "text": "We use the Freesound database to construct a dataset of tuples {s, T}, where s is a sound and T is the set of associated user-provided tags. We then aim to learn an embedding space for the tags that respects auditory grounding using sound information as cross-modal context \u2013 similar to word2vec (Mikolov et al., 2013) that uses neighboring words as context / supervision. We now explain our approach in detail.\nAudio Features and Clustering. We represent each sound s by a feature vector consisting of the mean and variance of the following audio descriptors that are readily available as part of Freesound database: \u2022 Mel-Frequency Cepstral Co-efficients: This\nfeature represents the short-term power spectrum of an audio and closely approximates the response of the human auditory system \u2013 computed as given in (Ganchev et al., 2005). \u2022 Spectral Contrast: It is the magnitude difference\nin the peaks and valleys of the spectrum \u2013 computed according to (Akkermans et al., 2009). \u2022 Dissonance: It measures the perceptual rough-\nness of the sound (Plomp and Levelt, 1965). \u2022 Zero-crossing Rate: It is the percentage of sign\nchanges between consecutive signal values and is indicative of noise content. \u2022 Spectral Spread: This feature is the concatena-\ntion of the k-order moments of the spectrum, where k \u2208 {0, 1, 2, 3, 4}. \u2022 Pitch Salience: This feature helps discriminate\nbetween musical and non-musical tones. While, pure tones and unpitched sounds have values near 0, musical sounds containing harmonics have higher values (Ricard, 2004). We then use K-Means algorithm to cluster the sounds in this feature space to assign each sound to a cluster C(s) \u2208 {1, . . .K}. We set K to 30 by evaluating the performance of the embeddings on text-based audio-retrieval on the held out validation set. Note that the clustering is only performed once, prior to representation learning described below.\nRepresentation Learning. We represent each tag t \u2208 T using a |V| dimensional one-hot encoding denoted by vt, where V is the set of all unique tags in the training set (the size of our dictionary). This one-hot vector vt is projected into a Ddimensional vector space via WP \u2208 R|V|\u00d7D, the projection matrix. This projection matrix computes the representation for each word in V . The idea of our approach is to use WP to accurately predict cluster assignments (for sounds associated with words), which enforces grounding in sound. For each data-point, we obtain the summary of the tags T , by averaging the projections of all tags in the set as 1|T | \u2211 t\u2208T W \u2032 Pvt. We then transform the so obtained summary representation via a\nlinear layer (with parameters WO) and pass the output through the softmax function to obtain a distribution, p(c|T ) over the K sound clusters. We perform maximum-likelihood training for the correct cluster assignment C(s)2, optimizing for parameters WP and WO:\nmax WP ,WO\nlogP (c = C(s)|T ) (1)\nWe use SGD with momentum to optimize this objective which essentially is the cross-entropy between cluster assignments and p(c|T ). We set D to 300 to be consistent with the publicly available word2vec embeddings.\nInitialization. We initialize WP with word2vec embeddings (Mikolov et al., 2013) trained on the Google news corpus dataset with \u223c3M words. We fine-tune on a subset of 9578 tags which are present in both Freesound as well as Google news corpus datasets, which is 55.68% of the original tags in the Freesound dataset. This helps us remove noisy tags unrelated to the content of the sound.\nIn addition to enlarging the vocabulary, the pretraining helps induce smoothness in the soundword2vec embeddings \u2013 allowing us to transfer semantics learnt from sounds to words that were not present as tags in the Freesound database. Indeed, we find that word2vec pre-training helps improve performance (Sec. 5.3). Our use of language embeddings as an initialization to fine-tune (specialize) from, as opposed to formulating a joint objective with language and audio context (Kiela and Clark, 2015) is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity."}, {"heading": "5 Results", "text": "Ablations. In addition to the language-only baseline word2vec (Mikolov et al., 2013), we compare against tag-word2vec \u2013 that predicts a tag using other tags of the sound as context, inspired by (Font et al., 2014). We also report results with a randomly initialized projection matrix (soundword2vec(r) to evaluate the effectiveness of pretraining with word2vec. Prior work. We compare against previous works Lopopolo and van Miltenburg (2015) and Kiela and Clark (2015). While the former uses a standard bag of words and SVD pipeline to arrive at\n2We also tried to regress directly to sound features instead of clustering, but found that it had poor performance.\ndistributional representations for words, the latter trains under a joint objective that respects both linguistic and auditory similarity. We use the openly available implementation for Lopopolo and van Miltenburg (2015) and re-implement Kiela and Clark (2015) and train them on our dataset for a fair comparison of the methods. In addition, we show a comparison to word-vectors released by (Kiela and Clark, 2015) in the supplementary material. All approaches use an embedding size of 300 for consistency."}, {"heading": "5.1 Text-based Sound Retrieval", "text": "Given a textual description of a sound as query, we compare it with tags associated with sounds in the database to retrieve the sound with the closest matching tags. Note that this is a purely textual task, albeit one that needs awareness of sound. In a sense, this task exactly captures what we want our model to be able to do \u2013 bridge the semantic gap between language and sound. We use the training split (Sec. 3) to learn the sound-word2vec vectors, validation to pick the number of clusters (K), and report results on the test split. For retrieval, we represent sounds by averaging the learnt embeddings for the associated tags. We embed the caption provided for the sound (in the Freesound database) in a similar manner, and use it as the query. We then rank sounds based on the cosine similarity between the tag and query representations for retrieval. We evaluate using standard retrieval metrics \u2013 Recall@{1,10,50,100}. Note that the entire testing set (\u224810k sounds) is present in the retrieval pool. So, recall@100 corresponds to obtaining the correct result in the top 1% of the search results, which is a relatively stringent evaluation criterion.\nResults. Table. 1 shows that our sound-word2vec embeddings outperform the baselines. We see that specializing the embeddings for sound using our two-stage training outperforms prior work(Kiela and Clark (2015) and Lopopolo and van Miltenburg (2015)), which did not do specialization. Among our approaches, tag-word2vec performs second best \u2013 this is intuitive since the tag distributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance."}, {"heading": "5.2 Foley Sound Discovery", "text": "In this task, we evaluate how well embeddings identify matching pairs of target sounds (flapping bird wings) and descriptions of Foley sound production techniques (rubbing a pair of gloves). Intuitively, one expects sound-aware word embeddings to do better at this task than sound-agnostic ones. We setup a ranking task by constructing a set of original Foley sound pairs and decoy pairs formed by pairing the target description with every word from the vocabulary. We rank using cosine similarity between the average word-vectors in each member of the pair. A good embedding is one in which the original Foley sound pair has the lowest rank. We use the mean rank of the Foley sound in the dataset for evaluation. We transfer the embeddings from Sec. 5.1 to this task, without additional training.\nResults. We find that Sound-word2vec performs the best with a mean rank of 34.6 compared to other baselines tag-word2vec (38.9), soundword2vec(r) (114.3) and word2vec (189.45). As observed previously, the second best performing approach is tag-word2vec. Lopopolo and van Miltenburg (2015) and Kiela and Clark (2015) perform worse than tag-word2vec with a mean rank of 48.4 and 42.1 respectively. Note that random chance gets a rank of (|V|+ 1)/2 = 4789.5."}, {"heading": "5.3 Evaluation on AMEN and ASLex", "text": "AMEN and ASLex (Kiela and Clark, 2015) are subsets of the MEN and SimLex-999 datasets for word relatedness grounded in sound. From Table 2, we can see that our embeddings outperform (Kiela and Clark, 2015) on both AMEN and ASLex. These datasets were curated by annotating\nconcepts related by sound; however we observe that relatedness is often confounded. For example, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already semantically related. In contrast, we are interested in how onomatopoeic words relate to regular words (Table 3), which we study by explicit grounding in sound. Thus while we show competitive performance on this dataset, it might not be best suited for studying the benefits of our approach."}, {"heading": "6 Discussion and Conclusion", "text": "We show nearest neighbors in both soundword2vec and word2vec space (Table 3) to qualitatively demonstrate the unique dependencies captured due to auditory grounding. While word2vec maps a word (say, apple) to other semantically similar words (other fruits), similar \u2018sounding\u2019 words (chips) or onomatopoeia (munch) are closer in our embedding space. Moreover, onomatopoeic words (say, boom and slam) are mapped to relevant objects (explosion and door). Interestingly, parts (e.g., lock, latch) and actions (closing) are also closer to the onomatopoeic query \u2013 exhibiting an understanding of the auditory scene. Conclusion. In this work we introduce a novel word embedding scheme that respects auditory grounding. We show that our embeddings provide strong performance on text-based sound retrieval, Foley sound discovery along with intuitive nearest neighbors for onomatopoeia that are tasks in text requiting auditory reasoning. We hope our work motivates further efforts on understanding and relating onomatopoeia words to \u201cregular\u201d words. Acknowledgements. We thank the Freesound team and Frederic Font in particular for helping us with the Freesound API. We also thank Khushi Gupta and Stefan Lee for fruitful discussions. This work was funded in part by an NSF CAREER, ONR Grant N0001416-1-2713, ONR YIP, Sloan Fellowship, Allen Distinguished Investigator, Google Faculty Research Award, Amazon Academic Research Award to DP."}], "references": [{"title": "Shape-based spectral contrast descriptor", "author": ["Vincent Akkermans", "Joan Serr\u00e0", "Perfecto Herrera."], "venue": "Proc. of the Sound and Music Computing Conf.(SMC).", "citeRegEx": "Akkermans et al\\.,? 2009", "shortCiteRegEx": "Akkermans et al\\.", "year": 2009}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research (JAIR).", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder", "author": ["Yu-An Chung", "Chao-Chung Wu", "Chia-Hao Shen", "Hung-yi Lee", "Lin-Shan Lee."], "venue": "CoRR.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Extending tagging ontologies with domain specific knowledge", "author": ["Frederic Font", "Sergio Oramas", "Gy\u00f6rgy Fazekas", "Xavier Serra."], "venue": "Proceedings of the International Semantic Web Conference.", "citeRegEx": "Font et al\\.,? 2014", "shortCiteRegEx": "Font et al\\.", "year": 2014}, {"title": "Freesound technical demo", "author": ["Frederic Font", "Gerard Roma", "Xavier Serra."], "venue": "Proceedings of the 21st ACM International Conference on Multimedia.", "citeRegEx": "Font et al\\.,? 2013", "shortCiteRegEx": "Font et al\\.", "year": 2013}, {"title": "Comparative evaluation of various mfcc implementations on the speaker verification task", "author": ["Todor Ganchev", "Nikos Fakotakis", "George Kokkinakis."], "venue": "Proceedings of the SPECOM.", "citeRegEx": "Ganchev et al\\.,? 2005", "shortCiteRegEx": "Ganchev et al\\.", "year": 2005}, {"title": "Multi-view recurrent neural acoustic word embeddings", "author": ["Wanjia He", "Weiran Wang", "Karen Livescu."], "venue": "arXiv preprint arXiv:1611.04496.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Multi-and cross-modal semantics beyond vision: Grounding in auditory perception", "author": ["Douwe Kiela", "Stephen Clark."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Kiela and Clark.,? 2015", "shortCiteRegEx": "Kiela and Clark.", "year": 2015}, {"title": "Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes", "author": ["Satwik Kottur", "Ramakrishna Vedantam", "Jose M.F. Moura", "Devi Parikh."], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recog-", "citeRegEx": "Kottur et al\\.,? 2016", "shortCiteRegEx": "Kottur et al\\.", "year": 2016}, {"title": "Learning to detect unseen object classes by betweenclass attribute transfer", "author": ["Christoph H. Lampert", "Hannes Nickisch", "Stefan Harmeling."], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Lampert et al\\.,? 2009", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni."], "venue": "arXiv preprint arXiv:1501.02598.", "citeRegEx": "Lazaridou et al\\.,? 2015", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Sound-based distributional models", "author": ["Alessandro Lopopolo", "Emiel van Miltenburg."], "venue": "IWCS 2015.", "citeRegEx": "Lopopolo and Miltenburg.,? 2015", "shortCiteRegEx": "Lopopolo and Miltenburg.", "year": 2015}, {"title": "The role of context types and dimensionality in learning word embeddings", "author": ["Oren Melamud", "David McClosky", "Siddharth Patwardhan", "Mohit Bansal."], "venue": "arXiv preprint arXiv:1601.00893.", "citeRegEx": "Melamud et al\\.,? 2016", "shortCiteRegEx": "Melamud et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Ambient Sound Provides Supervision for Visual Learning", "author": ["A. Owens", "J. Wu", "J.H. McDermott", "W.T. Freeman", "A. Torralba."], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Owens et al\\.,? 2016", "shortCiteRegEx": "Owens et al\\.", "year": 2016}, {"title": "Relative Attributes", "author": ["Devi Parikh", "Kristen Grauman."], "venue": "Proceedings of IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "Parikh and Grauman.,? 2011", "shortCiteRegEx": "Parikh and Grauman.", "year": 2011}, {"title": "Tonal consonance and critical bandwidth", "author": ["Reinier Plomp", "Willem Johannes Maria Levelt."], "venue": "The journal of the Acoustical Society of America.", "citeRegEx": "Plomp and Levelt.,? 1965", "shortCiteRegEx": "Plomp and Levelt.", "year": 1965}, {"title": "Towards computational morphological description of sound", "author": ["Julien Ricard."], "venue": "DEA pre-thesis research work, Universitat Pompeu Fabra, Barcelona.", "citeRegEx": "Ricard.,? 2004", "shortCiteRegEx": "Ricard.", "year": 2004}, {"title": "Discriminative acoustic word embeddings: Recurrent neural network-based approaches", "author": ["Shane Settle", "Karen Livescu."], "venue": "arXiv preprint arXiv:1611.02550.", "citeRegEx": "Settle and Livescu.,? 2016", "shortCiteRegEx": "Settle and Livescu.", "year": 2016}, {"title": "Art of foley", "author": ["Philip R. Singer"], "venue": null, "citeRegEx": "Singer.,? \\Q2017\\E", "shortCiteRegEx": "Singer.", "year": 2017}, {"title": "Vector-based representation and clustering of audio using onomatopoeia words", "author": ["Shiva Sundaram", "Shrikanth Narayanan."], "venue": "In Proceedings of AAAI 2006 Fall Symposia.", "citeRegEx": "Sundaram and Narayanan.,? 2006", "shortCiteRegEx": "Sundaram and Narayanan.", "year": 2006}, {"title": "Sound retrieval with intuitive verbal expressions", "author": ["Sanae Wake", "Toshiyuki Asahi."], "venue": "Proceedings of the 5th International Conference on Auditory Display (ICAD).", "citeRegEx": "Wake and Asahi.,? 1998", "shortCiteRegEx": "Wake and Asahi.", "year": 1998}], "referenceMentions": [{"referenceID": 10, "context": "tributes (Lampert et al., 2009; Parikh and Grauman, 2011).", "startOffset": 9, "endOffset": 57}, {"referenceID": 16, "context": "tributes (Lampert et al., 2009; Parikh and Grauman, 2011).", "startOffset": 9, "endOffset": 57}, {"referenceID": 22, "context": "Indeed, Wake and Asahi (1998) find that humans first communicate sounds using \u201conomatopoeia\u201d \u2013 words that are suggestive of the phonetics of sounds while having no explicit meaning e.", "startOffset": 8, "endOffset": 30}, {"referenceID": 21, "context": "Thus, for a large number of concepts there seems to be a gap between sound and its counterpart in language (Sundaram and Narayanan, 2006).", "startOffset": 107, "endOffset": 137}, {"referenceID": 8, "context": "Aurally-relevant word relatedness assessment on AMEN and ASLex (Kiela and Clark, 2015) (Sec.", "startOffset": 63, "endOffset": 86}, {"referenceID": 1, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings.", "startOffset": 34, "endOffset": 157}, {"referenceID": 11, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings.", "startOffset": 34, "endOffset": 157}, {"referenceID": 8, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings.", "startOffset": 34, "endOffset": 157}, {"referenceID": 9, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings.", "startOffset": 34, "endOffset": 157}, {"referenceID": 1, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and van Miltenburg (2015) show preliminary results on using sound to learn distributional representations, Kiela and Clark (2015) build on ideas from Bruni et al.", "startOffset": 35, "endOffset": 293}, {"referenceID": 1, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and van Miltenburg (2015) show preliminary results on using sound to learn distributional representations, Kiela and Clark (2015) build on ideas from Bruni et al.", "startOffset": 35, "endOffset": 397}, {"referenceID": 1, "context": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and van Miltenburg (2015) show preliminary results on using sound to learn distributional representations, Kiela and Clark (2015) build on ideas from Bruni et al. (2014) to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective.", "startOffset": 35, "endOffset": 437}, {"referenceID": 13, "context": "Similar to previous findings (Melamud et al., 2016), we observe that our specialized embeddings outperform language-only as well as other multi-modal embeddings in the downstream tasks of interest.", "startOffset": 29, "endOffset": 51}, {"referenceID": 2, "context": "In an orthogonal and interesting direction, other recent works (Chung et al., 2016; He et al., 2016; Settle and Livescu, 2016) learn word representations based on similarity in their pronunciation and not the sounds associated with them.", "startOffset": 63, "endOffset": 126}, {"referenceID": 6, "context": "In an orthogonal and interesting direction, other recent works (Chung et al., 2016; He et al., 2016; Settle and Livescu, 2016) learn word representations based on similarity in their pronunciation and not the sounds associated with them.", "startOffset": 63, "endOffset": 126}, {"referenceID": 19, "context": "In an orthogonal and interesting direction, other recent works (Chung et al., 2016; He et al., 2016; Settle and Livescu, 2016) learn word representations based on similarity in their pronunciation and not the sounds associated with them.", "startOffset": 63, "endOffset": 126}, {"referenceID": 2, "context": "In an orthogonal and interesting direction, other recent works (Chung et al., 2016; He et al., 2016; Settle and Livescu, 2016) learn word representations based on similarity in their pronunciation and not the sounds associated with them. In other words, phonetically similar words that have near identical pronunciations are brought closer in the embedding space (e.g., flower and flour). Sundaram and Narayanan (2006) study the appli-", "startOffset": 64, "endOffset": 419}, {"referenceID": 9, "context": "Kottur et al. (2016) and Owens et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Kottur et al. (2016) and Owens et al. (2016) use a surrogate modality to induce", "startOffset": 0, "endOffset": 45}, {"referenceID": 4, "context": "We use the freesound database (Font et al., 2013), also used in prior work (Kiela and Clark, 2015; Lopopolo and van Miltenburg, 2015) to learn the proposed sound-word2vec embeddings.", "startOffset": 30, "endOffset": 49}, {"referenceID": 8, "context": ", 2013), also used in prior work (Kiela and Clark, 2015; Lopopolo and van Miltenburg, 2015) to learn the proposed sound-word2vec embeddings.", "startOffset": 33, "endOffset": 91}, {"referenceID": 8, "context": "AMEN and ASLex (Kiela and Clark, 2015) are subsets of the standard", "startOffset": 15, "endOffset": 38}, {"referenceID": 1, "context": "MEN (Bruni et al., 2014) and SimLex (Hill et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 7, "context": ", 2014) and SimLex (Hill et al., 2015) word similarity datasets consisting of word-pairs that \u201ccan be associated with a distinctive associated sound\u201d.", "startOffset": 19, "endOffset": 38}, {"referenceID": 14, "context": "We then aim to learn an embedding space for the tags that respects auditory grounding using sound information as cross-modal context \u2013 similar to word2vec (Mikolov et al., 2013) that uses neighboring words as context / supervision.", "startOffset": 155, "endOffset": 177}, {"referenceID": 5, "context": "We represent each sound s by a feature vector consisting of the mean and variance of the following audio descriptors that are readily available as part of Freesound database: \u2022 Mel-Frequency Cepstral Co-efficients: This feature represents the short-term power spectrum of an audio and closely approximates the response of the human auditory system \u2013 computed as given in (Ganchev et al., 2005).", "startOffset": 371, "endOffset": 393}, {"referenceID": 0, "context": "in the peaks and valleys of the spectrum \u2013 computed according to (Akkermans et al., 2009).", "startOffset": 65, "endOffset": 89}, {"referenceID": 17, "context": "\u2022 Dissonance: It measures the perceptual roughness of the sound (Plomp and Levelt, 1965).", "startOffset": 64, "endOffset": 88}, {"referenceID": 18, "context": "have higher values (Ricard, 2004).", "startOffset": 19, "endOffset": 33}, {"referenceID": 14, "context": "We initialize WP with word2vec embeddings (Mikolov et al., 2013) trained on the Google news corpus dataset with \u223c3M words.", "startOffset": 42, "endOffset": 64}, {"referenceID": 8, "context": "objective with language and audio context (Kiela and Clark, 2015) is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity.", "startOffset": 42, "endOffset": 65}, {"referenceID": 14, "context": "In addition to the language-only baseline word2vec (Mikolov et al., 2013), we compare against tag-word2vec \u2013 that predicts a tag using other tags of the sound as context, inspired", "startOffset": 51, "endOffset": 73}, {"referenceID": 3, "context": "by (Font et al., 2014).", "startOffset": 3, "endOffset": 22}, {"referenceID": 3, "context": "by (Font et al., 2014). We also report results with a randomly initialized projection matrix (soundword2vec(r) to evaluate the effectiveness of pretraining with word2vec. Prior work. We compare against previous works Lopopolo and van Miltenburg (2015) and Kiela and Clark (2015).", "startOffset": 4, "endOffset": 252}, {"referenceID": 3, "context": "by (Font et al., 2014). We also report results with a randomly initialized projection matrix (soundword2vec(r) to evaluate the effectiveness of pretraining with word2vec. Prior work. We compare against previous works Lopopolo and van Miltenburg (2015) and Kiela and Clark (2015). While the former uses a standard bag of words and SVD pipeline to arrive at", "startOffset": 4, "endOffset": 279}, {"referenceID": 8, "context": "In addition, we show a comparison to word-vectors released by (Kiela and Clark, 2015) in the supplementary material.", "startOffset": 62, "endOffset": 85}, {"referenceID": 8, "context": "We use the openly available implementation for Lopopolo and van Miltenburg (2015) and re-implement Kiela and Clark (2015) and train them on our dataset for a fair comparison of the methods.", "startOffset": 99, "endOffset": 122}, {"referenceID": 8, "context": "We see that specializing the embeddings for sound using our two-stage training outperforms prior work(Kiela and Clark (2015) and Lopopolo and van Miltenburg (2015)), which did not do specialization.", "startOffset": 102, "endOffset": 125}, {"referenceID": 8, "context": "We see that specializing the embeddings for sound using our two-stage training outperforms prior work(Kiela and Clark (2015) and Lopopolo and van Miltenburg (2015)), which did not do specialization.", "startOffset": 102, "endOffset": 164}, {"referenceID": 8, "context": "(Kiela and Clark, 2015) 6.", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "04 (Kiela and Clark, 2015) 0.", "startOffset": 3, "endOffset": 26}, {"referenceID": 8, "context": "Table 2: Comparison to state of the art AMEN and ASLex datasets (Kiela and Clark, 2015) (higher is better).", "startOffset": 64, "endOffset": 87}, {"referenceID": 8, "context": "Table 2: Comparison to state of the art AMEN and ASLex datasets (Kiela and Clark, 2015) (higher is better). Our approach performs better than Kiela and Clark (2015).", "startOffset": 65, "endOffset": 165}, {"referenceID": 8, "context": "Lopopolo and van Miltenburg (2015) and Kiela and Clark (2015) perform worse than tag-word2vec with a mean rank of 48.", "startOffset": 39, "endOffset": 62}, {"referenceID": 8, "context": "AMEN and ASLex (Kiela and Clark, 2015) are subsets of the MEN and SimLex-999 datasets for word relatedness grounded in sound.", "startOffset": 15, "endOffset": 38}, {"referenceID": 8, "context": "From Table 2, we can see that our embeddings outperform (Kiela and Clark, 2015) on both AMEN and ASLex.", "startOffset": 56, "endOffset": 79}], "year": 2017, "abstractText": "To be able to interact better with humans, it is crucial for machines to understand sound \u2013 a primary modality of human perception. Previous works have used sound to learn embeddings for improved generic semantic similarity assessment. In this work, we treat sound as a first-class citizen, studying downstream 6textual tasks which require aural grounding. To this end, we propose sound-word2vec \u2013 a new embedding scheme that learns specialized word embeddings grounded in sounds. For example, we learn that two seemingly (semantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make. Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering Foley sound effects (used in movies). Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurallyrelevant word relatedness datasets such as AMEN and ASLex.", "creator": "LaTeX with hyperref package"}}}