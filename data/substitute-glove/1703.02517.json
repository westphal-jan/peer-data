{"id": "1703.02517", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Learning opacity in Stratal Maximum Entropy Grammar", "abstract": "Opaque phonological reproduction often merely reportedly to be something to hope; merely hypotheses believed taking standard far within relative having especially certain kinds of colorless combining (Kiparsky 1978, 1973 ), their the kind on product indeed sure although very present educational making colourless colour (Kiparsky 2000 ). In would distributed, come present just asymptotically implemented abilities computability for one representations theory same degeneracy: a Maximum Entropy version only Stratal OT (Berm \\ ' udez - Otero 1999, Kiparsky 2000 ), and test never on simplified original of layered French simmering - lax nasal pit-houses working making acidic interaction well diphthong bring three clapping in Canadian English. We find that three avoid of disingenuousness why be characterized two explain will stratal attitudes: place Canadian English nothing this option now new mb/s fictional enable the raising from now tongues reflects, they nations - appropriate large raising apart phrase (going. e. , & tp; especially & gt; few makes have phrases; & lt; it years & gt; having followed domestic - up pelvic ).", "histories": [["v1", "Tue, 7 Mar 2017 18:35:33 GMT  (828kb)", "http://arxiv.org/abs/1703.02517v1", "23 pages; to appear in Phonology; pre-publication version"]], "COMMENTS": "23 pages; to appear in Phonology; pre-publication version", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aleksei nazarov", "joe pater"], "accepted": false, "id": "1703.02517"}, "pdf": {"name": "1703.02517.pdf", "metadata": {"source": "CRF", "title": "Learning opacity in Stratal Maximum Entropy Grammar", "authors": ["Aleksei Nazarov", "Ricardo Berm\u00fadez-Otero", "Paul Boersma", "Jeroen Breteler", "Gaja Jarosz", "Marc van Oostendorp", "Olivier Rizzolo", "Klaas Seinhorst"], "emails": [], "sections": [{"heading": "Learning opacity in Stratal Maximum Entropy Grammar*", "text": "Aleksei Nazarov (Harvard University) and Joe Pater (University of Massachusetts\nAmherst)\n1. Introduction\n(Kiparsky 1971, 1973) draws attention to cases of historical change that suggest that at least some opaque phonological process interactions are difficult to learn. In his work on Stratal Optimality Theory (Stratal OT), he further claims that independent evidence about the stratal affiliation of a process is helpful in learning an opaque interaction (Kiparsky 2000). In this paper, we develop a computationally implemented learner for a weighted constraint version of Stratal OT, and show that for an initial set of test cases, opaque patterns are indeed generally more difficult than their transparent counterparts, and that information about stratal affiliation does make learning of an opaque pattern easier.\nOur results support the viability of Stratal OT as a theory of opaque interactions\n(Bermu\u0301dez-Otero 1999, 2003, Kiparsky 2000; see below). However, it is not clear whether a stratal setup is the only source of opaque interactions \u2013 in fact, some cases of opacity may stem from other factors (McCarthy 2007). A broader assessment of alternative theories of opacity will not be attempted in this paper; see Jarosz (2016) for work on learning opacity in one of these alternative frameworks (Jarosz 2014).\nIn the Stratal OT approach to opacity (see also especially Berm\u00fadez-Otero 1999,\n2003), grammars are chained together to produce outputs that would not be possible within a single level. For example, the classic case of the opaque interaction between diphthong raising and flapping in Canadian English illustrated in (1), mitre can be produced by raising in the first (word level) grammar, and then flapping in the second (phrase level) one. As we will see shortly, the final candidate cannot be produced by a single OT grammar, with the standard set of constraints assumed for this case (though see Pater 2014).\n(1) /ma\u026at\u025a/ \u2013 Grammar 1 \u2192 [m\u028c\u026at\u025a] \u2013 Grammar 2 \u2192 [m\u028c\u026a\u027e\u025a]\nThis chaining of grammars results in a hidden structure problem (Tesar and Smolensky 2000), insofar as only the output of the final grammar, and not any earlier ones, is available in the learning data. Berm\u00fadez-Otero (2003) develops a revision to the constraint demotion algorithm that is specifically tailored to the problem of hidden structure in Stratal OT. In this paper, we instead apply a general approach to learning hidden structure in probabilistic OT to the specific case of chained grammars.\n* We would like to thank Ricardo Berm\u00fadez-Otero, Paul Boersma, Jeroen Breteler, Ivy Hauser, Jeff Heinz, Coral Hughto, Gaja Jarosz, Marc van Oostendorp, Olivier Rizzolo, Klaas Seinhorst, and Robert Staubs as well as audiences at the 21st Manchester Phonology Meeting, at the University of Massachusetts Amherst, and at the University of Amsterdam for their very helpful feedback on this paper and for stimulating discussion. We also thank the editors of this volume and two anonymous reviewers for their very helpful and useful comments. We are grateful to the National Science Foundation for supporting this work through grants BCS-0813829 and BCS-1424077. All errors are ours.\nThe variant of probabilistic OT adopted here is Maximum Entropy Grammar\n(Goldwater & Johnson 2003, Hayes & Wilson 2008). The \u2018MaxEnt\u2019 formalism has a broad basis in applied mathematics, including connectionism (Smolensky 1986); see Johnson (2013) for an overview, as well as Goldwater & Johnson (2003). MaxEnt defines the probability distribution over a candidate set in a conveniently direct way: candidate probabilities are proportional to the exponential of the weighted sum of violations.\nCandidate probabilities are illustrated in the tableau in (2), in which weights are\nindicated under constraint names (which are simplified versions of the ones used for the case study in section 3.2). The weighted sum appears in the column headed \u2018H\u2019, for Harmony (Smolensky & Legendre 2006). The exponentiation of H appears under eH, and the resulting probability (obtained by normalization of eH to sum to 1 across all candidates) under p.\n(2) Computation of probabilities over output candidates in MaxEnt\n/ma\u026at\u025a/ *a\u026at\n7\nIdent(C)\n5\nIdent(V)\n1\n*VTV\n0\nH eH p\na. ma\u026at\u025a \u20131 \u20131 \u20137 0.001 0.002\nb. ma\u026a\u027e\u025a \u20131 \u20135 0.007 0.018 c. m\u028c\u026at\u025a \u20131 \u20131 \u20131 0.368 0.973\nd. m\u028c\u026a\u027e\u025a \u20131 \u20131 \u20136 0.002 0.007\nThe *a\u026at constraint penalises the unraised diphthong [a\u026a] before a voiceless consonant, and Ident(V) penalises raising to [\u028c\u026a]. With *a\u026at having greater weight than Ident(V), raising before a voiceless stop, as in (2c), has greater probability than the faithful (2a). *VTV penalises an alveolar stop between vowels, and Ident(C) penalises the change from stop to flap. With Ident(C) weighted higher than *VTV, the probability on flapping candidates (2b) and (2d) is lowered. Candidate (2d) is the correct output in Canadian English, but it has a proper superset of the violations of (2b), since flapping also satisfies *ait. No weighting of the constraints will give [m\u028c\u026a\u027e\u025a] the highest probability in the tableau. In this respect, the situation is the same as in single-level OT, which cannot make (2d) optimal. In MaxEnt, (2d) can tie with (2b): when the constraints they violate have zero weight, and they are given equal probability (see Pater 2016 for further discussion and references on harmonic bounding and probabilistic weighted constraint theories). In (26) in section 3.2, we will see that [m\u028c\u026a\u027e\u025a] can emerge from a second chained grammar with highest probability.\nOur way of dealing with the hidden structure problem posed by Stratal OT is an\nextension of the approach to Harmonic Serialism in Staubs & Pater (2016). The probability that the grammar assigns to an output form is the sum of the probabilities of the derivations that lead to it. The probability of a derivation is the product of the probabilities of each of its component steps. The probability of each step is determined by the weights of the constraints that apply at that level (word or phrase; see section 2.2 for more details). We use a batch learning approach in which the objective is to minimise the divergence between the predictions of the grammar and the learning data, subject to a regularization term that prefers lower weights. This is a standard approach to learning MaxEnt grammars (see e.g. Goldwater & Johnson 2003, Hayes & Wilson 2008).\nJarosz (2006, ms) shows how this approach of considering every possible hidden\nstructure in generating an overt candidate can be applied to phonology in her work on Expectation Maximization with Optimality Theory. Here, we follow Eisenstat (2009), Pater et al. (2012), Johnson et al. (2015), and Staubs & Pater (2016) in adopting a weighted constraint variant of this general approach.\nWhen there is hidden structure, the learner is not guaranteed to converge on the\nglobal optimum, that is, a set of weights that best satisfy the objective function (the solution space is not guaranteed to be convex). Instead it may only find a local optimum, a set of weights that is the best within the space that the learner can explore. We provide examples of such local optima in (13-14) and (27-28) in sections 2.3 and 3.3, respectively.\nA simple and standard way of finding a global optimum in such cases is to try\ndifferent starting positions for the learner (initializations), with the hope that one or more will allow the learner to find the global optimum. In the limit, as long as the space from which we are sampling initializations contains points from which there is a path towards an optimal solution, this will allow the learner to find a global optimum. In our test cases, there were always runs of the learner that granted more than 0.5 probability to the overt forms that had highest probability in the learning data, our criterion of success. Our measure of \u2018ease of learning\u2019 for the various patterns we study is the probability that a learner will be successful, over a set of random initializations.\nIn the next section we provide more details about our grammatical and learning\ntheories, in the context of our first test case, a relatively simple example of opaque tenselax vowel alternations in French. In section 3 we turn to the opaque interaction of diphthong raising and flapping in Canadian English, and the effect of supplying evidence to the learner of phrasal non-raising. Section 4 provides an exploration of the mechanisms that lead to incorrect learning of (predominantly) opaque patterns. Finally, overall conclusions and directions for further research can be found in section 5.\n2. Case study I: Southern French tensing/laxing\n2.1 Data and analysis\nAs reported by Moreux (1985, 2006), Rizzolo (2002), and Eychenne (2014), certain dialects of Southern French have a synchronic process which tenses mid vowels in open syllables, while laxing them in closed ones (as well as open syllables followed by a schwa syllable; we will disregard this complication, and refer the reader to Selkirk 1978, Moreux 1985, and Rizzolo 2002 for potential explanations of this). This is conventionally called loi de position (\u2018law of position\u2019).\n(3) Examples of loi de position (Eychenne 2014, Rizzolo 2002)\na. /sel/  [s\u025bl] \u2018salt\u2019\n/se/  [se] \u2018knows\u2019\nb. /p\u00f8\u0281/  [p\u0153\u0281] \u2018fear\u2019\n/p\u00f8\u0281-\u00f8/  [p\u00f8.\u0281\u00f8] \u2018easily frightened\u2019\nc. /po\u0281/  [p\u0254\u0281] \u2018pore\u2019\n/po\u0281-\u00f8/  [po.\u0281\u00f8] \u2018porous\u2019\nTensing/laxing is made opaque (is counterbled) by resyllabification across word boundaries, at least in some cases1, as shown in (4b) below, where the lax vowel [\u0153] is kept in the surface open syllable [p\u0153]: (4) Opaque interaction between resyllabification and tensing/laxing (Rizzolo 2002:51)\na. /k\u0251\u0303p\u00f8\u0281/  [k\u0251\u0303.p\u0153\u0281] \u2018camper\u2019\nb. /k\u0251\u0303p\u00f8\u0281 \u0251\u0303\u0281a\u0292e/  k\u0251\u0303.p\u0153\u0281 # \u0251\u0303.\u0281a.\u0292e  [k\u0251\u0303.p\u0153.\u0281\u0251\u0303.\u0281a.\u0292e] \u2018enraged camper\u2019\nIn this simulation, we investigated whether our learner could deal with this simple case of opacity. This also serves as a simple case to illustrate in more detail the functioning of the learner. The data that we offered to the learner consisted of exceptionless loi de position as well as exceptionless opacity through resyllabification:\n(5) Opaque interaction, as in actual Southern French\na. /set a/  (s\u025bt. # a ) [s\u025b.ta] \u2018this (letter) A\u2019 <cette A>\nb. /se ta/  (se. # ta ) [se.ta] \u2018it is \u201cta\u201d\u2019 <c\u2019est \u2018ta\u2019>\nThe constraint set that we used for this subset of real French was maximally simple, as shown in (6). Correct syllabification was taken for granted in order to keep the constraint and candidate sets as small as possible so that the interactions yielding opacity are easy to see.\n(6) Constraint set used for the Southern French case study a. *[-tense]/Open : One violation mark for every [-tense] segment in an open syllable. b. *[+tense]/Closed : One violation mark for every [+tense] segment in a closed syllable. c. Ident(V) : One violation mark for every vowel that is not identical to its input correspondent.\nThe original formulation of Stratal OT (Kiparsky 2000, Bermud\u00e9z-Otero 1999) allows for three levels of derivation: a stem level, a word level, and a phrase level \u2013 which all have the same constraints, but different rankings or weightings. For this problem, we will only consider two levels \u2013 a word level and a phrase level grammar. The word level grammar evaluates each word individually, without regard to its neighbors in the phrase. By contrast, the phrase level, which operates on the output of the word level grammar, does evaluate entire phrases together.\nWith this setup, then, the opaque interaction is obtained when word level\n*[+tense]/Closed and *[-tense]/Open have high weight and word level Ident(V) has low weight, while the opposite holds at the phrase level. This corresponds to activity of the loi de position at the word level, and its inactivity at the phrase level.\nThis latter scenario is the only option to derive this opaque interaction. As can be\nseen in tableau (8) below, phrase level Markedness disprefers surface [s\u025b.ta], so that the mapping from /e/ to [\u025b] cannot be derived at the phrase level. At the same time, word level Markedness prefers the word level output that does map /e/ to [\u025b], s\u025bt # a, because\n1 Rizzolo (2002:51) reports that this effect is not unexceptional, at least in the dialect he describes. In our case study, we will investigate an idealised version of this pattern in which the opaque interaction is unexceptional.\nthe first vowel is in a closed syllable at that level, since the phrasal context is invisible. This can be seen in tableau (7a) below.\nActivity of loi de position at the word level can be expressed with the following\nweights (found at successful runs of the learner described in section 2.2):\n(7) Word level tableaux for the data in (5)\na. /set#a/ *[-tense]/Open\n6.24\n*[+tense]/Cl\n6.24\nIdent(V)\n0\nH eH p\nset # a \u20131 \u20136.24 0.002 0.00\ns\u025bt # a \u20131 0 1 1.00\nb. /se#ta/ *[-tense]/Open\n6.24\n*[+tense]/Cl\n6.24\nIdent(V)\n0\nH eH p\nse # ta 0 1 1.00\ns\u025b # ta \u20131 \u20131 \u20136.24 0.002 0.00\nAt the phrase level, giving high weight to Ident(V) and zero weight to both Markedness constraints results in 1.00 probability for all faithful mappings. This is illustrated in (9) for the phrase level derivation that takes place if the word level form output is s\u025bt. # a: phrase level outputs retain the [\u025b] that was created by closed syllable tensing at the word level with a probability of 1.00.\n(8) Phrase level tableau for (5a)\n/s\u025bt # a/ *[-tense]/Open\n0\n*[+tense]/Cl\n0\nIdent(V)\n6.93\nH eH p\nse.ta \u20131 \u20136.93 0.001 0.00 s\u025b.ta \u20131 0 1 1.00\nThese single-stratum mappings shown in (7-8) are assembled into derivational paths that lead from an underlying representation (UR) to a surface representation (SR). The probability of a single derivational path is obtained by multiplying the probabilities of every step in the path, as illustrated in (9). This is because every derivational step in Stratal OT is, by definition, independent from earlier or later derivational steps (Berm\u00fadez-Otero 1999, Kiparsky 2000)2.\n(9) Probabilities of derivational paths\na. p(/set#a/  set # a  [se.ta]) =\np(/set#a/  set # a) x p(set # a  [se.ta]) = 0.00 x 1.00 = 0.00\nb. p(/set#a/  s\u025bt # a  [s\u025b.ta]) =\np(/set#a/  s\u025bt # a) x p(s\u025bt # a  [s\u025b.ta]) = 1.00 x 1.00 = 1.00\n2 See also Odden (2011) on the Markov property traditionally ascribed to ordered phonological rules.\nThe expected probability of a surface form (more accurately, a UR/SR mapping) is the sum of the probabilities of all derivational paths that lead from the input to that surface form, as in (10).3 This table shows that, given the weights in (7-8), the desired output candidates (cf. (5)) are generated with a probability of 1.00.\n(10) Expected probabilities of UR/SR mappings: sum over all derivational paths\nUnderlying Derivational path Probability Surface Probability\n/set#a/ /set#a/  set # a  [se.ta] 0.00 [se.ta] 0.00\n/set#a/  s\u025bt # a  [se.ta] 0.00\n/set#a/  set # a  [s\u025b.ta] 0.00 [s\u025b.ta] 1.00\n/set#a/  s\u025bt # a  [s\u025b.ta] 1.00\n/se#ta/ /se#ta/  se # ta  [se.ta] 1.00 [se.ta] 1.00\n/se#ta/  s\u025b # ta  [se.ta] 0.00\n/se#ta/  se # ta  [s\u025b.ta] 0.00 [s\u025b.ta] 0.00\n/se#ta/  s\u025b # ta  [s\u025b.ta] 0.00\nNow that we have explained and illustrated the generation of probabilities over UR/SR mappings given the successful weights in (7,8), we will explain the structure of the learner that arrived at these weights in section 2.2. Then, in section 2.3, we will show how often this learner arrives at this successful set of weights within a sample of 100 runs, and what happens within that same sample when the learner does not find this analysis.\n2.2 Learning\nStaubs (2014b) provides an implementation in R (R Core Team 2013) of a variant of Staubs & Pater\u2019s (2016) approach to learning serial grammars. We modified it minimally to allow different violations of the same constraint at different derivational stages, which is sometimes necessary in a Stratal framework.\nFor instance, the constraint *[+tense]/Closed in our Southern French simulation\nwill be violated at the word level by a sequence [set # a], because even through [t] and [a] could form a separate syllable, the word boundary between them prevents this. However, the same constraint remains unviolated at the phrase level for the same sequence, since the grammar can now look beyond word boundaries and evaluate entire phrases, so that [t] and [a] do form their own syllable: [se.ta].\nExpected probabilities of UR/SR mappings are computed given sets of candidates,\nviolation vectors, and constraint weights, as described and illustrated in section 2.1.\n3 Since there are several derivational paths per surface form, but only one surface form per derivational path, UR/SR mappings are bundles of non-overlapping sets of derivational paths. This means that, in computing expected probabilities of UR/SR mappings, each derivational path\u2019s probability is counted once, so that UR/SR probabilities are guaranteed to add up to 1.\nIn addition, the learner is also given observed probabilities of UR/SR mappings are provided to the learner. Since we are working with categorical data, all UR/SR mappings found in the data were given a probability of 1 on all UR/SR mappings found in the data, and all others were given a probability of 0 on all others. The learner then minimises the Kullback-Leibler-divergence (KL-divergence; Kullback & Leibler 1951) of the expected probabilities from the observed probabilities, which is a measure of how closely the grammar has been able to fit the data.\n(11) KL-divergence\nFor all UR/SR mappings URSRi=1, \u2026, URSRi=k:\n\ud835\udc37\ud835\udc3e\ud835\udc3f(\ud835\udc43\ud835\udc5c\ud835\udc4f\ud835\udc60 \u2225 \ud835\udc43\ud835\udc52\ud835\udc65\ud835\udc5d) = \u2211 \ud835\udc5d\ud835\udc5c\ud835\udc4f\ud835\udc60(\ud835\udc48\ud835\udc45 \u2192 \ud835\udc46\ud835\udc45\ud835\udc56) \u00d7 log ( \ud835\udc5d\ud835\udc5c\ud835\udc4f\ud835\udc60(\ud835\udc48\ud835\udc45 \u2192 \ud835\udc46\ud835\udc45\ud835\udc56)\n\ud835\udc5d\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc48\ud835\udc45 \u2192 \ud835\udc46\ud835\udc45\ud835\udc56) )\n\ud835\udc58\n\ud835\udc56=1\nMinimization of this objective function is done by the L-BFGS-B method of optimization (Byrd et al. 1995). This is a pseudo-Newtonian batch optimization method which uses approximations of the first-order and second-order derivatives of the objective function to take incremental steps towards an optimum. The \u2018optim\u2019 implementation of this method in R was used, specifying a minimum of zero on all constraint weights.\nTo place an upper bound on the objective function, an L2 regularisation term\n(Hoerl & Kennard 1970) with  = 0 and 2 = 10,000 is added to it. This regularisation term penalises weights as they increase, keeping the optimal solution finite, and driving a constraint\u2019s weight down to zero whenever it is not helpful in the analysis.\nSummarising, our algorithm calculates probabilities of derivations in Stratal\nMaxEnt, sums over them to get the expected probabilities of overt forms, and fits the observed distribution of UR/SR mappings by minimising KL-divergence.\n2.3 Results\nThe learner described in section 2.2 was run 100 times on the French loi de position data, with every run starting from random starting weights for each constraint, drawn i.i.d. from a uniform distribution over [0, 10]. The results are as follows:\n(12) Results for the Southern French data set\nData set Learned successfully out\nof 100 runs\n/set#a/  [s\u025b.ta] /se#ta/  [se.ta] 96\nThus, opaque loi de position was learned successfully for an overwhelming majority of start weights. However, there is still a slight chance of incorrect learning, which we defined as a probability of 0.5 or less on at least one surface form that has a probability of 1 in the learning data (see also section 1). The 4 unsuccessful runs yielded grammars which had free variation between tense [e] and lax [\u025b]:\n(13) UR/SR mapping probabilities for the local optimum\n/set#a/ probability /se#ta/ probability [se.ta] 0.50 [se.ta] 0.50\n[s\u025b.ta] 0.50 [s\u025b.ta] 0.50\nThis is a local optimum that arises when all constraint weights are set to zero, as in (14). KL-divergence from the actual data (our measure of error) is smaller for these weights than for any other weights within the weight space explored by the learner.\n(14) Weights that generate the local optimum\nWord level Phrase level *[-tense]/Open *[+tense]/Cl Ident(V) *[-tense]/Open *[+tense]/Cl Ident(V) 0 0 0 0 0 0\nAs will be explained in section 4.1, an initially incorrect weighting at the word level leads to a tendency for phrase level Faithfulness to lower its weight. At the same time, when phrase level Ident(V) reaches zero weight, any effect of closed syllable laxing from the word level cannot be transmitted to the final output. Once this happens, 0.50 probability for all candidates is the best possible fit to the data, and the all-zero weight version of that solution minimises the penalty on the regularization term, which prefers smaller weights.\nIt could be argued that this weight set is not a local optimum in the strict sense.\nThere is a locally available solution that is better than all-zero weights: assigning minimal weight (0+\u025b for any positive \u025b) to word level *[-tense]/Open and *[+tense]/Closed as well as phrase level Ident(V), while keeping all other constraints at 0, will lower the objective function with respect to all-zero weights, as this will nudge the probabilities of both\n/set#a/  [s\u025b.ta] and /se#ta/  [se.ta] slightly above 0.50. However, for the reasons outlined above, the learner cannot see this possibility when it retreats to all-zero weights.\n3 Case study II: Raising and flapping in Canadian English\n3.1 Data\nOur second case study is a classic case of opacity, attested in Canadian English (Joos 1942, Idsardi 2000 and references therein, Pater 2014): the interaction between Canadian Raising and flapping. Low-nucleus diphthongs [a\u026a, a\u028a] raise to [\u028c\u026a, \u028c\u028a] before voiceless consonants, as in (15a), and coronal oral stops /t, d/ become a voiced flap [\u027e] in a variety of contexts (De Jong 2011) \u2013 among others, between two vowels if the first is stressed and the second is not, as in (15b).\nThe two processes interact in a counterbleeding way: the fact that flapping cancels\nthe voicelessness of underlying /t/ does not prevent that /t/ from triggering raising. This is illustrated in (15c).\n(15) Canadian English counterbleeding interaction\na. /la\u026af/  [l\u028c\u026af] \u2018life\u2019 cf. /la\u026a/  [la\u026a] \u2018lie\u2019\nb. /k\u028ct-\u025a/  [\u02c8k\u028c\u027e\u025a] \u2018cutter\u2019\nc. /ma\u026at\u025a/  [m\u028c\u026a\u027e\u025a] \u2018mitre\u2019 cf. /sa\u026ad\u025a/  [sa\u026a\u027e\u025a] \u2018cider\u2019\nIn addition, raising is restricted to the word domain, as illustrated in (16a), while there is no evidence of such a restriction for flapping, as in (16b). This fits with a stratal analysis in which raising applies at the word level only, while flapping applies at the phrase level only \u2013 as illustrated in (17).\n(16) Word-boundedness of raising\na. /la\u026a#f\u0254\u0279/  [la\u026a f\u0254\u0279] \u2018lie for\u2019 *[l\u028c\u026a f\u0254\u0279], cf. [l\u028c\u026af]\nb. /la\u026a#tu/  [la\u026a \u027e\u0259] \u2018lie to\u2019 *[l\u028c\u026a \u027e\u0259], cf. [m\u028c\u026a\u027e\u025a]\n(17) Sketch of stratal analysis of flapping and raising\n/ma\u026at\u025a/  m\u028c\u026at\u025a  m\u028c\u026a\u027e\u025a Underlying Word Level: Phrase Level: Representation Raising only Flapping only\nThe transparent counterpart to this pattern, claimed to be spoken by some Canadian English speakers (Joos 1942), is a language which also has both raising and flapping as in (15ab) and (16), but the application of flapping blocks the application of raising, as in (18), since the flap [\u027e] is not voiceless. However, the existence of this transparent dialect has been disputed in later literature (Kaye 1990). In our simulations, we use this (perhaps hypothetical) transparent dialect as a non-opaque baseline against which to compare the mainstream, opaque dialect of Canadian English.\n(18) Transparent interaction between raising and flapping\n/ma\u026at\u025a/  [ma\u026a\u027e\u025a] \u2018mitre\u2019\nIn the next subsection, we will investigate the learnability of the opaque and the transparent versions of the Canadian English data. We will consider the contrast between opacity and transparency, and between various possible datasets.\n3.2 Simulation setup\nCanadian English provides at least two pieces of independent evidence regarding the stratal affiliation of the opaque raising process: evidence for its application outside the flapping context, as in (15a), and evidence for its word-boundedness, as in (16a). Based on Kiparsky (2000), we predict the availability of such evidence to make the opaque interaction more learnable. To investigate this, we considered four data sets representing various degrees of independent evidence for the stratal affiliation of the opaque process.\n\u2018Mitre-cider\u2019 has no independent evidence regarding the opaque process \u2013 its\nsynchronic activity and stratal affiliation must be inferred from the opaque interaction. \u2018Mitre-cider-life\u2019 has evidence for the opaque process\u2019 applying outside of the interaction, since the [f] in \u2018life\u2019 is not subject to flapping: raising must be synchronically active at some stratum. \u2018Mitre-cider-lie-for\u2019 provides evidence for the non-application of raising at the phrase level, since \u2018lie for\u2019 does not undergo raising despite having [f] after the diphthong across a word boundary. Finally, \u2018mitre-cider-life-lie-for\u2019 combines both\npieces of evidence. These data sets were investigated with both an opaque and a transparent interaction between raising and flapping.\n(19) Opaque datasets for Canadian English\n\u2018mitre-cider\u2019 \u2018mitre-cider-life\u2019 \u2018mitre-cider-lie-for\u2019 \u2018mitre-cider-life-lie-\nfor\u2019\nO p aq\nu e\n/ma\u026at\u025a/  [m\u028c\u026a\u027e\u025a]\n/sa\u026ad\u025a/  [sa\u026a\u027e\u025a]\n/ma\u026at\u025a/  [m\u028c\u026a\u027e\u025a]\n/sa\u026ad\u025a/  [sa\u026a\u027e\u025a]\n/la\u026af/  [l\u028c\u026af]\n/ma\u026at\u025a/  [m\u028c\u026a\u027e\u025a]\n/sa\u026ad\u025a/  [sa\u026a\u027e\u025a]\n/la\u026a#f\u0254\u0279/  [la\u026a f\u0254\u0279]\n/ma\u026at\u025a/  [m\u028c\u026a\u027e\u025a]\n/sa\u026ad\u025a/  [sa\u026a\u027e\u025a]\n/la\u026af/  [l\u028c\u026af]\n/la\u026a#f\u0254\u0279/  [la\u026a f\u0254\u0279]\nT ra\nn sp\nar en\nt /ma\u026at\u025a/  [ma\u026a\u027e\u025a]\n/sa\u026ad\u025a/  [sa\u026a\u027e\u025a]\n/ma\u026at\u025a/  [ma\u026a\u027e\u025a]\n/sa\u026ad\u025a/  [sa\u026a\u027e\u025a]\n/la\u026af/  [l\u028c\u026af]\n/ma\u026at\u025a/  [ma\u026a\u027e\u025a]\n/sa\u026ad\u025a/  [sa\u026a\u027e\u025a]\n/la\u026a#f\u0254\u0279/  [la\u026a f\u0254\u0279]\n/ma\u026at\u025a/  [ma\u026a\u027e\u025a]\n/sa\u026ad\u025a/  [sa\u026a\u027e\u025a]\n/la\u026af/  [l\u028c\u026af]\n/la\u026a#f\u0254\u0279/  [la\u026a f\u0254\u0279]\nOur learner had access to the four constraints in (20). As in the Southern French case, only two derivational levels were used: a word level and a phrase level.\nWhenever *a\u026a,a\u028a/_[-voice] is sufficiently higher weighted than Ident(low), the\nraising process will be in effect. The flapping process is active when *V\u0301TV is sufficiently above Ident(sonorant)4.\n(20) Constraint set for Canadian Raising simulations a. Ident(low) : One violation mark for raising or lowering a diphthong. b. Ident(sonorant) : One violation mark for any underlying consonant whose [\u00b1sonorant] specification is not identical to that of its output correspondent. This constraint penalises the transition from /t, d/ to flap, or vice versa. c. *V\u0301TV : One violation mark for an alveolar stop [t, d] in between two vowels of which the first is stressed and the second is not. d. *a\u026a,a\u028a/_[-voice] : One violation mark for a non-raised diphthong before a voiceless consonant.\nThe opaque interaction is captured if there is raising but no flapping at the word level, and flapping but no raising at the phrase level (see section 1 on the impossibility of capturing the interaction within one stratum). This is reflected in the weights found at successful learning trials for \u2018mitre-cider-life-lie-for\u2019. At the word level, *a\u026a,a\u028a/_[-voice] is far above Ident(low), and *V\u0301TV is far below Ident(sonorant); the opposite is true of the phrase level:\n4 We assume that the transition from /t,d/ to [\u027e] entails a change in [\u00b1sonorant].\n(21) Sample successful weights for opaque \u2018mitre-cider-life-lie-for\u2019 Word level Phrase level Ident (son) Ident (low) *V\u0301TV *a\u026a,a\u028a/_ [-vce] Ident (son) Ident (low) *V\u0301TV *a\u026a,a\u028a/_ [-vce]"}, {"heading": "10.44 5.02 0 11.13 0 6.81 6.12 0", "text": "The word level tableau in (22) below shows that the word level weights above ensure raising but no flapping at the word level.\n(22) Word level tableau for \u2018mitre\u2019\n/ma\u026at\u025a/ Ident(son)\n10.44\nIdent(low)\n5.02\n*V\u0301TV\n0\n*a\u026a,a\u028a/_[-vce]\n11.13\nH eH p\nma\u026at\u025a \u20131 \u20131 \u201311.13 0.00001 0.00\nma\u026a\u027e\u025a \u20131 \u201310.44 0.00003 0.00\nm\u028c\u026at\u025a \u20131 \u20131 \u20135.02 0.007 0.99 m\u028c\u026a\u027e\u025a \u20131 \u20131 \u201315.46 0.00000 0.00\nThe candidates considered at the phrase level are the same as the candidate set at the word level.6 As can be seen in the phrase level tableaux in (23), it is essential for the generation of these probabilities that /ma\u026at\u025a/ exhibit raising at the word level, since the phrase level must preserves diphthongs faithfully, or else /ma\u026at\u025a/ will not come out with both raising and flapping (i.e., as attested [m\u028c\u026a\u027e\u025a]).\n(23) Two of the phrase level tableaux for \u2018mitre\u2019\na. m\u028c\u026at\u025a Ident(son)\n0\nIdent(low)\n6.81\n*V\u0301TV\n6.12\n*a\u026a,a\u028a/_[-vce]\n0\nH eH p\n[ma\u026at\u025a] \u20131 \u20131 \u20131 \u201312.93 0.000 0.00 [ma\u026a\u027e\u025a] \u20131 \u20131 \u20136.81 0.001 0.00\n[m\u028c\u026at\u025a] \u20131 \u20136.12 0.002 0.00 [m\u028c\u026a\u027e\u025a] \u20131 0 1 1.00\nb. ma\u026a\u027e\u025a Ident(son)\n0\nIdent(low)\n6.81\n*V\u0301TV\n6.12\n*a\u026a,a\u028a/_[-vce]\n0\nH eH p\n[ma\u026at\u025a] \u20131 \u20131 \u20131 \u20136.12 0.002 0.00\n[ma\u026a\u027e\u025a] 0 1 1.00\n[m\u028c\u026at\u025a] \u20131 \u20131 \u20131 \u201312.93 0.000 0.00\n[m\u028c\u026a\u027e\u025a] \u20131 \u20136.81 0.001 0.00\n6 We did not consider derivational paths with the changes /t/\u027e [d] or /d/\u027e [t]: the phrase level candidate set did not include [m{a,\u028c}\u026ad\u025a] or [s{a,\u028c}\u026at\u025a], so that every tableau had four candidates. However, we are confident that the presence of these candidates would not have significantly changed our results: as shown in (23b), candidates that change \u027e to [t] or [d] at the phrase level incur extra violations of Ident(son) and *V\u0301TV, while not yielding any improvement on the other two constraints.\nThe surface form probabilities resulting from the weights in (21) match the actual opaque Canadian English data extremely closely, as shown in (24).\n(24) Surface form probabilities generated by (21) /ma\u026at\u025a/ probability /sa\u026ad\u025a/ probability /la\u026af/ probability /la\u026a#f\u0254\u0279/ probability [ma\u026at\u025a] 0.00 [sa\u026ad\u025a] 0.00 [la\u026af] 0.00 [la\u026a f\u0254\u0279] 0.99 [ma\u026a\u027e\u025a] 0.00 [sa\u026a\u027e\u025a] 0.99 [m\u028c\u026at\u025a] 0.00 [s\u028c\u026ad\u025a] 0.00 [l\u028c\u026af] 1.00 [l\u028c\u026a f\u0254\u0279] 0.01\n[m\u028c\u026a\u027e\u025a] 0.99 [s\u028c\u026a\u027e\u025a] 0.00\nAs opposed to the opaque interaction, the transparent interaction does not need to time flapping after raising. In fact, for transparent \u2018mitre-cider\u2019 and \u2018mitre-cider-lie-for\u2019, raising need not be represented in the grammar at all, because the raised diphthong is absent from the data \u2013 see the lack of weight on *a\u026a,a\u028a/_[-voice] at either level in (25a-b).\nFor transparent \u2018mitre-cider-life\u2019, raising can be represented either at the word\nlevel, by giving high weight to word level *a\u026a,a\u028a/_[-voice] and zero weight to word level Ident(sonorant), as in (25c) \u2013 or at the phrase level by giving high weight to *V\u0301TV and *a\u026a,a\u028a/_[-voice] at the phrase level, as in (25d).\nTransparent \u2018mitre-cider-life-lie-for\u2019, however, does require raising to take place\nat the word level, because the non-raised diphthong in /la\u026a#f\u0254\u0279/  [la\u026a f\u0254\u0279] precludes raising at the phrase level. The weights found for this dataset are very similar to the second set of weights for \u2018mitre-cider-life\u2019, as shown in (25e).\n(25) Sample weights for successful runs of various transparent datasets\nDataset (transparent)\nWord level Phrase level Ident (son) Ident (low) *V\u0301TV *a\u026a,a\u028a/_ [-vce] Ident (son) Ident (low) *V\u0301TV *a\u026a,a\u028a/_ [-vce]\na. \u2018mitre-cider\u2019 0 6.78 0 0 0 6.78 6.78 0 b. \u2018mitre-cider-\nlie-for\u2019 0 7.15 0 0 0 7.15 6.75 0\nc. \u2018mitre-cider-\nlife\u2019, var. 1 0 6.41 0 0 0 5.72 5.73 11.45\nd. \u2018mitre-cider-\nlife\u2019, var. 2 0 6.07 0 11.74 0 6.77 6.36 0\ne. \u2018mitre-cider-\nlife-lie-for\u2019 0 6.34 0 11.98 0 7.03 6.34 0\nThus, the opaque interaction of raising and flapping requires raising at the word level and flapping at the phrase level. The transparent interaction, however, only requires flapping at the word level, while raising can be represented at either level.\nFinally, the addition of both \u2018life\u2019 and \u2018lie for\u2019 to the transparent data set leads to\nthe necessity of representing raising at the word level, even though this is not required for the transparent interaction otherwise.\n3.3 Results\nSimulations were run as described in section 2.2, except that L2 regularization was made stronger by setting 2 to 9,000 instead of 10,000 to prevent the learner from considering constraint weights that tend towards infinity for the opaque \u2018mitre-cider\u2019 dataset.\nAll four data sets were examined with both opaque and transparent interaction\nbetween raising and flapping. The same 100 sets of initializations drawn i.i.d. from a uniform distribution over [0,10] were used for all 8 datasets. The results are as follows:\n(26) Results for Canadian English, for 100 sets of initializations\nDataset Opaque: learned successfully\nout of 100 runs\nTransparent: learned successfully out of 100 runs\n\u2018mitre-cider\u2019 51 100 \u2018mitre-cider-life\u2019 61 99\n\u2018mitre-cider-lie-for\u2019 87 100 \u2018mitre-cider-life-lie-for\u2019 92 93\nAs can be seen in (29), independent evidence regarding the opaque process yields a clear increase in learnability for the opaque cases. Evidence that the opaque process is wordbounded (\u2018lie for\u2019) has a stronger effect than evidence for independent activity of the opaque process (\u2018life\u2019).\nFor all transparent datasets except \u2018mitre-cider-life-lie-for\u2019, performance is\n(almost) at ceiling. For \u2018mitre-cider-life-lie-for\u2019, however, the opaque and transparent versions are learned at a near equal rate.\nWhenever (opaque or transparent) Canadian English was not learned successfully\n(i.e., at least one attested SR was given a probability of 0.5 or less by the grammar), the learner ended up in one of the 4 local optima that are summarised in (27). The table lists, for each underlying representation, the surface representations that have more than 0.00 probability in that local optimum. The symbol \u2018~\u2019 will be used as a shorthand for 0.50 probability on both surface representations shown, unless indicated otherwise. Weights that generate each local optimum are given in (28).\n(27) Local optima found for Canadian English simulations\nOptimum Occurs in: Inputs Outputs\nI\nOpaque\n\u2018mitre-cider\u2019, \u2018mitre-cider-lie-for\u2019, \u2018mitre-cider-life-lie-for\u2019\n/ma\u026at\u025a/ /sa\u026ad\u025a/\n(/la\u026af/)\n(/la\u026a#f\u0254\u0279/)\n[m\u028c\u026a\u027e\u025a]~[ma\u026a\u027e\u025a]\n[s\u028c\u026a\u027e\u025a]~[sa\u026a\u027e\u025a]\n([la\u026af]~[l\u028c\u026af])\n([la\u026a f\u0254\u0279]~[l\u028c\u026a f\u0254\u0279])\nII\nOpaque\n\u2018mitre-cider-lie-for\u2019, \u2018mitre-cider-life-lie-for\u2019\n/ma\u026at\u025a/ /sa\u026ad\u025a/\n(/la\u026af/)\n/la\u026a#f\u0254\u0279/\n[m\u028c\u026a\u027e\u025a] 0.67 ~ [ma\u026a\u027e\u025a]\n[s\u028c\u026a\u027e\u025a] 0.67 ~ [sa\u026a\u027e\u025a]\n([l\u028c\u026af])\n[la\u026a f\u0254\u0279] 0.67 ~ [l\u028c\u026a f\u0254\u0279]\nIII\nOpaque\n\u2018mitre-cider-life\u2019\nTransparent\n\u2018mitre-cider-life\u2019\n/ma\u026at\u025a/ /sa\u026ad\u025a/\n/la\u026af/\n-\n[m\u028c\u026a\u027e\u025a]~[ma\u026a\u027e\u025a]\n[s\u028c\u026a\u027e\u025a]~[sa\u026a\u027e\u025a]\n[l\u028c\u026af]\nIV\nTransparent\n\u2018mitre-cider-life-lie-for\u2019\n/ma\u026at\u025a/ /sa\u026ad\u025a/\n/la\u026af/\n/la\u026a#f\u0254\u0279/\n[ma\u026a\u027e\u025a]\n[sa\u026a\u027e\u025a]\n[la\u026af]~[l\u028c\u026af]\n[la\u026a f\u0254\u0279]~[l\u028c\u026a f\u0254\u0279]\n(28) Sample weights for local optima\nWord level Phrase level\nIdent (son) Ident (low) *V\u0301TV *a\u026a,a\u028a/_ [-vce] Ident (son) Ident (low) *V\u0301TV *a\u026a,a\u028a/_ [-vce]\nLocal optimum I 0 0 0 0 0 0 7.75 0\nLocal optimum II 0 6.60 0 0 0 5.91 5.92 5.90 Local optimum III 0 0.69 6.14 6.54 6.51 5.86 0.04 0\nLocal optimum IV 0 0 0 0 0 0 7.01 6.60\nThese four local optima have in common that they try to represent both processes with minimal appeal to the interaction between levels. For instance, local optima I and IV are attempts to represent the data without appealing to underlying representations, by setting all Faithfulness constraints to zero.\nLocal optimum II does not represent the raising process at the word level, as\nnecessary for any data set that involves \u2018life\u2019 with a raised diphthong and \u2018lie for\u2019 with a non-raised diphthong (see section 3.2). This means that the vowel contrast between \u2018life\u2019 and \u2018lie for\u2019 has to be modeled as within-word variation.\nFinally, local optimum III is a consequence of representing both raising and\nflapping at the word level. Since applying raising and flapping at the same level leads to lack of raising in \u2018mitre\u2019, variation between raising and non-raising is created by lowering the weight of word level Ident(low).\nAs in the case of Southern French, local optima I and IV are not a local optima in\nthe narrow sense: if the weights of word level Ident(son), Ident(low) (in the case of local optimum I), and **a\u026a,a\u028a/_[-vce] and phrase level Ident(low) are increased by even a little bit, this will decrease the value of the objective function. It is the \u2018bottleneck effect\u2019 (see section 4.1) that leads the learner to land in this state. However, local optima II and III are true local optima in the sense that any neighboring values will increase the value of the\nobjective function.\nAs a final note, none of these outcomes represent opaque analyses. Local optima\nII and III resemble opaque analyses, since they both assign a significant amount of probability to [m\u028c\u026a\u027e\u025a], but they crucially ignore the difference between underlying /t/ in \u2018mitre\u2019 and underlying /d/ in \u2018cider\u2019 and allow unmotivated raising in both words (because the constraints that regulate raising are either at 0 or tied with other constraints).\n3.4 Summary\nSummarising, we have found that independent evidence about the opaque pattern\u2019s stratal affiliation can significantly improve the learnability of the opaque interaction \u2013 especially the addition of evidence that the opaque process does not apply at the phrase level. Furthermore, the presence of both \u2018lie\u2019 and \u2018lie for\u2019 made the transparent and the opaque interaction equally learnable, in fact destroying the learning advantage of the transparent interaction over the opaque one.\nWhenever the languages are not learned successfully, either phrase level\nFaithfulness is given zero weight, making it impossible to transfer information from the word level to the phrase level, or raising and flapping are represented at the same level when they need to be represented at different levels. We will now turn to a discussion of what we call the \u2018bottleneck effect\u2019, an obstacle posed by the current hidden structure learning problem that leads to the learner\u2019s landing in local optima, as well as ways in which evidence for stratal affiliation helps the learner overcome this obstacle.\n4. Difficulties in learning hidden structure\n4.1 Cross-level dependencies\nThe relative difficulty of learning opaque interactions that we found in our results\nseems to stem from the fact that the effectiveness of the weightings on each level depends on the weights on the other level. Specifically, high weight on phrase level Faithfulness is only effective when word level constraints are weighted appropriately, while the result of word level weighting can only be transmitted to the surface representation when phrase level Faithfulness has a high enough weight.\nWe will show here how failure to find appropriate weights on both word level\nconstraints and phrase level Faithfulness simultaneously leads to local optima, and we will show that this scenario is more likely to occur in opaque cases than in transparent cases. Furthermore, we will show how the types of evidence for stratal affiliation of the opaque process offered in the Canadian English datasets increase the likelihood that the learner will find the global optimum.\nWhen the learner has not found weights that generate a desirable distribution at\nthe word level, the learner gets closer to its objective by lowering the weights of phrase level Faithfulness instead of making these weights higher. For instance, consider the weighting below, which leads to a local optimum for opaque \u2018mitre-cider\u2019:\n(29) Sample initialization which leads to local optimum for \u2018mitre-cider\u2019\nWord level Phrase level Ident(son) Ident(lo) *V\u0301TV *a\u026a,a\u028a/_[-vce] Ident(son) Ident(lo) *V\u0301TV *a\u026a,a\u028a/_[-vce]\n1 7 3 1 0 6.28 6.29 0\nAt the word level, this weighting gives a non-raised diphthong in \u2018mitre\u2019 highest probability, because high-weighted *V\u0301TV and low-weighted *a\u026a,a\u028a/_[-vce] leads to flapping, which blocks diphthong raising. At the same time, it has flapping and lack of raising on the phrase level, as desired for the opaque interaction (cf. section 3.2). This is illustrated for the word level tableau in (30), and the phrase level tableaux in (31).\n(30) Word level tableau for \u2018mitre\u2019 for the weights in (29)\n/ma\u026at\u025a/ Ident(son)\n1\nIdent(lo)\n7\n*V\u0301TV\n3\n*a\u026a,a\u028a/_[-vce]\n1\nH eH p\nma\u026at\u025a \u20131 \u20131 \u20134 0.01 0.05 ma\u026a\u027e\u025a \u20131 \u20131 0.37 0.95\nm\u028c\u026at\u025a \u20131 \u20131 \u201310 0.00 0.00 m\u028c\u026a\u027e\u025a \u20131 \u20131 \u20138 0.00 0.00\n(31) Two of the phrase level tableaux for \u2018mitre\u2019 for the weights in (29)\na. m\u028c\u026at\u025a Ident(son)\n0\nIdent(lo)\n6.28\n*V\u0301TV\n6.29\n*a\u026a,a\u028a/_[-vce]\n0\nH eH p\n[ma\u026at\u025a] \u20131 \u20131 \u20131 \u201312.57 0.000 0.00 [ma\u026a\u027e\u025a] \u20131 \u20131 \u20136.28 0.002 0.00 [m\u028c\u026at\u025a] \u20131 \u20136.29 0.002 0.00\n[m\u028c\u026a\u027e\u025a] \u20131 0 1 1.00\nb. ma\u026a\u027e\u025a Ident(son)\n0\nIdent(lo)\n6.28\n*V\u0301TV\n6.29\n*a\u026a,a\u028a/_[-vce]\n0\nH eH p\n[ma\u026at\u025a] \u20131 \u20131 \u20131 \u20136.29 0.002 0.00\n[ma\u026a\u027e\u025a] 0 1 1.00\n[m\u028c\u026at\u025a] \u20131 \u20131 \u20131 \u201312.93 0.000 0.00\n[m\u028c\u026a\u027e\u025a] \u20131 \u20136.28 0.002 0.00\nIn cases like this, KL-divergence decreases (and fit to the data increases) as the weight of phrase level Ident(low) at the phrase level goes to zero.\n(32) a. Observed distribution for opaque \u2018mitre\u2019 and \u2018cider\u2019\n/ma\u026at\u025a/ /sa\u026ad\u025a/\n[ma\u026at\u025a] [ma\u026a\u027e\u025a] [m\u028c\u026at\u025a] [m\u028c\u026a\u027e\u025a] [sa\u026ad\u025a] [sa\u026a\u027e\u025a] [s\u028c\u026ad\u025a] [s\u028c\u026a\u027e\u025a]\n0 0 0 1 0 1 0 0\nb. KL-divergence for grammars with word level weights as in (29) and various weights of phrase level Ident(low)\nWeights (word level) /ma\u026at\u025a/ /sa\u026ad\u025a/ DKL\nIdent (son) Ident (low) *V\u0301TV *a\u026a/_ [-vce] [a\u026at] [a\u026a\u027e] [\u028c\u026at] [\u028c\u026a\u027e] [a\u026at] [a\u026a\u027e] [\u028c\u026at] [\u028c\u026a\u027e]\n0 6.28 6.29 0 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 5.87 0 1 6.29 0 0.00 0.72 0.00 0.27 0.00 0.72 0.00 0.27 1.63 0 0 6.29 0 0.00 0.50 0.00 0.50 0.00 0.50 0.00 0.50 1.39\nIf the learner has assigned zero weight to phrase level Faithfulness, moving towards appropriate weights at the word level does not lower KL-divergence. This is illustrated in (33) below. Word level and phrase level expected probabilities are shown for \u2018mitre\u2019 only, but KL-divergence is computed for both \u2018mitre\u2019 and \u2018cider\u2019. Phrase level constraint weights are as given in (29), except for Ident(low), whose weight is set to zero.\n(33) KL-divergence for \u2018mitre-cider\u2019 when phrase level Ident(low) has zero weight\nWeights Word level outputs\nfor /ma\u026at\u025a/\nSurface representations\nDKL\nIdent (son) Word Ident (low) Word *V\u0301TV Word *a\u026a/_ [-vce] Word Ident (low) Phrase a\u026at a\u026a\u027e \u028c\u026at \u028c\u026a\u027e [a\u026at] [a\u026a\u027e] [\u028c\u026at] [\u028c\u026a\u027e]\n1 7 3 1 0 .05 .95 .00 .00 .00 .50 .00 .50 1.39 4 3 2 4 0 .09 .64 .23 .03 .00 .50 .00 .50 1.39 6 3 1 6 0 .04 .11 .84 .01 .00 .50 .00 .50 1.39 7 3 0 7 0 .02 .02 .96 .00 .00 .50 .00 .50 1.39\nAs can be seen in (33), raising the weights of word level Ident(low) and *V\u0301TV and lowering those of word level Ident(sonorant) and *a\u026a,a\u028a/_[-voice] leads to a desirable result at the word level: raising and no flapping (cf. (23) in section 3.2). However, this information is not factored into the distribution over UR/SR mappings if phrase level Ident(low) has a weight of 0.\nAt the same time, phrase level Ident(low) has a motivation to decrease its weight\nuntil word level outputs with a raised diphthong gain a cumulative probability of at least 0.5. The lower the weight of Ident(low), the closer the winning candidate will be to having 0.5 probability, since setting Ident(low) to zero means that the phrase level will consider raised diphthongs and non-raised ones with equal probability. As long as the current word level weights are such that the winning candidate has less than 0.5 probability, this is an improvement. In the particular case discussed here, Ident(low) will only be motivated to have non-zero weight when the word level gives a boost to raised diphthong outputs, so that the probability of [m\u028c\u026a\u027e\u025a] increases as the weight of Ident(low) increases.\nTaken together, this means that if the current word level weights lead to lower\nprobability on the attested candidate compared to all-zero weights at the word level, then phrase level Faithfulness might be set to zero before a more desirable set of word level\nweights might be found.7 We will call this the bottleneck effect: word level information needs to travel through phrase level Faithfulness constraints in order to have an effect on the distribution over UR/SR mappings. The bottleneck effect is not limited to just this data set, but is applies to any data set with dependence between levels, including other Canadian English data sets (see 4.2 below), and the Southern French case (see 2.3 above).\n4.2 Advantage from evidence for stratal affiliation\nAs we showed in section 3.3, the addition of various kinds of evidence for the stratal affiliation of the opaque process (raising) dramatically increases the likelihood that the opaque interaction will be learned.\nAdding \u2018life\u2019 to the opaque interaction means that raising at either the word level\nor the phrase level will be rewarded, regardless of the weighting of the flapping constraint *V\u0301TV. This means that increasing the weight of word level *a\u026a,a\u028a/_[-voice] leads to a sharper drop in KL-divergence for opaque \u2018mitre-cider-life\u2019 than for opaque \u2018mitre-cider-lie-for\u20198.\nIn this manner, the presence of \u2018life\u2019 in the data set means a sharp increase in the\ngradient of the word level constraint *a\u026a,a\u028a/_[-voice] in a position in the weight space that would otherwise have a low gradient for that constraint. We have not studied how the exact numerical increase in the gradient of this constraint, which depends on the relative frequency of data points like \u2018life\u2019 in the total corpus (we only considered a minimal corpus here), relates to learnability rates \u2013 this is a matter for future research. However, we can confidently say that the categorical presence of raising outside its interaction with flapping has a positive influence on the learnability of the interaction.\n(34) Adding \u2018life\u2019 leads to a stronger effect of representing raising at the word level\nWord level Phrase level DKL DKL Ident (son) Ident (low) *V\u0301TV *a\u026a,a\u028a/ _[-vce] Ident (son) Ident (low) *V\u0301TV *a\u026a,a\u028a/ _[-vce] \u2018mitre-\ncider\u2019\n\u2018mitrecider-life\u2019\n1 7 3 1 0 6.28 6.29 0 5.87 11.31 1 7 3 7 0 6.28 6.29 0 5.85 6.54\n1 7 3 14 0 6.29 6.29 0 5.85 5.85\nAdding \u2018lie for\u2019 to the opaque interaction means that high weight on phrase level Ident(low) is penalised less strongly when the word level weights do not generate the desirable candidates with high probability9. This is because word \u2018lie\u2019 itself contains\n7 More generally, the effect happens when the current word level weights produce a distribution over surface candidates that has a higher KL-divergence from the actual distribution than a uniform distribution (i.e., when the word level grammar makes it so that predictions for surface forms go in the opposite direction of the actual data). 8 However, lowering the weight of phrase level Ident(low) is also rewarded more strongly compared to opaque \u2018mitre-cider\u2019, so that the bottleneck effect becomes stronger with the addition of \u2018life\u2019. This is probably why \u2018life\u2019 only had a modest effect on learnability. 9 Another effect of adding \u2018lie for\u2019 is that high weight on phrase level *a\u026a,a\u028a/_[-voice] is penalised more strongly, because this constraint prefers a raised diphthong in \u2018lie for\u2019.\nneither a flapping nor a raising context, so that flapping and raising constraints do not interact with the identity of the diphthong at the word level. It is not necessary to determine the mutual weighting of word level *V\u0301TV and Ident(son) or *a\u026a,a\u028a/_[-voice] and Ident(low) to give the desirable word level output for \u2018lie for\u2019 more than 0.5 probability \u2013 non-zero weight on word level Ident(son) is sufficient.\n(35) Adding \u2018lie for\u2019 makes it less attractive to lower weight on Ident(low)\nWord level Phrase level DKL DKL Ident (son) Ident (low) *V\u0301TV *a\u026a,a\u028a/ _[-vce] Ident (son) Ident (low) *V\u0301TV *a\u026a,a\u028a/ _[-vce] \u2018mitre-\ncider\u2019\n\u2018mitre-ciderlie-for\u2019\n1 7 3 1 0 6.28 6.29 0 5.87 5.87 1 7 3 1 0 1 6.29 0 1.63 1.94 1 7 3 1 0 0 6.29 0 1.39 2.08\nWhen both \u2018life\u2019 and \u2018lie for\u2019 are present in the data set, the grammar must represent the raising process at the word level in order to generate lack of raising in \u2018lie for\u2019 (because word level *a\u026a,a\u028a/_[-voice] cannot see that \u2018lie for\u2019 has /a\u026a/ before /f/, while phrase level *a\u026a,a\u028a/_[-voice] can). As shown in section 3.2, this introduces an additional dependency between word and phrase level for the transparent datasets: while the transparent interaction between flapping and raising does not require raising to apply before flapping, the combination of \u2018life\u2019 and \u2018lie for\u2019 does.\nThis creates a bottleneck effect for the transparent dataset, which explains the\ndecrease in learnability for transparent \u2018mitre-cider-life-lie-for\u2019. The increase in learnability for opaque \u2018mitre-cider-life-lie-for\u2019, on the other hand, can be seen as the cumulative effect of \u2018life\u2019 and \u2018lie for\u2019 on the opaque interaction, as reviewed above.\n5. Concluding remarks\nWe have presented here an approach to learning opaque and transparent interactions in a MaxEnt version of Stratal OT. Our goal was to investigate whether the general setup of Stratal OT \u2013 chained parallel OT grammars with independent rankings or weightings of constraints \u2013 predicts learnability differences between opaque and transparent process interactions, and also whether evidence of a process\u2019 stratal affiliation makes it easier to learn opaque interactions.\nOur first case study was opaque tensing/laxing in Southern French. We found that\nit was learned at a high rate of accuracy, but the solution space has a local optimum \u2013 one where the grammar does not represent the phonological process at all, which yields free variation in the data.\nWe then looked at the opaque interaction between diphthong raising and flapping\nin Canadian English (Joos 1942, Idsardi 2000 and references therein). The opaque raising process also applies in contexts where flapping is irrelevant, and it does not apply across word boundaries \u2013 both of which constitute evidence for the stratal affiliation of raising.\nWe found that, without additional evidence for stratal affiliation, the opaque\ninteraction was learned at a rate of about 50%, while its transparent counterpart was learned at ceiling. However, addition of this additional evidence improved the learnability of the opaque interaction to a maximum of 92%, while the learnability of the\ntransparent interaction descended to a similar rate. This confirms Kiparsky\u2019s prediction: evidence of the stratal affiliation of raising does improve its learnability when it is opaque. We also found that the advantage of transparent over opaque interactions is relative: the dataset with full evidence for stratal affiliation did not produce a learnability difference between the two.\nOur explanation for the observed learning difficulties in this framework has to do\nwith a bottleneck effect in the transmission of information from earlier derivational levels through Faithfulness constraints. This effect makes it more difficult to find the global optimum when the learner starts with word level weights that predict the wrong surface form. However, this effect is mitigated for opaque Canadian English by information about an opaque process\u2019 stratal affiliation, because this information either boosts desirable word level weightings, punishes undesirable phrase level weightings, or diminishes the bottleneck effect in general. See section 4.2 for details.\nThe two cases that we considered differ in their learnability rate. The Southern\nFrench data set contains evidence that the opaque process (tensing/laxing) is wordbounded, making it analogous to the opaque \u2018mitre-cider-lie-for\u2019 data set in the Canadian English case study. While Southern French was learned at a rate of 96%, \u2018mitre-cider-liefor\u2019 was only learned at a rate of 87%. A possible explanation is that the Southern French case study did not include constraints on syllable structure, while the process that makes tensing/laxing opaque is resyllabification. We predict that the learnability of Southern French would go down slightly if such constraints were introduced into the simulation.\nWe limited ourselves here to two case studies and one learning implementation.\nOther approaches to learning Stratal OT are possible too: probabilistic ranked constraint learning with Expectation Maximization (Jarosz 2006, 2016, ms), Noisy Harmonic Grammar (Coetzee & Pater 2011), Stochastic OT (Boersma 1998). These might differ in their particular learning strategies, the local optima they find, and the distribution over outputs generated at local optima. Nonetheless, the mechanisms responsible for the learnability differences that we found are quite general. Interdependence between phrase level Faithfulness and appropriate weighting at the word level poses a challenge for finding a grammar that generates the learning data. For this reason, we predict that other learning approaches will find results similar to ours (an early confirmation of this can be found in Jarosz 2016). However, more work is needed that explores the learnability of these and other opaque interactions in other learning frameworks. Further work is also needed to examine how other cases of opacity behave in our framework. In particular, more complex interactions that involve more constraints and/or more derivational levels would be essential to test the predictions of a general advantage for transparency, and of improved learning for opacity given evidence of stratal affiliation. In addition, as pointed out in section 4.2, it is important to understand how prevalent evidence for stratal affiliation must be in the data to ensure successful learning.\nFinally, further research would need to be done before one could draw\nconclusions about the relationship of our findings to attested patterns of language learning, language change, and language typology. In naturalistic language learning, we are unaware of any evidence that learners have biases toward transparent interactions, or that they find some cases of opacity easier than others. Such evidence may be difficult to come by outside of the laboratory, since it is general to control for confounds when comparing learning of different patterns. There is some evidence that transparent\ninteractions are easier in artificial language learning (Kim in press), but this line of research is only beginning. There is also evidence that opacity can be innovated by children (e.g. Dinnsen & Farris-Trimble 2008); whether or not this is a challenge to our model, or other related theories, would require a detailed consideration of the constraints involved and the steps on the learning path. To apply our model to historical change would require adding the analogue of rule loss, that is the possibility of mis-learning an opaque interaction as a contrast; our constraint set cannot generate this possibility. And to make conclusions about how these results fit the relative degree of typological attestation of opaque and transparent interactions, we would need not only a model that connects learning to typological attestation (see e.g. Staubs 2014a), but also empirical research to determine whether opacity is underattested or not.\nReferences\nBerm\u00fadez-Otero, Ricardo. (1999). Constraint Interaction in Language Change [Opacity\nand Globality in Phonological Change]. PhD dissertation, University of Manchester & Universidad de Santiago de Compostela.\nBerm\u00fadez-Otero, Ricardo. (2003). The Acquisition of Phonological Opacity. In Spenader\net al. (2003). 25\u201336.\nBoersma, Paul. (1998). Functional Phonology: Formalizing the Interactions between\nArticulatory and Perceptual Drives. Holland Academic Graphics/IFOTT.\nByrd, Richard H., Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. (1995). A Limited\nMemory Algorithm for Bound Constrained Optimization. SIAM Journal on Scientific Computing 16:5. 1190\u20131208.\nCoetzee, Andries, and Joe Pater. (2011). The Place of Variation in Phonological Theory.\nIn van Oostendorp et al. (2011). 401\u2013434.\nDe Jong, Kenneth. (2011). Flapping in American English. In van Oostendorp et al.\n(2011). 2711-2729.\nDinnsen, Daniel A. & Ashley W. Farris-Trimble. (2008). An opacity-tolerant conspiracy\nin phonological acquisition. In Ashley W. Farris-Trimble & Daniel A. Dinnsen (eds.) IUWPL6: Phonological Opacity Effects in Optimality Theory. Bloomington, IN: IULC Publications. 99-118.\nEisenstat, Sarah. (2009). Learning Underlying Forms with MaxEnt. MA thesis, Brown\nUniversity.\nEychenne, Lucien. (2014). Schwa and the Loi de Position in Southern French. Journal of\nFrench Language Studies 24:2. 223\u201353.\nGoldwater, Sharon, and Mark Johnson. (2003). Learning OT Constraint Rankings Using\na Maximum Entropy Model. In Spenader et al. 2003. 111\u2013120.\nHayes, Bruce & Colin Wilson. (2008). A Maximum Entropy Model of Phonotactics and\nPhonotactic Learning. Linguistic Inquiry 39. 379\u2013440.\nHoerl, Arthur E & Robert W Kennard. (1970). Ridge Regression: Biased Estimation for\nNonorthogonal Problems. Technometrics 12:1. 55\u201367.\nIdsardi, William J. (2000). Clarifying Opacity. Linguistic Review 17:2/4. 337\u2013350. Jarosz, Gaja. manuscript. Expectation Driven Learning of Phonology. Jarosz, Gaja. (2006). Rich Lexicons and Restrictive Grammars: Maximum Likelihood\nLearning in Optimality Theory. PhD dissertation, Johns Hopkins University.\nJarosz, Gaja. (2014). Serial Markedness Reduction. In John Kingston, Claire Moore-\nCantwell, Joe Pater & Robert Staubs (eds.) Proceedings of the 2013 Annual Meeting on Phonology. Washington, DC: Linguistic Society of America.\nJarosz, Gaja. (2016). Learning Opaque and Transparent Interactions in Harmonic\nSerialism. Gunnar \u00d3lafur Hansson, Ashley Farris-Trimble, Kevin McMullin & Douglas Pulleyblank (eds.) Proceedings of the 2015 Annual Meeting on Phonology. Washington, DC: Linguistic Society of America.\nJohnson, Mark, Joe Pater, Robert Staubs & Emmanuel Dupoux. (2015). Sign Constraints\non Feature Weights Improve a Joint Model of Word Segmentation and Phonology. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Denver, CO: Association for Computational Linguistics. 303\u2013313.\nJoos, Martin. (1942). A Phonological Dilemma in Canadian English. Lg 18. 141\u2013144. Kaye, Jonathan. (1990). What Ever Happened to Dialect B? In Joan Mascar\u00f3 & Marina\nNespor (eds.) Grammar in Progress: GLOW Essays for Henk van Riemsdijk. Dordrecht: Foris. 259\u2013263. Kim, Yun J. (in press). Do learners prefer transparent rule ordering? An artificial\nlanguage learning study. CLS 49. Kiparsky, Paul. (1971). Historical Linguistics. In William O. Dingwall (ed.) A Survey of\nLinguistic Science. College Park, Maryland: University of Maryland Press. 576\u2013642.\nKiparsky, Paul. (1973). Abstractness, Opacity and Global Rules. Bloomington, IN:\nIndiana University Linguistics Club.\nKiparsky, Paul. (2000). Opacity and Cyclicity. The Linguistic Review 17:2/4. 351\u2013366. Kullback, Solomon & Richard A. Leibler. (1951). On Information and Sufficiency.\nAnnals of Mathematical Statistics 22:1. 79\u201386.\nMcCarthy, John J. (2007). Hidden Generalizations: Phonological Opacity in Optimality\nTheory. London: Equinox Press.\nMcCarthy, John J. & Joe Pater (eds.) (2016). Harmonic Grammar and Harmonic\nSerialism. London: Equinox Press\nMoreux, Bernard. (1985). La Loi de Position En Fran\u00e7ais Du Midi. I. Synchronie (B\u00e9arn).\nCahiers de Grammaire 9. 45\u2013138.\nMoreux, Bernard. (2006). Les Voyelles Moyennes En Fran\u00e7ais Du Midi: Une Tentative\nde Synth\u00e8se En 1985. Cahiers de Grammaire 30. 307\u2013317.\nOdden, David. (2011). Rules v. Constraints. In John A. Goldsmith, Jason Riggle & Alan\nC.L. Yu (eds.) The Handbook of Phonological Theory. Second Edition. Malden, MA: Wiley-Blackwell. 1\u201339.\nOostendorp, Marc van, Colin J. Ewen, Elizabeth Hume & Keren Rice (eds.) (2011). The\nBlackwell Companion to Phonology. Malden, MA: Wiley-Blackwell.\nPater, Joe. (2014). Canadian Raising with Language-Specific Weighted Constraints. Lg\n90:1. 230\u2013240.\nPater, Joe. (2016). Universal Grammar with Weighted Constraints. In McCarthy & Pater (2016). Pater, Joe, Karen Jesney, Robert Staubs & Brian Smith. (2012). Learning Probabilities\nover Underlying Representations. In Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology. Denver, CO: Association for Computational Linguistics. 62\u201371.\nR Core Team. (2013). R: A Language and Environment for Statistical Computing.\nVienna, Austria: R Foundation for Statistical Computing. http://www.R-project.org/.\nRizzolo, Olivier. (2002). Du Leurre Phon\u00e9tique Des Voyelles Moyennes En Fran\u00e7ais et\nDu Divorce Entre Licenciement et Licenciement Pour Gouverner. PhD dissertation, Universit\u00e9 de Nice-Sophia Antipolis.\nSelkirk, Elizabeth. (1978). The French Foot: On the Statute of \u2018mute\u2019 E. Studies in\nFrench Linguistics 2. 79\u2013141.\nSmolensky, Paul & Geraldine Legendre. (2006). The Harmonic Mind: From Neural\nComputation to Optimality-Theoretic Grammar. Cambridge, MA: MIT Press.\nSpenader, Jennifer, Anders Eriksson & Osten Dahl (eds.) (2003). Proceedings of the\nStockholm Workshop on Variation within Optimality Theory. Stockholm: Stockholm University.\nStaubs, Robert. (2014a). Computational Modeling of Learning Biases in Stress Typology.\nPhD dissertation, University of Massachusetts Amherst.\nStaubs, Robert. (2014b). Stratal MaxEnt Solver. Software package. Staubs, Robert & Joe Pater. (2016). Learning Serial Constraint-Based Grammars. In\nMcCarthy & Pater (2016).\nTesar, Bruce & Paul Smolensky. (2000). Learnability in Optimality Theory. Cambridge,\nMA: MIT Press."}], "references": [{"title": "Constraint Interaction in Language Change [Opacity and Globality in Phonological Change", "author": ["Berm\u00fadez-Otero", "Ricardo."], "venue": "PhD dissertation, University of Manchester & Universidad de Santiago de Compostela.", "citeRegEx": "Berm\u00fadez.Otero and Ricardo.,? 1999", "shortCiteRegEx": "Berm\u00fadez.Otero and Ricardo.", "year": 1999}, {"title": "The Acquisition of Phonological Opacity", "author": ["Berm\u00fadez-Otero", "Ricardo."], "venue": "Spenader et al. (2003). 25\u201336.", "citeRegEx": "Berm\u00fadez.Otero and Ricardo.,? 2003", "shortCiteRegEx": "Berm\u00fadez.Otero and Ricardo.", "year": 2003}, {"title": "Functional Phonology: Formalizing the Interactions between Articulatory and Perceptual Drives", "author": ["Boersma", "Paul."], "venue": "Holland Academic Graphics/IFOTT.", "citeRegEx": "Boersma and Paul.,? 1998", "shortCiteRegEx": "Boersma and Paul.", "year": 1998}, {"title": "A Limited Memory Algorithm for Bound Constrained Optimization", "author": ["Byrd", "Richard H.", "Peihuang Lu", "Jorge Nocedal", "Ciyou Zhu."], "venue": "SIAM Journal on Scientific Computing 16:5. 1190\u20131208.", "citeRegEx": "Byrd et al\\.,? 1995", "shortCiteRegEx": "Byrd et al\\.", "year": 1995}, {"title": "The Place of Variation in Phonological Theory", "author": ["Coetzee", "Andries", "Joe Pater."], "venue": "van Oostendorp et al. (2011). 401\u2013434.", "citeRegEx": "Coetzee et al\\.,? 2011", "shortCiteRegEx": "Coetzee et al\\.", "year": 2011}, {"title": "Flapping in American English", "author": ["De Jong", "Kenneth."], "venue": "van Oostendorp et al. (2011). 2711-2729.", "citeRegEx": "Jong and Kenneth.,? 2011", "shortCiteRegEx": "Jong and Kenneth.", "year": 2011}, {"title": "An opacity-tolerant conspiracy in phonological acquisition", "author": ["Dinnsen", "Daniel A", "Ashley W. Farris-Trimble"], "venue": "Phonological Opacity Effects in Optimality Theory. Bloomington, IN: IULC Publications", "citeRegEx": "Dinnsen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dinnsen et al\\.", "year": 2008}, {"title": "Learning Underlying Forms with MaxEnt", "author": ["Eisenstat", "Sarah."], "venue": "MA thesis, Brown University.", "citeRegEx": "Eisenstat and Sarah.,? 2009", "shortCiteRegEx": "Eisenstat and Sarah.", "year": 2009}, {"title": "Schwa and the Loi de Position in Southern French", "author": ["Eychenne", "Lucien."], "venue": "Journal of French Language Studies 24:2. 223\u201353.", "citeRegEx": "Eychenne and Lucien.,? 2014", "shortCiteRegEx": "Eychenne and Lucien.", "year": 2014}, {"title": "Learning OT Constraint Rankings Using a Maximum Entropy Model", "author": ["Goldwater", "Sharon", "Mark Johnson."], "venue": "Spenader et al. 2003. 111\u2013120.", "citeRegEx": "Goldwater et al\\.,? 2003", "shortCiteRegEx": "Goldwater et al\\.", "year": 2003}, {"title": "A Maximum Entropy Model of Phonotactics and Phonotactic Learning", "author": ["Hayes", "Bruce", "Colin Wilson"], "venue": "Linguistic Inquiry", "citeRegEx": "Hayes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hayes et al\\.", "year": 2008}, {"title": "Ridge Regression: Biased Estimation for Nonorthogonal Problems", "author": ["Hoerl", "Arthur E", "Robert W Kennard"], "venue": null, "citeRegEx": "Hoerl et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Hoerl et al\\.", "year": 1970}, {"title": "Clarifying Opacity", "author": ["Idsardi", "William J."], "venue": "Linguistic Review 17:2/4. 337\u2013350.", "citeRegEx": "Idsardi and J.,? 2000", "shortCiteRegEx": "Idsardi and J.", "year": 2000}, {"title": "Rich Lexicons and Restrictive Grammars: Maximum Likelihood Learning in Optimality Theory", "author": ["Jarosz", "Gaja."], "venue": "PhD dissertation, Johns Hopkins University.", "citeRegEx": "Jarosz and Gaja.,? 2006", "shortCiteRegEx": "Jarosz and Gaja.", "year": 2006}, {"title": "Serial Markedness Reduction", "author": ["Jarosz", "Gaja."], "venue": "John Kingston, Claire MooreCantwell, Joe Pater & Robert Staubs (eds.) Proceedings of the 2013 Annual Meeting on Phonology. Washington, DC: Linguistic Society of America.", "citeRegEx": "Jarosz and Gaja.,? 2014", "shortCiteRegEx": "Jarosz and Gaja.", "year": 2014}, {"title": "Learning Opaque and Transparent Interactions in Harmonic Serialism", "author": ["Jarosz", "Gaja."], "venue": "Gunnar \u00d3lafur Hansson, Ashley Farris-Trimble, Kevin McMullin & Douglas Pulleyblank (eds.) Proceedings of the 2015 Annual Meeting on Phonology. Washington, DC: Linguistic Society of America.", "citeRegEx": "Jarosz and Gaja.,? 2016", "shortCiteRegEx": "Jarosz and Gaja.", "year": 2016}, {"title": "Sign Constraints on Feature Weights Improve a Joint Model of Word Segmentation and Phonology", "author": ["Johnson", "Mark", "Joe Pater", "Robert Staubs", "Emmanuel Dupoux"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "A Phonological Dilemma in Canadian English", "author": ["Joos", "Martin."], "venue": "Lg 18. 141\u2013144.", "citeRegEx": "Joos and Martin.,? 1942", "shortCiteRegEx": "Joos and Martin.", "year": 1942}, {"title": "What Ever Happened to Dialect B", "author": ["Kaye", "Jonathan"], "venue": "Grammar in Progress: GLOW Essays for Henk van Riemsdijk. Dordrecht: Foris", "citeRegEx": "Kaye and Jonathan.,? \\Q1990\\E", "shortCiteRegEx": "Kaye and Jonathan.", "year": 1990}, {"title": "Historical Linguistics", "author": ["Kiparsky", "Paul."], "venue": "William O. Dingwall (ed.) A Survey of Linguistic Science. College Park, Maryland: University of Maryland Press. 576\u2013642.", "citeRegEx": "Kiparsky and Paul.,? 1971", "shortCiteRegEx": "Kiparsky and Paul.", "year": 1971}, {"title": "Abstractness, Opacity and Global Rules", "author": ["Kiparsky", "Paul."], "venue": "Bloomington, IN: Indiana University Linguistics Club.", "citeRegEx": "Kiparsky and Paul.,? 1973", "shortCiteRegEx": "Kiparsky and Paul.", "year": 1973}, {"title": "Opacity and Cyclicity", "author": ["Kiparsky", "Paul."], "venue": "The Linguistic Review 17:2/4. 351\u2013366.", "citeRegEx": "Kiparsky and Paul.,? 2000", "shortCiteRegEx": "Kiparsky and Paul.", "year": 2000}, {"title": "On Information and Sufficiency", "author": ["Kullback", "Solomon", "Richard A. Leibler"], "venue": "Annals of Mathematical Statistics", "citeRegEx": "Kullback et al\\.,? \\Q1951\\E", "shortCiteRegEx": "Kullback et al\\.", "year": 1951}, {"title": "Hidden Generalizations: Phonological Opacity in Optimality Theory", "author": ["McCarthy", "John J."], "venue": "London: Equinox Press.", "citeRegEx": "McCarthy and J.,? 2007", "shortCiteRegEx": "McCarthy and J.", "year": 2007}, {"title": "Harmonic Grammar and Harmonic Serialism", "author": ["McCarthy", "John J", "Joe Pater"], "venue": null, "citeRegEx": "McCarthy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 2016}, {"title": "La Loi de Position En Fran\u00e7ais Du Midi", "author": ["Moreux", "Bernard."], "venue": "I. Synchronie (B\u00e9arn). Cahiers de Grammaire 9. 45\u2013138.", "citeRegEx": "Moreux and Bernard.,? 1985", "shortCiteRegEx": "Moreux and Bernard.", "year": 1985}, {"title": "Les Voyelles Moyennes En Fran\u00e7ais Du Midi: Une Tentative de Synth\u00e8se En 1985", "author": ["Moreux", "Bernard."], "venue": "Cahiers de Grammaire 30. 307\u2013317.", "citeRegEx": "Moreux and Bernard.,? 2006", "shortCiteRegEx": "Moreux and Bernard.", "year": 2006}, {"title": "Rules v", "author": ["Odden", "David."], "venue": "Constraints. In John A. Goldsmith, Jason Riggle & Alan C.L. Yu (eds.) The Handbook of Phonological Theory. Second Edition. Malden, MA: Wiley-Blackwell. 1\u201339.", "citeRegEx": "Odden and David.,? 2011", "shortCiteRegEx": "Odden and David.", "year": 2011}, {"title": "Canadian Raising with Language-Specific Weighted Constraints", "author": ["Pater", "Joe."], "venue": "Lg 90:1. 230\u2013240.", "citeRegEx": "Pater and Joe.,? 2014", "shortCiteRegEx": "Pater and Joe.", "year": 2014}, {"title": "Universal Grammar with Weighted Constraints", "author": ["Pater", "Joe."], "venue": "McCarthy & Pater (2016).", "citeRegEx": "Pater and Joe.,? 2016", "shortCiteRegEx": "Pater and Joe.", "year": 2016}, {"title": "Learning Probabilities over Underlying Representations", "author": ["Pater", "Joe", "Karen Jesney", "Robert Staubs", "Brian Smith"], "venue": "In Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology", "citeRegEx": "Pater et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pater et al\\.", "year": 2012}, {"title": "R: A Language and Environment for Statistical Computing", "author": ["R Core Team."], "venue": "Vienna, Austria: R Foundation for Statistical Computing. http://www.R-project.org/.", "citeRegEx": "Team.,? 2013", "shortCiteRegEx": "Team.", "year": 2013}, {"title": "Du Leurre Phon\u00e9tique Des Voyelles Moyennes En Fran\u00e7ais et Du Divorce Entre Licenciement et Licenciement Pour Gouverner", "author": ["Rizzolo", "Olivier."], "venue": "PhD dissertation, Universit\u00e9 de Nice-Sophia Antipolis.", "citeRegEx": "Rizzolo and Olivier.,? 2002", "shortCiteRegEx": "Rizzolo and Olivier.", "year": 2002}, {"title": "The French Foot: On the Statute of \u2018mute\u2019 E", "author": ["Selkirk", "Elizabeth."], "venue": "Studies in French Linguistics 2. 79\u2013141.", "citeRegEx": "Selkirk and Elizabeth.,? 1978", "shortCiteRegEx": "Selkirk and Elizabeth.", "year": 1978}, {"title": "The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar", "author": ["Smolensky", "Paul", "Geraldine Legendre"], "venue": null, "citeRegEx": "Smolensky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smolensky et al\\.", "year": 2006}, {"title": "Computational Modeling of Learning Biases in Stress Typology", "author": ["Staubs", "Robert."], "venue": "PhD dissertation, University of Massachusetts Amherst.", "citeRegEx": "Staubs and Robert.,? 2014a", "shortCiteRegEx": "Staubs and Robert.", "year": 2014}, {"title": "Stratal MaxEnt Solver", "author": ["Staubs", "Robert."], "venue": "Software package.", "citeRegEx": "Staubs and Robert.,? 2014b", "shortCiteRegEx": "Staubs and Robert.", "year": 2014}, {"title": "Learning Serial Constraint-Based Grammars", "author": ["Staubs", "Robert", "Joe Pater"], "venue": null, "citeRegEx": "Staubs et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Staubs et al\\.", "year": 2016}, {"title": "Learnability in Optimality Theory", "author": ["Tesar", "Bruce", "Paul Smolensky"], "venue": null, "citeRegEx": "Tesar et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tesar et al\\.", "year": 2000}], "referenceMentions": [{"referenceID": 29, "context": "Here, we follow Eisenstat (2009), Pater et al. (2012), Johnson et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 16, "context": "(2012), Johnson et al. (2015), and Staubs & Pater (2016) in adopting a weighted constraint variant of this general approach.", "startOffset": 8, "endOffset": 30}, {"referenceID": 16, "context": "(2012), Johnson et al. (2015), and Staubs & Pater (2016) in adopting a weighted constraint variant of this general approach.", "startOffset": 8, "endOffset": 57}, {"referenceID": 31, "context": "Staubs (2014b) provides an implementation in R (R Core Team 2013) of a variant of Staubs & Pater\u2019s (2016) approach to learning serial grammars.", "startOffset": 55, "endOffset": 106}, {"referenceID": 3, "context": "Minimization of this objective function is done by the L-BFGS-B method of optimization (Byrd et al. 1995).", "startOffset": 87, "endOffset": 105}], "year": 2017, "abstractText": "(Kiparsky 1971, 1973) draws attention to cases of historical change that suggest that at least some opaque phonological process interactions are difficult to learn. In his work on Stratal Optimality Theory (Stratal OT), he further claims that independent evidence about the stratal affiliation of a process is helpful in learning an opaque interaction (Kiparsky 2000). In this paper, we develop a computationally implemented learner for a weighted constraint version of Stratal OT, and show that for an initial set of test cases, opaque patterns are indeed generally more difficult than their transparent counterparts, and that information about stratal affiliation does make learning of an opaque pattern easier. Our results support the viability of Stratal OT as a theory of opaque interactions (Berm\u00fadez-Otero 1999, 2003, Kiparsky 2000; see below). However, it is not clear whether a stratal setup is the only source of opaque interactions \u2013 in fact, some cases of opacity may stem from other factors (McCarthy 2007). A broader assessment of alternative theories of opacity will not be attempted in this paper; see Jarosz (2016) for work on learning opacity in one of these alternative frameworks (Jarosz 2014). In the Stratal OT approach to opacity (see also especially Berm\u00fadez-Otero 1999, 2003), grammars are chained together to produce outputs that would not be possible within a single level. For example, the classic case of the opaque interaction between diphthong raising and flapping in Canadian English illustrated in (1), mitre can be produced by raising in the first (word level) grammar, and then flapping in the second (phrase level) one. As we will see shortly, the final candidate cannot be produced by a single OT grammar, with the standard set of constraints assumed for this case (though see Pater 2014).", "creator": "Microsoft Word"}}}