{"id": "1703.02626", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Horde of Bandits using Gaussian Markov Random Fields", "abstract": "The bandit major armed (GOB) model \\ cite {cesa2013gang} exception a brought characterisation bandits outline suggested u.k. client between a set of cannibal problems, related followed giving known (any noisy) curve. This model way types in problems why recommender systems nearby on directly places which server makes does protect give transfer information during users. Despite own effectiveness, called funding GOB similar meant having without applied will small possibility possibly not its quadratic time - dependence on the number is chromosomes. Existing solutions the training took minimizes states checks this them - hypocritical clustering implied. By fear piece related able Gaussian Markov random rich (GMRFs ), never show that full GOB newest go be made came scale to longer larger random whether additional assumptions. In one, anyway apply a Thompson applications algorithm latter unique held though GMRF measure -. - fourier improvisation, carry means to production even even either problems (leading this a \" ilkhanate \" many tribesmen ). We consider sadness glanced and prototype surprising while GOB same Thompson sampling this epoch - bunch coding, underlying far related ways are soon we as meaning significantly anything larger frustration there graph other adopting taking isometric - it step. Finally, be term exist graph is not available, ? propose gave heuristic for learning still on held vessels bringing nbc promising surprise.", "histories": [["v1", "Tue, 7 Mar 2017 22:21:50 GMT  (651kb,D)", "http://arxiv.org/abs/1703.02626v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sharan vaswani", "mark schmidt", "laks v s lakshmanan"], "accepted": false, "id": "1703.02626"}, "pdf": {"name": "1703.02626.pdf", "metadata": {"source": "CRF", "title": "Horde of Bandits using Gaussian Markov Random Fields", "authors": ["Sharan Vaswani", "Mark Schmidt", "Laks V.S. Lakshmanan"], "emails": [], "sections": [{"heading": null, "text": "The gang of bandits (GOB) model [7] is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph. This model is useful in problems like recommender systems where the large number of users makes it vital to transfer information between users. Despite its effectiveness, the existing GOB model can only be applied to small problems due to its quadratic timedependence on the number of nodes. Existing solutions to combat the scalability issue require an often-unrealistic clustering assumption. By exploiting a connection to Gaussian Markov random fields (GMRFs), we show that the GOB model can be made to scale to much larger graphs without additional assumptions. In addition, we propose a Thompson sampling algorithm which uses the recent GMRF sampling-by-perturbation technique, allowing it to scale to even larger problems (leading to a \u201chorde\u201d of bandits). We give regret bounds and experimental results for GOB with Thompson sampling and epoch-greedy algorithms, indicating that these methods are as good as or significantly better than ignoring the graph or adopting a clustering-based approach. Finally, when an existing graph is not available, we propose a heuristic for learning it on the fly and show promising results."}, {"heading": "1 Introduction", "text": "Consider a newly established recommender system (RS) which has little or no information about the users\u2019 preferences or any available rating data. The unavailability of rating data implies that we can not use traditional collaborative filtering based methods [41]. Furthermore,\nProceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, Fort Lauderdale, Florida, USA. JMLR: W&CP volume 54. Copyright 2017 by the author(s).\nin the scenario of personalized news recommendation or for recommending trending Facebook posts, the set of available items is not fixed but instead changes continuously. This new RS can recommend items to the users and observe their ratings to learn their preferences from this feedback (\u201cexploration\u201d). However, in order to retain its users, at the same time it should recommend \u201crelevant\u201d items that will be liked by and elicit higher ratings from users (\u201cexploitation\u201d). Assuming each item can be described by its content (like tags describing a news article or video), the contextual bandits framework [29] offers a popular approach for addressing this exploration-exploitation trade-off.\nHowever, this framework assumes that users interact with the RS in an isolated manner, when in fact a RS might have an associated social component. In particular, given the large number of users on such systems, we may be able to learn their preferences more quickly by leveraging the relations between them. One way to use a social network of users to improve recommendations is with the recent gang of bandits (GOB) model [7]. In particular, the GOB model exploits the homophily effect [35] that suggests users with similar preferences are more likely to form links in a social network. In other words, user preferences vary smoothly across the social graph and tend to be similar for users connected with each other. This allows us to transfer information between users; we can learn about a user from his or her friends\u2019 ratings. However, the existing recommendation algorithm in the GOB framework has a quadratic time-dependence on the number of nodes (users) and thus can only be used for a small number of users. Several recent works have tried to improve the scaling of the GOB model by clustering the users into groups [17, 36], but this limits the flexibility of the model and loses the ability to model individual users\u2019 preferences.\nIn this paper, we cast the GOB model in the framework of Gaussian Markov random fields (GMRFs) and show how to exploit this connection to scale it to much larger graphs. Specifically, we interpret the GOB model as the optimization of a Gaussian likelihood on the users\u2019 observed ratings and interpret the user-user graph as the\nar X\niv :1\n70 3.\n02 62\n6v 1\n[ cs\n.L G\n] 7\nM ar\n2 01\n7\nprior inverse-covariance matrix of a GMRF. From this perspective, we can efficiently estimate the users\u2019 preferences by performing MAP estimation in a GMRF. In addition, we propose a Thompson sampling GOB variant that exploits the recent sampling-by-perturbation idea from the GMRF literature [37] to scale to even larger problems. This idea is fairly general and might be of independent interest in the efficient implementation of other Thompson sampling methods. We establish regret bounds (Section 4) and provide experimental results (Section 5) for Thompson sampling as well as an epoch-greedy strategy. These experiments indicate that our methods are as good as or significantly better than approaches which ignore the graph or that cluster the nodes. Finally, when the graph of users is not available, we propose a heuristic for learning the graph and user preferences simultaneously in an alternating minimization framework (Appendix A)."}, {"heading": "2 Related Work", "text": "Social Regularization: Using social information to improve recommendations was first introduced by Ma et al. [31]. They used matrix factorization to fit existing rating data but constrained a user\u2019s latent vector to be similar to their friends in the social network. Other methods based on collaborative filtering followed [38, 13], but these works assume that we already have rating data available. Thus, these methods do not address the exploration-exploitation trade-off faced by a new RS that we consider.\nBandits: The multi-armed bandit problem is a classic approach for trading off exploration and exploitation as we collect data [26]. When features (context) for the \u201carms\u201d are available and changing, it is referred to as the contextual bandit problem [4, 29, 9]. The contextual bandit framework is important for the scenario we consider where the set of items available is constantly changing, since the features allow us to make predictions about items we have never seen before. Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2]. Note that these standard contextual bandit methods do not model the user-user dependencies that we want to exploit.\nSeveral graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al. [7] is the first to exploit the network between users in the contextual bandit framework. They proposed a UCB-style algorithm and showed that using the graph leads to lower regret from both a theoretical and practical standpoint. However, their algorithm has a time complexity that is quadratic in the number of users. This makes it\ninfeasible for typical RS that have tens of thousands (or even millions) of users.\nTo scale up the GOB model, several recent works propose to cluster the users and assume that users in the same cluster have the same preferences [17, 36]. But this solution loses the ability to model individual users\u2019 preferences, and indeed our experiments indicate that in some applications clustering significantly hurts performance. In contrast, we want to scale up the original GOB model that learns more fine-grained information in the form of a preference-vector specific to each user.\nAnother interesting approach to relax the clustering assumption is to cluster both items and users [30], but this only applies if we have a fixed set of items. Some works consider item-item similarities to improve recommendations [42, 23], but this again requires a fixed set of items while we are interested in RS where the set of items may constantly be changing. There has also been work on solving a single bandit problem in a distributed fashion [24], but this differs from our approach where we are solving an individual bandit problem on each of the n nodes. Finally, we note that all of the existing graph-based works consider relatively small RS datasets (\u223c 1k users), while our proposed algorithms can scale to much larger RS."}, {"heading": "3 Scaling up Gang of Bandits", "text": "In this section we first describe the general GOB framework, then discuss the relationship to GMRFs, and finally show how this leads to more scalable method. In this paper Tr(A) denotes the trace of matrix A, A \u2297 B denotes the Kronecker product of matrices A and B, Id is used for the d-dimensional identity matrix, and vec(A) is the stacking of the columns of a matrix A into a vector."}, {"heading": "3.1 Gang of Bandits Framework", "text": "The contextual bandits framework proceeds in rounds. In each round t, a set of items Ct becomes available. These items could be movies released in a particular week, news articles published on a particular day, or trending stories on Facebook. We assume that |Ct| = K for all t. We assume that each item j can be described by a context (feature) vector xj \u2208 Rd. We use n as the number of users, and denote the (unknown) ground-truth preference vector for user i as w\u2217i \u2208 Rd. Throughout the paper, we assume there is only a single target user per round. It is straightforward extend our results to multiple target users.\nGiven a target user it, our task is to recommend an available item jt \u2208 Ct to them. User it then provides feedback on the recommended item jt in the form of a rating rit,jt . Based on this feedback, the estimated preference vector for user it is updated. The recommendation algorithm must trade-off between exploration\n(learning about the users\u2019 preferences) and exploitation (obtaining high ratings). We evaluate performance using the notion of regret, which is the loss in recommendation performance due to lack of knowledge of user preferences. In particular, the regret R(T ) after T rounds is given by:\nR(T ) = T\u2211 t=1 [ max j\u2208Ct (w\u2217Tit xj)\u2212w \u2217T it xjt ] . (1)\nIn our analysis we make the following assumptions:\nAssumption 1. The `2-norms of the true preference vectors and item feature vectors are bounded from above. Without loss of generality we\u2019ll assume ||xj ||2 \u2264 1 for all j and ||w\u2217i ||2 \u2264 1 for all i. Also without loss of generality, we assume that the ratings are in the range [0, 1].\nAssumption 2. The true ratings can be given by a linear model [29], meaning that ri,j = (w\u2217i )Txj + \u03b7i,j,t for some noise term \u03b7i,j,t.\nThese are standard assumptions in the literature. We denote the history of observations until round t as Ht\u22121 = {(i\u03c4 , j\u03c4 , ri\u03c4 ,j\u03c4 )}\u03c4=1,2\u00b7\u00b7\u00b7t\u22121 and the union of the set of available items until round t along with their corresponding features as Ct\u22121.\nAssumption 3. The noise \u03b7i,j,t is conditionally subGaussian [2][7] with zero mean and bounded variance, meaning that E[\u03b7i,j,t | Ct\u22121,Ht\u22121] = 0 and that there exists a \u03c3 > 0 such that for all \u03b3 \u2208 R, we have E[exp(\u03b3\u03b7i,j,t) | Ht\u22121,Ct\u22121] \u2264 exp(\u03b3 2\u03c32 2 ).\nThis assumption implies that for all i and j, the conditional mean is given by E[ri,j |Ct\u22121,Ht\u22121] = w\u2217Ti xj and that the conditional variance satisfies V[ri,j |Ct\u22121,Ht\u22121] \u2264 \u03c32.\nIn the GOB framework, we assume access to a (fixed) graphG = (V, E) of users in the form of a social network (or \u201ctrust graph\u201d). Here, the nodes V correspond to users, whereas the edges E correspond to friendships or trust relationships. The homophily effect implies that the true user preferences vary smoothly across the graph, so we expect the preferences of users connected in the graph to be close to each other. Specifically,\nAssumption 4. The true user preferences vary smoothly according to the given graph, in the sense that we have a small value of\u2211\n(i1,i2)\u2208E\n||w\u2217i1 \u2212w \u2217 i2 || 2.\nHence, we assume that the graph acts as a correctlyspecified prior on the users\u2019 true preferences. Note\nthat this assumption implies that nodes in dense subgraphs will have a higher similarity than those in sparse subgraphs (since they will have a larger number of neighbours).\nThis assumption is violated in some datasets. For example, in our experiments we consider one dataset in which the available graph is imperfect, in that user preferences do not seem to vary smoothly across all graph edges. Intuitively, we might think that the GOB model might be harmful in this case (compared to ignoring the graph structure). However, in our experiments, we observe that even in these cases, the GOB approach still lead to results as good as ignoring the graph.\nThe GOB model [7] solves a contextual bandit problem for each user, where the mean vectors in the different problems are related according to the Laplacian L1 of the graph G. Let wi,t be the preference vector estimate for user i at round t. Let wt and w\u2217 \u2208 Rdn (respectively) be the concatenation of the vectors wi,t and w\u2217i across all users. The GOB model solves the following regression problem to find the mean preference vector estimate at round t,\nwt = argminw [ n\u2211 i=1 \u2211 k\u2208Mi,t (wTi xk \u2212 ri,k)2\n+\u03bbwT (L\u2297 Id)w ] , (2)\nwhere Mi,t is the set of items rated by user i up to round t. The first term is a data-fitting term and models the observed ratings. The second term is the Laplacian regularization and equal to \u2211 (i,j)\u2208E \u03bb||wi,t\u2212 wj,t||22. This term models smoothness across the graph with \u03bb > 0 giving the strength of this regularization. Note that the same objective function has also been explored for graph-regularized multi-task learning [14]."}, {"heading": "3.2 Connection to GMRFs", "text": "Unfortunately, the approach of Cesa-Bianchi [7] for solving (2) has a computational complexity of O(d2n2). To solve (2) more efficiently, we now show that it can be interpreted as performing MAP estimation in a GMRF. This will allow us to apply the GOB model to much larger datasets, and lead to an even more scalable algorithm based on Thompson sampling (Section 4).\nConsider the following generative model for the ratings ri,j and the user preference vectors wi,\nri,j \u223c N (wTi xj , \u03c32), w \u223c N (0, (\u03bbL\u2297 Id)\u22121).\nThis GMRF model assumes that the ratings ri,j are independent given wi and xj , which is the standard\n1To ensure invertibility, we set L = LG + In where LG is the normalized graph Laplacian.\nregression assumption. Under this independence assumption the first term in (2) is equal up the negative log-likelihood for all of the observed ratings rt at time t, log p(rt | w,xt, \u03c3), up to an additive constant and assuming \u03c3 = 1. Similarly, the negative log-prior p(w | \u03bb, L) in this model gives the second term in (2) (again, up to an additive constant that does not depend on w). Thus, by Bayes rule minimizing (2) is equivalent to maximizing the posterior in this GMRF model.\nTo characterize the posterior, it is helpful to introduce the notation \u03c6i,j \u2208 Rdn to represent the \u201cglobal\u201d feature vector corresponding to recommending item j to user i. In particular, let \u03c6i,j be the concatenation of n d-dimensional vectors where the ith vector is equal to xj and the others are zero. The rows of the t\u00d7dn dimensional matrix \u03a6t correspond to these \u201cglobal\u201d features for all the recommendations made until time t. Under this notation, the posterior p(w | rt,w,\u03a6t) is given by a N (w\u0302t,\u03a3\u22121t ) distribution with \u03a3t = 1\u03c32 \u03a6 T t \u03a6t+\u03bb(L\u2297Id) and w\u0302t = 1\u03c32 \u03a3 \u22121 t bt with bt = \u03a6Tt rt. We can view the approach in [7] as explicitly constructing the dense dn \u00d7 dn matrix \u03a3\u22121t , leading to an O(d2n2) memory requirement. A new recommendation at round t is thus equivalent to a rank-1 update to \u03a3t, and even with the Sherman-Morrison formula this leads to an O(d2n2) time requirement for each iteration."}, {"heading": "3.3 Scalability", "text": "Rather than treating \u03a3t as a general matrix, we propose to exploit its structure to scale up the GOB framework to problems where n is very large. In particular, solving (2) corresponds to finding the mean vector of the GMRF, which corresponds to solving the linear system \u03a3tw = bt. Since \u03a3t is positivedefinite, the linear system can be solved using conjugate gradient [20]. Conjugate gradient notably does not require \u03a3\u22121t , but instead uses matrix-vector products \u03a3tv = (\u03a6Tt \u03a6t)v + \u03bb(L \u2297 Id)v for vectors v \u2208 Rdn. Note that \u03a6Tt \u03a6t is block diagonal and has only O(nd2) non-zeroes. Hence, \u03a6Tt \u03a6tv can be computed in O(nd2) time. For computing (L \u2297 Id)v, we use that (BT \u2297A)v = vec(AV B), where V is an n\u00d7 d matrix such that vec(V ) = v. This implies (L\u2297 Id)v can be written as V LT which can be computed in O(d \u00b7 nnz(L)) time, where nnz(L) is the number of non-zeroes in L. This approach thus has a memory requirement of O(nd2 + nnz(L)) and a time complexity of O(\u03ba(nd2 +d \u00b7nnz(L))) per mean estimation. Here, \u03ba is the number of conjugate gradient iterations which depends on the condition number of the matrix (we used warm-starting by the solution in the previous round for our experiments, which meant that \u03ba = 5 was enough for convergence). Thus, the algorithm scales linearly in n and in the number of edges of the network (which\ntends to be linear in n due to the sparsity of social relationships). This enables us to scale to large networks, of the order of 50K nodes and millions of edges."}, {"heading": "4 Alternative Bandit Algorithms", "text": "The above structure can be used to speed up the mean estimation for any algorithm in the GOB framework. However, the LINUCB-like algorithm in [7] needs to estimate the confidence intervals \u221a \u03c6Ti,j\u03a3\u22121t \u03c6i,j for each available item j \u2208 Ct. Using the GMRF connection, estimating these requires O(|Ct|\u03ba(nd2 + d \u00b7 nnz(L))) time since we need solve the linear system with |Ct| right-hand sides, one for each available item. But this becomes impractical when the number of available items in each round is large.\nWe propose two approaches for mitigating this: first, in this section we adapt the epoch-greedy [27] algorithm to the GOB framework. Epoch-greedy doesn\u2019t require confidence intervals and is thus very scalable, but unfortunately it doesn\u2019t achieve the optimal regret of O\u0303( \u221a T ). To achieve the optimal regret, we also propose a GOB variant of Thompson sampling [29]. In this section we further exploit the connection to GMRFs to scale Thompson sampling to even larger problems by using the recent sampling-by-perturbation trick [37]. This GMRF connection and scalability trick might be of independent interest for Thompson sampling in other large-scale problems."}, {"heading": "4.1 Epoch-Greedy", "text": "Epoch-greedy [27] is a variant of the popular -greedy algorithm that explicitly differentiates between exploration and exploitation rounds. An \u201cexploration\u201d round consists of recommending a random item from Ct to the target user it. The feedback from these exploration rounds is used to learn w\u2217. An \u201cexploitation\u201d round consists of choosing the available item j\u0302t which maximizes the expected rating, j\u0302t = argmaxj\u2208Ct w\u0302 T t \u03c6it,j . Epoch-greedy proceeds in epochs, where each epoch q consists of 1 exploration round and sq exploitation rounds.\nScalability: The time complexity for Epoch-Greedy is dominated by the exploitation rounds that require computing the mean and estimating the expected rating for all the available items. Given the mean vector, this estimation takes O(d|Ct|) time. The overall time complexity per exploitation round is thus O(\u03ba(nd2 + d \u00b7 nnz(L)) + d|Ct|).\nRegret: We assume that we incur a maximum regret of 1 in an exploration round, whereas the regret incurred in an exploitation round depends on how well we have learned w\u2217. The attainable regret is thus proportional to the generalization error for the class of hypothesis functions mapping the context vector to an expected rating [27]. In our case, the class of hypotheses is a set\nof linear functions (one for each user) with Laplacian regularization. We characterize the generalization error in the GOB framework in terms of its Rademacher complexity [34], and use this to bound the expected regret leading to the result below. For ease of exposition in the regret bounds, we suppress the factors that don\u2019t depend on either n, L, \u03bb or T . The complete bound is stated in the supplementary material (Appendix B).\nTheorem 1. Under the additional assumption that ||wt||2 \u2264 1 for all rounds t, the expected regret obtained by epoch-greedy in the GOB framework is given as:\nR(T ) = O\u0303 ( n1/3 ( Tr(L\u22121) \u03bbn ) 1 3 T 2 3 )\nProof Sketch. Let H be the class of valid hypotheses of linear functions coupled with Laplacian regularization. Let Err(q,H) be the generalization error for H after obtaining q unbiased samples in the exploration rounds. We adapt Corollary 3.1 from [27] to our context:\nLemma 1. If sq = \u230a\n1 Err(q,H)\n\u230b and QT is the smallest\nQ such that Q+ \u2211Q q=1 sq \u2265 T , the regret obtained by Epoch-Greedy can be bounded as R(T ) \u2264 2QT .\nWe use [34] to bound the generalization error of our class of hypotheses in terms of its empirical Rademacher complexity R\u0302nq (H). With probability 1\u2212 \u03b4,\nErr(q,H) \u2264 R\u0302nq (H) +\n\u221a 9 ln(2/\u03b4)\n2q . (3)\nUsing Theorem 2 in [34] and Theorem 12 from [5], we obtain\nR\u0302nq (H) \u2264 2 \u221a q\n\u221a 12Tr(L\u22121)\n\u03bb . (4)\nUsing (3) and (4) we obtain\nErr(q,H) \u2264\n[ 2 \u221a 12Tr(L\u22121)/\u03bb+ \u221a\n9 ln(2/\u03b4) 2 ] \u221a q . (5)\nThe theorem follows from (5) along with Lemma 1.\nThe effect of the graph on this regret bound is reflected through the term Tr(L\u22121). For a connected graph, we have the following upper-bound Tr(L\n\u22121) n \u2264\n(1\u22121/n) \u03bd2 + 1n [34]. Here, \u03bd2 is the second smallest eigenvalue of the Laplacian. The value \u03bd2 represents the algebraic connectivity of the graph [15]. For a more connected graph, \u03bd2 is higher, the value of Tr(L \u22121) n is\nlower, resulting in a smaller regret. Note that although this result leads to a sub-optimal dependence on T (T 23 instead of T 12 ), our experiments incorporate a small modification that gives similar performance to the more-expensive LINUCB."}, {"heading": "4.2 Thompson sampling", "text": "A common alternative to LINUCB and Epoch-Greedy is Thompson sampling (TS). At each iteration TS uses a sample w\u0303t from the posterior distribution at round t, w\u0303t \u223c N (wt,\u03a3\u22121t ). It then selects the item jt based on the obtained sample, jt = argmaxj\u2208Ct w\u0303 T t \u03c6it,j . We show below that the GMRF connection makes TS scalable, but unlike Epoch-Greedy it also achieves the optimal regret.\nScalability: The conventional approach for sampling from a multivariate Gaussian posterior involves forming the Cholesky factorization of the posterior covariance matrix. But in the GOB model the posterior covariance matrix is a dn-dimensional matrix where the fill-in from the Cholesky factorization can lead to a computational complexity of O(d2n2). In order to implement Thompson sampling for large values of n, we adapt the recent sampling-by-perturbation approach [37] to our setting, and this allows us to sample from a Gaussian prior and then solve a linear system to sample from the posterior.\nLet w\u03030 be a sample from the prior distribution and let r\u0303t be the perturbed (with standard normal noise) rating vector at round t, meaning that r\u0303t = rt + yt for yt \u223c N (0, It). In order to obtain a sample w\u0303t from the posterior, we can solve the linear system\n\u03a3tw\u0303t = (L\u2297 Id)w\u03030 + \u03a6Tt r\u0303t. (6)\nLet S be the Cholesky factor of L so that L = SST . Note that L\u2297 Id = (S\u2297 Id)(S\u2297 Id)T . If z \u223c N (0, Idn), we can obtain a sample from the prior by solving (S \u2297 Id)w\u03030 = z. Since S tends to be sparse (using for example [12, 25]), this equation can be solved efficiently using conjugate gradient. We can pre-compute and store S and thus obtain a sample from the prior in time O(d \u00b7 nnz(L)). Using that \u03a6Tt r\u0303t = bt + \u03a6Tt yt in (6) and simplifying we obtain\n\u03a3tw\u0303t = (L\u2297 Id)w\u03030 + bt + \u03a6Tt yt (7)\nAs before, this system can be solved efficiently using conjugate gradient. Note that solving (7) results in an exact sample from the dn-dimensional posterior. Computing \u03a6Tt yt has a time complexity of O(dt). Thus, this approach is faster than the original GOB framework whenever t < dn2. Since we focus on the case of large graphs, this condition will tend to hold in our setting.\nWe now describe an alternative method of constructing the right side of (7) that doesn\u2019t depend on t. Observe\nthat computing \u03a6Tt yt is equivalent to sampling from the distribution N (0,\u03a6Tt \u03a6t). To sample from this distribution, we maintain the Cholesky factor Pt of \u03a6Tt \u03a6t. Recall that the matrix \u03a6Tt \u03a6t is block diagonal (one block for every user) for all rounds t. Hence, its Cholesky factor Pt also has a block diagonal structure and requires O(nd2) storage. In each round, we make a recommendation to a single user and thus make a rank-1 update to only one d \u00d7 d block of Pt. This is an order O(d2) operation. Once we have an updated Pt, sampling from N (0,\u03a6Tt \u03a6t) and constructing the right side of (7) is an O(nd2) operation. The per-round computational complexity for our TS approach is thus O(min{nd2, dt}+ d \u00b7nnz(L)) for forming the right side in (7), O(nd2 +d \u00b7nnz(L)) for solving the linear system in (7) as well as for computing the mean, and O(d \u00b7 |Ct|) for selecting the item. Thus, our proposed approach has a complexity linear in the number of nodes and edges and can scale to large networks.\nRegret: To analyze the regret with TS, observe that TS in the GOB framework is equivalent to solving a single dn-dimensional contextual bandit problem, but with a modified prior covariance equal to (\u03bbL\u2297 Id)\u22121 instead of Idn. We obtain the result below by following a similar argument to Theorem 1 in [2]. The main challenge in the proof is to make use of the available graph to bound the variance of the arms. We first state the result and then sketch the main differences from the original proof.\nTheorem 2. Under the following additional technical assumptions: (a) log(K) < (dn\u2212 1) ln(2), (b) \u03bb < dn, and (c) log ( 3+T/\u03bbdn\n\u03b4\n) \u2264 log(KT ) log(T/\u03b4), with prob-\nability 1\u2212\u03b4, the regret obtained by Thompson Sampling in the GOB framework is given as:\nR(T ) = O\u0303 ( dn \u221a T\u221a \u03bb \u221a log ( 3 Tr(L\u22121) n + Tr(L \u22121)T \u03bbdn2\u03c32 ))\nProof Sketch. To make the notation cleaner, for the round t and target user it under consideration, we use j to index the available items. Let the index of the optimal item at round t be j\u2217t whereas the index of the item chosen by our algorithm is denoted jt. Let st(j) be the standard deviation in the estimated rating of item j at round t. It is given as st(j) = \u221a \u03c6Tj \u03a3 \u22121 t\u22121\u03c6j .\nFurther, let lt = \u221a dn log ( 3+t/\u03bbdn\n\u03b4\n) + \u221a\n3\u03bb. Let E\u00b5(t) be the event such that for all j,\nE\u00b5(t) : |\u3008wt,\u03c6j\u3009 \u2212 \u3008w \u2217,\u03c6j\u3009| \u2264 ltst(j)\nWe prove that, for \u03b4 \u2208 (0, 1), Pr(E\u00b5(t)) \u2265 1\u2212 \u03b4. Define gt = \u221a 4 log(tK)\u03c1t + lt, where \u03c1t = \u221a 9d log ( t \u03b4 ) . Let\n\u03b3 = 14e\u221a\u03c0 . Given that the event E \u00b5(t) holds with high probability, we follow an argument similar to Lemma 4 of [2] and obtain the following bound:\nR(T ) \u2264 3gT \u03b3 T\u2211 t=1 st(jt) + 2gT \u03b3 T\u2211 t=1 1 t2\n+6gT \u03b3\n\u221a 2T ln 2/\u03b4 (8)\nTo bound the variance of the selected items,\u2211T t=1 st(jt), we extend the analysis in [11, 43] to include the prior covariance term. We thus obtain the following inequality:\nT\u2211 t=1 st(jt) \u2264 \u221a dnT\n\u00d7 \u221a C log ( Tr(L\u22121)\nn\n) + log ( 3 + T\n\u03bbdn\u03c32 ) (9)\nwhere C = 1 \u03bb log(1+ 1 \u03bb\u03c32 ) . Substituting this into (8) completes the proof.\nNote that since n is large in our case, assumption (a) for the above theorem is reasonable. Assumptions (b) and (c) define the upper and lower bounds on the regularization parameter \u03bb. Similar to epoch-greedy, transferring information across the graph reduces the regret by a factor dependent on Tr(L\u22121). Note that compared to epoch-greedy, the regret bound for Thompson sampling has a worse dependence on n, but its O\u0303( \u221a T ) dependence on T is optimal. If L = Idn, we match the O\u0303(dn \u221a T ) regret bound for a dn-dimensional contextual bandit problem [1]. Note that we have a dependence on d and n similar to the original GOB paper [7] and that this method performs similarly in practice in terms of regret. However, as will see, our algorithm is much faster."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "Data: We first test the scalability of various algorithms using synthetic data and then evaluate their regret performance on two real datasets. For synthetic data we generate random d-dimensional context vectors and ground-truth user preferences, and generate the ratings according to the linear model. We generated a random Kronecker graph with sparsity 0.005 (which is approximately equal to the sparsity of our real datasets). It is well known that such graphs capture many properties of real-world social networks [28].\nFor the real data, we use the Last.fm and Delicious datasets which are available as part of the HetRec 2011 workshop. Last.fm is a music streaming website where each item corresponds to a music artist and the dataset consists of the set of artists each user has listened to. The associated social network consists of 1.8K users (nodes) and 12.7K friendship relations (edges). Delicious is a social bookmarking website, where an item corresponds to a particular URL and the dataset consists of the set of websites bookmarked by each user. Its corresponding social network consists of 1.8K users and 7.6K user-user relations. Similar to [7], we use the set of associated tags to construct the TF-IDF vector for each item and reduce the dimension of these vectors to d = 25. An artist (or URL) that a user has listened to (or has bookmarked) is said to be \u201cliked\u201d by the user. In each round, we select a target user uniformly at random and make the set Ct consist of 25 randomly chosen items such that there is at least 1 item liked by the target user. An item liked by the target user is assigned a reward of 1 whereas other items are assigned a zero reward. We use a total of T = 50 thousand recommendation rounds and average our results across 3 runs.\nAlgorithms: We denote our graph-based epochgreedy and Thompson sampling algorithms as G-EG and G-TS, respectively. For epoch-greedy, although the theory suggests that we update the preference estimates only in the exploration rounds, we observed better performance by updating the preference vectors in all rounds (we use this variant in our experiments). We use 10% of the total number of rounds for exploration, and we \u201cexploit\" in the remaining rounds. Similar to [17], all hyper-parameters are set using an initial validation set of 5 thousand rounds. The best validation performance was observed for \u03bb = 0.01 and \u03c3 = 1. To control the amount of exploration for Thompson sampling, we the use posterior reshaping trick [8] which reduces the variance of the posterior by a factor of 0.01.\nBaselines: We consider two variants of graph-based UCB-style algorithms: GOBLIN is the method proposed in the original GOB paper [7] while we use GOBLIN++ to refer to a variant that exploits the fast mean estimation strategy we develop in Section 3.3. Similar to [7], for both variants we discount the confidence bound term by a factor of \u03b1 = 0.01.\nWe also include baselines which ignore the graph structure and make recommendations by solving independent linear contextual bandit problems for each user. We consider 3 variants of this baseline: the LINUCBIND proposed in [29], an epoch-greedy variant of this approach (EG-IND), and a Thompson sampling variant (TS-IND). We also compared to a baseline that does no personalization and simply considers a single bandit\nproblem across all users (LINUCB-SIN). Finally, we compared against the state-of-the-art online clusteringbased approach proposed in [17], denoted CLUB. This method starts with a fully connected graph and iteratively deletes edges from the graph based on UCB estimates. CLUB considers each connected component of this graph as a cluster and maintains one preference vector for all the users belonging to a cluster. Following the original work, we make CLUB scalable by generating a random Erdos-Renyi graph Gn,p with p = 3lognn . 2 In all, we compare our proposed algorithms G-EG and G-TS with 7 reasonable baseline methods."}, {"heading": "5.2 Results", "text": "Scalability: We first evaluate the scalability of the various algorithms with respect to the number of network nodes n. Figure 1(a) shows the runtime in seconds/iteration when we fix d = 25 and vary the size of the network from 16 thousand to 33 thousand nodes. Compared to GOBLIN, our proposed GOBLIN++ is more efficient in terms of both time (almost 2 orders of magnitude faster) and memory. Indeed, the existing GOBLIN method runs out of memory even on very small networks and thus we do not plot it for larger networks. Further, our proposed G-EG and G-TS methods scale even more gracefully in the number of nodes and are much faster than GOBLIN++ (although not as fast as the clustering-based CLUB or methods that ignore the graph).\nWe next consider scalability with respect to d. Figure 1(b) fixes n = 1024 and varies d from 10 to 500. In this figure it is again clear that our proposed GOBLIN++ scales much better than the original GOBLIN algorithm. The EG and TS variants are again even faster, and other key findings from this experiment are (i) it was not faster to ignore the graph and (ii) our proposed G-EG and G-TS methods scale better with d than CLUB.\nRegret Minimization: We follow [17] in evaluating recommendation performance by plotting the ratio of cumulative regret incurred by the algorithm divided by the regret incurred by a random selection policy. Figure 2(a) plots this measure for the Last.fm dataset. In this dataset we see that treating the users independently (LINUCB-IND) takes a long time to drive down the regret (we do not plot EG-IND and TS-IND as they had similar performance) while simply aggregating across users (LINUCB-SIN) performs well initially (but eventually stops making progress). We see that the approaches exploiting the graph help learn the user\n2We reimplemented CLUB. Note that one of the datasets from our experiments was also used in that work and we obtain similar performance to that reported in the original paper.\n(a) (b)Figure 1: Synthetic network: Runtime (in seconds/iteration) vs (a) Number of nodes (b) Dimension\n(a) Last.fm (b) DeliciousFigure 2: Regret Minimization\npreferences faster than the independent approach and we note that on this dataset our proposed G-TS method performed similar to or slightly better than the state of the art CLUB algorithm.\nFigure 2(b) shows performance on the Delicious dataset. On this dataset personalization is more important and we see that the independent method (LINUCB-IND) outperforms the non-personalized (LINUCB-SIN) approach. The need for personalization in this dataset also leads to worse performance of the clustering-based CLUB method, which is outperformed by all methods that model individual users. On this dataset the advantage of using the graph is less clear, as the graph-based methods perform similar to the independent method. Thus, these two experiments suggest that (i) the scalable graph-based methods do no worse than ignoring the graph in cases where the graph is not helpful and (ii) the scalable graph-based methods can do significantly better on datasets where the graph is helpful. Similarly, when user preferences naturally form clusters our proposed methods perform similarly to CLUB, whereas on datasets where individual preferences are\nimportant our methods are significantly better."}, {"heading": "6 Discussion", "text": "This work draws a connection between the GOB framework and GMRFs, and uses this to scale up the existing GOB model to much larger graphs. We also proposed and analyzed Thompson sampling and epochgreedy variants. Our experiments on recommender systems datasets indicate that the Thompson sampling approach in particular is much more scalable than existing GOB methods, obtains theoretically optimal regret, and performs similar to or better than other existing scalable approaches.\nIn many practical scenarios we do not have an explicit graph structure available. In the supplementary material we consider a variant of the GOB model where we use L1-regularization to learn the graph on the fly. Our experiments there show that this approach works similarly to or much better than approaches which use the fixed graph structure. It would be interesting to explore the theoretical properties of this approach."}, {"heading": "A Learning the Graph", "text": "In the main paper, we assumed that the graph is known, but in practice such a user-user graph may not be available. In such a case, we explore a heuristic to learn the graph on the fly. The computational gains described in the main paper make it possible to simultaneously learn the user-preferences and infer the graph between users in an efficient manner. Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40]. However, prior works that learn the graph in related settings only tackle problem with tens or hundreds of tasks/labels while we\nlearn the graph and preferences across thousands of users.\nLet Vt \u2208 Rn\u00d7n be the inverse covariance matrix corresponding to the graph inferred between users at round t. Since zeroes in the inverse covariance matrix correspond to conditional independences between the corresponding nodes (users) [39], we use L1 regularization on Vt for encouraging sparsity in the inferred graph. We use an additional regularization term \u2206(Vt||Vt\u22121) to encourage the graph to change smoothly across rounds. This encourages Vt to be close to Vt\u22121 according to a distance metric \u2206. Following [40], we choose \u2206 to be the log-determinant Bregman divergence given by \u2206(X||Y ) = Tr(XY \u22121)\u2212 log |XY \u22121| \u2212 dn. If Wt \u2208 Rd\u00d7n = [w1w2 . . .wn] corresponds to the matrix of user preference estimates, the combined objective can\nbe written as:\n[wt, Vt] = argmin w,V\n||rt \u2212 \u03a6tw||22 + Tr ( V (\u03bbWTW + V \u22121t\u22121) ) + \u03bb2||V ||1 \u2212 (dn+ 1) ln |V | (10)\nThe first term in (10) is the data fitting term. The second term imposes the smoothness constraint across the graph and ensures that the changes in Vt are smooth. The third term ensures that the learnt precision matrix is sparse, whereas the last term penalizes the complexity of the precision matrix. This function is independently convex in both w and V (but not jointly convex), and we alternate between solving for wt and Vt in each round. With a fixed Vt, the w sub-problem is the same as the MAP estimation in the main paper and can be done\nefficiently. For a fixed wt, the V sub-problem is given by\nVt = argmin V\nTr ( (V [\u03bbWTt W t + V \u22121t\u22121) ) + \u03bb2||V ||1 \u2212 (dn+ 1) ln |V | (11)\nHere W t refers to the mean subtracted (for each dimension) matrix of user preferences. This problem can be written as a graphical lasso problem [16], minX Tr(SX) + \u03bb2||X||1 \u2212 log |X|, where the empirical covariance\nmatrix S is equal to \u03bbWTt W t + V \u22121t\u22121. We use the highly-scalable second order methods described in [21, 22] to solve (11). Thus, both sub-problems in the alternating minimization framework at each round can be solved\nefficiently.\nFor our preliminary experiments in this direction, we use the most scalable epoch-greedy algorithm for learning the graph on the fly and denote this version as L-EG. We also consider another variant, U-EG in which we start from the Laplacian matrix L corresponding to the given graph and allow it to change by re-estimating the graph according to (11). Since U-EG has the flexibility to infer a better graph than the one given, such a variant is important for cases where the prior is meaningful but somewhat misspecified (the given graph accurately reflects some but not all of the user similarities). Similar to [40], we start off with an empty graph and start learning the\ngraph only after the preference vectors have become stable, which happens in this case after each user has received 10 recommendations. We update the graph every 1K rounds. For both datasets, we allow the learnt\ngraph to contain at most 100K edges and tune \u03bb2 to achieve a sparsity level equal to 0.05 in both cases.\nTo avoid clutter, we plot all the variants of the EG algorithm, L-EG and U-EG, and use EG-IND, G-EG, EG-SIN as baselines. We also plot CLUB as a baseline. For the Last.fm dataset (Figure 3(b)(a)), U-EG performs slightly better than G-EG, which already performed well. The regret for L-EG is lower compared to LINUCB-IND indicating that learning the graph helps, but is worse as compared to both CLUB and LINUCB-SIN. On the other hand, for Delicious (Figure 3(b)(b)), L-EG and U-EG are the best performing methods. L-EG slightly outperforms EG-IND, underscoring the importance of learning the user-user graph and transferring information between users. It also outperforms G-EG, which implies that it is able to learn a graph which reflects user\nsimilarities better than the existing social network between users. For both datasets, U-EG is among the top performing methods, which implies that allowing modifications to a good (in that it reflects user similarities\n(a) Last.fm (b) DeliciousFigure 3: Regret Minimization while learning the graph\nreasonably well) initial graph to model the obtained data might be a good method to overcome prior misspecification. From a scalability point of view, for Delicious the running time for L-EG is 0.1083\nseconds/iteration (averaged across T ) as compared to 0.04 seconds/iteration for G-EG. This shows that even in the absence of an explicit user-user graph, it is possible to achieve a low regret in an efficient manner."}, {"heading": "B Regret bound for Epoch-Greedy", "text": "Theorem 1. Under the additional assumption that ||wt||2 \u2264 1 for all rounds t, the expected regret obtained by epoch-greedy in the GOB framework is given as:\nR(T ) = O\u0303 ( n1/3 ( Tr(L\u22121) \u03bbn ) 1 3 T 2 3 ) (12)\nProof. LetH be the class of hypotheses of linear functions (one for each user) coupled with Laplacian regularization. Let \u00b5(H, q, s) represent the regret or cost of performing s exploitation steps in epoch q. Let the number of exploitation steps in epoch q be sq.\nLemma 2 (Corollary 3.1 from [27]). If sq = b 1\u00b5(H,q,1)c and QT is the minimum Q such that Q+ \u2211Q q=1 sq \u2265 T , then the regret obtained by Epoch Greedy is bounded by R(T ) \u2264 2QT .\nWe now bound the quantity \u00b5(H, q, 1). Let Err(q,H) be the generalization error for H after obtaining q unbiased samples in the exploration rounds. Clearly,\n\u00b5(H, q, s) = s \u00b7 Err(q,H). (13)\nLet `LS be the least squares loss. Let the number of unbiased samples per user be equal to p. The empirical Rademacher complexity for our hypotheses class H under `LS can be given as R\u0302np (`LS \u25e6 H). The generalization error for H can be bounded as follows:\nLemma 3 (Theorem 1 from [34]). With probability 1\u2212 \u03b4,\nErr(q,H) \u2264 R\u0302np (`LS \u25e6 H) +\n\u221a 9 ln(2/\u03b4)\n2pn (14)\nAssume that the target user is chosen uniformly at random. This implies that the expected number of samples per user is at least p = b qnc. For simplicity, assume q is exactly divisible by n so that p = q n (this only affects the bound by a constant factor). Substituting p in (14), we obtain\nErr(q,H) \u2264 R\u0302np (`LS \u25e6 H) +\n\u221a 9 ln(2/\u03b4)\n2q . (15)\nThe Rademacher complexity can be bounded using Lemma 4 (see below) as follows:\nR\u0302np (`LS \u25e6 H) \u2264 1 \u221a p\n\u221a 48 Tr(L\u22121)\n\u03bbn = 1\u221a q\n\u221a 48 Tr(L\u22121)\n\u03bb (16)\nSubstituting this into (15) we obtain\nErr(q,H) \u2264 1\u221a q\n[\u221a 48 Tr(L\u22121) \u03bb + \u221a 9 ln(2/\u03b4) 2 ] . (17)\nWe set sq = 1Err(q,H) . Denoting [\u221a 48 Tr(L\u22121) \u03bb + \u221a 9 ln(2/\u03b4) 2 ] as C, sq = \u221a q C .\nRecall that from Lemma 2, we need to determine QT such that\nQT + QT\u2211 q=1 sq \u2265 T =\u21d2 QT\u2211 q=1 (1 + sq) \u2265 T\nSince sq \u2265 1, this implies that \u2211QT q=1 2sq \u2265 T . Substituting the value of sq and observing that for all q, sq+1 \u2265 sq, we obtain the following:\n2QT sQT \u2265 T =\u21d2 2 Q\n3/2 T C \u2265 T =\u21d2 QT \u2265\n( CT\n2\n) 2 3\nQT = [\u221a\n12 Tr(L\u22121) \u03bb\n+ \u221a\n9 ln(2/\u03b4) 8\n] 2 3\nT 2 3 (18)\nUsing the above equation with Lemma 2, we can bound the regret as\nR(T ) \u2264 2 [\u221a\n12 Tr(L\u22121) \u03bb\n+ \u221a\n9 ln(2/\u03b4) 8\n] 2 3\nT 2 3 (19)\nTo simplify this expression, we suppress the term \u221a\n9 ln(2/\u03b4) 8 in the O\u0303 notation, implying that\nR(T ) = O\u0303 ( 2 [\n12 Tr(L\u22121) \u03bb\n] 1 3\nT 2 3 ) (20)\nTo present and interpret the result, we keep only the factors which are dependent on n, \u03bb, L and T . We then obtain\nR(T ) = O\u0303 ( n1/3 ( Tr(L\u22121) \u03bbn ) 1 3 T 2 3 ) (21)\nThis proves Theorem 1. We now prove Lemma 4, which was used to bound the Rademacher complexity.\nLemma 4. The empirical Rademacher complexity for H under `LS on observing p unbiased samples for each of the n users can be given as:\nR\u0302np (`LS \u25e6 H) \u2264 1 \u221a p\n\u221a 48 Tr(L\u22121)\n\u03bbn (22)\nProof. The Rademacher complexity for a class of linear predictors with graph regularization for a 0/1 loss function `0,1 can be bounded using Theorem 2 of [34]. Specifically,\nR\u0302np (`0,1 \u25e6 H) \u2264 2M \u221a p\n\u221a Tr((\u03bbL)\u22121)\nn (23)\nwhere M is the upper bound on the value of ||L 1 2 W\u2217||2\u221a n\nand W \u2217 is the d\u00d7 n matrix corresponding to the true user preferences.\n(24)\nWe now upper bound ||L 1 2 W\u2217||2\u221a n .\n||L 12W \u2217||2 \u2264 ||L 1 2 ||2||W \u2217||2\n||W \u2217||2 \u2264 ||W \u2217||F = \u221a\u221a\u221a\u221a n\u2211 i=1 ||w\u2217i ||22 ||W \u2217||2 \u2264 \u221a n (Using assumption 1: For all i, ||w\u2217i ||2 \u2264 1) ||L 12 || \u2264 \u03bdmax(L 1 2 ) = \u221a \u03bdmax(L) \u2264 \u221a 3\n(The maximum eigenvalue of any normalized Laplacian LG is 2 [10] and recall that L = LG + In)\n=\u21d2 ||L 1 2W \u2217||2\u221a n \u2264 \u221a 3 =\u21d2 M = \u221a 3 (25)\nSince we perform regression using a least squares loss function instead of classification, the Rademacher complexity in our case can be bounded using Theorem 12 from [5]. Specifically, if \u03c1 is the Lipschitz constant of the least squares problem,\nR\u0302np (`LS \u25e6 H) \u2264 2\u03c1 \u00b7 Rnp (`0,1 \u25e6 H) (26)\nSince the estimates wi,t are bounded from above by 1 (additional assumption in the theorem), \u03c1 = 1. From Equations 24, 26 and the bound on M , we obtain that\nR\u0302np (`LS \u25e6 H) \u2264 4 \u221a p\n\u221a 3 Tr(L\u22121)\n\u03bbn (27)\nwhich proves the lemma. Theorem 2. Under the following additional technical assumptions: (a) log(K) < (dn\u2212 1) ln(2) (b) \u03bb < dn (c) log (\n3+T/\u03bbdn \u03b4\n) \u2264 log(KT ) log(T/\u03b4), with probability 1\u2212 \u03b4, the regret obtained by Thompson Sampling in the GOB\nframework is given as:\nR(T ) = O\u0303 ( dn\u221a \u03bb \u221a T \u221a log ( Tr(L\u22121) n ) + log ( 3 + T \u03bbdn\u03c32 )) (28)\nProof. We can interpret graph-based TS as being equivalent to solving a single dn-dimensional contextual bandit problem, but with a modified prior covariance ((L\u2297 Id)\u22121 instead of Idn). Our argument closely follows the proof structure in [2], but is modified to include the prior covariance. For ease of exposition, assume that the target user at each round is implicit. We use j to index the available items. Let the index of the optimal item at round t be j\u2217t , whereas the index of the item chosen by our algorithm is denoted jt.\nLet r\u0302t(j) be the estimated rating of item j at round t. Then, for all j,\nr\u0302t(j) \u223c N (\u3008wt,\u03c6j\u3009, st(j)) (29)\nHere, st(j) is the standard deviation in the estimated rating for item j at round t. Recall that \u03a3t\u22121 is the covariance matrix at round t. st(j) is given as:\nst(j) = \u221a \u03c6Tj \u03a3 \u22121 t\u22121\u03c6j (30)\nWe drop the argument in st(jt) to denote the standard deviation and estimated rating for the selected item jt i.e. st = st(jt) and r\u0302t = r\u0302t(jt).\nLet \u2206t measure the immediate regret at round t incurred by selecting item jt instead of the optimal item j\u2217t . The immediate regret is given by:\n\u2206t = \u3008w\u2217,\u03c6j\u2217t \u3009 \u2212 \u3008w \u2217,\u03c6jt\u3009 (31)\nDefine E\u00b5(t) as the event such that for all j,\nE\u00b5(t) : |\u3008wt,\u03c6j\u3009 \u2212 \u3008w \u2217,\u03c6j\u3009| \u2264 ltst(j) (32)\nHere lt = \u221a dn log ( 3+t/\u03bbdn\n\u03b4\n) + \u221a\n3\u03bb. If the event E\u00b5(t) holds, it implies that the expected rating at round t is close to the true rating with high probability.\nRecall that |Ct| = K and that w\u0303t is a sample drawn from the posterior distribution at round t. Define \u03c1t = \u221a 9dn log ( t \u03b4 ) and gt = min{ \u221a 4dn ln(t), \u221a 4 log(tK)}\u03c1t + lt. Define E\u03b8(t) as the event such that for all j,\nE\u03b8(t) : |\u3008w\u0303t,\u03c6j\u3009 \u2212 \u3008wt,\u03c6j\u3009| \u2264 min{ \u221a 4dn ln(t), \u221a 4 log(tK)}\u03c1tst(j) (33)\nIf the event E\u03b8(t) holds, it implies that the estimated rating using the sample w\u0303t is close to the expected rating at round t.\n(34)\nIn lemma 7, we prove that the event E\u00b5(t) holds with high probability. Formally, for \u03b4 \u2208 (0, 1),\nPr(E\u00b5(t)) \u2265 1\u2212 \u03b4 (35)\nTo show that the event E\u03b8(t) holds with high probability, we use the following lemma from [2].\nLemma 5 (Lemma 2 of [2]).\nPr(E\u03b8(t))|Ft\u22121) \u2265 1\u2212 1 t2\n(36)\nNext, we use the following lemma to bound the immediate regret at round t.\nLemma 6 (Lemma 4 in [2]). Let \u03b3 = 14e\u221a\u03c0 . If the events E \u00b5(t) and E\u03b8(t) are true, then for any filtration Ft\u22121, the following inequality holds:\nE[\u2206t|Ft\u22121] \u2264 3gt \u03b3 E[st|Ft\u22121] + 2gt \u03b3t2\n(37)\nDefine I(E) to be the indicator function for an event E . Let regret(t) = \u2206t \u00b7 I(E\u00b5(t)). We use Lemma 8 (proof is given later) which states that with probability at least 1\u2212 \u03b42 ,\nT\u2211 t=1 regret(t) \u2264 T\u2211 t=1 3gt \u03b3 st + T\u2211 t=1 2gt \u03b3t2 + \u221a\u221a\u221a\u221a2 T\u2211 t=1 36g2t \u03b32 ln(2/\u03b4) (38)\nFrom Lemma 7, we know that event E\u00b5(t) holds for all t with probability at least 1\u2212 \u03b42 . This implies that, with probability 1\u2212 \u03b42 , for all t\nregret(t) = \u2206t (39)\nFrom Equations 38 and 39, we have that with probability 1\u2212 \u03b4,\nR(T ) = T\u2211 t=1 \u2206t \u2264 T\u2211 t=1 3gt \u03b3 st + T\u2211 t=1 2gt \u03b3t2 + \u221a\u221a\u221a\u221a2 T\u2211 t=1 36g2t \u03b32 ln(2/\u03b4)\nNote that gt increases with t i.e. for all t, gt \u2264 gT\nR(T ) \u2264 3gT \u03b3 T\u2211 t=1 st + 2gT \u03b3 T\u2211 t=1 1 t2 + 6gT \u03b3 \u221a 2T ln(2/\u03b4) (40)\nUsing Lemma 9 (proof given later), we have the following bound on \u2211T t=1 st, the variance of the selected items:\nT\u2211 t=1 st \u2264 \u221a dnT\n\u221a C log ( Tr(L\u22121)\nn\n) + log ( 3 + T\n\u03bbdn\u03c32\n) (41)\nwhere C = 1 \u03bb log(1+ 1 \u03bb\u03c32 ) .\n(42)\nSubstituting this into Equation 40, we get\nR(T ) \u2264 3gT \u03b3 \u221a dnT\n\u221a C log ( Tr(L\u22121)\nn\n) + log ( 3 + T\n\u03bbdn\u03c32\n) + 2gT\n\u03b3 T\u2211 t=1 1 t2 + 6gT \u03b3 \u221a 2T ln(2/\u03b4)\nUsing the fact that \u2211T t=1 1 t2 < \u03c02 6\nR(T ) \u2264 3gT \u03b3 \u221a dnT\n\u221a C log ( Tr(L\u22121)\nn\n) + log ( 3 + T\n\u03bbdn\u03c32\n) + \u03c0\n2gT 3\u03b3 + 6gT \u03b3\n\u221a 2T ln(2/\u03b4) (43)\nWe now upper bound gT . By our assumption on K, log(K) < (dn \u2212 1) ln(2). Hence for all t \u2265 2, min{ \u221a 4dn ln(t), \u221a 4 log(tK)} = \u221a 4 log(tK). Hence,\ngT = 6 \u221a dn log(KT ) log(T/\u03b4) + lT\n= 6 \u221a dn log(KT ) log(T/\u03b4) + \u221a dn log ( 3 + T/\u03bbdn\n\u03b4\n) + \u221a 3\u03bb\nBy our assumption on \u03bb, \u03bb < dn. Hence,\ngT \u2264 8 \u221a dn log(KT ) log(T/\u03b4) + \u221a dn log ( 3 + T/\u03bbdn\n\u03b4\n)\nUsing our assumption that log (\n3+T/\u03bbdn \u03b4\n) \u2264 log(KT ) log(T/\u03b4),\ngT \u2264 9 \u221a dn log(KT ) log(T/\u03b4)\n(44)\nSubstituting the value of gT into Equation 43, we obtain the following:\nR(T ) \u2264 27dn \u03b3 \u221a T\n\u221a C log ( Tr(L\u22121)\nn\n) + log ( 3 + T\n\u03bbdn\u03c32 ) + 3\u03c02 \u221a dn ln(T/\u03b4) ln(KT )\n\u03b3 +\n54 \u221a dn ln(T/\u03b4) ln(KT ) \u221a 2T ln(2/\u03b4)\n\u03b3\nFor ease of exposition, we keep the just leading terms on d, n and T . This gives the following bound on R(T ).\nR(T ) = O\u0303 (\n27dn \u03b3 \u221a T\n\u221a C log ( Tr(L\u22121)\nn\n) + log ( 3 + T\n\u03bbdn\u03c32 )) Rewriting the bound to keep only the terms dependent on d, n, \u03bb, T and L. We thus obtain the following equation.\nR(T ) = O\u0303 ( dn\u221a \u03bb \u221a T \u221a log ( Tr(L\u22121) n ) + log ( 3 + T \u03bbdn\u03c32 )) (45)\nThis proves the theorem.\nWe now prove the the auxiliary lemmas used in the above proof.\nIn the following lemma, we prove that E\u00b5(t) holds with high probability, i.e., the expected rating at round t is close to the true rating with high probability.\nLemma 7.\nThe following statement is true for all \u03b4 \u2208 (0, 1):\nPr(E\u00b5(t)) \u2265 1\u2212 \u03b4 (46)\nProof.\nRecall that rt = \u3008w\u2217, \u03c6jt\u3009+ \u03b7t (Assumption 2) and that \u03a3twt = bt \u03c32 . Define St\u22121 = \u2211t\u22121 l=1 \u03b7l\u03c6jl .\nSt\u22121 = t\u22121\u2211 l=1 (rl \u2212 \u3008w\u2217, \u03c6jl\u3009) \u03c6jl = t\u22121\u2211 l=1 ( rl\u03c6jl \u2212 \u03c6jl\u03c6 T jl w\u2217 )\nSt\u22121 = bt\u22121 \u2212 t\u22121\u2211 l=1 ( \u03c6jl\u03c6 T jl ) w\u2217 = bt\u22121 \u2212 \u03c32(\u03a3t\u22121 \u2212 \u03a30)w\u2217 = \u03c32(\u03a3t\u22121wt \u2212 \u03a3t\u22121w\u2217 + \u03a30w\u2217)\nw\u0302t \u2212w\u2217 = \u03a3\u22121t\u22121 (\nSt\u22121 \u03c32 \u2212 \u03a30w\u2217\n)\nThe following holds for all j:\n|\u3008wt,\u03c6j\u3009 \u2212 \u3008w \u2217,\u03c6j\u3009| = |\u3008\u03c6j ,wt \u2212w \u2217\u3009| \u2264 \u2223\u2223\u2223\u2223\u03c6Tj \u03a3\u22121t\u22121(St\u22121\u03c32 \u2212 \u03a30w\u2217 ) \u2223\u2223\u2223\u2223 \u2264 ||\u03c6j ||\u03a3\u22121\nt\u22121 (\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223St\u22121\u03c32 \u2212 \u03a30w\u2217 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u03a3\u22121 t\u22121\n) (Since \u03a3\u22121t\u22121 is positive definite)\nBy triangle inequality,\n|\u3008wt,\u03c6j\u3009 \u2212 \u3008w \u2217,\u03c6j\u3009| \u2264 ||\u03c6j ||\u03a3\u22121\nt\u22121 (\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223St\u22121\u03c32 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u03a3\u22121 t\u22121\n+ ||\u03a30w\u2217||\u03a3\u22121 t\u22121\n) (47)\nWe now bound the term ||\u03a30w\u2217||\u03a3\u22121 t\u22121\n||\u03a30w\u2217||\u03a3\u22121 t\u22121 \u2264 ||\u03a30w\u2217||\u03a3\u221210 =\n\u221a w\u2217T\u03a3T0 \u03a3 \u22121 0 \u03a30w\u2217 (Since \u03c6jt\u03c6 T jt is positive definite for all t)\n= \u221a w\u2217T\u03a30w\u2217 (Since \u03a30 is symmetric)\n\u2264 \u221a \u03bdmax(\u03a30)||w\u2217||2\n\u2264 \u221a \u03bdmax(\u03bbL\u2297 Id) (||w\u2217||2 \u2264 1)\n= \u221a \u03bdmax(\u03bbL) (\u03bdmax(A\u2297B) = \u03bdmax(A) \u00b7 \u03bdmax(B))\n\u2264 \u221a \u03bb \u00b7 \u03bdmax(L)\n||\u03a30w\u2217||\u03a3\u22121 t\u22121 \u2264 \u221a\n3\u03bb (The maximum eigenvalue of any normalized Laplacian is 2 [10] and recall that L = LG + In)\nFor bounding ||\u03c6j ||\u03a3\u22121 t\u22121 , note that\n||\u03c6j ||\u03a3\u22121 t\u22121\n= \u221a\n\u03c6Tj \u03a3 \u22121 t\u22121\u03c6j = st(j)\nUsing the above relations, Equation 47 can thus be rewritten as:\n|\u3008wt,\u03c6j\u3009 \u2212 \u3008w \u2217,\u03c6j\u3009| \u2264 st(j) ( 1 \u03c3 ||St\u22121||\u03a3\u22121 t\u22121 + \u221a 3\u03bb )\n(48)\nTo bound ||St\u22121||\u03a3\u22121 t\u22121 , we use Theorem 1 from [1] which we restate in our context. Note that using this theorem with the prior covariance equal to Idn gives Lemma 8 of [2].\nTheorem 2 (Theorem 1 of [1]). For any \u03b4 > 0, t \u2265 1, with probability at least 1\u2212 \u03b4,\n||St\u22121||2\u03a3\u22121 t\u22121 \u2264 2\u03c32 log\n( det(\u03a3t)1/2 det(\u03a30)\u22121/2\n\u03b4 ) ||St\u22121||2\u03a3\u22121 t\u22121 \u2264 2\u03c32 ( log ( det(\u03a3t)1/2 ) + log ( det(\u03a3\u221210 )1/2 ) \u2212 log(\u03b4) )\nRewriting the above equation,\n||St\u22121||2\u03a3\u22121 t\u22121 \u2264 \u03c32\n( log (det(\u03a3t)) + log ( det(\u03a3\u221210 ) ) \u2212 2 log(\u03b4) )\nWe now use the trace-determinant inequality. For any n\u00d7 n matrix A, det(A) \u2264 ( Tr(A) n )n which implies that\nlog(det(A)) \u2264 n log ( Tr(A) n ) . Using this for both \u03a3t and \u03a3\u221210 , we obtain:\n||St\u22121||\u03a3\u22121 t\u22121 \u2264 dn\u03c32\n( log ((\nTr(\u03a3t) dn\n)) + log (( Tr(\u03a3\u221210 ) dn )) \u2212 2 dn log(\u03b4) )\n(49)\nNext, we use the fact that\n\u03a3t = \u03a30 + t\u2211 l=1 \u03c6jl\u03c6 T jl =\u21d2 Tr(\u03a3t) \u2264 Tr(\u03a30) + t (Since ||\u03c6jl ||2 \u2264 1)\nNote that Tr(A\u2297B) = Tr(A) \u00b7 Tr(B). Since \u03a30 = \u03bbL\u2297 Id, it implies that Tr(\u03a30) = \u03bbd \u00b7 Tr(L). Also note that Tr(\u03a3\u221210 ) = Tr((\u03bbL)\u22121 \u2297 Id) = d\u03bb Tr(L \u22121). Using these relations in Equation 49,\n||St\u22121||2\u03a3\u22121 t\u22121 \u2264 dn\u03c32\n( log ( \u03bbdTr(L) + t\ndn\n) + log ( Tr(L\u22121) \u03bbn ) \u2212 2 dn log(\u03b4) )\n\u2264 dn\u03c32 ( log (\nTr(L) Tr(L\u22121) n2 + tTr(L \u22121) \u03bbdn2\n) \u2212 log(\u03b4 2dn ) ) (log(a) + log(b) = log(ab))\n= dn\u03c32 log (\nTr(L) Tr(L\u22121) n2\u03b4 + tTr(L \u22121) \u03bbdn2\u03b4\n) (Redefining \u03b4 as \u03b4 2dn )\nIf L = In, Tr(L) = Tr(L\u22121) = n, we recover the bound in [2] i.e.\n||St\u22121||2\u03a3\u22121 t\u22121 \u2264 dn\u03c32 log\n( 1 + t/\u03bbdn\n\u03b4\n) (50)\nThe upper bound for Tr(L) is 3n, whereas the upper bound on Tr(L\u22121) is n. We thus obtain the following relation.\n||St\u22121||2\u03a3\u22121 t\u22121 \u2264 dn\u03c32 log ( 3 \u03b4 + t \u03bbdn\u03b4 ) ||St\u22121||\u03a3\u22121\nt\u22121 \u2264 \u03c3\n\u221a dn log ( 3 + t/\u03bbdn\n\u03b4\n) (51)\nCombining Equations 48 and 51, we have with probability 1\u2212 \u03b4,\n|\u3008wt,\u03c6j\u3009 \u2212 \u3008w \u2217,\u03c6j\u3009| \u2264 st(k)\n(\u221a dn log ( 3 + t/\u03bbdn\n\u03b4\n) + \u221a 3\u03bb )\n|\u3008wt,\u03c6j\u3009 \u2212 \u3008w \u2217,\u03c6j\u3009| \u2264 st(k)lt\nwhere lt = \u221a dn log ( 3+t/\u03bbdn\n\u03b4\n) + \u221a 3\u03bb. This completes the proof.\n(52)\nLemma 8. With probability 1\u2212 \u03b4,\nT\u2211 t=1 regret(t) \u2264 T\u2211 t=1 3gt \u03b3 st + T\u2211 t=1 2gt \u03b3t2 + \u221a\u221a\u221a\u221a2 T\u2211 t=1 36g2t \u03b32 ln 2 \u03b4\n(53)\nProof.\nLet Zl and Yt be defined as follows:\nZl = regret(l)\u2212 3gl \u03b3 sl \u2212 2gl \u03b3l2\nYt = t\u2211 l=1 Zl (54) E[Yt \u2212 Yt\u22121|Ft\u22121] = E[Xt] = E[regret(t)|Ft\u22121]\u2212 3gt \u03b3 st \u2212 2gt \u03b3t2 E[regret(t)|Ft\u22121] \u2264 E[\u2206t|Ft\u22121] \u2264 3gt \u03b3 st \u2212 2gt \u03b3t2 (Definition of regret(t) and using lemma 6)\nE[Yt \u2212 Yt\u22121|Ft\u22121] \u2264 0\nHence, Yt is a super-martingale process. We now state and use the Azuma-Hoeffding inequality for Yt\n(55)\nInequality 1 (Azuma-Hoeffding). If a super-martingale Yt (with t \u2265 0) and its the corresponding filtration Ft\u22121, satisfies |Yt \u2212 Yt\u22121| \u2264 ct for some constant ct, for all t = 1, . . . T , then for any a \u2265 0,\nPr(YT \u2212 Y0 \u2265 a) \u2264 exp ( \u2212a2\n2 \u2211T t=1 c 2 t\n) (56)\nWe define Y0 = 0. Note that |Yt \u2212 Yt\u22121| = |Zl| is bounded by 1 + 3gl\u03b3 \u2212 2gl \u03b3l2 . Hence, ct = 6gt \u03b3 . Setting a = \u221a 2 ln(2/\u03b4) \u2211T t=1 c 2 t in the above inequality, we obtain that with probability 1\u2212 \u03b42 ,\nYT \u2264 \u221a\u221a\u221a\u221a2 T\u2211 t=1 36g2t \u03b32 ln(2/\u03b4) T\u2211 t=1 ( regret(t)\u2212 3gt \u03b3 st \u2212 2gt \u03b3t2 ) \u2264 \u221a\u221a\u221a\u221a2 T\u2211 t=1 36g2t \u03b32 ln(2/\u03b4) (57) T\u2211 t=1 regret(t) \u2264 T\u2211 t=1 3gt \u03b3 st + T\u2211 t=1 2gt \u03b3t2 + \u221a\u221a\u221a\u221a2 T\u2211 t=1 36g2t \u03b32 ln(2/\u03b4) (58)\nLemma 9.\nT\u2211 t=1 st \u2264 \u221a dnT\n\u221a C log ( Tr(L\u22121)\nn\n) + log ( 3 + T\n\u03bbdn\u03c32\n) (59)\nProof.\nFollowing the proof in [11, 43],\ndet [\u03a3t] \u2265 det [ \u03a3t\u22121 +\n1 \u03c32 \u03c6jt\u03c6 T jt ] = det [ \u03a3 1 2 t\u22121 ( I + 1\n\u03c32 \u03a3\u2212 1 2 t\u22121\u03c6jt\u03c6 T jt \u03a3\u2212 1 2 t\u22121\n) \u03a3 1 2 t\u22121 ] = det [\u03a3t\u22121] det [ I + 1\n\u03c32 \u03a3\u2212 1 2 t\u22121\u03c6jt\u03c6 T jt \u03a3\u2212 1 2 t\u22121 ] det [\u03a3t] = det [\u03a3t\u22121] ( 1 + 1\n\u03c32 \u03c6Tjt \u03a3\u22121t\u22121\u03c6jt\n) = det [\u03a3t\u22121] ( 1 + s 2 t\n\u03c32 ) log (det [\u03a3t]) \u2265 log (det [\u03a3t\u22121]) + log ( 1 + s 2 t\n\u03c32 ) log (det [\u03a3T ]) \u2265 log (det [\u03a30]) +\nT\u2211 t=1 log ( 1 + s 2 t \u03c32 ) (60)\nIf A is an n\u00d7 n matrix, and B is an d\u00d7 d matrix, then det[A\u2297B] = det[A]d det[B]n. Hence,\ndet[\u03a30] = det[\u03bbL\u2297 Id] = det[\u03bbL]d\ndet[\u03a30] = [\u03bbn det(L)]d = \u03bbdn[det(L)]d\nlog (det[\u03a30]) = dn log (\u03bb) + d log (det[L]) (61)\nFrom Equations 60 and 61,\nlog (det [\u03a3T ]) \u2265 (dn log (\u03bb) + d log (det[L])) + T\u2211 t=1 log ( 1 + s 2 t \u03c32 ) (62)\nWe now bound the trace of Tr(\u03a3T+1).\nTr(\u03a3t+1) = Tr(\u03a3t) + 1 \u03c32 \u03c6jt\u03c6 T jt =\u21d2 Tr(\u03a3t+1) \u2264 Tr(\u03a3t) + 1 \u03c32\n(Since ||\u03c6jt || \u2264 1)\nTr(\u03a3T ) \u2264 Tr(\u03a30) + T \u03c32\nSince Tr(A\u2297B) = Tr(A) \u00b7 Tr(B)\nTr(\u03a3T ) \u2264 Tr (\u03bb(L\u2297 Id)) + T\n\u03c32 =\u21d2 Tr(\u03a3T ) \u2264 \u03bbdTr(L) +\nT \u03c32 (63)\nUsing the determinant-trace inequality, we have the following relation:( 1 dn Tr(\u03a3T ) )dn \u2265 (det[\u03a3T ])\ndn log (\n1 dn\nTr(\u03a3T ) ) \u2265 log (det[\u03a3T ]) (64)\nUsing Equations 62, 63 and 64, we obtain the following relation.\ndn log ( \u03bbdTr(L) + T\u03c32\ndn\n) \u2265 (dn log (\u03bb) + d log (det[L])) +\nT\u2211 t=1 log ( 1 + s 2 t \u03c32 )\nT\u2211 t=1 log ( 1 + s 2 t \u03c32 ) \u2264 dn log ( \u03bbdTr(L) + T\u03c32 dn ) \u2212 dn log (\u03bb)\u2212 d log (det[L])\n\u2264 dn log ( \u03bbdTr(L) + T\u03c32\ndn\n) \u2212 dn log (\u03bb) + d log ( det[L\u22121] ) (det[L\u22121] = 1/det[L])\n\u2264 dn log ( \u03bbdTr(L) + T\u03c32\ndn\n) \u2212 dn log (\u03bb) + dn log ( 1 n Tr(L\u22121) )\n(Using the determinant-trace inequality for log(det[L\u22121])) \u2264 dn log ( \u03bbdTr(L) Tr(L\u22121) + Tr(L \u22121)T \u03c32\n\u03bbdn2\n) (log(a) + log(b) = log(ab))\n\u2264 dn log (\nTr(L) Tr(L\u22121) n2 + Tr(L \u22121)T \u03bbdn2\u03c32 ) The maximum eigenvalue of any Laplacian is 2. Hence Tr(L) is upper-bounded by 3n.\nT\u2211 t=1 log ( 1 + s 2 t \u03c32 ) \u2264 dn log ( 3 Tr(L\u22121) n + Tr(L \u22121)T \u03bbdn2\u03c32 ) (65)\n(66)\ns2t = \u03c6 T j \u03a3 \u22121 t \u03c6j \u2264 \u03c6 T j \u03a3 \u22121 0 \u03c6j (Since we are making positive definite updates at each round t)\n\u2264 \u2016\u03c6j\u2016 2\u03bdmax(\u03a3\u221210 ) = \u2016\u03c6j\u2016 2 1 \u03bdmin(\u03bbL\u2297 Id) = \u2016\u03c6j\u2016 2 1 \u03bdmin(\u03bbL)\n(\u03bdmin(A\u2297B) = \u03bdmin(A)\u03bdmin(B))\n\u2264 1 \u03bb \u00b7 1 \u03bdmin(L)\n(||\u03c6j ||2 \u2264 1)\ns2t \u2264 1 \u03bb\n(Minimum eigenvalue of a normalized Laplacian LG is 0. L = LG + In)\nMoreover, for all y \u2208 [0, 1/\u03bb], we have log ( 1 + y\u03c32 ) \u2265 \u03bb log ( 1 + 1\u03bb\u03c32 ) y based on the concavity of log(\u00b7). To see this, consider the following function:\nh(y) = log ( 1 + y\u03c32 ) \u03bb log ( 1 + 1\u03bb\u03c32\n) \u2212 y (67) Clearly, h(y) is concave. Also note that, h(0) = h(1/\u03bb) = 0. Hence for all y \u2208 [0, 1/\u03bb], the function h(y) \u2265 0. This implies that log ( 1 + y\u03c32 ) \u2265 \u03bb log ( 1 + 1\u03bb\u03c32 ) y. We use this result by setting y = s2t .\nlog ( 1 + s 2 t\n\u03c32\n) \u2265 \u03bb log ( 1 + 1\n\u03bb\u03c32\n) s2t\ns2t \u2264 1 \u03bb log ( 1 + 1\u03bb\u03c32 ) log(1 + s2t \u03c32 ) (68)\nHence,\nT\u2211 t=1 s2t \u2264 1 \u03bb log ( 1 + 1\u03bb\u03c32 ) T\u2211 t=1 log ( 1 + s 2 t \u03c32 ) (69)\nBy Cauchy Schwartz,\nT\u2211 t=1 st \u2264 \u221a T \u221a\u221a\u221a\u221a T\u2211 t=1 s2t (70)\nFrom Equations 69 and 70,\nT\u2211 t=1 st \u2264 \u221a T \u221a\u221a\u221a\u221a 1 \u03bb log ( 1 + 1\u03bb\u03c32 ) T\u2211 t=1 log ( 1 + s 2 t \u03c32 ) T\u2211 t=1 st \u2264 \u221a T \u221a\u221a\u221a\u221aC T\u2211 t=1 log ( 1 + s 2 t \u03c32 ) (71)\nwhere C = 1 \u03bb log(1+ 1 \u03bb\u03c32 ) . Using Equations 65 and 71,\nT\u2211 t=1 st \u2264 \u221a dnT\n\u221a C log ( 3 Tr(L\u22121)\nn + Tr(L \u22121)T \u03bbdn2\u03c32 ) T\u2211 t=1 st \u2264 \u221a dnT \u221a C log ( Tr(L\u22121) n ) + log ( 3 + T \u03bbdn\u03c32 ) (72)"}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "arXiv preprint arXiv:1209.3352,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Nonstochastic multi-armed bandits with graph-structured feedback", "author": ["Noga Alon", "Nicolo Cesa-Bianchi", "Claudio Gentile", "Shie Mannor", "Yishay Mansour", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1409.8428,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Leveraging side observations in stochastic bandits", "author": ["St\u00e9phane Caron", "Branislav Kveton", "Marc Lelarge", "Smriti Bhagat"], "venue": "In Proceedings of the Twenty- Eighth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "A gang of bandits", "author": ["Nicolo Cesa-Bianchi", "Claudio Gentile", "Giovanni Zappella"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "An empirical evaluation of thompson sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Contextual bandits with linear payoff functions", "author": ["Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E Schapire"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In 21st Annual Conference on Learning Theory - COLT", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Algorithm 849: A concise sparse cholesky factorization package", "author": ["Timothy A Davis"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Socially enabled preference learning from implicit feedback data", "author": ["Julien Delporte", "Alexandros Karatzoglou", "Tomasz Matuszczyk", "St\u00e9phane Canu"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Regularized multi\u2013task learning", "author": ["Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Algebraic connectivity of graphs", "author": ["Miroslav Fiedler"], "venue": "Czechoslovak mathematical journal,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1973}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Online clustering of bandits", "author": ["Claudio Gentile", "Shuai Li", "Giovanni Zappella"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Multi-task sparse structure learning", "author": ["Andre R Goncalves", "Puja Das", "Soumyadeep Chatterjee", "Vidyashankar Sivakumar", "Fernando J Von Zuben", "Arindam Banerjee"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Multi-label structure learning with ising model selection", "author": ["Andr\u00e9 R Gon\u00e7alves", "Fernando J Von Zuben", "Arindam Banerjee"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Methods of conjugate gradients for solving linear systems, volume", "author": ["Magnus Rudolph Hestenes", "Eduard Stiefel"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1952}, {"title": "Sparse inverse covariance matrix estimation using quadratic approximation", "author": ["Cho-Jui Hsieh", "Inderjit S Dhillon", "Pradeep K Ravikumar", "M\u00e1ty\u00e1s A Sustik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Big & quic: Sparse inverse covariance estimation for a million variables", "author": ["Cho-Jui Hsieh", "M\u00e1ty\u00e1s A Sustik", "Inderjit S Dhillon", "Pradeep K Ravikumar", "Russell Poldrack"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Spectral thompson sampling", "author": ["Tom\u00e1\u0161 Koc\u00e1k", "Michal Valko", "R\u00e9mi Munos", "Shipra Agrawal"], "venue": "Horde of Bandits using Gaussian Markov Random Fields Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Distributed clustering of linear bandits in peer to peer networks", "author": ["Nathan Korda", "Bal\u00e1zs Sz\u00f6r\u00e9nyi", "Shuai Li"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Approximate gaussian elimination for laplacians-fast, sparse, and simple", "author": ["Rasmus Kyng", "Sushant Sachdeva"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1985}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Kronecker graphs: An approach to modeling networks", "author": ["Jure Leskovec", "Deepayan Chakrabarti", "Jon Kleinberg", "Christos Faloutsos", "Zoubin Ghahramani"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire"], "venue": "In Proceedings of the 19th international conference on World wide web,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Collaborative filtering bandits", "author": ["Shuai Li", "Alexandros Karatzoglou", "Claudio Gentile"], "venue": "In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Recommender systems with social regularization", "author": ["Hao Ma", "Dengyong Zhou", "Chao Liu", "Michael R Lyu", "Irwin King"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Latent bandits", "author": ["Odalric-Ambrym Maillard", "Shie Mannor"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "From bandits to experts: On the value of side-observations", "author": ["Shie Mannor", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "The rademacher complexity of linear transformation classes. In Learning Theory, pages 65\u201378", "author": ["Andreas Maurer"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Birds of a feather: Homophily in social networks", "author": ["Miller McPherson", "Lynn Smith-Lovin", "James M Cook"], "venue": "Annual review of sociology,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Dynamic clustering of contextual multi-armed bandits", "author": ["Trong T Nguyen", "Hady W Lauw"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Gaussian sampling by local perturbations", "author": ["George Papandreou", "Alan L Yuille"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Collaborative filtering with graph information: Consistency and scalable methods", "author": ["Nikhil Rao", "Hsiang-Fu Yu", "Pradeep K Ravikumar", "Inderjit S Dhillon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Gaussian Markov random fields: theory and applications", "author": ["Havard Rue", "Leonhard Held"], "venue": "CRC Press,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Online learning of multiple tasks and their relationships", "author": ["Avishek Saha", "Piyush Rai", "Suresh Venkatasubramanian", "Hal Daume"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "A survey of collaborative filtering techniques", "author": ["Xiaoyuan Su", "Taghi M Khoshgoftaar"], "venue": "Advances in artificial intelligence,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Spectral bandits for smooth graph functions", "author": ["Michal Valko", "R\u00e9mi Munos", "Branislav Kveton", "Tom\u00e1\u0161 Koc\u00e1k"], "venue": "In 31th International Conference on Machine Learning,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "The gang of bandits (GOB) model [7] is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph.", "startOffset": 32, "endOffset": 35}, {"referenceID": 39, "context": "The unavailability of rating data implies that we can not use traditional collaborative filtering based methods [41].", "startOffset": 112, "endOffset": 116}, {"referenceID": 27, "context": "Assuming each item can be described by its content (like tags describing a news article or video), the contextual bandits framework [29] offers a popular approach for addressing this exploration-exploitation trade-off.", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "One way to use a social network of users to improve recommendations is with the recent gang of bandits (GOB) model [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 33, "context": "In particular, the GOB model exploits the homophily effect [35] that suggests users with similar preferences are more likely to form links in a social network.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "Several recent works have tried to improve the scaling of the GOB model by clustering the users into groups [17, 36], but this limits the flexibility of the model and loses the ability to model individual users\u2019 preferences.", "startOffset": 108, "endOffset": 116}, {"referenceID": 34, "context": "Several recent works have tried to improve the scaling of the GOB model by clustering the users into groups [17, 36], but this limits the flexibility of the model and loses the ability to model individual users\u2019 preferences.", "startOffset": 108, "endOffset": 116}, {"referenceID": 35, "context": "In addition, we propose a Thompson sampling GOB variant that exploits the recent sampling-by-perturbation idea from the GMRF literature [37] to scale to even larger problems.", "startOffset": 136, "endOffset": 140}, {"referenceID": 29, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Other methods based on collaborative filtering followed [38, 13], but these works assume that we already have rating data available.", "startOffset": 56, "endOffset": 64}, {"referenceID": 11, "context": "Other methods based on collaborative filtering followed [38, 13], but these works assume that we already have rating data available.", "startOffset": 56, "endOffset": 64}, {"referenceID": 24, "context": "Bandits: The multi-armed bandit problem is a classic approach for trading off exploration and exploitation as we collect data [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 3, "context": "When features (context) for the \u201carms\u201d are available and changing, it is referred to as the contextual bandit problem [4, 29, 9].", "startOffset": 118, "endOffset": 128}, {"referenceID": 27, "context": "When features (context) for the \u201carms\u201d are available and changing, it is referred to as the contextual bandit problem [4, 29, 9].", "startOffset": 118, "endOffset": 128}, {"referenceID": 8, "context": "When features (context) for the \u201carms\u201d are available and changing, it is referred to as the contextual bandit problem [4, 29, 9].", "startOffset": 118, "endOffset": 128}, {"referenceID": 25, "context": "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].", "startOffset": 126, "endOffset": 132}, {"referenceID": 0, "context": "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].", "startOffset": 126, "endOffset": 132}, {"referenceID": 1, "context": "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].", "startOffset": 164, "endOffset": 167}, {"referenceID": 5, "context": "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.", "startOffset": 139, "endOffset": 153}, {"referenceID": 31, "context": "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.", "startOffset": 139, "endOffset": 153}, {"referenceID": 2, "context": "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.", "startOffset": 139, "endOffset": 153}, {"referenceID": 30, "context": "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.", "startOffset": 139, "endOffset": 153}, {"referenceID": 6, "context": "[7] is the first to exploit the network between users in the contextual bandit framework.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "To scale up the GOB model, several recent works propose to cluster the users and assume that users in the same cluster have the same preferences [17, 36].", "startOffset": 145, "endOffset": 153}, {"referenceID": 34, "context": "To scale up the GOB model, several recent works propose to cluster the users and assume that users in the same cluster have the same preferences [17, 36].", "startOffset": 145, "endOffset": 153}, {"referenceID": 28, "context": "Another interesting approach to relax the clustering assumption is to cluster both items and users [30], but this only applies if we have a fixed set of items.", "startOffset": 99, "endOffset": 103}, {"referenceID": 40, "context": "Some works consider item-item similarities to improve recommendations [42, 23], but this again requires a fixed set of items while we are interested in RS where the set of items may constantly be changing.", "startOffset": 70, "endOffset": 78}, {"referenceID": 21, "context": "Some works consider item-item similarities to improve recommendations [42, 23], but this again requires a fixed set of items while we are interested in RS where the set of items may constantly be changing.", "startOffset": 70, "endOffset": 78}, {"referenceID": 22, "context": "There has also been work on solving a single bandit problem in a distributed fashion [24], but this differs from our approach where we are solving an individual bandit problem on each of the n nodes.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "Also without loss of generality, we assume that the ratings are in the range [0, 1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 27, "context": "The true ratings can be given by a linear model [29], meaning that ri,j = (wi )xj + \u03b7i,j,t for some noise term \u03b7i,j,t.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "The noise \u03b7i,j,t is conditionally subGaussian [2][7] with zero mean and bounded variance, meaning that E[\u03b7i,j,t | Ct\u22121,Ht\u22121] = 0 and that there exists a \u03c3 > 0 such that for all \u03b3 \u2208 R, we have E[exp(\u03b3\u03b7i,j,t) | Ht\u22121,Ct\u22121] \u2264 exp( 2\u03c32 2 ).", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "The noise \u03b7i,j,t is conditionally subGaussian [2][7] with zero mean and bounded variance, meaning that E[\u03b7i,j,t | Ct\u22121,Ht\u22121] = 0 and that there exists a \u03c3 > 0 such that for all \u03b3 \u2208 R, we have E[exp(\u03b3\u03b7i,j,t) | Ht\u22121,Ct\u22121] \u2264 exp( 2\u03c32 2 ).", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "The GOB model [7] solves a contextual bandit problem for each user, where the mean vectors in the different problems are related according to the Laplacian L1 of the graph G.", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "Note that the same objective function has also been explored for graph-regularized multi-task learning [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "2 Connection to GMRFs Unfortunately, the approach of Cesa-Bianchi [7] for solving (2) has a computational complexity of O(d2n2).", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "We can view the approach in [7] as explicitly constructing the dense dn \u00d7 dn matrix \u03a3\u22121 t , leading to an O(d2n2) memory requirement.", "startOffset": 28, "endOffset": 31}, {"referenceID": 18, "context": "Since \u03a3t is positivedefinite, the linear system can be solved using conjugate gradient [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "However, the LINUCB-like algorithm in [7] needs to estimate the confidence intervals \u221a \u03c6i,j\u03a3 t \u03c6i,j for each available item j \u2208 Ct.", "startOffset": 38, "endOffset": 41}, {"referenceID": 25, "context": "We propose two approaches for mitigating this: first, in this section we adapt the epoch-greedy [27] algorithm to the GOB framework.", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "To achieve the optimal regret, we also propose a GOB variant of Thompson sampling [29].", "startOffset": 82, "endOffset": 86}, {"referenceID": 35, "context": "In this section we further exploit the connection to GMRFs to scale Thompson sampling to even larger problems by using the recent sampling-by-perturbation trick [37].", "startOffset": 161, "endOffset": 165}, {"referenceID": 25, "context": "1 Epoch-Greedy Epoch-greedy [27] is a variant of the popular -greedy algorithm that explicitly differentiates between exploration and exploitation rounds.", "startOffset": 28, "endOffset": 32}, {"referenceID": 25, "context": "The attainable regret is thus proportional to the generalization error for the class of hypothesis functions mapping the context vector to an expected rating [27].", "startOffset": 158, "endOffset": 162}, {"referenceID": 32, "context": "We characterize the generalization error in the GOB framework in terms of its Rademacher complexity [34], and use this to bound the expected regret leading to the result below.", "startOffset": 100, "endOffset": 104}, {"referenceID": 25, "context": "1 from [27] to our context:", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "We use [34] to bound the generalization error of our class of hypotheses in terms of its empirical Rademacher complexity R\u0302q (H).", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "Using Theorem 2 in [34] and Theorem 12 from [5], we obtain", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "Using Theorem 2 in [34] and Theorem 12 from [5], we obtain", "startOffset": 44, "endOffset": 47}, {"referenceID": 32, "context": "For a connected graph, we have the following upper-bound Tr(L \u22121) n \u2264 (1\u22121/n) \u03bd2 + 1 n [34].", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "The value \u03bd2 represents the algebraic connectivity of the graph [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 35, "context": "In order to implement Thompson sampling for large values of n, we adapt the recent sampling-by-perturbation approach [37] to our setting, and this allows us to sample from a Gaussian prior and then solve a linear system to sample from the posterior.", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "Since S tends to be sparse (using for example [12, 25]), this equation can be solved efficiently using conjugate gradient.", "startOffset": 46, "endOffset": 54}, {"referenceID": 23, "context": "Since S tends to be sparse (using for example [12, 25]), this equation can be solved efficiently using conjugate gradient.", "startOffset": 46, "endOffset": 54}, {"referenceID": 1, "context": "We obtain the result below by following a similar argument to Theorem 1 in [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "Given that the event E (t) holds with high probability, we follow an argument similar to Lemma 4 of [2] and obtain the following bound:", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "To bound the variance of the selected items, \u2211T t=1 st(jt), we extend the analysis in [11, 43] to include the prior covariance term.", "startOffset": 86, "endOffset": 94}, {"referenceID": 0, "context": "If L = Idn, we match the \u00d5(dn \u221a T ) regret bound for a dn-dimensional contextual bandit problem [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "Note that we have a dependence on d and n similar to the original GOB paper [7] and that this method performs similarly in practice in terms of regret.", "startOffset": 76, "endOffset": 79}, {"referenceID": 26, "context": "It is well known that such graphs capture many properties of real-world social networks [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "Similar to [7], we use the set of associated tags to construct the TF-IDF vector for each item and reduce the dimension of these vectors to d = 25.", "startOffset": 11, "endOffset": 14}, {"referenceID": 15, "context": "Similar to [17], all hyper-parameters are set using an initial validation set of 5 thousand rounds.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "To control the amount of exploration for Thompson sampling, we the use posterior reshaping trick [8] which reduces the variance of the posterior by a factor of 0.", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "Baselines: We consider two variants of graph-based UCB-style algorithms: GOBLIN is the method proposed in the original GOB paper [7] while we use GOBLIN++ to refer to a variant that exploits the fast mean estimation strategy we develop in Section 3.", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "Similar to [7], for both variants we discount the confidence bound term by a factor of \u03b1 = 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 27, "context": "We consider 3 variants of this baseline: the LINUCBIND proposed in [29], an epoch-greedy variant of this approach (EG-IND), and a Thompson sampling variant (TS-IND).", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Finally, we compared against the state-of-the-art online clusteringbased approach proposed in [17], denoted CLUB.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "Regret Minimization: We follow [17] in evaluating recommendation performance by plotting the ratio of cumulative regret incurred by the algorithm divided by the regret incurred by a random selection policy.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "[1] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Shipra Agrawal and Navin Goyal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Noga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Peter L Bartlett and Shahar Mendelson.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] St\u00e9phane Caron, Branislav Kveton, Marc Lelarge, and Smriti Bhagat.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Nicolo Cesa-Bianchi, Claudio Gentile, and Giovanni Zappella.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Olivier Chapelle and Lihong Li.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11] Varsha Dani, Thomas P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Timothy A Davis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Julien Delporte, Alexandros Karatzoglou, Tomasz Matuszczyk, and St\u00e9phane Canu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Theodoros Evgeniou and Massimiliano Pontil.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Miroslav Fiedler.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Jerome Friedman, Trevor Hastie, and Robert Tibshirani.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Claudio Gentile, Shuai Li, and Giovanni Zappella.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Andre R Goncalves, Puja Das, Soumyadeep Chatterjee, Vidyashankar Sivakumar, Fernando J Von Zuben, and Arindam Banerjee.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] Andr\u00e9 R Gon\u00e7alves, Fernando J Von Zuben, and Arindam Banerjee.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Magnus Rudolph Hestenes and Eduard Stiefel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Cho-Jui Hsieh, Inderjit S Dhillon, Pradeep K Ravikumar, and M\u00e1ty\u00e1s A Sustik.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Cho-Jui Hsieh, M\u00e1ty\u00e1s A Sustik, Inderjit S Dhillon, Pradeep K Ravikumar, and Russell Poldrack.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Tom\u00e1\u0161 Koc\u00e1k, Michal Valko, R\u00e9mi Munos, and Shipra Agrawal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Nathan Korda, Bal\u00e1zs Sz\u00f6r\u00e9nyi, and Shuai Li.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] Rasmus Kyng and Sushant Sachdeva.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] Tze Leung Lai and Herbert Robbins.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] John Langford and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Jure Leskovec, Deepayan Chakrabarti, Jon Kleinberg, Christos Faloutsos, and Zoubin Ghahramani.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Lihong Li, Wei Chu, John Langford, and Robert E Schapire.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] Odalric-Ambrym Maillard and Shie Mannor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] Shie Mannor and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] Andreas Maurer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] Miller McPherson, Lynn Smith-Lovin, and James M Cook.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36] Trong T Nguyen and Hady W Lauw.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37] George Papandreou and Alan L Yuille.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[38] Nikhil Rao, Hsiang-Fu Yu, Pradeep K Ravikumar, and Inderjit S Dhillon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[39] Havard Rue and Leonhard Held.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[40] Avishek Saha, Piyush Rai, Suresh Venkatasubramanian, and Hal Daume.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[41] Xiaoyuan Su and Taghi M Khoshgoftaar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[42] Michal Valko, R\u00e9mi Munos, Branislav Kveton, and Tom\u00e1\u0161 Koc\u00e1k.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40].", "startOffset": 126, "endOffset": 134}, {"referenceID": 16, "context": "Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40].", "startOffset": 126, "endOffset": 134}, {"referenceID": 38, "context": "Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40].", "startOffset": 180, "endOffset": 184}, {"referenceID": 37, "context": "Since zeroes in the inverse covariance matrix correspond to conditional independences between the corresponding nodes (users) [39], we use L1 regularization on Vt for encouraging sparsity in the inferred graph.", "startOffset": 126, "endOffset": 130}, {"referenceID": 38, "context": "Following [40], we choose \u2206 to be the log-determinant Bregman divergence given by \u2206(X||Y ) = Tr(XY \u22121)\u2212 log |XY \u22121| \u2212 dn.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "This problem can be written as a graphical lasso problem [16], minX Tr(SX) + \u03bb2||X||1 \u2212 log |X|, where the empirical covariance matrix S is equal to \u03bbWTt W t + V \u22121 t\u22121.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "We use the highly-scalable second order methods described in [21, 22] to solve (11).", "startOffset": 61, "endOffset": 69}, {"referenceID": 20, "context": "We use the highly-scalable second order methods described in [21, 22] to solve (11).", "startOffset": 61, "endOffset": 69}, {"referenceID": 38, "context": "Similar to [40], we start off with an empty graph and start learning the graph only after the preference vectors have become stable, which happens in this case after each user has received 10 recommendations.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "1 from [27]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "The generalization error for H can be bounded as follows: Lemma 3 (Theorem 1 from [34]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 32, "context": "The Rademacher complexity for a class of linear predictors with graph regularization for a 0/1 loss function `0,1 can be bounded using Theorem 2 of [34].", "startOffset": 148, "endOffset": 152}, {"referenceID": 4, "context": "Since we perform regression using a least squares loss function instead of classification, the Rademacher complexity in our case can be bounded using Theorem 12 from [5].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "Our argument closely follows the proof structure in [2], but is modified to include the prior covariance.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "To show that the event E(t) holds with high probability, we use the following lemma from [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Lemma 5 (Lemma 2 of [2]).", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "Lemma 6 (Lemma 4 in [2]).", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "To bound ||St\u22121||\u03a3\u22121 t\u22121 , we use Theorem 1 from [1] which we restate in our context.", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "Note that using this theorem with the prior covariance equal to Idn gives Lemma 8 of [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Theorem 2 (Theorem 1 of [1]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "If L = In, Tr(L) = Tr(L\u22121) = n, we recover the bound in [2] i.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "Following the proof in [11, 43],", "startOffset": 23, "endOffset": 31}], "year": 2017, "abstractText": "The gang of bandits (GOB) model [7] is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph. This model is useful in problems like recommender systems where the large number of users makes it vital to transfer information between users. Despite its effectiveness, the existing GOB model can only be applied to small problems due to its quadratic timedependence on the number of nodes. Existing solutions to combat the scalability issue require an often-unrealistic clustering assumption. By exploiting a connection to Gaussian Markov random fields (GMRFs), we show that the GOB model can be made to scale to much larger graphs without additional assumptions. In addition, we propose a Thompson sampling algorithm which uses the recent GMRF sampling-by-perturbation technique, allowing it to scale to even larger problems (leading to a \u201chorde\u201d of bandits). We give regret bounds and experimental results for GOB with Thompson sampling and epoch-greedy algorithms, indicating that these methods are as good as or significantly better than ignoring the graph or adopting a clustering-based approach. Finally, when an existing graph is not available, we propose a heuristic for learning it on the fly and show promising results.", "creator": "LaTeX with hyperref package"}}}