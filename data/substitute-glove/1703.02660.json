{"id": "1703.02660", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Towards Generalization and Simplicity in Continuous Control", "abstract": "This work highlights have strengthening from unusual polynomial way RBF parameterizations that may working to solve a includes on continuous allowing requires, example the OpenAI softball benchmarks. The performance of these engineer policies are strong with north of its society 2004, confidential with more elaborate parameterizations such instance would using neural devices. Furthermore, available training often testing scenarios typically fact taken be obviously example and cerebral next over - fitting, thus giving rise to only precise - oriented criticized. Training with a important initial control distribution called indicating to which less global efforts his certainly logical. This easier giving interactive direct cataclysmic also 's applications recovers from are on - for perturbations; time image which its revisions animation.", "histories": [["v1", "Wed, 8 Mar 2017 01:33:51 GMT  (1379kb,D)", "http://arxiv.org/abs/1703.02660v1", "Project page:this https URL"]], "COMMENTS": "Project page:this https URL", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO cs.SY", "authors": ["aravind rajeswaran", "kendall lowrey", "emanuel todorov", "sham kakade"], "accepted": true, "id": "1703.02660"}, "pdf": {"name": "1703.02660.pdf", "metadata": {"source": "META", "title": "Towards Generalization and Simplicity in Continuous Control", "authors": ["Aravind Rajeswaran", "Kendall Lowrey", "Emanuel Todorov", "Sham Kakade"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Deep learning has recently achieved impressive results on a number of hard problems, including sequential decision making in game domains (Mnih, 2015; Silver, 2016). This has motivated efforts to extend and adapt such techniques to the control of physical systems. Neural network controllers trained with Reinforcement Learning have recently produced rich motor behaviors on a number of simulated control tasks (Lillicrap et al., 2015; Schulman et al., 2016). The complexity of the systems being controlled is not yet at the level of what can be achieved through trajectory optimization in simulation (Tassa et al., 2012; Mordatch et al., 2012; Al Borno et al., 2013) or with hand-crafted controllers on physical robots (e.g. Boston Dynamics). However, RL approaches are exciting since in principle they are generic, model-free, and highly automatic.\nBrockman et al. (2016) released a suite of physics models and associated control tasks implemented in the MuJoCo simulator (Todorov et al., 2012). This has provided com-\n\u2217 Equal contributions. All authors are with the University of Washington, Department of Computer Science & Engineering. This work was supported by the US National Science Foundation. Correspond to: <aravraj> at <cs.washington.edu>\n1Project site: https://sites.google.com/view/simple-pol\nmon benchmarks which have accelerated research and enabled objective comparisons. Duan et al. (2016) surveyed various algorithms and studied their performance on these benchmark tasks. The best performing algorithms seem to draw upon the benefits of the Natural Gradient (Kakade, 2001) and expressive power of neural networks.\nWhile the training algorithms and resulting performance have been well studied, a clear understanding of design choices like policy architecture is lacking. Recent research effort in other areas of AI such as computer vision have been devoted to finding tailored architectures for the respective problems (He et al., 2016). However, for control tasks, recent works have focused almost exclusively on small fully connected neural networks and occasionally recurrent networks (see Duan et al. (2016) for a survey). As a first step towards identifying tailored architectures for control, this work considers simple linear and RBF policies.\nThis work empirically explores the issue of architecture choice for benchmark control problems, and finds that the research area is much more open than the recent literature might suggest. In particular, the growing sentiment in the community that (i) significant improvements on common benchmark control tasks are due to the use of neural network policies; and (ii) the existing benchmarks provide a sufficient basis for future research, appears to be inaccurate. This work shows that simpler architectures \u2013 linear and RBF policies \u2013 are competitive with state of the art results on continuous control tasks. The work also shows that existing task designs are insufficient to test and understand generalization capabilities of trained policies. For example, always initializing a walking agent upright does not enable the agent to learn how to get up if it falls down, since it overfits to specific regions of the state space. Combined, these observations suggest that trajectory centric policies are sufficient to succeed as per the gym-v1 task definitions, and that simple representations have adequate capacity for such policies. In order to promote generalization and training of global policies for the walking task, a more diverse initial state distribution can be used. In this way the same policy can learn to get up from the ground, locomote, as well as recover from a wide range of perturbations. The attached video illustrates simulations where the agent recovers from on-line perturbations. Interestingly, the RBF policy is able to learn a global policy to solve this modiar X iv :1 70 3. 02 66 0v 1\n[ cs\n.L G\n] 8\nM ar\n2 01\n7\nfied task as well. This suggests that expressive power of policies may not be the bottle neck for most of the gym-v1 continuous control tasks.\nThis work does not attempt to provide a definitive answer in terms of the ideal architecture choice for control. Rather, the results herein suggest that the current set of benchmark tasks are insufficient to provide insights to this question. In addition, this work suggests that a rethinking about the problem setting is required to understand robustness and generalization capabilities, as well as potential application in robotics. As a first step towards this goal, this work considers interactive variants of the tasks in the gym suite. Here, at test time, a user is allowed to interactively apply perturbations such as pushing or rotating the agent. The agent should be able to recover from these perturbations and succeed in its objective. We emphasize that these interactive tasks are not an ultimate test for generalization capability, but rather a step towards this goal.\nIdeally, a controlled scientific study would seek to isolate the challenges related to architecture, task design, and training method for separate study. In practice, this is not feasible as the results are coupled with the training methods (and there is little understanding of this interplay). For simplicity, this work utilizes a straightforward natural policy gradient method with normalized stepsize, which is closely related to the TRPO method (Schulman et al., 2015)."}, {"heading": "2. Problem Formulation", "text": "We consider Markov Decision Processes (MDPs) in the average reward setting, which is defined using the tuple: M = {S,A,R, T , \u03c10}. S \u2286 Rn, A \u2286 Rm, and R : S \u00d7A \u2192 R are (continuous) set of states, set of actions, and reward function respectively, and have the usual meaning. T : S \u00d7 A \u2192 S is the stochastic transition function and \u03c10 is the probability distribution over initial states. We wish to solve for a stochastic policy of the form\n\u03c0 : S \u00d7A \u2192 R, which optimizes the objective function:\n\u03b7(\u03c0) = lim T\u2192\u221e\n1 T E\u03c0,M [ T\u2211 t=1 rt ] . (1)\nSince we use simulations with finite length rollouts to estimate the objective and gradient, we approximate \u03b7(\u03c0) using a finite T . In this finite horizon rollout setting, we define the value, Q, and advantage functions as follows:\nV \u03c0(s, t) = E\u03c0,M [ T\u2211 t\u2032=t rt\u2032 ]\nQ\u03c0(s, a, t) = EM [ R(s, a) ] + Es\u2032\u223cT (s,a) [ V \u03c0(s\u2032, t+ 1) ] A\u03c0(s, a, t) = Q\u03c0(s, a, t)\u2212 V \u03c0(s, t)\nNote that even though the value functions are time-varying, we still optimize for a stationary policy. We consider parametrized policies \u03c0\u03b8, and hence wish to optimize for the parameters (\u03b8). Thus, we overload notation and use \u03b7(\u03c0) and \u03b7(\u03b8) interchangeably."}, {"heading": "3. Algorithm", "text": "Using the likelihood ratio approach and Markov property of the problem, the policy gradient is derived to be:\n\u02c6\u2207\u03b8\u03b7(\u03b8) = g = 1\nT T\u2211 t=0 \u2207\u03b8 log \u03c0\u03b8(at|st)A\u0302\u03c0(st, at, t) (2)\nGradient ascent using this \u201cvanilla\u201d gradient is sub-optimal since it is not the steepest ascent direction in the metric of the parameter space (Amari, 1998; Kakade, 2001). The steepest ascent direction is obtained by solving the following local optimization problem around iterate \u03b8k:\nmaximize \u03b8 gT (\u03b8 \u2212 \u03b8k) subject to (\u03b8 \u2212 \u03b8k)TF\u03b8k(\u03b8 \u2212 \u03b8k) \u2264 \u03b4, (3)\nwhere F\u03b8k is the Fisher Information Metric at the current iterate \u03b8k. We estimate this metric as\nF\u0302\u03b8k = 1\nT T\u2211 t=0 \u2207\u03b8 log \u03c0\u03b8(at|st)\u2207\u03b8 log \u03c0\u03b8(at|st)T . (4)\nThis yields the steepest ascent direction to be F\u0302\u22121\u03b8k g and corresponding update rule: \u03b8k+1 = \u03b8k + \u03b1F\u0302\u22121\u03b8k g. Here \u03b1 is the step-size or learning rate parameter. Empirically, we observed that choosing a fixed value for \u03b1 or an appropriate schedule is difficult. Thus, we use the normalized gradient ascent procedure, where the normalization is under the Fisher metric. This procedure can be viewed as picking a normalized step size \u03b4 as opposed to \u03b1, and solving the optimization problem in (3). This results in the following update rule:\n\u03b8k+1 = \u03b8k +\n\u221a \u03b4\ngT F\u0302\u22121\u03b8k g F\u0302\u22121\u03b8k g. (5)\nA dimensional analysis of these quantities reveal that \u03b1 has the unit of return\u22121 whereas \u03b4 is dimensionless. Though units of \u03b1 are consistent with a general optimization setting where step-size has units of objective\u22121, in these problems, picking a good \u03b1 that is consistent with the scales of the reward was difficult. On the other hand, a constant normalized step size was numerically more stable and easier to tune: for all the results reported in this paper, the same \u03b4 = 0.05 was used. Henceforth, we use the notation \u03b1n = \u221a \u03b4/gT F\u0302\u22121\u03b8k g. When more than one trajectory rollout is used per update, the above estimators can be used with an additional averaging over the trajectories.\nFor estimating the advantage function, we use the GAE procedure (Schulman et al., 2016). This requires learning a function that approximates V \u03c0k , which is used to compute A\u03c0k along trajectories for the update in (5). GAE helps with variance reduction at the cost of introducing bias, and requires tuning hyperparameters like a discount factor and an exponential averaging term. Good heuristics for these parameters have been suggested in prior work. The same batch of trajectories cannot be used for both fitting the value function baseline, as well as estimating g using (2), since it will lead to overfitting and a biased estimate. Thus, we use the trajectories from iteration k\u22121 to fit the value function, essentially approximating V \u03c0k\u22121, and use trajectories from iteration k for computing A\u03c0k and g. Similar procedures have been adopted in prior work (Duan et al., 2016).\nThe optimization procedure we use is closely related to TRPO, but with a few differences. In TRPO, the Fisher metric is estimated as F\u0302 TRPO\u03b8k = \u2207 2 \u03b8K\u0304L ( \u03c0(\u00b7; \u03b8k)||\u03c0(\u00b7; \u03b8) ) , which incurs finite sample errors. TRPO also avoids computing F\u0302\u22121\u03b8k fully by using a conjugate gradient procedure and Hessian-vector products. We also employ a similar\nAlgorithm 1 Policy Search with Natural Gradient 1: Initialize policy parameters to \u03b80 2: for k = 1 to K do 3: Collect trajectories {\u03c4 (1), . . . \u03c4 (N)} by rolling out\nthe stochastic policy \u03c0(\u00b7; \u03b8k). 4: Compute \u2207\u03b8 log \u03c0(at|st; \u03b8k) for each (s, a) pair along trajectories sampled in iteration k. 5: Compute advantages A\u03c0k based on trajectories in iteration k and approximate value function V \u03c0k\u22121. 6: Compute policy gradient according to (2). 7: Compute the Fisher matrix (4) and perform gradient ascent (5). 8: Update parameters of value function in order\nto approximate V \u03c0k (s (n) t ) \u2248 R(s (n) t ), where R(s (n) t ) is the empirical return computed as R(s (n) t ) = \u2211T t\u2032=t \u03b3 (t\u2032\u2212t)r (n) t . Here n indexes over\nthe trajectories. 9: end for\nprocedure for computing the Natural gradient. First, we recognize that F\u0302\u03b8k = 1 N \u2211N i=1\u2207\u03b8Li\u2207\u03b8LTi , where \u2207\u03b8Li is short hand for\u2207\u03b8 log \u03c0\u03b8(ai|si) for a sample (si, ai) collected in iteration k with policy \u03c0(\u00b7, \u03b8k). With this, efficient matrix-vector products (i.e. F\u0302\u03b8kv) can be computed as 1 N \u2211N i=1\u2207\u03b8Li ( \u2207\u03b8LTi v ) for a given vector v. This avoids explicit computation of F\u0302\u03b8k or F\u0302 \u22121 \u03b8k\n, which could be costly when optimizing many parameters. Due to the use of Conjugate Gradient for computing F\u0302\u22121\u03b8k g, this procedure is also sometimes called Truncated Natural Gradient. In addition, TRPO constrains the KL divergence between successive policy iterates. The constraint in (3) can be interpreted as an approximation to the KL-divergence. In practice, we found that the KL constraint is violated only a very small fraction of times during the training run, suggesting that with a good approximation of the Fisher metric, a hard bound on the KL divergence may not be required."}, {"heading": "3.1. Policy Architecture", "text": "We first consider a linear policy that directly maps from the observations to the motor torques. We use the same observations as specified in the Gym-v1 tasks which include joint positions, joint velocities, and for some tasks, information related to contacts. Thus, the policy mapping is at \u223c N (Wst + b, \u03c3), and the goal is to learn W , b, and \u03c3. For most of these tasks, the observations correspond to the state of the problem (in relative coordinates). Thus, we use the term states and observations interchangeably. In general, the policy is defined with observations as the input, and hence is trying to solve a POMDP.\nSecondly, we consider a more expressive policy based on Fourier features of the observations. Since these fea-\ntures approximate the RKHS features under an RBF Kernel (Rahimi & Recht, 2007), we call this policy parametrization the RBF policy. These features are constructed as:\ny (i) t = sin\n(\u2211 j Pijs (j) t\n\u03bd + \u03c6(i)\n) , (6)\nwhere each element Pij is drawn fromN (0, 1), \u03bd is a bandwidth parameter chosen approximately as the average pairwise distances between different observation vectors, and \u03c6 is a random phase shift drawn from U [\u2212\u03c0, \u03c0). Thus the policy is at \u223c N (Wyt+b, \u03c3), whereW , b, and \u03c3 are trainable parameters. This architecture can also be interpreted as a two layer neural network: the bottom layer with fixed random weights and sinusoidal activation and the top with learned weights and linear activation."}, {"heading": "4. Experiments and Results", "text": ""}, {"heading": "4.1. Gym-v1 tasks", "text": "We test the algorithm from Section 3 on the OpenAI gym-v1 benchmarks simulated in MuJoCo (Todorov et al., 2012). The set of tasks include the swimmer, hopper, halfcheetah, walker, and ant locomotion tasks. The experimental goal is to understand the performance level achievable with simple linear policies, and whether this performance can be further improved with the RBF policy.\nFigure 2 presents the learning curves for the Gym-v1 tasks along with the performance levels reported in prior work using TRPO and neural network policies. Table 1 also summarizes the final scores, where \u201cstoc\u201d refers to the stochastic policy with actions sampled as at \u223c \u03c0\u03b8(st), while \u201cmean\u201d refers to use of mean of the policy, with actions computed as at = E[\u03c0\u03b8(st)]. We see that the linear policy is competitive on most tasks, while the RBF policy can outperform previous results on four of the five considered tasks. Though we were able to train neural network policies that match the results reported in literature, we have used results reported in prior work for an objective comparison. Visualizations of the trained linear and RBF policies are presented in the supplementary video. Given the simplicity of these policies, it is surprising that they can produce such elaborate behaviors.\nTable 2 presents the number of samples needed for the policy performance to reach a threshold value for reward. The threshold value is computed as 90% of the final score achieved by the stochastic linear policy. We visually verified that policies with these scores are proficient at the task, and hence the chosen values correspond to meaningful performance thresholds. We see that linear and RBF policies are able to learn faster on three of the five tasks.\nThis work primarily considers comparison with neural network policies trained with TRPO, since it has demonstrated impressive results and is closest to the algorithm used in this work. We defer comparisons to policies trained with value function based methods or actor critic methods, such as DDPG (Lillicrap et al., 2015) for future work.\nAn interesting observation relates to the number of parameters and training time required for the different policy architectures. As an example, for the hopping task, the linear policy requires a total of 39 parameters to be optimized, in contrast to prior work which optimizes for 100x as many. In wall-clock times, it was possible to train policies (both linear and RBF) for all the tasks in under an hour, with the simpler tasks like swimmer and hopper requiring under 10 minutes. In contrast, more elaborate parameterizations require multiple hours of training.\nThe Gym-v1 tasks were designed in such a way that, in order to succeed, the policy needs to output correct actions only for a small subset of meaningful states. Thus, the representational capacity needed is minimal, which enables the linear policy to be competitive. In order to encourage learning of more global policies and to push the representational limits, we test the performance of these policy architectures on more challenging variants of these tasks in the next section. Finding global policies on these tasks with on-line model based optimization has been a challenging benchmark in literature due to under-actuation, high dimensionality, and presence of contacts (Erez et al., 2011)."}, {"heading": "4.2. Modified Tasks", "text": "To push the representational capabilities of the policy architectures, we modified the Gym-v1 tasks as follows:\n1. Removing termination conditions to allow agent access to potentially all parts of the state space.\n2. Wider initial state distribution to force generalization.\n3. Reward shaping and more realistic choice of physical properties (e.g. mass, friction) to achieve more natural looking behaviors such as walking gaits.\nCombined, these modifications require that the learned policies not only make progress towards maximizing the reward, but also recover from adverse conditions and resist perturbations. An example of this is illustrated in Figure 1, where the hopper model demonstrates a get-up sequence to continue forward movement.\nA short summary and high-level objective of the modified tasks are presented below and in Table 3:\n1. Swimmer and Ant: heading angle initialized randomly in [\u2212\u03c0/2, \u03c0/2] relative to the intended travel direction.\n2. Hopper and Walker: In the Gym-v1 tasks, the agent is always initialized upright and episode is terminated when the center of mass falls below a height threshold. We consider two variants: (a) no termination, but the agent is always initialized upright; (b) no termination, and the agent is initialized from a wider distribution, such as laying on the ground for half the episodes.\nTermination conditions help with reward shaping by penalizing sequences of actions that bring the agent to undesirable parts of the state space. From an MDP perspective, termination corresponds to transitioning to a pseudo absorbing state when the system moves outside the allowable state space. Removing termination allows the agent to both enter, and hopefully, leave the undesirable states, the goal being for additional robustness to unexpected consequences. To further encourage this goal, we induce exploration of unfavorable states by initializing our agents in these undesirable configurations. In order to achieve a high score, the agent must be able to learn and represent all the necessary skills, such as turning in case of swimmer, and getting up from the ground for hopper and walker.\nIn addition, termination conditions also interfere with the optimization procedure, especially if it is possible to enter the absorbing terminal state from the initial state through some actions. In such situations, two policies that produce similar actions (say, have small KL-divergence), can have very different evaluations. The noise in the policy could be driving the system away from these absorbing states, thereby adversely affecting the smoothness of the optimization landscape. This problem is further exacerbated when we consider the mean of the stochastic policy, which does not have the necessary noise to escape the absorbing states.\nWe first study the representational capacity of policies on the modified walker task in Figure 3. Increasing the Fourier features allows for more expressive policies and consequently allow for achieving a higher score. The policy with 500 RBF features out performs policies with only 300 and 100 features. The linear policy also makes forward progress but is unable to learn as efficient a walking gait.\nWe also test the robustness of our policies in these new environments by perturbing the system with an external force.\nThis external force represents an unforeseen change which the agent has to resist or overcome. In these tasks, perturbations are not applied to the system during the training phase. Thus, the ability to generalize and resist perturbations come entirely out of the states visited by the agent during training. Figure 4 shows that a more expressive policy is able to better perform in this setting, and also that diverse initializations are important to obtain the best results. For example, in the situation where the simulated robot is always initialized upright (but without any termination conditions), the agent learns to recover from perturbations that make it fall forward, but it cannot recover if it falls over backwards\u2013the agent is more likely to fall forward during training when learning to propel itself forward. Similarly, for the swimmer task, the policy is unable to generalize to various heading angles. This indicates that careful design of initial state distributions are crucial for generalization, and to enable the agent to learn a wide range of skills. Without diverse initializations and if termination conditions are used, the learned policy is effectively only trajectory centric, albeit with potentially larger coverage that those obtained using trajectory optimization methods like iLQG (Tassa et al., 2012)."}, {"heading": "5. Discussion and Future Work", "text": ""}, {"heading": "5.1. Sample Complexity and Optimization", "text": "Even though linear policies were able to produce surprisingly good results in Gym-v1 tasks, and perform well even in the modified tasks, they do not appear to be favorable in terms of sample complexity when optimized with policy gradient methods. As can be seen from the learning curves in Figure 2 as well as Figure 3, the linear policy is able to learn faster initially, but this benefit quickly diminishes after a few dozen iterations. Thus, over-parametrized representations appear to be beneficial from an optimization perspective, but the ability of these over-parametrized representations (neural networks, RBFs etc.) to generalize is yet to be understood completely, and is beyond the scope of this work. Understanding these issues likely requires a rethinking of the problem settings and more appropriate benchmark tasks. In addition, it would be interesting to see if simple policies can be optimized more efficiently using alternate direct policy search methods such as CMA-ES, and value function based methods.\nBased on the observation that linear and RBF policies work remarkably well on the Gym-v1 tasks, we chose these architectures for the interactive tasks we developed. A careful study of the performance with different architectures including fully connected and recurrent networks remains to be done on these tasks, and would make for interesting future work."}, {"heading": "5.2. Towards Harder Tasks", "text": "One explanation for the good performance of policy gradient methods, even with simplistic policies, is that the considered locomotion tasks are conducive to beneficial random exploration. In both the original Gym-v1 tasks, as well as the modified tasks, the agent is not required to find a sequence of controls with very delayed reward like in the case of tasks like Mountain Car or Montezuma\u2019s Revenge.\nWe were unable to train a successful policy with the linear or RBF architecture on the Humanoid-v1 task. This possibly indicates that the Humanoid-v1 task poses representational challenges and likely requires directed exploration. Manipulation tasks, while not considered in this work, also pose similar challenges."}, {"heading": "5.3. Robustness", "text": "The generalization and robustness we demonstrate in this work arise entirely out of the states the agent visits during training. No perturbations were applied during the training phase. Designing tasks with model ensembles (Rajeswaran et al., 2016) or perturbation forces during training could potentially further improve the robustness. We plan to pursue this in the near future."}, {"heading": "5.4. Interactive Control", "text": "Interactive control tasks have been suggested and successfully solved in prior work (Tassa et al., 2012; Mordatch et al., 2015). Even though the interactive tasks that we study are not as complex as those in Mordatch et al. (2015),\nto our knowledge, this is the first work that considers model-free approaches for continuous control tasks where a user can interact with the learned policy. As observed in prior work, interactive perturbations and ability to interactively set goals are an interesting way to study and characterize robustness and generalization as the human user can deliberately target policy weaknesses. Such approaches could also pave way for hierarchical planning, where the goals can be modulated by a higher level agent."}], "references": [{"title": "Trajectory Optimization for Full-Body Movements with Complex Contacts", "author": ["M. Al Borno", "M. de Lasa", "A. Hertzmann"], "venue": "IEEE Transactions on Visualization and Computer Graphics,", "citeRegEx": "Borno et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Borno et al\\.", "year": 2013}, {"title": "Natural gradient works efficiently in learning", "author": ["S. Amari"], "venue": "Neural Computation,", "citeRegEx": "Amari,? \\Q1998\\E", "shortCiteRegEx": "Amari", "year": 1998}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "In ICML,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Infinitehorizon model predictive control for periodic tasks with contacts", "author": ["Erez", "Tom", "Tassa", "Yuval", "Todorov", "Emanuel"], "venue": "In RSS,", "citeRegEx": "Erez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Erez et al\\.", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "A natural policy gradient", "author": ["S. Kakade"], "venue": "In NIPS,", "citeRegEx": "Kakade,? \\Q2001\\E", "shortCiteRegEx": "Kakade", "year": 2001}, {"title": "Continuous control with deep reinforcement learning", "author": ["T. Lillicrap", "J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": null, "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V Mnih"], "venue": "Nature, 518,", "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "Discovery of complex behaviors through contact-invariant optimization", "author": ["I. Mordatch", "E. Todorov", "Z. Popovic"], "venue": "ACM SIGGRAPH,", "citeRegEx": "Mordatch et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mordatch et al\\.", "year": 2012}, {"title": "Interactive Control of Diverse Complex Characters with Neural Networks", "author": ["I. Mordatch", "K. Lowrey", "G. Andrew", "Z. Popovic", "E. Todorov"], "venue": "In NIPS,", "citeRegEx": "Mordatch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mordatch et al\\.", "year": 2015}, {"title": "Random Features for Large-Scale Kernel Machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In NIPS,", "citeRegEx": "Rahimi and Recht,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "author": ["A. Rajeswaran", "S. Ghotra", "B. Ravindran", "S. Levine"], "venue": "ArXiv e-prints,", "citeRegEx": "Rajeswaran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajeswaran et al\\.", "year": 2016}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M. Jordan", "P. Abbeel"], "venue": "In ICML,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "In ICLR,", "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D Silver"], "venue": "search. Nature,", "citeRegEx": "Silver,? \\Q2016\\E", "shortCiteRegEx": "Silver", "year": 2016}, {"title": "Synthesis and stabilization of complex behaviors through online trajectory optimization", "author": ["Y. Tassa", "T. Erez", "E. Todorov"], "venue": "International Conference on Intelligent Robots and Systems,", "citeRegEx": "Tassa et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tassa et al\\.", "year": 2012}, {"title": "MuJoCo: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "In International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "Deep learning has recently achieved impressive results on a number of hard problems, including sequential decision making in game domains (Mnih, 2015; Silver, 2016).", "startOffset": 138, "endOffset": 164}, {"referenceID": 14, "context": "Deep learning has recently achieved impressive results on a number of hard problems, including sequential decision making in game domains (Mnih, 2015; Silver, 2016).", "startOffset": 138, "endOffset": 164}, {"referenceID": 6, "context": "Neural network controllers trained with Reinforcement Learning have recently produced rich motor behaviors on a number of simulated control tasks (Lillicrap et al., 2015; Schulman et al., 2016).", "startOffset": 146, "endOffset": 193}, {"referenceID": 13, "context": "Neural network controllers trained with Reinforcement Learning have recently produced rich motor behaviors on a number of simulated control tasks (Lillicrap et al., 2015; Schulman et al., 2016).", "startOffset": 146, "endOffset": 193}, {"referenceID": 15, "context": "The complexity of the systems being controlled is not yet at the level of what can be achieved through trajectory optimization in simulation (Tassa et al., 2012; Mordatch et al., 2012; Al Borno et al., 2013) or with hand-crafted controllers on physical robots (e.", "startOffset": 141, "endOffset": 207}, {"referenceID": 8, "context": "The complexity of the systems being controlled is not yet at the level of what can be achieved through trajectory optimization in simulation (Tassa et al., 2012; Mordatch et al., 2012; Al Borno et al., 2013) or with hand-crafted controllers on physical robots (e.", "startOffset": 141, "endOffset": 207}, {"referenceID": 16, "context": "(2016) released a suite of physics models and associated control tasks implemented in the MuJoCo simulator (Todorov et al., 2012).", "startOffset": 107, "endOffset": 129}, {"referenceID": 5, "context": "The best performing algorithms seem to draw upon the benefits of the Natural Gradient (Kakade, 2001) and expressive power of neural networks.", "startOffset": 86, "endOffset": 100}, {"referenceID": 2, "context": "Duan et al. (2016) surveyed various algorithms and studied their performance on these benchmark tasks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Recent research effort in other areas of AI such as computer vision have been devoted to finding tailored architectures for the respective problems (He et al., 2016).", "startOffset": 148, "endOffset": 165}, {"referenceID": 2, "context": "However, for control tasks, recent works have focused almost exclusively on small fully connected neural networks and occasionally recurrent networks (see Duan et al. (2016) for a survey).", "startOffset": 155, "endOffset": 174}, {"referenceID": 12, "context": "For simplicity, this work utilizes a straightforward natural policy gradient method with normalized stepsize, which is closely related to the TRPO method (Schulman et al., 2015).", "startOffset": 154, "endOffset": 177}, {"referenceID": 1, "context": "Gradient ascent using this \u201cvanilla\u201d gradient is sub-optimal since it is not the steepest ascent direction in the metric of the parameter space (Amari, 1998; Kakade, 2001).", "startOffset": 144, "endOffset": 171}, {"referenceID": 5, "context": "Gradient ascent using this \u201cvanilla\u201d gradient is sub-optimal since it is not the steepest ascent direction in the metric of the parameter space (Amari, 1998; Kakade, 2001).", "startOffset": 144, "endOffset": 171}, {"referenceID": 13, "context": "For estimating the advantage function, we use the GAE procedure (Schulman et al., 2016).", "startOffset": 64, "endOffset": 87}, {"referenceID": 2, "context": "Similar procedures have been adopted in prior work (Duan et al., 2016).", "startOffset": 51, "endOffset": 70}, {"referenceID": 16, "context": "We test the algorithm from Section 3 on the OpenAI gym-v1 benchmarks simulated in MuJoCo (Todorov et al., 2012).", "startOffset": 89, "endOffset": 111}, {"referenceID": 6, "context": "We defer comparisons to policies trained with value function based methods or actor critic methods, such as DDPG (Lillicrap et al., 2015) for future work.", "startOffset": 113, "endOffset": 137}, {"referenceID": 3, "context": "Finding global policies on these tasks with on-line model based optimization has been a challenging benchmark in literature due to under-actuation, high dimensionality, and presence of contacts (Erez et al., 2011).", "startOffset": 194, "endOffset": 213}, {"referenceID": 15, "context": "Without diverse initializations and if termination conditions are used, the learned policy is effectively only trajectory centric, albeit with potentially larger coverage that those obtained using trajectory optimization methods like iLQG (Tassa et al., 2012).", "startOffset": 239, "endOffset": 259}, {"referenceID": 11, "context": "Designing tasks with model ensembles (Rajeswaran et al., 2016) or perturbation forces during training could potentially further improve the robustness.", "startOffset": 37, "endOffset": 62}, {"referenceID": 15, "context": "Interactive control tasks have been suggested and successfully solved in prior work (Tassa et al., 2012; Mordatch et al., 2015).", "startOffset": 84, "endOffset": 127}, {"referenceID": 9, "context": "Interactive control tasks have been suggested and successfully solved in prior work (Tassa et al., 2012; Mordatch et al., 2015).", "startOffset": 84, "endOffset": 127}, {"referenceID": 8, "context": ", 2012; Mordatch et al., 2015). Even though the interactive tasks that we study are not as complex as those in Mordatch et al. (2015),", "startOffset": 8, "endOffset": 134}], "year": 2017, "abstractText": "This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of continuous control tasks, including the OpenAI gym benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, existing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory centric policies. Training with a diverse initial state distribution is shown to produce more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the video. 1", "creator": "LaTeX with hyperref package"}}}