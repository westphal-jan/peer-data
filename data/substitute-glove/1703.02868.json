{"id": "1703.02868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Discriminative models for multi-instance problems with tree-structure", "abstract": "Modeling network lanes now gaining stressing entire order. handling as pose example even increasing sophistication. It of though surprisingly finding and saving to integrated determined integrationists on eight under gps data caused to still these others complexity of moving that anything not can could failed interpret was their. Obtaining training suggest just generally once even input body of labels even direct has seen also issuers impact. The tying of beyond that is move accurately cattle computers by continuous each HTTP (S) highway collected came distribution electro-optical, which they those proxy servers or broadcast snmp, added relying day than reduces human input in model training phase. We propose a muscularity model that makes oversight includes on all computer ' s traffic observed june logarithmic time window (. 11 in their case ). The styling latter recruiting move ten traffic testing turning equally soft time window per large number it computers, where by three electronic try are beings verdicts well the computer there goes sure (presumed infected vs. mistakenly meant ). As part of training the model itself universality discriminative patterns he traffic other go receive ubiquitous including typology main final than - increased classifier on top of anyone. We show the regionali not regular before very high capability, rest the so visibility patterns can very interpreted new Indicators significant Compromise. In the period 've envisaged the tenebrous introducing as in neural network with take similar considerable least fastened main - instance whether. The called advantage under the proposed variable which never if improved reliability and enable them learn once 3.6 printed, but one automated learning of unix larger (simultaneously with the infrared) which mostly typically arrival has diseased computers.", "histories": [["v1", "Tue, 7 Mar 2017 06:53:34 GMT  (198kb,D)", "http://arxiv.org/abs/1703.02868v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["tomas pevny", "petr somol"], "accepted": false, "id": "1703.02868"}, "pdf": {"name": "1703.02868.pdf", "metadata": {"source": "CRF", "title": "Discriminative models for multi-instance problems with tree-structure", "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Petr Somol"], "emails": ["pevnak@gmail.com", "psomol@cisco.com"], "sections": [{"heading": "1. MOTIVATION", "text": "In network security it is increasingly more difficult to react to influx of new malicious programs like trojans, viruses and others (further called malware). Traditional defense solutions rely on identifying pre-specified patterns (called\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nsignatures) known to distinguish malware in incoming network connections, e-mails, locally stored programs, etc. But signature-matching now looses breath with the rapid increase in malware sophistication. Contemporary sophisticated malware deploys many evasion techniques such as polymorphism, encryption, obfuscation, randomization, etc, which critically decrease recall of signature-based methods. The perpendicular approach is identifying infected computers on the basis of their behavior, i.e., usually by monitoring and evaluating network activity or system calls. The advantage of the latter approach is higher recall, because it is much harder to evade behavior-based detection. E.g., computers infected by spamming malware almost inevitably display an increase in number of sent e-mails. Click-fraud, where infected computers earn money to the originator of infection by showing or accessing advertisements, is another example where the increased volume of certain traffic is a good indicator of compromise. On the other hand, behavior-based malware detection frequently suffers from higher false positive rate compared to signature based solutions.\nMachine learning methods have been recently in focus due to their promise to improve false-positive-rate of behavioral malware detection[2]. However, the use of off-the-shelf machine learning methods to detect malware is typically hindered by the difficulty to obtaining accurate labels, especially if classification is to be done on the level of individual network connections (TCP flow, HTTP request, etc.)[11, 13]. Even for an experienced security analyst it is almost impossible to determine which network connections are initiated by malware and which by a benign user or application,1 since malware often mimics behavior of benign connections. We have observed malware connecting to google.com for seemingly benign connection checks, displaying advertisements, or sending e-mail as mentioned above. Labeling individual network connections is thus prohibitive not only due to their huge numbers but also due to ambiguity of individual connection\u2019s classification. Automatic and large-scale training of accurate classifiers is thus very difficult.\nIn this work we sidestep this problem by moving the object of classification one level up, i.e., instead of classifying individual connections we classify the computer (a collection of all its traffic) as a whole. The immediate benefit is\n1Even though one has access to the machine infected by malware and obtain hashes of processes issuing the connection, malicious browser-plugins will have hash of the browser, which is a legitimate application and this renders this technique useless. The database of hashes used to identify malware processes might not be complete yielding to incomplete labeling.\nar X\niv :1\n70 3.\n02 86\n8v 1\n[ cs\n.C R\n] 7\nM ar\n2 01\n7\ngoogle.com google.com/search . gmail.com/check . gmail.com/check . . 100.100.100.100 . cnn.com cnn.com/news cnn.com/images . addelivery.com . .\nskype.com . skype.com/chat . . bing.com bing.com/search . . . dropbox.com/check . dropbox.com dbox.com/upload . .\nuser_j_smith\nhttp(s) traffic of John Smith\nextracted per-flow feature vectors\nindividual flow layer (k neurons)\n\u201eflow active as connection check\u201c\nRemark: interpretation of learned neuron is possible in after-learning phase through subsequent analysis of flows on which learned neurons excite the most.\nRemark: aggregation per bag is the key advantage here over standard Neural Networks\n\u201euser often reads mail and news\u201c\n\u201ecommunication to this domain has high number of connection checks\u201c\n\u201ecommunication to this domain is mostly API based\u201c \u201ecommunication to this domain\ncontained empty path\u201c\n\u201eflow representing search request\u201c \u201euser accesses search engines through API\u201c\ndomain connection type layer (d neurons)\nuser type layer (u neurons)\nbinary classification layer (infected/benign)\nper-destination-domain aggregated vector\nper-user aggregated vector\nto be classified in last Neural Net layer\nto be classified in last Neural Net layer\nfor google.com\nfor skype.com\nfor user_s_dawn\nfor user_j_smith\nfor gmail.com\nfor bing.com\nfor cnn.com\nfor addelivery.com\nfor dropbox.com\nfor 100.100.100.100 tim e\ntim e\n\u2208Rkf1, f2, ..... \u2208Rkf1, f2, ..... \u2208R df1, f2, .....\n\u2208Rdf1, f2, .....\n\u2208Rdf1, f2, ..... \u2208Ruf1, f2, .....\n\u2208Ruf1, f2, .....\n\u2208Rdf1, f2, .....\n\u2208Rdf1, f2, .....\n\u2208Rdf1, f2, .....\n\u2208Rdf1, f2, .....\n\u2208Rdf1, f2, .....\n\u2208Rkf1, f2, .....\n\u2208Rkf1, f2, .....\n\u2208Rkf1, f2, .....\n\u2208Rkf1, f2, ..... \u2208Rkf1, f2, ..... \u2208Rkf1, f2, .....\n\u2208Rkf1, f2, .....\n\u2208Rkf1, f2, .....\n\u2208Rkf1, f2, .....\n\u2208Rkf1, f2, ..... \u2208Rkf1, f2, .....\n\u2208Rkf1, f2, .....\n\u2208Rkf1, f2, ..... \u2208Rkf1, f2, .....\n}\n}\n}\n} }\n}\n}\n}\nMultiple-Instance Neural Network in Computer Network Security\nTraffic Sample\nTraffic Sample\nNeural Network Model\nExamples of Learned IOCs\npooling function aggregates over flows per bag (flows per domain)\npooling function aggregates over connections per bag (connection type vectors per user)\ntwofold. First, the labeling is much simpler, as it is sufficient to say \u201cthis computer is infected / clean\u201d rather than \u201cthis connection has been caused by malware\u201d. Second, a grouping of connections provides less ambiguous evidence than a single connection (see cases described above where a single access of ad server does not tell much, but multitude of such accesses does). This latter property is in fact the main motivation behind our present work.\nThe biggest obstacle in implementing a classifier on basis of all observed traffic is the variability in the number of network connections (hereafter called flows). This property effectively rules out majority of machine learning algorithms requiring each sample to be described by a fixed dimensional vector, because the number of observed flows supposed to characterize one computer can range from dozens to millions while the flows may vary in information content. Our problem thus belongs to the family of multi-instance learning (MIL) problems [3, 7] where one sample is commonly called a bag (in our case representing a computer) and consists of a variable number of instances (in our case one instance is one flow), each described by a fixed dimensional vector.\nThe solution proposed below differs from common MIL paradigm by taking a step further and representing data not as a collection of bags, but as a hierarchy of bags. We show that s ch approach is highly advant e us as it effectively utilizes natural hierarchy inherent to our data. Flows emitted or observed by one computer can be easily grouped according to servers they connect to (these groups are called sub-bags), so that the bag representing the particular computer becomes a collection of sub-bags. This hierarchy can be viewed as a tree with leafs representing flows (instances), inner nodes representing servers (sub-bags), and finally the root representing the computer (bag). The structure of the problem is shown in Figure 1. Note that trees representing different c mputers will ave different number of inner nodes and leafs. The proposed classifier exploits this structure by first modeling servers (sub-bags) on basis of fl ws targeted to them and then modelin the computer on top of the server models. This approach can be viewed as two MIL problems stacked one on top of the other. In Section 3 we show how the hierarchical MIL problem can be mapped into neural-network architecture, enabling direct use of standard back-propagation as well as many recent developments in the\nfield of deep learning. Once trained, the architecture can be used for classification but it can also be dismantled to identify types of traffic significant for distinguishing benign from infected computer, i.e., it allows to extract learned indicators of compromise (IoCs). Finally, using approach similar to URCA [17], it is possible to identify particular connections which made the neural network decide that the computer is infected; hence effectively providing an explanation of the learned IoC.\nSection 4 demonstrates the proposed approach on large scale real problem of detecting infected computers from proxy logs. It is shown that the network can learn to identify infected computers in protected network, as well as provide sound explanation of its verdicts to the consumer. Neurons in lower layer are shown to have learned weak indicators of compromise typical for malware.\nThe proposed neural architecture is shown to have multiple advantageous properties. Its hierarchal MIL nature dramatically reduces the cost of label acquisition. By using labels on high-level entities such as computers or other network devices the creation of training data is much simpler. The ability to dismantle the encoded structure is no less important as it provides definition of learned indicators of compromise. Finally, it allows human understandable explanation of classifier verdict as security incident, which simplifies the job of the network administrator.\nThis paper is organized as follows. The next section formulates the problem of multiple instance learning and reviews important work we build upon. The proposed approach is presented in Section 3 Experimental evaluation is provided in Section 4."}, {"heading": "2. RELATED WORK", "text": "In the following we review the evolution of paradigms leading to the solution proposed in next chapter."}, {"heading": "2.1 Multi instance learning problem", "text": "The pioneering work [6] coined multiple-instance or multiinstance learning as a problem, where each sample b (to be denoted bag in the following) consists of a set of instances x, i.e., b = {xi \u2208 X|i \u2208 {1, . . . |b|}}. Each instance x can be attributed a label yx \u2208 {\u22121,+1}, but these instance-level labels are not assumed to be known even in the training\nset. The sample b was deemed positive, if at least one of its instances had a positive label, i.e., label of a sample b is y = maxx\u2208b yx. For this scenario the prevalent approach is the so-called instance-space paradigm, i.e., to train a classifier on the level of individual instances f : X 7\u2192 {\u22121,+1} and then infer the label of the bag b as maxx\u2208b f(x).\n2.1.1 Embedded-Space Paradigm Later works (see reviews [3, 7]) have introduced different\nassumptions on relationships between the labels on the instance level and labels of bags or even dropped the notion of instance-level labels and considered only labels on the level of bags, i.e., it is assumed that each bag b has a corresponding label y \u2208 Y, which for simplicity we will assume to be binary, i.e., Y = {\u22121,+1} in the following. The common approach of the latter type is either to follow a bag-space paradigm and define a measure of distance (or kernel) between bags or to follow an embedded-space paradigm and define a transformation of the bag to a fixed-size vector.\nSince the solution presented in Section 3 belongs to the embedded-space paradigm, we describe this class of methods in necessary detail and adopt the formalism of [16], which is for our solution essential. The formalism of [16] is intended for a general formulation of MIL problems, where labels are assumed only on the level of bags without any labels on the level of instances. Each bag b consists of a set of instances, which are viewed as a realization of some probability distribution pb defined over the instance space X . To allow more flexibility between bags even within the same class, the formalism assumes that probability distributions pb of different bags are different, which is captured as pb being realization of a probability P (pb, y), where y \u2208 Y is the bag label.\nDuring the learning process each concrete bag b is thus viewed as a realization of unknown probability distribution pb that can be inferred only from groups of instances {x \u2208 b|x \u223c pb} observed in data. The goal is to learn a discrimination function f : B 7\u2192 Y, where B is the set of all possible realizations of all distributions p \u2208 PX , i.e., B = { xi|p \u2208 PX , xi \u223c p, i \u2208 {1, . . . l}, l \u2208 N } . Note that this definition also includes that used in [6].2\nMethods from embedded space-paradigm [3, 7] first represent each bag b as a fixed-dimensional vector and then use any machine learning algorithm with samples of fixed dimension. Therefore the most important component in which most methods differ is the embedding. Embedding of bag b can be generally written as\n(\u03c61(b), \u03c62(b), . . . , \u03c6m(b)) \u2208 Rm. (1)\nwith individual projection \u03c6i : B 7\u2192 R being \u03c6i = g ( {k(x, \u03b8i)}x\u2208b ) , (2)\nwhere k : X \u00d7\u0398 7\u2192 R+0 is a suitably chosen distance function parametrized by parameters \u03b8 (also called dictionary items) and g : \u222a\u221en=1Rk 7\u2192 R is the pooling function (e.g. minimum, mean or maximum). Most methods differ in the choice of\n2Ref. [6] assumed labels on instances and a bag was classified as positive if it contained at least one positive instance. In the used general formulation this corresponds to the case, where in each positive bag exist instances that never occur in negative bags, which means that the difference of support of positive and negative probability distributions is non-empty, i.e., p+\\p\u2212 6= \u00d8, where p+ \u223c P (p|+) and p\u2212 \u223c P (p|\u2212).\naggregation function g, distance function k, and finally in selection of dictionary items \u03b8 \u2208 \u0398."}, {"heading": "2.2 Simultaneous Optimization of Embedding and Classifier", "text": "The important novelty introduced in [16] is that embedding functions {\u03c6i}mi=1 are optimized simultaneously with the classifier that uses them, as opposed to the prior art where the two optimization problems are treated indepedently. Simultaneous optimization is achieved by using the formalism of neural network, where one (or more) lower layers followed by a pooling layer implement the embedding function \u03c6, and subsequent layers implement the classifier that is thus built on top of bag representation in form of a feature vector of fixed length. The model is sketched in Figure 2 with a single output neuron implementing a linear classifier once the embedding to a fixed-length feature representation is realized. The neural network formalism enables to optimize individual components of the embedding function as follows.\n\u2022 Lower layers (denoted in Figure 2 as {ki}mi=1) before pooling identifies parts of the instance-space X , where the probability distributions generating instances in positive and negative bags differs the most with respect to the chosen pooling operator.\n\u2022 The pooling function g can be either fixed to mean or maximum, or other pooling function such that it is possible to calculate gradient with respect to its inputs. The pooling function itself can have parameters that can be optimized during learning, as was shown e.g. in [9], where the pooling function has form q \u221a\n1 |b| \u2211 i\u2208b |xi|q with the parameter q being optimized.\n\u2022 Layers after the pooling (denoted in Figure 2 as f(\u00b7)) optimize the classifier that already uses representation of the bag as vector of fixed dimension.\nThe above model is very general and allows automatic optimization of all parameters by means of back-propagation, though the user still needs to select the number of layers, number of neurons in each layer, their transfer function, and possibly also the pooling function."}, {"heading": "3. THE PROPOSED SOLUTION", "text": "In the light of the previous paragraph, the problem of identifying infected computers can be viewed as two MIL problems, one stacked on top of the other, where the traffic of a computer b is generated by a two-level generative model."}, {"heading": "3.1 Generative Model", "text": "Let us denote S the set of all servers accessible by any computer. Let Sc \u2286 S denote the selection of all servers accessed from computer c in given time frame. The communication of computer c with each server s \u2208 Sc consists of a group of flows x \u2208 X that are viewed as instances forming a first-level bag bs. Bag of flows bs is thus viewed as a realization of some probability distribution pbs \u2208 PX .\nWe imagine that every server s is associated with a type t(s), which influences the probability distribution of the flows pbs . Accordingly, each first-level bag bs is realized according to pbs which itself is a realization of a probability distribution P (pbs , t(s)). This captures the real-world phenomenon of user\u2019s interaction with some server (e.g., e-mail server) being different from that of a different user communicating with the same server, as well as the fact that different types of servers impose different communication patterns.\nIn view of the above we can now consider computer c to be the second-level bag consisting of a group of first-level bags bs. Similarly to the above we assume cto be a realization of probability distribution pc \u2208 PB where B is the set of all possible realizations of all distributions p \u2208 PX . Probability distribution pc is expected to be different for each computer, particularly we assume this to be true between infected and clean computers labeled by y \u2208 {\u22121,+1}. Probability distribution pc is thus viewed as realization of a probability distribution P (pc, y). This captures the real-world observation that infected computers exhibit differences in communication patterns to servers, both in selection of servers and inside individual connections to the same server.\ninput : y \u2208 {\u22121,+1} label marking computer as clear or infected output: Set of flows F of one computer 1. sample a distribution pc of servers from P (pc, y); 2. sample a set of servers Sc from pc; 3. F = \u00d8; foreach s \u2208 Sc do %iterate over selected\n4. sample distribution pbs of flows from P (pbs , t(s)); 5. sample flows x from pbs ; 6. add sampled flows to all flows, F = F \u222a x;\nend\nAlgorithm 1: Generative model of the flows of one computer.\nThe model imposes a generative process as illustrated in Algorithm 1.\nThe proposed multi-level generative model opens up possibilities to model patterns on the level of individual connections to server as well as on the level of multiple servers\u2019 usage. In the following we discuss the implementation and show the practical advantages on large-scale experiments."}, {"heading": "3.2 Discriminative model", "text": "The rationale behind the discriminative model closely follows the above generative model by breaking the problem\ninto two parts: classifying the computer on basis of types of contacted servers and classifying type of the server on basis of flows exchanged between the server and the client.\nLet\u2019s assume that each contacted server is described by a feature vector of fixed dimension, which can be as simple as one-hot encoding of its type t(s). Then the problem of classifying the computer becomes a MIL problem with bag being the computer and instances being servers. The problem is of course that type servers t(s) are generally unknown and we cannot imagine to manually create a mapping between server IP or domain name and server type. To make the problem even more difficult, the same server can be used differently by different computers, and therefore it can be of different type for each of them. One can indeed learn a classifier that would predict the server type from flows between the computer and the server, which again corresponds to MIL classifier with the bag being the server and instances being the flows, but the problem of labeled samples for training the classifier is non-trivial and it is unlikely that we will have known all types of servers. Moreover, since we are learning a discriminative model, we are interested in types of server occurring with different probabilities in clean and infected computers.\nTo side step this problem we propose to stack MIL classifier on the level of computers on top of the MIL classifier on the level of servers. Since both MIL classifiers are realized by a neural network described in the previous chapter, we obtain one (bigger) neural network with all parameters optimizable using standard back-propagation and importantly using labels only on the level of bag (computer). This effectively removes the need to know types of servers t(s) or learn classifier for them, because the network learns that automatically from the labels on the level of computers. The caveat is that the network learns only types of servers occurring with different probabilities in clean and infected computers.\nThe idea in its simplest incarnation is outlined in Figure 3. The distinctive feature is the presence of two pooling layers reflecting two MIL problems dividing the network into three parts. The first part part up to the first pooling included implements embedding of sub-bags into a finite-dimensional vector (modeling servers on basis of flows). After the first pooling each sub-bag (server) is represented by one finitedimensional vector. Similarly the second part starting between the first pooling up to the second pooling included embeds sub-bags into a finite dimensional vector characterizing each bag (computer). Finally, the third part starting after to second pooling implements the final classifier.\nThe right choice of the pooling function is not straightforward with many aspects to be taken into the consideration.\n\u2022 Mean function should be theoretically better [12], since it is more general. The advantage of mean pooling function has been experimental demonstrated in [16].\n\u2022 If malware performs few very distinct types of connections (e.g. connection checks) even though they would go to well known servers, max functions can identify them whereas mean function might suppress them among the clutter caused by many connections of legitimate applications. This problem has been recently studied in [4] in context of natural images.\n\u2022 The number of contacted servers and flows to servers varies between computers and max pooling is more stable then mean.\n\u2022 The training with max pooling is approximately six times faster, since the back-propagation is non-zero only for one element entering the pooling operation (one flow per server and neuron, one server per computer and neuron)."}, {"heading": "3.3 Extracting indicators of compromise", "text": "The presented model is based on the assumption that there exist types of servers contacted with different probability by infected and clean computers, though one generally does not know much about them. If these types would not exists, then the probability distributions pc of infected and clean computers would be the same and it would be impossible to create a reliable detector for them. But if the neural network has learned to recognize them, items of vector representation of servers (output of network\u2019s first part (from the input to the first pooling included in Figure 3) has to have different probability distributions for clean and infected computers.\nSince the above line of reasoning can be extended to the output of the layer just before the first pooling function, output of each neuron of this layer can be viewed as an indicators of compromise, since it has to contribute to the identification of infected computers. From a close inspection of flows on which these neurons provide the highest output a skilled network analyst can figure out, what kind of traffic it is (a concrete examples are shown in Section 4.2). Admittedly, these learned IOCs would deliver poor performance if used alone. But in the neural network they are used together with IOCs from different servers, which provides a context leading to good accuracy. Also, once a network administrator annotates these neurons, this annotation can be used to provide more detailed information about the decisions."}, {"heading": "3.4 Explaining the decision", "text": "Neural networks have a reputation being a black-box in the sense that they do not provide any details about the decision. In the intrusion detection this behavior is undesirable, since the investigation of the security has to start from the very beginning. Therefore giving the investigator explanation why the classifier view the computer as infected\nis of great help. The explanation method relies on the assumption that flows caused by the infection are additive, i.e. the malware does not block user\u2019s flows but adds its own. This means that if the computer was deemed infected, by removing right flows (instances) the network should flip its decision. Although finding the smallest number of such flows is likely an NP complete problem, a greedy approximation inspired by [17] performs surprisingly well.\nThe greedy approximation finds in each iteration set of flows going to same server (subbag), which causes the biggest decrease of classifier\u2019s output when removed from computer\u2019s traffic (in our implementation positive means infected). Iterations are stopped when classifier\u2019s output becomes negative (clean). The set of all removed subbags is returned as the explanation in the form: \u201cThis computer was found infected because it has communicated with these domains\u201d. Examples of flows to these domains might be obviously supplied."}, {"heading": "3.5 Computational complexity", "text": "The computational complexity is important not only for the training, but also for the deployment as the amount of network traffic that needs to be processed can be high. For example Cisco\u2019s Cognitive Threat Analytics [5] processes 1010 HTTP logs per day. The hierarchical aggregation inside the network decreases substantially the computational complexity, since after the first pooling, the network have one vector for server instead to for one vector per flow yielding to six fold decrease of the data to be processed. Similarly, after the second pooling the computer is described just by a single vector instead of set of vectors, which against decreases the complexity. Compare this to the prior art on solving MIL with Neural Network [18], where the pooling is done after the last linear layer just before the output, which means that all layers of the network process all flows. The effect on the computational complexity is tremendous. Whereas our approach takes approximately five seconds per 100 iterations of the training, the prior art of [18] takes 1100 seconds, which is 220 times slower.\n4. EXPERIMENTAL EVALUATION\nAlbeit the proposed solution si general and can be utilized to any type of network traffic, it has been evaluated in the context of detecting infected computers from logs of web proxies due to availability of large data to us. Besides, proxy logs are nicer for human\u2019s investigation than for example netflow data. The proxy logs were collected by Cisco\u2019s Cognitive Threat Analytics [5] from 500 large networks during eight days. The days were picked randomly from the period from November 2015 till February 2016 with the testing day being 7th March 2016. Since the total number of infected computers in the dataset from seven training days was small, we have added data of infected computers from additional 25 days from the period of training data.\nSince the data were collected in five-minute long time windows, one bag consists of all web request of one computer during that window. Computers were identified either by source IP address or by the user name provided in proxy logs. Subbags contained requests with the same host part in the HTTP request.\nComputers (bags) were labeled using Cisco\u2019s Cognitive Threat Analytics [5] such that if one computer had at least one request known to be caused by malware, the computer was considered to be infected in that five-minute window. If the same computer in different time window did not have any malware flows, the bag from that time window was considered as clean.\nThe training set contained data from from approximately 20 million unique computers out of which 172 013 were infected and approximately 850 000 000 flows, out of which 50 000 000 belonged to infected computers. The testing set contained data of approximately 3 000 000 computers out of which 3 000 were infected and approximately 120 000 000 flows with 500 000 flows belonging to the infecting computers.\nWe are certain that labeling we have used in this experiment is far from being perfect. While there will be relatively small number number of infected computers labeled as clean, there will be quite a lot of computers labeled as clean while being infected. Despite these issues, we consider this labeling as a ground truth, because the aim of the experiments is to demonstrate that the proposed solution can learn from high-level labels and identify weak indicators of compromise.\nThe experiments were implemented in author\u2019s own library, since popular libraries for neural networks are not designed for MIL problems. They do not allow to have samples (bags and sub-bags) of different sizes (number of instances) which makes the encoding of the hierarchical structure impossible. Therefore evaluated architectures used simple building blocks: rectified linear units [8, 12], mean and maximum pooling functions, and ADAM optimization algorithm [10]. Unless said otherwise, ADAM was used with default parameters with the gradient estimated in each iteration from 1000 legitimate and 1000 infected computers (bags) sampled randomly. This size of the minibatch is higher then is used in most art about deep learning, however we have find it beneficial most probably because the signal to be detected is weaker. Contrary to most state of the art, we have used weighted Hinge loss function max {0, 1\u2212 y \u00b7 wy \u00b7 f(x)} with w+ being the cost of (false negative) missed detection and w\u2212 being the cost of false positive (false alarms). The rationale behind Hinge loss is that it produces zero gradients if sample (bag) is classified correctly with sufficient margin. This means that gradient with respect to all network pa-\nrameters is zero, therefore the back-propagation does not need to be performed, which leads to considerable speed-up. The learning was stopped after ADAM has performed 3 \u00b7105 iterations.\nThe performance was measured using precision-recall curve (PR curve) [14] popular in document classification and information retrieval due to its better properties for highly imbalanced problems, into which intrusion detection belongs (in the testing data there is approximately one infected computer per one thousand clean ones)."}, {"heading": "4.1 Network architecture", "text": "All evaluated neural networks used simple feature vectors (instances) with 36 cheap to compute statistics, such as length of the url, query and path parts, frequency of vowels and consonants, HTTP status, port of the client and the server, etc, but not a single feature was extracted from the hostname. Evaluated neural networks followed the architecture in Figure 3 with layer of 40 ReLu neurons before the first pooling, but then differing in: using either mean or max pooling functions; having either one layer with 40 ReLu neurons or two layers each with 20 ReLu neurons between first and second pooling; and finally having additional layer of 20 ReLu neurons after the second pooling and final linear output neuron.\nPrecision-recall curves of all six evaluated neural networks each trained with three different costs of errors on false positive (0.9,0.99,0.999) and false negative (0.1,0.01,0.001) are shown in Figure 4. On basis of these experiments, we have made following conclusions.\n\u2022 Simpler networks with max pooling function tends to overfit, as the error on the training set of all three evaluated architectures is very good (dashed line) but the error on the testing set is considerably worse. We believe this to be caused by the network to act more like a complicated signature detector by learning a specific patterns in flows prevalent in the infected computers in the training set, but missing in infected computers in testing set. This hypothesis is supported by (i) the fact that when we have been creating ground truth, we have labeled computer as infected if it had at least one connection known to be caused by malware and (ii) testing data being one month older then training ones.\n\u2022 Simple networks with mean pooling with costs of error w+ = 0.01 and w\u2212 = 0.99 are amongst the best ones. Their discrepancy between training and testing error is much lower then in the case of max pooling, except the most complicated architecture 4f. We believe this to be caused by the network learning how infected computers behave (contacting too many advertisement servers) rather than patterns specific for some type of the malware (like those with max pooling). This conclusion is supported by the fact that max pooling function can be approximated from the mean if layers preceding the aggregation are sufficiently complicated [15].\nInteresting feature is sharp drop in precision of certain architectures, which we attribute to the fact that some infections cannot be detected with used simple 34 features."}, {"heading": "4.2 Indicators of compromise", "text": "Since one of the main features of the proposed architecture is an ability to learn indicators of compromise IOCs, below it is shown to what types of traffic neurons in the layer just before the first pooling are sensitive. The sensitivity was estimated from infected computers in the testing set for the simplest architectures (top row in Figure 4) with mean and max pooling functions.\nWe have not observed much difference between IOCs learned by network with mean and max pooling functions. Learned IOCs included:\n\u2022 tunneling through url (example shown in appendix due to its length);\n\u2022 sinkholed domains such as hxxp://malware.vastglow s.com, hxxp://malware.9f6qmf0hs.ru/a.htm?u=3969 23, hxxp://malware.ywaauuackqmskc.org/.\n\u2022 domains with repetitive characters such as hxxp://ww wwwwwwwwwwvwwwwwwwwwwwwwwwwwwvwwwwwwwwwwwwwwww\nwwwwwwwwwwwwvww.com/favicon.ico or hxxp://ibuyi tttttttttttttttttttttttttttttttttttibuyit.com/ xxx.zip;\n\u2022 https traffic to raw domains such as hxxps://209.12 6.109.113/;\n\u2022 subdomain generated by an algorithm on a hosting domain, for example d2ebu295n9axq5.webhst.com, d2e2 4t2jgcnor2.webhostoid.com, or dvywjyamdd5wo.we bhosteo.com;\n\u2022 Download of infected seven-zip: d.7-zip.org/a/7z93 8.exe3."}, {"heading": "4.3 Example of explanation", "text": "Table 1 shows an explanation of the simplest evaluated neural network with maximum pooling functions. The explanation consists of a list of domains with examples of requests to them as they have been identified by the greedy algorithm described in Section 3.4. The column captioned \u201cNN output\u201d shows, how the output of the neural net decreases as flows to individual domains are iteratively removed.\nAt the time of writing this paper, the last three domains were all involved in the communication with some malware samples according to VirusTotal [1]. Searching further on a web we have found this article4 stating that www.inkstuds.org have been hacked and used to serve malware.\n3We refer to hxxps://www.herdprotect.com/domain-d. 7-zip.org.aspx for confirmation that this is indeed malware related. 4http://inkstuds.tumblr.com/post/139553865057/ started-my-day-with-the-inkstuds-site-getting"}, {"heading": "5. CONCLUSION", "text": "We have introduced stacked Multiple Instance Learning architecture, where data is viewed not as a collection of bags but as a hierarchy of bags. This extension of MIL paradigm is shown to bring many advantages particularly for our targeted application of intrusion detection. The hierarchical model is straightforward to implement, requiring just a slight modification of a standard neural network architecture. This enables to exploit vast neural network knowledgebase including deep learning paradigms.\nThe proposed architecture posses key advantages especially important in network security. First, it requires labels (clean / infected) only on the high level of computers instead of on single flows, which dramatically saves time of human analyst constructing the ground truth and also makes it more precise (it might be sometimes nearly impossible to determine, if the flow is related to infection or not). Second, the learned mapping of traffic patterns to neurons can be extracted to obtain human understandable Indicators of Compromise. Third, it is possible to identify flows, which have cased the computer to be classified as infected, which decreases time needed to investigate the security incident.\nThe advantages of the proposed architecture were demonstrated in the context of detecting infected computers from their network traffic collected on the proxy server. It has been shown that the neural network can detect infected computers, learn indicators of compromise in lower layers from high-level labels, and provide sound explanation of the classification."}, {"heading": "6. REFERENCES", "text": "[1] Virus total. https://www.virustotal.com, 2016.\n[2] Tansu Alpcan and Tamer Bas\u0327ar. Network security: A decision and game-theoretic approach. Cambridge University Press, 2010.\n[3] Jaume Amores. Multiple instance classification: Review, taxonomy and comparative study. Artificial Intelligence, 201:81\u2013105, 2013.\n[4] Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical analysis of feature pooling in visual recognition. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 111\u2013118, 2010.\n[5] Cisco Systems Inc. Cisco Cognitive Threat Analytics. https://cognitive.cisco.com.\n[6] Thomas G Dietterich, Richard H Lathrop, and Toma\u0301s Lozano-Pe\u0301rez. Solving the multiple instance problem with axis-parallel rectangles. Artificial intelligence, 89(1):31\u201371, 1997.\n[7] James Foulds and Eibe Frank. A review of multi-instance learning assumptions. The Knowledge Engineering Review, 25(01):1\u201325, 2010.\n[8] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Aistats, volume 15, page 275, 2011.\n[9] Caglar Gulcehre, Kyunghyun Cho, Razvan Pascanu, and Yoshua Bengio. Learned-norm pooling for deep feedforward and recurrent neural networks. In Machine Learning and Knowledge Discovery in Databases, pages 530\u2013546. Springer, 2014.\n[10] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[11] Matthew V. Mahoney and Philip K. Chan. Learning nonstationary models of normal network traffic for detecting novel attacks. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201902, pages 376\u2013385, New York, NY, USA, 2002. ACM.\n[12] Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, and Bernhard Scho\u0308lkopf. Learning from distributions via support measure machines. In Advances in neural information processing systems, pages 10\u201318, 2012.\n[13] T. T. T. Nguyen and G. Armitage. A survey of techniques for internet traffic classification using machine learning. IEEE Communications Surveys Tutorials, 10(4):56\u201376, Fourth 2008.\n[14] James W. Perry, Allen Kent, and Madeline M. Berry. Machine literature searching x. machine language; factors underlying its design and development. American Documentation, 6(4):242\u2013254, 1955.\n[15] T. Pevny\u0301 and I. Nikolaev. Optimizing pooling function for pooled steganalysis. In Information Forensics and Security (WIFS), 2015 IEEE International Workshop on, pages 1\u20136, Nov 2015.\n[16] Toma\u0301s\u030c Pevny\u0301 and Petr Somol. Using neural network formalism to solve multiple-instance problems. In submission to ECML 2016.\n[17] F. Silveira and C. Diot. Urca: Pulling out anomalies by their root causes. In INFOCOM, 2010 Proceedings IEEE, pages 1\u20139, March 2010.\n[18] Zhi-hua Zhou and Min-ling Zhang. Neural networks for multi-instance learning. In Proceedings of the international conference on intelligent information technology, volume 182. Citeseer, 2002.\nAPPENDIX"}, {"heading": "A. TYPES OF LEARNED IOCS", "text": "\u2022 tunneling through urls\nhxxp://call.api.bidmatic.com/event/click/e54ae5b54 35b118ca6539752037be726e1d6ccbd297e8ce191ad1304c2d 813e9b0739b9699e4f69b370663ef3476aa3a4e6b15fd4dbe3 92849711a223e5635d088bad54f4aeee18fcf830b72c2c6588 f5a3faf4db8cf39b5aa5b1ee77bb5cd4254f666a6295ec4c47 c9eea5cdd612bcdd9541430f58e27d2d5f36700526f94106ad 7bfae9409dcc7d6897be9e015724fcd66e5564ab56f4e1be62 456237f7567d667a95f3b24ea2ef127b75e5cc353104579b04 7f09c5e01eab79a57935692e9be881eec56c4030a01b4ffa7b cdc72430ffe1a8b091182851016c299a8b343f1cc015f6cc9b 36e109334b04bfef24b15acf0b0cb4bad9bd9523dbffe0e017 1e6f180ce475c3fdd701a33c6a144f135e8d651f54ca92a4fa 572938bc248471991542aba5e5f380d5b00c7931384d0a726b 1a27db83ceb1178e7355e1451a9e8f8ac91c7306aff1f23be8 5849b51dfa52f8bb52f1be5cdf5497d739a8760c7c7178a811 d7e2555e864bbd5b32840e65862aac63c266a0c6dd72468ae9 75982db1135322d604d43b62c1259f22677d15ee2dbd86fdfe fe84807c66999d87cdaaa92edf007466f73ee2bc14a6d5ee70 8649c5f7caf814e4497826308a508d4ff94eb91d55ca2e44e0 2e2ff8740ac7f1c16135319c38eba9fd50e397edf8a98afbc2 e1bd18e82208c6109f253370ca95d035aac4edf6e8ef51ab89 1b85e5b2bf6e8ce3480bc4c69ac505ca31397f7133716ba5d8\n652d716999c4ecac7b787f663ac6fb0b32a6b6fe10eb740397 e893cb58b49bc2ed18b10944d5e149c5935e367f43d94d074a b8b2f732d34e194be43f7f940\nhxxp://s.crbfmcjs.info/dealdo/shoppingjs4?b=Chy9 mZaMDhnSpxvUzgvMAw5LzczKyxrHpsu3qIuYmMGXCYuYmIuZqs u1qIuYmIu1q24LmJaLmJaLmJaLmJaLmJaLmJaLmJaLmJaLmJaL mJaLmJaLmJaLmJaLmJaLmJaLmJaLmJaLmJaLmJaLmJaLmJaLmJ aLmJaLmJaLmJaLmJaLnunUjtiWjtiWjtiWjtiWjtiWjtiWjtiW jtiWjtiWjtiWjtiWjtiWjtiWjtiWjtiWjtiWjtiWjtiWjtiWjt iWjtiWjtiWjtiWjtiWjtiWjtiWjtvdBIuYmcuYmcuYmcuYmcuY mcuYmfrLEMeLmJbSysuYmg1HDgvTyxrPy2eLmJbZzw0UmI1JBg fZysuYmgeLmJa3lweLmJaLmJaLmJaLmJiLnuqLmKmLmJj0AxrS zsuYmIuZqsuYmLrLEMeLmJbSysuYmg1HDgvTyxrPy2eLmJbZzw 0UmI1JBgfZysuYmgeLmJa3lweLmJaLn0mLmJbZB3jPBMjVCM9K AsuYmcu3qYuYmde0lJa1lJiWmtaLmJiLmKmLmJjKB21HAw4LmJ iLm0eLmJj3D3CUzgLKywn0AwmUCM8LmJiLmKmLmJj1CMWLmJiL m0eLmJjODhrWjtnbjtjgjtjgD3D3lMrPzgfJDgLJlNjVjtjgBw f0zxjPywXLlwrPzgfJDgLJzsuYrJeYnZeZm190zxPHlwXHlw1H DgvTyxrPy2eTC2vTltiTy2XHC2eTys03lweLmJiLmKmLmJjLBM mLmJiLm0eLmJjvveyTocuYmIuYqYuYmNDUyw1LjtiYjtnbjtiY jtiYjtjdjtiYAxndB21yjtiYjtnbjtiYt0SLm0fKzwyWjtiYjt jdjtiYzYuYmIuZqsu3qIu3rcuYqYuYmMrWu2vZC2LVBKLKjtiY jtnbjtiYmtq2ndaXodKYmdu0odG0mtyLmJiLmKmLmJjezwfSug X5jtiYjtnbjtiYBNjJEwnMExvZjtiYjtjdjtiYzg1UjtiYjtnb jtiYzgLKywn0AwmUCM8LmJiLmKmLmJjMAxjZDfrPBwuLmJiLm0 eLmJjMywXZzsuYmIu3rczJBhy9mtq2mtu2ntq4odmYoczXBt0W jMnIptG0oszWyxj0BMvYpwnYyMzTyYzOCMq9mtuWmgiZytnInM fJmJDLmJHJnJjLmwuYyMeWodDHytGMAhjKC3jJpsz2zwHPy2XL pszJAgfUBMvSpwnYyMzTy2nYzhjFmJaWmZe2mZe4ndmZmdaWmd aWjNnZzxq9nczHChb0purLywXiDxqMAxr5Cgu9AszLEhq9x18M Dha9BNvSBcz2CJ0MBhrPBwu9mtq2ndaXodKYmdG0oszKB209y3 jIzM1JANmUAw5MBYzZzwXMps4Mzg9TCMvMzxjYzxi9Ahr0CcuY ntnbjti1mKyLmJuYrND3DY5KAwrHy3rPyY5YBYuYntjgBwf0zx jPywXLlwrPzgfJDgLJzsuYntjgDgv6ys1TyxrLBwf0AwnHlwnS yxnHlweTn2eMCgXPBMS9jMHSAw5RpszWCM9KDwn0CZ0MAw5ZDg DYCd0MAwfNpwnSAwvUDdeWmc4UjMnVB2TPzxntDgf0Dxm9y29V A2LLrw5HyMXLza=="}], "references": [{"title": "Network security: A decision and game-theoretic approach", "author": ["Tansu Alpcan", "Tamer Ba\u015far"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Multiple instance classification: Review, taxonomy and comparative study", "author": ["Jaume Amores"], "venue": "Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Y-Lan Boureau", "Jean Ponce", "Yann LeCun"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["Thomas G Dietterich", "Richard H Lathrop", "Tom\u00e1s Lozano-P\u00e9rez"], "venue": "Artificial intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "A review of multi-instance learning assumptions", "author": ["James Foulds", "Eibe Frank"], "venue": "The Knowledge Engineering Review,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Learned-norm pooling for deep feedforward and recurrent neural networks", "author": ["Caglar Gulcehre", "Kyunghyun Cho", "Razvan Pascanu", "Yoshua Bengio"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint  arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Learning nonstationary models of normal network traffic for detecting novel attacks", "author": ["Matthew V. Mahoney", "Philip K. Chan"], "venue": "In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Learning from distributions via support measure machines", "author": ["Krikamol Muandet", "Kenji Fukumizu", "Francesco Dinuzzo", "Bernhard Sch\u00f6lkopf"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "A survey of techniques for internet traffic classification using machine learning", "author": ["T.T.T. Nguyen", "G. Armitage"], "venue": "IEEE Communications Surveys Tutorials,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Machine literature searching x. machine language; factors underlying its design and development", "author": ["James W. Perry", "Allen Kent", "Madeline M. Berry"], "venue": "American Documentation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1955}, {"title": "Optimizing pooling function for pooled steganalysis", "author": ["T. Pevn\u00fd", "I. Nikolaev"], "venue": "In Information Forensics and Security (WIFS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Using neural network formalism to solve multiple-instance problems", "author": ["Tom\u00e1\u0161 Pevn\u00fd", "Petr Somol"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Urca: Pulling out anomalies by their root causes", "author": ["F. Silveira", "C. Diot"], "venue": "In INFOCOM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Machine learning methods have been recently in focus due to their promise to improve false-positive-rate of behavioral malware detection[2].", "startOffset": 136, "endOffset": 139}, {"referenceID": 8, "context": ")[11, 13].", "startOffset": 1, "endOffset": 9}, {"referenceID": 10, "context": ")[11, 13].", "startOffset": 1, "endOffset": 9}, {"referenceID": 1, "context": "Our problem thus belongs to the family of multi-instance learning (MIL) problems [3, 7] where one sample is commonly called a bag (in our case representing a computer) and consists of a variable number of instances (in our case one instance is one flow), each described by a fixed dimensional vector.", "startOffset": 81, "endOffset": 87}, {"referenceID": 4, "context": "Our problem thus belongs to the family of multi-instance learning (MIL) problems [3, 7] where one sample is commonly called a bag (in our case representing a computer) and consists of a variable number of instances (in our case one instance is one flow), each described by a fixed dimensional vector.", "startOffset": 81, "endOffset": 87}, {"referenceID": 14, "context": "Finally, using approach similar to URCA [17], it is possible to identify particular connections which made the neural network decide that the computer is infected; hence effectively providing an explanation of the learned IoC.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "The pioneering work [6] coined multiple-instance or multiinstance learning as a problem, where each sample b (to be denoted bag in the following) consists of a set of instances x, i.", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "Later works (see reviews [3, 7]) have introduced different assumptions on relationships between the labels on the instance level and labels of bags or even dropped the notion of instance-level labels and considered only labels on the level of bags, i.", "startOffset": 25, "endOffset": 31}, {"referenceID": 4, "context": "Later works (see reviews [3, 7]) have introduced different assumptions on relationships between the labels on the instance level and labels of bags or even dropped the notion of instance-level labels and considered only labels on the level of bags, i.", "startOffset": 25, "endOffset": 31}, {"referenceID": 13, "context": "Since the solution presented in Section 3 belongs to the embedded-space paradigm, we describe this class of methods in necessary detail and adopt the formalism of [16], which is for our solution essential.", "startOffset": 163, "endOffset": 167}, {"referenceID": 13, "context": "The formalism of [16] is intended for a general formulation of MIL problems, where labels are assumed only on the level of bags without any labels on the level of instances.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "Note that this definition also includes that used in [6].", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "Methods from embedded space-paradigm [3, 7] first represent each bag b as a fixed-dimensional vector and then use any machine learning algorithm with samples of fixed dimension.", "startOffset": 37, "endOffset": 43}, {"referenceID": 4, "context": "Methods from embedded space-paradigm [3, 7] first represent each bag b as a fixed-dimensional vector and then use any machine learning algorithm with samples of fixed dimension.", "startOffset": 37, "endOffset": 43}, {"referenceID": 3, "context": "[6] assumed labels on instances and a bag was classified as positive if it contained at least one positive instance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "The important novelty introduced in [16] is that embedding functions {\u03c6i}i=1 are optimized simultaneously with the classifier that uses them, as opposed to the prior art where the two optimization problems are treated indepedently.", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "in [9], where the pooling function has form", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "\u2022 Mean function should be theoretically better [12], since it is more general.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "The advantage of mean pooling function has been experimental demonstrated in [16].", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "This problem has been recently studied in [4] in context of natural images.", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "Although finding the smallest number of such flows is likely an NP complete problem, a greedy approximation inspired by [17] performs surprisingly well.", "startOffset": 120, "endOffset": 124}, {"referenceID": 5, "context": "Therefore evaluated architectures used simple building blocks: rectified linear units [8, 12], mean and maximum pooling functions, and ADAM optimization algorithm [10].", "startOffset": 86, "endOffset": 93}, {"referenceID": 9, "context": "Therefore evaluated architectures used simple building blocks: rectified linear units [8, 12], mean and maximum pooling functions, and ADAM optimization algorithm [10].", "startOffset": 86, "endOffset": 93}, {"referenceID": 7, "context": "Therefore evaluated architectures used simple building blocks: rectified linear units [8, 12], mean and maximum pooling functions, and ADAM optimization algorithm [10].", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "The performance was measured using precision-recall curve (PR curve) [14] popular in document classification and information retrieval due to its better properties for highly imbalanced problems, into which intrusion detection belongs (in the testing data there is approximately one infected computer per one thousand clean ones).", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "This conclusion is supported by the fact that max pooling function can be approximated from the mean if layers preceding the aggregation are sufficiently complicated [15].", "startOffset": 166, "endOffset": 170}], "year": 2017, "abstractText": "Modeling network traffic is gaining importance in order to counter modern threats of ever increasing sophistication. It is though surprisingly difficult and costly to construct reliable classifiers on top of telemetry data due to the variety and complexity of signals that no human can manage to interpret in full. Obtaining training data with sufficiently large and variable body of labels can thus be seen as prohibitive problem. The goal of this work is to detect infected computers by observing their HTTP(S) traffic collected from network sensors, which are typically proxy servers or network firewalls, while relying on only minimal human input in model training phase. We propose a discriminative model that makes decisions based on all computer\u2019s traffic observed during predefined time window (5 minutes in our case). The model is trained on collected traffic samples over equally sized time window per large number of computers, where the only labels needed are human verdicts about the computer as a whole (presumed infected vs. presumed clean). As part of training the model itself recognizes discriminative patterns in traffic targeted to individual servers and constructs the final high-level classifier on top of them. We show the classifier to perform with very high precision, while the learned traffic patterns can be interpreted as Indicators of Compromise. In the following we implement the discriminative model as a neural network with special structure reflecting two stacked multi-instance problems. The main advantages of the proposed configuration include not only improved accuracy and ability to learn from gross labels, but also automatic learning of server types (together with their detectors) which are typically visited by infected computers.", "creator": "LaTeX with hyperref package"}}}