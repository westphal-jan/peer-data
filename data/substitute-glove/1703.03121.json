{"id": "1703.03121", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2017", "title": "Coordinated Multi-Agent Imitation Learning", "abstract": "We study the problem of vulgar emphasis from demonstrations addition possible coordinating whether. One further victory time particular setting but some learning after know model 's understanding can be difficult, only coordination common are rationale today the clampdown as must nothing derivation fact a latent variable. We propose perfect cooperation approach suggested simultaneously learns right latent enhance brand across put the individual changing. In particular, whole concept curriculum unsupervised solid ability new usually earthy teaching. We realities the power over change focus on from unlikely nothing created learning ranging policies for fine - grained disturbing modeling in team sports, here different players occupy such ensemble in made preparations best strategy. We shown that to once coordination newer to reasoning the ensemble of players yields substantially improves qualities overcome compared to require sphincters.", "histories": [["v1", "Thu, 9 Mar 2017 03:45:42 GMT  (2673kb,D)", "http://arxiv.org/abs/1703.03121v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hoang m le", "yisong yue", "peter carr 0001", "patrick lucey"], "accepted": true, "id": "1703.03121"}, "pdf": {"name": "1703.03121.pdf", "metadata": {"source": "META", "title": "Coordinated Multi-Agent Imitation Learning", "authors": ["Hoang M. Le", "Yisong Yue", "Peter Carr"], "emails": ["<hmle@caltech.edu>."], "sections": [{"heading": "1. Introduction", "text": "The areas of multi-agent planning and control have witnessed a recent wave of strong interest due to the practical desire to deal with complex real-world problems, such as smart-grid control, autonomous vehicles planning, managing teams of robots for emergency response, among others. From the learning perspective, (cooperative) multi-agent learning is not a new area of research (Stone & Veloso, 2000; Panait & Luke, 2005). However, compared to the progress in conventional supervised learning and singleagent reinforcement learning, the successes of multi-agent learning have remained relatively modest. Most notably, multi-agent learning suffers from extremely high dimensionality of both the state and actions spaces, as well as relative lack of data sources and experimental testbeds.\nThe growing availability of data sources for coordinated multi-agent behavior, such as sports tracking data\n1California Institute of Technology, Pasadena, CA 2Disney Research, Pittsburgh, PA. Correspondence to: Hoang M. Le <hmle@caltech.edu>.\nFigure 1. Our motivating example of learning coordinating behavior policies for team sports from tracking data. Red is the attacking team, blue is the defending team, and yellow is the ball.\n(Bialkowski et al., 2014), now enables the possibility of learning multi-agent policies from demonstrations, also known as multi-agent imitation learning. One particularly interesting aspect of domains such as team sports is that the agents must coordinate. For example, in the professional soccer setting depicted in Figure 1, different players must coordinate to assume different roles (e.g., defend left field). However, the roles and role assignment mechanism are unobserved from the demonstrations. Furthermore, the role for a player may change during the same play sequence. In the control community, this issue is known as \u201cindex-free\u201d multi-agent control (Kingston & Egerstedt, 2010).\nMotivated by these challenges, we study the problem of imitation learning for multiple coordinating agents from demonstrations. Many realistic multi-agent settings require coordination among collaborative agents to achieve some common goal (Guestrin et al., 2002; Kok et al., 2003). Beyond team sports, other examples include learning policies for game AI, controlling teams of multiple robots, or modeling collective animal behavior. As discussed above, we are interested in settings where agents have access to the outcome of actions from other agents, but the coordination mechanism is neither clearly defined nor observed, which makes the full state only partially observable.\nWe propose a semi-supervised learning framework that integrates and builds upon conventional imitation learning and unsupervised, or latent, structure learning. The latent structure model encodes a coordination mechanism, which approximates the implicit coordination in the demonstra-\nar X\niv :1\n70 3.\n03 12\n1v 1\n[ cs\n.L G\n] 9\nM ar\n2 01\n7\ntion data. In order to make learning tractable, we develop an alternating optimization method that enables integrated and efficient training of both individual policies and the latent structure model. For learning individual policies, we extend reduction-based single-agent imitation learning approaches into multi-agent domain, utilizing powerful black-box supervised techniques such as deep learning as base routines. For latent structure learning, we develop a stochastic variational inference approach.\nWe demonstrate the effectiveness of our method in two settings. The first is a synthetic experiment based on the popular predator-prey game. The second is a challenging task of learning multiple policies for team defense in professional soccer, using a large training set of play sequences illustrated by Figure 1. We show that learning a good latent structure to encode implicit coordination yields significantly superior imitation performance compared to conventional baselines. To the best of our knowledge, this is the first time an imitation learning approach has been applied to jointly learn cooperative multi-agent policies at large scale."}, {"heading": "2. Problem Formulation", "text": "In coordinated multi-agent imitation learning, we have K agents acting in coordination to achieve a common goal (or sequence of goals). Training data D consists of multiple demonstrations of K agents. Importantly, we assume the identity (or indexing) of the K experts may change from one demonstration to another. Each (unstructured) set of demonstrations is denoted by U \u201c tU1, . . . , UKu, where Uk \u201c tut,kuTt\u201c1 is the sequence of actions by agent k at time t. Note that each set of demonstrations can have varying sequence length T. Let C \u201c tctuTt\u201c1 be the context associated with each demonstration sequence.\nPolicy Learning. Our ultimate goal is to learn a (largely) decentralized policy, but for clarity we first present the problem of learning a fully centralized multi-agent policy. Following the notation of (Ross et al., 2011), let ~\u03c0p~sq :\u201c ~a denote the joint policy that maps the joint state, ~s \u201c rs1, . . . , sKs, of all K agents into K actions ~a \u201c ra1, . . . , aKs. The goal is to minimize imitation loss:\nLimitation \u201c E~s\u201ed~\u03c0 r`p~\u03c0p~sqqs ,\nwhere d~\u03c0 denotes the distribution of states experienced by joint policy ~\u03c0 and ` is the imitation loss defined over the demonstrations (e.g., squared loss for deterministic policies, or cross entropy for stochastic policies).\nThe decentralized setting decomposes the joint policy ~\u03c0 \u201c r\u03c01, . . . , \u03c0Ks into K policies, each tailored to a specific agent index or \u201crole\u201d.1 The loss function is then:\n1It is straightforward to extend our formulation to settings where multiple agents can occupy the same role, and where not\nLimitation \u201c K \u00ff\nk\u201c1 Es\u201ed\u03c0k r`p\u03c0kpskqqs .\nBlack-Box Policy Classes. In order to leverage powerful black-box policy classes such as random forests and deep learning, we take a learning reduction approach to training ~\u03c0. One consequence is that the state space representation s \u201c rs1, . . . , sKs must be consistently indexed, e.g., agent k in one instance must correspond to agent k in another instance. This requirement applies for both centralized and decentralized policy learning, and is often implicitly assumed in prior work on multi-agent learning.\nCoordination. Consider the task of imitating professional soccer players, where training data includes play sequences from different teams and games. Here the context C corresponds to the behavior of the opposing team and the ball. The demonstration data includes multiple sequences of Kset of trajectories U \u201c tU1, U2, . . . , UKu, where the actual identity of player generatingUk may not be consistent from one demonstration to the next. In addition, although team players tend to act in a coordinated fashion, we do not have information about which player is taking on which role, and roles can be switched in the same segment of play.\nOne important challenge is constructing a consistent indexing over the agents to yield a consistent state representation for black-box policy learning. For example, the same index should correspond to the left midfielder in all instances. Otherwise, the inputs to the policy will be inconsistent, making learning difficult if not impossible. A highly related issue arises in distributed control of index-free coordinating robots, e.g., to maintain a defined formation (Kloder & Hutchinson, 2006; Kingston & Egerstedt, 2010).\nCoordinated Policy Learning. We formulate the indexing mechanism as an assignment functionA maps the unstructured set U and some probabilistic structured model p to an indexed set of trajectory A rearranged from U , i.e.,\nA : tU1, .., UKu \u02c6 q \u00de\u00d1 rA1, .., AKs ,\nwhere the set tA1, .., AKu \u201d tU1, .., UKu. We view q as a latent variable model that infers the role assignments for each set of demonstrations. Thus, q drives the indexing mechanismA so that state vectors can be consistently constructed to facilitate optimizing for the imitation loss.\nNote that roles may change across within a demonstration, causing potential mis-alignment of expert demonstration to policy mapping. To resolve this confusion, we employ the well-known principle of maximum entropy (Jaynes, 1957), which is equivalent to regularizing the imitation loss function with some low entropy penalty (Grandvalet et al.,\nall roles are occupied across all execution sequences.\n2004; Dudik et al., 2004), yielding our overall objective:\nmin \u03c01,..,\u03c0K ,A\nK \u00ff k\u201c1 Esk\u201ed\u03c0k r`p\u03c0kpskqq|A,Ds\u00b4\u03bbHpA|Dq (1)\nwhere both imitation loss and entropy are measured with respect to the state distribution induced by the policies, and D is training data."}, {"heading": "3. Learning Approach", "text": "Optimizing (1) is challenging for two reasons. First, beyond the challenges inherited from single-agent settings, multi-agent imitation learning must account for multiple simultaneously learning agents, which is known to cause non-stationarity for multi-agent reinforcement learning (Busoniu et al., 2008). Second, the latent role assignment model, which forms the basis for coordination, depends on the actions of the learning policies, which in turn depend on the structured role assignment.\nWe propose an alternating optimization approach to solving (1), summarized in Figure 2. The main idea is to integrate imitation learning with unsupervised structure learning by taking turns to (i) optimize for imitation policies while fixing a structured model (minimizing imitation loss), and (ii) re-train the latent structure model and reassign roles while fixing the learning policies (maximizing role assignment entropy). The alternating nature allows us to circumvent the circular dependency between policy learning and latent structure learning. Furthermore, for (i) we develop a stable multi-agent learning reduction approach."}, {"heading": "3.1. Approach Outline", "text": "Algorithm 1 outlines our framework. We assume the latent structure model for computing role assignments is formulated as a graphical model. The multi-agent policy training procedure Learn utilizes a reduction approach, and can leverage powerful off-the-shelf supervised learning tools such as deep neural networks (Hochreiter & Schmidhuber, 1997). The structure learning LearnStructure and role assignment Assign components are based on graphi-\nAlgorithm 1 Coordinated Multi-Agent Imitation Learning Input: Multiple unstructured trajectory sets U \u201c tU1, . . . , UKu\nwith Uk \u201c tut,kuTt\u201c1 and context C \u201c tctuTt\u201c1. Input: Graphical model q with global/local parameters \u03b8 and z. Input: Initialized policies \u03c0k, k \u201c 1, . . . ,K Input: Step size sequence \u03c1n, n \u201c 1, 2, . . . 1: repeat 2: rA1, . . . , AKs \u00d0 AssigntU1, . . . , UK |qp\u03b8, zqu 3: r\u03c01, . . . , \u03c0Ks \u00d0 Learn rA1, . . . , AK , Cs 4: Roll-out \u03c01, . . . , \u03c0K to obtain pA1, . . . , pAK 5: Ak \u00d0 pAk @k\n(Alternatively: Ak \u00d0 pAk with prob \u03b7 for \u03b7 \u00d1 1) 6: qp\u03b8, zq \u00d0 LearnStructuretA1, . . . , AK , C, \u03b8, \u03c1nu 7: until No improvement on validation set\noutput K policies \u03c01, \u03c02, . . . , \u03c0K\ncal model training and inference. For efficient training, we employ alternating stochastic optimization (Hoffman et al., 2013; Johnson & Willsky, 2014; Beal, 2003) on the same mini-batches. Note that batch training can be deployed similarly, as illustrated by one of our experiments.\nWe interleave the three components described above into a complete learning algorithm. Given an initially unstructured set of training data, an initialized set of policies, and prior parameters of the structure model, Algorithm 1 performs alternating structure optimization on each mini-batch (size 1 in Algorithm 1).\n\u2022 Line 2: Role assignment is performed on trajectories tA1, . . . , AKu by running inference procedure (Algorithm 4). The result is an ordered set rA1, . . . , AKs, where trajectory Ak corresponds to policy \u03c0k.\n\u2022 Line 3-5: Each policy \u03c0k is updated using joint multiagent training on the ordered set rA1, . . . , AK , Cs (Algorithm 2). The updated models are executed to yield a rolled-out set of trajectories, which replace the previous set of trajectories tAku.\n\u2022 Line 6: Parameters of latent structured model are updated from the rolled-out trajectories (Algorithm 3).\nThe algorithm optionally includes a mixing step on line 5, where the rolled-out trajectories may replace the training trajectories with increasing probability approaching 1, which is similar to scheduled sampling (Bengio et al., 2015), and may help stabilize learning in the early phase of the algorithm. In our main experiment, we do not notice a performance gain using this option."}, {"heading": "3.2. Joint Multi-Agent Imitation Learning", "text": "In this section we describe the Learn procedure for multiagent imitation learning in Line 3 of Algorithm 1. As background, for single agent imitation learning, reductionbased methods operate by iteratively collecting a new data set Dn at each round n of training, consisting of stateaction pairs pst, a\u02dat q where a\u02dat is some optimal or demonstrated action given state st. A new policy can be formed\nAlgorithm 2 Joint Multi-Agent Imitation Learning LearnpA1, A2, . . . , AK , Cq Input: Ordered actions Ak \u201c tat,kuTt\u201c1 @k, context tctuTt\u201c1 Input: Initialized policies \u03c01, . . . , \u03c0K Input: base routine TrainpS,Aq mapping state to actions 1: Set increasing prediction horizon j P t1, . . . , T u 2: for t \u201c 0, j, 2j, . . . , T do 3: for i \u201c 0, 1, . . . , j \u00b4 1 do 4: Roll-out a\u0302t`i,k \u201c \u03c0kps\u0302t`i\u00b41,kq @ agent k 5: Cross-update for each policy k P t1, . . . ,Ku\ns\u0302t`i,k \u201c \u03d5k pra\u0302t`i,1, . . . , a\u0302t`i,k, . . . , a\u0302t`i,K , ct`isq 6: end for 7: Policy update for all agent k\n\u03c0k \u00d0 Trainpts\u0302t`i,k, a\u02dat`i`1,ku j i\u201c0q\n8: end for output K updated policies \u03c01, \u03c02, . . . , \u03c0K\nby (i) combining a new policy from this data set Dn with previously learned policy \u03c0 (Daume\u0301 III et al., 2009) or (ii) learning a new policy \u03c0 directly from the data set formed by aggregating D1, . . . ,Dn (Ross et al., 2011). Other variants of exist although we do not discuss them here.\nThe intuition behind the iterative reduction approach is to prevent a mismatch in training and prediction distributions due to sequential cascading errors (also called covariateshift). The main idea is to use learning policy\u2019s own prediction in the construction of subsequent states, thus simulating the test-time performance during training. This mechanism enables the agent to learn a policy that is robust to its own mistakes. Reduction-based methods also accommodate any black-box supervised training subroutine. We focus on using expressive function classes such as Long Short-Term Memory networks (LSTM) (Hochreiter & Schmidhuber, 1997) as the policy class.2\nAlgorithm 2 outlines the Learn procedure for stable multi-agent imitation learning. Assume we are given consistently indexed demonstrations A \u201c rA1, . . . , AKs, where each Ak \u201c tat,kuTt\u201c1 corresponds action of policy \u03c0k at time t. Let the corresponding expert action be a\u02dat,k. To lighten the notation, we denote the per-agent state vector by st,k \u201c \u03d5kprat,1, . . . , at,k, . . . , at,K , ctsq3\nAlgorithm 2 employs a roll-out horizon j, which divides the entire trajectory into T {j segments. The following happens for every segment:\n\u2022 Iteratively performs roll-out at each time step i for all K policies (line 4) to obtain actions tpai,ku.\n\u2022 Each policy simultaneously updates its state psi,k, us2Note that conventional training of LSTMs does not address the cascading error problem. While LSTMs are very good at sequence-to-sequence prediction tasks, they cannot naturally deal with the drifting of input state distribution drift caused by action output feedback in dynamical systems (Bengio et al., 2015).\n3Generally, state vector st,k of policy \u03c0k at time t can be constructed as st,k \u201c r\u03c6kpra1:t,1, c1:tsq, . . . , \u03c6kpra1:t,K , c1:tsqs\ning the prediction from all other policies (line 5).\n\u2022 At the end of the current segment, all policies are updated using the error signal from the deviation between predicted pai,k versus expert action a\u02dai,k, for all i along the sub-segment (line 7).\nAfter policy updates, the training moves on to the next jlength sub-segment, using the freshly updated policies for subsequent roll-outs. The iteration proceeds until the end of the sequence is reached. In the outer loop the roll-out horizon j is incremented.\nTwo key insights behind our approach are:\n\u2022 In addition to the training-prediction mismatch issue in single-agent learning, each agent\u2019s prediction must also be robust to imperfect predictions from other agents. This non-stationarity issue also arises in multiagent reinforcement learning (Busoniu et al., 2008) when agents learn simultaneously. We perform joint training by cross-updating each agent\u2019s state using previous predictions from other agents.\n\u2022 Many single-agent imitation learning algorithms assume the presence of a dynamic oracle to provide onestep corrections a\u02dat along the roll-out trajectories. In practice, dynamic oracle feedback is very expensive to obtain and some recent work have attempted to relax this requirement (Le et al., 2016; Ho & Ermon, 2016). Without dynamic oracles, the rolled-out trajectory can deviate significantly from demonstrated trajectories when the prediction horizon j is large (\u00ab T ), leading to training instability. Thus j is gradually increased to allow for slowly learning to make good sequential predictions over longer horizons.\nFor efficient training, we focus on stochastic optimization, which can invoke base routine Train multiple times and thus naturally accommodates varying j. Note that the batch-training alternatives to Algorithm 2 can also employ similar training schemes, with similar theoretical guarantees lifted to the multi-agent case. The Appendix shows how to use DAgger (Ross et al., 2011) for Algorithm 2, which we used for our synthetic experiment."}, {"heading": "3.3. Coordination Structure Learning", "text": "The coordination mechanism is based on a latent structured model that governs the role assignment. The training and inference procedures seek to address two main issues:\n\u2022 LearnStructure: unsupervised learning a probabilistic role assignment model q.\n\u2022 Assign: how q informs the indexing mechanism so that unstructured trajectories can be mapped to structured trajectories amenable to Algorithm 2.\nGiven an arbitrarily ordered set of trajectories U \u201c tU1, . . . , UK , Cu, let the coordination mechanism underlying each such U be governed by a true unknown model p, with global parameters \u03b8. We suppress the agent/policy subscript and consider a generic featurized trajectory xt \u201c rut, cts @t. Let the latent role sequence for the same agent be z \u201c z1:T . At any time t, each agent is acting according to a latent role zt \u201e Categoricalt1\u0304, 2\u0304, . . . , K\u0304u, which are the local parameters to the structured model.\nIdeally, role and index asignment can be obtained by calculating the posterior ppz|x, \u03b8q, which is often intractable. We instead aim to learn to approximate ppz|x, \u03b8q by a simpler distribution q via Bayesian inference. In particular, we employ techniques from stochastic variational inference (Hoffman et al., 2013), which allows for efficient stochastic training on mini-batches that can naturally integrate with our imitation learning subroutine.\nWe provide an overview of structured variational inference and describe our modified inference procedure. Posterior approximation is often cast as optimizing over a simpler model class Q, via searching for parameters \u03b8 and z that maximize the evidence lower bound (ELBO) L:\nlog ppxq \u011b Eq rlog ppz, \u03b8, xqs \u00b4 Eq rlog qpz, \u03b8qs fi L pqpz, \u03b8qq .\nMaximizing L is equivalent to finding q P Q to minimize the KL divergence KL pqpz, \u03b8|xq||ppz, \u03b8|xqq. We focus on the structured mean-field variational family, which factorizes q as qpz, \u03b8q \u201c qpzqqp\u03b8q (Hoffman & Blei, 2014) and decomposes the ELBO objective:\nL \u201c Eqrlog pp\u03b8s \u00b4 Eqrlog qp\u03b8s ` Eqrlogpppz, x|\u03b8qs \u00b4 Eqrlogpqpzqqs. (2)\nThis factorization breaks the dependency between \u03b8 and z, but not between single latent states zt, unlike variational inference for i.i.d data (Kingma & Welling, 2013)."}, {"heading": "3.3.1. TRAINING TO LEARN MODEL PARAMETERS", "text": "Optimization proceeds via alternating optimization of \u03b8 and z. Stochastic variational inference performs such updates efficiently in mini-batches. We slightly abuse notations and overload \u03b8 for the natural parameters of global parameter \u03b8 in the exponential family. Assuming the usual conjugacy in the exponential family, the stochastic natural gradient takes the convenient form (Hoffman et al., 2013):\n\u03b8n`1 \u201c \u03b8np1\u00b4\u03c1nq`\u03c1np\u03b8prior`bJEq\u02dapzq rtpz, xqsq, (3)\nwhere tpz, xq is the vector of sufficient statistics, b is a vector of scaling factors adjusting for the relative size of the mini-batches. Here the global update assumes optimal local update q\u02dapzq has been computed.\nAlgorithm 3 Coordinated Structure Learning LearnStructure tU1, . . . , UK , C, \u03b8, \u03c1u \u00de\u00d1 qp\u03b8, zq Input: Set of trajectories U \u201c tUkuKk\u201c1. Context C\nPrevious parameters \u03b8 \u201c pp0, \u03b8P , \u03b8\u03c6q, stepsize \u03c1 1: Xk \u201c txt,kuTt\u201c1 \u201c trut,k, ctsu @t, k.X \u201c tXkuKk\u201c1 2: Local update: Compute rP and rp per equation 5 and 6\nand compute qpzq \u201c Forward-BackwardpX, rP , rpq 3: Global update: \u03b8 \u00d0 \u03b8p1\u00b4 \u03c1q ` \u03c1p\u03b8prior ` bJEqpzq rtpz, xqsq\noutput Updated model qp\u03b8, zq \u201c qp\u03b8qqpzq\nFixing the global parameters, the local updates are based on message passing over the graphical model. The exact mathematical derivation depends on the specific graph structure. The simplest scenario is to assume independence among zt\u2019s, which resembles naive Bayes. We instead focus on Hidden Markov Models to capture first-order dependencies in role transitions over play sequences. In this case, global parameters \u03b8 \u201c pp0, P, \u03c6q where P \u201c rPijsKi,j\u201c1 is the transition matrix with Pij \u201c ppzt \u201c j|zt\u00b41 \u201c iq, \u03c6 \u201c t\u03c6iuKi\u201c1 are the emission parameters, and p0 is the initial distribution. For HMMs, we have a full probabilistic model: ppz, x|\u03b8q \u201c p0pz1q \u015bT t\u201c1 ppzt|zt\u00b41, P qppxt|zt, \u03c6q. Calculating the gradient w.r.t z yields the following optimal variational distribution over the latent sequence:\nq\u02dapzq9 exp \u00b4 EqpP qrlog p0pz1qs ` T \u00ff\nt\u201c2 EqpP qrlogPzt\u00b41,zts\n` T \u00ff\nt\u201c1 Eqp\u03c6q logrppxt|ztqs\n\u00af\n, (4)\nwhich gives the local updates for q\u02dapzq, given current estimates of P and \u03c6:\nrPj,k \u201c exp \u201c EqpP q logpPj,kq \u2030\n(5)\nrppxt|zt \u201c kq \u201c exp \u201c Eqp\u03c6q log ppxt|xt \u201c kq \u2030 , (6)\nfor k \u201c 1, . . . ,K, t \u201c 1, . . . , T , and then use p0, rP , rp to run the forward-backward algorithm to compute the update q\u02dapzt \u201c kq and q\u02dapzt\u00b41 \u201c j, zt \u201c kq. The forwardbackward algorithm in the local update step takesOpK2T q time for a chain of length T and K hidden states. The procedure to learn the parameter of our structured model is summarized in Algorithm 3. For completeness, derivation of Forward-Backward is included in the appendix."}, {"heading": "3.3.2. INFERENCE FOR ROLE AND INDEX ASSIGNMENT", "text": "We can compute two types of inference on a learned q:\nRole assignment. Compute the most likely role sequence tzt,kuTt\u201c1 P t1\u0304, . . . , K\u0304uT , e.g., using Viterbi (or dynamic programming-based forward message passing for graph structures). This most likely role sequence for agent k, which is the low-dimensional representation of the coordination mechanism, can be used to augment the contextual feature tctuTt\u201c1for each agent\u2019s policy training.\nAlgorithm 4 Multi-Agent Role Assignment Assign tU1, . . . , UK |qu \u00de\u00d1 rA1, . . . , AKs Input: Approximate inference model q. Unordered trajectories\nU \u201c tUkuKk\u201c1. 1: Calculate cost matrix M P RK\u02c6K per equation 7 2: A\u00d0 MinCostAssignmentpMq\noutput Ak \u201c UApkq @k \u201c 1, 2, . . . ,K\nIndex Assignment Transform the unstructured set U into an ordered set of trajectories A to facilitate the imitation learning step. This is the more important task for the overall approach. The intuitive goal an indexing mechanism that facilitate consistent agent trajectory to policy mapping. Assume for notational convenience that we want index k assigned to an unique agent who is most likely assuming role k\u0304. Our inference technique rests on the well-known Linear Assignment Problem (Papadimitriou & Steiglitz, 1982), which is solved optimally via the Kuhn-Munkres algorithm. Specifically, construct the cost matrix M as: M \u201cM1 dM2 (7)\nM1 \u201c \u201c qptxt,ku|zt,k \u201c k\u0304q \u2030 \u201c \u00ab T \u017a\nt\u201c1 qpxt,k|zt,k \u201c k\u0304q\nff\nM2 \u201c \u201c log qptxt,ku|zt,k \u201c k\u0304q \u2030 \u201c \u00ab T \u00ff\nt\u201c1 log qpxt,k|zt,k \u201c k\u0304q\nff\nwhere d is the Hadamard product, and matrices M1,M2 take advantage of the Markov property of the graphical model. Now solving the linear assignment problem for cost matrixM , we obtain the matchingA from role k\u0304 to index k, such that the total cost per agent is minimized. From here, we rearrange the unordered set tU1, . . . , UKu to the ordered sequence rA1, . . . , AKs \u201d rUAp1q, . . . , UApKqs according to the minimum cost mapping.\nTo see why this index assignment procedure results in an increased entropy in the original objective (1), notice that: HpA|Dq \u00ab \u00b4 K \u00ff\nk\u0304\u201c1\nP pk\u0304qqpApAkq \u201c k\u0304q log qpApAkq \u201c k\u0304q\n\u201c \u00b4 1 K\nK \u00ff\nk\u0304\u201c1\nMpk\u0304, kq,\nwhere we assume each latent role k\u0304 has equal probability. The RHS increases from the linear assignment and consequent role assignment procedure. Our inference procedure to perform role assignment is summarized in Algorithm 4."}, {"heading": "4. Experiments", "text": "We present empirical results from two settings. The first is a synthetic setting based on predator-prey, where the goal is to imitate a coordinating team of predators. The second is a large-scale imitation learning setting from player trajectores in professional soccer games, where the goal is to imitate defensive team play."}, {"heading": "4.1. Predator-Prey Domain", "text": "Setting. The predator-prey problem, also frequently called the Pursuit Domain (Benda, 1985), is a popular setting for multi-agent reinforcement learning. The traditional setup is with four predators and one prey, positioned on a grid board. At each time step, each agent has five moves:\nN,S,E,W or no move. The world is toroidal: the agents can move off one end of the board and come back on the other end. Agents make move simultaneously, but two agents cannot occupy the same position, and\ncollisions are avoided by assigning a random move priority to the agents at each time step. The predators can capture the prey only if the prey is surrounded by all four predators. The goal of the predators is to capture the prey as fast as possible, which necessarily requires coordination.\nData. The demonstration data is collected from 1000 game instances, where four experts, indexed 1 to 4, are prescribed the consistent and coordinated role as illustrated in the capture state of Figure 3. In other words, agent 1 would attempt to capture the prey on the right hand side, which allows for one fixed role for each expert throughout the game. However, the particular role assignment is hidden from the imitation learning task. Each expert is then exhaustively trained using Value Iteration (Sutton & Barto, 1998) in the reinforcement learning setting, with the reward of 1 if the agent is in the position next to the prey according to its defined role, and 0 otherwise. A separate set of 100 games was collected for evaluation. A game is terminated after 50 time steps if the predators fail to capture the prey. In the test set, the experts fail to capture the prey in 2% of the games, and on average take 18.3 steps to capture the prey.\nExperiment Setup. For this experiment, we use the batch version of Algorithm 1 (see appendix) to learn to imitate the experts using only demonstrations. Each policy is represented by a random forest of 20 trees, and were trained over 10 iterations. The expert correction for each rolled-out state is collected via Value Iteration. The experts thus act as dynamic oracles, which result in a multi-agent training setting analogous to DAgger (Ross et al., 2011). We compare two versions of multi-agent imitation learning:\n\u2022 Coordinated Training. We use our algorithm, with the latent structure model represented by a discrete Hidden Markov Model with binomial emission. We use Algorithm 4 to maximize the role consistency of the dynamic oracles across different games.\n\u2022 Unstructured Training. An arbitrary role is assigned to each dynamic oracle for each game, i.e., the agent index is meaningless.\nIn both versions, training was done using the same data aggregation scheme and batch training was conducted using the same random forests configuration.\nResults. Figure 4 compares the test performance of our method versus unstructured multi-agent imitation learning. We see that using a coordination mechanism results in substantially improved performance. Our method quickly approaches expert performance (average 22 steps with 8% failure rate in the last iteration), whereas unstructured multi-agent imitation learning performance did not improve beyond the first iteration (average 42 steps with 70% failure rate). Note that we even gave the unstructured baseline some advantage over our method, by forcing the prey to select the moves last after all predators make decisions (effectively making the prey slower). Without this advantage, the unstructured policies fail to capture the prey almost 100% of the time. Also, if the same restriction is applied to the policies obtained from our method, performance would be on par with the experts (100% success rate, with similar number of steps taken)."}, {"heading": "4.2. Multi-agent Imitation Learning for Soccer", "text": "Setting. Soccer is a popular domain for multi-agent learning. RoboCup, the robotic and simulation soccer platform, is perhaps the most popular testbed for multi-agent reinforcement learning research to date (Stone, 2016). The success of MARL has been limited, however, due to the extremely high dimensionality of the problem. In this experiment, we aim to learn multi-agent policies for team soccer defense, based on tracking data from real-life professional soccer (Bialkowski et al., 2014). Unlike predatorprey, states and actions are continuous, the game sequence can be much longer, and the roles are not well-defined. The same player may also switch roles between sequences, and two players can swap roles within a sequence.\nData. We use the tracking data from 45 games of real professional soccer from a recent European league. The\ndata was chunked into sequences with one team attacking and the other defending. Our goal is to learn up to 10 policies for team defense (11 players per team, minus the goal keeper). The training data consists of 7500 sets of trajectories A \u201c tA1, . . . , A10u and contexts C, where Ak \u201c tat,kuTt\u201c1 is the sequence of positions of one defensive player, and C is the context sequence consisting of opponents and the ball. Overall, there are about 1.3 million frames at 10 frames per second. The average sequence length is 176 steps, and the maximum is 1480.\nExperiment Setup. Each policy is represented by a recurrent neural network structure (LSTM), with two hidden layers of 512 units each. As LSTMs generally require fixed-length input sequences, we further chunk each trajectory into sub-sequences of length 50, with overlapping window of 25 time steps. The joint multi-agent imitation learning procedure follows Algorithm 2 closely. In this setup, without access to dynamic oracles for imitation learning in the style of SEARN (Daume\u0301 III et al., 2009) and DAgger (Ross et al., 2011), we gradually increase the horizon of the rolled-out trajectories from 1 to 10 steps lookahead. Empirically, this has the effect of stabilizing the policy networks early in training, and limits the cascading errors caused by rolling-out to longer horizons.\nThe structured model component is learned via stochastic variational inference on a continuous Hidden Markov Model, where the per-state emission distribution is a mixture of Gaussians. Training and inference operate on the same mini-batches used for joint policy learning.\nWe compare against two variations. The first is employs centralized policy that aggregates the state vectors of all decentralized learner and produces the actions for all players, i.e., a multi-task policy. The centralized approach generally requires more model parameters, but is potentially much more accurate. The second variation is to not employ joint multi-agent training: we modify Algorithm 2 to not cross-update states between agents, and each role is trained conditioned on the ground truth of the other agents.\nResults. Figure 5 shows the results. We see that our coordinated learning approach substantially outperforms conventional imitation learning without structured coordination. The imitation loss measures average distance of roll-outs and ground truth in meters (note the typical size of soccer field is 110 \u02c6 70 meters). As expected, average loss increases with longer sequences, due to cascading errors. However, this error scales sub-linearly with the length of the horizon, even though the policies were trained on sequences of length 50. Note also that the performance difference between decentralized and centralized policies is insignificant compared to the gap between coordinated and unstructured policies, further highlighting the benefits of structured coordination in multi-agent settings. The loss of a single network, non-joint training scheme is very large and thus omitted from Figure 5 (see the appendix).\nVisualizations. Imitation loss, of course, is not a full reflection of the quality of the learned policies. Unlike predator-prey, the long-term reward signal is not available, so we rely on visual inspection as part of evaluation. Figure 6 overlays policy prediction on top of the actual game sequence from Figure 1. Additional test examples are included in our supplemental video. We note that learned policies are qualitatively similar to the ground truth demonstrations, and can be useful for applications such as counterfactual replay analysis (Le et al., 2017).\nFigure 7 displays the Gaussian components of the underlying HMM. The components correspond to the dominant modes of the roles assigned. Unlike the predator-prey domain, roles can be switched during a sequence of play. See the appendix for more details such as role swap frequency."}, {"heading": "5. Other Related Work", "text": "The problem of multi-agent imitation learning has not been widely considered, perhaps with the exception of (Chernova & Veloso, 2007) which focused on very different applications and technical challenges (i.e., learning a model\nof a joint task by collecting samples from direct interaction with teleoperating human teachers). The actual learning algorithm there requires the learner to collect enough data points from human teachers for confident classification of task. It is not clear how well the proposed method would translate to other domains.\nIndex-free policy learning is generally difficult for blackbox machine learning techniques. Some recent work has called attention to the importance of order to learning when input or output are sets (Vinyals et al., 2015), motivated by classic algorithmic and geometric problems such as learning to sort a set of numbers, or finding convex hull for a set of points, where no clear indexing mechanism exists. Other permutation invariant approaches include those for standard classification (Shivaswamy & Jebara, 2006)."}, {"heading": "6. Limitations and Future Work", "text": "In principle, the training and inference of the latent structure model can accommodate different types of graphical models. However, the exact procedure varies depending on the graph structure. We rely on first-order Markov models to leverage well-studied inference techniques such as Viterbi. It would be interesting to find domains that can benefit from more general graphical models. Another possible direction is to develop fully end-to-end differentiable training methods that can accommodate our index-free policy learning formulation, especially deep learning-based method that could provide computational speed-up compared to traditional graphical model inference. One potential issue with the end-to-end approach is the need to depart from a learning-reductions style approach.\nAlthough we addressed learning from demonstrations in this paper, the proposed framework can also be employed for generative modeling, or more efficient structured exploration for reinforcement learning. Along that line, our proposed method could serve as a useful component of general reinforcement learning, especially in multi-agent settings where traditional exploration-based approaches such as Qlearning prove computationally intractable."}, {"heading": "B. Experimental Evaluation", "text": "B.1. Batch-Version of Algorithm 2 for Predator-Prey\nAlgorithm 5 Multi-Agent Data Aggregation Imitation Learning LearnpA1, A2, . . . , AK , C|Dq\nInput: Ordered actions Ak \u201c tat,kuTt\u201c1 @k, context tctuTt\u201c1 Input: Aggregating data set D1, .., DK for each policy Input: base routine TrainpS,Aq mapping state to ac-\ntions 1: for t \u201c 0, 1, 2, . . . , T do 2: Roll-out a\u0302t`1,k \u201c \u03c0kps\u0302t,kq @ agent k 3: Cross-update for each policy k P t1, . . . ,Ku s\u0302t`1,k \u201c \u03d5k pra\u0302t`1,1, . . . , a\u0302t`1,k, . . . , a\u0302t`1,K , ct`1sq 4: Collect expert action a\u02dat`1,k given state s\u0302t`1,k @k 5: Aggregate data set Dk \u201c Dk Yts\u0302t`1,k, a\u02dat`1,ku T\u00b41 t\u201c0 6: end for 7: \u03c0k \u00d0 TrainpDkq\noutput K new policies \u03c01, \u03c02, . . . , \u03c0K\nB.2. Visualizing Role Assignment for Soccer\nThe Gaussian components of latent structure in figure 7 give interesting insight about the latent structure of the demonstration data, which correspond to a popular formation arrangement in professional soccer. Unlike the predator-prey domain, however, the players are sometimes expected to switch and swap roles. Figure 8 displays the tendency that each learning policy k would takes on other roles outside of its dominant mode. Policies indexed 0\u00b4 3 tend to stay most consistent with the prescribed latent roles. We observe that these also correspond to players with the least variance in their action trajectories. Imitation loss is generally higher for less consistent roles (e.g. policies indexed 8\u00b49). Intuitively, entropy regularization encourages a decomposition of roles that result in learning policies as decoupled as possible, in order to minimize the imitation loss."}], "references": [{"title": "Variational algorithms for approximate Bayesian inference", "author": ["Beal", "Matthew James"], "venue": "University of London United Kingdom,", "citeRegEx": "Beal and James.,? \\Q2003\\E", "shortCiteRegEx": "Beal and James.", "year": 2003}, {"title": "On optimal cooperation of knowledge sources", "author": ["Benda", "Miroslav"], "venue": "Technical Report BCS-G2010-28,", "citeRegEx": "Benda and Miroslav.,? \\Q1985\\E", "shortCiteRegEx": "Benda and Miroslav.", "year": 1985}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "win at home and draw away\u201d: Automatic formation analysis highlighting the differences in home and away team behaviors", "author": ["Bialkowski", "Alina", "Lucey", "Patrick", "Carr", "Peter", "Yue", "Yisong", "Matthews", "Iain"], "venue": "In MIT Sloan Sports Analytics Conference (SSAC),", "citeRegEx": "Bialkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bialkowski et al\\.", "year": 2014}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["Busoniu", "Lucian", "Babuska", "Robert", "De Schutter", "Bart"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews,", "citeRegEx": "Busoniu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2008}, {"title": "Multiagent collaborative task learning through imitation", "author": ["Chernova", "Sonia", "Veloso", "Manuela"], "venue": "In Proceedings of the fourth International Symposium on Imitation in Animals and Artifacts,", "citeRegEx": "Chernova et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chernova et al\\.", "year": 2007}, {"title": "Searchbased structured prediction", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Performance guarantees for regularized maximum entropy density estimation", "author": ["Dudik", "Miroslav", "Phillips", "Steven J", "Schapire", "Robert E"], "venue": "In International Conference on Computational Learning Theory,", "citeRegEx": "Dudik et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dudik et al\\.", "year": 2004}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Grandvalet", "Yves", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Grandvalet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Grandvalet et al\\.", "year": 2004}, {"title": "Coordinated reinforcement learning", "author": ["Guestrin", "Carlos", "Lagoudakis", "Michail", "Parr", "Ronald"], "venue": "In ICML,", "citeRegEx": "Guestrin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2002}, {"title": "Generative adversarial imitation learning", "author": ["Ho", "Jonathan", "Ermon", "Stefano"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Structured stochastic variational inference", "author": ["Hoffman", "Matt", "Blei", "David"], "venue": "CoRR abs/1404.4114,", "citeRegEx": "Hoffman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Stochastic variational inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John William"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Information theory and statistical mechanics", "author": ["Jaynes", "Edwin T"], "venue": "Physical review,", "citeRegEx": "Jaynes and T.,? \\Q1957\\E", "shortCiteRegEx": "Jaynes and T.", "year": 1957}, {"title": "Stochastic variational inference for bayesian time series models", "author": ["Johnson", "Matthew James", "Willsky", "Alan S"], "venue": "In ICML, pp", "citeRegEx": "Johnson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Index-free multi-agent systems: An eulerian approach", "author": ["Kingston", "Peter", "Egerstedt", "Magnus"], "venue": "IFAC Proceedings Volumes,", "citeRegEx": "Kingston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kingston et al\\.", "year": 2010}, {"title": "Path planning for permutation-invariant multirobot formations", "author": ["Kloder", "Stephen", "Hutchinson", "Seth"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "Kloder et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kloder et al\\.", "year": 2006}, {"title": "Multi-robot decision making using coordination graphs", "author": ["Kok", "Jelle R", "Spaan", "Matthijs TJ", "Vlassis", "Nikos"], "venue": "In Proceedings of the 11th International Conference on Advanced Robotics, ICAR,", "citeRegEx": "Kok et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kok et al\\.", "year": 2003}, {"title": "Smooth imitation learning for online sequence prediction", "author": ["Le", "Hoang", "Kang", "Andrew", "Yue", "Yisong", "Carr", "Peter"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Le et al\\.", "year": 2016}, {"title": "Datadriven ghosting using deep imitation learning", "author": ["Le", "Hoang M", "Carr", "Peter", "Yue", "Yisong", "Lucey", "Patrick"], "venue": "In MIT Sloan Sports Analytics Conference (SSAC),", "citeRegEx": "Le et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Le et al\\.", "year": 2017}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["Panait", "Liviu", "Luke", "Sean"], "venue": "Autonomous agents and multi-agent systems,", "citeRegEx": "Panait et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Panait et al\\.", "year": 2005}, {"title": "Combinatorial optimization: algorithms and complexity", "author": ["Papadimitriou", "Christos H", "Steiglitz", "Kenneth"], "venue": "Courier Corporation,", "citeRegEx": "Papadimitriou et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 1982}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "Stephane", "Gordon", "Geoff", "Bagnell", "J. Andrew"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Permutation invariant svms", "author": ["Shivaswamy", "Pannagadatta K", "Jebara", "Tony"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Shivaswamy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shivaswamy et al\\.", "year": 2006}, {"title": "What\u2019s hot at RoboCup", "author": ["Stone", "Peter"], "venue": "In The AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Stone and Peter.,? \\Q2016\\E", "shortCiteRegEx": "Stone and Peter.", "year": 2016}, {"title": "Multiagent systems: A survey from a machine learning perspective", "author": ["Stone", "Peter", "Veloso", "Manuela"], "venue": "Autonomous Robots,", "citeRegEx": "Stone et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Stone et al\\.", "year": 2000}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Order matters: Sequence to sequence for sets", "author": ["Vinyals", "Oriol", "Bengio", "Samy", "Kudlur", "Manjunath"], "venue": "arXiv preprint arXiv:1511.06391,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "(Bialkowski et al., 2014), now enables the possibility of learning multi-agent policies from demonstrations, also known as multi-agent imitation learning.", "startOffset": 0, "endOffset": 25}, {"referenceID": 9, "context": "Many realistic multi-agent settings require coordination among collaborative agents to achieve some common goal (Guestrin et al., 2002; Kok et al., 2003).", "startOffset": 112, "endOffset": 153}, {"referenceID": 19, "context": "Many realistic multi-agent settings require coordination among collaborative agents to achieve some common goal (Guestrin et al., 2002; Kok et al., 2003).", "startOffset": 112, "endOffset": 153}, {"referenceID": 24, "context": "Following the notation of (Ross et al., 2011), let ~\u03c0p~sq :\u201c ~a denote the joint policy that maps the joint state, ~s \u201c rs1, .", "startOffset": 26, "endOffset": 45}, {"referenceID": 4, "context": "First, beyond the challenges inherited from single-agent settings, multi-agent imitation learning must account for multiple simultaneously learning agents, which is known to cause non-stationarity for multi-agent reinforcement learning (Busoniu et al., 2008).", "startOffset": 236, "endOffset": 258}, {"referenceID": 13, "context": "For efficient training, we employ alternating stochastic optimization (Hoffman et al., 2013; Johnson & Willsky, 2014; Beal, 2003) on the same mini-batches.", "startOffset": 70, "endOffset": 129}, {"referenceID": 2, "context": "The algorithm optionally includes a mixing step on line 5, where the rolled-out trajectories may replace the training trajectories with increasing probability approaching 1, which is similar to scheduled sampling (Bengio et al., 2015), and may help stabilize learning in the early phase of the algorithm.", "startOffset": 213, "endOffset": 234}, {"referenceID": 24, "context": ",Dn (Ross et al., 2011).", "startOffset": 4, "endOffset": 23}, {"referenceID": 2, "context": "While LSTMs are very good at sequence-to-sequence prediction tasks, they cannot naturally deal with the drifting of input state distribution drift caused by action output feedback in dynamical systems (Bengio et al., 2015).", "startOffset": 201, "endOffset": 222}, {"referenceID": 4, "context": "This non-stationarity issue also arises in multiagent reinforcement learning (Busoniu et al., 2008) when agents learn simultaneously.", "startOffset": 77, "endOffset": 99}, {"referenceID": 20, "context": "In practice, dynamic oracle feedback is very expensive to obtain and some recent work have attempted to relax this requirement (Le et al., 2016; Ho & Ermon, 2016).", "startOffset": 127, "endOffset": 162}, {"referenceID": 24, "context": "The Appendix shows how to use DAgger (Ross et al., 2011) for Algorithm 2, which we used for our synthetic experiment.", "startOffset": 37, "endOffset": 56}, {"referenceID": 13, "context": "In particular, we employ techniques from stochastic variational inference (Hoffman et al., 2013), which allows for efficient stochastic training on mini-batches that can naturally integrate with our imitation learning subroutine.", "startOffset": 74, "endOffset": 96}, {"referenceID": 13, "context": "Assuming the usual conjugacy in the exponential family, the stochastic natural gradient takes the convenient form (Hoffman et al., 2013):", "startOffset": 114, "endOffset": 136}, {"referenceID": 24, "context": "The experts thus act as dynamic oracles, which result in a multi-agent training setting analogous to DAgger (Ross et al., 2011).", "startOffset": 108, "endOffset": 127}, {"referenceID": 3, "context": "In this experiment, we aim to learn multi-agent policies for team soccer defense, based on tracking data from real-life professional soccer (Bialkowski et al., 2014).", "startOffset": 140, "endOffset": 165}, {"referenceID": 24, "context": ", 2009) and DAgger (Ross et al., 2011), we gradually increase the horizon of the rolled-out trajectories from 1 to 10 steps lookahead.", "startOffset": 19, "endOffset": 38}, {"referenceID": 21, "context": "We note that learned policies are qualitatively similar to the ground truth demonstrations, and can be useful for applications such as counterfactual replay analysis (Le et al., 2017).", "startOffset": 166, "endOffset": 183}, {"referenceID": 29, "context": "Some recent work has called attention to the importance of order to learning when input or output are sets (Vinyals et al., 2015), motivated by classic algorithmic and geometric problems such as learning to sort a set of numbers, or finding convex hull for a set of points, where no clear indexing mechanism exists.", "startOffset": 107, "endOffset": 129}], "year": 2017, "abstractText": "We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often implicit in the demonstrations and must be inferred as a latent variable. We propose a joint approach that simultaneously learns a latent coordination model along with the individual policies. In particular, our method integrates unsupervised structure learning with conventional imitation learning. We illustrate the power of our approach on a difficult problem of learning multiple policies for finegrained behavior modeling in team sports, where different players occupy different roles in the coordinated team strategy. We show that having a coordination model to infer the roles of players yields substantially improved imitation loss compared to conventional baselines.", "creator": "LaTeX with hyperref package"}}}