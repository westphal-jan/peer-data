{"id": "1703.04664", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2017", "title": "Optimal Densification for Fast and Accurate Minwise Hashing", "abstract": "Minwise more-or-less entire a norms and four of an several successful parceling probabilistic in the philosophical. Recent despite based held the why according densification ~ \\ cite {Proc: OneHashLSH_ICML14, Proc: Shrivastava_UAI14} especially appearing that it for however to formula_3 $ k $ minwise socket, part a formula_22, $ d $ nonzeros, since height $ (richard + |) $ formulae, a significant meaningful saw the operatic $ O (dk) $. These losing among taking though was argumentation improvement and present templates complexity significant particular indexing algorithms based new minwise modalities. Unfortunately, , viscosity of beginning focus occurences and/or becomes insidious along, which ahead will significantly poor aptitude decline also vanilla minwise hashing, all when well determine is remarkably. In if paper, do provide put seminal autoethnography scheme much contributes from done casual 26 - interactive hashes. We `` turn beginning possible savings is amplitudes - algorithms, and clear falling the apis accelerate, entire is significantly more proven turn existing densification mechanisms. As that resulted, we payment full significantly competitive portioned scheme. the the almost correlation along collision variables while minwise helical. Experimental evaluations on real alluvial already than - constructs fault-tolerant disseminate always earlier. We believe only their three significant efficiency, anything technique will had minwise splays higher-level in course.", "histories": [["v1", "Tue, 14 Mar 2017 18:49:57 GMT  (734kb,D)", "http://arxiv.org/abs/1703.04664v1", "Fast Minwise Hashing"]], "COMMENTS": "Fast Minwise Hashing", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["anshumali shrivastava"], "accepted": true, "id": "1703.04664"}, "pdf": {"name": "1703.04664.pdf", "metadata": {"source": "META", "title": "Optimal Densification for Fast and Accurate Minwise Hashing", "authors": ["Anshumali Shrivastava"], "emails": ["ANSHUMALI@RICE.EDU"], "sections": [{"heading": "1. Introduction and Motivation", "text": "Recent years have witnessed a dramatic increase in the dimensionality of modern datasets. (Weinberger et al., 2009) show dataset with 16 trillion (1013) unique features. Many studies have shown that the accuracy of models keeps climbing slowly with exponential increase in dimensionality. Large dictionary based representation for images,\nspeech, and text are quite popular (Broder, 1997; Fetterly et al., 2003). Enriching features with co-occurrence information leads to blow up in the dimensionality. 5-grams are common for text representations. With vocabulary size of 106, 5-grams representation requires dimensionality of 1030. Representing genome sequences with features consisting of 32-contiguous characters (or higher) (Ondov et al., 2016) leads to around 432 = 264 dimensions.\nTo deal with the overwhelming dimensionality, there is an increased emphasis on the use of hashing algorithms, such as minwise hashing. Minwise hashing provides a convenient way to obtain a compact representation of the data, without worrying about the actual dimensionality. These compact representations are directly used in large scale data processing systems for a variety of tasks.\nMinwise hashing is defined for binary vectors. Binary vectors can also be equivalently viewed as sets, over the universe of all the features, containing only attributes corresponding to the non-zero entries. Minwise hashing belongs to the Locality Sensitive Hashing (LSH) family (Broder et al., 1998; Charikar, 2002). The method applies a random permutation (or random hash function) \u03c0 : \u2126 \u2192 \u2126, on the given set S \u2282 \u2126, and stores the minimum value after the permutation mapping. Formally,\nh\u03c0(S) = min(\u03c0(S)). (1)\nGiven sets S1 and S2, it can be shown by elementary probability arguments that\nPr(h\u03c0(S1) = h\u03c0(S2)) = |S1 \u2229 S2| |S1 \u222a S2| = R. (2)\nThe quantity\nR = |S1 \u2229 S2| |S1 \u222a S2| = a f1 + f2 \u2212 a , (3)\nis the well known Jaccard Similarity (or resemblance) R which is the most popular similarity measure in information retrieval applications (Broder, 1997).\nThe probability of collision (equality of hash values), under minwise hashing, is equal to the similarity of interest\nar X\niv :1\n70 3.\n04 66\n4v 1\n[ cs\n.D S]\n1 4\nM ar\n2 01\n7\nR. This particular property, also known as the LSH property (Indyk & Motwani, 1998; Charikar, 2002), makes minwise hash functions h\u03c0 suitable for creating hash buckets, which leads to sublinear algorithms for similarity search. Because of this same LSH property, minwise hashing is a popular indexing technique for a variety of large-scale data processing applications, which include duplicate detection (Broder, 1997; Henzinger, 2006), all-pair similarity (Bayardo et al., 2007), temporal correlation (Chien & Immorlica, 2005), graph algorithms (Buehrer & Chellapilla, 2008; Chierichetti et al., 2009; Najork et al., 2009), and more. It was recently shown that the LSH property of minwise hashes can be used to generate kernel features for large-scale learning (Li et al., 2011).\nMinwise hashing is known to be theoretical optimal in many scenarios (Bavarian et al., 2016). Furthermore, it was recently shown to be provably superior LSH for angular similarity (or cosine similarity) compared to widely popular Signed Random Projections (Shrivastava & Li, 2014b). These unique advantages make minwise hashing arguably the strongest hashing algorithm both in theory and practice.\nHashing Cost is Bottleneck: The first step of algorithms relying on minwise hashing is to generate, some large enough, k minwise hashes (or fingerprints) of the data vectors. In particular, for every data vector x, hi(x) \u2200i \u2208 {1, 2, ..., k} is repeatedly computed with independent permutations (or hash functions). These k hashes are used for a variety of data mining tasks such as cheap similarity estimation, indexing for sub-linear search, kernel features for large scale learning, etc. Computing k hashes of a vector x with traditional minwise hashing requires O(dk) computation, where d is the number of non-zeros in vector x. This computation of the multiple hashes requires multiple passes over the data. The number of required hashes typically ranges from few hundreds to several thousand. For example, the number of hashes required by the famous LSH algorithm is O(n\u03c1) which grows with the size of the data. (Li, 2015) showed the necessity of around 4000 hashes per data vector in large-scale learning. Hashing time is the main computational and resource bottleneck step in almost all applications using minwise hashing.\nOther Related Fast Sketches are not LSH: Two notable techniques for estimating Jaccard Similarity are: 1) bottom-k sketches and 2) one permutation hashing (Li et al., 2012). Although these two sketches are cheap to compute, they do not satisfy the key LSH property and therefore are unsuitable for replacing minwise hashing (Shrivastava & Li, 2014a;c). There are also substantial empirical evidence that using these (non-LSH) sketches for indexing leads to a drastic bias in the expected behavior, leading to poor accuracy.\nThe Idea of \u201cDensified\u201d One Permutation Hashing: Re-\ncently, (Shrivastava & Li, 2014a) showed a technique for densifying sparse sketches from one permutation hashing which provably removes the bias associated with one permutation hashing. Please see Section 3.3 for details. This was the first success in creating efficient hashing scheme which satisfies the LSH property analogous to minwise hashing and at the same time the complete process only requires O(d + k) computations instead of the traditional bottleneck of O(dk). Having such an efficient scheme directly translates into algorithmic improvements for a variety of machine learning and data mining tasks.\nCurrent Densification is Inaccurate For Very Sparse Datasets: The densification process although efficient and unbiased was shown to have unnecessarily higher variance. It was shown in (Shrivastava & Li, 2014c) that the traditional \u201cdensification\u201d lacks sufficient randomness. It was further revealed that densification could be provably improved by using k extra random bits. The improved scheme has reduced variance, and it retained the computational efficiency, see (Shrivastava & Li, 2014c) for more details. An improved variance was associated with a significant performance gain in the task of near-neighbor search. In this work, we show that even the improved densification scheme is far from optimal. The findings of (Shrivastava & Li, 2014c) leaves an open curiosity: What is the best variance that can be achieved with \u201cdensification\u201d without sacrificing the running time? We close this by providing a variance-optimal scheme.\nOur Contributions: We show that the existing densification schemes, for fast minwise hashing, are not only suboptimal but, worse, their variances do not go to zero with increasing number of hashes. The variance with an increase in the number of hashes converges to a positive constant. This behavior implies that increasing the number of hashes after a point will lead to no improvement, which is against the popular belief that accuracy of randomized algorithms keeps improving with an increase in the number of hashes.\nTo circumvent these issues we present a novel densification scheme which has provably superior variance compared to existing schemes. We show that our proposal has the optimal variance that can be achieved by densification. Furthermore, the variance of new methodology converges to zero with an increase in the number of hashes, a desirable behavior absent in prior works. Our proposal makes novel use of 2-universal hashing which could be of independent interest in itself. The benefits of improved accuracy come with no loss in computational requirements, and our scheme retains the running time efficiency of the densification.\nWe provide rigorous experimental evaluations of existing solutions concerning both accuracy and running time efficiency, on real high-dimensional datasets. Our experiments validate all our theoretical claims and show significant im-\nprovement in accuracy, comparable to minwise hashing, with a significant gain in computational efficiency."}, {"heading": "2. Important Notations and Concepts", "text": "Equation 2 (i.e. the LSH Property) leads to an estimator of Jacard Similarity R, using k hashes, defined by:\nR\u0302 = 1\nk k\u2211 i=1 1(hi(S1) = hi(S2)). (4)\nHere 1 is the indicator function. In the paper, by variance, we mean the variance of the above estimator. Notations like V ar(h+), will mean the variance of the above estimator when the h+ is used as the hash function.\n[k] will denote the set of integers {1, 2, ..., k}. n denotes the number of points (samples) in the dataset. D will be used for dimensionality. We will use min{S} to denote the minimum element of the set S. A permutation \u03c0 : \u2126 \u2192 \u2126 applied to a set S is another set \u03c0(S), where x \u2208 S if and only if \u03c0(x) \u2208 \u03c0(S). Our hashing will generate k hashes hi i \u2208 {1, 2, ..., k}, generally from different bins. Since they all have same distribution and properties we will drop subscripts. We will use h and h+ to denote the hashing schemes of (Shrivastava & Li, 2014a) and (Shrivastava & Li, 2014c) respectively."}, {"heading": "3. Background: Fast Minwise Hashing via Densification", "text": ""}, {"heading": "3.1. 2-Universal Hashing", "text": "Definitions: A randomized function huniv : [l] \u2192 [k] is 2-universal if for all, i, j \u2208 [l] with i 6= j, we have the following property for any z1, z2 \u2208 [k]\nPr(huniv(i) = z1 and huniv(j) = z2) = 1\nk2 (5)\n(Carter & Wegman, 1977) showed that the simplest way to create a 2-universal hashing scheme is to pick a prime number p \u2265 k, sample two random numbers a, b and compute\nhuniv(x) = ((ax+ b) mod p) mod k"}, {"heading": "3.2. One Permutation Hashing and Empty Bins", "text": "It was shown in (Li et al., 2012; Dahlgaard et al., 2015) that instead of computing the global minimum in Equation 2, i.e., h(S) = min(\u03c0(S)), an efficient way to generate k sketches, using one permutation, is to first bin the range space of \u03c0, i.e. \u2126, into k disjoint and equal partitions followed by computing minimum in each bin (or partition).\nLet \u2126i denote the ith partition of the range space of \u03c0, i.e. \u2126. Formally, the ith one permutation hashes (OPH) of a set\nS is defined as\nhOPHi (S) = { min{\u03c0(S) \u2229 \u2126i}, if {\u03c0(S) \u2229 \u2126i} 6= \u03c6 E, otherwise.\n(6)\nAn obvious computational advantage of this scheme is that it is likely to generate many hash values, at most k, and only requires one permutation \u03c0 and only pass over the sets (or binary vectors) S. It was shown that for any two sets S1 and S2 we have a conditional collision probability similar to minwise hashing.\nLet Ei = 1 { hOPHi (S2) = h OPH i (S2) = E } (7)\nPr ( hOPHi (S1) = h OPH i (S2) \u2223\u2223 Ei = 0) = R (8)\nHowever, Pr ( hOPHi (S1) = h OPH i (S2) \u2223\u2223 Ei = 1) 6= R (9)\nHere Ei is an indicator random variable of the event that the ith partition corresponding to both S1 and S2 are empty. See Figure 1\nAny bin has a constant chance of being empty. Thus, there is a positive probability of the event {Ei = 1}, for any given pair S1 and S2 and hence for large datasets (big n) a constant fraction of data will consist of simultaneously empty bins (there are n2 \u00d7 k trials for the bad event {Ei = 1} to happen). This fraction further increases significantly with the sparsity of the data and k, as both sparsity and k increases the probability of the bad event {Ei = 1}. See Table 4 for statistics of empty bins on real scenarios.\nUnfortunately, whenever the outcome of the random permutation leads to simultaneous empty bins, i.e. event Ei = 1, the LSH Property is not valid. In fact, there is not sufficient information present in the simultaneous empty partitions for any meaningful statistics. Hence, one permutation hashing cannot be used as an LSH. Simple heuristics of handling empty bins as suggested in (Shrivastava & Li, 2014a) leads to a significant bias and it was shown both theoretically and empirically that this bias leads to significant deviation from the expected behavior of one permutation hashing when compared with minwise hashing. Thus, one permutation hashing although computationally lucrative is not a suitable replacement for minwise hashing."}, {"heading": "3.3. The Idea of Densification", "text": "In (Shrivastava & Li, 2014a), the authors proposed \u201cdensification\u201d or reassignment of values to empty bins by reusing the information in the non-empty bins to fix the bias of one permutation hashing. The overall procedure is quite simple. Any empty bin borrows the values of the clos-\nest non-empty bins towards the circular right (or left)1. See Figure 1 for an illustration. Since the positions of empty and non-empty bins were random, it was shown that densification (or reassignment) was equivalent to a stochastic reselection of one hash from a set of existing informative (coming from non-empty bins) hashes which have the LSH property. This kind of reassignment restores the LSH property and collision probability for any two hashes, after reassignment, is exactly same as that of minwise hashing.\nThe densification generates k hashes with the required LSH Property and only requires two passes over the one permutation sketches making the total cost of one permutation\n1In (Shrivastava & Li, 2014a) they also needed an offset because the value of a hash in any bin was always reset between [0,k]. We do not need the offset if we use the actual values of \u03c0(S).\nhashing plus densification O(d+k). This was a significant improvement over O(dk) with classical minwise hashing. O(d+ k) led to an algorithmic improvement over randomized algorithms relying on minwise hashing, as hash computation cost is bottleneck step in all of them."}, {"heading": "3.4. Lack of Randomness in Densification", "text": "It was pointed out in (Shrivastava & Li, 2014c) that the densification scheme of (Shrivastava & Li, 2014a) has unnecessarily high variance. In particular, the probability of two empty bins borrowing the information of the same nonempty bin was significantly higher. This probability was due to poor randomization (load balancing) which hurts the variance. (Shrivastava & Li, 2014c) showed that infusing more randomness in the reassignment process by utilizing k extra random bits provably improves the variance. See Figure 1 for an example illustration of the method. The running time of the improved scheme was again O(d+ k) for computing k hashes. This improvement retains the required LSH property, however this time with improved variance. An improved variance led to significant savings in the task of near-neighbor search on real sparse datasets."}, {"heading": "4. Issues with Current Densification", "text": "Our careful analysis reveals that the variance, even with the improved scheme, is still significantly higher. Worse, even in the extreme case when we take k \u2192 \u221e the variance converges to a positive constant rather than zero, which implies that even with infinite samples, the variance will not be zero. This positive limit further increases with the sparsity of the dataset. In particular, we have the following theorem about the limiting variances of existing techniques:\nTheorem 1 Give any two finite sets S1, S2 \u2208 \u2126, with A = |S1 \u222a S2| > a = |S1 \u2229 S2| > 0 and |\u2126| = D \u2192 \u221e. The limiting variance of the estimators from densification and improved densification when k = D \u2192\u221e is given by:\nlim k\u2192\u221e\nV ar(h) = a\nA\n[ A\u2212 a\nA(A+ 1)\n] > 0 (10)\nlim k\u2192\u221e\nV ar(h+) = a\nA\n[ 3(A\u2212 1) + (2A\u2212 1)(a\u2212 1)\n2(A+ 1)(A\u2212 1) \u2212 a A\n] > 0\n(11)\nThis convergence of variance to a constant value, despite infinite samples, of the existing densification is also evident in our experimental findings (see Figure 3) where we observe that the MSE (Mean Square Error) curves go flat with increasing k. Similar phenomenon was also reported in (Shrivastava & Li, 2014c). It should be noted that for classical minwise hashing the variance is R(1\u2212R)k \u2192 0 for any pair S1 and S2. Thus, current densification, although fast, loses significantly in terms of accuracy. We remove this issue with densification. In particular, we show in The-\norem 6 that the limiting variance of the proposed optimal densification goes to 0. Our experimental findings suggests that the new variance is very close to the classical minwise hashing. In addition, the new densification retains the speed of existing densified hashing thereby achieving the best of the both worlds."}, {"heading": "5. Optimal Densification", "text": "We argue that even with the improved densification there is not enough randomness (or load balancing) in the reassignment process which leads to reduced variance.\nFor given set S, the densification process reassigns every empty bin with a value from one of the existing non-empty bins. Note, the identity of empty and non-empty bins are different for different sets. To ensure the LSH property, the re-assignment should be consistent for any given pair of sets S1 and S2. In particular, as noted in (Shrivastava & Li, 2014a), given any arbitrary pair S1 and S2, whenever any given bin i is simultaneously empty, i.e. Ei = 1, the reassignment of this bin i should mimic the collision probability of one of the simultaneously non-empty bin j with Ej = 0. An arbitrary reassignment (or borrow) of values will not ensure this consistency across all pairs. We would like to point out that the reassignment of S1 has no idea about S2 or any other object in the dataset. Thus, ensuring the consistency is non-trivial. Although the current densification schemes achieve this consistency by selecting the nearest non-empty bin (as shown in (Shrivastava & Li, 2014c)), they lack sufficient randomness."}, {"heading": "5.1. Intuition: Load Balancing", "text": "In Figure 2, observe that if there are many contiguous non-empty bins (Bins 2, 3 and 4), then with densification\nschemes h, all of them are forced to borrow values from the same non-empty bin (Bin 5 in the example). Even though there are other informative bins (Bins 1, 6 and 7), their information is never used. This local bias increases the probability (p) that two empty bins get tied to the same information, even if there are many other informative non-empty bins. Adding k random bits improves this to some extent by allowing load sharing between the two ends instead of one (Bins 1 and 5 instead of just 5). However, the load balancing is far from optimal. The locality of information sharing is the main culprit with current densification schemes. Note, the poor load balancing does not change the expectation but affects the variance significantly.\nFor any given pairs of vectors S1 and S2, let m be the number of simultaneous non-empty bins (out of k), i.e.\u2211k i=1Ei = k \u2212 m. Note, m is a random variable whose value is different for every pair and depends on the outcome of random \u03c0. Formally, the variance analysis of (Shrivastava & Li, 2014c) reveals that the probability that any two simultaneous empty bin p and q (Ep = Eq = 1) reuses the same information is 2m+1 with the densification scheme h. This probability was reduced down to 1.5m+1 with h\n+ by utilizing k extra random bits to promote load balancing.\np = 1.5m+1 is not quite perfect load balancing. In a perfect load balancing with m simultaneous non-empty bins, the probability of two empty bins hitting the same nonempty bins is at best p = 1m . Can we design a densification scheme which achieves this pwhile maintaining the consistency of densification and at the same time does not hurt the running time? It is not clear if such a scheme even exists. We answer this question positively by constructing a densification method with precisely all the above requirements. Furthermore we show that achieving p = 1m is sufficient for having the limiting variance of zero."}, {"heading": "5.2. Simple 2-Universal Hashing Doesn\u2019t Help", "text": "To break the locality of the information reuse and allow a non-empty bin to borrow information from any other far off bin consistently, it seems natural to use universal hashing. The hope is to have a 2-universal hash function (Section 3.1) huniv : [k] \u2192 [k]. Whenever a bin i is empty, instead of borrowing information from neighbors, borrow information from bin huniv(i). The hash function allows consistency across any two S1 and S2 hence preserves LSH property. The value of huniv(i) is uniformly distributed, so any bin is equally likely. Thus, it seems to break the locality on the first thought. If huniv(i) is also empty then we continue using huniv(huniv(i)) until we reach a non-empty bins whose value we re-use.\nOne issue is that of cycles. If i = huniv(i) (which has 1k chance), then this creates a cycle and the assign-\nment will go into infinite loop. A cycle can even occur if huniv(huniv(i)) = i with both i and huniv(i) being empty. Note, the process runs until it finds a non-empty bin. However, cycles are not just our concern. Even if we manage to get away with cycles, this scheme does not provide the required load balancing.\nA careful inspection reveals that there is a very significant chance that both i and huniv(i) to be empty for any given set S. Observe that if i and huniv(i) are both empty, then we are bound to reuse the information of the same nonempty bin for both empty bins i and huniv(i). We should note that we have no control over the positions of empty and non-empty bins. In fact, if no cycles happen then it is not difficult to show that the simple assignment using universal hashing is equivalent to the original densification h with the order of bins reshuffled using huniv(.). It has worse variance than h+."}, {"heading": "5.3. The Fix: Carefully Tailored 2-Universal Hashing", "text": "Algorithm 1 Optimal Densification input k One Permutation Hashes hOPH [ ] of S. input huniv(., .)\nInitialize h\u2217[ ] = 0 for i = 1 to k do\nif OPH[i] 6= E then h\u2217[i] = hOPH [i] else attempt = 1 next = huniv(i, attempt) while hOPH [next] 6= E do attempt+ + next = huniv(i, attempt)\nend while h\u2217[i] = hOPH [next]\nend if end for RETURN h\u2217[ ]\nIt turns out that there is a way to use universal hashing that ensures no cycles as well as optimal load balancing. We describe the complete process in Algorithm 1. The key is to use a 2-universal hashing huniv : [k] \u00d7 N \u2192 [k] which takes two arguments: 1) The current bin id that needs to be reassigned and 2) the number of failed attempt made so far to reach a non-empty bin. This second argument ensures no infinite loops as it changes with every attempt. So even if we reach the same non-empty bin back (cycle), the next time we will visit a new set of bins. Also, even if both i and j = huniv(i, attempti) are empty, i and j are not bound to end to the same non-empty bin. This is because in the next attempt we seek bin value huniv(i, attempti + 1) for\ni which is independent of the huniv(j, attemptj) due to 2-universality of the hash function huniv . Thus, the probability that any two empty bins reuse the information of the same non-empty bin is 1m"}, {"heading": "5.4. Analysis and Optimality", "text": "We denote the final k hashes generated by the proposed densification scheme of Algorithm 1 using h\u2217 (* for optimality). Formally, with the optimal densification h\u2217, we have the following:\nTheorem 2\nPr ( h\u2217(S1) = h \u2217(S2) )\n= |S1 \u2229 S2| |S1 \u2229 S2| = R (12)\nV ar(h\u2217) = R\nk +A\nR k2 +B RR\u0304 k2 \u2212R2 (13)\nlim k\u2192\u221e\nV ar(h\u2217) = 0 (14)\nwhere Nemp is the number of simultaneous empty bins between S1 and S2 and the quantities A and B are given by\nA = E [ 2Nemp +\nNemp(Nemp \u2212 1) k \u2212Nemp ] B = E [ (k \u2212Nemp)(k \u2212Nemp \u2212 1) + 2Nemp(k \u2212Nemp \u2212 1)\n+ Nemp(Nemp \u2212 1)(k \u2212Nemp \u2212 1)\nk \u2212Nemp\n]\nUsing the formula for Pr(Nemp = i) from (Li et al., 2012), we can precisely compute the theoretical variance. The interesting part is that we can formally show that the variance of the proposed scheme is strictly superior compared to the densification scheme with random bits improvements.\nTheorem 3\nV ar(h\u2217) \u2264 V ar(h+) \u2264 V ar(h) (15)\nFinally, due to optimal pairwise load balancing the variance is the best possible we can hope with independent reassignments. Formally, we have\nTheorem 4 Among all densification schemes, where the reassignment process for bin i is independent of the reassignment process of any other bin j, Algorithm 1 achieves the best possible variance.\nNote: The variance can be reduced if we allow correlations in the assignment process, for example if we force bin i and bin j to not pick the same bin during reassignments, this will reduce p beyond the perfectly random load balancing value of 1m . However, such tied reassignment will require more memory and computations for generating structured hash functions."}, {"heading": "5.5. Running Time", "text": "We show that the expected running time of our proposal, including all constants, is very similar to the running time of the existing densification schemes.\nGiven set S with |S| = d, we are interested in computing k hash values. The first step involves computing k one permutation hashes (or sketches) which only requires a single pass over the elements of S. This takes max{d, k} \u2264 d+k time. Now the densification Algorithm 1 requires a for loop of size k and within each for loop, if the bin is empty, it requires an additional while loop. Let Nemp be the number of empty bins, and therefore k\u2212Nempk is the probability that the while loop will terminate in one iteration (next is not empty). Therefore, the expected number of iteration that each while loop will run is a binomial random variable with expectation kk\u2212Nemp . Thus, the expected running time of the algorithm is given by\nE[Running Time] = d+ 2k \u2212Nemp +Nemp k\nk \u2212Nemp\n\u2264 d+ (r + 2)k where r = Nemp Nnot\u2212emp\nThe quantity r, which is the ratio of number of empty bins to the number of non-empty bins, is generally very small. It is rarely more than 2 to 3 in practice. Observe that randomly throwing d items into k bins, the expected number of empty bins is E[Nemp] \u2248 k(1 \u2212 1k )\nd \u2248 ke\u2212 dk . Which makes r \u2248 e \u2212d/k\n1\u2212e\u2212d/k . The number of sketches is usually of the order of non-zeros. Even for very good concentration, the size of the sketches k is rarely much larger than the size of the set d. Even when k is 4 times d the value of r is approximately 3.5. Thus, the quantity r is negligible.\nIt should be further noted that the implementation cost of densification scheme h+ is d+ 4k which in not very different from the cost of our proposal."}, {"heading": "6. Evaluations", "text": "Our aim is to verify the theoretical claims of these papers empirically. We show that our proposal can replace minwise hashing for all practical purposes. To establish that, we focus on experiments with the following objectives:\n1. Verify that our proposed scheme has a significantly better accuracy (variance) than the existing densification schemes. Validate our variance formulas. 2. Empirically quantify the impact of optimal variance in practice. How does this quantification change with similarity and sparsity? Verify that the proposal has accuracy close to vanilla minwise hashing. 3. Verify that there is no impact on running time of the proposed scheme over existing densification schemes, and our proposal is significantly faster than vanilla minwise hashing. Understand how the running time changes with change in sparsity and k?"}, {"heading": "6.1. Accuracy", "text": "For objectives 1 and 2, we selected 9 different word pairs embedding, generated from new20 corpus, with varying level of similarity and sparsity. We use the popular termdocument vector representation for each word. The statistics of these word vector pairs are summarized in Table 1\nFor each word pairs, we generated k hashes using three different schemes: 1) Densification h, 2) Improved Densification h+ and the proposed densification h\u2217 (Algorithm 1). Using these hashes, we estimate the Jaccard similarity (Equation 4). We plot the mean square error (MSE) with varying the number of hashes. Since the process is randomized, we repeat the process 5000 times, for every k, and report the average over independent runs. We report all integer values of k in the interval [1, 214].\nIt should be noted that since all three schemes have the LSH Property, the bias is zero and hence the MSE is the theoretical variance. To validate our variance formula, we also compute and plot the theoretical value of the variance (Equation 19) of the optimal scheme. Also, to understand how all these fast methodologies compare with the accuracy of vanilla minwise hashing we also plot the theoretical variance of minwise hashing which is R(1\u2212R)k .\nFrom the results in Figure 3, we can conclude. Conclusion 1: The proposed densification is significantly more accurate, irrespective of the choice of sparsity and similarity, than the existing densification schemes especially for large k. Note the y-axis of plots is on log scale, so the accuracy gains are drastic. Conclusion 2: The gains with optimal densification is more for sparse data. Conclusion 3: The accuracy of optimal densification is very close the accuracy of costly minwise hashing.\nConclusion 4: The theoretical variance of our proposal overlaps with the empirical estimates, and it seems to go to zero validating Theorem 19. Conclusion 5: The variances (or the MSE) of existing densification seems to converge to constant and do not go to zero confirming Theorem 5."}, {"heading": "6.2. Speed", "text": "To compute the runtime, we use three publicly available text datasets: 1) RCV1, 2) URL and 3) News20. The dimensionality and sparsity of these datasets are an excellent representative of the scale and the size frequently encountered in large-scale data processing systems, such as Google\u2019s SIBYL (Chandra et al., 2010). The statistics of these datasets are summarized in Table 3.\nWe implemented three methodologies for computing hashes: 1) Densification Scheme h+, 2) The Proposed h\u2217 (Algorithm 1 and 3) Vanilla Minwise Hashing. The methods were implemented in C++. Cheap hash function replaced costly permutations. Clever alternatives to avoid mod operations were employed. These tricks ensured that our implementations2 are as efficient as the possible. We compute the wall clock time required to calculate 100, 200 and 300 hashes of all the three datasets. The time include the end-to-end hash computation of the complete data. Data loading time is not included. The results are presented in Table 2. All the experiments were done on Intel i7-6500U processor laptop with 16GB RAM.\nAlso, to get an estimate of the importance of densification, we also show the average number of empty bins generated\n2Codes are available at http://rush.rice.edu/ fastest-minwise.html\nby only using one permutation hashing and report the numbers in Table 4. We can clearly see that the number of empty bins is significantly larger and the hashes are unusable without densification.\nFrom the running time numbers Table 2, we conclude:\nConclusion 1: Optimal densification is as fast as traditional densification irrespective of k and the sparsity. However, optimal densification is significantly more accurate. Conclusion 2: Both the densification scheme is significantly faster than minwise hashing. They are 10-18x faster for computing 300 hashes on the selected datasets.\nGiven the simplicity, we hope our work gets adopted."}, {"heading": "A. Proofs", "text": "Theorem 5 Give any two finite sets S1, S2 \u2208 \u2126, with A = |S1 \u222a S2| > a = |S1 \u2229 S2| > 0 and |\u2126| = D \u2192 \u221e. The limiting variance of the estimators from densification and improved densification when k = D \u2192\u221e is given by:\nlim k\u2192\u221e\nV ar(h) = a\nA\n[ A\u2212 a\nA(A+ 1)\n] > 0 (16)\nlim k\u2192\u221e\nV ar(h+) = a\nA\n[ 3(A\u2212 1) + (2A\u2212 1)(a\u2212 1)\n2(A+ 1)(A\u2212 1) \u2212 a A\n] > 0\n(17)\nProof: When k = D, then Nemp = D \u2212 A. Substituting this value in the variance formulas from (Shrivastava & Li, 2014c) and taking the limit as D = k \u2192 \u221e, we get the above expression after manipulation. When 0 < R = aA < 1, they both are strictly positive.\nTheorem 6\nPr ( h\u2217(S1) = h \u2217(S2) )\n= |S1 \u2229 S2| |S1 \u2229 S2| = R (18)\nV ar(h\u2217) = R\nk +A\nR k2 +B RR\u0304 k2 \u2212R2 (19)\nlim k\u2192\u221e\nV ar(h\u2217) = 0 (20)\nwhere Nemp is the number of simultaneous empty bins between S1 and S2 and the quantities A and B are given by A = E [ 2Nemp +\nNemp(Nemp \u2212 1) k \u2212Nemp ] B = E [ (k \u2212Nemp)(k \u2212Nemp \u2212 1) + 2Nemp(k \u2212Nemp \u2212 1)\n+ Nemp(Nemp \u2212 1)(k \u2212Nemp \u2212 1)\nk \u2212Nemp\n]\nProof:\nThe collision probability is easy using a simple observation that values coming from different bin numbers can never match across S1 and S2, i.e. h\u2217i (Si) 6= h\u2217j (S2) if i 6= j, as they have disjoint different range. So whenever, for a simultaneous empty bin i, i.e. Ei = 1, we get h\u2217i (S1) = h \u2217 i (S2) after reassignment, the value must be coming from same non-empty bin, say numbers k which is not not empty. Thus,\nPr(h\u2217i (S1) = h \u2217 i (S2)) = Pr(h \u2217 k(S1) = h \u2217 k(S2)|Ek = 0) = R\nThe variance is little involved. From the collision probability, we have the following is unbiased estimator.\nR\u0302 = 1\nk k\u22121\u2211 j=0 1{h\u2217j (S1) = h\u2217j (S2)}. (21)\nFor variance, define the number of simultaneously empty bins by\nNemp = k\u22121\u2211 j=0 1{Ej = 1}, (22)\nwhere 1 is the indicator function. We partition the event( h\u2217j (S1) = h \u2217 j (S2) ) into two cases depending on Ej . Let MNj (Non-empty Match at j) and MEj (Empty Match at j) be the events defined as:\nMNj = 1{Ej = 0 and h\u2217j (S1) = h\u2217j (S2)} (23) MEj = 1{Ej = 1 and h\u2217j (S1) = h\u2217j (S2)} (24)\nNote that, MNj = 1 =\u21d2 MEj = 0 and MEj = 1 =\u21d2 MNj = 0. From the LSH property of estimator we have\nE(MNj |Ej = 0) = E(MEj |Ej = 1) = E(MEj +MNj ) = R \u2200j (25)\nIt is not difficult to show that, E ( MNj M N i \u2223\u2223i 6= j, Ej = 0 and Ei = 0) = RR\u0303, where R\u0303 = a\u22121f1+f2\u2212a\u22121 . Using these new events, we have\nR\u0302 = 1\nk k\u22121\u2211 j=0 [ MEj +M N j ] (26)\nWe are interested in computing\nV ar(R\u0302) = E  1 k k\u22121\u2211 j=0 [ MEj +M N j ]2 \u2212R2 (27)\nFor notational convenience we will use m to denote the event k \u2212 Nemp = m, i.e., the expression E(.|m) means E(.|k\u2212Nemp = m). To simplify the analysis, we will first compute the conditional expectation\nf(m) = E  1 k k\u22121\u2211 j=0 [ MEj +M N j ]2 \u2223\u2223\u2223\u2223m  (28)\nBy expansion and linearity of expectation, we obtain\nk2f(m) = E \u2211 i 6=j MNi M N j \u2223\u2223\u2223\u2223m + E \u2211 i6=j MNi M E j \u2223\u2223\u2223\u2223m \n+E \u2211 i 6=j MEi M E j \u2223\u2223\u2223\u2223m + E[ k\u2211 i=1 [ (MNj ) 2 + (MEj ) 2 ] \u2223\u2223\u2223\u2223m ]\nMNj = (M N j ) 2 and MEj = (M E j ) 2 as they are indicator functions and can only take values 0 and 1. Hence,\nE k\u22121\u2211 j=0 [ (MNj ) 2 + (MEj ) 2 ] \u2223\u2223\u2223\u2223m  = kR (29) The values of the first three terms are given by the following 3 expression using simple binomial enpension and using the fact that we are dealing with indicator random variable which can only take values 0 or 1.\nE \u2211 i 6=j MNi M N j \u2223\u2223\u2223\u2223m  = m(m\u2212 1)RR\u0303 (30)\nE \u2211 i 6=j MNi M E j \u2223\u2223\u2223\u2223m  = 2m(k \u2212m)[R m + (m\u2212 1)RR\u0303 m ] (31)\nLet p be the probability that two simultaneously empty bins i and j finally picks the same non-empty bin for reassignment. Then we have\nE \u2211 i 6=j MEi M E j \u2223\u2223\u2223\u2223m  = (k \u2212m)(k \u2212m\u2212 1) [pR+ (1\u2212 p)RR\u0303]\n(32)\nbecause with probability (1 \u2212 p), it uses estimators from different simultaneous non-empty bin and in that case the MEi M E j = 1 with probability RR\u0303. We know that Algorithm 1 which uses 2-universal hashing the value of p = 1m . This is because any pairwise assignment is perfectly random with 2-universal hashing.\nSubstituting for all terms with value of p and rearranging terms gives the required expression.\nWhen k = D, thenNemp = D\u2212A. Substituting this value in the variance formulas and taking the limit as D = k \u2192 \u221e, we get 0 for all R.\nTheorem 7\nV ar(h\u2217) \u2264 V ar(h+) \u2264 V ar(h) (33)\nProof: We have p\u2217 = 1m \u2264 p + = 1.5m+1 \u2264 p = 2 m+1 . The value of p+ and p comes from analysis in (Shrivastava & Li, 2014c)\nTheorem 8 Among all densification schemes, where the reassignment process for bin i is independent of the reassignment process of any other bin j, Algorithm 1 achieves the best possible variance.\nUnder any independent re-assigment, the probability that two empty bins chooses the same non-empty bin out of m non-empty bins is lower bounded by 1m which is achieved by optimal densification."}], "references": [{"title": "The optimality of correlated sampling", "author": ["Bavarian", "Mohammad", "Ghazi", "Badih", "Haramaty", "Elad", "Kamath", "Pritish", "Rivest", "Ronald L", "Sudan", "Madhu"], "venue": "CoRR, abs/1612.01041,", "citeRegEx": "Bavarian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bavarian et al\\.", "year": 2016}, {"title": "Scaling up all pairs similarity search", "author": ["Bayardo", "Roberto J", "Ma", "Yiming", "Srikant", "Ramakrishnan"], "venue": "In WWW, pp", "citeRegEx": "Bayardo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bayardo et al\\.", "year": 2007}, {"title": "On the resemblance and containment of documents", "author": ["Broder", "Andrei Z"], "venue": "In the Compression and Complexity of Sequences,", "citeRegEx": "Broder and Z.,? \\Q1997\\E", "shortCiteRegEx": "Broder and Z.", "year": 1997}, {"title": "Min-wise independent permutations", "author": ["Broder", "Andrei Z", "Charikar", "Moses", "Frieze", "Alan M", "Mitzenmacher", "Michael"], "venue": "In STOC,", "citeRegEx": "Broder et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Broder et al\\.", "year": 1998}, {"title": "A scalable pattern mining approach to web graph compression with communities", "author": ["Buehrer", "Gregory", "Chellapilla", "Kumar"], "venue": "In WSDM,", "citeRegEx": "Buehrer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Buehrer et al\\.", "year": 2008}, {"title": "Universal classes of hash functions", "author": ["Carter", "J. Lawrence", "Wegman", "Mark N"], "venue": "In STOC, pp", "citeRegEx": "Carter et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Carter et al\\.", "year": 1977}, {"title": "Sibyl: a system for large scale machine learning", "author": ["Chandra", "Tushar", "Ie", "Eugene", "Goldman", "Kenneth", "Llinares", "Tomas Lloret", "McFadden", "Jim", "Pereira", "Fernando", "Redstone", "Joshua", "Shaked", "Tal", "Singer", "Yoram"], "venue": "Technical report,", "citeRegEx": "Chandra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chandra et al\\.", "year": 2010}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Charikar", "Moses S"], "venue": "In STOC,", "citeRegEx": "Charikar and S.,? \\Q2002\\E", "shortCiteRegEx": "Charikar and S.", "year": 2002}, {"title": "Semantic similarity between search engine queries using temporal correlation", "author": ["Chien", "Steve", "Immorlica", "Nicole"], "venue": "In WWW, pp", "citeRegEx": "Chien et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chien et al\\.", "year": 2005}, {"title": "On compressing social networks", "author": ["Chierichetti", "Flavio", "Kumar", "Ravi", "Lattanzi", "Silvio", "Mitzenmacher", "Michael", "Panconesi", "Alessandro", "Raghavan", "Prabhakar"], "venue": "In KDD,", "citeRegEx": "Chierichetti et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chierichetti et al\\.", "year": 2009}, {"title": "Hashing for statistics over kpartitions", "author": ["Dahlgaard", "S\u00f8ren", "Knudsen", "Mathias B\u00e6k Tejs", "Rotenberg", "Eva", "Thorup", "Mikkel"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Dahlgaard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dahlgaard et al\\.", "year": 2015}, {"title": "A large-scale study of the evolution of web pages", "author": ["Fetterly", "Dennis", "Manasse", "Mark", "Najork", "Marc", "Wiener", "Janet L"], "venue": "In WWW,", "citeRegEx": "Fetterly et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Fetterly et al\\.", "year": 2003}, {"title": "Finding near-duplicate web pages: a large-scale evaluation of algorithms", "author": ["Henzinger", "Monika Rauch"], "venue": "In SIGIR,", "citeRegEx": "Henzinger and Rauch.,? \\Q2006\\E", "shortCiteRegEx": "Henzinger and Rauch.", "year": 2006}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["Indyk", "Piotr", "Motwani", "Rajeev"], "venue": "In STOC,", "citeRegEx": "Indyk et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Indyk et al\\.", "year": 1998}, {"title": "0-bit consistent weighted sampling", "author": ["Li", "Ping"], "venue": "In KDD,", "citeRegEx": "Li and Ping.,? \\Q2015\\E", "shortCiteRegEx": "Li and Ping.", "year": 2015}, {"title": "Hashing algorithms for largescale learning", "author": ["Li", "Ping", "Shrivastava", "Anshumali", "Moore", "Joshua", "K\u00f6nig", "Arnd Christian"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "One permutation hashing", "author": ["Li", "Ping", "Owen", "Art B", "Zhang", "Cun-Hui"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Less is more: sampling the neighborhood graph makes salsa better and faster", "author": ["Najork", "Marc", "Gollapudi", "Sreenivas", "Panigrahy", "Rina"], "venue": "In WSDM,", "citeRegEx": "Najork et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Najork et al\\.", "year": 2009}, {"title": "Mash: fast genome and metagenome distance estimation using minhash", "author": ["Ondov", "Brian D", "Treangen", "Todd J", "Melsted", "P\u00e1ll", "Mallonee", "Adam B", "Bergman", "Nicholas H", "Koren", "Sergey", "Phillippy", "Adam M"], "venue": "Genome Biology,", "citeRegEx": "Ondov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ondov et al\\.", "year": 2016}, {"title": "Densifying one permutation hashing via rotation for fast near neighbor search", "author": ["Shrivastava", "Anshumali", "Li", "Ping"], "venue": null, "citeRegEx": "Shrivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shrivastava et al\\.", "year": 2014}, {"title": "In defense of minhash over simhash", "author": ["Shrivastava", "Anshumali", "Li", "Ping"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Shrivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shrivastava et al\\.", "year": 2014}, {"title": "Improved densification of one permutation hashing", "author": ["Shrivastava", "Anshumali", "Li", "Ping"], "venue": null, "citeRegEx": "Shrivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shrivastava et al\\.", "year": 2014}, {"title": "Substituting this value in the variance formulas from (Shrivastava & Li, 2014c) and taking the limit as D = k \u2192 \u221e, we get the above expression after manipulation", "author": ["A. then Nemp = D"], "venue": null, "citeRegEx": "D and \u2212,? \\Q2014\\E", "shortCiteRegEx": "D and \u2212", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Large dictionary based representation for images, speech, and text are quite popular (Broder, 1997; Fetterly et al., 2003).", "startOffset": 85, "endOffset": 122}, {"referenceID": 18, "context": "Representing genome sequences with features consisting of 32-contiguous characters (or higher) (Ondov et al., 2016) leads to around 4 = 2 dimensions.", "startOffset": 95, "endOffset": 115}, {"referenceID": 3, "context": "Minwise hashing belongs to the Locality Sensitive Hashing (LSH) family (Broder et al., 1998; Charikar, 2002).", "startOffset": 71, "endOffset": 108}, {"referenceID": 1, "context": "Because of this same LSH property, minwise hashing is a popular indexing technique for a variety of large-scale data processing applications, which include duplicate detection (Broder, 1997; Henzinger, 2006), all-pair similarity (Bayardo et al., 2007), temporal correlation (Chien & Immorlica, 2005), graph algorithms (Buehrer & Chellapilla, 2008; Chierichetti et al.", "startOffset": 229, "endOffset": 251}, {"referenceID": 9, "context": ", 2007), temporal correlation (Chien & Immorlica, 2005), graph algorithms (Buehrer & Chellapilla, 2008; Chierichetti et al., 2009; Najork et al., 2009), and more.", "startOffset": 74, "endOffset": 151}, {"referenceID": 17, "context": ", 2007), temporal correlation (Chien & Immorlica, 2005), graph algorithms (Buehrer & Chellapilla, 2008; Chierichetti et al., 2009; Najork et al., 2009), and more.", "startOffset": 74, "endOffset": 151}, {"referenceID": 15, "context": "It was recently shown that the LSH property of minwise hashes can be used to generate kernel features for large-scale learning (Li et al., 2011).", "startOffset": 127, "endOffset": 144}, {"referenceID": 0, "context": "Minwise hashing is known to be theoretical optimal in many scenarios (Bavarian et al., 2016).", "startOffset": 69, "endOffset": 92}, {"referenceID": 16, "context": "Other Related Fast Sketches are not LSH: Two notable techniques for estimating Jaccard Similarity are: 1) bottom-k sketches and 2) one permutation hashing (Li et al., 2012).", "startOffset": 155, "endOffset": 172}, {"referenceID": 16, "context": "It was shown in (Li et al., 2012; Dahlgaard et al., 2015) that instead of computing the global minimum in Equation 2, i.", "startOffset": 16, "endOffset": 57}, {"referenceID": 10, "context": "It was shown in (Li et al., 2012; Dahlgaard et al., 2015) that instead of computing the global minimum in Equation 2, i.", "startOffset": 16, "endOffset": 57}, {"referenceID": 16, "context": "Using the formula for Pr(Nemp = i) from (Li et al., 2012), we can precisely compute the theoretical variance.", "startOffset": 40, "endOffset": 57}, {"referenceID": 16, "context": "Number of Empty Bins per vector (rounded) generated with One Permutation Hashing (Li et al., 2012).", "startOffset": 81, "endOffset": 98}, {"referenceID": 6, "context": "The dimensionality and sparsity of these datasets are an excellent representative of the scale and the size frequently encountered in large-scale data processing systems, such as Google\u2019s SIBYL (Chandra et al., 2010).", "startOffset": 194, "endOffset": 216}], "year": 2017, "abstractText": "Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification (Shrivastava & Li, 2014a;c) have shown that it is possible to compute k minwise hashes, of a vector with d nonzeros, in mere (d + k) computations, a significant improvement over the classical O(dk). These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and highdimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.", "creator": "LaTeX with hyperref package"}}}