{"id": "1703.04914", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Ensemble of Neural Classifiers for Scoring Knowledge Base Triples", "abstract": "This by describes our approach for before triple scoring establish at WSDM Cup 30-31. The undertake aims to instructions takes relevance score including and break of entities bringing giving distinct entire a nature base until return to safeguard the 6th results another entity on-line tasks. We propose taken flexible \u201c the modulation of multiple neural station pdes unlike less direct called voluntary machine studies definition. The experimental given show not our opposed method satisfying its professional both in giving break of three measures, including improvisational trialed following the other included measures.", "histories": [["v1", "Wed, 15 Mar 2017 04:00:27 GMT  (44kb)", "https://arxiv.org/abs/1703.04914v1", null], ["v2", "Wed, 5 Apr 2017 13:58:02 GMT  (45kb)", "http://arxiv.org/abs/1703.04914v2", "WSDM Cup 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ikuya yamada", "motoki sato", "hiroyuki shindo"], "accepted": false, "id": "1703.04914"}, "pdf": {"name": "1703.04914.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ikuya Yamada", "Motoki Sato", "Hiroyuki Shindo"], "emails": ["ikuya@ousia.jp", "sato.motoki.sa7@is.naist.jp", "shindo@is.naist.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n04 91\n4v 2\n[ cs\n.C L\n] 5\nA pr\n2 01\nThis paper describes our approach for the triple scoring task at the WSDM Cup 2017. The task required participants to assign a relevance score for each pair of entities and their types in a knowledge base in order to enhance the ranking results in entity retrieval tasks. We propose an approach wherein the outputs of multiple neural network classifiers are combined using a supervised machine learning model. The experimental results showed that our proposed method achieved the best performance in one out of three measures (i.e., Kendall\u2019s \u03c4 ), and performed competitively in the other two measures (i.e., accuracy and average score difference)."}, {"heading": "1. INTRODUCTION", "text": "In the last decade, huge online structured knowledge bases (KBs) such asWikidata [11], Freebase [4], and DBpedia [1] have emerged. These KBs contain an enormous number of entities (e.g., people) and their types (e.g., professions and nationalities).1 These data enable users to easily formulate a complex query to a KB such as querying a list of all scientists who are nationals of Japan.\nHowever, the KB also contains many entity types that are rarely useful for humans when querying a KB. For example, Barack Obama has four professions listed in Freebase, namely Politician, Lawyer, Law professor, and Author, but it is considered that people primarily want to retrieve Barack Obama as a Politician.\nRecently, Bast et al. [2] addressed this problem by assigning a relevance score to each pair consisting of an entity and its type in KB. These scores enable us to enhance the ranking results of entity retrieval tasks by sorting the results based on these relevance scores.\nIn this paper, we describe our approach for this task. We use multiple neural network classifiers with the objective of predicting the probability of an entity type when a KB entity is given. Notably, we introduce an attention mechanism to our neural network model in order to enable the model to prioritize a small number of relevant features. In addition, we use another supervised machine learning model (i.e., gradient boosted regression trees (GBRT) [6]) to convert the outputs of these classifiers into the final relevance scores.\nThe proposed method was applied to the triple scoring task at the WSDM Cup 2017 [3, 7] The results demonstrated that our method achieved the best results in one out of three measures (i.e.,\n1Entities and their types can be easily extracted from KB triples where their subjects refer to entities and their objects are the corresponding types. Here, the target triple is a triple describing a relation of which the object can be one among a limited set of values such as the nationalities of people.\nKendall\u2019s \u03c4 ), and exhibited competitive performance in the other two measures (i.e., accuracy and average score difference)."}, {"heading": "2. OUR APPROACH", "text": "Given a KB entity e and its target type t, our method predicts a score that represents the relevance of e belonging to t. Here, we adopt a two-step approach: the first step is a classification step that aims to estimate the probability of e belonging to t (P (t|e)) using multiple neural network-based classifiers. The second step is a scoring step that uses a supervised machine learning model to convert the outputs of these classifiers to the target relevance score. In accordance with the task specifications for WSDM Cup 2017, our model assigns relevance scores to pairs of people and their professions, and people and their nationalities."}, {"heading": "2.1 Classification Step", "text": "We train the classifier by using all the KB entities that only have a single type, as in the previous work by Bast et al. [2]. This configuration enables us to address this problem as a multi-class classification of entities over all possible types. It is important to note that, because our objective is assigning relevance scores to entities with multiple types, entities with only a single type can be safely used as training data.\n2.1.1 Model\nWe use sets of words and entities that are relevant to e as inputs to the classifier. We adopt the neural bag-of-items model with a simple item-level attention mechanism [9] to derive the representation of the set of items (i.e., words or entities). Specifically, given a set of items, x1, x2, ..., xN , we first compute the weighted sum of their corresponding embedding as follows:\nc = N\u2211\ni=1\na(xi)vxi , (1)\nHere, vx \u2208 R dw is an embedding of x, and a(x) is a function that computes the item-level attention weight for x, which is defined as the following softmax function:\na(x) = exp(wa \u22a4 ux + ba)\u2211N\nj=1 exp(wa \u22a4 uxj + ba)\n, (2)\nwhere wa \u2208 R da is a weight vector, ba \u2208 R is a bias, and ux \u2208 R da is an attention embedding of x. The function a(x) aims to capture the importance of the item x, thereby allowing the model to focus on a small number of relevant items.\nFinally, we adopt a multi-layer perceptron (MLP) classifier with a single hidden layer with l units, ReLU non-linearity, and dropout with a probability p. Using Eq. (1), we compute two feature vectors cw and ce using the sets of words and entities, respectively. We then build a feature vector by concatenating L2-normalized versions of vectors cw ||cw|| and ce ||ce|| 2, and feed the vector to MLP.\n2.1.2 Corpus\nAs explained in the previous section, we train the classifier by using sets of words and entities relevant to e. To extract words and entities relevant to e, we use the following two sources: (1) the corresponding Wikipedia articles of e (denoted by article), and (2) Wikipedia sentences that contain a link anchor that corresponds to e (denoted by sentence). In both cases, words are extracted simply by tokenizing the text, and entities are the referent entities of link anchors in the text. Further, in the latter case, we restrict the words to the contextual words of the link anchor in a window of length m.3\nWe extracted Wikipedia articles directly from the July 2016 Wikipedia dump obtained from Wikimedia Downloads4 . We also used the public wiki-sentences dataset5 to obtain the Wikipedia sentences. In addition, we used words and entities that appear five times or more in the corpus, and simply ignored the other words and entities.\n2.1.3 Training\nAll parameters used in this model were initialized randomly and updated using back-propagation. We trained the model using stochastic gradient descent (SGD) and the learning rate was controlled by Adam [8]. The batch size was fixed as 100, the training consisted of one epoch, and the categorical cross-entropy was used for the loss function. We used a NVIDIA Tesla K80 GPU to train the model.\nRegarding hyper-parameters, the number of embedding dimensions dw and da were 300 and 10, respectively; the number of units in the hidden layer l was 2,000, and the dropout probability p was 0.5. We also selected the context window size of the link anchors m from 5 and 10.\nIn addition, we optionally introduced class weights to the loss function because the distribution of the target type was highly imbalanced. We adopted a weighted loss function based on the class weight heuristic implemented in Scikit-learn6.\nWe trained classifiers with various configurations. Table 1 shows the list of configurations used to train the classifiers. For each of the two corpora (i.e., article and sentence), we created eight classifiers with different training configurations, such as class weights and an attention mechanism in the enabled or disabled states7, using either both words and entities or only entities as input, and changing the context window size. In addition, we trained these classifiers for both the profession and nationality domains. Therefore, the total number of classifier instances was 32.\n2We also tested the vector averaging ( c N ) rather than L2 normalization; however, L2 normalization, in general, performed marginally more accurate in terms of the classification accuracy. 3We do not include the words within the anchor text. 4https://dumps.wikimedia.org/ 5We downloaded the dataset from the Web site of the WSDM Cup 2017: http://www.wsdm-cup-2017.org/triple-scoring.html 6https://github.com/scikit-learn/ 7We disabled the attention mechanism by simply replacing a(x) in Eq.(1) by 1."}, {"heading": "2.2 Scoring Step", "text": "We converted the outputs of the above-mentioned classifiers into relevance scores by adopting gradient boosted regression trees (GBRT) [6]. Given an entity e and a type t, our scoring model predicts the relevance score ranging from 0 to 7.\nWe experimented with two models of GBRT: the regression model and the binary classification model. The regression model directly learns the target scores ranging from 0 to 7, whereas the binary classification model is trained using a modified dataset where the training instances with scores less than or equal to 2 are relabeled as false, while those with scores greater than or equal to 5 are relabeled as true, and the other instances are excluded from the training. During the inference stage, the regression model outputs an integer value that is the closest to the estimated score. The binary classification model predicts 5 if the predicted result is true, and predicts 2 otherwise. Moreover, we use exactly the same features for these two models.\n2.2.1 Features\nWe compute the features based on two types of outputs of each classifier, the probability P (t|e) and the unnormalized version of P (t|e), which is the corresponding input value to the softmax layer of the MLP. For each of the two values, we compute three features, the value itself, and the difference between the value and the minimum and the maximum value among all valid types. It should be noted that the maximum value corresponds to the output value of the predicted type of the classifier.\nFurther, we observe that some pairs of types co-occur very frequently in the KB (e.g., Singer and Singer-songwriter). In order to incorporate this into the model, we also use the point-wise mutual information (PMI) on the type co-occurrence data in the KB. In particular, we add the feature representing the PMI score between the target type t and the type predicted by each classifier when these two types are not equal. Moreover, apart from the classifier outputs, we also include the number of valid types associated with e in the feature set.\n2.2.2 Dataset\nWe train our model by using the dataset obtained from the WSDM Cup web site. This dataset comprises two domains, professions and nationalities, of person entities retrieved from Freebase. The profession dataset and the nationality dataset contain relevance scores for 515 and 162 entity\u2013type pairs with 134 and 77 distinct entities, respectively. We then use this dataset for feature selection and parameter tuning as described below.\n2.2.3 Training\nWe train the regression and classification models for both the profession and the nationality domains. Feature selection is used to select a subset of the most relevant features. We first perform a greedy forward feature selection based on the performance of 10-fold cross validation, and simply select the set of features that perform the best. We also tune the hyper-parameters of GBRT using the selected features and the 10-fold cross validation, and use the hyper-parameters that provide the best performance. In addition, the performance is evaluated using the mean absolute error for the regression model and the accuracy for the binary classification model."}, {"heading": "2.3 Implementation", "text": "We implemented the classifier described in Section 2.1 using Python, Keras8, and Theano [10]. Further, our scoring model de-\n8https://github.com/fchollet/keras\nscribed in Section 2.2 was implemented using Python and Scikitlearn. We also used Hyperopt9 for performing the hyper-parameter search of GBRT."}, {"heading": "3. EXPERIMENTS", "text": "In this section, we first describe the performance evaluation of the classifiers presented in Section 2.1. Then, we present the official results of the triple scoring task at the WSDM Cup 2017."}, {"heading": "3.1 Evaluating Classifiers", "text": "In order to independently evaluate the performances of the proposed classifiers, we randomly selected 10% of the KB entities with a single type, and measured the classification accuracy using these selected entities.\nTable 1 lists the accuracies of the classifiers corresponding to various training configurations presented in Section 2.1.3. As can be seen in the table, the attention mechanism effectively improved the performance, whereas the use of class weights degraded the accuracy in general. Further, the classifiers trained with the article corpus generally performed more accurately than those trained with the sentence corpus.\nWe also found in our experiments that incorporating the outputs of classifiers that achieve lower accuracies often improved the performance of the scorer. Therefore, the strategy we adopted used the outputs of various classifiers rather than focusing on the outputs of a single accurate classifier.\nFurther, in order to investigate how the attention model works in practice, we inspected the words and entities having large attention weights wa \u22a4 ux in Eq.(2). Table 2 and Table 3 presents the top 10 words and entities with large weights, respectively. These weights were extracted from classifier 1, which was trained for the profession domain. It appeared that our classifier effectively focused on words and entities that strongly indicate a profession. For example, the top words included various professions, such as physicists and economists, and all the top entities were lists or categories that were strongly associated with a profession.\n9http://hyperopt.github.io/hyperopt/"}, {"heading": "3.2 Competition Results", "text": "We submitted our proposed method to the triple scoring task at the WSDM Cup 2017. In this competition, the submitted methods were evaluated based on the following three measures:\n\u2022 Accuracy, which is the percentage for which the estimated score differs from the score from the ground truth by at most\n2.\n\u2022 Average score difference, which is the average score difference between the estimated scores and the ground truth\nscores.\n\u2022 Kendall\u2019s \u03c4 , which is the average Kendall\u2019s \u03c4 score10 between the estimated scores and the ground truth scores. The\n\u03c4 score is computed for each entity, and the final score is averaged over all entities.\nExperiments were conducted using the 710 entity\u2013type pairs containing the instances of 513 profession pairs and 197 nationality pairs. We used different scoring models trained with the corresponding dataset for each domain. Note that the accuracy described here is different from the accuracy used to evaluate the classifiers in the previous section.\nTable 4 contains the official results of our methods based on the regression model (reg) and the binary classification model (clf)\n10Following Bast et al. [2], we used the modified version of Kendall\u2019s \u03c4 score proposed in Fagin et al. [5]\ncompared with the other top five methods proposed by competitors in terms of accuracy. The table lists the accuracies (acc), the average score differences (asd), and the Kendall\u2019s \u03c4 scores (tau).\nOur regression model achieved the best performance in terms of Kendall\u2019s \u03c4 scores among all the methods, and performed competitively in the accuracy and the average score difference. Further, the performance of our binary classification model was superior, particularly in terms of accuracy."}, {"heading": "4. CONCLUSIONS", "text": "In this study, we proposed an approach for assigning a relevance score to each entity\u2013type pair in a given KB. We trained neural network-based multiple classifiers by directly using the KB data, and converted the results of these classifiers into target relevance scores using a supervised machine learning model (i.e., GBRT). It is worth noting that the item-based attention model we introduced to the neural network model had not been applied to this kind of task previously. The experimental results confirmed the superiority of our approach; we achieved the best performances in terms of Kendall\u2019s \u03c4 scores, and performed competitively in terms of the accuracy and average score difference. We publicized the source code of our proposed method at https://github.com/wsdm-cup-2017/lettuce to enable it to be used for further academic research."}], "references": [{"title": "DBpedia: A Nucleus for a Web of Open Data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives"], "venue": "The Semantic Web,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Relevance scores for triples from type-like relations", "author": ["H. Bast", "B. Buchhold", "E. Haussmann"], "venue": "In SIGIR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Overview of the Triple Scoring Task at the WSDM Cup 2017", "author": ["H. Bast", "B. Buchhold", "E. Haussmann"], "venue": "In WSDM Cup,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "In SIGMOD,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Comparing and Aggregating Rankings with Ties", "author": ["R. Fagin", "R. Kumar", "M. Mahdian", "D. Sivakumar", "E. Vee"], "venue": "In PODS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Greedy Function Approximation: A Gradient Boosting Machine", "author": ["J.H. Friedman"], "venue": "The Annals of Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "WSDM Cup 2017: Vandalism Detection and Triple Scoring", "author": ["S. Heindorf", "M. Potthast", "H. Bast", "B. Buchhold", "E. Haussmann"], "venue": "In WSDM", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Not All Contexts Are Created Equal: Better Word Representations with Variable Attention", "author": ["W. Ling", "Y. Tsvetkov", "S. Amir", "R. Fermandez", "C. Dyer", "A.W. Black", "I. Trancoso", "C.-C. Lin"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Wikidata: A Free Collaborative Knowledgebase", "author": ["D. Vrande\u010di\u0107", "M. Kr\u00f6tzsch"], "venue": "Communications of the ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "In the last decade, huge online structured knowledge bases (KBs) such asWikidata [11], Freebase [4], and DBpedia [1] have emerged.", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "In the last decade, huge online structured knowledge bases (KBs) such asWikidata [11], Freebase [4], and DBpedia [1] have emerged.", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "In the last decade, huge online structured knowledge bases (KBs) such asWikidata [11], Freebase [4], and DBpedia [1] have emerged.", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "[2] addressed this problem by assigning a relevance score to each pair consisting of an entity and its type in KB.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": ", gradient boosted regression trees (GBRT) [6]) to convert the outputs of these classifiers into the final relevance scores.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "The proposed method was applied to the triple scoring task at the WSDM Cup 2017 [3, 7] The results demonstrated that our method achieved the best results in one out of three measures (i.", "startOffset": 80, "endOffset": 86}, {"referenceID": 6, "context": "The proposed method was applied to the triple scoring task at the WSDM Cup 2017 [3, 7] The results demonstrated that our method achieved the best results in one out of three measures (i.", "startOffset": 80, "endOffset": 86}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "We adopt the neural bag-of-items model with a simple item-level attention mechanism [9] to derive the representa-", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "We trained the model using stochastic gradient descent (SGD) and the learning rate was controlled by Adam [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "We converted the outputs of the above-mentioned classifiers into relevance scores by adopting gradient boosted regression trees (GBRT) [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 1, "context": "[2], we used the modified version of Kendall\u2019s \u03c4 score proposed in Fagin et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5]", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "This paper describes our approach for the triple scoring task at the WSDM Cup 2017. The task required participants to assign a relevance score for each pair of entities and their types in a knowledge base in order to enhance the ranking results in entity retrieval tasks. We propose an approach wherein the outputs of multiple neural network classifiers are combined using a supervised machine learning model. The experimental results showed that our proposed method achieved the best performance in one out of three measures (i.e., Kendall\u2019s \u03c4 ), and performed competitively in the other two measures (i.e., accuracy and average score difference).", "creator": "LaTeX with hyperref package"}}}