{"id": "1703.06846", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions", "abstract": "Expressive efficiency another. describe both allows formally contrary about the nonobjective capacity well with platforms cpus. A network mathematical is melodramatically efficient made respect eventually. provide architecture nor next latter instead grow inaugural - formula_1 a order rest represent functions wonder by way held. A well - \u2019s suggests example from exponential textures capability of technical, namely, has in many ranging draining subscribers must disappear outstrips large in order given represent functions gone only through distribution. In perhaps print we suggests the luminous reducing brought ideas included two architectural feature only signalling, motivated brought once observation did far giving state of two art networks however nine types ceremonial linked manipulation, running layers in parallel while parties now lines give in various make. A legal severe a once though would fallen light on then guidance though modern functionality funds, and having addition, could provide called practical for its design. We efforts on zdenka d20 technologies, . family known beneath models gaining higher particular, underlying state of the art constructs much Google ' dropped WaveNet took ByteNet. By generic and studying also concept of mixed wavelet decompositions, we prove that interconnecting anula convolutional simultaneously be helped any seductive efficiency. In particular, get show that he band connection between grade layers can already lead as instance almost quadratic gain, which in used - units settings number makes the think extended well older reason though precise all one idea still so.", "histories": [["v1", "Mon, 20 Mar 2017 17:05:38 GMT  (2236kb,D)", "http://arxiv.org/abs/1703.06846v1", null], ["v2", "Mon, 17 Apr 2017 18:22:33 GMT  (2238kb,D)", "http://arxiv.org/abs/1703.06846v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["nadav cohen", "ronen tamari", "amnon shashua"], "accepted": false, "id": "1703.06846"}, "pdf": {"name": "1703.06846.pdf", "metadata": {"source": "CRF", "title": "Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions", "authors": ["Nadav Cohen", "Ronen Tamari", "Amnon Shashua", "TAMARI SHASHUA"], "emails": ["COHENNADAV@CS.HUJI.AC.IL", "RONENT@CS.HUJI.AC.IL", "SHASHUA@CS.HUJI.AC.IL"], "sections": [{"heading": null, "text": "capacity of deep network architectures. A network architecture is expressively efficient with respect to an alternative architecture if the latter must grow super-linearly in order to represent functions realized by the former. A well-known example is the exponential expressive efficiency of depth, namely, that in many cases shallow networks must grow exponentially large in order to represent functions realized by deep networks.\nIn this paper we study the expressive efficiency brought forth by the architectural feature of connectivity, motivated by the observation that nearly all state of the art networks these days employ elaborate connection schemes, running layers in parallel while splitting and merging them in various ways. A formal treatment of this question would shed light on the effectiveness of modern connectivity schemes, and in addition, could provide new tools for network design. We focus on dilated convolutional networks, a family of deep models gaining increased attention, underlying state of the art architectures like Google\u2019s WaveNet and ByteNet. By introducing and studying the concept of mixed tensor decompositions, we prove that interconnecting dilated convolutional networks can lead to expressive efficiency. In particular, we show that a single connection between intermediate layers can already lead to an almost quadratic gap, which in large-scale settings typically makes the difference between a model that is practical and one that is not.\nKeywords: Deep Learning, Expressive Efficiency, Dilated Convolutions, Tensor Decompositions"}, {"heading": "1. Introduction", "text": "One of the key attributes fueling the success of deep learning is the ability of deep networks to compactly represent rich classes of functions. This phenomenon has drawn considerable attention from the theoretical machine learning community in recent years. The primary notion for formally reasoning about the representational abilities of different models is expressive efficiency. Given two network architecturesA andB, with size parameters (typically the width of layers across a network) rA and rB , we say that architecture A is expressively efficient w.r.t. architecture B if the following two conditions hold: (i) any function realized by B with size rB can be realized (or approximated) by A with size rA \u2208 O(rB); (ii) there exist functions realized by A with size rA that cannot be realized (or approximated) by B unless its size meets rB \u2208 \u2126(f(rA)) for some super-linear function f . The nature of the function f in condition (ii) determines the type of efficiency taking place \u2013 if f is exponential then architecture A is said to be exponentially expressively efficient w.r.t. architecture B, and if f is polynomial so is the expressive efficiency of A over B.\nc\u00a9 2017 N. Cohen, R. Tamari & A. Shashua.\nar X\niv :1\n70 3.\n06 84\n6v 1\n[ cs\n.L G\n] 2\n0 M\nar 2\nTo date, works studying expressive efficiency in the context of deep learning (e.g. Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al. (2014); Telgarsky (2015); Eldan and Shamir (2015); Cohen et al. (2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question.\nA specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks.\nOur analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al. (2016b) and Cohen and Shashua (2016). We show that with dilated convolutional networks, the choice of dilations throughout a network corresponds to determination of the mode (dimension) tree underlying the respective decomposition. We then define the notion of a mixed tensor decomposition, which blends together multiple mode trees, effectively creating a large ensemble of hybrid trees formed from all possible combinations. Mixed tensor decompositions correspond to mixed dilated convolutional networks, i.e. mixtures formed by connecting intermediate layers of different dilated convolutional networks. This allows studying the expressive properties of such mixtures using mathematical machinery from the field of tensor analysis. We fully analyze a particular case of dilated convolutional arithmetic circuits, showing that a single connection between intermediate layers already leads to an almost quadratic expressive efficiency, which in large-scale settings typically makes the difference between a model that is practical and one that is not. An experiment on TIMIT speech recognition dataset (Garofolo et al. (1993)) demonstrates the gain brought forth by mixing different networks, showing that interconnectivity can indeed boost the performance of dilated convolutional networks.\nThe remainder of the paper is organized as follows. Sec. 2 provides preliminary background in the field of tensor analysis, and establishes notational conventions. Sec. 3 presents dilated convolutional networks, and their correspondence to tensor decompositions. In sec. 4 we define mixed tensor decompositions, and discuss their equivalence to mixed dilated convolutional networks. Our analysis of expressive efficiency is given in sec. 5, followed the experiment in sec. 6. Finally, sec. 7 concludes."}, {"heading": "2. Preliminaries", "text": "The constructions and analyses delivered in this paper rely on concepts from the field of tensor analysis. Below we provide the minimal background required in order to follow our arguments.1\nThe core concept in tensor analysis is a tensor, which for our purposes may simply be thought of as a multi-dimensional array. The order of a tensor is defined to be the number of indexing entries in the array, which are referred to as modes. The dimension of a tensor in a particular mode is defined as the number of values that may be taken by the index in that mode. For example, a 4-by-3 matrix is a tensor of order 2, i.e. it has two modes, with dimension 4 in mode 1 and dimension 3 in mode 2. If A is a tensor of order N and dimension Mi in each mode i \u2208 {1, . . . , N}, the space of all configurations it can take is denoted, quite naturally, by RM1\u00d7\u00b7\u00b7\u00b7\u00d7MN .\nA fundamental operator in tensor analysis is the tensor product (also known as outer product), which we denote by \u2297. It is an operator that intakes two tensors A \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MP and B \u2208 RMP+1\u00d7\u00b7\u00b7\u00b7\u00d7MP+Q (orders P and Q respectively), and returns a tensorA\u2297B \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MP+Q (order P+Q) defined by: (A\u2297B)d1...dP+Q = Ad1...dP \u00b7BdP+1...dP+Q . In Cohen and Shashua (2016) a generalization of the tensor product is defined, by replacing multiplication with a general operator g(\u00b7). Specifically, for a function g : R \u00d7 R \u2192 R that is commutative (g(a, b) = g(b, a) for all a, b \u2208 R), the generalized tensor product, denoted\u2297g, is defined to be the operator that for input tensors A \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MP and B \u2208 RMP+1\u00d7\u00b7\u00b7\u00b7\u00d7MP+Q (orders P and Q respectively), returns the tensor A\u2297g B \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MP+Q (order P +Q) given by: (A\u2297g B)d1...dP+Q = g(Ad1...dP ,BdP+1...dP+Q).\nAn additional operator we will make use of is mode permutation. Let A be a tensor of order N , and let \u03c3(\u00b7) be a permutation over N (bijective mapping from {1, . . . , N} to itself). The mode permutation ofA w.r.t. \u03c3(\u00b7), which by a slight abuse of notation is denoted \u03c3(A), is the order-N tensor defined by: \u03c3(A)d1...dN = Ad\u03c3(1)...d\u03c3(N) . In words, \u03c3(A) is the tensor obtained by rearranging the modes of A in accordance with \u03c3(\u00b7).\nWhen studying tensors, it is oftentimes useful to arrange them as matrices, a procedure referred to as matricization. Let A be a tensor of order N and dimension Mi in each mode i \u2208 {1, . . . , N}, and let I \u2282 {1, . . . , N} be a set of mode indexes, whose complement {1, . . . , N} \\ I we denote by Ic. We may write I = {i1, . . . , i|I|} where i1 < \u00b7 \u00b7 \u00b7 < i|I|, and similarly Ic = {j1, . . . , j|Ic|} where j1 < \u00b7 \u00b7 \u00b7 < j|Ic|. The matricization of A w.r.t. I, denoted JAKI , is the\n\u220f|I| t=1Mit-by-\u220f|Ic|\nt=1Mjt matrix holding the entries ofA such thatAd1...dN is placed in row index 1 + \u2211|I|\nt=1(dit \u2212 1) \u220f|I| t\u2032=t+1Mit\u2032 and column index 1 + \u2211|Ic| t=1(djt \u2212 1) \u220f|Ic| t\u2032=t+1Mjt\u2032 . If I = \u2205 or I = {1, . . . , N},\nthen by definition JAKI is a row or column (respectively) vector of dimension \u220fN t=1Mt holding\nAd1...dN in entry 1 + \u2211N t=1(dt \u2212 1) \u220fN t\u2032=t+1Mt\u2032 .\nTo conclude this section, we hereinafter establish notational conventions that will accompany us throughout the paper. We denote tensors with uppercase calligraphic letters, e.g. A, and in some cases with the Greek letters \u03c6, \u03d5 or \u03c8. Subscripts are used to refer to individual tensor entries, e.g. Ad1...dN \u2208 R, whereas superscripts indicate the location of a tensor in some annotated collection, for example Ay stands for the y\u2019th tensor in the collection A1 . . .Ar. Vectors are typically denoted with boldface lowercase letters, e.g. a, where again subscripts refer to an individual entry (e.g. a\u03b1 \u2208 R), and superscripts to the identity of a vector within some annotated collection (e.g. al,j\n1. The viewpoint we adopt is actually a concrete special case of a more abstract algebraic viewpoint of tensor analysis, as presented for example in Hackbusch (2012). We limit ourselves to this concrete viewpoint since it suffices for our needs and is easier to grasp.\nis the (l, j)\u2019th vector in the set {al,j}l=1...L,j=1...N ). We use non-boldface lowercase or uppercase letters (e.g. l or L respectively) to denote scalars, and in this case both subscripts and superscripts distinguish between objects in an annotated set (e.g. li, li, Li, Li \u2208 R). Finally, for a positive integer N \u2208 N, we use [N ] as shorthand for the set {1, . . . , N}."}, {"heading": "3. Dilated Convolutional Networks", "text": "Convolutional networks (LeCun and Bengio (1995)) are the cornerstone of modern deep learning, and have played a critical role in its resurgence. Since the work of Krizhevsky et al. (2012), nearly all state of the art systems for image and video processing, in both academia and industry, are heavily based on convolutional networks (see for example Szegedy et al. (2015); Taigman et al. (2014); He et al. (2015); Karpathy et al. (2014); Long et al. (2015)). In their basic form, convolutional networks consist of successive layers, each comprising convolutions with multiple filters followed by point-wise activation (non-linearity), which in turn is followed by spatial pooling (decimation).\nRecently, an alternative form of convolutional networks has emerged \u2013 dilated convolutional networks. These models are obtained by removing spatial pooling and introducing non-contiguity to convolutional filters. Although they have been used for more conventional image processing tasks (e.g. Yu and Koltun (2015)), arguably the most significant merit of dilated convolutional networks is that they thrive in application domains previously outside the realm of convolutional networks. The WaveNet model recently developed by Google (van den Oord et al. (2016)) is based on dilated convolutions applied to raw audio, and provides state of the art text-to-speech results, as well as promising phoneme recognition (speech classification) performance. The following ByteNet model (Kalchbrenner et al. (2016)) applies dilated convolutional networks to raw textual characters, delivering state of the art character-level language modeling, as well as excellent character-level machine translation results at a fraction of the run time required by competing methods. Taken together, these two developments demonstrate the ability of dilated convolutional networks to provide state of the art performance in sequence processing tasks."}, {"heading": "3.1. Baseline Architecture", "text": "The dilated convolutional network architecture considered as baseline in this paper is the one underlying WaveNet model, depicted in fig. 1. The input to the network is a sequence of vectors (x[t])t \u2282 Rr0 , where t is a natural time index. A size-2 convolutional layer with dilation-1, i.e. with contiguous filters, maps this input into the hidden sequence (h(1)[t])t \u2282 Rr1 . Specifically, entry \u03b3 \u2208 [r1] of h(1)[t] is obtained by applying the filter formed by a1,\u03b3,I,a1,\u03b3,II \u2208 Rr0 to time points t-1, t of the input: h(1)[t]\u03b3 = g( \u2329 a1,\u03b3,I,x[t-1] \u232a , \u2329 a1,\u03b3,II,x[t] \u232a ). For reasons that will shortly become apparent, we use g(\u00b7) here to denote the binary function combining two size-1 convolutions into a single size-2 convolution with non-linearity. Different choices of g(\u00b7) lead to different convolutional operators, for example g(a, b) := max{a + b, 0} leads to standard convolution followed by rectified linear activation (ReLU, Nair and Hinton (2010)), whereas g(a, b) = a\u00b7b gives rise to what is known as a convolutional arithmetic circuit (Cohen et al. (2016b)). Following the first hidden layer, L-1 size-2 convolutional layers with increasing dilations are applied. Specifically, for l = 2, . . ., L-1, hidden layer l maps the sequence (h(l-1)[t])t \u2282 Rrl\u22121 into (h(l)[t])t \u2282 Rrl using filters with dilation-2l-1, i.e. with an internal temporal gap of 2l-1-1 points: h(l)[t]\u03b3 = g( \u2329 al,\u03b3,I,h(l-1)[t-2l-1] \u232a , \u2329 al,\u03b3,II,h(l-1)[t] \u232a ). The last convolutional layer maps\n(h(L-1)[t])t into network output sequence (o[t])t \u2282 RrL using filters with dilation-2L-1: o[t]y = g( \u2329 aL,y,I,h(L-1)[t-2L-1] \u232a , \u2329 aL,y,II,h(L-1)[t] \u232a ).\nAltogether, the architectural parameters of the network are the number of convolutional layersL, the convolutional operator g(\u00b7), the input dimension r0, the number of channels rl for each hidden layer l \u2208 [L-1], and the output dimension rL. The learnable parameters are the convolution weights al,\u03b3,I,al,\u03b3,II \u2208 Rrl\u22121 for channel \u03b3 \u2208 [rl] of layer l \u2208 [L].\nOur interest lies on the representational abilities of the network, i.e. on the properties of the input-output mappings that may be realized by it. As illustrated in fig. 1, for some fixed time point t, o[t] \u2013 network output at time t, is a function of x[t-2L+1] . . .x[t] \u2013 network input over the last 2L time points. Taking into account the temporal stationarity of the network, and denoting N := 2L for brevity, we may write o[t]y = fy(x[t-N+1], . . . ,x[t]) for every y \u2208 [rL], where the functions {fy(\u00b7)}y are independent of the time index t. The latter functions, which obviously depend on the convolution weights {al,\u03b3,I,al,\u03b3,II}l,\u03b3 , completely characterize the input-output mapping realized by the network. We will study these functions through the process of discretization. Namely, fy(\u00b7) \u2013 a function of N vector-variables, will be represented by a lookup table (tensor) formed by varying each vector-variable over a finite number of possible values. Obviously, the size of such a lookup table is exponential in N , thus treating it directly is intractable. However, as we shall see, the network admits a compact parameterization of lookup tables in terms of the convolution weights {al,\u03b3,I,al,\u03b3,II}l,\u03b3 . This parameterization (eq. 2 below) entails an algebraic structure, and will be used to study the representational properties of the baseline dilated convolutional network.\nFor the discretization of fy(\u00b7), we choose a collection of vectors v(1) . . .v(M) \u2208 Rr0 , and define the following tensor Ay of order N and dimension M in each mode:\nAyd1...dN := fy(v (d1), . . . ,v(dN )) \u2200d1. . .dN \u2208 [M ] (1)\nThe vectors v(1) . . .v(M) are referred to as discretizers. They generate the tensor Ay by assigning, in all possible combinations, the N vector-variables of the function fy(\u00b7). We refer to Ay as the grid tensor of fy(\u00b7), reflecting the fact that it holds function values over a discrete grid.\nThe parameterization of {fy(\u00b7)}y discretizations mentioned above is in fact a hierarchical decomposition of the grid tensors {Ay}y. Accordingly, and for the sake of highlighting correspondence to the baseline dilated convolutional network (fig. 1), we refer to this parameterization as the\nbaseline decomposition. For conciseness, we defer the derivation of the baseline decomposition to app. A, and hereby lay out its final form:\nFor j = 1. . .N :\n\u03c60,j,\u03b3\ufe38 \ufe37\ufe37 \ufe38 order 1 = [v(1)\u03b3 , . . . , v (M) \u03b3 ] > \u2200\u03b3 \u2208 [r0]\nFor l = 1. . .L , j = 1. . .N/2l:\n\u03c6l,j,\u03b3\ufe38\ufe37\ufe37\ufe38 order 2l\n= (\u2211rl\u22121\n\u03b1=1 al,\u03b3,I\u03b1 \u00b7 \u03c6l\u22121,2j\u22121,\u03b1\n) \u2297g (\u2211rl\u22121\n\u03b1=1 al,\u03b3,II\u03b1 \u00b7 \u03c6l\u22121,2j,\u03b1\n) \u2200\u03b3 \u2208 [rl]\nAy = \u03c6L,1,y \u2200y \u2208 [rL] (2)\nal,\u03b3,I\u03b1 and a l,\u03b3,II \u03b1 here stand for coordinate \u03b1 of the convolution weights al,\u03b3,I and al,\u03b3,II respectively, while v(i)\u03b3 stands for coordinate \u03b3 of the discretizer v(i). Notice that the tensor products here are generalized (see sec. 2) \u2013 based on the network\u2019s convolutional operator g(\u00b7). Therefore, strictly speaking, the baseline decomposition is a generalized tensor decomposition, as defined in Cohen and Shashua (2016).\nTo conclude this subsection, we relate the material above to prior works in the literature, and highlight our contributions in the text to come. The first work to formalize the correspondence between convolutional networks and hierarchical tensor decompositions was Cohen et al. (2016b), in which only convolutional arithmetic circuits (convolutional networks with product pooling and linear activations) were considered. Cohen and Shashua (2016) later generalized the correspondence to account for other types of convolutional networks (e.g. ones with ReLU activation and max or average pooling) as well. The baseline decomposition above (eq. 2) \u2013 a hierarchical tensor decomposition characterizing the baseline dilated convolutional network (fig. 1), is essentially a direct outcome of the formulation presented in Cohen and Shashua (2016). Our contributions begin in the next subsection, where we establish a correspondence between hierarchical decompositions over general mode trees, and dilated convolutional networks with different dilations. Thereafter, in sec. 4, we present the idea of blending together multiple mode trees in a single mixed decomposition, and show that this corresponds to interconnections of different dilated convolutional networks. Subsequently, in sec. 5, we use the latter relation to demonstrate the expressive efficiency brought forth by the interconnections."}, {"heading": "3.2. Dilations and Mode Trees", "text": "The baseline decomposition (eq. 2), corresponding to the baseline dilated convolutional network (fig. 1), implicitly adheres to a tree structure \u2013 for every (l, j), there exists a group of tensors {\u03c6l,j,\u03b3}\u03b3 , formed through combinations of tensors from its \u201cchild\u201d groups {\u03c6l\u22121,2j\u22121,\u03b3}\u03b3 and {\u03c6l\u22121,2j\u22121,\u03b3}\u03b3 . In this subsection we generalize the underlying tree structure, and show that the resulting decompositions capture networks with various dilations throughout their convolutional layers. We begin by defining a general (binary) tree over tensor modes:\nDefinition 1 Let N \u2208 N. A binary mode tree2 over [N ] is a full binary tree3 in which: \u2022 Every node is labeled by a subset of [N ] \u2022 There are exactly N leaves, labeled {1} . . . {N} \u2022 The root node is labeled [N ] \u2022 The label of an interior (non-leaf) node is the union of the labels of its children\nIf T is a binary mode tree, we identify its nodes with their labels, i.e. with the corresponding subsets of [N ]. The set of all interior nodes is denoted by int(T ) \u2282 2[N ], the children of an interior node \u03bd \u2282 [N ] are denoted by CI(\u03bd;T ), CII(\u03bd;T ) \u2282 [N ], and the parent of a non-root node \u03bd \u2282 [N ] is denoted by P (\u03bd;T ).\nBinary mode trees induce hierarchical decompositions of grid tensors. Recall the definition of grid tensors in sec. 3.1 (eq. 1), and let T be a binary mode tree over [N ]. For every node \u03bd \u2282 [N ] in T , we define a collection of 2|\u03bd|-order tensors {\u03c6\u03bd,\u03b3}\u03b3\u2208[r], where r \u2208 N is some predetermined constant.4 In addition, we also define, for each interior node \u03bd\u2208int(T ), two collections of weight vectors \u2013 {a\u03bd,\u03b3,I}\u03b3\u2208[r] \u2282 Rr and {a\u03bd,\u03b3,II}\u03b3\u2208[r] \u2282 Rr. The hierarchical grid tensor decomposition induced by T traverses through the tree in a depth-first fashion, assigning the tensors of node \u03bd ({\u03c6\u03bd,\u03b3}\u03b3) through combinations of the tensors of its children ({\u03c6CI(\u03bd;T ),\u03b3}\u03b3 and {\u03c6CII(\u03bd;T ),\u03b3}\u03b3). This is laid out formally in eq. 3 below, which we refer to as the tree decomposition.\nFor j = 1. . .N :\n\u03c6{j},\u03b3\ufe38 \ufe37\ufe37 \ufe38 order 1 = [v(1)\u03b3 , . . . , v (M) \u03b3 ] > \u2200\u03b3 \u2208 [r]\nFor \u03bd in int(T ) (depth-first order):\n\u03c6\u03bd,\u03b3\ufe38\ufe37\ufe37\ufe38 order 2|\u03bd|\n= \u03c3(\u03bd;T ) ((\u2211r\n\u03b1=1 a\u03bd,\u03b3,I\u03b1 \u00b7 \u03c6CI(\u03bd;T ),\u03b1\n) \u2297g (\u2211r\n\u03b1=1 a\u03bd,\u03b3,II\u03b1 \u00b7 \u03c6CII(\u03bd;T ),\u03b1\n)) \u2200\u03b3 \u2208 [r]\nAy = \u03c6[N ],y \u2200y \u2208 [r] (3)\nAs in the baseline decomposition (eq. 2), v(i)\u03b3 here stands for coordinate \u03b3 of the discretizer v(i). The permutation \u03c3(\u03bd;T )(\u00b7), for an interior node \u03bd\u2208int(T ), arranges the modes of the tensor \u03c6\u03bd,\u03b3 such that these comply with a sorted ordering of \u03bd. Specifically, if we denote by i1 < \u00b7 \u00b7 \u00b7 < i|CI(\u03bd;T )| the elements of CI(\u03bd;T ) \u2282 [N ], and by j1 < \u00b7 \u00b7 \u00b7 < j|CII(\u03bd;T )| the elements of CII(\u03bd;T ) \u2282 [N ], the permutation \u03c3(\u03bd;T ) : [2|\u03bd|] \u2192 [2|\u03bd|] is the one that sorts the tuple (i1, . . . , i|CI(\u03bd;T )|, j1, . . . , j|CII(\u03bd;T )|) in ascending order. The final outcome of the decomposition, i.e. the generated grid tensors {Ay}y, are the tensors {\u03c6[N ],\u03b3}\u03b3 corresponding to the root of T .\n2. Binary mode trees lead to decompositions (eq. 3) that correspond to networks with size-2 convolutions. We limit ourselves to this special case merely for simplicity of presentation. Our formulation can easily be extended to account for convolutions of arbitrary size by considering mode trees that are not necessarily binary, and by modifying the decomposition in eq. 3 to take (generalized) tensor products between an arbitrary number of tensors (not necessarily two). 3. A full binary tree is a tree in which all interior (non-leaf) nodes have exactly two children. 4. In general the number of tensors in the collection may vary across nodes, but for simplicity of presentation we\nassume here that all collections comprise exactly r tensors.\nCompare the general tree decomposition in eq. 3 to the baseline decomposition in eq. 2. It is not difficult to see that the latter is a special case of the former. Namely, it corresponds to a binary mode tree T that is perfect (all leaves have the same depth L = log2N ), and whose depth-l nodes (l \u2208 {0, 1, . . . , L}) are (k \u2212 1)N/2l + [N/2l] for k \u2208 [2l].5 This implies that such a mode tree, when plugged into the tree decomposition (eq. 3), provides a characterization of the baseline dilated convolutional network (fig. 1), i.e. a network whose dilation in layer l is 2l\u22121 (see illustration in fig. 2(a)). If we were to choose a different mode tree, the corresponding dilated convolutional network would change.6 For example, assume that L = log2N is even, and consider a perfect binary mode tree T whose depth-l nodes (l \u2208 {0, 1, . . . , L}) are as follows: \u2022 Even l: depth-l nodes are (k \u2212 1)N/2l + [N/2l] for k \u2208 [2l] \u2022 Odd l: depth-l nodes are generated by splitting nodes of depth l-1, such that the first and third\nquadrants of a split node belong to one child, while the second and fourth belong to the other\nIn this case, the network characterized by the tree decomposition (eq. 3) is obtained by swapping dilations of even and odd layers in the baseline architecture, i.e. it has dilation in layer l of 2l\u22122 if l is even, and 2l if l is odd (see illustration in fig. 2(b)).\nTo conclude this subsection, we defined the notion of a tree over tensor modes (def. 1), and laid out a corresponding hierarchical decomposition of grid tensors (tree decomposition \u2013 eq. 3). Different choices of mode trees lead to decompositions characterizing networks with different dilations throughout their layers. The baseline decomposition (eq. 2), characterizing the baseline dilated convolutional network (dilation 2l\u22121 in layer l \u2013 see fig. 1), is now merely a special case that corresponds to a particular choice of mode tree. In the next section, we build on the constructions made\n5. If c is a scalar and S is a set, c+ S stands for the set obtained by adding c to each element in S. 6. It is important to stress that not all choices of mode trees lead to networks resembling ones used in practice. For\nexample, if different leaves in a tree have different depths, different inputs in the corresponding network pass through a different number of layers. Conversely, not every type of dilated convolutional network used in practice corresponds to a mode tree \u2013 only ones in which an input is connected to the output through a single path.\nhere, and define mixed tensor decompositions blending together multiple mode trees. These decompositions will be shown to correspond to multiple dilated convolutional networks interconnected to one another."}, {"heading": "4. Mixed Tensor Decompositions", "text": "Let T and T\u0304 be two binary mode trees over [N ] (def. 1). Consider the tree decomposition of grid tensors induced by T (eq. 3). This decomposition iteratively assigns a group of tensors {\u03c6\u03bd,\u03b3}\u03b3 for each node \u03bd in T , based on weight vectors {a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03b3 defined for each interior node \u03bd\u2208int(T ). The tree decomposition induced by T\u0304 operates similarly, but for distinction we use {\u03c6\u0304\u03bd\u0304,\u03b3}\u03b3 to denote the tensor group of node \u03bd\u0304 \u2208 T\u0304 , and {a\u0304\u03bd\u0304,\u03b3,I, a\u0304\u03bd\u0304,\u03b3,II}\u03b3 to denote the weights of interior node \u03bd\u0304\u2208int(T\u0304 ). We will define a mixed tensor decomposition, blending together the tree decompositions of T and T\u0304 . The latter is obtained by choosing a collection of mixture nodes \u2013 mix(T, T\u0304 )\u2282int(T )\u2229int(T\u0304 ). These are nodes (subsets of [N ]) that reside in the interior of both T and T\u0304 , defining locations in the tree decompositions at which tensors will be exchanged. If mix(T, T\u0304 ) is chosen as the empty set, the mixed decomposition simply sums the output tensors generated by the tree decompositions of T and T\u0304 ({\u03c6[N ],y}y and {\u03c6\u0304[N ],y}y respectively). Otherwise, the tree decompositions of T and T\u0304 progress in parallel, until reaching a mixture node \u00b5\u2208mix(T, T\u0304 ), where they exchange half the tensors corresponding to that node (half of {\u03c6\u00b5,\u03b3}\u03b3 is exchanged for half of {\u03c6\u0304\u00b5,\u03b3}\u03b3). The process continues until all mixture nodes are visited and the root node (of both trees) [N ] is reached. At this point tensors ({\u03c6[N ],y}y and {\u03c6\u0304[N ],y}y) are summed and returned as output.\nThe formal definition of the mixed decomposition is as follows:\n1 : For j = 1. . .N : 2 : \u03c6{j},\u03b3 = \u03c6\u0304{j},\u03b3 = [v(1)\u03b3 , . . . , v (M) \u03b3 ] > \u2200\u03b3 \u2208 [r]\n3 : For \u00b5 in mix(T, T\u0304 ) \u222a {[N ]} (inclusion order):\n4 : For \u03bd in int(T ) \u2229 2\u00b5 \\ {nodes in T already visited} (inclusion order): 5 : \u03c6\u03bd,\u03b3 = \u03c3(\u03bd;T ) ((\u2211r\n\u03b1=1 a\u03bd,\u03b3,I\u03b1 \u00b7 \u03c6CI(\u03bd;T ),\u03b1\n) \u2297g (\u2211r\n\u03b1=1 a\u03bd,\u03b3,II\u03b1 \u00b7 \u03c6CII(\u03bd;T ),\u03b1\n)) \u2200\u03b3 \u2208 [r]\n6 : For \u03bd\u0304 in int(T\u0304 ) \u2229 2\u00b5 \\ {nodes in T\u0304 already visited} (inclusion order): 7 : \u03c6\u0304\u03bd\u0304,\u03b3 = \u03c3(\u03bd\u0304;T\u0304 ) ((\u2211r\n\u03b1=1 a\u0304\u03bd\u0304,\u03b3,I\u03b1 \u00b7 \u03c6\u0304CI(\u03bd\u0304;T\u0304 ),\u03b1\n) \u2297g (\u2211r\n\u03b1=1 a\u0304\u03bd\u0304,\u03b3,II\u03b1 \u00b7 \u03c6\u0304CII(\u03bd\u0304;T\u0304 ),\u03b1\n)) \u2200\u03b3 \u2208 [r]\n8 : Swap \u03c6\u00b5,\u03b3 \u2190\u2192 \u03c6\u0304\u00b5,\u03b3 \u2200\u03b3 \u2208 [r/2]\n9 : Ay = \u03c6[N ],y + \u03c6\u0304[N ],y \u2200y \u2208 [r] (4)\nAs in the basic tree decomposition (eq. 3), the first step here (lines 1-2) is to assign tensors corresponding to the leaf nodes ({1} . . . {N}) via discretizers v(1) . . .v(M). The outer loop in line 3 traverses \u00b5 through mixture nodes and the root node in inclusion order, i.e. such that a node (subset of [N ]) is always reached after all nodes strictly contained in it. Lines 4-5 (respectively 6-7) are the same as in the tree decomposition (eq. 3), except that instead of running through the entire interior of T (respectively T\u0304 ), they cover a segment of it. This segment continues where the previous left off, and comprises only nodes (subsets of [N ]) contained in \u00b5 (including \u00b5 itself). Line 8 is where the mixing takes place \u2013 here half of the tensors corresponding to node \u00b5 in the\ndecomposition of T ({\u03c6\u00b5,\u03b3}\u03b3), are exchanged for half the tensors corresponding to \u00b5 in the decomposition of T\u0304 ({\u03c6\u0304\u00b5,\u03b3}\u03b3). Finally, after \u00b5 has reach the root [N ] and the decompositions of T and T\u0304 have concluded, line 9 sums the output tensors of these decompositions ({\u03c6[N ],y}y and {\u03c6\u0304[N ],y}y respectively), to produce the grid tensors {Ay}y.\nIn terms of computation and memory, the requirements posed by the mixed decomposition (eq. 4) are virtually identical to those of running two separate tree decompositions (eq. 3) with T and T\u0304 . Specifically, if the tree decompositions of T and T\u0304 correspond to input-output mappings computed by the dilated convolutional networksN and N\u0304 (respectively), the mixed decomposition would correspond to the computation of a mixed dilated convolutional network, formed by summing the outputs ofN and N\u0304 , and interconnecting their intermediate layers. The choice of mixture nodesmix(T, T\u0304 ) in the mixed decomposition determines the locations at which networksN and N\u0304 are interconnected, where an interconnection simply wires into N half the outputs of a convolutional layer in N\u0304 , and vice versa. For example, suppose thatN is the baseline dilated convolutional network (dilation 2l\u22121 in layer l \u2013 see sec. 3.1), whereas N\u0304 is the network obtained by swapping dilations of even and odd layers (such that layer l has dilation 2l\u22122 if l is even, and 2l if l is odd). The mode trees corresponding to these networks, illustrated in fig. 2 (for the case L := log2N = 4), share interior nodes (k \u2212 1)N/2l + [N/2l] for l \u2208 {2, 4, . . . , L}, k \u2208 [2l]. We may therefore choose mix(T, T\u0304 ) to be all such nodes (excluding root), and get a mixed decomposition that corresponds to a mixed network interconnecting all even layers of N and N\u0304 . Illustrations of such decomposition and network (again, for the case L = 4) are given in fig. 3.\nThe main advantage of the mixed decomposition (eq. 4), and the reason for its definition, is that it leads to expressive efficiency. That is to say, the mixed dilated convolutional network, formed by interconnecting intermediate layers of networks with different dilations, can realize functions that without the interconnections would be expensive, or even impractical to implement. We theoretically support this in the next section, providing a complete proof for a special case of convolutional arithmetic circuits (g(a, b) = a\u00b7b)."}, {"heading": "5. Expressive Efficiency Analysis", "text": "As in sec. 4, let N and N\u0304 be two dilated convolutional networks whose input-output mappings are characterized by the tree decomposition (eq. 3) with mode trees T and T\u0304 respectively. Consider the mixed decomposition (eq. 4) resulting from a particular choice of mixture nodes mix(T, T\u0304 ) (subset of the nodes interior to both T and T\u0304 ), and denote its corresponding mixed dilated convolutional network byM. We would like to show thatM is expressively efficient w.r.t. N and N\u0304 , meaning: (i) any function realized by N or N\u0304 can also be realized byM with no more than linear growth in network size (number of channels in the convolutional layers); (ii) there exist functions realizable by M that cannot be realized by N or N\u0304 (or a summation thereof) unless their size (number of convolutional channels) is allowed to grow super-linearly. We study the representational abilities of networks through their corresponding tensor decompositions, which as discussed in sec. 3, parameterize discretizations of input-output mappings (grid tensors). Before laying out the problem through the lens of tensor decompositions, a few remarks are in order:\n\u2022 The number of channels in each layer ofN or N\u0304 corresponds to the constant r in the respective tree decomposition (eq. 3 with underlying mode tree T or T\u0304 respectively). Similarly, the number of channels in each layer of each interconnected network inM corresponds to r in\nthe respective mixed decomposition (eq. 4). In both the tree and mixed decompositions, r, referred to hereafter as the size constant, stands for the number of tensors {\u03c6\u03bd,\u03b3}\u03b3 (respectively {\u03c6\u0304\u03bd\u0304,\u03b3}\u03b3) held in each node \u03bd (respectively \u03bd\u0304). We set this number uniformly across nodes, corresponding to uniformly sized layers across networks, merely for simplicity of presentation.7 Our formulations and analysis can easily be adapted to account for varying layer sizes, by allowing different nodes in a decomposition to hold a different number of tensors.\n\u2022 An additional simplification we made relates to weight sharing. In both the tree and mixed decompositions, each interior node \u03bd (respectively \u03bd\u0304) has a separate set of weight vectors {a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03b3 (respectively {a\u0304\u03bd\u0304,\u03b3,I, a\u0304\u03bd\u0304,\u03b3,II}\u03b3). This implies that in the corresponding networks, convolution filters may vary through time, i.e. different weights may be used against different portions of a convolved sequence. The more commonplace setting of stationary fil-\n7. An implication of this uniform setting is that a network\u2019s input and output dimensions vary along with the size of its hidden layers. When replicating a function realized by a network using a larger network, we simply pad input vectors with zeros, and ignore the excess output coordinates.\nters (standard convolutions) is obtained by restricting different nodes in a decomposition to possess the same weights. We do not introduce such restrictions into our formulations, as they make little difference in terms of the analysis, but on the other hand significantly burden presentation.\nWe are now in a position to formulate our expressive efficiency problem in terms of tensor decompositions. Our objective is to address the following two propositions (stated informally):\nProposition 2 Consider a tree decomposition (eq. 3) with underlying mode tree T or T\u0304 and size constant r. This decomposition can be realized by a mixed decomposition of T and T\u0304 (eq. 4) whose size constant is linear in r.\nProposition 3 Consider a mixed decomposition of T and T\u0304 (eq. 4) with size constant r. This decomposition can generate grid tensors {Ay}y that cannot be generated by tree decompositions of T or T\u0304 (eq. 3), or a summation of such, unless their size constant is super-linear in r.\nBefore heading to a formal treatment of prop. 2 and 3 above, we briefly convey the intuition behind our analysis. Recall from sec. 4 that the mixed decomposition (eq. 4) blends together tree decompositions (eq. 3) of different mode trees T and T\u0304 , by traversing upwards through the trees, while exchanging tensors at each of a preselected set of mixture nodes. We may think of each mixture node as a decision point that can propagate upwards one of two computations \u2013 that carried out by T , or that carried out by T\u0304 , where in both cases, the chosen computation is propagated upwards through both T and T\u0304 . Each combination of decisions across all mixture nodes gives rise to a computational path traversing between T and T\u0304 , equivalent to a tree decomposition based on a hybrid mode tree (see illustration in fig. 4). The number of possible hybrid trees is exponential in the number of mixture nodes, and thus a mixed decomposition is comparable to an exponential ensemble of tree decompositions. The original tree decompositions, based on T and T\u0304 , are included in the ensemble, thus may easily be replicated by the mixed decomposition. On the other hand, many of the hybrid trees in the mixed decomposition are significantly different from T and T\u0304 , requiring large size constants from tree decompositions of the latters.\nAs a first step in formalizing the above intuition, we define the notion of a hybrid mode tree:\nDefinition 4 Let T and T\u0304 be binary mode trees over [N ] (def. 1), and let mix(T, T\u0304 ) be a corresponding collection of mixture nodes, i.e. a set of nodes (subsets of [N ]) contained in the interior of both T and T\u0304 . We say that H is a hybrid mode tree of T and T\u0304 w.r.t. mix(T, T\u0304 ) if it is a binary mode tree over [N ], whose interior may be generated by the following process:\nint(H) = \u2205 For \u00b5 in mix(T, T\u0304 ) \u222a {[N ]} (inclusion order):\nS = int(T ) \u2229 2\u00b5 \\ {nodes in T already assigned to S}\nS\u0304 = int(T\u0304 ) \u2229 2\u00b5 \\ {nodes in T\u0304 already assigned to S\u0304}\nint(H) = int(H)\u222aS or int(H) = int(H)\u222aS\u0304\nIn words, for every \u00b5 that is either a mixture node or the root node, int(H) includes a segment from either int(T ) or int(T\u0304 ), where the segment comprises all descendants of \u00b5 that are not descendants of any other mixture node (see illustration in fig. 4).\nClaim 5 below states that with proper weight setting, a mixed decomposition of T and T\u0304 (eq. 4) with size constant r can realize any tree decomposition (eq. 3) with size constant r/2 whose underlying mode tree is a hybrid of T and T\u0304 . Since T and T\u0304 are in particular hybrid mode trees of themselves, we obtain an affirmative answer to prop. 2.\nClaim 5 Let T and T\u0304 be binary mode trees over [N ] (def. 1), and let mix(T, T\u0304 ) be a corresponding collection of mixture nodes (a set of nodes contained in the interior of both T and T\u0304 ). Consider a mixed decomposition of T and T\u0304 w.r.t. mix(T, T\u0304 ) (eq. 4), and denote its size constant by rmix. Let H be a hybrid mode tree of T and T\u0304 w.r.t. mix(T, T\u0304 ) (def. 4), and consider the respective tree decomposition (eq. 3), with a size constant of rmix/2. For any setting of weights {a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03bd,\u03b3 leading to grid tensors {Ay}y in this tree decomposition, there exists a setting of weights {a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03bd,\u03b3 and {a\u0304\u03bd\u0304,\u03b3,I, a\u0304\u03bd\u0304,\u03b3,II}\u03bd\u0304,\u03b3 in the mixed decomposition, independent of the discretizers v(1) . . .v(M) (see sec. 3), that leads to the same grid tensors.8\nProof See app. B.1.\nClaim 5 not only addresses prop. 2, but also paves the way to a treatment of prop. 3. In other words, not only does it imply that the mixed decomposition of T and T\u0304 can realize their individual tree decompositions with a linear growth in size, but it also brings forth a strategy for proving\n8. In accordance with the remark in the beginning of this section, when using the (larger) mixed decomposition, we pad discretizers with zeros, and ignore the excess output tensors.\nthat the converse does not hold, i.e. that the tree decompositions of T and T\u0304 cannot realize their mixed decomposition without a super-linear growth in size. The aforementioned strategy is to find a hybrid mode tree H distinct enough from T and T\u0304 , such that its tree decomposition, realized by the mixed decomposition according to claim 5, poses a significant challenge for the tree decompositions of T and T\u0304 . Hereinafter we pursue this line of reasoning, focusing on the particular case where the convolutional operator g(\u00b7) is a simple product \u2013 g(a, b) = a\u00b7b. In this case the tree and mixed decompositions (eq. 3 and 4 respectively) are standard (non-generalized) tensor decompositions (\u2297g \u2261 \u2297 \u2013 see sec. 2), and the corresponding dilated convolutional networks are convolutional arithmetic circuits. We focus on this special case since it allows the use of a plurality of algebraic tools for theoretical analysis, while at the same time corresponding to models showing promising results in practice (see for example Cohen et al. (2016a); Sharir et al. (2016)). Full treatment of additional cases, such as g(a, b) = max{a + b, 0}, corresponding to networks with ReLU activation, is left for future work.\nFor establishing the difficulty experienced by the tree decompositions of T and T\u0304 in replicating that of a hybrid tree H , we analyze ranks of matricized grid tensors. Specifically, we consider the tree decomposition (eq. 3) of a general mode tree, and derive upper and lower bounds on the ranks of generated grid tensors when these are subject to matricization w.r.t. a general index set I \u2282 [N ] (see sec. 2). The bounds we derive (theorem 7 below) highly depend on both the underlying mode tree and the index set, and this allows finding index sets for which ranks tend to be higher with the hybrid mode treeH than they are with the original mode trees T and T\u0304 . The only way for the latters to match ranks generated by the former is through a significant increase in the size constant r of their tree decompositions \u2013 precisely the sought after result.\nTo succinctly phrase our central theorem, we define the notion of an index set tiled by a mode tree:\nDefinition 6 Let T be a binary mode tree over [N ] (def. 1), and let I \u2282 [N ] be a non-empty set of indexes. A tiling of I by T is a collection of nodes in the tree, denoted \u0398(I;T ), which meets the following requirements: \u2022 \u22c3 \u03bd\u2208\u0398(I;T ) \u03bd = I\n\u2022 \u03bd \u2208 \u0398(I;T ) =\u21d2 P (\u03bd;T ) 6\u2282 I"}, {"heading": "In words, \u0398(I;T ) is a set of nodes in T whose disjoint union gives I, where each node is maximal,", "text": "i.e. its parent in the tree is not a subset of I (see illustration in fig. 5).\nIt is not difficult to see that for any mode tree T and non-empty index set I, the tiling \u0398(I;T ) always exists and is determined uniquely. As the theorem below states, this tiling, along with that of I\u2019s complement (Ic := [N ] \\ I), characterizes the ranks of grid tensors generated by the tree decomposition of T when these are matricized w.r.t. I.\nTheorem 7 Let T be a binary mode tree over [N ] (def. 1), and consider the corresponding tree decomposition (eq. 3) with discretizers v(1) . . .v(M) spanning Rr. Assume that g(\u00b7) is the product operator (g(a, b) = a\u00b7b), and suppose the generated grid tensors {Ay}y are matricized (see sec. 2) w.r.t. an index set I \u2282 [N ], \u2205 6= I 6= [N ], whose complement we denote by Ic := [N ] \\ I. Then, the ranks of the grid tensor matricizations {JAyKI}y are: \u2022 no greater than rmin{|\u0398(I;T )|,|\u0398(Ic;T )|}\n\u2022 at least r|{(\u03bd1,\u03bd2)\u2208\u0398(I;T )\u00d7\u0398(Ic;T ): \u03bd1 and \u03bd2 are siblings in T with depth>1}| almost always, i.e. for all configurations of weights {a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03bd,\u03b3 but a set of Lebesgue measure zero\nProof See app. B.2.\nAs stated previously, given two binary mode trees over [N ] (def. 1) \u2013 T and T\u0304 , with a corresponding collection of mixture nodes mix(T, T\u0304 ) (set of nodes interior to both T and T\u0304 ), the bounds in theorem 7 can be used to find an index set I \u2282 [N ] and a hybrid mode tree H (def. 4), such that the tree decomposition (eq. 3) of H generates grid tensors whose ranks under matricization w.r.t. I are much higher than those brought forth by the tree decompositions of T and T\u0304 . Consider our exemplar mode trees illustrated in fig. 2. Specifically, let T be the mode tree corresponding to the baseline dilated convolutional network (dilation 2l\u22121 in layer l \u2208 [L] = [log2N ] \u2013 see sec. 3.1), and let T\u0304 be the mode tree corresponding to the network obtained by swapping dilations of even and odd layers (such that layer l has dilation 2l\u22122 if l is even, and 2l if l is odd). As described in sec. 3.2, T is a perfect binary tree whose depth-l nodes, l \u2208 {0, 1, . . . , L}, are (k\u22121)N/2l+[N/2l] for k \u2208 [2l]. T\u0304 is also perfect and has the same even-depth nodes, but its odd-depth nodes differ \u2013 they are generated by splitting parents into children holding non-contiguous quadrants. Suppose we choose mix(T, T\u0304 ) to include the set of nodes in T and T\u0304 whose depth is L-2, and consider the hybrid mode tree H formed by taking the segments (see def. 4) of the first half of these nodes from T , and the rest of the tree from T\u0304 . An illustration of T , T\u0304 and H in this setting, for the case L = 4, is given in fig. 6. Now, let the index set I consist of every second index in [N/2], and every second pair of indexes in N/2 + [N/2], i.e. I := {2k\u2212 1 : k \u2208 [N/4]} \u222a {4k\u2212 k\u2032 : k \u2208 [N/8], k\u2032 = 2, 3}. As illustrated in fig. 6, the mode tree T tiles (see def. 6) the lower half of I into singletons, and its upper half into pairs. The same applies to T \u2019s tiling of I\u2019s complement Ic := [N ] \\ I. Moreover, for every node in the former tiling \u0398(I;T ), there exists a sibling in the latter \u0398(Ic;T ) (and vice versa). By theorem 7, this implies that the tree decomposition of T generates grid tensors whose matricizations w.r.t. I have rank rN/4+N/8. A similar situation occurs with the mode tree T\u0304 , under which I and Ic are tiled into pairs in their lower halves and singletons in their top halves (see illustration in fig. 6). This also leads to matricized grid tensors of rank rN/4+N/8. On the other hand, the hybrid mode tree H tiles I and Ic entirely into singletons (see illustration in fig. 6), leading (by theorem 7) to grid tensor matricization ranks of rN/2. This means that if we were to replicate grid tensors generated by the tree decomposition of H using those of T or T\u0304 (or a summation thereof), we would need to increase the size constant r super-linearly \u2013 by a power of 4/3.\nThe above example can be generalized, by considering swapping the dilations of more than two layers at once. In particular, if T is the mode tree corresponding to the baseline dilated convolutional network (dilation 2l\u22121 in layer l), T\u0304 is the mode tree corresponding to the network obtained by swapping dilations of groups of k layers (dilation 2dl/ke\u00b7k\u22121\u2212((l\u22121) mod k) in layer l), and the set of mixture nodes includes all nodes of depth L-k, a hybrid mode tree H and an index set I can be found, such that the tree decomposition of H generates grid tensors whose ranks when matricized w.r.t. I can only be matched by the tree decompositions of T and T\u0304 if the latters\u2019 size constant r is increased by a power of 2/(1+21\u2212k). Since the mixed decomposition of T and T\u0304 (eq. 4) can realize the tree decomposition of H with double the size constant (claim 5), we conclude that it can, with size constant 2r, generate grid tensors whose matricization ranks require the tree decompositions of T and T\u0304 to have size constant r2/(1+2\n1\u2212k) \u2013 super-linearly larger. Therefore, in this particular setting, prop. 3 holds and the mixed decomposition of T and T\u0304 is indeed expressively efficient w.r.t. their tree decompositions. Taking into account the fact that the mixed decomposition admits maximal matricization ranks almost always when g(\u00b7) is the product operator (see app. C), we formalize the result in network terms:\nCorollary 8 Let N be the baseline dilated convolutional network (dilation 2l\u22121 in layer l \u2013 see sec. 3.1), and let N\u0304 be the network obtained by swapping the dilations of groups of k layers (dilation 2dl/ke\u00b7k\u22121\u2212((l\u22121) mod k) in layer l). Denote by M the mixed dilated convolutional network obtained by summing the outputs of N and N\u0304 , while interconnecting their k\u2019th intermediate layer (and possibly additional layers). Assume the networks\u2019 convolutional operator g(\u00b7) is a product. Then, besides a negligible set, all functions realized by M with r channels in the layers of each interconnected network, cannot be realized by N or N\u0304 (or a summation thereof) if the number of channels in each layer is less than (r/2)2/(1+2 1\u2212k).\nCorollary 8 (along with claim 5) demonstrates that interconnecting intermediate layers of different dilated convolutional networks can bring forth expressive efficiency. That is to say, through cross-connections between networks, we are able to represent functions that would otherwise be expensive, or even impractical to implement. The lower bound in corollary 8 \u2013 (r/2)2/(1+2\n1\u2212k), is essentially quadratic for any k \u2265 4. For example, if k = 4 and the number of channels r in each interconnected network is 128, the lower bound would imply that in order to maintain representational abilities with an individual network (or a summation of the networks), over 1500 channels in each layer are required \u2013 far beyond acceptable practice in deep learning. In the next section we demonstrate empirically that this expressive advantage indeed translates to superior accuracies, i.e. that interconnecting intermediate layers indeed boosts the performance of dilated convolutional networks."}, {"heading": "6. Experiment", "text": "To assess the practical implications of the expressive efficiency brought forth by mixing dilated convolutional networks, a simple experiment was conducted. We trained a baseline dilated convolutional networkN (dilation 2l\u22121 in layer l \u2208 [L] \u2013 see sec. 3.1), with architectural parameters similar to those used in WaveNet (van den Oord et al. (2016)), to classify individual phonemes in the TIMIT acoustic speech corpus (Garofolo et al. (1993)). In addition to this baseline model, we also trained the companion network N\u0304 obtained by swapping dilations of even and odd layers (such that layer l has dilation 2l\u22122 if l is even, and 2l if l is odd). As discussed in sec. 4, the mode trees corresponding to these networks (illustrated in fig. 2) \u2013 T and T\u0304 , share interior nodes of even depth, thus any subset of those nodes may serve as mixture nodes for a mixed decomposition (eq. 4). We evaluate mixed dilated convolutional networksM corresponding to different choices of mixture nodes (see fig. 3 for illustration of a particular case). Specifically, we consider choices of the following form:\nmix(T, T\u0304 ) := {\u03bd\u2208int(T )\u2229int(T\u0304 ) : depth of \u03bd (in T and T\u0304 ) \u2265 threshold}\nVarying the threshold yields mixed networks with a varying number of interconnections. In the extreme case mix(T, T\u0304 ) = \u2205 (high threshold), M simply sums the outputs of N and N\u0304 . As the threshold decreases interconnections between hidden layers are added \u2013 starting from hidden layer 2, then including hidden layer 4, and so on. The intuition from our analysis (sec. 5) is that additional interconnections result in a larger number of hybrid mode trees, which in turn boosts the expressive power of the mixed dilated convolutional network. As fig. 7 shows, this intuition indeed complies with the results in practice \u2013 classification accuracy improves as we add interconnections between the networks, without any additional cost in terms of computation or model capacity.\nTIMIT dataset is an acoustic-phonetic corpus comprising 6300 sentences manually labeled at the phoneme level. We split the data into train and validation sets in accordance with Halberstadt (1998), and as advised by Lee and Hon (1989), mapped the 61 possible phoneme labels into 39 and an additional \u201cgarbage\u201d label. The task was then to classify individual phonemes into one of the latter categories. Following WaveNet, we used a baseline dilated convolutional network with ReLU activation (g(a, b) = max{a+ b, 0} \u2013 see sec. 3.1), 32 channels per layer, and input vectors of dimension 256 holding one-hot quantizations of the audio signal. The number of layers L was set to 12, corresponding to an input window of N=2L=4096 samples, spanning 250ms of audio signal \u2013 standard practice with TIMIT dataset. The framework chosen for running the experiment\nwas Caffe toolbox (Jia et al. (2014)), and we used Adam optimizer (Kingma and Ba (2014)) for training (with default hyper-parameters \u2013 \u03b21 = 0.9, \u03b22 = 0.999, learning rate \u03b1 = 0.001). Models were trained for 35000 iterations with batch size 128, and the learning rate was decreased by a factor of 10 after 80% of the iterations took place. Weight decay was set to the standard value of 10\u22125. Besides the mixed dilated convolutional network M, we also evaluated the individual networks N and N\u0304 \u2013 both reached accuracies comparable toM in the case of 0 interconnections (output summation only)."}, {"heading": "7. Summary", "text": "In this paper we presented a study of the representational capacity of dilated convolutional networks, showing that interconnecting networks with different dilations can lead to expressive efficiency. In particular, we showed that even a single connection between intermediate layers can already lead to an almost quadratic expressive efficiency (theorem 7 and corollary 8), which in large-scale settings typically makes the difference between a model that is practical and one that is not.\nWe began with the dilated convolutional network underlying WaveNet model (fig. 1), referring to it as the \u201cbaseline architecture\u201d, and couching it in a tensor algebraic setting (eq. 2). The key for introducing tensors into the framework is a discretization of the network\u2019s input-output mapping \u2013 theN input vectors, that propagate through the network to form the output, are sampled from a pool of M \u201ctemplates\u201d, thereby creating a tensor with MN entries, referred to as a \u201cgrid tensor\u201d. The WaveNet model is shown (app. A) to give rise to a hierarchical decomposition of grid tensors \u2013 eq. 2.\nGiven that the tensor decomposition associated with the baseline architecture adheres to a specific tree structure, the generalization of the framework to an arbitrary tree follows quite naturally. If T represents a general binary mode tree (as defined in def. 1), then eq. 3 provides a tensor decomposition that captures various dilated convolutional networks, i.e. networks with various dilation schemes. Fig. 2(b) illustrates the type of dilation schemes we chose to focus on, obtained by swapping dilations in the scheme of the baseline architecture (illustrated in fig. 2(a)).\nArmed with a framework for describing dilated convolutional networks through mode trees and tensor decompositions, we next presented how two networks can be \u201cmixed\u201d. This is achieved by choosing a set of \u201cmixture nodes\u201d in the trees of both networks, and defining a \u201cmixed tensor decomposition\u201d (eq. 4) that: (i) at each mixture node, exchanges tensors between the decompositions\nof the two networks; (ii) at the root node, sums up the tensors from both decompositions. From a computational viewpoint, the mixing process amounts to \u201crewiring\u201d intermediate layers between the two networks, and summing their outputs. Accordingly, the requirements posed by the mixed network are virtually identical to those of running the two individual networks separately.\nThe heart of our analysis is a theoretical study of the expressive efficiency brought forth by generating a mixed networkM from two dilated convolutional networks N and N\u0304 . Establishing expressive efficiency requires proving two propositions: (i) any function realized by N or N\u0304 can also be realized byM with no more than linear growth in network size; (ii) there exist functions realizable by M that cannot be realized by N or N\u0304 (or a summation thereof) unless their size is allowed to grow super-linearly. We treat the first proposition in claim 5, and the second in theorem 7. The latter is where the centrality of tensor algebra comes into play, as it is based entirely on ranks of tensor matricizations.\nThe results of our work shed light on one of the most prominent architectural features of modern deep learning \u2013 connectivity. Empirical evidence shows that running layers in parallel with various interconnection schemes yields improved performance. What our study shows, at least in the domain of dilated convolutional networks, is that these ideas are backed by theoretical principles, and in fact, provide a powerful boost to expressiveness."}, {"heading": "Acknowledgments", "text": "This work is supported by Intel grant ICRI-CI #9-2012-6133, by ISF Center grant 1790/12, and by the European Research Council (TheoryDL project). Nadav Cohen is supported by a Google Doctoral Fellowship in Machine Learning."}, {"heading": "Appendix A. Derivation of the Baseline Decomposition", "text": "In this appendix we derive the baseline decomposition (eq. 2) \u2013 a parameterization of grid tensors (eq. 1) discretizing input-output mappings of the baseline dilated convolutional network (fig. 1). As discussed in sec. 3.1, o[t] \u2013 the network output at time t, is a function of x[t-N+1] . . .x[t] \u2013 its input over the last N := 2L time points. We would like to show that for any d1. . .dN \u2208 [M ], entry (d1, . . . , dN ) of a tensor Ay generated by eq. 2, is equal to coordinate y of network output o[t] under the following input assignment: x[t-N+1] = v(d1), . . . ,x[t] = v(dN ). To achieve this, we prove by induction that under the latter assignment, for every l \u2208 [L]\u222a{0}, j \u2208 [N/2l] and \u03b3 \u2208 [rl], coordinate \u03b3 of the network\u2019s depth-l sequence (input (x[t])t for l = 0; hidden sequence (h(l)[t])t for l \u2208 [L\u2212 1]; output (o[t])t for l = L) at time t\u2212N + j\u00b72l, is equal to entry (d(j\u22121)2l+1, . . . , d(j\u22121)2l+2l) of the tensor \u03c6l,j,\u03b3 in the baseline decomposition (eq. 2). The desired result then follows from the case l = L, j = 1, \u03b3 = y.\nWhen l = 0, the inductive hypothesis is trivial \u2013 coordinate \u03b3 of the input sequence at time t \u2212 N + j, i.e. x[t \u2212 N + j]\u03b3 , is by definition of our assignment equal to v (dj) \u03b3 \u2013 entry dj of the tensor \u03c60,j,\u03b3 (see eq. 2). Assume now that the inductive hypothesis holds whenever l = k, and consider the tensor \u03c6k+1,j,\u03b3 for some j \u2208 [N/2k+1] and \u03b3 \u2208 [rk+1]. From the baseline decomposition (eq. 2):\n\u03c6k+1,j,\u03b3 = (\u2211rk\n\u03b1=1 ak+1,\u03b3,I\u03b1 \u00b7 \u03c6k,2j\u22121,\u03b1\n) \u2297g (\u2211rk\n\u03b1=1 ak+1,\u03b3,II\u03b1 \u00b7 \u03c6k,2j,\u03b1 ) Focusing on entry (d(j\u22121)2k+1+1, . . . , d(j\u22121)2k+1+2k+1) of the left-hand side, while recalling the definition of the generalized tensor product \u2297g (sec. 2), we may write:\n\u03c6k+1,j,\u03b3d (j\u22121)2k+1+1,...,d(j\u22121)2k+1+2k+1 = g (\u2211rk\n\u03b1=1 a k+1,\u03b3,I \u03b1 \u00b7 \u03c6 k,2j\u22121,\u03b1 d (2j\u22122)2k+1,...,d(2j\u22122)2k+2k , \u2211rk \u03b1=1 a k+1,\u03b3,II \u03b1 \u00b7 \u03c6 k,2j,\u03b1 d (2j\u22121)2k+1,...,d(2j\u22121)2k+2k ) (5)\nBy our inductive assumption:\n\u03c6k,2j\u22121,\u03b1d (2j\u22122)2k+1,...,d(2j\u22122)2k+2k = h(k)[t\u2212N + (2j \u2212 1)\u00b72k]\u03b1 \u2200\u03b1 \u2208 [rk] \u03c6k,2j,\u03b1d (2j\u22121)2k+1,...,d(2j\u22121)2k+2k = h(k)[t\u2212N + 2j\u00b72k]\u03b1 \u2200\u03b1 \u2208 [rk]\nwhere we overload notation in the case k = 0, letting (h(0)[t])t stand for the input sequence (x[t])t. Plugging the latter into eq. 5, we obtain:\n\u03c6k+1,j,\u03b3d (j\u22121)2k+1+1,...,d(j\u22121)2k+1+2k+1 = g (\u2329 ak+1,\u03b3,I,h(k)[t\u2212N + (2j \u2212 1)\u00b72k] \u232a , \u2329 ak+1,\u03b3,II,h(k)[t\u2212N + 2j\u00b72k] \u232a) By the definition of the baseline dilated convolutional network (sec. 3.1), the latter expression is precisely equal to coordinate \u03b3 of the sequence (h(k+1)[t])t (or (o[t])t if k = L \u2212 1) at time t \u2212 N + j\u00b72k+1. This proves that our inductive hypothesis holds when l = k + 1, and in general."}, {"heading": "Appendix B. Deferred Proofs", "text": "B.1. Proof of Claim 5 We initiate the proof by introducing notations that will allow a more compact presentation. Hereinafter, we let {aH,\u03bd,\u03b3,I,aH,\u03bd,\u03b3,II \u2208 Rrmix/2}\u03bd\u2208int(H),\u03b3\u2208[rmix/2] stand for the weights in the tree decomposition of the hybrid mode tree H (eq. 3 with size constant r = rmix/2 and underlying mode tree given by def. 4). Similarly, we use {aT,\u03bd,\u03b3,I,aT,\u03bd,\u03b3,II \u2208 Rrmix}\u03bd\u2208int(T ),\u03b3\u2208[rmix] and {aT\u0304 ,\u03bd,\u03b3,I,aT\u0304 ,\u03bd,\u03b3,II \u2208 Rrmix}\u03bd\u2208int(T\u0304 ),\u03b3\u2208[rmix] to denote the weights, corresponding to T and T\u0304 (respectively), in the mixed decomposition (eq. 4 with size constant r = rmix). Recall that by construction (def. 4), int(H) \u2013 the interior of H , consists of different\nsegments (collections of nodes), each taken from either int(T ) or int(T\u0304 ). We define t : int(H) \u2192 {T, T\u0304} to be the function indicating which tree an interior node in H came from. Specifically, if the node \u03bd\u2208int(H) originated from T we have t(\u03bd) = T , and on the other hand, if its source is T\u0304 then t(\u03bd) = T\u0304 . By convention, feeding t(\u00b7) with an argument outside int(H) yields something that is different from both T and T\u0304 . For example, if \u03bd\u2208int(H) is the root node, i.e. \u03bd = [N ], then P (\u03bd;H) \u2013 its parent in H , is undefined and we have t(P (\u03bd;H)) 6=t(\u03bd). Similarly, if the child CI(\u03bd;H) of \u03bd\u2208int(H) is a leaf, it is outside the domain of t(\u00b7) and thus t(\u03bd)6=t(CI(\u03bd;H)).\nGiven a particular setting of weights {aH,\u03bd,\u03b3,I,aH,\u03bd,\u03b3,II}\u03bd,\u03b3 for the tree decomposition of H , we would like to show that there exists a setting of weights {aT,\u03bd,\u03b3,I,aT,\u03bd,\u03b3,II}\u03bd,\u03b3 and {aT\u0304 ,\u03bd,\u03b3,I,aT\u0304 ,\u03bd,\u03b3,II}\u03bd,\u03b3 for the mixed decomposition of T and T\u0304 , such that the latter generates grid tensors identical to those of the former. More precisely, for any collection of discretizers {v(i) \u2208 Rrmix/2}i\u2208[M ] fed into the tree decomposition ofH , leading the latter to produce grid tensors {Ay}y\u2208[rmix/2], we would like the mixed decomposition to be such that when fed with the padded discretizers {[(v(i))> 0]> \u2208 Rrmix}i\u2208[M ], the first rmix/2 grid tensors it generates are equal to {Ay}y\u2208[rmix/2]. We prove existence of the sought after weight setting constructively, by presenting an explicit procedure for assigning {aT,\u03bd,\u03b3,I,aT,\u03bd,\u03b3,II}\u03bd,\u03b3 and {aT\u0304 ,\u03bd,\u03b3,I,aT\u0304 ,\u03bd,\u03b3,II}\u03bd,\u03b3 based on {aH,\u03bd,\u03b3,I,aH,\u03bd,\u03b3,II}\u03bd,\u03b3 :\nInitialize:\naT,\u03bd,\u03b3,I = aT,\u03bd,\u03b3,II = 0 \u2200\u03bd\u2208int(T ), \u03b3 \u2208 [rmix]\naT\u0304 ,\u03bd,\u03b3,I = aT\u0304 ,\u03bd,\u03b3,II = 0 \u2200\u03bd\u2208int(T\u0304 ), \u03b3 \u2208 [rmix]\nFor \u03bd in int(H) (depth-first order):\nat(\u03bd),\u03bd,\u03b3+ 1 2 rmix,I =\n{ [ 0> (aH,\u03bd,\u03b3,I)> ]> , t(\u03bd) = t(CI(\u03bd;H))[\n(aH,\u03bd,\u03b3,I)> 0> ]> , t(\u03bd) 6= t(CI(\u03bd;H)) \u2200\u03b3 \u2208 [rmix/2]\nat(\u03bd),\u03bd,\u03b3+ 1 2 rmix,II =\n{ [ 0> (aH,\u03bd,\u03b3,II)> ]> , t(\u03bd) = t(CII(\u03bd;H))[\n(aH,\u03bd,\u03b3,II)> 0> ]> , t(\u03bd) 6= t(CII(\u03bd;H)) \u2200\u03b3 \u2208 [rmix/2]\nIf t(P (\u03bd;H)) 6= t(\u03bd) :\nSwap at(\u03bd),\u03bd,\u03b3,I \u2190\u2192 at(\u03bd),\u03bd,\u03b3+ 12 rmix,I \u2200\u03b3 \u2208 [rmix/2]\nSwap at(\u03bd),\u03bd,\u03b3,II \u2190\u2192 at(\u03bd),\u03bd,\u03b3+ 12 rmix,II \u2200\u03b3 \u2208 [rmix/2] (6)\nThe idea behind this assignment is as follows. The computation corresponding to a node in the tree decomposition of H , is carried out, in the mixed decomposition of T and T\u0304 , by the respective node in the respective source tree. That is to say, the computation of \u03bd\u2208int(H) in the tree decomposition is carried out by \u03bd\u2208int(t(\u03bd)) in the mixed decomposition. \u03bd\u2208int(t(\u03bd)) uses half (rmix/2) of its weight vectors, and in each used weight vector, half (rmix/2) of the coordinates hold actual (non-zero) values \u2013 a copy of the respective weight from \u03bd\u2208int(H). The choice of which weight vectors to use, and which coordinates to use in the active weight vectors, depends on the tree-transitioning scheme. If the parent of \u03bd in H came from the same tree as \u03bd, i.e. t(P (\u03bd;H)) = t(\u03bd), \u03bd\u2208int(t(\u03bd)) in the mixed decomposition uses weight vectors with higher indexes (\u03b3\u2208rmix/2 + [rmix/2]), as these relate to tensors that are not exchanged (see eq. 4). On the other hand, if t(P (\u03bd;H)) 6=t(\u03bd), weight vectors with lower indexes (\u03b3 \u2208 [rmix/2]) are used, so that the computations (tensors) will be sent to the opposite tree. The analogous rationale holds for the children of \u03bd in H (CI(\u03bd;H) and CII(\u03bd;H)). If a child came from the same tree as \u03bd, upper coordinates of the corresponding weight vectors are used, so that computations (tensors) coming from the present tree are collected. On the other hand, if the child came from the opposite tree, lower coordinates are used and computations (tensors) from that tree are fetched.\nAltogether, the assignment in eq. 6 meets our requirements, and thus concludes the proof.\nB.2. Proof of Theorem 7 Since we are dealing with a single particular mode tree T , we omit it from our notations throughout the proof. Specifically, we denote by CI(\u03bd) and CII(\u03bd) (instead of CI(\u03bd;T ) and CII(\u03bd;T )) the children of an interior node \u03bd\u2208int(T ); by \u0398(I) and \u0398(Ic) (instead of \u0398(I;T ) and \u0398(Ic;T )) the tilings of I and Ic (respectively) w.r.t. T (see def. 6); and by \u03c3(\u03bd)(\u00b7) (instead of \u03c3(\u03bd;T )(\u00b7)) the permutation corresponding to \u03bd\u2208int(T ) in the tree decomposition (eq. 3).\nThe first stage of the proof is to derive a matricized form of the tree decomposition, shedding light into the manner in which grid tensor matricizations {JAyKI}y are generated. As a preparatory step in this direction, we define the notion of an index set reduction. Let \u03bd \u2282 [N ] be a node in T , whose elements we denote by i1 < \u00b7 \u00b7 \u00b7 < i|\u03bd|. The reduction of I onto \u03bd is defined as follows:\nI|\u03bd := {j \u2208 [|\u03bd|] : ij \u2208 I \u2229 \u03bd} (7)\nIn words, it is the set of indexes corresponding to the intersection I \u2229 \u03bd inside \u03bd. Besides index set reduction, an additional tool we will be using is the Kronecker product \u2013 a matrix operator we denote by . For two matrices A \u2208 RM1\u00d7M2 and B \u2208 RN1\u00d7N2 , A B is the matrix in RM1N1\u00d7M2N2 holding AijBkl in row index (i\u2212 1)N1 + k and column index (j \u2212 1)N2 + l.\nConsider the central relation in the tree decomposition (eq. 3), while noticing that \u2297 \u2261 \u2297g in our setting (g(\u00b7) is the product operator \u2013 see sec. 2):\n\u03c6\u03bd,\u03b3\ufe38\ufe37\ufe37\ufe38 order 2|\u03bd|\n= \u03c3(\u03bd) ((\u2211r\n\u03b1=1 a\u03bd,\u03b3,I\u03b1 \u00b7 \u03c6CI(\u03bd),\u03b1\n) \u2297 (\u2211r\n\u03b1=1 a\u03bd,\u03b3,II\u03b1 \u00b7 \u03c6CII(\u03bd),\u03b1\n)) (8)\nSuppose we would like to matricize the tensor \u03c6\u03bd,\u03b3 w.r.t. the reduction I|\u03bd . If all elements of CI(\u03bd) were smaller than those of CII(\u03bd), the permutation \u03c3(\u03bd)(\u00b7) would be the identity (see sec. 3.2), and the following matrix relation would hold:\nJ\u03c6\u03bd,\u03b3KI|\u03bd = r\u2211r\n\u03b1=1 a\u03bd,\u03b3,I\u03b1 \u00b7 \u03c6CI(\u03bd),\u03b1 z I|CI(\u03bd)\nr\u2211r\n\u03b1=1 a\u03bd,\u03b3,II\u03b1 \u00b7 \u03c6CII(\u03bd),\u03b1 z I|CII(\u03bd)\n= (\u2211r\n\u03b1=1 a\u03bd,\u03b3,I\u03b1 \u00b7 J\u03c6CI(\u03bd),\u03b1KI|CI(\u03bd)\n) (\u2211r\n\u03b1=1 a\u03bd,\u03b3,II\u03b1 \u00b7 J\u03c6CII(\u03bd),\u03b1KI|CII(\u03bd) ) In general however, elements in CI(\u03bd) could be greater than ones in CII(\u03bd), and so eq. 8 includes a tensor mode sorting via \u03c3(\u03bd)(\u00b7). In matricized form, this amounts to rearranging rows and columns through appropriate permutation matrices Q(\u03bd) and Q\u0304(\u03bd) respectively:\nJ\u03c6\u03bd,\u03b3KI|\u03bd = Q(\u03bd) ((\u2211r\n\u03b1=1 a\u03bd,\u03b3,I\u03b1 \u00b7 J\u03c6CI(\u03bd),\u03b1KI|CI(\u03bd)\n) (\u2211r\n\u03b1=1 a\u03bd,\u03b3,II\u03b1 \u00b7 J\u03c6CII(\u03bd),\u03b1KI|CII(\u03bd)\n)) Q\u0304(\u03bd)\nWe thus arrive at the following matrix form of eq. 3, referred to as the matricized tree decomposition:\nFor j = 1. . .N :\nJ\u03c6{j},\u03b3KI|{j} = r [v(1)\u03b3 , . . . , v (M) \u03b3 ] > z I|{j} \u2200\u03b3 \u2208 [r]\nFor \u03bd in int(T ) (depth-first order): J\u03c6\u03bd,\u03b3KI|\u03bd = Q(\u03bd) (( r\u2211 \u03b1=1 a\u03bd,\u03b3,I\u03b1 J\u03c6CI(\u03bd),\u03b1KI|CI(\u03bd) ) ( r\u2211 \u03b1=1 a\u03bd,\u03b3,II\u03b1 J\u03c6CII(\u03bd),\u03b1KI|CII(\u03bd) )) Q\u0304(\u03bd) \u2200\u03b3 \u2208 [r]\nJAyKI = J\u03c6[N ],yKI|[N] \u2200y \u2208 [r] (9)\nNext, we move on to the second stage of the proof, where we establish the upper bound stated in the theorem:\nrankJAyKI \u2264 rmin{|\u0398(I)|,|\u0398(I c)|} \u2200y (10)\nWe begin by \u201cpropagating outwards\u201d the permutation matrices Q([N ]) and Q\u0304([N ]) corresponding to the root node [N ] in the matricized tree decomposition (eq. 9). Namely, for every \u03b3 \u2208 [r], we replace the matrix J\u03c6[N ],\u03b3KI|[N] by:\nB[N ],\u03b3 :=\n( r\u2211\n\u03b1=1\na[N ],\u03b3,I\u03b1 J\u03c6CI([N ]),\u03b1KI|CI([N])\n) ( r\u2211\n\u03b1=1\na[N ],\u03b3,II\u03b1 J\u03c6CII([N ]),\u03b1KI|CII([N])\n)\nand accordingly move Q([N ]) and Q\u0304([N ]) to the assignments of {JAyKI}y . This gives rise to the following decomposition:\nFor j = 1. . .N :\nJ\u03c6{j},\u03b3KI|{j} = r [v(1)\u03b3 , . . . , v (M) \u03b3 ] > z I|{j} \u2200\u03b3 \u2208 [r]\nFor \u03bd in int(T ) \\ {[N ]} (depth-first order): J\u03c6\u03bd,\u03b3KI|\u03bd = Q(\u03bd) (( r\u2211 \u03b1=1 a\u03bd,\u03b3,I\u03b1 J\u03c6CI(\u03bd),\u03b1KI|CI(\u03bd) ) ( r\u2211 \u03b1=1 a\u03bd,\u03b3,II\u03b1 J\u03c6CII(\u03bd),\u03b1KI|CII(\u03bd) )) Q\u0304(\u03bd) \u2200\u03b3 \u2208 [r]\nB[N ],\u03b3 =\n( r\u2211\n\u03b1=1\na[N ],\u03b3,I\u03b1 J\u03c6CI([N ]),\u03b1KI|CI([N])\n) ( r\u2211\n\u03b1=1\na[N ],\u03b3,II\u03b1 J\u03c6CII([N ]),\u03b1KI|CII([N])\n) \u2200\u03b3 \u2208 [r]\nJAyKI = Q([N ])B[N ],yQ\u0304([N ]) \u2200y \u2208 [r]\nConsider now CI([N ]) \u2013 a child of the root node [N ], and suppose we would like to similarly propagate outwards its permutation matrices Q(CI([N ])) and Q\u0304(CI([N ])). We may define, for every \u03b3 \u2208 [r]:\nBCI([N ]),\u03b3 :=\n( r\u2211\n\u03b1=1\naCI([N ]),\u03b3,I\u03b1 J\u03c6CI(CI([N ])),\u03b1KI|CI(CI([N]))\n) ( r\u2211\n\u03b1=1\naCI([N ]),\u03b3,II\u03b1 J\u03c6CII(CI([N ])),\u03b1KI|CII(CI([N]))\n)\nwhich in turn implies:\nB[N ],\u03b3 =\n( r\u2211\n\u03b1=1\na[N ],\u03b3,I\u03b1 Q (CI([N ]))BCI([N ]),\u03b1Q\u0304(CI([N ]))\n) ( r\u2211\n\u03b1=1\na[N ],\u03b3,II\u03b1 J\u03c6CII([N ]),\u03b1KI|CII([N])\n)\n= ( Q(CI([N ])) ( r\u2211\n\u03b1=1\na[N ],\u03b3,I\u03b1 B CI([N ]),\u03b1\n) Q\u0304(CI([N ])) ) ( r\u2211\n\u03b1=1\na[N ],\u03b3,II\u03b1 J\u03c6CII([N ]),\u03b1KI|CII([N])\n)\nNow, for any matricesA,A\u2032, B,B\u2032 such thatAA\u2032 andBB\u2032 are defined, the following equality holds: (AA\u2032) (BB\u2032) = (A A\u2032)(B B\u2032) (see Bellman (1970) for proof). We may therefore write:\nB[N ],\u03b3 =( Q(CI([N ])) I ) ((\u2211r \u03b1=1 a [N ],\u03b3,I \u03b1 BCI([N ]),\u03b1 ) (\u2211r \u03b1=1 a [N ],\u03b3,II \u03b1 J\u03c6CII([N ]),\u03b1KI|CII([N]) )) ( Q\u0304(CI([N ])) I\u0304 )\nwhere I and I\u0304 are identity matrices of appropriate sizes. Propagating outwards the matrices Q(CI([N ])) I and Q\u0304(CI([N ])) I\u0304 (while redefining B[N ],\u03b3 appropriately), we arrive at the following decomposition:\nFor j = 1. . .N :\nJ\u03c6{j},\u03b3KI|{j} = r [v(1)\u03b3 , . . . , v (M) \u03b3 ] > z I|{j} \u2200\u03b3 \u2208 [r]\nFor \u03bd in int(T ) \\ {[N ], CI([N ])} (depth-first order): J\u03c6\u03bd,\u03b3KI|\u03bd = Q(\u03bd) (( r\u2211 \u03b1=1 a\u03bd,\u03b3,I\u03b1 J\u03c6CI(\u03bd),\u03b1KI|CI(\u03bd) ) ( r\u2211 \u03b1=1 a\u03bd,\u03b3,II\u03b1 J\u03c6CII(\u03bd),\u03b1KI|CII(\u03bd) )) Q\u0304(\u03bd) \u2200\u03b3 \u2208 [r]\nBCI([N ]),\u03b3 =\n( r\u2211\n\u03b1=1\naCI([N ]),\u03b3,I\u03b1 J\u03c6CI(CI([N ])),\u03b1KI|CI(CI([N])) ) (\nr\u2211 \u03b1=1 aCI([N ]),\u03b3,II\u03b1 J\u03c6CII(CI([N ])),\u03b1KI|CII(CI([N]))\n) \u2200\u03b3 \u2208 [r]\nB[N ],\u03b3 =\n( r\u2211\n\u03b1=1\na[N ],\u03b3,I\u03b1 B CI([N ]),\u03b1\n) ( r\u2211\n\u03b1=1\na[N ],\u03b3,II\u03b1 J\u03c6CII([N ]),\u03b1KI|CII([N])\n) \u2200\u03b3 \u2208 [r]\nJAyKI = ( Q([N ])(Q(CI([N ])) I) ) B[N ],y ( (Q\u0304(CI([N ])) I\u0304)Q\u0304([N ]) ) \u2200y \u2208 [r]\nContinuing this process, we propagate outwards the permutation matrices Q(\u03bd) and Q\u0304(\u03bd) of all nodes \u03bd in the tree that are not members of the tilings \u0398(I) or \u0398(Ic) (see def. 6), and are not descendants of such. This brings forth the following decomposition:\nFor j = 1. . .N :\nJ\u03c6{j},\u03b3KI|{j} = r [v(1)\u03b3 , . . . , v (M) \u03b3 ] > z I|{j} \u2200\u03b3 \u2208 [r]\nFor \u03bd in int(T )\u2229{nodes in \u0398(I) or \u0398(Ic) or descendants of such} (depth-first order): J\u03c6\u03bd,\u03b3KI|\u03bd = Q(\u03bd) (( r\u2211 \u03b1=1 a\u03bd,\u03b3,I\u03b1 J\u03c6CI(\u03bd),\u03b1KI|CI(\u03bd) ) ( r\u2211 \u03b1=1 a\u03bd,\u03b3,II\u03b1 J\u03c6CII(\u03bd),\u03b1KI|CII(\u03bd) )) Q\u0304(\u03bd) \u2200\u03b3 \u2208 [r]\nFor \u03bd in \u0398(I) \u222a\u0398(Ic): B\u03bd,\u03b3 = J\u03c6\u03bd,\u03b3KI|\u03bd \u2200\u03b3 \u2208 [r]\nFor \u03bd in int(T )\\{nodes in \u0398(I) or \u0398(Ic) or descendants of such} (depth-first order):\nB\u03bd,\u03b3 =\n( r\u2211\n\u03b1=1\na\u03bd,\u03b3,I\u03b1 B CI(\u03bd),\u03b1\n) ( r\u2211\n\u03b1=1\na\u03bd,\u03b3,II\u03b1 B CII(\u03bd),\u03b1\n) \u2200\u03b3 \u2208 [r]\nJAyKI = A\u00b7B[N ],y\u00b7A\u0304 \u2200y \u2208 [r], for appropriate matrices A and A\u0304\nConsider now a node \u03bd\u2208int(T ) whose child belongs to a tiling \u2013 without loss of generality CI(\u03bd) belongs to \u0398(I). Notice that in this case BCI(\u03bd),\u03b1 is a column vector for every \u03b1 \u2208 [r]. We may thus define BCI(\u03bd) to be the matrix whose \u03b1\u2019th column is BCI(\u03bd),\u03b1, and get the following equalities:\nB\u03bd,\u03b3 = ( BCI(\u03bd)a\u03bd,\u03b3,I ) (\u2211r\n\u03b1=1 a\u03bd,\u03b3,II\u03b1 B\nCII(\u03bd),\u03b1 ) = ( BCI(\u03bd) I )( a\u03bd,\u03b3,I \u2211r \u03b1=1 a\u03bd,\u03b3,II\u03b1 B CII(\u03bd),\u03b1 ) where again, I is an appropriately sized identity matrix. This implies that we can propagate outwardsBCI(\u03bd) I , just as we have done with permutation matrices. Applying this procedure to all nodes in the tilings \u0398(I)\nand \u0398(Ic), we arrive at the decomposition below:\nFor \u03bd in \u0398(I): B\u03bd,\u03b3 = e(\u03b3) \u2200\u03b3 \u2208 [r]\nFor \u03bd in \u0398(Ic): B\u03bd,\u03b3 = (e(\u03b3))> \u2200\u03b3 \u2208 [r]\nFor \u03bd in int(T )\\{nodes in \u0398(I) or \u0398(Ic) or descendants of such} (depth-first order):\nB\u03bd,\u03b3 =\n( r\u2211\n\u03b1=1\na\u03bd,\u03b3,I\u03b1 B CI(\u03bd),\u03b1\n) ( r\u2211\n\u03b1=1\na\u03bd,\u03b3,II\u03b1 B CII(\u03bd),\u03b1\n) \u2200\u03b3 \u2208 [r]\nJAyKI = A\u00b7B[N ],y\u00b7A\u0304 \u2200y \u2208 [r], for appropriate matrices A and A\u0304\nNotice that for compactness in writing we made use of the fact that a\u03bd,\u03b3,I = \u2211r \u03b1=1 a \u03bd,\u03b3,II \u03b1 e\n(\u03b1), where e(\u03b1), \u03b1 \u2208 [r], is the vector in Rr holding 1 in entry \u03b1 and 0 in the rest. Note also that in this decomposition, as opposed to the previous ones, the matrices A and A\u0304 are not global constants that depend only on T . Rather, they also depend on J\u03c6\u03bd,\u03b3KI|\u03bd for tiling nodes \u03bd \u2208 \u0398(I)\u222a\u0398(Ic), and thus are ultimately determined through a hidden computation that is not specified above. This hidden computation is outside our scope, as we are only interested in the size of the matrices {B[N ],y}y . It is not difficult to see that this size is precisely r|\u0398(I)|by-r|\u0398(I c)|, meaning that the ranks of {B[N ],y}y are no more than rmin{|\u0398(I)|,|\u0398(I c)|}. Since these ranks are greater than or equal to those of {JAyKI}y , the sought after upper bound (eq. 10) indeed holds.\nIn the third and final stage of the proof, we establish the lower bound stated in the theorem, namely, that for all configurations of weights {a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03bd,\u03b3 but a set of Lebesgue measure zero:\nrankJAyKI \u2265 r|{(\u03bd1,\u03bd2)\u2208\u0398(I)\u00d7\u0398(I c): \u03bd1 and \u03bd2 are siblings in T with depth>1}| \u2200y (11)\nWe reduce the problem in three successive steps:\n\u2022 A tree decomposition (eq. 3) with a product operator g(\u00b7) admits maximal matricization ranks almost always (see app. C). Therefore, to prove that eq. 11 holds for all weight settings but a set of Lebesgue measure zero, it suffices to find a particular weight setting for which the inequality holds.\n\u2022 By assumption, the discretizers {v(i)}i\u2208[M ] span Rr. Without loss of generality, assume that {v(i)}i\u2208[r] are linearly independent, and consider the sub-tensors of {Ay}y formed by restricting their indexes to the range 1. . .r (instead of 1. . .M ). The matricizations of these sub-tensors w.r.t. I are sub-matrices of {JAyKI}y , thus any lower bound on ranks of the former matricizations immediately translates to a lower bound on ranks of the latter. Since the sub-tensors are precisely the grid tensors that would have been generated by the tree decomposition (eq. 3) had we omitted the trailing discretizers {v(i)}i\u2208[M ]\\[r], establishing eq. 11 in the case M = r proves that it holds in general (M\u2265r).\n\u2022 Bearing in mind that we assume M = r (and linear independence of {v(i)}i\u2208[r]), denote by V the rby-r matrix holding v(i) in its i\u2019th row, i.e. V := [v(1) \u00b7 \u00b7 \u00b7v(r)]>. From the tree decomposition (eq. 3) it is evident that the discretizers affect generated grid tensors only through products of the form V a\u03bd,\u03b3I\nor V a\u03bd,\u03b3II, where \u03bd is a parent of a leaf node in T . Since V is invertible ({v(i)}i\u2208[r] are linearly independent), its exact value has no effect on the class of representable grid tensors \u2013 any change it undergoes may be accounted for by the weights a\u03bd,\u03b3I and a\u03bd,\u03b3II that multiply it (these weights do not appear elsewhere in the decomposition). Accordingly, for establishing a lower bound on achievable grid tensor matricization ranks, the value of V is irrelevant (so long as it is invertible), and we may assume, without loss of generality, that V is the identity matrix, i.e. that v(i) = e(i) for all i \u2208 [r].\nTaking into account the above reductions, our objective is to show that there exists a setting of weights {a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03bd,\u03b3 , such that the following special case of the matricized tree decomposition (eq. 9) generates matricizations meeting the lower bound in eq. 11:\nFor j in I: J\u03c6{j},\u03b3KI|{j} = e(\u03b3) \u2200\u03b3 \u2208 [r]\nFor j in Ic: J\u03c6{j},\u03b3KI|{j} = (e(\u03b3))> \u2200\u03b3 \u2208 [r]\nFor \u03bd in int(T ) (depth-first order): J\u03c6\u03bd,\u03b3KI|\u03bd = Q(\u03bd) (( r\u2211 \u03b1=1 a\u03bd,\u03b3,I\u03b1 J\u03c6CI(\u03bd),\u03b1KI|CI(\u03bd) ) ( r\u2211 \u03b1=1 a\u03bd,\u03b3,II\u03b1 J\u03c6CII(\u03bd),\u03b1KI|CII(\u03bd) )) Q\u0304(\u03bd) \u2200\u03b3 \u2208 [r]\nJAyKI = J\u03c6[N ],yKI|[N] \u2200y \u2208 [r]\nSimilarly to the procedure carried out in the second stage of the proof (establishing the upper bound in eq. 10), we now propagate outwards the permutation matrices Q(\u03bd) and Q\u0304(\u03bd) corresponding to all interior nodes \u03bd\u2208int(T ). This brings forth the following decomposition:\nFor j in I: B{j},\u03b3 = e(\u03b3) \u2200\u03b3 \u2208 [r]\nFor j in Ic: B{j},\u03b3 = (e(\u03b3))> \u2200\u03b3 \u2208 [r]\nFor \u03bd in int(T ) (depth-first order):\nB\u03bd,\u03b3 =\n( r\u2211\n\u03b1=1\na\u03bd,\u03b3,I\u03b1 B CI(\u03bd),\u03b1\n) ( r\u2211\n\u03b1=1\na\u03bd,\u03b3,II\u03b1 B CII(\u03bd),\u03b1\n) \u2200\u03b3 \u2208 [r]\nJAyKI = A\u00b7B[N ],y\u00b7A\u0304 \u2200y \u2208 [r], for appropriate matrices A and A\u0304 (12)\nThe matrices A and A\u0304 in the assignments of {JAyKI}y essentially collect all permutation matrices {Q(\u03bd)}\u03bd and {Q\u0304(\u03bd)}\u03bd (respectively) that have been propagated outwards. Specifically, A (respectively A\u0304) is a product of factors, each of the form Q(\u03bd) I (respectively I Q(\u03bd)) for a different interior node \u03bd and appropriately sized identity matrix. Since permutation matrices are invertible, and since the Kronecker product between two invertible matrices is invertible as well (see Bellman (1970) for proof), we conclude that the matrices A and A\u0304 are invertible. Therefore, for every y \u2208 [r], the rank of JAyKI is equal to that of B[N ],y . It thus suffices to find a setting of weights {a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03bd,\u03b3 for which:\nrank(B[N ],\u03b3) \u2265 r|{(\u03bd1,\u03bd2)\u2208\u0398(I)\u00d7\u0398(I c): \u03bd1 and \u03bd2 are siblings in T with depth>1}| \u2200\u03b3 \u2208 [r] (13)\nDisregard the trivial case where there exist siblings \u03bd1 \u2208 \u0398(I) and \u03bd2 \u2208 \u0398(Ic) of depth 1,9 and consider the following weight setting:\n\u2022 \u03bd is a node in \u0398(I) or \u0398(Ic), or a descendant of such:\na\u03bd,\u03b3,I = a\u03bd,\u03b3,II = e(\u03b3) \u2200\u03b3 \u2208 [r]\n\u2022 \u03bd has one child in \u0398(I) and the other in \u0398(Ic):\na\u03bd,\u03b3,I = a\u03bd,\u03b3,II = e(\u03b3) \u2200\u03b3 \u2208 [r]\n9. In this case I and Ic are the children of the root node [N ], and the maximal rank of B[N ],\u03b3 is 1 for every \u03b3 \u2208 [r].\n\u2022 \u03bd is the root node [N ]: a\u03bd,\u03b3,I = a\u03bd,\u03b3,II = e(1) \u2200\u03b3 \u2208 [r]\n\u2022 \u03bd meets neither of the above (0 and 1 here denote the all-zero and all-one vectors in Rr, respectively):\na\u03bd,1,I = { 1 , CI(\u03bd) has one child in \u0398(I) and the other in \u0398(Ic) e(1) , otherwise\na\u03bd,1,II = { 1 , CII(\u03bd) has one child in \u0398(I) and the other in \u0398(Ic) e(1) , otherwise\na\u03bd,\u03b3,I = a\u03bd,\u03b3,II = 0 \u2200\u03b3 \u2208 [r] \\ {1}\nPlugging this into the decomposition in eq. 12, one readily sees that:\n\u2022 For every \u03bd \u2208 \u0398(I), {B\u03bd,\u03b3}\u03b3\u2208[r] are indicator column vectors (one entry holds 1, the rest hold 0) such that B\u03bd,\u03b3 6=B\u03bd,\u03b3\u2032 if \u03b3 6= \u03b3\u2032. The same holds for \u03bd \u2208 \u0398(Ic), but with the vectors being rows.\n\u2022 If \u03bd has one child in \u0398(I) and the other in \u0398(Ic), {B\u03bd,\u03b3}\u03b3\u2208[r] are indicator matrices, where both the row and column indexes of the active entry do not repeat as \u03b3 varies.\n\u2022 The matrices {B[N ],\u03b3}\u03b3\u2208[r] corresponding to the root node [N ] are equal to one another, given by a joint Kronecker product between all of the following:\n\u2013 B\u03bd,1 for every node \u03bd in either \u0398(I) or \u0398(Ic) which does not have a sibling in the other \u2013 \u2211r \u03b1=1B \u03bd,\u03b1 for every node \u03bd that has one child in \u0398(I) and the other in \u0398(Ic)\nAccording to the first observation above, B\u03bd,1 has rank 1 for every \u03bd in \u0398(I) or \u0398(Ic). The second observation implies that \u2211r \u03b1=1B\n\u03bd,\u03b1 has rank r for every node \u03bd that has one child in \u0398(I) and the other in \u0398(Ic). In turn, and while taking into account the rank-multiplicative property of the Kronecker product (rank(A A\u2032) = rank(A)\u00b7rank(A\u2032) \u2013 see Bellman (1970) for proof), the third observation implies:\nrank(B[N ],\u03b3) = r|{(\u03bd1,\u03bd2)\u2208\u0398(I)\u00d7\u0398(I c): \u03bd1 and \u03bd2 are siblings in T}| \u2200\u03b3 \u2208 [r]\nWe thus have found weights {a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03bd,\u03b3 for which eq. 13 holds.10 This establishes the sought after lower bound on matricization ranks (eq. 11), and completes the proof of the theorem."}, {"heading": "Appendix C. Maximality of Matricization Ranks", "text": "In the proof of theorem 7 (app. B.2), and in the derivation of corollary 8 (sec. 5), we made use of the fact that a tree or mixed decomposition (eq. 3 or 4 respectively), with a product operator g(\u00b7), admits maximal matricization ranks almost always. That is to say, for any index set I \u2282 [N ], the ranks of generated grid tensors {Ay}y when matricized w.r.t. I, attain their maximum possible values (which depend on both the decomposition and I) for all configurations of weights ({a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03bd,\u03b3 for the tree decomposition, {a\u03bd,\u03b3,I,a\u03bd,\u03b3,II}\u03bd,\u03b3 and {a\u0304\u03bd\u0304,\u03b3,I, a\u0304\u03bd\u0304,\u03b3,II}\u03bd\u0304,\u03b3 for the mixed decomposition) but a set of Lebesgue measure zero. Hereinafter we justify this assertion.\nWhen equipped with the product operator (g(a, b) = a\u00b7b), a tree or mixed decomposition generates grid tensors {Ay}y whose entries are polynomials in the decomposition weights. Therefore, for any index set I \u2282 [N ], the entries of the matricizations {JAyKI}y are, too, polynomials in the decomposition weights. Claim 9 below implies that for a particular index y, the rank of JAyKI is maximal almost always, i.e. for all weight settings but a set of measure zero. Since the union of finitely many zero measure sets is itself a zero measure set (see Jones (2001) for example), we conclude that the ranks of {JAyKI}y are jointly maximal almost always, which is what we set out to prove.\n10. This applies to all but the trivial case where I is such that there exist siblings \u03bd1 \u2208 \u0398(I) and \u03bd2 \u2208 \u0398(Ic) of depth 1 (I and Ic are the children of the root node [N ]). In the latter case the lower bound in eq. 13 can be met trivially.\nClaim 9 Let D,M1,M2 \u2208 N, and consider a polynomial function mapping weights \u03b1 \u2208 RD to matrices A(\u03b1) \u2208 RM1\u00d7M2 (\u201cpolynomial\u201d here means that all entries of A(\u03b1) are polynomials in \u03b1). Denote R = max\u03b1\u2208RD rank(A(\u03b1)), and consider the set S := {\u03b1 \u2208 RD : rank(A(\u03b1)) < R}. This set has Lebesgue measure zero.\nProof We disregard the trivial case where R = 0. Let \u03b10 be a point at which R is attained (rank(A(\u03b10)) = R), and assume without loss of generality that the top-left R\u00d7R minor of A(\u03b10), i.e. the determinant of A(\u03b10)1:R,1:R, is non-zero. The function p : RD \u2192 R defined by p(\u03b1) = det(A(\u03b1)1:R,1:R) is a polynomial, which by construction does not vanish everywhere (p(\u03b10) 6= 0). The zero set of a polynomial is either the entire space, or a set of Lebesgue measure zero (see Caron and Traynor (2005) for proof). Therefore, the zero set of p(\u00b7) has Lebesgue measure zero. Now, for every \u03b1\u2208S:\nrank(A(\u03b1)) < R =\u21d2 rank(A(\u03b1)1:R,1:R) < R =\u21d2 p(\u03b1) := det(A(\u03b1)1:R,1:R) = 0\nS is thus contained in the zero set of p(\u00b7), and therefore too, has Lebesgue measure zero."}], "references": [{"title": "Introduction to matrix analysis, volume 960", "author": ["Richard Bellman"], "venue": null, "citeRegEx": "Bellman.,? \\Q1970\\E", "shortCiteRegEx": "Bellman.", "year": 1970}, {"title": "The zero set of a polynomial", "author": ["Richard Caron", "Tim Traynor"], "venue": "WSMR Report 05-02,", "citeRegEx": "Caron and Traynor.,? \\Q2005\\E", "shortCiteRegEx": "Caron and Traynor.", "year": 2005}, {"title": "Convolutional rectifier networks as generalized tensor decompositions", "author": ["Nadav Cohen", "Amnon Shashua"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Cohen and Shashua.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Shashua.", "year": 2016}, {"title": "Deep simnets", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "Conference On Learning Theory (COLT),", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Delalleau and Bengio.,? \\Q2011\\E", "shortCiteRegEx": "Delalleau and Bengio.", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "citeRegEx": "Eldan and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2015}, {"title": "Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "author": ["John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathon G Fiscus", "David S Pallett"], "venue": "NASA STI/Recon technical report n,", "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Tensor Spaces and Numerical Tensor Calculus, volume 42 of Springer Series in Computational Mathematics", "author": ["Wolfgang Hackbusch"], "venue": null, "citeRegEx": "Hackbusch.,? \\Q2012\\E", "shortCiteRegEx": "Hackbusch.", "year": 2012}, {"title": "Heterogeneous acoustic measurements and multiple classifiers for speech recognition", "author": ["Andrew K Halberstadt"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Halberstadt.,? \\Q1998\\E", "shortCiteRegEx": "Halberstadt.", "year": 1998}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q Weinberger", "Laurens van der Maaten"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Q Weinberger"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "CoRR abs/1506.08473,", "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1610.10099,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Largescale video classification with convolutional neural networks", "author": ["Andrej Karpathy", "George Toderici", "Sanketh Shetty", "Thomas Leung", "Rahul Sukthankar", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "LeCun and Bengio.,? \\Q1995\\E", "shortCiteRegEx": "LeCun and Bengio.", "year": 1995}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["K-F Lee", "H-W Hon"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Lee and Hon.,? \\Q1989\\E", "shortCiteRegEx": "Lee and Hon.", "year": 1989}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Learning real and boolean functions: When is deep better than shallow", "author": ["Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1603.00988,", "citeRegEx": "Mhaskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar et al\\.", "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Montufar", "Yoshua Bengio"], "venue": "arXiv preprint arXiv,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "I-theory on depth vs width: hierarchical function composition", "author": ["Tomaso Poggio", "Fabio Anselmi", "Lorenzo Rosasco"], "venue": "Technical report, Center for Brains, Minds and Machines (CBMM),", "citeRegEx": "Poggio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 2015}, {"title": "Training input-output recurrent neural networks through spectral methods", "author": ["Hanie Sedghi", "Anima Anandkumar"], "venue": "arXiv preprint arXiv:1603.00954,", "citeRegEx": "Sedghi and Anandkumar.,? \\Q2016\\E", "shortCiteRegEx": "Sedghi and Anandkumar.", "year": 2016}, {"title": "Tensorial mixture models", "author": ["Or Sharir", "Ronen Tamari", "Nadav Cohen", "Amnon Shashua"], "venue": "arXiv preprint arXiv:1610.04167,", "citeRegEx": "Sharir et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharir et al\\.", "year": 2016}, {"title": "Going Deeper with Convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "DeepFace: Closing the Gap to HumanLevel Performance in Face Verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}, {"title": "Representation benefits of deep feedforward networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1509.08101,", "citeRegEx": "Telgarsky.,? \\Q2015\\E", "shortCiteRegEx": "Telgarsky.", "year": 2015}, {"title": "Wavenet: A generative model for raw audio", "author": ["A\u00e4ron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "CoRR abs/1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "arXiv preprint arXiv:1511.07122,", "citeRegEx": "Yu and Koltun.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Koltun.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Delalleau and Bengio (2011); Pascanu et al.", "startOffset": 0, "endOffset": 28}, {"referenceID": 2, "context": "Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al.", "startOffset": 0, "endOffset": 51}, {"referenceID": 2, "context": "Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al. (2014); Telgarsky (2015); Eldan and Shamir (2015); Cohen et al.", "startOffset": 0, "endOffset": 75}, {"referenceID": 2, "context": "Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al. (2014); Telgarsky (2015); Eldan and Shamir (2015); Cohen et al.", "startOffset": 0, "endOffset": 93}, {"referenceID": 2, "context": "Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al. (2014); Telgarsky (2015); Eldan and Shamir (2015); Cohen et al.", "startOffset": 0, "endOffset": 118}, {"referenceID": 2, "context": "(2014); Telgarsky (2015); Eldan and Shamir (2015); Cohen et al. (2016b); Cohen and Shashua (2016); Poggio et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al.", "startOffset": 9, "endOffset": 34}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al.", "startOffset": 9, "endOffset": 56}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.", "startOffset": 9, "endOffset": 79}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results).", "startOffset": 9, "endOffset": 345}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al.", "startOffset": 9, "endOffset": 623}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al.", "startOffset": 9, "endOffset": 641}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al.", "startOffset": 9, "endOffset": 1089}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks.", "startOffset": 9, "endOffset": 1130}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks. Our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al.", "startOffset": 9, "endOffset": 2000}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks. Our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al.", "startOffset": 9, "endOffset": 2030}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks. Our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al. (2016b) and Cohen and Shashua (2016).", "startOffset": 9, "endOffset": 2183}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks. Our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al. (2016b) and Cohen and Shashua (2016). We show that with dilated convolutional networks, the choice of dilations throughout a network corresponds to determination of the mode (dimension) tree underlying the respective decomposition.", "startOffset": 9, "endOffset": 2212}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks. Our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al. (2016b) and Cohen and Shashua (2016). We show that with dilated convolutional networks, the choice of dilations throughout a network corresponds to determination of the mode (dimension) tree underlying the respective decomposition. We then define the notion of a mixed tensor decomposition, which blends together multiple mode trees, effectively creating a large ensemble of hybrid trees formed from all possible combinations. Mixed tensor decompositions correspond to mixed dilated convolutional networks, i.e. mixtures formed by connecting intermediate layers of different dilated convolutional networks. This allows studying the expressive properties of such mixtures using mathematical machinery from the field of tensor analysis. We fully analyze a particular case of dilated convolutional arithmetic circuits, showing that a single connection between intermediate layers already leads to an almost quadratic expressive efficiency, which in large-scale settings typically makes the difference between a model that is practical and one that is not. An experiment on TIMIT speech recognition dataset (Garofolo et al. (1993)) demonstrates the gain brought forth by mixing different networks, showing that interconnectivity can indeed boost the performance of dilated convolutional networks.", "startOffset": 9, "endOffset": 3302}, {"referenceID": 2, "context": "In Cohen and Shashua (2016) a generalization of the tensor product is defined, by replacing multiplication with a general operator g(\u00b7).", "startOffset": 3, "endOffset": 28}, {"referenceID": 8, "context": "The viewpoint we adopt is actually a concrete special case of a more abstract algebraic viewpoint of tensor analysis, as presented for example in Hackbusch (2012). We limit ourselves to this concrete viewpoint since it suffices for our needs and is easier to grasp.", "startOffset": 146, "endOffset": 163}, {"referenceID": 15, "context": "Dilated Convolutional Networks Convolutional networks (LeCun and Bengio (1995)) are the cornerstone of modern deep learning, and have played a critical role in its resurgence.", "startOffset": 55, "endOffset": 79}, {"referenceID": 15, "context": "Since the work of Krizhevsky et al. (2012), nearly all state of the art systems for image and video processing, in both academia and industry, are heavily based on convolutional networks (see for example Szegedy et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 15, "context": "Since the work of Krizhevsky et al. (2012), nearly all state of the art systems for image and video processing, in both academia and industry, are heavily based on convolutional networks (see for example Szegedy et al. (2015); Taigman et al.", "startOffset": 18, "endOffset": 226}, {"referenceID": 15, "context": "Since the work of Krizhevsky et al. (2012), nearly all state of the art systems for image and video processing, in both academia and industry, are heavily based on convolutional networks (see for example Szegedy et al. (2015); Taigman et al. (2014); He et al.", "startOffset": 18, "endOffset": 249}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al. (2014); Long et al.", "startOffset": 8, "endOffset": 49}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al. (2014); Long et al. (2015)).", "startOffset": 8, "endOffset": 69}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al. (2014); Long et al. (2015)). In their basic form, convolutional networks consist of successive layers, each comprising convolutions with multiple filters followed by point-wise activation (non-linearity), which in turn is followed by spatial pooling (decimation). Recently, an alternative form of convolutional networks has emerged \u2013 dilated convolutional networks. These models are obtained by removing spatial pooling and introducing non-contiguity to convolutional filters. Although they have been used for more conventional image processing tasks (e.g. Yu and Koltun (2015)), arguably the most significant merit of dilated convolutional networks is that they thrive in application domains previously outside the realm of convolutional networks.", "startOffset": 8, "endOffset": 620}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al. (2014); Long et al. (2015)). In their basic form, convolutional networks consist of successive layers, each comprising convolutions with multiple filters followed by point-wise activation (non-linearity), which in turn is followed by spatial pooling (decimation). Recently, an alternative form of convolutional networks has emerged \u2013 dilated convolutional networks. These models are obtained by removing spatial pooling and introducing non-contiguity to convolutional filters. Although they have been used for more conventional image processing tasks (e.g. Yu and Koltun (2015)), arguably the most significant merit of dilated convolutional networks is that they thrive in application domains previously outside the realm of convolutional networks. The WaveNet model recently developed by Google (van den Oord et al. (2016)) is based on dilated convolutions applied to raw audio, and provides state of the art text-to-speech results, as well as promising phoneme recognition (speech classification) performance.", "startOffset": 8, "endOffset": 866}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al. (2014); Long et al. (2015)). In their basic form, convolutional networks consist of successive layers, each comprising convolutions with multiple filters followed by point-wise activation (non-linearity), which in turn is followed by spatial pooling (decimation). Recently, an alternative form of convolutional networks has emerged \u2013 dilated convolutional networks. These models are obtained by removing spatial pooling and introducing non-contiguity to convolutional filters. Although they have been used for more conventional image processing tasks (e.g. Yu and Koltun (2015)), arguably the most significant merit of dilated convolutional networks is that they thrive in application domains previously outside the realm of convolutional networks. The WaveNet model recently developed by Google (van den Oord et al. (2016)) is based on dilated convolutions applied to raw audio, and provides state of the art text-to-speech results, as well as promising phoneme recognition (speech classification) performance. The following ByteNet model (Kalchbrenner et al. (2016)) applies dilated convolutional networks to raw textual characters, delivering state of the art character-level language modeling, as well as excellent character-level machine translation results at a fraction of the run time required by competing methods.", "startOffset": 8, "endOffset": 1110}, {"referenceID": 22, "context": "Different choices of g(\u00b7) lead to different convolutional operators, for example g(a, b) := max{a + b, 0} leads to standard convolution followed by rectified linear activation (ReLU, Nair and Hinton (2010)), whereas g(a, b) = a\u00b7b gives rise to what is known as a convolutional arithmetic circuit (Cohen et al.", "startOffset": 183, "endOffset": 206}, {"referenceID": 3, "context": "Different choices of g(\u00b7) lead to different convolutional operators, for example g(a, b) := max{a + b, 0} leads to standard convolution followed by rectified linear activation (ReLU, Nair and Hinton (2010)), whereas g(a, b) = a\u00b7b gives rise to what is known as a convolutional arithmetic circuit (Cohen et al. (2016b)).", "startOffset": 297, "endOffset": 318}, {"referenceID": 2, "context": "Therefore, strictly speaking, the baseline decomposition is a generalized tensor decomposition, as defined in Cohen and Shashua (2016). To conclude this subsection, we relate the material above to prior works in the literature, and highlight our contributions in the text to come.", "startOffset": 110, "endOffset": 135}, {"referenceID": 2, "context": "Therefore, strictly speaking, the baseline decomposition is a generalized tensor decomposition, as defined in Cohen and Shashua (2016). To conclude this subsection, we relate the material above to prior works in the literature, and highlight our contributions in the text to come. The first work to formalize the correspondence between convolutional networks and hierarchical tensor decompositions was Cohen et al. (2016b), in which only convolutional arithmetic circuits (convolutional networks with product pooling and linear activations) were considered.", "startOffset": 110, "endOffset": 423}, {"referenceID": 2, "context": "Therefore, strictly speaking, the baseline decomposition is a generalized tensor decomposition, as defined in Cohen and Shashua (2016). To conclude this subsection, we relate the material above to prior works in the literature, and highlight our contributions in the text to come. The first work to formalize the correspondence between convolutional networks and hierarchical tensor decompositions was Cohen et al. (2016b), in which only convolutional arithmetic circuits (convolutional networks with product pooling and linear activations) were considered. Cohen and Shashua (2016) later generalized the correspondence to account for other types of convolutional networks (e.", "startOffset": 110, "endOffset": 583}, {"referenceID": 2, "context": "Therefore, strictly speaking, the baseline decomposition is a generalized tensor decomposition, as defined in Cohen and Shashua (2016). To conclude this subsection, we relate the material above to prior works in the literature, and highlight our contributions in the text to come. The first work to formalize the correspondence between convolutional networks and hierarchical tensor decompositions was Cohen et al. (2016b), in which only convolutional arithmetic circuits (convolutional networks with product pooling and linear activations) were considered. Cohen and Shashua (2016) later generalized the correspondence to account for other types of convolutional networks (e.g. ones with ReLU activation and max or average pooling) as well. The baseline decomposition above (eq. 2) \u2013 a hierarchical tensor decomposition characterizing the baseline dilated convolutional network (fig. 1), is essentially a direct outcome of the formulation presented in Cohen and Shashua (2016). Our contributions begin in the next subsection, where we establish a correspondence between hierarchical decompositions over general mode trees, and dilated convolutional networks with different dilations.", "startOffset": 110, "endOffset": 978}, {"referenceID": 3, "context": "We focus on this special case since it allows the use of a plurality of algebraic tools for theoretical analysis, while at the same time corresponding to models showing promising results in practice (see for example Cohen et al. (2016a); Sharir et al.", "startOffset": 216, "endOffset": 237}, {"referenceID": 3, "context": "We focus on this special case since it allows the use of a plurality of algebraic tools for theoretical analysis, while at the same time corresponding to models showing promising results in practice (see for example Cohen et al. (2016a); Sharir et al. (2016)).", "startOffset": 216, "endOffset": 259}, {"referenceID": 31, "context": "1), with architectural parameters similar to those used in WaveNet (van den Oord et al. (2016)), to classify individual phonemes in the TIMIT acoustic speech corpus (Garofolo et al.", "startOffset": 76, "endOffset": 95}, {"referenceID": 7, "context": "(2016)), to classify individual phonemes in the TIMIT acoustic speech corpus (Garofolo et al. (1993)).", "startOffset": 78, "endOffset": 101}, {"referenceID": 9, "context": "We split the data into train and validation sets in accordance with Halberstadt (1998), and as advised by Lee and Hon (1989), mapped the 61 possible phoneme labels into 39 and an additional \u201cgarbage\u201d label.", "startOffset": 68, "endOffset": 87}, {"referenceID": 9, "context": "We split the data into train and validation sets in accordance with Halberstadt (1998), and as advised by Lee and Hon (1989), mapped the 61 possible phoneme labels into 39 and an additional \u201cgarbage\u201d label.", "startOffset": 68, "endOffset": 125}, {"referenceID": 14, "context": "was Caffe toolbox (Jia et al. (2014)), and we used Adam optimizer (Kingma and Ba (2014)) for training (with default hyper-parameters \u2013 \u03b21 = 0.", "startOffset": 19, "endOffset": 37}, {"referenceID": 14, "context": "was Caffe toolbox (Jia et al. (2014)), and we used Adam optimizer (Kingma and Ba (2014)) for training (with default hyper-parameters \u2013 \u03b21 = 0.", "startOffset": 19, "endOffset": 88}], "year": 2017, "abstractText": "Expressive efficiency is a concept that allows formally reasoning about the representational capacity of deep network architectures. A network architecture is expressively efficient with respect to an alternative architecture if the latter must grow super-linearly in order to represent functions realized by the former. A well-known example is the exponential expressive efficiency of depth, namely, that in many cases shallow networks must grow exponentially large in order to represent functions realized by deep networks. In this paper we study the expressive efficiency brought forth by the architectural feature of connectivity, motivated by the observation that nearly all state of the art networks these days employ elaborate connection schemes, running layers in parallel while splitting and merging them in various ways. A formal treatment of this question would shed light on the effectiveness of modern connectivity schemes, and in addition, could provide new tools for network design. We focus on dilated convolutional networks, a family of deep models gaining increased attention, underlying state of the art architectures like Google\u2019s WaveNet and ByteNet. By introducing and studying the concept of mixed tensor decompositions, we prove that interconnecting dilated convolutional networks can lead to expressive efficiency. In particular, we show that a single connection between intermediate layers can already lead to an almost quadratic gap, which in large-scale settings typically makes the difference between a model that is practical and one that is not.", "creator": "LaTeX with hyperref package"}}}