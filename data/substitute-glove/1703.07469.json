{"id": "1703.07469", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "RobustFill: Neural Program Learning under Noisy I/O", "abstract": "The without that automatically builds yet machine effort from probably specified has taking practiced since both begun rest all AI. Recently, two are shifting to blocking program learning whether although significant perhaps: (s) progenitor program catalyst, all to neural channel is roomier earlier applications / revenue (I / O) examples besides love give fraction a program, this (2) neural funded induction, where a neural operates generates both outputs whether e.g. a semantic program furthermore.", "histories": [["v1", "Tue, 21 Mar 2017 23:29:47 GMT  (1571kb,D)", "http://arxiv.org/abs/1703.07469v1", "8 pages + 9 pages of supplementary material"]], "COMMENTS": "8 pages + 9 pages of supplementary material", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jacob devlin", "jonathan uesato", "surya bhupatiraju", "rishabh singh", "abdel-rahman mohamed", "pushmeet kohli"], "accepted": true, "id": "1703.07469"}, "pdf": {"name": "1703.07469.pdf", "metadata": {"source": "META", "title": "RobustFill: Neural Program Learning under Noisy I/O", "authors": ["Jacob Devlin", "Jonathan Uesato", "Surya Bhupatiraju", "Rishabh Singh", "Abdel-rahman Mohamed", "Pushmeet Kohli"], "emails": ["<jdevlin@microsoft.com>."], "sections": [{"heading": null, "text": "Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task. We additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs. Our best synthesis model achieves 92% accuracy on a real-world test set, compared to the 34% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highlyengineered rule-based system fails entirely."}, {"heading": "1. Introduction", "text": "The problem of program learning, i.e. generating a program consistent with some specification, is one of the oldest problems in machine learning and artificial intelligence\n*Equal contribution 1Microsoft Research, Redmond, Washington, USA 2MIT, New London, Cambridge, Massachusetts, USA. Correspondence to: Jacob Devlin <jdevlin@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s).\nInput String Output String john Smith Smith, Jhn DOUG Q. Macklin Macklin, Doug Frank Lee (123) LEe, Frank Laura Jane Jones Jones, Laura Steve P. Green (9) ? Program GetToken(Alpha, -1) | \u2018,\u2019 | \u2018 \u2019 | ToCase(Proper, GetToken(Alpha, 1))\nFigure 1. An anonymized example from FlashFillTest with noise (typos). The goal of the task is to fill in the blank (i.e., \u2018?\u2019 = \u2018Green, Steve\u2019). Synthesis approaches achieve this by generating a program like the one shown. Induction approaches generate the new output string directly, conditioned on the the other examples.\nWaldinger & Lee (1969); Manna & Waldinger (1975). The classical approach has been that of rule-based program synthesis (Manna & Waldinger, 1980), where a formal grammar is used to derive a program from a well-defined specification. Providing a formal specification is often more difficult than writing the program itself, so modern program synthesis methods generally rely on input/output examples (I/O examples) to act as an approximate specification. Modern rule-based synthesis methods are typically centered around hand-crafted function semantics and pruning rules to search for programs consistent with the I/O examples (Gulwani et al., 2012; Alur et al., 2013).\nThese hand-engineered systems are often difficult to extend and fragile to noise, so statistical program learning methods have recently gained popularity, with a particular focus on neural network models. This work has fallen into two overarching categories: (1) neural program synthesis, where the program is generated by a neural network conditioned on the I/O examples (Balog et al., 2016; Parisotto et al., 2017; Gaunt et al., 2016; Riedel et al., 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al., 2014; 2016; Kurach et al., 2016; Kaiser & Sutskever, 2015; Joulin & Mikolov, 2015; Reed & de Freitas, 2016; Neelakantan et al., 2016). Although many of these papers have achieved impressive results on a variety of tasks, none have thoroughly compared induction and synthesis approaches on a real-world test set. In this work, we not only demonstrate strong empirical results compared ar X\niv :1\n70 3.\n07 46\n9v 1\n[ cs\n.A I]\n2 1\nM ar\n2 01\n7\nto past work, we also directly contrast the strengths and weaknesses of both neural program learning approaches for the first time.\nThe primary task evaluated for this work is a Programming By Example (PBE) system for string transformations similar to FlashFill (Gulwani et al., 2012; Gulwani, 2011). FlashFill allows Microsoft Excel end-users to perform regular expression-based string transformations using examples without having to write complex macros. For example, a user may want to extract zip codes from a text field containing addresses, or transform a timestamp to a different format. An example is shown in Figure 1. A user manually provides a small number of example output strings to convey the desired intent and the goal of FlashFill is to generalize the examples to automatically generate the corresponding outputs for the remaining input strings. Since the end goal is to emit the correct output strings, and not a program, the task itself is agnostic to whether a synthesis or induction approach is taken.\nFor modeling, we develop novel variants of the attentional RNN architecture (Bahdanau et al., 2014) to encode a variable-length unordered set of input-output examples. For program representation, we have developed a domain-specific language (DSL), similar to that of Gulwani et al. (2012), that defines an expressive class of regular expression-based string transformations. The neural network is then used to generate a program in the DSL (for synthesis) or an output string (for induction). Both systems are trained end-to-end using a large set of input-output examples and programs uniformly sampled from the DSL.\nWe compare our neural induction model, neural synthesis model, and the rule-based architecture of Gulwani et al. (2012) on a real-world FlashFill test set. We also inject varying amounts of noise (i.e., simulated typos) into the FlashFill test examples to model the robustness of different learning approaches. While the manual approaches work reasonably well for well-formed I/O examples, we show that its performance degrades dramatically in presence of even small amounts of noise. We show that our neural architectures are significantly more robust in presence of noise and moreover obtain an accuracy comparable to manual approaches even for non-noisy examples.\nThis paper makes the following key contributions:\n\u2022 We present a novel variant of the attentional RNN architecture, which allows for encoding of a variablesize set of input-output examples. \u2022 We evaluate the architecture on 205 real-world FlashFill instances and significantly outperform the previous best statistical system (92% vs. 34% accuracy). \u2022 We compare the model to a hand-crafted synthesis algorithm and show that while both systems achieve\nsimilar performance on clean test data, our model is significantly more robust to realistic noise (with noise, 80% accuracy vs. 6% accuracy). \u2022 We compare our neural synthesis architecture with a neural induction architecture, and demonstrate that each approach has its own strengths under different evaluation metrics and decoding constraints."}, {"heading": "2. Related Work", "text": "There has been an abundance of recent work on neural program induction and synthesis.\nNeural Program Induction: Neural Turing Machine (NTM) (Graves et al., 2014) uses a neural controller to read and write to an external memory tape using soft attention and is able to learn simple algorithmic tasks such as array copying and sorting. Stack-RNNs (Joulin & Mikolov, 2015) augment a neural controller with an external stackstructured memory and is able to learn algorithmic patterns of small description length. Neural GPU (Kaiser & Sutskever, 2015) presents a Turing-complete model similar to NTM, but with a parallel and shallow design similar to that of GPUs, and is able to learn complex algorithms such as long binary multiplication. Neural ProgrammerInterpreters (Reed & de Freitas, 2016) teach a controller to learn algorithms from program traces as opposed to examples. Neural Random-Access Machines (Kurach et al., 2016) uses a continuous representation of 14 high-level modules consisting of simple arithmetic functions and reading/writing to a variable-size random-access memory to learn algorithmic tasks requiring pointer manipulation and dereferencing to memory. The domain of string transformations is different than the domains handled by these approaches and moreover, unlike RobustFill, these approaches need to be re-trained per problem instance.\nNeural Program Synthesis: The most closely related work to ours uses a Recursive-Reverse-Recursive neural network (R3NN) to learn string transformation programs from examples (Parisotto et al., 2017), and is directly compared in Section 5.1. DeepCoder (Balog et al., 2016) trains a neural network to predict a distribution over possible functions useful for a given task from input-output examples, which is used to augment an external search algorithm. Unlike DeepCoder, RobustFill performs an end-toend synthesis of programs from examples. Terpret (Gaunt et al., 2016) and Neural Forth (Riedel et al., 2016) allow programmers to write sketches of partial programs to express prior procedural knowledge, which are then completed by training neural networks on examples.\nDSL-based synthesis: Non-statistical DSL-based synthesis approaches (Gulwani et al., 2012) exploit independence properties of DSL operators to develop a divide-and-\nconquer based search algorithm with several hand-crafted pruning and ranking heuristics (Polozov & Gulwani, 2015). In this work, we present a neural architecture to automatically learn an efficient synthesis algorithm. There is also some work on using learnt clues to guide the search in DSL expansions (Menon et al., 2013), but this requires handcoded textual features of examples."}, {"heading": "3. Problem Overview", "text": "We now formally define the problem setting and the domain-specific language of string transformations."}, {"heading": "3.1. Problem Formulation", "text": "Given a set of input-output (I/O) string examples (I1, O1), ..., (In, On), and a set of unpaired input strings Iy1 , ..., I y m, the goal of of this task is to generate the corresponding output strings, Oy1 , ..., O y m. For each example set, we assume there exists at least one program P that will correctly transform all of these examples, i.e., P (I1) \u2192 O1, ..., P (Iy1 ) \u2192 O y 1 , ... Throughout this work, we refer to (Ij , Oj) as observed examples and (I y j , O y j ) as assessment examples. We use InStr and OutStr to generically refer to I/O examples that may be observed or assessment. We refer to this complete set of information as an instance:\nI1 = January O1 = jan I2 = February O2 = feb I3 = March O3 = mar Iy1 = April O y 1 = apr Iy2 = May O y 2 = may P = ToCase(Lower, SubStr(1,3))\nIntuitively, imagine that a (non-programmer) user has a large list of InStr which they wish to process in some way. The goal is to only require the user to manually create a small number of corresponding OutStr, and the system will generate the remaining OutStr automatically.\nIn the program synthesis approach, we train a neural model which takes (I1, O1), ..., (In, On) as input and generates P as output, token-by-token. It is trained fully supervised on a large corpus of synthetic I/O Example + Program pairs. It is not conditioned on the assessment input strings Iyj , but it could be in future work. At test time, the model is provided with new set of observed I/O examples and attempts to generate the corresponding P which it (maybe) has never seen in training. Crucially, the system can actually execute the generated P on each observed input string Ij and check if it produces Oj .1 If not, it knows that P cannot be the correct program, and it can search for a different P . Of course, even if P is consistent on all observed examples,\n1This execution is deterministic, not neural.\nthere is no guarantee that it will generalize to new examples (i.e., assessment examples). We can think of consistency as a necessary, but not sufficient, condition. The actual success metric is whether this program generalizes to the corresponding assessment examples, i.e., P (Iyj ) = O y j . There also may be multiple valid programs.\nIn the program induction approach, we train a neural model which takes (I1, O1), ..., (In, On) and Iy as input and generates Oy as output, character-by-character. Our current model decodes each assessment example independently. Crucially, the induction model makes no explicit use use of program P at training or test time. Instead, we say that it induces a latent representation of the program. If we had a large corpus of real-world I/O examples, we could in fact train an induction model without any explicit program representation. Since such a corpus is not available, it is trained on the same synthesized I/O Examples as the synthesis model. Note that since the program representation is latent, there is no way to measure consistency.\nWe can comparably evaluate both approaches by measuring generalization accuracy, which is the percent of test instances for which the system has successfully produced the correct OutStr for all assessment examples. For synthesis this means P (Iyj ) = O y j \u2200(I y j , O y j ). For induction this means all Oy generated by the system are exactly correct. We typically use four observed examples and six assessment examples per test instance. All six must be exactly correct for the model to get credit."}, {"heading": "3.2. The Domain Specific Language", "text": "The Domain Specific Language (DSL) used here to represent P models a rich set of string transformations based on substring extractions, string conversions, and constant strings. The DSL is similar to the DSL described in Parisotto et al. (2017), but is extended to include nested expressions, arbitrary constant strings, and a powerful regexbased substring extraction function. The syntax of the DSL is shown in Figure 2 and the formal semantics are presented in the supplementary material.\nA program P : string \u21d2 string in the DSL takes as input a string and returns another string as output. The top-level operator in the DSL is the Concat operator that concatenates a finite list of string expressions ei. A string expression e can either be a substring expression f , a nesting expression n, or a constant string expression. A substring expression can either be defined using two constant positions indices k1 and k2 (where negative indices denote positions from the right), or using the GetSpan(r1, ii, y1, r2, i2, y2) construct that returns the substring between the ith1 occurrence of regex r1 and the ith2 occurrence of regex r2, where y1 and y2 denotes either the start or end of the corresponding regex matches. The\nnesting expressions allow for further nested string transformations on top of the substring expressions allowing to extract kth occurrence of certain regex, perform casing transformations, and replacing a delimiter with another delimiter. The notation e1 | e2 | ... is sometimes used as a shorthand for Concat(e1, e2, ...). The nesting and substring expressions take a string as input (implicitly as a lambda parameter). We sometimes refer expressions such as ToCase(Lower)(v) as ToCase(Lower,v).\nThere are approximately 30 million unique string expressions e, which can be concatenated to create arbitrarily long programs. Any search method that does not encode inverse function semantics (either by hand or with a statistical model) cannot prune partial expressions. Thus, even efficient techniques like dynamic programming (DP) with black-box expression evaluation would still have to search over many millions of candidates."}, {"heading": "3.3. Training Data and Test Sets", "text": "Since there are only a few hundred real-world FlashFill instances, the data used to train the neural networks was synthesized automatically. To do this, we use a strategy of random sampling and generation. First, we randomly sample programs from our DSL, up to a maximum length (10 expressions). Given a sampled program, we compute a simple set of heuristic requirements on the InStr such that the program can be executed without throwing an exception. For example, if an expression in the program retrieves the 4th number, the InStr must have at least 4 numbers. Then, each InStr is generated as a random sequence of ASCII characters, constrained to satisfy the requirements. The corresponding OutStr is generated by executing the\nprogram on the InStr.\nFor evaluating the trained models, we use FlashFillTest, a set of 205 real-world examples collected from Microsoft Excel spreadsheets, and provided to us by the authors of Gulwani et al. (2012) and Parisotto et al. (2017). Each FlashFillTest instance has ten I/O examples, of which the first four are used as observed examples and the remaining six are used as assessment examples.2 Some examples of FlashFillTest instances are provided in the supplementary material. Intuitively, it is possible to generalize to a realword test set using randomly synthesized training because the model is learning function semantics, rather than a particular data distribution."}, {"heading": "4. Program Synthesis Model Architecture", "text": "We model program synthesis as a sequence-to-sequence generation task, along the lines of past work in machine translation (Bahdanau et al., 2014), image captioning (Xu et al., 2015), and program induction (Zaremba & Sutskever, 2014). In the most general description, we encode the observed I/O using a series of recurrent neural networks (RNN), and generate P using another RNN one token at a time. The key challenge here is that in typical sequenceto-sequence modeling, the input to the model is a single sequence. In this case, the input is a variable-length, unordered set of sequence pairs, where each pair (i.e., an I/O example) has an internal conditional dependency. We describe and evaluate several multi-attentional variants of the attentional RNN architecture (Bahdanau et al., 2014) to model this scenario."}, {"heading": "4.1. Single-Example Representation", "text": "We first consider a model which only takes a single observed example (I,O) as input, and produces a program P as output. Note that this model is not conditioned on the assessment input Iy . In all models described here, P is generated using a sequential RNN, rather than a hierarchical RNN (Parisotto et al., 2017; Tai et al., 2015).3 As demonstrated in Vinyals et al. (2015), sequential RNNs can be surprisingly strong at representing hierarchical structures.\nWe explore four increasingly complex model architectures, shown visually in Figure 3:\n\u2022 Basic Seq-to-Seq: Each sequence is encoded with a non-attentional LSTM, and the final hidden state is used as the initial hidden state of the next LSTM. \u2022 Attention-A:O and P are attentional LSTMs, withO 2In cases where less than 4 observed examples are used, only the 6 assessment examples are used to measure generalization. 3Even though the DSL does allow limited hierarchy, preliminary experiments indicated that using a hierarchical representation did not add enough value to justify the computational cost.\nattending to I and P attending to O.4\n\u2022 Attention-B: Same as Attention-A, but P uses a double attention architecture, attending to both O and I simultaneously. \u2022 Attention-C: Same as Attention-B, but I and O are bidirectional LSTMs.\nIn all cases, the InStr and OutStr are processed at the character level, so the input to I and O are character embeddings. The vocabulary consists of all 95 printable ASCII tokens.\nThe inputs and targets for the P layer is the source-codeorder linearization of the program. The vocabulary consists of 430 total program tokens, which includes all function names and parameter values, as well as special tokens for concatenation and end-of-sequence. Note that numerical parameters are also represented with embedding tokens. The model is trained to maximize the log-likelihood of the reference program P ."}, {"heading": "4.2. Double Attention", "text": "Double attention is a straightforward extension to the standard attentional architecture, similar to the multimodal attention described in Huang et al. (2016). A typical attentional layer takes the following form:\nsi = Attention(hi\u22121, xi, S)\nhi = LSTM(hi\u22121, xi, si)\nWhere S is the set of vectors being attended to, hi\u22121 is the previous recurrent state, and xi is the current input. The Attention() function takes the form of the \u201cgeneral\u201d model from Luong et al. (2015). Double attention takes the form:\nsAi = Attention(hi\u22121, xi, S A) sBi = Attention(hi\u22121, xi, s A i , S B)\nhi = LSTM(hi\u22121, xi, s A i , s B i )\nNote that sAi is concatenated to hi\u22121 when computing attention on SB , so there is a directed dependence between the two attentions. Here, SA is O and SB is I . In the LSTM, sAi and s B i are concatenated."}, {"heading": "4.3. Multi-Example Pooling", "text": "The previous section only describes an architecture for encoding a single I/O example. However, in general we assume the input to consist of multiple I/O examples. The number of I/O examples can be variable between test instances, and the examples are unordered, which suggests a pooling-based approach. Previous work (Parisotto et al., 2017) has pooled on the final encoder hidden states, but this\n4A variant where O and I are reversed performs significantly worse.\napproach cannot be used for attentional models.\nInstead, we take an approach which we refer to as late pooling. Here, each I/O example has its own layers for I , O, and P (with shared weights across examples), but the hidden states of P1, ..., Pn are pooled at each timestep before being fed into a single output softmax layer. The architecture is shown at the bottom of Figure 3. We did not find it beneficial to add another fully-connected layer or recurrent layer after pooling.\nFormally, the layers labeled \u201cFC\u201d and \u201cMaxPool\u201d perform the operation mi = MaxPoolj\u2208n(tanh(Whji)), where i is the current timestep, n is the number of observed examples, hji \u2208 Rd is the output of Pj at the timestep i, and W \u2208 Rd\u00d7d is a set of learned weights. The layer denoted as \u201cOutput Softmax\u201d performs the operation yi = Softmax(V mi), where V \u2208 Rd\u00d7v is the output weight matrix, and v is the number of tokens in the program vocabulary. The model is trained to maximize the log-softmax of the reference program sequence, as is standard."}, {"heading": "4.4. Hyperparameters and Training", "text": "In all experiments, the size of the recurrent and fully connected layers is 512, and the size of the embeddings is 128. Models were trained with plain SGD + gradient clipping. All models were trained for 2 million minibatch updates, where each minibatch contained 128 training instances (i.e., 128 programs with four I/O examples each). Each minibatch was re-sampled, so the model saw 256 million random programs and 1024 million random I/O examples during training. Training took approximately 24 hours of 2 Titan X GPUs, using an in-house toolkit. A small\namount of hyperparameter tuning was done on a synthetic validation set that was generated like the training."}, {"heading": "5. Program Synthesis Results", "text": "Once training is complete, the synthesis models can be decoded with a beam search decoder (Sutskever et al., 2014). Unlike a typical sequence generation task, where the model is decoded with a beam k and then only the 1-best output is taken, here all k-best candidates are executed one-by-one to determine consistency. If multiple program candidates are consistent with all observed examples, the program with the highest model score is taken as the output.5 This program is referred to as P \u2217.\nIn addition to standard beam search, we also propose a variant referred to as \u201cDP-Beam,\u201d which adds a search constraint similar to the dynamic programming algorithm mentioned in Section 3.3. Here, each time an expression is completed during the search, the partial program is executed in a black-box manner. If any resulting partial OutStr is not a string prefix of the observed OutStr, the partial program is removed from the beam. This technique is effective because our DSL is largely concatenative.\nGeneralization accuracy is computed by applying P \u2217 to all six assessment examples. The percentage score reported in the figures represents the proportion of test instances for which a consistent program was found and it resulted in the exact correct output for all six assessment examples. Consistency is evaluated in Section 5.2.\nResults are shown in Figure 4. The most evident result is that all attentional variants outperform the basic seq-toseq model by a very large margin \u2013 roughly 25% absolute improvement. The difference between the three variants is smaller, but there is a clear improvement in accuracy as the models progress in complexity. Both AttentionB and Attention-C each add roughly 2-5% absolute accu-\n5We tried several alternative heuristics, such as taking the shortest program, but these did not perform better.\nracy, and this improvement appears even for a large beam. The DP-Beam variant also improves accuracy by roughly 5%. Overall, the best absolute accuracy achieved is 92% by Attention-C-DP w/ Beam=1000. Although we have not optimized our decoder for speed, the amortized end-to-end cost of decoding is roughly 0.3 seconds per test instance for Attention-C-DP w/ Beam=100 and four observed examples (89% accuracy), on a Titan X GPU."}, {"heading": "5.1. Comparison to Past Work", "text": "Prior to this work, the strongest statistical model for solving FlashFillTest was Parisotto et al. (2017). The generalization accuracy is shown below:\nSystem Beam 100 1000 Parisotto et al. (2017) 23% 34% Basic Seq-to-Seq 51% 56% Attention-C 83% 86% Attention-C-DP 89% 92%\nWe believe that this improvement in accuracy is due to several reasons. First, late pooling allows us to effectively incorporate powerful attention mechanisms into our model. Because the architecture in Parisotto et al. (2017) performed pooling at the I/O encoding level, it could not exploit the attention mechanisms which we show our critical to achieving high accuracy. Second, the DSL used here is more expressive, especially the GetSpan() function, which was required to solve approximately 20% of the test instances. 6\nComparison to the FlashFill implementation currently deployed in Microsoft Excel is given in Section 7."}, {"heading": "5.2. Consistency vs. Generalization Results", "text": "6However, this increased the search space of the DSL by 10x.\nfor both beam sizes, although this is significantly more pronounced for a Beam=100. Interestingly, the consistency is relatively constant when the number of observed examples increases. There was no a priori expectation about whether consistency would increase or decrease, since more examples are consistent with fewer total programs, but also give the network a stronger input signal. Finally, we can see that the Beam=1 decoding only generates consistent output roughly 50% of the time, which implies that the latent function semantics learned by the model are still far from perfect."}, {"heading": "6. Program Induction Results", "text": "An alternative approach to solving the FlashFill problem is program induction, where the output string is generated directly by the neural network without the need for a DSL. More concretely, we can train a neural network which takes as input a set of n observed examples (I1, O1), ...(In, On) as well an unpaired InStr, Iy , and generates the corresponding OutStr, Oy . As an example, from Figure 1, I1 = \u201cjohn Smith\u201d, O1 = \u201cSmith, Jhn\u201d, I2 = \u201cDOUG Q. Macklin\u201d, ... , Iy = \u201cSteve P. Green\u201d, Oy = \u201cGreen, Steve\u201d. Both approaches have the same end goal \u2013 determine the Oy corresponding to Iy \u2013 but have several important conceptual differences.\nThe first major difference is that the induction model does not use the program P anywhere. The synthesis model generates P , which is executed by the DSL to produced Oy . The induction model generates Oy directly by sequentially predicting each character. In fact, in cases where it is possible to obtain a very large amount of real-world I/O example sets, induction is a very appealing approach since it does not require an explicit DSL.7 The core idea is the model learns some latent program representation which can generalize beyond a specific DSL. It also eliminates the need to hand-design the DSL, unless the DSL is needed to synthesize training data.\nThe second major difference is that program induction has no concept of consistency. As described previously, in program synthesis, a k-best list of program candidates is executed one-by-one, and the first program consistent with all observed examples is taken as the output. As shown in Section 5.2, if a consistent program can be found, it is likely to generalize to new inputs. Program induction, on the other hand, is essentially a standard sequence generation task akin to neural machine translation or image captioning \u2013 we directly decode Oy with a beam search and take the highest-scoring candidate as our output.\n7In the results shown here, the induction model is trained on data synthesized with the DSL, but the model training is agnostic to this fact."}, {"heading": "6.1. Comparison of Induction and Synthesis Models", "text": "Despite these differences, it is possible to model both approaches using nearly-identical network architectures. The induction model evaluated here is identical to synthesis Attention-A with late pooling, except for the following two modifications:\n1. Instead of generating P , the system generates the new OutStr Oy character-by-character. 2. There is an additional LSTM to encode Iy . The decoder layer Oy uses double attention on Oj and Iy .\nThe induction network diagram is given in the supplementary material. Each (Iy, Oy) pair is decoded independently, but conditioned on all observed examples. The attention, pooling, hidden sizes, training details, and decoder are otherwise identical to synthesis. The induction model was trained on the same synthetic data as the synthesis models.\nResults are shown in Figure 6. The induction model is compared to synthesis Attention-A using the same measure of generalization accuracy as previous sections \u2013 all six assessment examples must be exactly correct. Induction performs similarly to synthesis w/ beam=1, but both are significantly outperformed by synthesis w/ beam=100. The generalization accuracy achieved by the induction model is 53%, compared to 81% for the synthesis model. The induction model uses a beam of 3, and does not improve with a larger search because there is no way to evaluate candidates after decoding."}, {"heading": "6.2. Average-Example Accuracy", "text": "All previous sections have used a strict definition of \u201cgeneralization accuracy,\u201d requiring all six assessment examples to be exactly correct. We refer to this as all-example accuracy. However, another useful metric is to measure the total percent of correct assessment examples, averaged over all instances.8 With this metric, generalizing on 5-out-of-6 assessment examples accumulates more credit than 0. We\n8The example still must be exactly correct \u2013 character edit rate is not measured here.\nrefer to this as average-example accuracy.\nIt is difficult to suggest which metric should be given more credence, since the utility depends on the downstream application. For example, if a user wanted to automatically fill in an entire column in a spreadsheet, they may prioritize all-example accuracy \u2013 If the system proposes a solution, they can be confident it will be correct for all rows. However, if the application instead offered auto-complete suggestions on a per-cell basis, then a model with higher average-example accuracy might be preferred."}, {"heading": "7. Handling Noisy I/O Examples", "text": "For the FlashFill task, real-world I/O examples are typically manually composed by the user, so noise (e.g., typos) is expected and should be well-handled. An example is given in Figure 1.\nBecause neural network methods (1) are inherently probabilistic, and (2) operate in a continuous space representation, it is reasonable to believe that they can learn to be robust to this type of noise. In order to explicitly account for noise, we only made two small modifications. First, noise was synthetically injected into the training data using random character transformations.9 Second, the best program P \u2217 was selected by using character edit rate (CER) (Marzal & Vidal, 1993) to the observed examples, rather\n9This did not degrade the results on the noise-free test set.\nthan exact match.10\nSince the FlashFillTest set does not contain any noisy examples, noise was synthetically injected into the observed examples. All noise was applied with uniform random probability into the InStr or OutStr using character insertions, deletions, or substitutions. Noise is not applied to the assessment examples, as this would make evaluation impossible.\nWe compare the models in this paper to the actual FlashFill implementation found in Microsoft Excel, as described in Gulwani et al. (2012). An overview of this model is described in Section 2. The results were obtained using a macro in Microsoft Excel 2016.\nThe noise results are shown in Figure 8. The neural models behave very similarly, each degrading approximately 2% absolute accuracy for each noise character introduced. The behavior of Excel FlashFill is quite different. Without noise, it achieves 92% accuracy,11 matching the best result reported earlier in this paper. However, with just one or two characters of noise, Excel FlashFill is effectively \u201cbroken.\u201d This result is expected, since the efficiency of their algorithm is critically centered around exact string matching (Gulwani et al., 2012). We believe that this robustness to noise is one of the strongest attributes of DNN-based approaches to program synthesis."}, {"heading": "8. Conclusions", "text": "We have presented a novel variant of an attentional RNN architecture for program synthesis which achieves 92% accuracy on a real-world Programming By Example task. This matches the performance of a hand-engineered system and outperforms the previous-best neural synthesis model by 58%. Moreover, we have demonstrated that our model remains robust to moderate levels of noise in the I/O examples, while the hand-engineered system fails for even small amounts of noise. Additionally, we carefully contrasted our\n10Standard beam is also used instead of DP-Beam. 11FlashFill was manually developed on this exact set.\nneural program synthesis system with a neural program induction system, and showed that even though the synthesis system performs better on this task, both approaches have their own strength under certain evaluation conditions. In particular, synthesis systems have an advantage when evaluating if all outputs are correct, while induction systems have strength when evaluating which system has the most correct outputs."}, {"heading": "A. DSL Extended Description", "text": "Section 3.2 of the paper provides the grammar of our domain specific language, which both defines the space of possible programs, and allows us to easily sample programs. The formal semantics of this language are defined below in Figure 9. The program takes as input a string v and produces a string as output (result of Concat operator).\nAs an implementational detail, we note that after sampling a program from the grammar, we flatten calls to nesting functions (as defined in Figure 2 of the paper) into a single token. For example, the function GetToken(t, i)would be tokenized as a single token GetTokent,i rather than 3 separate tokens. This is possible because for nesting functions, the size of the total parameter space is small. For all other functions, the parameter space is too large for us to flatten function calls without dramatically increasing the vocabulary size, so we treat parameters as separate tokens."}, {"heading": "B. Synthetic Evaluation Details", "text": "Results on synthetically generated examples are largely omitted from the paper since, in a vacuum, the synthetic dataset can be made arbitrarily easy or difficult via different generation procedures, making summary statistics difficult to interpret. We instead report results on an external real-world dataset to verify that the model has learned function semantics which are at least as expressive as programs observed in real data.\nNevertheless, we include additional details about our experiments on synthetically generated programs for readers interested in the details of our approach. As described in the paper, programs were randomly generated from the DSL by first determining a program length up to a maximum of 10 expressions, and then independently sampling each expression. We used a simple set of heuristics to restrict potential inputs to strings which will produce non-empty outputs (e.g. any program which references the third occurrence of a number will cause us to sample strings containing at least three numbers). We rejected any degenerate samples e.g. those resulting in empty outputs, or outputs longer than 100 characters.\nFigure 12 shows several random synthetically generated samples.\nFigure 10 shows the accuracy of each model on the synthetically generated validation set. Model accuracy on the synthetic validation set is generally consistent with accuracy on the FlashFill dataset, with stronger models on the synthetic dataset also demonstrating stronger performance on the real-world data."}, {"heading": "C. Examples of Synthesized Programs", "text": "Figure 13 shows several randomly sampled (anonymized) examples from the FlashFill test set, along with their predicted programs outputted by the synthesis model.\nFigure 14 shows several examples which were hand-selected to demonstrate interesting limitations of the model. In the case of the first example, the task is to reformat international telephone numbers. Here, the task is underconstrained given the observed input-output examples, because there are many different programs which are consistent with the observed examples. Note that to extract the first two digits, there are many other possible functions which would produce the correct output in the observed examples, some of which would generalize and some which would not: for exampling, getting the second and third characters, getting the first two digits, or getting the first number. In this case, the predicted program extracts the country code by taking the first two digits, a strategy which fails to generalize to examples with different country codes. The third example demonstrates a difficulty of using real world data. Because examples can come from a variety of sources, they may be irregularly formatted. In this case, although the program is consistent with the observed examples, it does not generalize when the second space in the address is removed. In the final example, the synthesis model completely fails, and none of the 100 highest scoring programs from the model were consistent with the observed output examples. The selected program is the closest program scored by character edit distance.\nD. Induction Network Architecture The network architecture used in the program induction setting is described in Section 6.1 of the paper. The network structure is a modification of synthesis Attention-A, using double attention to jointly attend to Ix andOj , and an additional LSTM to encode Ix. We include a complete diagram below in Figure 11."}], "references": [{"title": "Syntax-guided synthesis", "author": ["Alur", "Rajeev", "Bodik", "Rastislav", "Juniwal", "Garvit", "Martin", "Milo MK", "Raghothaman", "Mukund", "Seshia", "Sanjit A", "Singh", "Rishabh", "Solar-Lezama", "Armando", "Torlak", "Emina", "Udupa", "Abhishek"], "venue": null, "citeRegEx": "Alur et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alur et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Deepcoder: Learning to write programs", "author": ["Balog", "Matej", "Gaunt", "Alexander L", "Brockschmidt", "Marc", "Nowozin", "Sebastian", "Tarlow", "Daniel"], "venue": "arXiv preprint arXiv:1611.01989,", "citeRegEx": "Balog et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Balog et al\\.", "year": 2016}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Gaunt", "Alexander L", "Brockschmidt", "Marc", "Singh", "Rishabh", "Kushman", "Nate", "Kohli", "Pushmeet", "Taylor", "Jonathan", "Tarlow", "Daniel"], "venue": null, "citeRegEx": "Gaunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gaunt et al\\.", "year": 2016}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Graves", "Alex", "Wayne", "M Greg", "T Reynolds", "I Harley", "A Danihelka", "SG Grabska-Barwiska", "E Colmenarejo", "T Grefenstette", "J Ramalho", "Agapiou", "AP", "Badia"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Automating string processing in spreadsheets using input-output examples", "author": ["Gulwani", "Sumit"], "venue": "In ACM SIGPLAN Notices. ACM,", "citeRegEx": "Gulwani and Sumit.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani and Sumit.", "year": 2011}, {"title": "Spreadsheet data manipulation using examples", "author": ["Gulwani", "Sumit", "Harris", "William R", "Singh", "Rishabh"], "venue": "Communications of the ACM,", "citeRegEx": "Gulwani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gulwani et al\\.", "year": 2012}, {"title": "Attention-based multimodal neural machine translation", "author": ["Huang", "Po-Yao", "Liu", "Frederick", "Shiang", "Sz-Rung", "Oh", "Jean", "Dyer", "Chris"], "venue": "In Proceedings of the First Conference on Machine Translation, Berlin,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "In NIPS, pp", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong", "Minh-Thang", "Pham", "Hieu", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Knowledge and reasoning in program synthesis", "author": ["Manna", "Zohar", "Waldinger", "Richard"], "venue": "Artificial intelligence,", "citeRegEx": "Manna et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Manna et al\\.", "year": 1975}, {"title": "A deductive approach to program synthesis", "author": ["Manna", "Zohar", "Waldinger", "Richard"], "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),", "citeRegEx": "Manna et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Manna et al\\.", "year": 1980}, {"title": "Computation of normalized edit distance and applications", "author": ["Marzal", "Andres", "Vidal", "Enrique"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Marzal et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marzal et al\\.", "year": 1993}, {"title": "A machine learning framework for programming by example", "author": ["Menon", "Aditya Krishna", "Tamuz", "Omer", "Gulwani", "Sumit", "Lampson", "Butler W", "Kalai", "Adam"], "venue": "In ICML, pp", "citeRegEx": "Menon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Menon et al\\.", "year": 2013}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Neelakantan", "Arvind", "Le", "Quov V", "Sutskever", "Ilya"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Neuro-symbolic program synthesis", "author": ["Parisotto", "Emilio", "Mohamed", "Abdel-rahman", "Singh", "Rishabh", "Li", "Lihong", "Zhou", "Dengyong", "Kohli", "Pushmeet"], "venue": null, "citeRegEx": "Parisotto et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2017}, {"title": "Flashmeta: a framework for inductive program synthesis", "author": ["Polozov", "Oleksandr", "Gulwani", "Sumit"], "venue": "In OOPSLA,", "citeRegEx": "Polozov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Polozov et al\\.", "year": 2015}, {"title": "Programming with a differentiable forth", "author": ["Riedel", "Sebastian", "Bosnjak", "Matko", "Rockt\u00e4schel", "Tim"], "venue": "interpreter. CoRR,", "citeRegEx": "Riedel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "\u0141ukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In NIPS,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "A step toward automatic program writing", "author": ["Waldinger", "Richard J", "Lee", "Richard C.T. Prow"], "venue": "In IJCAI,", "citeRegEx": "Waldinger et al\\.,? \\Q1969\\E", "shortCiteRegEx": "Waldinger et al\\.", "year": 1969}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Modern rule-based synthesis methods are typically centered around hand-crafted function semantics and pruning rules to search for programs consistent with the I/O examples (Gulwani et al., 2012; Alur et al., 2013).", "startOffset": 172, "endOffset": 213}, {"referenceID": 0, "context": "Modern rule-based synthesis methods are typically centered around hand-crafted function semantics and pruning rules to search for programs consistent with the I/O examples (Gulwani et al., 2012; Alur et al., 2013).", "startOffset": 172, "endOffset": 213}, {"referenceID": 2, "context": "This work has fallen into two overarching categories: (1) neural program synthesis, where the program is generated by a neural network conditioned on the I/O examples (Balog et al., 2016; Parisotto et al., 2017; Gaunt et al., 2016; Riedel et al., 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al.", "startOffset": 167, "endOffset": 252}, {"referenceID": 16, "context": "This work has fallen into two overarching categories: (1) neural program synthesis, where the program is generated by a neural network conditioned on the I/O examples (Balog et al., 2016; Parisotto et al., 2017; Gaunt et al., 2016; Riedel et al., 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al.", "startOffset": 167, "endOffset": 252}, {"referenceID": 3, "context": "This work has fallen into two overarching categories: (1) neural program synthesis, where the program is generated by a neural network conditioned on the I/O examples (Balog et al., 2016; Parisotto et al., 2017; Gaunt et al., 2016; Riedel et al., 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al.", "startOffset": 167, "endOffset": 252}, {"referenceID": 18, "context": "This work has fallen into two overarching categories: (1) neural program synthesis, where the program is generated by a neural network conditioned on the I/O examples (Balog et al., 2016; Parisotto et al., 2017; Gaunt et al., 2016; Riedel et al., 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al.", "startOffset": 167, "endOffset": 252}, {"referenceID": 4, "context": ", 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al., 2014; 2016; Kurach et al., 2016; Kaiser & Sutskever, 2015; Joulin & Mikolov, 2015; Reed & de Freitas, 2016; Neelakantan et al., 2016).", "startOffset": 134, "endOffset": 283}, {"referenceID": 15, "context": ", 2016), and (2) neural program induction, where network learns to generate the output directly using a latent program representation (Graves et al., 2014; 2016; Kurach et al., 2016; Kaiser & Sutskever, 2015; Joulin & Mikolov, 2015; Reed & de Freitas, 2016; Neelakantan et al., 2016).", "startOffset": 134, "endOffset": 283}, {"referenceID": 7, "context": "The primary task evaluated for this work is a Programming By Example (PBE) system for string transformations similar to FlashFill (Gulwani et al., 2012; Gulwani, 2011).", "startOffset": 130, "endOffset": 167}, {"referenceID": 1, "context": "For modeling, we develop novel variants of the attentional RNN architecture (Bahdanau et al., 2014) to encode a variable-length unordered set of input-output examples.", "startOffset": 76, "endOffset": 99}, {"referenceID": 1, "context": "For modeling, we develop novel variants of the attentional RNN architecture (Bahdanau et al., 2014) to encode a variable-length unordered set of input-output examples. For program representation, we have developed a domain-specific language (DSL), similar to that of Gulwani et al. (2012), that defines an expressive class of regular expression-based string transformations.", "startOffset": 77, "endOffset": 289}, {"referenceID": 7, "context": "We compare our neural induction model, neural synthesis model, and the rule-based architecture of Gulwani et al. (2012) on a real-world FlashFill test set.", "startOffset": 98, "endOffset": 120}, {"referenceID": 4, "context": "Neural Program Induction: Neural Turing Machine (NTM) (Graves et al., 2014) uses a neural controller to read and write to an external memory tape using soft attention and is able to learn simple algorithmic tasks such as array copying and sorting.", "startOffset": 54, "endOffset": 75}, {"referenceID": 16, "context": "Neural Program Synthesis: The most closely related work to ours uses a Recursive-Reverse-Recursive neural network (R3NN) to learn string transformation programs from examples (Parisotto et al., 2017), and is directly compared in Section 5.", "startOffset": 175, "endOffset": 199}, {"referenceID": 2, "context": "DeepCoder (Balog et al., 2016) trains a neural network to predict a distribution over possible functions useful for a given task from input-output examples, which is used to augment an external search algorithm.", "startOffset": 10, "endOffset": 30}, {"referenceID": 3, "context": "Terpret (Gaunt et al., 2016) and Neural Forth (Riedel et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 18, "context": ", 2016) and Neural Forth (Riedel et al., 2016) allow programmers to write sketches of partial programs to express prior procedural knowledge, which are then completed by training neural networks on examples.", "startOffset": 25, "endOffset": 46}, {"referenceID": 7, "context": "DSL-based synthesis: Non-statistical DSL-based synthesis approaches (Gulwani et al., 2012) exploit independence properties of DSL operators to develop a divide-and-", "startOffset": 68, "endOffset": 90}, {"referenceID": 14, "context": "There is also some work on using learnt clues to guide the search in DSL expansions (Menon et al., 2013), but this requires handcoded textual features of examples.", "startOffset": 84, "endOffset": 104}, {"referenceID": 16, "context": "The DSL is similar to the DSL described in Parisotto et al. (2017), but is extended to include nested expressions, arbitrary constant strings, and a powerful regexbased substring extraction function.", "startOffset": 43, "endOffset": 67}, {"referenceID": 7, "context": "For evaluating the trained models, we use FlashFillTest, a set of 205 real-world examples collected from Microsoft Excel spreadsheets, and provided to us by the authors of Gulwani et al. (2012) and Parisotto et al.", "startOffset": 172, "endOffset": 194}, {"referenceID": 7, "context": "For evaluating the trained models, we use FlashFillTest, a set of 205 real-world examples collected from Microsoft Excel spreadsheets, and provided to us by the authors of Gulwani et al. (2012) and Parisotto et al. (2017). Each FlashFillTest instance has ten I/O examples, of which the first four are used as observed examples and the remaining six are used as assessment examples.", "startOffset": 172, "endOffset": 222}, {"referenceID": 1, "context": "We model program synthesis as a sequence-to-sequence generation task, along the lines of past work in machine translation (Bahdanau et al., 2014), image captioning (Xu et al.", "startOffset": 122, "endOffset": 145}, {"referenceID": 23, "context": ", 2014), image captioning (Xu et al., 2015), and program induction (Zaremba & Sutskever, 2014).", "startOffset": 26, "endOffset": 43}, {"referenceID": 1, "context": "We describe and evaluate several multi-attentional variants of the attentional RNN architecture (Bahdanau et al., 2014) to model this scenario.", "startOffset": 96, "endOffset": 119}, {"referenceID": 16, "context": "In all models described here, P is generated using a sequential RNN, rather than a hierarchical RNN (Parisotto et al., 2017; Tai et al., 2015).", "startOffset": 100, "endOffset": 142}, {"referenceID": 20, "context": "In all models described here, P is generated using a sequential RNN, rather than a hierarchical RNN (Parisotto et al., 2017; Tai et al., 2015).", "startOffset": 100, "endOffset": 142}, {"referenceID": 16, "context": "In all models described here, P is generated using a sequential RNN, rather than a hierarchical RNN (Parisotto et al., 2017; Tai et al., 2015).3 As demonstrated in Vinyals et al. (2015), sequential RNNs can be surprisingly strong at representing hierarchical structures.", "startOffset": 101, "endOffset": 186}, {"referenceID": 8, "context": "Double attention is a straightforward extension to the standard attentional architecture, similar to the multimodal attention described in Huang et al. (2016). A typical attentional layer takes the following form:", "startOffset": 139, "endOffset": 159}, {"referenceID": 10, "context": "The Attention() function takes the form of the \u201cgeneral\u201d model from Luong et al. (2015). Double attention takes the form:", "startOffset": 68, "endOffset": 88}, {"referenceID": 16, "context": "Previous work (Parisotto et al., 2017) has pooled on the final encoder hidden states, but this", "startOffset": 14, "endOffset": 38}, {"referenceID": 19, "context": "Once training is complete, the synthesis models can be decoded with a beam search decoder (Sutskever et al., 2014).", "startOffset": 90, "endOffset": 114}, {"referenceID": 16, "context": "Prior to this work, the strongest statistical model for solving FlashFillTest was Parisotto et al. (2017). The generalization accuracy is shown below:", "startOffset": 82, "endOffset": 106}, {"referenceID": 16, "context": "System Beam 100 1000 Parisotto et al. (2017) 23% 34% Basic Seq-to-Seq 51% 56% Attention-C 83% 86% Attention-C-DP 89% 92%", "startOffset": 21, "endOffset": 45}, {"referenceID": 16, "context": "Because the architecture in Parisotto et al. (2017) performed pooling at the I/O encoding level, it could not exploit the attention mechanisms which we show our critical to achieving high accuracy.", "startOffset": 28, "endOffset": 52}, {"referenceID": 7, "context": "We compare the models in this paper to the actual FlashFill implementation found in Microsoft Excel, as described in Gulwani et al. (2012). An overview of this model is described in Section 2.", "startOffset": 117, "endOffset": 139}, {"referenceID": 7, "context": "\u201d This result is expected, since the efficiency of their algorithm is critically centered around exact string matching (Gulwani et al., 2012).", "startOffset": 119, "endOffset": 141}], "year": 2017, "abstractText": "The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task. We additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs. Our best synthesis model achieves 92% accuracy on a real-world test set, compared to the 34% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highlyengineered rule-based system fails entirely.", "creator": "LaTeX with hyperref package"}}}