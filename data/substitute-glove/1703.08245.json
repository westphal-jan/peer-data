{"id": "1703.08245", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "On the Robustness of Convolutional Neural Networks to Internal Architecture and Weight Perturbations", "abstract": "Deep convolutional insertion networks are particularly distinction been boost function approximators. So however, this honesty entire operating took logarithm given external stimuli such as the clips up making types. Here we explore both robustness of convolutional correlates networks also fractals to the apparatus dosage once architecture including, network itself. We fun should convolutional users are surprisingly improving soon entire including of critical perturbations between although higher reboots insulating but own bottom pygostyle layers much gotten more mend. For instance, Alexnet focus less than a 100% decrease after determine performance eventually randomly limiting once 45% but weight operators in three will convolutional but marshy layers been performance is almost at meant addition similar and derivation in since then convolutional layer. Finally, everyone necessarily further investigations than could continue actually requested from self-control addition convolutional aol to internal triangulations.", "histories": [["v1", "Thu, 23 Mar 2017 22:25:05 GMT  (257kb,D)", "http://arxiv.org/abs/1703.08245v1", "under review at ICML 2017"]], "COMMENTS": "under review at ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["nicholas cheney", "martin schrimpf", "gabriel kreiman"], "accepted": false, "id": "1703.08245"}, "pdf": {"name": "1703.08245.pdf", "metadata": {"source": "META", "title": "On the Robustness of Convolutional Neural Networksto Internal Architecture and Weight Perturbations", "authors": ["Nicholas Cheney", "Martin Schrimpf", "Gabriel Kreiman"], "emails": ["<gabriel.kreiman@childrens.harvard.edu>."], "sections": [{"heading": "1. Introduction", "text": "Current deep learning techniques are able to learn rich feature representations from large datasets with a generalpurpose learning procedure by adjusting their internal parameters (LeCun et al., 2015). These weights in neural networks are generally fit to the data once during training and then kept fixed for testing.\nOnline learning algorithms, in which data become gradually available and are presented sequentially, may be preferable to these batch algorithms, which learn on a dedicated training set, as the size of datasets scale (LeCun et al., 2012;\n*Equal contribution 1Cornell University, Ithaca, New York, USA 2Program in Software Engineering, Augsburg University and Technische Universita\u0308t Mu\u0308nchen and Ludwig-Maximilians-Universita\u0308t Mu\u0308nchen, Germany 3Boston Children\u2019s Hospital, Harvard Medical School, Boston, Massachusetts, USA. Correspondence to: Gabriel Kreiman <gabriel.kreiman@childrens.harvard.edu>.\nBottou & LeCun, 2003) or if the data generating source changes over time, relaxing the assumption of independent and identically distributed random variables.\nUnsupervised (or semi-supervised) deep learning algorithms are also highly desirable (Salakhutdinov & Hinton, 2009; Lee et al., 2009; Bengio et al., 2012), as they allow neural networks to approach problems that do not have an abundance of labeled data and vastly scale their real world applicability.\nHowever, online learning (and especially unsupervised online learning) methods applied in practice may result in unpredictable changes to a deep neural network\u2019s architecture (potentially both topology and weights) during its real time use. Especially in high-risk scenarios \u2013 such as driverless cars (Huval et al., 2015), UAVs (Ross et al., 2013), or autonomous robotics (Levine et al., 2016) \u2013 it is important to understand how internal changes to a deep network during its execution will impact its immediate performance.\nWhile weight changes to synapses represent experiencedependent changes during execution, the performance effects of topological changes are also an area of concern. Embodied machines, especially those with plastic morphologies such as modular robotics (Yim et al., 2007), may benefit from neurogenesis (Chiel & Beer, 1997). Conversely, topological (or weight) changes could stem from defects in hardware implementations of neural networks (Mead & Ismail, 2012; Schneider, 2017).\nIn biological systems, connectivity and weights between neurons in the brain are continuously changing. Neurons die every day and, at least in some parts of the brain, new neurons are formed (Cunningham, 1982; Eriksson et al., 1998). Synapses are subject to learning mechanisms \u2013 such as spike-timing dependent plasticity (Song et al., 2000; Hebb, 2005) \u2013 that directly and dynamically modify the magnitude of connection strengths. It is remarkable that the brain is able to encode stable information in the presence of such drastic architectural changes. How brains simultaneously achieve plasticity and stability represents an open question in our understanding of biological information processing.\nIn the work reported here, we set forth to investigate the\nar X\niv :1\n70 3.\n08 24\n5v 1\n[ cs\n.L G\n] 2\n3 M\nar 2\n01 7\ndegree of robustness of information encoding (measured through image classification performance) under random destructive perturbations to the internal architecture (or topology) and weights of various deep convolutional neural network architectures."}, {"heading": "2. Background", "text": "Regularizers like weight decay (Moody et al., 1991) or dropout (Srivastava et al., 2014a) can be used during training to simulate weight changes with slowly decreasing parameter values and randomly dropped units respectively. During test time, analyses like adversarial perturbations of the images (Goodfellow et al., 2014; Nguyen et al., 2015) and randomized labels (Zhang et al., 2016) have shed light onto the robustness of neural networks to changes in the data.\nLearning curves of neural network performance during training are widely explored (including convolutional neural networks; e.g. (Hinton et al., 2012)), as the performance increases asymptotically due to the intentional and directed internal parameter changes from learning.\nThe robustness to perturbations to the internal architecture of neural networks have been studied in fully-connected neural networks (Widrow & Lehr, 1990), including: perceptron networks (Zurada et al., 1997), Hopfield networks (Liao & Yu, 1998), recurrent neural networks (Jabri & Flower, 1992).\nThe current work extends this previous knowledge by focusing on quantifying the robustness of pre-trained deep convolutional networks to dynamic changes in the architecture and weights."}, {"heading": "3. Methodology", "text": "We mainly examine topology and weight perturbations to the deep convolutional neural network \u201cAlexnet\u201d (Krizhevsky et al., 2012), pretrained on the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC2012) (Deng et al., 2009; Russakovsky et al., 2015) but later also generalize results to VGG-16 (Simonyan & Zisserman, 2014). This pretrained network is publicly available from the Caffe Zoo (Jia et al., 2014). All models are implemented using Keras (Chollet, 2015) with a Theano backend (Team et al., 2016).\nTo perform each perturbation, a single layer of the network is chosen and the weights and biases (both from now on jointly referred to as \u201cweights\u201d) leading into that layer are separately modified according to one of the following procedures:\n\u2022 Synapse knockouts. A proportion of weights leading\ninto the given layer are set to zero. These weights are randomly selected, as to evenly distribute the impact of the perturbation across all nodes in the layer.\n\u2022 Node knockouts. All of the weights leading into a proportion of nodes in the given layer are set to zero. This has the effect of functionally removing those nodes from the network, and represents a more concentrated perturbation than the synapse knockout treatment.\n\u2022 Weight perturbations. Every weight leading into a given layer is increased (or decreased) by a random value drawn from a Gaussian distribution of a given spread (and centered around zero). In this case, it is unlikely for any single synapse to be removed from the network, but instead all synapses will weight their incoming information slightly differently.\nPreliminary experiments applied perturbations to all layers of the network simultaneously. However, as evidenced in the results below, perturbations were found to have significantly different effects on classification performance when applied to different layers in the convolutional network. Thus all experiments included below apply targeted perturbations to only one layer of the network at a time, and report the effect of that perturbation separately for each layer.\nWe define performance as the proportion of images in the ILSVRC2012 validation set which the classifier correctly labels with one of its 5 most-highly-expressed output nodes (\u201ctop-5 Performance\u201d).\nFor all figures throughout this paper, values shown represent the mean classification performance of 3 to 5 runs with independent random perturbations, while error bars represent the standard deviation of the performance of these runs. All p-values are calculated using the Wilcoxon ranksum test (Wilcoxon, 1945)."}, {"heading": "4. Robustness to Topology Changes", "text": ""}, {"heading": "4.1. Synapse Knockouts", "text": "Perhaps the simplest topological change to a convolution neural network is the removal of a synapse. This method randomly sets the weight of a proportion of synapses going into a given layer to zero, removing information uniformly and randomly from all nodes throughout that layer.\nFig. 1 shows the fall-off in classification performance due to knocking out an increasing proportion of synapses in different layers of the Alexnet convolutional neural network.\nNote that, for all layers, knocking out 0% of synapses simply corresponds to the original unperturbed Alexnet, and thus results in its classification performance of 0.791. At the other extreme, knocking out 100% of any layer means\nthat no information about a given input image is able to reach the output layer of the network, and thus the classification performance is no better than chance (which is 5 guesses / 1000 classes = 0.005 top-5 performance).\nClassification performance for networks with synapse knockouts to the top 3 convolutional layers (conv 3, conv 4, and conv 5) show remarkable robustness, loosing less than 10.5% of their classification performance ability (from 0.791 to \u2265 0.708) with up to 50% of their synapses removed.\nNetworks with 50% of synapses removed from layer conv 2 perform significantly worse than the top 3 convolutional layers (p < 0.05), losing an average of 23.2% of their classification performance (from 0.791 to 0.607). Networks with 50% of synapses removed from the first convolutional layer (conv 1) perform significantly worse than the top 4 convolutional layers (p < 0.05), losing an average of 66.0% of their classification performance ability (from 0.791 to 0.269).\nThe differences in sensitivity to synapse knockout perturbations in Fig. 1 is striking, showing much more fragility in lower layers. For example, the removal of 30% of the synapses in conv 1 results in a network which performs significantly worse (p < 0.05) than the network resulting from removal of 70% of the synapses from any of the top 3 convolutional layers.\nThe dense layers of Fig. 1 also demonstrate extreme robustness to synapse knockouts \u2013 with dense 1 and dense 2 showing less than a 4.759% drop in performance for knockouts of 80% of synapses or less. The robustness of these two layers to synapse knockout is unsurprising, as these layers are explicitly trained to minimize reliance on any single feature using dropout (Srivastava et al., 2014b)."}, {"heading": "4.2. Node Knockouts", "text": "To further investigate topology changes to convolutional neural networks, we randomly removed a given number of nodes in each layer of the network. In the convolutional layers of a neural network, this corresponds to the removal of a convolutional filter. Compared to the synapse knockout experiments in the previous section, node knockouts represent a more clustered and focused removal of information processing ability from the network.\nFig. 2 shows the effect of node knockout on a given layer of Alexnet on the network\u2019s classification performance. As described in Fig. 1, the effect of node knockout also revealed a large difference across layers. The network was significantly more labile to modifications in the earlier layers.\nSince all nodes within a layer contain the same number of incoming synapses each, we are able to directly compare knockouts that are of the same \u201cmagnitude\u201d (in terms of the total number of synapses removed) \u2013 but differently distributed \u2013 by comparing perturbations of the same proportion under the synapse knockout and node knockout treat-\nments.\nClassification performance after knocking out a given proportion of nodes is significantly more damaging to the network than removing that same proportion of randomly distributed synapses. This is true for networks perturbed with all combinations of intermediate knockout proportions (0.1 - 0.9) and layers (all p < 0.05), with the exception of knocking out 90% of nodes in the second convolutional layer \u2013 which was not significantly different than knocking out a random 90% of synapses (p = 0.827).\nOne possible explanation for this may be that node knockout effectively reduces the size of a hidden layer, and that the function approximation abilities of a neural network can be constrained by its hidden layer size (Hornik et al., 1989).\nIt may also be the case that information encoded in certain convolutional filters is unique to that filter and not always replicated in others (in the extreme case of this, the filters would represent orthogonal basis vectors over the space of features). If this is the case, then removing that node would remove the information about its encoded \u201cfeature\u201d from the neural network. By \u201cfeature\u201d we simply mean some unique and invariant property of the image. This notion of \u201cfeature\u201d may be fitting with our intuition of angled edges (for lower layers) or object parts (for upper layers) that are seen in filter activation visualizations (Zeiler & Fergus, 2014) \u2013 but this need not be the case.\nIn contrast to this, the removal of a synapse from a filter would corrode the ability of that filter to capture all of the information it previously did, but the remaining synapses may still provide a projection of the originally encoded information \u2013 and perhaps a projection which is able to retain a greater proportion of its originally encoded information than the proportion of its original synapses which are left intact. If this is the case, it would represent an especially robust property of neural networks to synapse knockout (at least compared to similar proportions of node knockout \u2013 where the amount of previously encoded information that is removed may be proportional to the number of nodes that are knocked out).\nThe idea that nodes represent \u201cfeatures\u201d \u2013 especially in the higher level convolutional layers \u2013 is further supported by the fact that classification performance falls off almost linearly as the proportion of nodes knocked out increases. While this is the case in convolutional layers 2-5, as demonstrated by an r-squared value of 0.963 for a linear fit, this linear drop in performance is less evident in the first convolutional layer (where a linear fit gives an rsquared of 0.715). This observation should be contrasted with the fall-off in classification performance as the proportion of random synapses is removed (Fig. 1), where higher\nconvolutional layers (conv 2-5) demonstrate a less linear fit (r-squared of 0.553) and the first convolutional presents a linear decrease in classification performance as the proportion of randomly removed synapses increases (r-squared of 0.930).\nThe knockout of nodes in dense layers (dense 1 and dense 2), display more robustness than the linear drop-off of the convolutional layers. Again, the use of dropout allowed these layers to train in the presence of node knockouts, so the increased robustness here comes as no surprise. The linear drop-off in the output layer (dense 3) represents the notion that a class label becomes inaccessible if the output node corresponding to that class is knocked out. Thus the perfectly linear drop-off in classification in response to output layer provides little information, but acts as a test case for our implementation of node knockouts."}, {"heading": "5. Robustness to Weight Changes from Random Perturbations", "text": "Rather than removing synapses entirely and setting their weight to zero, both biological noise sources and computational applications can lead to scenarios where perturbations to a network alter the weights by some amount. These weight modification situations are likely to be a concern in the case of unsupervised (or semi-supervised) learning rules where weights may change in an undesired (or simply random) direction, or in the case of damage or noise to embedded circuits. Such sources of noise or weight fluctuations may only cause a short term detriment to the performance of a networks \u2013 but in critical use cases, even the characterization of short term effects may be of importance.\nIn this treatment, all of the synapses leading into a given layer are modified. The size of this perturbation for each individual synapse is randomly drawn from a Gaussian distribution. This Gaussian is centered at zero and has a standard deviation relative to the standard deviation of that layer\u2019s original weight distribution (e.g. a perturbation of magnitude 1 in the first convolutional layer corresponds to a change to each weight in that layer drawn from a Gaussian with a standard deviation equal to that of this first convolutional layer in the pretrained Alexnet). The statistical properties of the original weight distributions are given in Table 1 (located at the end of the text).\nThis treatment differs from the case of synapse knockouts above, because all weights are affected and because the system may be led to put more or less emphasis on a given piece of incoming information, but that information is not entirely removed for the network \u2013 as was the case in the treatments above.\nThe drop in performance is generally more pronounced for lower convolutional layers. In comparison between pertur-\nbations to different convolutional layers, the perturbations to the lower layer (i.e. closer to the input layer) resulted in worse performing networks than those perturbed to a higher layer (all p-values \u2264 0.0495). The exceptions were that perturbations to conv 4 and conv 5 were not worse that perturbations to convolutional layers above conv 3 (and conv 4).\nIt is likely the case that the perturbation effects to the fullyconnected dense layers are different that those to the convolutional layers, as their behaviors are fundamentally different. In the convolutional layers, higher-level features are hierarchically composed from \u201csub-features\u201d from layers below them \u2013 and these features are applied spatially throughout the image with a sliding window. Conversely, dense layers take these features and attempt to associate their presence or absence with the provided class labels. As perturbations to fully connected neural network layers have been previously studied (Background), we focus largely on the convolutional layers."}, {"heading": "6. Generalization to Other Metrics and Architectures", "text": "While the top-5 classification performance of the Alexnet architecture on the ILSVRC2012 validation set has served as the focus for our initial studies, these results generalize well to various implementation decisions.\nPreliminary trials for other thresholds of classification strictness show qualitatively consistent results to the top5 performance used throughout this paper \u2013 with absolute performance metrics differing, but relational trends and qualitative results remaining consistent. For exam-\nple, the top-1 performance for Alexnet classification on ILSVRC2012 in Fig. 4 shows a very similar trend to Fig. 3.\nPreliminary results suggest that performance drop off is also qualitatively similar in other network architectures. Fig. 5 shows a comparable effect of weight perturbations to the convolutional neural network VGG-16 (Simonyan & Zisserman, 2014) which exhibits an increased network depth (16 layers compared to Alexnet\u2019s 8)."}, {"heading": "7. Discussion", "text": "The results from this work support our intuition of deep convolutional networks as robust function approximators \u2013 and extends previous work on the robustness of networks classification towards perturbations of external stimuli to include an analysis of internal weight and architecture mutations.\nThe convolutional networks examined here showed a significant degree of robustness to multiple forms of weight and topological alterations \u2013 especially at the higher layers. One could knock out over 70% of synapses in any of the layers after conv 2 of Alexnet and loose less than 30% of its classification performance (Fig. 1). Similarly, one could perturb every weight in those same layers by an average amount of weight value deviations in each respective layer, yet result in a drop in performance of less than 6%.\nWe also observed that perturbations tended to be more impactful when they targeted the first layer of the network. Knocking out 30% of the nodes in the first convolutional\nlayer results in an average performance drop of over 70%, and it only takes the removal of 50% of nodes in this layer to loose over 95% of the network\u2019s original classification performance.\nChanges to the first convolutional layer markedly differ from those to the upper layers, presenting a non-linear falloff in classification performance. Specifically, the removal of a single node from a largely-intact first convolutional layer results in a large fall-off to performance of the network \u2013 while the removal of a node from an already sparse network has relatively little effect on performance. This suggest that the filters in the first convolutional layer are codependent on one another, and the presence of one without another may provide relatively little information to the following layers.\nTurning to the case of synapse knockouts, we see that the drop-off in performance for the first convolutional layers is linearly dependent on the proportion of randomly removed synapses. However the relationship between the number of synapses knocked out and the effect on performance in the higher convolutional layers (2-5) appears nonlinear.\nSpecifically, the removal of a synapse has relatively little effect until a large proportion of synapses are removed \u2013 after which each removed synapse then corresponds to a large performance drop. This suggests that multiple instances of the information corresponding to each independent feature may be encoded in multiple different synapses. Or it can be similarly interpreted to imply that nodes in these higher convolutional layers only rely on a subset of their incoming synapses to represent an independent feature.\nThis seems to fit our intuition that the activation of high level features depend on the presence or absence of multiple (often redundant) lower level features. Imagine the case where each node in a layer represents a single feature, and that feature is dependent on only one lower-level feature from the layer preceding it. If this is the case, then we would see no difference in the classification performance after knocking out a given proportion of those lower-level features (nodes) and knocking out that same proportion of incoming connections to the higher layer features.\nHowever, the added robustness of networks to synapse knockouts (in comparison to node knockouts) suggests that any given feature in a deep convolutional networks (on average) relies on the presence of multiple lower-level features \u2013 and that most of these lower-level features would be sufficient to provide correct classification in the absence of one of their \u201credundant features\u201d (as evidenced by the finding that classification does not drop off linearly as the number of incoming connections to a higher-level features is randomly pruned). There are, of course, a non-zero subset of images where one lower-level features provides information that is not contained as the classification performance monotomically drops for each progressive increase in synapse knockout proportion (p \u2264 0.0167 for all increases in proportion, increasing from no knockouts to removing 10% of synapses from layer dense 2, for which p = 0.425) \u2013 meaning that the the information in any two features is unlikely to be perfectly redundant.\nThe redundancy of deep convolutional networks has previously been evidenced by a robustness to different angles, sizes, or locations of objects in input images. It could also be inferred by the architecture (size of subsequent layers) and connectivity (number of non-zero synapses leading into each node). It also follows from intuition that higherlevel layers compose features from lower-level layers in visual cortex (Van Essen & Maunsell, 1983). However, the quantification of the degree to which each node \u2013 on average \u2013 is robust to removal of (or reliant upon the presence of) incoming information from lower-level features requires a knockout analysis of the internal architecture of the networks, such as the one provided in this work.\nThis explanation may also help to shed light onto the dichotomy between the first and higher-level convolutional\nlayers, as the first layer (receiving only pixel information) may not have access to, or rely on, redundant incoming information in the way that higher convolutional layers do.\nIt is also worth noting that part of the difference between perturbation impact on the lower and higher layers may also be due to the fact that information is fed forward sequentially through the network, such that perturbations to each layer also affect the activation pattern fed into all the subsequent layers. However, this effect would be expected to be dependent only on the number of layers above the perturbed one \u2013 and our results show that the performance decreases tend to stagnate in the higher convolutional layers (3-5), suggesting that this effect alone does not account for the patterns observed above.\nThe results stated above show the immediate impact of random perturbations on classification performance. However, it is likely that convolutional networks undergoing online learning will also continue to adapt and recover from these undesirable changes. Thus, while knowing the expected short term determent to performance is critical, we also plan to investigate the rate and maximum recoverability that would occur as a result of retraining in future work."}, {"heading": "8. Conclusion", "text": "We examined the robustness of a pretrained convolutional neural network to internal weight and architecture perturbations. We showed the classification performance effects of internal perturbations that removed nodes, removed synapses, and modified synapse weights. We demonstrated that convolutional networks showed a significant degree of robustness to such changes. Perturbations to lower convolutional layers was significantly more impactful than perturbations to higher layers. These results help us understand how information is encoded within the nodes and layers of deep convolutional neural networks."}], "references": [{"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron C", "Vincent", "Pascal"], "venue": "CoRR, abs/1206.5538,", "citeRegEx": "Bengio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2012}, {"title": "Large scale online learning", "author": ["Bottou", "L\u00e9on", "LeCun", "Yann"], "venue": "In NIPS,", "citeRegEx": "Bottou et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2003}, {"title": "The brain has a body: adaptive behavior emerges from interactions of nervous system, body and environment", "author": ["Chiel", "Hillel J", "Beer", "Randall D"], "venue": "Trends in neurosciences,", "citeRegEx": "Chiel et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chiel et al\\.", "year": 1997}, {"title": "URL https:// github.com/fchollet/keras", "author": ["Chollet", "Fran\u00e7ois"], "venue": "Keras,", "citeRegEx": "Chollet and Fran\u00e7ois.,? \\Q2015\\E", "shortCiteRegEx": "Chollet and Fran\u00e7ois.", "year": 2015}, {"title": "Naturally occurring neuron death and its regulation by developing neural pathways", "author": ["Cunningham", "Timothy J"], "venue": "International review of cytology,", "citeRegEx": "Cunningham and J.,? \\Q1982\\E", "shortCiteRegEx": "Cunningham and J.", "year": 1982}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Neurogenesis in the adult human hippocampus", "author": ["Eriksson", "Peter S", "Perfilieva", "Ekaterina", "Bj\u00f6rk-Eriksson", "Thomas", "Alborn", "Ann-Marie", "Nordborg", "Claes", "Peterson", "Daniel A", "Gage", "Fred H"], "venue": "Nature medicine,", "citeRegEx": "Eriksson et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Eriksson et al\\.", "year": 1998}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "The organization of behavior: A neuropsychological theory", "author": ["Hebb", "Donald Olding"], "venue": null, "citeRegEx": "Hebb and Olding.,? \\Q2005\\E", "shortCiteRegEx": "Hebb and Olding.", "year": 2005}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "An empirical evaluation of deep learning on highway driving", "author": ["Huval", "Brody", "Wang", "Tao", "Tandon", "Sameep", "Kiske", "Jeff", "Song", "Will", "Pazhayampallil", "Joel", "Andriluka", "Mykhaylo", "Rajpurkar", "Pranav", "Migimatsu", "Toki", "Cheng-Yue", "Royce"], "venue": "arXiv preprint arXiv:1504.01716,", "citeRegEx": "Huval et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huval et al\\.", "year": 2015}, {"title": "Weight perturbation: An optimal architecture and learning technique for analog vlsi feedforward and recurrent multilayer networks", "author": ["Jabri", "Marwan", "Flower", "Barry"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Jabri et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Jabri et al\\.", "year": 1992}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Robust stability for interval hopfield neural networks with time delay", "author": ["Liao", "Xiaofeng", "Yu", "Jeubang"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Liao et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Liao et al\\.", "year": 1998}, {"title": "Analog VLSI implementation of neural systems, volume 80", "author": ["Mead", "Carver", "Ismail", "Mohammed"], "venue": "Springer Science & Business Media,", "citeRegEx": "Mead et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mead et al\\.", "year": 2012}, {"title": "The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems", "author": ["Moody", "John E"], "venue": "In NIPS,", "citeRegEx": "Moody and E,? \\Q1991\\E", "shortCiteRegEx": "Moody and E", "year": 1991}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Nguyen", "Anh", "Yosinski", "Jason", "Clune", "Jeff"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Learning monocular reactive uav control in cluttered natural environments", "author": ["Ross", "St\u00e9phane", "Melik-Barkhudarov", "Narek", "Shankar", "Kumar Shaurya", "Wendel", "Andreas", "Dey", "Debadeepta", "Bagnell", "J Andrew", "Hebert", "Martial"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Ross et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2013}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Competitive hebbian learning through spike-timingdependent synaptic plasticity", "author": ["Song", "Sen", "Miller", "Kenneth D", "Abbott", "Larry F"], "venue": "Nature neuroscience,", "citeRegEx": "Song et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Song et al\\.", "year": 2000}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1958}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Hierarchical organization and functional streams in the visual cortex", "author": ["Van Essen", "David C", "Maunsell", "John HR"], "venue": "Trends in neurosciences,", "citeRegEx": "Essen et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Essen et al\\.", "year": 1983}, {"title": "30 years of adaptive neural networks: perceptron, madaline, and backpropagation", "author": ["Widrow", "Bernard", "Lehr", "Michael A"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Widrow et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Widrow et al\\.", "year": 1990}, {"title": "Individual comparisons by ranking methods", "author": ["Wilcoxon", "Frank"], "venue": "Biometrics bulletin,", "citeRegEx": "Wilcoxon and Frank.,? \\Q1945\\E", "shortCiteRegEx": "Wilcoxon and Frank.", "year": 1945}, {"title": "Modular self-reconfigurable robot systems [grand challenges of robotics", "author": ["Yim", "Mark", "Shen", "Wei-Min", "Salemi", "Behnam", "Rus", "Daniela", "Moll", "Lipson", "Hod", "Klavins", "Eric", "Chirikjian", "Gregory S"], "venue": "IEEE Robotics & Automation Magazine,", "citeRegEx": "Yim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yim et al\\.", "year": 2007}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In European conference on computer vision,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1611.03530,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Perturbation method for deleting redundant inputs of perceptron", "author": ["Zurada", "Jacek M", "Malinowski", "Aleksander", "Usui", "Shiro"], "venue": "networks. Neurocomputing,", "citeRegEx": "Zurada et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zurada et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Unsupervised (or semi-supervised) deep learning algorithms are also highly desirable (Salakhutdinov & Hinton, 2009; Lee et al., 2009; Bengio et al., 2012), as they allow neural networks to approach problems that do not have an abundance of labeled data and vastly scale their real world applicability.", "startOffset": 85, "endOffset": 154}, {"referenceID": 11, "context": "Especially in high-risk scenarios \u2013 such as driverless cars (Huval et al., 2015), UAVs (Ross et al.", "startOffset": 60, "endOffset": 80}, {"referenceID": 20, "context": ", 2015), UAVs (Ross et al., 2013), or autonomous robotics (Levine et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 15, "context": ", 2013), or autonomous robotics (Levine et al., 2016) \u2013 it is important to understand how internal changes to a deep network during its execution will impact its immediate performance.", "startOffset": 32, "endOffset": 53}, {"referenceID": 29, "context": "Embodied machines, especially those with plastic morphologies such as modular robotics (Yim et al., 2007), may benefit from neurogenesis (Chiel & Beer, 1997).", "startOffset": 87, "endOffset": 105}, {"referenceID": 6, "context": "Neurons die every day and, at least in some parts of the brain, new neurons are formed (Cunningham, 1982; Eriksson et al., 1998).", "startOffset": 87, "endOffset": 128}, {"referenceID": 23, "context": "Synapses are subject to learning mechanisms \u2013 such as spike-timing dependent plasticity (Song et al., 2000; Hebb, 2005) \u2013 that directly and dynamically modify the magnitude of connection strengths.", "startOffset": 88, "endOffset": 119}, {"referenceID": 7, "context": "During test time, analyses like adversarial perturbations of the images (Goodfellow et al., 2014; Nguyen et al., 2015) and randomized labels (Zhang et al.", "startOffset": 72, "endOffset": 118}, {"referenceID": 19, "context": "During test time, analyses like adversarial perturbations of the images (Goodfellow et al., 2014; Nguyen et al., 2015) and randomized labels (Zhang et al.", "startOffset": 72, "endOffset": 118}, {"referenceID": 31, "context": ", 2015) and randomized labels (Zhang et al., 2016) have shed light onto the robustness of neural networks to changes in the data.", "startOffset": 30, "endOffset": 50}, {"referenceID": 9, "context": "(Hinton et al., 2012)), as the performance increases asymptotically due to the intentional and directed internal parameter changes from learning.", "startOffset": 0, "endOffset": 21}, {"referenceID": 32, "context": "The robustness to perturbations to the internal architecture of neural networks have been studied in fully-connected neural networks (Widrow & Lehr, 1990), including: perceptron networks (Zurada et al., 1997), Hopfield networks (Liao & Yu, 1998), recurrent neural networks (Jabri & Flower, 1992).", "startOffset": 187, "endOffset": 208}, {"referenceID": 13, "context": "We mainly examine topology and weight perturbations to the deep convolutional neural network \u201cAlexnet\u201d (Krizhevsky et al., 2012), pretrained on the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC2012) (Deng et al.", "startOffset": 103, "endOffset": 128}, {"referenceID": 5, "context": ", 2012), pretrained on the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC2012) (Deng et al., 2009; Russakovsky et al., 2015) but later also generalize results to VGG-16 (Simonyan & Zisserman, 2014).", "startOffset": 95, "endOffset": 140}, {"referenceID": 10, "context": "One possible explanation for this may be that node knockout effectively reduces the size of a hidden layer, and that the function approximation abilities of a neural network can be constrained by its hidden layer size (Hornik et al., 1989).", "startOffset": 218, "endOffset": 239}], "year": 2017, "abstractText": "Deep convolutional neural networks are generally regarded as robust function approximators. So far, this intuition is based on perturbations to external stimuli such as the images to be classified. Here we explore the robustness of convolutional neural networks to perturbations to the internal weights and architecture of the network itself. We show that convolutional networks are surprisingly robust to a number of internal perturbations in the higher convolutional layers but the bottom convolutional layers are much more fragile. For instance, Alexnet shows less than a 30% decrease in classification performance when randomly removing over 70% of weight connections in the top convolutional or dense layers but performance is almost at chance with the same perturbation in the first convolutional layer. Finally, we suggest further investigations which could continue to inform the robustness of convolutional networks to internal perturbations.", "creator": "LaTeX with hyperref package"}}}