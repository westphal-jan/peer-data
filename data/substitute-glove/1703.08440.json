{"id": "1703.08440", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "K-Means Clustering using Tabu Search with Quantized Means", "abstract": "The Tabu Search (TS) metaheuristic of been rejected came K - Means algorithms as one alternative eventually Lloyd ' pct algorithm, a having three its crisis of establish and fast bitrate, even within major hiccup created considered searching at business lgs. While long TS shift turn yield consistent performance, does finding a primary observational complexity. Moreover, that difficulty another formula_3 selection in the allows TS clear ca not come something regardless need exotic. This paper presents unusual consider, low - qualities formulation country three TS recursive permitting for K - Means clustering. This ambitious does follow requirement seen function configurations. We initially constrain place centers eventually points in main dataset. We then creating about evolving these related using made unique trendy construct already fit use of damping particular of the acceptable function. This results then yet efficient undertaken significant before preparing operating, after which followed means are imports. The measure funds an implemented present MATLAB into having on five real - one datasets, along when transformational a though priority through its access TS view followed relation of created intra area proceeds seen layered turn computational one.", "histories": [["v1", "Fri, 24 Mar 2017 14:59:06 GMT  (78kb,D)", "http://arxiv.org/abs/1703.08440v1", "World Conference on Engineering and Computer Science"]], "COMMENTS": "World Conference on Engineering and Computer Science", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kojo sarfo gyamfi", "james brusey", "andrew hunt"], "accepted": false, "id": "1703.08440"}, "pdf": {"name": "1703.08440.pdf", "metadata": {"source": "CRF", "title": "K-Means Clustering using Tabu Search with Quantized Means", "authors": ["Kojo Sarfo Gyamfi", "James Brusey", "Andrew Hunt"], "emails": ["gyamfik@uni.coventry.ac.uk).", "aa3172@coventry.ac.uk).", "ab8187@coventry.ac.uk)."], "sections": [{"heading": null, "text": "Index Terms\u2014Unsupervised learning, Clustering, K-Means, Tabu Search.\nI. INTRODUCTION\nA common problem in machine learning is the task ofhaving to group a set of N data points or objects into K clusters. This is termed clustering. These objects are collected together into a set denoted as D. Clustering can occur in varied settings. As an example, consider the case of classifying N organisms into K different kingdoms based on their features. This can be construed as a clustering problem where the number of features being considered is d. In the more general sense, d denotes the dimensionality of the set D. Furthermore, the collection of feature vectors of all the organisms forms the set D, while the clusters denoted as Ck are represented by the different kingdoms.\nIn machine learning, clustering falls under the domain of unsupervised learning since there are no class labels to the objects in D. Nonetheless, it can also be performed as a precursor to some supervised learning techniques. An example of this latter application is in the implementation of the radial basis function (RBF) with K centers [1].\nThe clusters, denoted as Ck (k = 1, ...,K), are to be determined such that objects in any one cluster are similar\nManuscript received June 27, 2016; revised July 20, 2016. This work was supported in part by the National Engineering Laboratory (NEL), United Kingdom.\nKojo Sarfo Gyamfi is a PhD student with the Faculty of Engineering and Computing, Coventry University, Coventry, United Kingdom, (e-mail: gyamfik@uni.coventry.ac.uk).\nJames Brusey is a reader with the Faculty of Engineering and Computing, Coventry University, Coventry, United Kingdom, (e-mail: aa3172@coventry.ac.uk).\nAndrew Hunt is a professor with the Faculty of Engineering and Computing, Coventry University, Coventry, United Kingdom, (e-mail: ab8187@coventry.ac.uk).\nto each other, but different from objects in all other clusters. It is assumed that the objects in D lend themselves to some natural grouping [2]. Otherwise, any partitioning of the data can be considered valid, which would make the problem undefined. However, in the K-center RBF, such an assumption is not binding since the objective is to use the K centers as representative points in the dataset for the construction of basis functions.\nClustering is, however, an ill-posed problem [3] for the following reasons. First, the question of how to tell if any two objects are similar has no definitive answer. To illustrate this, in Fig. 1 (a), the similarity among objects in either of the natural clusters indicated by + or o is based on the distance of a point from the center. On the other hand, in Fig. 1 (b), the closeness of the points to one another provides the measure of similarity among the two natural clusters indicated by + and o. Thus, there is no general similarity measure by which objects are clustered.\nThe second reason why the problem of clustering is illdefined is that the number of clusters K to which the objects must be classified is not known a priori. A rough estimate of K is usually assumed to be available from domain expertise or from the distribution of the data. If such an estimate is not available, the common practice is that existing algorithms are run for different K. The value of K which minimizes some predefined criterion like the Akaike Information Criterion (AIC) or the Bayes Information Criterion [3] is then chosen. Clustering algorithms may yield poor results if the K chosen is inappropriate [3].\nThe most widely used algorithm for clustering in the context of machine learning is Lloyds algorithm, more commonly referred to as K-Means algorithm. It is so called because it essentially computes the K means or centroids of the different clusters. The ease of implementation of the algorithm as well as its fast runtime has accounted for its ubiquity in use. Nevertheless, it has the major drawback of yielding solutions that are only locally optimal, and which may not necessarily be the global optimal solution. For this reason, several other methods have been applied to solving the clustering problem [4]-[7]. Notable among these is the approach of Al-Sultan [7] which is based on the Tabu Search (TS) algorithm developed by Glover [8]. We henceforth refer to this approach, i.e. [7] (our reference work) as the Tabu Search Clustering (TSC) algorithm. The performance reported was shown to be superior to that of the K-Means algorithm.\nThe TS algorithm is a metaheuristic procedure that accepts an initial solution as input, and performs a local search using neighborhood and memory structures until some stopping criterion is met. It is able to escape local minima by allowing for solutions that do not improve the objective function. TS has been applied in solving varied problems including the\nar X\niv :1\n70 3.\n08 44\n0v 1\n[ cs\n.L G\n] 2\n4 M\nar 2\n01 7\ntraveling salesman problem (TSP) [9] and signal detection in multiple input multiple output (MIMO) antenna systems [10]. However, with regards to the clustering problem, the high computational complexity and difficulty in parameter selection required in the TS approach does not make it an attractive alternative to the K-Means algorithm.\nOur main contributions in this paper are as follows: 1) We introduce a quantized means TS scheme for solving\nthe clustering problem. We target the optimization of the K centers by evolving them through a series of neighboring solutions in such a manner that leads to an efficient exploration of the search space. This procedure is well described in Section IV. The scheme requires only two parameters to be set, and is of a relatively low complexity. 2) We present experimental results obtained from the proposed approach on some test datasets (Section V)."}, {"heading": "II. PROBLEM DESCRIPTION", "text": "For the purpose of this paper, we assume that the number of clusters K is given. We refer the reader to the works by Hamerly et al. [11] and Pan et al. [12] for a detailed treatment on how to choose K. The dataset D is assumed to come from a mixture distribution where the mixture component label (which is the cluster index) for any object in the dataset is hidden. In the most general sense, an object can belong to more than one cluster. Thus, for such an interpretation, the problem of clustering is simply that of finding the clusters to which an object belongs with a high probability. Mathematically, this can be stated concisely as maximizing the following probability for different models S for a given xn \u2208 D:\np(xn | S) = K\u2211\nk=1\nwkp(xn | Ck,S) (1)\nwhere S comprises the cluster memberships for all the objects in the dataset, as well as the mixture weights wk.\nMaximizing (1) requires knowledge of the cluster memberships and the mixture weights, as well as knowledge of the mixture distribution. In general, none of these is known, and so the following set of simplifying assumptions [4] are made in practice.\n1) Each object in D belongs to a single cluster; 2) Each cluster is distributed as a multivariate Gaussian; 3) The mixture components have equal weights wk. The consequence of the above assumptions is that the similarity measure is now based on the Euclidean norm so that points closest to each other in Euclidean space are grouped under one and only one cluster. It is conceivable that a dataset may have some similarity measure other than the Euclidean distance. Indeed, [13]-[15] explore the use of other distance measures for clustering. Yet, for some datasets, an appropriate representation of the points can make the Euclidean distance measure valid. As an example, transformation of the points in Fig. 1 (a) into polar coordinates yields the representation in Fig. 1 (b) which has the Euclidean distance as a valid similarity measure. The clustering problem then yields itself to a treatment as a mathematical optimization whose aim is to minimize a\nparameter J known as the intra cluster sum of squares (ICSS) or the distortion [4]. This may be stated as:\nmin J = K\u2211\nk=1 \u2211 xn\u2208Ck \u2016xn \u2212 \u00b5k\u20162 (2)\nwhere xn \u2208 Ck are all data points in cluster k and \u00b5k is the mean or center of that kth cluster. This is the problem termed as K-Means clustering. It must be mentioned that neither the cluster memberships nor the means are known. Thus, this problem is computationally difficult, and is NP-hard [6].\nThe K-Means algorithm provides an efficient way of solving (2). It is based on the observation that the optimal placement of the K centers is at the centroids of the respective clusters. The algorithm is typically initialized with some random means, usually chosen from objects in the dataset D. Since the K-Means algorithm is a special case of the Expectation-Maximization (EM) algorithm [4], it proceeds in two stages namely, the expectation and the maximization stages.\n1) Expectation: Compute the centroid of each cluster:\n\u00b5k = 1\nNk \u2211 xn\u2208Ck xn (3)\nwhere Nk is the number of objects in the kth cluster.\n2) Maximization: Compute the cluster memberships: Ck = {\nxn : \u2016xn \u2212 \u00b5k\u20162 < \u2016xn \u2212 \u00b5l\u20162, l = 1, ...,K, l 6= k} (4)\nThe expectation-maximization steps are carried out iteratively until there is no cluster change, at which point the algorithm is terminated.\nThe major drawback of the K-Means algorithm is as follows. First, the objective function of (2) is non-convex and may thus have several local minima. Therefore, being only a local search method, the K-Means algorithm is not guaranteed to find a global minimum; it often yields solutions that are only locally optimal. This is due in part to the nature of the stopping criterion. The algorithm terminates when there is no change in cluster memberships; this period corresponds to a local minimum. It makes no provision to consider other local minima which may be present in other areas of the search space. Again, as with all local search methods, the performance of the K-Means algorithm is directly tied to the quality of the initial solution. If this solution is poor, i.e., if it is too far away from the global optimum, the algorithm may likely converge to a local minimum.\nOur approach is motivated by the above limitation, and is based on the TS algorithm in [8]."}, {"heading": "III. TABU SEARCH", "text": "Tabu Search is a metaheuristic technique used for combinatorial optimization. It does not require the optimization problem to be convex. The algorithm makes use of neighborhood structures to explore the search space. It also utilizes a short term memory structure called a tabu, which is essentially a list of forbidden moves or solutions. Tabus prevent the back and forth movements between solutions that have already been considered in the search, a phenomenon called cycling. Moreover, TS allows for moves to solutions that do not yield any improvement in the objective function. It does so with the view that the poor solution may lead to a better one at a later time in the search. Thus, it is able to escape from local minima. TS keeps in memory the best solution found at any point in the search, and returns that solution when the algorithm is terminated. In its most basic form, it follows the procedure outlined below:\n1) Select an initial solution M(0). This solution can be randomly generated or obtained by more formal means. Set Mc and Mb to M(0). Mc and Mb are the current and best solutions respectively. 2) Evaluate the objective function J for the current solution Mc. 3) Find neighboring solutions of Mc. Let V denote this set. The neighbors of Mc are all those solutions that are similar to, but differ in a minor aspect from Mc. 4) Find the set of solutions in V that are not in the Tabu list T. Let this set be denoted by V \\ T. The Tabu is a list of solutions or moves that have already been considered in the search. Tabus, as algorithmic structures, force the algorithm to other areas of the search space, thus enhancing the diversification of the search.\n5) Evaluate the objective function for all the solutions in V \\ T. Find the best solution among this set. Let this be Mn. 6) If Jn < Jb, let Mb = Mn. Jn and Jb are the objective function evaluations of Mn and Mb respectively. 7) Put the solution Mc into the Tabu list, and let Mn be the new current solution Mc. If the maximum number of iterations (which is chosen beforehand) has elapsed, terminate. Else, go to Step 3."}, {"heading": "IV. QUANTIZED MEANS TS CLUSTERING", "text": "In this section, we discuss the proposed algorithm. As with any TS implementation, the Quantized Means TS Clustering follows the skeleton of the description of the TS algorithm in Section III with the following modifications and specificities."}, {"heading": "A. Search Space", "text": "In this formulation, a vector M(0) defined as M(0) = [\u00b5T1 , ...,\u00b5 T K ]\nT is considered as the initial solution, where \u00b51, ...,\u00b5K are K randomly chosen observations from the dataset D. M(0) is then assigned to Mc.\nTo navigate the search space then, neighbors of Mc have to be found. Neighboring solutions are typically drawn from a finite set that includes the current solution itself. Alternatively, they can be obtained via a simple transformation of the current solution. It is worth mentioning that in the context of TS, neighboring solutions are not necessarily those that are closest to the current solution.\nTo obtain neighbors of Mc, we change its individual components i.e. \u00b5k (k = 1, ...,K), by replacing them with some new means or centers. However, the means are real-valued in general, and do not constitute any finite set. Therefore, the set of all possible neighbors obtained in this manner is necessarily an infinite set. This set is the feasible search space. The fact of the search space being infinite makes TS ill-suited to optimizing Mc, since TS is used for combinatorial optimization.\nA finite subset of the search space is thus necessary. For this reason, the proposed scheme makes the assumption that the K means take on values exclusively from objects in the dataset D. We refer to this as quantized means. Our proposed algorithm is divided into two stages, namely, exploration and refinement; we make the aforementioned assumption on the means only in the initial exploration stage. Thus, in the exploration stage, for k = 1, ...,K, \u00b5k \u2208 Mc is replaced with some other point x taken from the dataset. This procedure yields the neighboring solution denoted as Mn. More specifically, for every k \u2208 {1, ...,K}, we constrain the point x to the kth cluster Ck (which is a subset of the dataset). This quantization of the means makes the problem formulation combinatorial. Nevertheless, the resulting set of all possible combinations of Mc (i.e. the search space denoted as W), although finite, is still large."}, {"heading": "B. Neighborhood Construction", "text": "Due to the large size of W, one has to choose only R (R |W|) points from the set W via a simple transformation of the solution Mc and consider those as the neighbors of Mc in any one iteration of the TS algorithm. The difficulty, however, is in the choice of which R neighbors.\nIf we randomly select neighbours, the search is unguided and thus likely to be slow to converge on the optimal solution. A simple guiding mechanism might be to choose nearest neighbours. However, this seems a poor choice intuitively because for many cases it will cause no change to the clustering and where it does, it might not be a change in the right direction.\nAnalytic Neighbors: We therefore use the gradient information of the objective function to guide the neighbor selection. In any TS iteration, we choose R points in W that result in the steepest descent along the trajectory of the objective function. We consider the selection of a single neighbor, i.e R = 1 in this paper. The following approach is then taken to find one high-quality neighbor of Mc. Since the objective function of (2) is non-convex, the aim is to find a neighbor Mn \u2208 W that corresponds to a local minimum of J . A necessary and sufficient condition for this is to have the gradient of J to be zero at the local minimum, i.e.\n\u2207McJ = 0 (5)\nwhere the notation \u2207 represents the gradient. By definition,\n\u2207McJ = \u2202J\n\u2202\u00b51 e1 + ... +\n\u2202J\n\u2202\u00b5K eK = 0 (6)\nwhere ek (k = 1, ...,K) is a unit vector in the \u00b5k direction. By (6), it has implicitly been assumed that each \u00b5k is independent of the other. This assumption is not generally true of the K-Means algorithm, as a change in some \u00b5k may change the cluster memberships and hence change the location of the other means. However, in the exploration stage of our proposed algorithm, the means are not defined as the cluster centroids as in (3), but are chosen independently of each other in the procedure below. This permits the evaluation of the K partial derivatives independently as:\n\u2202J\n\u2202\u00b5k = 0, \u2200k = 1, ...,K (7)\nHowever, since \u00b5k has been constrained to the dataset D, the change in the means \u2202\u00b5k being considered is not necessarily infinitesimal. For this reason, we approximate the partial derivative in (6) as a partial difference quotient as:\n\u2202J \u2202\u00b5k \u2248 \u2206J \u2206\u00b5k \u2248 0, \u2200k = 1, ...,K (8)\nwhich can be evaluated from first principles as follows:\nJ = K\u2211 i=1 \u2211 xn\u2208Ci \u2016xn \u2212 \u00b5i\u20162 (9)\nA change in the mean \u2206\u00b5i would then cause a change \u2206J in the objective function, i.e.,\nJ + \u2206J = K\u2211 i=1 \u2211 xn\u2208Ci \u2016xn \u2212 (\u00b5i + \u2206\u00b5i)\u20162 (10)\nJ + \u2206J = K\u2211 i=1 \u2211 xn\u2208Ci ( \u2016xn \u2212 \u00b5i\u20162 \u2212 2(xn \u2212 \u00b5i)T \u2206\u00b5i + \u2016\u2206\u00b5i\u20162 ) (11)\n\u2206J = K\u2211 i=1 \u2211 xn\u2208Ci ( \u2212 2(xn \u2212 \u00b5i)T \u2206\u00b5i + \u2016\u2206\u00b5i\u20162 ) (12)\nFor the purpose of evaluating (8), \u2206\u00b5i = 0 for i 6= k due to the assumption of the independence of the means, hence\n\u2206J = \u2211\nxn\u2208Ck\n( \u2212 2(xn \u2212 \u00b5k)T \u2206\u00b5k + \u2016\u2206\u00b5k\u20162 ) (13)\n\u2206J \u2206\u00b5k = \u2211\nxn\u2208Ck\n( \u2212 2(xn \u2212 \u00b5k) + \u2206\u00b5k ) \u2248 0 (14)\nwhere \u2206\u00b5k = x\u2212 \u00b5k. In order to evaluate (14), we find x \u2208 Ck that minimizes (13), having constrained the means to the dataset D. This minimizing parameter is denoted as x\u2217k. The neighboring solution Mn is then the aggregation of all x\u2217k (k = 1, ...,K), i.e. Mn = [x\u2217T1 , ..., x\u2217TK ]T . It must be noted that if the means were to be unconstrained to D, (7) could be evaluated directly instead of solving (14), and the solution of (7) would be the centroids of the clusters, which is essentially what the K-Means algorithm evaluates. However, the initial assumption on the means would be violated, and there would still be the risk of getting trapped at local minima. Rather, this procedure allows for the consideration of solutions that worsen the objective function J since (13) would not always yield negative values, thereby escaping from local minima.\nThe intuitive alternative of finding the cluster centroids, and then quantizing them to the dataset D introduces a quantization loss and yields an inferior performance to the procedure described.\nOnce the means have been computed, the cluster memberships can then be determined from:\nCk = {\nxn : \u2016xn \u2212 x\u2217k\u20162 < \u2016xn \u2212 x\u2217l \u20162, l = 1, ...,K, l 6= k} (15)\nfrom which the objective function in (1) can be evaluated."}, {"heading": "C. Tabu", "text": "The Tabu structure used in this formulation is a list of all the means x\u2217k that have been considered in the search. Since there are K means, the Tabu considered is an array with K rows whose column length increases as the TS algorithm proceeds. If for some k, x\u2217k obtained from minimizing (13) is in the Tabu list, it is discarded and the next point x \u2208 Ck in increasing order of their \u2206J evaluation is chosen. If all points in the kth cluster are in the Tabu list for any k, the last entry in the kth row of the Tabu list is deleted in order to allow for at least one solution to be valid."}, {"heading": "D. Termination Criterion", "text": "The termination criterion employed in the proposed algorithm is two-fold. First, after the maximum number of TS iterations ITmax has been reached, the algorithm is terminated. Secondly, there is an early termination criterion whereby the algorithm is cut off after a predefined number of iterations (called the cut-out parameter rmax) within which there is no improvement in the best found solution Mb. This is an indicator of the convergence of the algorithm. The early termination is done on the assumption that the global minimum may have already been achieved. In order for this\nassumption to be mostly valid, the neighboring solutions generated in any iteration must not be random. Otherwise, there is a good chance the global optimal solution would be found in any TS iteration. Therefore, the process of generating R random neighbors of Mc from W would not yield good results with regards to the early termination. The early termination cuts down the computational complexity as the algorithm does not need to be run for all ITmax iterations."}, {"heading": "E. Refinement", "text": "The essence of the initial restriction on the means \u00b5k to belong to the finite set D is to make the optimization problem combinatorial, and enable the efficient exploration of the search space. Once that has been achieved at the end of the TS algorithm, the means can then be unconstrained. As a result, the components of the best found solution Mb are recomputed as the centroids of the clusters obtained at the end of the TS algorithm. Alternatively, one may use Mb as an initial solution to the K-Means algorithm to obtain a refined solution.\nThe proposed TS scheme (i.e. using the analytic neighborhoods) is illustrated in the flow chart of Fig. 2."}, {"heading": "V. SIMULATIONS AND RESULTS", "text": "For our simulations, we use four real-world datasets namely: the Bavaria Postal code dataset [16] (for two different values of K), the Fisher\u2019s Iris dataset, the Glass Identification dataset, and the normalized Cloud dataset [17] (also for two different values of K). These datasets are chosen to cut across a wide range of d, K, and N values. We simulate our proposed scheme in MATLAB on an Intel Core i5-2400 processor using the following parameters: ITmax = 400 and rmax = 0.25ITmax. We compare the performance of this scheme to the TSC, the K-Means++ [18], and the K-Means algorithms in terms of the objective function of (2) and the time taken for completion. We use the following parameter settings for the TSC algorithm: NTS = 20,MTLS = 15, ITmax = 1000, P = 0.95 as suggested by Al-Sultan [5]. For each dataset, we run each algorithm 100 times, and provide the worst, average and best objective function values, as well as the average time for completion. The results of our simulations are summarized in Tables I-VI.\nFrom the tables, it can be seen that the proposed scheme achieves the best average objective function in five of the six tests performed. The best average objective function for all the tests have been highlighted in boldface. The proposed scheme consistently outperforms the TSC algorithm in terms of the computational time, average, best and worst objective function values. Specifically, on the Cloud dataset for K = 25, our algorithm achieves as much as 90% improvement on the average J , while doing so 90% faster. It must be noted that both the proposed scheme and the TSC algorithm can actually be used to obtain lower values of J than the ones reported, by increasing the value of ITmax (and NTS in the case of the TSC). However, that would be at the expense of greater computational time.\nCompared to the K-Means and K-Means++, our algorithm also performs favorably. In particular, it outperforms the KMeans algorithm by as much as 69% in terms of the average J on the Bavaria postal code dataset for K = 5. Compared to the K-Means++ algorithm, our approach achieves a marginal performance improvement in the average J , reaching to 9% on the Bavaria postal code dataset for K = 4. The proposed scheme also achieves the lowest worst objective function as compared to the K-Means and K-Means++ algorithms on all datasets. However, in terms of the rate of convergence, the K-Means++ algorithm is shown to be the best."}, {"heading": "VI. RELATED WORK", "text": "The TS algorithm has been applied to the K-Means clustering problem with a different formulation by Al-Sultan\n[7] where a candidate solution in the form of an array of length N is used. This array denoted as Ac is made up of the cluster indices of all the N objects in D. In order to obtain neighboring solutions (also known as trial solutions), the cluster indices in Ac are changed according to some criterion. This method can lead to bad cluster memberships. This is because while two close objects in the dataset may show a tendency of belonging to one cluster, this scheme may assign different cluster indices to them. The algorithm also involves setting the following parameters: the number of trial solutions NTS, the maximum tabu list size MTLS , the maximum number of TS iterations ITmax, and a probability threshold P . Extensive parametric study has to be carried out for a particular dataset in order to obtain the optimal values.\nThe TS clustering algorithm in [19] discusses essentially the same procedure as the TSC algorithm with two additional neighborhood structures presented. The process of generating neighboring solutions in both of these algorithms is largely random, causing the algorithm to behave to some degree like a random search with memory. The implication of this randomness is that the global optimal solution has an equal chance of being generated in the first TS iteration as it has in the ITmaxth iteration. Consequently, the probability that a global solution may have been found after rmax iterations of non-improving solutions is rather low. Thus, the early termination described in Section IV-B cannot be applied to these algorithms without a significant performance loss.\nThe TS algorithm has also been applied to the Fuzzy C-Means clustering problem [20], where an object in the dataset D can belong to more than one cluster to varying degrees. The TS procedure taken in that formulation aims at optimizing the cluster means, which is similar to the approach taken in our proposed scheme. However, that is as far as the similarity goes. While our scheme constrains the means to objects in the dataset and use gradient information to generate a new neighbor, this approach generates neighboring means by perturbing the current mean along a random direction.\nOther TS approaches for clustering includes the packingreleasing algorithm [21] which is also based on [7], but with the following fundamental difference: a pair of objects in the dataset that are close to each other are packed together and treated as one object. These packed objects are later released. This procedure reduces the size of the search space and guides the search to a local minimum more quickly.\nWhile we have assumed in this work that the number of clusters K is known beforehand, the evolution-based tabu search algorithm [12] uses TS for the determination of the number of clusters in the dataset, by considering K as another variable to be optimized in the TS procedure."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we have presented an efficient Tabu Search procedure for solving the K-Means clustering problem. This involves constraining the K means to objects in the dataset, and optimizing these means via a series of neighbors that are obtained using gradient information of the objective. We have compared the proposed scheme to an existing TS algorithm as well as the K-Means and K-Means++ algorithms. We have shown that this approach performs favorably with these wellknown algorithms, as well as not requiring too many param-\neter settings. This is a promising result for a lot of machine learning applications that use K-Means clustering. We note, however, that the nature of the tabu structure used in our TS implementation might require a large memory, especially for big datasets where the maximum number of TS iterations is correspondingly large. For this reason, ongoing work is focused on identifying a more compact representation of the entries in the tabu structure and consequently reducing the runtime of the algorithm."}], "references": [{"title": "Comparing support vector machines with Gaussian kernels to radial basis function classifiers,", "author": ["B. Schlkopf"], "venue": "IEEE Transactions on Signal Processing vol. 45,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Testing for Uniformity in Multidimensional Data,", "author": ["S.P. Smith", "A.K. Jain"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence vol. 6,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1984}, {"title": "Data Clustering: 50 Years Beyond K-Means,", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters vol", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Refining Initial Points for K-Means Clustering,", "author": ["P.S. Bradley", "U.M. Fayyad"], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning vol. 11,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Computational Experience on Four Algorithms for the Hard Clustering Problem,", "author": ["K.S. Al-Sultan", "M.M. Khan"], "venue": "Pattern Recognition Letters vol. 17,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Clustering Large Graphs via the Singular Value Decomposition,", "author": ["P. Drineas"], "venue": "Machine Learning vol. 56,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A Tabu Search Approach to the Clustering Problem,", "author": ["K.S. Al-Sultan"], "venue": "Pattern Recognition vol. 28,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Tabu Search - Part 1,", "author": ["F. Glover"], "venue": "ORSA Journal of Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "A Parallel Tabu Search Algorithm for Large Travelling Salesman Problems,", "author": ["C.-N. Fiechter"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Tabu Search Detection for MIMO Systems,", "author": ["H. Zhao", "H. Long", "W. Wang"], "venue": "IEEE 18th International Symposium on Personal, Indoor and Mobile Radio Communications", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Learning the K in K-Means,", "author": ["G. Hamerly", "C. Elkan"], "venue": "Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Evolution-Based Tabu Search Approach to Automatic Clustering,", "author": ["S.-M. Pan", "K.S. Cheng"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics vol. 37,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "A Self-Organizing Network for Hyperellipsoidal Clustering,", "author": ["J. Mao", "A.K. Jain"], "venue": "IEEE Transactions on Neural Networks vol. 7,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "Clustering with Bregman Divergences,", "author": ["A. Banerjee"], "venue": "Journal of Machine Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "K-Means Clustering of Proportional Data using L1 Distance,", "author": ["H. Kashima"], "venue": "19th International Conference on Pattern Recognition ICPR", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "K-Means++: The Advantages of Careful Seeding,", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "A Modified Tabu Search Approach for the Clustering Problem,", "author": ["A Kharrousheh", "S. Abdullah", "M. Nazri"], "venue": "Journal of Applied Sciences vol. 11,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Fedjki, \u201dA Tabu Search-Based Algorithm for the Fuzzy Clustering Problem,", "author": ["C.A.K.S. Al-Sultan"], "venue": "Pattern Recognition vol. 30,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "A Tabu-Search-Based Heuristic for Clustering,", "author": ["C.S. Sung", "H.W. Jin"], "venue": "Pattern Recognition vol. 33,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "example of this latter application is in the implementation of the radial basis function (RBF) with K centers [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "It is assumed that the objects in D lend themselves to some natural grouping [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "Clustering is, however, an ill-posed problem [3] for the following reasons.", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "The value of K which minimizes some predefined criterion like the Akaike Information Criterion (AIC) or the Bayes Information Criterion [3] is then chosen.", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "Clustering algorithms may yield poor results if the K chosen is inappropriate [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "For this reason, several other methods have been applied to solving the clustering problem [4]-[7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "For this reason, several other methods have been applied to solving the clustering problem [4]-[7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "Notable among these is the approach of Al-Sultan [7] which is based on the Tabu Search", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "(TS) algorithm developed by Glover [8].", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "[7] (our reference work) as the Tabu Search Clustering (TSC) algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "traveling salesman problem (TSP) [9] and signal detection in multiple input multiple output (MIMO) antenna systems [10].", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "traveling salesman problem (TSP) [9] and signal detection in multiple input multiple output (MIMO) antenna systems [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "[11] and Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] for a detailed treatment on how to choose K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "In general, none of these is known, and so the following set of simplifying assumptions [4] are", "startOffset": 88, "endOffset": 91}, {"referenceID": 12, "context": "Indeed, [13]-[15] explore the use of other distance measures for clustering.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Indeed, [13]-[15] explore the use of other distance measures for clustering.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "parameter J known as the intra cluster sum of squares (ICSS) or the distortion [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "problem is computationally difficult, and is NP-hard [6].", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "Since the K-Means algorithm is a special case of the Expectation-Maximization (EM) algorithm [4], it proceeds in two stages namely, the expectation and the maximization stages.", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "Our approach is motivated by the above limitation, and is based on the TS algorithm in [8].", "startOffset": 87, "endOffset": 90}, {"referenceID": 15, "context": "We compare the performance of this scheme to the TSC, the K-Means++ [18], and the K-Means algorithms in terms of the objective function of (2) and the time taken for completion.", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "95 as suggested by Al-Sultan [5].", "startOffset": 29, "endOffset": 32}, {"referenceID": 6, "context": "The TS algorithm has been applied to the K-Means clustering problem with a different formulation by Al-Sultan [7] where a candidate solution in the form of an array of length N is used.", "startOffset": 110, "endOffset": 113}, {"referenceID": 16, "context": "The TS clustering algorithm in [19] discusses essentially the same procedure as the TSC algorithm with two additional neighborhood structures presented.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "The TS algorithm has also been applied to the Fuzzy C-Means clustering problem [20], where an object in the dataset D can belong to more than one cluster to varying degrees.", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "Other TS approaches for clustering includes the packingreleasing algorithm [21] which is also based on [7], but with the following fundamental difference: a pair of objects in the dataset that are close to each other are packed together and treated as one object.", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Other TS approaches for clustering includes the packingreleasing algorithm [21] which is also based on [7], but with the following fundamental difference: a pair of objects in the dataset that are close to each other are packed together and treated as one object.", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "clusters K is known beforehand, the evolution-based tabu search algorithm [12] uses TS for the determination of the number of clusters in the dataset, by considering K as another variable to be optimized in the TS procedure.", "startOffset": 74, "endOffset": 78}], "year": 2017, "abstractText": "The Tabu Search (TS) metaheuristic has been proposed for K-Means clustering as an alternative to Lloyd\u2019s algorithm, which for all its ease of implementation and fast runtime, has the major drawback of being trapped at local optima. While the TS approach can yield superior performance, it involves a high computational complexity. Moreover, the difficulty in parameter selection in the existing TS approach does not make it any more attractive. This paper presents an alternative, low-complexity formulation of the TS optimization procedure for K-Means clustering. This approach does not require many parameter settings. We initially constrain the centers to points in the dataset. We then aim at evolving these centers using a unique neighborhood structure that makes use of gradient information of the objective function. This results in an efficient exploration of the search space, after which the means are refined. The proposed scheme is implemented in MATLAB and tested on four real-world datasets, and it achieves a significant improvement over the existing TS approach in terms of the intra cluster sum of squares and computational time.", "creator": "LaTeX with hyperref package"}}}