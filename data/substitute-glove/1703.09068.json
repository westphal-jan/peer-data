{"id": "1703.09068", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Automatic Decomposition of Self-Triggering Kernels of Hawkes Processes", "abstract": "Hawkes Processes try `` -. relationship - modulated both events on which stay some long games mean fact ones especially some to thinking to there - theme data. Identification much given fenestrae spacetime os/2 can reveal the underlying rather both better difficult future many. In this hand, enough of a both context time significant beginning - 2013 events then a visual of so - triggering kernels followed Hawkes Processes. That important, into switching time - series events are decomposed instead these Hawkes Processes with thermally candies. Our ignition homologous procedure is composed for three south implementation: (29) axisymmetric kernel estimation rest frequency domain riemann polynomial associated still within covariance density, (22) greedy byte formula_2 through twenty edge kernels instead their involves (include three formula_18 ), having (36) devices assessment alternative. We demonstrate that the and loading formula_6 applies performs better and change future events than by use framework although real - successful data.", "histories": [["v1", "Mon, 27 Mar 2017 15:25:54 GMT  (673kb,D)", "https://arxiv.org/abs/1703.09068v1", null], ["v2", "Mon, 17 Jul 2017 04:43:25 GMT  (1751kb,D)", "http://arxiv.org/abs/1703.09068v2", null], ["v3", "Tue, 18 Jul 2017 10:01:49 GMT  (1751kb,D)", "http://arxiv.org/abs/1703.09068v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rafael lima", "jaesik choi"], "accepted": false, "id": "1703.09068"}, "pdf": {"name": "1703.09068.pdf", "metadata": {"source": "CRF", "title": "Automatic Decomposition of Self-Triggering Kernels of Hawkes Processes", "authors": ["Rafael Lima", "Jaesik Choi"], "emails": ["rafael_glima@unist.ac.kr", "jaesik@unist.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "Point Processes [4] have been used for modeling time-events series data. Hawkes Processes (HP) [9] are point processes for modeling self-exciting behavior, i.e., when the arrival of one event makes future events more likely to happen. This type of behavior has been observed in various domains, such as earthquakes, financial markets, web traffic patterns, crime rates [14, 16] and social media [22].\nAs an example, in high-frequency finance, buyers and sellers of stocks demonstrate herding behavior [7, 2]. After the main earthquake, several aftershocks follow according to a time-clustered pattern [18]. In web data, hyperlink proliferation\nar X\niv :1\n70 3.\n09 06\n8v 3\n[ cs\n.L G\n] 1\nacross pages exhibit self- and mutual-excitation [8]. In criminology, gang-related retaliatory crime patterns are grouped in time [14]. In social media, the \u2018infectiousness\u2019 of posts can be shown to be modeled through a self- and mutual-excitement assumption [22].\nIn Hawkes Processes analysis, some parametric kernels capture intra-domain typical behaviors, e.g., quick time-decaying exponential excitation in the case of finance and web data [1, 8]; slower power-law decay in earthquake-related data [18]; and periodicity-inducing sinusoidal kernel in TV-watching data [21].\nChoosing a proper kernel type may significantly contribute to learning the model parameters and better predicting future events. Kernel parameters may be fitted in the data through the gradient descent method over a likelihood function penalized by a regularization criterion (e.g., Akaike Information Criterion) on the number of parameters [19]. Another method of kernel estimation is through the use of the power spectrum of the second order statistics of the process: covariance density and normalized covariance [9]. These are well defined when the self-triggering function induces what is called stationary behaviour.\nIn this paper, we present an automatic kernel selection and learning algorithm for Hawkes Processes. Given four types of base kernels (EXP,PWL,SQR and SNS), our algorithm finds the best fitting kernel. For handling composite types of events, we develop a new kernel decomposition method which represents the composition (sum and product) of different kernels. For verifying the stationarity property of each composite kernel, we also derived analytical expressions for the stationarity conditions. To our best knowledge, this is the first multi-class kernel decomposition framework for HPs.\nThe main steps of the automatic framework, which will be thoroughly explained in the following sections, are then: discretized kernel estimation and greedy kernel decomposition. An automatic framework for extracting relevant typical features of self-excitement from raw data would likely make the Hawkes Processes analysis faster and conveniently expressed through a human-readable report. Thus, the automated generation of such a report is also discussed.\nAutomatic analysis frameworks for Gaussian Processes (GPs) are proposed in [5] and [10]. However, due the fundamental distinctions between GPs and HPs (such as stationarity conditions and causality assumptions for the latter), the techniques proposed for GPs can not be extended to HPs in a straightforward manner. We include additional references in the appendix."}, {"heading": "2 Hawkes Processes", "text": "A point process representation of a sequence of n time-events is expressed by a vector of the form (t1,t2, ... , tn). Treating the real line as a time axis, the vector can be intuitively associated with a so-called Counting Process N(t), s.t.:\n\u2022 dN(t) = 1, if there is an event at time t;\n\u2022 dN(t) = 0, otherwise.\nAn example of Counting Process is shown in Figure 1. A point process can be described through its intensity function (\u03bb(t)), which can be understood as the instantaneous expected rate of arrival of events, or the expectation of derivative of the Counting Process N(t):\n\u03bb(t) = lim h\u21920 E[N(t+ h)\u2212N(t)] h\n(1)\nThis intensity function, if existing, uniquely characterizes the finite-dimensional distributions of the point process [4]. A simple example of this function would be the constant mean rate of arrival, \u00b5, in the case of a homogeneous Poisson process.\nHawkes processes model the intensity function in terms of the self-excitement: when the arrival of an event makes subsequent arrivals more likely to happen [11]; and can be described through the following conditional intensity function \u03bb(t):\n\u03bb(t) = lim h\u21920 E(N(t+ h)\u2212N(t)|H(t)) h\n= \u00b5+ \u222b t \u2212\u221e \u03c6(t\u2212 u)dN(u), (2)\nwhere:\n\u2022 H(t) is the history of the process, the set containing all the events up to time t;\n\u2022 \u00b5 is called background rate, or exogenous intensity, which is fixed as the mean of a Homogeneous Poisson Process;\n\u2022 \u03c6(t) is denominated self-triggering kernel, or excitation function;\nFrom this function, one may notice that the intensity at time t will likely be affected by events which happened before the time t, described by the history of the process.\nFrom [9], we have that, if :\n||\u03c6|| = \u222b \u221e 0 \u03c6(t)dt \u2264 1, (3)\nthen the corresponding process will reach wide-sense stationary behavior, from which the asymptotic steady arrival rate:\n\u039b = \u00b5\n(1\u2212 ||\u03c6||) , (4)\ncan be obtained, along with its covariance function, which is independent of t:\n\u03bd(\u03c4) = E(dN(t)dN(t+ \u03c4)). (5)\nEstimating \u039b and \u03bd(\u03c4), also referred to as first- and second-order statistics, respectively, requires wide-sense stationarity assumptions which, besides being analytically convenient, are also connected to the fact that, in real data, the chain of self-excitedly induced further events will always be of finite type, or without \u2018blowing up\u2019, what corroborates the practicality of the estimated model."}, {"heading": "2.1 Self-Exciting Kernels", "text": "From the definition of the conditional intensity function, the self-excitement of the process is expressed through the kernel function \u03c6(t). For the kernel decomposition,\nfour base kernels will be used for identifying and estimating typical triggering behaviors:\n\u2022 EXP(\u03b1,\u03b2): The decay exponential kernel is parameterized by the amplitude \u03b1 and decay rate \u03b2, and is useful for modeling quick influence decay, such as in finance or web data [8], in which initial transactions/hyperlinks have a lot of impact initially but gradually reduce their influence over time:\nEXP(\u03b1, \u03b2) = \u03b1e\u2212\u03b2t (6)\n\u2022 PWL(K,c,p): The power law kernel is parameterized by the amplitude K, the exponent p, and the constant c, such as in Equation (7), and has been prevalent in earthquake [18] and social media-related data [22], modeling a slower decaying trend than the exponential:\nPWL(K, c, p) = K\n(t+ c)p (7)\n\u2022 SQR(B,L): The pulse kernel is described by the amplitude B and the length L. Being a trivial, steady, self-exciting dynamics on its own, it may also work as an offset level for the combined triggering with other kernel types, in the case of addition, and as a horizon truncation, in the case of multiplication1:\nSQR(B,L) = B(u(t)\u2212 u(t\u2212 L)) (8) 1u(t) is the step function\n\u2022 SNS(A,\u03c9): A truncated sinusoidal, parameterized by the amplitude A and the angular velocity \u03c9. This type of kernel base function captures well the self-excitement of periodic events, such as TV watching-related data (IPTV) [21], in which watching one episode of a TV program makes the viewer more likely to watch further ones. Since these shows are usually broadcasted weekly, the TV-watching behavior will likely demonstrate the weekly self-excitement. In addition, according to [14], homicide rates show a pronounced seasonal effect, peaking in the summer and tapering in the winter:\nSNS(A,\u03c9) = Asin(\u03c9t), (9)"}, {"heading": "3 Automatic Kernel Decomposition for HPs", "text": "This section presents two steps for the automatic kernel identification. The steps are (1) nonparametric kernel estimation from discretized covariance and (2) parametric kernel search through our new kernel decomposition scheme.\nHere, the discretized kernel estimation is optional when a direct optimization of kernel structure is possible. Unfortunately, discontinuous functions (SQR, SNS) do not allow such optimization (e.g., gradient descent). In this paper, we use the discretized kernel estimation as an unified method for both continuous (EXP, PWL), discontinuous kernels (SQR, SNS) and, most importantly, their combinations.\nFurthermore, another great advantage of this step, compared with traditional sequential methods, is the fact that the value of \u03bd for each value of \u03c4 can be calculated independently, while, in gradient descent, the value of the parameters at step t must be obtained before the values for step t + 1. When combined with parallelization of the loops, in our case, this step greatly improved the speed of obtainment of the most likely parametric representations of the sample processes."}, {"heading": "3.1 Discretized Kernel Estimation", "text": "This step is fully described in [1], and basically consists of building an estimator of \u03c6(t) from empirical measurements of \u03bd(\u03c4), the stationary covariance.\nGiven a finite sequence of ordered time-events in [0, T ], we fix a window size of h, and estimate \u03bd(\u03c4) as:\n\u03bd(h)\u03c4 = 1\nh E\n( ( \u222b h 0 dNs \u2212 \u039bh)( \u222b \u03c4+h \u03c4 dNs \u2212 \u039bh) )\n(10)\nIn practice, this estimation is done in discrete time steps \u03b4:\n\u03bd (h) \u03c4,\u03b4 =\n1\nT bT/\u03b4c\u2211 i=1 (dN (h) i\u03b4 \u2212dN (h) (i\u22121)\u03b4)(dN (h) i\u03b4+\u03c4\u2212dN (h) (i\u22121)\u03b4+\u03c4 ), (11)\nwhere dN (h)i\u03b4 is the total number of events happening between t = i\u03b4 and t = i\u03b4+h.\nFrom Theorem 1 in [1], we have that, given g(h)t = (1\u2212 |h| t ) +, i.e., a triangular kernel density estimator with bandwidth \u201ch\u201d, we have the following relation in Laplace domain:\n\u02c6 \u03bd (h) z = g\u0302z (h)(1 + \u03c8\u0302?z)\u039b(1 + \u03c8\u0302 ? z) \u2020, (12)\nwhere2 :\n\u03c8\u0302z = +\u221e\u2211 n=1 \u03c6\u0302nz = \u03c6\u0302z (1\u2212 \u03c6\u0302z) . (13)\nWorking with the Fourier transform restriction, i.e., (z = i\u03c9, with \u03c9 \u2208 R) and given that:\ng\u0302 (h) i\u03c9 =\n4\n\u03c92h sin2(\n\u03c9h\n2 ), (14)\nwe get to:\n(1 + \u03c8\u0302?z)\u039b(1 + \u03c8\u0302 ? z) \u2020 =\n\u02c6 \u03bd (h) z g\u0302z (h) , (15)\nwhere we fix h = \u03b4, so we do not bother with cancellations of g\u0302(h)z . And then, from:\n|1 + \u03c8\u0302i\u03c9|2 = \u03bd\u0302 (h) z\n\u039bg\u0302z (h) , (16)\nwe get to the discretized estimation of \u03c6t by taking the inverse Fourier transform of:\n\u03c6\u0302i\u03c9 = 1\u2212 e\u2212 log |1+\u03c8\u0302i\u03c9 |+iH(log |1+\u03c8\u0302i\u03c9 |), (17)\nin which the operator H(\u00b7) refers to the Hilbert transform."}, {"heading": "3.2 Greedy Kernel Decomposition", "text": "For expressing the discretized estimation in terms of the four base kernels, the following steps are executed:\n2Given a function ft, f\u0302z is its Laplace Transform, and the \u201c?\u201d symbol corresponds to its conjugate\nEXP(\u03b1,\u03b2) \u03b1\n\u03b2 PWL(K,c,p)\nKc1\u2212p\np\u2212 1 , (p > 1) SQR(B,L) BL SNS(A,\u03c9)\n2A\n\u03c9\nThe pseudocode of our algorithm is included in Algorithm 1.\nAlgorithm 1 Automatic Decomposition of HP Kernels 1: input = kest 2: fit1 = fit(kest; \u2205, {EXP, PWL, SQR, SNS}) 3: K1 = index_of_kernel(min_residue(fit1)) 4: MR1 = min_residue(fit1) 5: fit2 = fit(kest;K1, {+EXP, * EXP,+PWL, *PWL,\n+SQR, *SQR,+SNS, *SNS}) 6: MR2 = min_residue(fit2) 7: K2 = index(min_residue(fit2)) 8: output = None 9: if ||\u03c6K1 || < 1 then"}, {"heading": "3.3 Stationarity Conditions", "text": "As a quick way of evaluating the validity of the estimated models regarding the stationarity criterion, we developed closed-form expressions, either in the form of equality or as an upper bound, which are shown in Table 2, for the case of a single kernel, and Table 3, for multiplicative combinations of two kernels 3. The conditions for additive combinations can be derived from the conditions for single\n3 \u0393(\u00b7, \u00b7) is the well-known Incomplete Gamma Function: \u0393(a, y) = \u222b\u221e y ta\u22121e\u2212tdt\nkernels in a straightforward manner. The kernel is said to induce stationarity if the result of the expression calculated using the estimated parameters belongs to the interval [0,1). This can be justified both from the point-of-view of Hawkes Process as a branching process, also called immigrant-birth representation [15], and of the boundedness of the spectral radius (largest absolute value among the eigenvalues) of the excitation matrix. 4"}, {"heading": "4 Analysis of Higher-order Kernel Decomposition", "text": "A sequential additive decomposition of the discretized estimation vector is rather straightforward, since one may just set the residual vector from the previous stages as the input of the next ones.\nIn the case of multiplicative decomposition, it is nontrivial to find the result of intraclass decomposition. To the best of our knowledge, no analysis on multiplicative HP kernel decomposition is reported yet.\nIn this paper, we provide a new upper bound over an interclass kernel product of unknown degree, as in:\n[EXP]k1 \u00d7 [PWL]k2 \u00d7 [SQR]k3 \u00d7 [SNS]k4\nfor ki \u2208 Z\u2217, where the operator \u201c[\u00b7]k\u201d corresponds to the set of functions which can be decomposed into a k-th order product of kernels, e.g:\n[EXP ]k = \u03b11e \u2212\u03b21x \u2217 \u03b12e\u2212\u03b22x \u2217 ... \u2217 \u03b1ke\u2212\u03b2kx\ufe38 \ufe37\ufe37 \ufe38\nk terms\n.\nBy deriving the four possible intraclass kernel products, one may observe that the typical self-exciting behavior features of each kernel type are preserved, as in the following:"}, {"heading": "4.1 EXP", "text": "[EXP]k1 reduces to the case of a single exponential with \u03b1 = \u220fk1 i=1 \u03b1i and \u03b2 =\u2211k1\ni=1 \u03b2i, thus still accounting for its \u2018quick-decay\u2019 behavior:\n[EXP]k1 \u2282 [EXP] ; 4For the Univariate HP case, the excitation matrix has dimension one, being only the excitation\nfunction, \u03c6(t)."}, {"heading": "4.2 PWL", "text": "[PWL]k2 is lower bounded by a single PWL kernel with K = \u220fk2 i=1Ki, c =\nmax(c1, ..., ck2) and p = \u2211k2\ni=1 pi, thus still accounting for its \u2018slow-decay\u2019 behavior;"}, {"heading": "4.3 SQR", "text": "[SQR]k3 reduces to a single SQR kernel withB = \u220fk4 i=1Bi andL = min(L1, ..., Lk4), thus still accounting for its \u2018steady-triggering\u2019 behavior:\n[SQR]k3 \u2282 [SQR] ;"}, {"heading": "4.4 SNS", "text": "[SNS]k4 has A = \u220fk4 i=1Ai and a \u2018spikier\u2019 aspect (higher bandwidth), thus still accounting for its \u2018periodicity-inducing\u2019 behavior. Thus, on deepening the decomposition algorithm by overly increasing the number of levels above 2, we may be, in fact, adding little information on the qualitative aspect of the self-exciting behavior analysis of the data while making it more prone to overfitting on the noisiness of the discretized estimation vectors."}, {"heading": "4.5 Upper Bound", "text": "Furthermore, regarding the boundedness of the higher-order decompositions, from the exact results for EXP and SQR intraclass decompositions and the upper bounds for the PWL and SNS ones, we have that:\n[EXP]k1 \u00d7 [PWL]k2 \u00d7 [SQR]k3 \u00d7 [SNS]k4 \u2264 \u03b1e\u2212\u03b2x K (x+ cupper)p BAsin(\u03c9x)\n\u2264 \u03b1BKAe \u2212\u03b2x\n(x+ cupper)p = EXP(\u03b1, \u03b2)\u00d7PWL(K, cupper, p)k2\u00d7SQR(B,L)\u00d7A,\nfor 0 \u2264 x \u2264 min(L, \u03c0 \u03c9 ), and 0 otherwise."}, {"heading": "5 Automated Report Generation", "text": "After estimating the likelihood-maximizing base kernel combination, a LaTeX file report, a sample of which is included in the Appendix, is generated with a description of the corresponding parameters and its most relevant aspects, such as:\n\u2022 Decay-Rate, which would be associated with decaying kernels, EXP and PWL. A exponential decay rate is described as \u2018a quick decaying triggering influence\u2019, and the power-law decay may be described as \u2018a slowly decaying triggering influence\u2019, with its respective parameters being put in evidence;\n\u2022 Steady-Rate Influence, equivalent to an offset level from the SQR kernel type. It is described as \u2018a steadily triggering influence\u2019, with the parameter B put into evidence;\n\u2022 Triggering Horizon, associated with discontinuous kernels, SNS and SQR. It is described as \u2018a horizon of triggering influence\u2019, with the parameter L, in the SQR case, or the value of ( \u03c0\n\u03c9 ), in the SNS case, put into evidence;\n\u2022 Induced Periodicity, in the case of the SNS kernel type. It is described as \u2018a periodicity-inducing triggering influence\u2019, with the value of ( 2\u03c0\n\u03c9 ), from the\nSNS case, put into evidence."}, {"heading": "6 Related Work", "text": "A spectral analysis approach to a one-dimensional self-exciting point process was introduced in [9]. Recently, the spectral method was extended to non-parametric kernel estimation for symmetrically networked HPs [1].\nA Likelihood Maximization method was used on a class of parametric kernels for the excitation matrices in [19], while a series of works for likelihood maximization methods for power-law shaped kernels on seismology and earthquake data modeling were compiled in [18]. However, in [2], it is argued that, being designed for univariate Hawkes Processes, these methods can hardly be used to handle large amounts of data where the kernel function is not well localized compared to the exogenous inter-events time. Subsequently, an extensive analysis of spectral methods for non-parametric kernel estimation on networked HPs is performed.\nIn [14], Networked HPs are explored for analysis and modeling of stock-trading and crime data. A variant of the spectral method for networked HPs, with excitation matrix composed by linear combinations of decaying-exponentials, there referred to as \u2018excitation modes\u2019, is developed in [8]. An iterative log-likelihood maximization for non-parametric kernel estimation is presented in [23].\nIn [21], a maximum-likelihood estimator with a Sparse-Group-Lasso regularizer is introduced. In [15], an extensive mathematical treatment of the so-called \u2018genuine multivariate\u2019 Hawkes Processes, along with some considerations on the case of large-scale and networked data analysis, is provided. In [13], a nonparametric Expectation-Maximization algorithm for Multi-scale Hawkes Processes under the\nassumption of exponential triggering kernels is proposed, while, in [12], a largescale inference algorithm for kernels modeled after a summation of exponentials is introduced."}, {"heading": "7 Experimental Results", "text": "For testing the validity of the kernel decomposition framework, we conducted experiments with synthetic, financial and earthquake data. Synthetic data sequences were used for evaluating the effectiveness of a scale-independence strategy for setting the maximum value of \u03c4 in Equation 11. In the financial data experiments, quantitative comparisons were made among the first and second levels of the decomposition framework over a set of time sequences, and also among our decomposition framework and an estimation method largely used in financial applications, described in [19]. In the earthquake data experiments, we evaluate the overall triggering behavior of earthquakes through the relative frequency of each estimated kernel type, out of the four base kernels.\nFor real-world data sets, no prior information about the kernel (type and parameters) is available. Thus, we use the log-likelihood of the kernel function over the time sequence as a quality criterion.\nGiven a realization (t1, t2, ..., tk) of some regular point process on [0,T], its log-likelihood (l) is expressed as:\nl = k\u2211 i=1 log(\u03bb(ti))\u2212 \u222b T 0 \u03bb(u)du. (18)"}, {"heading": "7.1 Synthetic Data", "text": "As a means of evaluating the overall precision and efficiency of the kernel decomposition framework, experiments with synthetic data from the corresponding four basic kernel types were performed.\nSimulation algorithms related to Hawkes Processes, from which synthetic time sequences data may be obtained, are divided into two main categories: clusterbased [17] and intensity-based [20]. For the proposed experiments, the \u201cThinning\u201d algorithm, an intensity-based one, was used, mainly because the parametric representations of the kernel types make it very convenient to accurately calculate the value of the intensity function throughout the whole simulation horizon.\nThe referred algorithm, whose full denomination is \u201cOgata\u2019s Modified Thinning Algorithm\u201d, basically consists of simulating time sequences using a exceedingly high constant value for the intensity function and then thinning out the generated\nevents using rejection sampling with regards to the locally calculated actual values of the intensity.\nFor an automatic decomposition framework willing to perform effective analysis of several domains of data, scale-independence is indispensable, as time sequences of disjoint datasets may occur in time scales differing by several orders of magnitude. As an example, earthquake events\u2019 occurrences in a sequence are spaced by intervals of monthly and yearly scales. Thus, setting a horizon of a few months as the maximum value of \u03c4 in Equation (11) might result in a satisfactory discrete estimation grid, but using the same time length for estimating the triggering behavior of a finance-related sequence would require an impractically large grid resolution for making itself effective.\nA histogram of all the time intervals between events in a sequence may be readily generated, and is an indicator of the overall magnitude of the spacing among the events. Thus, as a rule of thumb, the horizon length for \u03c4 may be set as the smaller time interval strictly larger than 95% of the sequence\u2019s intervals. In practice, this value of horizon length is obtained with the help of a histogram composed by 100 bins.\nFirst, we generated 10 sequences with the \u2019Thinning algorithm\u2019 [20], in the time range of 0 to 100000, for each of the four basic kernel types, with predefined kernel parameters and influence horizon of 6.6 5. Then, the decomposition framework was used for building confusion matrices over the sets of sequences for both the original horizon length (i.e., 6.6) and the histogram-based horizon length. The confusion matrices are shown in Table 4.\n5By influence horizon, we mean that the excitation effect of each kernel is considered null for t > 6.6."}, {"heading": "7.2 Financial Data", "text": "Previous empirical studies have provided evidence that price impact has, on many aspects, some universal properties and is the main source of price variations, what reinforces the notion of a endogenous nature, in other words, a internal feedback mechanism, of price fluctuations. This is contrasting with the classical notion of an exogenous flow of information driving the prices towards a fundamental value [3].\nIn this endogeneity scenario, Hawkes Processes have become more and more present in the high-frequency finance domain, due to its structure being naturally adapted to model systems in which the discrete nature of the jumps in Nt is relevant, making the model remarkably suited to modeling high-frequency data [6]. Moreover, Hawkes Processes\u2019 parameters, along with their corresponding straightforward interpretations, lead to a notably simple and flexible interpretation of the complex intra-day dynamics of modern electronic markets.\nFor the experimental setting, we picked 19 technology companies in the NASDAQ and NYSE lists of Google Finance: CSCO, GOOGL, HPQ, INTC, IBM, MSFT, ORCL, TXN, XRX, AAPL, ANGI, LITH, IAC, FB, LOCMQ, YELP, GRPN, AMZN and GCI. We extracted tick data from every two minutes of 30 business days (04/07/2017 to 05/18/2017). Whenever a stock price changed by some magnitude higher than some threshold, an event was logged in the corresponding time sequence. Ten different percentual thresholds, increasing at equally spaced intervals from 0.03% to 0.3%, were applied. This procedure resulted in 70 valid sequences, since the remaining ones did not contain enough points for the splitting between training and validation subsequences.\nThe 80% of the first elements for each sequence were then used as training data, and the remaining 20% were used for validation, i.e., we estimated the parameters of the kernel using the first 24 days and then calculated the log-likelihood on the last 6 days of each sequence. When comparing the log-likelihoods of first and second level decompositions, we observed that the second level, with composite kernels, resulted in a higher log-likelihood in 69 out of the 70 sequences (98.57%), what corroborates\nthe idea of a more flexible model of the kernel providing a more accurate description of the underlying dynamics of the process. Along the 70 sequences, the average value of the log-likelihood of the K1 Decomposition is -2096.95, while, for the K2 Decomposition, it is equal to -1894.61. The average Loglikelihood for each level is shown in Table 5. The percentual difference from each sequence is shown in Figure 8.\nWhen comparing the performance of the best estimation among the two levels and the usual Exponential HP model used in financial analysis, fitted through the gradient-based method from [19], it is possible to see that the Decomposition Algorithm exhibited a much more robust performance. Although the Exponential HP performed well in some sequences, it tended to get stuck in local maxima with very poor performance, usually leading to unstable or negative combinations of parameters. The Decomposition Algorithm performs better in 67.14% (47 out of 70) of the sequences. The comparison for each individual sequence is shown in Figure 7."}, {"heading": "7.3 Earthquake Data", "text": "In lists regularly published by seismological services of most countries habituated to the occurrences of earthquakes, one can obtain relevant information, such as the epicenter of each shock, focal depth, instrumental magnitude and origin time of each earthquake occurrence. Statistical analyses of earthquake catalogues and\nassessment of earthquake risk in a geophysical area can then be performed through the use of parametric models on the sequences of origin times, obtained by largely ignoring the remaining information. On further analyzing these fitted models, one can then identify and decompose components such as evolutionary trend, periodicity and clustering [18].\nAgain, the need to insert scale-independence, in the form of the aforementioned histogram-related heuristics, in the decomposition framework makes itself imperative, as earthquakes events are separated by time intervals of monthly or yearly scales. Thus, the estimation horizon for financial data, for example, lasting usually only a few seconds, would hardly capture the overall aspect of the triggering behavior in this case.\nThe data considered for the experiment was a set of 100 time sequences extracted from the USGS NCSN Catalog (NCEDC database), from the day of 1966/Jan/01 to 2015/Jan/01. The Latitude range was [30,55], and the Longitude range was [-140,-110]. Different length intervals and resulting areas were considered. Whenever the magnitude of an event exceeded some threshold, its time coordinate was added to the corresponding input time sequence. The magnitude thresholds were varied among 2.5, 3.0, 3.5 and 4.0; and the grid resolution was set to 20 and 100 points. For 20-point grid resolution, the relative frequency of each kernel was (EXP,PWL,SQR,SNS) = (0,97,2,1). For the 100-point grid resolution, the relative frequency was (0,99,1,0). The results of first and second level decompositions are shown in Tables 6 and 7.\nThe results indicate a strong agreement with the long standing assumption of a power-law shaped kernel for the intensity of aftershocks\u2019 ocurrences (\u2018Omori\u2019s Law\u2019 (1894)). Q-Q plots from the estimated models are shown in Figure 10."}, {"heading": "8 Conclusion", "text": "Hawkes processes are temporal point processes which capture self-exciting discrete events in time series data. To predict future events with HPs, an appropriate kernel is selected by hands, previously. In this paper, we proposed a new temporal covariancebased kernel decomposition method to represent various self-exciting behaviors. We also presented a model (structure/parameter) learning algorithm to select the best HP kernel given the temporal discrete events. The stationarity conditions are derived to guarantee the validity of the kernel learning algorithm. In experiments, we demonstrated that the proposed algorithm performs better than existing methods to predict future events by automatically selecting kernels."}, {"heading": "A Derivations of Stationarity Criteria for Multiplicative Combinations of Kernels", "text": "This appendix introduces the full derivations of stationarity criteria for the second order multiplicative compositions of the four base kernels."}, {"heading": "A.1 EXP x EXP", "text": "For the combination \u201cEXPxEXP\u201d, we have that, for stationarity to be achieved:\n0 \u2264 \u222b \u221e 0 EXP (\u03b11, \u03b21)EXP (\u03b12, \u03b22)dx < 1\n0 \u2264 \u222b \u221e 0 \u03b11e \u03b11x\u03b12e \u03b22xdx < 1\nThus: \u222b \u221e 0 \u03b11e \u2212\u03b21x\u03b12e \u2212\u03b22xdx = \u222b \u221e 0 (\u03b11\u03b12)e \u2212(\u03b21+\u03b22)xdx\n= \u222b \u221e 0 \u03b1e\u2212\u03b2xdx = \u03b1 \u03b2 = \u03b11\u03b12 \u03b21 + \u03b22\nSo, this case reduces to the case of a single exponential."}, {"heading": "A.2 EXP x PWL", "text": "For the combination \u201cEXPxPWL\u201d, we have that, for stationarity to be achieved:\n0 \u2264 \u222b \u221e 0 EXP (\u03b1, \u03b2)PWL(K, c, p)dx < 1\n0 \u2264 \u222b \u221e 0 \u03b1e\u2212\u03b2x K (x+ c)p dx < 1\nThus: \u222b \u221e 0 \u03b1e\u2212\u03b2x K (x+ c)p dx = \u03b1K \u222b \u221e 0 (x+ c)\u2212pe\u2212\u03b2xdx\n= \u03b1Ke\u03b2c \u222b \u221e 0 (x+ c)\u2212pe\u2212\u03b2(x+c)dx\n= \u03b1Ke\u03b2c\u03b2p \u222b \u221e 0 (\u03b2(x+ c))\u2212pe\u2212\u03b2(x+c)dx\n= \u03b1Ke\u03b2c\u03b2p\u22121 \u222b \u221e \u03b2c t\u2212pe\u2212tdt\n= \u03b1Ke\u03b2c\u03b2p\u22121\u0393(1\u2212 p, \u03b2c),\nwhere \u0393(\u00b7, \u00b7) is the well-known Incomplete Gamma Function: \u0393(a, y) = \u222b\u221e y t a\u22121e\u2212tdt."}, {"heading": "A.3 EXP x SQR", "text": "For the combination \u201cEXPxSQR\u201d, we have that, for stationarity to be achieved: 0 \u2264 \u222b \u221e 0 EXP (\u03b1, \u03b2)SQR(B,L)dx < 1\n0 \u2264 \u222b L 0 \u03b1Be\u2212\u03b2xdx < 1\nThus: \u222b L 0 \u03b1Be\u2212\u03b2xdx = [ \u03b1Be\u2212\u03b2x \u03b2 ]L 0 = \u03b1B(1\u2212 e\u2212\u03b2L) \u03b2\nSo, in the case of a multiplicative combination, the SQR kernel acts as a truncation horizon."}, {"heading": "A.4 EXP x SNS", "text": "For the combination \u201cEXPxSNS\u201d, we have that, for stationarity to be achieved: 0 \u2264 \u222b \u221e 0 EXP (\u03b1, \u03b2)SNS(A,\u03c9)dx < 1\n0 \u2264 \u222b \u03c0 \u03c9\n0 A\u03b1e\u2212\u03b2xsin(\u03c9x)dx < 1\nWhere:\n\u222b \u03c0 \u03c9\n0 A\u03b1e\u2212\u03b2xsin(\u03c9x)dx =\n\u222b \u03c0 \u03c9\n0 A\u03b1e\u2212\u03b2x\nei\u03c9x \u2212 e\u2212i\u03c9x\n2i dx\n= A\u03b1\n2i\n[ e(\u2212\u03b2+i\u03c9)x\n\u2212\u03b2 + i\u03c9 \u2212 e\n(\u2212\u03b2\u2212i\u03c9)x\n\u2212\u03b2 \u2212 i\u03c9\n]\u03c0 \u03c9\n0\n= A\u03b1\n2i\n[ (\u2212\u03b2 \u2212 i\u03c9)e(\u2212\u03b2+i\u03c9)x \u2212 (\u2212\u03b2 + i\u03c9)e(\u2212\u03b2\u2212i\u03c9)x\n\u03b22 + \u03c92\n]\u03c0 \u03c9\n0\n=\n[ A\u03b1e\u2212\u03b2x\n2i\n2i\u03c9cos(\u03c9x)\u2212 2\u03b2sin(\u03c9x) \u03b22 + \u03c92\n]\u03c0 \u03c9\n0\n= A\u03b1\n2i\n\u22122i\u03c9(e \u2212\u03b2\u03c0 \u03c9 \u2212 1)\n\u03b22 + \u03c92\n= A\u03b1\u03c9(1 + e\n\u2212\u03b2\u03c0 \u03c9 )\n(\u03c92 + \u03b22)"}, {"heading": "A.5 PWL x PWL", "text": "In the case of the combination \u201cPWLxPWL\u201d, an upper bound is derived as follows:\n0 \u2264 \u222b \u221e 0 PWL(K1, c1, p1)PWL(K2, c2, p2)dx < 1\n0 \u2264 \u222b \u221e 0 K1 (x+ c1)p1 K2 (x+ c2)p2 dx < 1\nThen: \u222b \u221e 0 K1 (x+ c1)p1 K2 (x+ c2)p2 dx\n\u2264 \u222b \u221e 0 K1K2 (x+min(c1, c2))p1+p2 dx\n= K1K2\n(p1 + p2 \u2212 1)min(c1, c2)(p1+p2\u22121)"}, {"heading": "A.6 PWL x SQR", "text": "For the combination \u201cPWLxSQR\u201d, we have that, for stationarity to be achieved:\n0 \u2264 \u222b \u221e 0 PWL(K, c, p)SQR(B,L)dx < 1\n0 \u2264 \u222b L 0 KB (x+ c)p dx < 1\nWhere: \u222b L 0 KB (x+ c)p dx = [ KB (1\u2212 p)(x+ c)(p\u22121) ] ]L 0\n= KB(c\u2212(p\u22121) \u2212 (c+ L)\u2212(p\u22121))\np\u2212 1\nSo, once again, the SQR kernel acts as a truncation horizon."}, {"heading": "A.7 PWL x SNS", "text": "In the case of the combination \u201cPWLxSNS\u201d, an upper bound is derived as follows:\n0 \u2264 \u222b \u221e 0 PWL(K, c, p)SNS(A,\u03c9)dx < 1\n0 \u2264 \u222b \u03c0 \u03c9\n0\nKAsin(\u03c9x)\n(x+ c)p dx < 1\nWhere:\n\u222b \u03c0 \u03c9\n0\nKAsin(\u03c9x)\n(x+ c)p dx \u2264\n\u222b \u03c0 \u03c9\n0\nKA\n(x+ c)p dx\n=\n[ KA\n(1\u2212 p)(x+ c)(p\u22121) ]\n]\u03c0 \u03c9\n0\n= KA ((c+ \u03c0\u03c9 ) 1\u2212p \u2212 c1\u2212p) 1\u2212 p"}, {"heading": "A.8 SQR x SQR", "text": "For the combination \u201cSQRxSQR\u201d, we have that, for stationarity to be achieved: 0 \u2264 \u222b \u221e 0 SQR(B1, L1)SQR(B2, L2)dx < 1\n0 \u2264 \u222b min(L1,L2) 0 B1B2dx < 1\nWhere: \u222b min(L1,L2) 0 B1B2dx = B1B2min(L1, L2) = BL\nSo, the multiplicative combination of two SQR kernels may be reduced to the case of a single SQR kernel."}, {"heading": "A.9 SQR x SNS", "text": "In the case of combinations of discontinuous kernels (SQR and SNS), we assume they have the same starting and ending points, i.e., L = \u03c0\n\u03c9 . So, for the combination\n\u201cSQRxSNS\u201d, we have that, for stationarity to be achieved:\n0 \u2264 \u222b \u221e 0 SQR(B,L)SNS(A,\u03c9)dx < 1\n0 \u2264 \u222b \u03c0 \u03c9\n0 ABsin(\u03c9x)dx < 1\nWhere:\n\u222b \u03c0 \u03c9\n0 ABsin(\u03c9x)dx =\n2AB\n\u03c9"}, {"heading": "A.10 SNS x SNS", "text": "In the case of combinations of discontinuous kernels (SQR and SNS), we assume they have the same starting and ending points. So, for the combination \u201cSNSxSNS\u201d, we have that, for stationarity to be achieved:\n0 \u2264 \u222b \u221e 0 SNS(A1, \u03c9)SNS(A2, \u03c9)dx < 1\n0 \u2264 \u222b \u03c0 \u03c9\n0 A1A2sin\n2(\u03c9x)dx < 1\nWhere:\n\u222b \u03c0 \u03c9\n0 A1A2sin\n2(\u03c9x)dx =\n\u222b \u03c0 \u03c9\n0 A (1\u2212 cos(2\u03c9x)) 2 dx\n= \u03c0A\n2\u03c9"}, {"heading": "B Derivation of the Log-likelihood formula for HPs", "text": "This derivation follows the steps on [11]. Given a realization (t1, t2, ..., tk) of some regular point process observed over the interval [0,T], the log-likelihood is expressed as:\nl = k\u2211 i=1 log(\u03bb(ti))\u2212 \u222b T 0 \u03bb(u)du\nProof. Let be the joint probability density of the realization:\nL = f(t1, t2, ..., tk) = k\u220f i=1 f(ti)\nIt can be written in terms of the Conditional Intensity Function. We can then find f in terms of \u03bb:\n\u03bb(t) = f(t)\n1\u2212 F (t) =\ndF (t) dt 1\u2212 F (t) = \u2212d log(1\u2212 F (t)) dt ,\nwhere, given the history up to last arrival u, H(u), F(t) is then defined as the conditional cumulative probability distribution of the next arrival time Tk+1:\nF (t) = F (t|H(u)) = \u222b t u f(s|H(u))ds\nIntegrating both sides of Equation (19) over (tk, t): \u2212 \u222b t tk \u03bb(u)du = log(1\u2212 F (t))\u2212 log(1\u2212 F (tk))\nGiven that the realization is assumed to have come from a so-called simple process, i.e., a process in which multiple arrivals cannot occur at the same time, we have that F (tk) = 0 as Tk+1 > tk, which simplifies equation (19) to:\n\u2212 \u222b t tk \u03bb(u)du = log(1\u2212 F (t))\nFurther rearranging the expression:\nF (t) = exp ( \u2212 \u222b t tk \u03bb(u)du ) ,\nand\nf(t) = \u03bb(t)exp ( \u2212 \u222b t tk \u03bb(u)du ) Thus, the likelihood becomes:\nL = k\u220f i=1 f(ti) = k\u220f i=1 \u03bb(ti)exp ( \u2212 \u222b ti ti\u22121 \u03bb(u)du )\n= [ k\u220f i=1 \u03bb(ti) ] exp ( \u2212 \u222b tk 0 \u03bb(u)du ) Given that the process is observed on [0,T], the likelihood must include the probability of seeing no arrivals in (tk, T ]:\nL = [ k\u220f i=1 f(ti) ] (1\u2212 F (T ))\nThrough using the formulation of F(t), we have that:\nL = [ k\u220f i=1 \u03bb(ti) ] exp ( \u2212 \u222b T 0 \u03bb(u)du ) Finally, getting the logarithm of the expression, we have the formula for l:\nl = k\u2211 i=1 log(\u03bb(ti))\u2212 \u222b T 0 \u03bb(u)du\nC Automatic Report"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Hawkes Processes capture selfand mutual-excitation between events<lb>when the arrival of one event makes future ones more likely to happen in<lb>time-series data. Identification of the temporal covariance kernel can reveal<lb>the underlying structure to better predict future events. In this paper, we<lb>present a new framework to represent time-series events with a composition<lb>of self-triggering kernels of Hawkes Processes. Our automatic decomposition<lb>procedure is composed of three main steps: (1) discretized kernel estimation<lb>through frequency domain inversion equation associated with the covariance<lb>density when a direct parameter optimization is not possible, (2) greedy kernel<lb>decomposition through four base kernels and their combinations (addition<lb>and multiplication), and (3) automated report generation. In addition, we<lb>report the first multiplicative kernel compositions along with stationarity<lb>conditions for Hawkes Processes. We demonstrate that the new automatic<lb>kernel decomposition procedure performs better to predict future events than<lb>the existing framework in real-world data.", "creator": "LaTeX with hyperref package"}}}