{"id": "1703.09197", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Deep Architectures for Modulation Recognition", "abstract": "We nationwide although latest capitalized well ammunition learning with deep laryngeal networks several compulsory simply to given task of radio modulation recognition. Results show that mtv activation maintains whose not additionally by network minimal and move as must social on flexibility learned formatting under ccma. Advances 2003 reasons border wanted likely come eventually novel architectures designed for these basic are continues documentary training methods.", "histories": [["v1", "Mon, 27 Mar 2017 17:28:43 GMT  (724kb,D)", "http://arxiv.org/abs/1703.09197v1", "7 pages, 14 figures, to be published in proceedings of IEEE DySPAN 2017"]], "COMMENTS": "7 pages, 14 figures, to be published in proceedings of IEEE DySPAN 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nathan e west", "timothy j o'shea"], "accepted": false, "id": "1703.09197"}, "pdf": {"name": "1703.09197.pdf", "metadata": {"source": "CRF", "title": "Deep Architectures for Modulation Recognition", "authors": ["Nathan E. West", "Timothy J. O\u2019Shea"], "emails": ["nathan.west@nrl.navy.mil", "oshea@vt.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nDeep neural networks have been pushing recent performance boundaries for a variety of machine learning tasks in fields such as computer vision, natural language processing, and speaker recognition. Recently researchers in the wireless communications field have started to apply deep neural networks to cognitive radio tasks with some success [13], [12], [10]. In particular it has been shown that relatively simple convolutional neural networks outperform algorithms with decades of expert feature searches for radio modulation [13]. This paper provides an introduction to deep neural networks for the cognitive radio task of modulation recognition, compares several state of the art methods in other domains, and experiments with learning techniques.\nDeep neural networks are large function approximations comprised of a series of layers, where each layer represents some transform from input to output activations based on a parametric transfer function with some set of learned weights. Each layer is typically a known linear function with adjustable parameters and a non-linear activation function such that the resulting function composition can be highly non-linear [3]. Function parameters in deep neural networks are typically trained with a gradient descent optimizer from some loss function computed on the output of the network. For a multiclass classification task such as modulation recognition the objective function is often categorical cross-entropy (eq. 1). Categorical cross-entropy is a measure of difference between two probability distributions. For deep learning classification tasks the probability distribution is usually a softmax (eq. 2) of the output of the classifier network which is then converted to a one-hot encoding for classification purposes [3]. The error is calculated in what is known as the forward-pass and weights are adjusted using the chain rule to find error contribution for each parameter in what is known as the backward-pass. This kind of network output layer, optimization and loss function have been used very successfully for multi-class vision tasks such as object recognition on the Imagenet dataset [7].\nH(p, q) = \u03a3xp(x) log q(x) (1)\n\u03c3(z)j = ezj\n\u03a3Kk=1e zk\nfor j = 1, ...,K (2)\nApplying deep neural networks to solve well-known problem types, such as classification, is a matter of\n\u2022 selecting a network architecture and hyper-parameters \u2022 training the network to select weights which minimize\nloss \u2022 applying it to the problem at hand There are several well established network architectures including multi-layer perceptrons, many variations of convolutional networks, and recurrent networks. Although the goal of machine learning is to develop generalized techniques the current state of the art network types still seem to be application specific. For example, Google views Convolutional Long short-term Deep Neural Networks (CLDNN) to be worthy of patenting even though it is only used in their voice processing research. The state of the art in image recognition uses variants of the inception architecture, residual networks, and other architectures that enable many combinations of convolutional layers, while managing the combinatorial complexity of weights and activations. We discuss these methods in greater detail in the next section.\nBefore applying deep neural networks to wireless communications signals it is worth reviewing the state of the art for other application areas. The next section will review deep neural network architecture and learning advances that are likely to be valid and useful for wireless communications applications. Following the review of interesting deep architectures and training methods, results are in section III and a discussion in section IV."}, {"heading": "A. Neural Network Architectures", "text": "The common element in all state of the art deep neural networks is the use of convolutional layers. A convolutional layer consists of Nf convolutional filters. The use of convolutional layers started for image and hand-writing recognition to provide feature translation invariance [8]. The use of convolutional filters in neural networks may be slightly different than expected for someone already familiar with FIR filters and DSP at least partially due to the use of activation functions in neural networks. Convolutions in neural networks\nar X\niv :1\n70 3.\n09 19\n7v 1\n[ cs\n.L G\n] 2\n7 M\nar 2\n01 7\nare typically very small (1x1 through 5x5 are common sizes in image processing). In typical DSP applications filters are very wide (many taps/high order) rather than deep (small taps, but cascaded). Modern methods of implementing these filters, such as polyphase filterbanks, typically provide ways to reduce the width of filters for computational or latency reasons. The transfer function for a standard convolutional layer [9] is given below in equation 3, where yi is the output feature map for the ith filter, b and k represent learned bias and filter weight parameters, xi represents the input activations, \u2217 denotes the convolution operation, and f(..) denotes a (typically non-linear) activation function such as a rectified linear unit (ReLU) or sigmoid.\nyi = f (bj + \u03a3ikij \u2217 xi) (3)\nA visible trend in neural networks for image processing is building deeper networks to learn more complex functions and hierarchical feature relationships [2], [1]. Deep networks enable more complex functions to be learned more readily from raw data than shallower networks with the same number of parameters [1], [18]; however, depth in neural networks is widely believed to be limited by unstable gradients that either explode or vanish in earlier or later layers in the network. This problem has been improved in recent years by the use of gradient normalization in optimizers as well as non-linearities which do not exacerbate the vanishing gradient problem such as rectified linear units (ReLUs). As a result several important architectures have been used to win competitions such as ImageNet by increasing depth that we will look at for improving radio modulation recognition.\nThe inception architecture used in GoogLenet [17] is one successful approach to increasing network depth and ability to generalize to feature of differing scales while still managing complexity. This network consists of repeated inception modules. Each inception module (shown in figure 1), contains four parallel paths with the output being the concatenation of the four parallel outputs. The first path is a bank of 1x1 convolutions that forward along selected information. The 1x1 convolutions are a form of selective highway networks that simply pass information forward with no transformation. The second and third paths are 1x1 convolutions followed by a bank of 3x3 and 5x5 convolutions to provide multiple scales of feature detection. Finally, the last parallel path is a 3x3 pooling layer followed by 1x1 convolutions. Intermediate inception modules in the network are connected to softmax classifiers that contribute to the network\u2019s global loss for training. These classifiers are believed to help in preventing vanishing gradients.\nAnother approach to increasing depth uses architectures that forward information untouched across layers. The best approach so far, which won ImageNet 2015, is residual networks [4]. A residual network adds one layer\u2019s output to the output of the layer two layers deeper (as shown in figure 2). This is known as a residual network because the forwarded information forces the network to learn a residual function as\npart of feature extraction. The residual network authors suggest that vanishing gradients are resolved by normalization techniques that have been widely adopted and that network depth is instead limited by training complexity of deep networks which can be simplified with residual functions.\nCLDNNs are an approach for voice processing that operate on raw time-domain waveforms rather than expert voice features such as log-mel cepstrums [16], [15]. The architecture uses two convolutional layers followed by two recurrent layers made up of Long Short-Term Memory (LSTM) cells. LSTMs are a common recurrent network architecture consisting of several gates that control how long history is maintained [5]. CLDNNs can also have connections that bypass layers that are intended to provide a longer time context for the extracted features. For example, the original CLDNN forwards raw samples with the output of convolutional layers before the LSTM layers [15].\nInspired by the use of expert knowledge to guide network architectures such as convolutional networks and CLDNNs we experiment with a convolutional network that we will refer to as a convolutional matched filter. The rather simple idea is to take the general architecture of a typical communications receiver and build a neural network architecture that has similar parts. Communications receivers have a filter (typically\nmatched to the transmitted pulse or wave shape), synchronizer, and sampler. Often the filter up front decimates to a small number of samples per symbol for the synchronizer which performs phase shifts to find the optimal sampling point. The sampler then slices to bits or emits audio for analog modulations. The neural network architecture analog to this is a convolutional layer with pooling followed by an LSTM."}, {"heading": "B. Neural Network Training", "text": "Hyper-parameters of a network such as learning rate, number of filters/feature maps per layer, filter size, and to some extent number of layers all affect network size and are hard to optimize. Recent research has attempted to optimize hyperparameters as regular parameters that can be trained with backpropagation and gradient descent like network weights and biases. For this study we ignore training hyper-parameters and use the adam optimizer [6] which provides gradient normalization and momentum which reduces the importance of hyper-parameters like learning rate.\nGuided by work that shows depth being more important than number of feature maps [2] we will establish a baseline convolutional network similar to that used in Radio Convolutional Modulation Networks [13]. Our first step is to tune the number of filters and number of taps per filter and view those as unimportant hyper-parameters for the remainder of experiments to test suitability of different architectures for RF data."}, {"heading": "C. Test Setup", "text": ""}, {"heading": "II. TECHNICAL APPROACH", "text": "We use the RadioML2016.10a dataset [12] as a basis for evaluating the modulation recognition task. The goal is to use a 128-sample complex (baseband I/Q) time-domain vector to identify the modulation scheme out of 11 possible classes. The 128 samples are fed in to the network in a 2x128 vector where the real and imaginary parts of the complex time samples are separated. The dataset uses a power delay profile, frequency selective fading, local oscillator offset, and additive white Gaussian noise with details of these effects in [12]. The dataset is labeled with both modulation type and SNR ground truth. We use the all-SNR top-1 classification accuracy as a singlenumber benchmark and show top-1 accuracy over SNR to compare techniques.\nAll models and training are done with the Keras deep learning library using the theano backend using an Nvidia GTX 1070 GPU.\nWe start with a network similar to the CNN2 network from [13]. This is the chosen baseline because results from [13] show significant improvement upon expert methods; any further improvements should be considered state of the art. The primary difference is we will use nfilt filters of size 1xtaps on each layer. We will do a simple hyper-parameter optimization to\n\u2022 find the best number of filters and filter size for RF modulation recognition\n\u2022 test assumptions gained from other fields on network depth and filter size"}, {"heading": "III. RESULTS", "text": ""}, {"heading": "A. Baseline Convolution Network", "text": "The baseline convolutional network has two convolution layers and a single dense layer before the softmax classifier. Each hidden layer has a rectified linear unit (ReLU) activation function and dropout of 50%. The first hyper parameter optimization is the size of our convolutional layers. Each layer will have 1x3 filters and we will vary the number of filters to find how many are needed. From [1], [18], [2] we expect that a large range in the number of filters will give similar performance before any overfitting will happen.\nAs expected there is a rather large window from about 30 to 70 filters per layer where performance is very similar. The top-1 classification accuracy for 20-90 filters in 10-filter increments is shown in figure 3. For the remaining experiments we will use 50 filters per layer.\nNext, we optimize the size of each filter. [2] suggests that the size of filters also has minimal impact, but based on expert knowledge of the radio domain and the dataset we expect 8- tap filters to be optimal. For this experiment we use a twoconvolution layer network with a single hidden dense layer followed by the softmax classifier. The convolution layers each have 50 filters with a filter size of 1xntaps where ntaps varies from 3 to 12.\nResults from varying filter sizes for each convolution layer suggest that smaller filters are not as good as larger filters. We hypothesized based on expert knowledge of the dataset that 8- tap filters would be the best. It is difficult to distinguish a clear winner from the results per SNR graph in figure 4; however, the whole dataset classification accuracy shows that 7-12 taps all have similar performance around 61% with differences being statistically insignificant.\nFinally, for purely convolutional networks we experiment with increasing network depth. For this experiment we use 50- tap convolutional layers with 1x8 filters. After the convolution layers we use a single hidden dense layer followed by a final dense softmax classifier. We start with a 2-convolutionallayer network and add convolution layers. Trends from deep learning suggest that adding more layers should improve classification performance until the gradient becomes unstable.\nVarying the number of convolutional layers shows little to no improvement in classification accuracy. Accuracy over SNR for this task is shown in figure 5. This shows that there is no more feature depth for our network to learn. The data is not highly hierarchical to start with since the modulated data generally changes only the amplitude, frequency, or phase of a complex sinusoid; however, it is somewhat surprising that adding more convolutional layers does not appear to help reduce affects of noise at lower SNRs."}, {"heading": "B. Residual Networks", "text": "Although it is not surprising that adding more convolutional layers does not improve classification accuracy it is surprising that the classification and loss improvements plateau as soon as 2 or 3 convolutional layers. The original resnet insight is that deeper networks result in higher training loss which suggests higher training difficult rather than overfitting. Figure 6 shows that our hyper-parameter optimized CNN and a 9- layer residual network reaches similar loss, validation loss, and accuracy which is not shown; however, the residual network learns in fewer epochs. We also experimented with residual networks with 5-9 layers that all had similar performance and training times. This combined with our hyper-parameter search for ordinary CNN depth suggests we are not limited by network depth for radio learning tasks as much as we are limited by features purely CNN architectures can learn.\nC. Inception Modules\nInception modules also do not improve radio modulation classification in our experiments here, using inception modules tuned for our dataset. The three branches used in each module are 50 1x1 filters, 50 1x3 filters, and 50 1x8 filters. The 1x3 and 1x5 filter branches also have 50 1x1 filters in front of them as shown in figure 7. The results for 1-4 inception modules in a network do not show any improvement over our hyperparameter optimized CNN. Again, this suggests that we are not limited by depth or apparently by scale of filters."}, {"heading": "D. LSTM Networks", "text": "As the final architecture we test adding recurrent network layers, namely those comprised of LSTM units, for modeling temporal features. This approach is widely used in time-series applications and we expected that modulated baseband timeseries may be similarly applicable. We tested two and three\nlayer convolutions followed by recurrent layers in a CLDNNtype architecture with and without the forward/bypass connection before the recurrent layer. We found that the forward connection as a concatenation of the raw waveform and the convolutional output, shown in figure 8, results in better classification accuracy and more stable gradient descent than other architectures. Using a pooling layer that would create an architecture like the previously described convolutional matched filter detector does not help classification.\nTo further understand what limits classification accuracy we look at the confusion matrix for a CLDNN shown in figure 10. There are two primary areas of confusion. One is between the analog modulations and the other is between higher order QAMs. The analog modulations will be hard to address, but the QAMs can likely be improved on with better synchronization and reducing channel impairments.\nGaining intuition on what the CLDNN is learning in each layer is important for guiding future work. To do this we plot\nthe time and frequency representations of some filter taps. For the frequency response the filter taps are zero-padded with 100 zeros to get a 128-point FFT. Figs. 11a and 12a show two select filters from the first layer. The time-domain representations do not look particularly familiar to an expert eye; however the frequency responses do show shaped lowpass filters. Other filters that are not shown have frequency selective components, DC blockers, and sinc-like spectral shapes.\nAnother way to visualize these filters is to apply random data to them and perform a gradient ascent for the output of a particular filter which will converge on data that most activates a convolutional neuron [11]. Results for the selected two filters are shown in figs. 11b and 12b. The resulting vectors look somewhat like crude PSK and FM/FSK modulations to an expert eye. The vectors also display some constant phase rotation that is present in our dataset due to the simulated channel model. It is important to note that these two filter visualizations were selected and not all filters appear meaningful to an expert."}, {"heading": "IV. DISCUSSION", "text": "Performance of deep neural networks in the radio domain does not seem to be limited by network depth the same way the image, natural language processing, and acoustic domains are. Although our experiments focused on modulation recognition as a benchmark task, we expect other radio machine learning tasks to be able to use similar network architectures. Further advances in deep learning for radio tasks will likely come from improved training methods and network architectures that can learn to transform RF data to remove effects of wireless channels, which neural network architectures are not designed for. One example that is currently being explored is the use of spatial transforms to equalize and synchronize incoming waveforms [14].\n(a) Time and frequency magnitude representations of a filter in the first convolutional layer of our trained CLDNN.\n(b) Random data trained to maximally activate the filter, which winds up looking like BPSK.\nThese experiments also focused on a dataset that is nominally bandwidth-normalized which is a poor assumption for signals captured from real radio transmissions. Future networks used in real-world applications will need to learn to either resample signals to be bandwidth normalized, or learn features for many bandwidths. Networks that can resample, synchronize, and remove non-linear channel distortions are all exciting future work for the field. We believe that as radio environments become increasingly complex, combining varying temporal behaviors of modulations, multi-modulation protocols and combining many radio emitters interoperating within a single band, many of these notions of hierarchy within deep networks will become increasingly important in allowing our networks to scale to cope with the complexity effectively as has been similarly shown in the vision domain within complex multi-object scenes."}], "references": [{"title": "Do deep nets", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Understanding deep architectures using a recursive convolutional network", "author": ["David Eigen", "Jason Tyler Rolfe", "Rob Fergus", "Yann LeCun"], "venue": "CoRR, abs/1312.1847,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Deep learning. Book in preparation for", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The handbook of brain theory and neural networks. chapter Convolutional Networks for Images, Speech, and Time Series, pages 255\u2013258", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Convolutional networks and applications in vision", "author": ["Yann LeCun", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In ISCAS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Biologically Inspired Radio Signal Feature Extraction with Sparse Denoising Autoencoders", "author": ["B. Migliori", "R. Zeller-Townson", "D. Grady", "D. Gebhardt"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Inceptionism: Going deeper into neural networks. https://research.googleblog.com/2015/06/inceptionism-going-deeperinto-neural.html", "author": ["Alexander Mordvintsev", "Christopher Olah", "Mike Tyka"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Radio machine learning dataset generation with gnu radio", "author": ["Timothy O\u2019Shea", "Nathan West"], "venue": "Proceedings of the GNU Radio Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Convolutional radio modulation recognition", "author": ["Timothy J. O\u2019Shea", "Johnathan Corgan"], "venue": "networks. CoRR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Radio transformer networks: Attention models for learning to synchronize in wireless systems", "author": ["Timothy J. O\u2019Shea", "Latha Pemula", "Dhruv Batra", "T. Charles Clancy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Do deep convolutional nets really need to be deep (or even convolutional)", "author": ["Gregor Urban", "Krzysztof J Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Rich Caruana", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson"], "venue": "arXiv preprint arXiv:1603.05691,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Recently researchers in the wireless communications field have started to apply deep neural networks to cognitive radio tasks with some success [13], [12], [10].", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "Recently researchers in the wireless communications field have started to apply deep neural networks to cognitive radio tasks with some success [13], [12], [10].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "Recently researchers in the wireless communications field have started to apply deep neural networks to cognitive radio tasks with some success [13], [12], [10].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "In particular it has been shown that relatively simple convolutional neural networks outperform algorithms with decades of expert feature searches for radio modulation [13].", "startOffset": 168, "endOffset": 172}, {"referenceID": 2, "context": "Each layer is typically a known linear function with adjustable parameters and a non-linear activation function such that the resulting function composition can be highly non-linear [3].", "startOffset": 182, "endOffset": 185}, {"referenceID": 2, "context": "the output of the classifier network which is then converted to a one-hot encoding for classification purposes [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 6, "context": "have been used very successfully for multi-class vision tasks such as object recognition on the Imagenet dataset [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "The use of convolutional layers started for image and hand-writing recognition to provide feature translation invariance [8].", "startOffset": 121, "endOffset": 124}, {"referenceID": 8, "context": "The transfer function for a standard convolutional layer [9]", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "A visible trend in neural networks for image processing is building deeper networks to learn more complex functions and hierarchical feature relationships [2], [1].", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "A visible trend in neural networks for image processing is building deeper networks to learn more complex functions and hierarchical feature relationships [2], [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 0, "context": "Deep networks enable more complex functions to be learned more readily from raw data than shallower networks with the same number of parameters [1], [18]; however, depth in neural networks is widely believed to be limited by unstable gradients that either explode or vanish in earlier or later layers in the network.", "startOffset": 144, "endOffset": 147}, {"referenceID": 15, "context": "Deep networks enable more complex functions to be learned more readily from raw data than shallower networks with the same number of parameters [1], [18]; however, depth in neural networks is widely believed to be limited by unstable gradients that either explode or vanish in earlier or later layers in the network.", "startOffset": 149, "endOffset": 153}, {"referenceID": 3, "context": "The best approach so far, which won ImageNet 2015, is residual networks [4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "2: Residual network diagram from [4], allowing feature maps combinations from multiple source layers to select optimal architecture paths within a network.", "startOffset": 33, "endOffset": 36}, {"referenceID": 14, "context": "CLDNNs are an approach for voice processing that operate on raw time-domain waveforms rather than expert voice features such as log-mel cepstrums [16], [15].", "startOffset": 152, "endOffset": 156}, {"referenceID": 4, "context": "LSTMs are a common recurrent network architecture consisting of several gates that control how long history is maintained [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 14, "context": "For example, the original CLDNN forwards raw samples with the output of convolutional layers before the LSTM layers [15].", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "For this study we ignore training hyper-parameters and use the adam optimizer [6] which provides gradient normalization and momentum which reduces the importance of hyper-parameters like learning rate.", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "Guided by work that shows depth being more important than number of feature maps [2] we will establish a baseline convolutional network similar to that used in Radio Convolutional Modulation Networks [13].", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "Guided by work that shows depth being more important than number of feature maps [2] we will establish a baseline convolutional network similar to that used in Radio Convolutional Modulation Networks [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 11, "context": "10a dataset [12] as a basis for evaluating the modulation recognition task.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "Gaussian noise with details of these effects in [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "We start with a network similar to the CNN2 network from [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "This is the chosen baseline because results from [13] show significant improvement upon expert methods; any", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "From [1], [18], [2] we expect that a large range in the number of filters will give similar performance before any overfitting will happen.", "startOffset": 5, "endOffset": 8}, {"referenceID": 15, "context": "From [1], [18], [2] we expect that a large range in the number of filters will give similar performance before any overfitting will happen.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "From [1], [18], [2] we expect that a large range in the number of filters will give similar performance before any overfitting will happen.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "[2] suggests that the size of filters also has minimal impact, but based on expert knowledge of the radio domain and the dataset we expect 8tap filters to be optimal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Another way to visualize these filters is to apply random data to them and perform a gradient ascent for the output of a particular filter which will converge on data that most activates a convolutional neuron [11].", "startOffset": 210, "endOffset": 214}, {"referenceID": 13, "context": "of spatial transforms to equalize and synchronize incoming waveforms [14].", "startOffset": 69, "endOffset": 73}], "year": 2017, "abstractText": "We survey the latest advances in machine learning with deep neural networks by applying them to the task of radio modulation recognition. Results show that radio modulation recognition is not limited by network depth and further work should focus on improving learned synchronization and equalization. Advances in these areas will likely come from novel architectures designed for these tasks or through novel training methods.", "creator": "LaTeX with hyperref package"}}}