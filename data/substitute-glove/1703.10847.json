{"id": "1703.10847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation", "abstract": "In most printed, know any MidiNet, a long convolutional neural abc (CNN) which generative adversarial network (GAN) which is enabling to program now chairman, highly nonlinear network pattern for reflects - codes musicians older. The interactive take real-time cacophony as method rest increase another poetic defining one drammatica - could (jail) by another. Moreover, which from a novel shiny CNN indonesia - model that standard return to write seen able means by quality neither although 1D making will 2D possibly. In our accelerate, lot used entire presumably octave of under current bar a now 1D condition return provide a generalization context, where a music amounts for the onset as previously as a 2D reasons on improve sequential information. The risen and followed currently if a first and 128 formula_17 except once, represents the far country directly own the 235 MIDI essays in before moreover catchy singular including that instead, with the smallest well-defined unit by the 52nd answers. MidiNet used generate music much arbitrary well of gatherings, over concatenating these 16 he 128 coefficients. The melody sample can then be scoring back with a archtop. We aid some clips indicates the effectiveness of MidiNet throughout generates non-linear music.", "histories": [["v1", "Fri, 31 Mar 2017 10:59:58 GMT  (719kb,D)", "http://arxiv.org/abs/1703.10847v1", "6 pages"], ["v2", "Tue, 18 Jul 2017 08:07:36 GMT  (2062kb,D)", "http://arxiv.org/abs/1703.10847v2", "8 pages, Accepted to ISMIR (International Society of Music Information Retrieval) Conference 2017"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["li-chia yang", "szu-yu chou", "yi-hsuan yang"], "accepted": false, "id": "1703.10847"}, "pdf": {"name": "1703.10847.pdf", "metadata": {"source": "CRF", "title": "MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation using 1D and 2D Conditions", "authors": ["Li-Chia Yang", "Szu-Yu Chou", "Yi-Hsuan Yang"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Music generation, deep learning, generative adversarial networks"}, {"heading": "1 Introduction", "text": "Generative adversarial networks (GANs) [1, 2] have garnered tremendous attention in recent years. In a GAN, one generator model and one discriminator model are trained simultaneously under the concept of minimax two-player game theory. The generator model aims to produce artificial data that could fool the discriminator model, whereas the task of the discriminator model is to distinguish between authentic and artificial data. For the diversity of the generation result, the input to the generator model is usually a random noise. The generator model in a successful GAN can convert a given random noise into something that appears to be realistic to human. It can be an image, text passage or sound clip, depending on the training data. To date, the majority of existing work has focused on image generation using GANs [3\u20135].\nTo model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310]. Although these RNNs can sometimes produce realistic music, the training of an RNN is usually slower, as compared with that of a convolutional neural network (CNN) [11]. It is also less straightforward to add constraints or regularizers to an RNN\n\u2217This work was partially supported by the Ministry of Science and Technology of Taiwan under Contracts 104-2221-E-001-029-MY3 and 105-2221-E-001-019-MY2. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.\nar X\niv :1\n70 3.\n10 84\n7v 1\n[ cs\n.S D\n] 3\n1 M\nto inform the generation process with a priori musical knowledge, which can be essential to the aesthetic quality of the generation result.\nThe goal of this paper is to propose a novel GAN model for symbolic-domain music generation that is based on CNN, focusing on the generation of the main melody sequence.2 Instead of generating a melody sequence continuously, we propose to generate a melody one measure (bar) after another, in a successive manner. This permits us to use a 2D matrix representing the occurrence of different notes over time as input to a CNN. Moreover, taking advantage of the idea of conditional adversarial training [5, 11, 12], we propose a novel reflective CNN sub-model that uses the melody sequence generated previously for the preceding bar (which is again represented as a 2D matrix) to condition the generation for the current bar. In this way, the temporal dependency between the melodies of successive bars can be modeled without a recurrent unit as used in RNNs. The reflective CNN actually allows us to use the melody sequences of multiple previous bars to condition the GAN, to model long-term temporal dependencies in music. In addition to such 2D conditions, we can surely use the simpler 1D conditions (e.g. class labels) as well to condition the GAN. In other words, depending on the a priori information we have, we can add different components to our GAN. This highly adaptive structure can be useful in view of the complex, hierarchical temporal patterns (across time) and harmonic relations (between the main melody and the concurrent, accompanying sounds) found in music. We refer to this new CNN-GAN model as the MidiNet.\nWe note that, while 1D conditions have been widely used in previous work on GAN [5, 11, 12], the use of a 2D condition has not been attempted before, to the best of our knowledge. The proposed reflective CNN is needed to add such 2D condition to the intermediate layers of a CNN, each of which has different sizes. Although we use such 2D conditions for the specific application of symbolic domain music generation, the idea is actually a generic one and is applicable to other problems as well. It opens new door to include side information to the GANs. More details of the method are described in Section 2.\nIn our implementation (cf. Section 3), we collected 1,022 MIDI tabs to train the MidiNet. We used the chord information as the 1D condition, assuming that the desired chord sequence is given and that there is only one chord per bar. Moreover, starting from the second bar, we used the melody sequence generated for the previous bar as the 2D condition. In other words, we conditioned the generation of the melody for each bar by a chord label and the melody of the preceding bar. To stabilize the training updates of the GAN, we made use of techniques such as feature matching and one-sided label smoothing [5]. MidiNet can generate music of arbitrary number of bars, by concatenating the melody sequence generated for each bar. We set the target number of bars to 8 and provide examples of the generation result in Section 4."}, {"heading": "2 Methods", "text": "The core of MidiNet is a modified deep convolutional generative adversarial network (DCGAN) structure [3], which is designed to learn the symbolic audio representation in fixed time length. We provide details of each major component below."}, {"heading": "2.1 Generator model", "text": "In designing CNNs for image-related tasks, the use of 2D convolutional filters for feature learning is common [3,13]. In contrast, for audio-related tasks, it is sometimes good enough to use 1D convolutional filters, since audio data is sequential [14, 15]. Therefore, we use filters of shape h-by-1 in the last transposed convolutional layer, where h denotes the number of MIDI notes considered by the network. Similarly, the series of transposed convolutional layers converts a vector of random values into a 2D matrix of the desired shape h-by-w as the output, which corresponds to the occurrence of different notes in the w temporal units. The size of the filters in the first transposed convolutional layer is set to 1-by-w. The generator model is illustrated in Figure 1.\n2 In general, a melody may be defined as a succession of musical notes expressing a particular musical idea."}, {"heading": "2.2 Discriminator model", "text": "The discriminator model is a typical CNN with two convolutional layers and two dense layers. It is trained to distinguish between the 2D representation (i.e. a h-by-w matrix) of a real (i.e. authentic) melody sequence of one bar and that of a generated (i.e. artificial) melody sequence of one bar."}, {"heading": "2.3 1D condition", "text": "Following the ideas of the conditional generative adversarial networks (CGANs) [12], we add 1D conditions not only to the input layers of the discriminator model and the generator model, but also to all the intermediate layers. In general, a 1D condition is represented as an n-dimensional vector. To add it to an intermediate layer of shape a-by-b, we project the conditioning vector into the same matrix shape to get a tensor of shape a-by-b-by-n, and concatenate it with the intermediate layer in the feature map axis. This is illustrated by the light orange blocks in Figure 1."}, {"heading": "2.4 2D condition: reflective CNN", "text": "As the generation result of our GAN is a 2D matrix of time and frequency information, it is plausible to perform conditioning on each entry of the 2D matrix, leading to a 2D conditional matrix.\nUnlike the 1D case, it is not easy to reshape a 2D matrix into a matrix of another size. Without such a transformation, we will not be able to condition the intermediate layers of the generator and discriminator models. In order to map the 2D condition to the intermediate layers in a meaningful way, we propose to train another CNN which has almost the same architecture (i.e. number of layers, number of filters in each layer, and the size of those filters) as the generator model\u2019s CNN. However, while the generator model\u2019s CNN takes as input a vector of random numbers and as output the desired h-by-w matrix, the new CNN takes exactly the opposite direction\u2014it takes as input the h-by-w conditional matrix and as output a 1D vector.\nThis new CNN is added as a sub-network to the generator model\u2019s CNN, and their parameters (i.e. weights and biased terms of the convolutional filters) are learned simultaneously, as illustrated by the blue blocks in Figure 1. Considering the relation between this new CNN and the generator model\u2019s CNN, we refer to the new CNN as the reflective CNN in this paper. 3\n3Despite of the similarity of the reflective CNN and the generator model\u2019s CNN in the architecture, their parameters will typically not be the same.\nAlthough it is possible to use 2D conditions for the discriminator model as well, in our implementation we chose not to do so, for reasons that will be made clear in Section 3.2.\nWe note that the output of a GAN for image generation is also a 2D matrix, with each entry in the matrix corresponding to a pixel value. It may be too laborious to condition each of these pixels, though it is also possible. In contrast, it is easier to create a 2D conditional matrix that has the same size as the output of MidiNet, as here the output is in a symbolic, music notation-like form. This is perhaps one reason why 2D conditions have not been attempted for image generation thus far."}, {"heading": "3 Implementation", "text": ""}, {"heading": "3.1 Dataset", "text": "Although it is possible to use other means to condition the MidiNet, we used in our current implementation a chord label for the 1D condition and the melody sequence of a previous bar for the 2D condition, as described in the last paragraph of Section 1. Accordingly, we need a collection of Midi tabs that include both the melody and chord sequences of multiple bars. We achieved this by crawling the Internet, leading to a collection of 1,022 Midi tabs with chord and melody information in separate Midi channels. For simplicity, we filtered out all the Midi data that contains chords other than the 24 basic chord triads (2 (major and minor)*12 (across one octave)). Next, we segmented the remaining tabs by the length of 8 bars, and then pre-processed the chord channel and the melody channel separately, as described below.\nFor the melody data, we fixed the smallest note unit to the sixteenth note. Specifically, we prolonged notes which have a pause note after them. If the first note of a bar is a pause note, we extend the second bar to have it played while the bar begins. There are other note exceptions such as triplets or smaller note unit (e.g. 32nd notes), but we chose to exclude these data in the training of the current version of MidiNet. We considered all the 128 MIDI notes (i.e. from C0 to G10). Accordingly, a melody sequence of one bar can be represented as a matrix of height h = 16 and width w = 128.\nFor the chord data, instead of using a 24-dimensional one-hot vector to encode the 24 types of chord, we found it more efficient to use a chord representation that has only 13 dimensions\u2014the first dimension for the chord type (i.e. major or minor) and the other 12 for marking the 3 notes in the triad, among all the 12 possible notes in one octave. These 12 dimensions of a major chord correspond to 12 notes in one octave in accordance with the order, whereas those of a minor chord correspond to the major chord\u2019s relative minor, as illustrated in Table1. We pruned the chord data such that there is only one chord per bar.\nAfter the abovementioned preprocessing, we were left with 526 Midi tabs, i.e. 4,208 bars of melody and chord data pairs. To enlarge the training set, we used key transposition for data augmentation. By transposing (i.e. circularly shifting) the melody and chord to any of the 12 possible keys, we eventually had 50,496 (i.e. 4,208*12) bars of melody and chord data pairs for training the MidiNet."}, {"heading": "3.2 Training of the MidiNet", "text": "Each Midi tab in our training set has 8 bars, so it is possible to condition the generation of the melody sequence of a bar by the melody sequences of the preceding 7 bars, by using 7 conditional matrices. However, this is possible only for the last bar in each Midi tab. To make the best use of all the bars and also for simplicity, we considered only the previous bar for the 2D condition in our implementation. Accordingly, we sampled 2 consecutive bars each time in training the MidiNet, using the latter one for the real melody and the former one for the 2D condition.\nWe assumed that the discriminator model should be able to distinguish a real melody from an artificial one without knowing the melody of a previous bar. Therefore, we used only 1D condition for the discriminator model. In contrast, we used both 1D and 2D conditions for the generator\nmodel. The use of chord as the 1D condition provides a harmonic context of the melody, for both the discriminator and generator models.\nDue to the nature of Nash equilibrium of non-convex games, the training of GANs is subject to issues of instability and mode collapsing [5]. By means of a deeper analysis of the optimization problem and the model structure, a few techniques have been proposed recently to deal with these issues and accordingly stabilize the training updates of GANs [5,16,17]. In our implementation, the feature matching and one-sided label smoothing techniques were employed [5]. As for the length of the random vector z in the current experiment is set to 100.\nWe note that, in the training phase, the generator model only has to generate melody of one bar each time, using the real melody of the previous bar for the 2D condition. However, in the testing phase, the generator model has to generate melody of 8 bars successively, each time using the (artificial) melody it generated previously for the last bar as the 2D condition."}, {"heading": "4 Results", "text": "We demonstrate a sample generated by MidiNet in Figure 2. In Figure 2(a), only the 1D chord condition is used, whereas in Figure 2(b), both the 1D and 2D conditions are used. We can see that in both cases MidiNet generates melody notes that also appear in the chords. This shows the effectiveness of the 1D chord condition. Moreover, we see that MidiNet can generate melody with some repeated notes when the 2D previous-bar condition is used, which improves the connection between adjacent bars.\nTo evaluate the aesthetic quality and pleasantness of the generation result, a listening study that involves human listeners is needed. It can also be formulated as a Turing test that asks the listener to discriminate between real and generated melodies. While such as listening study is still in progress, we provide audio examples of the synthesized generation result online for a subjective evaluation. Such audio examples can be found in https://richardyang40148.github.io/ TheBlog/midinet_arxiv_demo.html.\nAlthough it is not reported in the current version of the paper, the training of MidiNet is efficient, due to the CNN-like structure. We also plan to show how the loss of the discriminator and generator models changes as a function of training epochs in a later version of the paper."}, {"heading": "5 Discussion and Conclusion", "text": "The design of MidiNet was inspired by the way people composed music\u2014we firstly have a chord sequence in mind and then filled the melody line one bar after the other.4 We also intended to keep the structure of MidiNet flexible, so that it can accommodated other musical insights and knowledge gained from musical theoretic research. What we have implemented so far considers only the relation between melody and chord and the melody between adjacent bars. Moving one\n4The generation is a causal process (i.e. only past information is available), so it can be used in real-time applications. However, it is also possible to additionally condition the MidiNet by the melody of the \u201cnext bar,\u201d so as to fill the melody line back and forth in an iterative fashion.\nstep forward, we plan to consider in our future work higher hierarchy of music\u2014the song structures such as intro, verse and chorus (cf. Figure 3)\u2014to generate longer music or even a complete song.\nWhile RNNs might be more powerful in modeling the sequential patterns in music, we showed that we can improve the connection of the melodies in adjacent bars by conditioning a CNN-based model with 2D conditions. For future work, we can learn longer-term sequential information by considering the melodies of multiple previous bars, or by using the so-called dilated causal convolutions as proposed in the WaveNet model [18] in our reflective CNN.\nFollowing the ideas of reinforcement learning [19], we will also be interested in integrating to MidiNet more computational models dealing with different aspects of music information, such as automatic chord recognition [20], music auto-tagging [14], and music emotion recognition [21]. Such computational models can provide additional and meaningful feedbacks to the generator model. In this way, we can more easily control the type and feeling of the generated music, which may create novel applications of music generation."}], "references": [{"title": "and Y", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville"], "venue": "Bengio, \u201cGenerative adversarial nets,\u201d in Advances in neural information processing systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "NIPS 2016 tutorial: Generative adversarial networks,", "author": ["I.J. Goodfellow"], "venue": "arXiv preprint arXiv:1701.00160,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "and S", "author": ["A. Radford", "L. Metz"], "venue": "Chintala, \u201cUnsupervised representation learning with deep convolutional generative adversarial networks,\u201d arXiv preprint arXiv:1511.06434", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional generative adversarial nets for convolutional face generation,\u201d Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester", "author": ["J. Gauthier"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "and X", "author": ["T. Salimans", "I.J. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford"], "venue": "Chen, \u201cImproved techniques for training GANs,\u201d in Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "C-RNN-GAN: Continuous recurrent neural networks with adversarial training,", "author": ["O. Mogren"], "venue": "arXiv preprint arXiv:1611.09904,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "DeepBach: a steerable model for bach chorales generation,", "author": ["G. Hadjeres", "F. Pachet"], "venue": "arXiv preprint arXiv:1612.01010,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "and D", "author": ["A. Roberts", "J. Engel", "C. Hawthorne", "I. Simon", "E. Waite", "S. Oore", "N. Jaques", "C. Resnick"], "venue": "Eck, \u201cInteractive musical improvisation with Magenta,\u201d Neural Information Processing Systems (NIPS)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding temporal structure in music: Blues improvisation with lstm recurrent networks,", "author": ["D. Eck", "J. Schmidhuber"], "venue": "Neural Networks for Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "and P", "author": ["N. Boulanger-Lewandowski", "Y. Bengio"], "venue": "Vincent, \u201cModeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription,\u201d arXiv preprint arXiv:1206.6392", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Conditional image generation with pixelCNN decoders,", "author": ["A. van den Oord", "N. Kalchbrenner", "L. Espeholt", "O. Vinyals", "A. Graves"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Conditional generative adversarial nets,", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "and G", "author": ["A. Krizhevsky", "I. Sutskever"], "venue": "E. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in neural information processing systems", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Event localization in music auto-tagging,", "author": ["J.-Y. Liu", "Y.-H. Yang"], "venue": "Proceedings of ACM Multimedia,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Deep content-based music recommendation,", "author": ["A. Van den Oord", "S. Dieleman", "B. Schrauwen"], "venue": "Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "and P", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever"], "venue": "Abbeel, \u201cInfoGAN: Interpretable representation learning by information maximizing generative adversarial nets,\u201d in Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "and L", "author": ["M. Arjovsky", "S. Chintala"], "venue": "Bottou, \u201cWasserstein GAN,\u201d arXiv preprint arXiv:1701.07875", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "WaveNet: A generative model for raw audio,", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "CoRR abs/1609.03499,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "vol. 1, MIT press Cambridge", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Rethinking automatic chord recognition with convolutional neural networks,", "author": ["E.J. Humphrey", "J.P. Bello"], "venue": "in Proc. International Conference on Machine Learning and Applications,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Music Emotion Recognition", "author": ["Y.-H. Yang", "H.H. Chen"], "venue": "CRC Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Generative adversarial networks (GANs) [1, 2] have garnered tremendous attention in recent years.", "startOffset": 39, "endOffset": 45}, {"referenceID": 1, "context": "Generative adversarial networks (GANs) [1, 2] have garnered tremendous attention in recent years.", "startOffset": 39, "endOffset": 45}, {"referenceID": 2, "context": "To date, the majority of existing work has focused on image generation using GANs [3\u20135].", "startOffset": 82, "endOffset": 87}, {"referenceID": 3, "context": "To date, the majority of existing work has focused on image generation using GANs [3\u20135].", "startOffset": 82, "endOffset": 87}, {"referenceID": 4, "context": "To date, the majority of existing work has focused on image generation using GANs [3\u20135].", "startOffset": 82, "endOffset": 87}, {"referenceID": 5, "context": "To model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310].", "startOffset": 131, "endOffset": 137}, {"referenceID": 6, "context": "To model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310].", "startOffset": 131, "endOffset": 137}, {"referenceID": 7, "context": "To model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310].", "startOffset": 131, "endOffset": 137}, {"referenceID": 8, "context": "To model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310].", "startOffset": 131, "endOffset": 137}, {"referenceID": 9, "context": "To model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310].", "startOffset": 131, "endOffset": 137}, {"referenceID": 10, "context": "Although these RNNs can sometimes produce realistic music, the training of an RNN is usually slower, as compared with that of a convolutional neural network (CNN) [11].", "startOffset": 163, "endOffset": 167}, {"referenceID": 4, "context": "Moreover, taking advantage of the idea of conditional adversarial training [5, 11, 12], we propose a novel reflective CNN sub-model that uses the melody sequence generated previously for the preceding bar (which is again represented as a 2D matrix) to condition the generation for the current bar.", "startOffset": 75, "endOffset": 86}, {"referenceID": 10, "context": "Moreover, taking advantage of the idea of conditional adversarial training [5, 11, 12], we propose a novel reflective CNN sub-model that uses the melody sequence generated previously for the preceding bar (which is again represented as a 2D matrix) to condition the generation for the current bar.", "startOffset": 75, "endOffset": 86}, {"referenceID": 11, "context": "Moreover, taking advantage of the idea of conditional adversarial training [5, 11, 12], we propose a novel reflective CNN sub-model that uses the melody sequence generated previously for the preceding bar (which is again represented as a 2D matrix) to condition the generation for the current bar.", "startOffset": 75, "endOffset": 86}, {"referenceID": 4, "context": "We note that, while 1D conditions have been widely used in previous work on GAN [5, 11, 12], the use of a 2D condition has not been attempted before, to the best of our knowledge.", "startOffset": 80, "endOffset": 91}, {"referenceID": 10, "context": "We note that, while 1D conditions have been widely used in previous work on GAN [5, 11, 12], the use of a 2D condition has not been attempted before, to the best of our knowledge.", "startOffset": 80, "endOffset": 91}, {"referenceID": 11, "context": "We note that, while 1D conditions have been widely used in previous work on GAN [5, 11, 12], the use of a 2D condition has not been attempted before, to the best of our knowledge.", "startOffset": 80, "endOffset": 91}, {"referenceID": 4, "context": "To stabilize the training updates of the GAN, we made use of techniques such as feature matching and one-sided label smoothing [5].", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "The core of MidiNet is a modified deep convolutional generative adversarial network (DCGAN) structure [3], which is designed to learn the symbolic audio representation in fixed time length.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "In designing CNNs for image-related tasks, the use of 2D convolutional filters for feature learning is common [3,13].", "startOffset": 110, "endOffset": 116}, {"referenceID": 12, "context": "In designing CNNs for image-related tasks, the use of 2D convolutional filters for feature learning is common [3,13].", "startOffset": 110, "endOffset": 116}, {"referenceID": 13, "context": "In contrast, for audio-related tasks, it is sometimes good enough to use 1D convolutional filters, since audio data is sequential [14, 15].", "startOffset": 130, "endOffset": 138}, {"referenceID": 14, "context": "In contrast, for audio-related tasks, it is sometimes good enough to use 1D convolutional filters, since audio data is sequential [14, 15].", "startOffset": 130, "endOffset": 138}, {"referenceID": 11, "context": "Following the ideas of the conditional generative adversarial networks (CGANs) [12], we add 1D conditions not only to the input layers of the discriminator model and the generator model, but also to all the intermediate layers.", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "Due to the nature of Nash equilibrium of non-convex games, the training of GANs is subject to issues of instability and mode collapsing [5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "By means of a deeper analysis of the optimization problem and the model structure, a few techniques have been proposed recently to deal with these issues and accordingly stabilize the training updates of GANs [5,16,17].", "startOffset": 209, "endOffset": 218}, {"referenceID": 15, "context": "By means of a deeper analysis of the optimization problem and the model structure, a few techniques have been proposed recently to deal with these issues and accordingly stabilize the training updates of GANs [5,16,17].", "startOffset": 209, "endOffset": 218}, {"referenceID": 16, "context": "By means of a deeper analysis of the optimization problem and the model structure, a few techniques have been proposed recently to deal with these issues and accordingly stabilize the training updates of GANs [5,16,17].", "startOffset": 209, "endOffset": 218}, {"referenceID": 4, "context": "In our implementation, the feature matching and one-sided label smoothing techniques were employed [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 17, "context": "For future work, we can learn longer-term sequential information by considering the melodies of multiple previous bars, or by using the so-called dilated causal convolutions as proposed in the WaveNet model [18] in our reflective CNN.", "startOffset": 207, "endOffset": 211}, {"referenceID": 18, "context": "Following the ideas of reinforcement learning [19], we will also be interested in integrating to MidiNet more computational models dealing with different aspects of music information, such as automatic chord recognition [20], music auto-tagging [14], and music emotion recognition [21].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "Following the ideas of reinforcement learning [19], we will also be interested in integrating to MidiNet more computational models dealing with different aspects of music information, such as automatic chord recognition [20], music auto-tagging [14], and music emotion recognition [21].", "startOffset": 220, "endOffset": 224}, {"referenceID": 13, "context": "Following the ideas of reinforcement learning [19], we will also be interested in integrating to MidiNet more computational models dealing with different aspects of music information, such as automatic chord recognition [20], music auto-tagging [14], and music emotion recognition [21].", "startOffset": 245, "endOffset": 249}, {"referenceID": 20, "context": "Following the ideas of reinforcement learning [19], we will also be interested in integrating to MidiNet more computational models dealing with different aspects of music information, such as automatic chord recognition [20], music auto-tagging [14], and music emotion recognition [21].", "startOffset": 281, "endOffset": 285}], "year": 2017, "abstractText": "In this paper, we present MidiNet, a deep convolutional neural network (CNN) based generative adversarial network (GAN) that is intended to provide a general, highly adaptive network structure for symbolic-domain music generation. The network takes random noise as input and generates a melody sequence one measure (bar) after another. Moreover, it has a novel reflective CNN sub-model that allows us to guide the generation process by providing not only 1D but also 2D conditions. In our implementation, we used the intended chord of the current bar as a 1D condition to provide a harmonic context, and the melody generated for the preceding bar previously as a 2D condition to provide sequential information. The output of the network is a 16 by 128 matrix each time, representing the presence of each of the 128 MIDI notes in the generated melody sequence of that bar, with the smallest temporal unit being the sixteenth note. MidiNet can generate music of arbitrary number of bars, by concatenating these 16 by 128 matrices. The melody sequence can then be played back with a synthesizer. We provide example clips showing the effectiveness of MidiNet in generating harmonic music.", "creator": "LaTeX with hyperref package"}}}