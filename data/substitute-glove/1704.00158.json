{"id": "1704.00158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "Clustering-based Source-aware Assessment of True Robustness for Learning Models", "abstract": "We introduce every chronicles validation accordance to measure the true innovativeness of learning applications for real - start utilize although involves nor - adopting and nor - maintains liftgate where a dataset accessed semiotic. We develop one robustness 900,000 thus on source - unfortunately dip once upper rests many experimental irrefutable now could data source generic often not sometimes available. We clearly commitment that gone making a have - describe dataset make MNIST, extremely centers pitfalls wo make constructed soon over proposed methodology framework for two separate yet equally important electronic: talking) more qualifications teaching car changing and order) dataset solvency evaluation. In limited, rather findings nothing less promise a more partial identification another trade - cuts between newer advantages, accuracy to robustness always get example send science prioritize their push in files collection which identifying the hardly robust and more challenging competitive alternative.", "histories": [["v1", "Sat, 1 Apr 2017 11:58:24 GMT  (3529kb,D)", "http://arxiv.org/abs/1704.00158v1", "Submitted to UAI 2017"]], "COMMENTS": "Submitted to UAI 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ozsel kilinc", "ismail uysal"], "accepted": false, "id": "1704.00158"}, "pdf": {"name": "1704.00158.pdf", "metadata": {"source": "CRF", "title": "Clustering-based Source-aware Assessment of True Robustness for Learning Models", "authors": ["Ozsel Kilinc"], "emails": [], "sections": [{"heading": null, "text": "We introduce a novel validation framework to measure the true robustness of learning models for real-world applications by creating sourceinclusive and source-exclusive partitions in a dataset via clustering. We develop a robustness metric derived from source-aware lower and upper bounds of model accuracy even when data source labels are not readily available. We clearly demonstrate that even on a well-explored dataset like MNIST, challenging training scenarios can be constructed under the proposed assessment framework for two separate yet equally important applications: i) more rigorous learning model comparison and ii) dataset adequacy evaluation. In addition, our findings not only promise a more complete identification of trade-offs between model complexity, accuracy and robustness but can also help researchers optimize their efforts in data collection by identifying the less robust and more challenging class labels."}, {"heading": "1 INTRODUCTION", "text": "The main goal in machine learning is to obtain a predictive model performing well on the input sample it has never seen before. This is called generalization and it separates machine learning from an optimization problem trying to find the minimum training error [Goodfellow et al., 2016]. Statistical learning theory [Vapnik, 1991] defines this objective as risk minimization. However, expected risk cannot be directly calculated since exact data distribution is unknown and only available information is contained in a finite dataset, which is, in theory, an infinitesimally small sample of an otherwise infinite population. Instead, it is estimated\nby the empirical risk, i.e. the mean error over a test set whose examples are specifically not introduced during training. The assumption that among all the family of functions that can be modeled by the algorithm based on the training set, the one minimizing the empirical risk also minimizes the expected risk, turns the overall learning procedure into an empirical risk minimization problem. Empirical risk minimization also assumes that the examples in training and test sets are independent and identically distributed, i.e. i.i.d samples. Under these two assumptions, one can divide the dataset into training and test subsets and try to obtain the model providing the best generalization by validating model performance on the test subset. There exist a variety of partitioning methods commonly used in the literature most of which depend on random selection of samples in the dataset.\nHoldout method is the simplest partitioning technique which divides the dataset into mutually exclusive training and test sets, however it may imply statistical uncertainty around the empirical risk, specifically when the chosen test set is small. Furthermore, since it does not provide any variance with respect to the training set, when comparing different algorithms, holdout method may not be considered optimal [Dietterich, 1998]. In such cases, computer intensive resampling methods can be used such as folded cross-validation, random subsampling or bootstrapping [Kohavi, 1995].\nIn random subsampling, the holdout method is repeated k times and the results of these runs are averaged. Unlike random subsampling, in bootstrapping, dataset is sampled with replacement, hence the resample size can be greater than the sample size. In cross-validation methods such as k-fold, the dataset is randomly divided into k non-overlapping subsets, one of which is used as test set and the remaining k \u2212 1 subsets are used for the training. This procedure is repeated k times to test the model on a different subset every time and the errors are averaged to estimate the overall test error [Kohavi, 1995]. It is shown that cross-validation pro-\nar X\niv :1\n70 4.\n00 15\n8v 1\n[ cs\n.L G\n] 1\nA pr\n2 01\nvides an unbiased estimate of the test error but its variance may be very large [Breiman et al., 1996]. Also, [Bengio and Grandvalet, 2004] studied the problem of that there exists no universal unbiased estimator of the variance of k-fold cross-validation.\nDespite the claim of learning theory that a machine learning algorithm can generalize reasonably well using a finite training set, the \u201cno free lunch\u201d theorem [Wolpert, 1996] states that when averaged over all possible data-generating distributions, every classification algorithm has the same generalization potential for the previously unobserved points. However, the motivation behind designing learning algorithms is to make the best assumptions about the kinds of probability distributions we might encounter in real-world applications. In other words, the goal of machine learning is not to seek a universally optimal learning algorithm, but to understand the relevant real-world data distributions and design algorithms performing well on data drawn from them [Goodfellow et al., 2016].\nEven under these assumptions, we cannot ignore the possibility that the AI agent we have designed might encounter data drawn from a different kind of datagenerating distribution which may not have been available in our finite dataset used for training. Hence, there is need for evaluating the designed algorithms under such worst-case scenarios especially for critically deployed applications. This should, of course, be relevant to the use-case the algorithm is designed for, i.e. a hand-written digit classifier should not be tested with a picture of an animal, instead its performance should be measured on a different hand-written digit dataset whose examples are drawn from a distribution not used for training and testing. Collecting a secondary dataset for this purpose is not feasible or practical for every single learning problem. A more intelligent solution to test for these worse-case scenarios would be to find the examples belonging to different distributions in the original dataset and make the dataset partitioning according to this information. The assumption in this case is that since the model is trained and tested using less likely examples, obtained generalization will be a better representation of worst-case scenarios.\n[Kilinc and Uysal, 2015] studied this problem on a small activity recognition dataset where user IDs of the volunteers performing the activities were available for each sample along with the activity labels. Assuming that each user, i.e. data source, generates different distributions for the same activity label, they proposed two types of source-aware partitioning. While inclusive sourceaware ensures that samples from each source, i.e. datagenerating process, are arranged to be included in all sub-\nsets, exclusive source-aware ensures mutually exclusive use of these sources. Validated results through different resampling methods show that while inclusive sourceaware type partitioning ends up with the estimation of test error close to the one obtained without using sourceawareness, i.e. completely randomized division of training and test subsets, exclusive source-aware partitioning results in significantly higher error rates, which may possibly be interpreted as a better estimation of the model generalization when it encounters samples less similar to those in the existing dataset.\nUnfortunately, not all datasets have this kind of labeled information about the source identities. In this paper, we propose to use a clustering algorithm to artificially find different sources within the dataset to generalize the use of source-aware partitioning to all datasets. Moreover, in order to obtain a more robust evaluation of different learning models, we propose the use of these two estimations of expected generalization, i.e. test accuracy obtained using inclusive source-aware and exclusive source-aware data partitioning which respectively simulate the best-case and the worst-case scenarios that might be encountered in real-world applications. We also introduce a similar comparison to be made on the actual adequacy of the dataset itself to see if it\u2019s sufficient to obtain a good enough generalization or if more samples need to be collected. We used MNIST [LeCun et al., 1998] to show that dataset partitioning that takes the source distribution into account is not only crucial for small datasets, but even for larger, and well-explored datasets such as MNIST where state-of-the-art solutions can readily obtain error rates around 0.21%.\nThis paper is organized as follows. In the following section, we explain the clustering methods applied to find source distributions. The third section describes how the obtained clusterings are used to implement source-aware partitioning. Then, the fourth section introduces the proposed robustness metrics and framework. In the fifth section, results obtained for model robustness and dataset adequacy are presented and then, the paper is concluded with final comments."}, {"heading": "2 USING CLUSTERING TO FIND SOURCE DISTRIBUTIONS", "text": "Any clustering algorithm can be applied to find the different sources, i.e. data generating processes or distributions within the dataset. Samples of each class are clustered into k clusters where each cluster is assumed to correspond to a different source distribution within the same class label. The hypothesis is that, for each class, the samples in different clusters are generated by differ-\nent data generating processes and if the samples of these clusters are completely isolated from the training set, yet included in the test set, the generalization of the model on these samples will get worse.\nWe cluster the each one of the 10 digit classes of MNIST dataset using k-means clustering algorithm [Arthur and Vassilvitskii, 2007] by choosing k = 5 to divide the dataset into approximately 20% sized clusters. Since the training and test sets will be reconstructed based on source-aware partitioning, we abandon MNIST\u2019s existing partitioning to combine existing training and test sets and apply partitioning to all of the 70,000 available samples. Figure 1 visualizes the means of clusters found by using k-means.\nMNIST dataset happens to have some good features which help k-means obtain good clusters. Namely, the images are centered on a 28x28 plane with respect to the center of mass of the pixels. Besides, since we are interested in clustering among the samples of each digit where variance between samples is significantly small, this creates a considerably simple clustering problem which can easily be handled by the relatively simple computation of k-means. However, k-means may not always perform optimally when doing in-class clustering for more complex datasets.\n[Kilinc and Uysal, 2017] recently proposed a method called auto-clustering output layer (ACOL) which enables deep neural networks to find the subclasses of parent classes. ACOL can be used for semi-supervised problems where labeling is partially available or for completely unsupervised problems using the proposed \u201cpseudo-class\u201d trick. It is shown that ACOL outperforms k-means by a significantly large margin in clustering applications created on MNIST for both types of learning. They state that ACOL enables the use of deep learning models - originally proposed for supervised classification - for clustering problems and claim that this paradigm\nshift can be called \u201cdeep clustering\u201d because the capacities of resulting clustering models can be increased by adding additional layers as we do for deep learning classifiers.\nTo demonstrate that the effects of source-aware partitioning are independent of the clustering algorithm used, we have also applied ACOL with K = 5 and other hyperparameters as described in [Kilinc and Uysal, 2017] to obtain in-class clustering. Clustering is performed on a convolutional network model we labeled \u201cCNN2\u201d whose specifications are described in the subsequent sections along with other models (such as support vector machines, or other neural network architectures) used in experiments. Similar to Figure 1, Figure 2 shows the means of clusters found by using ACOL clustering. We have used the silhouette score [Rousseeuw, 1987] to evaluate the consistency within formed clusters in which k-means and ACOL clustering get the scores of 0.0456 and 0.0280 respectively. Hence, we may possibly observe more significant differences between inclusive source-aware and exclusive source-aware partitioning when k-means generated clustering is used. It is important to point out that traditional clustering performance cannot be computed in this case since groundtruth labels are simply not known. The main goal is to create more challenging subsets than simply using random resampling or hold-out as MNIST can already be considered as having a challenging test set because sets of writers for its training set and test set are disjoint. Clustering in this case can further help us find the groups of writers generating similar digits (in terms of roundness, tilt, etc.) and apply partitioning based on this information.\nIn the end, both schemes are used to apply source-aware partitioning. The following section describes how we apply inclusive and exclusive source-awareness using these clusters."}, {"heading": "3 APPLYING SOURCE-AWARE PARTITIONING", "text": "In source-aware partitioning, assuming that each one of the 5 clusters obtained via k-means or ACOL correspond to a different source which generates data with a different distribution, training and test subsets are constructed based on this information instead of completely randomized sampling.\nMore specifically, for exclusive source-aware partitioning, for each one of the 10 digit classes, one of the 5 clusters is randomly chosen and all its samples are added to the test set while the samples of the remaining 4 clusters are added to training set effectively creating a 20% to 80% split. This partitioning ensures that the samples of the selected test cluster for each class are never introduced to the algorithm during training. Random selection of the clusters added to test set is performed independently for each class. Since the correlations between the clusters of different classes are not known, i.e. we do not know that samples in which cluster of digit-1 are generated by the same source generating the samples in the first cluster of digit-0, we cannot cover all combinations in 5 trials as we do in k-fold cross-validation. Instead, it requires 510 combinations for 5 clusters and 10 class labels, which is computationally almost impossible to cover completely. Hence, we randomly picked clusters for each class and repeated the same experiment 100 times to satisfy the desired statistical significance.\nFor inclusive source-aware partitioning, 20% of the samples from each of the 5 clusters are randomly chosen and added to test set while the remaining 80% of samples are added to training set. Although the resulting subsets are also mutually exclusive, i.e. a sample is added either to training set or test set but not both at the same time, in terms of source distributions, they are forced to have the same or similar distributions. In other words, this partitioning ensures that both training and test set involve samples from all source distributions. Similar to exclusive source-aware random selection of the clusters added to test set is performed independently for each class instead of k-fold cross-validation due to the number of different combinations possible. Each experiment for inclusive source-aware partitioning has also been repeated for 100 times as we do for exclusive source-aware partitioning for desired statistical significance."}, {"heading": "4 PROPOSED ROBUSTNESS METRICS AND FRAMEWORK", "text": "We claim that the accuracy obtained using exclusive source-aware partitioning, AccX , is a better estimation\nfor the expected accuracy of the trained algorithm when it encounters unlikely examples compared to those we have used for training, i.e. worst-case scenario, and in the same manner, accuracy obtained using inclusive sourceaware partitioning, AccI , is a better estimation for the expected accuracy in the best-case scenario. Hence, we propose to use an interval, instead of a single value, to define the expected real-world performances of the learning algorithms such that:\nAccuracy Interval = I := [ AccX , AccI ] (1)\nSince we observe natural variance in obtained accuracies with respect to sampled training and test sets for the 100 repeated trials, it is more accurate to define this interval as expected accuracy interval such that\nI\u0304 = E[I] = [ E [AccX] ,E [AccI] ] (2)\nwhere we expect to observe E [AccI] \u2265 E [AccX] given that the source identification process, i.e. applied clustering algorithm, is successful in distinguishing the sources. We also expect to observe that E [AccI] is approximately equal to the expected accuracy obtained using a completely random partitioning method, E [Acc], i.e. without using any source awareness, if the dataset is sufficiently large. Otherwise, E [AccI] > E [Acc] since there might possibly exist cases where samples of a source are mostly isolated from the training set during source-unaware random sampling.\nIn order to compare the learning algorithms with respect to stability in their expected real-life performances, i.e. robustness, we can use the range of the expected accuracy interval and normalize it by E [AccI] to obtain a metric scaled between 0 and 1. Hence, robustness becomes\n\u03c1 := 1\u2212 E [AccI]\u2212 E [AccX] E [AccI] = E [AccX] E [AccI]\n(3)\nwhich emphasis the narrowness of the expected accuracy interval regardless of the magnitudes of the obtained accuracies."}, {"heading": "5 RESULTS", "text": "In this section we will present detailed analysis of the proposed partitioning algorithms for two separate applications: i) measuring learning model robustness and ii) measuring dataset robustness."}, {"heading": "5.1 MODEL ROBUSTNESS", "text": "We compare the following learning models.\n\u2022 SVM:\n\u2013 C = 1, kernel = rbf, gamma = 0.01\n\u2022 MLP:\n\u2013 Feedforward 2048 - 50% Dropout - maxnorm(2) \u2013 Feedforward 2048 - 50% Dropout - maxnorm(2) \u2013 Feedforward 2048 - 50% Dropout - maxnorm(2)\n\u2022 CNN-1:\n\u2013 32x3x3 - 32x3x3 - MP2x2 - 25% Dropout \u2013 Feedforward 2048 - 50% Dropout\n\u2022 CNN-2:\n\u2013 32x3x3 - 32x3x3 - MP2x2 - 25% Dropout \u2013 64x3x3 - 64x3x3 - MP2x2 - 25% Dropout \u2013 Feedforward 2048 - 50% Dropout\n\u2022 CNN-3:\n\u2013 32x3x3 - 32x3x3 - MP2x2 - 25% Dropout \u2013 64x3x3 - 64x3x3 - MP2x2 - 25% Dropout \u2013 128x3x3 - 128x3x3 - MP2x2 - 25% Dropout \u2013 Feedforward 2048 - 50% Dropout\nFor all the experiments, we have trained all the neural network models for the same number of epochs which is sufficiently large to ensure that testing error stops decreasing for all models in all cases. To eliminate the effect of overtraining, especially in smaller models, we have recorded the maximum testing accuracy observed during each training. Evaluating 5 models (SVM, MLP, CNN-1, CNN-2 and CNN-3) using 2 types of partitioning (inclusive source-aware and exclusive source-aware and 2 different kinds of cluster (k-means and ACOL) result in 5 x 2 x 2 = 20 sets of experiments which are then repeated 100 times for desired statistical significance.\nFigure 3 presents the box plots of test accuracies where source-aware partitioning is performed according to the clustering scheme obtained using ACOL. The figure very clearly shows that the test accuracies obtained using inclusive source-aware partitioning, AccI , have a narrow dispersion, whereas, exclusive source-aware partitioning results in a significantly wider distribution of test accuracies, AccX , due to the diversity of combinations observed when distributing the sources exclusively into training and test sets. Besides, while the expected value and variance of AccI only slightly vary with respect to the learning algorithm, both expected value and variance of AccX drastically improve with increasing model capacity to represent a better decision metric in determining which model outperforms the rest.\nFor better understanding, Figure 4 provides the visualization of expected accuracy interval I\u0304, where the upper and lower bounds of the colored regions represent\nthe expected AccI and the expected AccX respectively. We propose that these colored regions show the range of accuracies expected to be observed in real-world applications depending on the likelihood of the encountered inputs being similar to those included in our finite dataset. This figure illustrates two different intervals obtained using two different clustering schemes where red and blue colored regions represent clustering schemes obtained using ACOL and k-means respectively. Upper bounds of these two regions correlate well as both clustering schemes result in approximately equal expected AccI . The clustering obtained by k-means infer a wider interval at the bottom by yielding a lower AccX . One may interpret this difference as an indicator pointing that k-means provides a more challenging, yet not necessarily better, mutually exclusive separation of the sources during in-class clustering. Another observation is that the differences between lower bounds, together with the ranges of both regions, shrink as the model capacity increases.\nDevised robustness metric can be seen in Figure 5 and Table 1 summarizes the results obtained on all models using both clustering approaches. Even as SVM and MLP algorithms obtain over 97% accuracy when they are tested with likely examples, their generalizations on unlikely ones are clearly not as successful as CNN models, which is slightly more apparent when k-means clustering is used. Through their convolution layers, CNN models obtain a better representation of the visual input at the classifier layer, which might help extracting additional information to compensate for the effect of excluded sources. In fact, as seen in the table, each added convolution layer improves their generalization ability to unlikely examples, which evidently makes them more robust. Moreover, CNNs are also indifferent to the two clustering schemes whose effects are more apparent on exclusive source-aware performances of SVM and MLP."}, {"heading": "5.2 DATASET ADEQUACY", "text": "Using source-aware partitioning, one can also analyze whether the number of examples in the training dataset is enough to obtain good generalization on unlikely examples. This kind of analysis may be useful to help the researcher determine the necessity to collect more sam-\nples. To observe the effect of number of training samples, we reduced the size of the training set while keeping test set the same as in the experiments with the full sized training set. For these experiments, we fixed our learning models to CNN and applied partitioning using the ACOL clustering scheme.\nAs an example, for CNN-1 model, box plots of test accuracies obtained in these experiments are given in Figure 6. One can observe that, for both AccI and AccX , expected values increase and variance decrease with increasing training set size. In order to interpret this behavior with respect to the model capacity, Figure 7 and Figure 8 respectively illustrate expected accuracy interval and robustness observed on all three CNN models. One can observe that to obtain a good expected generalization on likely examples, which is evaluated by AccI , using 10000 training samples is sufficient. However, further increases on the training set size improve the expected\ngeneralization on unlikely test examples, which is evaluated by AccX , and thus increases robustness. Another remarkable observation is that adding an additional convolution layer on CNN-2 has no effect on its generalization for likely examples (as shown by the blue dotted line coinciding with the boundary of CNN-3) but improves generalization for the unlikely samples. Considering that we expect to obtain Acc \u2248 AccI , without source-aware partitioning one observes almost no difference between using CNN-2 and CNN-3 and might consequently think that there is no need to use the bigger model. However, exclusive source-aware performances of these two models clearly show CNN-3 outperforming CNN-2 through better generalization on unlikely samples as a better representation of expected real life performance. Table 2 and Table 3 summarize the expected accuracy interval and robustness metrics observed on all three models respectively."}, {"heading": "5.2.1 Class Label Specific Dataset Adequacy", "text": "We have performed further analysis to understand dataset robustness on individual class label basis. More specifi-\ncally, one of the classes in our dataset might have more variety among its samples and create clusters with a wider range of distributions. In such a case, we might need to use more examples for that class to obtain a good generalization, while other classes are less variant and existing samples are enough for the algorithm to perform well even on unlikely test samples. Analyzing the class specific results of dataset robustness may help the researcher to optimize efforts for extra data collection by identifying the less robust classes.\nFigure 9 and Figure 10 illustrate the class-based expected accuracy interval and robustness observed on CNN-3 model for three sample classes of MNIST - digit-0, digit1 and digit-5 - showing considerably different characteristics with respect to the increasing number of training samples. When the number of training samples is limited to 100, i.e. 10 samples/class, the expected generalization of the model is most accurate when encountering likely digit-1 test samples. However in terms of the robustness metric, the model performs better for digit-0 where expected accuracy interval is narrower. In other words, the model needs more examples of digit-1 in order to develop a better generalization on unlikely test samples as it has already done so for likely ones. On the other\nhand, for digit-5 class, the model does not perform well on either likely or unlikely test samples when the training set size is small. Remarkably, as more training samples are added, the model becomes more robust for digit-5 than for digit-1. Without using source-aware partitioning, one might detect the difficult class labels like digit-5. However, it is impossible to identify those behaving like digit-1 as they are likely to yield good performance on test sets constructed by random cross-validation. Sourceawareness enables us to identify such classes by creating more challenging subset partitioning."}, {"heading": "6 CONCLUSION", "text": "The expected generalization error, which is traditionally calculated over a test set constructed by random subsampling from the entire dataset, can be a quick measure to compare different learning algorithms. However, it is not sufficient to make a realistic assessment of performance under real-world conditions in which encountering an unlikely input compared to those within the dataset is highly possible. To simulate such conditions, we can perform dataset partitioning considering the mutually exclusive distribution of the sources, i.e. data-generating processes, existing in the dataset. When data-sources are not readily apparent, we can approximate this kind of information by applying clustering algorithms to the samples of each class such that each cluster corresponds to a different distribution or source of its class. In this paper, we have applied two source-aware partitioning schemes to simulate two extreme cases on MNIST. We show that Inclusive source-aware partitioning, which ensures that samples from each source are included in all subsets, is a better evaluation of the expected accuracy for the best case real-world scenarios when learning model encounters inputs that are similar to the dataset samples. On the other hand, obtained accuracy using exclusive source-\naware partitioning, which ensures the mutually exclusive use of sources, represent a better evaluation of the expected accuracy for the worst case real-world scenarios when learning model encounters inputs statistically more dissimilar to the dataset samples. We proposed using an interval defined by these two accuracy values and their ratio, expected accuracy interval and robustness, for more thorough real-world performance assessment. On MNIST, we have shown that the proposed evaluation can be used for two separate applications: i) more rigorous model comparison and ii) dataset adequacy evaluation and class specific dataset robustness which can help researchers optimize their efforts for data collection by identifying the less robust classes. In this paper, we have only considered two-way splitting of the dataset, i.e. training and testing. However, as future work, the proposed method can also be applied for three-way splitting, i.e. training, validation and testing. Validation set constructed by using the sources whose samples are never introduced during training can help us find better hyperparameter settings resulting in better generalization on test set or for more informed early-stopping for less overfitting on the training set."}], "references": [{"title": "k-means++: the advantages of careful seeding", "author": ["Arthur", "Vassilvitskii", "D. 2007] Arthur", "S. Vassilvitskii"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Arthur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2007}, {"title": "No unbiased estimator of the variance of k-fold cross-validation", "author": ["Bengio", "Grandvalet", "Y. 2004] Bengio", "Y. Grandvalet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2004}, {"title": "Heuristics of instability and stabilization in model selection", "author": ["Breiman et al", "L 1996] Breiman"], "venue": "The annals of statistics,", "citeRegEx": "al. and Breiman,? \\Q1996\\E", "shortCiteRegEx": "al. and Breiman", "year": 1996}, {"title": "Source-aware partitioning for robust crossvalidation", "author": ["Kilinc", "Uysal", "O. 2015] Kilinc", "I. Uysal"], "venue": "In 14th IEEE International Conference on Machine Learning and Applications,", "citeRegEx": "Kilinc et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kilinc et al\\.", "year": 2015}, {"title": "Deep clustering using auto-clustering output layer. CoRR, abs/1702.08648", "author": ["Kilinc", "Uysal", "O. 2017] Kilinc", "I. Uysal"], "venue": null, "citeRegEx": "Kilinc et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kilinc et al\\.", "year": 2017}, {"title": "The mnist database of handwritten digits", "author": ["LeCun et al", "Y. 1998] LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "We introduce a novel validation framework to measure the true robustness of learning models for real-world applications by creating sourceinclusive and source-exclusive partitions in a dataset via clustering. We develop a robustness metric derived from source-aware lower and upper bounds of model accuracy even when data source labels are not readily available. We clearly demonstrate that even on a well-explored dataset like MNIST, challenging training scenarios can be constructed under the proposed assessment framework for two separate yet equally important applications: i) more rigorous learning model comparison and ii) dataset adequacy evaluation. In addition, our findings not only promise a more complete identification of trade-offs between model complexity, accuracy and robustness but can also help researchers optimize their efforts in data collection by identifying the less robust and more challenging class labels.", "creator": "LaTeX with hyperref package"}}}