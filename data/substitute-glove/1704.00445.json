{"id": "1704.00445", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "On Kernelized Multi-armed Bandits", "abstract": "We merely the stochastic bandit why well close continuous set this and, brought of could reward consequently set seen arms name making be longer he unknown. We allows one new Gaussian methods - america predictive without continuous chaebol optimisation - Improved GP - UCB (IGP - UCB) have GP - Thomson sampling (GP - TS ), and derive corresponding insult deflected. Specifically, new right need when 's recent ,000 parameters is to over evolved vector Hilbert payload (RKHS) also naturally corresponds to a Gaussian process desktop may considered input under the methods. Along is out, tell derive place new hard - in-phase increasing isoperimetric time vector - totaled commutators large lawful, possibly infinite, dimension. Finally, experimental program several suggesting already existing parameters. soluble together give - time conditions are earlier over done memorable the optimistic advantage a the planned strategies opened like certain.", "histories": [["v1", "Mon, 3 Apr 2017 06:47:42 GMT  (127kb,D)", "http://arxiv.org/abs/1704.00445v1", null], ["v2", "Wed, 17 May 2017 09:04:40 GMT  (127kb,D)", "http://arxiv.org/abs/1704.00445v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sayak ray chowdhury", "aditya gopalan"], "accepted": true, "id": "1704.00445"}, "pdf": {"name": "1704.00445.pdf", "metadata": {"source": "CRF", "title": "On Kernelized Multi-armed Bandits", "authors": ["Sayak Ray Chowdhury", "Aditya Gopalan"], "emails": ["SRCHOWDHURY@ECE.IISC.ERNET.IN", "ADITYA@ECE.IISC.ERNET.IN"], "sections": [{"heading": null, "text": "over the arms assumed to be fixed but unknown. We provide two new Gaussian process-based algorithms for continuous bandit optimization \u2013 Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and derive corresponding regret bounds. Specifically, the bounds hold when the expected reward function belongs to the reproducing kernel Hilbert space (RKHS) that naturally corresponds to a Gaussian process kernel used as input by the algorithms. Along the way, we derive a new self-normalized concentration inequality for vectorvalued martingales of arbitrary, possibly infinite, dimension. Finally, experimental evaluation and comparisons to existing algorithms on synthetic and real-world environments are carried out that highlight the favorable gains of the proposed strategies in many cases."}, {"heading": "1. Introduction", "text": "Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al. (2008). A typical feature of such problems is a large, or potentially infinite, domain of decision points or covariates (prices, actions, transmit powers), together with only partial and noisy observability of the associated outcomes (demand, state/reward, communication rate); reward/loss information is revealed only for decisions that are chosen. This often makes it hard to balance exploration and exploitation, as available knowledge must be transferred efficiently from a finite set of observations so far to estimates of the values of infinitely many decisions. A classic case in point is that of the canonical stochastic MAB with finitely many arms, where the effort to optimize scales with the total number of arms or decisions; the effect of this is catastrophic for large or infinite arm sets.\nWith suitable structure in the values or rewards of arms, however, the challenge of sequential optimization can be efficiently addressed. Parametric bandits, especially linearly parameterized bandits Rusmevichientong and Tsitsiklis (2010), represent a well-studied class of structured decision making settings. Here, every arm corresponds to a known, finite dimensional vector (its feature vector), and its expected reward is assumed to be an unknown linear function of its feature vector. This allows for a large, or even infinite, set of arms all lying in space of finite dimension, say d, and a rich line of work gives algorithms that attain sublinear regret with a polynomial dependence on the dimension, e.g., Confidence Ball Dani et al. (2008), OFUL Abbasi-Yadkori et al. (2011) (a strengthening of Confidence Ball) and Thompson sampling for linear bandits\nar X\niv :1\n70 4.\n00 44\n5v 1\n[ cs\n.L G\n] 3\nAgrawal and Goyal (2013)1 The insight here is that even though the number of arms can be large, the number of unknown parameters (or degrees of freedom) in the problem is really only d, which makes it possible to learn about the values of many other arms by playing a single arm.\nA different approach to modelling bandit problems with a continuum of arms is via the framework of Gaussian processes (GPs) Rasmussen and Williams (2006). GPs are a flexible class of nonparametric models for expressing uncertainty over functions on rather general domain sets, which generalize multivariate Gaussian random vectors. GPs allow tractable regression for estimating an unknown function given a set of (noisy) measurements of its values at chosen domain points. The fact that GPs, being distributions on functions, can also help quantify function uncertainty makes it attractive for basing decision making strategies on them. This has been exploited to great advantage to build nonparametric bandit algorithms, such as GP-UCB Srinivas et al. (2009), GP-EI and GP-PI Wang et al. (2016). In fact, GP models for bandit optimization, in terms of their kernel maps, can be viewed as the parametric linear bandit paradigm pushed to the extreme, where each feature vector associated to an arm can have infinite dimension 2.\nAgainst this backdrop, our work revisits the problem of bandit optimization with stochastic rewards. Specifically, we consider stochastic multiarmed bandit (MAB) problems with a continuous arm set, and whose (unknown) expected reward function is assumed to lie in a reproducing kernel Hilbert space (RKHS), with bounded RKHS norm \u2013 this effectively enforces smoothness on the function3. We make the following contributions-\n\u2022 We design a new algorithm \u2013 Improved Gaussian Process-Upper Confidence Bound (IGP-UCB) \u2013 for stochastic bandit optimization. The algorithm can be viewed as a variant of GP-UCB Srinivas et al. (2009), but uses a significantly reduced confidence interval width resulting in an order-wise improvement in regret compared to GP-UCB. IGP-UCB also shows a markedly improved numerical performance over GP-UCB.\n\u2022 We develop a nonparametric version of Thompson sampling, called Gaussian Process Thompson sampling (GP-TS), and show that enjoys a regret bound of O\u0303 ( \u03b3T \u221a dT )\n. Here, T is the total time horizon and \u03b3T is a quantity depending on the RKHS containing the reward function. This is, to our knowledge, the first known regret bound for Thompson sampling in the agnostic setup with nonparametric structure.\n\u2022 We prove a new self-normalized concentration inequality for infinite-dimensional vector-valued martingales, which is not only key to the design and analysis of the IGP-UCB and GP-TS algorithms, but also potentially of independent interest. The inequality generalizes a corresponding self-normalized bound for martingales in finite dimension proven by Abbasi-Yadkori et al. (2011).\n\u2022 Empirical comparisons of the algorithms developed above, with other GP-based algorithms, are presented, over both synthetic and real-world setups, demonstrating performance improvements of the proposed algorithms, as well as their performance under misspecification."}, {"heading": "2. Problem Statement", "text": "We consider the problem of sequentially maximizing a fixed but unknown reward function f : D \u2192 R over a (potentially infinite) set of decisions D \u2282 Rd, also called actions or arms. An algorithm for this problem chooses, at each round t, an action xt \u2208 D, and subsequently observes a reward yt = f(xt) + \u03b5t, which is a noisy version of the function value at xt. The action xt is chosen causally depending upon the arms played and rewards obtained upto round t \u2212 1, denoted by the history Ht\u22121 = {(xs, ys) : s = 1, . . . , t \u2212 1}. We\n1. Roughly, for rewards bounded in [\u22121, 1], these algorithms achieve optimal regret O\u0303 ( d \u221a T )\n, where O\u0303 (\u00b7) hides polylog(T ) factors. 2. The completion of the linear span of all feature vectors (images of the kernel map) is precisely the reproducing kernel Hilbert space\n(RKHS) that characterizes the GP. 3. Kernels, and their associated RKHSs,\nassume that the noise sequence {\u03b5t}\u221et=1 is conditionally R-sub-Gaussian for a fixed constant R \u2265 0, i.e.,\n\u2200t \u2265 0, \u2200\u03bb \u2208 R, E [ e\u03bb\u03b5t \u2223\u2223 Ft\u22121] \u2264 exp(\u03bb2R2 2 ) , (1)\nwhere Ft\u22121 is the \u03c3-algebra generated by the random variables {xs, \u03b5s}t\u22121s=1 and xt.This is a mild assumption on the noise (it holds, for instance, for distributions bounded in [\u2212R,R]) and is standard in the bandit literature Abbasi-Yadkori et al. (2011); Agrawal and Goyal (2013).\nRegret. The goal of an algorithm is to maximize its cumulative reward or alternatively minimize its cumulative regret \u2013 the loss incurred due to not knowing f \u2019s maximum point beforehand. Let x? \u2208 argmaxx\u2208D f(x) be a maximum point of f (assuming the maximum is attained). The instantaneous regret incurred at time t is rt = f(x\n?)\u2212f(xt), and the cumulative regret in a time horizon T (not necessarily known a priori) is defined to be RT = \u2211T t=1 rt. A sub-linear growth of RT in T signifies that RT /T \u2192 0 as T \u2192 \u221e, or vanishing per-round regret. Regularity Assumptions. Attaining sub-linear regret is impossible in general for arbitrary reward functions f and domains D, and thus some regularity assumptions are in order. In what follows, we assume that D is compact. The smoothness assumption we make on the reward function f is motivated by Gaussian processes4 and their associated reproducing kernel Hilbert spaces (RKHSs, see Scho\u0308lkopf and Smola (2002)). Specifically, we assume that f has small norm in the RKHS of functions D \u2192 R, with positive semi-definite kernel function k : D \u00d7 D \u2192 R. This RKHS, denoted by Hk(D), is completely specified by its kernel function k(\u00b7, \u00b7) and vice-versa, with an inner product \u3008\u00b7, \u00b7\u3009k obeying the reproducing property: f(x) = \u3008f, k(x, \u00b7)\u3009k for all f \u2208 Hk(D). In other words, the kernel plays the role of delta functions to represent the evaluation map at each point x \u2208 D via the RKHS inner product. The RKHS norm \u2016f\u2016k = \u221a \u3008f, f\u3009 k is a measure of the smoothness5 of f , with respect to the kernel function k, and satisfies: f \u2208 Hk(D) if and only if \u2016f\u2016k <\u221e.\nWe assume a known bound on the RKHS norm of the unknown target function6: \u2016f\u2016k \u2264 B. Moreover, we assume bounded variance by restricting k(x, x) \u2264 1, for all x \u2208 D. Two common kernels that satisfy bounded variance property are Squared Exponential and Mate\u0301rn, defined as\nkSE(x, x \u2032) = exp ( \u2212 s2/2l2 ) ,\nkMate\u0301rn(x, x \u2032) =\n21\u2212\u03bd\n\u0393(\u03bd) (s\u221a2\u03bd l )\u03bd B\u03bd (s\u221a2\u03bd l ) ,\nwhere l > 0 and \u03bd > 0 are hyperparameters, s = \u2016x\u2212 x\u2032\u20162 encodes the similarity between two points x, x\u2032 \u2208 D, and B\u03bd(\u00b7) is the modified Bessel function. Generally the bounded variance property holds for any stationary kernel, i.e. kernels for which k(x, x\u2032) = k(x \u2212 x\u2032) for all x, x\u2032 \u2208 Rd. These assumptions are required to make the regret bounds scale-free and are standard in the literature Agrawal and Goyal (2013). Instead if k(x, x) \u2264 c or \u2016f\u2016k \u2264 cB, then our regret bounds would increase by a factor of c."}, {"heading": "3. Algorithms", "text": "Design philosophy. Both the algorithms we propose use Gaussian likelihood models for observations, and Gaussian process (GP) priors for uncertainty over reward functions. A Gaussian process over D, denoted by GPD(\u00b5(\u00b7), k(\u00b7, \u00b7)), is a collection of random variables (f(x))x\u2208D, one for each x \u2208 D, such that every finite sub-collection of random variables (f(xi))mi=1 is jointly Gaussian with mean E [f(xi)] = \u00b5(xi) and covariance E [(f(xi)\u2212 \u00b5(xi))(f(xj)\u2212 \u00b5(xj))] = k(xi, xj), 1 \u2264 i, j \u2264 m, m \u2208 N. The algorithms use\n4. Other work has also studied continuum-armed bandits with weaker smoothness assumptions such as Lipschitz continuity \u2013 see Related work for details and comparison. 5. One way to see this is that for every element g in the RKHS, |g(x)\u2212g(y)| = |\u3008g, k(x, \u00b7)\u2212k(y, \u00b7)\u3009| 6 \u2016g\u2016k \u2016k(x, \u00b7)\u2212 k(y, \u00b7)\u2016k by Cauchy-Schwarz. 6. This is analogous to the bound on the weight \u03b8 typically assumed in regret analyses of linear parametric bandits.\nGPD(0, v 2k(\u00b7, \u00b7)), v > 0, as an initial prior distribution for the unknown reward function f over D, where k(\u00b7, \u00b7) is the kernel function associated with the RKHS Hk(D) in which f is assumed to have \u2018small\u2019 norm at mostB. The algorithms also assume that the noise variables \u03b5t = yt\u2212f(xt) are drawn independently, across t, from N (0, \u03bbv2), with \u03bb \u2265 0. Thus, the prior distribution for each f(x), is assumed to be N(0, v2k(x, x)), x \u2208 D. Moreover, given a set of sampling points At = (x1, . . . , xt) within D, it follows under the assumption that the corresponding vector of observed rewards y1:t = [y1, . . . , yt]T has the multivariate Gaussian distribution N (0, v2(Kt + \u03bbI)), where Kt = [k(x, x\u2032)]x,x\u2032\u2208At is the kernel matrix at time t. Then, by the properties of GPs, we have that y1:t and f(x) are jointly Gaussian given At:[\nf(x) y1:t\n] \u223c N ( 0, [ v2k(x, x) v2kt(x) T\nv2kt(x) v 2(Kt + \u03bbI)\n]) ,\nwhere kt(x) = [k(x1, x), . . . , k(xt, x)]T . Therefore conditioned on the historyHt, the posterior distribution over f is GPD(\u00b5t(\u00b7), v2kt(\u00b7, \u00b7)), where\n\u00b5t(x) = kt(x) T (Kt + \u03bbI) \u22121y1:t, (2) kt(x, x\n\u2032) = k(x, x\u2032)\u2212 kt(x)T (Kt + \u03bbI)\u22121kt(x\u2032), (3) \u03c32t (x) = kt(x, x). (4)\nThus for every x \u2208 D, the posterior distribution of f(x), givenHt, is N (\u00b5t(x), v2\u03c32t (x)). Remark. Note that the GP prior and Gaussian likelihood model described above is only an aid to algorithm design, and has nothing to do with the actual reward distribution or noise model as in the problem statement (Section 2). The reward function f is a fixed, unknown, member of the RKHS Hk(D), and the true sequence of noise variables \u03b5t is allowed to be a conditionally R-sub-Gaussian martingale difference sequence (Equation 1). In general, thus, this represents a misspecified prior and noise model, also termed the agnostic setting by Srinivas et al. (2009).\nThe proposed algorithms, to follow, assume the knowledge of only the sub-Gaussianity parameter R, kernel function k and upper bound B on the RKHS norm of f . Note that v, \u03bb are free parameters (possibly time-dependent) that can be set specific to the algorithm."}, {"heading": "3.1 Improved GP-UCB (IGP-UCB) algorithm", "text": "We introduce the IGP-UCB algorithm (Algorithm 1), that uses a combination of the current posterior mean \u00b5t\u22121(x) and standard deviation v\u03c3t\u22121(x) to (a) construct an upper confidence bound (UCB) envelope for the actual function f over D, and (b) choose an action to maximize it. Specifically it chooses, at each round t, the action\nxt = argmax x\u2208D \u00b5t\u22121(x) + \u03b2t\u03c3t\u22121(x), (5)\nwith the scale parameter v set to be 1. Such a rule trades off exploration (picking points with high uncertainty \u03c3t\u22121(x)) with exploitation (picking points with high reward \u00b5t\u22121(x)), with \u03b2t = B+R \u221a 2(\u03b3t\u22121 + 1 + ln(1/\u03b4)) being the parameter governing the tradeoff, which we later show is related to the width of the confidence interval for f at round t. \u03b4 \u2208 (0, 1) is a free confidence parameter used by the algorithm, and \u03b3t is the maximum information gain at time t, defined as:\n\u03b3t := max A\u2282D:|A|=t I(yA; fA).\nHere, I(yA; fA) denotes the mutual information between fA = [f(x)]x\u2208A and yA = fA + \u03b5A, where \u03b5A \u223c N (0, \u03bbv2I) and quantifies the reduction in uncertainty about f after observing yA at points A \u2282 D. \u03b3t is a problem dependent quantity and can be found given the knowledge of domain D and kernel function k. For a compact subset D of Rd, \u03b3T is O((lnT )d+1) and O(T d(d+1)/(2\u03bd+d(d+1)) lnT ), respectively, for the Squared Exponential and Mate\u0301rn kernels (Srinivas et al. (2009)), depending only polylogarithmically on the time T .\nAlgorithm 1 Improved-GP-UCB (IGP-UCB) Input: Prior GP (0, k), parameters B, R, \u03bb, \u03b4. for t = 1, 2, 3 . . . T do\nSet \u03b2t = B +R \u221a\n2(\u03b3t\u22121 + 1 + ln(1/\u03b4)). Choose xt = argmax\nx\u2208D \u00b5t\u22121(x) + \u03b2t\u03c3t\u22121(x).\nObserve reward yt = f(xt) + \u03b5t. Perform update to get \u00b5t and \u03c3t using 2, 3 and 4.\nend for\nDiscussion. Srinivas et al. (2009) have proposed the GP-UCB algorithm, and Valko et al. (2013) the KernelUCB algorithm, for sequentially optimizing reward functions lying in the RKHS Hk(D). Both algorithms play an arm at time t using the rule: xt = argmaxx\u2208D \u00b5t\u22121(x) + \u03b2\u0303t\u03c3t\u22121(x). GP-UCB uses the\nexploration parameter \u03b2\u0303t = \u221a 2B2 + 300\u03b3t\u22121 ln 3(t/\u03b4), with \u03bb set to \u03c32, where \u03c3 is additionally assumed to be a known, uniform (i.e., almost-sure) upper bound on all noise variables \u03b5t (see Theorem 3 in Srinivas et al. (2009)). Compared to GP-UCB, IGP-UCB (Algorithm 1) reduces the width of the confidence interval by a factor roughly O(ln3/2 t) at every round t, and, as we will see, this small but critical adjustment leads to much better theoretical and empirical performance compared to GP-UCB. In KernelUCB, \u03b2\u0303t is set as \u03b7/\u03bb1/2, where \u03b7 is the exploration parameter and \u03bb is the regularization constant. Thus IGP-UCB can be viewed as a special case of KernelUCB where \u03b7 = \u03b2t."}, {"heading": "3.2 Gaussian Process Thompson sampling (GP-TS)", "text": "Our second algorithm, GP-TS (Algorithm 2), inspired by the success of Thompson sampling for standard and parametric bandits (Agrawal and Goyal, 2012; Kaufmann et al., 2012; Gopalan et al., 2014; Agrawal and Goyal, 2013), uses the time-varying scale parameter vt = B +R \u221a 2(\u03b3t\u22121 + 1 + ln(2/\u03b4)) and operates as follows. At each round t, GP-TS samples a random function ft(\u00b7) from the GP with mean function \u00b5t\u22121(\u00b7) and covariance function v2t kt\u22121(\u00b7, \u00b7). Next, it chooses a decision set Dt \u2282 D, and plays the arm xt \u2208 Dt that maximizes ft7. We call it GP-Thompson-Sampling as it falls under the general framework of Thompson Sampling, i.e., (a) assume a prior on the underlying parameters of the reward distribution, (b) play the arm according to the prior probability that it is optimal, and (c) observe the outcome and update the prior. However, note that the prior is nonparametric in this case.\nAlgorithm 2 GP-Thompson-Sampling (GP-TS) Input: Prior GP (0, k), parameters B, R, \u03bb, \u03b4. for t = 1, 2, 3 . . . , do\nSet vt = B +R \u221a\n2(\u03b3t\u22121 + 1 + ln(2/\u03b4)). Sample ft(\u00b7) from GPD(\u00b5t\u22121(\u00b7), v2t kt\u22121(\u00b7, \u00b7)). Choose the current decision set Dt \u2282 D. Choose xt = argmax\nx\u2208Dt ft(x).\nObserve reward yt = f(xt) + \u03b5t. Perform update to get \u00b5t and kt using 2 and 3.\nend for\n7. If Dt = D for all t, then this is simply exact Thompson sampling. For technical reasons, however, our regret bound is valid when Dt is chosen as a suitable discretization of D, so we include Dt as an algorithmic parameter."}, {"heading": "4. Main Results", "text": "We begin by presenting two key concentration inequalities which are essential in bounding the regret of the proposed algorithms.\nTheorem 1 Let {xt}\u221et=1 be an Rd-valued discrete time stochastic process predictable with respect to the filtration {Ft}\u221et=0, i.e., xt is Ft\u22121-measurable \u2200t \u2265 1. Let {\u03b5t}\u221et=1 be a real-valued stochastic process such that for some R \u2265 0 and for all t \u2265 1, \u03b5t is (a) Ft-measurable, and (b) R-sub-Gaussian conditionally on Ft\u22121. Let k : Rd \u00d7 Rd \u2192 R be a symmetric, positive-semidefinite kernel, and let 0 < \u03b4 \u2264 1. For a given \u03b7 > 0, with probability at least 1\u2212 \u03b4, the following holds simultaneously over all t \u2265 0:\n\u2016\u03b51:t\u20162((Kt+\u03b7I)\u22121+I)\u22121 \u2264 2R 2 ln\n\u221a det((1 + \u03b7)I +Kt)\n\u03b4 . (6)\n(Here, Kt denotes the t \u00d7 t matrix Kt(i, j) = k(xi, xj), 1 \u2264 i, j \u2264 t.) Moreover, if Kt is positive definite \u2200t \u2265 1 with probability 1, then the conclusion above holds with \u03b7 = 0.\nTheorem 1 represents a self-normalized concentration inequality: the \u2018size\u2019 of the increasing-length sequence {\u03b5t}t of martingale differences is normalized by the growing quantity ((Kt + \u03b7I)\u22121 + I)\u22121 that explicitly depends on the sequence. The following lemma helps provide an alternative, abstract, view of the self-normalized process of Theorem 1, based on the feature space representation induced by a kernel.\nLemma 1 Let k : Rd \u00d7 Rd \u2192 R be a symmetric, positive-semidefinite kernel, with associated feature map \u03d5 : Rd \u2192 H and the reproducing kernel Hilbert space8 (RKHS) H . Letting St = \u2211t s=1 \u03b5s\u03d5(xs) and the\n(possibly infinite dimensional) matrix9 Vt = I + \u2211t s=1 \u03d5(xs)\u03d5(xs)\nT , we have, whenever Kt is positive definite, that\n\u2016\u03b51:t\u2016(K\u22121t +I)\u22121 = \u2016St\u2016V \u22121t , where \u2016St\u2016V \u22121t := \u2225\u2225\u2225V \u22121/2t St\u2225\u2225\u2225 H denotes the norm of V \u22121/2t St in the RKHS H .\nObserve that St is Ft-measurable and also E [ St \u2223\u2223 Ft\u22121] = St\u22121. The process {St}t\u22650 is thus a martingale with values10 in the RKHS H , which can possibly be infinite-dimensional, and moreover, whose deviation is measured by the norm weighted by V \u22121t , which is itself derived from St. Theorem 1 represents the kernelized generalization of the finite-dimensional result of Abbasi-Yadkori et al. (2011), and we recover their result under the special case of a linear kernel: \u03d5(x) = x for all x \u2208 Rd.\nWe remark that when \u03d5 is a mapping to a finite-dimensional Hilbert space, the argument of AbbasiYadkori et al. (2011, Theorem 1) can be lifted to establish Theorem 1, but it breaks down in the generalized, infinite-dimensional RKHS setting, as the self-normalized bound in their paper has an explicit, growing dependence on the feature dimension. Specifically, the method of mixtures (de la Pena et al., 2009) or Laplace method, as dubbed by Maillard (2016) (Lemma 5.2), fails to hold in infinite dimension. The primary reason for this is that the mixture distribution for finite dimensional spaces can be chosen independently of time, but in a nonparametric setup like ours, where the dimensionality of the self-normalizing factor ( K\u22121t + I\n)\u22121 itself grows with time, the use of (random) stopping times, precludes using time-dependent mixtures. We get around this difficulty by applying a novel \u2018double mixture\u2019 construction, in which a pair of mixtures on (a) the space of real-valued functions on Rd, i.e., the support of a Gaussian process, and (b) on real sequences is simultaneously used to obtain a more general result, of potentially independent interest (see Section 5 and the appendix for details).\nOur next result shows that how the posterior mean is concentrated around the unknown reward function f .\n8. Such a pair (\u03d5,H) always exists, see e.g., Rasmussen and Williams (2006). 9. More formally, Vt : H \u2192 H is the linear operator defined by Vt(z) = z + \u2211t s=1 \u03d5(xs)\u3008\u03d5(xs), z\u3009 \u2200z \u2208 H .\n10. We ignore issues of measurability here.\nTheorem 2 Under the same hypotheses as those of Theorem 1, let D \u2208 Rd, and f : D \u2192 R be a member of the RKHS of real-valued functions on D with kernel k, with RKHS norm bounded by B. Then, with probability at least 1\u2212 \u03b4, the following holds for all x \u2208 D and t \u2265 1:\n|\u00b5t\u22121(x)\u2212 f(x)| \u2264 ( B +R \u221a 2(\u03b3t\u22121 + 1 + ln(1/\u03b4)) ) \u03c3t\u22121(x),\nwhere \u03b3t\u22121 is the maximum information gain after t\u22121 rounds and \u00b5t\u22121(x), \u03c32t\u22121(x) are mean and variance of posterior distribution defined as in Equation 2, 3, 4, with \u03bb set to 1 + \u03b7 and \u03b7 = 2/T .\nTheorem 3.5 of Maillard (2016) states a similar result on the estimation of the unknown reward function from the RKHS. We improve upon it in the sense that the confidence bound in Theorem 2 is simultaneous over all x \u2208 D, while the bound has been shown only for a single, fixed x in the Kernel Least-squares setting. We are able to achieve this result by virtue of Theorem 1."}, {"heading": "4.1 Regret Bound of IGP-UCB", "text": "Theorem 3 Let \u03b4 \u2208 (0, 1), \u2016f\u2016k \u2264 B and \u03b5t is conditionally R-sub-Gaussian. Running IGP-UCB for a function f lying in the RKHS Hk(D), we obtain a regret bound of O (\u221a T (B \u221a \u03b3T + \u03b3T ) ) with high\nprobability. More precisely, with probability at least 1\u2212 \u03b4, RT = O ( B \u221a T\u03b3T + \u221a T\u03b3T (\u03b3T + ln(1/\u03b4)) ) .\nImprovement over GP-UCB. Srinivas et al. (2009), in the course of analyzing the GP-UCB algorithm, show that when the reward function lies in the RKHS Hk(D), GP-UCB obtains regret O (\u221a T (B \u221a \u03b3T +\n\u03b3T ln 3/2(T )) ) with high probability (see Theorem 3 therein for the exact bound). Furthermore, they assume\nthat the noise \u03b5t is uniformly bounded by \u03c3, while our sub-Gaussianity assumption (see Equation 1) is slightly more general, and we are able to obtain a O(ln3/2 T ) multiplicative factor improvement in the final regret bound thanks to the new self-normalized inequality (Theorem 1). Additionally, in our numerical experiments, we observe a significantly improved performance of IGP-UCB over GP-UCB, both on synthetically generated function, and on real-world sensor measurement data (see Section 6).\nComparison with KernelUCB. Valko et al. (2013) show that the cumulative regret of KernelUCB is O\u0303( \u221a d\u0303T ), where d\u0303, defined as the effective dimension, measures, in a sense, the number of principal directions over which the projection of the data in the RKHS is spread. They show that d\u0303 is at least as good as \u03b3T , precisely \u03b3T \u2265 \u2126(d\u0303 ln lnT ) and thus the regret bound of KernelUCB is roughly O\u0303( \u221a T\u03b3T ), which is \u221a \u03b3T factor better than IGP-UCB. However, KernelUCB requires the number of actions to be finite, so the regret bound is not applicable for infinite or continuum action spaces."}, {"heading": "4.2 Regret Bound of GP-TS", "text": "For technical reasons, we will analyze the following version of GP-TS. At each round t, the decision set used by GP-TS is restricted to be a unique discretization Dt of D with the property that |f(x)\u2212 f([x]t)| \u2264 1/t2 for all x \u2208 D, where [x]t is the closest point to x in Dt. (This can always be achieved by choosing a compact and convex domain D \u2282 [0, r]d and discretization Dt with size |Dt| = (BLrdt2)d such that\n\u2016x\u2212 [x]t\u20161 \u2264 rd/BLrdt2 = 1/BLt2 for all x \u2208 D, where L = sup x\u2208D sup j\u2208[d]\n( \u22022k(p,q) \u2202pj\u2202qj |p=q=x )1/2 ). This\nimplies, for every x \u2208 D,\n|f(x)\u2212 f([x]t)| \u2264 \u2016f\u2016k L \u2016x\u2212 [x]t\u20161 \u2264 1/t 2, (7)\nas any f \u2208 Hk(D) is Lipschitz continuous with constant \u2016f\u2016k L (De Freitas et al., 2012, Lemma 1).\nTheorem 4 (Regret bound for GP-TS) Let \u03b4 \u2208 (0, 1), D \u2282 [0, r]d be compact and convex, \u2016f\u2016k \u2264 B and {\u03b5t}t a conditionally R-sub-Gaussian sequence. Running GP-TS for a function f lying in the RKHS Hk(D)\nand with decision sets Dt chosen as above, with probability at least 1 \u2212 \u03b4, the regret of GP-TS satisfies RT = O (\u221a (\u03b3T + ln(2/\u03b4))d ln(BdT ) (\u221a T\u03b3T +B \u221a T ln(2/\u03b4) )) .\nComparison with IGP-UCB. Observe that regret scaling of GP-TS is O\u0303(\u03b3T \u221a dT ) which is a multiplica-\ntive \u221a d factor away from the bound O\u0303(\u03b3T \u221a T ) obtained for IGP-UCB and similar behavior is reflected in our simulations on synthetic data. The additional multiplicative factor of \u221a\nln(BdT ) in the regret bound of GP-TS is essentially a consequence of discretization. How to remove this extra logarithmic dependency, and make the analysis discretization-independent, remains an open question.\nRemark. The regret bound for GP-TS is inferior compared to IGP-UCB in terms of the dependency on dimension d, but to the best of our knowledge, Theorem 4 is the first (frequentist) regret guarantee of Thompson Sampling in the agnostic, non-parametric setting of infinite action spaces.\nLinear Models and a Matching Lower Bound. If the mean rewards are perfectly linear, i.e. if there exists a \u03b8 \u2208 Rd such that f(x) = \u03b8Tx for all x \u2208 D, then we are in the parametric setup, and one way of casting this in the kernelized framework is by using the linear kernel k(x, x\u2032) = xTx\u2032. For this kernel, \u03b3T = O(d lnT ), and the regret scaling of IGP-UCB is O\u0303(d \u221a T ) and that of GP-TS is O\u0303(d3/2 \u221a T ), which recovers the regret bounds of their linear, parametric analogues OFUL (Abbasi-Yadkori et al., 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively. Moreover, in this case d\u0303 = d, thus the regret of IGP-UCB is \u221a d factor away from that of KernelUCB. But the regret bound of KernelUCB also depends on the number of arms N , and if N is exponential in d, then it also suffers O\u0303(d \u221a T ) regret. We remark that a similar O(ln3/2 T ) factor improvement, as obtained by IGP-UCB over GP-UCB, was achieved in the linear parametric setting by Abbasi-Yadkori et al. (2011) in the OFUL algorithm, over its predecessor ConfidenceBall Dani et al. (2008). Finally we see that the for linear bandit problem with infinitely many actions, IGP-UCB attains the information theoretic lower bound of \u2126(d \u221a T ) (see Dani et al. (2008)), but GP-TS is a factor of \u221a d away from it."}, {"heading": "5. Overview of techniques", "text": "We briefly outline here the key arguments for all the theorems in Section 4. Formal proofs and auxiliary lemmas required are given in the supplementary material.\nProof Sketch for Theorem 1. It is convenient to assume that Kt, the induced kernel matrix at time t, is invertible, since this is where the crux of the argument lies. First we show that for any function g : D \u2192 R and for all t \u2265 0, thanks to the sub-Gaussian property (1), the process { Mgt := exp(\u03b5 T 1:tg1:t \u2212 12 \u2016g1:t\u2016 2 ) } t is a non-negative super-martingale with respect to the filtration Ft, where g1:t := [g(x1), . . . , g(xt)]T and in fact satisfies E [Mgt ] \u2264 1. The chief difficulty is to handle the behavior of Mt at a (random) stopping time, since the sizes of quantities such as \u03b51:t at the stopping time will be random.\nWe next construct a mixture martingale Mt by mixing M g t over g drawn from an independent GPD(0, k) Gaussian process, which is a measure over a large space of functions, i.e., the space RD. Then, by a change of measure argument, we show that this induces a mixture distribution which is essentially N (0,Kt) over any desired finite dimension t, thus obtaining Mt = 1\u221a\ndet(I+Kt) exp\n( 1 2 \u2016\u03b51:t\u2016 2 (I+K\u22121t ) \u22121 ) . Next from the\nfact that E [M\u03c4 ] \u2264 1 and from Markov\u2019s inequality, for any \u03b4 \u2208 (0, 1), we obtain\nP [ \u2016\u03b51:\u03c4\u20162(K\u22121\u03c4 +I)\u22121 > 2 ln (\u221a det(I +K\u03c4 )/\u03b4 )] \u2264 \u03b4.\nFinally, we lift this bound simultaneously for all t through a standard stopping time construction as in AbbasiYadkori et al. (2011).\nProof Sketch for Theorem 2. Here we sketch the special case of \u03b7 = 0, i.e. \u03bb = 1. Observe that |\u00b5t(x)\u2212 f(x)| is upper bounded by sum of two terms, P := \u2223\u2223kt(x)T (Kt + I)\u22121\u03b51:t\u2223\u2223 and Q :=\u2223\u2223kt(x)T (Kt + I)\u22121f1:t \u2212 f(x)\u2223\u2223. Now we observe that \u03c32t (x) = \u03d5(x)T (\u03a6Tt \u03a6t + I)\u22121\u03d5(x) and use this observation to show that P = \u2223\u2223\u03d5(x)T (\u03a6Tt \u03a6t + I)\u22121\u03a6Tt \u03b51:t\u2223\u2223 and Q = \u2223\u2223\u03d5(x)T (\u03a6Tt \u03a6t + I)\u22121f \u2223\u2223, which are\nin turn upper bounded by the terms \u03c3t(x) \u2016St\u2016V \u22121t and \u2016f\u2016k \u03c3t(x) respectively. Then the result follows using Theorem 1, along with the assumption that \u2016f\u2016k \u2264 B and the fact that 1 2 ln(det(I+Kt)) \u2264 \u03b3t almost surely (see Lemma 3) when Kt is invertible. Proof Sketch for Theorem 3. First from Theorem 2 and the choice of xt in Algorithm 1, we show that the instantaneous regret rt at round t is upper bounded by 2\u03b2t\u03c3t\u22121(xt) with probability at least 1\u2212 \u03b4. Then the result follows by essentially upper bounding the term \u2211T t=1 \u03c3t\u22121(xt) by O( \u221a T\u03b3T ) (Lemma 4 in the appendix). Proof Sketch for Theorem 4. We follow a similar approach given in Agrawal and Goyal (2013) to prove the regret bound of GP-TS. First observe that from our choice of discretization sets Dt, the instantaneous regret at round t is given by rt = f(x?) \u2212 f([x?]t) + f([x?]t) \u2212 f(xt) \u2264 1t2 + \u2206t(xt), where \u2206t(x) := f([x?]t)\u2212 f(x) and [x?]t is the closest point to x? in Dt. Now at each round t, after an action is chosen, our algorithm improves the confidence about true reward function f , via an update of \u00b5t(\u00b7) and kt(\u00b7, \u00b7). However, if we play a suboptimal arm, the regret suffered can be much higher than the improvement of our knowledge. To overcome this difficulty, at any round t, we divide the arms (in the present discretization Dt) into two groups: saturated arms, St, defined as those with \u2206t(x) > ct\u03c3t\u22121(x) and unsaturated otherwise, where ct is an appropriate constant (see Definition 1, 3). The idea is to show that the probability of playing a saturated arm is small and then bound the regret of playing an unsaturated arm in terms of standard deviation. This is useful because the inequality \u2211T t=1 \u03c3t\u22121(xt) \u2264 O( \u221a T\u03b3T ) (Lemma 4) allows us to bound the total regret due to unsaturated arms. First we lower bound the probability of playing an unsaturated arm at round t. We define a filtration F \u2032t\u22121 as the history Ht\u22121 up to round t \u2212 1 and prove that for \u201cmost\u201d (in a high probability sense) F \u2032t\u22121, P [ xt \u2208 Dt \\ St\n\u2223\u2223 F \u2032t\u22121] \u2265 p \u2212 1/t2, where p = 1/4e\u221a\u03c0 ( Lemma 9). This observation, along with concentration bounds for ft(x) and f(x) (Lemma 6) and \u201csmoothness\u201d of f (Equation 7), allow us to show that the expected regret at round t is upper bounded in terms of \u03c3t\u22121(xt), i.e. in terms of regret due to playing an unsaturated arm. More precisely, we show that for \u201cmost\u201d F \u2032t\u22121, E [ rt \u2223\u2223 F \u2032t\u22121] \u2264\n11ct p E [ \u03c3t\u22121(xt) \u2223\u2223 F \u2032t\u22121]+ 2B+1t2 (Lemma 10), and use it to prove thatXt ' rt\u2212 11ctp \u03c3t\u22121(xt)\u2212 2B+1t2 ; t \u2265 1 is a super-martingale difference sequence adapted to filtration {F \u2032t}t\u22651 (Lemma 12). Now, using the AzumaHoeffding inequality (Lemma 13), along with the bound on \u2211T t=1 \u03c3t\u22121(xt), we obtain the desired highprobability regret bound."}, {"heading": "6. Experiments", "text": "In this section we provide numerical results on both synthetically generated test functions and functions from real-world data. We compare GP-UCB, IGP-UCB and GP-TS with GP-EI and GP-PI11.\nSynthetic Test Functions. We use the following procedure to generate test functions from the RKHS. First we sample 100 points uniformly from the interval [0, 1] and use that as our decision set. Then we compute a kernel matrix K on those points and draw reward vector y \u223c N(0,K). Finally, the mean of the resulting posterior distribution is used as the test function f . We set noise parameter R2 to be 1% of function range and use \u03bb = R2. We used Squared Exponential kernel with lengthscale parameter l = 0.2 and Mate\u0301rn kernel with parameters \u03bd = 2.5, l = 0.2. Parameters \u03b2t, \u03b2\u0303t, vt of IGP-UCB, GP-UCB and GP-TS are chosen as given in Section 3, with \u03b4 = 0.1, B2 = fTKf and \u03b3t set according to theoretical upper bounds for corresponding kernels. We run each algorithm for T = 30000 iterations, over 25 independent trials (samples from the RKHS) and plot the average cumulative regret along with standard deviations (Figure 1). We see a significant improvement in the performance of IGP-UCB over GP-UCB. In fact IGP-UCB performs the best in the pool of competitors, while GP-TS also fares reasonably well compared to GP-UCB and GP-EI/GP-PI.\nWe next sample 25 random functions from the GP (0,K) and perform the same experiment (Figure 2) for both kernels with exactly same set of parameters. The relative performance of all methods is similar to\n11. GP-EI and PI perform similarly and thus are not separately distinguishable in the plots.\nthat in the previous experiment, which is the arguably harder \u201cagnostic\u201d setting of a fixed, unknown target function.\nStandard Test Functions. We consider 2 well-known synthetic benchmark functions for Bayesian Optimization: Rosenbrock and Hartman3 (see Azimi et al. (2012) for exact analytical expressions). We sample 100 d points uniformly from the domain of each benchmark function, d being the dimension of respective domain, as the decision set. We consider the Squared Exponential kernel with l = 0.2 and set all parameters exactly as in previous experiment. The cumulative regret for 25 independent trials on Rosenbrock and Hartman3 benchmarks is shown in Figure 3. We see GP-EI/PI perform better than the rest, while IGP-UCB and GP-TS show competitive performance. Here no algorithm is aware of the underlying kernel function, hence\nwe conjecture that the UCB- and TS- based algorithms are somewhat less robust on the choice of kernel than EI/PI.\nTemperature Sensor Data. We use temperature data12 collected from 54 sensors deployed in the Intel Berkeley Research lab between February 28th and April 5th, 2004 with samples collected at 30 second intervals. We tested all algorithms in the context of learning the maximum reading of the sensors collected between 8 am to 9 am. We take measurements of first 5 consecutive days (starting Feb. 28th 2004) to learn algorithm parameters. Following Srinivas et al. (2009), we calculate the empirical covariance matrix of the sensor measurements and use it as the kernel matrix in the algorithms. Here R2 is set to be 5% of the average empirical variance of sensor readings and other algorithm parameters is set similarly as in the previous experiment with \u03b3t = 1 (found via cross-validation). The functions for testing consist of one set of measurements from all sensors in the two following days and the cumulative regret is plotted over all such test functions. From Figure 4, we see that IGP-UCB and GP-UCB performs the same, while GP-TS outperforms all its competitors.\nLight Sensor Data. We take light sensor data collected in the CMU Intelligent Workplace in Nov 2005, which is available online as Matlab structure13 and contains locations of 41 sensors, 601 train samples and 192 test samples. We compute the kernel matrix, estimate the noise and set other algorithm parameters exactly as in the previous experiment. Here also GP-TS is found to perform better than the others, with IGP-UCB performing better than GP-EI/PI (Figure 4).\nRelated work. An alternative line of work pertaining toX -armed bandits Kleinberg et al. (2008); Bubeck et al. (2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u2126(T d+1 d+2 ) in Rd. Other Bayesian approaches to function optimization are GP-EI Moc\u030ckus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al. (2013) and noise-free De Freitas et al. (2012) settings. Other relevant work focuses on best arm identification problem in the Bayesian setup considering pure exploration Gru\u0308newa\u0308lder et al. (2010). For Thompson sampling (TS), Russo and Van Roy (2014) analyze the Bayesian\n12. http://db.csail.mit.edu/labdata/labdata.html 13. http://www.cs.cmu.edu/\u02dcguestrin/Class/10708-F08/projects/lightsensor.zip\nregret of TS, which includes the case where the target function is sampled from a GP prior. Our work obtains the first frequentist regret of TS for unknown, fixed functions from an RKHS."}, {"heading": "7. Conclusion", "text": "For bandit optimization, we have improved upon the existing GP-UCB algorithm, and introduced a new GPTS algorithm. The proposed algorithms perform well in practice both on synthetic and real-world data. An interesting case is when the kernel function is also not known to the algorithms a priori and needs to be learnt adaptively. Moreover, one can consider classes of time varying functions from the RKHS, and general reinforcement learning with GP techniques. There are also important questions on computational aspects of optimizing functions drawn from GPs."}, {"heading": "A. Proof of Theorem 1", "text": "For a function g : D \u2192 R and a sequence of reals n \u2261 (nt)\u221et=1, define for any t \u2265 0\nMg,nt = exp ( \u03b5T1:tg1:t,n \u2212 R2\n2 \u2016g1:t,n\u20162\n) ,\nwhere the vector g1:t,n := [g(x1) + n1, . . . , g(xt) + nt]T . We first establish the following technical result, which resembles Abbasi-Yadkori et al. (2011, Lemma 8).\nLemma 2 For fixed g and n, {Mg,nt }\u221et=0 is a super-martingale with respect to the filtration {Ft}\u221et=0.\nProof First, define\n\u2206g,nt := exp ( \u03b5t(g(xt) + nt)\u2212 R2\n2 (g(xt) + nt)\n2 ) .\nSince xt is Ft\u22121-measurable and \u03b5t is Ft-measurable, Mg,nt as well as \u2206 g,n t are Ft measurable. Also, by the conditional R-sub-Gaussianity of \u03b5t, we have\n\u2200\u03bb \u2208 R, E [ e\u03bb\u03b5t \u2223\u2223 Ft\u22121] \u2264 exp(\u03bb2R2 2 ) ,\nwhich in turn implies E [ \u2206g,nt \u2223\u2223 Ft\u22121] \u2264 1. We also have E [ Mg,nt\n\u2223\u2223 Ft\u22121] = E [ Mg,nt\u22121\u2206 g,n t \u2223\u2223 Ft\u22121] = Mg,nt\u22121E [\u2206g,nt \u2223\u2223 Ft\u22121] \u2264Mg,nt\u22121,\nshowing that {Mg,nt }\u221et=0 is a non-negative super-martingale and proving the lemma.\nNow, let \u03c4 be a stopping time with respect to the filtration {Ft}\u221et=0. By the convergence theorem for nonnegative super-martingales (Durrett, 2005), Mg\u221e = lim\nt\u2192\u221e Mgt exists almost surely, and thus M g \u03c4 is well-defined.\nNow let Qgt = M g min{\u03c4,t}, t \u2265 0, be a stopped version of {M g t }t. By Fatou\u2019s lemma (Durrett, 2005),\nE [Mg\u03c4 ] = E [\nlim t\u2192\u221e\nQgt\n] = E [ lim inf t\u2192\u221e Qgt ] \u2264 lim inf\nt\u2192\u221e E [Qgt ]\n= lim inf t\u2192\u221e\nE [ Mgmin{\u03c4,t} ] \u2264 1, (8)\nsince the stopped super-martingale ( Mgmin{\u03c4,t} ) t is also a super-martingale (Durrett, 2005).\nNow, let F\u221e be the \u03c3-algebra generated by {Ft}\u221et=0, and let N \u2261 (Nt)\u221et=1 be a sequence of independent and identically distributed Gaussian random variables with mean 0 and variance \u03b7, independent of F\u221e. Let h : D \u2192 R be a random function distributed according to the Gaussian process measure GPD(0, k), and independent of both F\u221e and (Nt)\u221et=1.\nFor each t \u2265 0, define Mt = E [ Mh,Nt \u2223\u2223 F\u221e]. In words, (Mt)t is a mixture of super-martingales of the form Mg,nt , and it is not hard to see that (Mt)t is also a (non-negative) super-martingale w.r.t. the filtration {Ft}t, hence M\u221e = lim\nt\u2192\u221e Mt is well-defined almost surely. We can write\nE [Mt] = E [ Mh,Nt ] = E [ E [ Mh,Nt \u2223\u2223 h,N]] \u2264 E [1] = 1 \u2200t. An argument similar to (8) also shows that E [M\u03c4 ] \u2264 1 for any stopping time \u03c4 . Now, without loss of generality, we assume R = 1 (this can always be achieved through appropriate scaling), and compute\nMt = E [ exp ( \u03b5T1:th1:t,N \u2212 1\n2 \u2016h1:t,N\u20162 ) \u2223\u2223 F\u221e] =\n\u222b RD \u222b Rt exp ( \u03b5T1:t([h(x1) . . . h(xt)] T + z)\u2212 1 2 \u2225\u2225[h(x1) . . . h(xt)]T + z\u2225\u22252 )d\u00b51(h)d\u00b52(z) =\n\u222b Rt exp ( \u03b5T1:t\u03bb\u2212 1 2 \u2016\u03bb\u20162 ) f(\u03bb)d\u03bb,\nwhere \u00b51 is the Gaussian process measure GPD(0, k) over the function space RD \u2261 {g : D \u2192 R}, \u00b52 is the multivariate Gaussian distribution on Rt with mean 0 and covariance \u03b7I where I is the identify, du is standard Lebesgue measure on Rt, and f is the density of the random vector [h(x1) . . . h(xt)]T + z, which is distributed as the multivariate Gaussian N (0,Kt + \u03b7I) given the sampled points x1, . . . , xt up to round t, where Kt is the induced kernel matrix at time t given by Kt(i, j) = k(xi, xj), 1 \u2264 i, j \u2264 t. (Note: Kt is not positive definite and invertible when there are repetitions among (x1, . . . , xt), but Kt + \u03b7I is.)\nThus, we have\nMt = 1\u221a\n(2\u03c0)t det(Kt + \u03b7I) \u222b Rt exp ( \u03b5T1:t\u03bb\u2212 \u2016\u03bb\u20162 2 \u2212 \u2016\u03bb\u20162(Kt+\u03b7I)\u22121 2 ) d\u03bb\n= exp\n( \u2016\u03b51:t\u20162\n2 ) \u221a\n(2\u03c0)t det(Kt + \u03b7I) \u222b Rt exp ( \u2212 \u2016\u03bb\u2212 \u03b51:t\u2016 2 2 \u2212 \u2016\u03bb\u20162(Kt+\u03b7I)\u22121 2 ) d\u03bb.\nNow for positive-definite matrices P and Q\n\u2016x\u2212 a\u20162P + \u2016x\u2016 2 Q = \u2225\u2225x\u2212 (P +Q)\u22121Pa\u2225\u22252 P+Q + \u2016a\u20162P \u2212 \u2016Pa\u2016 2 (P+Q)\u22121 .\nTherefore,\n\u2016\u03bb\u2212 \u03b51:t\u20162I + \u2016\u03bb\u2016 2 (Kt+\u03b7I)\u22121 = \u2225\u2225\u03bb\u2212 (I + (Kt + \u03b7I)\u22121)\u22121I\u03b51:t\u2225\u22252I+(Kt+\u03b7I)\u22121 + \u2016\u03b51:t\u20162I \u2212 \u2016I\u03b51:t\u20162(I+(Kt+\u03b7I)\u22121)\u22121 ,\nwhich yields\nMt = 1\u221a\n(2\u03c0)t det(Kt + \u03b7I) exp (1 2 \u2016\u03b51:t\u20162(I+(Kt+\u03b7I)\u22121)\u22121 ) \u00d7 \u222b Rt exp ( \u2212 1 2\n\u2225\u2225\u03bb\u2212 (I + (Kt + \u03b7I)\u22121)\u22121\u03b51:t\u2225\u22252I+(Kt+\u03b7I)\u22121 )d\u03bb =\n1\u221a det(Kt + \u03b7I) det((Kt + \u03b7I)\u22121 + I)\nexp (1\n2 \u2016\u03b51:t\u20162(I+(Kt+\u03b7I)\u22121)\u22121 ) =\n1\u221a det(I +Kt + \u03b7I)\nexp (1\n2 \u2016\u03b51:t\u20162(I+(Kt+\u03b7I)\u22121)\u22121\n) ,\nsince for any positive definite matrix A \u2208 Rt,\u222b Rt exp ( \u2212 1 2 (x\u2212 a)TA(x\u2212 a) ) dx = \u222b Rt exp ( \u2212 1 2 \u2016x\u2212 a\u20162A ) dx = \u221a (2\u03c0)t/ det(A).\nNow as E [M\u03c4 ] \u2264 1, using Markov\u2019s inequality gives, for any \u03b4 \u2208 (0, 1), P [ \u2016\u03b51:\u03c4\u20162((K\u03c4+\u03b7I)\u22121+I)\u22121 > 2 ln (\u221a det((1 + \u03b7)I +K\u03c4 )/\u03b4 )] = P [M\u03c4 > 1/\u03b4] < \u03b4E [M\u03c4 ] \u2264 \u03b4. (9)\nTo complete the proof, we now employ a stopping time construction as in Abbasi-Yadkori et al. (2011). For each t \u2265 0, define the \u2018bad\u2019 event\nBt(\u03b4) = { \u03c9 \u2208 \u2126 : \u2016\u03b51:t\u20162((Kt+\u03b7I)\u22121+I)\u22121 > 2 ln (\u221a det((1 + \u03b7)I +Kt)/\u03b4 )} ,\nso that\nP \u22c3 t\u22650 Bt(\u03b4)  = P [ \u2203t \u2265 0 : \u2016\u03b51:t\u20162((Kt+\u03b7I)\u22121+I)\u22121 \u2264 2 ln (\u221a det((1 + \u03b7)I +Kt)/\u03b4 )] ,\nwhich is the probability required to be bounded by \u03b4 in the statement of the theorem. Let \u03c4 \u2032 be the first time when the bad event Bt(\u03b4) happens, i.e., \u03c4 \u2032(\u03c9) := min{t \u2265 0 : \u03c9 \u2208 Bt(\u03b4)}, with min{\u2205} :=\u221e by convention. Clearly, \u03c4 \u2032 is a stopping time, and\u22c3 t\u22650 Bt(\u03b4) = {\u03c9 \u2208 \u2126 : \u03c4 \u2032(\u03c9) <\u221e}.\nTherefore, we can write\nP \u22c3 t\u22650 Bt(\u03b4)  = P [\u03c4 \u2032 <\u221e]\n= P [ \u2016\u03b51:\u03c4 \u2032\u20162((K\u03c4\u2032+\u03b7I)\u22121+I)\u22121 > 2 ln (\u221a det((1 + \u03b7)I +K\u03c4 \u2032)/\u03b4 ) , \u03c4 \u2032 <\u221e ] \u2264 P [ \u2016\u03b51:\u03c4 \u2032\u20162((K\u03c4\u2032+\u03b7I)\u22121+I)\u22121 > 2 ln (\u221a det((1 + \u03b7)I +K\u03c4 \u2032)/\u03b4 )] \u2264 \u03b4,\nby the inequality (9). When Kt is positive definite (and hence invertible) for each t \u2265 1, one can use a similar construction as in Part 1, with \u03b7 = 0 (i.e., N is the all-zeros sequence with probability 1), to recover the corresponding conclusion (6) with \u03b7 = 0.\nProof of Lemma 1 Define, for each time t, the t \u00d7\u221e matrix \u03a6t := [\u03d5(x1) \u00b7 \u00b7 \u00b7\u03d5(xt)]T , and observe that Vt = I + \u03a6Tt \u03a6t and Kt = \u03a6t\u03a6 T t . With this, we can compute\n\u2016St\u2016V \u22121t = S T t V \u22121 t St = t\u2211 s=1 \u03b5s\u03d5(xs) T ( I + \u03a6Tt \u03a6t )\u22121 t\u2211 s=1 \u03b5s\u03d5(xs)\n= \u03b5T1:t\u03a6t ( I + \u03a6Tt \u03a6t )\u22121 \u03a6Tt \u03b51:t\n= \u03b5T1:t\u03a6t\u03a6 T t ( \u03a6t\u03a6 T t + I )\u22121 \u03b51:t = \u03b5T1:tKt (Kt + I) \u22121 \u03b51:t\n= \u03b5T1:t ( K\u22121t + I )\u22121 \u03b51:t = \u2016\u03b51:t\u2016(K\u22121t +I)\u22121 ,\ncompleting the proof.\nB. Information Theoretic Results Lemma 3 For every t \u2265 0, the maximum information gain \u03b3t, for the points chosen by Algorithm 1 and 2 satisfy, almost surely, the following :\n\u03b3t \u2265 1\n2 ln(det(I + \u03bb\u22121Kt)),\n\u03b3t \u2265 1\n2 t\u2211 s=1 ln(1 + \u03bb\u22121\u03c32s\u22121(xs)).\nProof At round t after observing the reward vector y1:t at points At = {x1, . . . , xt} \u2282 D, the information gain - by the algorithm - about the unknown reward function f is given by the mutual information between f1:t and y1:t sampled at points At:\nI(y1:t; f1:t) = H(y1:t)\u2212H(y1:t \u2223\u2223 f1:t),\nwhere y1:t = f1:t + \u03b51:t = [y1, . . . , yt]T , f1:t = [f(x1), . . . , f(xt)]T and \u03b51:t = [\u03b51, . . . , \u03b5t]T . Clearly, given f1:t the randomness - as perceived by the algorithm - in y1:t are only in the noise vector \u03b51:t and thus\nH(y1:t \u2223\u2223 f1:t) = 1\n2 ln(det(2\u03c0e\u03bbv2I)) =\nt 2 log(2\u03c0e\u03bbv2),\nas \u03b51:t is assumed to follow the distributionN (0, \u03bbv2I) andH(N (\u00b5,\u03a3)) = 12 ln(det(2\u03c0e\u03a3)). Now y1:t sampled at pointsAt is believed to be distributed asN (0, v2(Kt+\u03bbI)), which givesH(y1:t) = 12 ln(det(2\u03c0ev\n2(\u03bbI+ Kt))) = t 2 log(2\u03c0e\u03bbv 2) + 12 ln(det(I + \u03bb \u22121Kt)), and therefore\nI(y1:t; f1:t) = 1\n2 ln(det(I + \u03bb\u22121Kt)). (10)\nAgain, conditioned on reward vector y1:s\u22121 observed at points As\u22121, the reward ys at round s observed at xs is believed to follow the distribution N (\u00b5s\u22121(xs), v2(\u03bb + \u03c32s\u22121(xs))), which gives H(ys \u2223\u2223 y1:s\u22121) = 1 2 ln(2\u03c0ev 2(\u03bb + \u03c32s\u22121(xs))) = 1 2 ln(2\u03c0e\u03bbv\n2) + 12 ln(1 + \u03bb \u22121\u03c32s\u22121(xs)). Now by chain rule H(y1:t) =\u2211t s=1H(ys \u2223\u2223 y1:s\u22121) = t2 ln(2\u03c0e\u03bbv2) + 12 \u2211ts=1 ln(1 + \u03bb\u22121\u03c32s\u22121(xs)), and therefore\nI(y1:t; f1:t) = 1\n2 t\u2211 s=1 ln(1 + \u03bb\u22121\u03c32s\u22121(xs)). (11)\nNow I(y1:t; f1:t) is a function of At \u2282 D, the random points chosen by the algorithm and thus\nI(y1:t; f1:t) \u2264 max A\u2282D:|A|=t I(yA; fA) = \u03b3t, a.s.,\nNow the proof follows from Equation 10 and 11.\nLemma 4 Let x1, . . . xt be the points selected by the algorithms. The sum of predictive standard deviation at those points can be expressed in terms of the maximum information gain. More precisely,\nT\u2211 t=1 \u03c3t\u22121(xt) \u2264 \u221a 4(T + 2)\u03b3T .\nProof First note that, by Cauchy-Schwartz inequality, \u2211T t=1 \u03c3t\u22121(xt) \u2264 \u221a T \u2211t t=1 \u03c3 2 t\u22121(xt). Now since 0 \u2264 \u03c32t\u22121(x) \u2264 1 for all x \u2208 D and by our choice of \u03bb = 1 + \u03b7, \u03b7 \u2265 0, we have \u03bb\u22121\u03c32t\u22121(xt) \u2264 2 ln(1+\u03bb\u22121\u03c32t\u22121(xt)), where in the last inequality we used the fact that for any 0 \u2264 \u03b1 \u2264 1, ln(1+\u03b1) \u2265 \u03b1/2. Thus we get \u03c32t\u22121(xt) \u2264 2\u03bb ln(1 + \u03bb\u22121\u03c32t\u22121(xt)). This implies\nT\u2211 t=1 \u03c3t\u22121(xt) \u2264 \u221a\u221a\u221a\u221a2T T\u2211 t=1 \u03bb ln(1 + \u03bb\u22121\u03c32t\u22121(xt)) \u2264 \u221a\u221a\u221a\u221a4T\u03bb T\u2211 t=1 1 2 ln(1 + \u03c32t\u22121(xt)) \u2264 \u221a 4T (1 + \u03b7)\u03b3T ,\nwhere the last inequality follows from Lemma 3. Now the result follows by choosing \u03b7 = 2/T ."}, {"heading": "C. Proof of Theorem 2", "text": "First define \u03d5(x) as k(x, \u00b7), where \u03d5 : Rd \u2192 H maps any point x in the primal space Rd to the RKHS H associated with kernel function k. For any two functions g, h \u2208 H , define the inner product \u3008g, h\u3009k as gTh and the RKHS norm \u2016g\u2016k as \u221a gT g. Now as the unknown reward function f lies in the RKHS Hk(D), these definitions along with reproducing property of the RKHS imply f(x) = \u3008f, k(x, \u00b7)\u3009k = \u3008f, \u03d5(x)\u3009k = fT\u03d5(x) and k(x, x\u2032) = \u3008k(x, \u00b7), k(x\u2032, \u00b7)\u3009k = \u3008\u03d5(x), \u03d5(x\u2032)\u3009k = \u03d5(x)T\u03d5(x\u2032) for all x, x\u2032 \u2208 D. Now defining \u03a6t := [ \u03d5(x1) T , . . . , \u03d5(xt) T ]T\n, we get the kernel matrix Kt = \u03a6t\u03a6Tt , kt(x) = \u03a6t\u03d5(x) for all x \u2208 D and f1:t = \u03a6tf . Since the matrices (\u03a6Tt \u03a6t + I) and (\u03a6t\u03a6 T t + \u03bbI) are strictly positive definite and (\u03a6 T t \u03a6t + \u03bbI)\u03a6Tt = \u03a6 T t (\u03a6t\u03a6 T t + \u03bbI), we have\n\u03a6Tt (\u03a6t\u03a6 T t + \u03bbI) \u22121 = (\u03a6Tt \u03a6t + \u03bbI) \u22121\u03a6Tt . (12)\nAlso from the definitions above (\u03a6Tt \u03a6t + \u03bbI)\u03d5(x) = \u03a6 T t kt(x) + \u03bb\u03d5(x), and thus from 12 we deduce that\n\u03d5(x) = \u03a6Tt (\u03a6t\u03a6 T t + \u03bbI) \u22121kt(x) + \u03bb(\u03a6 T t \u03a6t + \u03bbI) \u22121\u03d5(x),\nwhich gives\n\u03d5(x)T\u03d5(x) = kt(x) T (\u03a6t\u03a6 T t + \u03bbI) \u22121kt(x) + \u03bb\u03d5(x) T (\u03a6Tt \u03a6t + \u03bbI) \u22121\u03d5(x).\nThis implies\n\u03bb\u03d5(x)T (\u03a6Tt \u03a6t + \u03bbI) \u22121\u03d5(x) = k(x, x)\u2212 kt(x)T (Kt + \u03bbI)\u22121kt(x) = \u03c32t (x) (13)\nNow observe that\u2223\u2223f(x)\u2212 kt(x)T (Kt + \u03bbI)\u22121f1:t\u2223\u2223 = \u2223\u2223\u03d5(x)T f \u2212 \u03d5(x)T\u03a6Tt (\u03a6t\u03a6Tt + \u03bbI)\u22121\u03a6tf \u2223\u2223 =\n\u2223\u2223\u03d5(x)T f \u2212 \u03d5(x)T (\u03a6Tt \u03a6t + \u03bbI)\u22121\u03a6Tt \u03a6tf \u2223\u2223 =\n\u2223\u2223\u03bb\u03d5(x)T (\u03a6Tt \u03a6t + \u03bbI)\u22121f \u2223\u2223 \u2264\n\u2225\u2225\u03bb(\u03a6Tt \u03a6t + \u03bbI)\u22121\u03d5(x)\u2225\u2225k \u2016f\u2016k = \u2016f\u2016k \u221a \u03bb\u03d5(x)T (\u03a6Tt \u03a6t + \u03bbI) \u22121\u03bbI(\u03a6Tt \u03a6t + \u03bbI) \u22121\u03d5(x)\n\u2264 B \u221a \u03bb\u03d5(x)T (\u03a6Tt \u03a6t + \u03bbI) \u22121(\u03a6Tt \u03a6t + \u03bbI)(\u03a6 T t \u03a6t + \u03bbI) \u22121\u03d5(x)\n= B \u03c3t(x),\nwhere the second equality uses 12, the first inequality is by Cauchy-Schwartz and the final equality is from 13. Again see that\u2223\u2223kt(x)T (Kt + \u03bbI)\u22121\u03b51:t\u2223\u2223 = \u2223\u2223\u03d5(x)T\u03a6Tt (\u03a6t\u03a6Tt + \u03bbI)\u22121\u03b51:t\u2223\u2223\n= \u2223\u2223\u03d5(x)T (\u03a6Tt \u03a6t + \u03bbI)\u22121\u03a6Tt \u03b51:t\u2223\u2223\n\u2264 \u2225\u2225\u2225(\u03a6Tt \u03a6t + \u03bbI)\u22121/2\u03d5(x)\u2225\u2225\u2225\nk \u2225\u2225\u2225(\u03a6Tt \u03a6t + \u03bbI)\u22121/2\u03a6Tt \u03b51:t\u2225\u2225\u2225 k\n= \u221a \u03d5(x)T (\u03a6Tt \u03a6t + \u03bbI) \u22121\u03d5(x) \u221a (\u03a6Tt \u03b51:t) T (\u03a6Tt \u03a6t + \u03bbI) \u22121\u03a6Tt \u03b51:t\n= \u03bb\u22121/2\u03c3t(x) \u221a \u03b5T1:t\u03a6t\u03a6 T t (\u03a6t\u03a6 T t + \u03bbI) \u22121\u03b51:t\n= \u03bb\u22121/2\u03c3t(x) \u221a \u03b5T1:tKt(Kt + \u03bbI) \u22121\u03b51:t\nwhere the second equality is from 12, the first inequality is by Cauchy-Schwartz and the fourth inequality uses both 12 and 13. Now recall that, at round t, the posterior mean function \u00b5t(x) = kt(x)T (Kt + \u03bbI)\u22121y1:t = kt(x) T (Kt +\u03bbI) \u22121(f1:t + \u03b51:t), where f1:t = [ f(x1), . . . , f(xt) ]T and \u03b51:t = [ \u03b51, . . . , \u03b5t ]T . Thus we have\n|\u00b5t(x)\u2212 f(x)| \u2264 \u2223\u2223kt(x)T (Kt + \u03bbI)\u22121\u03b51:t\u2223\u2223+ \u2223\u2223f(x)\u2212 kt(x)T (Kt + \u03bbI)\u22121f1:t\u2223\u2223\n\u2264 \u03c3t(x) ( B + (1 + \u03b7)\u22121/2 \u221a \u03b5T1:tKt(Kt + (1 + \u03b7)I) \u22121\u03b51:t ) ,\nwhere we have used \u03bb = 1 + \u03b7, where \u03b7 \u2265 0 as stated in Theorem 1. Now observe that when K is invertible, K(K + I)\u22121 = ((K + I)K\u22121)\u22121 = (I +K\u22121)\u22121. Using K = Kt + \u03b7I , we get\n(Kt + \u03b7I)(Kt + (1 + \u03b7)I) \u22121 = ((Kt + \u03b7I) \u22121 + I)\u22121.\nNow see that\n\u03b5T1:tKt(Kt + (1 + \u03b7)I) \u22121\u03b51:t \u2264 \u03b5T1:t(Kt + \u03b7I)(Kt + (1 + \u03b7)I)\u22121\u03b51:t = \u03b5T1:t((Kt + \u03b7I)\u22121 + I)\u22121\u03b51:t\nNow using Theorem 1, for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, \u2200t \u2265 0,\u2200x \u2208 D, we obtain\n|\u00b5t(x)\u2212 f(x)| \u2264 \u03c3t(x) ( B + \u2016\u03b51:t\u2016(Kt+\u03b7I)\u22121+I)\u22121 ) \u2264 \u03c3t(x) ( B +R \u221a 2 ln \u221a det((1 + \u03b7)I +Kt)\n\u03b4\n) .\nNow observe that det((1 + \u03b7)I +Kt) = det(I + (1 + \u03b7)\u22121Kt) det((1 + \u03b7)I). Thus we have\nln(det((1 + \u03b7)I +Kt)) = ln(det(I + (1 + \u03b7) \u22121Kt)) + t ln(1 + \u03b7) \u2264 2\u03b3t + \u03b7t,\nfrom lemma 3. Now choosing \u03b7 = 2/T we have |\u00b5t(x)\u2212 f(x)| \u2264 \u03c3t(x) ( B + R \u221a 2 ( \u03b3t + 1 + ln(1/\u03b4) )) and hence the result follows."}, {"heading": "D. Analysis of IGP-UCB (Theorem 3)", "text": "Observe that at each round t \u2265 1, by the choice of xt in Algorithm 1, we have \u00b5t\u22121(xt) + \u03b2t\u03c3t\u22121(xt) \u2265 \u00b5t\u22121(x ?) + \u03b2t\u03c3t\u22121(x ?) and from Lemma 2, we have f(x?) \u2264 \u00b5t\u22121(x?) + \u03b2t\u03c3t\u22121(x?) and \u00b5t\u22121(xt) \u2212 f(xt) \u2264 \u03b2t\u03c3t\u22121(xt). Therefore for all t \u2265 1 with probability at least 1\u2212 \u03b4,\nrt = f(x ?)\u2212 f(xt)\n\u2264 \u03b2t\u03c3t\u22121(xt) + \u00b5t\u22121(xt)\u2212 f(xt) \u2264 2\u03b2t\u03c3t\u22121(xt),\nand hence T\u2211 t=1 rt \u2264 2\u03b2T T\u2211 t=1 \u03c3t\u22121(xt). Now from Lemma 4, T\u2211 t=1 \u03c3t\u22121(xt) = O( \u221a T\u03b3T ) and by definition\n\u03b2T \u2264 B +R \u221a 2(\u03b3T + 1 + ln(1/\u03b4)). Hence with probability at least 1\u2212 \u03b4,\nRT = T\u2211 t=1 rt = O ( B \u221a T\u03b3T + \u221a T\u03b3T (\u03b3T + ln(1/\u03b4)) ) ,\nand thus with high probability, RT = O (\u221a T (B \u221a \u03b3T + \u03b3T ) ) ."}, {"heading": "E. Analysis of GP-TS (Theorem 4)", "text": "Lemma 5 For any \u03b4 \u2208 (0, 1) and any finite subset D\u2032 of D,\nP [ \u2200x \u2208 D\u2032, |ft(x)\u2212 \u00b5t\u22121(x)| \u2264 vt \u221a 2 ln(|D\u2032| t2) \u03c3t\u22121(x) \u2223\u2223 Ht\u22121] \u2265 1\u2212 1/t2, for all possible realizations of historyHt\u22121.\nProof Fix x \u2208 D and t \u2265 1. Given historyHt\u22121, ft(x) \u223c N (\u00b5t\u22121(x), v2t \u03c32t\u22121(x)). Thus using Lemma B4 of Hoffman et al. (2013), for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4\n|ft(x)\u2212 \u00b5t\u22121(x)| \u2264 \u221a 2 ln(1/\u03b4) vt\u03c3t\u22121(x),\nand now applying union bound, |ft(x)\u2212 \u00b5t\u22121(x)| \u2264 vt \u221a 2 ln(|D\u2032| /\u03b4) \u03c3t\u22121(x) \u2200x \u2208 D\u2032\nholds with probability at least 1\u2212 \u03b4, given any possible realizations of history Ht\u22121. Now setting \u03b4 = 1/t2, the result follows.\nDefinition 1 Define For all t \u2265 1, c\u0303t = \u221a\n4 ln t+ 2d ln(BLrdt2) and ct = vt(1 + c\u0303t), where vt = B + R \u221a\n2(\u03b3t\u22121 + 1 + ln(2/\u03b4)). Clearly, ct increases with t.\nDefinition 2 Define Ef (t) as the event that for all x \u2208 D,\n|\u00b5t\u22121(x)\u2212 f(x)| \u2264 vt\u03c3t\u22121(x),\nand Eft(t) as the event that for all x \u2208 Dt,\n|ft(x)\u2212 \u00b5t\u22121(x)| \u2264 vtc\u0303t\u03c3t\u22121(x).\nDefinition 3 Define the set of saturated points St in discretization Dt at round t as\nSt := {x \u2208 Dt : \u2206t(x) > ct\u03c3t\u22121(x)},\nwhere \u2206t(x) := f([x?]t)\u2212 f(x), the difference between function values at the closest point to x? in Dt and at x. Clearly \u2206t([x?]t) = 0 for all t, and hence [x?]t \u2208 Dt is unsaturated at every t.\nDefinition 4 Define filtration F \u2032t\u22121 as the history until time t, i.e., F \u2032 t\u22121 = Ht\u22121. By definition, F \u2032 1 \u2286 F \u20322 \u2286 \u00b7 \u00b7 \u00b7 . Observe that given F \u2032t\u22121, the set St and the event Ef (t) are completely deterministic.\nLemma 6 Given any \u03b4 \u2208 (0, 1), P [ \u2200t \u2265 1, Ef (t) ] \u2265 1\u2212\u03b4/2 and for all possible filtrationsF \u2032t\u22121, P [ Eft(t) \u2223\u2223 F \u2032t\u22121] \u2265 1\u2212 1/t2.\nProof The probability bound for the event Ef (t) follows from Theorem 2 by replacing \u03b4 with \u03b42 and for the event Eft(t) follows from Lemma 5 by setting D\u2032 = Dt andHt\u22121 = F \u2032 t\u22121.\nLemma 7 (Gaussian Anti-concentration) For a Gaussian random variable X with mean \u00b5 and standard deviation \u03c3, for any \u03b2 > 0,\nP [ X \u2212 \u00b5 \u03c3 > \u03b2 ] \u2265 e \u2212\u03b22 4 \u221a \u03c0\u03b2 .\nLemma 8 For any filtration F \u2032t\u22121 such that Ef (t) is true,\nP [ ft(x) > f(x) \u2223\u2223 F \u2032t\u22121] \u2265 p, for any x \u2208 D, where p = 1\n4e \u221a \u03c0 .\nProof Fix any x \u2208 D. Given filtration F \u2032t\u22121, ft(x) is a Gaussian random variable with mean \u00b5t\u22121(x) and standard deviation vt\u03c3t\u22121(x) and since event Ef (t) is true, |\u00b5t\u22121(x)\u2212 f(x)| \u2264 c1,t\u03c3t\u22121(x). Now using the anti-concentration inequality in Lemma 7, we have\nP [ ft(x) > f(x) \u2223\u2223 F \u2032t\u22121] = P [ft(x)\u2212 \u00b5t\u22121(x)vt\u03c3t\u22121(x) > f(x)\u2212 \u00b5t\u22121(x)vt\u03c3t\u22121(x) \u2223\u2223 F \u2032t\u22121 ]\n\u2265 P [ ft(x)\u2212 \u00b5t\u22121(x)\nvt\u03c3t\u22121(x) > |f(x)\u2212 \u00b5t\u22121(x)| vt\u03c3t\u22121(x) \u2223\u2223 F \u2032t\u22121] \u2265 1\n4 \u221a \u03c0\u03b2t\ne\u2212\u03b8 2 t ,\nwhere, from Definition 2, \u03b8t = |f(x)\u2212\u00b5t\u22121(x)| vt\u03c3t\u22121(x)\n\u2264 1. Therefore P [ ft(x) > f(x) \u2223\u2223 F \u2032t\u22121] \u2265 14e\u221a\u03c0 , and hence the result follows.\nLemma 9 For any filtration F \u2032t\u22121 such that Ef (t) is true,\nP [ xt \u2208 Dt \\ St \u2223\u2223 F \u2032t\u22121] \u2265 p\u2212 1/t2. Proof At round t our algorithm chooses the point xt \u2208 Dt, at which the highest value of ft, within current decision set Dt, is attained. Now if ft([x?]t) is greater than ft(x) for all saturated points at round t, i.e.,ft([x?]t) > ft(x),\u2200x \u2208 St, then one of the unsaturated points (which includes [x?]t) in Dt must be played and hence xt \u2208 Dt \\ St. This implies\nP [ xt \u2208 Dt \\ St \u2223\u2223 F \u2032t\u22121] \u2265 P [ft([x?]t) > ft(x),\u2200x \u2208 St \u2223\u2223 F \u2032t\u22121] . (14) Now form Definition 3, \u2206t(x) > ct\u03c3t\u22121(x), for all x \u2208 St. Also if both the events Ef (t) and Eft(t) are true, then from Definition 1 and 2, ft(x) \u2264 f(x) + ct\u03c3t\u22121(x), for all x \u2208 Dt. Thus for all x \u2208 St, ft(x) < f(x) + \u2206t(x). Therefore, for any filtration F \u2032 t\u22121 such that E f (t) is true, either Eft(t) is false, or else for all x \u2208 St, ft(x) < f([x?]t). Hence, for any F \u2032 t\u22121 such that E f (t) is true,\nP [ ft([x ?]t) > ft(x),\u2200x \u2208 St \u2223\u2223 F \u2032t\u22121] \u2265 P [ft([x?]t) > f([x?]t) \u2223\u2223 Ft\u22121]\u2212P [Eft(t) \u2223\u2223 F \u2032t\u22121] \u2265 p\u22121/t2,\nwhere we have used Lemma 6 and Lemma 8. Now the proof follows from Equation 14.\nLemma 10 For any filtration F \u2032t\u22121 such that Ef (t) is true,\nE [ rt \u2223\u2223 F \u2032t\u22121] \u2264 11ctp E [\u03c3t\u22121(xt) \u2223\u2223 F \u2032t\u22121]+ 2B + 1t2 ,\nwhere rt is the instantaneous regret at round t.\nProof Let x\u0304t be the unsaturated point in Dt with smallest \u03c3t\u22121(x), i.e.,\nx\u0304t = argmin x\u2208Dt\\St \u03c3t\u22121(x). (15)\nSince \u03c3t\u22121(\u00b7) and St are deterministic given F \u2032 t\u22121, so is x\u0304t. Now for any F \u2032 t\u22121 such that E f (t) is true,\nE [ \u03c3t\u22121(xt) \u2223\u2223 F \u2032t\u22121] \u2265 E [\u03c3t\u22121(xt) \u2223\u2223 F \u2032t\u22121, xt \u2208 Dt \\ St]P [xt \u2208 Dt \\ St \u2223\u2223 F \u2032t\u22121] \u2265 \u03c3t\u22121(x\u0304t)(p\u2212 1/t2), (16)\nwhere we have used Equation 15 and Lemma 9. Now, if both the events Ef (t) and Eft(t) are true, then from Definition 1 and 2, f(x) \u2212 ct\u03c3t\u22121(x) \u2264 ft(x) \u2264 f(x) + ct\u03c3t\u22121(x), for all x \u2208 Dt. Using this observation along with Definition 3 and the facts that ft(xt) \u2265 ft(x) for all x \u2208 Dt and x\u0304t \u2208 Dt \\ St, we have\n\u2206t(xt) = f([x ?]t)\u2212 f(x\u0304t) + f(x\u0304t)\u2212 f(xt)\n\u2264 \u2206t(x\u0304t) + ft(x\u0304t) + ct\u03c3t\u22121(x\u0304t)\u2212 ft(xt) + ct\u03c3t\u22121(xt) \u2264 ct\u03c3t\u22121(x\u0304t) + ct\u03c3t\u22121(x\u0304t) + ct\u03c3t\u22121(xt) \u2264 ct ( 2\u03c3t\u22121(x\u0304t) + ct\u03c3t\u22121(xt) ) .\nTherefore, for any F \u2032t\u22121 such that Ef (t) is true, either \u2206t(xt) \u2264 ct ( 2\u03c3t\u22121(x\u0304t) + ct\u03c3t\u22121(xt) ) , or Eft(t) is false. Now from our assumption of bounded variance, for all x \u2208 D, |f(x)| \u2264 \u2016f\u2016k k(x, x) \u2264 B, and hence\n\u2206t(x) \u2264 2 sup x\u2208D |f(x)| \u2264 2B. Thus, using Equation 16, we get\nE [ \u2206t(xt) \u2223\u2223 F \u2032t\u22121] \u2264 E [ct(2\u03c3t\u22121(x\u0304t) + ct\u03c3t\u22121(xt)) \u2223\u2223 F \u2032t\u22121]+ 2BP [Eft(t) \u2223\u2223 F \u2032t\u22121] \u2264 2ct p\u2212 1/t2 E [ \u03c3t\u22121(xt)\n\u2223\u2223 F \u2032t\u22121]+ ctE [\u03c3t\u22121(xt) \u2223\u2223 F \u2032t\u22121]+ 2Bt2 \u2264 11ct p E [ \u03c3t\u22121(xt)\n\u2223\u2223 F \u2032t\u22121]+ 2Bt2 , (17) where in the last inequality we used that 1/(p\u2212 1/t2) \u2264 5/p, which holds trivially for t \u2264 4 and also holds for t \u2265 5, as t2 > 5e \u221a \u03c0. Now using Equation 7, we have the instantaneous regret at round t,\nrt = f(x ?)\u2212 f([x?]t) + f([x?]t)\u2212 f(xt) \u2264\n1 t2 + \u2206t(xt),\nand then taking conditional expectation on both sides, the result follows from Equation 17.\nDefinition 5 Let us define Y0 = 0, and for all t = 1, . . . , T :\nr\u0304t = rt \u00b7 I{Ef (t)},\nXt = r\u0304t \u2212 11ct p \u03c3t\u22121(xt)\u2212 2B + 1 t2 ,\nYt = t\u2211 s=1 Xs.\nDefinition 6 A sequence of random variables (Zt; t \u2265 0) is called a super-martingale corresponding to a filtration Ft, if for all t, Zt is Ft-measurable, and for t \u2265 1,\nE [ Zt \u2223\u2223 Ft\u22121] \u2264 Zt\u22121.\nLemma 11 (Azuma-Hoeffding Inequality) If a super-martingale (Zt; t \u2265 0), corresponding to filtration Ft, satisfies |Zt \u2212 Zt\u22121| \u2264 \u03b1t for some constant \u03b1t, for all t = 1, . . . , T , then for any \u03b4 \u2265 0,\nP ZT \u2212 Z0 \u2264 \u221a\u221a\u221a\u221a2 ln(1/\u03b4) T\u2211\nt=1\n\u03b12t\n \u2265 1\u2212 \u03b4.\nLemma 12 (Yt; t = 0, ..., T ) is a super-martingale process with respect to filtration F \u2032 t .\nProof From Definition 6, we need to prove that for all t \u2208 {1, . . . , T} and any possibleF \u2032t\u22121, E [ Yt \u2212 Yt\u22121 \u2223\u2223 F \u2032t\u22121] \u2264 0, i.e.\nE [ r\u0304t|F \u2032\nt\u22121\n] \u2264 11ct p E [ \u03c3t\u22121(xt)|F \u2032 t\u22121 ] + 2B + 1 t2 . (18)\nNow if F \u2032t\u22121 such thatEf (t) is false, then r\u0304t = rt \u00b7I{Ef (t)} = 0, and Equation 18 holds trivially. Moreover, for F \u2032t\u22121 such that both Ef (t) is true, Equation 18 follows from Lemma 10.\nLemma 13 Given any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4,\nRT = T\u2211 t=1 r(t) = 11cT p T\u2211 t=1 \u03c3t\u22121(xt) + (2B + 1)\u03c02 6 + (4B + 11)cT p \u221a 2T ln(2/\u03b4),\nwhere T is the total number of rounds played.\nProof First note that from Definition 5 for all t = 1, . . . , T ,\n|Yt \u2212 Yt\u22121| = |Xt| \u2264 |r\u0304t|+ 11ct p \u03c3t\u22121(xt) + 2B + 1 t2 .\nNow as r\u0304t \u2264 rt \u2264 2 sup x\u2208D |f(x)| \u2264 2B and \u03c32t\u22121(xt) \u2264 \u03c320(xt) \u2264 1, we have\n|Yt \u2212 Yt\u22121| \u2264 2B + 11ct p + 2B + 1 t2 \u2264 (4B + 11)ct p ,\nwhich follows from the fact that 2B \u2264 2Bct/p and also (2B + 1)/t2 \u2264 2Bct/p. Thus, we can apply Azuma-Hoeffding inequality (Lemma 11) to obtain that with probability at least 1\u2212 \u03b4/2,\nT\u2211 t=1 r\u0304t \u2264 T\u2211 t=1 11ct p \u03c3t\u22121(xt) + T\u2211 t=1 2B + 1 t2 + \u221a\u221a\u221a\u221a2 ln(2/\u03b4) T\u2211 t=1 (4B + 11)2c2t p2\n\u2264 11cT p T\u2211 t=1 \u03c3t\u22121(xt) + (2B + 1)\u03c02 6 + (4B + 11)cT p \u221a 2T ln(2/\u03b4),\nas by definition ct \u2264 cT for all t \u2208 {1, . . . , T}. Now, as the event Ef (t) holds holds for all t with probability at least 1 \u2212 \u03b4/2 (see Lemma 6), then from Definition 5, r\u0304t = rt for all t with probability at least 1 \u2212 \u03b4/2. Now by applying union bound, the result follows.\nProof of Theorem 4 From Lemma 4 we have, T\u2211 t=1 \u03c3t\u22121(xt) = O( \u221a T\u03b3T ). Also from Definition 1,\nCT \u2264 B +R \u221a 2(\u03b3T + 1 + ln(2/\u03b4)) + ( B +R \u221a 2(\u03b3T + 1 + ln(2/\u03b4)) )\u221a 4 lnT + 2d ln(BLrdT 2)\n= O (\u221a (\u03b3T + ln(2/\u03b4))(lnT + d ln(BdT )) +B \u221a d ln(BdT ) ) = O (\u221a (\u03b3T + ln(2/\u03b4))d ln(BdT ) ) .\nHence, from Lemma 13, with probability at least 1\u2212 \u03b4,\nRT = O (\u221a (\u03b3T + ln(2/\u03b4))d ln(BdT ) \u00b7 (\u221a T\u03b3T +B \u221a T ln(2/\u03b4) )) and thus with high probability,\nRT = O (\u221a T\u03b32T d ln(BdT ) +B \u221a T\u03b3T d ln(BdT ) ) = O (\u221a Td ln(BdT ) ( B \u221a \u03b3T + \u03b3T )) ."}, {"heading": "F. Recursive Updates of Posterior Mean and Covariance", "text": "We now describe a procedure to update the posterior mean and covariance function in a recursive fashion through the properties of Schur complement (Zhang (2006)) rather than evaluating Equation 2 and 3 at each round. Specifically for all t \u2265 1 we show the following:\n\u00b5t(x) = \u00b5t\u22121(x) + kt\u22121(x, xt)\n\u03bb+ \u03c32t\u22121(xt) (yt \u2212 \u00b5t\u22121(xt)), (19)\nkt(x, x \u2032) = kt\u22121(x, x \u2032)\u2212 kt\u22121(x, xt)kt\u22121(xt, x \u2032)\n\u03bb+ \u03c32t\u22121(xt) , (20)\n\u03c32t (x) = \u03c3 2 t\u22121(x)\u2212\nk2t\u22121(x, xt)\n\u03bb+ \u03c32t\u22121(xt) . (21)\nThese update rules make our algorithms easy to implement and we are not aware of any literature which explicitly states or uses these relations. First we write the matrix Kt + \u03bbI as [ A B C D ] , where A = Kt\u22121 + \u03bbI , B = kt\u22121(xt), C = BT and D = \u03bb+ k(xt, xt). Now using Schur\u2019s complement we get[ A B C D ]\u22121 = [ A\u22121 +A\u22121B\u03b2CA\u22121 \u2212A\u22121B\u03b2 \u2212\u03b2CA\u22121 \u03b2\n] = [ A\u22121 + \u03b2A\u22121BBTA\u22121 \u2212\u03b2A\u22121B\n\u2212\u03b2BTA\u22121 \u03b2 ] = [ A\u22121 + \u03b2\u03b3 \u2212\u03b2\u03b1 \u2212\u03b2\u03b1T \u03b2 ] ,\nwhere \u03b2 = (D \u2212 CA\u22121B)\u22121 = 1/(D \u2212 BTA\u22121B), \u03b3 = A\u22121BBTA\u22121 and \u03b1 = A\u22121B. Therefore we have\n\u00b5t(x) = kt(x) T (Kt + \u03bbI) \u22121y1:t\n= [ kt\u22121(x) T k(xt, x) ] [A\u22121 + \u03b2\u03b3 \u2212\u03b2\u03b1 \u2212\u03b2\u03b1T \u03b2 ] [ y1:t\u22121 yt ] = kt\u22121(x) T (A\u22121 + \u03b2\u03b3)y1:t\u22121 \u2212 \u03b2k(xt, x)\u03b1T y1:t\u22121 \u2212 \u03b2yt\u03b1T kt\u22121(x) + \u03b2ytk(xt, x)\n= kt\u22121(x) TA\u22121y1:t\u22121 + \u03b2 ( kt\u22121(x) T \u03b3y1:t\u22121 \u2212 k(xt, x)\u03b1T y1:t\u22121 \u2212 yt\u03b1T kt\u22121(x) + ytk(xt, x) ) ,\nwhere\nkt\u22121(x) TA\u22121y1:t\u22121 = kt\u22121(x) T (Kt\u22121 + \u03bbI) \u22121y1:t\u22121 = \u00b5t\u22121(x),\nkt\u22121(x) T \u03b3y1:t\u22121 = kt\u22121(x) TA\u22121kt\u22121(xt)kt\u22121(xt) TA\u22121y1:t\u22121 = ( kt\u22121(xt) TA\u22121kt\u22121(x) ) \u00b5t\u22121(xt),\n\u03b1T y1:t\u22121 = kt\u22121(xt) TA\u22121y1:t\u22121 = \u00b5t\u22121(xt),\n\u03b1T kt\u22121(x) = kt\u22121(xt) TA\u22121kt\u22121(x).\nThus we have \u00b5t(x) = \u00b5t\u22121(x) + \u03b2 ( kt\u22121(xt) TA\u22121kt\u22121(x) (\u00b5t\u22121(xt)\u2212 yt) + k(xt, x) (yt \u2212 \u00b5t\u22121(xt)) )\n= \u00b5t\u22121(x) + \u03b2 (yt \u2212 \u00b5t\u22121(xt)) ( k(xt, x)\u2212 kt\u22121(xt)TA\u22121kt\u22121(x) ) = \u00b5t\u22121(x) + \u03b2kt\u22121(xt, x)(yt \u2212 \u00b5t\u22121(xt)).\nNow as\nD \u2212BTA\u22121B = \u03bb+ k(xt, xt)\u2212 kt\u22121(xt)T (Kt\u22121 + \u03bbI)\u22121kt\u22121(xt) = \u03bb+ \u03c32t\u22121(xt),\nputting \u03b2 = 1/(\u03bb+ \u03c32t\u22121(xt)), we obtain Equation 19. Again observe that\nkt(x, x \u2032) = k(x, x\u2032)\u2212 kt(x)T (Kt + \u03bbI)\u22121kt(x\u2032) = k(x, x\u2032)\u2212 kt\u22121(x)TA\u22121kt\u22121(x\u2032) + \u03b2 ( kt\u22121(x) T \u03b3kt\u22121(x \u2032)\u2212 k(xt, x)\u03b1T kt\u22121(x\u2032)\u2212 k(xt, x\u2032)\u03b1T kt\u22121(x) + k(xt, x)k(xt, x\u2032) ) .\nNow we have\nk(x, x\u2032)\u2212 kt\u22121(x)TA\u22121kt\u22121(x\u2032) = k(x, x\u2032)\u2212 kt\u22121(x)T (Kt\u22121 + \u03bbI)\u22121kt\u22121(x\u2032) = kt\u22121(x, x\u2032),\nalso\nkt\u22121(x) T \u03b3kt\u22121(x \u2032)\u2212 k(xt, x)\u03b1T kt\u22121(x\u2032) = kt\u22121(x)TA\u22121kt\u22121(xt)kt\u22121(xt)TA\u22121kt\u22121(x\u2032)\u2212 k(xt, x)kt\u22121(xt)TA\u22121kt\u22121(x\u2032) = (kt\u22121(x)\nTA\u22121kt\u22121(xt)\u2212 k(xt, x))kt\u22121(xt)TA\u22121kt\u22121(x\u2032) = \u2212kt\u22121(xt, x)kt\u22121(xt)TA\u22121kt\u22121(x\u2032),\nand\nk(xt, x)k(xt, x \u2032)\u2212 k(xt, x\u2032)\u03b1T kt\u22121(x) = k(xt, x\u2032)(k(xt, x)\u2212 kt\u22121(xt)TA\u22121kt\u22121(x))\n= k(xt, x \u2032)kt\u22121(xt, x).\nPutting all these together we get\nkt(x, x \u2032) = kt\u22121(x, x \u2032)\u2212 \u03b2 ( k(xt, x \u2032)kt\u22121(xt, x)\u2212 kt\u22121(xt, x)kt\u22121(xt)TA\u22121kt\u22121(x\u2032) )\n= kt\u22121(x, x \u2032)\u2212 \u03b2 ( kt\u22121(xt, x) ( k(xt, x \u2032)\u2212 kt\u22121(xt)TA\u22121kt\u22121(x\u2032) ))\n= kt\u22121(x, x \u2032)\u2212 \u03b2kt\u22121(xt, x)kt\u22121(xt, x\u2032).\nNow Equation 20 and 21 follows by using \u03b2 = 1/(\u03bb+ \u03c32t\u22121(x)) and \u03c3 2 t (x) = kt(x, x)."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In COLT,", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In ICML,", "citeRegEx": "Agrawal and Goyal.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2013}, {"title": "Online stochastic optimization under correlated bandit feedback", "author": ["Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill"], "venue": "In ICML,", "citeRegEx": "Azar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2014}, {"title": "Hybrid batch bayesian optimization", "author": ["Javad Azimi", "Ali Jalali", "Xiaoli Fern"], "venue": "arXiv preprint arXiv:1202.5597,", "citeRegEx": "Azimi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Azimi et al\\.", "year": 2012}, {"title": "Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms", "author": ["Omar Besbes", "Assaf Zeevi"], "venue": "Operations Research,", "citeRegEx": "Besbes and Zeevi.,? \\Q2009\\E", "shortCiteRegEx": "Besbes and Zeevi.", "year": 2009}, {"title": "Time-varying gaussian process bandit optimization", "author": ["Ilija Bogunovic", "Jonathan Scarlett", "Volkan Cevher"], "venue": "arXiv preprint arXiv:1601.06650,", "citeRegEx": "Bogunovic et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bogunovic et al\\.", "year": 2016}, {"title": "Simple regret for infinitely many armed bandits", "author": ["Alexandra Carpentier", "Michal Valko"], "venue": "In ICML,", "citeRegEx": "Carpentier and Valko.,? \\Q2015\\E", "shortCiteRegEx": "Carpentier and Valko.", "year": 2015}, {"title": "Power control in wireless cellular networks", "author": ["Mung Chiang", "Prashanth Hande", "Tian Lan", "Chee Wei Tan"], "venue": "Foundations and Trends in Networking,", "citeRegEx": "Chiang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2008}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P Hayes", "Sham M Kakade"], "venue": "In COLT,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Exponential regret bounds for gaussian process bandits with deterministic observations", "author": ["Nando De Freitas", "Alex Smola", "Masrour Zoghi"], "venue": "arXiv preprint arXiv:1206.6457,", "citeRegEx": "Freitas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Freitas et al\\.", "year": 2012}, {"title": "High-dimensional gaussian process bandits", "author": ["Josip Djolonga", "Andreas Krause", "Volkan Cevher"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Djolonga et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Djolonga et al\\.", "year": 2013}, {"title": "Probability: Theory and Examples", "author": ["Rick Durrett"], "venue": null, "citeRegEx": "Durrett.,? \\Q2005\\E", "shortCiteRegEx": "Durrett.", "year": 2005}, {"title": "Thompson sampling for complex online problems", "author": ["Aditya Gopalan", "Shie Mannor", "Yishay Mansour"], "venue": "In ICML,", "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2014}, {"title": "Safe exploration for optimization with gaussian processes", "author": ["Alkis Gotovos", "ETHZ CH", "Joel W Burdick"], "venue": null, "citeRegEx": "Gotovos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gotovos et al\\.", "year": 2015}, {"title": "Regret bounds for gaussian process bandit problems", "author": ["Steffen Gr\u00fcnew\u00e4lder", "Jean-Yves Audibert", "Manfred Opper", "John Shawe-Taylor"], "venue": "In AISTATS,", "citeRegEx": "Gr\u00fcnew\u00e4lder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gr\u00fcnew\u00e4lder et al\\.", "year": 2010}, {"title": "Exploiting correlation and budget constraints in bayesian multi-armed bandit optimization", "author": ["Matthew W Hoffman", "Bobak Shahriari", "Nando de Freitas"], "venue": "arXiv preprint arXiv:1303.6746,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Thompson sampling: An asymptotically optimal finite-time analysis", "author": ["Emilie Kaufmann", "Nathaniel Korda", "R\u00e9mi Munos"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Contextual gaussian process bandit optimization", "author": ["Andreas Krause", "Cheng S Ong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krause and Ong.,? \\Q2011\\E", "shortCiteRegEx": "Krause and Ong.", "year": 2011}, {"title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise", "author": ["Harold J Kushner"], "venue": "Journal of Basic Engineering,", "citeRegEx": "Kushner.,? \\Q1964\\E", "shortCiteRegEx": "Kushner.", "year": 1964}, {"title": "Self-normalization techniques for streaming confident regression. 2016", "author": ["Odalric-Ambrym Maillard"], "venue": null, "citeRegEx": "Maillard.,? \\Q2016\\E", "shortCiteRegEx": "Maillard.", "year": 2016}, {"title": "On bayesian methods for seeking the extremum", "author": ["J Mo\u010dkus"], "venue": "In Optimization Techniques IFIP Technical Conference,", "citeRegEx": "Mo\u010dkus.,? \\Q1975\\E", "shortCiteRegEx": "Mo\u010dkus.", "year": 1975}, {"title": "Gaussian processes for machine learning", "author": ["Carl Edward Rasmussen", "Christopher KI Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Linearly parameterized bandits", "author": ["Paat Rusmevichientong", "John N. Tsitsiklis"], "venue": "Math. Oper. Res.,", "citeRegEx": "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong and Tsitsiklis.", "year": 2010}, {"title": "Learning to optimize via posterior sampling", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo and Roy.,? \\Q2014\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2014}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "MIT press,", "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Practical reinforcement learning in continuous spaces", "author": ["William D Smart", "Leslie Pack Kaelbling"], "venue": "In ICML,", "citeRegEx": "Smart and Kaelbling.,? \\Q2000\\E", "shortCiteRegEx": "Smart and Kaelbling.", "year": 2000}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["Niranjan Srinivas", "Andreas Krause", "Sham M Kakade", "Matthias Seeger"], "venue": "arXiv preprint arXiv:0912.3995,", "citeRegEx": "Srinivas et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2009}, {"title": "Finite-time analysis of kernelised contextual bandits", "author": ["Michal Valko", "Nathaniel Korda", "R\u00e9mi Munos", "Ilias Flaounas", "Nelo Cristianini"], "venue": "arXiv preprint arXiv:1309.6869,", "citeRegEx": "Valko et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Valko et al\\.", "year": 2013}, {"title": "Optimization as estimation with gaussian processes in bandit settings", "author": ["Zi Wang", "Bolei Zhou", "Stefanie Jegelka"], "venue": "In International Conf. on Artificial and Statistics (AISTATS),", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Bayesian optimization in high dimensions via random embeddings", "author": ["Ziyu Wang", "Masrour Zoghi", "Frank Hutter", "David Matheson", "N Freitas"], "venue": "AAAI Press/International Joint Conferences on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "The Schur complement and its applications, volume 4", "author": ["Fuzhen Zhang"], "venue": "Springer Science & Business Media,", "citeRegEx": "Zhang.,? \\Q2006\\E", "shortCiteRegEx": "Zhang.", "year": 2006}], "referenceMentions": [{"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al.", "startOffset": 188, "endOffset": 212}, {"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al.", "startOffset": 188, "endOffset": 296}, {"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al.", "startOffset": 188, "endOffset": 324}, {"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al. (2008). A typical feature of such problems is a large, or potentially infinite, domain of decision points or covariates (prices, actions, transmit powers), together with only partial and noisy observability of the associated outcomes (demand, state/reward, communication rate); reward/loss information is revealed only for decisions that are chosen.", "startOffset": 188, "endOffset": 390}, {"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al. (2008). A typical feature of such problems is a large, or potentially infinite, domain of decision points or covariates (prices, actions, transmit powers), together with only partial and noisy observability of the associated outcomes (demand, state/reward, communication rate); reward/loss information is revealed only for decisions that are chosen. This often makes it hard to balance exploration and exploitation, as available knowledge must be transferred efficiently from a finite set of observations so far to estimates of the values of infinitely many decisions. A classic case in point is that of the canonical stochastic MAB with finitely many arms, where the effort to optimize scales with the total number of arms or decisions; the effect of this is catastrophic for large or infinite arm sets. With suitable structure in the values or rewards of arms, however, the challenge of sequential optimization can be efficiently addressed. Parametric bandits, especially linearly parameterized bandits Rusmevichientong and Tsitsiklis (2010), represent a well-studied class of structured decision making settings.", "startOffset": 188, "endOffset": 1427}, {"referenceID": 4, "context": "Introduction Optimization over large domains under uncertainty is an important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics Besbes and Zeevi (2009), reinforcement learning with continuous state/action spaces Kaelbling et al. (1996); Smart and Kaelbling (2000), and power control in wireless communication Chiang et al. (2008). A typical feature of such problems is a large, or potentially infinite, domain of decision points or covariates (prices, actions, transmit powers), together with only partial and noisy observability of the associated outcomes (demand, state/reward, communication rate); reward/loss information is revealed only for decisions that are chosen. This often makes it hard to balance exploration and exploitation, as available knowledge must be transferred efficiently from a finite set of observations so far to estimates of the values of infinitely many decisions. A classic case in point is that of the canonical stochastic MAB with finitely many arms, where the effort to optimize scales with the total number of arms or decisions; the effect of this is catastrophic for large or infinite arm sets. With suitable structure in the values or rewards of arms, however, the challenge of sequential optimization can be efficiently addressed. Parametric bandits, especially linearly parameterized bandits Rusmevichientong and Tsitsiklis (2010), represent a well-studied class of structured decision making settings. Here, every arm corresponds to a known, finite dimensional vector (its feature vector), and its expected reward is assumed to be an unknown linear function of its feature vector. This allows for a large, or even infinite, set of arms all lying in space of finite dimension, say d, and a rich line of work gives algorithms that attain sublinear regret with a polynomial dependence on the dimension, e.g., Confidence Ball Dani et al. (2008), OFUL Abbasi-Yadkori et al.", "startOffset": 188, "endOffset": 1938}, {"referenceID": 0, "context": "(2008), OFUL Abbasi-Yadkori et al. (2011) (a strengthening of Confidence Ball) and Thompson sampling for linear bandits", "startOffset": 13, "endOffset": 42}, {"referenceID": 0, "context": "The inequality generalizes a corresponding self-normalized bound for martingales in finite dimension proven by Abbasi-Yadkori et al. (2011). \u2022 Empirical comparisons of the algorithms developed above, with other GP-based algorithms, are presented, over both synthetic and real-world setups, demonstrating performance improvements of the proposed algorithms, as well as their performance under misspecification.", "startOffset": 111, "endOffset": 140}, {"referenceID": 0, "context": "This is a mild assumption on the noise (it holds, for instance, for distributions bounded in [\u2212R,R]) and is standard in the bandit literature Abbasi-Yadkori et al. (2011); Agrawal and Goyal (2013).", "startOffset": 142, "endOffset": 171}, {"referenceID": 0, "context": "This is a mild assumption on the noise (it holds, for instance, for distributions bounded in [\u2212R,R]) and is standard in the bandit literature Abbasi-Yadkori et al. (2011); Agrawal and Goyal (2013). Regret.", "startOffset": 142, "endOffset": 197}, {"referenceID": 0, "context": "This is a mild assumption on the noise (it holds, for instance, for distributions bounded in [\u2212R,R]) and is standard in the bandit literature Abbasi-Yadkori et al. (2011); Agrawal and Goyal (2013). Regret. The goal of an algorithm is to maximize its cumulative reward or alternatively minimize its cumulative regret \u2013 the loss incurred due to not knowing f \u2019s maximum point beforehand. Let x \u2208 argmaxx\u2208D f(x) be a maximum point of f (assuming the maximum is attained). The instantaneous regret incurred at time t is rt = f(x )\u2212f(xt), and the cumulative regret in a time horizon T (not necessarily known a priori) is defined to be RT = \u2211T t=1 rt. A sub-linear growth of RT in T signifies that RT /T \u2192 0 as T \u2192 \u221e, or vanishing per-round regret. Regularity Assumptions. Attaining sub-linear regret is impossible in general for arbitrary reward functions f and domains D, and thus some regularity assumptions are in order. In what follows, we assume that D is compact. The smoothness assumption we make on the reward function f is motivated by Gaussian processes4 and their associated reproducing kernel Hilbert spaces (RKHSs, see Sch\u00f6lkopf and Smola (2002)).", "startOffset": 142, "endOffset": 1154}, {"referenceID": 1, "context": "These assumptions are required to make the regret bounds scale-free and are standard in the literature Agrawal and Goyal (2013). Instead if k(x, x) \u2264 c or \u2016f\u2016k \u2264 cB, then our regret bounds would increase by a factor of c.", "startOffset": 103, "endOffset": 128}, {"referenceID": 29, "context": "In general, thus, this represents a misspecified prior and noise model, also termed the agnostic setting by Srinivas et al. (2009). The proposed algorithms, to follow, assume the knowledge of only the sub-Gaussianity parameter R, kernel function k and upper bound B on the RKHS norm of f .", "startOffset": 108, "endOffset": 131}, {"referenceID": 29, "context": "For a compact subset D of R, \u03b3T is O((lnT )) and O(T d(d+1)/(2\u03bd+d(d+1)) lnT ), respectively, for the Squared Exponential and Mat\u00e9rn kernels (Srinivas et al. (2009)), depending only polylogarithmically on the time T .", "startOffset": 141, "endOffset": 164}, {"referenceID": 29, "context": "Srinivas et al. (2009) have proposed the GP-UCB algorithm, and Valko et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "Srinivas et al. (2009) have proposed the GP-UCB algorithm, and Valko et al. (2013) the KernelUCB algorithm, for sequentially optimizing reward functions lying in the RKHS Hk(D).", "startOffset": 0, "endOffset": 83}, {"referenceID": 29, "context": "Srinivas et al. (2009) have proposed the GP-UCB algorithm, and Valko et al. (2013) the KernelUCB algorithm, for sequentially optimizing reward functions lying in the RKHS Hk(D). Both algorithms play an arm at time t using the rule: xt = argmaxx\u2208D \u03bct\u22121(x) + \u03b2\u0303t\u03c3t\u22121(x). GP-UCB uses the exploration parameter \u03b2\u0303t = \u221a 2B2 + 300\u03b3t\u22121 ln (t/\u03b4), with \u03bb set to \u03c3, where \u03c3 is additionally assumed to be a known, uniform (i.e., almost-sure) upper bound on all noise variables \u03b5t (see Theorem 3 in Srinivas et al. (2009)).", "startOffset": 0, "endOffset": 510}, {"referenceID": 1, "context": "2 Gaussian Process Thompson sampling (GP-TS) Our second algorithm, GP-TS (Algorithm 2), inspired by the success of Thompson sampling for standard and parametric bandits (Agrawal and Goyal, 2012; Kaufmann et al., 2012; Gopalan et al., 2014; Agrawal and Goyal, 2013), uses the time-varying scale parameter vt = B +R \u221a 2(\u03b3t\u22121 + 1 + ln(2/\u03b4)) and operates as follows.", "startOffset": 169, "endOffset": 264}, {"referenceID": 18, "context": "2 Gaussian Process Thompson sampling (GP-TS) Our second algorithm, GP-TS (Algorithm 2), inspired by the success of Thompson sampling for standard and parametric bandits (Agrawal and Goyal, 2012; Kaufmann et al., 2012; Gopalan et al., 2014; Agrawal and Goyal, 2013), uses the time-varying scale parameter vt = B +R \u221a 2(\u03b3t\u22121 + 1 + ln(2/\u03b4)) and operates as follows.", "startOffset": 169, "endOffset": 264}, {"referenceID": 13, "context": "2 Gaussian Process Thompson sampling (GP-TS) Our second algorithm, GP-TS (Algorithm 2), inspired by the success of Thompson sampling for standard and parametric bandits (Agrawal and Goyal, 2012; Kaufmann et al., 2012; Gopalan et al., 2014; Agrawal and Goyal, 2013), uses the time-varying scale parameter vt = B +R \u221a 2(\u03b3t\u22121 + 1 + ln(2/\u03b4)) and operates as follows.", "startOffset": 169, "endOffset": 264}, {"referenceID": 2, "context": "2 Gaussian Process Thompson sampling (GP-TS) Our second algorithm, GP-TS (Algorithm 2), inspired by the success of Thompson sampling for standard and parametric bandits (Agrawal and Goyal, 2012; Kaufmann et al., 2012; Gopalan et al., 2014; Agrawal and Goyal, 2013), uses the time-varying scale parameter vt = B +R \u221a 2(\u03b3t\u22121 + 1 + ln(2/\u03b4)) and operates as follows.", "startOffset": 169, "endOffset": 264}, {"referenceID": 0, "context": "Theorem 1 represents the kernelized generalization of the finite-dimensional result of Abbasi-Yadkori et al. (2011), and we recover their result under the special case of a linear kernel: \u03c6(x) = x for all x \u2208 R.", "startOffset": 87, "endOffset": 116}, {"referenceID": 0, "context": "Theorem 1 represents the kernelized generalization of the finite-dimensional result of Abbasi-Yadkori et al. (2011), and we recover their result under the special case of a linear kernel: \u03c6(x) = x for all x \u2208 R. We remark that when \u03c6 is a mapping to a finite-dimensional Hilbert space, the argument of AbbasiYadkori et al. (2011, Theorem 1) can be lifted to establish Theorem 1, but it breaks down in the generalized, infinite-dimensional RKHS setting, as the self-normalized bound in their paper has an explicit, growing dependence on the feature dimension. Specifically, the method of mixtures (de la Pena et al., 2009) or Laplace method, as dubbed by Maillard (2016) (Lemma 5.", "startOffset": 87, "endOffset": 670}, {"referenceID": 24, "context": ", Rasmussen and Williams (2006). 9.", "startOffset": 2, "endOffset": 32}, {"referenceID": 22, "context": "5 of Maillard (2016) states a similar result on the estimation of the unknown reward function from the RKHS.", "startOffset": 5, "endOffset": 21}, {"referenceID": 29, "context": "Srinivas et al. (2009), in the course of analyzing the GP-UCB algorithm, show that when the reward function lies in the RKHS Hk(D), GP-UCB obtains regret O (\u221a T (B \u221a \u03b3T + \u03b3T ln (T )) ) with high probability (see Theorem 3 therein for the exact bound).", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "Srinivas et al. (2009), in the course of analyzing the GP-UCB algorithm, show that when the reward function lies in the RKHS Hk(D), GP-UCB obtains regret O (\u221a T (B \u221a \u03b3T + \u03b3T ln (T )) ) with high probability (see Theorem 3 therein for the exact bound). Furthermore, they assume that the noise \u03b5t is uniformly bounded by \u03c3, while our sub-Gaussianity assumption (see Equation 1) is slightly more general, and we are able to obtain a O(ln T ) multiplicative factor improvement in the final regret bound thanks to the new self-normalized inequality (Theorem 1). Additionally, in our numerical experiments, we observe a significantly improved performance of IGP-UCB over GP-UCB, both on synthetically generated function, and on real-world sensor measurement data (see Section 6). Comparison with KernelUCB. Valko et al. (2013) show that the cumulative regret of KernelUCB is \u00d5( \u221a d\u0303T ), where d\u0303, defined as the effective dimension, measures, in a sense, the number of principal directions over which the projection of the data in the RKHS is spread.", "startOffset": 0, "endOffset": 821}, {"referenceID": 0, "context": "For this kernel, \u03b3T = O(d lnT ), and the regret scaling of IGP-UCB is \u00d5(d \u221a T ) and that of GP-TS is \u00d5(d \u221a T ), which recovers the regret bounds of their linear, parametric analogues OFUL (Abbasi-Yadkori et al., 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively.", "startOffset": 188, "endOffset": 217}, {"referenceID": 2, "context": ", 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively.", "startOffset": 37, "endOffset": 62}, {"referenceID": 0, "context": "For this kernel, \u03b3T = O(d lnT ), and the regret scaling of IGP-UCB is \u00d5(d \u221a T ) and that of GP-TS is \u00d5(d \u221a T ), which recovers the regret bounds of their linear, parametric analogues OFUL (Abbasi-Yadkori et al., 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively. Moreover, in this case d\u0303 = d, thus the regret of IGP-UCB is \u221a d factor away from that of KernelUCB. But the regret bound of KernelUCB also depends on the number of arms N , and if N is exponential in d, then it also suffers \u00d5(d \u221a T ) regret. We remark that a similar O(ln T ) factor improvement, as obtained by IGP-UCB over GP-UCB, was achieved in the linear parametric setting by Abbasi-Yadkori et al. (2011) in the OFUL algorithm, over its predecessor ConfidenceBall Dani et al.", "startOffset": 189, "endOffset": 699}, {"referenceID": 0, "context": "For this kernel, \u03b3T = O(d lnT ), and the regret scaling of IGP-UCB is \u00d5(d \u221a T ) and that of GP-TS is \u00d5(d \u221a T ), which recovers the regret bounds of their linear, parametric analogues OFUL (Abbasi-Yadkori et al., 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively. Moreover, in this case d\u0303 = d, thus the regret of IGP-UCB is \u221a d factor away from that of KernelUCB. But the regret bound of KernelUCB also depends on the number of arms N , and if N is exponential in d, then it also suffers \u00d5(d \u221a T ) regret. We remark that a similar O(ln T ) factor improvement, as obtained by IGP-UCB over GP-UCB, was achieved in the linear parametric setting by Abbasi-Yadkori et al. (2011) in the OFUL algorithm, over its predecessor ConfidenceBall Dani et al. (2008). Finally we see that the for linear bandit problem with infinitely many actions, IGP-UCB attains the information theoretic lower bound of \u03a9(d \u221a T ) (see Dani et al.", "startOffset": 189, "endOffset": 777}, {"referenceID": 0, "context": "For this kernel, \u03b3T = O(d lnT ), and the regret scaling of IGP-UCB is \u00d5(d \u221a T ) and that of GP-TS is \u00d5(d \u221a T ), which recovers the regret bounds of their linear, parametric analogues OFUL (Abbasi-Yadkori et al., 2011) and Linear Thompson sampling (Agrawal and Goyal, 2013), respectively. Moreover, in this case d\u0303 = d, thus the regret of IGP-UCB is \u221a d factor away from that of KernelUCB. But the regret bound of KernelUCB also depends on the number of arms N , and if N is exponential in d, then it also suffers \u00d5(d \u221a T ) regret. We remark that a similar O(ln T ) factor improvement, as obtained by IGP-UCB over GP-UCB, was achieved in the linear parametric setting by Abbasi-Yadkori et al. (2011) in the OFUL algorithm, over its predecessor ConfidenceBall Dani et al. (2008). Finally we see that the for linear bandit problem with infinitely many actions, IGP-UCB attains the information theoretic lower bound of \u03a9(d \u221a T ) (see Dani et al. (2008)), but GP-TS is a factor of \u221a d away from it.", "startOffset": 189, "endOffset": 949}, {"referenceID": 1, "context": "We follow a similar approach given in Agrawal and Goyal (2013) to prove the regret bound of GP-TS.", "startOffset": 38, "endOffset": 63}, {"referenceID": 4, "context": "We consider 2 well-known synthetic benchmark functions for Bayesian Optimization: Rosenbrock and Hartman3 (see Azimi et al. (2012) for exact analytical expressions).", "startOffset": 111, "endOffset": 131}, {"referenceID": 17, "context": "Following Srinivas et al. (2009), we calculate the empirical covariance matrix of the sensor measurements and use it as the kernel matrix in the algorithms.", "startOffset": 10, "endOffset": 33}, {"referenceID": 11, "context": "An alternative line of work pertaining toX -armed bandits Kleinberg et al. (2008); Bubeck et al.", "startOffset": 58, "endOffset": 82}, {"referenceID": 11, "context": "An alternative line of work pertaining toX -armed bandits Kleinberg et al. (2008); Bubeck et al. (2011); Carpentier and Valko (2015); Azar et al.", "startOffset": 58, "endOffset": 104}, {"referenceID": 5, "context": "(2011); Carpentier and Valko (2015); Azar et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure.", "startOffset": 37, "endOffset": 56}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R.", "startOffset": 37, "endOffset": 150}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al.", "startOffset": 37, "endOffset": 400}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al.", "startOffset": 37, "endOffset": 422}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al.", "startOffset": 37, "endOffset": 449}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al.", "startOffset": 37, "endOffset": 508}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al.", "startOffset": 37, "endOffset": 549}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al.", "startOffset": 37, "endOffset": 569}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al.", "startOffset": 37, "endOffset": 607}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al.", "startOffset": 37, "endOffset": 642}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al. (2013) and noise-free De Freitas et al.", "startOffset": 37, "endOffset": 683}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al. (2013) and noise-free De Freitas et al. (2012) settings.", "startOffset": 37, "endOffset": 723}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al. (2013) and noise-free De Freitas et al. (2012) settings. Other relevant work focuses on best arm identification problem in the Bayesian setup considering pure exploration Gr\u00fcnew\u00e4lder et al. (2010). For Thompson sampling (TS), Russo and Van Roy (2014) analyze the Bayesian", "startOffset": 37, "endOffset": 873}, {"referenceID": 3, "context": "(2011); Carpentier and Valko (2015); Azar et al. (2014) studies continuum-armed bandits with smoothness structure. For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms based on discretizing the domain yield nontrivial regret guarantees, of order \u03a9(T d+1 d+2 ) in R. Other Bayesian approaches to function optimization are GP-EI Mo\u010dkus (1975), GP-PI Kushner (1964), GP-EST Wang et al. (2016) and GP-UCB, including the contextual Krause and Ong (2011), high-dimensional Djolonga et al. (2013); Wang et al. (2013), time-varying Bogunovic et al. (2016) safety-aware Gotovos et al. (2015), budget-constraint Hoffman et al. (2013) and noise-free De Freitas et al. (2012) settings. Other relevant work focuses on best arm identification problem in the Bayesian setup considering pure exploration Gr\u00fcnew\u00e4lder et al. (2010). For Thompson sampling (TS), Russo and Van Roy (2014) analyze the Bayesian", "startOffset": 37, "endOffset": 927}, {"referenceID": 12, "context": "By the convergence theorem for nonnegative super-martingales (Durrett, 2005), M \u221e = lim t\u2192\u221e M t exists almost surely, and thus M g \u03c4 is well-defined.", "startOffset": 61, "endOffset": 76}, {"referenceID": 12, "context": "By Fatou\u2019s lemma (Durrett, 2005), E [M \u03c4 ] = E [ lim t\u2192\u221e Qgt ] = E [ lim inf t\u2192\u221e Qgt ]", "startOffset": 17, "endOffset": 32}, {"referenceID": 12, "context": "t is also a super-martingale (Durrett, 2005).", "startOffset": 29, "endOffset": 44}, {"referenceID": 0, "context": "(9) To complete the proof, we now employ a stopping time construction as in Abbasi-Yadkori et al. (2011). For each t \u2265 0, define the \u2018bad\u2019 event", "startOffset": 76, "endOffset": 105}, {"referenceID": 16, "context": "Thus using Lemma B4 of Hoffman et al. (2013), for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4 |ft(x)\u2212 \u03bct\u22121(x)| \u2264 \u221a 2 ln(1/\u03b4) vt\u03c3t\u22121(x), and now applying union bound,", "startOffset": 23, "endOffset": 45}, {"referenceID": 33, "context": "Recursive Updates of Posterior Mean and Covariance We now describe a procedure to update the posterior mean and covariance function in a recursive fashion through the properties of Schur complement (Zhang (2006)) rather than evaluating Equation 2 and 3 at each round.", "startOffset": 199, "endOffset": 212}], "year": 2017, "abstractText": "We consider the stochastic bandit problem with a continuous set of arms, with the expected reward function over the arms assumed to be fixed but unknown. We provide two new Gaussian process-based algorithms for continuous bandit optimization \u2013 Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and derive corresponding regret bounds. Specifically, the bounds hold when the expected reward function belongs to the reproducing kernel Hilbert space (RKHS) that naturally corresponds to a Gaussian process kernel used as input by the algorithms. Along the way, we derive a new self-normalized concentration inequality for vectorvalued martingales of arbitrary, possibly infinite, dimension. Finally, experimental evaluation and comparisons to existing algorithms on synthetic and real-world environments are carried out that highlight the favorable gains of the proposed strategies in many cases.", "creator": "LaTeX with hyperref package"}}}