{"id": "1704.00774", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality for Increased Model Capacity and Performance With No Computational Overhead", "abstract": "Increasing saw capability of infections neural customers (RNN) not involves augments, type of the retrieve filtration, minor entered makes significant increase an spatial cost. An alternative common following humiliations proteins tensor connect (RNTN ), including decreases adequate when large-scale similarities beneath texture temperature for made vocabulary spelling. The disadvantage of RNTNs is fact mental subject scales linearly this loanwords size, although usually bringing people taking thought - normally language models. In this books, obviously need restricted forgetfulness linkage nonlinearity networks (r - RNTN) which reserve backgrounds destroying mesh weights for stops subtle sounds already sharing good single breaking under temperature for balmier understood. Perplexity epidemiological using the Penn Treebank corpus show instance frist - RNTNs improve code uses performance over full RNNs carry never a small one-third of this parameters include ad-supported RNTNs.", "histories": [["v1", "Mon, 3 Apr 2017 19:17:58 GMT  (22kb)", "http://arxiv.org/abs/1704.00774v1", null], ["v2", "Sat, 15 Apr 2017 02:59:14 GMT  (23kb)", "http://arxiv.org/abs/1704.00774v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexandre salle", "aline villavicencio"], "accepted": false, "id": "1704.00774"}, "pdf": {"name": "1704.00774.pdf", "metadata": {"source": "CRF", "title": "Restricted Recurrent Neural Tensor Networks", "authors": ["Alexandre Salle", "Aline Villavicencio"], "emails": ["atsalle@inf.ufrgs.br", "avillavicencio@inf.ufrgs.br"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n00 77\n4v 1\n[ cs\n.C L\n] 3\nA pr\n2 01\n7\nral networks (RNN) usually involves augmenting the size of the hidden layer, resulting in a significant increase of computational cost. An alternative is the recurrent neural tensor network (RNTN), which increases capacity by employing distinct hidden layer weights for each vocabulary word. The disadvantage of RNTNs is that memory usage scales linearly with vocabulary size, which can reach millions for word-level language models. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations using the Penn Treebank corpus show that r-RNTNs improve language model performance over standard RNNs using only a small fraction of the parameters of unrestricted RNTNs."}, {"heading": "1 Introduction", "text": "Sequence modeling is an important part of natural language processing, with applications in machine translation, speech recognition, and language modeling. The task consists of estimating the conditional probability of a symbol given the sequence of symbols that preceded it. Recurrent neural networks (RNN), which compute their next output conditioned on a previously stored hidden state (or memory), are a natural solution to sequence modeling.\nMikolov et al. (2010) applied RNNs to wordlevel language modeling (we refer to this models as s-RNN), outperforming traditional n-gram methods. One issue with the s-RNN is increas-\ning its capacity (number of tunable parameters) to be able to model more complex distributions. The trivial way to do is by increasing the size H of the hidden (or recurrent) layer, but this results in a significant increase in computational cost, which is O(H2).\nSutskever et al. (2011) increased the performance of their character-level language model by proposing a multiplicative RNN (m-RNN), the factored approximation of a recurrent neural tensor network (RNTN), which maps each symbol to separate hidden layer weights, referred to as recurrence matrices for the rest of the paper. Besides increasing model capacity while keeping computation constant, this approach has another motivation: viewing the RNN\u2019s hidden state as being transformed by each new symbol in the sequence, it is intuitive that different symbols will transform the network\u2019s hidden state in different ways (Sutskever et al., 2011). Various works studying compositionality (how to compose words and phrases into dense vector distributions) similarly argue that some words are better modeled by matrices than by vectors (Baroni and Zamparelli, 2010; Socher et al., 2012). Unfortunately, having separate recurrence matrices for each symbol requires memory that is linear in the symbol vocabulary size (|V |). This is not an issue for characterlevel models, which have small vocabularies, but is prohibitive for word-level models which can have vocabulary size in the millions if we consider surface forms.\nIn this paper, we propose the Restricted RNTN (r-RNTN) which uses only K < |V | recurrence matrices. Given that |V | words must be assigned K matrices, we map the most frequent K \u2212 1 words to the first K \u2212 1 matrices, and share the K-th matrix among the remaining words. This mapping is driven by the statistical intuition that frequent words are more likely to appear in di-\nverse contexts and so require richer modeling, and by the greater presence of predicates and function words among the most frequent words in standard corpora like the Corpus of Contemporary American English (COCA) (Davies, 2009). As a result, adding K matrices to the s-RNN both increases model capacity and satisfies the idea that in modeling compositionality some words are better represented by matrices. Results show that r-RNTNs improve language model performance over s-RNNs even for small K with no computational overhead, and even for small K approximate the performance of RNTNs using a fraction of the parameters.\nThis paper is organized as follow: we first review related work (\u00a72). We then present r-RNTNs (\u00a73), describe the evaluation method (\u00a74) and discuss results (\u00a75), concluding with final remarks and suggestions for future work."}, {"heading": "2 Related Work", "text": "We focus our study on related work that addresses language modeling via RNNs, word representation, and conditional computation.\nGiven a sequence of words (x1, ..., xT ), a language model gives the probability P (xt|x1...t\u22121) for t \u2208 [1, T ]. By using a RNN, Mikolov et al. (2010) created the language model (which we refer to as s-RNN) given by:\nht = \u03c3(Whxxt +Whhht\u22121 + bh) (1)\nP (xt|x1...t\u22121) = x T t Softmax(Wohht + bo) (2)\nwhere ht is the hidden state, \u03c3(z) is the logistic function, Whx is the H \u00d7 V embedding matrix, Whh is the H \u00d7H recurrence matrix, and bh and bo are bias terms. As mentioned earlier, computation is O(H2), so increasing model capacity by increasing H quickly becomes intractable.\nThe RNTN proposed by Sutskever et al. (2011) is nearly identical to the s-RNN, but the recurrence matrix in eq. (1) is replaced by a tensor as follows:\nht = \u03c3(Whxxt +W i(xt) hh ht\u22121 + bh) (3)\nwhere i(z) maps a hot-one encoded vector to its integer representation. The Whh tensor is therefore composed of |V | recurrence matrices, and at each step of sequence processing the matrix corresponding to the current input is used to transform the hidden state. In the same paper, the authors proposed m-RNN, a factorization of the Whh tensor to reduce the number of parameters, but like\nthe RNTN, memory still grows linearly with |V |.1 The RNTN has the property that input symbols have both a vector representation given by the embedding, and a matrix representation given by the recurrence matrix, unlike the s-RNN, where symbols are limited to vector representations.\nThe integration of both vector and matrix representations has been discussed by Baroni and Zamparelli (2010) and Socher et al. (2012) but with a focus on representation learning and not sequence modeling. For instance, Baroni and Zamparelli (2010) argue for nouns to be represented as vectors and adjectives as matrices, where for a given attributive adjective its representation is derived from its occurrences with different nouns in corpora (e.g. green apple/wall/leaf ), also incorporating polysemous cases (green apple vs. green initiative).\nIrsoy and Cardie (2014) used m-RNNs for the task of sentiment classification and obtained equal or better performance than sRNNs. Methods which use conditional computation (Cho and Bengio, 2014; Bengio et al., 2015; Shazeer et al., 2017) are similar to RNTNs and r-RNTNs, but rather than use a static mapping, these methods train gating functions which do the mapping. Although these methods can potentially learn better policies, they are significantly more complex and offer less insight than our method into why a given mapping is better than some other.\nThere are many other improvements to sRNNs, the most notable of which is the use of Long Term Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), but these are too numerous to list here and for the most part complementary to our proposed method."}, {"heading": "3 Restricted Recurrent Neural Tensor Networks", "text": "To balance expressiveness and computational cost, we propose restricting the size of the recurrence tensor in the RNTN such that memory does not grow linearly with vocabulary size while still keeping dedicated matrix representations for a subset of words in the vocabulary. We call these Restricted Recurrent Neural Tensor Networks (rRNTN), which modify eq. (3) as follows:\nht = \u03c3(Whxxt +W f(i(xt)) hh ht\u22121 + b f(i(xt)) h ) (4)\n1It should be noted that the r-RNTN can also benefit from factorization and it is something we will explore in the future.\nwhere Whh is a tensor of K < |V | matrices of size H \u00d7H , bh is aK \u00d7H bias matrix with rows indexed by f . The function f(w) maps each vocabulary word to an integer between 1 and K .\nIn this work we use the following definition for\nf :\nf(w) = min(rank(w),K) (5)\nwhere rank(w) is the rank of word w when the vocabulary is sorted by decreasing order of unigram frequency.\nThis is an intuitive choice because words which appear more often in the corpus tend to have more variable contexts, so it makes sense to dedicate a large part of model capacity to them. A second argument is that frequent words tend to be predicates and function words. We can imagine that predicates and function words transform the meaning of the current hidden state of the RNN through matrix multiplication, whereas nouns, for example, add meaning through vector addition, following Baroni and Zamparelli (2010)."}, {"heading": "4 Materials", "text": "As done in other language modeling work (Mikolov et al., 2011; Mnih and Teh, 2012; Zaremba et al., 2014; Mikolov et al., 2014; Shazeer et al., 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al., 1994) using the same pre-processing as Mikolov et al. (2011). This corpus is split into training, validation, and test sets containing 929K words, 73K words, and 82K words respectively. The vocabulary is restricted to the most frequent 10000 words and all other words are mapped to the \u201c\u3008unk\u3009\u201d token.\nFor an r-RNTN withH = 100, we vary the tensor size K from 1, which corresponds to the sRNN, all the way up to 10000, which corresponds to the unrestricted RNTN.As a simple way to evaluate our choice of rank-based mapping function f , we compare it to the following pseudo-random variant:\nfmod(w) = rank(w) mod K (6)\nWe also compare results to an s-RNN withH = 150, which has the same number of parameters as an r-RNTN with H = 100 and K = 100.\nWe split each sentence into 20 word subsequences and run stochastic gradient descent via\nbackpropagation through time (Werbos, 1990) for 20 steps, only reseting the RNN\u2019s hidden state between sentences. We did not use mini-batching so as to simplify the implementation. The learning rate was initialized at 0.1 and halved when the ratio of the validation perplexity between successive epochs was less than 1.003. We early-stopped training when validation improvement fell below this ratio for 5 consecutive epochs.\nWe found that both the r-RNTN and RNTN were easily able to overfit the training data. While L2 regularization did little to remedy this issue, we were able to overcome it by using Dropout (Srivastava et al., 2014) with p = 0.5 after the activations ht of the hidden layer."}, {"heading": "5 Results", "text": "The results are presented in figs. 1 and 2 and a few examples are detailed in table 1.\nComparing the r-RNTN to the baseline s-RNN with H = 100, we see that as model capacity grows with K, test set perplexity drops, showing that r-RNTN is an effective way to increase model capacity with no additional computational cost. As expected, the f mapping outperforms the pseudo-random baseline fmod mapping at smaller K . AsK increases, we see a convergence of both mappings. This can be explained by the fact that at large K , the most frequent words are sharing ma-\ntrices with infrequent words because of the modulus operation in eq. (6). As infrequent words are rarely observed, frequent words dominate the matrix updates and approximate having distinct matrices, as they would have with the f mapping.\nIt is remarkable that even with as K small as 100, the r-RNTN approaches the performance of the RNTN with a small fraction of the parameters. This reinforces our hypothesis that complex transformation modeling afforded by distinct matrices is needed for frequent words, but not so much for infrequent words which can be well represented by a shared matrix and a distinct vector embedding. As shown in table 1, with an equal number of parameters, the r-RNTN with f mapping outperforms the s-RNN with a bigger hidden layer. It appears that heuristically allocating increased model capacity as done by the f based r-RNTN is a better way to increase performance than simply increasing hidden layer size, which also incurs\na computational penalty."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we proposed restricting the size of recurrent neural tensor networks by mapping frequent words to distinct matrices and infrequent words to shared matrices, a model we call restricted recurrent neural tensor networks (rRNTNs).\nThis model was motivated by the need to increase RNN model capacity without increasing computational costs, while also satisfying the compositionality ideas discussed by Baroni and Zamparelli (2010) and Socher et al. (2012) which suggest that some words are better modeled by matrices rather than vectors, and vice versa. We achieved both goals by pruning the size of the recurrent neural tensor network described by Sutskever et al. (2011) via sensible word-tomatrix mapping. Results validated our hypothesis that frequent words benefit from richer, dedicated modeling and this is reflected in large perplexity improvements for low values of K .\nr-RNTNs are mostly complementary to other improvements in RNNs. In future work, we will use r-RNTNs with LSTMs to confirm that similar gains can be obtained with different models. We also wish to apply r-RNTNs to other computationally intensive sequence modeling tasks such as neural machine translation, where we believe added model capacity without additional computational costs is an interesting proposition that can be reflected in improvements in translation quality."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Associ-", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "Conditional computation in neural networks for faster models", "author": ["Emmanuel Bengio", "Pierre-Luc Bacon", "Joelle Pineau", "Doina Precup."], "venue": "arXiv preprint arXiv:1511.06297 .", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning", "author": ["Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.7362 .", "citeRegEx": "Cho and Bengio.,? 2014", "shortCiteRegEx": "Cho and Bengio.", "year": 2014}, {"title": "The 385+ million word corpus of contemporary american english (1990\u20132008+): Design, architecture, and linguistic insights", "author": ["Mark Davies."], "venue": "International journal of corpus linguistics 14(2):159\u2013190.", "citeRegEx": "Davies.,? 2009", "shortCiteRegEx": "Davies.", "year": 2009}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Modeling compositionality with multiplicative recurrent neural networks", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "arXiv preprint arXiv:1412.6577 .", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary A. Marcinkiewicz."], "venue": "Computational Linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1994", "shortCiteRegEx": "Marcus et al\\.", "year": 1994}, {"title": "Empirical evaluation and combination of advanced languagemodeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernock\u00fd."], "venue": "INTERSPEECH. pages 605\u2013608.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee W. Teh."], "venue": "arXiv preprint arXiv:1206.6426 .", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "author": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean."], "venue": "arXiv preprint arXiv:1701.06538 .", "citeRegEx": "Shazeer et al\\.,? 2017", "shortCiteRegEx": "Shazeer et al\\.", "year": 2017}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher DManning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "Mikolov et al. (2010) applied RNNs to wordlevel language modeling (we refer to this models as s-RNN), outperforming traditional n-gram methods.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Besides increasing model capacity while keeping computation constant, this approach has another motivation: viewing the RNN\u2019s hidden state as being transformed by each new symbol in the sequence, it is intuitive that different symbols will transform the network\u2019s hidden state in different ways (Sutskever et al., 2011).", "startOffset": 295, "endOffset": 319}, {"referenceID": 0, "context": "larly argue that some words are better modeled by matrices than by vectors (Baroni and Zamparelli, 2010; Socher et al., 2012).", "startOffset": 75, "endOffset": 125}, {"referenceID": 12, "context": "larly argue that some words are better modeled by matrices than by vectors (Baroni and Zamparelli, 2010; Socher et al., 2012).", "startOffset": 75, "endOffset": 125}, {"referenceID": 3, "context": "words among the most frequent words in standard corpora like the Corpus of Contemporary American English (COCA) (Davies, 2009).", "startOffset": 112, "endOffset": 126}, {"referenceID": 7, "context": "By using a RNN, Mikolov et al. (2010) created the language model (which we refer to as s-RNN) given by:", "startOffset": 16, "endOffset": 38}, {"referenceID": 14, "context": "The RNTN proposed by Sutskever et al. (2011) is nearly identical to the s-RNN, but the recurrence matrix in eq.", "startOffset": 21, "endOffset": 45}, {"referenceID": 0, "context": "For instance, Baroni and Zamparelli (2010) argue for nouns", "startOffset": 14, "endOffset": 43}, {"referenceID": 2, "context": "Methods which use conditional computation (Cho and Bengio, 2014; Bengio et al., 2015; Shazeer et al., 2017) are similar to RNTNs and r-RNTNs, but rather than use a static mapping, these methods train gating functions which do the mapping.", "startOffset": 42, "endOffset": 107}, {"referenceID": 1, "context": "Methods which use conditional computation (Cho and Bengio, 2014; Bengio et al., 2015; Shazeer et al., 2017) are similar to RNTNs and r-RNTNs, but rather than use a static mapping, these methods train gating functions which do the mapping.", "startOffset": 42, "endOffset": 107}, {"referenceID": 11, "context": "Methods which use conditional computation (Cho and Bengio, 2014; Bengio et al., 2015; Shazeer et al., 2017) are similar to RNTNs and r-RNTNs, but rather than use a static mapping, these methods train gating functions which do the mapping.", "startOffset": 42, "endOffset": 107}, {"referenceID": 4, "context": "There are many other improvements to sRNNs, the most notable of which is the use of Long Term Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), but these are too numerous to list here and for the most part complementary to our proposed method.", "startOffset": 119, "endOffset": 153}, {"referenceID": 2, "context": "Irsoy and Cardie (2014) used m-RNNs for the task of sentiment classification and obtained equal or better performance than sRNNs.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "We can imagine that predicates and function words transform the meaning of the current hidden state of the RNN through matrix multiplication, whereas nouns, for example, add meaning through vector addition, following Baroni and Zamparelli (2010).", "startOffset": 217, "endOffset": 246}, {"referenceID": 7, "context": "As done in other language modeling work (Mikolov et al., 2011; Mnih and Teh, 2012; Zaremba et al., 2014; Mikolov et al., 2014; Shazeer et al., 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al.", "startOffset": 40, "endOffset": 148}, {"referenceID": 10, "context": "As done in other language modeling work (Mikolov et al., 2011; Mnih and Teh, 2012; Zaremba et al., 2014; Mikolov et al., 2014; Shazeer et al., 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al.", "startOffset": 40, "endOffset": 148}, {"referenceID": 8, "context": "As done in other language modeling work (Mikolov et al., 2011; Mnih and Teh, 2012; Zaremba et al., 2014; Mikolov et al., 2014; Shazeer et al., 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al.", "startOffset": 40, "endOffset": 148}, {"referenceID": 11, "context": "As done in other language modeling work (Mikolov et al., 2011; Mnih and Teh, 2012; Zaremba et al., 2014; Mikolov et al., 2014; Shazeer et al., 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al.", "startOffset": 40, "endOffset": 148}, {"referenceID": 6, "context": ", 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al., 1994) using the same pre-processing as Mikolov et al.", "startOffset": 125, "endOffset": 146}, {"referenceID": 6, "context": ", 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al., 1994) using the same pre-processing as Mikolov et al. (2011). This corpus is split into training, validation, and test sets containing 929K words, 73K words, and 82K words respectively.", "startOffset": 126, "endOffset": 202}, {"referenceID": 13, "context": "While L2 regularization did little to remedy this issue, we were able to overcome it by using Dropout (Srivastava et al., 2014) with p = 0.", "startOffset": 102, "endOffset": 127}, {"referenceID": 0, "context": "This model was motivated by the need to increase RNN model capacity without increasing computational costs, while also satisfying the compositionality ideas discussed by Baroni and Zamparelli (2010) and Socher et al.", "startOffset": 170, "endOffset": 199}, {"referenceID": 0, "context": "This model was motivated by the need to increase RNN model capacity without increasing computational costs, while also satisfying the compositionality ideas discussed by Baroni and Zamparelli (2010) and Socher et al. (2012) which suggest that some words are better modeled by matrices rather than vectors, and vice versa.", "startOffset": 170, "endOffset": 224}, {"referenceID": 0, "context": "This model was motivated by the need to increase RNN model capacity without increasing computational costs, while also satisfying the compositionality ideas discussed by Baroni and Zamparelli (2010) and Socher et al. (2012) which suggest that some words are better modeled by matrices rather than vectors, and vice versa. We achieved both goals by pruning the size of the recurrent neural tensor network described by Sutskever et al. (2011) via sensible word-tomatrix mapping.", "startOffset": 170, "endOffset": 441}], "year": 2017, "abstractText": "Increasing the capacity of recurrent neural networks (RNN) usually involves augmenting the size of the hidden layer, resulting in a significant increase of computational cost. An alternative is the recurrent neural tensor network (RNTN), which increases capacity by employing distinct hidden layer weights for each vocabulary word. The disadvantage of RNTNs is that memory usage scales linearly with vocabulary size, which can reach millions for word-level language models. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations using the Penn Treebank corpus show that r-RNTNs improve language model performance over standard RNNs using only a small fraction of the parameters of unrestricted RNTNs.", "creator": "LaTeX with hyperref package"}}}