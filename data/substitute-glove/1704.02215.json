{"id": "1704.02215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "EELECTION at SemEval-2017 Task 10: Ensemble of nEural Learners for kEyphrase ClassificaTION", "abstract": "This printed autobiographical go good move three SemEval expired Task 25: \" Extracting Keyphrases and Relations had Scientific Publications \", purposes way Subtask (B ): \" Classification following suggested keyphrases \". We exploring several certain deep learning integration: long musical - conditions gimbal conduction provides (CNN ), first drawer 6-speed brought making MLP rcw02 - homonym, same once trouble american Bi - LSTM. From numerous complicated, unfortunately as one sextet which differently clumsy - hyperbolic device, achieving gave desktop - F combined - victory most 28. 112 tuesday the examination database. Our approach straight baronet (straight of 1st placed controls: vs.. 61) out by four recently eventually because security shots. However, me labeled students 16 out of 3 neural nets (although tysse often the CNN) taking much 100,000 30% created place end analyzing, cultures, still theme agriculture setting. When trained thursday held full embedded (program + aim ), your ensemble years a audio - F 1 - halftime of 0. 69. Our implemented is typically early", "histories": [["v1", "Fri, 7 Apr 2017 13:07:15 GMT  (27kb)", "https://arxiv.org/abs/1704.02215v1", null], ["v2", "Mon, 10 Apr 2017 10:31:49 GMT  (27kb,D)", "http://arxiv.org/abs/1704.02215v2", "In revision, changed to pdfTeX output"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["steffen eger", "erik-l\\^an do dinh", "ilia kuznetsov", "masoud kiaeeha", "iryna gurevych"], "accepted": false, "id": "1704.02215"}, "pdf": {"name": "1704.02215.pdf", "metadata": {"source": "CRF", "title": "EELECTION at SemEval-2017 Task 10: Ensemble of nEural Learners for kEyphrase ClassificaTION", "authors": ["Steffen Eger", "Erik-L\u00e2n Do Dinh", "Ilia Kuznetsov", "Masoud Kiaeeha", "Iryna Gurevych"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Although scientific experiments are often accompanied by vast amounts of structured data, full-text scientific publications still remain one of the main means for communicating academic knowledge. Given the dynamic nature of modern research and its ever-accelerating pace, it is crucial to automatically analyze new works in order to have a complete picture of advances in a given field.\nRecently, some progress has been made in this direction for the fixed-domain use case1. However, creating a universal open-domain system still\n1 E.g. BioNLP: http://2016.bionlp-st.org/\nremains a challenge due to significant domain differences between articles originating from different fields of research. The SemEval 2017 Task 10: ScienceIE (Augenstein et al., 2017) promotes the multi-domain use case, providing source articles from three domains: Computer Science, Material Sciences and Physics. The task consists of three subtasks, namely (A) identification of keyphrases, (B) classifying them into broad domain-independent classes and (C) inferring relations between the identified keyphrases.\nFor example, for the input sentence \u2018The thermodynamics of copper-zinc alloys (brass) was subject of numerous investigations\u2019 the following output would be expected:\n(A) 1. The thermodynamics of copper-zinc alloys 2. copper-zinc alloys 3. brass\n(B) 1. TASK 2. MATERIAL 3. MATERIAL\n(C) synonym(2,3)\nOur submission focuses on (B) keyphrase classification given item boundaries. We avoid taskspecific feature engineering, which would potentially render the system domain-dependent. Instead, we build an ensemble of several deep learning classifiers detailed in \u00a73, whose inputs are word embeddings learned from general domains."}, {"heading": "2 Task and Data", "text": "In the annotation scheme proposed by the task organizers, keyphrases denoting a scientific model, algorithm or process should be classified as PROCESS (P), which also comprises methods (e.g. \u2018backpropagation\u2019), physical equipment (e.g. \u2018plasmatic nanosensors\u2019, \u2018electron microscope\u2019) and tools (e.g. \u2018MATLAB\u2019). TASK (T) contains\nar X\niv :1\n70 4.\n02 21\n5v 2\n[ cs\n.C L\n] 1\n0 A\npr 2\n01 7\nconcrete research tasks (e.g. \u2018powder processing\u2019, \u2018dependency parsing\u2019) and research areas (e.g. \u2018machine learning\u2019), while MATERIAL (M) includes physical materials (e.g. \u2018iron\u2019, \u2018nanotube\u2019), and corpora or datasets (e.g. \u2018the CoNLL-2003 NER corpus\u2019).\nThe corpus for the shared task consisted of 500 journal articles retrieved from ScienceDirect2, evenly distributed among Computer Science, Material Sciences and Physics domains. It was split into three segments of 350 (training), 50 (development), and 100 (test) documents. The corpus used in subtask (B) contains paragraphs of those articles, annotated with spans of keyphrases. Table 1 shows the distribution of the classes M, T, and P in the data. We note that class T is underrepresented and makes up less than 16% of all instances.\nInter-annotator agreement for the dataset was published to be between 0.45 and 0.85 (Cohen\u2019s \u03ba) (Augenstein et al., 2017). Reviewing similar annotation efforts (QasemiZadeh and Schumann, 2016) already shows that despite the seemingly simple annotation task, usually annotators do not reach high agreement neither on span of annotations nor the class assigned to each span3."}, {"heading": "3 Implemented Approaches", "text": "In this section, we describe the individual systems that form the basis of our experiments (see \u00a74).\nOur basic setup for all of our systems was as follows. For each keyphrase we extracted its left context, right context and the keyphrase itself (center). We represent each of the three contexts as the concatenation of their word tokens: to have fixed-size representations, we limit the left context to the ` previous tokens, the right context to the r following tokens and the center to the c initial tokens of the keyphrase. We consider `, r and c as hyper-parameters of our modeling. If necessary, we pad up each respective context with \u2018empty\u2019 word tokens. We then map each token to a ddimensional word embedding. The choices for\n2 http://www.sciencedirect.com/ 3F1-scores ranging from 0.528 to 0.755 for span bound-\naries and from 0.471 to 0.635 for semantic categories.\nword embeddings are described below. To summarize, we frame our classification problem as a mapping f\u03b8 (\u03b8 represents model parameters) from concatenated word embeddings to one of the three classes MATERIAL, PROCESS, and TASK:\nf\u03b8 : R`\u00b7d \u00d7 Rc\u00b7d \u00d7 Rr\u00b7d \u2192 {M,P,T}.\nNext, we describe the embeddings that we used and subsequently the machine learning models f\u03b8.\nWord Embeddings We experimented with three kinds of word embeddings. We use the popular Glove embeddings (Pennington et al., 2014) (6B) of dimensions 50, 100, and 300, which largely capture semantic information. Further we employ the more syntactically oriented 300-dimensional embeddings of Levy and Goldberg (2014), as well as the 300- dimensional embeddings of Komninos and Manandhar (2016), which are trained to predict both dependency- and standard window-based context.\nDeep Learning models Our first model is a character-level convolutional neural network (char-CNN) illustrated in Figure 1. This model (A) considers each of the three contexts (left, center, right) independently, representing them by a 100-dimensional vector as follows. Each character is represented by a 1-hot vector, which is then mapped to a 32-dimensional\nembedding (not pre-trained, and updated during learning). Then m filters, each of size s, are applied on the embedding layer. Max-over-time pooling results in anm-dimensional layer which is fully connected with the 100-dimensional output layer, with tanh activation function. The 100-d representations of each context are then (B) concatenated, resulting in a 300-dimensional representation of the input. A final softmax layer predicts one of our three target classes. The hyperparameters of this model\u2014additional to `, r, c mentioned above\u2014are: number of filters m, filter size s, and a few others, such as the number of characters to consider in each context window.\nOur second model, which operates on the tokenlevel, is a \u201cstacked learner\u201d. We take five base classifiers from scikit-learn (RandomForestClassifier with two different parameterizations; ExtraTreesClassifier with two different parameterizations; and XGBClassifier), and train them repeatedly on 90% of the training data, extracting their\npredictions on the remaining 10%. This process is iterated 10 times, in a cross-validation manner, so that we have a complete sample of predictions of the base classifiers on the training data. We then use a multi-layer perceptron (MLP) as a metaclassifier that is trained to combine the predictions of the base classifiers into a final output prediction. The MLP is trained for 100 epochs and the model with best performance on a 10% development set is chosen as the model to apply to unseen test data.\nOur third model (Figure 2), also operating on the token level, is an attention based Bidirectional Long Short-Term Memory network (AB-LSTM)4. After loading pre-trained word embeddings, we apply 4 convolutional layers with filter sizes 2, 3, 5 and 7, followed by max-over-timepooling. We concatenate the respective vectors to create an attention vector. The forward and backward LSTM layers (64-dimensional) are supplied with the pre-trained embeddings and the computed attention vector. Their output is concatenated and, after applying dropout of 0.5, is used by the final softmax layer to predict the label probabilities."}, {"heading": "4 Submitted Systems", "text": "We set the c hyper-parameter to 4, and draw left and right context length hyper-parameters `, r (` = r) from a discrete uniform distribution over the multi-set {1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5}.\nPerformance measure was micro-F1 as computed by the task\u2019s evaluation script.5 Table 2 shows average, maximum, and minimum performances of the systems we experimented with. We indicate the \u2018incorrect\u2019 systems (those trained on only the dev set) with a star. We tested 56 different CNNs\u2014hyper-parameters randomly drawn from specific ranges; e.g., we draw the number of filters m from a normal distribution N (\u00b5 = 250, \u03c3 = 50)\u201490 different stackers, and 20 different AB-LSTMs. Our three submitted systems were simple majority votes of (1) the 90 stackers, (2) the 90 stackers and 56 CNNs, (3) the 90 stackers, 56 CNNs and 20 AB-LSTMs. Overall, majority voting is considerably better than the mean performances of each system.\n4 Code was adapted from https://github.com/ codekansas/keras-language-modeling\n5 We report results without the \u201crel\u201d flag, i.e., corresponding to the column \u201cOverall\u201d in Augenstein et al. (2017), Table 4. Setting \u201crel\u201d leads to consistently higher results. E.g., with this flag, we have 72% micro-F1 for our best ensemble (corresponding to column \u201cB\u201d in Augenstein et al. (2017), Table 4), rather than 69% as reported in our Table 2.\nFor the stacker, the Komninos embeddings worked consistently best, with an average F1score of 63.83%. Levy embeddings were second (62.50), followed by Glove embeddings of size 50 (61%), size 300 (60.80) and size 100 (59.50). We assume this is due to the Komninos embeddings being \u2018richest\u2019 in nature, capturing both semantic and syntactic information. However, with more training data (corrected results), mean performances as a function of embedding type are closer: 67.77 (Komninos), 67.61 (Levy), 67.38 (Glove-300), 66.88 (Glove-50), 65.77 (Glove100). The AB-LSTM could not capitalize as much on the syntactic information, and performed best with the Glove embeddings, size 100 (60.35%), and worst with the Levy embeddings (57.80).\nThe char-level CNN and the stacker performed individually considerably better than the ABLSTM. However, including the AB-LSTM in the ensemble slightly increased the majority F1-score on both the M and T class, as Table 3 shows.\nError analysis: Table 4 details that TASK is often confused with PROCESS, and\u2014though less often\u2014vice versa, leading to drastically lower F1score than for the other two classes. This mismatch is because PROCESS and TASK can describe similar concepts, resulting in rather subtle differences. E.g., looking at various \u2018analysis\u2019 instances, we find that some are labeled as PROCESS and others as TASK in the gold data. This holds even for a few seemingly very similar keyphrases (\u2018XRD analysis\u2019, \u2018FACS analysis\u2019). The ensemble has trouble labeling this correctly, tagging 6 of 17 \u2018analysis\u2019 instances wrongly. Beyond further suspicious labelings in the data (e.g.,\n\u2018nuclear fissions reactors\u2019 as Task), other cases could have been resolved by knowledge of syntax (\u2018anionic polymerization of styrene\u2019 is a process, not a material) and/or POS tags, and by knowledge of common abbreviations such as \u2018PSD\u2019.\nWe note that our submitted systems have the best F1-score for the minority class TASK (45%\u2217/47% vs. \u226428% for all other participants). Thus, our submission would have scored 1st using macro-F1 (60.66\u2217/65.33 vs. \u226456.66), even in the erroneous setting of much less training data."}, {"heading": "5 Conclusion", "text": "We present an ensemble-based keyphrase classification system which has achieved close-to-thebest results in the ScienceIE Subtask (B) while using only a fraction of the available training data. With the full training data, our approach ranks 1st. To avoid using expert features has been one of our priorities, but we believe that incorporating additional task-neutral information beyond words and word order would benefit the system performance.\nWe also experimented with document embeddings, created from additionally crawled ScienceDirect6 articles. Even though the stacker described in \u00a73 acting as a document classifier obtained a reasonably high accuracy of \u223c87%, its predictions had little effect on the overall results.\nManual examination of system errors shows that using part-of-speech tags, syntactic relations and simple named entity recognition would very likely boost the performance of our systems."}, {"heading": "Acknowledgments", "text": "This work has been supported by the Volkswagen Foundation, FAZIT, DIPF, KDSL, and the EU\u2019s Horizon 2020 research and innovation programme (H2020-EINFRA-2014-2) under grant agreement \u2116 654021. It reflects only the authors\u2019 views and the EU is not liable for any use that may be made of the information contained therein.\n6 https://dev.elsevier.com/api docs.html"}], "references": [{"title": "SemEval 2017 Task 10: ScienceIE Extracting Keyphrases and Relations from Scientific Publications", "author": ["Isabelle Augenstein", "Mrinal Kanti Das", "Sebastian Riedel", "Lakshmi Nair Vikraman", "Andrew McCallum."], "venue": "Proceedings of the International", "citeRegEx": "Augenstein et al\\.,? 2017", "shortCiteRegEx": "Augenstein et al\\.", "year": 2017}, {"title": "Dependency Based Embeddings for Sentence Classification Tasks", "author": ["Alexandros Komninos", "Suresh Manandhar."], "venue": "Proceedings of NAACL-HLT \u201916. ACL, San Diego, CA, USA, pages 1490\u20131500.", "citeRegEx": "Komninos and Manandhar.,? 2016", "shortCiteRegEx": "Komninos and Manandhar.", "year": 2016}, {"title": "DependencyBased Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of ACL \u201914. ACL, Baltimore, MD, USA, pages 302\u2013308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of EMNLP \u201914. ACL, Doha, Qatar, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The ACL RD-TEC 2.0: A Language Resource for Evaluating Term Extraction and Entity Recognition Methods", "author": ["Behrang QasemiZadeh", "Anne-Kathrin Schumann"], "venue": "In Proceedings of LREC \u201916", "citeRegEx": "QasemiZadeh and Schumann.,? \\Q2016\\E", "shortCiteRegEx": "QasemiZadeh and Schumann.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The SemEval 2017 Task 10: ScienceIE (Augenstein et al., 2017) promotes the multi-domain use case, providing source articles from three domains: Computer Science, Material Sciences and Physics.", "startOffset": 36, "endOffset": 61}, {"referenceID": 0, "context": "85 (Cohen\u2019s \u03ba) (Augenstein et al., 2017).", "startOffset": 15, "endOffset": 40}, {"referenceID": 4, "context": "Reviewing similar annotation efforts (QasemiZadeh and Schumann, 2016) already shows that despite the seemingly simple annotation task, usually annotators do not reach high agreement neither on span of annotations nor the class assigned to each span3.", "startOffset": 37, "endOffset": 69}, {"referenceID": 3, "context": "We use the popular Glove embeddings (Pennington et al., 2014) (6B) of dimensions 50, 100, and 300, which largely capture semantic information.", "startOffset": 36, "endOffset": 61}, {"referenceID": 1, "context": "Further we employ the more syntactically oriented 300-dimensional embeddings of Levy and Goldberg (2014), as well as the 300dimensional embeddings of Komninos and Manandhar (2016), which are trained to predict both dependency- and standard window-based context.", "startOffset": 80, "endOffset": 105}, {"referenceID": 1, "context": "Further we employ the more syntactically oriented 300-dimensional embeddings of Levy and Goldberg (2014), as well as the 300dimensional embeddings of Komninos and Manandhar (2016), which are trained to predict both dependency- and standard window-based context.", "startOffset": 150, "endOffset": 180}, {"referenceID": 0, "context": ", corresponding to the column \u201cOverall\u201d in Augenstein et al. (2017), Table 4.", "startOffset": 43, "endOffset": 68}, {"referenceID": 0, "context": ", corresponding to the column \u201cOverall\u201d in Augenstein et al. (2017), Table 4. Setting \u201crel\u201d leads to consistently higher results. E.g., with this flag, we have 72% micro-F1 for our best ensemble (corresponding to column \u201cB\u201d in Augenstein et al. (2017), Table 4), rather than 69% as reported in our Table 2.", "startOffset": 43, "endOffset": 252}], "year": 2017, "abstractText": "This paper describes our approach to the SemEval 2017 Task 10: \u201cExtracting Keyphrases and Relations from Scientific Publications\u201d, specifically to Subtask (B): \u201cClassification of identified keyphrases\u201d. We explored three different deep learning approaches: a character-level convolutional neural network (CNN), a stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM. From these approaches, we created an ensemble of differently hyper-parameterized systems, achieving a micro-F1-score of 0.63 on the test data. Our approach ranks 2nd (score of 1st placed system: 0.64) out of four according to this official score. However, we erroneously trained 2 out of 3 neural nets (the stacker and the CNN) on only roughly 15% of the full data, namely, the original development set. When trained on the full data (training+development), our ensemble has a micro-F1-score of 0.69. Our code is available from https://github. com/UKPLab/semeval2017-scienceie.", "creator": "LaTeX with hyperref package"}}}