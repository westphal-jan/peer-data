{"id": "1704.02312", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "A Constrained Sequence-to-Sequence Neural Model for Sentence Simplification", "abstract": "Sentence simplification reduces associative illustrate immediately promising to with language enhancements. Previous simplification comparative friday taken sentence each from word level appear success promising results put also meet great struggles. For retrial - level studies, sentences after simplification are fluently rest sometimes are not just simplified. For you - capacity psychiatry, words each grammatical but also to benefit waverley score due would ways alphabets well words before and after simplification. In what plastic, we reform that only - attempt magyarization framework although uses two the word - moving and the absentia - regular generalisations, making meant though coming spacing preference. Based however before addition - transition framework, understand enact entire novel correspondingly nodes popularity vision to downsize committing given modification times. The advance last set Wikipedia from Simple Wikipedia aligned templates indicate that sure method average does performance than and/or tabar\u00e9.", "histories": [["v1", "Fri, 7 Apr 2017 17:53:24 GMT  (702kb,D)", "http://arxiv.org/abs/1704.02312v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["yaoyuan zhang", "zhenxu ye", "yansong feng", "dongyan zhao", "rui yan"], "accepted": false, "id": "1704.02312"}, "pdf": {"name": "1704.02312.pdf", "metadata": {"source": "CRF", "title": "A Constrained Sequence-to-Sequence Neural Model for Sentence Simplification", "authors": ["Yaoyuan Zhang", "Zhenxu Ye", "Yansong Feng", "Dongyan Zhao", "Rui Yan"], "emails": ["ruiyan}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Sentence simplification is a standard NLP task of reducing reading complexity for people who have limited linguistic skills to read. In particular, children, non-native speakers and individuals with language impairments such as Dyslexia (Rello et al., 2013), Aphasic (Carroll et al., 1999) and Autistic (Evans et al., 2014), would benefit from the task which makes sentences easier to understand. There are two categories for the task: lexical simplification and sentence simplification. Both categories enable a paraphrasing pro-\n\u2020Equal contribution.\ncess, turning a normal input into a simpler output while maintaining the same/similar semantics between the input and the output.\nInspired by the great achievements of machine translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016). These studies apply the phrase-based statistical machine translation (PB-SMT) model or the syntacticbased translation model (SB-SMT). Both PBSMT and SB-SMT require high-level features or even rules empirically chosen by humans. Recently, neural machine translation (NMT) based on sequence-to-sequence model (seq2seq) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) shows more powerful capabilities than traditional SMT systems. NMT applies deep learning regimes and extracts features automatically without human supervision.\nWe observe that sentence simplification usually means the simple output sentence is derived from the normal input sentence with parts of terms changed, as shown in Table 1. Due to such an intrinsic character, applying machine translation methods directly is likely to generate a completely identical sentence as the input sentence, no matter standard SMT or NMT. Although MT methods indeed have advanced the research of sentence simplification tasks, there is still plenty of room for improvements. To the best of our knowledge, there are few competitive simplification models using translation models even with neural network architectures so far.\nBesides sentence-level simplification, there is lexical simplification which substitutes long and infrequent words with their shorter and more frequent synonyms by complex word identification, substitution generation, substitution selection and\nar X\niv :1\n70 4.\n02 31\n2v 1\n[ cs\n.C L\n] 7\nA pr\n2 01\n7\nother processes. Recent lexical simplification models by Horn et al. (2014), Glavas\u030c et al. (2015), Paetzold et al. (2016) and Pavlick et al. (2016) have accumulated substantial numbers of synonymous word pairs. This makes it possible for us to simplify complex words before simplifying the whole sentence. However, even though synonyms have similar semantic meaning, they might have different usages. Replacing complex words with their simpler synonyms is an intuitive way to simplify sentences, but not always works due to potential grammatical errors after the switchings (shown in Table 1). Moreover, lexical substitution is just one way to simplify sentences. We can also simplify sentences by splitting, deletion, reordering and so on.\nFor the sentence-level simplification, we generally obtain output results with few grammar errors although it does not guarantee that they are simplified; for the lexical-level simplification, we can simplify the complicated parts of the sentences but it does not always guarantee that they are grammatically fluent. It is an intuitive and exciting idea to combine both methods together and make use of their corresponding advantages so that we can obtain simplified sentences with good readability and fluency. To be more specific, the simplification process of an input sentence is conducted in two steps. 1) We first identify complex word(s) and replace them with their simpler synonyms according to a pre-constructed knowledge base1. 2) The second step is to generate a legitimate sentence given the simplified word(s) with appropriate syntactic structures and grammar. Another key issue for the second step is that we need to maintain the same/similar semantic meaning of the input sentence. To this end, we still stick to the translation paradigm by translating the complex sentence into a simple sentence.\nIn this paper, our contributions are as follows: \u2022 We propose a two-step simplification framework to combine the advantages of both wordlevel simplification and sentence-level simplification to make the generated sentences fluent, readable and simplified. \u2022 We implement a novel constrained seq2seq model which fits our task scenario: certain word(s) are required to exist in the seq2seq process. We start from the constraint of one given word and ex-\n1A knowledge base such as PPDB contains millions of paraphrasing word pairs to change between simple words and complex words (Pavlick and Callison-Burch, 2016)\ntend the constraints to multiple given words. We evaluate the proposed method and neural model on English Wikipedia and Simple English Wikipedia datasets. The experimental results indicate that our model achieves better results than a series of baseline algorithms in terms of iBLEU scores, Flesch readability and human judgments. This paper is organized as follows. In Section 2 we review related works, and describe our proposed method and model in Section 3. In Section 4, we describe the experimental setups and show results. In Section 5, we conclude our paper and discuss the future work."}, {"heading": "2 Related Work", "text": "In previous studies, researchers of sentence-level simplification mostly address the simplification task as a monolingual machine translation problem. Specia et al. (2010) use the standard PB-SMT implemented in Moses toolkit (Koehn et al., 2007) to translate the original sentences to the simplified ones. Similarly, Coster and Kauchak (2011) extend the PB-SMT model by adding phrase deletion. Wubben et al. (2012) make a further effort by reranking the Moses\u2019 nbest output based on their dissimilarity to the input. Most recently, Xu et al. (2016) have proposed a SB-SMT model, achieving better performance than Wubben\u2019s system. In general, sentence-level simplification maintains the semantic meaning and fluency but does not always guarantee the literal simplification.\nAs for word-level simplification, there are im-\npressive results as well. Horn et al. (2014) extract over 30,000 paraphrase rules for lexical simplification by identifying aligned words in English Wikipedia and Simple English Wikipedia. Glavas\u030c et al. (2015) employ GloVe (Pennington et al., 2014) to generate synonyms for the complex words. Instead of using the parallel datasets, their approach only requires a single corpus. Paetzold et al. (2016) propose a new word embeddings model to deal with the limitation that the traditional models do not accommodate ambiguous lexical semantics. Pavlick et al. (2016) release about 4,500,000 simple paraphrase rules by extracting normal paraphrases rules from a bilingual corpus and reranking the simplicity scores of these rules by a supervised model. Thanks to their efforts, there is a large number of effective methods for identifying complex words, finding corresponding simple synonyms and selecting qualified substitutions. However, sometimes simplifying complicated words directly with simple synonyms violates grammar rules and usages.\nRecent progress in deep learning with neural networks brings great opportunities for the development of stronger NLP systems such as neural machine translation (NMT). Deep learning is heavily driven by data, requiring few human efforts to create high-level features. Specifically, the sequence-to-sequence RNN model (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Mou et al., 2015, 2016) has a remarkable ability to characterize word sequences and generate natural language sentences. However, the seq2seq NMT model still shares the problem with other MT-based methods: lack of literal simplification from time to time.\nOverall, sentence-level and word-level sentence simplification both have their strengths and weaknesses. In this paper, we propose a two-step method fusing their corresponding advantages."}, {"heading": "3 Neural Generation Model", "text": "We generate simplified sentences using a sequence-to-sequence model trained on a parallel corpus, namely English Wikipedia and Simple English Wikipedia. We have simplified word(s) identified from the first step, but the standard sequence-to-sequence model cannot guarantee the existence of such word(s). Therefore, we propose a constrained sequence-to-sequence (Constrained Seq2Seq) model with the given simplified word(s)\nas constraints during the sentence generation."}, {"heading": "3.1 Methodology", "text": "Since there has been many efforts working on the establishment of word simplification pairs (Horn et al., 2014; Glavas\u030c and S\u030ctajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016), we do not focus on the identification of words that require simplification or the methods of selecting what simpler words to switch. Instead, we change words according to these knowpledge base and proceed to the neural sentence generation model, assuming the word substitutions are correct based on previous studies on synonym alignments. To be more specific, given an input sentence, the simplification process is conducted in two steps: \u2022 Step 1. According to previous studies on lexical substitution, we first identify complex words in the input sentence and then substitute them with their simpler synonyms; \u2022 Step 2. Given the simplified words from the first step as constraints, we propose a constrained seq2seq model which encodes the input sentence as a vector (encoder) and decodes the vector into a simplified sentence (decoder). The generation process is conditioned on the simplified word(s) and consists of both backward and forward generation.\nWe proceed to introduce the proposed constrained seq2seq model in the next section."}, {"heading": "3.2 Constrained Seq2Seq Model", "text": "Given an input sequence x = (x1, . . . , xn), x \u2208 RV and a switching word pair of a complex word xc and its simpler synonym ys, we aim to generate a simplified sentence y = (y1, . . . , ym), y \u2208 RV as the output where ys is contained in y, i.e., ys \u2208 y. There could be multiple constraint words and we start from the simplest situation with only one constraint word. Here n and m denote the lengths of the source and target sentences respectively, V is the vocabulary size of the source and target sentences.\nThe simpler word ys splits the output sentence into two sequences: backward sequence yb = (ys\u22121, . . . , y1) and forward sentence yf = (ys+1, . . . , ym). The joint probabilities of yb and yf are:\np(yb) = s\u22121\u220f i=1 p(ys\u2212i|ys, . . . , ys\u2212i+1,x)\np(yf ) = m\u2212s\u220f i=1 p(ys+i|y1, . . . , ys, . . . , ys+i\u22121,x)\n(1) As shown in Figure 1, to generate a sequence y with a constraint ys, we first generate a sequence from ys to y1. Then, we generate a forward sequence yf conditioned on the generated sequence from y1 to ys. In this way, the output sequence is y = {y\u22121b , ys, yf}, where y \u22121 b is the reverse of yb. In our paper, we apply the bi-directional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) with gated recurrent units (GRUs) (Cho et al., 2014) for both the backward and forward generation process. We encode the input sequence as follows:\nzt = \u03c3(Wzet + Uz \u2212\u2192 h t\u22121) rt = \u03c3(Wret + Ur \u2212\u2192 h t\u22121) h\u0303t = tanh(Whet + Uh[rt \u25e6 \u2212\u2192 h t\u22121]) \u2212\u2192 h t = (1\u2212 zt) \u25e6 \u2212\u2192 h t\u22121 + zt \u25e6 h\u0303t\n(2)\nwhere et \u2208 RE is the embedding vector of word xt and E denotes the word embedding dimensionality; Wz,Wr,Wh \u2208 Rdim\u00d7E , Uz, Ur, Uh \u2208 Rdim\u00d7dim are weight matrices2 and dim denotes the number of hidden states; \u2212\u2192 h t \u2208 Rdim is the hidden state of BiRNN\u2019s forward direction at time t. Likewise, the hidden state of BiRNN\u2019s backward direction is denoted as \u2190\u2212 h t.\nAs last, we concatenate the bidirectional hidden states as the sentence embedding:\nht = [ \u2212\u2192 h>t ; \u2190\u2212 h>t ]> (3)\n2Bias term are omitted for simplicity\nRNN Decoder We adapt the gated recurrent unit with the attention mechanism at the decoder part, where the hidden state st \u2208 Rdim is computed by:\nz \u2032 t = \u03c3(W \u2032 ze \u2032 t\u22121 + U \u2032 zst\u22121 + Czct) r \u2032 t = \u03c3(W \u2032 re \u2032 t\u22121 + U \u2032 rst\u22121 + Crct) s\u0303t = tanh(Wse \u2032 t\u22121 + U \u2032 s[r \u2032 t \u25e6 st\u22121] + Csct) st = (1\u2212 z \u2032 t) \u25e6 st\u22121 + z \u2032 t \u25e6 s\u0303t\n(4) where e\n\u2032 t\u22121 \u2208 RE is the embedding vector of word\nyt and E denotes the word embedding dimensionality; W \u2032 z,W \u2032 r,Ws \u2208 Rdim\u00d7E , U \u2032 z, U \u2032 r, U \u2032 s \u2208 Rdim\u00d7dim, Cz, Cr, Cs \u2208 Rdim\u00d72dim are weight matrices and dim denotes the number of hidden states. The initial hidden state s0 is computed by s0 = tanh(WshHmean), where Hmean is the mean value of all hidden states of Encoder and Wsh \u2208 Rdim\u00d7dim.\nThe context vector ct is recomputed at each step by an alignment model:\nct = n\u2211\nj=1\n\u03b1tjhj\n\u03b1tj = exp(etj)\u2211n k=1 exp(etk)\n(5)\netj = a(st\u22121, hj)\nwhere \u03b1tj is the alignment weight implemented by a function which is an attention mechanism to align the input token xj and the output token yt. The more tightly they match each other, the higher scores they obtain.\nWith the decoder state st and the context vector ct, we approximately compute each conditional probability either in the backward sequence yb or the forward sequence yf as Eq.(7):\np(yt|y1, . . . , yt\u22121,x) = p(yt|yt\u22121, st, ct) (6)\nAccording to the Eq.(1) and Eq.(2), we in turn obtain the backward sentence yb = (ys\u22121, . . . , y1) and the forward sentence yf = (ys+1, . . . , ym), with the maximal estimated probability by beam search. Finally, we concatenate the reverse backward sequence yb, the simpler word ys and the forward sequence yf to output the entire sentence y = (y1, . . . ,ys, . . . , ym). Notice that ys can exist at any position in y."}, {"heading": "3.3 Multi-constrained Seq2Seq", "text": "We just illustrated how to put a single constraint word into the sequence-to-sequence generation process, while actually there can be more than one constraint words which are simplified before the sentence generation, as shown in Table 1. We extend the single constraint into multiple constraints by a Multi-Constrained Seq2Seq model. Without loss of generosity, we define the multiple keywords as ys1,ys2, . . . , ysk and illustrate MultiConstrained Seq2Seq in Figure 2.\nWe first illustrate the situation with two simplified words, i.e., k = 2, namely ys1 and ys2. We generally take the complex word with the least term frequency as the first constrained word ys1 and use the same method as in Section 3.2 to generate the first output sentence y1 = (y11, . . . , ys1, . . . , y 1 m). In the second round of generation, we take the first output sentence y1 as the input and generate the second output sentence y2 with ys2 as the constraint. Compared with the singlepass generation with a single constraint word, we have an output sentence y2 = (y21, . . . ,ys2, . . . , ys1, . . . , y2m) with two constraint words after a two-pass generation. The relative position of ys1 and ys2 depends on the input sentence .\nWhen there are more than two constraint words, i.e., k > 2, the system architecture remains the same with more repeating passes of generation in-\ncluded (shown in Figure 2). To decode the k-th output sentence yk, we encode the output sentence yk\u22121 to the next word embeddings of the multiconstrained seq2seq model.\nNote that after each pass of generation, other constrained words may be deleted or simplified already. if there are complex word(s) which need to be simplified, the system will repeat the simplification process."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Dataset and Setups", "text": "We evaluate our proposed approach on the parallel corpus from Wikipedia and Simple Wikipedia in English3. We randomly split the corpus into 123,626 sentence pairs (each pair as a normal sentence and its simplification in parallel) for training, 5,000 sentences for validation and 600 sentences for testing. There can be noises in the dataset. We filter out test samples when the output is identical as the input without any simplification. We also applied lowercasing preprocess to all samples. Our vocabulary size is 60,000 while out-ofvocabulary words are mapped to the token \u201cunk\u201d.\nThe RNN encoder and decoder of our model both have 1,000 hidden units; the word embedding dimensionality is 620. We use the Adadelta\n3This dataset is available at http://www.cs. pomona.edu/\u02dcdkauchak/simplification\n(Zeiler, 2012) to optimize all parameters."}, {"heading": "4.2 Comparison Methods", "text": "In this paper, we conduct the experiments on the English Wikipedia and Simple English Wikipedia datasets to compare our proposed method against several representative algorithms. Moses. It is a standard phrase-based machine translation model (Koehn et al., 2007). SBMT. SBMT is a syntactic-based machine translation model (Xu et al., 2016), which is implemented on the open-source Joshua toolkit (Post et al., 2013). The simplification model is optimized to the SARI metric and leverages the PPDB dataset (Pavlick and Callison-Burch, 2016) as a rich source of simplification operations. Lexical Substitution. This method only substitutes the complex words with the simplified word(s) which we use as the constraint word(s) in our model and leave other words of the input sentence unchanged. This model shares the same hypothesis as our model. Seq2Seq. The sequence-to-sequence model is the state-of-the-art neural machine translation model (Cho et al., 2014) with the attention mechanism applied (Bahdanau et al., 2014). Constrained Seq2Seq. We propose a novel neural sentence generation model based on sequence-tosequence paradigm with one constraint word. We use Multi-Constrained Seq2Seq to denote the scenario when there is more than one constraint words."}, {"heading": "4.3 Evaluation Metrics", "text": "Automatic Evaluation To evaluate the performance of different methods for the simplification task, we leverage four automatic evaluation metrics4: Flesch-Kincaid grade level (FK) (Kincaid et al., 1975), SARI (Xu et al., 2016), BLEU (Papineni et al., 2002) and iBLEU (Sun and Zhou, 2012). FK is widely used for readability. SARI evaluates the simplicity by explicitly measuring the goodness of words that are added, deleted and kept. BLEU is originally designed for MT and evaluates the output by using n-gram matching between the output and the reference. Several studies indicate that BLEU alone is not really suitable for the simplification task (Zhu et al., 2010; S\u030ctajner et al., 2015; Xu et al., 2016). In many\n4The highest n-gram order of all correlation related metrics is set to 4 in our experiments\ncases of sentence simplification, the output sequence looks similar to the input sequence with only part of it simplified. Due to this situation, there is prominent insufficiency for the standard BLEU metric: even though the output sequence does not perform any simplification operations on the input sequence, it is still likely to obtain a high BLEU score. It is necessary to penalize the output sentence that is too similar to the input sentence. Therefore, the iBLEU metric is more suitable for simplification as it balances similarity and simplicity. Given an output sentence O, a reference sentence R and input sentence I , iBLEU5 is defined as:\niBLEU = \u03b1\u00d7BLEU(O,R)\u2212 (1\u2212 \u03b1)\u00d7BLEU(O, I)\n(7)\nHuman Evaluation Human judgment is the ultimate evaluation metric for all natural language processing tasks. We randomly select 120 source sentences from our test dataset and invite 20 graduate students (include native speakers) to evaluate the simplified sentences by all systems according to the source sentence. For fairness, we conduct a blind review: the evaluators are not aware which methods produce the simplification results. Following earlier studies (Wubben et al., 2012; Xu et al., 2016), we asked participants to rate Grammaticality (the extent to which the simplified sentence is grammatically correct and fluently readable), Meaning (the extent to which the simplified sentence has the same meaning as the input) and Simplicity (the extent to which the simplified sequence maintains similar meaning). All these three human evaluation metrics are in 5-points scale from lowest 0 points to highest 4 points. Note that if one generated sentence is identical to the source sentence, we rate Grammaticality with 4 points, Meaning with 4 points and Simplicity with 0 points for this target sentence."}, {"heading": "4.4 Overall Performance", "text": "The automatic evaluation results are listed in Table 2. Moses has the worst performance. It obtains a fair BLEU(O, R) score that is 28.28. But its BLEU(O, I) score is 99.62, indicating that Moses fails to simplify most of the sentences. As this failure, its FK, iBLEU and SARI scores are all quite low. SBMT has the similar performance like Moses and neither simplify the output sen-\n5\u03b1 is set to 0.9 as suggested by Sun et al. (2012)\ntences nor promote the readability. The overall results of the Seq2Seq system are better than Moses and SBMT. Though its BLEU(O, R) score is little lower than Moses and SBMT, its output sentences are not mostly identical to the input sentences as its BLEU(O, I) score is only 66.94. It also achieves better FK(12.74), iBLEU(16.27) and SARI(33.16) scores than Moses and SBMT. Lexical Substitution only substitutes the complex words so that it obtains the highest BLEU(O, R) and SARI scores. But it gets the worst FK readability. In general, both Constrained Seq2Seq and Multi-Constrained Seq2Seq under our proposed framework outperform baselines. They have higher similarities to the reference and lower similarities to the input than other systems. So the iBLEU scores of our two systems are higher than baselines, which are 20.26 and 19.87 respectively. The SARI score of our two systems is also pretty high. As for FK readability, our two systems achieve the best result.\nThe human evaluation results are displayed in Table 3. Moses generates 116 sentences that are completely identical to the input sentences.\nAs the Grammaticality of the identical sentences are rated with 4 points, the Meaning with 4 points and the Simplicity with 0 points, Moses gets the highest score (3.99) both in Grammaticality and Meaning but obtains the lowest score (0.02) in Simplicity. Similar to Moses, SBMT generates 99 sentences that are not really simplified so that SBMT obtains similar results like Moses. Seq2Seq outperforms Moses and SBMT systems judged by the overall performance and obtains 3.28 in Grammaticality, 3.45 in Meaning and 0.96 in Simplicity. The results of Lexical Substitution in Meaning and Simplicity are rather high. But as shown by the score of Grammaticality, the sentences generated by Lexical Substitution contain many grammar errors which are not surprising. Our Constrained Seq2Seq and Multi-Constrained Seq2Seq outperform in Simplicity than baselines. The Meaning scores of our systems are 2.81 and 2.65. Simple English Wikipedia has a quite similar score, 2.83, which indicates that to some extent, both our systems and Simple English Wikipedia have a semantic loss when simplifying sentences. As for Grammati-\ncality, Constrained Seq2Seq is better than Lexical Substitution. Multi-Constrained Seq2Seq performs worse than Constrained Seq2Seq in Grammaticality but better in Simplicity. Judged by the overall performances, Constrained Seq2Seq and Multi-Constrained Seq2Seq outperform other off-the-shelf sentence-level simplification methods as their generated sentences are literally simplified and legitimate."}, {"heading": "4.5 Analysis and Case Studies", "text": "In Table 4, we show some typical examples of all systems. Among them, Moses, SBMT and the Seq2Seq model generate a completely identical sentence to the input sentence as they do in most cases. Lexical Substitution paraphrases the complex words \u201ckey\u201d, \u201chub\u201d and \u201ca great deal of\u201d with the simple words \u201cimportant\u201d, \u201ccenter\u201d and \u201cmany\u201d. As seen, the article for the word important should be changed from \u201ca\u201d to \u201can\u201d but Lexical Substitution fails to deal with such kind of errors. As for our proposed model, it generates the output sentences conditioned on the simplified\nword \u201ccenter\u201d and deletes the complex phrase \u201ca great deal of\u201d. Taking the generated sentence of Constrained Seq2Seq model as a input, the MultiConstrained Seq2Seq model substitutes the less frequent words \u201ckey\u201d with the word \u201cimportant\u201d. It also changes the adverbial clause \u201cserving as ... until the 1980s\u201d with a simple sentence structure \u201cit became ... until the 1980s\u201d, which shows that our models are more flexible and more effective than other baseline systems."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a new two-step method for sentence simplification by combining wordlevel simplification and sentence-level simplification. We run experiments on the parallel datasets of Wikipedia and Simple Wikipedia and the results show that our methods outperform various baselines with better readability, flexibility and simplicity achieved. In the future, we plan to take more factors (e.g., sentence length or grammar rules) into account and formulate them as constraints into our proposed model."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Simplifying text for language-impaired readers", "author": ["John Carroll", "Guido Minnen", "Darrenz Pearce", "Canning Yvonne", "Devlin Siobhan", "John Tait."], "venue": "Proceedings of EACL. pages 269\u2013270.", "citeRegEx": "Carroll et al\\.,? 1999", "shortCiteRegEx": "Carroll et al\\.", "year": 1999}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning to simplify sentences using wikipedia", "author": ["William Coster", "David Kauchak."], "venue": "Proceedings of the workshop on monolingual text-to-text generation. Association for Computational Linguistics, pages 1\u20139.", "citeRegEx": "Coster and Kauchak.,? 2011", "shortCiteRegEx": "Coster and Kauchak.", "year": 2011}, {"title": "An evaluation of syntactic simplification rules for people with autism", "author": ["Richard Evans", "Constantin Orasan", "Iustin Dornescu."], "venue": "Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR). pages 121\u2013", "citeRegEx": "Evans et al\\.,? 2014", "shortCiteRegEx": "Evans et al\\.", "year": 2014}, {"title": "Simplifying lexical simplification: Do we need simplified corpora", "author": ["Goran Glava\u0161", "Sanja \u0160tajner."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "citeRegEx": "Glava\u0161 and \u0160tajner.,? 2015", "shortCiteRegEx": "Glava\u0161 and \u0160tajner.", "year": 2015}, {"title": "Learning a lexical simplifier using wikipedia", "author": ["Colby Horn", "Cathryn Manduca", "David Kauchak."], "venue": "ACL (2). pages 458\u2013463.", "citeRegEx": "Horn et al\\.,? 2014", "shortCiteRegEx": "Horn et al\\.", "year": 2014}, {"title": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel", "author": ["J. Peterand Robert P. Fishburne Jr Kincaid", "Richard L. Rogers", "Brad S."], "venue": "Naval Technical Training Command", "citeRegEx": "Kincaid et al\\.,? 1975", "shortCiteRegEx": "Kincaid et al\\.", "year": 1975}, {"title": "Moses: Open source toolkit", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris C.Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "author": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin."], "venue": "arXiv preprint arXiv:1607.00970.", "citeRegEx": "Mou et al\\.,? 2016", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Backward and forward language modeling for constrained sentence generation", "author": ["Lili Mou", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin."], "venue": "arXiv preprint arXiv:1512.06612.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Unsupervised lexical simplification for non-native speakers", "author": ["Gustavo H. Paetzold", "Lucia Specia."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI Press, pages 3761\u20133767.", "citeRegEx": "Paetzold and Specia.,? 2016", "shortCiteRegEx": "Paetzold and Specia.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Simple ppdb: A paraphrase database for simplification", "author": ["Ellie Pavlick", "Chris Callison-Burch."], "venue": "The 54th Annual Meeting of the Association for Computational Linguistics. page 143.", "citeRegEx": "Pavlick and Callison.Burch.,? 2016", "shortCiteRegEx": "Pavlick and Callison.Burch.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "EMNLP. volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Joshua 5.0: Sparser, better, faster, server", "author": ["Matt Post", "Juri Ganitkevitch", "Luke Orland", "Jonathan Weese", "Yuan Cao", "Chris Callison-Burch"], "venue": "In Proceedings of the Eighth Workshop on Statistical Machine Translation", "citeRegEx": "Post et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Post et al\\.", "year": 2013}, {"title": "The impact of lexical simplification by verbal paraphrases for people with and without dyslexia", "author": ["Luz Rello", "Ricardo Baeza-Yates", "Horacio Saggion."], "venue": "International Conference on Intelligent Text Processing and Computational Linguistics. Springer Berlin", "citeRegEx": "Rello et al\\.,? 2013", "shortCiteRegEx": "Rello et al\\.", "year": 2013}, {"title": "Bidirectional recurrent neural networks 11:2673\u20132681", "author": ["Mike Schuster", "Kuldip K. Paliwal"], "venue": null, "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Translating from complex to simplified sentences", "author": ["Lucia Specia."], "venue": "International Conference on Computational Processing of the Portuguese Language. Springer Berlin Heidelberg, pages 30\u201339.", "citeRegEx": "Specia.,? 2010", "shortCiteRegEx": "Specia.", "year": 2010}, {"title": "Joint learning of a dual smt system for paraphrase generation", "author": ["Hong Sun", "Ming Zhou."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers. Association for Computational Linguistics, volume 2.", "citeRegEx": "Sun and Zhou.,? 2012", "shortCiteRegEx": "Sun and Zhou.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A deeper exploration of the standard pb-smt approach to text simplification and its evaluation", "author": ["Sanja \u0160tajner", "Hannah B\u00e9chara", "Horacio Saggion."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "\u0160tajner et al\\.,? 2015", "shortCiteRegEx": "\u0160tajner et al\\.", "year": 2015}, {"title": "Sentence simplification by monolingual machine translation", "author": ["Sander Wubben", "Antal Van Den Bosch", "Emiel Krahmer."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers. volume 1, pages 1015\u20131024.", "citeRegEx": "Wubben et al\\.,? 2012", "shortCiteRegEx": "Wubben et al\\.", "year": 2012}, {"title": "Optimizing statistical machine translation for text simplification", "author": ["Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch."], "venue": "Transactions of the Association for Computational Linguistics 4. pages 401\u2013415.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "A monolingual tree-based translation model for sentence simplification", "author": ["Zhemin Zhu", "Delphine Bernhard", "Iryna Gurevych"], "venue": "In Proceedings of the 23rd international conference on computational linguistics. Association for Computational Linguistics,", "citeRegEx": "Zhu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "In particular, children, non-native speakers and individuals with language impairments such as Dyslexia (Rello et al., 2013), Aphasic (Carroll et al.", "startOffset": 104, "endOffset": 124}, {"referenceID": 1, "context": ", 2013), Aphasic (Carroll et al., 1999) and Autistic (Evans et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 4, "context": ", 1999) and Autistic (Evans et al., 2014), would benefit from the task which makes sentences easier to understand.", "startOffset": 21, "endOffset": 41}, {"referenceID": 18, "context": "translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016).", "startOffset": 134, "endOffset": 230}, {"referenceID": 25, "context": "translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016).", "startOffset": 134, "endOffset": 230}, {"referenceID": 3, "context": "translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016).", "startOffset": 134, "endOffset": 230}, {"referenceID": 22, "context": "translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016).", "startOffset": 134, "endOffset": 230}, {"referenceID": 23, "context": "translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016).", "startOffset": 134, "endOffset": 230}, {"referenceID": 2, "context": "cently, neural machine translation (NMT) based on sequence-to-sequence model (seq2seq) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) shows more powerful capabilities than traditional SMT systems.", "startOffset": 87, "endOffset": 152}, {"referenceID": 20, "context": "cently, neural machine translation (NMT) based on sequence-to-sequence model (seq2seq) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) shows more powerful capabilities than traditional SMT systems.", "startOffset": 87, "endOffset": 152}, {"referenceID": 0, "context": "cently, neural machine translation (NMT) based on sequence-to-sequence model (seq2seq) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) shows more powerful capabilities than traditional SMT systems.", "startOffset": 87, "endOffset": 152}, {"referenceID": 6, "context": "Recent lexical simplification models by Horn et al. (2014), Glava\u0161 et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 6, "context": "Recent lexical simplification models by Horn et al. (2014), Glava\u0161 et al. (2015), Paetzold et al.", "startOffset": 40, "endOffset": 81}, {"referenceID": 6, "context": "Recent lexical simplification models by Horn et al. (2014), Glava\u0161 et al. (2015), Paetzold et al. (2016) and Pavlick et al.", "startOffset": 40, "endOffset": 105}, {"referenceID": 6, "context": "Recent lexical simplification models by Horn et al. (2014), Glava\u0161 et al. (2015), Paetzold et al. (2016) and Pavlick et al. (2016) have accumulated substantial numbers of synonymous word pairs.", "startOffset": 40, "endOffset": 131}, {"referenceID": 13, "context": "A knowledge base such as PPDB contains millions of paraphrasing word pairs to change between simple words and complex words (Pavlick and Callison-Burch, 2016) Normal Sentence In the last decades of his life, dukas became well known as a teacher of composition, with many famous students.", "startOffset": 124, "endOffset": 158}, {"referenceID": 8, "context": "(2010) use the standard PB-SMT implemented in Moses toolkit (Koehn et al., 2007) to translate the original sentences to the simplified ones.", "startOffset": 60, "endOffset": 80}, {"referenceID": 16, "context": "Specia et al. (2010) use the standard PB-SMT implemented in Moses toolkit (Koehn et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Similarly, Coster and Kauchak (2011) extend the PB-SMT model by adding phrase deletion.", "startOffset": 11, "endOffset": 37}, {"referenceID": 3, "context": "Similarly, Coster and Kauchak (2011) extend the PB-SMT model by adding phrase deletion. Wubben et al. (2012) make a further effort by reranking the Moses\u2019 nbest output based on their dissimilarity to the input.", "startOffset": 11, "endOffset": 109}, {"referenceID": 3, "context": "Similarly, Coster and Kauchak (2011) extend the PB-SMT model by adding phrase deletion. Wubben et al. (2012) make a further effort by reranking the Moses\u2019 nbest output based on their dissimilarity to the input. Most recently, Xu et al. (2016) have proposed a SB-SMT model, achieving better performance than Wubben\u2019s system.", "startOffset": 11, "endOffset": 243}, {"referenceID": 14, "context": "(2015) employ GloVe (Pennington et al., 2014) to generate synonyms for the complex words.", "startOffset": 20, "endOffset": 45}, {"referenceID": 6, "context": "Horn et al. (2014) extract over 30,000 paraphrase rules for lexical simplification by identifying aligned words in English Wikipedia and Simple English Wikipedia.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Horn et al. (2014) extract over 30,000 paraphrase rules for lexical simplification by identifying aligned words in English Wikipedia and Simple English Wikipedia. Glava\u0161 et al. (2015) employ GloVe (Pennington et al.", "startOffset": 0, "endOffset": 184}, {"referenceID": 6, "context": "Horn et al. (2014) extract over 30,000 paraphrase rules for lexical simplification by identifying aligned words in English Wikipedia and Simple English Wikipedia. Glava\u0161 et al. (2015) employ GloVe (Pennington et al., 2014) to generate synonyms for the complex words. Instead of using the parallel datasets, their approach only requires a single corpus. Paetzold et al. (2016) propose a new word embeddings model to deal with the limitation that the traditional models do not accommodate ambiguous lexical semantics.", "startOffset": 0, "endOffset": 376}, {"referenceID": 6, "context": "Horn et al. (2014) extract over 30,000 paraphrase rules for lexical simplification by identifying aligned words in English Wikipedia and Simple English Wikipedia. Glava\u0161 et al. (2015) employ GloVe (Pennington et al., 2014) to generate synonyms for the complex words. Instead of using the parallel datasets, their approach only requires a single corpus. Paetzold et al. (2016) propose a new word embeddings model to deal with the limitation that the traditional models do not accommodate ambiguous lexical semantics. Pavlick et al. (2016) release about 4,500,000 simple paraphrase rules by extracting normal paraphrases rules from a bilingual corpus and reranking the simplicity scores of these rules by a supervised model.", "startOffset": 0, "endOffset": 538}, {"referenceID": 6, "context": "Since there has been many efforts working on the establishment of word simplification pairs (Horn et al., 2014; Glava\u0161 and \u0160tajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016), we do not focus on the identification of words that require simplification or the methods of selecting what simpler words to switch.", "startOffset": 92, "endOffset": 198}, {"referenceID": 5, "context": "Since there has been many efforts working on the establishment of word simplification pairs (Horn et al., 2014; Glava\u0161 and \u0160tajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016), we do not focus on the identification of words that require simplification or the methods of selecting what simpler words to switch.", "startOffset": 92, "endOffset": 198}, {"referenceID": 11, "context": "Since there has been many efforts working on the establishment of word simplification pairs (Horn et al., 2014; Glava\u0161 and \u0160tajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016), we do not focus on the identification of words that require simplification or the methods of selecting what simpler words to switch.", "startOffset": 92, "endOffset": 198}, {"referenceID": 13, "context": "Since there has been many efforts working on the establishment of word simplification pairs (Horn et al., 2014; Glava\u0161 and \u0160tajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016), we do not focus on the identification of words that require simplification or the methods of selecting what simpler words to switch.", "startOffset": 92, "endOffset": 198}, {"referenceID": 17, "context": "In our paper, we apply the bi-directional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) with gated recurrent units (GRUs) (Cho et al.", "startOffset": 75, "endOffset": 103}, {"referenceID": 2, "context": "In our paper, we apply the bi-directional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) with gated recurrent units (GRUs) (Cho et al., 2014) for both the backward and for-", "startOffset": 138, "endOffset": 156}, {"referenceID": 24, "context": "(Zeiler, 2012) to optimize all parameters.", "startOffset": 0, "endOffset": 14}, {"referenceID": 8, "context": "It is a standard phrase-based machine translation model (Koehn et al., 2007).", "startOffset": 56, "endOffset": 76}, {"referenceID": 23, "context": "SBMT is a syntactic-based machine translation model (Xu et al., 2016), which is implemented on the open-source Joshua toolkit (Post et al.", "startOffset": 52, "endOffset": 69}, {"referenceID": 15, "context": ", 2016), which is implemented on the open-source Joshua toolkit (Post et al., 2013).", "startOffset": 64, "endOffset": 83}, {"referenceID": 13, "context": "The simplification model is optimized to the SARI metric and leverages the PPDB dataset (Pavlick and Callison-Burch, 2016) as a rich source of simplification operations.", "startOffset": 88, "endOffset": 122}, {"referenceID": 2, "context": "The sequence-to-sequence model is the state-of-the-art neural machine translation model (Cho et al., 2014) with the attention mechanism applied (Bahdanau et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 0, "context": ", 2014) with the attention mechanism applied (Bahdanau et al., 2014).", "startOffset": 45, "endOffset": 68}, {"referenceID": 7, "context": "Automatic Evaluation To evaluate the performance of different methods for the simplification task, we leverage four automatic evaluation metrics4: Flesch-Kincaid grade level (FK) (Kincaid et al., 1975), SARI (Xu et al.", "startOffset": 179, "endOffset": 201}, {"referenceID": 23, "context": ", 1975), SARI (Xu et al., 2016), BLEU (Papineni et al.", "startOffset": 14, "endOffset": 31}, {"referenceID": 12, "context": ", 2016), BLEU (Papineni et al., 2002) and iBLEU (Sun and Zhou, 2012).", "startOffset": 14, "endOffset": 37}, {"referenceID": 19, "context": ", 2002) and iBLEU (Sun and Zhou, 2012).", "startOffset": 18, "endOffset": 38}, {"referenceID": 25, "context": "Several studies indicate that BLEU alone is not really suitable for the simplification task (Zhu et al., 2010; \u0160tajner et al., 2015; Xu et al., 2016).", "startOffset": 92, "endOffset": 149}, {"referenceID": 21, "context": "Several studies indicate that BLEU alone is not really suitable for the simplification task (Zhu et al., 2010; \u0160tajner et al., 2015; Xu et al., 2016).", "startOffset": 92, "endOffset": 149}, {"referenceID": 23, "context": "Several studies indicate that BLEU alone is not really suitable for the simplification task (Zhu et al., 2010; \u0160tajner et al., 2015; Xu et al., 2016).", "startOffset": 92, "endOffset": 149}, {"referenceID": 22, "context": "Following earlier studies (Wubben et al., 2012; Xu et al., 2016), we asked participants to rate Gram-", "startOffset": 26, "endOffset": 64}, {"referenceID": 23, "context": "Following earlier studies (Wubben et al., 2012; Xu et al., 2016), we asked participants to rate Gram-", "startOffset": 26, "endOffset": 64}], "year": 2017, "abstractText": "Sentence simplification reduces semantic complexity to benefit people with language impairments. Previous simplification studies on the sentence level and word level have achieved promising results but also meet great challenges. For sentencelevel studies, sentences after simplification are fluent but sometimes are not really simplified. For word-level studies, words are simplified but also have potential grammar errors due to different usages of words before and after simplification. In this paper, we propose a two-step simplification framework by combining both the word-level and the sentence-level simplifications, making use of their corresponding advantages. Based on the twostep framework, we implement a novel constrained neural generation model to simplify sentences given simplified words. The final results on Wikipedia and Simple Wikipedia aligned datasets indicate that our method yields better performance than various baselines.", "creator": "LaTeX with hyperref package"}}}