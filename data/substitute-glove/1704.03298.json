{"id": "1704.03298", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "The MATLAB Toolbox SciXMiner: User's Manual and Programmer's Guide", "abstract": "The Matlab annotation SciXMiner is designed for the prototyping and behavioral of making five well exhibits with his special important and classification problems. It december developed opened for Institute of Applied Computer Science of long Karlsruhe Institute of Technology (KIT ), part founded with its Helmholtz Association created German Research Centres ago Germany. The achieve before to appropriate particular row mobile for into development on improvement is data mining rigorous many far automated will number received and technical troubles. SciXMiner guarding extended Matlab (although up the studio 2017a ). Many forms everybody not introduce actual code parvenus but even northern latter Signal, Statistics are Wavelet sinecures each make one role involve. The unless make an Matlab - provided solution being followed allow directly now wide mathematical servers including though package provided by The Mathworks Inc. SciXMiner still controlled by short semantic user interface (GUI) on appetizer additional some rule elements like three-yard selected, checkboxes over script elements. This comes so easier one once with SciXMiner which inexperienced users. Furthermore, it automatization it producing standardization among visualization problem possible allow macros. The standard Matlab beautiful conventional new control ran though such for. SciXMiner any to enter refers processing. The desktop original becomes", "histories": [["v1", "Tue, 11 Apr 2017 14:17:47 GMT  (2623kb,D)", "http://arxiv.org/abs/1704.03298v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ralf mikut", "reas bartschat", "wolfgang doneit", "jorge \\'angel gonz\\'alez ordiano", "benjamin schott", "johannes stegmaier", "simon waczowicz", "markus reischl"], "accepted": false, "id": "1704.03298"}, "pdf": {"name": "1704.03298.pdf", "metadata": {"source": "CRF", "title": "The MATLAB Toolbox SciXMiner: User\u2019s Manual and Programmer\u2019s Guide", "authors": ["Ralf Mikut", "Andreas Bartschat", "Wolfgang Doneit", "Jorge \u00c1ngel Gonz\u00e1lez Ordiano", "Benjamin Schott", "Johannes Stegmaier", "Simon Waczowicz", "Markus Reischl"], "emails": ["ralf.mikut@kit.edu"], "sections": [{"heading": null, "text": "The MATLAB Toolbox SciXMiner:\nUser\u2019s Manual and Programmer\u2019s Guide\nRalf Mikut, Andreas Bartschat, Wolfgang Doneit, Jorge \u00c1ngel Gonz\u00e1lez Ordiano, Benjamin\nSchott, Johannes Stegmaier, Simon Waczowicz, Markus Reischl\nKarlsruhe Institute of Technology (KIT), Institute for Applied Computer Science\nP.O. Box 3640, 76021 Karlsruhe, Germany\nPhone: ++49/721/608-25731, Fax: ++49/721/608-25702\nEmail: ralf.mikut@kit.edu\nVersion 2017a (12.04.2017)\nar X\niv :1\n70 4.\n03 29\n8v 1\n[ cs\n.L G\n] 1\n1 A\npr 2\n01 7\nii\nContents\nContents iii"}, {"heading": "1 Motivation 1", "text": ""}, {"heading": "2 Installation 3", "text": ""}, {"heading": "3 Methods 4", "text": "3.1 Handling and functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 Problem description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.3 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.4 Further toolboxes for classification (selection) . . . . . . . . . . . . . . . . . . . . . . . 8"}, {"heading": "4 Working with SciXMiner 9", "text": "4.1 Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4.2 Import and export of projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n4.2.1 Import of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4.2.2 Export of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.2.3 Manual creation of SciXMiner projects . . . . . . . . . . . . . . . . . . . . . . 13\n4.3 Automation of analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.4 Errors and warnings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.5 Generation of application-specific extension packages . . . . . . . . . . . . . . . . . . . 17 4.6 Known errors and problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18"}, {"heading": "5 Sample projects 19", "text": "5.1 Building data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 5.1.2 Preparation of the data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 5.1.3 Application of cluster algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.1.4 Time series prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n5.2 Iris data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.2.2 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.2.3 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29"}, {"heading": "6 Menu items 32", "text": "6.1 Menu items \u2019File\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n6.1.1 Load project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6.1.2 Save project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6.1.3 Save project as... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\niii\niv Contents\n6.1.4 Mean value project (based on the selected output variable) . . . . . . . . . . . . 32 6.1.5 Export data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6.1.6 Import data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.1.7 Fusion of projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.1.8 Apply SciXMiner batch file . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 6.1.9 Apply SciXMiner batch file (debug mode) . . . . . . . . . . . . . . . . . . . . . 35 6.1.10 Apply SciXMiner batch file (step and debug mode) . . . . . . . . . . . . . . . . 35 6.1.11 Normative data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.1.12 Data mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.1.13 Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.1.14 Exit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n6.2 Menu items \u2019Edit\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.2.1 Select . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.2.2 Extract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.2.3 Convert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 6.2.4 Delete . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 6.2.5 Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6.2.6 Outlier detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6.2.7 Category . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 6.2.8 Rename... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 6.2.9 Generating trigger time series . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 6.2.10 Edit trigger time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.2.11 Looking for missing data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.3 Menu items \u2019View\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.3.1 Classes for selected data points . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.3.2 Number of terms for selected data points . . . . . . . . . . . . . . . . . . . . . 48 6.3.3 Time series (TS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 6.3.4 Single features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6.3.5 Single features (multivariate) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.3.6 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.3.7 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 6.3.8 Aggregated features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 6.3.9 Output variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 6.3.10 Spectrogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 6.3.11 Morlet spectrogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 6.3.12 Cross and Auto Correlation Functions . . . . . . . . . . . . . . . . . . . . . . . 56"}, {"heading": "6.3.13 FFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58", "text": "6.3.14 Fuzzy systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 6.3.15 Decision tree (LaTeX) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 6.3.16 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 6.3.17 Self Organizing Maps... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 6.3.18 Data point distances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 6.3.19 Project report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.3.20 Recoding table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.4 Menu items \u2019Data mining\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.4.1 Selection and evaluation of single features . . . . . . . . . . . . . . . . . . . . . 61 6.4.2 Evaluation of time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 6.4.3 Evaluation of output variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\nContents v\n6.4.4 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 6.4.5 Time series classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 6.4.6 Hierarchical Bayes classifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 6.4.7 Fuzzy systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 6.4.8 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 6.4.9 Association analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 6.4.10 Self Organizing Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 6.4.11 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 6.4.12 Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6.5 Menu items \u2019Extras\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.1 Play macro... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.2 Play macro (debug mode)... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.3 Record macro... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.4 Stop macro record . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.5 Edit macro... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.6 Reset macro names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.7 Execute M-file... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 6.5.8 Edit M-file... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 6.5.9 Edit SciXMiner batch file ... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 6.5.10 Generate PDF project report (needs Latex) . . . . . . . . . . . . . . . . . . . . 71 6.5.11 Translate German Gait-CAD m-files and macros into English . . . . . . . . . . 72 6.5.12 Choose application-specific extension packages... . . . . . . . . . . . . . . . . . 72 6.5.13 Search path for m-files and plugins . . . . . . . . . . . . . . . . . . . . . . . . 72 6.5.14 Matlab Parallel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 6.6 Menu items \u2019Favorites\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.6.1 Edit user-defined favorites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.6.2 Delete all favorites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.7 Menu items \u2019Window\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.7.1 Close figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.7.2 Arrange figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.7.3 Logarithmic scaling of current figures . . . . . . . . . . . . . . . . . . . . . . . 74 6.7.4 Remove Latex codes in MATLAB figures . . . . . . . . . . . . . . . . . . . . . 74 6.7.5 Update font and font size in figures . . . . . . . . . . . . . . . . . . . . . . . . 74 6.7.6 Plot all figures as images in files . . . . . . . . . . . . . . . . . . . . . . . . . . 74 6.8 Menu items \u2019Help\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 6.8.1 Show SciXMiner documentation (PDF) . . . . . . . . . . . . . . . . . . . . . . 74 6.8.2 About SciXMiner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 6.8.3 License information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74"}, {"heading": "7 Control elements 75", "text": "7.1 Control elements for \u2019Project overview\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . 75 7.2 Control elements for \u2019Time series: General options\u2019 . . . . . . . . . . . . . . . . . . . . 76 7.3 Control elements for \u2019Time series: Extraction - Parameters\u2019 . . . . . . . . . . . . . . . . 78 7.4 Control elements for \u2019Plugin sequence\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . 81 7.5 Control elements for \u2019Single features\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 7.6 Control elements for \u2019Data preprocessing\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . 86 7.7 Control elements for \u2019View: Single features\u2019 . . . . . . . . . . . . . . . . . . . . . . . . 91 7.8 Control elements for \u2019View: Time series\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . 94\nvi Contents\n7.9 Control elements for \u2019View: Spectrogram, FFT, CCF\u2019 . . . . . . . . . . . . . . . . . . . 97 7.10 Control elements for \u2019View: Classification and regression\u2019 . . . . . . . . . . . . . . . . 100 7.11 Control elements for \u2019Data mining: Classification of single features\u2019 . . . . . . . . . . . 102 7.12 Control elements for \u2019Data mining: Classification of time series\u2019 . . . . . . . . . . . . . 105 7.13 Control elements for \u2019Data mining: Regression\u2019 . . . . . . . . . . . . . . . . . . . . . . 111 7.14 Control elements for \u2019Data mining: Clustering\u2019 . . . . . . . . . . . . . . . . . . . . . . 113 7.15 Control elements for \u2019Data mining: Special methods\u2019 . . . . . . . . . . . . . . . . . . . 115 7.16 Control elements for \u2019Data mining: Statistical options\u2019 . . . . . . . . . . . . . . . . . . 134 7.17 Control elements for \u2019Data mining: Validation\u2019 . . . . . . . . . . . . . . . . . . . . . . 136 7.18 Control elements for \u2019General options\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . 138"}, {"heading": "8 Feature extraction from time series 141", "text": "8.1 Definition of feature types by plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 8.2 Standard plugins in SciXMiner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 8.3 Plugins for single features from single features . . . . . . . . . . . . . . . . . . . . . . 144 8.4 Defining intervals via files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 8.5 Definition of a feature ontology by categories . . . . . . . . . . . . . . . . . . . . . . . 145"}, {"heading": "9 Conclusions and perspectives 147", "text": "A Important file structures 148\nB Important internal data structures 150"}, {"heading": "C Needed Standard Toolboxes 152", "text": "D Included External Toolboxes (GNU-License) 153"}, {"heading": "E Symbols and abbreviations 157", "text": ""}, {"heading": "F Known errors and problems 158", "text": "G Version history 160 G.1 Versions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 G.2 Selected changes between Gait-CAD version 2014b and SciXMiner version 2016b . . . 160 G.3 Selected changes between SciXMiner versions 2016b and 2017a . . . . . . . . . . . . . 161"}, {"heading": "H Plugins 162", "text": "Bibliography 181"}, {"heading": "1 Motivation", "text": "The Matlab toolbox SciXMiner is designed for the visualization and analysis of time series and features with a special focus to classification problems. It was developed at the Institute of Applied Computer Science of the Karlsruhe Institute of Technology (KIT), a member of the Helmholtz Association of German Research Centres in Germany. The aim was to provide an open platform for the development and improvement of data mining methods and its applications to various medical and technical problems.\nSciXMiner bases on Matlab (tested for the version 2017a). Many functions do not require additional standard toolboxes but some parts of Signal, Statistics and Wavelet toolboxes are used for special cases. The decision to a Matlab-based solution was made to use the wide mathematical functionality of this package provided by The Mathworks Inc. MATLAB R\u00a9and Simulink R\u00a9are registered trademarks of The MathWorks, Inc.\nSciXMiner is controlled by a graphical user interface (GUI) with menu items and control elements like popup lists, checkboxes and edit elements. This makes it easier to work with SciXMiner for inexperienced users. Furthermore, an automatization and batch standardization of analyzes is possible using macros. The standard Matlab style using the command line is also available.\nSciXMiner is an open source software. The download page is\nhttp://sourceforge.net/projects/SciXMiner/.\nIt is licensed under the conditions of the GNU General Public License (GNU-GPL) of The Free Software Foundation (see http://www.fsf.org/).\nThe toolbox bases on earlier internal versions of Gait-CAD [64, 72]. Please refer to [80] if you use SciXMiner for your scientific work.\nSciXMiner contains a base toolbox and various application-specific extension packages. Up to know, six extension packages were published:\n\u2022 analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7],\n\u2022 object tracking [102, 101],\n\u2022 tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]),\n\u2022 feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71],\n\u2022 image processing and feature extraction for segmentation of grains in asphalt samples [92], and\n\u2022 extended measures and visualization to evaluate the quality of data and regression models [28]."}, {"heading": "2 Chapter 1. Motivation", "text": "This manual is organized as follows: Chapter 2 explains the installation procedure. Chapter 3 outlines the implemented functionality of SciXMiner followed by some recommendations for working with SciXMiner. The analysis of two benchmark data sets is discussed in Chapter 5. Detailed information for the use of menu items (Chapter 6) and control elements (Chapter 7) follow. Chapter 8 present possible application-specific extensions for the feature extraction from time series.\nThe appendix provide information about file formats (Appendix A), internal data structures (Appendix B), necessary standard toolboxes (Appendix C) and integrated GNU-GPL-Matlab toolboxes of different groups (Appendix D), a list of symbols and abbreviations (Appendix E) and a list of known bugs and problems (Appendix F)."}, {"heading": "2 Installation", "text": "The file SciXMiner-Installer.exe contains all necessary files as self-extracting executable. After starting the destination folder for SciXMiner has to be selected, e.g. d:\\matlab\\scixminer\\.\nIn a next step, the recent user path of the installed MATLAB version must be defined. An example is C:\\Dokumente und Einstellungen\\firstname.surname\\My Documents\\MATLAB. SciXMiner uses a subdirectory called scixminer of this directory to save options files. The user needs writing permissions for this directory.\nA starting file called scixminer.m is individually created for this computer and stored in the user path. For a different user on the same computer, scixminer.m must be copied to his/her user path. Alternatively, scixminer.m can be copied to any other path of the MATLAB search path (matlabpath).\nAlternatively, a zipped version scixminer.zip can be used. For the installation, the following steps are necessary:\n1. extract in a directory,\n2. copy start file admin\\scixminer.m in a directory of the matlab work directory (check using command matlabpath),\n3. modify the directories in this file with the directory from the first step.\nSciXMiner is now ready installed and can be started with scixminer.\nFor developers, a SVN repository is available at: https : //svn.code.sf.net/p/scixminer/scixminer.\nPlease check SciXMiner out in a directory on your computer, e.g. d : scixminer."}, {"heading": "3 Methods", "text": ""}, {"heading": "3.1 Handling and functionality", "text": "Data mining methods are useful in searching for unknown or partially known relations in large data sets (KDD: Knowledge Discovery from Databases). A well known definition is given by [30]:\nData mining is a step in the KDD process that consists of applying data analysis and discovery algorithms that produce a particular enumeration of patterns (or models) over the data.\nPattern describes typical significant characteristics in features of the data set. Hereby, a feature is an input variable for the data mining algorithm, which is relevant with respect to the data mining problem. In this manual every input variable is regarded as possible feature, as it may be helpful solving the problem.\nStarting with a verbalized data mining problem, an adequate formalization has to be found. This formalization influences as well the collection of the training data set from an (external) data base (i. e. by special import routines, like HeiDATAProViT in gait analysis [94]) as the collective of possible evaluation measures in SciXMiner (Figure 3.1)."}, {"heading": "3.2. Problem description 5", "text": "SciXMiner permits a comfortable handling of numerous algorithms for the\n\u2022 selection of data points (i. e. detection of outliers, discarding of incomplete data points and features, selection of parts of data sets),\n\u2022 extraction of features (i. e. spectrograms, FFT analysis, correlation analysis, linear filtering, calculation of extrema, mean values, fuzzification etc.),\n\u2022 evaluation of features and their selection (i. e. multivariate analysis of variances, t-test, information measures, regression analysis),\n\u2022 aggregation of features (synonym: feature transformation, e.g. Discriminant Analysis, Principal Component Analysis (PCA), Independent Component Analysis (ICA)),\n\u2022 supervised and unsupervised classification (i. e. decision trees, cluster algorithms, Bayes classifier, Artificial Neural Networks (ANN), Nearest Neighbor algorithms, Support Vector Machines (SVM), fuzzy systems) and\n\u2022 validation strategies (i. e. cross-validation, bootstrap).\nAdditionally, there are various possibilities to visualize results, import and export data sets, automatically log results and process steps in text and LATEXfiles, rename variables etc.\nDepending on the availability, functions from different toolboxes are called, thereunder standard Matlab functions, functions from internal Matlab toolboxes (see Appendix C), free available Matlab toolboxes (FastICA1, SVM and Kernel Methods Matlab Toolbox [24]2, SOM Toolbox [109]3, lp_solve4, see Appendix D) and many SciXMiner internal functions."}, {"heading": "3.2 Problem description", "text": "A mandatory item to start a calculation is a training data set with n = 1, . . . , N data points, each containing\n\u2022 sz time series (matrices XTS [n] with xTS,r[k, n] \u2208 R, r = 1, . . . , sz , k = 1, . . . ,K sample points),\n\u2022 s features (vectors x[n] \u2208 R with xl[n], l = 1, . . . , s) and\n\u2022 sy discrete output variables (vector y[n] \u2208 N+ with yj [n], j = 1, . . . , sy).\nHere, R is the set of real numbers and N+ the set of natural numbers. Ordinal, interval-scaled, and rational-scaled features may be processed. Ordinal features are discrete in value with respect to order scale (i. e. quantities with values like very small, small, middle, and big). The values do not contain any information about the semantics of their distances. If distances between all values are the same the scale is called interval-scaled (i. e. temperature in [\u25e6C] or [\u25e6F]). Rational-scaled values additionally contain a natural zero-point (i. e. length in [m], temperature in [K]).\n1http://www.cis.hut.fi/projects/ica/fastica/ 2http://asi.insa-rouen.fr/\u223carakotom/toolbox/index.html 3http://www.cis.hut.fi/projects/somtoolbox/ 4http://lpsolve.sourceforge.net/5.5/"}, {"heading": "6 Chapter 3. Methods", "text": "Besides further information like a priori preferences for features may be processed.\nThe purpose is mainly the generation of static or quasi-static estimations\ny\u0302j [N + 1] =f(x[N + 1]) resp. (3.1)\ny\u0302j [N + 1] =f(x[N + 1](XTS [N + 1])) (3.2)\nfor a data point N + 1 with an unknown output variable, as well es the generation of intermediate results like tables or catalogues of relevant features for certain problems.\nThe management of multiple output variables (i. e. diagnoses with respect to diseases in medical applications, decisions for therapies, qualitative evaluations of therapy successes, gender, age-groups etc.) for each data point allows a flexible selection of multiple classification problems. Additionally, input and output variables may be changed in dependence of the problem.\nThe training data set is given by a binary Matlab project file, containing matrices with standardized names (i. e. d_orgs for time series, d_org for features and code_alle for output variables). Additionally, matrices and structures are possible (not mandatory) to denote textual identifiers and further information. Missing information is compensated by standard values and identifiers. Using 1 GB of RAM a SciXMiner project file may have a size up to 500 MB without causing any problems with memory. Larger project files are problematic, due to the chosen data structures (arrays instead of cell arrays)."}, {"heading": "3.3 Further reading", "text": "A comprehensive illustration of all algorithms mentioned in this section can not be done within this manual. Therefore, some examples for further reading are given:\n\u2022 basic knowledge about multivariate statistics and classification [107, 69, 49] and specialties for time series [73],\n\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58],\n\u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64],\n\u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]),\n\u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]\n\u2022 a priori relevances [114, 76],\n\u2022 Independent Component Analysis [47, 106],\n\u2022 Support Vector Machines [18, 24, 98],\n\u2022 k-Nearest Neighbor Methods [26],\n\u2022 Artificial Neural Networks [42],\n\u2022 feature aggregation and selection using wrapper approaches [88],\n\u2022 validation strategies [69], and"}, {"heading": "3.3. Further reading 7", "text": "\u2022 alternative data mining software as Weka [41], Knime [14], Apache Spark\u2019s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]\nSelected SciXMiner applications are listed in Table 3.1.\nFurther details about parameterization are given in the description of menu items (Chapter 6) and control elements (Chapter 7)."}, {"heading": "8 Chapter 3. Methods", "text": ""}, {"heading": "3.4 Further toolboxes for classification (selection)", "text": "\u2022 PRTools (http://www.prtools.org/) University of Delft (The Netherlands): Matlab toolbox, e.g. Principal Component Analysis, Subspace classifiers, Artificial Neural Networks, without GUI, free for academic use\n\u2022 NEFCLASS (http://fuzzy.cs.uni-magdeburg.de/nefclass/) University Magdeburg (Germany): JAVA, e.g. Neuro-Fuzzy classifiers, free for academic use"}, {"heading": "4 Working with SciXMiner", "text": ""}, {"heading": "4.1 Handling", "text": "The graphical user interface (GUI) of SciXMiner contains menu items and control elements like listboxes, checkboxes and text fields. They are implemented using Matlab functions (uicontrol, uimenu), which are partially called by encapsulated SciXMiner functions with additional functionality. These elements are using callback functions to exchange data with variables from the workspace and call functions which are independent from the GUI. Thus, the Matlab-typical way of programming using a command prompt and variables is possible, too.\nThe elements of the graphical user interface are described in the following sections."}, {"heading": "4.2 Import and export of projects", "text": "SciXMiner offers the opportunity for an import and export of projects from and into ASCII files. The possible options will be explained in the next two sections with a focus to the import and export of time series. Some differences for the export of single features will be also explained."}, {"heading": "4.2.1 Import of data", "text": "Data can be imported from one file or from many files in a directory. The latter option is also possible for existing subdirectories, but requires special naming conventions.\nFor the import of time series, each data point is read from a separate file. It contains the time series in the columns and the sample points in the rows. For single features, all data points are normally read from one file containing the single features in the columns and the data points in the rows. An import from multiple files is also possible to characterize different output classes by the file and directory names.\nThe window in Fig. 4.1 will be shown for an import using File - Import data - From a directory....\nThe option \u201dSearching in subdirectories\u201d defines if subdirectories are scanned for matching files or not. The option \u201dWrite in separate projects\u201d defines that separate SciXMiner projects are generated for each import file. These separate projects can later fused by File - Fusion of projects.\n\u201dFile extension\u201d specifies all files for the import. Wildcards are not possible (e.g. .t*)! The \u201dSeparator for output variables\u201d controls the assignment of output variables. As a consequence, it is extremely important for a comfortable assignment of output variables to data points - especially for time series. It can be specified for directory names and for file names."}, {"heading": "10 Chapter 4. Working with SciXMiner", "text": "Example:\nThe import of three files\nd:\\prj\\names\\Anna\\Post-therapeutic.txt\nd:\\prj\\names\\Anna\\Pre-therapeutic.txt\nd:\\prj\\names\\Peter\\Post-therapeutic.txt\nfrom a folder d:\\prj\\names and the file extension \u2019*.txt\u2019 generates two terms for the first output variable variables (\u201dAnna\u201d, \u201dPeter\u201d) and two terms for the second output variable (\u201dPre-therapeutic\u201d, \u201dPost-therapeutic\u201d).\nFor File - Import data - From a file..., the single file to be imported will be selected by a standard window. After this, a slightly different window is shown to specify the import, but the lower part of the windows agrees with Fig. 4.1.\nInside a file, two different separators are expected for the separation of columns (e.g. tabulators for different time series or single features) and decimal numbers (e.g. \u201dPoint\u201d: 17.3, \u201dComma: 17,3\u201d). Such different separators for decimal numbers are necessary for compatibility reason for some foreign language versions as e.g. German software.\nThe option \u201dFirst row contains names\u201d reads the names of the single features and time series from the first row of the import file. Furthermore, a fix number of the rows at the beginning of the file (\u201dRead from"}, {"heading": "4.2. Import and export of projects 11", "text": "row\u201d) resp. all rows starting with a specified sign (\u201dIgnore rows starting with\u201d) can be ignored. This is especially useful for the import of files with a header. Empty rows will be ignored too.\nThe option \u201dImport as\u201d switches between an import of single features and an import of time series. A simultaneous import is not possible at the moment.\nThe option \u201dNormalization of time series length\u201d is only necessary if time series with different length will be imported. SciXMiner assumes always identical lengths. Different options exist: a resampling by the standard Matlab command (from the signal toolbox) to get the length of the longest time series, a filling of shorter time series with zeros or the last valid value.\nThe last option \u201dImport with\u201d switches between different import techniques. The \u201dNormal mode\u201d generates a temporary file after a possible separator correction. It is normally the best version especially for large files. \u201dWrite copy and read again\u201d make the same line by line. \u201dLine-Option\u201d is a standard line-wise reading by Matlab.\n\u201dStandard-ASCII\u201d uses the Matlab command load -ascii and works only, if the file contains only numbers (without variable names etc.). This option has a reduced functionality but it can be very fast.\n\u201dStructured (with Strings)\u201d tries to import numbers and strings for files with single features. The strings are converted into linguistic terms of output variables. This option should be used together with the option \u201dWrite in separate projects\u201d.\n\u201dImportdata (MATLAB)\u201d uses the MATLAB function \u201dimportdata\u201d. It tries to import numbers and strings for files with single features. The strings are converted into linguistic terms of output variables. All columns with a string element in the first data row are interpreted as string columns. Such data is read from \u201dimportdata\u201d as \u201dtextdata\u201d. This option is faster than \u201dStructured (with Strings)\u201d."}, {"heading": "4.2.2 Export of data", "text": "For the export of time series, each data point is written into a separate file. It contains the time series in the columns and the sample points in the rows. The possible options are controlled be the window in Fig. 4.2.\nThe target directory must be defined manually for compatibility reasons with Matlab Version 5.3 (due to a missing window for the selection of directories). A copy from the clipboard is possible. The target directory must exist.\nThe directory and file structure names are mainly determined by the following options.\n\u201dWrite output variables in subdirectories\u201d generates subdirectories for the first (n \u2212 1) output variables and codes the n-th output variable in the file name. The file for each data point is saved in the directories and file with the corresponding name.\nOtherwise, the file name is generated by all terms. The parts are separated by the separator defined by \u201dSeparator for the output variables for file names\u201d. The option \u201dNone\u201d works without separators.\nIf more than one data point for a term combination exists, a further directory splitting will be done. In this case, the file name is only the number of the data point.\n\u201dFile extension\u201d control the extension for the generated files.\nExample:"}, {"heading": "12 Chapter 4. Working with SciXMiner", "text": "An export of time series into the destination folder d:\\prj\\names in a project with the output variables \u201dNames\u201d (terms: \u201dAnna\u201d, \u201dMaria\u201d, \u201dThomas\u201d, \u201dPeter\u201d) and \u201dExamination\u201d (terms: \u201dPre-therapeutic\u201d, \u201dPost-therapeutic\u201d) give the following file structure using the option \u201dWrite output variables in subdirectories\u201d:\nd:\\prj\\names\\Anna\\Post-therapeutic.txt\nd:\\prj\\names\\Anna\\Pre-therapeutic.txt\nd:\\prj\\names\\Peter\\Post-therapeutic.txt\nd:\\prj\\names\\Peter\\Pre-therapeutic.txt\nd:\\prj\\names\\Maria\\Post-therapeutic.txt\nd:\\prj\\names\\Maria\\Pre-therapeutic.txt\nd:\\prj\\names\\Thomas\\Post-therapeutic.txt\nd:\\prj\\names\\Thomas\\Pre-therapeutic.txt\nIf more than one data point for a term combination exist, the directory and file names are slightly different, e.g.:\nd:\\prj\\names\\Anna\\Post-therapeutic\\1.txt\n4.2. Import and export of projects 13\nd:\\prj\\names\\Anna\\Post-therapeutic\\5.txt\nThe separators for columns and decimal numbers are the same as for the import.\nThe option \u201dWrite names in first row\u201d writes the names of the single features and time series in the first row of the export file.\n\u201dExport only selected data points and features\u201d uses the recent selection in the project to reduce the data for the export.\nFor the export of single features, all data points are written in one file. It contains the single features in the columns and the data points in the rows.\nTwo differences exist for the export of single features:\n\u2022 A dialog box is shown for a file selection. Single features will be always exported into one file.\n\u2022 An additional option \u201dExport output variables as single features\u201d add all output variables to the single features for the export. It simplifies the matching by data points and output classes. For time series, this task is solved by the file and directory names."}, {"heading": "4.2.3 Manual creation of SciXMiner projects", "text": "In this section the creation of a SciXMiner project file is described, either manual or semi-automatic, if a fully automatic import is not possible. An overview of identifiers which are used by SciXMiner is given in Appendix B.\nSciXMiner projects are binary Matlab files. These files contain variables and at least one of the following identifiers has to be defined within the project file:\n\u2022 d_orgs: contains time series of the project. 3D-array with dimension N \u00d7K \u00d7 sz\n\u2022 d_org: contains features of the project. 2D-array with dimension N \u00d7 s.\nN denotes the number of data points, K denotes the number of sample points of the time series, sz is the number of time series and s is the number of features (see Section 3.2).\nThe most comfortable way is to use the function generate_new_scixminer_project.m. This function supports the conversion of MATLAB variables into SciXMiner projects. All possible elements are explained in the following text and in the help text of the function, including some examples for the conversion.\nIf output variables are known, they may be saved in an identifier called code_alle.\nThis identifier contains an N \u00d7 sy matrix with the sy different types of output variables in the columns and the class assignments of N data points in the rows. Thereby, each class is represented by a (positive) integer number. If this identifier is not defined, the class assignment of each data point is set to 1.\nOptionally, the following identifier may be defined. In order to simplify the handling of SciXMiner, the definition is recommended:"}, {"heading": "14 Chapter 4. Working with SciXMiner", "text": "\u2022 Names for output variables may be given in the identifier bez_code. This identifier contains a string-matrix, where the number of rows has to match the number of output variables. The number of columns depends on the length of the strings. An easy method to create bez_code is the integrated Matlab function strvcat. If bez_code is not defined, the output variables are named \u201dy1\u201d, \u201dy2\u201d, ...\n\u2022 The classes of the output variable may also be named by an identifier. Therefore, the struct zgf_y_bez with the dimension sy \u00d7max(my,i) is used. my,i denotes the number of linguistic terms of the i-th output variable. The identifier is contained in the field name.\nExample: There are two output variables, containing two respectively four classes. The identification of these classes can be done using zgf_y_bez(1,1).name = \u2019Anna\u2019; zgf_y_bez(1,2).name = \u2019Maria\u2019; zgf_y_bez(1,3).name = \u2019Thomas\u2019; zgf_y_bez(1,4).name = \u2019Peter\u2019; zgf_y_bez(2,1).name = \u2019pre-therapeutic\u2019; zgf_y_bez(2,2).name = \u2019post-therapeutic\u2019;\n\u2022 The identifiers of time series are arranged in a matrix called var_bez. The number of rows of the matrix is equal to the number of time series within the project. The number of columns depends on the length of the identifiers (equal to bez_code).\n\u2022 The identifiers of features are arranged in a matrix called dorgbez. The number of rows of the matrix is equal to the number of features within the project. The number of columns depends on the length of the identifiers (equal to bez_code).\nAnother two variables may be inserted optionally, although the use is rather untypical:\n\u2022 Project-specific struct projekt: This struct might contain additional information for the project. It will read out during loading the project and are written to parameter.projekt.\n\u2022 To weight features so called a priori relevances may be used. They are multiplied by the calculated feature relevances. Thus, features with doubtful values based on difficult environments for sensor measurements may be downgraded, to force the feature selection to use reliable features for classification. These a priori relevances have to be stored in a variable called interpret_merk. interpret_merk is a s\u00d7 1 vector.\nThe project has to be saved using the suffix \u201d.prjz\u201d. An easy way is the use of the Matlab command save. As additional parameter -mat has to be used."}, {"heading": "4.3 Automation of analysis", "text": "The main strategy elements for the automation of analysis processes are\n\u2022 macros,"}, {"heading": "4.3. Automation of analysis 15", "text": "\u2022 SciXMiner batch files and\n\u2022 generated PDF files with project results.\nMacros are files containing sequences of clicked menu items and control elements. A manual modification of this file is possible due to its textual Matlab syntax. They can be recorded, executed, and modified (see remarks at Extras - Record macro... and example in Section 5.1).\nTo Save a SciXMiner project automatically in a macro, the name of the project must be manually defined inside a macro to avoid the input from a GUI. This can be done by adding the following lines in a macro:\n%adds \u2019_new\u2019 to the name of the current SciXMiner project, %e.g. myproject.prjz will be saved as myproject_new.prjz next_function_parameter = [parameter.projekt.datei \u2019_new\u2019];\n%% save the new project eval(gaitfindobj_callback(\u2019MI_Speichern\u2019));\nTo execute a macro inside a other macro, the following lines must be added in the calling macro:\n%defines the name of the macro %here: computing the exponential function for all selected single features next_function_parameter = \u2019feature_plugin_exp.makrog\u2019;\n%execute the macro execute_macro_inside_macro;\nA SciXMiner batch file is a file for the automatic execution of macros for one or more projects. The batch file might contain one project or a complete directory with all SciXMiner projects in all subdirectories. Optionally, a file with a list of control elements with the extension *.uihdg is loaded by File - Options - Load options. For each of these projects, the following list of macros or *.m-files or variable assignments into the fields of the variable gaitcad_extern.user is executed. These variable assignments have to be complete matlab commands. They are valid for all following macros. A recursive definition with other batch files is possible.\nDuring execution, the related projects are loaded and the macros are executed. All files must be exist. Errors does not caused a stop of the execution. Errors are only written in a file called error.log.\nThe files must hold the following scheme:\n1-Inf x following block: [ 1 x *.batch file or [ 1 x *.prjz file or directory 0-1 x *.uihdg file 1-Inf x *.makrog file or *.m file or variable assignments in gaitcad_extern.user]]\nExample for a SciXMiner batch file:\nbuilding.prjz building_regression.makrog show_report.makrog building_trigger.makrog building_teilen.makrog building_day.prjz building_cluster.makrog gaitcad_extern.user.test = 1; show_report.makrog"}, {"heading": "16 Chapter 4. Working with SciXMiner", "text": "All project, batch, macro, option and m files can be written with absolute directory names. Relative directory names should be predefined in the variable gaitcad_extern.user. This strategy simplifies the modification of SciXMiner batch files with complex function calls, e.g., for the transfer to a computer with a different folder structure. Here, a project and a macro directory as well as the recent working directory (pwd) are supported.\nAs an example, the directories can be defined in a SciXMiner batch file called local_directories.batch:\ngaitcad_extern.user.project_directory = \u2019d:\\projects\u2019 gaitcad_extern.user.macro_directory = \u2019d:\\macros\u2019\nThe main SciXMiner batch file can be organized as follows:\nlocal_directories.batch project_directory\\building.prjz macro_directory\\building_regression.makrog macro_directory\\show_report.m macro_directory\\building_trigger.makrog macro_directory\\building_teilen.makrog project_directory\\building_day.prjz macro_directory\\building_cluster.makrog gaitcad_extern.user.test = 1; macro_directory\\show_report.m\nSciXMiner batch files can be started from the GUI or using the Matlab command line:\ngaitbatchfile = \u2019d:\\matlab\\scixminer\\prj\\building_demo.batch\u2019;scixminerbatch;\nA special option is the use of an empty project called noproject.prjz. After loading, the menu items can be enabled. Consequently, otherwise disabled menu items as e.g. the import of text files can be executed.\nPDF files can be generated for the documentation of project containing figures and latex files. This option requires a Latex installation (Open-Source). Further remarks can be found under Extras - Generate PDF project report (needs Latex).\nUsing these strategy elements, even larger projects with a high computational effort or complete directories with many projects can be processed automatically."}, {"heading": "4.4 Errors and warnings", "text": "Many errors and warnings in functions are displayed in a separated message box (see example for a warning message in Fig. 4.3).\nThree different options are available for a warning message:\n\u2022 \u2019Continue\u2019: The function will be continued.\n\u2022 \u2019Cancel\u2019: The function will be canceled. An error message \u2019Canceled by user!\u2019 will be shown in the Matlab command window.\n\u2022 \u2019Ignore all warnings\u2019: It is similar to \u2019Continue\u2019, but all future warning methods will be ignored for the rest of the SciXMiner session."}, {"heading": "4.5 Generation of application-specific extension packages", "text": ""}, {"heading": "4.5. Generation of application-specific extension packages 17", "text": ""}, {"heading": "18 Chapter 4. Working with SciXMiner", "text": "with functions as e.g. menu_elements_diagnosis.m (Menu items), control_elements_diagnosis.m (Control elements), optionen_felder_diagnosis.m (location of control elements). In addition, the templates for callback functions are can be used for the integration of own functions using any parameter from the control elements or available variables."}, {"heading": "4.6 Known errors and problems", "text": "The toolbox was tested using Matlab 2017a. However, we may have overlooked one or another bug. To some extent, there arise problems based on Matlab itself. Especially, many problems can be traced back to a missing downward compatibility from Matlab which leads to differences in the syntax of some commands and demands for long-winded compromises.\nRegarding the activation and deactivation of menu items, we decided to activate some menu items although the recently chosen parameters do not permit the execution of the underlying function. However, in this way the user is enabled to recognize and to correct his mistake. If the menu items remained deactivated until correct parameters are chosen, the user would have to try various combinations without getting an output.\nErrors in functions are normally caught by separate warning and error messages. Possible Matlab editor settings \u201dStop if Error\u201d or \u201dStop if Warning\u201d lead the user in the case of warnings or errors to the corresponding source code. For safe use of Matlab, these options are better deactivated.\nKnown bugs and problems can be looked up in detail in Appendix F."}, {"heading": "5 Sample projects", "text": ""}, {"heading": "5.1 Building data set", "text": ""}, {"heading": "5.1.1 Introduction", "text": "The goal of the Building-data set was to examine the effect of different parameters on the power consumption of a known building [59]. It contains a single example with ten time series (e.g. year, month, day, hour, energy consumption, temperature) and no further features. The time series are recorded for 175 days, 24 measures a day (every hour).\nWith this data set the following topics are addressed:\n\u2022 splitting of time series and manipulation of data sets\n\u2022 usage of macros and plugins\n\u2022 application of cluster algorithms\nThe given problem is the examination and comparison of the power consumption over a day.\nThe subdirectory \"prj\" of the SciXMiner installation contains the Building data set as a SciXMiner project file (building.prjz)."}, {"heading": "5.1.2 Preparation of the data set", "text": "For the analysis of the power consumption over a day the time series have to be split in several data points, so that only the data of a single day is included in the time series. This is done via Edit - Convert - Separate time series with trigger events.... The time series is split with respect to a given trigger signal.\nA typical application is the extraction of segments of time series out of long recordings. Examples for trigger events are the occurrence of errors, start of a new measurement or external events. The trigger signal contains only zeros, except the sample points where a trigger event occurs. At these sample points, the amplitude of the trigger signal equals the linguistic term of the output variable.\nFirst, since this data set contains no defined trigger events, a trigger signal has to be created. Three different options exist\n\u2022 using a graphical editor for a manual definition by Edit - Generating trigger time series resp. Edit - Edit trigger time series,"}, {"heading": "20 Chapter 5. Sample projects", "text": "\u2022 using a macro, or\n\u2022 using a plugin for extraction of features or time series.\nThe result is the same for all approaches: a time series containing values unequal to 0 at trigger events. This time series is called trigger signal.\nIn this project, the trigger events are extracted out of a time series containing the hours of a day. The trigger events are defined as the sample points at which the hour declines from 23 to 0.\nFor the sake of completeness, we consider all three approaches for the extraction of the trigger events:"}, {"heading": "5.1.2.1 Graphical generation and editing of a trigger time series", "text": "A trigger time series can be generated or edited with the window in Fig 5.1. The details are explained in Edit - Generating trigger time series. Fig. 5.1 shows four trigger events for the sample points 23, 47, 71 and 95 with two different classes for the output variable (weekend: 1 and working day: 2). The displayed time series show the \u201dHour\u201d and the \u201dEnergy\u201d. The temporal resolution is tuned by the two fields in the lower left corner (left: length of the shown time series segment, here: 110, right: each n-th element is shown, here: 1)."}, {"heading": "5.1. Building data set 21", "text": "The disadvantage of this methods is very high manual effort for long time series. Consequently, the two following methods are better:"}, {"heading": "5.1.2.2 Extraction of trigger signal by use of macros", "text": "The following code have been created automatically by recording of a macro. The used time series and interval for the extraction are specific for every project. Thus, a macro is generally not usable for different projects.\nThe recording contains the following commands (applied directly in the GUI):\n\u2022 Extras - Record macro...\n\u2022 Edit - Extract - Time series -> Time series, Time series -> Single features...\n\u2022 Selection of time series \"Day\", interval \"Whole time series\" and plugin \"Velocity (V)\". Finish the selection by a click on \"OK\"\n\u2022 Extras - Stop macro record\nThe content of the macro file is as follows:\n% MAKRO AUSWAHLFENSTER Z e i t r e i h e \u2212> Z e i t r e i h e , Z e i t r e i h e \u2212> Einze lmerkmal 2 auswahl . gen = [ ] ;\nauswahl . gen {1}={ \u2019DAY\u2019 } ; auswahl . gen {2}={ \u2019 Whole t ime s e r i e s ( 0 . . . 1 0 0 % ) \u2019 } ;\n5 auswahl . gen {3}={ \u2019 V e l o c i t y (V) \u2019 } ; e v a l ( g a i t f i n d o b j _ c a l l b a c k ( \u2019 MI_Extraktion_ZRZR \u2019 ) ) ; e v a l ( g e t ( f i g u r e _ h a n d l e ( s i z e ( f i g u r e _ h a n d l e , 1 ) , 1 ) , \u2019 c a l l b a c k \u2019 ) ) ;\nDue to the computation of the velocity in SciXMiner by the formula\nf \u2032(t) = f(t+ 1)\u2212 f(t\u2212 1)\n2\nthe trigger signal contains two sample points with values unequal to zero at the trigger events. Thus, the macro has to be expanded by the following code.\n% The new t ime s e r i e s i s t h e l a s t one 2 d a y _ a b l = s q u e e z e ( d _o rgs ( 1 , : , end ) ) ;\n% S ea rc h f o r a l l s amples wi th v a l u e u n e q u a l t o z e r o indx = f i n d ( d a y _ a b l ~= 0) ;\n5 % Two s u b s e q u e n t samples a r e z e r o . The second % i s t h e wanted sample . S e t t h e f i r s t t o z e r o . d_ o r gs ( 1 , i ndx ( 1 : 2 : end ) , end ) = 0 ; 8 % The f i r s t day i s n o t i d e n t i f i e d by t h i s a p p r o a c h . % S e t i t manua l ly : d_ o r gs ( 1 , 1 , end ) = 1 ;\n11 % The a m p l i t u d e o f t h e t r i g g e r s i g n a l c o r r e s p o n d s t o t h e % c l a s s o f t h i s t r i a l . Add t e m p o r a r i l y a new c l a s s % c o n t a i n i n g t h e s u b s e q u e n t number o f t h e day : 14 i ndx = f i n d ( s q u e e z e ( d _o rgs ( 1 , : , end ) ) ~= 0) ; d_ o r gs ( 1 , indx , end ) = [ 1 : l e n g t h ( i ndx ) ] ; % C l e a r used v a r i a b l e s 17 c l e a r d a y _ a b l i ndx ;"}, {"heading": "22 Chapter 5. Sample projects", "text": "Applying this macro by Extras - Play macro... leads to a new time series which is called \"Day V\".\nThe subdirectory \"prj\" of the SciXMiner installation contains the file of the macro (comments in German) (building_trigger.makrog)."}, {"heading": "5.1.2.3 Extraction of trigger signal by plugins", "text": "A further approach of extracting a trigger signal is the usage of plugins. A plugin is a function which is automatically included in SciXMiner. The related M file has to be in the subdirectory \"\\plugins\\mgenerierung\". The code of a plugin for the extraction of the trigger signal is as follows (for a detailed description of plugins refer to Chapter 8):\n1 f u n c t i o n [ da tenOut , r e t , i n f o ] = p l u g i n _ n s p r u n g ( p a r a s , d a t e n I n )\na n z _ z r = 1 ; 4 i n f o = s t r u c t ( \u2019 b e s c h r e i b u n g \u2019 , \u2019 Jump t o zero \u2019 , \u2019 b e z e i c h n e r \u2019 , \u2019 NSprung \u2019 , \u2019\nanz_zr \u2019 , anz_zr , \u2019 anz_em \u2019 , 0 , \u2019 l a e n g e _ z r \u2019 , p a r a s . p a r . l a e n g e _ z e i t r e i h e , \u2019 typ \u2019 , \u2019TS \u2019 ) ;\ni n f o . einzug_OK = 0 ; i n f o . r i c h t u n g _ e n t f e r n e n = 0 ;\n7 i n f o . a n z _ b e n o e t i g t _ z r = 1 ;\ni n f o . e x p l a n a t i o n = s t r c a t ( \u2019 s e t v a l u e = 1 f o r a jump t o z e r o s , and v a l u e = 0 o t h e r w i s e . \u2019 ) ;\n10\ni f ( n a r g i n < 2 | i s e m p t y ( d a t e n I n ) ) d a t e n O u t = [ ] ;\n13 r e t = [ ] ; r e t u r n ;\nend ; 16\n%d e t e c t z e r o v a l u e s d a t e n O u t . d a t _ z r = ( d a t e n I n . d a t == 0) ;\n19\n%d e t e c t a jump form a non\u2212z e r o v a l u e t o a z e r o v a l u e d a t e n O u t . d a t _ z r ( : , 2 : end , : ) = d a t e n O u t . d a t _ z r ( : , 2 : end , : ) & ( d a t e n I n . d a t ( : , 1 :\ns i z e ( d a t e n I n . da t , 2 ) \u22121 , : ) ~=0) ; 22\nr e t . u n g u e l t i g = 0 ; r e t . b e z e i c h n e r = \u2019 NSprung \u2019 ;\nA sample of the new time series is zero if the sample of the original time series is zero and the preceding sample of the original time series is unequal to zero. For the extraction of the trigger signal we have to apply the plugin by Edit - Extract - Time series -> Time series, Time series -> Single features... with the selections \"Original data (time series)\": \"Hour\", \"interval\": \"Whole time series\" and plugin \"Jump to zero (NSprung)\". This plugin is unable to detect the first day, since there is no \"jump\" to zero. Thus, the splitting of the time series by usage of the macro leads to one data point more than with extraction by plugin.\nThe subdirectory \"prj\" of the SciXMiner installation contains the file of the plugin (plugin_nsprung.m). It is automatically detected when a project is loaded."}, {"heading": "5.1. Building data set 23", "text": ""}, {"heading": "5.1.2.4 Splitting of the time series", "text": "After extracting the trigger signal, the project is ready to be converted. This is done by Edit - Convert - Separate time series with trigger events.... The options are set as shown in the following figure (use \"Hour NSprung\" as time series if the trigger signal is extracted by the plugin).\nIf the first option is set to \"Workspace\" instead of \"time series\", variables from the workspace can be used as trigger signals. Since the extraction of the single trials should be start exactly at a trigger event and end 23 sample points after the trigger event (resulting in 24 hours after the trigger event), the offset is set to [0, 23].\nBy clicking on \"OK\" the project is saved with a new name and loaded automatically. The new project contains the same amount of time series (10), but 175 examples instead of 1 (resp. 174 after extracting the trigger signal by the plugin)."}, {"heading": "5.1.3 Application of cluster algorithms", "text": "In this section the application of clustering is shown. It is applied to the time series \"Energy\" in the project containing the split time series. If you did not apply the conversion of the time series described in the previous sections please use the project \"building_day.prjz\", included in the subdirectory \"prj\" of the SciXMiner installation.\nIn the options window Control elements: Time series: General options the time series \"Energy\" is selected. The settings for the cluster algorithm are shown in Fig. 5.3.\nThe clustering is started by a click at Data mining - Clustering - Design and apply. If the computation is finished the result can be visualized by View - Clustering - Cluster memberships (sorted by data points).\nThe clusters correspond to weekday (Cluster 2, green), resp. weekend or vacations (Cluster 1, red). The rather long red interval belongs to Christmas time. To visualize the single time series with respect to the cluster membership, the following steps have to be accomplished (as a prerequisite, Control element:"}, {"heading": "24 Chapter 5. Sample projects", "text": "Data mining: Clustering - Append cluster as output variable has to be set to \"new cluster number\"): Set the output variable to the new one created by application of the cluster algorithm (option window Control elements: Time series: General options) and the time series to \"Energy\". By clicking at View -"}, {"heading": "5.1. Building data set 25", "text": "Time series (TS) - Original time series every example of time series \"Energy\" is plotted as single curve (cf. Fig. 5.5). The class means of the time series are plotted by clicking at View - Time series (TS) - Mean time series.\nAll commands are included in the macro building_cluster.makrog in subdirectory \"prj\" of the SciXMiner installation."}, {"heading": "5.1.4 Time series prediction", "text": "In this section the prediction of the energy time series of the building dataset is shown. The used settings for the regression are included in Fig. 5.6. All time series must be selected, including the energy time series!\nIn this example, all selected time series are used for the regression. An example using linear regression coefficients for a feature preselection is shown later using the iris dataset. The samples of the previous day are used as features for the regression (samples k \u2212 24, k \u2212 25, . . . , k \u2212 48). To edit the samples normal Matlab syntax is allowed (e.g. -48:-24 or -72:-24:-168). The sample 0 is automatically removed from the vector to avoid trivial prediction.\nNecessary settings for the polynomial are the degree (here: 2, Control element: Data mining: Special methods - Polynomial degree) and the maximal number of internal features (here: 4, Control element: Data mining: Special methods - Maximal number of internal features). The maximal number of internal features specifies how many coefficients should be computed by the regression algorithm. A selection of 10 time series and 24 samples leads to a total amount of possible coefficients of 240. In this case, 4 are selected by the regression algorithm."}, {"heading": "26 Chapter 5. Sample projects", "text": "By clicking Data mining - Regression - Design and apply the regression algorithm computes the coefficients and a prediction of the time series. A graphical examination of the result can be obtained by the functions included in View - Regression (see Fig. 5.7 for a scatter plot of the output variable and the estimation).\nThe scatter plot includes all values of the output variable versus the estimation of the output variable for all samples. A disadvantage of this plot is the lost time information. But since the prediction of the output variable is added as a new time series to the project all build-in visualizations can be used, e.g. the visualization of the original time series versus the predicted time series. To force SciXMiner to plot both time series in the same subplot, the option Control element: View: Time series - Show time series as subplots must be turned off. A section of such a plot is included in Fig. 5.8.\nBy Data mining - Evaluation of time series - Linear regression coefficients (univariate) univariate re-"}, {"heading": "5.2.2 Classification", "text": ""}, {"heading": "5.2.1 Introduction", "text": ""}, {"heading": "5.2 Iris data set", "text": ""}, {"heading": "5.2. Iris data set 27", "text": ""}, {"heading": "150 data points 2 features ... Complete ...", "text": ""}, {"heading": "28 Chapter 5. Sample projects", "text": ""}, {"heading": "5.2.3 Regression", "text": ""}, {"heading": "5.2. Iris data set 29", "text": ""}, {"heading": "30 Chapter 5. Sample projects", "text": "this example the degree of the polynomial is set to 1 and the maximal number of internal features to 4 (see the example using the building dataset for a description of this option).\nThe design and application of the regression is started by Data mining - Regression - Design and apply. After exiting the algorithm the command window contains:\nFitness of regression: Mean absolute value: 0.154808 Correlation coefficient between true value and estimation: 0.964228\nto give a first impression of the regression\u2019s quality. The estimation of the feature \"petal width\" has been added as a new feature. Make sure that you exclude the estimated feature from the list of used features for further regressions. Select a feature selection method including \"selected features\" in the description. (e.g.. \"linear regression coefficients (multivariat, selected features)\"). The output variable is automatically removed from the list of used features, but not estimations of the output variable.\nMore detailed information about the regression\u2019s quality can be obtained by the functions in View - Regression, e.g. the real value versus the estimated value, see picture 5.13 and the coefficients of the polynomial model.\nThe used polynomial is:\npetal width = \u22120.722 + 0.427 \u00b7 petal length (5.1) + 0.103 \u00b7 sepal width\nAn examination of single regression coefficients is possible via Data mining - Selection and evaluation of single features - Linear regression coefficients (univariate) (visualization of the result by View - Single features - Show feature relevances (sorted table))."}, {"heading": "5.2. Iris data set 31", "text": ""}, {"heading": "6 Menu items", "text": ""}, {"heading": "6.1 Menu items \u2019File\u2019", "text": "This point includes all file operations."}, {"heading": "6.1.1 Load project", "text": "A SciXMiner project file *.prjz will be loaded and default values are added. Necessary variables are \u201dd_org\u201d or \u201dd_orgs\u201d. In addition, the variables \u201dcode\u201d, \u201dcode_alle\u201d, \u201ddorgbez\u201d, \u201dvar_bez\u201d, \u201dzgf_y_bez\u201d, \u201dbez_code\u201d, \u201doptionen\u201d, \u201dprojekt\u201d, \u201dinterpret_merk\u201d are read."}, {"heading": "6.1.2 Save project", "text": "saves the recent SciXMiner project *.prjz using the same file name. This function saves the data set (but without analysis results as designed classifiers) and optionally the current settings of the control elements (if chosen in Control elements: General options)."}, {"heading": "6.1.3 Save project as...", "text": "saves the SciXMiner project file *.prjz with a new name (see File - Save project)."}, {"heading": "6.1.4 Mean value project (based on the selected output variable)", "text": "generates a new SciXMiner project with averaged values for all linguistic terms of the selected output variable. In the new project, only one data point per linguistic term exists."}, {"heading": "6.1.5 Export data", "text": "exports time series or single features in an ASCII file. The separators for columns and decimal numbers can be selected in the opened configuration window.\n\u2022 Time series in multiple files...: The time series of the given project (variable \u201dd_orgs\u201d) will be exported into different files. We assume at least one output variable. If more output variables exist, the first (n-1) output variables\n32"}, {"heading": "6.1. Menu items \u2019File\u2019 33", "text": "build a directory structure, the information about the n-th output variable is encoded in the file name. Alternatively, all n output variables can be encoded in the file name if the option \u201dSeparator for output variables in file name\u201d is chosen.\n\u2022 Single features in a file...: The single feature of the given project (variable \u201dd_org\u201d) will be exported as file (all data points in one file)."}, {"heading": "6.1.6 Import data", "text": "includes all functions for the import of data from files.\n\u2022 From a directory...: imports data from a directory structure. Its definition is explained in Section 4.2 and for the menu item File - Export data - Single features in a file.... The parallel import of single features and time series is impossible. Output variables should be included as single features in the file and can converted after import using Edit - Convert - Selected output variable\u2192 Single feature. Some import versions support more complex data formats including strings (see Section 4.2 \u201dImport and export of projects\u201d in the SciXMiner documentation). Such strings are converted in output variables.\n\u2022 From a file...: imports data from a file. The filenames encode the output classes. The parallel import of single features and time series is impossible. Output variables should be included as single features in the file and can converted after import using Edit - Convert - Selected output variable \u2192 Single feature.\nSome import versions support more complex data formats including strings (see Section 4.2 \u201dImport and export of projects\u201d in the SciXMiner documentation). Such strings are converted in output variables.\n\u2022 Interactive from a file: imports data with the MATLAB Import Wizard.\n\u2022 Import of time series names from file: imports identifiers for time series from a file. It is expected that exactly one identifier exists for every time series. The identifiers could either be separated by line breaks (i.e. one identifier per row) or by a separating character (i.e. tabulator, comma, or semicolon).\n\u2022 Import of single feature names from file: see File - Import data - Import of time series names from file, but for single features instead of time series."}, {"heading": "6.1.7 Fusion of projects", "text": "contains functions for the fusion of existing SciXMiner projects to one joint project called fusion.prjz. It uses all projects from a directory selected by a configuration window. Incompatible projects will be ignored. Four different types are available."}, {"heading": "34 Chapter 6. Menu items", "text": "\u2022 Additional time series and single features: fuses data from all SciXMiner projects in a directory by adding the data as new time series and single features. The number of data points, the length of time series, and the number and values of output variables must be identical for all projects.\nExample:\nSciXMiner-Projekt a.prjz with 5 data points, 2 time series, 1 single feature, and 1 output variable\nSciXMiner-Projekt b.prjz with 5 data points, 2 time series, 1 single feature, and 1 output variable\nSciXMiner-Projekt fusion.prjz with 5 data points, 4 time series, 2 single feature, and 1 output variable\n\u2022 Additional data points: fuses data from all SciXMiner projects in a directory by adding the data as new data points. The number and names of single features, time series and output variables and the length of time series must be identical for all projects. Linguistic terms of the output variable are matched by their names.\nExample:\nSciXMiner-Projekt a.prjz with 5 data points, 2 time series, 1 single feature, and 1 output variable\nSciXMiner-Projekt b.prjz with 5 data points, 2 time series, 1 single feature, and 1 output variable\nSciXMiner-Projekt fusion.prjz with 10 data points, 2 time series, 1 single feature, and 1 output variable\n\u2022 Additional time series and single features (tolerant for the selected output variable and data points, NaNs are deleted): like File - Fusion of projects - Additional time series and single features, but more tolerant to differences between data points. The values of the selected output variable of the selected data points (e.g. an ID resp. identifier) in the open project are used as the base for the fusion. Each data point will be deleted if the corresponding data points in the other project for the fusion cannot be found. Non selected data points are not used for fusion.\n\u2022 Additional time series and single features (tolerant for the selected output variable and data points, NaNs are retained): like File - Fusion of projects - Additional time series and single features, but more tolerant to differences between data points. The values of the selected output variable of the selected data points (e.g. an ID resp. identifier) in the open project are used as the base for the fusion. The fused time series resp. single features of each data point are set to NaN if the corresponding data points in the other project for the fusion cannot be found. Non selected data points are not used for fusion.\n\u2022 Additional events, event types and data points:"}, {"heading": "6.1.8 Apply SciXMiner batch file", "text": "executes a SciXMiner batch file with the extension *.batch. It can include one or more projects or even directories containing projects. Optionally, a file with a list of control elements with the extension *.uihdg is loaded by File - Options - Load options. For each of these projects, a list of macros is executed. A recursive definition with other batch projects is possible."}, {"heading": "6.1. Menu items \u2019File\u2019 35", "text": ""}, {"heading": "6.1.9 Apply SciXMiner batch file (debug mode)", "text": "like File - Apply SciXMiner batch file, but with a stop after errors and warnings. This option is useful for debugging SciXMiner batch files. After a stop, the processing can be continued by return."}, {"heading": "6.1.10 Apply SciXMiner batch file (step and debug mode)", "text": "like File - Apply SciXMiner batch file (debug mode), but with an additional step-wise execution of each macro in a SciXMiner batch file (continue with return)."}, {"heading": "6.1.11 Normative data", "text": "contains all operation for the handling of normative data. Examples for normative data are measurements of healthy people in medical data analysis or a desired process behavior for technical diagnosis. Normative data are defined by mean value and standard deviations for all time series and single features. They can be used for a comparison to a freely definable reference (e.g. for a typical behavior of a plant).\n\u2022 Load normative data: loads normative data (mean and standard deviation) for time series and single features from a \u2019*.norm\u2019 file.\n\u2022 Save normative data: saves normative data (mean values and standard deviations) for time series and single features into a \u2019*.norm\u2019 file. In detail, the variables \u201ddorgbez\u201d, \u201dmstd\u201d, \u201dmstd_em\u201d, \u201dmy\u201d, \u201dmy_em\u201d, \u201dparameter\u201d, \u201dtitelzeile\u201d, \u201dvar_bez\u201d will be saved. This function requires an executing of File - Normative data - Mean value to normative data for the generation of normative data.\n\u2022 Mean value to normative data: uses the recent data selection for the generation of normative data. Therefore, only one class (= linguistic value for the output variable) has to be selected (e.g. in medicine: healthy people, for technical applications: error-free and normal behavior of a process or device)."}, {"heading": "6.1.12 Data mining", "text": "This menu item contains functions for loading and saving of parameters from designed data mining systems.\n\u2022 Load fuzzy system: loads a designed fuzzy system from a file with the extension \u201d*.fuzzy\u201d. The assignment of input (single features) and output variables is based on the variable names. The file must contain the variables \u201dgaitcad_struct\u201d, \u201dmerkmale_projekt\u201d, and \u201dmode\u201d. The use of files from other projects gives misleading results, especially in case of identical variable names.\nThe loaded fuzzy system is not yet directly usable as a fuzzy classifier. This application requires an export by Data mining - Fuzzy systems - Export fuzzy system to classifier."}, {"heading": "36 Chapter 6. Menu items", "text": "\u2022 Load fuzzy system (only membership functions): loads only the membership functions of a pre-designed fuzzy system in a file and sets Control element: Data mining: Special methods - Type of membership function to \u201dFix\u201d.\n\u2022 Save fuzzy system: exports a designed fuzzy system in a file. The results are saved in the variables \u201dgaitcad_struct\u201d, \u201dmerkmale_projekt\u201d, and \u201dmode\u201d.\n\u2022 Export ANSI-C-Code fuzzy rulebase: exports the recent fuzzy rulebase in a C file. This C file needs a pointer to a vector x containing the selected input variables (single features) and to a pointer of membership values of the output variable. The numbers of the input variables are explained in the function header. The output of the function is the linguistic term of the output variable. The feature numbers start with x[1] instead of x[0]!\n\u2022 Load classifier: loads a designed classifier from a file with the extension \u201d*.class\u201d. It contains all settings specified in Control elements: Data mining: Classification of single features and Control elements: Data mining: Special methods during the design. The assignment of input (single features) and output variables is based on the variable names. The file must contain the variables \u201dgaitcad_struct\u201d, \u201dmerkmale_projekt\u201d, and \u201dmode\u201d. The use of files from other projects might give misleading results, especially in case of identical variable names.\n\u2022 Save classifier: exports a designed classifier in a file. The results are saved in the variables \u201dgaitcad_struct\u201d, \u201dmerkmale_projekt\u201d, and \u201dmode\u201d. They contain all settings defined by the parameters in Control elements: Data mining: Classification of single features and Control elements: Data mining: Special methods.\n\u2022 Export ANSI C Code of the Bayes classifier: exports the recent Bayes classifier with feature selection and feature transformation into an ANSI C file.\n\u2022 Load regression model: loads a designed regression model from a file with the extension \u201d*.regression\u201d. It contains all steps specified by Control elements: Data mining: Regression and Control elements: Data mining: Special methods during the design. The assignment of input (single features) and output variables is based on the variable names. The file must contain the variables \u201dgaitcad_struct\u201d, \u201dmerkmale_projekt\u201d, and \u201dmode\u201d. The use of files from other projects might give misleading results, especially in case of identical variable names.\n\u2022 Save regression model: exports a designed regression model in a file. The results are saved in the variables \u201dgaitcad_struct\u201d, \u201dmerkmale_projekt\u201d, and \u201dmode\u201d. They contain all steps defined by the parameters in Control elements: Data mining: Regression and Control elements: Data mining: Special methods."}, {"heading": "6.1.13 Options", "text": "contains all functions for loading and saving SciXMiner options.\n6.2. Menu items \u2019Edit\u2019 37\n\u2022 Load options: restores all options for the control elements as defined in the loaded option file with extension *.uihdg.\n\u2022 Save options: saves all options for the control elements and the command lines of the plugins into an option file with extension *.uihdg.\n\u2022 Save standard options: saves all options for the control elements and the command lines of the plugins into an option file with extension standard_options.uihdg. This file will be loaded at each SciXMiner start.\n\u2022 Load frequency list: loads a MATLAB file for the definition of frequency regions and overtones.\n6.1.14 Exit\nexits the application."}, {"heading": "6.2 Menu items \u2019Edit\u2019", "text": "contains all functions to edit a data set. It includes the selection of data points, time series and single features, the extraction of new single features from time series or other single features, several conversions, functions for deletion and operations for a data preprocessing."}, {"heading": "6.2.1 Select", "text": "contains functions to select data points, single features and time series.\n\u2022 All data points: selects all data points and removes any chosen selection of data points.\n\u2022 Data points using classes ...: performs a selection of data points using a conjunction and disjunction of linguistic terms of output variables. It includes\n1. a disjunction of all selected terms of one output variable and\n2. a conjunction of the so selected data points for all output variables.\nA selection of \u2019ALL\u2019 means that no data point is excluded due to its value for this output variable. The result should be evaluated using View - Classes for selected data points.\n\u2022 Data points using numbers ...: enables a manual selection of data points using its numbers.\n\u2022 Data point from GUI: selects the data points defined by Control element: General options - Manual selection of data points. This function is especially useful if the data point numbers for a desired selection are known."}, {"heading": "38 Chapter 6. Menu items", "text": "\u2022 Random data points (defined percentage of selected data points): selects a pre-defined percentage of the selected data points by chance. The percentage value is defined by Control element: General options - Percent for selection.\n\u2022 Data points via most frequent terms: selects all data points belonging to frequent terms for the selected output variable. The minimal number of data points is defined by Control element: Data preprocessing - Minimal number of data points in one class.\nExample: 100 data points for o.k., 50 data points for Error Type A, 2 data points for Error Type B; Parameter 10; Selection of 150 data points for o.k. and Error Type A\n\u2022 Data points via values of single features: selects all selected data points with logical true values in Control element: Data preprocessing - Feature values for selection for the first selected single features. Multiple queries have to be executed step by step.\nExample for the selection of all values between 5 and 10 for a single features: 1. Select single feature, 2. Write in Control element: Data preprocessing - Feature values for selection \u201d>5\u201d, 3. Execute Edit - Select - Data points via values of single features, 4. Write in Control element: Data preprocessing - Feature values for selection \u201d<10\u201d, 5. Execute Edit - Select - Data points via values of single features.\n\u2022 Deselect missing values for selected time series: deselects all data points from the selected data points if at least one selected time series contains missing values. The data points will no be deleted.\n\u2022 Deselect missing values for selected single features: deselects all data points from the selected data points if at least one selected single features contains missing values. The data points will no be deleted.\n\u2022 All time series: selects all time series of a project. Pushing the button \u201dALL\u201d left from control element Control element: Time series: General options - Selection of time series (TS) gives the same result.\nA selection of some time series can be done by Control element: Time series: General options - Selection of time series (TS) or the related edit field.\n\u2022 All single features: selects all single features of a project. Pushing the button \u201dALL\u201d left from control element Control element: Single features - Selection of single feature(s) (SF) gives the same result.\nA selection of some single features can be done by Control element: Single features - Selection of single feature(s) (SF) or the related edit field."}, {"heading": "6.2.2 Extract", "text": "contains all functions for the extraction of time series from time series and single features from time series or other single features.\n\u2022 Time series\u2192 Time series, Time series\u2192 Single features...: opens a configuration window to extract new time series or single features from existing time series. Here, the existing time series, the segment and the type of the extracted feature will be selected. The extraction is done by plugins (see Section 8.1)."}, {"heading": "6.2. Menu items \u2019Edit\u2019 39", "text": "\u2022 Time series\u2192 Time series, Time series\u2192 Single features (via plugin sequence)...: similar to Edit - Extract - Time series \u2192 Time series, Time series \u2192 Single features..., but uses a pre-defined sequence of plugins for extraction. In the configuration windows, only the existing time series and the segments will be selected. The feature extraction uses the sequence of plugins defined by Control element: Plugin sequence - Selection of plugins. The scheduling of the plugins might be changed in the edit field from Control element: Plugin sequence - Selection of plugins. All temporary results will be deleted.\n\u2022 Single features\u2192 Single features (with the selected feature aggregation from Options-Data Mining: Classification of single features): extracts new single features from existing single features. It uses the parameters for feature selection and aggregation from View - Classification.\n\u2022 Statistics for single features for linguistic terms of the selected output variable: extracts new single features for each selected single feature and each data point. These new features are minimum, maximum, median, mean value and number of all selected data points with the same linguistic term of the selected output variables.\n\u2022 Term statistics for output variable...: extracts a new output variable for each output variable that is selected in the following configuration window. Each data point gets the most frequent term for all selected data points with the same linguistic term of Control element: Time series: General options - Selection of output variable. This function is especially useful to summarize data points that are replicas of the same measurement.\n\u2022 Save time series segment for feature extraction: defines the selected segment of the time series as new segment for the feature extraction. The selection is explained in Control element: Time series: General options - Time series segment from and Control element: Time series: General options - to."}, {"heading": "6.2.3 Convert", "text": "contains all functions to convert time series, single features, classes, and data points to each other.\n\u2022 Selected single features\u2192 Output variables: converts the selected single features into output variables. The same values are used as linguistic values if the features have discrete values with not more values than a defined maximal number (defined by Control element: Single features - Number of terms for Single feature\u2192Output variable) or if the option Control element: Single features - All values was chosen. Otherwise, a discretization is done by automatically designed fuzzy membership functions and a subsequent generation of crisp membership functions. Its number of values is chosen by Control element: Single features - Number of terms for Single feature\u2192Output variable. The term name depends on the maximum of the fuzzy membership function (e.g. \u201dappr. 7.5\u201d).\n\u2022 Selected single features\u2192 Time series: This function assumes a temporal order of all existing single features. It converts s selected single features withN data points into 1 time series withK = s sample points. If the project yet contains time series, the number of sample points must be equal to s. Alternatively, all existing time series might be deleted."}, {"heading": "40 Chapter 6. Menu items", "text": "\u2022 Selected single feature (Timestamp)\u2192 Single features (Year, month, day, hour, minute, second): assumes that the selected single feature is a MATLAB timestamp. This function generates new single features with numbers for the year, month, day, hour, minute and second.\n\u2022 Selected single feature (Timestamp)\u2192Weekday: assumes that the selected single feature is a MATLAB time stamp. The result is converted in a new output variable containing the weekday.\n\u2022 All single features: Data point\u2192 Time series: The function assumes a temporal order of all data points. The data points of all single features are converted to sample points of time series. Existing time series will be deleted. These functions make a visualization of data points as time series possible.\nExample:\nA project consists of 100 data points and 2 single features. The function generates a project with 2 time series and 100 sample points.\n\u2022 All single features: Data point\u2192 Time series (class-wise): The function assumes a temporal order of all data points with identical values for the selected output variable. The data points of all single features are converted to sample points of time series. Existing time series will be deleted. All output variables with identical values for all new time series remain output variables. Output variables with different values are converted to new time series. Incomplete sample points are set to zero. The number of valid sample points is saved in a new single feature. These functions make a visualization of data points as time series possible.\nExample:\nA project consists of 102 data points and 2 single features. Blocks of 10 data points belong to a sensor (sensor number in first output variable). The second output variable codes the number of a measurement (1..10 for sensor 1..9, 1..12 for sensor 10).\nThe first output variable is selected. The function generates 10 new data points with 3 time series (from both single features and the second output variable) and 12 sample points. The 11th and 12th sample point is zero for the first ten data points.\n\u2022 All single features: Data point\u2192 Time series (with sample point in output variable): converts the data points into time series, each single feature will generate a separate time series. The selected output variable must contain the sample point number in its term numbers.\n\u2022 Data points\u2192 Single features and Single features\u2192 Data points: exchanges data points vs. single features in a SciXMiner project or vice versa (exchange s vs. N , see Appendix Important Internal Data Structures). Existing time series will be deleted.\n\u2022 Selected time series: Sample points\u2192 Single features: generates a new single feature from each sample of a time series. The new features are named with the time series name and \u201dSP k\u201d for the k-th sample.\nExample:\nA project consists of one data point with two time series and 100 samples, the second and third time series are selected. With Edit - Convert - Selected time series: Sample points\u2192 Single features, 200 new single features are generated by the 100 samples of the time series 2 and 3."}, {"heading": "6.2. Menu items \u2019Edit\u2019 41", "text": "\u2022 All time series: Sample points\u2192 Data points: converts the sample points of all time series into data points of single features. The number of the sample point is saved as new feature. All single features of the old project will be deleted. The results are saved as new project. This enables a classification of time series with a separate class assignment of each sample point.\nThis function is an inverse function for Edit - Convert - All single features: Data point \u2192 Time series.\nExample:\nA project consists of 1 data point with 2 time series and 100 sample points. The menu items Edit - Convert - All time series: Sample points \u2192 Data points generates a new project without time series, but with 3 new single features (both time series and the number of the sample point) and 100 data points.\n\u2022 Time series\u2192 Data points: converts time series into data points in a SciXMiner project (converts sz into N , see Appendix Important Internal Data Structures). This function is available if only one data point exist.\n\u2022 Separate time series with trigger events...: A time series will be segmented using a trigger time series and the resulting segments are written as new time series into data points from the variable d_orgs. In this way, events can be easily processed. A detail description can be found in Section 5.1.\nThe trigger time series is expected to be the same for all data points.\n\u2022 Shorten time series: contains all functions to shorten time series, e.g. for an easier handling of large projects.\n\u2022 Selected time series\u2192 Project-specific time scale: converts the selected time series into a project-specific time scale. Here, only one time series has to be selected. The conversion is based on the values for the first selected data point.\n\u2022 Selected output variable\u2192 Single feature: copies the selected output variable into an additional single feature.\n\u2022 Selected output variable\u2192 Single features (requires digits in term names): converts the selected output variable into a single feature if the names of the linguistic terms contain numbers.\n\u2022 Selected output variable (with filenames) \u2192 Multiple output variables (path components, file): assumes filenames containing the complete path as terms of the selected output variable. New output variables will be generated. Their terms result from splitting these filenames into separate subdirectories and the filenames. The file extension (e.g. .dat) is ignored.\n\u2022 Selected output variable (Date)\u2192 Single feature (Timestamp): interprets the terms of the selected output variable as data, the date format is defined by Control element: Data preprocessing - Date format. A single feature is generated based of a conversion of date and time into a MATLAB timestamp format.\n\u2022 Selected output variable (with filenames) \u2192 Multiple output variables (path components, file, extension):"}, {"heading": "42 Chapter 6. Menu items", "text": "like Edit - Convert - Selected output variable (with filenames)\u2192 Multiple output variables (path components, file), but with extension as separate output variable.\n\u2022 Estimated output variable\u2192 Output variable: converts the estimated output variable generated by a classifier (Data mining - Classification - Apply) to a new output variable.\n\u2022 Estimated output variable\u2192 Single feature: converts the result of the last classification into a new discrete-valued single feature. This feature is called \u201dEstimation\u201d + the name of the selected output variable. This menu item can be selected if a valid classification result exists resulting from an applied classifier.\n\u2022 Estimated output variable (percental)\u2192 Single features: converts the estimated percental membership values (resulting from the last application of a classifier) to classes (linguistic terms) to single features.\n\u2022 Errors of regression model\u2192 Single features: exports the error of the recent regression model into a single feature.\n\u2022 Add output with identical term for all data points: adds an additional output variable with a term value of 1 for all data points (useful for visualizations in a project with many output terms).\n\u2022 Class\u2192 Class: Combine terms (OR-operation with data point selection, name of the current output variable): generates a new output variable with two linguistic terms. It results from the selected output variables and the selected data points. The first linguistic term consists of all selected data points, the second from all other data points. This is a comfortable way to combine multiple terms. Later, they can be labeled by the renaming function.\nExample:\n1. Select output variable \u201dPerson\u201d with linguistic terms Anna, Maria, Thomas and Peter\n2. Select terms Anna and Maria in Edit - Select - Data points using classes ...\n3. Select menu item Edit - Convert - Class \u2192 Class: Combine terms (OR-operation with data point selection, name of the current output variable)\n4. A new output variable \u201dPerson (OR)\u201d with terms \u201dAnna, Maria\u201d and \u201dRest: Thomas, Peter\u201d will be generated\n5. It can be renamed with Edit - Rename... in \u201dSex\u201d with the new terms \u201dFemale\u201d and \u201dMale\u201d.\n\u2022 Class\u2192 Class: conjunction (AND) of output variables ...: generates a new output variable. The linguistic terms results from a conjunction of a set of output variables which are selected in the following configuration window.\nExample:\nConjunction of \u201dName\u201d (Terms: \u201dAnna\u201d, \u201dMaria\u201d, \u201dThomas\u201d, \u201dPeter\u201d) and \u201dExamination\u201d (\u201dPretherapeutic\u201d, \u201dPost-therapeutic\u201d) results in a new output variable \u201dAND: Name Examination\u201d with 8 terms (\u201dAnna Pre-therapeutic\u201d, ..., \u201dPeter Post-therapeutic\u201d).\n\u2022 Resort linguistic terms (order from GUI): sorts the linguistic terms of the selected output variable using the new order from Control element: General options - Order of linguistic terms."}, {"heading": "6.2. Menu items \u2019Edit\u2019 43", "text": "\u2022 Clean up all variable names (start and end blanks): deletes end blanks in the names of output variables, time series and single features."}, {"heading": "6.2.4 Delete", "text": "deletes data points, time series, single features, or output variables.\n\u2022 Selected data points: deletes all selected data points. Finally, the data points will be renumbered.\n\u2022 Unselected data points: deletes all non-selected data points. Finally, the data points will be renumbered.\n\u2022 Double single features, time series, and output variables: deletes doubles of single features, time series, and output variables with identical names. Such doubles result normally from a multiple feature extraction.\n\u2022 Selected time series: deletes all selected time series.\n\u2022 Unselected time series: deletes all non-selected time series.\n\u2022 Selected sample points: deletes all selected sample points out of all time series.\n\u2022 Unselected sample points: deletes all unselected sample points.\n\u2022 All single features: deletes all single features.\n\u2022 Selected single features: deletes selected single features.\n\u2022 Unselected single features: deletes unselected single features.\n\u2022 Output variable ...: deletes output variables. An additional configuration window is shown to select these output variables (multiple selection by SHIFT + left mouse button or CTRL + left mouse button).\n\u2022 Non-existing terms of the output variable: deletes these terms of the output variables without any data point in the SciXMiner project. This makes some functions (e.g., the selection of data points based on terms) easier to use.\n\u2022 Missing data: removes time series or single features with NaN- or Inf values and time series containing only zeros for all sample points of a data point. All cases are interpreted as missing values for these data points and time series resp. single features."}, {"heading": "44 Chapter 6. Menu items", "text": "Consequently, either the data point or the time series resp. the single feature will be deleted. The preference is controlled by the threshold in Control element: Data preprocessing - Threshold for deleting [Perc. missing data].\nExample: Threshold 20 %\n1. All time series with missing values for more than 20 % of the data points will be deleted.\n2. All single features with missing values for more than 20 % of the data points will be deleted.\n3. All remaining data points with missing values will be deleted.\nTip: Misleading standard values for missing values of a single feature (e.g. -1) can be set to NaN values using a macro to enable a unified handling."}, {"heading": "6.2.5 Sorting", "text": "alphabetic sorting of names in a project (i.e., very useful for fusing of projects with identical variable names in different order).\n\u2022 Single features: sorts single features in a project alphabetically.\n\u2022 Time series (TS): sorts time series in a project alphabetically.\n\u2022 Output variables: sorts output variables in a project alphabetically.\n\u2022 Images: sorts images in a project alphabetically."}, {"heading": "6.2.6 Outlier detection", "text": "generates a classifier to detect outliers for the selected single features and data points. It can be applied to unknown data points. Some hints to the algorithms are given in [19].\n\u2022 Design data set (selected data points, options from classification): The outlier detection bases on an one class classification problem. It decides if a data point belongs to this one class or not.\nHowever, the implemented methods for the feature selection and classification assume data points of at least two classes. To solve this problem, an appropriate data set must be generated. A first step to prepare this data set is the feature selection in Control element: Data mining: Classification of single features - Selection of single features. Normally, the outlier detection uses the \u201dselected features\u201d.\n\u2022 Design (selected data points, designed data set): designs a classifier for the outlier detection. This step uses the data set prepared by Edit - Outlier detection - Design data set (selected data points, options from classification). Three different methods might be used: an one-class method, a distance-based method, and a density-based method. The method is chosen by Control elements: Data preprocessing."}, {"heading": "6.2. Menu items \u2019Edit\u2019 45", "text": "The one-class method uses a SVM optimization proposed by [23]. The distance-based method computes mean value and standard deviation for the selected data points and uses the Mahalanobis distance to the mean value as criterion. The density-based method evaluates the number of data points in a pre-defined neighborhood.\n\u2022 Apply (selected data points, designed data set): applies the classifier design by Edit - Outlier detection - Design (selected data points, designed data set) for the outlier detection. The parameter in Control element: Data preprocessing - Threshold for outliers tunes the threshold for this decision. The value of Control element: Data preprocessing - Save result as output variable decides if the result is added as new output variable.\nBy the menu point Edit - Outlier detection - Compute class discriminant functions (selected data points, designed data set) the outlier detection is applied for all classes. It generates a struct called \u201dausreisser\u201d containing indices of outlier data points including the values for the decision (e.g. distances). The indices address the data point numbers of all existing data points of the project.\nThe function does not select data points, e.g. to delete outliers. This functionality is done by adding the outlier decision as new output variable (see Control element: Data preprocessing - Save result as output variable) and the use of data point selection in Edit - Select - Data points using classes ....\n\u2022 Design and apply (selected data points, designed data set): designs and applies the outlier detection.\n\u2022 Compute class discriminant functions (selected data points, designed data set): analyzes for all selected data points the linguistic terms of the output variable and computes for each class limits for the outlier detection. The resulting parameters are saved in the variable \u201dklassen_grenzen\u201d for each class. The element \u201dlp_detect\u201d describes the parameters and the element \u201dausreisser\u201d contains the application results to the selected data points.\nIf \u201dNew output variable\u201d or \u201dReplace identical output variable\u201d is chosen in Control element: Data preprocessing - Save result as output variable, the result is saved as output variable. It can be used to select outlier e.g. for a later visualization and deletion."}, {"heading": "6.2.7 Category", "text": "Categories summarize similar single features and time series for different analysis and visualization functions (see Section 8.5, [64]). They classify single features and time series into different output classes of multiple output variables called categories (e.g. according to the sensor type or to the kind of feature extraction). The matching is done by category files with the extension *.categories.\n\u2022 Select single features ...: enables a selection of single features by a conjunction or disjunction of multiple categories and their linguistic terms. The function assumes computed categories using Edit - Category - Compute single features.\n\u2022 Compute single features: assigns categories for all existing single features using their names. For safety reasons, the categories will be deleted if the number of single features is changed (by adding or deleting single features)."}, {"heading": "46 Chapter 6. Menu items", "text": "\u2022 Compute a priori relevances: computes a priori relevances of single features using the existing categories.\n\u2022 A priori relevances of selected features...: set the a priori relevances for all selected single features to a defined value.\n\u2022 Selection of time series ...: enables a selection of time series by a conjunction or disjunction of multiple categories and their linguistic terms. The function assumes computed categories using Edit - Category - Compute time series.\n\u2022 Compute time series: assigns categories for all existing time series using their names. For safety reasons, the categories will be deleted if the number of time series will be changed (by adding or deleting time series).\n\u2022 Search time series related to the selected single features: selects all time series belonging to the selected features. The criterion is an extraction of the selected single features from the related time series."}, {"heading": "6.2.8 Rename...", "text": "This function opens a window to rename time series, single features, output variables, and the corresponding linguistic terms (possible discrete values).\nThe upper element is used to select the type of variable to be renamed. In the element below, one time series, single feature, or output variable will be selected represented by the (old) name. For linguistic terms, an additional element is shown to the selected the term.\nThe new name must be written in the lowest element and saved by pressing ENTER.\nThe corresponding figure shows Fig. 6.1."}, {"heading": "6.2.9 Generating trigger time series", "text": "shows a separate figure (TS navigator) to generate a trigger time series (see Section 5.1.2.1 and Fig. 5.1).\nThe upper part of the figure shows time series and existing trigger events (red, dotted thin lines). The selection of the time series is done in the element at the right bottom corner of the figure. The class number for a trigger event is at the trigger line. The standard value is one. The time series window can be zoomed by two control elements in the left bottom corner (left element: length of time series segment, right: each n-th sample point will be shown). The grid is switched on or off by the corresponding element. The slide bar below the time series enables a time shift.\nA click to the time series part of the window selects a new sample point (black dashed line). A new trigger event is added for this sample point by clicking the \u201dAdd\u201d button. The list of recent trigger events is shown with the syntax sample point (class), e.g. sample point k=23 and class type 1: 23(1). The selected trigger event is marked. Its class label might be changed by \u201d+\u201d und \u201d-\u201d. The use of \u201d\u2192\u201d resp. \u201d\u2190\u201d shifts the temporal position of the event. It can be removed by the \u201dRemove\u201d button. \u201dGo to\u201d moves to the selected event.\n\u201dImport\u201d loads a variable with a pre-defined trigger time series from Matlab workspace. The length must be equal to the length of time series of the project.\nThe button \u201dSave trigger time series\u201d saves the time series in the project and close the figure."}, {"heading": "6.3. Menu items \u2019View\u2019 47", "text": ""}, {"heading": "6.2.10 Edit trigger time series", "text": "edits an existing trigger time series selected by Control element: Time series: General options - Selection of time series (TS). A wrong selection leads to an error message or a time series with many trigger events. The handling is explained for Edit - Generating trigger time series."}, {"heading": "6.2.11 Looking for missing data", "text": "searches for missing values and shows the results for time series vs. data points and single features vs. data points. See Edit - Delete - Missing data for the definition of missing values."}, {"heading": "6.3 Menu items \u2019View\u2019", "text": "contains all functions for visualization including many results of the applied algorithms."}, {"heading": "6.3.1 Classes for selected data points", "text": "shows the selected data points with their output variables as text file."}, {"heading": "48 Chapter 6. Menu items", "text": ""}, {"heading": "6.3.2 Number of terms for selected data points", "text": "shows the number of terms for each output variable and all selected data points. Only frequent terms are listed in detail, terms with one or two data points are only summarized. This function is suited for large projects, because in contrast to View - Classes for selected data points not all infrequent terms and data point descriptions are listed. The result is saved in a file called [project_name]_ind_terms.txt."}, {"heading": "6.3.3 Time series (TS)", "text": "contains all functions for the visualization of time series.\nIn addition, the figures can be used for the definition of segments from time series and of trigger events. The definition of segments is done by zooming to the region of interest and pushing the \u201dSelect time series segment\u201d item in the menu bar of the figure. The segment is now defined by the minimal and maximal value of the x-axis. A more detailed explanation is given in Control element: Time series: General options - Time series segment from and Control element: Time series: General options - to.\n\u2022 Original time series: shows all selected data points of the selected time series (Control element: Time series: General options - Selection of time series (TS)) vs. time.\n\u2022 Mean time series: shows the mean values of all selected time series (Control element: Time series: General options - Selection of time series (TS)) versus time for all linguistic terms (classes) of the selected output variable.\n\u2022 Standard deviation time series: shows the standard deviations of all selected time series (Control element: Time series: General options - Selection of time series (TS)) versus time for all linguistic terms (classes) of the selected output variable.\n\u2022 Mean and standard deviation time series: shows the mean values and standard deviations of all selected time series (Control element: Time series: General options - Selection of time series (TS)) versus time for all linguistic terms (classes) of the selected output variable.\n\u2022 Scatterplot time series: shows all sample points of the selected time series as scatter plot. The number of selected time series must be even. The corresponding linguistic terms of the selected data points are coded by color.\nExample:\nFour time series x1[k], ..., x4[k] with K = 100 sample points and N = 2 data points are selected. The results are two figures with x2 = f(x1) resp. x4 = f(x3) with 200 points each.\n\u2022 Poincare plot time series (2D): show the selected time series as 2D-Poincare plot (x-axis: x[k], y-axis x[k+1]). Subsequent time points can be plotted as connected or unconnected points, depending on the selection in Control element: View: Time series - Poincare plot: Connect points."}, {"heading": "6.3. Menu items \u2019View\u2019 49", "text": "\u2022 Poincare plot time series (3D): show the selected time series as 3D-Poincare plot (x-axis: x[k], y-axis x[k+1], z-axis x[k+2]). Subsequent time points can be plotted as connected or unconnected points, depending on the selection in Control element: View: Time series - Poincare plot: Connect points.\n\u2022 Transformations vectors for feature extraction: defines if data-dependent transformations for the feature extractions are computed from the recent data, saved and/or loaded from file. The main difference is the handling of e.g. membership functions or a principal component analysis. The saving in a file is necessary for a design with a training data set and a later usage for test data by applying the same parameters. The data is saved in a file called [project name].plugpar.\n\u2022 Bode plot for filter in plugin: shows the Bode plot for the selected filter in the plugin called plugin_filter.m. The related parameters are defined in Control element: Plugin sequence - Plugin parameter. The plot is influenced by Control element: Time series: General options - Sampling frequency of time series and Control element: Time series: General options - Unit.\n\u2022 Show all feature relevances (sorted table): shows relevances of time series sorted by descending values.\n\u2022 Show all feature relevances (unsorted table): shows relevances of time series sorted by time series numbers."}, {"heading": "6.3.4 Single features", "text": "contains all functions for the visualization of single features and their derived features.\n\u2022 Single features vs. output classes: shows term-wise histograms for the first selected single feature on the x-axis versus the number of the corresponding linguistic term of the selected output variable on y-axis.\n\u2022 Single features vs. single features: shows a scatter plot of the selected single features. The functions shows pairwise scatter plots (x2 = f(x1), x4 = f(x3) etc.) if more than three features were selected.\n\u2022 Manual class assignment via single features: enables an interactive assignment of linguistic terms of output variables or the selection of data points with a figure containing two selected output variables. Here, a figure will be opened with different options in the menu bar.\n\u201dMark\u201d contains all operations for the handling of a list with interactively selected data points. This list is initialized as empty list. Different menu items exist. \u201dAdd\u201d adds all visible data points to the list depending on the recent axes scaling. With this option, data points can be selected by zooming and subsequent adding. \u201dRemove\u201d is used to remove all visible data points from the list. All marked data points are shown with a circle mark. \u201dReset\u201d removes all data points and generates an empty list. Finally, the list can be transferred to an output variable (\u201dSave as output variable\u201d) or used to select data points for further analysis (\u201dSelect data points\u201d).\nExisting corresponding images for the visible data can be displayed using \u201dShow related images\u201d."}, {"heading": "50 Chapter 6. Menu items", "text": "Alternatively, \u201dChange: Term of the output variable ...\u201d can assign new linguistic terms to all visible data points for the selected output variable. Here, all existing terms or a new term (\u201dNew term\u201d) are available.\nThe figure can be opened different times with different single features. \u201dRefresh\u201d updates all open figures with the new linguistic terms or the entries of the list.\n\u2022 Discrete single features (2D histogram): shows a two-dimensional histogram for the first two selected single features. Error messages are generated if the single features have continuous values or if only one or more than two single feature have been selected.\n\u2022 Discrete single features (2D histogram with output classes): shows a two-dimensional histogram for two selected single features. In contrast to View - Single features - Discrete single features (2D histogram), separate bars are shown for different linguistic terms of the selected output variable. Error messages are generated if the single features have continuous values or if only one or more than two single feature have been selected.\n\u2022 Mean and standard deviation: plots mean values and standard deviations for all selected single features in a file. The computation is done for each class of the selected output variable separately.\n\u2022 Mean, standard deviation, minimum, maximum: like View - Single features - Mean and standard deviation, but with additional minimum and maximum values.\n\u2022 Median, Minimum, Maximum: like View - Single features - Mean and standard deviation, but with median, minimum and maximum values.\n\u2022 Absolute values: writes the values of all selected single features and data points in a file tmp.txt and opens the file in an editor window. It contains also the corresponding value of the output variable. The features and output variables are tabulator separated and can easily be exported to other programs.\n\u2022 Boxplot (with Statistics Toolbox!): plots boxplots for all selected data points, single features and the selected output variable using the function boxplot.m of the Statistic Toolbox.\n\u2022 Membership function: plots a figure for each selected single feature containing the membership functions of the recent fuzzy system. Such a fuzzy system is designed by Data mining - Fuzzy systems - Design (single rules), Data mining - Fuzzy systems - Design (rule base) or Data mining - Fuzzy systems - Import fuzzy system from classifier. New membership functions will be generated for this figure if a fuzzy system does not exist or if the single feature is not a part of the fuzzy system.\n\u2022 Membership function and total histogram: like View - Single features - Membership function, but with an additional histogram describing the selected data points of the single feature.\n\u2022 Membership function and class histogram: like View - Single features - Membership function, but with separate histograms for each class of the output variable belonging to the selected data points of the single feature.\n6.3. Menu items \u2019View\u2019 51\n\u2022 Entropy: shows the entropy balance (input entropy, output entropy, and mutual information) for each selected single feature vs. the selected output variable. The single feature will be discretized using the parameters from Control element: Data mining: Special methods - Number of linguistic terms and Control element: Data mining: Special methods - Type of membership function.\n\u2022 Correlation coefficients (Pearson): writes relevant (linear) Pearson correlation coefficients between all selected output variables in a file. The relevance is defined as a larger absolute value than the threshold in Control element: Data mining: Statistical options - Threshold for correlation coefficient. The function is only able to show linear relations and assumes normal distributions of all features. Optionally, the computation can be restricted by Control element: Data mining: Statistical options - Show correlation only for one selected feature to the selected single features defined by Control element: Data mining: Regression - Output variable of regression. The reporting of p-values for correlation coefficients unequal zero is switched on and off by Control element: Data mining: Statistical options - Show p values for correlation. A method for the correction of multiple tests can be defined by Control element: Data mining: Statistical options - Bonferroni correction.\n\u2022 Correlation coefficients (Spearman): like View - Single features - Correlation coefficients (Pearson), but with Spearman correlation coefficients. They evaluate rank correlations of features. This function gives better results for nonlinear but monotone relations between features.\n\u2022 Correlation coefficients (Kendall): like View - Single features - Correlation coefficients (Pearson), but with Kendall\u201ds tau coefficient. They evaluate rank correlations of features. This function gives better results for nonlinear but monotone relations between features.\n\u2022 Show correlation coefficients: shows linear correlation coefficients of selected single features as figure. Green values visualize positive, red values negative correlations. The color intensity is a measure for the absolute value of the correlation coefficient. Hereby, the correlation coefficient of a type according to Control element: Data mining: Statistical options - Type for correlation parameters is used.\n\u2022 Show feature relevances (graphic): summarizes calculated feature relevances as figure(s). The main advantage is a first overview about problems with a large set of single features.\nFor univariate relevances, the relevance is shown versus the feature number.\nFor multivariate relevances, two figures will be shown:\nThe first figure visualizes step-wise relevance improvements of each feature as bars versus the feature number. The lowest bar shows the univariate relevance. The second bar contains improvements for the multivariate relevance by the second feature after selecting the best univariate feature in first step. All following bars show improvements of step-wise selections of the sm = 3, 4, ... feature after preselecting sm \u2212 1 feature in the steps before. Important features are characterized by high bars.\nThe second figure contains the calculated multivariate relevances for the selection the 1-st,..., smth feature. The relevances assume also the step-wise preselection of sm \u2212 1 best feature in the steps before."}, {"heading": "52 Chapter 6. Menu items", "text": "\u2022 Show feature relevances (sorted table): shows all computed feature relevances sorted by descending relevance values.\n\u2022 Show feature relevances (unsorted table): shows all computed feature relevances sorted by ascending feature numbers.\n\u2022 Show a priori feature relevances (unsorted table): shows all a priori relevances sorted by ascending feature numbers.\n\u2022 Add feature relevances to an archive: saves the recent feature relevances to a archive, e.g., to compare different methods for feature evaluation. This archive can be saved as separate SciXMiner project using View - Single features - Save archive with feature relevances.\n\u2022 Save archive with feature relevances: save the recent archive with feature relevances as separate SciXMiner project ([Name of the Project \u2019_feature_relevances.prjz\u2019]). In this SciXMiner project, the single features are represented as terms of an output variable and the values of the feature relevances are saved as single features."}, {"heading": "6.3.5 Single features (multivariate)", "text": "groups multidimensional visualizations for single features.\n\u2022 Parallel coordinates: plots the selected single features in parallel coordinates form.\n\u2022 Andrews Plot: plots the selected single features as an \"\u2018Andrews Plot\"\u2019.\n\u2022 Glyph Plot: plots a glyph plot of the selected features.\n\u2022 Heatmap: plots a heatmap of the selected single features."}, {"heading": "6.3.6 Classification", "text": "contains all functions for the visualization of classification results.\n\u2022 Result: shows a scatterplot of the last classification result in the (optionally aggregated) feature space of the classifier. The kind of chosen linguistic terms (e.g. true class assignment, result of the classifier, a different output variable defined by Control element: View: Single features - Different output variable etc.) can be switched by the options in Control element: View: Classification and regression - Display classes for output variables.\n\u2022 2D classification with covariance matrixes: like View - Classification - Result, but adds estimated covariance matrices for the linguistic terms of the output variable. This menu item is only available if a Bayes classifier was applied."}, {"heading": "6.3. Menu items \u2019View\u2019 53", "text": "\u2022 2D plot classifier with support vectors: like View - Classification - Result, but marks Support vectors by triangles. This menu item is only available if a Support Vector Machine was applied as classifier.\n\u2022 ROC curve...: computes a Receiver-Operating-Characteristic curve (ROC curve). It shows all possible compromises between sensitivity and specificity with respect to the classifier decision for one selected linguistic term of the output variable. This term is chosen by a separate configuration window. The tuning parameter for the ROC curve is the gradual class membership for a decision of the last applied classifier.\n\u2022 Time series classification error vs. time: shows the classification error of a time series classification vs. time.\n\u2022 Used features of the time series classifier: shows a figure with the selected time series (marked by points) of a time series classifier vs. time. The method of feature selection and the number of selected features are controlled by Control element: Single features - Selection of single feature(s) (SF) resp. Control element: Data mining: Classification of single features - Number of selected features."}, {"heading": "6.3.7 Regression", "text": "shows the results of the most recent applied regression model. The selection of data points is fixed to the application of the regression model.\n\u2022 Output variable and estimation: shows the output variable of the regression vs. the estimation by the regression model.\n\u2022 Output variable and error: shows the output variable of the regression vs. the regression error.\n\u2022 Input variable(s), output variable, and regression function (2D resp. 3D): shows the function of the estimated regression model. It is computed by a grid of additional data points. The number of data points is defined by Control element: View: Classification and regression - Number of grid points. The grid is tuned for the analysis of the interpolation and extrapolation behavior. In addition, the values of the input variables vs. the output variables during the design are plotted as scatterplot. If a normalization or aggregation of features was chosen, the visualization shows these features instead of the original ones.\nThe function is only available for regression models with one or two inputs.\n\u2022 Input variable(s), output variable, and regression function (top view 3D): like View - Regression - Input variable(s), output variable, and regression function (2D resp. 3D), but with a two-dimensional projection (overhead) of the function. The estimation of the output variable is color-coded.\n\u2022 Protocol regression model application into file: protocol the errors statistics of the regression model to a file.\n\u2022 GUI for multidimensional visualization: opens a GUI to create a 3D extraction out of a regression model. Two input parameters can be set"}, {"heading": "54 Chapter 6. Menu items", "text": "to variable, by typing \u201dx\u201d resp. \u201dy\u201d into the box. The input parameters left can be set to a constant value, by typing the value into the box. This Visualization works on the same principles as View - Regression - Input variable(s), output variable, and regression function (2D resp. 3D). Moreover, the region for the scatterplot can be defined with the columns \u201dMin:\u201d and \u201dMax:\u201d.\nThe function is only available for existing regression models with more than two inputs.\n\u2022 Generate macro for multidimensional visualization: creates and opens a macro for 3D visualization of a regression model. Further information on handling can be found within the macro.\n\u2022 Apply macro for multidimensional visualization: executes the macro created by View - Regression - Generate macro for multidimensional visualization. Alternatively, the macro can be started the usual way via Extras - Play macro....\n\u2022 Coefficients of the polynomial model: writes the coefficients of the recent polynomial model for the regression into a file called *_poly.tex resp. *_poly.txt.\n\u2022 Structure of the MLP net: shows the structure of a MLP net for a regression model. Excitatory links are green, inhibitory links red. The color intensity is a measure for the strength of a link.\n\u2022 Multidimensional response surface (RSM, with Statistics Toolbox): opens the Matlab function rstools.m for the visualization of a Multidimensional response surface (RSM) with the selected single features and the selected output variable of the regression Control element: Data mining: Regression - Output variable of regression."}, {"heading": "6.3.8 Aggregated features", "text": "contain all functions for the visualization of aggregated single features. It includes the transformation matrices for the dimension reduction and the related membership functions for the aggregated single features.\n\u2022 Eigenvectors and transformation vectors: shows the factor loadings resp. weights of the single features for the generation of the aggregated features. For each aggregated features, a different color is chosen.\n\u2022 Factor loadings (2D eigenvectors or transformation vectors): plots the elements of the first two transformation vectors for feature aggregation as scatterplot in a two-dimensional space.\n\u2022 Membership function: plots for each aggregated feature a figure with the membership functions for all linguistic terms.\n\u2022 Membership function and total histogram: like View - Aggregated features - Membership function, but with an additional histogram for all selected data points.\n\u2022 Membership function and class histogram: like View - Aggregated features - Membership function, but with separate subfigures with histograms for each term of the selected output variable."}, {"heading": "6.3. Menu items \u2019View\u2019 55", "text": ""}, {"heading": "6.3.9 Output variables", "text": "contain all functions for the visualization of output variables.\n\u2022 Qualitative output variables: plots the selected output variable vs. the number of the related data points.\n\u2022 2D histogram...: shows the distribution of two output variables as histogram. Both output variables are selected by a configuration window.\n\u2022 Term number of output variable: writes the number of selected data points for all linguistic terms of the output variable in a protocol file."}, {"heading": "6.3.10 Spectrogram", "text": "contains all functions for the visualization of spectrograms.\n\u2022 Compute and show: computes and shows a separate spectrogram for each selected data point and each selected time series. Each spectrogram is normalized to values between zero and one (see View - Spectrogram - Compute and show (mean for selected data points) resp. View - Spectrogram - Compute and show (mean for selected class) for mean values of spectrograms). A warning is shown for very large numbers of spectrograms due to many selected data points.\nIn many cases, the interpretability of spectrograms might be improved by adapting the selection of Control element: View: Spectrogram, FFT, CCF - Function for color bar and the related parameters. This controls the color scaling especially to a finer resolution for lower amplitudes. The compromise between time and frequency resolution is tuned by Control element: View: Spectrogram, FFT, CCF - Window size [sample points].\n\u2022 Compute: similar to View - Spectrogram - Compute and show, but only for computation of spectrograms. This saves time especially for large project with a script-based generation.\n\u2022 Show: similar to View - Spectrogram - Compute and show but only for visualization. This saves time by avoiding a new computation, e.g. during tuning of visualization parameters.\n\u2022 Principal component analysis for spectrograms: makes a Principal Component Analysis on the recent computed spectrogram. The number of principal components is defined by Control element: View: Spectrogram, FFT, CCF - Number of principal components for spectrogram. The result are figures with eigenvectors vs. frequency, percentages of eigenvalues, and aggregated features vs. time. The function is able to detect and visualize dominating signals containing a mixture of different frequencies. The interpretation might be sophisticated due to window effects. The window length is defined by (Control element: View: Spectrogram, FFT, CCF - Window size [sample points])."}, {"heading": "56 Chapter 6. Menu items", "text": "\u2022 Compute and show (mean for selected data points): computes firstly a separate spectrogram for each selected data point and each selected time series. Each spectrogram is normalized to values between zero and one. In a next step, the mean value over all data points of the computed spectrograms are computed and finally shown. The resulting \u201dmean spectrogram\u201d gives only a qualitative impression due to the artifacts caused by separate normalization.\n\u2022 Compute and show (mean for selected class): similar to View - Spectrogram - Compute and show (mean for selected data points), but with separate mean values of spectrograms for all existing classes of the selected output variable. Due to the separate normalization, the result is rather qualitatively."}, {"heading": "6.3.11 Morlet spectrogram", "text": "contains all functions for the visualization of Morlet spectrograms.\n\u2022 Compute and show (selected data points and time series): computes new time series for the selected time series by convolution with complex Morlet wavelets. After the convolution, the new time series are filtered by an IIR-Filter. The IIR parameters are defined by Control element: Time series: Extraction - Parameters - Parameter IIR filter. The results are shown as tree-dimensional plots (x axis: time, y axis: frequencies, color: coefficient of the filtered signal).\nThe frequency range of the Morlet wavelets is defined by Control element: Time series: Extraction - Parameters - Frequencies (FIL, Morlet spectrogram) and Control element: Time series: Extraction - Parameters - Morlet wavelet: eigenfrequency. The parameters describe the lower and higher limit. Control element: View: Spectrogram, FFT, CCF - Morlet spectrogram: Frequency stepwidth could be used to adapt the step length (default: 1).\nThe spectrogram refers to absolute values or to relative values to a baseline. These parameters could be modified in Control element: View: Spectrogram, FFT, CCF - Morlet spectrogram (relative to baseline) and Control element: View: Spectrogram, FFT, CCF - Sample points baseline.\n\u2022 Compute and show (mean for selected class): similar to View - Morlet spectrogram - Compute and show (selected data points and time series). Here, the mean values will be computed for the different classes using the selected data points resulting in a mean Morlet spectrogram for each linguistic term.\n\u2022 Plot only: shows again the computed Morlet spectrogram."}, {"heading": "6.3.12 Cross and Auto Correlation Functions", "text": "contains all functions to show Cross and Auto Correlation Functions.\n\u2022 Separately for each data point: computes and shows auto resp. Cross Correlation Functions for each selected data point. The selection of time series is done in a separate configuration window with two fields. The selection"}, {"heading": "6.3. Menu items \u2019View\u2019 57", "text": "of identical time series leads to auto correlation functions, different time series produce Cross Correlation Functions. A multiple selection of time series is possible. See also View - Cross and Auto Correlation Functions - Mean for selected data points and View - Cross and Auto Correlation Functions - Mean for classes for a fast overview for many selected data points.\n\u2022 Separately for selected data points (Short-time analysis): similar to View - Cross and Auto Correlation Functions - Separately for each data point, but with a computation and visualization of a short-time correlation analysis. The lengths of the time window is chosen in Control element: View: Spectrogram, FFT, CCF - Window size [sample points]. This function deals with an overview about time-variant changes of correlation.\n\u2022 Mean values of correlation coefficients (defined time shift): similar to View - Cross and Auto Correlation Functions - Mean for selected data points, but extract only one sample of the computed Cross resp. Auto Correlation Function. This sample is controlled by a time shift defined in Control element: View: Spectrogram, FFT, CCF - Time shift Tau [SP]. Hereby, the correlation coefficient of a type according to Control element: Data mining: Statistical options - Type for correlation parameters is used. The results are displayed in a text file for all relevant correlations with absolute values larger than Control element: Data mining: Statistical options - Threshold for correlation coefficient. Finally, all correlations are shown in a color figure similar to View - Single features - Show correlation coefficients.\nThe main advantage of this function is a fast analysis of correlations between many time series.\n\u2022 Class mean values of correlation coefficients (with defined time shift): similar to View - Cross and Auto Correlation Functions - Mean values of correlation coefficients (defined time shift), but with separate computations for each class (see also View - Cross and Auto Correlation Functions - Mean for classes).\n\u2022 Mean for selected data points: works as View - Cross and Auto Correlation Functions - Separately for each data point, but computes mean values for all selected data points. The function is only enabled if at least two data points have been selected.\n\u2022 Mean for classes: works as View - Cross and Auto Correlation Functions - Separately for each data point, but computes mean values for all selected data points of each existing class using the selected output variable. The function is only enabled if at least two data points have been selected.\n\u2022 Correlation function for different data points and selected time series: computes and shows Cross Correlation Functions for all selected combinations of data points and time series. The selection of data points is done by a separate configuration window. Only one data point should be selected in its upper control element. In the lower control element, a multiple selection is possible (CTRL + left mouse button). The Cross Correlation Functions are generated for each possible combination of reference with comparison data points. The selection of the time series is shown in the main window.\nExample:\nReference data point: 1,\nComparison data points: 3, 4, 5,\nCombinations: 1-3, 1-4, 1-5."}, {"heading": "58 Chapter 6. Menu items", "text": "\u2022 Correlation function for different data points and selected time series (mean value): like View - Cross and Auto Correlation Functions - Correlation function for different data points and selected time series, but mean values for the correlation functions will be computed. As a result, for each time series only one function is generated.\n\u2022 Correlation coefficients selected data point and time series (pre-defined time-shift Tau): like View - Cross and Auto Correlation Functions - Correlation function for different data points and selected time series, but the data points are selected by the main window. A correlation coefficient will be computed for each combination of data points and a predefined time shift \u03c4 . The result is shown as color matrix. Hereby, the correlation coefficient of a type according to Control element: Data mining: Statistical options - Type for correlation parameters is used.\n\u2022 Mean correlation coefficients over time series for selected data points and time series (defined time shift Tau): like View - Cross and Auto Correlation Functions - Correlation coefficients selected data point and time series (pre-defined time-shift Tau), but mean values for the different time series will be computed."}, {"heading": "6.3.13 FFT", "text": "includes all functions for computation and analysis of the Fast-Fourier-Transformation (FFT).\n\u2022 Compute and show FFT (selected data points and time series): computes a Fast-Fourier-Transformation (FFT) for all selected time series and data points and plots the results. The control element Control element: View: Spectrogram, FFT, CCF - Plot FFT vs. period length (instead of frequency) switches between a plot vs. period lengths and frequency (default value).\nThe function reduces the number of sampling points to the next smaller power of two to compute a FFT and to avoid window artifacts (e.g. 5000 sampling points\u2192 reduction to 4096).\n\u2022 Show dominant frequencies (sorted by amplitude): shows the most dominant frequencies of the FFT sorted by amplitudes. The number is defined under Control element: View: Spectrogram, FFT, CCF - Number of dominant frequencies for visualization.\n\u2022 Show dominant frequencies (sorted by frequency): shows the most dominant frequencies of the FFT sorted by frequencies. The number is defined under Control element: View: Spectrogram, FFT, CCF - Number of dominant frequencies for visualization."}, {"heading": "6.3.14 Fuzzy systems", "text": "contains all functions for the visualization of a fuzzy system designed by Data mining - Fuzzy systems - Design (single rules), Data mining - Fuzzy systems - Design (rule base) or by a chosen fuzzy classifier.\n\u2022 Show rules (with variable names): shows a list of the recent fuzzy rules. It results from loading a rule base from file, from generating single rules or rule bases. The input and output variables are written in form of the variable names."}, {"heading": "6.3. Menu items \u2019View\u2019 59", "text": "\u2022 Show rules (with variable numbers): like View - Fuzzy systems - Show rules (with variable names), but uses the numbers of input variables and a symbolic output variable y.\n\u2022 Show rules (figure): shows a fuzzy rules with a maximum number of three input variables in a figure.\nThe corresponding figure shows Fig. 6.2."}, {"heading": "6.3.16 Clustering", "text": ""}, {"heading": "6.3.15 Decision tree (LaTeX)", "text": "60 Chapter 6. Menu items\n\u2022 Clustergram: computes a two-dimensional clustering for the selected single features and data points. This command executes the function \u201dclustergram\u201d of the MATLAB Bioinformatics Toolbox."}, {"heading": "6.3.17 Self Organizing Maps...", "text": "contains all functions for the visualization of Self Organizing Maps.\n\u2022 Positions: shows the positions of the weight vectors of Self Organizing Maps in the (single) feature space (for 2D using the MATLAB function plotsompos.m).\n\u2022 Hits: show the number of hits per neuron for Self-Organizing Maps (based on the design)."}, {"heading": "6.3.18 Data point distances", "text": "contains all functions to compute data point distances.\n\u2022 Compute (selected data points): computes pairwise distances between all selected data points. Here, the Frobenius norm for all selected single features will be applied. In addition, an optional normalization defined by Control element: Data mining: Classification of single features - Normalization of single features is used.\n\u2022 Compute (selected data points vs. manual selection): computes pairwise distances between all selected data points vs. all data points in Control element: General options - Manual selection of data points.\n\u2022 Compute (searching for neighbors of the first element of the manual selection): computes the nearest neighbors for the first data point in Control element: General options - Manual selection of data points using the data point distances to the selected data points.\n\u2022 Sort (VAT algorithm): resorts data point in the plot based on VAT (visual assessment of tendency) algorithm to improve the interpretation of data point distance matrices (similar data points are grouped near together).\n\u2022 Show vector: shows the computed data point distances in vector form (x: data point number, y: distance).\n\u2022 Show matrix: shows the computed data point distances in matrix form (x,y: data point numbers, color: distance).\n\u2022 Show neighbors: plots the k nearest neighbors computed by View - Data point distances - Compute (searching for neighbors of the first element of the manual selection) in the MATLAB workspace. The value of k is chosen by Control element: Data mining: Special methods - k."}, {"heading": "6.4. Menu items \u2019Data mining\u2019 61", "text": ""}, {"heading": "6.3.19 Project report", "text": "shows the information about the recent project (name, number of data points etc.) and all selected parameters in the control elements."}, {"heading": "6.3.20 Recoding table", "text": "shows modified class numbers if an output variable was not original numbered with 1 ... n in the code variable."}, {"heading": "6.4 Menu items \u2019Data mining\u2019", "text": "contains all functions for the supervised and unsupervised classification and for the regression of single features and time series."}, {"heading": "6.4.1 Selection and evaluation of single features", "text": "contains all functions for the evaluation and selection of single features using data mining methods.\n\u2022 ANOVA, univariate: computes feature relevances with the ANOVA method. In contrast to the MANOVA method redundancies between features are not investigated. The method assumes a normal distribution for each output class (linguistic term) in the input space.\n\u2022 MANOVA, multivariate: computes feature relevances with the MANOVA method. It performs a step-wise selection by adding one best single feature to an existing set of selected features. The selection terminates if the desired number of features in Control element: Data mining: Classification of single features - Number of selected features is reached or if the next feature does not increase the feature relevance of the set of features. The method takes redundancies between features into account.\n\u2022 Information theoretic measures: similar to Data mining - Selection and evaluation of single features - ANOVA, univariate, but the method does not assume a normal distribution.\nAll features are fuzzified and the mutual information per output entropy for the selected output variable is computed (evaluation measure of the ID3 method). However, a split by existing membership functions is used instead of a new computed binary split in each node [76].\n\u2022 Classification accuracy: similar to Data mining - Selection and evaluation of single features - MANOVA, multivariate, but computes a relative classification error as performance measure (see e.g. Eq. (3.2) in [87]). In each selection step, the design Bayes classifier bases on the pre-selected features for former steps and a new candidate feature.\n\u2022 Classification accuracy (univariate): like Data mining - Selection and evaluation of single features - Classification accuracy, but only for the selection of one single feature."}, {"heading": "62 Chapter 6. Menu items", "text": "\u2022 Fuzzy classification accuracy: like Data mining - Selection and evaluation of single features - Classification accuracy, but with the mean membership value resp. probability for the true classification decision as evaluation measure (see e.g. Eq. (3.4) in [87]).\nExample:\nData point 1: true class 1, gradual decision with membership degree 0.9 for class 1\nData point 2: true class 2, gradual decision with membership degree 0.6 for class 2\nEvaluation = (0.9+0.6)/2 = 0.75 (instead of 1.0 using Data mining - Selection and evaluation of single features - Classification accuracy)\n\u2022 Fuzzy classification accuracy (univariate): like Data mining - Selection and evaluation of single features - Fuzzy classification accuracy, but only for the selection of one feature.\n\u2022 Weighted fuzzy classification accuracy: like Data mining - Selection and evaluation of single features - Classification accuracy, but with the weighted mean membership value resp. probability for the true classification decision as evaluation measure (see e.g. Eq. (3.6) and (3.7) in [87]). The weighting leads to a higher preference of true classification results in comparison to Data mining - Selection and evaluation of single features - Fuzzy classification accuracy.\n\u2022 Weighted fuzzy classification accuracy (univariate): like Data mining - Selection and evaluation of single features - Weighted fuzzy classification accuracy, but only for the selection of one feature.\n\u2022 Compute T-test (with Statistics Toolbox!): tests the selected single features for statistical relevant difference between all pairs of linguistic terms of the selected output variable and shows the results. Only selected data points are used. A method for the correction of multiple tests can be defined by Control element: Data mining: Statistical options - Bonferroni correction. The Statistical Toolbox for Matlab is required for this function.\n\u2022 Paired T-Test (with Statistics Toolbox!): similar to Data mining - Selection and evaluation of single features - Compute T-test (with Statistics Toolbox!), but performs a paired test between two classes expressed by linguistic terms of an output variable. Therefor, for each data point belonging to a linguistic term a unique relationship to a particular data point of the other linguistic term is assumed. This relation is defined by identical values for all other output variables (e.g. patient ID,...). A method for the correction of multiple tests can be defined by Control element: Data mining: Statistical options - Bonferroni correction.\nTypical examples are pre-therapeutic and post-therapeutic data of a collective of patients. For the test, exactly one pre-therapeutic and one post-therapeutic data point must exist for each included patient. In general, this pairwise relation is not given for all data points of a project (e.g. containing healthy subjects without post-therapeutic data points). For a valid test, it can be generated by an appropriate selection of all patient data points which have to be included.\n\u2022 Compute Wilcoxon ranksum test (with Statistics Toolbox!): similar to Data mining - Selection and evaluation of single features - Compute T-test (with Statistics Toolbox!), but performs a non-parametric Wilcoxon-Ranksum-Test. The test results are also valid for data with other distributions beyond normal distributions. A method for the correction of"}, {"heading": "6.4. Menu items \u2019Data mining\u2019 63", "text": "multiple tests can be defined by Control element: Data mining: Statistical options - Bonferroni correction.\n\u2022 Test of normal distribution: tests the selected single features to normal distribution and protocols the result, for all selected data points and for each single class of the selected output variable. Here, the test defined by Control element: Data mining: Statistical options - Test of normal distribution is applied. The Statistical Toolbox for Matlab is required for this function.\n\u2022 Chi-Square test output variable vs. discrete-valued single feature: like Data mining - Evaluation of output variables - Chi-Square test for contingency tables of output variables, but for the selected output variable and the selected single features. The latter one are assumed to have discrete values.\n\u2022 Linear regression coefficients (univariate): absolute values for univariate linear regression coefficients (square root of the coefficient of determinationR2). They deal with the prognosis of the single feature defined by Control element: Data mining: Regression - Output variable of regression. The input variables are all single features except the output variable and all previous estimations.\nUnivariate relevances ignore correlations between different input variables.\nA better combination might be found by Data mining - Selection and evaluation of single features - Linear regression coefficients (multivariate). If Control element: Data mining: Regression - Preselection of features is marked, all selected input variables are preselected as first features.\nThe computed relevances can be shown e.g. by View - Single features - Show feature relevances (sorted table).\n\u2022 Linear regression coefficients (multivariate): like Data mining - Selection and evaluation of single features - Linear regression coefficients (univariate), but with multivariate instead of univariate regression coefficients.\n\u2022 Regression accuracy (univariate): like Data mining - Selection and evaluation of single features - Linear regression coefficients (univariate), but with complete regression models defined by Control elements: Data mining: Regression instead of univariate regression coefficients. The evaluation measure is the regression coefficient between the true value and the estimation generated by the regression model.\n\u2022 Regression accuracy (multivariate): like Data mining - Selection and evaluation of single features - Linear regression coefficients (univariate), but with regression models with multiple inputs.\n\u2022 Regression accuracy via improvement of the mean error (univariate): evaluates the fitness of a regression model via the relative improvement of the mean absolute error in contrast to the trivial model resulting from the mean value of the output variable.\n\u2022 Regression accuracy via improvement of the mean error (multivariate): like Data mining - Selection and evaluation of single features - Regression accuracy via improvement of the mean error (univariate), but with regression models with multiple inputs."}, {"heading": "64 Chapter 6. Menu items", "text": ""}, {"heading": "6.4.2 Evaluation of time series", "text": "contains all functions for an automatic evaluation of time series.\n\u2022 ANOVA (with sample point for Time series\u2192 Single feature): computes for all time series a relevance using a univariate variance analysis ANOVA and selects the best time series. The analysis is only performed for the sampling point chosen by Control element: Time series: Extraction - Parameters - Sample point for Time series\u2192Single feature. The number of selected time series is defined by Control element: Data mining: Classification of single features - Number of selected features.\n\u2022 MANOVA (with sample point for Time series\u2192 Single feature): see Data mining - Evaluation of time series - ANOVA (with sample point for Time series\u2192 Single feature) but using a multivariate analysis of variances (MANOVA).\n\u2022 Compute best sample points: contains all functions to search for the best sample points of a time series for a subsequent feature selection.\n\u2022 Feature maps (all time series, selected data points): computes feature relevances of all time series and shows the results as scatter plot (x axis: Time, y axis: number of time series, color: feature relevance) [19]. The resulting so-called feature map visualizes the contained information to predict the selected output variable from the selected data points using different methods.\n\u2022 Linear regression coefficients (univariate): computes absolute values for univariate linear regression coefficients (square root of the coefficient of determination R2). They deal with the prognosis of the time series defined by Control element: Data mining: Regression - Output variable of regression. The input variables are past ([k \u2212 i]) and/or present [k] sample points of all or selected time series defined by Control element: Data mining: Regression - Feature selection resp. Control element: Data mining: Regression - Sample points. Univariate relevances ignore correlations between different input variables.\nA better combination might be found by Data mining - Evaluation of time series - Linear regression coefficients (multivariate). The value of Control element: Data mining: Regression - Preselection of features is ignored, no preselection is done.\nThe computed relevances can be shown e.g. by View - Time series (TS) - Show all feature relevances (sorted table).\nOnly one data point can be selected.\n\u2022 Linear regression coefficients (multivariate): like Data mining - Evaluation of time series - Linear regression coefficients (univariate), but with multivariate instead of univariate regression coefficients."}, {"heading": "6.4.3 Evaluation of output variables", "text": "contains functions for the evaluation of output variables."}, {"heading": "6.4. Menu items \u2019Data mining\u2019 65", "text": "\u2022 Chi-Square test for contingency tables of output variables: performs a Pearson Chi-Square-Test (using the function \u201dcrosstab\u201d of the MATLAB Statistics Toolbox) that the output variables are pairwise independent (null hypothesis). The p-value indicates that this null hypothesis is rejected by chance. The results are valid for sample size >5 data points in each cell of the cross tabulation. Otherwise for cross tabulations with only two values 0 and 1, the (more conservative) Two-tail Exact Fisher Test is used in the implementation of [108] (function \u201dfisherextest\u201d). A method for the correction of multiple tests can be defined by Control element: Data mining: Statistical options - Bonferroni correction."}, {"heading": "6.4.4 Classification", "text": "contains all functions for a complete classification run based on single features (incl. feature selection and aggregation).\n\u2022 Design: trains a classifier based on the selected data points. The output variable for the supervised classification is defined by Control element: Time series: General options - Selection of output variable (also found in other windows, see e.g. Control elements: Data mining: Classification of single features). This run includes all steps defined by Control elements: Data mining: Classification of single features (optional feature selection, normalization, feature aggregation, normalization of aggregated features, and classification itself). The run is defined by all parameters from Control elements: Data mining: Classification of single features and Control elements: Data mining: Special methods.\n\u2022 Apply: applies a classifier design by Data mining - Classification - Design to all selected data points. This application includes all designed steps for feature selection, normalization, feature aggregation, normalization of aggregated features, and the classification itself.\n\u2022 Design and apply: performs a subsequent processing of Data mining - Classification - Design and Data mining - Classification - Apply."}, {"heading": "6.4.5 Time series classification", "text": "contains all functions to classify time series.\n\u2022 Design: designs a time series classifier using the parameters in Control elements: Data mining: Classification of time series. The different classifier types are described in [22]. The detailed parameters for the classifiers are defined by Control elements: Data mining: Special methods.\n\u2022 Apply: applies a classifier designed by Data mining - Time series classification - Design to the selected data points.\n\u2022 Design and apply: performs a subsequent processing of Data mining - Time series classification - Design and Data mining - Time series classification - Apply.\n66 Chapter 6. Menu items\n\u2022 Time aggregation: aggregates classifier decisions over the time using the filter parameters in (Control element: Data mining: Classification of time series - Filtering of results) resulting in modified classifier decisions.\n\u2022 Time aggregation with result plot: like Data mining - Time series classification - Time aggregation, but with an additional plot of the results."}, {"heading": "6.4.6 Hierarchical Bayes classifier", "text": "contains functions for the design and application of hierarchical Bayes classifiers.\n\u2022 Design: designs a Hierarchical Bayes classifier by a step-wise separation of single classes from all other classes [87]. The maximum number of selected and aggregated features are defined by (Control element: Data mining: Classification of single features - Number of selected features) resp. Control element: Data mining: Classification of single features - Number of aggregated features. The feature aggregation is done by a Discriminant Analysis with optimization. This function always uses a Bayes classifier.\n\u2022 Apply: applies a classifier designed by Data mining - Hierarchical Bayes classifier - Design to all selected data points.\n\u2022 Design and apply: performs a subsequent processing of Data mining - Hierarchical Bayes classifier - Design and Data mining - Hierarchical Bayes classifier - Apply."}, {"heading": "6.4.7 Fuzzy systems", "text": "contain all functions for the design and application of fuzzy systems. The design algorithms are described in [76].\n\u2022 Design (single rules): searches for relevant fuzzy rules using only the selected single features and the selected output variable. This design does not consider the cooperation of the designed rules in a rule base (e.g. to avoid redundancies). Preexisting rules will be deleted.\n\u2022 Design (rule base): searches for a relevant fuzzy rule base using only the selected single features and the selected output variable. This design searches in first step for single rules and collects a small set of cooperating rules by an optimization. The evaluation measure prefers rules without redundancies and a complete coverage of the input space. Preexisting rules will be deleted.\n\u2022 Delete rules: open a window to delete fuzzy rules manually."}, {"heading": "6.4. Menu items \u2019Data mining\u2019 67", "text": "\u2022 Design membership functions for single features: designs membership functions for all single features using the method specified by Control element: Data mining: Special methods - Type of membership function.\n\u2022 Export fuzzy system to classifier: exports a designed fuzzy system as classifier. A design by Data mining - Fuzzy systems - Design (rule base) corresponds to a classifier design using the selected features without an aggregation. A design by Data mining - Fuzzy systems - Design (single rules) is also possible, but it leads in many cases to bad results due to redundant rules and missing priorities of good single rules.\n\u2022 Import fuzzy system from classifier: imports a fuzzy system from a designed fuzzy classifier. This function is only available if \u201dfuzzy classifier\u201d was chosen in Control element: Data mining: Classification of time series - Classifier type during last design. It is useful to list and to visualize fuzzy rules from classifiers (see also View - Fuzzy systems).\n\u2022 Import fuzzy system from regression model: imports a fuzzy system from a regression model designed by Data mining - Regression - Design resp. Data mining - Regression - Design and apply. Before designing, \u201dFuzzy system\u201d has to be selected in Control element: Data mining: Regression - Type."}, {"heading": "6.4.8 Clustering", "text": "contains all functions for the design and application of clusters.\n\u2022 Design and apply: contains different methods to search for subgroups with fuzzy cluster methods (FCM) for the selected time series and resp. or single features (see e.g. [64]). The design methods use only the selected data points. The application for the cluster assignment is done for all data points. The necessary parameters and methods as well as the options for the conversion of clusters into new output variables are defined in Control elements: Data mining: Clustering. Here, own clustering functions are applied.\n\u2022 Design and apply (Statistic Toolbox): computes clusters for the selected single features and resp. or time series. The necessary parameters and methods as well as the options for the conversion of clusters into new output variables are defined in Control elements: Data mining: Clustering. In contrast to Data mining - Clustering - Design and apply, the crisp and hierarchical cluster algorithms of the Statistic Toolbox of Matlab (e.g. \u201dpdist\u201d, \u201dlinkage\u201d and \u201dcluster\u201d) are used. A detailed description can be found in the handbook of this toolbox."}, {"heading": "6.4.9 Association analysis", "text": "performs an Association analysis for all output variables. Here, an implementation of Narine Manukyan was integrated. The results are saved as file. If this analysis should include single features, these single features have to be converted to output variables by Edit - Convert - Selected single features\u2192 Output variables."}, {"heading": "68 Chapter 6. Menu items", "text": "Output variables with too many linguistic terms will be ignored, the maximum of term numbers is defined by Control element: Data mining: Special methods - Ignore output variables with many terms. Only rules with a minimum confidence and support are shown (Definition in Control element: Data mining: Special methods - Minimum confidence resp. Control element: Data mining: Special methods - Minimum support)."}, {"heading": "6.4.10 Self Organizing Maps", "text": "contains the functions for the design and application of Self Organizing Maps. Here, the function selforgmap.m from MATLAB Neural Network Toolbox is used.\n\u2022 Design: designs Self Organizing Maps for all selected single features with the function selforgmap.m from MATLAB Neural Network Toolbox. The dimensionality is defined by Control element: Data mining: Special methods - Dimension, the number of neurons per dimension by Control element: Data mining: Special methods - Number of neurons and the number of training epochs by Control element: Data mining: Special methods - Number of learning epochs.\n\u2022 Apply: applies a Self Organizing Map designed by Data mining - Self Organizing Maps - Design to the selected data points. A new output variable with the number of winner neurons in the terms is generated."}, {"heading": "6.4.11 Regression", "text": "contains all functions for the design and application of regression models.\n\u2022 Design: designs a regression model using the selected data points and the parameters chosen in Control elements: Data mining: Regression.\nThe related options (regression of time series or single features, feature selection and aggregation, normalization, regression method) are chosen in Control elements: Data mining: Regression. The methods can be selected and parameterized using Control elements: Data mining: Special methods.\n\u2022 Apply: applies the recent regression model to the selected data points. The recent model results from the last design step or a loaded regression model.\nHereby, all related operations (regression of time series or single features, feature selection, and aggregation, normalization, regression method) are executed.\n\u2022 Design and apply: performs a subsequent processing of Data mining - Regression - Design and Data mining - Regression - Apply."}, {"heading": "6.4. Menu items \u2019Data mining\u2019 69", "text": ""}, {"heading": "6.4.12 Validation", "text": "validates the classification of single features and time series.\n\u2022 Classification of single features: validates classifiers. Possible validation strategies are a cross-validation (with diverse subtypes) and a bootstrap method (selection by Control element: Data mining: Validation - Validation strategy). The menu item Data mining - Classification - Design is performed for each produced training data set using the defined options. The resulting classifier is applied for each validation data set by means of Data mining - Classification - Apply. The results are aggregated and written into protocol files.\n\u2022 Classification of single features (selected macros): similar to Data mining - Validation - Classification of single features but with macros for classifier design and application instead of the corresponding menu items. The macros are defined by a configuration window or by the values of the variables \u201dmakro_lern\u201d (design) resp. \u201dmakro_test\u201d (application) for a script-based technique.\n\u2022 Hierarchical Bayes Classifier for single features: similar to Data mining - Validation - Classification of single features but for Hierarchical Bayes classifiers designed by Data mining - Hierarchical Bayes classifier - Design.\n\u2022 Time series classification: see Data mining - Validation - Time series classification (selected macros). Here, the design and application of the time series classifiers are executed. A selection of user-defined macros is not possible.\n\u2022 Time series classification (selected macros): validates time series classifiers. Two macros are executed for the classifier design and its application. Both macros are selected by a configuration window. Different types of cross-validation and bootstrap are available as validation strategies. The parameters are tuned by Control elements: Data mining: Validation. The results are separately computed for each trial. In addition, the mean value and the standard deviation for all trials are shown.\nFor the cross-validation of time series, a variable zr_fehl_proz is introduced in the struct relevanz_cv_alle. It contains the classification error vs. time. The mean value and the standard deviation is defined by the minimum error of each trial.\n\u2022 Regression: see Data mining - Validation - Regression (own macros). Here, the design and application of the regression models are executed using the standard macros \u201dregr_em_en.makrog\u201d (design) and \u201dregr_em_an.makrog\u201d (application). A selection of user-defined macros is not possible.\n\u2022 Regression (own macros): validates regression models. Two macros are executed for the regression model design and its application. Each macro is selected by a configuration window. Different types of cross-validation and bootstrap are available as validation strategies. The parameters are tuned by Control elements: Data mining: Validation. The results (regression error and (Pearson) correlation coefficient) are separately computed for each trial. In addition, the mean value and the standard deviation for all trials are shown."}, {"heading": "70 Chapter 6. Menu items", "text": ""}, {"heading": "6.5 Menu items \u2019Extras\u2019", "text": "contains all functions to work with macros and for the administration of application-specific extension packages."}, {"heading": "6.5.1 Play macro...", "text": "loads and runs a recorded macro. If an error message about a missing macro in the search path occurs, possible reasons are deleted macros or errors during the last macro run. In latter case, the use of Extras - Reset macro names might help."}, {"heading": "6.5.2 Play macro (debug mode)...", "text": "executes a macro in debug mode. The selected macro is copied into a m-file called \u201dmakro_m_file.m\u201d and it is started. With this option, the complete debugging functionality MATLAB is available. After a new start, SciXMiner checks if \u201dmakro_m_file.m\u201d was changed manually. If yes, it asks if these changes should be transferred also to the macro."}, {"heading": "6.5.3 Record macro...", "text": "records a sequence of clicked menu items and control elements as macro file *.makrog. A manual modification of this file is possible due to its textual Matlab syntax. The macro name is defined by an additional window. The background of the SciXMiner window is yellow during recording. All menu items are executed in parallel. The recording is stopped by Extras - Stop macro record. Some functions (especially file operations) are not available during macro recording. They can be added manually by using the related callbacks from the file menu_elements.m. A critical evaluation of the recorded macros is strongly recommended especially for complex tasks.\nA modified assignment of plotted figures (e.g. to plot multiple figures into subplots instead of separate windows) is possible by Control element: General options - For macros: plot always in current figure. Such commands (e.g. subplot) should be placed directly before the menu item in the macro to avoid a plot into wrong figures (especially in the SciXMiner main window)."}, {"heading": "6.5.4 Stop macro record", "text": "terminates the record of a macro."}, {"heading": "6.5.5 Edit macro...", "text": "opens an existing macro in an editor window for a manual modification."}, {"heading": "6.5.6 Reset macro names", "text": "resets the names of the recent macro in the Matlab workspace. This function is important after an occurring error during the macro execution. Otherwise, problems during the execution of different macros are possible."}, {"heading": "6.5. Menu items \u2019Extras\u2019 71", "text": ""}, {"heading": "6.5.7 Execute M-file...", "text": "executes a M file."}, {"heading": "6.5.8 Edit M-file...", "text": "opens an existing M-file in an editor window for a manual modification."}, {"heading": "6.5.9 Edit SciXMiner batch file ...", "text": "opens an existing SciXMiner batch file in an editor window for a manual modification."}, {"heading": "6.5.10 Generate PDF project report (needs Latex)", "text": "generates PDF reports using Latex for the given project or all projects within a directory. Latex must be installed and the functions texify and dvipdfm must be found in the Windows search path.\nThe function generates a subdirectory \u201dreport\u201d in the recent working directory. Here, all new or modified files for the project report are saved. The title page is generated from the Latex file \u201dreporttemplate.tex\u201d, located in the directory \u201dstandardmakros\u201d of SciXMiner. It can be modified for individual needs. Only for the first report, the template content is copied to a new file \u201dproject_results_*.*\u201d in the \u201dreport\u201d directory. After this, it can be also modified to individual needs only for this project or directory.\nThe following contents are added if they exist:\nOnce per PDF:\n1. File \u201dgeneral_comments.tex\u201d. Here, all project related metadata can be added (e.g. experiment planning).\n2. Error protocol \u201derror.log\u201d.\n3. All jpeg images located in a subdirectory \u201dimages\u201d.\nOnce per project:\n1. All jpeg image generated by SciXMiner for a project.\n2. All Latex files generated by SciXMiner for a project. Here, only the parts between the begin and the end of the document are used. Optional sections SciXMiner parameters are removed starting from the first subsection{Parameter}.\nAll modified files (and only these, not the images and unmodified Latex files) are written in the subdirectory \u201dreport\u201d. They should not be modified manually to an overwriting by the next generation of a project report.\n\u2022 for the current project: generated the PDF project report only for the recent project.\n\u2022 for all projects in the directory: generated the PDF project report for all projects in the recent directory. The different projects are included as sections. The name of the report is set to the name of the directory."}, {"heading": "72 Chapter 6. Menu items", "text": ""}, {"heading": "6.5.11 Translate German Gait-CAD m-files and macros into English", "text": "this menu item supports the translation of German Gait-CAD m-files and macros into English. It converts strings in all macros and m-files of a chosen directory using a dictionary. The related directory and a further directory for a safety copy are selected by a separate window."}, {"heading": "6.5.12 Choose application-specific extension packages...", "text": "switch packages with application-specific extensions on or off. This selection will be done after the next SciXMiner start. At the moment, the extension packages \u201dPeptides\u201d, \u201dImages and Videos\u201d and \u201dTracking\u201d are available. The beta versions of the extension packages \u201dGait analysis\u201d and \u201dText mining\u201d are available on request."}, {"heading": "6.5.13 Search path for m-files and plugins", "text": "adds a directory to the MATLAB search path to use m-files and plugins in this directory in SciXMiner.\n\u2022 Permanent: adds a directory permanently to the MATLAB search path to use m-files and plugins in this directory in SciXMiner. This information is saved in read_gaitcad_searchpath.m.\n\u2022 Temporary for the session: adds a directory for the recent SciXMiner session to the MATLAB search path to use m-files and plugins in this directory in SciXMiner.\n\u2022 Reset (permanent search path): resets the permanent search path, that was added by Extras - Search path for m-files and plugins - Permanent and saved in read_gaitcad_searchpath.m.\n\u2022 Reset (only temporarily for the session): resets the search path for the SciXMiner session, that was defined by Extras - Search path for m-files and plugins - Temporary for the session."}, {"heading": "6.5.14 Matlab Parallel", "text": "controls the use of configurations of the MATLAB Parallel Computing Toolbox.\n\u2022 Start: starts the use of the selected configuration of the MATLAB Parallel Computing Toolbox. Here, the configuration with the name defined by Control element: General options - Configuration name for MATLAB parallel is used (see also Parallel - Manage Configurations in the menu of the MATLAB command window).\n\u2022 Stop: stops the use of the recent configuration of the MATLAB Parallel Computing Toolbox."}, {"heading": "6.6. Menu items \u2019Favorites\u2019 73", "text": ""}, {"heading": "6.6 Menu items \u2019Favorites\u2019", "text": "contains all functions for a fast access to frequently used or user-defined menu points and macros. All deactivated entries (e.g. due to missing single features) are shown with a light gray color."}, {"heading": "6.6.1 Edit user-defined favorites", "text": "opens a configuration window to add and delete menu points from the favorite list.\n6.6.2 Delete all favorites\ndeletes all favorites."}, {"heading": "6.7 Menu items \u2019Window\u2019", "text": "activates or closes all open Matlab figures except the SciXMiner main window."}, {"heading": "6.7.1 Close figures", "text": "closes all figure except the SciXMiner main window."}, {"heading": "6.7.2 Arrange figures", "text": "contains different options for the placement of figures.\n\u2022 Horizontal: places all figures horizontally.\n\u2022 Vertical: places all figures vertically.\n\u2022 Cascade: places all figures as a cascade.\n\u2022 Position of the current figure: places all figures to the position of the last opened figure. This command is especially useful for the comparison of figures with small differences by switching between figures with the same size and position."}, {"heading": "74 Chapter 6. Menu items", "text": ""}, {"heading": "6.7.3 Logarithmic scaling of current figures", "text": "applies a logarithmic scale to all open MATLAB figures.\n\u2022 only x axis: applies a logarithmic scale to the x-axis of all open MATLAB figures.\n\u2022 only y-axis: applies a logarithmic scale to the y-axis of all open MATLAB figures.\n\u2022 x- and y axis: applies a logarithmic scale to the x- and the y-axis of all open MATLAB figures."}, {"heading": "6.7.4 Remove Latex codes in MATLAB figures", "text": "removes Latex-style variables interpreted as equation (e.g. x1 instead of x_1) from all open MATLAB figures."}, {"heading": "6.7.5 Update font and font size in figures", "text": "sets font type and size in all open MATLAB figures to the values defined by Control element: View: Single features - Font and Control element: View: Single features - Font size."}, {"heading": "6.7.6 Plot all figures as images in files", "text": "plots the content of all figures in files. The filename is generated from the project and the figure name. The file type is defined by Control element: General options - File type for images. The image in the file depends on the size on the monitor."}, {"heading": "6.8 Menu items \u2019Help\u2019", "text": "contains SciXMiner version and license information as well as SciXMiner handbooks."}, {"heading": "6.8.1 Show SciXMiner documentation (PDF)", "text": "opens the SciXMiner handbook and help file as PDF."}, {"heading": "6.8.2 About SciXMiner", "text": "contains the version information and the contact address to the developer team."}, {"heading": "6.8.3 License information", "text": "shows the license file for the GNU licence."}, {"heading": "7 Control elements", "text": ""}, {"heading": "7.1 Control elements for \u2019Project overview\u2019", "text": "75"}, {"heading": "76 Chapter 7. Control elements", "text": ""}, {"heading": "7.2 Control elements for \u2019Time series: General options\u2019", "text": "\u2022 Selection of output variable: selects an output variable. This selection influences many functions as the evaluation of single features and time series, the design of classifiers, almost all visualization functions etc.\n\u2022 Sampling frequency of time series: defines the sampling frequency for the time series in the data set. It is assumed that this value is equal for all time series. The parameter mainly influences the visualization of time series using time or frequency.\n\u2022 Unit: defines the physical quantity for time series (e.g. sampling frequency in kHz, per day etc.). The parameter mainly influences the visualization of time series or spectrograms using time or frequency.\n\u2022 Selection of time series (TS): selects time series for visualization and all following processing steps. The selection can be done by mouse clicks in the listbox, by writing the numbers in the edit field on the left hand side or by the \u2019ALL\u2019 button to select all time series. The different fields will be synchronized after the input.\nTIP: An input in the edit field enables a selection with a different order (e.g. 2-4-1 for a visualization) in contrast to the listbox."}, {"heading": "7.2. Control elements for \u2019Time series: General options\u2019 77", "text": "\u2022 Complete time series: selects all sample points of the time series.\n\u2022 Time series segment from: defines the begin of a time series segment for the visualization of sample points. This parameter might also defined by a zooming in a time series visualization (View - Time series (TS)) and pushing the button \u201dSelect time series segment\u201d in the menu bar. The begin is the left limit of the current x-axis.\nThe button \u201dComplete time series\u201d set the time series segment to all sample points.\n\u2022 to: defines the end of a time series segment for the visualization of sample points. This parameter might also defined by a zooming in a time series visualization (View - Time series (TS)) and pushing the button \u201dSelect time series segment\u201d in the menu bar. The end is the right limit of the current x-axis.\nThe button \u201dComplete time series\u201d set the time series segment to all sample points.\n\u2022 Forbid segments of length 1: forbid time series segments of length 1. This option is almost always useful except some special cases for feature extraction."}, {"heading": "78 Chapter 7. Control elements", "text": ""}, {"heading": "7.3 Control elements for \u2019Time series: Extraction - Parameters\u2019", "text": "\u2022 Data-dependent feature extraction in plugins: defines the handling of plugins with data-dependent transformation functions (and not only datadependent output variables for time series and single features!). Typical examples are a transformation matrix for a Principal Component Analysis or fuzzy membership functions. Possible options are a computation with and without saving the results into a *.plugpar file with the same project name or a loading from these file. By loading, the name of the time series, the plugin description, and the segments must be the same. The handling has to be separately implemented for each plugin!\n\u2022 Sample point for Time series\u2192Single feature: select the sample point for all functions using only one sample point for a conversion from time series to single features.\n\u2022 Parameter IIR filter: defines the filter parameter a for an Infinite-Impulse-Response (IIR) filter. The filter is a first-order lowpass with xf [k + 1] = a \u2217 xf [k] + (1 \u2212 a) \u2217 x[k]. The parameter defines the influence of old values. A higher value leads to a slower behavior due to a higher weighting of past values. Some functions and plugins for the feature extraction use this parameter. For stability reasons, the value is limited to [0, 1]."}, {"heading": "7.3. Control elements for \u2019Time series: Extraction - Parameters\u2019 79", "text": "\u2022 IIR parameter (aF, aS, aSigma): defines the parameters for three different IIR filters for trend and standard deviation computation (see [90]).\n\u2022 Filter order (FIL): defines the order of the filter for feature extraction.\n\u2022 High-: switches to a high-pass filter for feature extraction.\n\u2022 Low-: switches to a low-pass filter for feature extraction.\n\u2022 Bandpass: switches to a band-pass filter for feature extraction.\n\u2022 Frequencies (FIL, Morlet spectrogram): defines the cutoff frequencies for different filters. The parameter is used for a Butterworth filter in feature extraction and for Morlet spectrograms. A high-pass filter and a low-pass filter use only the first value, whereas the band-pass filter uses both values.\n\u2022 Wavelet: selects the wavelet type for a wavelet decomposition (see e.g. [63] or the documentation of the Matlab Wavelet Toolbox).\n\u2022 Matlab wavelet decomposition: uses the Matlab implementation instead of the implementation of [63] for the wavelet decomposition. It requires the Matlab Wavelet Toolbox, but it is faster especially in case of many data points and long time series.\n\u2022 Wavelets: number of levels: defines the number of levels for a wavelet decomposition. It computes for the half of the sampling frequency fA an high-pass filter and a low-pass filter. The high-pass in level i has a cutoff frequency of (i = 1, . . .) with\nfA 2i+1 .\nThe cut-off frequency for the low-pass filter is computed accordingly.\n\u2022 Morlet wavelet: frequency: defines the frequency for the generation of new time series by means of complex Morlet wavelets. This frequency describes the center of the frequency range which will not be damped by the wavelet. The width of this range is defined by the eigenfrequency of the Morlet wavelet (Control element: Time series: Extraction - Parameters - Morlet wavelet: eigenfrequency). For the computation of Morlet spectrograms, the parameter from Control element: Time series: Extraction - Parameters - Frequencies (FIL, Morlet spectrogram) is used instead of this parameter.\n\u2022 Causal Morlet wavelet: temporal shift the complex Morlet wavelet for a causal filtering. In addition, the wavelet will be limited in time.\n\u2022 Morlet wavelet: eigenfrequency: see Control element: Time series: Extraction - Parameters - Morlet wavelet: frequency."}, {"heading": "80 Chapter 7. Control elements", "text": "\u2022 Shortening of time series: Window length or x-th sample point: defines the window lengths (Edit - Convert - Shorten time series - Shortening by window methods) resp. the step width (Edit - Convert - Shorten time series - Use only each x-th sample point) to short time series.\n\u2022 Shortening of time series: method: selects the method for shortening all time series in a project using Control element: Time series: Extraction - Parameters - Shortening of time series: Window length or x-th sample point. First, a window of with a length defined by Edit - Convert - Shorten time series - Shortening by window methods is chosen. Second, the new value for the short time series is computed by the mean, median, minimum, or maximum value of these windows. The windows do not overlap.\n\u2022 Match time series length to: defines the length for the shortening of the time series via Edit - Convert - Shorten time series - Match time series lengths (use 0 and NaN as undefined values). The new length of the time series is fixed (percent: 100, per thousand 1000) or the shortest time series in the data set is used. The length of the shortest time series is defined by the sample after which the time series is only zero, Inf or NaN."}, {"heading": "7.4. Control elements for \u2019Plugin sequence\u2019 81", "text": ""}, {"heading": "7.4 Control elements for \u2019Plugin sequence\u2019", "text": "\u2022 Show plugins: select the shown plugins depending on type (e.g. time series\u2192 time series).\n\u2022 Add: adds the selected plugins from Control element: Plugin sequence - Selection of plugins to a plugin sequence.\n\u2022 Update plugins: reads the available plugins from all plugin files. It is useful for the development of plugins to avoid SciXMiner restarts.\n\u2022 Show plugin list: writes the characteristics and descriptions of all available plugins in a file and displays this file.\n\u2022 Selection of plugins: selects the plugins used by Edit - Extract - Time series \u2192 Time series, Time series \u2192 Single features (via plugin sequence).... Only a definition in the edit window guarantees the selected order of the plugins. A definition in the list box ignores this order and sorts the plugins by their numbers.\n\u2022 Plugin parameter: set the parameters of the selected plugin from Control element: Plugin sequence - Selection of"}, {"heading": "82 Chapter 7. Control elements", "text": "plugins resp. Control element: Plugin sequence - Selected plugin sequence. The switch between multiple parameters is done by Control element: Plugin sequence - No..\n\u2022 No.: selects a specific parameter for plugins with multiple parameters.\n\u2022 Forward: moves the selected plugins in Control element: Plugin sequence - Selected plugin sequence to earlier positions in the sequence.\n\u2022 XPIWIT: Execute pipeline stepwise: execute plugins in XPIWIT separately. In this option, for each plugin a separate call of XPIWIT.exe is executed. This option is only used by the extension package \u201dImages and Videos\u201d.\n\u2022 Down: moves the selected plugins in Control element: Plugin sequence - Selected plugin sequence to later positions in the sequence.\n\u2022 Delete: deletes the recent plugin from the plugin sequence.\n\u2022 Delete all: deletes all plugins from a plugin sequence.\n\u2022 Load: loads a plugin sequence from a plugin sequence file (*.plugseq). The matching of the plugins is done by the function names.\n\u2022 Selected plugin sequence: shows the recent plugin sequence. The plugin sequence is performed step by step using Control element: Plugin sequence - Execute resp. Edit - Extract - Time series\u2192 Time series, Time series \u2192 Single features (via plugin sequence)... (plugins for time series) or Images and Videos - Apply plugin sequence to the selected images (plugins for images). Existing parameters of the plugins are set by Control element: Plugin sequence - Plugin parameter. The order of plugins can be changed by Control element: Plugin sequence - Down etc.\nWARNING! If the plugin sequence is recorded by a macro, each plugin must be added separately and its parameters have to be chosen before the next plugin can be added. Otherwise, the macros recording might cause problems if the plugin sequence contains more than one plugin with the same name. In this case, the parameter of the last plugin with the same name are adapted.\n\u2022 Save: saves a plugin sequence with the defined plugin parameters in a plugin sequence file (*.plugseq).\n\u2022 Show results: defines the visualization and save options for plugin sequences of images. Intermediate results are the result of all plugins except the last one. \u201dShow\u201d means a visualization as image in a separate Matlab figure, \u201dSave\u201d the generation of a new image file and the generation of a new image type.\nPlugins in projects without images ignores this option and set it to \u201dSave final result\u201d.\n\u2022 Show parameters: shows all used parameters in a plugin sequence."}, {"heading": "7.4. Control elements for \u2019Plugin sequence\u2019 83", "text": "\u2022 Save performance log file: defines if computing times for single plugins or complete plugin sequences should be written in a log file \u201d*_PerformanceLog.csv\u201d in the recent project directory.\n\u2022 Execute: executes the recent plugin sequence in Control element: Plugin sequence - Selected plugin sequence.\n\u2022 Use the same transformation matrix (e.g. PCA) for time series reduction of all data points: influences the reduction of time series by the Principal Component Analysis. If the option is marked, an identical transformation matrix is used for all data points. Otherwise, a separate transformation matrix is computed for each data point."}, {"heading": "84 Chapter 7. Control elements", "text": ""}, {"heading": "7.5 Control elements for \u2019Single features\u2019", "text": "\u2022 Selection of output variable: selects an output variable. This selection influences many functions as the evaluation of single features and time series, the design of classifiers, almost all visualization functions etc.\n\u2022 Selection of single feature(s) (SF): selects single features for visualization and all following processing steps. The selection can be done by mouse clicks in the listbox, by writing the numbers in the edit field on the left hand side or by the \u2019ALL\u2019 button to select all single features. The different fields will be synchronized after the input.\nTIP: An input in the edit field enables a selection with a different order (e.g. 2-4-1 for a visualization) in contrast to the listbox.\n\u2022 Number of terms for Single feature\u2192Output variable: defines the number of terms for a new output variable generated by Edit - Convert - Selected single features\u2192 Output variables (see also Control element: Single features - All values). The transformation is done by a maximum defuzzification. The membership functions are computed by a cluster method.\n\u2022 All values: uses all different values of a feature as separate linguistic terms (see Edit - Convert - Selected"}, {"heading": "7.5. Control elements for \u2019Single features\u2019 85", "text": "single features\u2192 Output variables). The value of Control element: Single features - Number of terms for Single feature\u2192Output variable will be ignored. Example:\nA single feature has the values 1, 2.5, 3.5 and 7. The result is an output variable with four terms called \u201dca. 1\u201d, \u201dca. 2.5\u201d, \u201dca. 3.5\u201d and \u201dca. 7\u201d. In contrast, new terms with the number defined in Control element: Single features - Number of terms for Single feature\u2192Output variable will be computed if the checkbox is deactivated.\n\u2022 A priori feature relevances: switches on the use of a priori relevances of single features. For feature selection, features with higher a priori relevances are preferred. The values are between zero (bad feature) and one (preferable feature).\nThey can be\n- defined in the project file (variable interpret_merk)\n- generated by means of categories, or\n- manually modified (e.g. by Control element: Single features - A priori feature relevances (fix value)).\n\u2022 Preference exponent alpha (Interpretability): defines an exponent for feature relevances using a priori relevances. The exponent weights the first value of the a priori relevances mostly associated with an interpretability value. A value of zero ignores the interpretability, large values prefer features with higher interpretability.\n\u2022 A priori feature relevances (fix value): sets a defined value for a priori relevances for a manual setting using Edit - Category - A priori relevances of selected features... - set to a fix value (from GUI) for all selected single features.\n\u2022 Preference exponent beta (Implementability): defines an exponent for feature relevances using a priori relevances. The exponent weights the second value of the a priori relevances mostly associated with the implementability. A value of zero ignores the implementability, large values prefer features with higher implementability."}, {"heading": "86 Chapter 7. Control elements", "text": ""}, {"heading": "7.6 Control elements for \u2019Data preprocessing\u2019", "text": "\u2022 Save result as output variable: save the result of the outlier detection as a new output variable.\nThe option \u201dnew output variable\u201d adds always a new output variable with the result of the outlier detection. The variable name is \u201dOutlier\u201d with the name of the used method.\nThe option \u201dReplace identical output variable\u201d replace the results in an existing output variable computed by the same method. Otherwise, a new output variable is added.\n\u2022 Threshold for outliers: defines a threshold parameter if a data point is classified as outlier or not (see Edit - Outlier detection - Apply (selected data points, designed data set)).\nThe meaning of the parameter depends on the method:\nSVM - a value smaller than zero (tolerated distance to the class border)\ndistance-based: distance value\ndensity-based: number of neighbors\nThe level curves in the visualization are helpful for the parameter definition (see Control element: View: Classification and regression - Show outlier detection)."}, {"heading": "7.6. Control elements for \u2019Data preprocessing\u2019 87", "text": "\u2022 Manual selection of outliers: removes the data points with the indices in the edit field from the training data set. This function supports the outlier detection in complicated cases because these data points are not evaluated as good measurements. Depending on the related positions in feature space, it normally increases the probability of an automatic outlier detection of these data points by applying the outlier detection. All selected data points are used if the field is empty or \u201d[]\u201d is written in the field.\nDuring the application, all data points will be evaluated.\nThe number of data points is shown if Control element: View: Single features - Show data point number is marked.\n\u2022 Method: selects the classifier type for the outlier detection.\nThe one-class method optimizes the coefficients ai and b of the function: f(z) = \u2211\ni aiK(z,xi)+ b, with an unknown data point z and the data points xi of the training data set. K is a kernel function known from Support Vector Machines. A data point z is handled as outlier if f(z) < Threshold (see [23] for algorithm details). The threshold is defined by Control element: Data preprocessing - Threshold for outliers.\nThe distance-based method uses the Mahalanobis distance to the mean value of the training data set. All data points with a distance larger than Control element: Data preprocessing - Threshold for outliers (positive value) are handled as outliers.\nThe density-based method classifies all data points without a minimal number of neighbors (defined by Control element: Data preprocessing - Threshold for outliers) as outliers. The neighborhood is defined by a maximal ([0,1]-normalized Euclidean) distance. The distance is tuned by Control element: Data preprocessing - Max. distance.\n\u2022 Compute class discriminant functions: switches between hard and soft limits for the one-class method. For hard limits, all training data are handled as class members. For soft limits, some training data could be automatically classified as outliers. It makes especially sense if some data points in the training data set look suspicious.\n\u2022 Kernel order: set the order of the kernel of the SVM. It is used for the classification with the SVM and by the SVM-based outlier detection.\n\u2022 Use cityblock distance: switches on the 1-norm (city block distance) instead of the Euclidean distance for the Gaussian RBF kernel in the one-class method of the outlier detection (see Control element: Data mining: Special methods - Kernel). This results normally in rougher class borders.\n\u2022 Penalty term Lambda (for soft limits): tunes the number of training data accepted as outliers. A larger value reduces the number of outliers.\nThis parameter is only used if \u201dsoft limits\u201d are selected in Control element: Data preprocessing - Compute class discriminant functions.\n\u2022 Minimal number of data points in one class: minimal number of data points for the selection with Edit - Select - Data points via most frequent terms."}, {"heading": "88 Chapter 7. Control elements", "text": "\u2022 Threshold for deleting [Perc. missing data]: defines the percental threshold in data preprocessing for the deletion of time series or single features in case of missing values. Otherwise, each data point with at least one missing value in any single feature or data point will be deleted. A time series contains missing values if at least one sample point is missing. Missing values must be coded with NaN values or Inf values. Time series with zero values for all sample points are also considered as missing values.\nExample:\nProject with 100 data points and 15 % missing values (data point 1-15) for time series x1 and 5 % missing values for time series x2 (data point 96-100) and threshold of 10 %: Time series x1 will be completely deleted, data points 96-100 will be deleted.\n\u2022 Feature values for selection: value range for the selection of data points based on single feature values in Control element: Data preprocessing - Feature values for selection.\n\u2022 Date format: defines the date format for the conversion of dates and hours in an output variable into a MATLAB timestamp format using Edit - Convert - Selected output variable (Date) \u2192 Single feature (Timestamp)."}, {"heading": "7.6. Control elements for \u2019Data preprocessing\u2019 89", "text": "\u2022 Save result as output variable: save the result of the outlier detection as a new output variable.\nThe option \u201dnew output variable\u201d adds always a new output variable with the result of the outlier detection. The variable name is \u201dOutlier\u201d with the name of the used method.\nThe option \u201dReplace identical output variable\u201d replace the results in an existing output variable computed by the same method. Otherwise, a new output variable is added.\n\u2022 Threshold for outliers: defines a threshold parameter if a data point is classified as outlier or not (see Edit - Outlier detection - Apply (selected data points, designed data set)).\nThe meaning of the parameter depends on the method:\nSVM - a value smaller than zero (tolerated distance to the class border)\ndistance-based: distance value\ndensity-based: number of neighbors\nThe level curves in the visualization are helpful for the parameter definition (see Control element: View: Classification and regression - Show outlier detection).\n\u2022 Manual selection of outliers: removes the data points with the indices in the edit field from the training data set. This function supports the outlier detection in complicated cases because these data points are not evaluated as"}, {"heading": "90 Chapter 7. Control elements", "text": "good measurements. Depending on the related positions in feature space, it normally increases the probability of an automatic outlier detection of these data points by applying the outlier detection. All selected data points are used if the field is empty or \u201d[]\u201d is written in the field.\nDuring the application, all data points will be evaluated.\nThe number of data points is shown if Control element: View: Single features - Show data point number is marked.\n\u2022 Method: selects the classifier type for the outlier detection.\nThe one-class method optimizes the coefficients ai and b of the function: f(z) = \u2211\ni aiK(z,xi)+ b, with an unknown data point z and the data points xi of the training data set. K is a kernel function known from Support Vector Machines. A data point z is handled as outlier if f(z) < Threshold (see [23] for algorithm details). The threshold is defined by Control element: Data preprocessing - Threshold for outliers.\nThe distance-based method uses the Mahalanobis distance to the mean value of the training data set. All data points with a distance larger than Control element: Data preprocessing - Threshold for outliers (positive value) are handled as outliers.\nThe density-based method classifies all data points without a minimal number of neighbors (defined by Control element: Data preprocessing - Threshold for outliers) as outliers. The neighborhood is defined by a maximal ([0,1]-normalized Euclidean) distance. The distance is tuned by Control element: Data preprocessing - Max. distance.\n\u2022 Max. distance: defines the accepted maximum distance for a neighborhood (see Control element: Data mining: Special methods - Evaluate minimum number of neighbors).\n\u2022 Minimal number of data points in one class: minimal number of data points for the selection with Edit - Select - Data points via most frequent terms.\n\u2022 Threshold for deleting [Perc. missing data]: defines the percental threshold in data preprocessing for the deletion of time series or single features in case of missing values. Otherwise, each data point with at least one missing value in any single feature or data point will be deleted. A time series contains missing values if at least one sample point is missing. Missing values must be coded with NaN values or Inf values. Time series with zero values for all sample points are also considered as missing values.\nExample:\nProject with 100 data points and 15 % missing values (data point 1-15) for time series x1 and 5 % missing values for time series x2 (data point 96-100) and threshold of 10 %: Time series x1 will be completely deleted, data points 96-100 will be deleted.\n\u2022 Feature values for selection: value range for the selection of data points based on single feature values in Control element: Data preprocessing - Feature values for selection.\n\u2022 Date format: defines the date format for the conversion of dates and hours in an output variable into a MATLAB timestamp format using Edit - Convert - Selected output variable (Date) \u2192 Single feature (Timestamp)."}, {"heading": "7.7. Control elements for \u2019View: Single features\u2019 91", "text": ""}, {"heading": "7.7 Control elements for \u2019View: Single features\u2019", "text": "\u2022 Show figures: switches between color-coded, number-coded (for the number of the linguistic term of the output variable), and (black and white) symbol-coded features and time series.\n\u2022 Color style: modifies the visualization of output variables by means of different color and style combinations for time series and single features. It defines the mapping of linguistic terms to colors and styles.\n\u2022 User-defined colors: defines the order of user-defined colors for the visualization of linguistic terms of the selected output variable in time series and single feature plots. Here, the color abbreviations of Matlab are used (see help text for the function plot). The option is only active if User-defined (color or symbol) resp. User-defined (color and symbol)) is selected in Control element: View: Single features - Color style and Colored is selected in Control element: View: Single features - Show figures.\n\u2022 User-defined symbols: defines the order of user-defined symbols for the visualization of linguistic terms of the selected output variable in time series and single feature plots. Here, the symbol abbreviations of Matlab are used (see help text for the function plot). The option is only active if User-defined (color or symbol) resp. User-defined (color and symbol)) is selected in Control element: View: Single"}, {"heading": "92 Chapter 7. Control elements", "text": "features - Color style and Black-and-white symbol is selected in Control element: View: Single features - Show figures.\n\u2022 Different output variable: shows the results of classification and regression with a free selectable output variable if \u201dDifferent output variable\u201d was chosen in Control element: View: Classification and regression - Display classes for output variables.\n\u2022 SF display x-y-turn: rotates a figure by switching the x- and y-axes for a two-dimensional plot.\n\u2022 Inverse plot order terms: plots all data points in reverse order of the terms of the output variables.\n\u2022 Show data point number: switches the display of data point numbers on or off.\n\u2022 Evaluate only selected single features: evaluates only the selected single features.\n\u2022 Show legend: plot the legend with the linguistic terms of the output variable.\n\u2022 Show term names instead of data point numbers: shows the term names instead of the data point number in figures. The option is only active if Control element: View: Single features - Show data point number is activated.\n\u2022 Relative ratio noise term: relative ratio (relative to the axis scale of each single feature in the original plot) for an additional noise (see Control element: View: Single features - Add noise term).\n\u2022 Add noise term: adds a uniformly distributed noise with the factor from Control element: View: Single features - Relative ratio noise term to the shown single features. This make sense for the easier visualization of discrete-valued single features.\n\u2022 Centers for histogram bins: defines bin centers for histograms manually (e.g. 0:10:100). This bin centers are used if Control element: View: Single features - Automatic is deactivated.\n\u2022 Automatic: defines the bin centers for histograms automatically.\n\u2022 Shown feature relevances in lists: defines the number of shown features and their relevance values in relevance lists. The value is only used if Control element: View: Single features - All is switched off. The reduction of the numbers allows more compact lists for documentation purposes.\n\u2022 All: shows all features and their relevance values in relevance lists.\n\u2022 Sorting of lists: defines the order of single features in lists and text files: ascending or descending sorted resp. unsorted."}, {"heading": "7.7. Control elements for \u2019View: Single features\u2019 93", "text": "\u2022 Decimal symbol in German format (comma): defines a German style number format with commas as decimal point.\n\u2022 Output variables protocol files absolute values: defines the protocol option of output variables in files generated by View - Single features - Absolute values. Possible values are all, the selected or none output variables.\n\u2022 Classes as rows: switches the style of protocol files for mean values etc. as generated by View - Single features - Mean, standard deviation, minimum, maximum. If the option is activated, each class is written in a row and each single feature in a column. Otherwise, each single feature is written in a row and each class in a column.\n\u2022 Font: defines the font type, that is set by Window - Update font and font size in figures for all open MATLAB figures.\n\u2022 Font size: defines the font size, that is set by Window - Update font and font size in figures for all open MATLAB figures.\n\u2022 Marker size: defines the marker size in plots of single features."}, {"heading": "94 Chapter 7. Control elements", "text": ""}, {"heading": "7.8 Control elements for \u2019View: Time series\u2019", "text": "\u2022 Show figures: switches between color-coded, number-coded (for the number of the linguistic term of the output variable), and (black and white) symbol-coded features and time series.\n\u2022 Color style: modifies the visualization of output variables by means of different color and style combinations for time series and single features. It defines the mapping of linguistic terms to colors and styles.\n\u2022 User-defined colors: defines the order of user-defined colors for the visualization of linguistic terms of the selected output variable in time series and single feature plots. Here, the color abbreviations of Matlab are used (see help text for the function plot). The option is only active if User-defined (color or symbol) resp. User-defined (color and symbol)) is selected in Control element: View: Single features - Color style and Colored is selected in Control element: View: Single features - Show figures.\n\u2022 User-defined symbols: defines the order of user-defined symbols for the visualization of linguistic terms of the selected output variable in time series and single feature plots. Here, the symbol abbreviations of Matlab are used (see help text for the function plot). The option is only active if User-defined (color or symbol) resp. User-defined (color and symbol)) is selected in Control element: View: Single"}, {"heading": "7.8. Control elements for \u2019View: Time series\u2019 95", "text": "features - Color style and Black-and-white symbol is selected in Control element: View: Single features - Show figures.\n\u2022 Percental (ignore sampling frequency): switches to a percental x-axis for the visualization of time series. The time series length is set to 100%. The chosen sampling frequency will be ignored.\n\u2022 Sample points: switches to the number of sample points on the x-axis for the visualization of time series. The chosen sampling frequency will be ignored.\n\u2022 Time [Unit]: switches to the time on the x-axis for the visualization of time series. The chosen sampling frequency in Control element: Time series: General options - Sampling frequency of time series will be used to transform sample points to the time. The time starts always with zero.\n\u2022 Project-specific: uses a project-specific time scale defined by (parameter.)projekt.timescale for the visualization of time series. This struct contains the elements .name for the name of the time scale and .time for the time values of each sample point. This option is useful for non-equidistant time scales or other onedimensional scales (e.g. mass for mass spectroscopy). An existing time series can be converted to a project-specific time scale by Edit - Convert - Selected time series\u2192 Project-specific time scale.\n\u2022 Show time series as subplots: plots selected time series to individual axes, if the option is activated.\nOtherwise, all time series are plotted to the same axis. Different time series are displayed with different colors. The color mapping to linguistic terms of output values gets lost.\n\u2022 Imageplot Time series (Color coding): shows time series in a color code. The scale is tuned by Control element: View: Spectrogram, FFT, CCF - Limits for color axis and Control element: View: Spectrogram, FFT, CCF - Function for color bar, the colormap by Control element: View: Spectrogram, FFT, CCF - Colormap. The selected data points are sorted by number or by the linguistic terms of the selected output variable depending on the value of Control element: View: Time series - Sorting classes for image plot.\nIf only one time series is visualized, the option is switched off temporarily.\n\u2022 Sorting classes for image plot: switches between sorted by number or class-wise by the linguistic terms of the selected output variable. The selection is only relevant if Control element: View: Time series - Imageplot Time series (Color coding) is activated.\n\u2022 Show legend: plot the legend with the linguistic terms of the output variable.\n\u2022 Show data point number: switches the display of data point numbers on or off.\n\u2022 Poincare plot: Connect points: defines if the sample points in a Poincare plot will be connected by lines or not."}, {"heading": "96 Chapter 7. Control elements", "text": "\u2022 Show normative data: shows normative data as light-gray region in the visualization of time series. Normative data has to be produced by File - Normative data - Mean value to normative data (internally from the project) or by File - Normative data - Load normative data from a file.\n\u2022 Normative data in foreground: the visualization of many original time series may overlay the normative data. Activate this option to plot the normative data in the foreground.\n\u2022 Logarithmic visualization: select an optional logarithmic scaling of times and resp. or amplitudes of time series for the visualization.\n\u2022 Linewidth: modifies the line width for the visualization of time series. It makes sense e.g. for the export of figures in LATEXfigures with values of e.g. 1.5 or 2.\n\u2022 Font: defines the font type, that is set by Window - Update font and font size in figures for all open MATLAB figures.\n\u2022 Font size: defines the font size, that is set by Window - Update font and font size in figures for all open MATLAB figures."}, {"heading": "7.9. Control elements for \u2019View: Spectrogram, FFT, CCF\u2019 97", "text": ""}, {"heading": "7.9 Control elements for \u2019View: Spectrogram, FFT, CCF\u2019", "text": "\u2022 Window size [sample points]: defines the window size for the computation of spectrograms. It controls the compromise between a temporal and frequency resolution. A small value prefers a better temporal resolution. The value is automatically reduced to the next power of two to preserve the efficiency of the FFT algorithm (e.g. 27 = 128, 210 =1024 etc.). The value is limited by the number of sample points.\n\u2022 Normalization for crosscorrelation: defines the mode of normalization for Auto and Cross Correlation Functions:\n- \u201dbiased\u201d, scales to the length of the time series, see MATLAB function xcorr\n- \u201dunbiased\u201d, scales to (length of time series - lag), see MATLAB function xcorr (less robust results for large lags)\n- \u201dcoeff\u201d, normalizes the result to autocorrelation 1 with lag 0, see MATLAB function xcorr\n- \u201dcoeff_local\u201d - as \u201dbiased\u201d, but with an additional mean correction and variance normalization to one (identical to \u201dcoeff_mean\u201d, hold for compatibility reasons)\n- \u201dnone\u201d - without normalization, see MATLAB function xcorr\nIn addition, options with mean value correction (+\u201d_mean\u201d) and linear trend compensation (+\u201d_detrend\u201d) of the time series exist."}, {"heading": "98 Chapter 7. Control elements", "text": "\u2022 Copy correlation results to workspace: defines if the auto or cross-correlation functions compute by View - Cross and Auto Correlation Functions - Separately for each data point are saved into a workspace variable kkfs.\n\u2022 Time shift Tau [SP]: sets the time shift for the visualization of correlation coefficients in View - Cross and Auto Correlation Functions - Mean values of correlation coefficients (defined time shift) and View - Cross and Auto Correlation Functions - Class mean values of correlation coefficients (with defined time shift). A value of zero uses the same sample point for both time series.\n\u2022 Show correlation figures: defines if the figures of correlation functions are shown or not. The disabling is useful if the results are saved into a variable (see Control element: View: Spectrogram, FFT, CCF - Copy correlation results to workspace).\n\u2022 Morlet spectrogram: Frequency stepwidth: defines distances for frequencies, e.g. 1:100:10000.\n\u2022 Frequencies (FIL, Morlet spectrogram): defines the cutoff frequencies for different filters. The parameter is used for a Butterworth filter in feature extraction and for Morlet spectrograms. A high-pass filter and a low-pass filter use only the first value, whereas the band-pass filter uses both values.\n\u2022 Sample points baseline: see Control element: View: Spectrogram, FFT, CCF - Morlet spectrogram (relative to baseline).\n\u2022 Morlet spectrogram (relative to baseline): computes the Morlet spectrogram relatively to a baseline if the option is activated (see View - Morlet spectrogram - Compute and show (selected data points and time series)). Instead of absolute value, the temporal changes will be shown. The limits of the baseline are defined by Control element: View: Spectrogram, FFT, CCF - Sample points baseline.\n\u2022 Reduce sampling points for Morlet spectrogram: reduces the number of shown sample points, because exported figures with Morlet spectrograms need a lot of memory. If a value k is greater than one, only each k-th sample point will be shown. The loss of information is usually acceptable for small values.\n\u2022 Number of principal components for spectrogram: defines the number of principal components for View - Spectrogram - Principal component analysis for spectrograms.\n\u2022 Show color bar: shows the mapping between numbers and the color code for some visualizations as e.g. spectrograms.\n\u2022 Show phase response: shows the phase response in addition to the amplitudes of a spectrogram.\n\u2022 Function for color bar: changes the color code for the mapping of numbers (e.g. for spectrograms). It is useful especially for a better resolution for small values of amplitudes. Here, the roots square or the inverse exponential function increase the resolution for small amplitude values. The parameters Control element:"}, {"heading": "7.9. Control elements for \u2019View: Spectrogram, FFT, CCF\u2019 99", "text": "View: Spectrogram, FFT, CCF - Exponent for exponential function resp. Control element: View: Spectrogram, FFT, CCF - Index of the root control the quantitative characteristic.\n\u2022 Exponent for exponential function: see Control element: View: Spectrogram, FFT, CCF - Function for color bar (exponent for exponential characteristic). A larger value increases the resolution for smaller amplitudes.\n\u2022 Index of the root: see Control element: View: Spectrogram, FFT, CCF - Function for color bar (n-fold root). A larger value increases the resolution for smaller amplitudes.\n\u2022 Colormap: changes the color style for some functions (e.g. for spectrograms). The colormaps are explained in the Matlab documentation (e.g. for Jet: help jet). The colormap 1 - Gray values inverts the gray map: 0 is now white and 1 black.\n\u2022 Limits for color axis: defines user-defined limits for some visualizations (e.g. for spectrograms). Larger and lower values are limited to the minimal resp. maximal value. All values between are scaled to identical values. This option is useful for an identical color code in different figures with different minimal or maximal values. The option is switched off by Control element: View: Spectrogram, FFT, CCF - Automatic (esp. for root and exponential).\n\u2022 Automatic (esp. for root and exponential): uses an automatic scaling (minimum to maximum value) to compute the colormap. Otherwise, the values from Control element: View: Spectrogram, FFT, CCF - Limits for color axis are used as limits.\n\u2022 Number of dominant frequencies for visualization: defines the number of shown frequencies in a frequency list file (*.freq).\n\u2022 Plot FFT vs. period length (instead of frequency): plot the FFT results vs. the period length. If the option is deactivated, the results are shown vs. frequency.\n100 Chapter 7. Control elements"}, {"heading": "7.10 Control elements for \u2019View: Classification and regression\u2019", "text": "\u2022 Display classes for output variables: switches the visualization of classification results with View - Classification - Result and similar functions:\n\u201donly learning data:\u201d shows the true class assignments for the learning data set.\n\u201donly classification:\u201d shows the classifier decisions.\n\u201dDS-No misclassification:\u201d shows the true class assignments for the learning data set with the data point number for misclassified data points.\n\u201dClass-No misclassification:\u201d shows the true class assignments for the learning data set with the number of the class decision for misclassified data points.\n\u2022 Show discriminant functions: plots the discrimination functions for the class borders using the recent classifier in the feature space.\n\u2022 Number of grid points: defines the number of grid points (in x- and y-direction) for the computation of regression functions or discriminant functions for class borders. The function will be applied to each grid point, resulting in (number of grid points)*(number of grid points) computations for two-dimensional problems.\n7.10. Control elements for \u2019View: Classification and regression\u2019 101\n\u2022 Different output variable: shows the results of classification and regression with a free selectable output variable if \u201dDifferent output variable\u201d was chosen in Control element: View: Classification and regression - Display classes for output variables.\n\u2022 Show outlier detection: contains the options for the outlier detection.\n\u201dNone:\u201d computes the result, but does not show any visualization.\n\u201donly result:\u201d shows the classification of the selected data points as outliers or normal data points.\n\u201dResult with contour plot:\u201d shows the classification of the selected data points as outliers or normal data points. In addition, lines with the values of the decision functions are plotted. The values of the feature space are expanded. The corresponding parameter can be changed in Control element: View: Classification and regression - Number of grid points.\n\u201donly discriminant function:\u201d shows only the class borders in form of discriminant functions.\n\u2022 Regression visualization: Normalize and aggregate input features: show the results of the estimated output variable of the regression model in View - Regression - Input variable(s), output variable, and regression function (2D resp. 3D) as a function of normalized and aggregated single features if such features were chosen in the design of the regression, see parameters in Control elements: Data mining: Regression. If not, the option does not influence the figure. If the option is switched off, the estimated output variable is shown as function of the original single features without normalization and aggregation. For more than two single features, View - Regression - GUI for multidimensional visualization has to be used.\n102 Chapter 7. Control elements"}, {"heading": "7.11 Control elements for \u2019Data mining: Classification of single features\u2019", "text": "contains the most important elements for the parameterization of data mining methods. The parameterization of the chosen classifier is done using Control elements: Data mining: Special methods.\n\u2022 Selection of output variable: selects an output variable. This selection influences many functions as the evaluation of single features and time series, the design of classifiers, almost all visualization functions etc.\n\u2022 Selection of single features: defines the method for the feature selection (e.g. use the recent selected features, automatic selection by ANOVA), see Data mining - Selection and evaluation of single features.\n\u2022 Number of selected features: defines the maximum number of single features for an automatic feature selection.\n\u2022 Normalization of single features: performs a normalization of features before a classification. It improves the results for all methods which are sensitive against different scaled features. Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.\n7.11. Control elements for \u2019Data mining: Classification of single features\u2019 103\n\u2022 Preselection of features: preselects all manually selected features (by Control element: Single features - Selection of single feature(s) (SF)) for an automatic feature selection.\nExample:\nFeatures x5 and x7 are manually selected, selection of four features by ANOVA, best features are x8, x9, x10, x13 leads to the selection of x5, x7, x8, x9.\n\u2022 Downgrading of correlated single features: downgrades a feature to a relevance of zero if it correlates significantly to at least one better feature. This significance is measured by a positive (linear) Pearson correlation coefficient with a value larger than a threshold define by the Control element: Data mining: Statistical options - Threshold for correlation coefficient). The aim of this function is the generation of a short list with different features. Otherwise, some very similar features might dominate at the first places of the list.\n\u2022 Feature aggregation: defines the method for the linear feature aggregation by means of a multiplication with a weighting matrix. Possible options are \u201dNo aggregation\u201d, a (linear) Discriminant Analysis, a Discriminant Analysis followed by a numerical optimization (\u201dDA with optimization\u201d, see Control element: Data mining: Classification of single features - Criterion for optimized DA for the optimization criterion), a \u201dPrincipal Component Analysis (PCA)\u201d, an \u201dIndependent Component Analysis (ICA)\u201d or the sum resp. mean value.\n\u2022 Number of aggregated features: defines the number of aggregated features (sd) in a classification (see e.g. [87]). The value is ignored if \u201dNo aggregation\u201d is chosen in Control element: Data mining: Classification of single features - Feature aggregation. A value of sd = 1 is always used for \u201dSum\u201d and \u201dMean value\u201d.\n\u2022 Criterion for optimized DA: specifies the criterion for a Discriminant Analysis with (numerical) optimization (see also Control element: Data mining: Classification of single features - Feature aggregation). Available options are an a maximization of the classification accuracy (\u201dbest_class\u201d) or a maximization of the minimal distance between two classes using class-specific covariance matrixes as metric (\u201dbest_ldf\u201d) [87].\n\u2022 Normalization of aggregated features: performs a normalization of the aggregated features before a classification. It improves the results for all methods which are sensitive against different scaled features. Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.\n\u2022 Chosen classifier: defines the type of the classifier. Possible options are a \u201dBayes classifier\u201d, an \u201dArtificial Neural Network\u201d, a \u201dSupport vector machine\u201d, a \u201dk-nearest neighbor\u201d classifier in two different implementation options, a \u201dFuzzy classifier\u201d, a \u201dDecision tree\u201d and Ensemble learning (fitensemble). Due to incompatibilities, the parameter of Control element: Data mining: Classification of single features - Multi-class problems might be modified as well. Here, an additional warning is shown in the Matlab command window.\nThe selected classifier type is parameterized in more detail by Control elements: Data mining: Special methods.\n104 Chapter 7. Control elements\n\u2022 Multi-class problems: selects the approach for the optional decomposition of problems with more than two classes of the output variable:\n1. Pure multi-class problems solve the problem without a decomposition.\n2. The one-against-all method decomposes a problem with C classes in C two-class problems with class c vs. the union of all remaining classes.\n3. The one-against-one method decomposes a problem with C classes in all pairwise problems with class c vs. class j (j 6= c). For the cases 2 and 3, a fusion of the single results is automatically done.\n\u2022 Save confusion matrix in file: saves the confusion matrix for the training data set of the recent classifier in a file, if the option is activated.\n\u2022 Graphical evaluation of classification results: opens a figure with classification results in the feature space of the classifier (if the option was activated).\n7.12. Control elements for \u2019Data mining: Classification of time series\u2019 105"}, {"heading": "7.12 Control elements for \u2019Data mining: Classification of time series\u2019", "text": "\u2022 Trigger time series: selects the trigger time series out of the existing time series. It is zero until the trigger event, contains increasing values to measure the number of samples since the trigger and zeros after the end of the recent event. If no trigger time series was selected, the sample numbers of the complete time series are used: It starts with a value of one for the first sample point and ends up with the length of the time series.\n\u2022 Classifier type: selects a classifier type for the time series:\nK1 contains only one classifier and ignores the trigger events. K5 is similar, but it uses the time since the trigger event as additional input variable. K2 is a special kind of single feature classifier for all sample points and aggregates results via a discriminant analysis. K3 and K4 have separate sub-classifiers for each sample point since the trigger event. K3 has a fix selection of time series, K4 a variable one for each sample point. TSK-Fuzzy is similar to K4, but it aggregates similar sample points by means of fuzzy sets.\n[22, 73] give a detailed description of the classifiers.\nThe feature selection and the underlying classifiers (e.g. Bayes classifiers) are parameterized using Control elements: Data mining: Classification of single features and Control element: Data mining: Special methods - Method.\n106 Chapter 7. Control elements\n\u2022 Time-weighting of time series relevances: gives the opportunity to prefer an earlier classification, e.g. for the feature selection of the K1 and K3 classifier. The method modifies the feature relevances by\nRk = R\nk \u2212 ktrig , for k > (ktrig + 0.15 \u00b7 ktrig)\nktrig is the sample point of the last trigger event. It increases feature relevances near the trigger event in contrast to later ones.\n\u2022 Filtering of results: select the method for filtering. The following options are available:\n\u201dNo\u201d filter is used..\n\u201dIIR-Filter\u201d uses the filter parameters in Control element: Data mining: Classification of time series - Parameter for IIR filter.\n\u201dClassification error for training data\u201d prefers samples with a better classification accuracy in the training data set in contrast to samples with a lower accuracy. This results in time-variant filter constants.\nThe \u201dMost frequent decision in a window\u201d selects the estimated class with the highest frequency in a sliding window. The window length is defined in Control element: Data mining: Classification of time series - Temporal aggregation of features. The first sample points with a sample number up to the window length use all sample points up to this time for decision.\n\u2022 Results for filtering: switches between absolute and relative values as input values for the fusion by filtering. As an example, absolute values of a Bayes classifier are the estimated a-posteriori probabilities, relative values the percentage of the estimated a-posteriori probabilities for the given class relative to all classes.\n\u2022 Parameter for IIR filter: set the parameter for the IIR filtering for the temporal fusion of classification results in time series classification (see Control element: Data mining: Classification of time series - Filtering of results).\n\u2022 Temporal aggregation of features: tunes the temporal aggregation of sample points before the classification. The number and the names of time series remain unchanged.\nA selection of \u201dNo\u201d does not use a temporal aggregation.\nThe \u201dMean value\u201d is computed in a sliding window defined by Control element: Data mining: Classification of time series - Temporal aggregation: Window length and Control element: Data mining: Classification of time series - Temporal aggregation: Step width.\n\u201dMinimum\u201d, \u201dMaximum\u201d and \u201dROM\u201d (Range of Motion) work with the same window, but with different operators.\nThe aggregation is always causal, i.e. only past sample points are aggregated. The first window does not start before the trigger event.\n\u2022 Temporal aggregation: Window length: defines the length of the sliding window for the temporal aggregation of features (see also Control\n7.12. Control elements for \u2019Data mining: Classification of time series\u2019 107\nelement: Data mining: Classification of time series - Temporal aggregation of features). The aggregation is always causal, i.e. only past sample points are aggregated. The first window does not start before the trigger event. Consequently, the first window terminates at \u201dTrigger event + Window length - 1\u201d.\n\u2022 Temporal aggregation: Step width: step width for the distance between two sliding windows. If all sample points should be used, a value of \u201d1\u201d must be chosen.\n\u2022 K5: use each x. sample point: The classifier K5 handles a training data set of a classification problem for time series as a samplewise classification problem with the time since the trigger event as an additional feature. As a consequence, it has a high memory consumption because all sample points are handled as separate data points. It might cause problems up to Matlab crashes especially for Support Vector Machines (SVM). The option reduces the number of sample points for the training of a K5 classifier.\n108 Chapter 7. Control elements\n\u2022 Trigger time series: selects the trigger time series out of the existing time series. It is zero until the trigger event, contains increasing values to measure the number of samples since the trigger and zeros after the end of the recent event. If no trigger time series was selected, the sample numbers of the complete time series are used: It starts with a value of one for the first sample point and ends up with the length of the time series.\n\u2022 Classifier type: selects a classifier type for the time series:\nK1 contains only one classifier and ignores the trigger events. K5 is similar, but it uses the time since the trigger event as additional input variable. K2 is a special kind of single feature classifier for all sample points and aggregates results via a discriminant analysis. K3 and K4 have separate sub-classifiers for each sample point since the trigger event. K3 has a fix selection of time series, K4 a variable one for each sample point. TSK-Fuzzy is similar to K4, but it aggregates similar sample points by means of fuzzy sets.\n[22, 73] give a detailed description of the classifiers.\nThe feature selection and the underlying classifiers (e.g. Bayes classifiers) are parameterized using Control elements: Data mining: Classification of single features and Control element: Data mining: Special methods - Method.\n\u2022 Time-weighting of time series relevances: gives the opportunity to prefer an earlier classification, e.g. for the feature selection of the K1 and\n7.12. Control elements for \u2019Data mining: Classification of time series\u2019 109\nK3 classifier. The method modifies the feature relevances by\nRk = R\nk \u2212 ktrig , for k > (ktrig + 0.15 \u00b7 ktrig)\nktrig is the sample point of the last trigger event. It increases feature relevances near the trigger event in contrast to later ones.\n\u2022 Filtering of results: select the method for filtering. The following options are available:\n\u201dNo\u201d filter is used..\n\u201dIIR-Filter\u201d uses the filter parameters in Control element: Data mining: Classification of time series - Parameter for IIR filter.\n\u201dClassification error for training data\u201d prefers samples with a better classification accuracy in the training data set in contrast to samples with a lower accuracy. This results in time-variant filter constants.\nThe \u201dMost frequent decision in a window\u201d selects the estimated class with the highest frequency in a sliding window. The window length is defined in Control element: Data mining: Classification of time series - Temporal aggregation of features. The first sample points with a sample number up to the window length use all sample points up to this time for decision.\n\u2022 Results for filtering: switches between absolute and relative values as input values for the fusion by filtering. As an example, absolute values of a Bayes classifier are the estimated a-posteriori probabilities, relative values the percentage of the estimated a-posteriori probabilities for the given class relative to all classes.\n\u2022 Parameter for IIR filter: set the parameter for the IIR filtering for the temporal fusion of classification results in time series classification (see Control element: Data mining: Classification of time series - Filtering of results).\n\u2022 Temporal aggregation of features: tunes the temporal aggregation of sample points before the classification. The number and the names of time series remain unchanged.\nA selection of \u201dNo\u201d does not use a temporal aggregation.\nThe \u201dMean value\u201d is computed in a sliding window defined by Control element: Data mining: Classification of time series - Temporal aggregation: Window length and Control element: Data mining: Classification of time series - Temporal aggregation: Step width.\n\u201dMinimum\u201d, \u201dMaximum\u201d and \u201dROM\u201d (Range of Motion) work with the same window, but with different operators.\nThe aggregation is always causal, i.e. only past sample points are aggregated. The first window does not start before the trigger event.\n\u2022 Temporal aggregation: Window length: defines the length of the sliding window for the temporal aggregation of features (see also Control element: Data mining: Classification of time series - Temporal aggregation of features). The aggregation is always causal, i.e. only past sample points are aggregated. The first window does not start before the trigger event. Consequently, the first window terminates at \u201dTrigger event + Window length - 1\u201d.\n110 Chapter 7. Control elements\n\u2022 Temporal aggregation: Step width: step width for the distance between two sliding windows. If all sample points should be used, a value of \u201d1\u201d must be chosen.\n\u2022 Number of clusters for TSK-Fuzzy: The TSK fuzzy classifier computes clusters to determine features with similar feature relevances vs. time. The parameter sets the number of clusters to be found. An interval value (e.g. 2:5) computes a set of clusters for each candidate number of clusters. The number with the first local minimum of the separation index will be selected.\n\u2022 Show cluster memberships: shows the result of the clustering by the search for time regions. It finds similar feature relevances for the time series classifier.\n\u2022 Threshold for fuzzy cluster: contains a threshold used for deletion of clusters representing very short regions. It represents the maximal value of the membership function. A large value simplifies the resulting classifier by the reduction of the number of linguistic terms for the TSK fuzzy classifier. A too low value might lead to an oversimplification.\n7.13. Control elements for \u2019Data mining: Regression\u2019 111"}, {"heading": "7.13 Control elements for \u2019Data mining: Regression\u2019", "text": "\u2022 Feature type (input): switches between a regression model for single features with the model y = f(x1, ..., xs) and for time series with the model y[k] = f(x1[k \u2212 i], ..., x1[k \u2212 j], ..., xs[k \u2212 i], ..., xs[k \u2212 j]). The selection of input variables xi controls Control element: Data mining: Regression - Feature selection, the selection of output variables is done by Control element: Data mining: Regression - Output variable of regression. For time series, the sample points i, ..., j for the time shift are selected by Control element: Data mining: Regression - Sample points.\nThe selection of the related model is only possible, if single features resp. time series exist in the project.\n\u2022 Feature selection: selects the input variables of the regression model. Possible selections are \u201dAll features\u201d, \u201dSelected features\u201d (in Control element: Single features - Selection of single feature(s) (SF) or Control element: Time series: General options - Selection of time series (TS)) and automatically by univariate resp. multivariate regression coefficients. To set the number of automatically selected features, use Control element: Data mining: Regression - Number of selected features.\n\u2022 Number of selected features: defines the number of selected features by an automatic feature selection. For time series, each\n112 Chapter 7. Control elements\ncombination of time series and sample points is handled as separate feature. As an example, x1[k \u2212 1] and x1[k \u2212 2] are two features. The value will be ignored if \u201dAll features\u201d or \u201dSelected features\u201d are chosen in Control element: Data mining: Regression - Feature selection.\n\u2022 Preselection of features: preselects all manually selected features (by Control element: Single features - Selection of single feature(s) (SF)) for an automatic feature selection for regression of single features.\n\u2022 Normalization: performs a normalization of features before a regression. It improves the results especially for Artificial Neural Networks. Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.\n\u2022 Feature aggregation: defines the method for the linear feature aggregation by means of a multiplication with a weighting matrix. Possible options are \u201dNo aggregation\u201d, a \u201dPrincipal Component Analysis\u201d or the sum resp. mean value.\n\u2022 Number of aggregated features: defines the number of aggregated features (sd) in a regression (see e.g. [87]). The value is ignored if \u201dNo aggregation\u201d is chosen in Control element: Data mining: Regression - Feature aggregation. A value of sd = 1 is always used for \u201dSum\u201d and \u201dMean value\u201d.\n\u2022 Normalization of aggregated features: performs a normalization of aggregated features before a regression. It improves the results especially for Artificial Neural Networks. Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.\n\u2022 Output variable of regression: selects a single feature or a time series at sample point [k] as output of the regression model. Control element: Data mining: Regression - Feature type (input) switches between single features and time series.\n\u2022 Type: selects the method for the regression model. The methods are parameterized in detail using Control elements: Data mining: Special methods.\n\u2022 Sample points: defines the time shift for the regression of time series. The values must not be positive avoiding acausal models. If a time series is input and output variable, the value k = 0 is deleted for this time series.\nExample:\nThe values \u201d0 -1 -24\u201d with the time series x1, x2 as input variables and the time series x1 as output variable are chosen. The resulting model is x1[k] = f(x1[k\u22121], x1[k\u221224], x2[k], x2[k\u22121], x2[k\u2212 24]).\n7.14. Control elements for \u2019Data mining: Clustering\u2019 113"}, {"heading": "7.14 Control elements for \u2019Data mining: Clustering\u2019", "text": "\u2022 Feature classes: switches between a clustering of the selected time series (using the defined segment), the selected single features, or a co-clustering of time series and single features.\n\u2022 Number of clusters: defines the number of clusters to be computed by the cluster algorithms. A multiple selection (e.g. 2:8) is possible. In this case, the number is defined according to the first local minimum of the separation index.\n\u2022 Iteration steps: defines the maximal number of iteration steps of the cluster algorithm, if Control element: Data mining: Clustering - Unlimited iteration steps is deactivated. If the algorithms converge before this step, the algorithm is also terminated.\n\u2022 Unlimited iteration steps: executes the cluster algorithms until the internally defined convergence criterion was reached.\n\u2022 Distance measure: selects the metric for the distance between each data point and the cluster prototype.\n\u2022 Distance measure for the noise cluster: defines the method for the definition of a noise cluster [27] for a Fuzzy C-Means method. The\n114 Chapter 7. Control elements\noption \u201dNone\u201d does not generate a noise cluster. Alternatively, a new cluster is defined to which all data points have the same distance. Outliers are mainly assigned to this noise cluster. As a consequence, the influence of outliers to the other clusters is reduced. Possible distance measures are the mean distance of all data points to all other clusters [27] or the median of these distances. For the latter option, the distance is scaled by Control element: Data mining: Clustering - Factor for the distance to the noise cluster.\n\u2022 Factor for the distance to the noise cluster: defines a positive scaling factor for the distance to a noise cluster. A large value reduces the assignment of data points to the noise cluster (see also Control element: Data mining: Clustering - Distance measure for the noise cluster).\n\u2022 Fuzzifier for clustering: defines the fuzzifier in the fitness function of the Fuzzy C-Means method. The value should be at least 1.1 for numerical reasons. A higher value generates increases the fuzziness of the cluster membership values. For a value of 1, a crisp k-means method is used.\n\u2022 Compute start prototypes for clusters: selects the algorithms to compute start prototypes of the clusters. The possible options are \u201dequally distributed\u201d clusters in the input space, \u201drandom positions\u201d of start prototypes, or \u201drandom data points\u201d selected as start prototypes.\n\u2022 Append cluster as output variable: adds the results of the cluster algorithms (the found crisp cluster memberships) to the project in form of a new output variable.\n\u2022 Video: shows the iterations of the cluster algorithms as video.\n\u2022 Plot original TS: shows the existing time series during the visualization of the cluster iterations. Possible options are \u201dall time series\u201d, \u201dmean values\u201d, and \u201dnone\u201d.\n\u2022 Fusion algorithm (Statistic Toolbox only): select the fusion algorithm for the hierarchical cluster algorithm of the Statistic Toolbox of MATLAB (see documentation of this toolbox).\n7.15. Control elements for \u2019Data mining: Special methods\u2019 115"}, {"heading": "7.15 Control elements for \u2019Data mining: Special methods\u2019", "text": "\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Metrics of Bayes classifier: defines the metric for the Bayes classifier.\n\u2022 Use a priori probabilities: switches the use of estimated a priori probabilities of the different classes for the Bayes classifier on resp. off. The estimation uses the training data set.\n116 Chapter 7. Control elements\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Artificial Neural Network: Type: sets the type of the Artificial Neural Network. The available options include a Multi Layer Perceptron Net (MLP) and a Radial Base Function Net (RBF). The nets are used from the Neural Network Toolbox of Matlab.\n\u2022 MLP: number of output neurons: defines the number of output neurons for MLP nets. Possible options are \u201done neuron\u201d (values of the output variable: 1 to maximum class number) or \u201done neuron per class\u201d (values of the output variable: 0 ...1 for the related class membership). The latter case generates a higher number of parameters in the net, but gives often better results. A main reason is a better fit for more complex class topologies in the input space.\nFor regression models, only one output neuron will be used.\n\u2022 Number of neurons per layer: defines the number of neurons for the hidden layer.\n\u2022 MLP: learning algorithm: defines the learning algorithms for the MLP net (see documentation of the Neural Network Toolbox).\n7.15. Control elements for \u2019Data mining: Special methods\u2019 117\n\u2022 MLP: neuron type hidden layer: defines the type of neurons in the input layer of a MLP net (see documentation of the Neural Network Toolbox).\n\u2022 MLP: input weighting: describes the method to combine the input of a neuron with the weights. The option \u201ddotprod\u201d uses a multiplication.\n\u2022 MLP: input function: describes the method to combine the weighted inputs of a neuron. The standard option \u201dnetsum\u201d adds all weighted inputs.\n\u2022 Error goal: defines the error goal for the neural net. If the goal is reached, the design process will be terminated.\n\u2022 Number of learning epochs: defines the maximum number of learning epochs for MLP nets (see documentation of the Neural Network Toolbox).\n\u2022 MLP: Visualization step width: defines the number of iterations between the visualizations of the MLP net. The value is only relevant if Control element: Data mining: Special methods - MLP: plot is activated.\nThe design process of a neural net is restarted after each visualization with the last net as starting parameter. However, some internal strategy parameter for the design might be lost for some learning algorithms. Larger values mostly result in lower errors and faster convergence.\n\u2022 MLP: plot: switches plots for the recent learning progress of MLP nets on or off (see documentation of the Neural Network Toolbox).\n\u2022 Show training GUI: switches the use of the Learning GUI window of the Neural Network Toolbox on or off.\n118 Chapter 7. Control elements\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Artificial Neural Network: Type: sets the type of the Artificial Neural Network. The available options include a Multi Layer Perceptron Net (MLP) and a Radial Base Function Net (RBF). The nets are used from the Neural Network Toolbox of Matlab.\n\u2022 Number of neurons per layer: defines the number of neurons for the hidden layer.\n\u2022 Error goal: defines the error goal for the neural net. If the goal is reached, the design process will be terminated.\n\u2022 RBF: Spread: defines the spread of the Radial Base Function. A larger spread smoothes the function approximation. Too large spreads can cause numerical problems.\n7.15. Control elements for \u2019Data mining: Special methods\u2019 119\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Artificial Neural Network: Type: sets the type of the Artificial Neural Network. The available options include a Multi Layer Perceptron Net (MLP) and a Radial Base Function Net (RBF). The nets are used from the Neural Network Toolbox of Matlab.\n\u2022 Number of layers: defines the number of hidden layers of feedforward nets.\n\u2022 Number of neurons per layer: defines the number of neurons for the hidden layer.\n\u2022 MLP: learning algorithm: defines the learning algorithms for the MLP net (see documentation of the Neural Network Toolbox).\n\u2022 Error goal: defines the error goal for the neural net. If the goal is reached, the design process will be terminated.\n120 Chapter 7. Control elements\n\u2022 Number of learning epochs: defines the maximum number of learning epochs for MLP nets (see documentation of the Neural Network Toolbox).\n\u2022 MLP: Visualization step width: defines the number of iterations between the visualizations of the MLP net. The value is only relevant if Control element: Data mining: Special methods - MLP: plot is activated.\nThe design process of a neural net is restarted after each visualization with the last net as starting parameter. However, some internal strategy parameter for the design might be lost for some learning algorithms. Larger values mostly result in lower errors and faster convergence.\n\u2022 MLP: plot: switches plots for the recent learning progress of MLP nets on or off (see documentation of the Neural Network Toolbox).\n\u2022 Show training GUI: switches the use of the Learning GUI window of the Neural Network Toolbox on or off.\n7.15. Control elements for \u2019Data mining: Special methods\u2019 121\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Artificial Neural Network: Type: sets the type of the Artificial Neural Network. The available options include a Multi Layer Perceptron Net (MLP) and a Radial Base Function Net (RBF). The nets are used from the Neural Network Toolbox of Matlab.\n\u2022 Number of neurons: defines the number of neurons per dimension for the design of Self Organizing Maps with Data mining - Self Organizing Maps - Design. The number is used for all dimensions, e.g., a value of 8 with a dimensionality of 2 means the design of a net with 8x8 neurons.\n\u2022 Dimension: defines the dimensionality for the design of Self Organizing Maps with Data mining - Self Organizing Maps - Design.\n\u2022 Number of learning epochs: defines the maximum number of learning epochs for MLP nets (see documentation of the Neural Network Toolbox).\n\u2022 Show training GUI: switches the use of the Learning GUI window of the Neural Network Toolbox on or off.\n122 Chapter 7. Control elements\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Use SVM-internal one-against-one or one-against-all: uses the internal decomposition of multi-class problems of the SVM toolbox. The function is faster and uses less memory compared to the external decomposition of SciXMiner. This external decomposition works with the one-against-one resp. the one-against-all option in Control element: Data mining: Classification of single features - Multi-class problems.\n\u2022 Kernel: defines the used kernel function for Support Vector Machines [18]. Homogenous polynomial kernels (without bias term), polynomial kernels (with bias term), and Gaussian radial base functions are implemented. The order is tuned by Control element: Data preprocessing - Kernel order. A first-order polynomial kernel means a linear hyperplane.\n\u2022 Kernel order: set the order of the kernel of the SVM. It is used for the classification with the SVM and by the SVM-based outlier detection.\n\u2022 Penalty term C: tunes the compromise between a small number of classification errors and the simplification of the hyperplanes to discriminate classes for the SVM. Larger values of C lead to a higher number of classification errors to simplify the hyperplanes.\n7.15. Control elements for \u2019Data mining: Special methods\u2019 123\n\u2022 Epsilon: defines the width of the insensitive region of the loss function in Support Vector Regression.\n124 Chapter 7. Control elements\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 k: defines the number of the k neighbors for the k-nearest neighbor classifier.\n\u2022 Metrics of k-NN: defines the metric to compute the nearest neighbors.\n\u2022 Distance weighting: enables a weighting of different distances.\nIf \u201dNone\u201d is selected, all neighbors contribute equally to the decision (standard option).\n\u201dInverse linear\u201d prefers the nearest neighbor by a weighting with 1. The farthest of the k neighbors is weighted with 0. All neighbors between have an equidistant weighting, e.g. 0.25, 0.5 and 0.75 for k = 4.\n\u201dInverse distance\u201d uses 1/Distance as weight to prefer neighbors with smaller distances. A neighbor with distance 0 gets a maximum weight of 1/1E \u2212 10. \u201dInverse exponential\u201d is similar but with more equal weights by using e\u2212d with the distance d.\n\u2022 Region size: specifies the type of environment around a data point. If the option Control element: Data min-\n7.15. Control elements for \u2019Data mining: Special methods\u2019 125\ning: Special methods - Use all neighbors in a region was activated, all neighbors in the defined environment are included as neighbors (and not necessary only the k nearest).\n\u2022 Use all neighbors in a region: includes all neighbors in an environment defined by Control element: Data mining: Special methods - Region size. The classifier normalizes all values to an interval [0, . . . , 1]. Consequently, the value should be smaller than 1!\nIf less than k neighbors are localized in the environment, only the k nearest neighbors are used.\n\u2022 Minimum neighbor number in max. distance: defines the minimum number of neighbors in a maximum accepted distance defined by Control element: Data preprocessing - Max. distance to make a decision.\n\u2022 Evaluate minimum number of neighbors: defines if a minimum number of neighbors in an environment (defined by a distance) should be used. If the number is not reached and the option is activated, the decision is set to rejection. The distance is tuned by Control element: Data preprocessing - Max. distance, the number by Control element: Data mining: Special methods - Minimum neighbor number in max. distance.\n\u2022 Max. distance: defines the accepted maximum distance for a neighborhood (see Control element: Data mining: Special methods - Evaluate minimum number of neighbors).\n126 Chapter 7. Control elements\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 k: defines the number of the k neighbors for the k-nearest neighbor classifier.\n7.15. Control elements for \u2019Data mining: Special methods\u2019 127\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Type of membership function: defines the algorithm for the design of a membership function (see Control element: Data mining: Special methods - Number of linguistic terms).\n\u201dMedian\u201d generates terms with approximately equal data point frequencies for all terms in the training data set.\n\u201dEqual distribution\u201d generate terms with similar distances between the maximum values of the membership functions.\n\u201dClustering\u201d uses cluster prototypes as parameters.\n\u201dFix\u201d set the parameters of the membership functions to the values defined by Control element: Data mining: Special methods - Parameters MBF (fix).\nThe choice \u201dwith interpretability\u201d rounds the found parameters using a method described in [76] to improve the interpretability.\nRemark:\nAll design methods set the random number generator of Matlab to a fix value. It improves the reproducibility of found membership functions. Nevertheless, all methods with random elements might be sensitive to the state of the random number generator (e.g. cluster algorithms with random\n128 Chapter 7. Control elements\nstart clusters). It can also influence other methods using these membership functions, e.g. by different selected features in a decision tree (e.g. Data mining - Selection and evaluation of single features - Information theoretic measures).\n\u2022 Number of linguistic terms: defines the desired number of linguistic terms for the design of the membership functions. The designed membership functions are standard partitions: The first and the last term have trapezoidal membership functions, middle terms triangular membership functions. The membership functions sum up to one for each value of the variable.\nThe term number might be automatically reduced by the designed method, e.g. if only two different values exist and three terms should be designed.\n\u2022 Parameters MBF (fix): defines the parameters of the membership functions if \u201dfix\u201d was chosen in Control element: Data mining: Special methods - Type of membership function.\n\u2022 Type of decision tree: defines the splitting criterion for the design of the decision tree. The type ID3 uses a maximized mutual information. The type C4.5 maximizes the mutual information divided by the input entropy. In addition, a statistical correction of the entropy can be chosen. It reduces the influence of an overestimation of feature relevances for small training data sets and avoids overfitted trees.\nAll trees use the design fuzzy membership functions instead of computed binary splits for each node.\n\u2022 One-against-all-trees: uses C different decision trees for the rule generation. The trees are generated by a one-against-all approach of a class against all other classes.\n\u2022 Exponent for clearness: defines the evaluation method for the pruning of fuzzy rules [76]. An increasing clearness prefers highly specialized error-free rules to more generalized rules with some errors.\n\u2022 Inference: defines the inference method for the fuzzy system. The MIN-MAX inference uses the minimum operator for all conjunctions (e.g. for the aggregation) and the maximum for all disjunctions (e.g. for the accumulation of all rules with identical conclusions. The SUM-PROD inference uses the bounded sum for all disjunctions and the product for all conjunctions. An additional correction of overlapping rules [75] was implemented for artifacts by rules with the same or concurrent conclusions in case of the SUM-PROD inference.\n\u2022 Number of rules for a rulebase: defines the maximal number of selected rules in a fuzzy rulebase.\n\u2022 Looking for cooperating rules from rulebase: switches the search for cooperating rule bases on or off. If the option is off, all found single rules are used for classification or regression.\n\u2022 Significance level: influences the statistical evaluation of fuzzy rules. Values towards 100 % tends to the generation of only few, but very confident rules.\n7.15. Control elements for \u2019Data mining: Special methods\u2019 129\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Type of membership function: defines the algorithm for the design of a membership function (see Control element: Data mining: Special methods - Number of linguistic terms).\n\u201dMedian\u201d generates terms with approximately equal data point frequencies for all terms in the training data set.\n\u201dEqual distribution\u201d generate terms with similar distances between the maximum values of the membership functions.\n\u201dClustering\u201d uses cluster prototypes as parameters.\n\u201dFix\u201d set the parameters of the membership functions to the values defined by Control element: Data mining: Special methods - Parameters MBF (fix).\nThe choice \u201dwith interpretability\u201d rounds the found parameters using a method described in [76] to improve the interpretability.\nRemark:\nAll design methods set the random number generator of Matlab to a fix value. It improves the reproducibility of found membership functions. Nevertheless, all methods with random elements might be sensitive to the state of the random number generator (e.g. cluster algorithms with random\n130 Chapter 7. Control elements\nstart clusters). It can also influence other methods using these membership functions, e.g. by different selected features in a decision tree (e.g. Data mining - Selection and evaluation of single features - Information theoretic measures).\n\u2022 Number of linguistic terms: defines the desired number of linguistic terms for the design of the membership functions. The designed membership functions are standard partitions: The first and the last term have trapezoidal membership functions, middle terms triangular membership functions. The membership functions sum up to one for each value of the variable.\nThe term number might be automatically reduced by the designed method, e.g. if only two different values exist and three terms should be designed.\n\u2022 Type of decision tree: defines the splitting criterion for the design of the decision tree. The type ID3 uses a maximized mutual information. The type C4.5 maximizes the mutual information divided by the input entropy. In addition, a statistical correction of the entropy can be chosen. It reduces the influence of an overestimation of feature relevances for small training data sets and avoids overfitted trees.\nAll trees use the design fuzzy membership functions instead of computed binary splits for each node.\n\u2022 Significance level: influences the statistical evaluation of fuzzy rules. Values towards 100 % tends to the generation of only few, but very confident rules.\n7.15. Control elements for \u2019Data mining: Special methods\u2019 131\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Polynomial degree: defines the maximal degree m of a polynomial function y = a0 + a1x+ ...+ amxm.\n\u2022 Set a0 to zero: forces a polynomial regression model to estimate a model without a constant term called a0.\n\u2022 Maximal number of internal features: enables an internal feature reduction during the computation of the polynomial model. It is useful especially for the selection of terms with the most relevant power coefficients.\n\u2022 Weight matrix: selects a weighting matrix for data points during a Least Square estimation. \u201dNone\u201d uses equal weights (1), \u201dinverse proportional to class frequency\u201d prefers data points from rare linguistic terms of the selected output variable.\n132 Chapter 7. Control elements\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Fit: specifies the method for the interpolation with the MATLAB function fit (part of the Curve Fitting Toolbox). For more details type \u201dhelp fit\u201d in the MATLAB command window. The following methods are implemented (in parentheses: number of input variables and specialties): weibull (1D, no negative inputs), exponential (1D), fourier (1D), gauss (1D), cubicinterp (1D or 2D, no interpolation), pchipinterp (1D, no interpolation), poly curve (1D or 2D), power (1D, no negative inputs), rational (1D), sinus (1D), cubicspline (1D, no interpolation), smoothingspline (1D), biharmonicinterpolation (2D), local linear regr (2D), local quadratic regr (2D).\n\u2022 Parameter 1: select context-sensitive the first parameter for the selected method in Control element: Data mining: Special methods - Fit. This parameter is not used by every method.\n\u2022 Parameter 2: select context-sensitive the second parameter for the selected method in Control element: Data mining: Special methods - Fit. This parameter is not used by every method.\n7.15. Control elements for \u2019Data mining: Special methods\u2019 133\n\u2022 Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n\u2022 Ignore output variables with many terms: defines the number of linguistic terms to ignore output variables with many terms (e.g., names or IDs) in an association analysis.\n\u2022 Number of rules: defines the maximum number of rules for an association analysis.\n\u2022 Minimum support: defines the minimal support P(A AND B) of a rule IF A THEN B.\n\u2022 Minimum confidence: defines the minimal confidence P(B|A) of a rule IF A THEN B.\n\u2022 Sorting: defines the sorting order for the found rules by support or by confidence.\n134 Chapter 7. Control elements"}, {"heading": "7.16 Control elements for \u2019Data mining: Statistical options\u2019", "text": "\u2022 Type for correlation parameters: defines the type of correlation (Linear: Pearson; Rank-oriented: Kendall, Spearman) to be used in visualizations or output files for correlations.\n\u2022 Threshold for correlation coefficient: defines a threshold for the correlation coefficient. A larger value is interpreted as a relevant pairwise correlation between two features. This value influences e.g. the listing of relevant correlation coefficients or the downgrading of correlated features in the feature selection.\n\u2022 Show correlation only for one selected feature: computes only the correlations of all selected single features to the feature defined by Control element: Data mining: Regression - Output variable of regression.\n\u2022 p-value for t test: defines the p-value for the t-test (accepted error probability of a statistical test). Usual values are 0.01...0.05.\n\u2022 Show all: defines whether all tested or only significant different single features (measured by the t-test) for the output variable should be shown.\n7.16. Control elements for \u2019Data mining: Statistical options\u2019 135\n\u2022 Bonferroni correction: defines if statistics tests and correlations are computed with or without (\u201dno\u201d) corrections for multiple testings. A Bonferroni corrections divides the \u03b1 values in Control element: Data mining: Statistical options - p-value for t test by the number of tests. The less conservative BonferroniHolm correction divides \u03b1 by the number of test - the ranking of the tested values + 1. It accepts all successful tests before the first rejected test.\n\u2022 Test of normal distribution: selects a methods to test the normality of a distribution using Data mining - Selection and evaluation of single features - Test of normal distribution. Possible values are Chi Square Test, Lillie Test, Anderson Darling Test and Jarque Bera Test from the MATLAB Statistics Toolbox.\n\u2022 Show p values for correlation: defines the option for the reporting of p-values for correlation coefficients unequal zero in the correlation visualization (View - Single features - Correlation coefficients (Pearson) etc.).\n\u2022 Norm (Data point distances): select the used norm to compute data point distances.\n136 Chapter 7. Control elements"}, {"heading": "7.17 Control elements for \u2019Data mining: Validation\u2019", "text": "\u2022 n-fold cross-validation: defines the number n of parts for a n-fold cross-validation. As an example, a value of 10 means that the training data set is separated in 10 parts. For each run, 9 parts are used as training data and one part as validation data set. Consequently, a complete trial of a cross-validation requires 10 runs containing each selected data point exactly one times in the validation data set. Typical values are n = 2 . . . 10.\nThe value will be ignored for the selection of \u2019Leave-one-out\u2019 (n is equal to the number of selected data points) or \u2019Bootstrap\u2019 in Control element: Data mining: Validation - Validation strategy.\n\u2022 Number of trials: defines the number of trials for a cross-validation. Internally, a random assignment of data points to the different parts will be chosen. This assignment takes the output variables into account to get an approximately equal distribution of linguistic output terms for the different parts. As a consequence, the results of different trials will differ. A higher number of trials costs computing time, but improves the estimation for the classification accuracy. Typical values are 1...50 depending on the necessary computing time.\nThe value will be ignored for the selection of \u2019Leave-one-out\u2019 in Control element: Data mining: Validation - Validation strategy (always 1 due to the deterministic selection of training data).\n7.17. Control elements for \u2019Data mining: Validation\u2019 137\n\u2022 Validation strategy: switches the validation type between cross-validation, leave-one-out (N -fold cross-validation with the number N of selected data points and exactly one data point in each validation data set) and bootstrap method (training data set: random selection of data points with replacement resulting usually in a multiple selection of some data points, validation data set: all none selected data points) (see e.g. [29]).\n\u2022 Plot in file: controls if the validation results are written into a protocol file or not.\n\u2022 Crossvalidation (separated by classes): use the classes of the selected output variables to split the dataset. As a consequence, the data points belonging to a class are used as validation dataset, all other data points as training dataset. As usual for a crossvalidation, this split is rotated so that each selected data points is used exactly once as in the validation dataset.\n138 Chapter 7. Control elements"}, {"heading": "7.18 Control elements for \u2019General options\u2019", "text": "\u2022 TEX protocol: generates most generated protocol files in a LATEXformat. Otherwise, ASCII files are produced.\n\u2022 For tex tables: show tabularx: switches the tabularx style in the generation of Latex tables on or off.\n\u2022 Memory-optimized feature generation: modifies the algorithm for feature extraction from time series. If the option is chosen, the computing time for extraction increases and the risk of memory problems decreases.\n\u2022 Percentages 2D histogram: switches the generation of percentage information (per row and per column) in the file containing quantitative information of 2D histograms on or off.\n\u2022 Save options by saving the project: switches the saving of options (all GUI elements) on or off, if a project is saved.\n\u2022 Show tool tips: switches the context-sensitive help for control elements on or off, if the mouse arrow is over an element.\n7.18. Control elements for \u2019General options\u2019 139\n\u2022 Invisible figures in SciXMiner batch files: hides all figures during execution of a SciXMiner batch file. This option is useful for parallel working on the same computer without disturbances of opening figures. The SciXMiner main figure is also hidden. If a batch file stops by error, the hidden figures can be restored by the input of the command \u201drestore_figures\u201d in the MATLAB command window.\n\u2022 Detail plot of function interna: influences the level detail for some texts and figures with intermediate results (marked = more details).\n\u2022 Show menu tips: if activated, a short description of the clicked menu item is shown as part of the project overview (Control elements: Project overview).\n\u2022 Show short protocol: switches off the complete listing for all control elements in the protocol files. Here, only some project characteristics are written into the files.\n\u2022 For macros: plot always in current figure: plots all visualizations in the recent figure. This option is useful for the generation of user-defined subplots in macros. However, it requires a manual modification of the macro with figure and sub-figure commands. Otherwise, figures are overwritten without any warning.\n\u2022 Project names as part of the variable names: adds project names during the project fusion for new single features and time series to mark the source project of a single feature or time series.\n\u2022 For macros: stop if error: switches off the error handling with try/catch in macros. It allows a better backtracking of error messages.\n\u2022 Output variable: few terms first: shows in View - Classes for selected data points the output variables with only few terms first. This option is useful in projects with many linguistic terms and data points.\n\u2022 Open files in MATLAB editor: defines whether generated text files (e.g., for feature evaluation) will be immediately opened by the MATLAB editor or not.\n\u2022 Number format for result plots: defines the format of numbers for most visualization and protocol functions, especially the number of digits before and after the decimal point . \u201d%g\u201d is the automatically chosen standard Matlab format, \u201d%f\u201d means floating points. The parameter does not influence export functions.\n\u2022 Name conventions for feature generation: defines the name conventions for extracted features. Possible options are a combination of name fragments by blanks or underscores.\n\u2022 File type for images: defines the file type for the hardcopy of all open figures into files. The abbreviations are the same as for the MATLAB print command. The EPS format is subdivided into black-and-white images (eps) and color images (epsc).\n140 Chapter 7. Control elements\n\u2022 Manual selection of data points: sets the numbers of data points for the selection with Edit - Select - Data point from GUI.\n\u2022 Percent for selection: defines the percentage of selected data points by using Edit - Select - Random data points (defined percentage of selected data points).\n\u2022 Save mode for MATLAB file: defines the compatibility mode for binary MATLAB files. \u2019-V6\u2019 is compatible to the MATLAB versions 5 and 6, \u2019-V7.0\u2019 to version 7 and \u2019-V7.3\u2019 to version 7.3. For lower versions, only a reduced set of modes is available.\n\u2022 Load plugins at start: defines the modus of loading and initializing of plugins for feature extraction during loading a project: never, always, only if the project contains time series or images.\n\u2022 Order of linguistic terms: defines a new order for the linguistic terms of the selected output variable. The resorting is done by Edit - Convert - Resort linguistic terms (order from GUI).\n\u2022 Configuration name for MATLAB parallel: defines the name of the configuration for the MATLAB Parallel Computing Toolbox that can be started by Extras - Matlab Parallel - Start (see also Parallel - Manage Configurations in the menu of the MATLAB command window)."}, {"heading": "8 Feature extraction from time series", "text": ""}, {"heading": "8.1 Definition of feature types by plugins", "text": "The feature extraction of time series is realized with plugins. Please notice that SciXMiner was actually developed in German language, so some variable names are German. Unfortunately, it was impossible to change this inconvenience.\nPlugins are single Matlab functions named plugin_*.m, which are included in a special directory of the SciXMiner installation (...\\scixminer\\plugins\\mgenerierung) or in the directory which contains the loaded project. To work with SciXMiner properly, they have to abide by the following format:\nfunction [datenOut, ret, info] = pluginfct(paras, datenIn).\nThe return value info contains the following elements:\n\u2022 info.beschreibung: short description of the new feature with one or two words (e.g. minimum, maximum, velocity). Only one description is allowed, independent from the numbers of computed features. The description must not start with a hyphen (minus)!\n\u2022 info.bezeichner: short identifier of the new feature (e.g. MIN, MAX, V). The matrix has to contain one identifier per feature or time series (every row is a single identifier). Use str2mat(\u2019String\u2019,\u2019OtherString\u2019) to create a matrix with one identifier per row.\n\u2022 info.explanation: explanation of the plugin (approx. 1-2 sentences)\n\u2022 info.explanation_long: optionally an additional longer text, used only for documentation\n\u2022 info.anz_zr: numbers of time series extracted by the plugin (has to be 0, if info.anz_em 6= 0)\n\u2022 info.anz_benoetigt_zr: numbers of time series needed for the computation of the new time series. If > 1 the first anz_benoetigt_zr time series are always given to the plugin. The plugin have to check the correct amount of time series! If the plugin is able to deal with an arbitrary amount of time series (e.g. computation of minimum), use Inf for anz_benoetigt_zr.\n\u2022 info.anz_em: numbers of features extracted by the plugin (has to be 0, if info.anz_zr 6= 0)\n\u2022 info.anz_benoetigt_em: number of input single features of the plugin (will be used in future versions)\n\u2022 info.anz_im: number of images generated in the plugin (only used together with Image Extension Package)\n142 Chapter 8. Feature extraction from time series\n\u2022 info.anz_benoetigt_im: number of images needed for the generation of new images or single features (only used together with Image Extension Package)\n\u2022 info.callback: optional callback function to avoid the transfer of images into functions (only used together with Image Extension Package)1\n\u2022 info.einzug_OK: If the plugin is able to deal with intervals other than the complete time series, set einzug_OK to 1, otherwise to 0. For a description of intervals, see Section 8.4).\n\u2022 info.richtung_entfernen: special parameter for plugins for the gait analysis. Removes identifier \"left\" or \"right\" if this variable is set to 1. The default is 0.\n\u2022 info.typ: specifies the type of the result: \u2019TS\u2019: time series, \u2019SF\u2019: feature. The type \u2019Norm\u2019 requires additional normative data. For images, the additional types \u2019IM\u2019 (image) and \u2019IMSlice\u2019 (3D stack of images) are in preparation. A plugin must not extract both time series and features.\nMissing info elements are replaces by standard entries if it is possible.\nPlugins can contain parameters. Here, the use of one or more plugin parameters with edit or popup elements is possible. For each parameter p, the following definitions exist:\n\u2022 info.commandline.description{p}: name of the parameters\n\u2022 info.commandline.parameter_commandline{p}: default value (string, number or vector of numbers; scalar for popup elements means number of the selected string)\n\u2022 info.commandline.popup_string{p}: string with alternatives, e.g. \u2019yes|no\u2019 (only for popup elements)\n\u2022 info.commandline.tooltext{p}: text for tool tips\n\u2022 info.commandline.wertebereich{p}: possible range for numbers in edit elements, the two elements means the minimum and maximum value, e.g. {1 Inf }. The use of variables is possible, e.g. {1 \u2019paras.par.laenge_zeitreihe\u2019 } for the length of the time series\n\u2022 info.commandline.ganzzahlig{p}: optional restriction for the use of integer values in edit elements, if the value is 1\n\u2022 info.commandline.format{p}: ignoring of the internal format supervision, if the value is \u201dany\u201d\nThe parameters are defined using Control element: Plugin sequence - Plugin parameter for the recent selected plugin in Control element: Plugin sequence - Selection of plugins resp. Control element: Plugin sequence - Selected plugin sequence. The change between different parameters of a plugin is done using Control element: Plugin sequence - No.. All parameters can be modified using a macro.\nIn a plugin, the access to the parameters is possible with paras.parameter_commandline{2} for the second parameter etc.\nThe inputs are as follows:\n\u2022 datenIn 1Operations on images needs more time if images are transferred to functions. As an alternative, the callbacks are used in\nplugin sequences to allow a direct access to the variable myimage in the workspace.\n8.1. Definition of feature types by plugins 143\n\u2013 datenIn.dat: matrix of the input data (size paras.par.anz_dat \u00d7 sample points in interval \u00d7 info.anz_benoetigt_zr). The third dimension is missing, if info.anz_benoetigt_zr == 1!\n\u2013 datenIn.ref : data of a reference.)\n\u2022 paras\n\u2013 paras.par: par vector from SciXMiner \u2013 paras.code: currently selected output variable \u2013 paras.code_alle: all output variables \u2013 paras.var_bez: identifier of time series \u2013 paras.merk_red: number of features to select \u2013 paras.einzuege: interval from which the data has been copied out of the complete time series\n(size 1\u00d7 2) \u2013 paras.ind_zr_merkmal: indices of selected time series (size 1\u00d7 anz_benoetigt_zr). \u2013 paras.anz_gew_merk: number of selected time series \u2013 paras.parameter: SciXMiner parameter struct (contains options from the GUI) \u2013 paras.parameter_commandline: cell array with values of the plugin parameters\nThe parameters are set in the file \"merkmalsgenerierung_plugins.m\". One can add new elements to the parameter struct. Use the Matlab command isfield to check, whether an element exists in the struct. The following elements has been added for some plugins:\n\u2013 iirfilter: parameter for an IIR-Filter (included due to plugin_iirfilter) \u2013 iirfilter_aS_aL_aSigma: Some plugins (e.g. Trend and standard deviation estimation) use\nmultiple IIR-Filters\n\u2013 abtastfrequenz: sampling rate of the time series [Hz] \u2013 samplepunkt: single sample for the computation of a new feature (e.g. extraction of a specific\nsample out of a given time series\nIf the plugin is called with an empty matrix for datenIn, the info-Struct has to be returned by the plugin. datenOut and ret remain empty. This proceeding is necessary to query some information about the existing plugins. The paras-Struct is always committed by the calling function. The first rows of the plugin could be:\ninfo = struct(\u2019identifier\u2019, \u2019MIN\u2019, \u2019anz_zr\u2019, 1, ... \u2019anz_em\u2019, 0, \u2019typ\u2019, \u2019ZR\u2019, ...); if (nargin < 2 | isempty(datenIn)) datenOut = []; ret = []; return; end;\nThe outputs have to contain the following elements:\n\u2022 datenOut:\n\u2013 datenOut.dat_zr: new time series, if the plugin extracts time series (size paras.par.anz_dat \u00d7 paras.par.laenge_zeitreihe \u00d7 info.anz_zr). Empty otherwise.\n144 Chapter 8. Feature extraction from time series\n\u2013 datenOut.dat_em: new features, if the plugin extracts features (size paras.par.anz_dat \u00d7 info.anz_em). Empty otherwise.\n\u2022 ret:\n\u2013 ret.ungueltig: set to 1, if the extraction of the feature or time series has failed. The result of the plugin is not accepted by SciXMiner.\n\u2013 ret.bezeichner: contains the identifier of the time series or features if the amount of extracted time series or features depends on parameters in the GUI (e.g. Principal Component Analysis or IIR-Filter). If this element exists and is not empty it is used as identifier instead of the identifier from the info-Struct. The identifier from the info-Struct must not be changed."}, {"heading": "8.2 Standard plugins in SciXMiner", "text": "A list of include plugins in the standard installation of SciXMiner is shown in Section H. In a project, the list of all available plugins can be shown by Control element: Plugin sequence - Show plugins."}, {"heading": "8.3 Plugins for single features from single features", "text": "Plugins for the extraction of new single features from existing single features are not yet implemented in a formal way. Two different alternatives exist:\n\u2022 implementation of individual plugins via macros (see two examples in the directory standardmakros: feature_plugin_div2.makrog for the division of two selected single features and feature_plugin_log.makrog to compute the logarithm of all selected features)\n\u2022 computing aggregated features (see Edit - Extract - Single features -> Single features (with the selected feature aggregation from Options-Data Mining: Classification of single features))"}, {"heading": "8.4 Defining intervals via files", "text": "Intervals are used to restrict the feature extraction to specific sample points of the time series (only available for plugins with info.einzug_OK = 1.\nAfter loading a SciXMiner project, all *.einzug files in the current directory and in the subdirectory plugins/einzuggenerierung of the SciXMiner installation are imported. An interval file is an ASCII file including a chart of the intervals. The rows are separated with line breaks, columns with tabs. To avoid an empty interval do not add a line break after the last interval. The first row of the file has to include the following four column names (separated by tabs):\nIdentifier Short Identifier Start Stop\nThe intervals can be defined in the following rows, e.g.:\nStandphase ST_P 1 [FootOff] Schwungphase SW_P [FootOff]+1 -1\n8.5. Definition of a feature ontology by categories 145\nSpecial features are:\n\u2022 In the definition, features and mathematical operations can be used, e.g. [OppositeFootOff]+([OppositeFootContact]-[OppositeFootOff])/2. Pay attention, that used features exist in the project!\n\u2022 The end of a time series can be labeled by -1 or maxtime.\nFunction calls or variables from the Matlab workspace can not be used."}, {"heading": "8.5 Definition of a feature ontology by categories", "text": "Categories are useful to define an ontology for single features and time series. Here, similar features and time series can be grouped into classes. They defined one or more categories. Such categories might be used in different analysis and visualization functions (see [64, 114]).\nThe assignment to categories can be done by plugins, segments, or name fragments. It is defined in one or more files with the extension *.categories. Here, all files in the current working directory or in the subdirectory \u201dplugins/mgenerierung\u201d of SciXMiner or a user-defined extension package are considered.\nThese functions are text files and have a pre-defined format:\n#CategoryName \u2019Temporal derivation\u2019\n#DefaultTermName \u2019No\u2019\n#TermName \u2019Velocity\u2019 #TermRelevance 0.8 \u2019plugin_geschwindigkeit\u2019 \u2019plugin_geschwindigkeit_kausal\u2019 \u2019_V\u2019 \u2019 V \u2019\n#TermName \u2019Acceleration\u2019 #TermRelevance 0.6 \u2019plugin_beschleunigung\u2019 \u2019_A\u2019 \u2019 A \u2019\nThe following key words are used:\n\u2022 #CategoryName: name of the category.\n\u2022 #TermName: name of a term of the recent category followed by a string. In the next rows, identifiers for a mapping of time series or single features are defined. Such identifiers can be name fragments (e.g. \u2019_V\u2019), names of used plugins (e.g. \u2019plugin_geschwindigkeit\u2019) or names of segments using the key word einzug_short_description (e.g. einzug_ST_P). Consequently, each time series or single feature containing any of these identifiers belongs to the\n146 Chapter 8. Feature extraction from time series\nterm \u2019Velocity\u2019 of the category \u2019Temporal derivation\u2019 if \u2019_V\u2019 is part of the name, the plugin \u2019plugin_geschwindigkeit\u2019 was used for feature extraction by Edit - Extract - Time series -> Time series, Time series -> Single features....\n\u2022 #DefaultTermName (optional): defines the name for the default term. It used if as time series or a single feature does not belong to an other term of the category. Its standard name is \u2019unknown\u2019.\n\u2022 #TermRelevance (optional): defines the a priori relevances for the recent term. It might be defined with one or two values between zero and one separated by a space. The first value is the interpretability weighted by the exponent alpha, the second the implementability weighted by the exponent beta. If the values are missing, the standard value is one."}, {"heading": "9 Conclusions and perspectives", "text": "This manual describes the functionality of the open source Matlab toolbox SciXMiner. The aim of this toolbox is to provide an interface to apply and compare data mining methods. The architecture of the toolbox is chosen in a way that the developer crew from Karlsruhe, Germany and/or other developer crews are easily able to enlarge the toolbox by further algorithms. Everyone is kindly invited to support the further development of SciXMiner.\nThanks:\nThanks to all the busy programmers, developers of algorithms, testers, especially to R\u00fcdiger Alshut, Alessandro Angelin, Martin Ashby, Sebastian Beck, Sebastian Braun, Ole Burmeister, Joachim Dieterle, Patrick Gerland, Sebastian Gollmer, Andreas Gommlich, Lutz Gr\u00f6ll, Markus Grube, Eduard H\u00fcbner, Jens J\u00e4kel, Sina Keller, Thilo Kr\u00fcger, Jessica Legradi, Tobias Loose, Mihai Lipovei, J\u00f6rg Matthes, Dimitrios Patikas, Sebastian Pfeiffer, Tim Pychynski, Matthias Schablowski, Oliver Schill, Martin Wieland, Sebastian Wolf, Mohamed Zayani and Baifan Zhou. The support by the Deutsche Forschungsgemeinschaft (German research association) within the project \u2019Diagnosis support in gait analysis\u201d was a great help to build the gait analysis-specific part and thus, to build a basis for the further development of the toolbox. The Collaborative Research Center for Humanoid Robots, also sponsored by DFG, has inspired us to many technical applications. The present contribution was supported by the Helmholtz Association in the program \u201dBioInterfaces in Technology and Medicine\u201d and under the Joint Initiative \u201cEnergy System 2050\u2013 A Contribution of the Research Field Energy\u201d.\n147\nA Important file structures\n\u2022 A SciXMiner batch file (*.batch) is an ASCII file. It contains directories, projects, options and macros for automated analyses.\n\u2022 A classifier file (*.class) is a Matlab file. It contains data to apply a designed classifier as klass_single struct array. It includes the selected single features, aggregation transformations and the classifier itself as well as chosen standardizations. The mapping of single features and of the output variables uses the feature names. One-against-x classifiers are described by a vector- shaped klass_single struct.\n\u2022 A frequency list file (*.freq) is a Matlab file. It contains a struct freqlist with two substructs: freqlist.segments characterizes segments with elements freqlist.segments.f (vector with two elements for the lower and the upper frequency) and .name (segment name). freqlist.multiples characterizes overtones with freqlist.segments.f (scalar for frequency), .ftol (tolerance for overtones) and .name (overtone name). An example is shown in sounds.freq in the directory standardmakros.\n\u2022 A fuzzy system file (*.fuzzy) is a Matlab file. It contains membership functions and a fuzzy rulebase. The mapping to single features and to the output variable is done by variable names.\n\u2022 A segment file (*.einzug) is an ASCII file. It defines segments for the feature extraction from time series (see Section 8.4 for details).\n\u2022 A macro file (*.makrog) is a text file containing a sequence of pushed menu items and control elements. This file can be manually modified, e.g. by adding other Matlab commands.\n\u2022 A plugin sequence file (*.plugseq) is (binary) Matlab file. It contains a plugin sequence with plugin parameters.\n\u2022 A SciXMiner project file (*.prjz) is a (binary) Matlab file. It contains at least the three-dimensional matrix of time series (d_orgs) or the matrix with single features (d_org). In addition, some additional data can be included as e.g. names of time series. This file can be loaded and saved using the SciXMiner GUI or the Matlab command window (see Section 4.2.3).\n\u2022 A LATEXfile (*.tex) is an ASCII file containing various results. It is prepared to be included using LATEX. The user can switch between normal text files and LATEXfiles with the control element Control element: General options - TEX protocol. Some examples for result protocols are rule bases (*_cp*.tex), feature relevances (*_alle_merkmale.tex, *_besten_merkmale.tex), mean values (*_MWST.tex) or correlation coefficients (*_CORR.tex).\n\u2022 A text file (*.txt) is an ASCII file containing various results. It can be opened with any text editor. The user can switch between text files and LATEXfiles with the control element Control element: General options - TEX protocol. The results are similar to LATEXfiles.\n149\n\u2022 An option file (*.uihdg) is a Matlab file containing all options defined by the control elements in Chapter 7.\nB Important internal data structures\nPlease notice that SciXMiner was actually developed in German language. From this time, some variable names are German. Unfortunately, it was impossible to change this inconvenience.\nName Dimension Remarks bez_code (sy, any) Vector (string array) with names of output variables cluster_ergebnis (Struct) Struct for results and parameters of cluster methods code (N, 1) Vector of the selected output variable, classes must be coded as serial integer values between 1 and my code_alle (N, sy) Matrix of all existing output variables, classes must be coded\nas serial integer values between 1 and my code contains a redundant copy for the selected output.\nd_org (N, s) Matrix of single features d_orgs (N,K, sz) Matrix of time series (WARNING! Definition as global vari-\nable to avoid memory problems due to multiple copies in the called functions)\ndorgbez (s, any) Matrix (string array). The rows contain names of single features. If the names are not defined in the project, standard names as x1 ... xs are chosen fuzzy_system (Struct) Struct for a designed fuzzy system gaitcad_extern (Struct) Struct for different variables (especially for macros and\nSciXMiner batch files), which are reloaded after the start of SciXMiner\nind_auswahl (Nred, 1) Indices of selected data points interpret_merk (s, 1) a priori relevances of single features (set to an empty variable\nby deactivation of Control element: Single features - A priori feature relevances)\ninterpret_merk_rett (s, 1) saved a priori relevances of single features (are not changed by deactivation of Control element: Single features - A priori feature relevances, but are necessary to restore interpret_merk after (re-activation)) klass_single (Struct) Struct with settings and parameters for feature classifiers klass_zr (Struct) Struct with settings and parameters for time-series classifiers L (Struct) Struct with settings for decision theory. It can be initialized\nas empty. At the moment in use: L.ld: Matrix of decision costs for the recent output variable, L.ld_alle (struct): Matrix of decision costs of all output variables\n151\nName Dimension Remarks makro_lern (1, any) contains the macro as string, which is used for the design of classifier in a validation process makro_test (1, any) contains the macro as string, which is used for the application of classifier in a validation process mu_y (N,my) matrix, membership values of the estimated output variable as a result of the fuzzy analysis parameter (Struct) Struct with parameters about the project. A detailed description is given later-on in this chapter pos (N, 1) contains the estimated class membership for a chosen classifier. Data points which are not classified are marked with 0. prz (N, 1) contains the relative probability for a class estimation. Data points which are not classified are marked with 0. var_bez (sz, beliebig) matrix (string array), rows contain the denotations of the time\nseries. They are set to standard terms, if they are not defined within the project file. After the last row of this variable the output variable is appended.\nzgf (s+ 1,max(ml,my)) parameter of the membership functions (MBF) of the linguistic terms of the s features and of the output variable in the element s + 1, features: only trapezoid MBFs at the margins and triangular MBFs for all others permitted, output variable: singletons zgf_bez (s+ 1,max(ml,my)) array, contains the denotation of the linguistic terms of the s features and of the output variable in the element name. It is set to standard terms if it is not defined within the project file. zgf_y_bez (sy,max(my,i)) array, contains the denotations of the linguistic terms of the output variables in the element name. It is set to standard terms if it is not defined within the project file.\nThe parameter-Struct contains three elements, allgemein, gui, projekt. These elements again contain Structs.\nparameter.allgemein contains general information about paths, extensions, and status variables.\nparameter.gui encapsulates manages the control elements and contains variables, which correspond to the control elements.\nBy encapsulated functions it is ensured, that these variables always contain the current content. Furthermore, the setting of these variables and the following actualization of the graphical user interface is implemented, too.\nparameter.projekt contains values of the recent project, for instance file name, path of the project and further status variables. In parameter.projekt.abtastfrequenz the sampling frequency of the time series within the project may be saved."}, {"heading": "C Needed Standard Toolboxes", "text": "Some functions need standard toolboxes from Matlab:\nthe Signal Processing Toolbox is used\n\u2022 to filter time-series (functions filter and butter),\n\u2022 to calculate spectrograms (function specgram),\n\u2022 to calculate Cross and Auto Correlation Functions (function xcorr),\n\u2022 to calculate Matlab wavelet decompositions (function interp) and\n\u2022 to import time series with varying lengths (function resample)\nThe use of resample is checked when time-series are imported. If an error occurs, a warning will be displayed and the import is proceeded with appended zeros.\nThe Neural Network Toolbox is necessary, to calculate and apply a Artificial Neural Network as classifier.\nThe Statistic Toolbox is necessary,\n\u2022 to calculate paired or an unpaired t-test,\n\u2022 to use k-NN classifiers from this toolbox with other metrics than the Euclidean (function pdist) and\n\u2022 to use the cluster-function (out of this toolbox).\nThe Wavelet Toolbox is needed, to do a Matlab wavelet decomposition. If this toolbox is not available, a SciXMiner-integrated version may be used.\nD Included External Toolboxes (GNU-License)\nICA \u2212 Toolbox 3 % FastICA f o r Mat lab 5 . x\n% V e r s i o n 2 . 1 , J a n u a r y 15 2001 % C o p y r i g h t ( c )\n6 %Hugo G\u00e4ver t , Jarmo Hur r i , J aakko S \u00e4 r e l \u00e4 , and Aapo Hyv\u00e4r inen (GNU\u2212GPL) .\nf a s t i c a .m 9 f p i c a .m\npcamat .m remmean .m\n12 w h i t e e n v .m\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 15\n% SVM and K er ne l Methods Mat lab Toolbox \\ c i t e { Canu03 } % h t t p : / / a s i . i n s a\u2212rouen . f r / ~ arako tom / t o o l b o x / i n d e x . h tml\n18 % C o p y r i g h t S CANU \u2212 scanu@insa\u2212rouen . f r (GNU\u2212GPL)\n21 c o u t .m monqp .m s v m c l a s s .m 24 svmkerne l .m s v m m u l t i c l a s s .m s v m m u l t i c l a s s o n e a g i a n s t o n e .m 27 s v m m u l t i v a l .m s v m m u l t i v a l o n e a g i a n s t o n e .m svmreg .m 30 svmval .m\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 33\nSOM Toolbox % V e r s i o n 2 . 0 be t a , May 30 2002\n36 % C o p y r i g h t 1997\u22122000 by Esa Alhoniemi , Johan Himberg , % Juha P a r h a n k a n g a s and Juha Vesan to (GNU\u2212GPL) % h t t p : / / www. c i s . h u t . f i / p r o j e c t s / somtoo lbox / 39 knn .m s o m _ e u c d i s t 2 .m v i s _ v a l u e t y p e .m\n42\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\n154 Appendix D. Included External Toolboxes (GNU-License)\nl p _ s o l v e 5 . 5 . 0 . 7 45\n% h t t p : / / l p s o l v e . s o u r c e f o r g e . n e t / 5 . 5 / % Co\u2212d e v e l o p e r s : Michel B e r k e l a a r , K j e l l E ik l and , P e t e r N o t e b a e r t\n48 % L i c e n c e t e r m s : GNU LGPL ( L e s s e r G e n e r a l P u b l i c L i c e n c e ) % C i t a t i o n p o l i c y : G e n e r a l r e f e r e n c e s a s p e r LGPL % Module s p e c i f i c r e f e r e n c e s a s s p e c i f i e d t h e r e i n\n51\nmxlpso lve . d l l l p s o l v e 5 5 . d l l\n54 mxlpso lve .m lp_maker .m\n57 The \u2217 .DLL have t o be c o p i e d i n < m a t l a b r o o t > / b i n / !\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 60 Kolmogorov\u2212Smirnov\u2212T e s t f o r d i s t r i b u t i o n s\n% Armin G\u00fcnther , U n i v e r s i t \u00e4 t G r e i f s w a l d , Germany 63\nk s t e s t .m\n66 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 XML r e a d (BSD l i c e n c s e )\n69 %J a r e k Tuszynsk i , SAIC %h t t p : / / www. mathworks . com / m a t l a b c e n t r a l / f i l e e x c h a n g e /12907\u2212 x m l i o t o o l s\n72 xmlread .m\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 75 F i s h e r e x t e s t : F i s h e r \u2019 s Exac t P r o b a b i l i t y T e s t\n%T r u j i l l o \u2212O r t i z , A. , R . Hernandez\u2212Walls , A. Cas t ro\u2212Perez , L . 78 %Rodr iguez\u2212Cardozo , N.A. Ramos\u2212Delgado and R . Garc ia\u2212Sanchez .\n%(2004) . F i s h e r e x t e s t : F i s h e r \u2019 s Exac t P r o b a b i l i t y T e s t . A %MATLAB f i l e . [WWW document ] . h t t p : / / www. mathworks . com /\n81 %m a t l a b c e n t r a l / f i l e e x c h a n g e / l o a d F i l e . do ? o b j e c t I d =5957\nf i s h e r e x t e s t .m 84\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 VAT ( v i s u a l a s s e s s m e n t o f t e n d e n c y ) \u2212 an a l g o r i t h m t o improve t h e\nv i s u a l i z a t i o n o f d a t a p o i n t d i s t a n c e m a t r i c e s 87\n% C o p y r i g h t by Timothy Havens , Michigan Tech % h t t p : / / www. ece . mtu . edu / ~ t h a v e n s / code /VAT.m\n90\nv a t .m\n93 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\nA s s o c i a t i o n A n a l y s i s ( A p r i o r i A lgo r i t hm ) : 96 Given a s e t o f t r a n s a c t i o n s , f i n d r u l e s t h a t w i l l p r e d i c t t h e o c c u r r e n c e o f\nan i t em based on t h e o c c u r r e n c e s o f o t h e r\n155\ni t e m s i n t h e t r a n s a c t i o n\n99 % a u t h o r : N a r in e Manukyan 0 7 / 0 8 / 2 0 1 3 % C o p y r i g h t ( c ) 2013 , N a r i ne Manukyan\n102 f i n d R u l e s .m\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 105\nU i p i c k f i l e s : GUI program t o s e l e c t f i l e s and / o r f o l d e r s\n108\n% V e r s i o n : 1 . 1 5 , 2 March 2012 % Author : Douglas M. Schwarz\n111 % Email : dmschwarz= i e e e \u2217org , dmschwarz= u r g r a d \u2217 r o c h e s t e r \u2217 edu % R e a l _ e m a i l = r e g e x p r e p ( Email , { \u2019 = \u2019 , \u2019\u2217 \u2019 } , { \u2019@\u2019 , \u2019 . \u2019 } ) % h t t p : / / www. mathworks . com / m a t l a b c e n t r a l / f i l e e x c h a n g e /10867\u2212 u i p i c k f i l e s \u2212\u2212 u i g e t f i l e \u2212on\u2212s t e r o i d s 114\nu i p i c k f i l e s .m u i p i c k f i l e s _ l i c e n s e . t x t\n117\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 MATLAB2TIKZ Save f i g u r e i n n a t i v e LaTeX ( TikZ / P g f p l o t s ) .\n120\n% C o p y r i g h t ( c ) 2008\u2212\u22122014, Nico Schloemer < n i c o . schloemer@gmai l . com>\n123 m2tUpdater .m c l e a n f i g u r e .m f i g u r e 2 d o t .m 126 m 2 t I n p u t P a r s e r .m m a t l a b 2 t i k z .m\n129 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 LIBSVM\n132 % h t t p : / / www. c s i e . n t u . edu . tw / ~ c j l i n / l i b sv m % V e r s i o n : 3 . 2 0 % Author : Chih\u2212Chung Chang and Chih\u2212Jen Lin 135 % L i c e n s e : m o d i f i e d BSD l i c e n s e\nl ibsvm \u22123 . 2 0 \\\u2217 .\u2217 138\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 ARESLab\n141\n% h t t p : / / www. cs . r t u . l v / j e k a b s o n s / % V e r s i o n : 1 . 1 3 . 0\n144 % Author : G i n t s J e k a b s o n s ( g i n t s . j e k a b s o n s @ r t u . l v ) % L i c e n s e : GNU GPL\n147 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 CSV r e a d\n150 % h t t p s : / / g i t h u b . com / g i d e o n g f e l l e r / m a t l ab / b lob / m a s t e r / r ead_mixed_csv .m\n156 Appendix D. Included External Toolboxes (GNU-License)\n% l i c e n s e : none\n153 r ead_mixed_csv .m"}, {"heading": "E Symbols and abbreviations", "text": "Symbol Meaning ACF Auto correlation function ANOVA Analysis of Variances CCF Cross Correlation Function CCOEF Cross Correlation Function DA Discriminant analysis FFT Fast Fourier transformation ICA Independent Component Analysis k-NN k-nearest-neighbor MBF Membership function MEAN Mean value MLP Multi-layer perceptron MANOVA Multivariate Analysis of Variances PCA Principal Component Analysis SF Single feature STD Standard deviation SVM Support Vector Machines TS Time series K Number of sample points m Number of linguistic terms of all features ml Number of linguistic terms of the l-th feature xl my Number of linguistic terms (classes) of the output variable N Number of data points s Number of features sm Number of selected features sd Number of transformed (aggregated) features sz Number of time-series"}, {"heading": "F Known errors and problems", "text": "\u2022 SciXMiner needs certain variable, which must not be overwritten or deleted. For instance, the variable parameter encapsulates the control elements and enables SciXMiner to internally access these variables. If this Variable is deleted or does not contain the needed elements, a further execution of SciXMiner is not possible. The only possible solution is a restart. Further necessary variables are listed in Appendix B.\n\u2022 To execute a macro as well the needed features and/or time series have to exist in within the project as the output variables. There may be the possibility that a macro has to be recorded separately for each project to ensure a proper functionality.\n\u2022 If a macro is recorded, it saves the names of selected elements in a popup window for a reconstruction. If the name is not available during a replay, an error message \u201dInconsistent selection fields (see MATLAB command window for further details)\u201d is shown. In this case, you should verify the available elements in this field. Possible reasons are new names for variables or changed names for program options.\n\u2022 Frequently opening and closing of the SciXMiner GUI leads to a slower processing of figures. This problem may be traced back to an incorrect deletion of control and menu items in Matlab.\n\u2022 At the moment, massive problems with lp_solve occur due to version conflicts.\n\u2022 Using the novelty detection with the one-class method the following error message occurs: \u201dError! lp_solve library not found. Please copy *.dll from the toolbox path in ...\\bin\u201d: The novelty detection with the one-class method uses an external toolbox, which needs a dynamic link libraries (DLL). Matlab is not able to find these libraries, if they are not contained in the general Matlab path for libraries. This directory is located in \u201d<matlabroot>\\bin\u201d. The directory <matlabroot> is given in Matlab by using the command matlabroot. The following files have to be copied from the SciXMiner subdirectory \u201dtoolbox\u201d to the above mentioned directory: \u201dmxlpsolve.dll\u201d and \u201dlpsolve55.dll\u201d.\n\u2022 Applying the Matlab wavelet decomposition the following error message occurs: \u201dError using ==> interp Length of data sequence must be at least 9 You either need more data or a shorter filter (L).\u201d: The integrated Matlab function calculates time-series which are shorter than the original time series. A plugin interpolates them to the correct length. The used command for interpolation (interp) interrupts if time series are too short. Indeed, the used number of sample points may be reduced for the interpolation. However, this error correlates with another error in the wavelet decomposition, which originates from a too high number of levels. To solve the problem reduce the number of levels or deactivate the Matlab wavelet decomposition. Then, an alternative implementation will be used.\n\u2022 Errors occur for the macro recording if popup elements have digits as first characters.\n159\n\u2022 Missing MATLAB functions can cause SciXMiner errors in older MATLAB versions:\n\u2013 The MATLAB function \u201discolumn\u201d is available from MATLAB version 2011a.\nG Version history\nG.1 Versions\n\u2022 earlier Gait-CAD-Versions, see version history in Gait-CAD documentation\n\u2022 Version 2016b (13.03.2017)\n\u2022 Version 2017a (12.04.2017)\nG.2 Selected changes between Gait-CAD version 2014b and SciXMiner version 2016b\n\u2022 integration of support vector regression\n\u2022 option to generate polynomial models without absolute value (a0 term)\n\u2022 handling of timestamp data (conversion from strings in linguistic terms of output variables to timestamps resp. from timestamps to date and time)\n\u2022 visualization of multidimensional single features\n\u2022 optional hiding of figures in batch files\n\u2022 extended conversion options from data points into time series\n\u2022 export of the error of the recent regression model into a single feature\n\u2022 deletion of selected or unselected sample points in time series\n\u2022 deletion of unselected single features\n\u2022 additional style options for protocol files of classes and single features\n\u2022 switchable percentage statistics in 2D histograms\n\u2022 various internal changes related to the transfer from Gait-CAD to SciXMiner (naming, internal code optimizations etc.)\n\u2022 large variety of bug fixes\n\u2022 compatibility to MATLAB 2016b\n160\nG.3. Selected changes between SciXMiner versions 2016b and 2017a 161\nG.3 Selected changes between SciXMiner versions 2016b and 2017a\n\u2022 compatibility to MATLAB 2017a"}, {"heading": "H Plugins", "text": "\u2022 COG SF (COG): computes the center of gravity of a time series or a time series segment.\n\u2013 Function name: plugin_cog_em.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Maximum (MAX): computes the maximum of a time series or a time series segment as single feature.\n\u2013 Function name: plugin_max.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Maximum position (MAPO): computes the position of the maximum (number of the sample point) for a time series or a time series segment as single feature.\n\u2013 Function name: plugin_mapo.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Mean value SF (MEAN): computes the mean of a time series or a time series segment as single feature.\n\u2013 Function name: plugin_mean_em.m\n162\n163\n\u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Mean value SF NaN (MEAN): computes the mean of a time series or a time series segment as single feature. NaN values in a time series will be ignored.\n\u2013 Function name: plugin_mean_em_nan.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Median SF (MEDIAN): computes the median of a time series or a time series segment as single feature.\n\u2013 Function name: plugin_median_em.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Median SF NaN (MEDIAN): computes the median of a time series or a time series segment as single feature. NaN values in a time series will be ignored.\n\u2013 Function name: plugin_median_em_nan.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Minimum (MIN): computes the minimum of a time series or a time series segment as single feature.\n\u2013 Function name: plugin_min.m \u2013 Type: SF\n164 Appendix H. Plugins\n\u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Minimum position (MIPO): computes the position of the minimum (number of the sample point) for a time series or a time series segment as single feature.\n\u2013 Function name: plugin_mipo.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Norm deviation (absolute value) (ND Abs): computes the mean absolute norm deviation of a time series in a segment as single feature (mean value of Eq. (3) in [114] for a segment)\n\u2013 Function name: plugin_normzahl_betrag.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Norm deviation (direction) (ND Dir): computes the mean value of the directed deviation from a time series to a norm time series in a segment as a single feature. (mean value of the directed norm deviation for a segment, Eq. (3) in [114] without the absolute value in the nominator)\n\u2013 Function name: plugin_normzahl_richtung.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Range of Motion (ROM): computes the range (of motion) of a time series or a time series segment as single feature.\n\u2013 Function name: plugin_rom.m \u2013 Type: SF\n165\n\u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes\n\u2013 Single features: 0 inputs, 1 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 0\n\u2022 STD SF (STD SF): computes the standard deviation of all values of a time series or a time series segment. Normalization to K instead of K-1\n\u2013 Function name: plugin_std_em.m\n\u2013 Type: SF\n\u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes\n\u2013 Single features: 0 inputs, 1 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 0\n\u2022 Sum SF (SUM): computes the sum if a time series or of a time series segment as single feature.\n\u2013 Function name: plugin_sum_em.m\n\u2013 Type: SF\n\u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes\n\u2013 Single features: 0 inputs, 1 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 0\n\u2022 TS->DISCR SF MEAN (TERMQ1TERMQ2TERMQ3TERMQ4TERMQ5): computes the frequencies for (crisp) discretizations in time series.\n\u2013 Function name: plugin_discr_em.m\n\u2013 Type: SF\n\u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes\n\u2013 Single features: 0 inputs, 5 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 2\n\u2217 Number of terms (Number of terms for discretization) \u2217 Type of membership function (defines the algorithm for the design of a membership\nfunction (see \u201dControl element: Data mining: Special methods - Number of linguistic terms\u201d). \u201dMedian\u201d generates terms with approximately equal data point frequencies for all terms in the training data set.\n166 Appendix H. Plugins\n\u201dEqual distribution\u201d generate terms with similar distances between the maximum values of the membership functions. \u201dClustering\u201d uses cluster prototypes as parameters. \u201dFix\u201d set the parameters of the membership functions to the values defined by \u201dControl element: Data mining: Special methods - Parameters MBF (fix)\u201d. The choice \u201dwith interpretability\u201d rounds the found parameters using a method described in Literature: Mikut05 to improve the interpretability. Remark: All design methods set the random number generator of Matlab to a fix value. It improves the reproducibility of found membership functions. Nevertheless, all methods with random elements might be sensitive to the state of the random number generator (e.g. cluster algorithms with random start clusters). It can also influence other methods using these membership functions, e.g. by different selected features in a decision tree (e.g. \u201dData mining - Selection and evaluation of single features - Information theoretic measures\u201d). )\n\u2022 TS->FUZZY SF MEAN (TERM1TERM2TERM3TERM4TERM5): computes the frequency of fuzzy terms in a fuzzified time series as single features.\n\u2013 Function name: plugin_fuzzy_em.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 5 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 2\n\u2217 Number of terms (Number of terms for discretization) \u2217 Type of membership function (defines the algorithm for the design of a membership\nfunction (see \u201dControl element: Data mining: Special methods - Number of linguistic terms\u201d). \u201dMedian\u201d generates terms with approximately equal data point frequencies for all terms in the training data set. \u201dEqual distribution\u201d generate terms with similar distances between the maximum values of the membership functions. \u201dClustering\u201d uses cluster prototypes as parameters. \u201dFix\u201d set the parameters of the membership functions to the values defined by \u201dControl element: Data mining: Special methods - Parameters MBF (fix)\u201d. The choice \u201dwith interpretability\u201d rounds the found parameters using a method described in Literature: Mikut05 to improve the interpretability. Remark: All design methods set the random number generator of Matlab to a fix value. It improves the reproducibility of found membership functions. Nevertheless, all methods with random elements might be sensitive to the state of the random number generator (e.g. cluster algorithms with random start clusters). It can also influence other methods using these membership functions, e.g. by different selected features in a decision tree (e.g. \u201dData mining - Selection and evaluation of single features - Information theoretic measures\u201d). )\n167\n\u2022 TS->PC SF (PC1PC2): computes new single features using a Principal Component Analysis of a time series or a time series segment (K -> s_d).\n\u2013 Function name: plugin_zrhkem.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 2 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 2\n\u2217 Number of aggregated PCA features (Number s_d of principal components from a time series (Transformation number of sample points K -> s_d))\n\u2217 Normalize standard deviations (defines the normalization for the variances of the sample points)\n\u2022 TS->SF (TSSP): extracts the value of the time series for one sample point as single feature.\n\u2013 Function name: plugin_zr_em.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: none \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Number of the sample point (Sample point for Time series -> Single feature)\n\u2022 below/above norm mean value (ND Sign): computes a single feature indicating values above or below the norm time series\n\u2013 Function name: plugin_normzahl_mittelwert.m \u2013 Type: SF \u2013 Time series: 1 inputs, 0 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 1 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Absolute value (ABS): rectifies a time series by computing the absolute values\n\u2013 Function name: plugin_abs.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs\n168 Appendix H. Plugins\n\u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Acausal median of a window (FE-MED-AC): sets all values of the time series in the window to the median value of the window. Here, an acausal filter is used.\n\u2013 Function name: plugin_zr_fenster_medac.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Window length (determines the window length in sample points for the filtering with a sliding window.)\n\u2022 Acceleration (A): computes the second time derivation of a time series. see Eq. (3.3) in [64]).\n\u2013 Function name: plugin_beschleunigung.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Time (defines the scaling (per sample point resp. per time using the sampling frequency from the GUI))\n\u2022 Addition of time series (ADDTS): adds all selected time series.\n\u2013 Function name: plugin_add_zr.m \u2013 Type: TS \u2013 Time series: Inf inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Change sign (SIGNIN): multiplies a time series with \u22121\n\u2013 Function name: plugin_vorzeichen_umkehr.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs\n169\n\u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Compute square root (ROOT): Compute root of the amplitudes of a time series.\n\u2013 Function name: plugin_root.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Compute trend (Trend): Here, two first order IIR filters with the parameter aF (fast) and aS (slow) are used. The computation is explained in [90].\n\u2013 Function name: plugin_iirfilter_trend.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Parameter aF aS (to 0: no smoothing, to 1: strong smoothing. aSlow must be greater than aFast.)\n\u2022 Difference of two time series (DIFF): computes the difference between the first and second time series.\n\u2013 Function name: plugin_diffzr.m \u2013 Type: TS \u2013 Time series: 2 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Estimate standard deviation (StdTS): applies different digital lowpass filters x_f [k + 1] = a \u2217 x_f [k] + (1 \u2212 a) \u2217 x[k] with parameters aFast, aSlow and aSigma to compute the standard deviation. see Eq. (2-6) in [90] with aF = aSigma, aL = aSlow, aS = aFast.\n\u2013 Function name: plugin_iirfilter_stdschaetzer.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n170 Appendix H. Plugins\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 1\n\u2217 Parameters aFast aSlow aSigma (to 0: no smoothing, to 1: strong smoothing. aSlow must be greater than aFast.)\n\u2022 Filtered maximum (Fil-MAX): sliding maximum value with exponential forgetting. Here, a causal filter is used.\n\u2013 Function name: plugin_zr_gefiltert_max.m\n\u2013 Type: TS\n\u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 0\n\u2022 Filtered minimum (Fil-MIN): sliding minimum value with exponential forgetting. Here, a causal filter is used.\n\u2013 Function name: plugin_zr_gefiltert_min.m\n\u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 0\n\u2022 Filtering (FIL): filters a time series. A Butterworth filter is used.This plugin needs the Signal Processing Toolbox of Matlab. The Bode plot can be shown under Show - Time series.\n\u2013 Function name: plugin_filter.m\n\u2013 Type: TS\n\u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 4\n\u2217 Filter type (determines the filter characteristics in the frequency domain.) \u2217 Frequencies (defines the critical frequencies for the filter using the unit of the sample\nfrequency (e.g. in Hz). The second parameter is only used for bandpass filters.) \u2217 Filter order (FIL) (determines the order of a filter.)\n171\n\u2217 Initial values (computes initial values of the filter, Filtic+Static assumes a steady state filter at k=1, computed with MATLAB function filtic.m)\n\u2022 Filtering with Morlet wavelet (Morl):\n\u2013 Function name: plugin_morletfilter.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 3\n\u2217 Morlet wavelet: frequency (This frequency describes the center of the region which is not damped by the Morlet-Wavelet.)\n\u2217 Morlet wavelet: frequency (The width of the region is defined by the eigenfrequency of the Morlet wavelet)\n\u2217 Causal Morlet wavelet (Use the same transformation matrix (e.g. PCA) for time series reduction of all data points)\n\u2022 IIR filter (IIR): applies a digital lowpass filter x_f [k + 1] = a \u2217 x_f [k] + (1\u2212 a) \u2217 x[k].\n\u2013 Function name: plugin_iirfilter.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Parameter a (to 0: no smoothing, to 1: strong smoothing)\n\u2022 Individual norm deviation (NABW): Here, the plugins IIR (Parameter aF) and StdTS are used for the estimation\n\u2013 Function name: plugin_normabweichung.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Parameters aFast aSlow aSigma (to 0: no smoothing, to 1: strong smoothing. aSlow must be greater than aFast.)\n\u2022 Jerk (J): computes the jerk (3rd time derivation) of a time series (acausal).\n172 Appendix H. Plugins\n\u2013 Function name: plugin_ruck.m\n\u2013 Type: TS\n\u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 1\n\u2217 Time (defines the scaling (per sample point resp. per time using the sampling frequency from the GUI))\n\u2022 Jump to zero (ZEROJump): set value = 1 for a jump to zeros, and value = 0 otherwise.\n\u2013 Function name: plugin_nsprung.m\n\u2013 Type: TS\n\u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 0\n\u2022 Logarithm 10 TS (LOG10): computes the logarithm to the base 10 for all values of a time series.\n\u2013 Function name: plugin_log10_zr.m\n\u2013 Type: TS\n\u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 0\n\u2022 Maximum of a window (FE-MAX): sets all values of the time series in the window to the maximum value of the window. Here, a causal filter is used.\n\u2013 Function name: plugin_zr_fenster_max.m\n\u2013 Type: TS\n\u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 1\n\u2217 Window length (determines the window length in sample points for the filtering with a sliding window.)\n173\n\u2022 Maximum of multiple time series (MAXTS): computes the maximum for all time series in the configuration window.\n\u2013 Function name: plugin_max_zr.m \u2013 Type: TS \u2013 Time series: Inf inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Mean of a window (FE-MEAN): sets all values of the time series in the window to the mean value of the window. Here, a causal filter is used.\n\u2013 Function name: plugin_zr_fenster_mean.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Window length (determines the window length in sample points for the filtering with a sliding window.)\n\u2022 Mean value of multiple time series (MEANTS): computes the mean for all time series in the configuration window.\n\u2013 Function name: plugin_mean_zr.m \u2013 Type: TS \u2013 Time series: Inf inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Median of a window (FE-MED): sets all values of the time series in the window to the median value of the window. Here, a causal filter is used.\n\u2013 Function name: plugin_zr_fenster_median.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none\n174 Appendix H. Plugins\n\u2013 Number of parameters: 1\n\u2217 Window length (determines the window length in sample points for the filtering with a sliding window.)\n\u2022 Minimum of a window (FE-MIN): sets all values of the time series in the window to the minimum value of the window. Here, a causal filter is used.\n\u2013 Function name: plugin_zr_fenster_min.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Window length (determines the window length in sample points for the filtering with a sliding window.)\n\u2022 Minimum of multiple time series (MINTS): computes the minimum for all time series in the configuration window.\n\u2013 Function name: plugin_min_zr.m \u2013 Type: TS \u2013 Time series: Inf inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Multiplication of time series (MULTTS): multiplies all time series that are selected in the configuration window.\n\u2013 Function name: plugin_mult_zr.m \u2013 Type: TS \u2013 Time series: Inf inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Multiplication with a constant (CONST): Multiplication with a constant.\n\u2013 Function name: plugin_mult_const.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs\n175\n\u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Constant gain\n\u2022 Norm deviation time series absolute value (NDTS ABS): computes the absolute value of the norm deviation as time series. (Eq. (3) in [114])\n\u2013 Function name: plugin_normzeitreihe_abs.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Norm time series (NDTS): computes the deviation from a time series to a norm time series as a new time series. (Eq. (3.9) in [64])\n\u2013 Function name: plugin_normzeitreihe.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Normalized time series (NORM): normalizes a time series or a time series segment.\n\u2013 Function name: plugin_normalized_ts.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Type of normalization (defines the type of normalization for a time series)\n\u2022 Normalized to mean value (NORMMEAN): normalizes the time series to the mean value normalizes the time series to the mean value\n\u2013 Function name: plugin_mean_norm_ts.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n176 Appendix H. Plugins\n\u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 ROM of a window (FE-ROM): sets all values of the time series in the window to the range of the window. Here, a causal filter is used.\n\u2013 Function name: plugin_zr_fenster_rom.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Window length (determines the window length in sample points for the filtering with a sliding window.)\n\u2022 Relative ratio of two time series (RELRAT): computes the relative ratio of the first time series to the sum of both time series selected in the configuration window.\n\u2013 Function name: plugin_verhzr.m \u2013 Type: TS \u2013 Time series: 2 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Remove trend (DETREND): corrects the linear trend of a time series (MATLAB function detrend) corrects the linear trend of a time series (MATLAB function detrend)\n\u2013 Function name: plugin_detrend.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Detrend method (<html>Direction of model:<br>linear - removes a continuous, piecewise linear trend;<br>constant - removes just the mean value<html>)\n\u2022 Sorted time series (SORT TS): sorts all values of a time series in ascending order.\n\u2013 Function name: plugin_sort_zr.m\n177\n\u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 Square (SQR): computes the square of the amplitudes of a time series.\n\u2013 Function name: plugin_square.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 0\n\u2022 TS->DISCR TS MEAN (TERMQ): computes a discretized time series with a tunable number of terms for discretization.\n\u2013 Function name: plugin_discr_zr.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: yes \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 2\n\u2217 Number of terms (Number of terms for discretization) \u2217 Type of membership function (defines the algorithm for the design of a membership\nfunction (see \u201dControl element: Data mining: Special methods - Number of linguistic terms\u201d). \u201dMedian\u201d generates terms with approximately equal data point frequencies for all terms in the training data set. \u201dEqual distribution\u201d generate terms with similar distances between the maximum values of the membership functions. \u201dClustering\u201d uses cluster prototypes as parameters. \u201dFix\u201d set the parameters of the membership functions to the values defined by \u201dControl element: Data mining: Special methods - Parameters MBF (fix)\u201d. The choice \u201dwith interpretability\u201d rounds the found parameters using a method described in Literature: Mikut05 to improve the interpretability. Remark: All design methods set the random number generator of Matlab to a fix value. It improves the reproducibility of found membership functions. Nevertheless, all methods with random elements might be sensitive to the state of the random number generator (e.g. cluster algorithms with random start clusters). It can also influence other methods\n178 Appendix H. Plugins\nusing these membership functions, e.g. by different selected features in a decision tree (e.g. \u201dData mining - Selection and evaluation of single features - Information theoretic measures\u201d). )\n\u2022 TS->PC TS (PCTS1PCTS2): computes new time series by means of a Principal Component Analysis of time series (Transformation number of time series s_z -> s_d).\n\u2013 Function name: plugin_zrhkzr.m\n\u2013 Type: TS\n\u2013 Time series: Inf inputs, 2 outputs, Segments possible: none\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 3\n\u2217 Number of aggregated PCA features (Number s_d of principal components computed of a time series) \u2217 Normalize standard deviations (defines the normalization for the variances of the sample\npoints) \u2217 Transformation matrix (Use the same transformation matrix (e.g. PCA) for time series\nreduction of all data points)\n\u2022 Time shift (SHIFT): shift the time series K samples (positive values shift to the future).\n\u2013 Function name: plugin_shift_ts.m\n\u2013 Type: TS\n\u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 1\n\u2217 Shift of a time series (positive values shift to the future, negative values to the past)\n\u2022 Value in region (REGION): set all values in a region of a time series to 1 and all other to 0.\n\u2013 Function name: plugin_timeseries_region.m\n\u2013 Type: TS\n\u2013 Time series: 1 inputs, 1 outputs, Segments possible: none\n\u2013 Single features: 0 inputs, 0 outputs\n\u2013 Images: 0 inputs, 0 outputs\n\u2013 Direct callback: none\n\u2013 Number of parameters: 2\n\u2217 Lower threshold (Lower threshold for the binarization of the time series) \u2217 Upper threshold (Upper threshold for the binarization of the time series)\n179\n\u2022 Value larger than threshold (THRES): set all values of a time series above a threshold to 1, all other values to 0.\n\u2013 Function name: plugin_timeseries_threshold.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Threshold (Threshold for a binarization of time series)\n\u2022 Velocity (V): computes the first time derivation of a time series. The result is a single time series. Here, an acausal filter is used. see Eq. (3.2) in [64])\n\u2013 Function name: plugin_geschwindigkeit.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Time (defines the scaling (per sample point resp. per time using the sampling frequency from the GUI))\n\u2022 Velocity (causal) (V_kausal): computes the first time derivation of a time series. The result is a single time series. Here, a causal filter is used.\n\u2013 Function name: plugin_geschwindigkeit_kausal.m \u2013 Type: TS \u2013 Time series: 1 inputs, 1 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs \u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 1\n\u2217 Time (defines the scaling (per sample point resp. per time using the sampling frequency from the GUI))\n\u2022 Wavedec (db1, Low 4 db1, High 1db1, High 2db1, High 3db1, High 4):\n\u2013 Function name: plugin_wavedec.m \u2013 Type: TS \u2013 Time series: 1 inputs, 5 outputs, Segments possible: none \u2013 Single features: 0 inputs, 0 outputs\n180 Appendix H. Plugins\n\u2013 Images: 0 inputs, 0 outputs \u2013 Direct callback: none \u2013 Number of parameters: 3\n\u2217 Wavelet (defines the wavelet type.) \u2217 Wavelets: number of levels (defines the number of levels.) \u2217 Matlab wavelet decomposition (defines the used implementation.)"}], "references": [{"title": "KEEL Data-mining Software Tool: Data Set Repository, Integration of Algorithms and Experimental Analysis Framework", "author": ["J. ALCAL\u00c1", "A. FERN\u00c1NDEZ", "J. LUENGO", "J. DERRAC", "S. GARC\u00cdA", "L. S\u00c1NCHEZ", "F. HER- RERA"], "venue": "Journal of Multiple-Valued Logic and Soft Computing", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Konzept f\u00fcr Bildanalysen in Hochdurchsatz-Systemen am Beispiel des Zebrab\u00e4rblings", "author": ["R. ALSHUT"], "venue": "Dissertation, Karlsruher Institut fu\u0308r Technologie (KIT),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Methods for Automated High-Throughput Toxicity Testing using Zebrafish Embryos", "author": ["R. ALSHUT", "J. LEGRADI", "U. LIEBEL", "L. YANG", "J. VAN WEZEL", "U. STR\u00c4HLE", "R. MIKUT", "M. REISCHL"], "venue": "Lecture Notes in Artificial Intelligence", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Automatische Klassifikation von Bildzeitreihen f\u00fcr toxikologische Hochdurchsatz- Untersuchungen", "author": ["R. ALSHUT", "R. MIKUT", "J. LEGRADI", "U. LIEBEL", "U. STR\u00c4HLE", "G. BRETTHAUER", "M. REIS- CHL"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "The Irises of the Gasp\u00e9 Peninsula", "author": ["E. ANDERSON"], "venue": "Bulletin of the American Iris Society", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1935}, {"title": "Application of Data Mining Methods for Power Forecast of Wind Power Plants", "author": ["A. ARNOLDT", "S. K\u00d6NIG", "R. MIKUT", "P. BRETSCHNEIDER"], "venue": "Proc., 9th International Workshop on Large-scale Integration of Wind Power and Transmission Networks for Offshore Wind Farms, Quebec,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "XPIWIT - An XML Pipeline Wrapper for the Insight Toolkit", "author": ["A. BARTSCHAT", "E. H\u00dcBNER", "M. REISCHL", "R. MIKUT", "J. STEGMAIER"], "venue": "Bioinformatics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Neues Konzept zur Bewegungsanalyse und -synthese f\u00fcr Humanoide Roboter basierend auf Vorbildern aus der Biologie", "author": ["C. BAUER"], "venue": "Dissertation, Karlsruher Institut fu\u0308r Technologie, KIT Scientific Publishing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Hardware Design and Mathematical Modeling for an Artificial Pneumatic Spine for a Biped Humanoid Robot", "author": ["C. BAUER", "M. ENGELMANN", "I. GAISER", "T. STEIN", "A. FISCHER", "R. MIKUT", "S. SCHULZ"], "venue": "Proc., German Conference on Robotics, S. 7\u201311,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Human-like Reflexes for Robotic Manipulation using Leaky Integrate-and-Fire Neurons", "author": ["C. BAUER", "G. MILIGHETTI", "W. YAN", "R. MIKUT"], "venue": "Proc., IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Konzept f\u00fcr einen biologisch inspirierten, semi-passiven pneumatisch angetriebenen zweibeinigen Prothesen- Roboter-Hybrid", "author": ["C. BAUER", "M. MORS", "A. FISCHER", "T. STEIN", "R. MIKUT", "S. SCHULZ"], "venue": "at-Automatisierungstechnik", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Ein Beitrag zum automatischen Entwurf von Fuzzy-Entscheidungssystemen bei unvollst\u00e4ndiger Information", "author": ["S. BECK"], "venue": "Dissertation, Universita\u0308t Karlsruhe, Universita\u0308tsverlag Karlsruhe,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "A Cost-Sensitive Learning Algorithm for Fuzzy Rule-Based Classifiers", "author": ["S. BECK", "R. MIKUT", "J. J\u00c4KEL"], "venue": "Mathware and Soft Computing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "KNIME: The Konstanz Information Miner", "author": ["M.R. BERTHOLD", "N. CEBRON", "F. DILL", "T.R. GABRIEL", "T. K\u00d6TTER", "T. MEINL", "P. OHL", "C. SIEB", "K. THIEL", "B. WISWEDEL"], "venue": "Data Analysis, Machine Learning and Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Data Mining with Graphical Models", "author": ["C. BORGELT"], "venue": "Dissertation, O.-v.-Guericke Universita\u0308t Magdeburg,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Classification and Regression Trees", "author": ["L. BREIMAN", "J.H. FRIEDMAN", "R.A. OLSHEN", "C.J. STONE"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1984}, {"title": "Neue Methodik zur Modellierung und zum Entwurf keramischer Aktorelemente", "author": ["B.W. BR\u00dcCKNER"], "venue": "Dissertation, Karlsruher Institut fu\u0308r Technologie (KIT),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "A Tutorial on Support Vector Machines for Pattern Recognition. Knowledge Discovery and Data Mining", "author": ["C. BURGES"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Analyse von Zeitreihen in der Medizin: Informationsgehalt, Klassifikation und Unsicherheit", "author": ["O. BURMEISTER"], "venue": "Proc., 16. Workshop Computational Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Zeitvariante Klassifikatoren zur Analyse und Interpretation multimodaler Biosignale und deren Anwendung in der Prothetik und Rehabilitation", "author": ["O. BURMEISTER"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Data-Mining-Analysen mit der MATLAB-Toolbox Gait-CAD", "author": ["O. BURMEISTER", "M. REISCHL", "G. BRETTHAUER", "R. MIKUT"], "venue": "at-Automatisierungstechnik", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Zeitvariante Klassifikatoren zur Steuerung von Brain Machine Interfaces und Neuroprothesen", "author": ["O. BURMEISTER", "M. REISCHL", "L. GR\u00d6LL", "R. MIKUT"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "A Linear Programming Approach to Novelty Detection", "author": ["C. CAMPBELL", "K.P. BENNETT"], "venue": "Proc., Neural Information Processing Systems Conference,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "SVM and Kernel Methods Matlab Toolbox", "author": ["S. CANU", "Y. GRANDVALET", "A. RAKOTOMAMONJY"], "venue": "Perception Syste\u0300mes et Information, INSA de Rouen, Rouen, France,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Hg.): Medical Data Mining and Knowledge Discovery, Bd. 60 von Studies in Fuzziness and Soft Computing", "author": ["K. CIOS"], "venue": "Heidelberg: Physica,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Nearest Neighbor Pattern Classification", "author": ["T. COVER", "P. HART"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1967}, {"title": "Robust Fuzzy Clustering of Relational Data", "author": ["R. DAV\u00c9", "S. SEN"], "venue": "IEEE Transactions on Fuzzy Systems", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "DaMoQ: An Open Source MATLAB Toolbox for Data and Model Quality Assessment", "author": ["W. DONEIT", "R. MIKUT", "L. GR\u00d6LL", "T. PYCHYNSKI", "M. REISCHL"], "venue": "at-Automatisierungstechnik", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2017}, {"title": "Cross-Validation and the Bootstrap: Estimating the Error Rate of a Prediction Rule", "author": ["B. EFRON", "R. TIBSHIRANI"], "venue": "Techn. Ber. TR-477,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1995}, {"title": "From Data Mining to Knowledge Discovery in Databases", "author": ["U. FAYYAD", "G. PIATETSKY-SHAPIRO", "P. SMYTH"], "venue": "AI Magazine", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Interpretability of Linguistic fuzzy Rule-based Systems: An Overview of Interpretability Measures", "author": ["M.J. GACTO", "R. ALCAL\u00c1", "F. HERRERA"], "venue": "Information Sciences", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Automated High Throughput Mapping of Promoter- Enhancer Interactions in Zebrafish Embryos", "author": ["J. GEHRIG", "M. REISCHL", "E. KALMAR", "M. FERG", "Y. HADZHIEV", "A. ZAUCKER", "C. SONG", "S. SCHINDLER", "U. LIEBEL", "F. M\u00dcLLER"], "venue": "Nature Methods", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Robust Adaptive Fault Detection using Global State Information and Application to Mobile Working Machines", "author": ["P. GERLAND", "D. GROSS", "H. SCHULTE", "A. KROLL"], "venue": "In: Conference on Control and Fault-Tolerant Systems (SysTol),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Probability-based Global State Detection of Complex Technical Systems and Application to Mobile Working Machines", "author": ["P. GERLAND", "H. SCHULTE", "A. KROLL"], "venue": "Control Conference (ECC),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Nearest-Neighbor Based Non-Parametric Probabilistic Forecasting with Applications in Photovoltaic Systems", "author": ["J.\u00c1. GONZ\u00c1LEZ ORDIANO", "W. DONEIT", "S. WACZOWICZ", "L. GR\u00d6LL", "R. MIKUT", "V. HAGEN- MEYER"], "venue": "Proc., 26. Workshop Computational Intelligence,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Photovoltaic Power Forecasting using Simple Data-driven Models without Weather Data. Computer Science - Research and Development", "author": ["J.\u00c1. GONZ\u00c1LEZ ORDIANO", "S. WACZOWICZ", "M. REISCHL", "R. MIKUT", "V. HAGENMEYER"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Ein neues Konzept zur Diagnose elektrochemischer Sensoren am Beispiel von pH- Glaselektroden. Dissertation, Karlsruher Institut f\u00fcr Technologie (KIT), KIT Scientific Publishing", "author": ["M. GRUBE"], "venue": "Vorbereitung,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Small Angle X-ray Scattering as a High-throughput Method to Classify Antimicrobial Modes of Action", "author": ["A. VON GUNDLACH", "V. GARAMUS", "T. GORNIAK", "H. DAVIES", "M. REISCHL", "R. MIKUT", "K. HILPERT", "A. ROSENHAHN"], "venue": "Biochimica et Biophysica Acta (BBA)-Biomembranes", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Scalable Machine-Learning Algorithms for Big Data Analytics: A Comprehensive Review. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "author": ["P. GUPTA", "A. SHARMA", "R. JINDAL"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "The WEKA Data Mining Software: An Update", "author": ["M. HALL", "E. FRANK", "G. HOLMES", "B. PFAHRINGER", "P. REUTEMANN", "I.H. WITTEN"], "venue": "Association for Computing Machinery SIGKDD Explorations Newsletter", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Neural Networks: A Comprehensive Foundation", "author": ["S. HAYKIN"], "venue": "Upper Saddle River, NJ: Prentice Hall,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1994}, {"title": "The UCI KDD Archive", "author": ["S. HETTICH", "S.D. BAY"], "venue": "University of California, Department of Information and Computer Science. http://kdd.ics.uci.edu,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1999}, {"title": "In vivo Assessment of Tissue Compatibility and Functionality of a Polyimide Cuff Electrode for Recording Afferent Peripheral Nerve Signals", "author": ["B. HIEBL", "S. BOG", "R. MIKUT", "C. BAUER", "O. GEMEINHARDT", "F. JUNG", "T. KR\u00dcGER"], "venue": "Applied Cardiopulmonary Pathophysiology", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "Gene Responses in the Central Nervous System of Zebrafish Embryos Exposed to the Neurotoxicant Methyl Mercury", "author": ["N.Y. HO", "L. YANG", "J. LEGRADI", "O. ARMANT", "M. TAKAMIYA", "S. RASTEGAR", "U. STR\u00c4HLE"], "venue": "Environmental Science & Technology", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Fuzzy Cluster Analysis", "author": ["F. H\u00d6PPNER", "F. KLAWONN", "R. KRUSE"], "venue": "New York: John Wiley,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1999}, {"title": "Survey on Independent Component Analysis", "author": ["A. HYV\u00c4RINEN"], "venue": "Neural Computing Surveys", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1999}, {"title": "Data Clustering: 50 Years Beyond K-means", "author": ["A.K. JAIN"], "venue": "Pattern Recognition Letters", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Statistical Pattern Recognition: A Review", "author": ["A.K. JAIN", "R.P.W. DUIN", "J. MAO"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2000}, {"title": "Tree-Oriented Hypothesis Generation for Interpretable Fuzzy Rules", "author": ["J. J\u00c4KEL", "L. GR\u00d6LL", "R. MIKUT"], "venue": "Proc., 7th European Congress on Intelligent Techniques and Soft Computing", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1999}, {"title": "Datenbasierte Analyse und Modellbildung zur Absch\u00e4tzung spezifischer Gefahren des Klimawandels f\u00fcr Stra\u00dfen-Methodik und Szenarien", "author": ["S. KELLER"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2015}, {"title": "Feedback-Driven Design of Normalization Techniques for Biological Images Using Fuzzy Formulation of a Priori Knowledge", "author": ["A. KHAN", "M. REISCHL", "B. SCHWEITZER", "C. WEISS", "R. MIKUT"], "venue": "Studies in Computational Intelligence", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "A Benchmark Data Set to Evaluate the Illumination Robustness of Image Processing Algorithms for Object Segmentation and Classification", "author": ["A.U.M. KHAN", "R. MIKUT", "M. REISCHL"], "venue": "PLoS One", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "A New Feedback-Based Method for Parameter Adaptation in Image Processing Routines", "author": ["A.U.M. KHAN", "R. MIKUT", "M. REISCHL"], "venue": "PloS one", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2016}, {"title": "Fuzzy Control methodenorientiert", "author": ["H. KIENDL"], "venue": "Mu\u0308nchen: Oldenbourg,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1997}, {"title": "Optimization of Oncocin for Antibacterial Activity using a SPOT Synthesis Approach: Extending the Pathogen Spectrum to Staphylococcus aureus", "author": ["D. KNAPPE", "S. RUDEN", "S. LANGANKE", "T. TIKKOO", "J. RITZER", "R. MIKUT", "L.L. MARTIN", "R. HOFFMANN", "K. HILPERT"], "venue": "Amino Acids", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Identification of Non-visual Photomotor Response Cells in the Vertebrate Hindbrain", "author": ["D. KOKEL", "T. DUNN", "M. AHRENS", "R. ALSHUT", "C.Y. CHEUNG", "L. SAINT-AMANT", "G. BRUNI", "R. MATEUS", "T. VAN HAM", "T. SHIRAKI", "Y. FUKADA", "D. KOJIMA", "J.- R. YEH", "R. MIKUT", "J. VON LINTIG", "F. ENGERT", "R. PETERSON"], "venue": "The Journal of Neuroscience", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2013}, {"title": "Tuned Data Mining: A Benchmark Study on Different Tuners", "author": ["W. KONEN", "P. KOCH", "O. FLASCH", "T. BARTZ-BEIELSTEIN", "M. FRIESE", "B. NAUJOKS"], "venue": "Proc., 13th Annual Conference on Genetic and Evolutionary", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2011}, {"title": "The Great Energy Predictor Shootout", "author": ["J.F. KREIDER", "J.S. HABERL"], "venue": "Proc., ASHRAE Meeting,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1993}, {"title": "Computational Intelligence", "author": ["A. KROLL"], "venue": "De Gruyter Oldenbourg,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2016}, {"title": "Foundations of Fuzzy Systems", "author": ["R. KRUSE", "F.K.J. GEBHARDT"], "venue": "New York: John Wiley,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1994}, {"title": "A Survey of Open Source Tools for Machine Learning with Big Data in the Hadoop Ecosystem", "author": ["S. LANDSET", "T.M. KHOSHGOFTAAR", "A.N. RICHTER", "T. HASANIN"], "venue": "Journal of Big Data", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2015}, {"title": "Evaluation of Biometric Signal Characteristics for Movement Classification. Diplomarbeit, Universit\u00e4t Bukarest", "author": ["M. LIPOVEI"], "venue": "Forschungszentrum Karlsruhe,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2004}, {"title": "Konzept f\u00fcr eine modellgest\u00fctzte Diagnostik mittels Data Mining am Beispiel der Bewegungsanalyse", "author": ["T. LOOSE"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2004}, {"title": "STUCKY, K.-U.; KUEHNAPFEL, U.: First Evaluation Results Using the New Electrical Data Recorder for Power Grid Analysis", "author": ["H. MAASS", "H. CAKMAK", "W. SUESS", "A. QUINTE", "W. JAKOB"], "venue": "IEEE Transactions on Instrumentation and Measurement", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2013}, {"title": "Data Processing of High Rate Low Voltage Distribution Grid Recordings for Smart Grid Monitoring and Analysis", "author": ["H. MAASS", "H.K. CAKMAK", "F. BACH", "R. MIKUT", "A. HARRABI", "W. S\u00dcSS", "W. JAKOB", "K.-U. STUCKY", "U.G. K\u00dcHNAPFEL", "V. HAGENMEYER"], "venue": "EURASIP Journal on Advances in Signal Processing", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2015}, {"title": "An Automated and High-throughput Photomotor Response Platform for Chemical Screens", "author": ["D. MARCATO", "R. ALSHUT", "H. BREITWIESER", "R. MIKUT", "U. STR\u00c4HLE", "C. PYLATIUK", "R. PERAVALI"], "venue": "Proc., 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS),", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2015}, {"title": "MLlib: Machine Learning in Apache Spark", "author": ["X. MENG", "J. BRADLEY", "B. YUVAZ", "E. SPARKS", "S. VENKATARAMAN", "D. LIU", "J. FREEMAN", "D. TSAI", "M. AMDE", "S OWEN"], "venue": "Journal of Machine Learning Research", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2016}, {"title": "Machine Learning, Neural and Statistical Classification", "author": ["D. MICHIE", "D. SPIEGELHALTER", "C. TAYLOR"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 1994}, {"title": "Data Mining in der Medizin und Medizintechnik", "author": ["R. MIKUT"], "venue": "Universita\u0308tsverlag Karlsruhe,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2008}, {"title": "Computer-based Analysis, Visualization, and Interpretation of Antimicrobial Peptide Activities", "author": ["R. MIKUT"], "venue": "Methods in Molecular Biology", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2010}, {"title": "The Open Source Matlab Toolbox Gait-CAD and its Application to Bioelectric Signal Processing", "author": ["R. MIKUT", "O. BURMEISTER", "S. BRAUN", "M. REISCHL"], "venue": "Proc., DGBMT-Workshop Biosignalverarbeitung,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2008}, {"title": "Takagi-Sugeno-Kang Fuzzy Classifiers for a Special Class of Time-Varying Systems", "author": ["R. MIKUT", "O. BURMEISTER", "L. GR\u00d6LL", "M. REISCHL"], "venue": "IEEE Transactions on Fuzzy Systems", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2008}, {"title": "Interpretable Features for the Activity Prediction of Short Antimicrobial Peptides", "author": ["R. MIKUT", "K. HILPERT"], "venue": "Using Fuzzy Logic. International Journal of Peptide Research and Therapeutics", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2009}, {"title": "Inference Methods for Partially Redundant Rule Bases. In: Fuzzy Control: Theory and Practice (HAMPEL, R.; WAGENKNECHT, M.; CHAKER, N., Hg.)", "author": ["R. MIKUT", "J. J\u00c4KEL", "L. GR\u00d6LL"], "venue": "Advances in Soft Computing,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2000}, {"title": "Interpretability Issues in Data-Based Learning of Fuzzy Systems", "author": ["R. MIKUT", "J. J\u00c4KEL", "L. GR\u00d6LL"], "venue": "Fuzzy Sets and Systems", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2005}, {"title": "Data Mining Tools. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "author": ["R. MIKUT", "M. REISCHL"], "venue": null, "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2011}, {"title": "Data Mining in Medical Time Series", "author": ["R. MIKUT", "M. REISCHL", "O. BURMEISTER", "T. LOOSE"], "venue": "Biomedizinische Technik", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2006}, {"title": "Improving Short Antimicrobial Peptides Despite Elusive Rules for Activity", "author": ["R. MIKUT", "S. RUDEN", "M. REISCHL", "F. BREITLING", "R. VOLKMER", "K. HILPERT"], "venue": "Biochimica et Biophysica Acta (BBA)-Biomembranes", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2016}, {"title": "SciXMiner: A MATLAB Toolbox for Data Mining of Multidimensional Data", "author": ["R. MIKUT", "J. STEGMAIER", "A. BARTSCHAT", "W. DONEIT", "J. \u00c1NGEL GONZ\u00c1LEZ ORDIANO", "N. PETER", "B. SCHOTT", "S. WACZOWICZ", "M. REISCHL"], "venue": null, "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2017}, {"title": "Neues Konzept f\u00fcr ein modulares Robotersystem zur automatischen Untersuchung von Zebrab\u00e4rblingen in Hochdurchsatzverfahren", "author": ["A. PFRIEM"], "venue": null, "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2016}, {"title": "Modelling the Labyrinth Seal Discharge Coefficient Using Data Mining Methods", "author": ["T. PYCHYNSKI", "G. BLESINGER", "R. MIKUT", "K. DULLENKOPF", "H.-J. BAUER"], "venue": "Proc., ASME TURBO EXPO; Glasgow,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2010}, {"title": "Comparison of Surface EMG Monitoring Electrodes for Long-term Use in Rehabilitation Device Control", "author": ["C. PYLATIUK", "M. M\u00dcLLER-RIEDERER", "A. KARGOV", "S. SCHULZ", "O. SCHILL", "M. REISCHL", "G. BRETTHAUER"], "venue": "Proc., International Conference on Rehabilitation Robotics,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2009}, {"title": "Automatic Zebrafish Heartbeat Detection and Analysis for Zebrafish Embryos", "author": ["C. PYLATIUK", "D. SANCHEZ", "R. MIKUT", "R. ALSHUT", "M. REISCHL", "S. HIRTH", "W. ROT- TBAUER", "S. JUST"], "venue": "Zebrafish", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2014}, {"title": "Programs for Machine Learning", "author": ["QUINLAN", "J.R.: C"], "venue": null, "citeRegEx": "85", "shortCiteRegEx": "85", "year": 1993}, {"title": "Targeting Mycobacterium tuberculosis and other Microbial Pathogens using Improved Synthetic Antibacterial Peptides", "author": ["S. RAMON-GARCIA", "R. MIKUT", "C. NG", "S. RUDEN", "R. VOLKMER", "M. REISCHL", "K. HILPERT", "C.J. THOMPSON"], "venue": "Antimicrobial Agents and Chemotherapy", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2013}, {"title": "Ein Verfahren zum automatischen Entwurf von Mensch-Maschine-Schnittstellen am Beispiel myoelektrischer Handprothesen", "author": ["M. REISCHL"], "venue": null, "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2006}, {"title": "Optimized Classification of Multiclass Problems Applied to EMG-Control of Hand Prostheses", "author": ["M. REISCHL", "L. GR\u00d6LL", "R. MIKUT"], "venue": "Proc., IEEE International Joint Conference on Neural Networks,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2004}, {"title": "Evaluation of Data Mining Approaches for the Control of Multifunctional Arm Prostheses", "author": ["M. REISCHL", "L. GR\u00d6LL", "R. MIKUT"], "venue": "Integrated Computer-Aided Engineering", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2011}, {"title": "Steuerungs- und Signalverarbeitungskonzepte f\u00fcr eine multifunktionale Handprothese", "author": ["M. REISCHL", "R. MIKUT", "C. PYLATIUK", "S. SCHULZ", "S. BECK", "G. BRETTHAUER"], "venue": "at- Automatisierungstechnik", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2002}, {"title": "Einfluss von Trainingseffekten auf die Parameteradaption f\u00fcr Mensch-Maschine- Schnittstellen in der Medizintechnik", "author": ["M. REISCHL", "M.R. TUGA", "L. MEISTER", "E. ALBERG", "W. DONEIT", "D. LIEBETANZ", "R. RUPP", "R. MIKUT"], "venue": "at-Automatisierungstechnik", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2016}, {"title": "Asphalt Image Miner: A Tool for Automatic Quantification of Grains", "author": ["M. REISCHL", "A. WITTENBERG", "C. KARCHER", "R. MIKUT"], "venue": "Asphalt Samples. at-Automatisierungstechnik", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2014}, {"title": "Data Analytics: Models and Algorithms for Intelligent Data Analysis", "author": ["T.A. RUNKLER"], "venue": null, "citeRegEx": "93", "shortCiteRegEx": "93", "year": 2016}, {"title": "HeiDATAProVIT-Heidelberg Data Archiving, Tag Assembling, Processing and Visualization Tool", "author": ["M. SCHABLOWSKI", "J. SCHWEIDLER", "R. RUPP"], "venue": "Computer Methods and Programs in Biomedicine", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2004}, {"title": "Konzept zur Analyse der Lokomotion auf dem Laufband bei inkompletter Querschnittl\u00e4hmung mit Verfahren der nichtlinearen Dynamik", "author": ["M. SCHABLOWSKI-TRAUTMANN"], "venue": null, "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2006}, {"title": "Konzept zur automatisierten Anpassung der neuronalen Schnittstellen bei nichtinvasiven Neuroprothesen. Dissertation, Karlsruher Institut f\u00fcr Technologie", "author": ["O. SCHILL"], "venue": "KIT Scientific Publishing,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2014}, {"title": "Automatic Adaptation of a Self-Adhesive Multi-Electrode Array for Active Wrist Joint Stabilization in Tetraplegics", "author": ["O. SCHILL", "R. RUPP", "C. PYLATIUK", "S. SCHULZ", "M. REISCHL"], "venue": "Proc., IEEE Toronto International Conference\u2013Science and Technology for Humanity,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2009}, {"title": "Support Vector Learning", "author": ["B. SCH\u00d6LKOPF"], "venue": "Mu\u0308nchen: Oldenbourg,", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 1997}, {"title": "Robust Individual Circadian Parameter Estimation for Biosignal-based Personalisation of Cancer Chronotherapy", "author": ["B. SCHOTT", "J. STEGMAIER", "A. ARBAUD", "M. REISCHL", "R. MIKUT", "F. L\u00c9VI"], "venue": "Proc., Workshop Biosignal Processing, Berlin, arXiv preprint arXiv:1604.02909,", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2016}, {"title": "Zebrafish Biosensor for Toxicant Induced Muscle Hyperactivity", "author": ["M. SHAHID", "M. TAKAMIYA", "J. STEGMAIER", "V. MIDDEL", "M. GRADL", "N. KL\u00dcVER", "R. MIKUT", "T. DICKMEIS", "S. SCHOLZ", "S. RASTEGAR", "L. YANG", "U. STR\u00c4HLE"], "venue": "Scientific Reports", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2016}, {"title": "New Methods to Improve Large-Scale Microscopy Image Analysis with Prior Knowledge and Uncertainty", "author": ["J. STEGMAIER"], "venue": "Dissertation, Karlsruhe Institute of Technology,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2016}, {"title": "Information Fusion of Image Analysis, Video Object Tracking, and Data Mining of Biological Images using the Open Source MATLAB Toolbox Gait-CAD", "author": ["J. STEGMAIER", "R. ALSHUT", "M. REISCHL", "R. MIKUT"], "venue": "Biomedizinische Technik (Biomedical Engineering)", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2012}, {"title": "Real-Time Three-Dimensional Cell Segmentation in Large-Scale Microscopy Data of Developing Embryos", "author": ["J. STEGMAIER", "F. AMAT", "W.B. LEMON", "K. MCDOLE", "Y. WAN", "G. TEODORO", "R. MIKUT", "P.J. KELLER"], "venue": "Developmental Cell", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2016}, {"title": "Automation Strategies for Large-Scale 3D", "author": ["J. STEGMAIER", "B. SCHOTT", "E. H\u00dcBNER", "M. TRAUB", "M. SHAHID", "M. TAKAMIYA", "A. KO- BITSKI", "V. HARTMANN", "R. STOTZKA", "J. VAN WEZEL", "A. STREIT", "G.U. NIENHAUS", "U. STR\u00c4HLE", "M. REISCHL", "R. MIKUT"], "venue": "Image Analysis. at-Automatisierungstechnik", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2016}, {"title": "Automated Prior Knowledge-Based Quantification of Neuronal Patterns in the Spinal Cord of Zebrafish", "author": ["J. STEGMAIER", "M. SHAHID", "M. TAKAMIYA", "L. YANG", "S. RASTEGAR", "M. REISCHL", "U. STR\u00c4HLE", "R. MIKUT"], "venue": "Bioinformatics", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2014}, {"title": "Independent Component Analysis: An Introduction", "author": ["J. STONE"], "venue": "Trends in Cognitive Sciences", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2002}, {"title": "Multivariate Analysis", "author": ["M.M. TATSUOKA"], "venue": "New York: Macmillan,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 1988}, {"title": "RAMOS-DELGADO; GARCIA-SANCHEZ, R.: Fisherextest: Fisher\u2019s Exact Probability Test", "author": ["A. TRUJILLO-ORTIZ", "R. HERNANDEZ-WALLS", "A. CASTRO-PEREZ", "N.L. RODRIGUEZ-CARDOZO"], "venue": "A MATLAB file. [WWW document]", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 2004}, {"title": "SOM Toolbox for MAT- LAB", "author": ["J. VESANTO", "J. HIMBERG", "E. ALHONIEMI", "J. PARHANKANGAS"], "venue": "Techn. Ber., Helsinki University of Technology,", "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2000}, {"title": "Data mining to analyse the effects of price signals on household electricity customers", "author": ["S. WACZOWICZ", "S. KLAIBER", "P. BRETSCHNEIDER", "I. KONOTOP", "D. WESTERMANN", "M. REISCHL", "R. MIKUT"], "venue": "at-Automatisierungstechnik", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2014}, {"title": "Virtual Storages as Theoretically Motivated Demand Response Models for Enhanced Smart Grid Operations", "author": ["S. WACZOWICZ", "M. REISCHL", "S. KLAIBER", "P. BRETSCHNEIDER", "I. KONOTOP", "D. WESTER- MANN", "V. HAGENMEYER", "R. MIKUT"], "venue": "Energy Technology", "citeRegEx": "111", "shortCiteRegEx": "111", "year": 2016}, {"title": "Data Mining with Rattle and R: The Art of Excavating Data for Knowledge Discovery", "author": ["G. WILLIAMS"], "venue": "Springer Science & Business Media,", "citeRegEx": "112", "shortCiteRegEx": "112", "year": 2011}, {"title": "Gait analysis may help to distinguish hereditary spastic paraplegia from cerebral palsy", "author": ["S. WOLF", "F. BRAATZ", "D. METAXIOTIS", "P. ARMBRUST", "T. DREHER", "L. D\u00d6DERLEIN", "R. MIKUT"], "venue": "Gait & Posture", "citeRegEx": "113", "shortCiteRegEx": "113", "year": 2011}, {"title": "Automated Feature Assessment in Instrumented Gait Analysis", "author": ["S. WOLF", "T. LOOSE", "M. SCHABLOWSKI", "L. D\u00d6DERLEIN", "R. RUPP", "H.J. GERNER", "G. BRET- THAUER", "R. MIKUT"], "venue": "Gait & Posture", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 2006}, {"title": "Which Functional Impairments are the Main Contributors to Pelvic Anterior Tilt during Gait in Individuals with Cerebral Palsy", "author": ["S.I. WOLF", "R. MIKUT", "A. KRANZL", "T. DREHER"], "venue": "Gait & Posture", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 2014}, {"title": "Interaction of Blood Components with Cathelicidins and their Modified Versions", "author": ["K. YU", "B.F. LAI", "J. GANI", "R. MIKUT", "K. HILPERT", "J.N. KIZHAKKEDATHU"], "venue": "Biomaterials", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 2015}], "referenceMentions": [{"referenceID": 62, "context": "The toolbox bases on earlier internal versions of Gait-CAD [64, 72].", "startOffset": 59, "endOffset": 67}, {"referenceID": 70, "context": "The toolbox bases on earlier internal versions of Gait-CAD [64, 72].", "startOffset": 59, "endOffset": 67}, {"referenceID": 78, "context": "Please refer to [80] if you use SciXMiner for your scientific work.", "startOffset": 16, "endOffset": 20}, {"referenceID": 100, "context": "\u2022 analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], \u2022 object tracking [102, 101], \u2022 tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), \u2022 feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], \u2022 image processing and feature extraction for segmentation of grains in asphalt samples [92], and \u2022 extended measures and visualization to evaluate the quality of data and regression models [28].", "startOffset": 32, "endOffset": 42}, {"referenceID": 99, "context": "\u2022 analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], \u2022 object tracking [102, 101], \u2022 tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), \u2022 feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], \u2022 image processing and feature extraction for segmentation of grains in asphalt samples [92], and \u2022 extended measures and visualization to evaluate the quality of data and regression models [28].", "startOffset": 32, "endOffset": 42}, {"referenceID": 6, "context": "\u2022 analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], \u2022 object tracking [102, 101], \u2022 tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), \u2022 feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], \u2022 image processing and feature extraction for segmentation of grains in asphalt samples [92], and \u2022 extended measures and visualization to evaluate the quality of data and regression models [28].", "startOffset": 109, "endOffset": 112}, {"referenceID": 100, "context": "\u2022 analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], \u2022 object tracking [102, 101], \u2022 tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), \u2022 feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], \u2022 image processing and feature extraction for segmentation of grains in asphalt samples [92], and \u2022 extended measures and visualization to evaluate the quality of data and regression models [28].", "startOffset": 132, "endOffset": 142}, {"referenceID": 99, "context": "\u2022 analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], \u2022 object tracking [102, 101], \u2022 tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), \u2022 feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], \u2022 image processing and feature extraction for segmentation of grains in asphalt samples [92], and \u2022 extended measures and visualization to evaluate the quality of data and regression models [28].", "startOffset": 132, "endOffset": 142}, {"referenceID": 31, "context": "\u2022 analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], \u2022 object tracking [102, 101], \u2022 tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), \u2022 feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], \u2022 image processing and feature extraction for segmentation of grains in asphalt samples [92], and \u2022 extended measures and visualization to evaluate the quality of data and regression models [28].", "startOffset": 233, "endOffset": 237}, {"referenceID": 69, "context": "\u2022 analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], \u2022 object tracking [102, 101], \u2022 tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), \u2022 feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], \u2022 image processing and feature extraction for segmentation of grains in asphalt samples [92], and \u2022 extended measures and visualization to evaluate the quality of data and regression models [28].", "startOffset": 345, "endOffset": 349}, {"referenceID": 90, "context": "\u2022 analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], \u2022 object tracking [102, 101], \u2022 tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), \u2022 feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], \u2022 image processing and feature extraction for segmentation of grains in asphalt samples [92], and \u2022 extended measures and visualization to evaluate the quality of data and regression models [28].", "startOffset": 439, "endOffset": 443}, {"referenceID": 27, "context": "\u2022 analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], \u2022 object tracking [102, 101], \u2022 tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), \u2022 feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], \u2022 image processing and feature extraction for segmentation of grains in asphalt samples [92], and \u2022 extended measures and visualization to evaluate the quality of data and regression models [28].", "startOffset": 541, "endOffset": 545}, {"referenceID": 29, "context": "A well known definition is given by [30]: Data mining is a step in the KDD process that consists of applying data analysis and discovery algorithms that produce a particular enumeration of patterns (or models) over the data.", "startOffset": 36, "endOffset": 40}, {"referenceID": 92, "context": "by special import routines, like HeiDATAProViT in gait analysis [94]) as the collective of possible evaluation measures in SciXMiner (Figure 3.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "Depending on the availability, functions from different toolboxes are called, thereunder standard Matlab functions, functions from internal Matlab toolboxes (see Appendix C), free available Matlab toolboxes (FastICA1, SVM and Kernel Methods Matlab Toolbox [24]2, SOM Toolbox [109]3, lp_solve4, see Appendix D) and many SciXMiner internal functions.", "startOffset": 256, "endOffset": 260}, {"referenceID": 107, "context": "Depending on the availability, functions from different toolboxes are called, thereunder standard Matlab functions, functions from internal Matlab toolboxes (see Appendix C), free available Matlab toolboxes (FastICA1, SVM and Kernel Methods Matlab Toolbox [24]2, SOM Toolbox [109]3, lp_solve4, see Appendix D) and many SciXMiner internal functions.", "startOffset": 275, "endOffset": 280}, {"referenceID": 105, "context": "\u2022 basic knowledge about multivariate statistics and classification [107, 69, 49] and specialties for time series [73],", "startOffset": 67, "endOffset": 80}, {"referenceID": 67, "context": "\u2022 basic knowledge about multivariate statistics and classification [107, 69, 49] and specialties for time series [73],", "startOffset": 67, "endOffset": 80}, {"referenceID": 47, "context": "\u2022 basic knowledge about multivariate statistics and classification [107, 69, 49] and specialties for time series [73],", "startOffset": 67, "endOffset": 80}, {"referenceID": 71, "context": "\u2022 basic knowledge about multivariate statistics and classification [107, 69, 49] and specialties for time series [73],", "startOffset": 113, "endOffset": 117}, {"referenceID": 29, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 24, "endOffset": 44}, {"referenceID": 14, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 24, "endOffset": 44}, {"referenceID": 24, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 24, "endOffset": 44}, {"referenceID": 76, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 24, "endOffset": 44}, {"referenceID": 91, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 24, "endOffset": 44}, {"referenceID": 56, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 71, "endOffset": 75}, {"referenceID": 44, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 131, "endOffset": 139}, {"referenceID": 46, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 131, "endOffset": 139}, {"referenceID": 62, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 169, "endOffset": 173}, {"referenceID": 15, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 201, "endOffset": 209}, {"referenceID": 83, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 201, "endOffset": 209}, {"referenceID": 48, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 241, "endOffset": 249}, {"referenceID": 74, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 241, "endOffset": 249}, {"referenceID": 53, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 277, "endOffset": 294}, {"referenceID": 59, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 277, "endOffset": 294}, {"referenceID": 58, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 277, "endOffset": 294}, {"referenceID": 48, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 326, "endOffset": 342}, {"referenceID": 74, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 326, "endOffset": 342}, {"referenceID": 12, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 326, "endOffset": 342}, {"referenceID": 11, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 326, "endOffset": 342}, {"referenceID": 30, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 388, "endOffset": 396}, {"referenceID": 74, "context": "\u2022 basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], \u2022 cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], \u2022 decision trees (basics: [16, 85], implemented design algorithms [50, 76]), \u2022 fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]", "startOffset": 388, "endOffset": 396}, {"referenceID": 112, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 22, "endOffset": 31}, {"referenceID": 74, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 22, "endOffset": 31}, {"referenceID": 45, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 66, "endOffset": 75}, {"referenceID": 104, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 66, "endOffset": 75}, {"referenceID": 17, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 103, "endOffset": 115}, {"referenceID": 23, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 103, "endOffset": 115}, {"referenceID": 96, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 103, "endOffset": 115}, {"referenceID": 25, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 146, "endOffset": 150}, {"referenceID": 40, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 181, "endOffset": 185}, {"referenceID": 86, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 248, "endOffset": 252}, {"referenceID": 67, "context": "\u2022 a priori relevances [114, 76], \u2022 Independent Component Analysis [47, 106], \u2022 Support Vector Machines [18, 24, 98], \u2022 k-Nearest Neighbor Methods [26], \u2022 Artificial Neural Networks [42], \u2022 feature aggregation and selection using wrapper approaches [88], \u2022 validation strategies [69], and", "startOffset": 278, "endOffset": 282}, {"referenceID": 90, "context": "Application Data Domain References Asphalt grain analysis Images Engineering [92] Brain-Machine-Interfaces Time series Medicine [70] Cell classification Images and single features Biology [52]", "startOffset": 77, "endOffset": 81}, {"referenceID": 68, "context": "Application Data Domain References Asphalt grain analysis Images Engineering [92] Brain-Machine-Interfaces Time series Medicine [70] Cell classification Images and single features Biology [52]", "startOffset": 128, "endOffset": 132}, {"referenceID": 50, "context": "Application Data Domain References Asphalt grain analysis Images Engineering [92] Brain-Machine-Interfaces Time series Medicine [70] Cell classification Images and single features Biology [52]", "startOffset": 188, "endOffset": 192}, {"referenceID": 16, "context": "Ceramic actuator optimization Time series and single features Engineering [17]", "startOffset": 74, "endOffset": 78}, {"referenceID": 97, "context": "Circadian parameter estimation Time series Medical technology [99]", "startOffset": 62, "endOffset": 66}, {"referenceID": 49, "context": "Climate change events on streets Time series Engineering [51]", "startOffset": 57, "endOffset": 61}, {"referenceID": 81, "context": "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]", "startOffset": 73, "endOffset": 97}, {"referenceID": 85, "context": "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]", "startOffset": 73, "endOffset": 97}, {"referenceID": 87, "context": "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]", "startOffset": 73, "endOffset": 97}, {"referenceID": 89, "context": "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]", "startOffset": 73, "endOffset": 97}, {"referenceID": 95, "context": "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]", "startOffset": 73, "endOffset": 97}, {"referenceID": 94, "context": "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]", "startOffset": 73, "endOffset": 97}, {"referenceID": 1, "context": "Heartbeat detection of zebrafish Videos Biology [2, 84, 81]", "startOffset": 48, "endOffset": 59}, {"referenceID": 82, "context": "Heartbeat detection of zebrafish Videos Biology [2, 84, 81]", "startOffset": 48, "endOffset": 59}, {"referenceID": 79, "context": "Heartbeat detection of zebrafish Videos Biology [2, 84, 81]", "startOffset": 48, "endOffset": 59}, {"referenceID": 99, "context": "Light-sheet microscopy 3D-Images, 3DVideos Biology [101, 103, 104]", "startOffset": 51, "endOffset": 66}, {"referenceID": 101, "context": "Light-sheet microscopy 3D-Images, 3DVideos Biology [101, 103, 104]", "startOffset": 51, "endOffset": 66}, {"referenceID": 102, "context": "Light-sheet microscopy 3D-Images, 3DVideos Biology [101, 103, 104]", "startOffset": 51, "endOffset": 66}, {"referenceID": 32, "context": "Mobile working machines Time series Engineering [33, 34] Modeling of labyrinth seals Single features Engineering [82]", "startOffset": 48, "endOffset": 56}, {"referenceID": 33, "context": "Mobile working machines Time series Engineering [33, 34] Modeling of labyrinth seals Single features Engineering [82]", "startOffset": 48, "endOffset": 56}, {"referenceID": 80, "context": "Mobile working machines Time series Engineering [33, 34] Modeling of labyrinth seals Single features Engineering [82]", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "Morphology analysis of zebrafish Images Biology [2, 3, 100, 105]", "startOffset": 48, "endOffset": 64}, {"referenceID": 2, "context": "Morphology analysis of zebrafish Images Biology [2, 3, 100, 105]", "startOffset": 48, "endOffset": 64}, {"referenceID": 98, "context": "Morphology analysis of zebrafish Images Biology [2, 3, 100, 105]", "startOffset": 48, "endOffset": 64}, {"referenceID": 103, "context": "Morphology analysis of zebrafish Images Biology [2, 3, 100, 105]", "startOffset": 48, "endOffset": 64}, {"referenceID": 19, "context": "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]", "startOffset": 39, "endOffset": 62}, {"referenceID": 93, "context": "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]", "startOffset": 39, "endOffset": 62}, {"referenceID": 111, "context": "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]", "startOffset": 39, "endOffset": 62}, {"referenceID": 112, "context": "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]", "startOffset": 39, "endOffset": 62}, {"referenceID": 113, "context": "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]", "startOffset": 39, "endOffset": 62}, {"referenceID": 51, "context": "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]", "startOffset": 120, "endOffset": 128}, {"referenceID": 52, "context": "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]", "startOffset": 120, "endOffset": 128}, {"referenceID": 37, "context": "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]", "startOffset": 59, "endOffset": 88}, {"referenceID": 54, "context": "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]", "startOffset": 59, "endOffset": 88}, {"referenceID": 69, "context": "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]", "startOffset": 59, "endOffset": 88}, {"referenceID": 72, "context": "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]", "startOffset": 59, "endOffset": 88}, {"referenceID": 84, "context": "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]", "startOffset": 59, "endOffset": 88}, {"referenceID": 77, "context": "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]", "startOffset": 59, "endOffset": 88}, {"referenceID": 114, "context": "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]", "startOffset": 59, "endOffset": 88}, {"referenceID": 42, "context": "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]", "startOffset": 142, "endOffset": 146}, {"referenceID": 36, "context": "pH sensor diagnosis Time series Engineering [37] Photo-Motor Response Images, Time series Biology [57, 67]", "startOffset": 44, "endOffset": 48}, {"referenceID": 55, "context": "pH sensor diagnosis Time series Engineering [37] Photo-Motor Response Images, Time series Biology [57, 67]", "startOffset": 98, "endOffset": 106}, {"referenceID": 65, "context": "pH sensor diagnosis Time series Engineering [37] Photo-Motor Response Images, Time series Biology [57, 67]", "startOffset": 98, "endOffset": 106}, {"referenceID": 34, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 45, "endOffset": 53}, {"referenceID": 35, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 45, "endOffset": 53}, {"referenceID": 7, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 87, "endOffset": 101}, {"referenceID": 9, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 87, "endOffset": 101}, {"referenceID": 8, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 87, "endOffset": 101}, {"referenceID": 10, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 87, "endOffset": 101}, {"referenceID": 20, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 145, "endOffset": 171}, {"referenceID": 63, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 145, "endOffset": 171}, {"referenceID": 64, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 145, "endOffset": 171}, {"referenceID": 108, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 145, "endOffset": 171}, {"referenceID": 109, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 145, "endOffset": 171}, {"referenceID": 2, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 202, "endOffset": 212}, {"referenceID": 3, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 202, "endOffset": 212}, {"referenceID": 43, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 202, "endOffset": 212}, {"referenceID": 5, "context": "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]", "startOffset": 256, "endOffset": 259}, {"referenceID": 39, "context": "\u2022 alternative data mining software as Weka [41], Knime [14], Apache Spark\u2019s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "\u2022 alternative data mining software as Weka [41], Knime [14], Apache Spark\u2019s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]", "startOffset": 55, "endOffset": 59}, {"referenceID": 66, "context": "\u2022 alternative data mining software as Weka [41], Knime [14], Apache Spark\u2019s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "\u2022 alternative data mining software as Weka [41], Knime [14], Apache Spark\u2019s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]", "startOffset": 112, "endOffset": 115}, {"referenceID": 110, "context": "\u2022 alternative data mining software as Weka [41], Knime [14], Apache Spark\u2019s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]", "startOffset": 126, "endOffset": 131}, {"referenceID": 75, "context": "\u2022 alternative data mining software as Weka [41], Knime [14], Apache Spark\u2019s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]", "startOffset": 148, "endOffset": 160}, {"referenceID": 60, "context": "\u2022 alternative data mining software as Weka [41], Knime [14], Apache Spark\u2019s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]", "startOffset": 148, "endOffset": 160}, {"referenceID": 38, "context": "\u2022 alternative data mining software as Weka [41], Knime [14], Apache Spark\u2019s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]", "startOffset": 148, "endOffset": 160}, {"referenceID": 57, "context": "The goal of the Building-data set was to examine the effect of different parameters on the power consumption of a known building [59].", "startOffset": 129, "endOffset": 133}, {"referenceID": 22, "context": "Since the extraction of the single trials should be start exactly at a trigger event and end 23 sample points after the trigger event (resulting in 24 hours after the trigger event), the offset is set to [0, 23].", "startOffset": 204, "endOffset": 211}, {"referenceID": 41, "context": "The Iris data set (downloadable from UCI-Repository [43]) is one of the most-famous benchmark data sets for the comparison of classifiers.", "startOffset": 52, "endOffset": 56}, {"referenceID": 4, "context": "The data set contains 150 examples of three different irises, every single iris is represented by 50 examples [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 0, "context": "The feature numbers start with x[1] instead of x[0]!", "startOffset": 32, "endOffset": 35}, {"referenceID": 18, "context": "Some hints to the algorithms are given in [19].", "startOffset": 42, "endOffset": 46}, {"referenceID": 22, "context": "The one-class method uses a SVM optimization proposed by [23].", "startOffset": 57, "endOffset": 61}, {"referenceID": 62, "context": "5, [64]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 74, "context": "However, a split by existing membership functions is used instead of a new computed binary split in each node [76].", "startOffset": 110, "endOffset": 114}, {"referenceID": 85, "context": "2) in [87]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 85, "context": "4) in [87]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 85, "context": "7) in [87]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "\u2022 Feature maps (all time series, selected data points): computes feature relevances of all time series and shows the results as scatter plot (x axis: Time, y axis: number of time series, color: feature relevance) [19].", "startOffset": 213, "endOffset": 217}, {"referenceID": 106, "context": "Otherwise for cross tabulations with only two values 0 and 1, the (more conservative) Two-tail Exact Fisher Test is used in the implementation of [108] (function \u201dfisherextest\u201d).", "startOffset": 146, "endOffset": 151}, {"referenceID": 21, "context": "The different classifier types are described in [22].", "startOffset": 48, "endOffset": 52}, {"referenceID": 85, "context": "\u2022 Design: designs a Hierarchical Bayes classifier by a step-wise separation of single classes from all other classes [87].", "startOffset": 117, "endOffset": 121}, {"referenceID": 74, "context": "The design algorithms are described in [76].", "startOffset": 39, "endOffset": 43}, {"referenceID": 62, "context": "[64]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For stability reasons, the value is limited to [0, 1].", "startOffset": 47, "endOffset": 53}, {"referenceID": 88, "context": "\u2022 IIR parameter (aF, aS, aSigma): defines the parameters for three different IIR filters for trend and standard deviation computation (see [90]).", "startOffset": 139, "endOffset": 143}, {"referenceID": 61, "context": "[63] or the documentation of the Matlab Wavelet Toolbox).", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "\u2022 Matlab wavelet decomposition: uses the Matlab implementation instead of the implementation of [63] for the wavelet decomposition.", "startOffset": 96, "endOffset": 100}, {"referenceID": 22, "context": "A data point z is handled as outlier if f(z) < Threshold (see [23] for algorithm details).", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "The neighborhood is defined by a maximal ([0,1]-normalized Euclidean) distance.", "startOffset": 42, "endOffset": 47}, {"referenceID": 22, "context": "A data point z is handled as outlier if f(z) < Threshold (see [23] for algorithm details).", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "The neighborhood is defined by a maximal ([0,1]-normalized Euclidean) distance.", "startOffset": 42, "endOffset": 47}, {"referenceID": 0, "context": "Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.", "startOffset": 48, "endOffset": 54}, {"referenceID": 85, "context": "[87]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 85, "context": "Available options are an a maximization of the classification accuracy (\u201dbest_class\u201d) or a maximization of the minimal distance between two classes using class-specific covariance matrixes as metric (\u201dbest_ldf\u201d) [87].", "startOffset": 212, "endOffset": 216}, {"referenceID": 0, "context": "Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.", "startOffset": 48, "endOffset": 54}, {"referenceID": 21, "context": "[22, 73] give a detailed description of the classifiers.", "startOffset": 0, "endOffset": 8}, {"referenceID": 71, "context": "[22, 73] give a detailed description of the classifiers.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[22, 73] give a detailed description of the classifiers.", "startOffset": 0, "endOffset": 8}, {"referenceID": 71, "context": "[22, 73] give a detailed description of the classifiers.", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.", "startOffset": 48, "endOffset": 54}, {"referenceID": 85, "context": "[87]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.", "startOffset": 48, "endOffset": 54}, {"referenceID": 26, "context": "\u2022 Distance measure for the noise cluster: defines the method for the definition of a noise cluster [27] for a Fuzzy C-Means method.", "startOffset": 99, "endOffset": 103}, {"referenceID": 26, "context": "Possible distance measures are the mean distance of all data points to all other clusters [27] or the median of these distances.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "\u2022 Kernel: defines the used kernel function for Support Vector Machines [18].", "startOffset": 71, "endOffset": 75}, {"referenceID": 74, "context": "The choice \u201dwith interpretability\u201d rounds the found parameters using a method described in [76] to improve the interpretability.", "startOffset": 91, "endOffset": 95}, {"referenceID": 74, "context": "\u2022 Exponent for clearness: defines the evaluation method for the pruning of fuzzy rules [76].", "startOffset": 87, "endOffset": 91}, {"referenceID": 73, "context": "An additional correction of overlapping rules [75] was implemented for artifacts by rules with the same or concurrent conclusions in case of the SUM-PROD inference.", "startOffset": 46, "endOffset": 50}, {"referenceID": 74, "context": "The choice \u201dwith interpretability\u201d rounds the found parameters using a method described in [76] to improve the interpretability.", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "[29]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "Such categories might be used in different analysis and visualization functions (see [64, 114]).", "startOffset": 85, "endOffset": 94}, {"referenceID": 112, "context": "Such categories might be used in different analysis and visualization functions (see [64, 114]).", "startOffset": 85, "endOffset": 94}, {"referenceID": 112, "context": "(3) in [114] for a segment)", "startOffset": 7, "endOffset": 12}, {"referenceID": 112, "context": "(3) in [114] without the absolute value in the nominator)", "startOffset": 7, "endOffset": 12}, {"referenceID": 62, "context": "3) in [64]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 88, "context": "The computation is explained in [90].", "startOffset": 32, "endOffset": 36}, {"referenceID": 88, "context": "(2-6) in [90] with aF = aSigma, aL = aSlow, aS = aFast.", "startOffset": 9, "endOffset": 13}, {"referenceID": 112, "context": "(3) in [114])", "startOffset": 7, "endOffset": 12}, {"referenceID": 62, "context": "9) in [64])", "startOffset": 6, "endOffset": 10}, {"referenceID": 62, "context": "2) in [64])", "startOffset": 6, "endOffset": 10}], "year": 2017, "abstractText": null, "creator": "LaTeX with hyperref package"}}}