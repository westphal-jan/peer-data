{"id": "1704.04259", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Identity and Granularity of Events in Text", "abstract": "In similar sheet we nor any method it methods event descrip - shallying in rarely here articles though to model the mathematic now reasons for their existing equipment RDF mathematical. We ought these descriptions to resolve on long - document outdoor coreference task. Our aol - ponent methods to held semantics imperative any and malleability according events at they levels. It music half to state - it - created - examples approaches on the cross - submit event coreference possible, instead outperforming people art when assuming. our created summer ballistics. We demonstrate how ctd have particular even inter-related others thinking discuss if algo - pansexual anomaly instead be which supposed principles reflects both coreference, subevent and dosing trade.", "histories": [["v1", "Thu, 13 Apr 2017 19:23:43 GMT  (46kb)", "http://arxiv.org/abs/1704.04259v1", "Invited keynote speech by Piek Vossen at Cicling 2016"]], "COMMENTS": "Invited keynote speech by Piek Vossen at Cicling 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["piek vossen", "agata cybulska"], "accepted": false, "id": "1704.04259"}, "pdf": {"name": "1704.04259.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Agata Cybulska"], "emails": ["piek.vossen@vu.nl,", "a.k.cybulska@vu.nl"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n04 25\n9v 1\n[ cs\n.C L\n] 1\n3 A\npr 2\nKeywords: Event coreference, event identity, event relations"}, {"heading": "1 Introduction", "text": "News and blogs are common media that report on events that took place in the world. In the case of events with impact, we can expect that many different sources discuss the same event, partially providing the same information and partially differing from each other either in terms of the facts or perspective on these facts. Collections of news and blogs around the same topic therefore represent a challenging and natural task for cross-document event coreference. If done properly, event coreference resolution can be used to link event data across many different sources, resulting in deduplication and aggregation of data around events but also showing the different perspectives of these sources [32].\nThe task of cross-document event coreference is however far from trivial. Events exist within their temporal boundaries. The same type of event involving the same participants and the same action at a different point of time is not the same event, e.g. John gave Mary the book on Tuesday is a different event from John gave Mary the book on Wednesday. On the other hand, Mary gave John the book on Tuesday is also a different event, even though all event components are the same as for the first event but the roles differ. The precise semantics of these descriptions can be used to define identity across events. This becomes more\ncomplicated when we underspecify and quantify the information. In the case of John gave Mary several books this week or Mary often gets books from people the precise details are not given and matching with the previous descriptions becomes more difficult. Also, it is not clear if the quantification should be seen as a quantification of the object books or of the event of giving. Furthermore, the action itself can be described in many different ways (gets/takes/receives/borrows/buys/obtains), exhibiting different manners or perspectives on the same event.\nThese examples clearly show that establishing identity across event descriptions is a hard problem to solve and on the one hand involves semantic components that play a role but on the other hand requires a robust matching across these components. In [12], we therefore described a model to measure identity across events as a function of the similarity of the event components. Such a model can be optimized on an annotated data set to weigh the contribution of the components for establishing event identity.\nWhen implementing such a model for extracting and identifying events from raw text, another problem arises. The components necessary to compare events are spread over a complete article and are hardly found in a single sentence. Sentence-based approaches to event coreference cannot deal with the fact that event components are mentioned at different places of the text. In [15] a bag of events approach is suggested to group event components per source article and compare these across different articles. However, the implementation of the bag of events approach described in [15] was only tested on true mentions and did not consider the extraction of events from text. Furthermore, in the data set used in the experiments reported in [15] only a few sentences per news article are annotated, which means that only little information could be aggregated across these sentences.\nIn this paper, we describe a new implementation of the bag of events model proposed in [15] which completely processes news articles starting from raw text. First event instance representations are build from all mentions throughout the text within a single article and next these representations are compared across articles. The method employed here makes a distinction between mentions within an article and across articles on the one hand and event instances that are stable across these mentions on the other hand [17]. Likewise, we can compare event mentions but also event instance representations. Our system can be highly parameterized to establish identity. This can result in different levels of granularity of determined event instances. More loose constraints result in extreme aggregation and lumping, whereas very strict definitions result in each mention to be unique. We show that our instance-based approach performs close to state-ofthe-art machine learning approaches and it out-performs these approaches when a similar quality of event detection is assumed.\nIn section 2, we summarize previous work that is related to this task. In section 3, we describe the event model and data set on which we evaluate our approach. Section 4 describes the bag of events approach that we proposed previously, which collects event information per document so that it can be used\nto solve event coreference. We present earlier implementation examples of the approach and their evaluation results achieved on true mentions in section 5. Next in section 6 we propose a new, door-to-door implementation of the bag of events approach. We evaluate the implementation and compare results with the state-of-the-art in section 7. In section 8, we discuss the results and speculate on the notion of granularity in relation to identity. We conclude in section 9."}, {"heading": "2 Related work", "text": "Event coreference resolution is a difficult task of determining whether two event mentions refer to the same event instance. After the original focus on entity coreference, recently there is more and more interest in the field in event coreference resolution.\nSome supervised approaches to event coreference resolution have been proposed by Humphreys et al. [19], Bagga and Baldwin [3], Ahn [1], or Chen and Ji [11]. These methods rely on supervised learning of rich linguistics features to determine coreference in a pairwise model and are strongly domain dependent. This is not the case for the method presented in section 7 which is unsupervised. Furthermore, these works depend on local decisions without consideration of global event distribution at document level. The bag of events model implemented here allows us to overcome this deficiency.\nMore recently there are a few more works also using rich linguistic features to solve event coreference. One of them is the unsupervised approach of Bejan and Harabagiu [5] which relies on lexical, POS, event class and WordNet features as well as feature combinations. Another approach is the one of Liu et al. [24] and the most recent feature-rich model of Yang et al. [34]. Our method is similar to the approach of [24] in that it facilitates propagation of information about event participants between event mentions but our heuristic does this more globally by employing an instance level of event representation that aggregates this information at the document level as is also done [34]. We differ from [34] in that we use an RDF representation to do a logical comparison, whereas they do clustering using document level features. Our approach also differs from another recent approach to event coreference by Lee et al. [23], by making a distinction between specific entity types, whereas [23] disregard entity type information."}, {"heading": "3 Event model and data", "text": "We model events as a combination of five slots. These five slots correspond to different elements of event information such as the action slot (or event trigger following the ACE ([21]) terminology) and four kinds of event arguments: time, location, human and non-human participant slots (see [13]). The next quote shows an excerpt from topic one, text number seven of the ECB corpus ([5]).\nThe \u201cAmerican Pie\u201d actress has entered Promises for undisclosed reasons. The actress, 33, reportedly headed to a Malibu treatment facility on Tuesday.\nConsider two event templates presenting the distribution of event information over the five event slots in the two example sentences (tables 1 and 2). This event\nmodel has been employed to annotate the ECB+ data set ([14]) which is used in the experiments with event coreference described in the following sections. The approach to event coreference used in this work determines coreference between mentions of events through compatibility of slots of the five slot template. If event mentions are coreferent, they refer to the same event instance. Two or more event mentions are coreferent if they describe actions that happen or hold true in the same time and place and with involvement of the same participants.\nThe EventCorefBank (ECB, [5]) was developed to test cross-document event coreference resolution. It consists of 43 different topics which correspond to seminal events, each topic with about 10 to 20 news articles reporting on a seminal event. Across articles from a topic, mentions of events are coreferent. The ECB+ corpus [14] is an extended and re-annotated version of the ECB. To each of the 43 ECB topics new texts were added about different event instances of the same event type. For example in addition to the topic of a particular celebrity checking into rehab covered in the ECB, to ECB+ descriptions were added of another event instance involving a different celebrity checking into another rehab facility. Likewise, the authors of ECB+ increased the referential ambiguity of event mentions. Table 3 shows some examples of seminal events represented in ECB+ with two different event instances per topic. Table 4 shows some statistics on the data set, most notably there are 6833 mentions of events annotated and 1958 coreference chains (instances). On average, 1.8 sentence per article was annotated for experiments on event coreference."}, {"heading": "4 Modelling event coreference: bag of events approach", "text": "It is pretty much common practice to use information coming from event arguments for event coreference resolution ([19], [11], [10], [9], [5], [23], [12], [24] among others). But using entities for event coreference resolution is complicated by the fact that event descriptions within a sentence often lack pieces of information. As pointed out by [19] it could be the case however that a lacking piece of information might be available elsewhere within discourse borders. This is a challenge for pairwise models comparing separate event mentions with one another on the sentence level. To be able to fully make use of information coming from event arguments, instead of looking at event information available within the same sentence, we propose to take a broader look at event descriptions surrounding the event mention in question within a unit of discourse. In this study we consider a document (a news article) as the unit of discourse.\nThe bag of events approach (for details see [15]) translates the structure of event descriptions into event templates for event coreference resolution. An event template can be created on different levels of information, such as a sentence, a paragraph or an entire document. The approach explicitly employs discourse structure to account for challenges following from the uneven distribution of event information across sentences of a document. Two templates are filled: a\nsentence and a document template. A sentence template collects event information from the sentence of an active action mention (tables 1 and 2). By filling in a document template, one creates a \u201cbag of events\u201d for a document, that could be seen as a kind of document \u201csummary\u201d (table 5).\nThis heuristic employs clues coming from discourse structure and namely those implied by discourse borders. Descriptions of different event mentions occurring within a discourse unit, whether coreferent or related in some other way, unless stated otherwise, tend to share elements of their context. In our example text fragment the first sentence reveals that an actress has entered a rehab facility. From the second sentence the reader finds out where the facility is located (Malibu) and when the \u201cAmerican Pie\u201d actress headed to the treatment center. It is clear to the reader of the example text fragment from the quotation that both events described in sentence one and two, happened on Tuesday. Also both sentences mention the same rehab center in Malibu. These observations are crucial for the approach.\nThe bag of events method can be implemented in different ways. In the following section 5 we will briefly look at experiments with implementations as oneand as two-step classification tasks (the two-step implementation is described in detail in [15]). This implementation uses true mentions of event data and represents the document-based bag of events features as a loose set. In the one-step approach document-based bag of events features are combined with sentencebased features for pairwise mention comparison. In the two-step approach the bag of events features are first used for document clustering and the sentencebased features are used for pairwise mention comparison within a cluster. In section 6, we describe another two-step implementation that extracts all event and entity data from the text and represents events as RDF instances, which aggregate all event data across the coreferential mentions in a single news article. These document-based RDF representations are more specific than the bag of events representations. In the second step, these RDF representations are compared semantically across all documents within a topic. The final instance results are mapped back to all the mentions for evaluation. We evaluate the RDF implementation in section 7."}, {"heading": "5 Experiments with coreference on true mentions", "text": ""}, {"heading": "5.1 One- vs. two-step classification", "text": "The specifics of the implementation of the two-step approach can be found in [15].\nThe two-step bag of events approach starts with filling in a document template, accumulating mentions of the five event slots: actions, locations, times, human and non-human participants from a document, as exemplified in table 5. In a document template there is no distinction made between pieces of event information coming from different sentences of a document and no information is kept about elements being part of different mentions. A document template can be seen as a bag of events and event arguments. The template stores unique lemmas, to be precise a set of unique lemmas per event template slot. Documentbased features from the template are used for preliminary document clustering. A supervised decision tree classifier (hereafter DT ) determines whether two document templates share corefering event mentions. After all unique pairs of document templates from the test set have been classified by means of the DT classifier, \u201ccompatible\u201d pairs are merged into document clusters based on pair overlap.\nIn the second step of the approach coreference is solved in a pairwise model between action mentions per document cluster created in step 1. For this task sentence templates are filled per true action mention from the corpus. Sentence templates collect event information from the sentence (consider examples in table 1 and 2). Pairs of sentence templates translate into features indicating compatibility across five template slots. A supervised classifier solves coreference between all unique pairs of action mentions per document cluster and finally pairs sharing common mentions are chained into equivalence classes.\nIn the one-step implementation of the approach all possible unique pairs of action mentions from the corpus are used as the starting point for classification. No initial document clustering is performed. For every action mention a sentence template is filled (see examples in table 1 and 2). Also, for every corpus document a document template is filled. Bag of events features indicating the degree of overlap between documents, from which two active mentions come from, are used for classification. In the one-step approach document features are used by a classifier together with sentence-based features; these combined create a feature vector per active action pair. One DT classifier is used to determine event coreference in a pairwise model. Pairs of mentions are classified based on a mix of information from a sentence and from a document. Finally, corefering pairs with overlap are merged into equivalence classes.\nThe one-step classification is implementation-wise simpler but it is computationally much more expensive. Ultimately every action mention has to be compared with every other action mention from the data set. This is a drawback of the one-step approach. On the other hand, it could be of advantage to have different types of information (sentence- and document-based) available simultaneously to determine event mention coreference."}, {"heading": "5.2 Experiment set-up", "text": "For coreference experiments on true mentions we used a subset of ECB+ annotations (based on a list of 1840 selected sentences, [14]), that were additionally reviewed with focus on coreference relations. We divided the corpus into a training set (topics 1-35) and test set (topics 36-45).\nThe ECB+ texts are available in the XML format. The texts are tokenized, hence no sentence segmentation nor tokenization needed to be done. We POStagged (for the purpose of proper verb lemmatization) and lemmatized the corpus sentences. We used tools from the Natural Language Toolkit ([6], NLTK version 2.0.4): the NLTK\u2019s default POS tagger, Word-Net lemmatizer1 as well as WordNet synset assignment by the NLTK2. For machine learning experiments we used scikit-learn ([27]).\nIn the experiments different features were assigned values per event slot (see Table 6). Note, that frequently one ends up with multiple entity mentions from the same sentence for an action mention (the relation between an action and\n1 www.nltk.org/modules/nltk/stem/wordnet.html 2 http://nltk.org/ modules/nltk/corpus/reader/wordnet.html\ninvolved entities is not annotated in ECB+). All entity mentions from the sentence (or a document in case of bag of events features) are considered. In case of document templates features referring to active action mentions were disregarded, instead action mentions from a document were considered. All feature values were rounded to the first decimal point.\nWe experimented with a few feature sets, considering per event slot lemma features only (L), or combining them with other features described in Table 6. Before fed to a classifier, missing values were imputed (no normalization was needed for the scikit-learn DT algorithm). All classifiers were trained on an unbalanced number of pairs of examples from the training set. We used grid search with ten fold cross-validation to optimize the hyper-parameters (maximum depth, criterion, minimum samples leafs and split) of the decision-tree algorithm."}, {"heading": "5.3 Evaluation on true mentions from ECB+", "text": "We will consider two baselines: a singleton baseline and a rule-based lemma match baseline. The singleton baseline considers event coreference evaluation scores generated taking into account all action mentions as singletons. The rulebased lemma baseline generates event coreference clusters based on full overlap between lemma or lemmas of compared event triggers (action slot) from the test set. Table 7 presents baselines\u2019 results in terms of recall (R), precision (P) and F-score (F) by employing the following metrics: MUC ([31]), B3 ([2]), CEAF ([25]), BLANC ([29]), and CoNLL F1 ([28]).\nWhen discussing event coreference scores must be noted that some of the commonly used metrics depend on the evaluation data set, with scores going up or down with the number of singleton items in the data [29]. Our singleton baseline gives us zero scores in MUC, which is understandable due to the fact that the MUC measure promotes longer chains. B3 on the other hand seems to give additional points to responses with more singletons, hence the remarkably high scores achieved by the baseline in B3. CEAF and BLANC as well as the CoNLL measures (the latter being an average of MUC, B3 and entity CEAF) give more realistic results. The lemma baseline reaches 62% CoNLL F1.\nTable 8 evaluates final clusters of coreferent action mentions produced in the experiments by means of the one- and two-step classification when employing different features.\nWhen considering bag of events classifiers using exclusively lemma features L (row two and three), the two-step approach reached a 1% higher CoNLL Fscore than the one-step approach with document-based lemma features (docL). The one-step method achieved in BLANC a 2% better precision but a 2% lower recall. This is understandable. In a two-step implementation when document clusters are created some precision is lost. In a one-step classification specific sentence information is always available for the classifier hence we see slightly higher precision scores (also in other metrics).\nThe best coreference evaluation scores with the highest CoNLL F-score of 73% and BLANC F of 72% were reached by the two-step bag of events approach with a combination of the DT document classifier using feature set L (documentbased hence docL) across five event slots and the DT sentence classifier when employing features LDES (see Table 6 for a description of features). Adding action similarity (A) on top of LDES features in step two, does not make any difference on decision tree classifiers with a maximum depth of 5 using five slot templates. Our best CoNLL F-score of 73% is an 11% improvement over the strong rule based event trigger lemma baseline, and a 34% increase over the singleton baseline.\nTo quantify the contribution of document features, we contrast the results of classifiers using bag of events features with scores achieved when disregarding document features. The results reached with sentence template classification only (without any document features, row one in table 8), give us some insights into the impact of the document features on our experiment. Note that one-step classification without preliminary document template clustering is computationally much more expensive than a two-step approach, which ultimately takes into account much less item pairs thanks to the initial document template clustering. The DT sentence template classifier trained on an unbalanced training set reaches 70% CoNLL F. This is 8% better than the strong baseline disregarding event arguments, but only 3% less than the two-step bag of events approach and 2% less than the one-step classification with document features. The reason for the relatively small contribution by document features could owe to the fact that in the ECB+ corpus not that many sentences are annotated per text. 1840 sentences are annotated in 982 corpus texts, i.e. 1.87 sentence per text. We expect that the impact of document features would be bigger, if more event\ndescriptions from a discourse unit were taken into account than only the ground truth mentions.\nWe run an additional experiment with the two-step approach in which four entity types were bundled into one entity slot. Locations, times, human and nonhuman participants were combined into a cumulative entity slot resulting in a simplified two-slot template. When using two-slot templates for both, document and sentence classification on the ECB+ 70% CoNLL F score was reached. This is 3% less than with five-slot templates."}, {"heading": "6 Door-to-door implementation", "text": "In the previous sections 4 and 5, true mentions have been used to evaluate the approach. In this section, we describe how we extract event data from raw text (system mentions) and then establish cross-document event coreference in a two-step-approach.\nFor extracting event data from text we use the NLP pipeline developed in the NewsReader project.3 NewsReader applies a cascade of semantic modules among which: named-entity recognition and classification (NERC), namedentity-linking (NEL), semantic role labeling (SRL), time expression detection and normalization, time anchoring, event and entity coreference. The same entity, event or time expression can be mentioned several times in a document. The above modules will interpret each occurrence separately and annotate the tokens accordingly. The output of the NLP pipeline is thus mention-based.\nFrom the mention-based annotation of tokens, we derive an instance-based representation of events, entities, time objects and relations between them. Our instance-based model follows the Simple Event Model (SEM, [18]). SEM is an RDF model for presenting event-instances through URIs (Unique Resource Identifiers) with triple relations to participants and dates, which also have unique URIs. We extended SEM with the Grounded Annotation Framework (GAF, [17]) to link each instance URI to mentions in the source documents. In Figure 1, we show the SEM representations for the entities Ka\u2019loni Flynn and Christopher Simpson resulting from processing topic 45 in ECB+. Each RDF structure has a URI representing an entity as the subject and various properties, such as the rdfs:label for the surface forms, skos:prefLabel for the most frequent surface form and gaf:denotedBy to link to char offsets of mentions. We can see that all mentions are offsets in different ECB+ files and there are no mentions in ECB files, which is correct.\nFor both persons, we see that not every mention was resolved to the same URI. For example, Christopher Simpson has one DBpedia URI, one representation as an entity without a DBpedia match and one representation as a so-called non-entity, i.e. phrases not detected as an entity by NERC but playing an important role in an event.4 In the case of Ka\u2019loni Flynn, there is no DBpedia entry to match to and we see 3 entities and 1 non-entity. There are various reasons\n3 www.newsreader-project.eu 4 We used FrameNet frame elements to decide on relevance of a participant\nwhy the NLP modules did not match these mentions to the same entity. Such differences in URIs may also hamper the matching of events as we will see later.\nIn the same way as for entities, also events are represented through unique URIs with properties as shown in Figure 2. Since events are normally not stored in DBpedia and cannot be identified by their surface forms, we create meaningless unique identifiers (nwr:45 12ecbplus#ev10 and nwr:45 6ecbplus#ev16 ). We further see similar properties as for the entities, such as rdfs:label, skos:prefLabel and gaf:denotedBy. We do not show the full list of mentions linked by the gaf:denotedBy property to save space but both events have mentions across various ECB+ files, implying that information has been aggregated from mentions in these files.\nOther properties for events are the class information (property a), sem:Actor relations and a sem:hasTime relation. The class information consists of WordNet\nsynsets [16]5, their hypernyms and the FrameNet frames associated with the events [4]. The WordNet synsets are selected from the highest scoring senses of all the mentions according to word-sense-disambiguation (WSD). As for the actors, we listed only the entities using their URIs with their surface forms between brackets.\nNote that actors show some degree of lumping of mentions and in some cases have wrong URIs, e.g. dbp:Jerome Flynn (Flynn , Herbert Flynn , his , Ka\u2019Loni Flynn , Ka\u2019Loni Flynn\u2019s , Ka\u2019loni Flynn), where references to both the murdered daughter Ka\u2019Loni Flynn and her father Herbert Flynn got the\n5 WordNet synsets are represented here as Inter Lingual Index (ili) records: www.globalwordnet.org/ili [33]. This enables us to compare events across different languages [32]\nsame URI dbp:Jerome Flynn, which is also to the wrong person. This lumping is mainly the result of wrong links coming from DBpedia spotlight [26] which gives preference to more popular entities. Wrong URIs will not harm our coreference matches as long as they are systematic, i.e. all mentions of Simpson get matched to the same URI dbp:O. J. Simpson. When different URIs are given, they can still be matched through surface forms (see below). Finally, we see that the events are linked to a date (sem:hasTime), which is a specific day for the first event and a year for the second event.\nThe above event instance representations are the result of a two-step approach that is implemented as follows:\n1. Collect all event data for a single document and represent this as a SEM instance, giving access to all information on the action, the participants and the dates present in a document. 2. Compare the aggregated SEM representations across different documents to decide on identity. If identity is established then the SEM instance representations are merged.\nFor the first step, we collect all event mentions from a single document and then collect all participants and time anchors for these mentions into a single instance representation. We first group all mentions of the same lemma and next we determine the similarity across lemmas on the basis of the WordNet similarity scores [22] of their WordNet senses with the highest WSD score. Next, we check the Semantic Role structure created by the NewsReader pipeline to find all the roles for these mentions as well as all the time expressions to which these mentions are anchored. We thus already get aggregated instance representations with multiple surface forms across mentions in different sentences with their WordNet synsets, FrameNet frames, dates and actors.\nIn the second step compare these event structures across the different documents within a topic using the following heuristic:\n1. the event actions need to match sufficiently (above a threshold) in terms of WordNet synsets or, if there are no synsets, in terms of the surface forms; 2. the time needs to match if present; 3. there must be overlap of actors in any role or the specified roles;\nFor comparing actions, we check the proportion of overlap across the WordNet synsets. In case two events have no synsets associated, we use the surface forms. If the overlap is mutually above a predefined threshold, we continue, otherwise the events do not match.\nAfter passing the test for action similarity, we compare the overlap of the participants. We first check the URI. If the URI does not match, we check if the preferred form matches any of the surface forms. Participant matches can be required for specific semantic roles, e.g. PropBank [20] A0, A1, A2, or the role can be ignored. At least one participant needs to match in any role or, in the former case, per specified role.\nIf the above test fails, there is no coreference, otherwise we continue to compare the time anchors. Time is matched either per year, month or day, where a more specific time constraint also requires the more general ones, i.e. same month also implies same year, and same day also implies the same month and year. If one event has no time anchor and the other does, there is no coreference. If both events have no time anchor, they match.\nIn the case of Figure 2, the two event structures did not get merged by our software. Their time anchors matched in terms of the year and there is an overlapping actor but none of the WordNet synsets overlap. Nevertheless, each of the event instances shows already quite some lumping of other mentions across the documents, as indicated by the mentions in different files in topic 45."}, {"heading": "7 Experiments with coreference on system mentions", "text": "We evaluate the NewsReader system on system mentions on the same ECB+ data set. We compare the NewsReader results with Yang et al. (2015) [34], who report the best results for event coreference resolution system mentions for ECB+ and who also compare their results to other systems that have so far only been tested on ECB and not on ECB+. Yang et al use a distance-dependent Chinese Restaurant Process (DDCRP [8]), which is an infinite clustering model that can account for data dependencies. They define a hierarchical variant (HDDCRP) in which they first cluster event mentions and data within a document and next cluster the within document clusters across documents. Their hierarchical strategy is similar to our approach using event components, in the sense that event data can be scattered over multiple sentences in a document and needs to be gathered first. Our approach differs from theirs in that we use a semantic representation to capture all event properties and do a logical comparison, while Yang et al. used machine learning methods (both unsupervised clustering and supervised mention based comparison). Yang et al. also report on a lemma-baseline as proposed by Cybulska and Vossen (2014) [14], where all event mentions with the same lemma within and across documents are simply joined in a single coreference set.\nYang et al. test their system on topics 24-43 while they used topics 1-20 as training data and topics 21-23 as the development set. They do not report on topics 44 and 45. To compare our results with theirs, we also used topics 24-43 for testing. In Table 9, we show Yang\u2019s lemma baseline (LEMMA), Yang\u2019s best results (HDDCRP), and the results for NewsReader (NWR). The NewsReader systems are not trained on ECB+ data and use logical comparison of event data. We tested the following variants of the NewsReader system, where systems starting with NWR-X (out-of-the-box), NWR-T (with TimeEval2013 training for event detection) or NWR-G (with true mentions of events). The remainder of the code expresses the time filter (Y=year, M=month, D=Day, N=none), the participant filter (A=any role, A1=PropBank A1, N=none) and the action filter (c10,30,50,70=concept overlap, p10,30,50,70=phrase overlap):\nECB+ MUC BCUB CEAFe CoNLL Mention Topics 24-43 R P F1 R P F1 R P F1 F1 F1 LEMMA 55.4 75.10 63.80 39.60 71.70 51 61.10 36.20 45.50 53.40 95 HDDCRP 67.10 80.30 73.10 40.60 73.10 53.50 68.90 38.60 49.50 58.70 95 NWR-X-YAc30p30 44.85 50.16 47.35 46.88 45.3 46.08 47.45 34.89 40.22 44.55 67.99 NWR-T-YAc30p30 48.99 58.5 53.33 45.37 55.48 49.92 41.37 45.56 43.36 48.87 75.03 NWR-G-YAc30p30 64.12 72.03 67.85 65.21 74.89 69.72 66.35 57.39 61.55 66.37 99.84 NWR-G-MAc30p30 64.12 72.03 67.85 65.21 74.89 69.72 66.35 57.39 61.55 66.37 99.84 NWR-G-DAc30p30 62.12 70.99 66.26 61.93 75.69 68.12 66.57 56.52 61.14 65.17 99.84 NWR-G-YAc10p10 64.81 70.6 67.58 65.57 72.84 69.02 63.75 57.1 60.24 65.61 99.84 NWR-G-YAc50p50 63.49 72.55 67.72 64.63 75.84 69.79 67.48 57.29 61.97 66.49 99.84 NWR-G-YAc70p70 62.61 72.81 67.33 63.8 76.92 69.75 67.9 56.61 61.74 66.27 99.84 NWR-G-YNc30p30 77.4 69.68 73.34 72.92 64.24 68.31 54.99 65.39 59.74 67.13 99.84 NWR-G-YA1c30p30 52.31 71.27 60.34 58 80.27 67.34 69.89 50.67 58.75 62.14 99.84 NWR-G-NAc30p30 64.12 72.03 67.85 65.21 74.89 69.72 66.35 57.39 61.55 66.37 99.84\nWe first compare the NewsReader out-of-the-box system (NWR-X-YAc30p3) with Yang\u2019s results (LEMMA baseline and HDDCRP). The NewsReader system uses the following matching settings: year, a single participant in any role and 30% of the event concepts or, if not present, the event surface forms. We see that\nboth Yang\u2019s HDDCRP and the lemma baseline outperform NWR-X-YAc30p3 by 14 and 9 points respectively in CoNLL F1 score [28]. However, Yang et al. report that their system at first had an out-of-the-box accuracy for event detection of 56%. They therefore trained a separate Conditional Random Field (CRF) event detection system with event annotations of the first 20 topics (about half of the data set). They report an accuracy for this classifier of 95% on event detection and they used it as input for both the LEMMA baseline and HDDCRP. For comparison, the NewsReader system has an out-of-the-box accuracy of 67.99%, where events are detected by the MATE tool [7] which is trained on PropBank data. Clearly, what events have been annotated and how they were annotated has a big impact on the results. To measure this impact on the actual eventcoreference, we created two other variants of the NewsReader system: 1) NWR-T replaces the MATE event detection by a CRF classifier trained with SemEval 2013 - TempEval3 gold data [30] and 2) NWR-G uses the true mentions of the ECB+ annotation as the events (Gold). The event detection accuracy of NWR-T is 75.03% and the accuracy for NWR-G is 99.84%.6\nIn the last column of Table 9, we list the F1 measures for the detection of event mentions by each variant system. It clearly shows that the differences in coreference results across systems are mainly due to the performance on the event detection. The NWR-G variants for example outperform HDDCRP by almost 10 points, while scoring only 5 points higher in event detection. NWRT-YAc30p30 performs 7 points higher in event detection and 4 points higher in event coreference than NWR-X-YAc30p30. The NWR-X-YAc30p30, NWRT-YAc30p30, NWR-G-YAc30p30 only differ in the extraction of the events with accuracies of 68%, 75% and 100% respectively. The other parameters for time, participant and action match are the same. Note that all NewsReader systems apply logical comparison and are not trained on the ECB+ data set. We thus can expect their performance to be relatively stable across data sets, whereas Yang et al\u2019s system is expected to perform significantly lower when applied to out-of-domain data.\nNext, Tabe 9 lists the applied different variants of the NewsReader system using the true mentions of events (NWR-G) to see the impact on event coreference given the perfect event detection. Varying the settings for matching using the true mentions for events, shows only small differences. When we make the time more strict (year (Y), month (M), day (D)), we see that the month is as discriminative as the year but CoNLL F1 is 1 point lower when the time needs to match at the level of the day. Next, we varied the threshold for overlapping concepts and surface forms (10%, 30%, 50%, 70%). We can see that precision for MUC and BCUB are higher when the thresholds are higher. This is in line with our expectation. For CEAFe, we see the same trend for recall. The last rows of Table 9 show the impact of the participants. In case of NWR-G-YNc30p30, no matching participant is required, for NWR-G-YA1c30p30 the PropBank A1 role should be identical, and for NWR-G-NAc30p30 we match any participant but\n6 The reason that NWR-G is not 100% is because the NewsReader system could not process one of the evaluation files due to formatting problems.\nwe dropped the time constraint. Remarkably, dropping the participant match constraint gives best results, while requiring a matching A1 participant gives highest precision scores for BCUB and recall for CEAFe.\nOverall, we can conclude that there is a slight tendency for more strict parameters to increase the precision but that we always loose more recall with slightly lower F1 scores as a result. It is also clear that the event detection itself is the most important factor for improving event coreference.\nNote that the best performance obtained with the true mentions of the events (67.13), scores only 6 points below our bag-of-events approach (73) using true mentions for all event components and it outperforms the lemma baseline on true mentions (63) reported in section 5. Obviously, the NewsReader approach uses more data from the document than the annotated data (1.8 sentences) but on the other hand it also introduces more noise. We can expect that improving the linking of participants to entity mentions and improving the event detection is likely to bring the out-of-the-box system closer to 73 F1 scores."}, {"heading": "8 Discussion", "text": "The results have shown that cross-document event coreference is a hard task that is not completely solved despite the progress. We demonstrated that event components are critical but that they need to be collected from the complete document. Nevertheless, we have also seen that event detection as such is a major factor for the current performance starting from raw text.\nWith respect to the granularity of the matching of the components, we have used various ways to abstract from surface forms:\n\u2013 different forms are matched to the same URI assigned by DBpedia Spotlight, created from surface forms or through nominal coreference; \u2013 time expressions are normalised to the ISO dates; \u2013 event mentions with different forms are matched throughWordNet similarity\nand their word-senses;\nFurthermore, we can parameterize the matching by setting loose or strict constraints:\n\u2013 dates can be mapped by year or month instead of day; \u2013 more or less participants to be shared, with or without their roles; \u2013 degree of overlap of concepts and the range of concepts above a word-sense-\ndisambiguation (WSD) threshold;\nWe have seen that making these constraints more tight results in more precision and lower recall. Making them more loose has the opposite effect.\nThere are still mentions that are not mapped to the same instances, e.g. \u201dhis girlfriend\u201d, \u201dthe daughter\u201d to \u201dKa\u2019loni Flynn\u201d, and there are many missed URIs as well as wrong URIs assigned. The quality of modules such as NERC, NED\nand WSD is crucial in this respect. Furthermore, time-anchors are very sparse and difficult to infer from the text as such.7\nHowever, it is also the case that the ECB+ database is still too restricted to measure the true contribution of the component-based approach. We have seen that matching years instead of months makes no difference due to the fact that the events can already be distinguished by the year. Adding more seminal events to create more referential ambiguity for similar events around similar periods in time will require more precise analysis and component matching.\nDefining the granularity of event descriptions provides an interesting view on event-coreference. How far can we go to lump together event data? In a way, we could lump all events that make up a story or a topic together and define a period of time in which the topic or story takes place with all the involved participants. This does not necessarily violate the idea of event-coreference since peoples\u2019 intuitions on decomposing events to smaller units are also not clear-cut. Obviously, at some point lumping of event data generates unclarity of scope relations between events and participants, such as more than one person murdering the same or different persons, or even semantic anomalies such as or the same person being at different places at the same time. This is where event coreference could set a hard border but this also means that annotation and evaluation of data sets may need to be different, e.g. assigning not only event-coreference relations but also subevent and topical relations.\nA final aspect that still needs to be investigated is variation. Even though ECB+ has documents from various sources, events that are annotated as coreferential have mostly the same lemma and most events have no coreference relation at all (90% of all mentions). Annotation of event coreference is not an easy task and annotators tend to be conservative. More variation in reference to events is also expected to put higher demands on a semantic approach rather than approaches that are trained on mentions only."}, {"heading": "9 Conclusion", "text": "We described a new method to detect event descriptions in text and to model semantics of events, their components and event coreference using RDF representations. The proposed heuristic outperforms the state-of-the-art system when assuming equal quality in event detection. Our approach collects the component information from the complete document rather than from the local context of the event mentions that are compared. We have shown that event components play a role in obtaining precision and recall and that their matching needs to be adapted to the granularity of the task.\nIn future work, we want to investigate event-coreference in relation to topical structures and storylines. We believe that this also helps creating annotated data in which more variation and referential ambiguity is reflected. This will make\n7 We left out the document creation time as a baseline time-anchor because it may interfer with the task since the articles on each seminal event were published on different dates.\nboth the annotation of data and the task more natural. Such data sets provide additional possibilities and challenges to obtain more precise temporal ordering of events for event coreference.\nAcknowledgments. The NewsReader project was co-funded by the European Union as project number: 316404, FP7 Work Programme Call FP7-ICT-20118 Objective Cooperation Research theme \u201dInformation and Communication Technologies\u201d, challenge 4.4 - Area Intelligent Information Management."}], "references": [{"title": "The stages of event extraction", "author": ["D. Ahn"], "venue": "Proceedings of the Workshop on Annotating and Reasoning about Time and Events", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Algorithms for scoring coreference chains", "author": ["A. Bagga", "B. Baldwin"], "venue": "Proceedings of the International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Cross-document event coreference: Annotations, experiments, and observations", "author": ["A. Bagga", "B. Baldwin"], "venue": "Proceedings of the ACL Workshop on Coreference and its Applications. p. 18", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe"], "venue": "Proceedings of the 17th international conference on Computational linguistics-Volume 1. pp. 86\u201390. Association for Computational Linguistics", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Unsupervised event coreference resolution with rich linguistic features", "author": ["C.A. Bejan", "S. Harabagiu"], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural Language Processing with Python", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": "O\u2019Reilly Media Inc., http://nltk.org/book", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "A high-performance syntactic and semantic dependency parser", "author": ["A. Bj\u00f6rkelund", "B. Bohnet", "L. Hafdell", "P. Nugues"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations. pp. 33\u201336", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Distance dependent chinese restaurant processes", "author": ["D.M. Blei", "P.I. Frazier"], "venue": "The Journal of Machine Learning Research 12, 2461\u20132488", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A unified event coreference resolution by integrating multiple resolvers", "author": ["B. Chen", "J. Su", "S.J. Pan", "C.L. Tan"], "venue": "Proceedings of the 5th International Joint Conference on Natural Language Processing. Chiang Mai, Thailand", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Event coreference resolution: Feature impact and evaluation", "author": ["Z. Chen", "H. Ji"], "venue": "Proceedings of Events in Emerging Text Types (eETTs) Workshop", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Graph-based event coreference resolution", "author": ["Z. Chen", "H. Ji"], "venue": "TextGraphs-4 Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing. pp. 54\u201357", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Semantic relations between events and their time, locations and participants for event coreference resolution", "author": ["A. Cybulska", "P. Vossen"], "venue": "Angelova, G., Bontcheva, K., Mitkov, R. (eds.) Proceedings of Recent Advances in Natural Language Processing", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Guidelines for ECB+ annotation of events and their coreference", "author": ["A. Cybulska", "P. Vossen"], "venue": "Tech. Rep. NWR-2014-1, VU University Amsterdam", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Using a sledgehammer to crack a nut? Lexical diversity and event coreference resolution", "author": ["A. Cybulska", "P. Vossen"], "venue": "Proceedings of the 9th Language Resources and Evaluation Conference (LREC2014). Reykjavik, Iceland", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Bag of events\u201d approach to event coreference resolution", "author": ["A. Cybulska", "P. Vossen"], "venue": "Supervised classification of event templates. In: proceedings of the 16th Cicling 2015 (co-located: 1st International Arabic Computational Linguistics Conference). Cairo, Egypt", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "WordNet", "author": ["Fellbaum", "C. (ed."], "venue": "An Electronic Lexical Database. The MIT Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "NAF and GAF: Linking linguistic annotations", "author": ["A. Fokkens", "A. Soroa", "Z. Beloki", "N. Ockeloen", "G. Rigau", "W.R. van Hage", "P. Vossen"], "venue": "Proceedings 10th Joint ISO-ACL SIGSEM Workshop on Interoperable Semantic Annotation. p. 9. Reykjavik, Iceland", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Design and use of the Simple Event Model (SEM)", "author": ["W.R. van Hage", "V. Malais\u00e9", "R. Segers", "L. Hollink", "G. Schreiber"], "venue": "J. Web Sem. 9(2), 128\u2013136", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Event coreference for information extraction", "author": ["K. Humphreys", "R. Gaizauskas", "S. Azzam"], "venue": "ANARESOLUTION \u201997 Proceedings of a Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "From treebank to propbank", "author": ["P. Kingsbury", "M. Palmer"], "venue": "LREC. Citeseer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "ACE (Automatic Content Extraction) English Annotation Guidelines for Events ver", "author": ["LDC"], "venue": "5.4.3 2005.07.01. In: Linguistic Data Consortium", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Combining local context with wordnet similarity for word sense identification", "author": ["C. Leacock", "M. Chodorow"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Joint entity and event coreference resolution across documents", "author": ["H. Lee", "M. Recasens", "A. Chang", "M. Surdeanu", "D. Jurafsky"], "venue": "Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Natural Language Learning (EMNLPCoNLL)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Supervised within-document event coreference using information propagation", "author": ["Z. Liu", "J. Araki", "E. Hovy", "T. Mitamura"], "venue": "Proceedings of the International Conference on Language Resources and Evaluation (LREC 2014)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "On coreference resolution performance metrics", "author": ["X. Luo"], "venue": "Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP-2005)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Dbpedia spotlight: shedding light on the web of documents", "author": ["P.N. Mendes", "M. Jakob", "A. Gar\u0107\u0131a-Silva", "C. Bizer"], "venue": "Proceedings of the 7th International Conference on Semantic Systems. pp. 1\u20138. ACM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research 12, 2825\u20132830", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Conll2011 shared task: Modeling unrestricted coreference in ontonotes", "author": ["S. Pradhan", "L. Ramshaw", "M. Marcus", "M. Palmer", "R. Weischedel", "N. Xue"], "venue": "Proceedings of CoNLL 2011: Shared Task", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Blanc: Implementing the rand index for coreference evaluation", "author": ["M. Recasens", "E. Hovy"], "venue": "Natural Language Engineering,17, (4). pp. 485\u2013510", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations (2013) 22 Vossen and Cybulska: Identity and Granularity of Events in Text", "author": ["N. UzZaman", "H. Llorens", "L. Derczynski", "M. Verhagen", "J. Allen", "J. Pustejovsky"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "A model theoretic coreference scoring scheme", "author": ["M. Vilain", "J. Burger", "J. Aberdeen", "D. Connolly", "L. Hirschman"], "venue": "Proceedings of MUC-6", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1995}, {"title": "Toward a truly multilingual global wordnet grid", "author": ["P. Vossen", "F. Bond", "J. McCrae"], "venue": "Proceedings of the 8th Global Wordnet Conference", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "A hierarchical distance-dependent bayesian model for event coreference resolution", "author": ["B. Yang", "C. Cardie", "P.I. Frazier"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "In [12], we therefore described a model to measure identity across events as a function of the similarity of the event components.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In [15] a bag of events approach is suggested to group event components per source article and compare these across different articles.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "However, the implementation of the bag of events approach described in [15] was only tested on true mentions and did not consider the extraction of events from text.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "Furthermore, in the data set used in the experiments reported in [15] only a few sentences per news article are annotated, which means that only little information could be aggregated across these sentences.", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "In this paper, we describe a new implementation of the bag of events model proposed in [15] which completely processes news articles starting from raw text.", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "The method employed here makes a distinction between mentions within an article and across articles on the one hand and event instances that are stable across these mentions on the other hand [17].", "startOffset": 192, "endOffset": 196}, {"referenceID": 18, "context": "[19], Bagga and Baldwin [3], Ahn [1], or Chen and Ji [11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[19], Bagga and Baldwin [3], Ahn [1], or Chen and Ji [11].", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "[19], Bagga and Baldwin [3], Ahn [1], or Chen and Ji [11].", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "[19], Bagga and Baldwin [3], Ahn [1], or Chen and Ji [11].", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "One of them is the unsupervised approach of Bejan and Harabagiu [5] which relies on lexical, POS, event class and WordNet features as well as feature combinations.", "startOffset": 64, "endOffset": 67}, {"referenceID": 23, "context": "[24] and the most recent feature-rich model of Yang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Our method is similar to the approach of [24] in that it facilitates propagation of information about event participants between event mentions but our heuristic does this more globally by employing an instance level of event representation that aggregates this information at the document level as is also done [34].", "startOffset": 41, "endOffset": 45}, {"referenceID": 32, "context": "Our method is similar to the approach of [24] in that it facilitates propagation of information about event participants between event mentions but our heuristic does this more globally by employing an instance level of event representation that aggregates this information at the document level as is also done [34].", "startOffset": 312, "endOffset": 316}, {"referenceID": 32, "context": "We differ from [34] in that we use an RDF representation to do a logical comparison, whereas they do clustering using document level features.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "[23], by making a distinction between specific entity types, whereas [23] disregard entity type information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23], by making a distinction between specific entity types, whereas [23] disregard entity type information.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "These five slots correspond to different elements of event information such as the action slot (or event trigger following the ACE ([21]) terminology) and four kinds of event arguments: time, location, human and non-human participant slots (see [13]).", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "These five slots correspond to different elements of event information such as the action slot (or event trigger following the ACE ([21]) terminology) and four kinds of event arguments: time, location, human and non-human participant slots (see [13]).", "startOffset": 245, "endOffset": 249}, {"referenceID": 4, "context": "The next quote shows an excerpt from topic one, text number seven of the ECB corpus ([5]).", "startOffset": 85, "endOffset": 88}, {"referenceID": 13, "context": "model has been employed to annotate the ECB+ data set ([14]) which is used in the experiments with event coreference described in the following sections.", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "The EventCorefBank (ECB, [5]) was developed to test cross-document event coreference resolution.", "startOffset": 25, "endOffset": 28}, {"referenceID": 13, "context": "The ECB+ corpus [14] is an extended and re-annotated version of the ECB.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "It is pretty much common practice to use information coming from event arguments for event coreference resolution ([19], [11], [10], [9], [5], [23], [12], [24] among others).", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "It is pretty much common practice to use information coming from event arguments for event coreference resolution ([19], [11], [10], [9], [5], [23], [12], [24] among others).", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "It is pretty much common practice to use information coming from event arguments for event coreference resolution ([19], [11], [10], [9], [5], [23], [12], [24] among others).", "startOffset": 127, "endOffset": 131}, {"referenceID": 8, "context": "It is pretty much common practice to use information coming from event arguments for event coreference resolution ([19], [11], [10], [9], [5], [23], [12], [24] among others).", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "It is pretty much common practice to use information coming from event arguments for event coreference resolution ([19], [11], [10], [9], [5], [23], [12], [24] among others).", "startOffset": 138, "endOffset": 141}, {"referenceID": 22, "context": "It is pretty much common practice to use information coming from event arguments for event coreference resolution ([19], [11], [10], [9], [5], [23], [12], [24] among others).", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "It is pretty much common practice to use information coming from event arguments for event coreference resolution ([19], [11], [10], [9], [5], [23], [12], [24] among others).", "startOffset": 149, "endOffset": 153}, {"referenceID": 23, "context": "It is pretty much common practice to use information coming from event arguments for event coreference resolution ([19], [11], [10], [9], [5], [23], [12], [24] among others).", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "As pointed out by [19] it could be the case however that a lacking piece of information might be available elsewhere within discourse borders.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "The bag of events approach (for details see [15]) translates the structure of event descriptions into event templates for event coreference resolution.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "In the following section 5 we will briefly look at experiments with implementations as oneand as two-step classification tasks (the two-step implementation is described in detail in [15]).", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "The specifics of the implementation of the two-step approach can be found in [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "For coreference experiments on true mentions we used a subset of ECB+ annotations (based on a list of 1840 selected sentences, [14]), that were additionally reviewed with focus on coreference relations.", "startOffset": 127, "endOffset": 131}, {"referenceID": 5, "context": "We used tools from the Natural Language Toolkit ([6], NLTK version 2.", "startOffset": 49, "endOffset": 52}, {"referenceID": 26, "context": "For machine learning experiments we used scikit-learn ([27]).", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "Table 7 presents baselines\u2019 results in terms of recall (R), precision (P) and F-score (F) by employing the following metrics: MUC ([31]), B3 ([2]), CEAF ([25]), BLANC ([29]), and CoNLL F1 ([28]).", "startOffset": 131, "endOffset": 135}, {"referenceID": 1, "context": "Table 7 presents baselines\u2019 results in terms of recall (R), precision (P) and F-score (F) by employing the following metrics: MUC ([31]), B3 ([2]), CEAF ([25]), BLANC ([29]), and CoNLL F1 ([28]).", "startOffset": 142, "endOffset": 145}, {"referenceID": 24, "context": "Table 7 presents baselines\u2019 results in terms of recall (R), precision (P) and F-score (F) by employing the following metrics: MUC ([31]), B3 ([2]), CEAF ([25]), BLANC ([29]), and CoNLL F1 ([28]).", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "Table 7 presents baselines\u2019 results in terms of recall (R), precision (P) and F-score (F) by employing the following metrics: MUC ([31]), B3 ([2]), CEAF ([25]), BLANC ([29]), and CoNLL F1 ([28]).", "startOffset": 168, "endOffset": 172}, {"referenceID": 27, "context": "Table 7 presents baselines\u2019 results in terms of recall (R), precision (P) and F-score (F) by employing the following metrics: MUC ([31]), B3 ([2]), CEAF ([25]), BLANC ([29]), and CoNLL F1 ([28]).", "startOffset": 189, "endOffset": 193}, {"referenceID": 28, "context": "When discussing event coreference scores must be noted that some of the commonly used metrics depend on the evaluation data set, with scores going up or down with the number of singleton items in the data [29].", "startOffset": 205, "endOffset": 209}, {"referenceID": 17, "context": "Our instance-based model follows the Simple Event Model (SEM, [18]).", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "We extended SEM with the Grounded Annotation Framework (GAF, [17]) to link each instance URI to mentions in the source documents.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "synsets [16], their hypernyms and the FrameNet frames associated with the events [4].", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "synsets [16], their hypernyms and the FrameNet frames associated with the events [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 31, "context": "org/ili [33].", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "This lumping is mainly the result of wrong links coming from DBpedia spotlight [26] which gives preference to more popular entities.", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "We first group all mentions of the same lemma and next we determine the similarity across lemmas on the basis of the WordNet similarity scores [22] of their WordNet senses with the highest WSD score.", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "PropBank [20] A0, A1, A2, or the role can be ignored.", "startOffset": 9, "endOffset": 13}, {"referenceID": 32, "context": "(2015) [34], who report the best results for event coreference resolution system mentions for ECB+ and who also compare their results to other systems that have so far only been tested on ECB and not on ECB+.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "Yang et al use a distance-dependent Chinese Restaurant Process (DDCRP [8]), which is an infinite clustering model that can account for data dependencies.", "startOffset": 70, "endOffset": 73}, {"referenceID": 13, "context": "also report on a lemma-baseline as proposed by Cybulska and Vossen (2014) [14], where all event mentions with the same lemma within and across documents are simply joined in a single coreference set.", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "[34] for state-of-the-art machine learning systems as compared to various NewsReader based systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "both Yang\u2019s HDDCRP and the lemma baseline outperform NWR-X-YAc30p3 by 14 and 9 points respectively in CoNLL F1 score [28].", "startOffset": 117, "endOffset": 121}, {"referenceID": 6, "context": "99%, where events are detected by the MATE tool [7] which is trained on PropBank data.", "startOffset": 48, "endOffset": 51}, {"referenceID": 29, "context": "To measure this impact on the actual eventcoreference, we created two other variants of the NewsReader system: 1) NWR-T replaces the MATE event detection by a CRF classifier trained with SemEval 2013 - TempEval3 gold data [30] and 2) NWR-G uses the true mentions of the ECB+ annotation as the events (Gold).", "startOffset": 222, "endOffset": 226}], "year": 2017, "abstractText": "In this paper we describe a method to detect event descriptions in different news articles and to model the semantics of events and their components using RDF representations. We compare these descriptions to solve a cross-document event coreference task. Our component approach to event semantics defines identity and granularity of events at different levels. It performs close to state-of-the-art approaches on the cross-document event coreference task, while outperforming other works when assuming similar quality of event detection. We demonstrate how granularity and identity are interconnected and we discuss how semantic anomaly could be used to define differences between coreference, subevent and topical relations.", "creator": "LaTeX with hyperref package"}}}