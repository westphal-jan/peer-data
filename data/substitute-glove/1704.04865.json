{"id": "1704.04865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking", "abstract": "Traditional implements adversarial networks (GAN) and fewer of bringing phonetically each specialized by considerations through KL them JS - divergence shock that measures be close the initial specific distribution so recently full why specific availability. A year enter called the WGAN established on Wasserstein around well infrastructure, their KL this JS - implies based GANs, few alleviate long formula_2 unobserved, instability, and definition collapsed issues reason them form day the GAN specialists. In this work, really enabling at improving on given WGAN although first misinterpretations larger discriminator loss only next measure - include taken, would with bring a better discriminator, different in own own done storage, but then carrying only a advocate training dynamic murders instance GANs continue affect time the greater loss delegate loss so that this GAN began later trials that encouraging upon week breakthrough. We call appears method Gang of GANs (GoGAN ). We rarely so theoretically that the implemented GoGAN can offset now dramatically between first true web functions the came revenues signal distribution following today 15 half in in calibrated trained WGAN. We did this initiative was with think of temperature GAN provides instead is corp. on lacks ayp scheduling. We others researchers own synthesis one on measurement datasets: CelebA, LSUN Bedroom, CIFAR - 32, and 50K - SSFF, and. those one visual are research improvement back baseline WGAN.", "histories": [["v1", "Mon, 17 Apr 2017 04:42:56 GMT  (8407kb,D)", "http://arxiv.org/abs/1704.04865v1", "16 pages. 11 figures"]], "COMMENTS": "16 pages. 11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["felix juefei-xu", "vishnu naresh boddeti", "marios savvides"], "accepted": false, "id": "1704.04865"}, "pdf": {"name": "1704.04865.pdf", "metadata": {"source": "CRF", "title": "Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking", "authors": ["Felix Juefei-Xu", "Vishnu Naresh Boddeti", "Marios Savvides"], "emails": ["felixu@cmu.edu", "vishnu@msu.edu", "msavvid@ri.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Generative approaches can learn from the tremendous amount of data around us and generate new instances that are like the data they have observed, in any domain. This line of research is extremely important because it has the potential to provide meaningful insight into the physical world we human beings can perceive. Take visual perception for instance, the generative models have much smaller number of parameters than the amount of visual data out there in the world, which means that in order for the generative models to come up with new instances that are like the actual\ntrue data, they have to search for intrinsic pattern and distill the essence. We can in turn capitalize on that and make machines understand, describe, and model the visual world better. Recently, three classes of algorithms have emerged as successful generative approaches to model the visual data in an unsupervised manner.\nVariational autoencoders (VAEs) [17] formalize the generative problem in the framework of probabilistic graphical models where we are to maximize a lower bound on the log likelihood of the training data. The probabilistic graphical models with latent variables allow us to perform both learning and Bayesian inference efficiently. By projecting into a learned latent space, samples can be reconstructed from that space. The VAEs are straightforward to train but at the cost of introducing potentially restrictive assumptions about the approximate posterior distribution. Also, their generated samples tend to be slightly blurry. Autoregressive models such as PixelRNN [32] and PixelCNN [33] get rid of the latent variables and instead directly model the conditional distribution of every individual pixel given previous pixels starting from top-left corner. PixelRNN/CNN have a stable training process via softmax loss and currently give the best log likelihoods on the generated data, which is an indicator of high plausibility. However, they are relatively inefficient during sampling and do not easily provide simple low-dimensional latent codes for images.\nGenerative adversarial networks (GANs) [10] simultaneously train a generator network for generating realistic images, and a discriminator network for distinguishing between the generated images and the samples from the training data (true distribution). The two players (generator and discriminator) play a two-player minimax game until Nash equilibrium where the generator is able to generate images as genuine as the ones sampled from the true distribution, and the discriminator is no longer able to distinguish between the two sets of images, or equivalently is guessing at random chance. In the traditional GAN formulation, the generator and the discriminator are updated by receiving gradient signals from the loss induced by observing discrepancies between the two distributions by the discriminator.\n1\nar X\niv :1\n70 4.\n04 86\n5v 1\n[ cs\n.C V\n] 1\n7 A\npr 2\n01 7\nFrom our perspective, GANs are able to generate images with the highest visual quality by far. The image details are sharp as well as semantically sound.\nMotivation: Although we have observed many successes in applying GANs to various scenarios as well as in many GAN variants that come along, there has not been much work dedicated to improving GAN itself from a very fundamental point of view. Ultimately, we are all interested in the end-product of a GAN, which is the image it can generate. Although we are all focusing on the performance of the GAN generator, we must know that its performance is directly affected by the GAN discriminator. In short, to make the generator stronger, we need a stronger opponent, which is a stronger discriminator in this case. Imagine if we have a weak discriminator which does a poor job telling generated images from the true images, it takes only a little effort for the generator to win the two-player minimax game as described in the original work of GAN [10]. To further improve upon the state-of-the-art GAN method, one possible direction is to enforce a maximum margin ranking loss in the optimization of the discriminator, which will result in a stronger discriminator that attends to the fine details of images, and a stronger discriminator helps obtain a stronger generator in the end.\nIn this work, we are focusing on how to further improve the GANs by incorporating a maximum margin ranking criterion in the optimization, and with a progressive training paradigm. We call the proposed method Gang of GANs (GoGAN)1. Our contributions include (1) generalizing on the Wasserstein GAN discriminator loss with a marginbased discriminator loss; (2) proposing a progressive training paradigm involving multiple GANs to contribute to the maximum margin ranking loss so that the GAN at later GoGAN stages will improve upon early stages; (3) showing theoretical guarantee that the GoGAN will bridge the gap between true data distribution and generated data distribution by at least half; and (4) proposing a new quality measure for the GANs through image completion tasks."}, {"heading": "2. Related Work", "text": "In this section, we review recent advances in GAN research as well as many of its variants and related work.\nDeep convolutional generative adversarial networks (DCGAN) [28] are proposed to replace the multilayer perceptron in the original GAN [10] for more stable training, by utilizing strided convolutions in place of pooling layers, and fractional-strided convolutions in place of image upsampling. Conditional GAN [22] is proposed as a variant of GAN by extending it to a conditional model, where both the generator and discriminator are conditioned on some extra\n1Implementation and future updates will be available at http:// xujuefei.com/gogan.\nauxiliary information, such as class labels. The conditioning is performed by feeding the auxiliary information into both the generator and the discriminator as additional input layer. Another variant of GAN is called auxiliary classifier GAN (AC-GAN) [25], where every generated sample has a corresponding class label in addition to the noise. The generator needs both for generating images. Meanwhile, the discriminator does two things: giving a probability distribution over image sources, and giving a probability distribution over the class labels. Bidirectional GAN (BiGAN) [7] is proposed to bridge the gap that conventional GAN does not learn the inverse mapping which projects the data back into the latent space, which can be very useful for unsupervised feature learning. The BiGAN not only trains a generator, but also an encoder that induces a distribution for mapping data point into the latent feature space of the generative model. At the same time, the discriminator is also adapted to take input from the latent feature space, and then predict if an image is generated or from the true distribution. There is a pathway from the latent feature z to the generated data G(z) via the generator G, as well as another pathway from the data x back to the latent feature representation E(x) via the newly added encoder E. The generated image together with the input latent noise (G(z), z), and the true data together with its encoded latent representation (x, E(x)) are fed into the discriminator D for classification. There is a concurrent work proposed in [8] that has the identical model. A sequential variant of the GAN is the Laplacian generative adversarial networks (LAPGAN) [6] model which generates images in a coarse-to-fine manner by generating and upsampling in multiple steps. It is worth mentioning the sequential variant of the VAE is the deep recurrent attentive writer (DRAW) [11] model that generates images by accumulating updates into a canvas using a recurrent network. Built upon the idea of sequential generation of images, the recurrent adversarial networks [15] has been proposed to let the recurrent network to learn the optimal generation procedure by itself, as opposed to imposing a coarse-to-fine structure on the procedure. Introspective adversarial network (IAN) [4] is proposed to hybridize the VAE and the GAN. It leverages the power of the adversarial objective while maintaining the efficient inference mechanism of the VAE. The generative multi-adversarial networks (GMAN) [9] extends the GANs to multiple discriminators. For a fixed generator G, N randomly instantiated copies of the discriminators are utilized to present the maximum value of each value function as the loss for the generator. Requiring the generator to minimize the max forces G to generate high fidelity samples that must hold up under the scrutiny of all N discriminators. Layered recursive generative adversarial networks (LR-GAN) [35] generates images in a recursive fashion. It first generates a background, and then generates a foreground by conditioning on the back-\nground, along with a mask and an affine transformation that together define how the background and foreground should be composed to obtain a complete image. The foregroundbackground mask is estimated in a completely unsupervised way without using any object masks for training. Authors of [24] have shown that the generative-adversarial approach in GAN is a special case of an existing more general variational divergence estimation approach, and that any f - divergence can be used for training generative neural samplers. InfoGAN [5] method is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. A lower bound of the mutual information can be derived and optimized efficiently. Rather than a single unstructured noise vector to be input into the generator, InfoGAN decomposes the noise vector into two parts: a source of incompressible noise z and a latent code c that will target the salient structured semantic features of the data distribution, and the generator thus becomes G(z, c). The authors have added an information-theoretic regularization to ensure there is high mutual information between the latent code c and the generator distribution G(z, c). To strive for a more stable GAN training, the energy-based generative adversarial networks (EBGAN) [38] is proposed which views the discriminator as an energy function that assigns low energy to the regions near the data manifold and higher energy to other regions. The authors have shown one instantiation of EBGAN using an autoencoder architecture, with the energy being the reconstruction error. The boundary-seeking GAN (BGAN) [13] aims at generating samples that lie on the decision boundary of a current discriminator in training at each update. The hope is that a generator can be trained in this way to match a target distribution at the limit of a perfect discriminator. Least squares GAN [21] adopts a least squares loss function for the discriminator, which is equivalent to a multi-class GAN with the `2 loss function. The authors have shown that the objective function yields minimizing the Pearson \u03c72 divergence. The stacked GAN (SGAN) [14] consists of a top-down stack of GANs, each trained to generate plausible lower-level representations, conditioned on higher-level representations. Discriminators are attached to each feature hierarchy to provide intermediate supervision. Each GAN of the stack is first trained independently, and then the stack is trained end-to-end.\nPerhaps the most seminal GAN-related work since the inception of the original GAN [10] idea is the Wasserstein GAN (WGAN) [3]. Efforts have been made to fully understand the training dynamics of generative adversarial networks through theoretical analysis [2], which leads to the creation of the WGAN. The two major issues with the original GAN and many of its variants are the vanishing gradient issues and the mode collapse issue. By incorporating a smooth Wasserstein distance metric and objective, as op-\nposed to the KL-divergence and JS-divergence, the WGAN is able to overcome the vanishing gradient and mode collapse issues. WGAN also has made training and balancing between the generator and discriminator much easier in the sense that one can now train the discriminator till optimality, and then gradually improve the generator. Moreover, it provides an indicator (based on the Wasserstein distance) for the training progress, which correlates well with the visual image quality of the generated samples.\nOther applications include cross-domain image generation [30] through a domain transfer network (DTN) which employs a compound of loss functions including a multiclass GAN loss, an f -constancy component, and a regularization component that encourages the generator to map samples from the target domain to themselves. The imageto-image translation approach [16] is based on conditional GAN, and learns a conditional generative model for generating a corresponding output image at a different domain, conditioned on an input image. The image superresolution GAN (SRGAN) [19] combines both the image content loss and the adversarial loss for recovering highresolution counterpart of the low-resolution input image. The plug and play generative networks (PPGN) [23] is able to produce high quality images at higher resolution for all 1000 ImageNet categories. It is composed of a generator that is capable of drawing a wide range of image types, and a replaceable condition network that tells the generator what to draw, hence plug and play."}, {"heading": "3. Proposed Method: Gang of GANs", "text": "In this section we will review the original GAN [10] and its convolutional variant DCGAN [28]. We will then analyze how to further improve the GAN model with WGAN [3], and introduce our Gang of GANs (GoGAN) method."}, {"heading": "3.1. GAN and DCGAN", "text": "The GAN [10] framework trains two networks, a generator G\u03b8(z) : z \u2192 x, and a discriminator D\u03c9(x) : x \u2192 [0, 1]. G maps a random vector z, sampled from a prior distribution pz(z), to the image space. D maps an input image to a likelihood. The purpose of G is to generate realistic images, while D plays an adversarial role to discriminate between the image generated from G, and the image sampled from data distribution pdata. The networks are trained by optimizing the following minimax loss function: min\nG max D V (G,D) = Ex\u223cpdata(x)[log(D(x))] + Ez\u223cpz(z)[log(1\u2212D(G(z))] where x is the sample from the pdata distribution; z is randomly generated and lies in some latent space. There are many ways to structure G(z). The DCGAN [28] uses fractionally-strided convolutions to upsample images instead of fully-connected neurons as shown in Figure 1. The generator G is updated to fool the dis-\ncriminator D into wrongly classifying the generated sample, G(z), while the discriminator D tries not to be fooled. Here, both G and D are deep convolutional neural networks and are trained with an alternating gradient descent algorithm. After convergence, D is able to reject images that are too fake, and G can produce high quality images faithful to the training distribution (true distribution pdata)."}, {"heading": "3.2. Wasserstein GAN and Improvement over GAN", "text": "In the original GAN, Goodfellow et al. [10] have proposed the following two loss functions for the generator: Ez\u223cPz(z)[log(1\u2212D(G(z)))] andEz\u223cPz(z)[\u2212 logD(G(z))]. The latter one is referred to as the \u2212 logD trick [10, 2, 3].\nUnfortunately, both forms can lead to potential issues in training the GAN. In short, the former loss function can lead to gradient vanishing problem, especially when the discriminator is trained to be very strong. The real image distribution Pr and the generated image distribution Pg have support contained in two closed manifolds that don\u2019t perfectly align and don\u2019t have full dimension. When the discriminator is near optimal, minimizing the loss of the generator is equivalent to minimizing the JS-divergence between Pr and Pg , but due to the aforementioned reasons, the JSdivergence will always be a constant 2 log 2, which allows the existence of an optimal discriminator to (almost) perfectly carve the two distributions, i.e., assigning probability 1 to all the real samples, and 0 to all the generated ones, which renders the gradient of the generator loss to go to 0.\nFor the latter case, it can be shown that minimizing the loss function is equivalent to minimizing KL(Pg\u2016Pr) \u2212 2JS(Pr\u2016Pg), which leads to instability in the gradient because it simultaneous tries to minimize the KL-divergence and maximize the JS-divergence, which is a less ideal loss function design. Even the KL term by itself has some issues. Due to its asymmetry, the penalty for two types of errors is quite different. For example, when Pg(x) \u2192 0 and Pr(x) \u2192 1, we have Pg(x) log Pg(x)Pr(x) \u2192 0, which has almost 0 contribution to KL(Pg\u2016Pr). On the other hand, when Pg(x) \u2192 1 and Pr(x) \u2192 0, we have Pg(x) log\nPg(x) Pr(x) \u2192 +\u221e, which has gigantic contribution\nto KL(Pg\u2016Pr). So the first type of error corresponds to that the generator fails to produce realistic samples, which has tiny penalty, and the second type of error corresponds to that the generator produces unrealistic samples, which has enormous penalty. Under this reality, the generator would rather produce repetitive and \u2018safe\u2019 samples, than samples with high diversity with the risk of triggering the second type of error. This causes the infamous mode collapse.\nWGAN [2, 3] avoids the gradient vanishing and mode collapse issues in the original GAN and many of its variants by adopting a new distance metric: the Wasserstein-1 distance, or the earth-mover distance as follows:\nW (Pr,Pg) = inf \u03b3\u2208\u0393(Pr,Pg)\nE(x,y)\u223c\u03b3 [\u2016x\u2212 y\u2016] (1)\nwhere \u0393(Pr,Pg) is the set of all joint distributions \u03b3(x, y) whose marginals are Pr and Pg respectively. One of the biggest advantages of the Wasserstein distance over KL and JS-divergence is that it is smooth, which is very important in providing meaningful gradient information when the two distributions have support contained in two closed manifolds that don\u2019t perfectly aligned don\u2019t have full dimension, in which case KL and JS-divergence would fail to provide gradient information successfully. However, the infimum inf\u03b3\u2208\u0393(Pr,Pg) is highly intractable. Thanks to the Kantorovich-Rubinstein duality [34], the Wasserstein distance becomes: W (Pr,Pg) = sup\u2016f\u2016L\u22641Ex\u223cPr [f(x)] \u2212 Ex\u223cPg [f(x)], where the supremum is over all the 1- Lipschitz functions. Therefore, we can have a parameterized family of functions {fw}w\u2208W that are K-Lipschitz for some K, and the problem we are solving now becomes: maxw:|fw|L\u2264K Ex\u223cPr [fw(x)]\u2212Ez\u223cp(z)[fw(g\u03b8(x))] \u2248 K \u00b7 W (Pr,Pg). Let the fw (discriminator) be a neural network with weights w, and maximize L = Ex\u223cPr [f(x)] \u2212 Ex\u223cPg [f(x)] as much as possible so that it can well approximate the actual Wasserstein distance between real data distribution and generated data distribution, up to a multiplicative constant. On the other hand, the generator will try to minimize L, and since the first term in L does not concern the generator, its loss function is to minimize \u2212Ex\u223cPg [f(x)], and the loss function for the discriminator is to minimize Ex\u223cPg [f(x)]\u2212 Ex\u223cPr [f(x)] = \u2212L."}, {"heading": "3.3. Gang of GANs (GoGAN)", "text": "In this section, we will discuss our proposed GoGAN method which is a progressive training paradigm to improve the GAN, by allowing GANs at later stages to contribute to a new ranking loss function that will improve the GAN performance further. Also, at each GoGAN stage, we generalize on the WGAN discriminator loss, and arrive at a marginbased discriminator loss, and we call the network margin GAN (MGAN). The entire GoGAN flowchart is shown in Figure 2, and we will introduce the components involved.\nBased on the previous discussion, we have seen that WGAN has several advantages over the traditional GAN. Recall that Dwi(x) and Dwi(G\u03b8i(z)) are the discriminator score for the real image x and generated image G\u03b8i(z) in Stage-(i + 1) GoGAN. In order to further improve it, we have proposed a margin-based WGAN discriminator loss:\nLdisc = [Dwi+1(G\u03b8i+1(z)) + \u2212Dwi+1(x)]+ (2)\nwhere [x]+ = max(0, x) is the hinge loss. This MGAN loss function is a generalization of the discriminator loss in WGAN. When the margin \u2192 \u221e, this loss becomes WGAN discriminator loss.\nThe intuition behind the MGAN loss is as follows. WGAN loss treats a gap of 10 or 1 equally and it tries to increase the gap even further. The MGAN loss will focus on increasing separation of examples with gap 1 and leave the samples with separation 10, which ensures a better discriminator, hence a better generator. We will see next that the MGAN loss can be extended even further by incorporating margin-based ranking when go beyond a single MGAN.\nRankerR: When going from Stage-i GoGAN to Stage(i + 1) GoGAN, we incorporate a margin-based ranking loss in the progressive training of the GoGAN for ensuring that the generated images from later GAN training stage is better than those from previous stages. The idea is fairly straight-forward: the discriminator scores coming from the generated images at later stages should be ranked closer to that of the images sampled from the true distribution. The ranking loss is:\nLrank = [Dwi(G\u03b8i(z)) + 2 \u2212Dwi+1(x)]+ (3)\nCombing (2) and (3), the Ldisc and Lrank loss together are equivalent to enforcing the following ranking strategy. Notice that such ranking constraint only happens between adjacent GoGAN pairs, and it can be easily verified that it has intrinsically established an ordering among all the MGANs involved, which will be further discussed in Section 4.\nDwi+1(x) \u2265 Dwi+1(G\u03b8i+1(z)) + (4) Dwi+1(x) \u2265 Dwi(G\u03b8i(z)) + 2 (5)\nThe weights of the rankerR and the discriminator D are tied together. Conceptually, from Stage-2 and onward, the ranker is just the discriminator which takes in extra ranking loss in addition to the discriminator loss already in place for the MGAN. In Figure 2, the ranker is a separate block, but only for illustrative purpose. Different training stages are encircled by green dotted lines with various transparency levels. The purple solid lines show the connectivity within the GoGAN, with various transparency levels in accordance with the progressive training stages. The arrows on both ends of the purple lines indicate forward and backward pass of the information and gradient signal. If the entire GoGAN is trained, the ranker will have achieved the following desired goal: R(G1(z)) R(G2(z)) R(G3(z)) \u00b7 \u00b7 \u00b7 R(GK(z)) R(x), where indicates relative ordering. The total loss for GoGAN can be written as: LGoGAN = \u03bb1 \u00b7 Ldisc + \u03bb2 \u00b7 Lrank, where weighting parameters \u03bb1 and \u03bb2 controls the relative strength."}, {"heading": "4. Theoretical Analysis", "text": "In WGAN [3], the following loss function involving the weights updating of the discriminator and the generator is\na good indicator of the EM distance during WGAN training: maxw\u2208W Ex\u223cPr [Dw(x)] \u2212 Ez\u223cpz [Dw(G\u03b8(x))]. This loss function is essentially the Gap \u0393 between real data distribution and generated data distribution, and of course the discriminator is trying to push the gap larger. The realization of this loss function for one batch is as follows:\nGap = \u0393 = 1\nm m\u2211 i=1 Dw(x(i))\u2212 1 m m\u2211 i=1 Dw(G\u03b8(z(i))) (6)\nTheorem 4.1. GoGAN with ranking loss (3) trained at its equilibrium will reduce the gap between the real data distribution Pr and the generated data distribution P\u03b8 at least by half for Wasserstein GAN trained at its optimality.\nProof. Let D\u2217w1 and G \u2217 \u03b81 be the optimally trained discriminator and generator for the original WGAN (Stage-1 GoGAN). Let D\u2217w2 and G \u2217 \u03b82\nbe the optimally trained discriminator and generator for the Stage-2 GoGAN in the proposed progressive training framework.\nThe gap between real data distribution and the generated data distribution for Stage-1 to Stage-N GoGAN is:\n\u03931 = 1\nm m\u2211 i=1 D\u2217w1(x (i))\u2212 1 m m\u2211 i=1 D\u2217w1(G \u2217 \u03b81(z (i))) (7)\n\u0393N = 1\nm m\u2211 i=1 D\u2217wN (x (i))\u2212 1 m m\u2211 i=1 D\u2217wN (G \u2217 \u03b8N (z (i))) (8)\nLet us first establish the relationship between gap \u03931 and gap \u03932, and then extends to the \u0393N case.\nAccording to the ranking strategy, we enforce the following ordering:\nD\u2217w2(x (i)) > D\u2217w2(G \u2217 \u03b82(z (i))) > D\u2217w1(G \u2217 \u03b81(z (i))) (9)\nwhich means that\nD\u2217w2(x (i))\u2212D\u2217w2(G \u2217 \u03b82(z (i))) < D\u2217w2(x (i))\u2212D\u2217w1(G \u2217 \u03b81(z (i)))\nOn the left hand side, it is the new gap from Stage-2 GoGAN for one image, and for the the whole batch, this relationship follows:\n\u03932 = 1\nm m\u2211 i=1 [ D\u2217w2(x (i))\u2212D\u2217w2(G \u2217 \u03b82(z (i))) ]\n(10)\n< 1\nm m\u2211 i=1 [ D\u2217w2(x (i))\u2212D\u2217w1(G \u2217 \u03b81(z (i))) ]\n(11)\n= 1\nm m\u2211 i=1 [ D\u2217w2(x (i))\u2212D\u2217w1(x (i)) ] \ufe38 \ufe37\ufe37 \ufe38\n\u03b71\n+ 1\nm m\u2211 i=1 [ D\u2217w1(x (i))\u2212D\u2217w1(G \u2217 \u03b81(z (i))) ]\n\ufe38 \ufe37\ufe37 \ufe38 \u03931\n(12)\nTherefore, we have 0 < \u03932 < \u03be1 + \u03931, where the term \u03be1 can be positive, negative, or zero. But only when \u03be1 \u2264 0, the relation \u03932 < \u03931 can thus always hold true. In other words, according to the ranking strategy, we have a byproduct relation \u03be1 \u2264 0 established, which is equivalent to the following expressions:\n\u03be1 = 1\nm m\u2211 i=1 [ D\u2217w2(x (i))\u2212D\u2217w2(x (i)) ] \u2264 0 (13)\n1\nm m\u2211 i=1 D\u2217w1(x (i)) \u2265 1 m m\u2211 i=1 D\u2217w2(x (i)) (14)\nCombing relations (9) and (14), we can arrive at the new ordering:\n1\nm m\u2211 i=1 D\u2217w1(x (i)) \u2265 1 m m\u2211 i=1 D\u2217w2(x (i)) >\n1\nm m\u2211 i=1 D\u2217w2(G \u2217 \u03b82(z (i))) > 1 m m\u2211 i=1 D\u2217w1(G \u2217 \u03b81(z (i))) (15)\nNotice the nested ranking strategy as a result of the derivation. Therefore, when going from Stage-2 to Stage-3 GoGAN, similar relationship can be obtained (for notation simplification, we drop the (i) super script and use bar to represent average over m instances):\nD\u2217w2(x) \u2265 D \u2217 w3(x) > D \u2217 w3(G \u2217 \u03b83(z)) > D \u2217 w2(G \u2217 \u03b82(z)) (16)\nwhich is equivalent to the following expression when considering the already-existing relationship from Stage-1 to Stage-2 GoGAN:\nD\u2217w1(x) \u2265 D \u2217 w2(x) \u2265 D \u2217 w3(x) >\nD\u2217w3(G \u2217 \u03b83(z)) > D \u2217 w2(G \u2217 \u03b82(z)) > D \u2217 w1(G \u2217 \u03b82(z)) (17)\nSimilar ordering can be established for all the way to StageN GoGAN. Let us assume that the distance between the first and last term: D\u2217w1(x) and D \u2217 w1(G \u2217 \u03b82\n(z)) is \u03b2 which is finite, as shown in Figure 3. Let us also assume that the distance betweenD\u2217wi(x) andD \u2217 wi+1(x) is \u03b7i, and the distance between D\u2217wi+1(G \u2217 \u03b8i+1 (z)) and D\u2217wi(G \u2217 \u03b8i\n(z)) is \u03d5i. Extending the pairwise relationship established by the ranker in (4, 5) to the entire batch, we will have equal margins between the terms D\u2217wi+1(x), D \u2217 wi+1(G \u2217 \u03b8i+1 (z)), and D\u2217wi(G \u2217 \u03b8i\n(z)); and the margin between D\u2217wi+1(x) and D\u2217wi(x) remains flexible.\nTherefore, we can put the corresponding terms in order as shown in Figure 3, with the distances between the terms \u03b7i and \u03d5i also showing. The homoscedasticity assumption from the ranker is illustrated by dashed line with the same color. For instance, the distances between adjacent purple dots are the same.\nWe can establish the following iterative relationship:\n\u03d51 = \u03b2 \u2212 \u03b71\n2 , \u03d52 = \u03d51 \u2212 \u03b72 2 , \u03d5N = \u03d5N\u22121 \u2212 \u03b7N 2 (18)\nThe total gap reduction TGR(N + 1) all the way to Stage-(N + 1) GoGAN is: TGR(N + 1) = \u2211N i=1(\u03b7i + \u03d5i). TGR(\u00b7) is an increasing function TGR(N + 1) > TGR(N), and we have:\nTGR(N + 1) > TGR(2) = \u03b71 + \u03d51 = \u03b71 + 1\n2 (\u03b2 \u2212 \u03b71)\n= \u03b2 2 + \u03b71 2 > \u03b2 2 (19)\nTherefore, GoGAN with ranking loss (3) trained at its equilibrium will reduce the gap between the real data distribution and the generated data distribution at least by half for Wasserstein GAN trained at its optimality.\nCorollary 4.2. The total gap reduction up to Stage-(N+1) GoGAN is equal to \u03b2 \u2212 \u03d5N .\nProof. Recall the iterative relation from (18):\n\u03d5N = 1\n2 \u03d5N\u22121 \u2212\n1 2 \u03b7N (20)\n\u21d2 2\u03d5N + \u03b7N = \u03d5N\u22121 (21)\nCombining (20) and (21), we can have the following:\n\u03d5N + \u03b7N = 1\n2 \u03d5N\u22121 +\n1 2 \u03b7N (22)\n\u03d5N\u22121 + \u03b7N\u22121 = 1\n2 \u03d5N\u22122 +\n1 2 \u03b7N\u22121 (23)\n\u00b7 \u00b7 \u00b7\n\u03d52 + \u03b72 = 1\n2 \u03d51 +\n1 2 \u03b72 (24)\nSumming up all the LHS and RHS gives (notice the changes in lower and upper bound of summation):\nN\u2211 i=2 (\u03d5i + \u03b7i) = 1 2 N\u22121\u2211 i=1 \u03d5i + 1 2 N\u2211 i=2 \u03b7i (25)\nN\u2211 i=1 (\u03d5i + \u03b7i) = 1 2 N\u22121\u2211 i=1 \u03d5i + 1 2 N\u2211 i=2 \u03b7i + (\u03d51 + \u03b71) (26)\nN\u2211 i=1 (\u03d5i + \u03b7i) = 1 2 N\u2211 i=1 \u03d5i + 1 2 N\u2211 i=1 \u03b7i + (\u03d51 + \u03b71)\n\u2212 1 2 \u03d5N \u2212 1 2 \u03b71 (27)\nTGR(N + 1) = 1 2 TGR(N + 1)\u2212 \u03d5N 2 + \u03b71 2 + \u03d51 (28) TGR(N + 1) = (2\u03d51 + \u03b71)\u2212 \u03d5N = \u03b2 \u2212 \u03d5N (29)\nTherefore, the total gap reduction up to Stage-(N + 1) GoGAN is equal to \u03b2 \u2212 \u03d5N ."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Evaluating GANs via Image Completion Tasks", "text": "There hasn\u2019t been a universal metric to quantitatively evaluate the GAN performance, and often times, we rely on visual examination. This is largely because of the lack of an objective function: what are the generated images gonna be compared against, since there is no corresponding groundtruth images for the generated ones? These are the questions needed to be addressed.\nDuring the WGAN training, we have seen a successful gap indicator that correlates well with image quality. However, it is highly dependent on the particular WGAN model it is based on, and it will be hard to fairly evaluate generated image quality across different WGAN models. We need a metric that is standalone and do not depend on the models.\nPerhaps, the Inception score [29] is by far the best solution we have. The score is based on pretrained Inception model. Generated images are pass through the model and those containing meaningful objects will have a conditional label distribution p(y|x) with low entropy. At the same time, the marginal \u222b p(y|x = G(z))dz should have high entropy because we expect the GAN to generate varied images. However, we argue that the Inception score will be biased towards the seen objects during the Inception model training, and it measures more of the \u201cobjectness\u201d in the generated images, rather than the \u201crealisticity\u201d the GAN is intended to strive towards.\nIn this work, we propose a new way to evaluate GAN performance. It is simple and intuitive. We ask the GANs to carry out image completion tasks [36], and the GAN performance is measured by the fidelity (PSNR, SSIM) of the completed image against its ground-truth. There are several advantages: (1) this quality measure works on image level, rather than on the image distribution; (2) the optimization in the image completion procedure utilizes both the generator and the discriminator of the trained GAN, which is a direct indicator of how good the GAN model is; (3) having 1-vs-1 comparison between the ground-truth and the completed image allows very straightforward visual examination of the GAN quality, and also allows head-to-head comparison between various GANs; (4) this is a direct measure of the \u201crealisticity\u201d of the generated image, and also the diversity. Imagine a mode collapse situation happens, the generated images would be very different from the groundtruth images since the latter ones are diverse."}, {"heading": "5.2. Details on the Image Completion Tasks", "text": "As discussed above, we propose to use the image completion tasks as a quality measure for various GAN models. In short, the quality of the GAN models can be quantitatively measured by the image completion fidelity, in terms of PSNR and SSIM. The motivation is that the image com-\npletion tasks require both the discriminator D and the generator G to work well in order to reach high quality image completion results, as we will see next.\nTo take on the missing data challenge such as the image completion tasks, we need to utilize both the G and D networks from the GoGAN (and its benchmark WGAN), pre-trained with uncorrupted data. After training, G is able to embed the images from pdata onto some non-linear manifold of z. An image that is not from pdata (e.g. images with missing pixels) should not lie on the learned manifold. Therefore, we seek to recover the \u201cclosest\u201d image on the manifold to the corrupted image as the proper image completion. Let us denote the corrupted image as y. To quantify the \u201cclosest\u201d mapping from y to the reconstruction, we define a function consisting of contextual loss and perceptual loss, following the work of Yeh et al. [36].\nThe contextual loss is used to measure the fidelity between the reconstructed image portion and the uncorrupted image portion, which is defined as:\nLcontextual(z) = \u2016M G(z)\u2212M y\u20161 (30)\nwhere M is the binary mask of the uncorrupted region and denotes the Hadamard product operation.\nThe perceptual loss encourages the reconstructed image to be similar to the samples drawn from the training set (true distribution pdata). This is achieved by updating z to fool D, or equivalently to have a small gap between D(x) and D(G(z)), where x is sampled from the real data distribution. As a result, D will predict G(z) to be from the real data with a high probability. The same loss for fooling D as in WGAN and the proposed GoGAN is used:\nLperceptual(z) = D(x)\u2212D(G(z)) (31)\nThe corrupted image with missing pixels can now be mapped to the closest z in the latent representation space with the defined perceptual and contextual losses. z is updated using back-propagation with the total loss:\nz\u0302 = arg min z\n(Lcontextual(z) + \u03bbLperceptual(z)) (32)\nwhere \u03bb (set to \u03bb = 0.1 in our experiments) is a weighting parameter. After finding the optimal solution z\u0302, the image completion ycompleted can be obtained by:\nycompleted = M y + (1\u2212M) G(z\u0302) (33)"}, {"heading": "5.3. Methods to be Evaluated and Dataset", "text": "The WGAN baseline uses the Wasserstein discriminator loss [3]. The MGAN uses margin-based discriminator loss function discussed in Section 3.3. It is exactly the Stage1 GoGAN, which is a baseline for subsequent GoGAN stages. Stage-2 GoGAN incorporates margin-based ranking loss discussed in Section 3.3. These 3 methods will be evaluated on three large-scale visual datasets.\nThe CelebA dataset [20] is a large-scale face attributes dataset with more than 200K celebrity images. The images in this dataset cover large pose variations and background clutter. The dataset includes 10,177 number of subjects, and 202,599 number of face images. We pre-process and align the face images using dLib as provided by the OpenFace [1]. The LSUN Bedroom dataset [37] is meant for large-scale scene understanding. We use the bedroom portion of the dataset, with 3,033,042 images. The CIFAR-10 [18] is an image classification dataset containing a total of 60K 32 \u00d7 32 color images, which are across the following 10 classes: airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships, and trucks. The processed image size is 64\u00d7 64, and the training-testing split is 90-10."}, {"heading": "5.4. Training Details of GoGAN", "text": "For all the experiments presented in this paper we use the same generator architecture and parameters. We use the DCGAN [28] architecture for both the generator and the discriminator at all stages of the training. Both the generator and the discriminator are learned using optimizers (RMSprop [31]) that are not based on momentum as recommended in [3] with a learning rate of 5e-5. For learning the model at Stage-2 we initialize it with the model learned from Stage-1. In the second stage the model is updated with the ranking loss while the model from stage one held fixed. Lastly, no data augmentation was used for any of our experiments. Different GoGAN stages are trained with the same number total epochs for fair comparison. We will make our implementation publicly available, so readers can refer to it for more detailed hyper-parameters, scheduling, etc."}, {"heading": "5.5. Results and Discussion", "text": "The GoGAN framework is designed to sequentially train generative models and reduce the gap between the true data\ndistribution the learned generative model. Figure 4 demonstrates this effect of our proposed approach where the gap between the discriminator scores between the true distribution and the generated distribution reduces from Stage-1 to Stage-2. To quantitatively evaluate the efficacy of our approach we consider the task of image completion i.e., missing data imputation through the generative model. This task is evaluated on three different visual dataset by varying the\namount of missing data. We consider five different level of occlusions, occluding the center square region (9%, 25%, 49%, 64%, and 81%) of the image. The image completion task is evaluated by measuring the fidelity between the generated images and the ground-truth images through two metrics: PSNR and SSIM. The results are consolidated in Table 2, 3, and 4 for the 3 datasets respectively. GoGAN consistently outperforms WGAN with the Stage-2 model\nalso demonstrating improvements over the Stage-1 generator. Our results demonstrate that by enforcing a margin based ranking loss, we can learn sequentially better generative models. We also show qualitative image completion results of Stage-1 GoGAN and Stage-2 GoGAN in Table 1."}, {"heading": "5.6. Ablation Studies", "text": "In this section, we provide additional experiments and ablation studies on the proposed GoGAN method, and show its improvement over WGAN. For this set of experiments we collect a single-sample dataset containing 50K frontal face images from 50K individuals, which we call the 50KSSFF dataset. They are sourced from several frontal face datasets including the FRGC v2.0 dataset [27], the MPIE dataset [12], the ND-Twin dataset [26], and mugshot dataset from Pinellas County Sheriff\u2019s Office (PCSO). Training and testing split is 9-1, which means we train on 45K images, and test on the remaining 5K. This dataset is single-sample, which means there is only image of a particular subject throughout the entire dataset. Images are aligned using two anchor points on the eyes, and cropped to 64\u00d7 64.\nOne-shot Learning: Different from commonly used\ncelebrity face dataset such as CelebA [20], our collected 50K-SSFF dataset is dedicated for one-shot learning in the GAN context due to its single-sample nature. We will explore how the proposed GoGAN method performs under the one-shot learning setting. The majority of the singlesample face images in this dataset are PCSO mugshots, and therefore, we draw a black bar on the original and generated images (see Figures 9, 10, 11) for the sake of privacy protection and is not an artifact of the GAN methods studied.\nTraining: The GAN models were trained for 1000 epochs each which corresponds to about 135,000 iterations of generator update for a batch size of 64 images. We used the same DCGAN architecture as in the rest of the experiments in the ablation studies.\nMargin of Separation: Here we study the impact of the choice of margin in the hinge loss. Figure 5 compares the margin of separation as WGAN and Stage-1 GoGAN are trained to optimality. Figure 6 compares the generators through the image completion task with 49% occlusion. Figure 7 compares the generators through the image completion task with 25% occlusion.\nImage Completion with Iterations: Here we show the quality of the image generator as the training proceeds by evaluating the generated models on the image completion task. Figure 8 compares the generators through the image completion task with 25% occlusion.\nQualitative Results: We first show some example real and generated images (64\u00d764) in Figure 9. The real images shown in this picture are used for the image completion task. Figure 10 shows qualitative image completion results with 25% occlusion. Figure 11 shows qualitative image completion results with 49% occlusion.\nQuantitative Results: We compare the quality of the image generators of WGAN, Stage-1 GoGAN and Stage-2 GoGAN through the image completion task. We measure the fidelity of the image completions via PSNR and SSIM. Table 5 shows results for our test set consisting of 5000 test faces, averaged over 10 runs, with 25% and 49% occlusions respectively."}, {"heading": "6. Conclusions", "text": "In order to improve on the WGAN, we first generalize its discriminator loss to a margin-based one, which leads to a better discriminator, and in turn a better generator, and\nthen carry out a progressive training paradigm involving multiple GANs to contribute to the maximum margin rank-\ning loss so that the GAN at later stages will improve upon early stages. We have shown theoretically that the proposed GoGAN can reduce the gap between the true data distribution and the generated data distribution by at least half in an optimally trained WGAN. We have also proposed a new way of measuring GAN quality which is based on image completion tasks. We have evaluated our method on four visual datasets: CelebA, LSUN Bedroom, CIFAR-10, and 50K-SSFF, and have seen both visual and quantitative improvement over baseline WGAN. Future work may include extending the GoGAN for other GAN variants and study how other divergence-based loss functions can benefit from the ranking loss and progressive training."}], "references": [{"title": "Openface: A general-purpose face recognition library with mobile applications", "author": ["B. Amos", "L. Bartosz", "M. Satyanarayanan"], "venue": "Technical report, CMU-CS-16-118, CMU School of Computer Science", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards principled methods for training generative adversarial networks", "author": ["M. Arjovsky", "L. Bottou"], "venue": "ICLR (under review)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "arXiv preprint arXiv:1701.07875", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2017}, {"title": "Neural photo editing with introspective adversarial networks", "author": ["A. Brock", "T. Lim", "J. Ritchie", "N. Weston"], "venue": "arXiv preprint arXiv:1609.07093", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "arXiv preprint arXiv:1606.03657", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["E.L. Denton", "S. Chintala", "R. Fergus"], "venue": "Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pages 1486\u20131494", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "arXiv preprint arXiv:1605.09782", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarially learned inference", "author": ["V. Dumoulin", "I. Belghazi", "B. Poole", "A. Lamb", "M. Arjovsky", "O. Mastropietro", "A. Courville"], "venue": "arXiv preprint arXiv:1606.00704", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative multiadversarial networks", "author": ["I. Durugkar", "I. Gemp", "S. Mahadevan"], "venue": "arXiv preprint arXiv:1611.01673", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, pages 2672\u20132680", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-pie", "author": ["R. Gross", "I. Matthews", "J. Cohn", "T. Kanade", "S. Baker"], "venue": "Image and Vision Computing, 28(5):807\u2013813", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Boundary-seeking generative adversarial networks", "author": ["R.D. Hjelm", "A.P. Jacob", "T. Che", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1702.08431", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Stacked generative adversarial networks", "author": ["X. Huang", "Y. Li", "O. Poursaeed", "J. Hopcroft", "S. Belongie"], "venue": "arXiv preprint arXiv:1612.04357", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating images with recurrent adversarial networks", "author": ["D.J. Im", "C.D. Kim", "H. Jiang", "R. Memisevic"], "venue": "arXiv preprint arXiv:1602.05110", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Imageto-image translation with conditional adversarial networks", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": "arXiv preprint arXiv:1611.07004", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "CIFAR-10 Database", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["C. Ledig", "L. Theis", "F. Husz\u00e1r", "J. Caballero", "A. Cunningham", "A. Acosta", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang"], "venue": "Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Least squares generative adversarial networks", "author": ["X. Mao", "Q. Li", "H. Xie", "R.Y. Lau", "Z. Wang"], "venue": "arXiv preprint arXiv:1611.04076", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["A. Nguyen", "J. Yosinski", "Y. Bengio", "A. Dosovitskiy", "J. Clune"], "venue": "arXiv preprint arXiv:1612.00005", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["S. Nowozin", "B. Cseke", "R. Tomioka"], "venue": "arXiv preprint arXiv:1606.00709", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["A. Odena", "C. Olah", "J. Shlens"], "venue": "arXiv preprint arXiv:1610.09585", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Distinguishing identical twins by face recognition", "author": ["P.J. Phillips", "P.J. Flynn", "K.W. Bowyer", "R.W.V. Bruegge", "P.J. Grother", "G.W. Quinn", "M. Pruitt"], "venue": "Automatic Face & Gesture Recognition and Workshops ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Overview of the face recognition grand challenge", "author": ["P.J. Phillips", "P.J. Flynn", "T. Scruggs", "K.W. Bowyer", "J. Chang", "K. Hoffman", "J. Marques", "J. Min", "W. Worek"], "venue": "Computer vision and pattern recognition, 2005. CVPR 2005. IEEE computer society conference on, volume 1, pages 947\u2013954. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Unsupervised representation learning with deep convolutional generative adver-  sarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "Advances in Neural Information Processing Systems, pages 2226\u20132234", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised crossdomain image generation", "author": ["Y. Taigman", "A. Polyak", "L. Wolf"], "venue": "arXiv preprint arXiv:1611.02200", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Pixel recurrent neural networks", "author": ["A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "O. Vinyals", "L. Espeholt", "A. Graves", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.05328,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Optimal transport: old and new", "author": ["C. Villani"], "venue": "volume 338. Springer Science & Business Media", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Lr-gan - layered recursive generative adversarial networks for image generation", "author": ["J. Yang", "A. Kannan", "B. Batra", "D. Parikh"], "venue": "ICLR (under review)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}, {"title": "Semantic image inpainting with perceptual and contextual losses", "author": ["R. Yeh", "C. Chen", "T.Y. Lim", "M. Hasegawa-Johnson", "M.N. Do"], "venue": "arXiv preprint arXiv:1607.07539", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["F. Yu", "Y. Zhang", "S. Song", "A. Seff", "J. Xiao"], "venue": "arXiv preprint arXiv:1506.03365", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Energy-based generative adversarial network", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "arXiv preprint arXiv:1609.03126", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Variational autoencoders (VAEs) [17] formalize the generative problem in the framework of probabilistic graphical models where we are to maximize a lower bound on the log likelihood of the training data.", "startOffset": 32, "endOffset": 36}, {"referenceID": 31, "context": "Autoregressive models such as PixelRNN [32] and PixelCNN [33] get rid of the latent variables and instead directly model the conditional distribution of every individual pixel given previous pixels starting from top-left corner.", "startOffset": 39, "endOffset": 43}, {"referenceID": 32, "context": "Autoregressive models such as PixelRNN [32] and PixelCNN [33] get rid of the latent variables and instead directly model the conditional distribution of every individual pixel given previous pixels starting from top-left corner.", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "Generative adversarial networks (GANs) [10] simultaneously train a generator network for generating realistic images, and a discriminator network for distinguishing between the generated images and the samples from the training data (true distribution).", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "Imagine if we have a weak discriminator which does a poor job telling generated images from the true images, it takes only a little effort for the generator to win the two-player minimax game as described in the original work of GAN [10].", "startOffset": 233, "endOffset": 237}, {"referenceID": 27, "context": "Deep convolutional generative adversarial networks (DCGAN) [28] are proposed to replace the multilayer perceptron in the original GAN [10] for more stable training, by utilizing strided convolutions in place of pooling layers, and fractional-strided convolutions in place of image upsampling.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "Deep convolutional generative adversarial networks (DCGAN) [28] are proposed to replace the multilayer perceptron in the original GAN [10] for more stable training, by utilizing strided convolutions in place of pooling layers, and fractional-strided convolutions in place of image upsampling.", "startOffset": 134, "endOffset": 138}, {"referenceID": 21, "context": "Conditional GAN [22] is proposed as a variant of GAN by extending it to a conditional model, where both the generator and discriminator are conditioned on some extra", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "Another variant of GAN is called auxiliary classifier GAN (AC-GAN) [25], where every generated sample has a corresponding class label in addition to the noise.", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "Bidirectional GAN (BiGAN) [7] is proposed to bridge the gap that conventional GAN does not learn the inverse mapping which projects the data back into the latent space, which can be very useful for unsupervised feature learning.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "There is a concurrent work proposed in [8] that has the identical model.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "A sequential variant of the GAN is the Laplacian generative adversarial networks (LAPGAN) [6] model which generates images in a coarse-to-fine manner by generating and upsampling in multiple steps.", "startOffset": 90, "endOffset": 93}, {"referenceID": 10, "context": "It is worth mentioning the sequential variant of the VAE is the deep recurrent attentive writer (DRAW) [11] model that generates images by accumulating updates into a canvas using a recurrent network.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "Built upon the idea of sequential generation of images, the recurrent adversarial networks [15] has been proposed to let the recurrent network to learn the optimal generation procedure by itself, as opposed to imposing a coarse-to-fine structure on the procedure.", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "Introspective adversarial network (IAN) [4] is proposed to hybridize the VAE and the GAN.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "The generative multi-adversarial networks (GMAN) [9] extends the GANs to multiple discriminators.", "startOffset": 49, "endOffset": 52}, {"referenceID": 34, "context": "Layered recursive generative adversarial networks (LR-GAN) [35] generates images in a recursive fashion.", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "Authors of [24] have shown that the generative-adversarial approach in GAN is a special case of an existing more general variational divergence estimation approach, and that any f divergence can be used for training generative neural samplers.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "InfoGAN [5] method is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation.", "startOffset": 8, "endOffset": 11}, {"referenceID": 37, "context": "To strive for a more stable GAN training, the energy-based generative adversarial networks (EBGAN) [38] is proposed which views the discriminator as an energy function that assigns low energy to the regions near the data manifold and higher energy to other regions.", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "The boundary-seeking GAN (BGAN) [13] aims at generating samples that lie on the decision boundary of a current discriminator in training at each update.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "Least squares GAN [21] adopts a least squares loss function for the discriminator, which is equivalent to a multi-class GAN with the `2 loss function.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "The stacked GAN (SGAN) [14] consists of a top-down stack of GANs, each trained to generate plausible lower-level representations, conditioned on higher-level representations.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "Perhaps the most seminal GAN-related work since the inception of the original GAN [10] idea is the Wasserstein GAN (WGAN) [3].", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "Perhaps the most seminal GAN-related work since the inception of the original GAN [10] idea is the Wasserstein GAN (WGAN) [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "Efforts have been made to fully understand the training dynamics of generative adversarial networks through theoretical analysis [2], which leads to the creation of the WGAN.", "startOffset": 129, "endOffset": 132}, {"referenceID": 29, "context": "Other applications include cross-domain image generation [30] through a domain transfer network (DTN) which employs a compound of loss functions including a multiclass GAN loss, an f -constancy component, and a regularization component that encourages the generator to map samples from the target domain to themselves.", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "The imageto-image translation approach [16] is based on conditional GAN, and learns a conditional generative model for generating a corresponding output image at a different domain, conditioned on an input image.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "The image superresolution GAN (SRGAN) [19] combines both the image content loss and the adversarial loss for recovering highresolution counterpart of the low-resolution input image.", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "The plug and play generative networks (PPGN) [23] is able to produce high quality images at higher resolution for all 1000 ImageNet categories.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "In this section we will review the original GAN [10] and its convolutional variant DCGAN [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "In this section we will review the original GAN [10] and its convolutional variant DCGAN [28].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "We will then analyze how to further improve the GAN model with WGAN [3], and introduce our Gang of GANs (GoGAN) method.", "startOffset": 68, "endOffset": 71}, {"referenceID": 9, "context": "The GAN [10] framework trains two networks, a generator G\u03b8(z) : z \u2192 x, and a discriminator D\u03c9(x) : x \u2192 [0, 1].", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "The GAN [10] framework trains two networks, a generator G\u03b8(z) : z \u2192 x, and a discriminator D\u03c9(x) : x \u2192 [0, 1].", "startOffset": 103, "endOffset": 109}, {"referenceID": 27, "context": "The DCGAN [28] uses fractionally-strided convolutions to upsample images instead of fully-connected neurons as shown in Figure 1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "[10] have proposed the following two loss functions for the generator: Ez\u223cPz(z)[log(1\u2212D(G(z)))] andEz\u223cPz(z)[\u2212 logD(G(z))].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "The latter one is referred to as the \u2212 logD trick [10, 2, 3].", "startOffset": 50, "endOffset": 60}, {"referenceID": 1, "context": "The latter one is referred to as the \u2212 logD trick [10, 2, 3].", "startOffset": 50, "endOffset": 60}, {"referenceID": 2, "context": "The latter one is referred to as the \u2212 logD trick [10, 2, 3].", "startOffset": 50, "endOffset": 60}, {"referenceID": 1, "context": "WGAN [2, 3] avoids the gradient vanishing and mode collapse issues in the original GAN and many of its variants by adopting a new distance metric: the Wasserstein-1 distance, or the earth-mover distance as follows:", "startOffset": 5, "endOffset": 11}, {"referenceID": 2, "context": "WGAN [2, 3] avoids the gradient vanishing and mode collapse issues in the original GAN and many of its variants by adopting a new distance metric: the Wasserstein-1 distance, or the earth-mover distance as follows:", "startOffset": 5, "endOffset": 11}, {"referenceID": 33, "context": "Thanks to the Kantorovich-Rubinstein duality [34], the Wasserstein distance becomes: W (Pr,Pg) = sup\u2016f\u2016L\u22641Ex\u223cPr [f(x)] \u2212 Ex\u223cPg [f(x)], where the supremum is over all the 1Lipschitz functions.", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "In WGAN [3], the following loss function involving the weights updating of the discriminator and the generator is", "startOffset": 8, "endOffset": 11}, {"referenceID": 28, "context": "Perhaps, the Inception score [29] is by far the best solution we have.", "startOffset": 29, "endOffset": 33}, {"referenceID": 35, "context": "We ask the GANs to carry out image completion tasks [36], and the GAN performance is measured by the fidelity (PSNR, SSIM) of the completed image against its ground-truth.", "startOffset": 52, "endOffset": 56}, {"referenceID": 35, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The WGAN baseline uses the Wasserstein discriminator loss [3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 19, "context": "The CelebA dataset [20] is a large-scale face attributes dataset with more than 200K celebrity images.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "We pre-process and align the face images using dLib as provided by the OpenFace [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 36, "context": "The LSUN Bedroom dataset [37] is meant for large-scale scene understanding.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "The CIFAR-10 [18] is an image classification dataset containing a total of 60K 32 \u00d7 32 color images, which are across the following 10 classes: airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships, and trucks.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "We use the DCGAN [28] architecture for both the generator and the discriminator at all stages of the training.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "Both the generator and the discriminator are learned using optimizers (RMSprop [31]) that are not based on momentum as recommended in [3] with a learning rate of 5e-5.", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "Both the generator and the discriminator are learned using optimizers (RMSprop [31]) that are not based on momentum as recommended in [3] with a learning rate of 5e-5.", "startOffset": 134, "endOffset": 137}, {"referenceID": 26, "context": "0 dataset [27], the MPIE dataset [12], the ND-Twin dataset [26], and mugshot dataset from Pinellas County Sheriff\u2019s Office (PCSO).", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "0 dataset [27], the MPIE dataset [12], the ND-Twin dataset [26], and mugshot dataset from Pinellas County Sheriff\u2019s Office (PCSO).", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "0 dataset [27], the MPIE dataset [12], the ND-Twin dataset [26], and mugshot dataset from Pinellas County Sheriff\u2019s Office (PCSO).", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "celebrity face dataset such as CelebA [20], our collected 50K-SSFF dataset is dedicated for one-shot learning in the GAN context due to its single-sample nature.", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "Traditional generative adversarial networks (GAN) and many of its variants are trained by minimizing the KL or JS-divergence loss that measures how close the generated data distribution is from the true data distribution. A recent advance called the WGAN based on Wasserstein distance can improve on the KL and JS-divergence based GANs, and alleviate the gradient vanishing, instability, and mode collapse issues that are common in the GAN training. In this work, we aim at improving on the WGAN by first generalizing its discriminator loss to a margin-based one, which leads to a better discriminator, and in turn a better generator, and then carrying out a progressive training paradigm involving multiple GANs to contribute to the maximum margin ranking loss so that the GAN at later stages will improve upon early stages. We call this method Gang of GANs (GoGAN). We have shown theoretically that the proposed GoGAN can reduce the gap between the true data distribution and the generated data distribution by at least half in an optimally trained WGAN. We have also proposed a new way of measuring GAN quality which is based on image completion tasks. We have evaluated our method on four visual datasets: CelebA, LSUN Bedroom, CIFAR-10, and 50K-SSFF, and have seen both visual and quantitative improvement over baseline WGAN.", "creator": "LaTeX with hyperref package"}}}