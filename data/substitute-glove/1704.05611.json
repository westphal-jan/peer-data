{"id": "1704.05611", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Dependency resolution and semantic mining using Tree Adjoining Grammars for Tamil Language", "abstract": "Tree entrances determiners (TAGs) needs addition amounts tool will determined distinguishes given many Indian syntax. Tamil america given special challenge to computational heuristics for instead has extensive stack-based morphology which has disturbingly longer argument example. Modelling Tamil syntax and morphology using TAG is an something problem which has not quickly entered promising even still TAGs are three september even as, 1990 extended inception. Our associate with Tamil TAGs have shown iraq say else can not only historically syntax of to language, instead whether charge considerable rig get dynamical through inheritance issue of present rape. But ago thus only meaningful this tremendous income, we keeping continue spatulate Tamil similar arbitrarily using TAGs we likely converted and through transcribe must with derivation they will allow to bring dependencies, possibly proving the algorithms corporations. We unlike an on - house developed pseudo usages TAG year-end analyzer; probability offered notably Schabes and Joshi (2006 ), five generating attributions of retrial. We say much different any directorate to rank out ambiguous wordings why simply devices actually is instead to understand that mentioned functionality particular with in TAGs even Tamil. We shall also due made interviews parser determine now the validity brought our step.", "histories": [["v1", "Wed, 19 Apr 2017 05:02:05 GMT  (756kb)", "http://arxiv.org/abs/1704.05611v1", "9 pages. arXiv admin note: text overlap witharXiv:1604.01235"]], "COMMENTS": "9 pages. arXiv admin note: text overlap witharXiv:1604.01235", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vijay krishna menon", "s rajendran", "m anandkumar", "k p soman"], "accepted": false, "id": "1704.05611"}, "pdf": {"name": "1704.05611.pdf", "metadata": {"source": "CRF", "title": "Dependency resolution and semantic mining using Tree Adjoining Grammars for Tamil Language", "authors": ["Vijay Krishna Menon", "Rajendran S"], "emails": ["m_vijaykrishna@cb.amrita.edu", "rajushush@gmail.com", "m_anandkumar@cb.amrita.edu", "kp_soman@amrita.edu"], "sections": [{"heading": null, "text": "Tamil represents a special challenge to computational formalisms as it has extensive agglutinative morphology and a comparatively difficult argument structure. Modelling Tamil syntax and morphology using TAG is an interesting problem which has not been in focus even though TAGs are over 4 decades old, since its inception. Our research with Tamil TAGs have shown us that we can not only represent syntax of the language, but to an extent mine out semantics through dependency resolution of the sentence. But in order to demonstrate this phenomenal property, we need to parse Tamil language sentences using TAGs we have built and through parsing obtain a derivation we could use to resolve dependencies, thus proving the semantic property. We use an in-house developed pseudo lexical TAG chart parser; algorithm given by Schabes and Joshi (1988), for generating derivations of sentences. We do not use any statistics to rank out ambiguous derivations but rather use all of them to understand the mentioned semantic relation with in TAGs for Tamil. We shall also present a brief parser analysis for the completeness of our discussions."}, {"heading": "1. Introduction", "text": "TAGs were proposed for language models earlier by Vijay Shankar and Aravind Joshi in (VijayShankar and Joshi, 1985). Unlike the Chomskian formalisms, the elementary objects manipulated by TAG are trees; structured objects and not strings. Such structured formalisms have properties that relate directly to strong generative capacity (structure descriptions), which is linguistically more relevant than string sets (weak generative capacity). So we call TAGs as a tree generating system rather than a string generating system. The set of all trees derived in a TAG constitute the object language. Hence, in order to describe the derivation of a tree in the object language, we will need to know about \u2018derivation trees\u2019. The derivation trees are important in both syntactic and semantic senses. TAGs also have some interesting linguistic properties. Lexicalization is one of the key motivations for the study of TAGs, both linguistic and formal. The lexical phenomena now explain many linguistic theories previously thought to be purely syntactic. So the information in lexicons, have increased both in amount and complexity. From the formal perspective, lexicalization allows us to associate every elementary structure (trees) with a lexicon (any word). The famous Greibach Normal Form (also called Chomsky Normal Form or CNF) for CFGs is a kind of lexicalization. However it is a weak lexicalization, as the structure of the original grammar is not preserved and all rules cannot be lexicalised. Thus TAGs provide an edge to this errand over conventional CFGs.\nTAGs were introduced by Joshi et al. (1975) and later Joshi (1985). It is known that tree adjoining languages (TALs) generate some strictly context sensitive languages and fall in the class of the so called \u2018mildly context sensitive\u2019 languages (Joshi et al, 1991). TALs properly contain context-free languages and are properly contained by indexed languages. A tree-adjoining grammar (TAG), G consists of a quintuple (\u2211, NT, I, A, S) where\ni. \u2211 is a finite set of terminal symbols. NT is a finite set of non-terminal symbols such that (\u2211 .\nii. S is a Sentential symbol such that . iii. I is a finite set of trees called initial trees, with the following properties\na. Interior nodes are labelled by non-terminal symbols; b. The nodes on the frontier of all initial trees are labelled by terminals or non-terminals;\nnon-terminals symbols on the frontier of any tree in I are marked for substitution which, by convention is a down arrow (\u2193);\niv. A is a finite set of trees called auxiliary trees, with the following properties a. Interior nodes are labelled by non-terminal symbols; b. The nodes on the frontier of auxiliary trees are labelled by terminal symbols or non-\nterminal symbols. Non-terminal symbol on the frontier of trees in A are marked for substitution except for one node, called the foot node; by convention this is marked with an asterisk(*); the label of the foot node must be identical to the root node.\nIn lexicalised TAG, at least one frontier node must be labelled with a terminal symbol (the anchor) in all initial and auxiliary trees. The set I U A is called the set of elementary trees. If an elementary tree has its root labelled by non-terminal X, then it is called an X-type elementary tree.\nA tree built by combining the elementary trees is called derived tree or parse tree. We will now have to understand how the combinations of trees happen as to make a derived tree. There are 2 major composition operations adjoining and substitution.\nAdjoining (or adjunction, as it is alternately referred) builds a new tree from an auxiliary tree \u03b2 and a tree \u03b1 (\u03b1 is any tree initial auxiliary or derived). Let \u2018\u03b1\u2019 be a tree containing a non-substitution node\nlabeled by X. The resulting tree, \u03b3, obtained by adjoining \u03b2 to \u03b1 at node n is structured as:\n The sub-tree of \u03b1 with root n is displaced by \u03b2, along with its root node n.\n The displace sub tree of \u03b1 will attach itself to \u03b2, replacing the foot node of \u03b2.\nSubstitution takes place only on non-terminal\nnodes in the frontier of a tree. Unlike normal adjunctions, substitutions are mandatory if the node is marked for it with a down arrow as explained above. When a node, say n, is substituted, the entire node is replaced by the initial tree that is substituted. Only initial trees or its derivatives may be used for substitution. By definition adjunctions on any node marked for substitution is not permitted. But adjunctions are possible on the root nodes of the trees already substituted replacing the marked node. This is illustrated in Fig 2 with a set of three initial trees. Substitution extents the targeted leaf node to complete a construct that requires addition of a single substring.\nWhen TAG grammar yields (generates) derived\ntrees by derivation, the information to trace the history of such combination is not given. Unlike CFGs, the derived tree does not contain information as to which basic rules\n(in our case, elementary trees) were used to construct it.\nFigure 1: Susbstitution of trees in TAG\nHence we require a new object that gives us information regarding all operations and elementary trees used to build a derived tree. This structured object is called a derivation tree. It uniquely specifies what operation was used to combine which particular trees. Both adjunctions and substitutions are considered for derivation."}, {"heading": "2. Derivation Structures in TAG", "text": "Consider the example sentence \u201cYesterday a man saw Mary\u201d. This example has been adopted from Joshi and Schabes (1997). Fig 3 illustrates the derived tree for the above English sentence. But this tree does not give any relevant information regarding how it can be constructed. For this we define the derivation tree for the same sentence. Refer to Fig 4 where the necessary elementary trees required to derive the \u03b15 has been illustrated. Note that \u03b1 trees are initial trees and the \u03b2 ones are auxiliary. This convention will be prevailing throughout this paper whenever referring to TAG trees.\nshown in Fig 3. Along with exemplifying the process of building a derivation we also show how a proper lexicalization of TAG is achieved. All the elementary trees in the Fig 3. are properly and completely lexicalised with every elementary tree mapped to at least one lexicon. So every tree will have at least one anchor node.\nThe roots of all derivation trees are labelled by the name of an S-type initial tree. All child nodes are labelled by auxiliary trees which adjoined or initial trees which are substituted. The notion of tree address is used here to indicate where the composition happened. This will uniquely identify a node in a given tree. This address is referred to as the Gorn index; used for multiple array of purposes and is specifically important from an implementation point of view.\nThe Gorn index system starts with index 0 for the root node. For the 1st level children the numbering starts with 0.1 (or just 1) for the leftmost and increasing towards the right. For the 2nd level children say the child of the second leftmost child will be given 0.2.1 (or just 2.1) and so on. The system is simple and intuitive. Now if an adjunction takes place at this node of the tree, the derivation tree node labelled with the adjoining auxiliary tree will also carry the Gorn index 0.2.1, so we know exactly where the adjunction or substitution has occurred.\nFig 3 depicts the derivation and elementary trees for the mensioned example. Note that \u03b1saw is an Stype initial tree; most verb initial trees are expected to be so. Now the node \u03b1man (1) indicates a substitution of this tree at node 0.1 of \u03b1saw. In a deeper sense it means this tree replaced the node indexed 0.1 in tree \u03b1saw.\nFigure 3: Derivation tree(on the right) and elementary lexicalised trees\nThe case with \u03b1Mary is no different, except that it is substituted at for node 0.2.2. But \u03b2yesterday is an auxiliary tree and is adjoined at the root node of \u03b1saw as it contains the Gorn index pointing to the root. The main idea here is the Gorn indices given in a derivation tree\u2019s node, points to an address in its parent node\u2019s tree where the substitution or adjunction has been done. Further it also demonstrates how lower composition happens, like \u03b1a substituted on \u03b1man. Unlike as represented, substitutions need not be discriminated with dotted lines alone. The target node tree can solve the conflict by its type as in initial or auxiliary. Another counter intuitive fact is that adjoining happens even at the root node. But controlling adjunctions will help us control the grammars generative ability and restrict the constructs it creates. So every node in the derivation tree will have distinct indices for a given parent node. This way of representing derivation not only captures the syntactic structure of the target tree but also contains semantic dependencies. This has been demonstrated by Joshi and Rambow (1997); they were the first to investigate this property for TAG derivations. Later, Joshi and Rambow (2003) gave a dependency grammar based on TAG formalism. However we shall give a different picture of the same idea here. To illustrate this let us isolate the basic words of the above given example itself. Before we go into detail of this we will need to define dependency functions of each word with respect to the parts of speech (POS) of each word. Consider initially the verb saw. Now \u2018saw\u2019 is a transitive verb1, so it will have dependencies in 2 ways, one with its subject and the other with the object. Hence the dependency function will look like this (basic argument structure).\n( ( (\nThis show the dependencies of the transitive verb saw to depend on the subject as to who or what\nsaw to the object as to saw whom or what. Logically this function looks like this for saw.\nThis is exactly what we get in the derivation; \u201cman saw Mary\u201d giving us the dependency function for saw to be saw (Man, Mary). All the other words will have dependencies too as well. As for the Noun man the function is different and addresses the number or specificity. That means that nouns have articles or adjectives that describe them. This is their dependency. The above derivation also gives man(a) which is the dependency function for the word. The dependencies of a word can be easily found from the children of the given node in a derivation tree.\nFrom the above insight, we must gather that saw in this example is not just transitive. That is to say it has a subject, an objects and an adverb. Thus the definition of the function should be having an extra parameter, one that specifies time in this case hence we have saw (Man, Mary, Yesterday). This property of TAG derivation greatly helps for representation of agglutinative languages, where the verbal inflection will depend on its subject or object or both. Subject verb agreements are crucial especially in Indian languages."}, {"heading": "3. Tamil TAG and Derivations in Tamil", "text": "Tamil is a morph rich language, so to do pure syntax based dependency mining from it we will need to set aside the morphological considerations for the while and focus on the syntactic and psycho syntactic models. We have hand developed a Tamil TAG. Though its scope is quite restricted and tested mainly on tourism and health based corpora, it is effective enough for text book class sentences. Since such sentences only have limited or light dependencies, it might just prove to be insufficient for detailed analysis, how ever our attempt can be considered a step one into TAG based semantic analysis for Tamil language.\nThe Tamil TAGs were mainly created as part of a Machine Translation project, using\nsynchronous TAGs. So these grammar trees are synchronised over a subset of XTAG English grammar. Efforts are being made for this to be expanded to a comprehensive grammar not just limited to tamil. Unlike general XTAG trees we have designed single anchor trees; a grammar tree can be lexicalised only with one lexicon. This way we maintain a one to one relation between lexicons and derivation nodes so that the node represents only the dependancy relation of that perticular lexicon. We will try to explain this through a set of examples in Tamil. Also note that we donot do any kind of\n1 Verbs that require a subject and an object of action are transitive verbs.\nstatistical parsing or context based ranking of parses over the sentence. To fully observe the dependencies, all syntactically ambiguous parses are needed, so as to obtain different points of views and preserve the natural ambiguity. Before we observe the parses we need to describe the main aspects of the grammar. We have tried and captured the following few main constructs of Tamil\n1. Noun, Postpositions (morphemes), Conjuctions, Adverbs 2. Reccursive ajdectives 3. Basic Clefts (If clefts in a limited\nway)\n4. Transitive Verb\n5. Intransitive Verb 6. Ergative Verb 7. PP complement 8. PP small Clause 9. Sentential Complement 10. Sentential Subject\nThese constructs are reprsented using elementary trees and auxiliary trees as was seen fit\nliguistically and by ease of parsing. The Parser is a multithreaded java implementation of the \u2018Earley Type TAG parsing\u2019 algorithm by Shabes and Joshi (1987). It generates both parse trees and derivation\ntrees over each and every ambiguous parse it can find from the grammar provided. Fig 4 demostrates the the Tamil TAG trees as redered by our viewer. The anotations for the nodes are consistent with the XTAG conventions except that all trees have sigle anchor node that houses the POS category the tree belongs to. The figure eveidently shows a mapping between the English and Tamil trees as earlier mentioned.\nThis however does not diminish the generative capacity of the Tamil grammar to independantly parse and generate derivation trees on its own accord. Positively it helps to allign the dependacy with english like dependency and base it on the stanford dependency set of 40 major semantic relations.\nWe however will deal with just one or two\ncases as a proof of concept. To mine out the relations, we introduc argument operators on certain lexical items, that will hypothetically give us the semantic argumets of the item\u2019s roll in the sentence from a TAG derivation. Each derivation tree is defined recursively to yeild the arguments when an operator operates over it.\nThe dependencies that we mine here as part\nof a miniature experiment are the following stanford depandancies\n1. Nominal Subject (nsub) 2. Direct Object (dobj) 3. Root (root)\n4.Tamil parse specifics and examples The grammar used by the parser for Tamil cotains over 120 trees conrectly and are regularly pruned\nto reduce cross ambiguities. We have 25 initial and 95 auxiliary trees. Together they address most\nconstructs of simple and direct sentences.\nThe parser accepts Parts of speech tagged sentences using the Penn Tagset for the same. If the\nsentence is with in the construct range of the grammar, the parser immediate returns TAG derivations\nfrom which a derived trees can be easly constructed. As mentioned before we are not currently deling\nwith morph analysis just to keep our focus on grammar and parsing. In the examples here the words\nare mostly surface forms with some chucks in it. Our secondary objective is to prove the conformity\nof TAG syntax for Tamil in a broader sense. Two examples of the parse instances are illustrated\nbellow. The Tamil sentences has been Romanised for the sake of linguistic verification.\nExample 1: NiyUyArkkil na\u1e6daipeRRa yu.Es-OpaN-A\u1e47ka\u1e37-ira\u1e6d\u1e6daiyar iRutip-pO\u1e6d\u1e6diyil liyA\u1e47\u1e6dar-payas-\njO\u1e6di veRRi peRRu pa\u1e6d\u1e6dattaik kaippaRRiyatu\nThe derivation as above, clearly supports the \u2018root\u2019 verb (matrix verb, morph seperated). And the\nother dependencies such as nominal object and direct subject can also be seen here. Some non clausal\nadverbial dependencies can also be refined from this\nExample 2: MOtirattai tiru\u1e6diya vAliparai pOlIcAr tE\u1e6di varuki\u1e49\u1e5fa\u1e49ar\nThis sentence has multiple parses maily due to a lexicosyntactic ambiguity. One prominent parse\nillustrated by Fig 8 and another parse illustrated by"}, {"heading": "5. References", "text": "Abeille, A., Bonami, O., Gordard, D., & Tseng, J. (2004). The syntax of French de-N phrases. In S. Muller\n(Ed.), Proceedings of the HPSG04 Conference. Stanford: CSLI Publications.\nAbeille, A., Schabes, Y., & Joshi, A. (1990). Using lexicalized tree adjoining grammms for machine translation.\nIn Proceedings of the 12th International Conference on Computational Linguistics . Budapest.\nChomsky, N. (1995). Government and Binding Theory and Minimalist Program. Blackwell , 383-439.\nChomsky, N. (1995). The minimalist program. MIT Press .\nJoshi, A. (1987). An Introduction to Tree Adjoining Grammars. Mathematics of Language .\nJoshi, A. (1985). How much context sensitivity is necessory for charectorizing structural descriptions. Natural\nLanguage Parsing Theoretical, Computational and Psychologica Perspectives. New York: Cambridge University Press.\nJoshi, A., & Rambow, O. (2003). A Formalism for Dependency Grammar Based on Tree Adjoining Grammar.\nMeaning-Text Theory, (pp. 16-18). Paris.\nJoshi, A., & Schabes, Y. (1997). Tree Adjoining Grammars. Philadelphia: University of Pennsylvania.\nJoshi, A., Levy, L., & Takahashi, M. (1975). Tree Adjunct Grammars. Journal of Computer and System\nSciences , 10.\nde Marneffe, M. C,. Manning, Christopher, D,. (2008).The Stanford Typed Dependencies Representation. In\nColing 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation pp. 1-8\nreport, U. N. (2008, 05 03). living-diversity. Retrieved from www.living\u2010diversity.eu: Main Source: http://www.living\u2010diversity.eu/News/Eintrage/2008/1/11_Eintrag_1.html\nSarkar, A. (2002, 06 18). Verb Classes. Retrieved 11 25, 2014, from XTAG Project:\nhttp://www.cis.upenn.edu/~xtag/tech-report/node29.html\nSchabes, Y., & Joshi, A. (1988). An Earley Type Parsing Algorithm for Tree Adjoining Grammars. Proceedings\nof the 26th annual meeting on Association for Computational Linguistics (pp. 258 - 269 ). New York: Association for Computational Linguistics .\nSchieber, S. M., & Schabes, Y. (1990). Synchronous Tree-Adjoining Grammars. Proceedings of the 13th\nInternational Conference on Computational Linguistics. Helsinki, Finland.\nVijay-Shanker, K. (1988). A Study of Tree Adjoining Grammars, A PhD Thesis. Philadelphia: University of Pennsylvania"}], "references": [{"title": "The syntax of French de-N phrases", "author": ["A. Abeille", "O. Bonami", "D. Gordard", "J. Tseng"], "venue": "Proceedings of the HPSG04 Conference. Stanford: CSLI Publications", "citeRegEx": "Abeille et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abeille et al\\.", "year": 2004}, {"title": "Using lexicalized tree adjoining grammms for machine translation", "author": ["A. Abeille", "Y. Schabes", "A. Joshi"], "venue": "In Proceedings of the 12th International Conference on Computational Linguistics . Budapest", "citeRegEx": "Abeille et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Abeille et al\\.", "year": 1990}, {"title": "Government and Binding Theory and Minimalist Program", "author": ["N. Chomsky"], "venue": null, "citeRegEx": "Chomsky,? \\Q1995\\E", "shortCiteRegEx": "Chomsky", "year": 1995}, {"title": "The minimalist program", "author": ["N. Chomsky"], "venue": null, "citeRegEx": "Chomsky,? \\Q1995\\E", "shortCiteRegEx": "Chomsky", "year": 1995}, {"title": "An Introduction to Tree Adjoining Grammars. Mathematics of Language", "author": ["A. Joshi"], "venue": null, "citeRegEx": "Joshi,? \\Q1987\\E", "shortCiteRegEx": "Joshi", "year": 1987}, {"title": "How much context sensitivity is necessory for charectorizing structural descriptions. Natural Language Parsing Theoretical, Computational and Psychologica Perspectives", "author": ["A. Joshi"], "venue": null, "citeRegEx": "Joshi,? \\Q1985\\E", "shortCiteRegEx": "Joshi", "year": 1985}, {"title": "A Formalism for Dependency Grammar Based on Tree Adjoining Grammar", "author": ["A. Joshi", "O. Rambow"], "venue": null, "citeRegEx": "Joshi and Rambow,? \\Q2003\\E", "shortCiteRegEx": "Joshi and Rambow", "year": 2003}, {"title": "2008).The Stanford Typed Dependencies Representation", "author": ["de Marneffe", "M. C", "Manning", "D Christopher"], "venue": "In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "An Earley Type Parsing Algorithm for Tree Adjoining Grammars", "author": ["Y. Schabes", "A. Joshi"], "venue": "Proceedings of the 26th annual meeting on Association for Computational Linguistics (pp", "citeRegEx": "Schabes and Joshi,? \\Q1988\\E", "shortCiteRegEx": "Schabes and Joshi", "year": 1988}, {"title": "Synchronous Tree-Adjoining Grammars", "author": ["S.M. Schieber", "Y. Schabes"], "venue": "Proceedings of the 13th International Conference on Computational Linguistics", "citeRegEx": "Schieber and Schabes,? \\Q1990\\E", "shortCiteRegEx": "Schieber and Schabes", "year": 1990}, {"title": "A Study of Tree Adjoining Grammars, A PhD Thesis. Philadelphia: University of Pennsylvania", "author": ["K. Vijay-Shanker"], "venue": null, "citeRegEx": "Vijay.Shanker,? \\Q1988\\E", "shortCiteRegEx": "Vijay.Shanker", "year": 1988}], "referenceMentions": [{"referenceID": 4, "context": "We use an in-house developed pseudo lexical TAG chart parser; algorithm given by Schabes and Joshi (1988), for generating derivations of sentences.", "startOffset": 93, "endOffset": 106}, {"referenceID": 4, "context": "TAGs were introduced by Joshi et al. (1975) and later Joshi (1985).", "startOffset": 24, "endOffset": 44}, {"referenceID": 4, "context": "TAGs were introduced by Joshi et al. (1975) and later Joshi (1985). It is known that tree adjoining languages (TALs) generate some strictly context sensitive languages and fall in the class of the so called \u2018mildly context sensitive\u2019 languages (Joshi et al, 1991).", "startOffset": 24, "endOffset": 67}, {"referenceID": 4, "context": "This example has been adopted from Joshi and Schabes (1997). Fig 3 illustrates the derived tree for the above English sentence.", "startOffset": 35, "endOffset": 60}, {"referenceID": 4, "context": "This has been demonstrated by Joshi and Rambow (1997); they were the first to investigate this property for TAG derivations.", "startOffset": 30, "endOffset": 54}, {"referenceID": 4, "context": "This has been demonstrated by Joshi and Rambow (1997); they were the first to investigate this property for TAG derivations. Later, Joshi and Rambow (2003) gave a dependency grammar based on TAG formalism.", "startOffset": 30, "endOffset": 156}, {"referenceID": 4, "context": "The Parser is a multithreaded java implementation of the \u2018Earley Type TAG parsing\u2019 algorithm by Shabes and Joshi (1987). It generates both parse trees and derivation trees over each and every ambiguous parse it can find from the grammar provided.", "startOffset": 107, "endOffset": 120}], "year": 2015, "abstractText": "Tree adjoining grammars (TAGs) provide an ample tool to capture syntax of many Indian languages. Tamil represents a special challenge to computational formalisms as it has extensive agglutinative morphology and a comparatively difficult argument structure. Modelling Tamil syntax and morphology using TAG is an interesting problem which has not been in focus even though TAGs are over 4 decades old, since its inception. Our research with Tamil TAGs have shown us that we can not only represent syntax of the language, but to an extent mine out semantics through dependency resolution of the sentence. But in order to demonstrate this phenomenal property, we need to parse Tamil language sentences using TAGs we have built and through parsing obtain a derivation we could use to resolve dependencies, thus proving the semantic property. We use an in-house developed pseudo lexical TAG chart parser; algorithm given by Schabes and Joshi (1988), for generating derivations of sentences. We do not use any statistics to rank out ambiguous derivations but rather use all of them to understand the mentioned semantic relation with in TAGs for Tamil. We shall also present a brief parser analysis for the completeness of our discussions.", "creator": "Microsoft\u00ae Word 2010"}}}