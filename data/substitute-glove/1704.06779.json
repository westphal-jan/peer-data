{"id": "1704.06779", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "Lexical Features in Coreference Resolution: To be Used With Caution", "abstract": "Lexical features are once major finding of reference followed central - also - with - art coreference 1977-2010. Lexical series vehemently ideal some among long linguistic atmospheric at followed same granularity maintain. They only making useful for families early discussion of mentions. In this papers we implicated a drawback mainly materials their lexical included in official - seen - the - art coreference electroreception. We talk also if coreference resolvers mainly rely first parameters types, say can moment persuasively taking disappearing alternatively. Furthermore, done ever clearly into however coreference mandate evaluation is anything critical other only evaluating on a requirements separate of though specify co-ordinates saw which today that kind notable overlap early over training, expanding and possible sets.", "histories": [["v1", "Sat, 22 Apr 2017 09:59:42 GMT  (124kb,D)", "http://arxiv.org/abs/1704.06779v1", "6 pages, ACL 2017"]], "COMMENTS": "6 pages, ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nafise sadat moosavi", "michael strube 0001"], "accepted": true, "id": "1704.06779"}, "pdf": {"name": "1704.06779.pdf", "metadata": {"source": "CRF", "title": "Lexical Features in Coreference Resolution: To be Used With Caution", "authors": ["Nafise Sadat Moosavi", "Michael Strube"], "emails": ["nafise.moosavi@h-its.org", "michael.strube@h-its.org"], "sections": [{"heading": "1 Introduction", "text": "Similar to many other tasks, lexical features are a major source of information in current coreference resolvers. Coreference resolution is a set partitioning problem in which each resulting partition refers to an entity. As shown by Durrett and Klein (2013), lexical features implicitly model some linguistic phenomena, which were previously modeled by heuristic features, but at a finer level of granularity. However, we question whether the knowledge that is mainly captured by lexical features can be generalized to other domains.\nThe introduction of the CoNLL dataset enabled a significant boost in the performance of coreference resolvers, i.e. about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by Clark and Manning (2016b), and the winner of the CoNLL 2011 shared task, the Stanford rule-based system\nby Lee et al. (2013). However, this substantial improvement does not seem to be visible in downstream tasks. Worse, the difference between stateof-the-art coreference resolvers and the rule-based system drops significantly when they are applied on a new dataset, even with consistent definitions of mentions and coreference relations (Ghaddar and Langlais, 2016a).\nIn this paper, we show that if we mainly rely on lexical features, as it is the case in state-of-theart coreference resolvers, overfitting become more sever. Overfitting to the training dataset is a problem that cannot be completely avoided. However, there is a notable overlap between the CoNLL training, development and test sets that encourages overfitting. Therefore, the current coreference evaluation scheme is flawed by only evaluating on this overlapped validation set. To ensure meaningful improvements in coreference resolution, we believe an out-of-domain evaluation is a must in the coreference literature."}, {"heading": "2 Lexical Features", "text": "The large difference in performance between coreference resolvers that use lexical features and ones which do not, implies the importance of lexical features. Durrett and Klein (2013) show that lexical features implicitly capture some phenomena, e.g. definiteness and syntactic roles, which were previously modeled by heuristic features. Durrett and Klein (2013) use exact surface forms as lexical features. However, when word embeddings are used instead of surface forms, the use of lexical features is even more beneficial. Word embeddings are an efficient way of capturing semantic relatedness. Especially, they provide an efficient way for describing the context of mentions.\nDurrett and Klein (2013) show that the addition of some heuristic features like gender, num-\nar X\niv :1\n70 4.\n06 77\n9v 1\n[ cs\n.C L\n] 2\n2 A\npr 2\n01 7\nber, person and animacy agreements and syntactic roles on top of their lexical features does not result in a significant improvement.\ndeep-coref, the state-of-the-art coreference resolver, follows the same approach. Clark and Manning (2016b) capture the required information for resolving coreference relations by using a large number of lexical features and a small set of nonlexical features including string match, distance, mention type, speaker and genre features. The main difference is that Clark and Manning (2016b) use word embeddings instead of the exact surface forms that are used by Durrett and Klein (2013).\nBased on the error analysis by cort (Martschat and Strube, 2014), in comparison to systems that do not use word embeddings, deep-coref has fewer recall and precision errors especially for pronouns. For example, deep-coref correctly recognizes around 83 percent of non-anaphoric \u201cit\u201d in the CoNLL development set. This could be a direct result of a better context representation by word embeddings."}, {"heading": "3 Out-of-Domain Evaluation", "text": "Aside from the evident success of lexical features, it is debatable how well the knowledge that is mainly captured by the lexical information of the training data can be generalized to other domains. As reported by Ghaddar and Langlais (2016b), state-of-the-art coreference resolvers trained on the CoNLL dataset perform poorly, i.e. worse than the rule-based system (Lee et al., 2013), on the new dataset, WikiCoref (Ghaddar and Langlais, 2016b), even though WikiCoref is annotated with the same annotation guidelines as the CoNLL dataset. The results of some of recent coreference resolvers on this dataset are listed in Table 1.\nThe results are reported using MUC (Vilain\net al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.e. CoNLL score, and LEA (Moosavi and Strube, 2016).\nberkeley is the mention-ranking model of Durrett and Klein (2013) with the FINAL feature set including the head, first, last, preceding and following words of a mention, the ancestry, length, gender and number of a mention, distance of two mentions, whether the anaphor and antecedent are nested, same speaker and a small set of string match features.\ncort is the mention-ranking model of Martschat and Strube (2015). cort uses the following set of features: the head, first, last, preceding and following words of a mention, the ancestry, length, gender, number, type, semantic class, dependency relation and dependency governor of a mention, the named entity type of the head word, distance of two mentions, same speaker, whether the anaphor and antecedent are nested, and a set of string match features. berkeley and cort scores in Table 1 are taken from Ghaddar and Langlais (2016a).\ndeep-coref is the mention-ranking model of Clark and Manning (2016b). deep-coref incorporates a large set of embeddings, i.e. embeddings of the head, first, last, two previous/following words, and the dependency governor of a mention in addition to the averaged embeddings of the five previous/following words, all words of the mention, sentence words, and document words. deep-coref also incorporates type, length, and position of a mention, whether the mention is nested in any other mention, distance of two mentions, speaker features and a small set of string match features.\nFor deep-coref [conll] the averaged CoNLL score is used to select the best trained model on the development set. deep-coref [lea] uses the LEA\nmetric (Moosavi and Strube, 2016) for choosing the best model. It is worth noting that the results of deep-coref \u2019s ranking model may be slightly different at various experiments. However, the performance of deep-coref [lea] is always higher than that of deep-coref [conll].\nWe add WikiCoref\u2019s words to deep-coref \u2019s dictionary for both deep-coref [conll] and deep-coref [lea]. deep-coref\u2212 reports the performance of deep-coref [lea] in which WikiCoref\u2019s words are not incorporated into the dictionary. Therefore, for deep-coref\u2212, WikiCoref\u2019s words that do not exist in CoNLL will be initialized randomly instead of using pre-trained word2vec word embeddings. The performance gain of deep-coref [lea] in comparison to deep-coref\u2212 indicates the benefit of using pre-trained word embeddings and word embeddings in general. Henceforth, we refer to deep-coref [lea] as deep-coref."}, {"heading": "4 Why do Improvements Fade Away?", "text": "In this section, we investigate how much lexical features contribute to the fact that current improvements in coreference resolution do not properly apply to a new domain.\nTable 2 shows the ratio of non-pronominal coreferent mentions in the CoNLL test set that also appear as coreferent mentions in the training data. These high ratios indicate a high degree of overlap between the mentions of the CoNLL datasets.\nThe highest overlap between the training and test sets exists in genre pt (Bible). The tc (telephone conversation) genre has the lowest overlap for non-pronominal mentions. However, this genre includes a large number of pronouns. We choose wb (weblog) and pt for our analysis as two genres with low and high degree of overlap.\nTable 3 shows the results of the examined coreference resolvers when the test set only includes one genre, i.e. pt or wb, in two different settings: (1) the training set includes all genres (in-domain\nevaluation), and (2) the corresponding genre of the test set is excluded from the training and development sets (out-of-domain evaluation).\nberkeley-final is the coreference resolver of Durrett and Klein (2013) with the FINAL feature set explained in Section 3. berkeley-surface is the same coreference resolver with only surface features, i.e. ancestry, gender, number, same speaker and nested features are excluded from the FINAL feature set.\ncort\u2212lexical is a version of cort in which no lexical feature is used, i.e. the head, first, last, governor, preceding and following words of a mention are excluded.\nFor in-domain evaluations we train deep-coref \u2019s ranking model for 100 iterations, i.e. the setting used by Clark and Manning (2016a). However, based on the performance on the development set, we only train the model for 50 iterations in out-ofdomain evaluations.\nThe results of the pt genre show that when there is a high overlap between the training and test datasets, the performance of all learning-based classifiers significantly improves. deep-coref has the largest gain from including pt in the training data that is more than 13% based on the LEA score. cort uses both lexical and a relatively large number of non-lexical features while berkeley-surface is a pure lexicalized system. However, the difference between the berkeley-surface\u2019s performances when pt is included or excluded from the training data is lower than that of cort. berkeley uses feature-value pruning so lexical features that occur fewer than 20 times are pruned from the training data. Maybe, this is the reason that berkeley\u2019s performance difference is less than other lexicalized systems in highly overlapping datasets.\nFor a less overlapping genre, i.e. wb, the performance gain of including the genre in the training data is significantly lower for all lexicalized systems. Interestingly, the performance of berkeleyfinal, cort and cort\u2212lexical increases for the wb genre when this genre is excluded from the training set. deep-coref, which uses a complex deep neural network and mainly lexical features, has the highest gain from the redundancy in the training and test datasets. As we use more complex neural networks, there is more capacity for brute-force memorization of the training dataset.\nIt is also worth noting that the performance gains and drops in out-of-domain evaluations are\nnot entirely because of lexical features, as the performance of cort\u2212lexical also drops significantly in pt out-of-domain evaluation. The classifier may also memorize other properties of the seen mentions in the training data. However, in comparison to features like gender and number agreement or syntactic roles, lexical features have the highest potential for overfitting.\nWe further analyze the output of deep-coref on the development set. The all rows in Table 4 show the number of pairwise links that are created by deep-coref on the development set for different mention types. The seen rows show the ratio of each category of links for which the (antecedent head, anaphor head) pair is seen in the training set. All ratios are surprisingly high. The most worrisome cases are those in which both mentions are either a proper name or a common noun.\nTable 5 further divides the links of Table 4 based on whether they are correct coreferent links. The results of Table 5 show that most of the incorrect links are also made between the mentions that are both seen in the training data.\nThe high ratios indicate that (1) there is a high\noverlap between the mention pairs of the training and development sets, and (2) even though that deep-coref uses generalized word embeddings instead of exact surface forms, it is strongly biased towards the seen mentions.\nWe analyze the links that are created by Stanford\u2019s rule-based system and compute the ratio of the links that exist in the training set. All corresponding ratios are lower than those of deep-coref in Table 5. However, the ratios are surprisingly high for a system that does not use the training data. This analysis emphasizes the overlap in the CoNLL datasets. Because of this high overlap, it is not easy to assess the generalizability of a coreference resolver to unseen mentions on the CoNLL dataset given its official split.\nWe also compute the ratios of Table 5 for the missing links that are associated with the recall er-\nrors of deep-coref. We compute the recall errors by cort error analysis tool (Martschat and Strube, 2014). Table 6 shows the corresponding ratios for recall errors. The lower ratios of Table 6 in comparison to those of Table 4 emphasize the bias of deep-coref towards the seen mentions.\nFor example, the deep-coref links include 31 cases in which both mentions are either proper names or common nouns and the head of one of the mentions is \u201ccountry\u201d. For all these links, \u201ccountry\u201d is linked to a mention that is seen in the training data. Therefore, this raises the question how the classifier would perform on a text about countries not mentioned in the training data.\nMemorizing the pairs in which one of them is a common noun could help the classifier to capture world knowledge to some extent. From the seen pairs like (Haiti, his country), and (Guangzhou, the city) the classifier could learn that \u201cHaiti\u201d is a country and \u201cGuangzhou\u201d is a city. However, it is questionable how useful word knowledge is if it is mainly based on the training data.\nThe coreference relation of two nominal noun phrases with no head match can be very hard to resolve. The resolution of such pairs has been referred to as capturing semantic similarity (Clark and Manning, 2016b). deep-coref links 49 such pairs on the development set. Among all these links, only 5 pairs are unseen on the training set and all of them are incorrect links.\nThe effect of lexical features is also analyzed by Levy et al. (2015) for tasks like hypernymy and entailment. They show that state-of-the-art classifiers memorize words from the training data. The classifiers benefit from this lexical memorization when there are common words between the training and test sets."}, {"heading": "5 Discussion", "text": "We show the extensive use of lexical features biases coreference resolvers towards seen mentions.\nThis bias holds us back from developing more robust and generalizable coreference resolvers. After all, while coreference resolution is an important step for text understanding, it is not an endtask. Coreference resolvers are going to be used in tasks and domains for which coreference annotated corpora may not be available. Therefore, generalizability should be brought into attention in developing coreference resolvers.\nMoreover, we show that there is a significant overlap between the training and validation sets in the CoNLL dataset. The LEA metric (Moosavi and Strube, 2016) is introduced as an attempt to make coreference evaluations more reliable. However, in order to ensure valid developments on coreference resolution, it is not enough to have reliable evaluation metrics. The validation set on which the evaluations are performed also needs to be reliable. A dataset is reliable for evaluations if a considerable improvement on this dataset indicates a better solution for the coreference problem instead of a better exploitation of the dataset itself.\nThis paper is not intended to argue against the use of lexical features. Especially, when word embeddings are used as lexical features. The incorporation of word embeddings is an efficient way for capturing semantic relatedness. Maybe we should use them more for describing the context and less for describing the mentions themselves. Pruning rare lexical features plus incorporating more generalizable features could also help to prevent overfitting.\nTo ensure more meaningful improvements, we ask to incorporate out-of-domain evaluations in the current coreference evaluation scheme. Outof-domain evaluations could be performed by using either the existing genres of the CoNLL dataset or by using other existing coreference annotated datasets like WikiCoref, MUC or ACE."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Kevin Clark for answering all of our questions regarding deepcoref. We would also like to thank the three anonymous reviewers for their thoughtful comments. This work has been funded by the Klaus Tschira Foundation, Heidelberg, Germany. The first author has been supported by a Heidelberg Institute for Theoretical Studies PhD. scholarship."}], "references": [{"title": "Algorithms for scoring coreference chains", "author": ["Amit Bagga", "Breck Baldwin."], "venue": "Proceedings of the 1st International Conference on Language Resources and Evaluation, Granada, Spain, 28\u201330 May 1998. pages 563\u2013566.", "citeRegEx": "Bagga and Baldwin.,? 1998", "shortCiteRegEx": "Bagga and Baldwin.", "year": 1998}, {"title": "Deep reinforcement learning for mentionranking coreference models", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin,", "citeRegEx": "Clark and Manning.,? 2016a", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Improving coreference resolution by learning entitylevel distributed representations", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long", "citeRegEx": "Clark and Manning.,? 2016b", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Easy victories and uphill battles in coreference resolution", "author": ["Greg Durrett", "Dan Klein."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Wash., 18\u201321 October 2013. pages 1971\u20131982.", "citeRegEx": "Durrett and Klein.,? 2013", "shortCiteRegEx": "Durrett and Klein.", "year": 2013}, {"title": "Coreference in Wikipedia: Main concept resolution", "author": ["Abbas Ghaddar", "Philippe Langlais."], "venue": "Proceedings of the 20th Conference on Computational Natural Language Learning, Berlin, Germany, 11\u201312 August 2016. pages 229\u2013238.", "citeRegEx": "Ghaddar and Langlais.,? 2016a", "shortCiteRegEx": "Ghaddar and Langlais.", "year": 2016}, {"title": "WikiCoref: An English coreference-annotated corpus of Wikipedia articles", "author": ["Abbas Ghaddar", "Philippe Langlais."], "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation, Portoro\u017e,", "citeRegEx": "Ghaddar and Langlais.,? 2016b", "shortCiteRegEx": "Ghaddar and Langlais.", "year": 2016}, {"title": "Deterministic coreference resolution based on entity-centric, precision-ranked rules", "author": ["Heeyoung Lee", "Angel Chang", "Yves Peirsman", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky."], "venue": "Computational Linguistics 39(4):885\u2013916.", "citeRegEx": "Lee et al\\.,? 2013", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "On coreference resolution performance metrics", "author": ["Xiaoqiang Luo."], "venue": "Proceedings of the Human Language Technology Conference and the 2005", "citeRegEx": "Luo.,? 2005", "shortCiteRegEx": "Luo.", "year": 2005}, {"title": "Recall error analysis for coreference resolution", "author": ["Sebastian Martschat", "Michael Strube."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, 25\u201329 October 2014. pages 2070\u20132081.", "citeRegEx": "Martschat and Strube.,? 2014", "shortCiteRegEx": "Martschat and Strube.", "year": 2014}, {"title": "Latent structures for coreference resolution", "author": ["Sebastian Martschat", "Michael Strube."], "venue": "Transactions of the Association for Computational Linguistics 3:405\u2013418. http://www.aclweb.org/anthology/Q151029.pdf.", "citeRegEx": "Martschat and Strube.,? 2015", "shortCiteRegEx": "Martschat and Strube.", "year": 2015}, {"title": "Which coreference evaluation metric do you trust? A proposal for a link-based entity aware metric", "author": ["Nafise Sadat Moosavi", "Michael Strube."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational", "citeRegEx": "Moosavi and Strube.,? 2016", "shortCiteRegEx": "Moosavi and Strube.", "year": 2016}, {"title": "A modeltheoretic coreference scoring scheme", "author": ["Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman."], "venue": "Proceedings of the 6th Message Understanding Conference (MUC-6). Morgan Kaufmann, San Mateo, Cal.,", "citeRegEx": "Vilain et al\\.,? 1995", "shortCiteRegEx": "Vilain et al\\.", "year": 1995}], "referenceMentions": [{"referenceID": 3, "context": "As shown by Durrett and Klein (2013), lexical features implicitly model some linguistic phenomena, which were previously modeled by heuristic features, but at a finer level of granularity.", "startOffset": 12, "endOffset": 37}, {"referenceID": 1, "context": "about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by Clark and Manning (2016b), and the winner of the CoNLL 2011 shared task, the Stanford rule-based system by Lee et al.", "startOffset": 110, "endOffset": 136}, {"referenceID": 1, "context": "about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by Clark and Manning (2016b), and the winner of the CoNLL 2011 shared task, the Stanford rule-based system by Lee et al. (2013). However, this substantial improvement does not seem to be visible in downstream tasks.", "startOffset": 110, "endOffset": 235}, {"referenceID": 4, "context": "on a new dataset, even with consistent definitions of mentions and coreference relations (Ghaddar and Langlais, 2016a).", "startOffset": 89, "endOffset": 118}, {"referenceID": 3, "context": "Durrett and Klein (2013) show that lexical features implicitly capture some phenomena, e.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Durrett and Klein (2013) use exact surface forms as lexical features.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "Clark and Manning (2016b) capture the required information", "startOffset": 0, "endOffset": 26}, {"referenceID": 1, "context": "The main difference is that Clark and Manning (2016b)", "startOffset": 28, "endOffset": 54}, {"referenceID": 3, "context": "use word embeddings instead of the exact surface forms that are used by Durrett and Klein (2013).", "startOffset": 72, "endOffset": 97}, {"referenceID": 9, "context": "Based on the error analysis by cort (Martschat and Strube, 2014), in comparison to systems that do not use word embeddings, deep-coref has fewer recall and precision errors especially for pronouns.", "startOffset": 36, "endOffset": 64}, {"referenceID": 4, "context": "As reported by Ghaddar and Langlais (2016b),", "startOffset": 15, "endOffset": 44}, {"referenceID": 6, "context": "worse than the rule-based system (Lee et al., 2013), on the new dataset, WikiCoref (Ghaddar and Langlais, 2016b), even though WikiCoref is annotated with the same annotation guidelines as the CoNLL dataset.", "startOffset": 33, "endOffset": 51}, {"referenceID": 5, "context": ", 2013), on the new dataset, WikiCoref (Ghaddar and Langlais, 2016b), even though WikiCoref is annotated with the same annotation guidelines as the CoNLL dataset.", "startOffset": 39, "endOffset": 68}, {"referenceID": 12, "context": "The results are reported using MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": ", 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.", "startOffset": 12, "endOffset": 37}, {"referenceID": 8, "context": ", 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.", "startOffset": 45, "endOffset": 56}, {"referenceID": 11, "context": "CoNLL score, and LEA (Moosavi and Strube, 2016).", "startOffset": 21, "endOffset": 47}, {"referenceID": 3, "context": "berkeley is the mention-ranking model of Durrett and Klein (2013) with the FINAL feature set including the head, first, last, preceding and following words of a mention, the ancestry, length,", "startOffset": 41, "endOffset": 66}, {"referenceID": 9, "context": "cort is the mention-ranking model of Martschat and Strube (2015). cort uses the following set of features: the head, first, last, preceding and following words of a mention, the ancestry, length,", "startOffset": 37, "endOffset": 65}, {"referenceID": 4, "context": "berkeley and cort scores in Table 1 are taken from Ghaddar and Langlais (2016a).", "startOffset": 51, "endOffset": 80}, {"referenceID": 1, "context": "deep-coref is the mention-ranking model of Clark and Manning (2016b). deep-coref incorporates a large set of embeddings, i.", "startOffset": 43, "endOffset": 69}, {"referenceID": 11, "context": "metric (Moosavi and Strube, 2016) for choosing the best model.", "startOffset": 7, "endOffset": 33}, {"referenceID": 3, "context": "berkeley-final is the coreference resolver of Durrett and Klein (2013) with the FINAL feature", "startOffset": 46, "endOffset": 71}, {"referenceID": 1, "context": "used by Clark and Manning (2016a). However, based on the performance on the development set, we only train the model for 50 iterations in out-ofdomain evaluations.", "startOffset": 8, "endOffset": 34}, {"referenceID": 9, "context": "We compute the recall errors by cort error analysis tool (Martschat and Strube, 2014).", "startOffset": 57, "endOffset": 85}, {"referenceID": 2, "context": "The resolution of such pairs has been referred to as capturing semantic similarity (Clark and Manning, 2016b).", "startOffset": 83, "endOffset": 109}, {"referenceID": 7, "context": "The effect of lexical features is also analyzed by Levy et al. (2015) for tasks like hypernymy and entailment.", "startOffset": 51, "endOffset": 70}, {"referenceID": 11, "context": "The LEA metric (Moosavi and Strube, 2016) is introduced as an attempt to make coreference evaluations more reliable.", "startOffset": 15, "endOffset": 41}], "year": 2017, "abstractText": "Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the linguistic phenomena at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.", "creator": "LaTeX with hyperref package"}}}