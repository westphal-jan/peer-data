{"id": "1704.06850", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "Testing from One Sample: Is the casino really using a riffle shuffle?", "abstract": "Classical reduced laboratory depends require intended whatever. thinking. levin. extracts few be distributions made are appeared anabolic. We initiate made work important Markov shops testing, impossible enter bring his single exact from created Markov Chains come are so cases. In particular, might them would observe first single trajectories X_0, .. ., X_t, .. . of an unknown Markov Chain M, for though we do either probably do to administration the distribution of which starting county X_0. Our score exists turn test believe M long present give a developed Markov Chain M_0. In a first new an next press, our implementation his considering entire balance between past Markov chains, which brilliant the m3 physical over the 38 correlation distance four uttered acoustically where there Markov chains addition under length of numerous phrase grows. We provide efficient but identical port - functional fagor for expression testing end our proposed example the combination. In however second part although, well, get graduate Markov merchandise well local array become exponentially part their location, receiving retest given testing explain of card shuffles. We necessary mind advance one testing the authorship of is Gilbert, Shannon, and Reeds model for came riffle rung.", "histories": [["v1", "Sat, 22 Apr 2017 21:02:31 GMT  (682kb,D)", "http://arxiv.org/abs/1704.06850v1", "35 pages, 5 figures"]], "COMMENTS": "35 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["constantinos daskalakis", "nishanth dikkala", "nick gravin"], "accepted": false, "id": "1704.06850"}, "pdf": {"name": "1704.06850.pdf", "metadata": {"source": "CRF", "title": "Testing from One Sample: Is the casino really using a riffle shuffle?", "authors": ["Constantinos Daskalakis", "Nishanth Dikkala", "Nick Gravin"], "emails": ["costis@mit.edu", "nishanthd@csail.mit.edu", "ngravin@gmail.com"], "sections": [{"heading": null, "text": "In the first part of the paper, we propose a measure of difference between two Markov chains, which captures the scaling behavior of the total variation distance between words sampled from the Markov chains as the length of these words grows. We provide efficient and sample nearoptimal testers for identity testing under our proposed measure of difference. In the second part of the paper, we study Markov chains whose state space is exponential in their description, providing testers for testing identity of card shuffles. We apply our results to testing the validity of the Gilbert, Shannon, and Reeds model for the riffle shuffle.\n\u2217Supported by a Microsoft Research Faculty Fellowship, and NSF Award CCF-1551875, CCF-1617730 and CCF1650733.\n\u2020Supported by NSF Award CCF-1551875, CCF-1617730 and CCF-1650733. \u2021Supported by NSF Award CCF-1551875, CCF-1617730 and CCF-1650733.\nar X\niv :1\n70 4.\n06 85\n0v 1\n[ cs\n.L G\n] 2\n2 A\npr 2"}, {"heading": "1 Introduction", "text": "We formulate theories about the laws that govern physical phenomena by making observations and testing them against our hypotheses. A common scenario is when our observations can be reasonably modeled as i.i.d. samples from a distribution that we are trying to understand. This is the setting tackled by most classical work in Statistics. Of course, having access to i.i.d. samples from a distribution is rare and quite commonly a mere approximation of reality. We typically only have access to approximate samples from a stationary distribution, sampled by a Markov chain whose transition matrix/kernel is unknown to us and which can only be observed for some finite time horizon. In fact, to the best of our knowledge, the underlying Markov chain may not even be rapidly mixing, so there is no guarantee that we will ever see samples that are approximately distributed according to the stationary distribution.\nThese issues are exacerbated in high-dimensional settings, e.g., when observing the configurations of a deck of cards or a weather system, where it may also be completely impractical to work with the high-dimensional stationary distribution itself. Moreover, there are several ways to sample a stationary distribution, so it may be more important to know how it comes to be sampled. For all these considerations, it may be both more interesting and more practical to understand the \u201cmechanics\u201d of the process that generates our observations, namely the transition matrix/kernel of the Markov chain whose evolution we get to observe.\nMotivated by these considerations, in this paper we study the problem of testing the identity of Markov chains. We are given access to a single trajectory X0, X1, . . . , Xt, . . . of some unknown Markov chain M over some state space [n], and we want to test the identity of M to some given Markov chain M\u2032. Importantly, we do not get to control the distribution of the starting state X0, and we can only observe a single trajectory of M, i.e. we cannot restart the Markov chain. What could we hope to achieve?\nIf there is any difference in the transition matrices ofM andM\u2032, one would think that we would ultimately be able to identify it by observing a sufficiently long trajectory. This is certainly true if the transition matrices of the two chains differ at a state that belongs to the strongly connected component of M absorbing the observed trajectory (Xt)t. For instance, suppose that M is a chain on states {1, 2, . . . , 7} whose transition matrix is the random walk matrix on a graph that is the disjoint union of a square on nodes {1, . . . , 4} and a triangle on nodes {5, 6, 7}, while M\u2032\u2019s transition matrix is the random walk matrix on a graph that is the disjoint union of a clique on nodes {1, . . . , 4} and a triangle on nodes {5, 6, 7}. If our observed trajectory ofM lies in the strong connected component defined by states {1, . . . , 4}, we will easily identify its difference to M\u2032. On the other hand, if our observed trajectory of M lies on the strong component defined by states {5, 6, 7}, we will not be able to identify that we are not observing a trajectory of M\u2032, no matter how long the trajectory is.\nFor some notion of difference, Dist (M,M\u2032), between Markov chains, we would like to quantify how long a trajectory X0, . . . , X` from an unknown chain, M, we need to observe to be able to distinguish, with probability at least 1\u2212 \u03b4:\nM =M\u2032 versus Dist ( M,M\u2032 ) > , (1)\nfor some given parameters \u03b4 \u2208 (0, 1) and > 0. Let us call this problem single-sample goodness-offit (or identity) testing for Markov chains. We will study it taking \u03b4 = 1/3, with the understanding that this probability can be boosted to any small constant at the cost of a O(log(1/\u03b4)-multiplicative factor in the length ` of the observed trajectory.\nWhat notion of difference between Markov chains is the right one to use to study the aforedescribed goodness-of-fit testing problem is not clear. One of our contributions is to identify a meaningful measure of difference in Section 2. What are the desiderata for such a measure? Let us discuss this:\n1. First, as our simple example above illustrates, under a worst-case starting state X0, we may not be able to identify that M 6=M\u2032 from a single trajectory. So, we would like to identify a notion of difference that takes a value Dist (M,M\u2032) = 0, whenever chains M and M\u2032 are indistinguishable from a single trajectory.\n2. Whenever M and M\u2032 are distinguishable from a single trajectory, whose starting state we do not get to control, i.e. from any starting state, we would like that our difference measure quantifies how different the chains are. Clearly, our notion of difference could not just be a combinatorial property of the connectivity of the state space of M and M\u2032, since the combinatorial structure won\u2019t reflect the magnitude of the differences in the chains.\nA natural notion of difference between two chains M and M\u2032 is the total variation distance between the trajectories (a.k.a. words) WtM def = X0X1 \u00b7 \u00b7 \u00b7Xt and WtM\u2032 def = Y0Y1 \u00b7 \u00b7 \u00b7Yt sampled from the two chains in some t steps and starting at some state X0 = s0 = Y0. Indeed, this distance captures how different our t-step observations from the two chains are. As the starting state s0 is out of our control, we should rather use the\nmin s0 dTV\n( WtM ,W t M\u2032 | X0 = Y0 = s0 ) . (2)\nIn particular, taking the min makes sure that Property 1 above is satisfied. While there is sometimes a natural t to use in (2) as we will see in Section 4, where we analyze card shuffles, it is mostly unclear how to set t. Setting t to infinity will make the above quantity take 0/1 values, which is un-informative about how different the two chains are. Setting t too small would only capture differences in the proximity of the starting state s0. Hence, setting t to some reasonably large but finite value makes more sense, but how to choose it?\nTo avoid the conundrum, we propose a notion of difference between Markov chainsM andM\u2032, which captures the scaling behavior, as t \u2192 \u221e, of (2). Interestingly, we argue in Section 2 that this scaling behavior is tightly captured by spectral properties of the following matrix:\n[P,Q]\u221a def = [\u221a Pij \u00b7Qij ] ij\u2208[n\u00d7n] ,\nwhere P and Q are the transition matrices of the two chains, i.e. Pij and Qij denote the probabilities of transitioning from state i to state j in the two chains. In Eq. (4), we show a recursive decomposition that allows us to exactly express the square Hellinger similarity, 1\u2212d2 Hel ( WtM ,W t M\u2032 ) of `-length words sampled from the two chains in terms of the `-th power of the above matrix, and the distribution of the starting states X0 and Y0 in the two words. Given the relationship between Hellinger and total variation distance (see Eq. (3)), the `-the power of [P,Q]\u221a also captures the total variation distance between words sampled from the two chains.\nTo identify a word-length independent measure of difference between the two chains, we argue that the scaling behavior of the Hellinger distance/total variation distance is captured by the largest eigenvalue \u03bb1 = \u03c1([P,Q]\u221a) of matrix [P,Q]\u221a. We show that always \u03bb1 \u2264 1 (Claim 1), and that \u03bb1 = 1\nif and only if the two chains have an identical connected component (Claim 1), hence we would be unable to identify the difference between the two chains from a single trajectory and a worst-case starting state, as per our discussion above. Furthermore, the slowest (with respect to the choice of the starting state) that the square Hellinger similarity of the two chains can drop as a function of the length ` is \u03bb`1, up to factors that do not depend on `; this follows from (4) and (6). That is, the slowest that the square Hellinger distance of the two chains can increase is 1 \u2212 O(\u03bb`1). Given these, and the intimate relationship between Hellinger and total variation distance (Eq. (3)), we propose the use of Dist (M,M\u2032) = 1\u2212\u03c1([P,Q]\u221a) as a scale-free and meaningful measure of difference between Markov chains. As per our discussion, this notion of distance satisfies Desiderata 1 and 2 outlined above. For symmetric Markov chains, our notion of difference is even more tightly related to the scaling behavior of trajectories sampled from the two chains even for a starting state sampled from the stationary distribution, as per Claim 2. Figure 1 illustrates how Dist (M,M\u2032) behaves for different pairs of Markov chains M and M\u2032.\nOur Results. Using our proposed measure of difference between Markov chains we provide algorithms for goodness-of-fit testing of Markov chains, namely Problem (1), targeting two interesting regimes. The first targets applications where the state space is polynomial in the representation of the target Markov chain M\u2032. The second targets settings where the state space is exponential in the representation of the target chain, but the chain has sparsity and structure. Importantly, this is applicable to testing card shuffles. Our results in the two settings are the following.\nIn Section 3, we study Problem (1) under Dist (M,M\u2032) = 1 \u2212 \u03c1([P,Q]\u221a), where P and Q are the transition matrices of chains M and M\u2032. We study this problem when M and M\u2032 are both symmetric, and provide near-optimal upper and lower bounds for the minimum length ` of a trajectory from the unknown chain M that is needed to determine the correct answer with probability at least 2/3. In particular, Theorems 3.1 and 3.2 combined show that the length of the required trajectory from M to answer Problem (1) is n/ , where n is the size of the state space, up to logarithmic factors and an additive term that does not depend on orM. Our upper bound is established via an information-efficient reduction from single-sample identity testing for Markov chains with n states to the classical problem of identity testing of distributions over O(n2) elements, from i.i.d. samples. The naive way to obtain such a reduction is to look at every MixTM\u2032th step of the trajectory of M, where MixTM\u2032 is the mixing time of chain M\u2032, pretending that these transitions are i.i.d. samples from the distribution { 1nPij}ij\u2208[n2]. This incurs an unnecessary blow-up of a factor of MixTM\u2032 in the required length of the observed trajectory, which we show how to avoid, exchanging it with an additive term that is quasi-linear in the hitting time.\nIn Section 4, we take on the challenge of testing Markov chains whose nominal state-space is exponentially large in their representation, such as shuffles. Often, shuffles have symmetries that allow studying their transitions in a state space of manageable size. For example, the random choices that the riffle shuffle makes in the course of one step do not depend on the permutation of the cards at the beginning of the step, and can be described succinctly. Moreover, viewed appropriately the transitions out of any state are typically sparse. In the riffle shuffle, starting from any permutation there are n+ 1 places to cut the deck. And, starting from a cut, every little step of the riffle has two options (whether to drop a card from the left or the right stack). So breaking down one step of the riffle shuffle into a sequence of simple steps makes the transitions very sparse.\nIn Definition 2 we provide a model of sparse Markov chains, capturing succinct representations of Markov chains resulting from \u201cbreaking up into trivial steps and projecting into a smaller state\nspace\u201d of Markov chains with exponentially large state-spaces such as different variants of the riffle shuffle and other shuffles [BD92]. Roughly speaking a sparse Markov chain in our model performs transitions according to a sequence of transition matrices P1, . . . , Pn, over and over again, ad infinitum. We discuss how this model captures the essential mechanics of the riffle shuffle after Definition 2.\nKeeping the riffle shuffle as our running application, we develop tools that allow us to perform goodness-of-fit testing of sparse Markov chains according to our model. What difference measure between chains should we use? Given two sparse chains P = (Pi)i\u2208[n] and Q = (Qi)i\u2208[n], conforming to our model, we could still define our difference measure between them using spectral properties of matrix [P1, Q1]\u221a \u00b7 \u00b7 \u00b7 [Pt, Qt]\u221a \u00b7 \u00b7 \u00b7 [Pn, Qn]\u221a. However, we find it more natural in this case to use as difference measure the total variation distance between words sampled in one round of sampling transitions from the sequence of matrices P1, . . . , Pn and Q1, . . . , Qn.\n1 This total variation distance captures the divergence of the two chains in one iteration through their transitions matrices; in our running application to riffle shuffle, they correspond to the divergence of two riffle shuffles of different parameters in one iteration of the shuffle. With this notion of difference we provide efficient testers, and sample complexity lower bounds; see Theorems 4.1 and 4.4. Our upper bounds imply, in particular, that we can test goodness-of-fit of a given riffle shuffle model, such as the GilbertShannon-Reeds model, against all competing riffle shuffle models at distance \u2265 , from O(n3/2/ 2) shuffles. Our tester, applying to testing any sparse Markov chain model, is based on a modified and pruned \u03c72-style statistic, inspired by [ADK15], which tracks the number of times a particular transition between two states occured in the observed trajectory of the Markov chain.\nRelated Work. Testing goodness-of-fit for distributions has a long history in Statistics; for some old and more recent references see, e.g., [Pea00, Fis35, RS81, Agr12]. In this literature the emphasis has been on the asymptotic analysis of tests, pinning down their error exponents as the number of samples tends to infinity [Agr12, TAW10]. In the last two decades or so, distribution testing has also piqued the interest of theoretical computer scientists [BFF+01, Pan08, LRR13, VV14, CDVV14, ADK15, CDGR16, DK16, DDS+13, CRS14], where the emphasis, in contrast, has been on minimizing the number of samples required to test hypotheses with a strong control for both type I and type II errors. A few recent works have identified tight upper and lower bounds on the sample complexity of various testing problems [Pan08, VV14, ADK15, DK16]. All of the papers in this vast body of literature assume access to i.i.d. samples from the underlying distribution.\nSome work in Statistics has considered the problem of testing with dependent samples. For instance, [Bar51, M+82, GM+83, MMPV02] and the references therein study goodness-of-fit testing for Markov chains. [TA83] and more recently [BPR16] study the problem of testing the stationary distribution of Markov chains. [Kaz78] studies the problem of detection between two Markov chains. All these works focus on the asymptotic regime where the length of the observed trajectories tends to infinity, as opposed to the non-asymptotic regime that we study here. In the computer science literature [BFR+13] considered the problem of testing the property whether a Markov chain is fast mixing or not. They defined a notion of closeness between two random walks started at different states of the same chain, which is similar in spirit to the distance notion we define in this work.\nThere is a large body of statistical literature on estimating properties of Markov chains such as mixing time. The question of estimation is related to but different than the goodness-of-fit testing\n1We argue, in Section 4, that the spectral and total variation distance approaches to define difference in this model are tightly related, as they are in the symmetric case.\nthat we perform here. A particularly important parameter is the mixing time of a Markov chain, as it is useful in designing MCMC algorithms. [HKS15] and the references therein study the problem of mixing time estimation in Markov chains.\nOrganization We start with a description of the notational conventions we use and a formal definition of our distance notion between Markov chains in Section 2. In Section 3, we study the problem of testing identity of symmetric Markov chains. We present our tester along with a sample complexity lower bound for this problem in this Section. In Section 4, we study the identity testing question on riffle shuffles. We present our formulation of this question as a problem of testing identity for sparse Markov chains and present upper and lower bounds for this problem. Finally, in Section 5, we conclude with some open questions from the framework introduced in this paper."}, {"heading": "2 Preliminaries", "text": "A discrete-time Markov chain is a stochastic process {Xt}t\u2208{0,1,...} over a state space S which satisfies the Markov property: the probability of being in state s at time t+ 1 depends only on the state at previous time t. In this paper, we only consider Markov chains with a finite state space such that |S| = n. Such Markov chains can be completely specified by a n\u00d7n transition matrix (kernel) that contains probabilities of transitioning from state i to state j in the ith row and jth column. The transition matrix has non-negative entries and is a stochastic matrix. We use capital letters P,Q,M to represent Markov chains as well as their respective transition matrices. The stationary distribution \u03c0 of a Markov chain P is a distribution over the state space S (written as a column vector) such that it satisfies \u03c0> \u25e6 P = \u03c0>. An important parameter in the study of Markov chains is the distribution of the starting state s0 which we denote by ~p (for the Markov chain P ). It may or not may not be the stationary distribution. In many cases it will be the distribution with all probability mass at a single state. Two popular notions of distance between distributions will be used heavily in this paper. We state their formal definitions below.\nDefinition 1. The total variation and Hellinger distances between distributions p, q over [k]: dTV (p, q) def = 12 \u2211 i\u2208[k] |pi \u2212 qi|; d2Hel (p, q) def = 12 \u2211 i\u2208[k] (\u221a pi \u2212 \u221a qi )2 = 1\u2212 \u2211 i\u2208[k] \u221a pi \u00b7 qi;\n\u221a 2 \u00b7 d\nHel (p, q) \u2265 dTV (p, q) \u2265 d 2 Hel (p, q) . (3)\nWe now state without proof a well-known result which relates the total variation distance between two distributions to the best achievable distinguishability between the two. This will be useful in understanding our proposed notion of distance between two chains.\nLemma 2.1. Given two distributions p, q and a sample x drawn from either p or q, no algorithm can distinguish whether x was drawn from p or q with probability more than 1+d TV (p,q)\n2 ."}, {"heading": "2.1 Notations", "text": "We list the general notational conventions used in this paper. We denote vectors by small letters such as ~v and matrices by capital letters such as A,B,P ,Q. The ith entry of vector ~v is denoted by vi or v[i] and the (ij)th entry of matrix A (ith row, jth column) is denoted by Aij or A[ij]; ~ei denotes the\nstandard basis vector with 1 in its ith coordinate and 0 elsewhere; ~1 denotes the vector of all ones. The \u201centrywise\u201d L1 and L2 norms of a matrix A are respectfully denoted as \u2016A\u20161 and \u2016A\u20162; \u03c1 (A) denotes the spectral radius of matrix A, i.e., the maximum absolute eigenvalue of A. The eigenvalues of A are denoted by \u03bb1, . . . , \u03bbi, . . . , \u03bbn and the respective right eigenvectors by ~v1, . . . , ~vi, . . . , ~vn (left eigenvectors by ~u1, . . . , ~un); for symmetric matrix A we assume that \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbi \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbn.\nBefore formulating the precise question we study, we need a notion of distance between Markov chains to work with."}, {"heading": "2.2 Distance between Markov Chains", "text": "We begin by considering the following simple question: how close is the behavior of two given Markov chains P and Q? A natural notion of distance would tell us how easy it is to distinguish which Markov chain P or Q a word w = s0 \u2192 s1 \u00b7 \u00b7 \u00b7 \u2192 s` of certain length ` was generated from. The answer to this question is precisely captured by the total variation distance dTV ( W` P ,W` Q\n) between word distributions W`\nP , W` Q for words of length ` generated by Markov chains P , and\nrespectively Q (see Lemma 2.1). As a proxy for the total variation distance dTV\n( W`\nP ,W` Q\n) , it\nis more convenient to use square of the Hellinger distance d2 Hel\n( W`\nP ,W` Q\n) or the closely related\nBhattacharya coefficient2, which is useful for studying divergence of non-stationary and continuous Markov chains as was observed in [Kaz78]. Similar to [Kaz78], we can establish nice recurrence relations for the Bhattacharya coefficient of two word distributions, which is captured by the matrix\n[P,Q]\u221a def = [\u221a Pij \u00b7Qij ] i,j\u2208[n\u00d7n].\n1\u2212 d2 Hel\n( W`\nP ,W` Q ) = [~p, ~q]>\u221a \u25e6 ( [P,Q]\u221a )` \u25e6 ~1, (4)\n1\u2212 d2 Hel\n( W`\nP ,W` Q\n) = \u2211 w=s0...s` \u221a Pr P [w] Pr Q [w] =  \u2211 w=s0...s` s`=s \u221a Pr P [w] Pr Q [w]  >\ns\u2208[n]\n\u25e6 ~1\n= \u2211 r\u2208[n] \u221a Pr P [r \u2192 s] Pr Q [r \u2192 s] \u2211\nw=s0...s`\u22121 s`\u22121=r\n\u221a Pr P [w] Pr Q [w]  >\ns\u2208[n]\n\u25e6 ~1\n=  \u2211 w=s0...s`\u22121 s`\u22121=r \u221a Pr P [w] Pr Q [w]  >\nr\u2208[n]\n\u25e6  ... \u00b7 \u00b7 \u00b7 \u221a Prs \u00b7Qrs \u00b7 \u00b7 \u00b7\n...\n r,s\u2208[n\u00d7n] \u25e6 ~1\n=  \u2211 w=s0...s`\u22121 s`\u22121=r \u221a Pr P [w] Pr Q [w]  >\nr\u2208[n]\n\u25e6 [P,Q]\u221a \u25e6 ~1 = [~p, ~q] > \u221a \u25e6 ( [P,Q]\u221a )` \u25e6 ~1,\n2Hellinger distance is tightly related to the Bhattacharya coefficient between two distributions which is defined as BC(p, q) = \u2211 i\u2208[k] \u221a pi \u00b7 qi. It captures similarity of two distributions and lies in [0, 1].\nwhere ~p and ~q are vectors of initial distributions of s0 over [n], and [~p, ~q]\u221a def = [\u221a ps \u00b7 qs ] s\u2208[n]. An important observation is that the distance between W` P and W` Q\nabove depends on the initial distribution of the first state in w, and also the length ` of the word.\nAssumption on the starting state. We study two reasonable models for the choice of the starting state: (i) a worst-case model where both P and Q begin from the same state i, which is chosen in adversarial manner to make P and Q look as much alike as possible; (ii) an average-case model, where the initial distributions ~p = ~q for P and Q either are given to us, or are related to P and Q in some natural way3. Given the assumption on the starting state we want to answer the question of what ` to pick, so that W`\nP and W` Q are far apart in squared Hellinger distance (say\n\u2265 0.50). For the worst-case and average-case starting state models we respectfully get\nmin `>0\n` : \u2200i \u2208 [n] 0.5 \u2265 1\u2212 d2 Hel\n( W`\nP ,W` Q ) = ~e>i \u25e6 ( [P,Q]\u221a )` \u25e6 ~1. (5)\nmin `>0\n` : 0.5 \u2265 1\u2212 d2 Hel\n( W`\nP ,W` Q ) = [~p, ~q]>\u221a \u25e6 ( [P,Q]\u221a )` \u25e6 ~1\nDue to the relation between Hellinger and total variation distances, the inequality (5) holds for 1 \u2212 dTV ( W` P ,W` Q ) but for a slightly different than 0.5 constant. We call minimal ` that satisfies\ndTV\n( W`\nP ,W` Q ) \u2265 23 either for all starting states i \u2208 [n], or for fixed ~p, ~q the minimal distinguishing\nlength. We note that (5) gives us an estimate on ` up to a constant factor. We note that when ` is large, the behavior of RHS of (5) is governed by the largest eigenvalue \u03bb1 = \u03c1 ( [P,Q]\u221a ) of [P,Q]\u221a. By Perron-Frobenius theorem, we have that the largest eigenvalue of [P,Q]\u221a is non-negative and the corresponding left eigenvector ~u1 : ~u > 1 \u25e6 [P,Q]\u221a = \u03bb1 \u00b7 ~u>1 has non-negative coordinates. In particular, if we choose initial distributions ~p = ~q proportional to ~u1, then\n~p> \u25e6 (\n[P,Q]\u221a )` \u25e6 ~1 = \u03bb`1 \u00b7 \u3008~p, ~1\u3009 = \u03bb`1. (6)\nClaim 1. It is always true that \u03bb1 = \u03c1 ( [P,Q]\u221a ) \u2264 1. Moreover, \u03bb1 = 1 iff P and Q have an identical connected component4.\nProof. Note that P+Q2 is a stochastic matrix that entry-wise dominates matrix [P,Q]\u221a with nonnegative entries. Therefore, \u03bb1 \u00b7 \u3008~u1, ~1\u3009 = ~u>1 \u25e6 [P,Q]\u221a \u25e6 ~1 \u2264 ~u>1 \u25e6 [ P+Q 2 ] \u25e6 ~1 = ~u>1 \u25e6 ~1 = \u3008~u1, ~1\u3009, where ~1 is vector with all 1 entries. We get \u03bb1 \u2264 1, since \u3008~u1, ~1\u3009 > 0. For the case of equality, if P and Q have the same connected component C, then matrix [P,Q]\u221a has the same transition probabilities as Markov chains P and Q restricted to the vertices of C. We note that C is a stochastic matrix and, therefore, its largest positive eigenvalue is one. Hence,\n\u03c1 (\n[P,Q]\u221a\n) \u2265 \u03c1 (C) = 1.\n3For example ~p and ~q could be respective stationary distributions of P and Q. However, we still want the assumption that ~p = ~q, as otherwise there might be another strategy to distinguish P and Q than observing a long stream of samples w by making the decision right away based on one initial sample from ~p. Example 1d illustrates how two Markov chains can produce very similar distributions of words W`\nP ,W` Q starting from any state for some\nlarge `, and yet have vastly different stationary distributions. 4or essential communicating class in the terminology of Book [LPW09]\nIf \u03c1 (\n[P,Q]\u221a ) = 1, we apply Perron-Frobinius theorem to [P,Q]\u221a to get that the largest eigen-\nvalue \u03bb1 = \u03c1 ( [P,Q]\u221a ) = 1 has corresponding (left) eigenvector ~u1 with non-negative entries.\nSimilar to the proof of Claim 1 we observe that ~u>1 \u25e6 ( P+Q 2 \u2212 [P,Q]\u221a ) \u25e6 ~1 = 0, and all entries of the matrix in this expression are non-negative. This implies that Pij = Qij for every strictly positive coordinates i of the eigenvector ~u1 and any j \u2208 [n]. Since ~u>1 \u25e6 [P,Q]\u221a = ~u>1 , we also have Pij = Qij = 0 for any positive coordinate i and zero coordinate j of eigenvector ~u1. Therefore, the set of vertices corresponding to positive coordinates of ~u1 form a component (which might have more than one connected component of P and Q) such that P = Q on these vertices.\nWe consider the quantity \u03b5 def = 1\u2212 \u03c1 ( [P,Q]\u221a ) as a proxy for the closeness of Markov chains P and Q. In particular in (5) if ~p = ~q is proportional to ~u1, then ` \u00b7 ln(1 \u2212 \u03b5) \u2264 ln 0.5 =\u21d2 ` \u2265 ln 22\u03b5 . This shows that in worst-case we need to observe a trajectory of length at least \u2126(1/\u03b5) before we can satisfactorily distinguish the two chains. Note however that, in general, ` might need to be larger than O(1\u03b5 ) as is illustrated in Example 1c. In the remainder of this section and the following section we study an interesting special case of symmetric Markov chains that avoids such irregular behavior and dependency on the starting state.\nSymmetric Markov Chains. The stationary distribution for any symmetric Markov chain is the uniform distribution over all states. In this case the starting distributions in the average-case part of equation (5) are ~p = ~q = 1n\n~1. In this setting of symmetric Markov chains, we can provide sharp bounds on the minimal distinguishing length `.\nClaim 2. The necessary and sufficient distinguishing length `, which allows to distinguish P vs. Q with high probability, is \u0398\u0303 (\n1 \u03b5\n) (up to a log n factor), where \u03b5 = 1\u2212\u03c1 ( [P,Q]\u221a ) under both worst-case\nand average-case assumptions for the starting state.\nProof. We first consider the average-case model for the starting state. Note that [P,Q]\u221a is a symmetric matrix. Let ~v1, . . . , ~vn be normalized orthogonal eigenvectors of [P,Q]\u221a, corresponding to real \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbn eigenvalues. Then for RHS of (5) we have\n1 n ~1> \u25e6\n( [P,Q]\u221a )` \u25e6 ~1 = 1\nn ~1> \u25e6 ( n\u2211 i=1 \u03bbi \u00b7 ~vi \u25e6 ~v>i )` \u25e6 ~1 = n\u2211 i=1 \u03bb`i \u00b7 1 n \u3008~1, ~vi\u30092 = (\u2217) (7)\nNow we can write an upper and lower bound on (\u2217) in terms of \u03bb`1 (assuming that ` is even):\n\u03bb`1 n = \u03bb`1 n \u2016~v1\u201622 \u2264 \u03bb ` 1 \u00b7 1 n \u2016~v1\u201621 \u2264 (\u2217) \u2264 n\u2211 i=1 \u03bb`i \u00b7 1 n \u2016~vi\u201621 \u2264 n\u2211 i=1 \u03bb`i \u00b7 \u2016~vi\u2016 2 2 = n\u2211 i=1 \u03bb`i \u2264 n \u00b7 \u03bb`1,\nwhere in the second inequality we used Perron-Frobenius theorem stating that all coordinates of ~v1 are non negative. Consequently, these bounds imply that ` = \u0398 ( 1 \u03b5 ) up to a log n factor, if\n\u03c1 (\n[P,Q]\u221a ) = \u03bb1 = 1\u2212 \u03b5. I.e., ` = \u0398\u0303 ( 1 \u03b5 ) .\nFor the worst-case assumption on the starting state, it is sufficient to show an upper bound ` = O (\nlogn \u03b5\n) . In this case (7) becomes\n~e>i \u25e6 ( [P,Q]\u221a )` \u25e6 ~1 = n\u2211 i=1 \u03bb`i \u00b7 \u3008~ei, ~vi\u3009 \u00b7 \u3008~1, ~vi\u3009 \u2264 n\u2211 i=1 |\u03bbi|` \u00b7 \u2016~vi\u2016\u221e \u00b7 \u2016~vi\u20161 \u2264 n\u2211 i=1 |\u03bbi|` \u00b7 \u221a n \u2264 n1.5 \u00b7 \u03bb`1,\nsince \u2016~vi\u20161 \u2264 \u221a n \u2016~vi\u20162 = \u221a n, and \u2016~vi\u2016\u221e \u2264 \u2016~vi\u20162 = 1.\nWe note that, if one could pick the starting state instead of working with average-case or worstcase assumptions of Claim 2, then then ` can be much smaller (see Example 1b). Claim 2 gives a strong evidence that Dist (P,Q) def = 1 \u2212 \u03c1 ( [P,Q]\u221a ) is a meaningful and important parameter that captures closeness between P and Q. In the following section we will use it as analytical proxy for the distance between Markov Chains5.\nFixed word length. In some applications the length ` of the observed word might be given a priori. One such example corresponds to card riffle shuffle, where the random choices in the process can be described (see Section 4 for more detail) as a Markov chain over O(n2) states (n = 52 for the card deck), where the process terminates after ` = n steps. In this case we can expect a few i.i.d. samples of length-` words. For such examples and more generally for the Markov Chains with a specified number of steps T it is natural to define Dist (P,Q) def = dTV ( WT P ,WT Q ) . Note that now the distance Dist (P,Q) satisfies triangle inequality. Moreover, due to the relation between Hellinger and total variation distances we can estimate 1\u2212 Dist 2(P,Q) 2 \u2265 1\u2212 d 2 Hel ( WT P ,WT Q ) , where the RHS term admits a nice analytical expression similar to (4).\n5In general this notion of distance should be used with care. One thing about parameter Dist (P,Q) = 1 \u2212 \u03c1 (\n[P,Q]\u221a\n) , is that it is not a metric. In particular, Dist (P,Q) violates the triangle inequality (Dist (M1,M2) =\nDist (M2,M3) = 0, but Dist (M1,M3) > 0 for some M1,M2,M3) as is illustrated by Example 1a. We note that this problem can only appear for reducible chains, as is shown in Claim 1. Also it is not always possible to extend the sharp bounds on ` of Claim 2 from symmetric Markov chains to non-symmetric Markov chains, even if both MC have the uniform distribution as their stationary distribution (see Example 1e)"}, {"heading": "3 Identity Testing of Symmetric Markov Chains", "text": "As we have formalized a notion of distance between symmetric Markov Chains in the previous section we get a well defined framework from property testing literature [BFF+01, Pan08, LRR13] for testing properties of distributions generated by Markov Chains. Arguably, the next fundamental\nquestion, after deciding one out of two given distributions (we call it A-B testing), is identity testing problem. In this problem the goal is to test from a stream of samples whether the real distribution (which unlike the case of A-B testing is completely unknown to us) coincides with a given hypothesis distribution. In this section, we present our results for identity testing of symmetric Markov chains. We first present a polynomial time algorithm that provides an efficient reduction to the identity testing problem with i.i.d. samples. The algorithm improves on the performance of a naive reduction, which waits for a period of time to get an independent sample and as such suffers a multiplicative loss of mixing time MixTQ of Markov Chain Q. Our algorithm suffers only an additive loss of O\u0303 (HitTQ \u00b7 log (HitTQ)) in sampling complexity and allows us to reduce the problem to testing identity with respect to squared Hellinger distance of a distribution supported on a domain of size n2 and with access to i.i.d. samples. In the next subsection we provide a nearly matching lower bound for the identity testing problem. We begin by giving a formal statement of the identity testing problem below:\nInput: \u03b5; explicit symmetric Markov chain Q; m consecutive samples s1 \u00b7 \u00b7 \u00b7 sm from a symmetric Markov Chain P . Output: P = Q, or P 6= Q if 1\u2212 \u03c1 (\n[P,Q]\u221a\n) > \u03b5.\nOur approach. We consider a mapping K~k from infinite words W \u221e M of an irreducible Markov chain M to \u220fn i=1[n]\nki , where ~k = (k1, \u00b7 \u00b7 \u00b7 , kn) is a vector of n non negative integers, as follows. For each infinite word w = s1s2 \u00b7 \u00b7 \u00b7 and each state i \u2208 [n] we look at the first ki visits to state i (i.e., at times t = t1, . . . , tki with st = i) and write down the corresponding transitions in the infinite word w, i.e., st+1. We note that every state is visited almost surely in w, since M is an irreducible finite-state Markov chain. Therefore, mapping K~k defines a probability distribution on \u220fn i=1[n]\nki . We note that this distribution is independent across all different states and/or independent for a particular state i because of the Markov property of Markov chains. Furthermore, a specific draw for a copy of a state i is distributed according to the i-th row of the transition matrix M .\nIn Lemma 3.1 we show that for a big enough number of samples m = O\u0303 (HitTQ log (HitTQ) + n\u03b5 ) 6 and ki = O(E[# visits to i]) = O( m n ) the mapping K~k is well defined for a finite m number of samples for all but a small fraction of the words in Wm M\n. This effectively allows us to generate a large number of independent samples from a discrete distribution corresponding to matrix P : pick uniformly at random a state i \u2208 [n] and then observe transition from i according to transition probabilities of P . Indeed, to this end, we first simulate m\u2032 = \u0398 ( m\nlog2(n/\u03b5)\n) i.i.d. samples from\n[n]. Let ~k be the histogram of these m\u2032 samples (note that maxi ki \u2264 O(m\u2032 log n/n) with high probability). We apply K~k mapping to our stream of m consecutive samples of Markov chain P , which is well defined with high probability. Apart from some small probability events (maxi ki is too large, or K~k is not defined) we obtain the desired m \u2032 i.i.d. samples.\nLemma 3.1. If m = O\u0303 (log (HitTQ) HitTQ), then Pr[\u2203 state i : |{t : i = st \u2208 w}| < m8e\u00b7n ] \u2264 \u03b52 n .\nProof. To simplify notations we denote by \u2206 def = 2HitTQ. By union bound over all states i it is enough to show that Pr[|{t : i = st \u2208 w}| < m8e\u00b7n ] \u2264 \u03b52 n2 for each fixed state i. We can make sure that in the first m2 steps state i is visited at least once with probability at least 1 \u2212 \u03b52 n3 . Once\n6in this paper, O\u0303 always hides poly log(n/\u03b5) factors.\nwe visited state i, instead of hitting time for state i we can analyze the return time Returni for i. Note that for symmetric Markov chains 1n\n~1 (uniform distribution) is a stationary distribution. Therefore, every state appears at average once in every n steps in an infinite word from W\u221e\nQ . In\nother terms, the expectation of Returni for each state i is exactly n. By definition of hitting time we have that in \u22062 steps the probability of reaching a particular state i from any other state j is greater than 1\u2212 1/e (or any other given constant). It implies that Pr[Returni \u2265 \u22062 \u00b7 C] \u2264 e\n\u2212C for any C \u2208 N. Indeed, one can show this by induction on parameter C. Notice that if the random walk did not return to i after C \u2212 1 steps it has stopped at some state j 6= i. Then for any choice of j by definition of the hitting time the random walk will return to i with probability at least 1/e in the next \u22062 steps. It is not hard to get a similar bound Pr[Returni \u2265 \u2206 \u00b7 C] \u2264 e\n\u2212C for any C \u2265 1, C \u2208 R. To simplify notations we use X to denote the random variable Returni and X1, . . . , X` to denote ` i.i.d. samples of X. We have\nX \u2265 0 and \u2200C \u2208 R\u22651,Pr [X \u2265 \u2206 \u00b7 C] \u2264 e\u2212C and E [X] = n. (8)\nWe only need to show that Pr[X1 + \u00b7 \u00b7 \u00b7+X` > m/2] \u2264 \u03b5 2 n2 for ` = m8e\u00b7n . To this end, we use a standard technique for large deviations and apply Markov\u2019s inequality to the moment generating function of X1 + \u00b7 \u00b7 \u00b7+X`,\nPr [X1 + \u00b7 \u00b7 \u00b7+X` > m/2] = Pr [ e\u03b8\u00b7(X1+\u00b7\u00b7\u00b7+X`) > e\u03b8\u00b7m/2 ] \u2264 E[e \u03b8\u00b7(X1+\u00b7\u00b7\u00b7+X`)]\ne\u03b8\u00b7m/2 =\nE[e\u03b8X ]`\ne\u03b8\u00b7m/2 (9)\nWe note that given restrictions (8) on X maximum of E[e\u03b8X ] for any fixed \u03b8 > 0 is attained at\nX\u2217 \u223c\n{ \u2206 \u00b7 x x \u2208 [C0,\u221e) with probability density function e\u2212x\n0 with remaining probability 1\u2212 e\u2212C0 ,\nwhere constant C0 > 1 is such that E[X \u2217] = n. Indeed, distribution X\u2217 maximizes (9) due to simple variational inequality: \u00b7 e\u03b8\u00b7a + \u00b7 e\u03b8\u00b7b < \u00b7 e\u03b8\u00b7(a\u2212c) + \u00b7 e\u03b8\u00b7(b+c) for any b \u2265 a \u2265 c > 0, and probability mass > 0. This inequality allows us to increase E[e\u03b8\u00b7X ] and not change E[X] by tweaking density function f(x) of X f \u2032(a \u2212 c) = f(a \u2212 c) + , f \u2032(a) = f(a) \u2212 , f \u2032(b) = f(b) \u2212 , f \u2032(b + c) = f \u2032(b + c) + , (f \u2032(x) = f(x) for all other x), for some c \u2264 a. The only time we cannot apply this incremental change is when X = X\u2217.\nWe have E [X\u2217] = \u2206(C0 + 1)e \u2212C0 = n. (10)\nWe set \u03b8 def = 12\u2206 log \u2206 in (9). Now we are ready to estimate E[e \u03b8\u00b7X ]. To simplify notations we denote \u03b3 def = 12 log \u2206 .\nE [ e\u03b8\u00b7X ] = 1\u2212 e\u2212C0 + \u222b \u221e C0 e\u03b8\u00b7\u2206\u00b7x \u00b7 e\u2212x dx = 1\u2212 e\u2212C0 + \u222b \u221e C0 e \u2212x\u00b7(1\u2212 1 2 log \u2206 ) dx\n= 1\u2212 e\u2212C0 + e \u2212C0(1\u2212\u03b3)\n1\u2212 \u03b3 = 1 + e\u2212C0\n( eC0\u03b3 1\u2212 \u03b3 \u2212 1 ) . (11)\nWe notice that \u03b3C0 < 1, since from (10) we can conclude that eC0 C0+1 = \u2206n =\u21d2 C0 < 2 log \u2206 = 1/\u03b3. The last implication can be obtained as follows: for C0 > 2.52, we have C0\u2212 C02 \u2264 C0\u2212 ln(1+C0) =\nln(\u2206n ). Now, we can estimate e \u03b3C0 \u2264 1 + e \u00b7 \u03b3C0 in (11). Furthermore, since \u03b3 < 1/2 we have the term e C0\u03b3\n1\u2212\u03b3 \u2212 1 in (11) to be at most 2e\u03b3(C0 + 1). With this estimate we continue (11)\nE [ e\u03b8\u00b7X ] \u2264 1 + e\u2212C02e\u03b3(C0 + 1) = 1 +\ne \u00b7 n \u2206 log \u2206 . (12)\nWe apply estimate (12) and formula \u03b8 = 12\u2206 log \u2206 to (9) to obtain\nPr [X1 + \u00b7 \u00b7 \u00b7+X` > m/2] \u2264\n( 1 + e\u00b7n\u2206 log \u2206 )` em/4\u2206 log \u2206 \u2264 e m/8\u2206 log \u2206 em/4\u2206 log \u2206 = e \u2212m 8\u2206 log \u2206 < \u03b52 n2 ,\nwhere in the second inequality we used the fact (\n1 + e\u00b7n\u2206 log \u2206\n)\u2206 log \u2206 e\u00b7n\n< e, and to get the last inequality\nwe used m = \u2126\u0303 (\u2206 log \u2206) (where in \u2126\u0303 the hidden dependency is only on log \u03b5 and log n).\n1 ~k \u2190 Histogram (\u0398 (\nm log2(n/\u03b5)\n) i.i.d. Uniform [n] samples);\n2 for t\u2190 1 to m\u2212 1 do 3 if |Samples[st]| < ~k[st] then Add (st \u2192 st+1) to Samples[st]; 4 end 5 if \u2203i, s.t., |Samples[i]| < ~k[i] then 6 return Reject; 7 else 8 Samples\u2190 Samples[1] \u222a \u00b7 \u00b7 \u00b7 \u222a Samples[n]; 9 return IdentityTestIID (\u03b5, {qij = 1n \u00b7Qij}i,j\u2208[n], Samples);\n10 end\nAlgorithm 1: Independent Edges Sampler.\nWe use as a black-box the following recently proposed identity test under the Hellinger distance7.\nLemma 3.2. Given a discrete distribution q supported on [n] and access to i.i.d. samples from a discrete distribution p on the same support, there is an algorithm which can distinguish whether\np = q or d Hel\n(p, q) \u2265 \u03b5 with probability \u2265 2/3 using O (\u221a\nn \u03b52\n) samples.\nAs a corollary of the lemma, we get a test that can distinguish whether P = Q, or d2 Hel ( 1 nP, 1 nQ ) \u2265\n\u03b5 using m = O ( n \u03b52 ) i.i.d samples from 1nP , which can be viewed as a distribution on a support of size n2. Lemma 3.3 shows that the required distance condition for the i.i.d. sampler is implied by our input guarantee. Lemma 3.3. 12 \u2211\ni,j\u2208[n]\n(\u221a Pij n \u2212 \u221a Qij n )2 = d2 Hel ( 1 nP, 1 nQ ) \u2265 \u03b5.\nProof. We note that, as P and Q are symmetric matrices, so is [P,Q]\u221a. Thus we have 1\u2212 \u03b5 = \u03c1 (\n[P,Q]\u221a ) = max \u2016~v\u20162=1 ~v> \u25e6 [P,Q]\u221a \u25e6 ~v. (13)\n7this result uses a test similar to [ADK15] and is based on private communication with an author on that paper\nIf we use a particular ~v = 1\u221a n ~1 in (13), then we get the following inequality.\n1\u2212 \u03b5 \u2265 1\u221a n ~1> \u25e6 [P,Q]\u221a \u25e6 1\u221a n ~1 = 1 n \u2211 i,j \u221a Pij \u00b7Qij = 1\u2212 d2Hel ( 1 n P, 1 n Q ) ,\nwhich implies d2 Hel ( 1 nP, 1 nQ ) \u2265 \u03b5.\nNext we get a bound on sampling complexity of Algorithm 1.\nTheorem 3.1. Algorithm 1 provides correct output with probability at least 2/3, with a single sample stream of length m = O\u0303 ( HitTQ \u00b7 log (HitTQ) + n\u03b5 ) from P .\nProof. In the case P = Q, the probability that Algorithm 1 proceeds to IID tester, i.e., it does not reject P , because of small number of visits to a state, is at least Pr[\u2200i : |{t \u2208 w : st = i}| > m8e\u00b7n ] \u00b7 Pr[\u2200i : m 8e\u00b7n > ki] \u2265 ( 1\u2212 \u03b52n ) \u00b7 ( 1\u2212 \u03b52n ) \u2265 1 \u2212 2\u03b52n . In the previous estimate, we used Lemma 3.1 to bound Pr[\u2200i : |{t \u2208 w : st = i}| > m8e\u00b7n ], the fact that Pr[ m 8e\u00b7n \u2264 ki] \u2264 \u03b52 n2 (follows from a Chernoff bound), and a union bound. IID tester then correctly accepts P = Q with probability at least 4/5. Hence, the error probability is at most 1/5 + 2\u03b5 2\nn < 1/3. For the case P 6= Q, Lemma 3.3 says that if 1 \u2212 \u03c1 (\n[P,Q]\u221a\n) > \u03b5, then distributions passed\ndown to the IID tester {p : pij = 1nPij} and {q : qij = 1 nQij} are at least \u03b5 far in Hellingersquared distance. Classic results on identity testing with independent samples give sharp bounds of \u0398( n\n\u03b52 ) on sampling complexity with respect to total variation distance for distributions with\nsupport size n2. This estimate can be improved to work for Hellinger distance (Lemma 3.2). In our case this implies a O ( n \u03b5 ) sampling complexity for the IID tester. Furthermore, random mapping K~k : W \u221e P \u2192 p (where ~k is a histogram of m\u2032 = \u0398 ( m log2(n/\u03b5) ) i.i.d. uniform samples from [n]) produces m\u2032 i.i.d. samples from p. Hence, if Algorithm 1 has sufficient samples from P to define the mapping K~k, it would be able to distinguish p and q with probability at least 2/3. On the other hand, if Algorithm 1 gets finite number of samples which are not sufficient to define the mapping K~k, then it correctly rejects P before even running the IID tester.\nThus in both cases the probability of error is at most 1/3."}, {"heading": "3.1 Lower Bound.", "text": "In this section we provide nearly matching lower bound to our result in Theorem 3.1.\nTheorem 3.2. There is an instance of Identity testing problem for symmetric Markov chain Q that requires a word of length at least \u2126(n\u03b5 ) to check identity of Q with 99% confidence.\nProof. We use Le Cam\u2019s two point method and construct a class of Markov chains P s.t. (i) every P \u2208 P is at least \u03b5 far from Q for a given constant \u03b5. That is 1 \u2212 \u03c1 ( [P,Q]\u221a ) \u2265 \u03b5 for any P \u2208 P; (ii) there is a constant c > 0, s.t. it is impossible to distinguish a word of length m generated by a randomly chosen Markov chain P\u0304 \u223c P, from a word of length m produced by Q with probability equal to or greater than 99100 for m \u2264 cn \u03b5 . To prove (ii) we show that the total variation distance between the m-word distributions obtained from the two processes, Q and P\u0304 , is small when m < cn\u03b5 for some constant c. We denote distribution of length m words obtained from Q by Wm\nQ , and from\nMC P\u0304 \u223c P by WmP . We represent symmetric MC as undirected weighted graphs G = (V,E). We allow graph to have multi-edges (this is helpful to provide an intuitive understanding of the lower bound construction and is not essential). We can ultimately remove all multi-edges and give a construction with only simple edges by doubling the number of states.\nMarkov Chain Q: complete double graph on n vertices with uniform weights, i.e.,\n\u2200 i 6= j (ij)1, (ij)2 \u2208 E Q(ij)1 = Q(ij)2 = 1\n2(n\u2212 1) .\nFamily P: for any pair of vertices i 6= j there are two bidirectional edges (ij)1, (ij)2 with weights randomly (and independently for each pair of (i, j)) chosen to be either\nP(ij)1 , P(ij)2 = 1\u00b1 \u221a 8\u03b5\n2(n\u2212 1) , or P(ij)1 , P(ij)2 =\n1\u2213 \u221a 8\u03b5 2(n\u2212 1) .\nTo make this instance a simple graph with at most one bidirectional edge between any pair of vertices we apply a standard graph theoretic transformation: we make a copy i\u2032 for each vertex i; for each pair of double edges e1 = (ij)1, e2 = (ij)2 construct 4 edges (ij), (ij\n\u2032), (i\u2032j), (i\u2032j\u2032) with weights w(ij) = w(i\u2032j\u2032) = w(e1) and w(ij\n\u2032) = w(i\u2032j) = w(e2). As all Markov chains Q and P \u2208 P are symmetric with respect to the starting state, we can assume without loss of generality that word w starts from the state i = 1. First, we observe that for the simple graph 2n-state representation\nLemma 3.4. Every Markov chain P \u2208 P is at least \u03b5-far from Q.\nProof. For any P \u2208 P, it can be seen that\n[P,Q]\u221a \u25e6 ~1 =\n(\u221a 1 + \u221a 8\u03b5+ \u221a 1\u2212 \u221a 8\u03b5\n2\n) \u00b7 ~1.\nBy Perron-Frobenius theorem ~1 is the unique eigenvector corresponding to the largest absolute value eigenvalue. Hence, \u03c1 (\n[P,Q]\u221a\n) = \u221a 1+ \u221a 8\u03b5+ \u221a 1\u2212 \u221a\n8\u03b5 2 which by Taylor series expansion implies 1\u2212 \u03c1 (\n[P,Q]\u221a ) \u2265 \u03b5+ 52\u03b5 2 + o(\u03b52) \u2265 \u03b5 for any \u03b5 < 18 .\nWe say that a given word w = s1 . . . sm from a Markov chain P represented as a multi-edge graph on n states has a (ij) collision, if any state transition between states i and j (in any direction along any of the edges (ij)1, (ij)2) occurs more than once in w. We now state and prove the following claims about the Markov chain family P.\nLemma 3.5. Consider a word w of length m drawn from Q. The expected number of collisions in w is at most O ( m2\nn2\n) = O ( 1 \u03b52 ) .\nProof of Lemma 3.5: Let Iw(t1, t2, (ij)) indicate the event that in the multi-edge interpretation of the Markov chain P , the transition along (ij) edge occurs at times t1 < t2 in w. First, we observe that Pr[st1 = s|st1\u22121 = x] \u2264 1n\u22121 and Pr[st2 = s|st1\u22121 = x] \u2264 1 n\u22121 for all x and both s = i or s = j.\nThus for any t2 \u2265 t1 + 2 by a union bound for all four possible cases of st1 , st1+1, st2 , st2+1 \u2208 {i, j} we have\nE [Iw(t1, t2, (ij))] \u2264 4\n(n\u2212 1)4 .\nSimilarly, for the case t2 = t1 + 1 we can obtain\nE [Iw(t1, t2, (ij))] \u2264 2\n(n\u2212 1)3 .\nLet X denote the random variable which is equal to the total number of collisions in the word w. Then,\nE [X] = \u2211\nt2\u2265t1+2 \u2211 i 6=j E [Iw(t1, t2, (ij))] + m\u22121\u2211 t1=1 \u2211 i 6=j E [Iw(t1, t1 + 1, (ij))]\n\u2264 4 (n\u2212 1)4 \u00b7 m 2 2 \u00b7 n(n\u2212 1) 2 +\n2 (n\u2212 1)3 \u00b7m \u00b7 n(n\u2212 1) 2 = O\n( m2\nn2\n)\nWe also consider 3-way collisions which are collisions where there was at least 3 different transition between a pair of states i and j in the word w.\nLemma 3.6. Consider a word w of length m drawn from Q. The probability of w having a 3-way collision is at most O(m 3\nn4 ) = o(1).\nProof of Lemma 3.6: Similar to the proof of Lemma 3.5 we can give a sharp upper bound on the expected number of 3-way collisions with the most significant term being 8m 3\n6(n\u22121)6 \u00b7 n(n\u22121) 2 , i.e., the expected number of 3-way collisions is O ( m3\nn4\n) . By Markov inequality we obtain the required\nbound on the probability of a 3-way collision.\nNow consider a typical word w generated by Q. As we know from Lemma 3.6 it has no 3- way collisions and by Markov inequality and Lemma 3.5 has at most O( 1\n\u03b52 ) collisions with high\nprobability. As we show next a typical word w has similar probabilities under Q or P\u0304 \u223c P models.\nLemma 3.7. For m = O(n\u03b5 ) at least 1 2 fraction of words w = s1 \u00b7 \u00b7 \u00b7 sm generated by Q satisfy\n1 2 \u00b7PrQ [w] < PrP\u0304\u223cP [w] < 2 \u00b7PrQ [w]\nProof of Lemma 3.7: For each feasible word w in Q, i.e., w such that PrQ[w] > 0\nPr Q [w] =\n( 1\n2(n\u2212 1) )m\u22121 Pr P\u0304\u223cP [w] = \u220f j>i \u2211 P\u0304(ij)1= 1\u00b1 \u221a 8\u03b5 2(n\u22121) P\u0304 |{(ij)1\u2208w}| (ij)1 \u00b7 P\u0304 |{(ij)2\u2208w}|(ij)2\nFirst, if w has only one transition along edge (ij), then the corresponding term in PrP\u0304\u223cP [w]\u2211 P\u0304(ij)1 P\u0304 |{(ij)1\u2208w}| (ij)1 \u00b7 P\u0304 |{(ij)2\u2208w}|(ij)2 = 1 2 ( 1 + \u221a 8\u03b5 2(n\u2212 1) + 1\u2212 \u221a 8\u03b5 2(n\u2212 1) ) = 1 2(n\u2212 1) .\nFrom Lemma 3.6, we know that probability of a 3-way collision in w is o(1) under Q model. We observe that for a 2-way collision (ij) (a collision which is not a 3-way collision), the corresponding term in PrP\u0304\u223cP [w] for the case of transition along two different edges (ij)1 and (ij)2 is\u2211\nP\u0304(ij)1\nP\u0304 |{(ij)1\u2208w}| (ij)1 \u00b7 P\u0304 |{(ij)2\u2208w}|(ij)2 = 1 + \u221a 8\u03b5 2(n\u2212 1) \u00b7 1\u2212 \u221a 8\u03b5 2(n\u2212 1) = (1\u2212 8\u03b5) 4(n\u2212 1)2 .\nWe call this type of collision type I collision. For the other case (type II collisions) of transition along the same edges the respective probability is (1+8\u03b5) 4(n\u22121)2 . By Lemma 3.5 and by Markov inequality the total number of collisions is O( 1 \u03b52\n) with probability 34 . We can also make sure that out of these collisions number of type I and type II collisions is roughly the same. More precisely, the difference between numbers of type I and type II collisions is at most O(1\u03b5 ) with probability of at least 3 4 . Indeed, the choice of edge collision type in w is uniform between type I and type II, and is independent across all collision edges. Now, for small enough m we can make sure that at least 1 2 fraction of words w has number of collisions at most c1 \u03b52\nand the difference between number of type I and II collisions is at most c2\u03b5 , for some small constants c1, c2 > 0. Thus the corresponding density functions can be related as follows.\n2 > (1 + 8\u03b5) c2 \u03b5 > PrP\u0304\u223cP [w] PrQ[w] > ( 1\u2212 64\u03b52 ) c1 2\u03b52 \u00b7 (1\u2212 8\u03b5) c2 \u03b5 > 1/2\nLemma 3.7 shows that dTV\n( Wm\nQ ,WmP ) \u2264 34 , which implies that no algorithm can successfully\ndistinguish Q from the family P with probability greater than 34 for some m = \u2126( n \u03b5 )."}, {"heading": "4 Card Shuffling", "text": "A commonly used technique to shuffle decks of n = 52 cards is the riffle shuffle: first, the dealer cuts the deck into two piles. Then, the piles are \u201criffled\u201d together: the shuffler successively drops cards from the bottom of each pile to form a new pile. There are two variable aspects in this procedure. First, the numbers of cards in each pile after the initial cut can vary. Second, each time the dealer drops a card she needs to choose the pile from which the card is dropped.\nThe most well studied mathematical model for riffle shuffle is due to Gilbert, Shannon, and Reeds (GSR-model for short): first, the deck is cut into two packs according to a (n, 0.5)-binomial random variable where n is the number of cards in the deck; next, cards are dropped one by one from the bottom of one or the other pile with probability proportional to the relative sizes of the piles (i.e., if the left pile contains a cards and the right pile b cards, the next card drops from the left pile with probability aa+b). A well known result in this model is due to Bayer and Diaconis [BD92] who gave a sharp mathematical analysis of the mixing time of the riffle shuffle Markov chain showing that \u201cseven shuffles are necessary and sufficient to approximately randomize 52 cards,\u201d which actually convinced Las Vegas casinos to increase the number of shuffles in their shuffling procedures.\nThere have been some statistical studies validating the accuracy of the GSR model for riffle shuffles in practice. For example Diaconis and Reeds (see [Dia88]) did empirical analysis of about\na hundred riffle shuffles performed by each of them. They looked at a few different statistics including the count of consecutive cards dropped from each of the piles. In other work (see [Dia02] open problem 5), Chakraborty and Diaconis pointed out that some shuffling machines performing riffle shuffles do not conform to the GSR model. Instead they proposed another model to capture this observed phenomenon (we call it CD-model), where, in contrast to the GSR model, cards are dropped with probability aa+b from the pile containing b cards and b a+b from the pile containing a cards. Despite a large interest in card shuffling and all the existing work mentioned above, there has not been much theoretical statistical analysis on the question of testing whether a particular shuffling model is accurate. In this section we propose a new theoretical framework for the statistical analysis of riffle shuffles. We aim to address the following question:\nHow many trials are needed to test that shuffles are performed according to a specified probabilistic model?\nIt is natural to parametrize a riffle shuffle by (i) the distribution of possible cuts, and (ii) the probability pab of choosing the next card to be dropped from the left pile for each profile (a, b) of the number of cards left in the two piles. In this description each riffle shuffle can be represented as a random walk on a 2-dimensional grid (a, b) \u2208 Z2 that starts at a position chosen from a specified distribution on the diagonal a + b = n and decreases the sum of the coordinates a + b by one at each move. In fact, we get an almost one-to-one correspondence between riffle shuffles and the aforementioned random walks on the n \u00d7 n grid (the only shuffle that corresponds to more than one path on the grid is the identical permutation). Therefore, by knowing the initial permutation of cards at the beginning of the shuffle, and by scanning the permutation of cards obtained after one riffle shuffle we can reconstruct (except for the unimportant case of void shuffle) the random walk of n steps taken by our grid Markov Chain. We note that this grid representation might not be enough to accurately model the behavior of a dealer. For example, shufflers often tend to drop chunks of consecutive cards from left or right pile regardless of the pile sizes. To address this issue one might want to introduce an extra parameter \u2013 which pile the last card was taken from \u2013 to our grid parameterization, which can be done by doubling the number of states in the Markov chain. To capture this and other potential extensions of the Markov chain model for a riffle shuffle we introduce the following general theoretical framework.\nDefinition 2 (Sparse MC). Consider a Markov chain defined by transition matrices P = {Pt}n1t=1 over n2 states that proceeds in n1-step rounds, as follows. Starting from some state X0 = s0 it follows a transition according to P1, then P2, etc, then Pn1 to arrive at some state X1. Starting from X1 it repeats transitions according to P1, . . . , Pn1 , in sequence, and so on, ad infinitum. The transition matrices are assumed sparse having O(n3) non-zero entries.\nTo relate the above definition to the riffle shuffle, we should think of the state space as the set {(a, b)|0 \u2264 a + b \u2264 n}, where n = 52. So in particular, there are n2 = O(n2) states. There are n1 = n+ 1 transition matrices. P1 takes us from an uncut deck of cards, corresponding to the state (0, 0), to a cut deck corresponding to a state in set {(a, b)|a + b = n}. Then each other transition matrix Pt, t > 1, maps a state in {(a, b) : a+ b = n\u2212 t+ 2} to states in {(a, b) : a+ b = n\u2212 t+ 1}. All transition matrices have O(n) non-zero entries. Note that this way of modeling the riffle shuffle, while forgetting the specific ordering of cards, maintains the essential information that we need to test a riffle shuffle model, and in particular saves exponentially in the size of the state space.\nTesting sparse Markov chains. We develop tools for goodness-of-fit testing of sparse Markov chain models.\nSimplification: To avoid carrying around several parameters, we will henceforth take n1 = n3 = n and n2 = O(n\n2), which is what we would need for the riffle shuffle. Our results, namely the use and analysis of our edge tester, extend to the general case. We will also assume that Pn1 is the trivial matrix taking all states into a fixed state s0, which is also what we would need for the riffle shuffle, namely s0 = (0, 0). Again, our results easily extend to the general case.\nWith these simplifications in place, we can break an observed trajectory from a sparse Markov chain model into \u201csamples.\u201d One sample is a word w = s0 \u00b7 \u00b7 \u00b7 sn, whose transitions st\u22121 \u2192 st are performed according to transition matrix Pt.\nAs described in Section 2.2, a natural measure of distance between two Markov chains Q = {Qt}nt=1 and P = {Pt}nt=1 is the total variation distance between words of certain length sampled from these chains. As we have a natural length n to use here, we can take our distance between chains to be Dist (P,Q) def = dTV ( Wn P ,Wn Q ) .\nWe note that Dist 2(P,Q) 2 =\nd2 TV ( Wn P ,Wn Q ) 2 \u2264 d 2 Hel ( Wn P ,Wn Q ) and for Hellinger-squared distance we\ncan derive a formula similar to (4): 1\u2212d2 Hel\n( Wn\nP ,Wn Q ) = ~e>1 \u25e6[P1, Q1]\u221a\u25e6\u00b7 \u00b7 \u00b7\u25e6[Pt, Qt]\u221a\u25e6\u00b7 \u00b7 \u00b7 [Pn, Qn]\u221a\u25e6~1.\nIn particular, we could alternatively define our distance using the spectral approach we took in Section 3. To this end we can define a large matrix Q\u2217 for Markov chain Q that acts on n(n+ 1) states (n + 1 distinct copies nt of n states for each t \u2208 {0, \u00b7 \u00b7 \u00b7 , n}), such that Q\u2217 behaves exactly like matrix Qt on states nt\u22121 transitioning them to sates nt, and for t = n states nt are transitioned to the initial state s1 with t = 0. We similarly define large matrix P \u2217 for the Markov chain P .\nThen it turns out that the spectral radius \u03c1 (\n[Q\u2217, P \u2217]\u221a\n)n+1 = 1\u2212 d2\nHel\n( Wn\nP ,Wn Q\n) .\nWith these definitions we are interested in the following testing problem:\nInput: Q = {Qt}nt=1, s.t. each Qt is sparse, i.e., it has only O(n) non zero entries in total; m samples of w = s0 \u00b7 \u00b7 \u00b7 sn from a sparse Markov chain P = {Pt}nt=1. Output: P = Q, or P 6= Q if Dist (P,Q) \u2265 \u03b5."}, {"heading": "4.1 Upper Bound", "text": "For the upper bound, one might consider an appropriately defined statistic on the number of visits to a particular state si,t to distinguish between the two cases. Such statistics however can be mathematically difficult to deal with and we obtain worse bounds on the moments. In this paper, instead, we consider a different statistic. We look at all one-step transitions that can have positive probability in the Markov chain Q or P , we call these one-step transitions as edges. We denote a generic edge from a state si,t\u22121 to a state sj,t by e; the set of all possible edges by E; the set of transitions in E between states at time t\u22121 and time t by E(t) for each t \u2208 [n]. For each edge e, let qe and pe be the probabilities that there was a transition along edge e in one sample s1 \u00b7 \u00b7 \u00b7 sn from Q, or from P , respectively. Our statistic is defined on these edges. From a high level perspective, it consists of two steps:\nPruning. We remove all rare edges, i.e., edges that are traversed with probability less than O( \u03b5 2\nn2 )\nin Q. We show that the Markov chain obtained post pruning and renormalization is still close to\nthe original chain (Lemma 4.5). This step is necessary as rare instances of such transitions along rare edges could potentially shift the value of the statistic by a lot and we want to avoid that. Let E\u2217 be the resulting pruned set of edges. We reject all samples from P that go along any removed edge e /\u2208 E\u2217. We return P 6= Q if there are too many rejected samples. Otherwise, we continue to the next step. In Lemma 4.6 we show that returning P 6= Q in this step doesn\u2019t affect the success probability by too much.\n\u03c72-statistic on edges. For each non rare edge e \u2208 E\u2217 we count the number of transitions ne along e. We define \u03c72 edge statistic Ze def = (ne\u2212qe\u00b7m) 2\u2212ne qe\u00b7m . Our main statistic is\nZ def = \u2211 e\u2208E\u2217 Ze = \u2211 e\u2208E\u2217 (ne \u2212 qe \u00b7m)2 \u2212 ne qe \u00b7m .\nWe accept or reject P = Q depending on Z being smaller or larger than a certain threshold. This test is similar in spirit to that of [ADK15] but requires much more involved analysis. Indeed, in our stetting it is not clear which statistic to use: one can attempt to count frequencies of state visits in the MC, or employ other state dependent statistics. After many trials and errors we figured out that doing the analysis across separate edges was the best approach. Indeed, since we are dealing with non i.i.d. but dependent samples, obtaining a non-trivial variance bound for our edge statistic proves to be a challenging task. Similar to the classical i.i.d. setting, poissonisation helps to ease the analysis in our setting too. However, we rely on it in more subtle way: the effects of poissonisation at the top layer of n states percolate nicely through to the bottom layers of the chain as shown in Lemma 4.1. We show that Var[Z] = O ( kn3 ) in Lemmas 4.3 and 4.4. Another challenge for us was to relate the new definition of distance between two non stationary Markov chains with the parameters in the description of their kernels, as e.g. in our Lemma 4.2. Our \u03c72 test yields the following guarantee on the number of samples\nTheorem 4.1. There is an algorithm that can tell whether P = Q, or P 6= Q, when Dist (P,Q) \u2265 \u03b5, with probability at least 23 using O( n3/2 \u03b52 ) samples.\nNote that, while we state the above theorem in terms of the number of samples, we really mean that we observe a single trajectory from the sparse Markov chain which we have partitioned into segments of length n. In particular, the length of the required trajectory for the above statements is a factor of n larger than the stated number of samples. Additionally note that to properly compare to our results from Section 3 we should note that the number of states here is O(n2). All details are provided in Section 4.2. We also have a complementary lower bound which is presented in Section 4.3."}, {"heading": "4.2 More Details on the Tester for Sparse Markov Chains", "text": "We consider all one-step transitions that can have positive probability in the Markov chain Q or P , we call these one-step transitions as edges. We denote a generic edge from a state si,t\u22121 to a state sj,t by e; the set of all possible edges by E; the set of transitions in E between states at time t \u2212 1 and time t by E(t) for each t \u2208 [n]. For each edge e, let qe and pe be the probabilities that there was a transition along edge e in one sample s1 \u00b7 \u00b7 \u00b7 sn from Q, or from P , respectively. Our test, from a high level perspective, consists of two steps:\nPruning. We remove all rare edges, i.e., edges that are traversed with probability less than O( \u03b5 2\nn2 )\nin Q. Let E\u2217 be the resulting pruned set of edges. We reject all samples from P that go along any removed edge e /\u2208 E\u2217. We return P 6= Q if there are too many rejected samples. Otherwise, we continue to the next step.\n\u03c72-statistic on edges. For each non rare edge e \u2208 E\u2217 we count the number of transitions ne along e. We define \u03c72 edge statistic Ze def = (ne\u2212qe\u00b7m) 2\u2212ne qe\u00b7m . Our main statistic is\nZ def = \u2211 e\u2208E\u2217 Ze = \u2211 e\u2208E\u2217 (ne \u2212 qe \u00b7m)2 \u2212 ne qe \u00b7m .\nWe accept or reject P = Q depending on Z being smaller or larger than a certain threshold.\nIn the remainder of this section we mostly focus on the latter step. Specifically, we analyze the \u03c72 edge statistic in the case when qe \u2265 \u2126( \u03b5 2\nn2 ) for all e \u2208 E and pe = 0 for all e /\u2208 E. At the end of the\nsection we explain why after the pruning step these conditions are satisfied.\nPoisson Sampling. Throughout the analysis of \u03c72 statistic, we use the standard Poissonization approach. Instead of drawing exactly m samples from P , we first draw m\u2032 \u223c Poisson (m), and then draw m\u2032 samples from P . The benefit of this is that the number of times different elements in the support of the cut distribution occur in the sample become independent, giving simpler analysis. Moreover, the number of transitions observed along the edge e \u2208 E(t), ne, for a fixed t will be distributed as Poisson (m \u00b7 pe), independently for all e \u2208 E(t) (see Lemma 4.1). As Poisson (m) is tightly concentrated around m, this additional flexibility comes only at a sub-constant cost in the sample complexity with an inversely exponential in m, additive increase in the error probability. We note that as in equations (1\u2212 2) from [ADK15] the expectation and variance of our \u03c72 statistic are as follows.\nE [Ze] = m \u00b7 (pe \u2212 qe)2\nqe and Var [Ze] = 2 p2e q2e +m \u00b7 pe(pe \u2212 qe) 2 q2e .\nLemma 4.1. The number of transitions ne along an edge e is distributed as Poisson (m \u00b7 pe) and all ne are independent for e \u2208 E(t), for any t \u2208 [n].\nProof. The proof proceeds by induction on t and is based on the following two standard observations on Poisson random variables:\n(Observation I) for any discrete distribution D, k \u223c Poisson (\u03bb) i.i.d. samples from D form a collection of jointly independent Poisson random variables for the occurrences of each element in the support of D, i.e., distribution \u220f i Poisson (\u03bb \u00b7Prx\u223cD[x = i]);\n(Observation II) the sum of two independent Poisson random variables with distributions Poisson (\u03bb1) and Poisson (\u03bb2) is a Poisson random variable with the distribution Poisson (\u03bb1 + \u03bb2).\nFor t = 1 as we start with Poisson (m) samples, Observation I gives us the desired result. For the induction step (from t = k to t = k + 1), we observe that counts of visits to each particular state i at time t are independent Poisson random variables by Observation II. Now, Observation I applied to the states at time t yields the desired result.\nThe following procedure correctly distinguishes between the case P = Q, or Dist (P,Q) \u2265 \u03b5, in the regime when qe \u2265 \u2126( \u03b5 2\nn2 ) for all e \u2208 E(Q) and pe = 0 for all e /\u2208 E.\nInput: \u03b5; an explicit k-sparse Markov Chain Q = {Qt}nt=1; (Poisson) m samples from a Markov Chain P = {Pt}nt=1, where ne denotes the number of transitions along the edge e.\nOutput: Accept if P = Q, or reject if Dist (P,Q) \u2265 \u03b5 1 E \u2190 {e : qe > 0}; 2 Z \u2190\n\u2211 e\u2208E (ne\u2212qe\u00b7m)2\u2212ne qe\u00b7m ;\n3 if Z \u2264 2 \u221a kn3/2 then 4 return Accept; 5 else 6 return Reject; 7 end\nAlgorithm 2: \u03c72 Edge Test\nTheorem 4.2. Algorithm 2 is correct with probability at least 4/5, if m \u2265 Cn3/2 \u03b52 for some C = O(1), qe \u2265 \u03b5 2\nkn2 for all e \u2208 E, and pe = 0 for all e /\u2208 E.\nProof. To get the desired result we analyze expectation and variance of Z. First, we relate the expected value of Z with the distance Dist (P,Q) between P and Q.\nLemma 4.2. E[Z] \u2265 m4 \u00b7Dist 2 (P,Q) .\nProof. We recall that\nd2 Hel (P,Q) = 1\u2212 ~e>1 \u25e6 [P1, Q1]\u221a \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 [Pt, Qt]\u221a \u25e6 \u00b7 \u00b7 \u00b7 [Pn, Qn]\u221a \u25e6 ~1 = ~e>1 \u25e6 ( P1 +Q1\n2\n) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 ( Pn +Qn\n2\n) \u25e6 ~1\u2212 ~e>1 \u25e6 [P1, Q1]\u221a \u25e6 \u00b7 \u00b7 \u00b7 [Pn, Qn]\u221a \u25e6 ~1\n= n\u2211 t=1 ~e>1 \u25e6 [P1, Q1]\u221a \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 ( Pt +Qt 2 \u2212 [Pt, Qt]\u221a ) \u25e6 ( Pt+1 +Qt+1 2 ) \u00b7 \u00b7 \u00b7 ( Pn +Qn 2 ) \u25e6 ~1\n= 1\n2 n\u2211 t=1 ~e>1 \u25e6 [P1, Q1]\u221a \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 [Pt\u22121, Qt\u22121]\u221a \u25e6 [Pt, Qt](\u221a-\u221a)2 \u25e6 ~1, (14)\nwhere in the last line [Pt, Qt](\u221a-\u221a)2 =\n((\u221a Pt(ij)\u2212 \u221a Qt(ij) )2) ij . Indeed, the first equality holds\ntrue as Pt \u25e6~1 = Qt \u25e6~1 = ~1 for any t \u2208 [n]; the second equality is a telescopic sum; the last equality is simply the formula for the complete square. Let qt and pt be the respective distribution vectors over the states si,t, i \u2208 [n] in Q and P Markov chains. We also define distributions p0 = q0 = ~e1.\nBy applying Cauchy-Schwarz inequality to the corresponding Bhattacharya coefficient of P and\nQ at a fixed state i and time t we obtain ~e>1 \u25e6 [P1, Q1]\u221a \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 [Pt, Qt]\u221a \u25e6 ~ei = HSi ( Wt P ,Wt Q ) = \u2211 w=s0...st s.t. st=i \u221a Pr P [w] Pr Q [w]\n\u2264 \u221a\u221a\u221a\u221a \u2211\nw=s1...st s.t. st=i\nPr P\n[w] \u2211\nw=s1...st s.t. st=i\nPr Q\n[w] = \u221a pt(i) \u00b7 qt(i).\nWe plug in this estimate to (14) and obtain\n1 4 \u00b7Dist2 (P,Q) = 1 4 d2 TV (P,Q) \u2264 d2 Hel (P,Q) \u2264 1 2 n\u2211 t=1 [pt\u22121, qt\u22121] > \u221a \u25e6 [Pt, Qt](\u221a-\u221a)2 \u25e6 ~1,\nWe examine each term of the summation in the right hand side of the last equation\n1 2 [pt\u22121, qt\u22121] > \u221a \u25e6 [Pt, Qt](\u221a-\u221a)2 \u25e6 ~1 = \u2211 i\n\u221a pt\u22121(i) \u00b7 qt\u22121(i)\n2 \u00b7 \u2211 j:(ij)\u2208E(t) (\u221a Pt(ij)\u2212 \u221a Qt(ij) )2 , (15)\nWe show that corresponding terms in E[Z] = \u2211 i,t \u2211 e:(ij)\u2208E(t) E[Ze] give an upper bound on (15) for each fixed state i.\n1\nm \u2211 e:(ij)\u2208E(t) E [Ze] = \u2211 e:(ij)\u2208E(t) (pe \u2212 qe)2 qe = \u2211 j:(ij)\u2208E(t) (pt\u22121(i)Pt(ij)\u2212 qt\u22121(i)Qt(ij))2 qt\u22121(i)Qt(ij)\n= p2t\u22121(i)\nqt\u22121(i)  \u2211 j:(ij)\u2208E(t) P 2t (ij) Qt(ij) \u2212 2pt\u22121(i) + qt\u22121(i) = p2t\u22121(i)\nqt\u22121(i) \u2211 j:(ij)\u2208E(t) (Pt(ij)\u2212Qt(ij))2 Qt(ij) + (pt\u22121(i)\u2212 qt\u22121(i))2 qt\u22121(i)\n\u2265 p2t\u22121(i)\nqt\u22121(i) \u2211 j:(ij)\u2208E(t) (\u221a Pt(ij)\u2212 \u221a Qt(ij) )2 + (pt\u22121(i)\u2212 qt\u22121(i))2 qt\u22121(i) , (16)\nwhere the third and forth equalities hold true as \u2211 j:(ij)\u2208E(t) Pt(ij) = \u2211 j:(ij)\u2208E(t)Qt(ij) = 1, and to get the last inequality one can simply use identity (a\u2212b) 2 b = ( \u221a a \u2212 \u221a b)2 (\u221a a+ \u221a b\u221a\nb\n)2 . Now, we\nclaim that the expression in RHS of (16) is at least RHS of (15) for a given i, i.e., we need to show that(\np2t\u22121(i) qt\u22121(i) \u2212 \u221a pt\u22121(i)qt\u22121(i) 2 ) \u2211 j:(ij)\u2208E(t) (\u221a Pt(ij)\u2212 \u221a Qt(ij) )2 + (pt\u22121(i)\u2212 qt\u22121(i))2 qt\u22121(i) \u2265 0. (17)\nThe inequality is obviously true, if p2t\u22121(i) qt\u22121(i) \u2265 \u221a pt\u22121(i)qt\u22121(i)\n2 . Otherwise, without loss of generality, we can substitute the term \u2211\nj:(ij)\u2208E(t)\n(\u221a Pt(ij)\u2212 \u221a Qt(ij) )2\nwith an upper bound of 2. Furthermore, by denoting x = \u221a\npt\u22121(i) qt\u22121(i) the inequality (17) can be rewritten as qt\u22121(i) \u00b7 ( 2x4 \u2212 x+ (x2 \u2212 1)2 ) \u2265 0, and, indeed, one can verify that this fourth degree polynomial is always positive.\nNow we estimate the variance of random variable Z Lemma 4.3. E[Z] \u2265 ( 12 \u221a\n2k C +\n2 \u221a 2+ \u221a k\u221a\nC\n) \u00b7 \u221a\nVar[Z], when m \u2265 C\u00b7n3/2 \u03b52 and Dist (P,Q) \u2265 \u03b5.\nProof. We recall that Z = \u2211\ne\u2208E Ze and by Lemma 4.1 all Ze for a fixed t and e \u2208 E(t) are independent. For any random variables X1, . . . , Xn it is true that\nVar [X1 + \u00b7 \u00b7 \u00b7+Xn] \u2264 (\u221a Var [X1] + \u00b7 \u00b7 \u00b7+ \u221a Var [Xn] )2 .\nWe use this estimate for Xt = \u2211 e\u2208E(t) Ze to obtain\n\u221a Var [Z] \u2264 n\u2211 t=1 \u221a \u2211 e\u2208E(t) Var [Ze] = n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E(t) 2 p2e q2e +m \u00b7 pe(pe \u2212 qe) 2 q2e\nWe further simplify the above expression by using the fact that \u221a x+ y \u2264 \u221a x+ \u221a y.\n\u221a Var [Z] \u2264 n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E(t) 2 p2e q2e + \u221a m n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E(t) pe(pe \u2212 qe)2 q2e\n(18)\nOn the other hand, by Lemma 4.2\nE [Z] \u2265 1 2 E [Z] + m 8 Dist2 (P,Q) = m \u00b7\n( 1\n2 \u2211 e\u2208E (pe \u2212 qe)2 qe + \u03b52 8\n) . (19)\nIn the following we use (19) to give separate upper bounds on each of the two summation terms in the RHS of (18).\nFirst term of (18). To estimate the first term, we split E into two sets\nE1 def = {e \u2208 E : pe \u2264 2qe} and E2 def = {e \u2208 E : pe > 2qe}\nWe define accordingly the sets E1(t) and E2(t) for each t \u2208 [n]. Again we have\nn\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E(t) 2 p2e q2e \u2264 n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E1(t) 2 p2e q2e + n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E2(t) 2 p2e q2e .\nEstimate for E1. We have 2 p2e q2e \u2264 8 for any edge e \u2208 E1. Therefore, by the sparsity condition\u2211\ne\u2208E1(t) 2 p2e q2e \u2264 8n \u00b7 k, where k = O(1). Thus,\nn\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E1(t) 2 p2e q2e \u2264 \u221a 8k \u00b7 n3/2 \u2264 \u221a 8k C \u00b7m \u00b7 \u03b52 \u2264 4 \u221a 8k C \u00b7E [Z] (20)\nEstimate for E2. We have (pe\u2212qe)2 qe \u2265 p 2 e 4qe for any e \u2208 E2. Therefore,\n\u221a 32k\nC \u00b7E [Z] \u2265\n\u221a 32km C \u00b7 n\u2211 t=1  1 16 \u2211 e\u2208E2(t) 2 p2e qe + \u03b52 8n  \u2265 \u221a32km C \u221a 4\u03b52 16 \u00b7 8n n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E2(t) 2 p2e qe\n\u2265 \u221a 32km\nC\n\u221a \u03b52 32n \u00b7 \u221a \u03b52 kn2 n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E2(t) 2 p2e q2e \u2265 n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E2(t) 2 p2e q2e , (21)\nwhere the first inequality holds by (19); the second inequality is simply AM-GM inequality; and to get the third inequality we used the bound qe \u2265 \u03b5 2\nkn2 .\nSecond term of (18). We split E into another two sets E3, E4 (similarly define E3(t) and E4(t)):\nE3 def = {e \u2208 E : pe \u2264 2 \u221a nqe} and E4 def = {e \u2208 E : pe > 2 \u221a nqe}\nAgain n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E(t) pe(pe \u2212 qe)2 q2e \u2264 n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E3(t) pe(pe \u2212 qe)2 q2e + n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E4(t) pe(pe \u2212 qe)2 q2e\nEstimate for E3. We have\n2\n\u221a 2\nC \u00b7E [Z] \u2265\n\u221a 2\nC m \u00b7 n\u2211 t=1  \u2211 e\u2208E3(t) (pe \u2212 qe)2 qe + \u03b52 4n  \u2265\u221a 2 C m \u221a \u03b52 n n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E3(t) (pe \u2212 qe)2 qe\n\u2265 m \u221a 2\u03b52 Cn \u00b7 \u221a 1 2n1/2 n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E3(t) pe(pe \u2212 qe)2 q2e = \u221a m n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E3(t) pe(pe \u2212 qe)2 q2e , (22)\nwhere the first inequality holds by (19), the second inequality is AM-GM inequality, to get the third inequality we use definition of E3 that pe qe \u2264 2 \u221a n.\nEstimate for E4. We have pe \u2212 qe \u2265 (\n1\u2212 1 2 \u221a n\n) pe for any e \u2208 E4. Therefore,\n\u221a k \u221a 2C (\n1\u2212 1 2 \u221a n\n) \u00b7E [Z] \u2265 m\u221ak\u221a 2C \u2211 e\u2208E4 pe(pe \u2212 qe) qe \u2265 \u221a m\n\u221a km \u00b7 2\u03b52\n2C \u00b7 kn3/2 \u2211 e\u2208E4 p 1/2 e (pe \u2212 qe) qe\n\u2265 \u221a m n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E4(t) p 1/2 e (pe \u2212 qe) qe , (23)\nwhere the first inequality follows from(19), to get the second inequality we estimate \u221a pe \u2265\u221a 2n1/2qe \u2265 \u221a 2\u03b52\nkn3/2 , to get the last inequality we simply use that\n\u221a x+ \u221a y \u2265 \u221a x+ y.\nFinally, combining estimates (20),(21),(22),(23) we obtain the desired bound on the variance.\nLemma 4.4. If P = Q, then Var[Z] \u2264 \u221a kn3/2.\nProof. Using similar estimate as in the proof of Lemma 4.3 we get\n\u221a Var [Z] \u2264 n\u2211 t=1 \u221a \u2211 e\u2208E(t) Var [Ze] = n\u2211 t=1 \u221a\u221a\u221a\u221a \u2211 e\u2208E(t) 2 q2e q2e \u2264 n \u00b7 \u221a kn.\nTo conclude the proof of Theorem 4.2 it remains to notice that in case P = Q the error probability (Algorithm 2 returns \u201creject\u201d) is at most\nPr [ Z > 2 \u221a kn3/2 ] \u2264 Pr [ Z > 2 \u221a Var [Z] ] \u2264 1\n5 ,\nwhere the first inequality follows from Lemma 4.4, the last is Cantelli\u2019s inequality, since E[Z] = 0. On the other hand, when Dist (P,Q) \u2265 \u03b5, then E[Z] \u2265 m \u00b7 \u03b524 . If Algorithm 2 makes an error by returning \u201caccept\u201d, then Z \u2264 2 \u221a kn3/2 \u2264 2 \u221a k C E[Z]. Thus Z \u2212E[Z] \u2264 ( 2 \u221a k\nC \u2212 1) E[Z]. Now if C is such that 1\u2212 2 \u221a k C \u2265 2 \u00b7 ( 12 \u221a 2k C + 2 \u221a 2+ \u221a k\u221a\nC\n) (any constant C \u2265 max (\u221a k(42 \u221a 2 + 4), (8 \u221a 2 + 4 \u221a k)2 )\nwould work), then Z \u2212E[Z] \u2264 \u22122 \u221a Var[Z] by Lemma 4.3. Furthermore, by Cantelli\u2019s inequality\nPr [ Z \u2264 2 \u221a kn3/2 ] \u2264 Pr [ Z \u2212E [Z] \u2264 \u22122 \u221a Var [Z] ] \u2264 1\n5 ,\ni.e., probability of Algorithm 2\u2019s error in the case P 6= Q is at most 15 .\nAnalysis of pruning step. Here we slightly modify Markov Chains Q and P so that probability of traversing any edge in Q is \u2126( \u03b5 2\nn2 ) and that P only uses these edges. To this end, we sequentially\nremove edges from Q that have too low probability of traversal in Q. Effectively, in the pruning process we are sampling from Q, but rejecting all the samples that use a \u201crare\u201d edge. We choose the threshold for the \u201crare\u201d edges in such a way that we would reject at most \u03b52 fraction of samples from Q. Recall that the Markov chain obtained by pruning Q is denoted as Q\u2217. After pruning of Q, we do the corresponding empirical rejection sampling for P (see Algorithm 3) which is equivalent to obtaining samples from the modified Markov chain P \u2217. If there are too many rejected samples, we conclude that Dist (P, P \u2217) > Dist (Q,Q\u2217) with high probability, and thus P 6= Q. Otherwise we proceed to Algorithm 2 for the modified Q and pruned samples from P .\nLet Q\u2217 be a modified Markov Chain Q that sample w \u223c Q and reject any w with e /\u2208 E\u2217.\nLemma 4.5. Dist (Q,Q\u2217) \u2264 2\u03b52\nProof. By definition of the set of pruned edges E\u2217, by removing each new edge we lose at most \u03b5 2\nkn2\nfraction of samples. As there are only at most kn2 edges in Q, the probability of avoiding removed edges is at least (1\u2212 \u03b52\nkn2 )kn 2 \u2265 1\u2212 \u03b52. Thus at most \u03b52 fraction of words Wn Q are rejected in Wn Q\u2217 , which implies that 2\u03b52 \u2264 dTV ( Wn Q ,Wn Q\u2217 ) = Dist (Q,Q\u2217).\nWe need to argue about probability of error in Algorithm 3. First, in the case P = Q Algorithm 3 could incorrectly return \u201creject\u201d. Our next Lemma 4.6 provides the necessary bound. On the other hand, when P 6= Q and Algorithm 3 does not return \u201creject\u201d, we want Algorithm 2 to be able to distinguish P \u2217 and Q\u2217. To this end, Lemma 4.6 shows that Dist (P \u2217, Q\u2217) = \u2126(\u03b5) with high probability.\nInput: \u03b5; an explicit k-sparse Markov Chain Q = {Qt}nt=1; m samples from a Markov Chain P = {Pt}nt=1 Output: m \u00b7 (1\u2212 2\u03b52) i.i.d. samples from pruned P \u2217 and pruned Q\u2217 s.t., each qe = \u2126( \u03b5 2\nn2 );\nreject P if there are not enough samples.\n1 Let E\u2217 = {e : qe > 0}; 2 while \u2203 e \u2208 E\u2217 : qe < \u03b5 2\nk\u00b7n2 do 3 E\u2217 \u2190 E\u2217 \\ e; /* e = (ij), e \u2208 E(t) */ 4 Delete (e,Q); /* set Qt(ij) = 0, re-normalize row i in Qt */ 5 foreach edge e \u2208 E\u2217 do Recompute(qe); 6 end 7 for w = s1 \u00b7 \u00b7 \u00b7 sn \u223c P do 8 if \u2200 t (stst+1) \u2208 E\u2217 then Add w to PrunedSamples; 9 else increase RejectCount;\n10 end 11 if RejectCount > 2m\u03b52 then 12 return Reject; 13 else 14 return PrunedSamples; 15 end\nAlgorithm 3: Pruning Test. In the pruned Markov Chain Q\u2217 all edges with qe > 0 satisfy qe \u2265 \u03b5 2 kn2 ; samples from Markov Chain P \u2217 can only go along these edges.\nLemma 4.6. (1) If P = Q, then Algorithm 3\u2019s error of rejecting P is at most 0.1. (2) If Dist (P,Q) \u2265 \u03b5, then either Dist (P \u2217, Q\u2217) = \u2126(\u03b5), or Algorithm 3 correctly rejects P with probability at least 0.9.\nProof. Let X` be Bernoulli random variables for ` \u2208 [m] denoting (X` = 1) whether `-th sample from P was rejected, or accepted to P \u2217 (X` = 0). Then the number of rejected samples X =\u2211m\n`=1X`. Variables X` are i.i.d. Let x = Pr[X = 1]. 1. If P = Q, then probability of rejecting a sample from P is not more than \u03b52, i.e., x \u2264 \u03b52.\nE[X] = m \u00b7 x,Var[X] = m \u00b7 (x\u2212 x2). Then\nPr [ X \u2265 2m\u03b52 ] \u2264 Pr [ X \u2265 E [X] + 3 \u221a Var [X] ] \u2264 1\n10 ,\nwhere the first inequality holds because 2m\u03b52 > m \u00b7 \u03b52 + 3 \u221a m\u03b52 > E[X] + 3 \u221a Var[X]; the second is Cantelli\u2019s inequality. 2. We first observe that Dist (P \u2217, Q\u2217) \u2265 \u03b5 \u2212 Dist (Q,Q\u2217) \u2212 Dist (P, P \u2217). We want to argue that if Dist (P, P \u2217) \u2265 6\u03b52, then Algorithm 3 rejects P with probability at least 9/10. Indeed, then probability x of rejecting a sample from P must be at least 12Dist (P, P\n\u2217) \u2265 3\u03b52, then E[X] \u2265 3m\u03b52 and\nPr [ X \u2264 2m\u03b52 ] \u2264 Pr [ X \u2212E [X] \u2264 \u2212E[X]\n3\n] \u2264 Pr [ X \u2212E [X] \u2264 \u22123 \u221a Var [X] ] \u2264 1\n10 .\nThus we get Dist (P \u2217, Q\u2217) \u2265 \u03b5\u2212Dist (Q,Q\u2217)\u2212Dist (P, P \u2217) \u2265 \u03b5\u2212 2\u03b52\u2212 6\u03b52 \u2265 \u03b52 if \u03b5 is small enough constant.\nTheorem 4.3. Together, Algorithm 3 and Algorithm 2 tell whether P = Q, or P 6= Q, when Dist (P,Q) \u2265 \u03b5, with probability at least 23 using O( n3/2 \u03b52 ) samples.\nProof. In case P = Q, Algorithm 3 produces P \u2217 = Q\u2217 and m(1 \u2212 2\u03b52) samples from P \u2217 with probability at least 0.9. Furthermore, Algorithm 2 converts these m(1\u2212 2\u03b52) samples into Poisson m\u2032 = \u2126(m) samples and accepts P = Q with 4/5 probability. Overall, we have 4/5 \u00b7 0.9 > 2/3 probability of correctly accepting P = Q.\nWhen Dist (P,Q) \u2265 \u03b5, Algorithm 3 either correctly rejects P \u2217, or produces P \u2217 and Q\u2217 s.t. Dist (P \u2217, Q\u2217) \u2265 \u03b52 with probability at least 0.9. In the latter case Algorithm 2 correctly rejects P \u2217 with probability at least 4/5. Overall, we get the probability of correctly rejecting P to be at least 4/5 \u00b7 0.9 > 2/3."}, {"heading": "4.3 Lower Bound", "text": "In this section we will show that any algorithm that tests identity of a sparse Markov chain representing card riffle shuffling requires at least \u2126 ( n \u03b52 ) independent trials, where each trial is a nlength word generated by a sparse Markov chain. We recall Definition 2 of a sparse Markov chain adapting it slightly for the convenience of lower bound presentation. Sparse Markov chains P = {Pt}Tt=1, Q = {Qt}Tt=1: each independent run consists of T = O(n) time steps; has O(n) states for each time t; Markov chain starts from a single state at t = 0 ; there are only O(1) possible transitions Pt(ij) 6= 0 from each state i to other states for all but the very first time step t > 0.\nTheorem 4.4. There is an instance of Identity testing problem for a sparse Markov chain Q that requires at least m \u2265 \u2126( n\n\u03b52 ) i.i.d. samples to check identity of Q with 99% confidence8.\nProof. The high-level proof idea is similar to that of Theorem 3.2, but particular details and proofs are more involved. At an abstract level, we construct a sparse Markov chain Q, with respect to which we are interested in testing identity, and a class of sparse Markov chains P such that\n1. Every P \u2208 P is at least \u03b5 far from Q, i.e., dTV ( WT P ,WT Q ) \u2265 \u03b5 for any P \u2208 P.\n2. There is a constant c > 0, such that it is impossible to distinguish m i.i.d. samples of T -length words generated by a random Markov chain P\u0304 \u223c P from the samples produced by Q with probability equal to or greater than 99100 , for m \u2264 cn \u03b52 .\nWe denote the joint distribution of m i.i.d. samples from Q by Q\u2297m and that from P\u0304 by P\u0304\u2297m. To prove the last point we show that dTV ( Q\u2297m, P\u0304\u2297m ) is small for some m = \u2126 ( n \u03b52 ) . We now describe our construction. To simplify presentation, we use multi-edges in the description of sparse Markov chains. We convert this instance into simple edge graph by the same duplicating trick we employed in the lower bound construction of Theorem 3.2.\nMarkov Chain Q: T = 2n + 1 time steps; single state at t = 0, states [2n] at each 1 \u2264 t \u2264 T . All states are divided into two categories: Frequently visited (F ) def = {2i \u2212 1, i \u2208 [n]} and\n8We assume \u03b5 = \u03c9(n\u22121/6) in the requirement Dist (P,Q) \u2265 \u03b5, when P 6= Q.\nRare (R) def = {2i, i \u2208 [n]}. Table below describes weighted multi-graph Q = {Qt}Tt=1: x \u2208 F, y \u2208 R, x, y \u2208 [2n] denote respective generic frequent and rare states. We adopt notational convention: 0 def = 2n, 2n+ 1 def = 1.\nF \u2192 F F \u2192 R R\u2192 F R\u2192 R\nt=1: (1\u2192 x) \u2208 E \u2205 \u2205 \u2205 Q1(1, x) = 1 n t=2k: (x\u2192 x) \u2208 E (x\u2192 x\u00b1 1) \u2208 E (y \u2192 y \u00b1 1) \u2208 E \u2205\nQt(x, x) = 1\u2212 2n Qt(x, x\u00b1 1) = 1 n Qt(y, y \u00b1 1) = 1 2\nt=2k+1: (x\u2192 x) \u2208 E \u2205 \u2205 (y \u2192 y)1,2 \u2208 E Qt(x, x) = 1 Qt(e1) = Qt(e2) = 1 2\nFamily P: every P \u2208 P has the same set of states and edges as Q. Similar to the construction in Section 3.1, we only change weights of the multi-edge pairs (independently and uniformly at random for each pair (y \u2192 y)1,2 and time t):\nPt(e1,2) = 1\u00b1 4\u03b5\n2 or Pt(e1,2) = 1\u2213 4\u03b5 2 .\nThe idea of this construction is that a typical trajectory of Markov chain Q or P \u2208 P stays in frequent states almost all the time and very rarely visits one of the rare states. However, a typical trajectory has constant probability of visiting a rare state.\nLemma 4.7. Any P \u2208 P is at least \u03b5-far from Q, i.e., Dist (P,Q) = dTV ( WT P ,WT Q ) \u2265 \u03b5.\nProof. Consider a P \u2208 P. First, we argue that probability of any word w with only a single visit to a rare state (at even t = 2k) satisfies |PrQ[w]\u2212PrP [w]| = 4\u03b5 \u00b7 PrQ[w]. Indeed, let visit to the rare state 2i in w happen at the time t = 2k, then{\nPrQ[w] = PrQ[s1 . . . s2k] \u00b7 12 \u00b7PrQ[s2k+1 . . . s2n+1|s2k+1] PrP [w] = PrP [s1 . . . s2k] \u00b7 12(1\u00b1 4\u03b5) \u00b7PrP [s2k+1 . . . s2n+1|s2k+1].\nTherefore, as Q and P have the same transitional probabilities of st \u2192 st+1 for t 6= 2k, we get\n|PrQ [w]\u2212PrP [w]| = 4\u03b5 \u00b7PrQ [w] .\nNow, the probability of visiting a rare state exactly once (at even t) in Q is precisely\n\u2211 w with\n1 rare visit\nPrQ [w] = n \u00b7 2 n \u00b7 ( 1\u2212 2 n )n\u22121 > 2 \u00b7 e\u22122 > 1 4 ,\nwhere we used inequality (1\u2212 1n) n\u22121 > e\u22121 to get the estimate in the right hand side. Hence,\ndTV\n( WT\nP ,WT Q\n) \u2265 \u2211 w with\n1 rare visit\n|PrQ [w]\u2212PrP [w]| = \u2211 w with\n1 rare visit\n4\u03b5 \u00b7PrQ [w] \u2265 4\u03b5 \u00b7 1\n4 = \u03b5.\nWe define collisions and 3-way collisions similar to the proof of Theorem 3.2. Namely, a collision is a transition that occurs in two samples (3-way collision \u2013 in at least three samples) of Q\u2297m or P\u2297m from the same state and at the same time. Moreover, as P \u223c P and Q are the same except for the double edges between rare states, we are only interested in the collisions along multi-edges between rare states.\nLemma 4.8. The expected number of collisions in samples Q\u2297m is O ( m2\nn2\n) = O ( 1 \u03b54 ) .\nProof. Let Iw(m1,m2, y, t) indicate the event that either of the transitions along edges e1,2 = y \u2192 y at time t, occurred in samples m1 and m2. Note that at each fixed time all of the n frequent states are equally likely to occur in a sample from Q. Therefore, the probability of visiting given rare state y at time t in a single run is at most O(1/n2). This implies PrQ[Iw(m1,m2, y, t) = 1] = O (\n1 n2 \u00b7 1 n2\n) = O(1/n4). Let X denote the total number of collisions.\nE [X] = \u2211\nm1 6=m2 \u2211 i 6=j E [Iw(m1,m2, y, t)] = O ( m2n2 1 n4 ) = O ( m2 n2 )\nLemma 4.9. The probability of a 3-way collision in samples Q\u2297m is o(1).\nProof. Similar to the proof of Lemma 4.8 we can give an upper bound on the expected number of 3-way collisions being O ( n2m3 \u00b7 1 n6 \u00b7 ) = O ( m3 n4 ) . Markov\u2019s inequality concludes the proof.\nNow consider a typical set of m words generated by Q (or equivalently drawn from Q\u2297m). As we know from Lemma 4.9 it has no 3-way collisions and by Markov\u2019s inequality and Lemma 4.8 has at most O( 1\n\u03b52 ) collisions with probability greater than 9/10 (for sufficiently small c > 0 and\nm = cn \u03b52\n). As we show next a typical set of m words drawn from Q has similar probability under the P\u0304 \u223c P and Q models.\nLemma 4.10. At least 1/2 of the sets S = {w1, . . . , wm} of samples from Q\u2297m satisfy\n1 2 \u00b7PrQ\u2297m [S] < PrP\u0304\u2297m [S] < 2 \u00b7PrQ\u2297m [S]\nProof. We consider the ratio of the respective probabilities (\u2217) def= PrP\u0304\u2297m [S]PrQ\u2297m [S] . As in Lemma 3.7, both probabilities in the numerator and denominator can be expressed in terms of simple statistics for the set S, specifically the number of single step transitions between pairs of states. Also similar to the Lemma 3.7, the corresponding multiplicative terms for P\u0304\u2297m and Q\u2297m are identical except for the collisions along multi-edges between two rare states (at least two transitions between a pair of rare states). Moreover, we also differentiate type I and type II collisions between rare states: (type I) transitions between rare states y \u2192 y at time t were made along different edges e1 and e2; (type II) two transitions were made along the same edge either e1, or e2. Per type I and II collisions the corresponding terms in (\u2217) are respectively (1\u2212 4\u03b5)(1 + 4\u03b5) = 1\u2212 16\u03b52 and 1 + 16\u03b52.\nWe further continue following the proof of the Lemma 3.7, and make sure that with high probability there are only X = O( 1\n\u03b54 ) collisions and no 3-way collisions in S. We can make sure\nthat the difference between numbers of type I and type II collisions is at most O( \u221a X) = O( 1\n\u03b52 )\nwith probability at least 3/4, as the choice of collision type in w under Q model is uniform between\ntype I and type II and independent across different collisions. For small enough m = \u2126( n \u03b52\n) we can make sure that at least 12 fraction of words w under Q model have the number of collisions at most c1 \u03b54 and also have the difference between number of type I and type II collisions at most c2 \u03b52\n, for some small constants c1, c2 > 0. In this case we get the following bounds on (\u2217).\n2 > ( 1 + 16\u03b52 ) c2 \u03b52 > PrP\u0304\u2297m [S] PrQ\u2297m [S] > ( 1\u2212 256\u03b54 ) c1 2\u03b54 \u00b7 ( 1\u2212 16\u03b52 ) c2 \u03b52 > 1/2\nLemma 4.10 shows that dTV ( Q\u2297m, P\u0304\u2297m ) \u2264 34 for m \u2264 cn \u03b52\nfor some constant c, which implies that no algorithm can successfully distinguish Q from the family P with probability greater than 3 4 for some m = \u2126( n \u03b52 )."}, {"heading": "5 Open Questions", "text": "In this paper, we proposed a new framework for studying property testing questions on Markov chains. There seem to be multiple avenues for future research and abundant number of open problems arising from this framework. We first list some questions which may be of interest here.\n1. What is the optimal sample complexity for identity testing on symmetric Markov chains? In this paper, we show an upper bound of O\u0303 ( HitTQ \u00b7 log (HitTQ) + n\u03b5 ) samples (Theorem 3.1).\nWe conjecture that \u0398 ( n \u03b5 ) (same as our lower bound) is the right sample complexity for this problem and an explicit dependence on the hitting time of chain Q may not be necessary. It is implicitly captured to an extent by the guarantee we get from the parameter \u03b5.\n2. What is the optimal sample complexity for identity testing on the sparse Markov chains defined in Section 4? In this paper, we show an upper bound of O\u0303 ( n3/2\n2\n) (Theorem 4.2).\nWe conjecture that \u0398 ( n \u03b52 ) (same as our lower bound) is the right sample complexity for this problem.\n3. As there is a natural operation of taking a convex combination of Markov chains, it is natural to ask how our spectral definition of distance 1\u2212 \u03c1 (\n[P,Q]\u221a\n) between two symmetric chains\nchanges if we substitute either P or Q with a convex combination of P and Q. How does the distance now relate to the original value?\n4. How is the distance \u03b5 = 1 \u2212 \u03c1 (\n[P,Q]\u221a\n) between two Markov chains P and Q related to\nthe distance between Markov chains P k and Qk,i.e., states in Markov chains P and Q being observed only at intervals of size k?\n5. Given \u03b52 \u2265 \u03b51, and access to words from each of two chains, can we distinguish whether the two chains are \u2264 \u03b51-close or \u2265 \u03b52-far? This problem, known as closeness testing in literature, is another interesting direction using our framework."}], "references": [{"title": "Optimal testing for properties of distributions", "author": ["Jayadev Acharya", "Constantinos Daskalakis", "Gautam Kamath"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Acharya et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2015}, {"title": "The frequency goodness of fit test for probability chains", "author": ["Maurice S Bartlett"], "venue": "In Mathematical Proceedings of the Cambridge Philosophical Society,", "citeRegEx": "Bartlett.,? \\Q1951\\E", "shortCiteRegEx": "Bartlett.", "year": 1951}, {"title": "Trailing the dovetail shuffle to its lair", "author": ["Dave Bayer", "Persi Diaconis"], "venue": "Ann. Appl. Probab., 2(2):294\u2013313,", "citeRegEx": "Bayer and Diaconis.,? \\Q1992\\E", "shortCiteRegEx": "Bayer and Diaconis.", "year": 1992}, {"title": "Testing random variables for independence and identity", "author": ["Tugkan Batu", "Eldar Fischer", "Lance Fortnow", "Ravi Kumar", "Ronitt Rubinfeld", "Patrick White"], "venue": "In Proceedings of the 42nd Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Batu et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Batu et al\\.", "year": 2001}, {"title": "Testing closeness of discrete distributions", "author": ["Tu\u011fkan Batu", "Lance Fortnow", "Ronitt Rubinfeld", "Warren D Smith", "Patrick White"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Batu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Batu et al\\.", "year": 2013}, {"title": "Hypothesis testing for markovian models with random time observations", "author": ["Flavia Barsotti", "Anne Philippe", "Paul Rochet"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Barsotti et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barsotti et al\\.", "year": 2016}, {"title": "Testing shape restrictions of discrete distributions", "author": ["Cl\u00e9ment L. Canonne", "Ilias Diakonikolas", "Themis Gouleakis", "Ronitt Rubinfeld"], "venue": "In Proceedings of the 33rd Symposium on Theoretical Aspects of Computer Science,", "citeRegEx": "Canonne et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Canonne et al\\.", "year": 2016}, {"title": "Optimal algorithms for testing closeness of discrete distributions", "author": ["Siu-On Chan", "Ilias Diakonikolas", "Gregory Valiant", "Paul Valiant"], "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Chan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2014}, {"title": "Testing equivalence between distributions using conditional samples", "author": ["Cl\u00e9ment L. Canonne", "Dana Ron", "Rocco A. Servedio"], "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Canonne et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Canonne et al\\.", "year": 2014}, {"title": "Testing k -modal distributions: Optimal algorithms via reductions", "author": ["Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A. Servedio", "Gregory Valiant", "Paul Valiant"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Daskalakis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daskalakis et al\\.", "year": 2013}, {"title": "Group representations in probability and statistics", "author": ["Persi Diaconis"], "venue": "Lecture notes. Hayward, Calif. Institute of Mathematical Statistics,", "citeRegEx": "Diaconis.,? \\Q1988\\E", "shortCiteRegEx": "Diaconis.", "year": 1988}, {"title": "Mathematical Developments from the Analysis of Riffle Shuffling", "author": ["P. Diaconis"], "venue": "Technical report (Stanford University. Dept. of Statistics)", "citeRegEx": "Diaconis.,? \\Q2002\\E", "shortCiteRegEx": "Diaconis.", "year": 2002}, {"title": "A new approach for testing properties of discrete distributions", "author": ["Ilias Diakonikolas", "Daniel M. Kane"], "venue": "In Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Diakonikolas and Kane.,? \\Q2016\\E", "shortCiteRegEx": "Diakonikolas and Kane.", "year": 2016}, {"title": "The effect of dependence on chi-squared and empiric distribution tests of fit", "author": ["Leon J Gleser", "David S Moore"], "venue": "The Annals of Statistics,", "citeRegEx": "Gleser and Moore,? \\Q1983\\E", "shortCiteRegEx": "Gleser and Moore", "year": 1983}, {"title": "Mixing time estimation in reversible markov chains from a single sample path", "author": ["Daniel J Hsu", "Aryeh Kontorovich", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Hsu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2015}, {"title": "The bhattacharyya distance and detection between markov chains", "author": ["Dimitri Kazakos"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Kazakos.,? \\Q1978\\E", "shortCiteRegEx": "Kazakos.", "year": 1978}, {"title": "Markov chains and mixing times. Providence, R.I", "author": ["David Asher Levin", "Yuval Peres", "Elizabeth Lee Wilmer"], "venue": "American Mathematical Society,", "citeRegEx": "Levin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2009}, {"title": "Testing properties of collections of distributions", "author": ["Reut Levi", "Dana Ron", "Ronitt Rubinfeld"], "venue": "Theory of Computing,", "citeRegEx": "Levi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Levi et al\\.", "year": 2013}, {"title": "The effect of dependence on chi squared tests of fit", "author": ["David S Moore"], "venue": "The Annals of Statistics,", "citeRegEx": "Moore,? \\Q1982\\E", "shortCiteRegEx": "Moore", "year": 1982}, {"title": "On size increase for goodness of fit tests when observations are positively dependent", "author": ["I Molina", "D Morales", "L Pardo", "I Vajda"], "venue": "Statistics & Risk Modeling,", "citeRegEx": "Molina et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Molina et al\\.", "year": 2002}, {"title": "A coincidence-based test for uniformity given very sparsely sampled discrete data", "author": ["Liam Paninski"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Paninski.,? \\Q2008\\E", "shortCiteRegEx": "Paninski.", "year": 2008}, {"title": "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling", "author": ["Karl Pearson"], "venue": "Philosophical Magazine Series", "citeRegEx": "Pearson.,? \\Q1900\\E", "shortCiteRegEx": "Pearson.", "year": 1900}, {"title": "The analysis of categorical data from complex sample surveys: Chi-squared tests for goodness of fit and independence in two-way tables", "author": ["Jon N.K. Rao", "Alastair J. Scott"], "venue": "Journal of the Americal Statistical Association,", "citeRegEx": "Rao and Scott.,? \\Q1981\\E", "shortCiteRegEx": "Rao and Scott.", "year": 1981}, {"title": "Serial dependence of observations leading to contingency tables, and corrections to chi-squared statistics", "author": ["Simon Tavare", "Patricia M.E. Altham"], "venue": null, "citeRegEx": "Tavare and Altham.,? \\Q1983\\E", "shortCiteRegEx": "Tavare and Altham.", "year": 1983}, {"title": "Error exponents for composite hypothesis testing of Markov forest distributions", "author": ["Vincent Y.F. Tan", "Animashree Anandkumar", "Alan S. Willsky"], "venue": "In Proceedings of the 2010 IEEE International Symposium on Information Theory, ISIT", "citeRegEx": "Tan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2010}, {"title": "An automatic inequality prover and instance optimal identity testing", "author": ["Gregory Valiant", "Paul Valiant"], "venue": "In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Valiant and Valiant.,? \\Q2014\\E", "shortCiteRegEx": "Valiant and Valiant.", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "Classical distribution testing assumes access to i.i.d. samples from the distributions that are being tested. We initiate the study of Markov chain testing, assuming access to a single sample from the Markov Chains that are being tested. In particular, we get to observe a single trajectory X0, . . . , Xt, . . . of an unknown Markov Chain M, for which we do not even get to control the distribution of the starting state X0. Our goal is to test whether M is identical to a model Markov Chain M\u2032. In the first part of the paper, we propose a measure of difference between two Markov chains, which captures the scaling behavior of the total variation distance between words sampled from the Markov chains as the length of these words grows. We provide efficient and sample nearoptimal testers for identity testing under our proposed measure of difference. In the second part of the paper, we study Markov chains whose state space is exponential in their description, providing testers for testing identity of card shuffles. We apply our results to testing the validity of the Gilbert, Shannon, and Reeds model for the riffle shuffle. Supported by a Microsoft Research Faculty Fellowship, and NSF Award CCF-1551875, CCF-1617730 and CCF1650733. Supported by NSF Award CCF-1551875, CCF-1617730 and CCF-1650733. Supported by NSF Award CCF-1551875, CCF-1617730 and CCF-1650733. ar X iv :1 70 4. 06 85 0v 1 [ cs .L G ] 2 2 A pr 2 01 7", "creator": "LaTeX with hyperref package"}}}