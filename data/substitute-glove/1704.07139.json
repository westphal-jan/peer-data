{"id": "1704.07139", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "An Aposteriorical Clusterability Criterion for $k$-Means and Simplicity of Clustering", "abstract": "We imply way indeed of it well - clusterable data move same the fact now concept following both regards other $ k $ - means clustering generalizes one changes seem. The creations produced go still that once can entire posteriori (was gets $ \u03b2 $ - means) you if the product set is though - clusterable types follow.", "histories": [["v1", "Mon, 24 Apr 2017 10:55:41 GMT  (151kb,D)", "http://arxiv.org/abs/1704.07139v1", "37 pages"]], "COMMENTS": "37 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mieczys{\\l}aw a k{\\l}opotek"], "accepted": false, "id": "1704.07139"}, "pdf": {"name": "1704.07139.pdf", "metadata": {"source": "CRF", "title": "An Aposteriorical Clusterability Criterion for k-Means and Simplicity of Clustering", "authors": ["Mieczys\u0142aw A. K\u0142opotek"], "emails": ["klopotek@ipipan.waw.pl"], "sections": [{"heading": null, "text": "The novelty introduced here is that one can a posteriori (after running k-means) check if the data set is well-clusterable or not."}, {"heading": "1 Introduction", "text": "It is a commonly observed phenomenon that most practically used clustering algorithms (like k-means) have a high theoretical computational complexity (are NP-hard), but at the same time in many (though not all) practical applications they perform quite well (converge quickly enough) yielding more or less usable output. Apparently then, the data must have the property that some data sets are better clusterable than other.\nThough a number of attempts have been made to capture formally the intuition behind clusterability, none of these efforts seems to have been successful, as Ben-David exhibits in [8] in depth. He points at two important shortcomings of current state-of-the-art research results: the clusterability cannot be checked prior to applying a potentially NP-hard clustering algorithm to the data, and beside this, known clusterability criteria impose strong separation constraints. Though a recent paper by Ackerman [1] partially eliminates these problems, but regrettably at the expense of introducing\nar X\niv :1\n70 4.\n07 13\n9v 1\n[ cs\n.L G\n] 2\nuser-defined parameters that do not seem to be intuitive (in terms of one\u2019s imagination about what well-clusterable data are.).\nTherefore in this paper we try a different approach to defining what well clusterable data are. In particular we do not require that we have to say beforehand (before clustering) whether or not the data is well-clusterable. Instead we require that one shall be able to state aposterioriacally whether or not the data is well-clusterable according to well-clusterability criteria that were assumed.\nOur contribution encompasses:\n\u2022 Two brands of well-clusterability criteria for data to be clustered via k-means algorithm, that can be verified ex-post (both positively and negatively) without great computational burden.\n\u2022 Demonstration, that the structure of well-clusterable data (according to these criteria) is easy to recover.\n\u2022 Demonstration that if well-clusterable data structure (in that sense) was not discovered by k-means++, then there is no such structure in the data.\n\u2022 Demonstration that large gaps between data clusters are not sufficient to ensure well-clustrability.\nThe structure of this paper is as follows: In Section 2 we recall the previous work on the topic. In Section 3 we show that large gaps are not sufficient for well-clusterability. In Section 4 we introduce the first version of well-clusterability concept and show that data well-clustered in this sense are easily learnable via k-means++. This concept has the drawback that no data points (outliers) can lie in wide areas between the clusters. Therefore in Section 7 we propose a core-based well-clusterability concept and show that data well-clustered in this sense are also easily learnable via k-means++. The concept of cluster core itself is introduced and investigated in Section 5 and a method determining proper gap size under these new conditions is derived in Section 6. In Section 8 we draw some conclusions from this research."}, {"heading": "2 The problem of clusterability in the previ-", "text": "ous work\nIntuitively the clusterability shall be a function taking a set of points and returning a real value saying how \u201dstrong\u201d or \u201dconclusive\u201d is the clustering\nstructure of the data [2]. This intuition, however, turns out not to be formalized in a uniform way so that quite a large number of formal definitions have been proposed. Ackerman and Ben-David in [2] studied several of these notions. They concluded that across the various formalizations, two phenomena co-occur: on the one hand well-clusterable data sets (with high \u201dclusterability\u201d value) are computationally easy to cluster (in polynomial time), but on the other hand identification whether or not the data is well-clusterable is NP-hard.\nBen-David [8] performed an interesting investigation of the concepts of clusterability from the point of view of the capability of \u201dnot too complex\u201d algorithms to discover the cluster structure, (negatively) verifying the working hypothesis that \u201cClustering is difficult only when it does not matter\u201d (the CDNM thesis).\nHe considered the following notions of clusterability, present in the literature:\n\u2022 Perturbation Robustness meaning that small perturbations of distances / positions in space of set elements do not result in a change of the optimal clustering for that data set. Two brands may be distinguished: additive [2] and multiplicative ones [9] (the limit of perturbation is upper-bounded either by an absolute value or by a coefficient).\n\u2022 -Separatedness meaning that the cost of optimal clustering into k clusters is less than 2 times the cost of optimal clustering into k\u22121 clusters [13]\n\u2022 (c, )-Approximation- Stability [6] meaning that if the cost function values of two partitions differ by the factor c, then the distance (in some space) between the partitions is at most . As Ben-David recalls, this implies the uniqueness of optimal solution.\n\u2022 \u03b1-Centre Stability [5] meaning, for any centric clustering, that the distance of an element to its cluster centre is \u03b1 times smaller than the distance to any other cluster centre under optimal clustering.\n\u2022 (1+\u03b1) Weak Deletion Stability [4] meaning that given an optimal cost function value OPT for k centric clusters, then the cost function of a clustering obtained by deleting one of the cluster centres and assigning elements of that cluster to one of the remaining clusters should be bigger that (1 + \u03b1) \u00b7OPT .\nUnder these notions of clusterability algorithms have been developed clustering the data nearly optimally in polynomial times, when some constraints are matched by the mentioned parameters.\nHowever, these conditions seem to be rather extreme. For example, given the (c, )-Approximation- Stability [6], polynomial time clustering requires that, in the optimal clustering (beside its uniqueness), all but an -fraction of the elements, are 20 times closer to their own cluster centre than to every other cluster centre. -Separatedness requires that the distance to its own cluster centre must be at least 200 times closer than to every other cluster element [13]. And this is still insufficient if the clusters are not balanced. A ratio of 107 is deemed by these authors as sufficient. (1 + \u03b1) Weak Deletion Stability [4] demands distances to other clusters being log(k) times the \u201daverage radius\u201d of the own cluster. The perturbational stability [2] induces exponential dependence on the sample size.\nAnyway, we can draw a certain important conclusion from these concepts of clusterability mentioned above: Peop0le agree that a data set is well clusterable if each cluster is distant (widely separated) from the other clusters.\nThis idea occurs in many other clusterability concepts. Epter et al. [10] considers the data as clusterable when the minimum between - cluster separation exceeds the maximum in - cluster distance (called elsewhere \u201dperfect separation\u201d). 1 Balcan et al. [7] proposes to consider data as clusterable if each element is closer to all elements in its cluster than to all other data (called also \u201dnice separation\u201d). 2 Interestingly, k-means reflects the Balcan concept \u201don average\u201d that is each element average squared distance to elements of the same cluster is smaller than the minimum (over other clusters) averaged squared distance to elements of a different cluster.\nRecently also Ackerman et al. [1] derived a method for testing clusterability of data based on the large gap assumption. They investigate the histogram of (all) mutual dissimilarities between data points. If there is no data structure, the distribution should be unimodal. If there are distant clusters, then there will occur one mode for short distances (within clusters) and at least one for long distances (between clusters). Hence, to detect clusterability, they apply tests of multimodality, namely the Dip [11] and Silverman [14] tests.\nBut the criterion of a sufficiently large gap between clusters is not reflected in various clustering function objectives, like for example k-means which may reach an optimum with poorly separated clusters in spite of the fact that there exists an alternative partition of data with a clear separation between clusters in the data, as we will demonstrate in Section 3. Also in Section 3 we will\n1It has been shown in the literature that under this notion of well-clusterability single link algorithm can detect clusters separated in such a way. It has also been shown that centre based algorithms like k-means may fail to detect such clusters. 2It has been shown in the literature that this notion of well-clusterability is hard to decide in a data set.\ndemonstrate, that multimodel distributions can be detected by Ackerman\u2019s method even if there is no structure in the data.\nBen-David [8] raises a further important point that it is usually (in practically all above mentioned methods except [1], which has a flaw by itself) impossible to verify apriori if the data fulfils the clusterability criterion because the conditions refer either to all possible clusterings or to optimal clustering so that we do not have the possibility to verify whether or not the data set is clusterable, before one starts clustering (but usually computing the optimum is NP-hard).\nIn this paper, however, we would like to stress that the situation is even worse. Even at the termination of the clustering algorithm we are unable to say whether or not the clustered data set turned out to be well-clusterable. For example, the -Separatedness criterion requires that we know the nearly optimal solution for clustering into k and k \u2212 1 elements. While we can usually get the upper approximations for the cost functions in both cases, we need actually the lower approximation for k\u2212 1 in order to decide ex post if the data was well-clusterable, and hence whether or not we can say that we approximated the correct solution in some way. But we get it only for k = 2, hence for higher k the issue is not decidable.\nThe issue of ex-post decision on clusterability seems nevertheless to be simpler to solve than the apriorical one, therefore we will attack it in this paper. We are unaware that such an issue was even raised in the past. Though the criteria of [10] and [7] can clearly be applied ex post to see that in the resulting clustering the clusterability criteria hold, but these approaches lack the solving of the inverse issue: what if the clusterability criteria are not matched by the result clustering - is the data unclustrable? Could no other algorithm discover the clusterable structure?\nOne shall note at this point that the approach in [1] is different with this respect. Compared to methods requiring finding the optimum first, Ackerman\u2019s approach seems to fulfil Ben-David requirement, that we can see if there is clusterability in the data before starting the clustering process as the clusterability method is computationally optimal as the computation of the histogram of dissimilarities is quadratic in sample size. But at an in-depth-investigation, the Ackerman\u2019s clusterability determination method misses one important point: it requires a user-defined parameter and the user may or may not make the right guess. Furthermore, even if clusterability is decided by Ackerman\u2019s tests, it is still uncertain if k-means algorithm will be willing to find such a clustering that fits Ackerman\u2019s clusterability criterion. Beside this, as visible in Figure 1, one can easily find counterexamples to their concept of clusterability.\nSo in summary the issue of an aposteriorical determination if the data\nwere clusterable, remains an open issue.\nTherefore it seems to be justified to restrict oneself to a problem as simple as possible in order to show that the issue is solvable at all. So in this paper we will limit ourselves to the issue of clusterability for the purposes of k-means algorithm.3 Furthermore we restrict ourselves to determine such cases when the clusterability is decidable \u201dfor sure\u201d.\nThe first problem to solve seems to be to get rid of the dependence on the undecidedness of optimality of the obtained solution.\nBut before proceeding let us recall the k-means cost function definition.\nQ(C) = m\u2211 i=1 k\u2211 j=1 uij\u2016xi \u2212 \u00b5j\u20162 = k\u2211 j=1 1 nj \u2211 xi,xl\u2208Cj \u2016xi \u2212 xl\u20162 (1)\nfor a dataset X under some partition C = {C1, . . . , Ck} into the predefined number k of clusters, C1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ck = X, where uij is an indicator of the membership of data point xi in the cluster Cj having the centre at \u00b5j = 1 |Cj || \u2211 x\u2208Cj x.\nThe k-means algorithm starts with some initial guess of the positions of \u00b5j for j = 1, . . . , k and then alternating two steps: cluster assignment and centre update till some convergence criterion is reached, e.g. no changes in cluster membership. The cluster assignment step updates uij values so that each element xi is assigned to a cluster represented by the closest \u00b5j. The centre update step uses the update formula \u00b5j = 1 |Cj || \u2211 x\u2208Cj x.\nk-means++ algorithm is a special case of k-means where the initial guess of cluster centres proceeds as follows. \u00b51 is set to be a data point uniformly sampled from X. The subsequent cluster centres are data points picked from X with probability proportional to the squared distance to the closest cluster centre chosen so far. For details check [3]. Note that the algorithm proposed by [13] differs from the k-means++ only by the non-uniform choice of the first cluster centre (the first pair of cluster centres should be distant, and the choice of this pair is proportional in probability to the squared distances between data elements).\n3 Non-suitability of gap-based clusterability criteria for k-means\nLet us discuss more closely the relationship between the gap-based wellclusterability concepts developed in the literature and the actual optimality criterion of k-means. Specifically let us consider the approaches to clusterability of [1], [10], [5] and [7].\nHuman intuition will tell us that if the groups of data points occur in the data and there are large spaces between these groups, then it should be these groups that will be chosen as the actual clustering. On the other hand if there are no gaps between the groups of data points, then one would expect that the data are not considered as well-clusterable. Furthermore, if the data is well-clusterable, one would expect a reasonable clustering algorithm to discover easily such a well-clusterable data structure.\nHowever, these intuitions prove wrong in case of k-means. Let us first point to the fact that [1] may indicate a clear bimodal structure in the data where there are no gaps in the data. We are unaware of anybody pointing at this weakness of well-clusterability in [1]: Imagine a thin ring uniformly covered with data points. We would be reluctant to say that there is a clustering structure in such data. Nonetheless we will see two obvious nodes in such data. See Figure 1. The thinner the ring, the more obvious\n3 The k-means algorithm seems to be quite popular in various variants both in traditional, kernel and spectral clustering. Hence the results may be still of sufficiently broad importance.\nthe reason for the multimodality will be: we will get closer and closer to a cosine function.\nOn the other hand, even if there are gaps between groups of data, for example those required by [10], [5] or [7], k-means optimum may not lie in the partition exhibiting gap based well-clusterability property in spite of its existence, And not only for these gaps, but also for any arbitrary many times larger ones. As [10] is concerned, it may be considered as a special case of [7]. [5] may be viewed in turn as a strengthening of the concept of [10]. So let us discuss a situation in which both perfect and nice separation criteria are identical that is of two clusters. We will show that whatever \u03b1 we assume in the \u03b1-stability concept, k-means fails to be optimal under unequal class cardinalities. Let these clusters, CA, CB be each enclosed in a ball of radius r and the distance between ball centres should be at least 4r. We have demonstrated in [12] that under these circumstances the clustering of data into CA, CB reflects a local minimum of k-means cost function. But it is not the global minimum, as we will show subsequently. So at least for k-means the criteria of Epter and Balcan and Awasthi cannot be viewed as realistic definitions of well-clusterability.\nFor purposes of demonstration we assume that both clusters are of different cardinalities nA, nB and let nA > nB. We show that whatever distance between both clusters, we can get such a proportion of nA/nB that the clustering into CA, CB is not optimal.\nLet us consider a d-dimensional space. Let us select the dimension that contributes most to the variance in cluster CA. So the variance along this direction amounts to at least the overall variance divided by d. Let us denote this variance component as Vd. Consider this coordinate axis responsible for Vd to have the origin at the cluster centre of CA. Project all the points of cluster CA on this axis. The variance of projected points will be just Vd. Split the projected data set into two parts P1, P3, one with coordinate above 0 and the rest. Let the centres of P1, P3 lie x1, x2 away from the cluster centre. Let there be n1 data points of P1 be at most x1 distant from the origin, and n2 more than x1 from the origin. Let there be n3 data points of P3 be at most x3 distant from the origin, and n4 more than x3 from the origin. Obviously, n1 + n2 + n3 + n4 = nA. Under these circumstances, let us ask the question whether for a Vd some minimal values of x1, x3 are implied. Because if so, then by splitting the cluster CA into P1, P3 and by increasing the cardinality of CA the split into P1, P2 \u222a CB will deliver a lower Q value so that for sure the clustering into CA, CB will be not be optimal.\nSo observe that Vd \u2264 ( x21(n1 + n 2 1/n2 + n1 + n2)\nn1 + n2 \u00b7 (n1 + n2) + x23 \u00b7 (n3 + n23/n4 + n3 + n4) (n3 + n4)\n\u00b7 (n3 + n4) ) /nA\nthat is\nVd \u2264 (x21 \u00b7 (n1 + n21/n2 + n1 + n2) + x23 \u00b7 (n3 + n23/n4 + n3 + n4))/nA\nNote that we can delimit n2, n4 from below due to the relationship: (r\u2212 x1) \u00b7 n2 \u2265 n1 \u00b7 x1, (r \u2212 x3) \u00b7 n4 \u2265 n3 \u00b7 x3.\nTherefore\nVd \u2264 (x21\u00b7(n1+n21\u00b7(r\u2212x1)/n1/x1+n1+n2)+x23\u00b7(n3+n23\u00b7(r\u2212x3)/n3/x3+n3+n4))/nA\nHence\nVd \u2264 (x21\u00b7(2\u00b7n1+n2)+n21\u00b7(r\u2212x1)\u00b7x1/n1+x23\u00b7(2\u00b7n3+n4)+n23\u00b7(r\u2212x3)\u00b7x3/n3)/nA\nVd \u2264 (x21 \u00b7 (2 \u00b7n1+n2)+n1 \u00b7 (r\u2212x1) \u00b7x1+x23 \u00b7 (2 \u00b7n3+n4)+n3 \u00b7 (r\u2212x3) \u00b7x3)/nA\nVd \u2264 (x21 \u00b7 (n1 + n2) + n1 \u00b7 r \u00b7 x1 + x23 \u00b7 (n3 + n4) + n3 \u00b7 r \u00b7 x3)/nA\nLet n1 + n2 be the minority among data points - then x1 is larger and x3 is smaller of the two.\nVd \u2264 (x21 \u00b7(n1+n2)+n1 \u00b7r\u00b7x1+(x1 \u00b7(n1+n2)/(n3+n4))2 \u00b7(n3+n4)+n3 \u00b7r\u00b7x3)/nA\nVd \u2264 (x21 \u00b7 (n1 + n2) + n1 \u00b7 r \u00b7 x1 + x21 \u00b7 (n1 + n2)2/(n3 + n4) + n3 \u00b7 r \u00b7 x3)/nA\nVd \u2264 (x21 \u00b7 (n1 + n2) \u00b7 nA/(n3 + n4) + n1 \u00b7 r \u00b7 x1 ++n3 \u00b7 r \u00b7 x3)/nA\nThe above implies\nVd \u2264 (x21 \u00b7 (n1 + n2) \u00b7 nA/(n3 + n4) + (n1 + n2) \u00b7 r \u00b7 x1 ++(n3 + n4) \u00b7 r \u00b7 x3)/nA\nHence\nVd \u2264 ((x3 \u00b7(n3+n4)/(n1+n2))2 \u00b7(n1+n2)\u00b7nA/(n3+n4)++2\u00b7(n3+n4)\u00b7r\u00b7x3)/nA\nWe can delimit n1 + n2 from below due to relationship x3 \u00b7 (n3 + n4) \u2264 (n1 + n2) \u00b7 r. Therefore\nVd \u2264 (x23 \u00b7 (n3 + n4)2/(n1 + n2) \u00b7 nA/(n3 + n4) + +2 \u00b7 (n3 + n4) \u00b7 r \u00b7 x3)/nA\nVd \u2264 (x23 \u00b7 (n3+n4)2 \u00b7 r/x3/(n3+n4) \u00b7nA/(n3+n4)++2 \u00b7 (n3+n4) \u00b7 r \u00b7x3)/nA\nVd \u2264 (x3 \u00b7 r \u00b7 nA ++2 \u00b7 (n3 + n4) \u00b7 r \u00b7 x3)/nA\nSo we obtain Vd \u2264 3 \u00b7 x3 \u00b7 r\nThis means that x3 \u2265 Vd/3/r\nwhat had to be shown in order to ensure that by scaling up nA it pays off to split the first cluster and to attach the contents of the second one to one of the parts of the first, if we keep Vd when increasing nA. ."}, {"heading": "4 Our basic approach to clusterability", "text": "Let us stress at this point that the issue of well-clusterability is not only a theoretical issue, but it is of practical interest too. For example in creating synthetic data sets for investigating suitability of various clustering algorithms. But also after having performed the clustering process with whatever method we have, we need to answer one important question: whether or not the obtained clustering meets the expectation of the analyst.\nThese expectations may be divided into several categories:\n\u2022 matching business goals,\n\u2022 matching underlying algorithm assumptions,\n\u2022 proximity to the optimal solutions.\nBusiness goals of the clustering may be difficult to express in terms of data for an algorithm, or may not fit the algorithm domain or may be too expensive to collect prior to performing an approximate clustering.\nFor example, when one seeks a clustering that would enable efficient collection of cars to be scrapped (disassembly network), then one has to match multiple goals, like covering the whole country, maximum distance from client to the disassembly station, and of course the number of prospective clients, which is know with some degree of uncertainty. The distances to the clients are frequently not Euclidean in nature (due to geographical obstacles like rivers mountains etc.), while the preferred k-means algorithm works best with geometrical distances, no upper distance can be imposed etc. Other algorithms may induce same or different problems. So a posteriori one has to check if the obtained solution meets all criteria, does not violate constraints and is stable under fluctuation of the actual set of clients.\nThe other two problems are somehow related to one another. For example, you may have clustered the data being a subsample of the proper data set and the question may be raised how close the sub-sample cluster centres are to the cluster centres of the proper data set. Known methods allow to estimate this discrepancy given that we know that the cluster sizes do not differ too much. So prior to evaluating the correctness of cluster centre estimation we have to check if cluster proportions are within a required range (or if sub-sample size is relevant for such a verification). As another example consider methods of estimating closeness to optimal clustering solution under some general data distributions (like for the k-means++[3]), but the guarantees are quite loose. But at the same time the guarantees can be much tighter if the clusters are well-separated in some sense. So if we want to be sure with a reasonable probability that the obtained solution is sufficiently close to the optimum, we would need to check if the obtained clusters are well separated in the defined sense.\nWith this in mind, as mentioned, a number of researchers developed the concept of data clusterability. The notion of clusterability should intuitively reflect the following idea: if it is easy to see that there are clear-cut clusters in the data, then one would say that the data set is clusterable. \u201dEasy to see\u201d may mean either a visual inspection or some algorithm that quickly identifies the clusters. The well-established notion of clusterability would improve our understanding of the concept of the cluster itself - a well-defined clustering would be a clustering of clusterable points. This also would be a foundation for objective evaluation of clustering algorithms. The algorithm shall perform well for well-clusterable data and when the clusterability condition would be violated to some degree, the performance of a clustering algorithm is allowed to deteriorate also, but the algorithm quality would be measured on how the\nclusterability violation impacts the deterioration of algorithm performance. However, the issue turns out not to be that simple. As is well known, each algorithm seeking to discover a clustering may be betrayed somehow to fail to discover a clustering structure that is visible upon human inspection of data. So instead of trying to reflect human vision of clusterability of the data set independently of the algorithm, let us rather concentrate on finding a concept of clusterability that is both reflecting human perception and the minimum of cost function of a concrete algorithm, in our case k-means. We will particularly concentrate on its version called k-means++.\nSo let us define:\nDefinition 1. A data set is well-clusterable with respect to k-means if (a) the data points may be split into subsets that are clearly separated by an appropriately chosen gap such that (b) the global minimum of k-means cost function coincides with this split and (c) with high probability the k-means++ algorithm discovers this split and (d) if the split was found, it may be verified that the data subsets are separated by the abovementioned gap and (e) if the k-means++ did not discover a split of the data fulfilling the requirement of the existence of the gap, then with high probability the split described by points (a) and (b) does not exist.\nIn the paper [12] we have investigated conditions under which one can ensure that the minimum of k-means cost function is related to a clustering with (wide) gaps between clusters.\nThe conditions for clusterable data set therein are rather rigid, but serve the purpose of demonstration that it is possible to define properties of the data set that ensure this property of the minimum of k-means. Let us recall below the main result in this respect.\nSo assume that the data set encompassing n data points consists of k subsets such that each subset i = 1, . . . , k can be enclosed in a ball of radius ri. Let the gap (distance between surfaces of enclosing balls) between each pair of subsets amount to at least g, that is described below.\ng \u2265 rmax \u221a k M + n\nm (2)\nand\ng \u2265 krmax \u221a np/2 + nq/2 + n/2 \u221a 2n\nnpnq (3)\nfor any p, q = 1, . . . , k; p 6= q, when ni, i = 1, . . . , k is the cardinality of the cluster i, M = maxi ni, m = mini ni,\nThen it is claimed in that paper that the optimum of k-means objective is reached when splitting the data into the aforementioned subsets.\nWhat are the implications? The most fundamental one is that the problem is decidable.\nTheorem 1. (i) If the data set is well-clusterable with a gap defined by formulas (2) and (3), then with high probability k-means++ (after an appropriately chosen number of repetitions) will discover the respective clustering. (ii) If k-means++ (after an appropriately chosen number of repetitions) does not discover a clustering matching formulas (2) and (3), then with high probability the data set is not well clusterable.\nThe rest of the current section is devoted to the proof of the claims of this new theorem, proposed in the current paper.\nIf we obtained the split, then for each cluster we are able to compute the cluster centre, the radius of the ball containing all the data points of the cluster, and finally we can check if the gaps between the clusters meet the requirement of formulas (2) and (3). So we are able to decide that we have found that the data set is well-clusterable.\nSo let us look at the claim (i). As we already know, the global minimum of k-means coincides with the separation by abovementioned gaps. Hence if there exists a positive probability, that k-means++ discovers the appropriate split, then by repeating independent runs of k-means++ and picking the split minimising k-means cost function we will increase the probability of finding the global minimum. We will show that we know the number of repetitions needed in advance, if we assume the maximum value of the quotient M/m.\nFirst consider the easiest case of all clusters being of equal sizes (M = m). Then the above equations can be reduced to (r = rmax)\ng \u2265 r \u221a k(k + 1) (4)\ng \u2265 rk \u221a 2k + k2 (5)\nA diagram of dependence of g/r on k is depicted in Figure 2 Now let us turn to k-means++ seeding. If already i distinct clusters were seeded, then the probability that a new cluster will be seeded (under our assumptions) amounts to at least\n(k \u2212 i)g2 (k \u2212 i)g2 + ir2 \u2265 (k \u2212 i)r 2k2(k + 1)2 (k \u2212 i)r2k2(k + 1)2 + ir2\n= (k \u2212 i)k2(k + 1)2 (k \u2212 i)k2(k + 1)2 + i \u2265 k 2(k + 1)2 k2(k + 1)2 + (k \u2212 1)\nDependence of the gap on k\nDependence of the accurate seeding on k\nHence the probability of accurate seeding amounts to( k2(k + 1)2\nk2(k + 1)2 + (k \u2212 1) )k\u22121 The diagram of dependence of this expression on k is depicted in Figure 3.\nLet us denote with Prsucc the required probability of success in finding the global minimum. To ensure that the seeding was successful in Prsucc (e.g. 95% ) of cases, we need to rerun k-means++ at least R times, with R given by (\n1\u2212 (\nk2(k + 1)2\nk2(k + 1)2 + (k \u2212 1)\n)k\u22121)R < 1\u2212 Prsucc\nR \u2265 log(1\u2212 Prsucc)\nlog ( 1\u2212 ( k2(k+1)2\nk2(k+1)2+(k\u22121) )k\u22121) But look at the following relationship:(\nk2(k + 1)2\nk2(k + 1)2 + (k \u2212 1)\n)k\u22121\n= ( 1\u2212 k \u2212 1\nk2(k + 1)2 + (k \u2212 1) )k\u22121 = ( 1\u2212 (k \u2212 1) 2\nk2(k + 1)2 + (k \u2212 1) 1 k \u2212 1 )k\u22121 \u2248 e \u2212 ( (k\u22121)2 k2(k+1)2+(k\u22121) )\nThe exponent of the last expression approaches rapidly zero, so that with increasing k within a single pass of k-means++ the optimum is reached. In fact, already for k=2 we have an error of below 3%, for k=8, below 1%, for k=30 below 0.1%. See the Figure 3 for illustration.\nLet us discuss clusters with same radius, but different cardinalities. Let m be the cluster minimum cardinality, and M respectively the maximum.\ng \u2265 r \u221a k M + n\nm (6)\ng \u2265 kr\n\u221a n(np + nq + n)\nnpnq (7)\nDependence of the gap on M/m for k=5\nfor any p, q = 1, . . . , k; p 6= q, when ni, i = 1, . . . , k is the cardinality of the cluster i, M = maxi ni, m = mini ni, Worst case g/r values are illustrated in Figure 4.\nNow let us turn to k-means++ seeding. If already i distinct clusters were seeded, then the probability that a new cluster will be seeded (under our assumptions) amounts to at least\n(k \u2212 i)mg2 (k \u2212 i)mg2 + iMr2 \u2265 (k \u2212 i)mk 2n(1/m+ 1/m+ n/m2) (k \u2212 i)mk2n(1/m+ 1/m+ n/m2) + iM\n= (k \u2212 i)k2n(2 + n/m) (k \u2212 i)k2n(2 + n/m) + iM \u2265 k 2n(2 + n/m)\nk2n(2 + n/m) + (k \u2212 1)M So again the probability of successful seeding will amount to at least:(\nk2n(2 + n/m)\nk2n(2 + n/m) + (k \u2212 1)M\n)k\u22121\n= ( 1\u2212 (k \u2212 1)M\nk2n(2 + n/m) + (k \u2212 1)M\n)k\u22121\n= ( 1\u2212 (k \u2212 1) 2M\nk2n(2 + n/m) + (k \u2212 1)M 1 k \u2212 1 )k\u22121 \u2248 exp ( \u2212 (k \u2212 1) 2M\nk2n(2 + n/m) + (k \u2212 1)M ) Even if M is 20 times as big as m, still the convergence to 1 is so rapid that already for k = 2 the clustering success is achieved with 95% success probability in a single repetition. An illustration is visible in Figure 5\nSo far we have concentrated on showing that if the data is well-clusterable, then within practically a single clustering run the seeding will have the property that each cluster obtains a single seed. But what about the rest of the run of k-means? As in all these cases g \u2265 2r, then, as shown in [12], the cluster centres will never switch to balls encompassing other clusters, so that eventually the true cluster structure is detected and minimum of Q is reached. This would complete the proof of claim (i). The demonstration of claim (ii) is straight forward. If the data were well-clusterable then k-means++ would have failed to identify it with probability of at most 1\u2212 Prsucc. As the wellclusterable data are in practice extremely rare, the failure of the algorithm to identify a well-clusterable structure induces with probability of at least Prsucc that no such structure exists in the data.\n5 Smaller gaps between clusters\n. In the previous section we considered well-clusterability under the assumption of large areas between clusters where no data points of any cluster will occur. Subsequently we show that this assumption may be relaxed so that spurious points are allowed between the major concentrations of cluster points. But to ensure that the presence of such points will not lead the k-means procedure astray, we will distinguish core parts of the clusters and will ensure by the subsequent theorem 3 that once a cluster core is hit by k-means initialisation procedure, the cluster is preserved over subsequent k-means iterations.\nIn [12] we have proven that\nTheorem 2. Let A,B be cluster centres. Let \u03c1AB be the radius of a ball centred at A and enclosing its cluster and it also is the radius of a ball centred at B and enclosing its cluster. If the distance between the cluster centres A,B amounts to 2\u03c1AB + g, g > 0 (g being the \u201dgap\u201d between clusters), if we pick any two points, X from the cluster of A and Y from the cluster of B, and recluster both clusters around X and Y , then the new clusters will preserve the balls centred at A and B of radius g/2 (called subsequently \u201dcores\u201d) each (X the core of A, Y the core of B).\nHere we shall demonstrate the validity of a complementary theorem.\nTheorem 3. Let A,B be cluster centres. Let \u03c1AB be the radius of a ball centred at A and enclosing its cluster and it also is the radius of a ball centred at B and enclosing its cluster. Let \u03c1cAB be the radius of a ball centred at A and enclosing \u201dvast majority\u201d of its cluster and it also is the radius of a ball centred at B and enclosing \u201dvast majority\u201d of its cluster. If the distance between the cluster centres A,B amounts to 2\u03c1AB + g, g > 0 (g = 2rcAB being the \u201dgap\u201d between clusters), if we pick any two points, X from the ball B(A, rcAB) and Y from the ball B(A, rcAB), and recluster both clusters around X and Y , then the new clusters will be identical to the original clusters around A and B.\nDefinition 2. If the gap between each pair of clusters fulfils the condition of either of the above two theorems, then we say that we have core-clustering.\nProof. For the illustration of the proof see Figure 6. The proof does not differ too much from the previous one and in fact the previous theorem is\ngap\nConsider the two points A,B being the two centres of double balls. The inner call represents the core of radius rcAB = g/2, the outer ball of radius \u03c1 (\u03c1 = \u03c1AB), enclosing the whole cluster. Consider two points, X, Y , one being in each core ball (presumably the cluster centres at some stage of the k-means algorithm). To represent their distances faithfully, we need at most a 3D space.\nLet us consider the plane established by the line AB and parallel to the line XY . Let X \u2032 and Y \u2032 be orthogonal projections of X, Y onto this plane. Now let us establish that the hyperplane \u03c0 orthogonal to X, Y , and passing through the middle of the line segment XY , that is the hyperplane containing the boundary between clusters centreed at X and Y does not cut any of the balls centreed at A and B. This hyperplane will be orthogonal to the plane of the Figure 6 and so it will manifest itself as an intersecting line l that should not cross outer circles around A and B, being projections of the respective balls. Let us draw two solid lines k,m between circles O(A, \u03c1AB) and O(B, \u03c1AB) tangential to each of them. Line l should lie between these lines, in which case the cluster centre will not jump to the other ball.\nLet the line X \u2032Y \u2032 intersect with the circles O(A, rcAB) and O(B, rcAB) at points C,D,E, F as in the figure.\nIt is obvious that the line l would get closer to circle A, if the points X \u2032, Y \u2032 would lie closer to C and E, or closer to circle B if they would be closer to D and F .\nTherefore, to show that it does not cut the circle O(A, \u03c1) it is sufficient to consider X \u2032 = C and Y \u2032 = E. (The case with ball Ball(B, \u03c1) is symmetrical).\nLet O be the centre of the line segment AB. Let us draw through this point a line parallel to CE that cuts the circles at points C \u2032, D\u2032, E \u2032 and F \u2032. Now notice that centric symmetry through point O transforms the circles O(A, rcAB),O(B, rcAB) into one another, and point C \u2032inF \u2032 and D\u2032inE \u2032. Let E\u2217 and F \u2217 be images of points E and F under this symmetry.\nIn order for the line l to lie between m and k, the middle point of the line segment CE shall lie between these lines.\nLet us introduce a planar coordinate system centreed at O with X axis parallel to lines m, k, such that A has both coordinates non-negative, and B non-positive. Let us denote with \u03b1 the angle between the lines AB and k. As we assume that the distance between A and B equals 2\u03c1+2rcAB, then the distance between lines k and m amounts to 2((\u03c1 + rcAB) sin(\u03b1)\u2212 \u03c1). Hence the Y coordinate of line k equals ((\u03c1+ rcAB) sin(\u03b1)\u2212 \u03c1).\nSo the Y coordinate of the centre of line segment CE shall be not higher than this. Let us express this in vector calculus:\n4(yOC + yOE)/2 \u2264 ((\u03c1+ rcAB) sin(\u03b1)\u2212 \u03c1)\nNote, however that\nyOC +yOE = yOA+yAC +yOB+yBE = yAC +yBE = yAC\u2212yAE\u2217 = yAC +yE\u2217A\nSo let us examine the circle with centre at A. Note that the lines CD and E\u2217F \u2217 are at the same distance from the line C \u2032D\u2032. Note also that the absolute values of direction coefficients of tangentials of circle A at C \u2032 and D\u2032 are identical. The more distant these lines are, as line CD gets closer to A, the yAC gets bigger, and yE\u2217A becomes smaller. But from the properties of the circle we see that yAC increases at a decreasing rate, while yE\u2217A decreases at an increasing rate. So the sum yAC + yE\u2217A has the biggest value when C is identical with C \u2032 and we need hence to prove only that\n(yAC\u2032 + yD\u2032A)/2 = yAC\u2032 \u2264 ((\u03c1+ rcAB) sin(\u03b1)\u2212 \u03c1)\nLet M denote the middle point of the line segment C \u2032D\u2032. As point A has the coordinates ((\u03c1 + rcAB) cos(\u03b1), (\u03c1 + rcAB) sin(\u03b1)), the point M is at distance of (\u03c1+rcAB) cos(\u03b1) from A. But C \u2032M2 = r2cAB\u2212((\u03c1+rcAB) cos(\u03b1))2.\nSo we need to show that\nr2cAB \u2212 ((\u03c1+ rcAB) cos(\u03b1))2 \u2264 ((\u03c1+ rcAB) sin(\u03b1)\u2212 \u03c1)2\nIn fact we get from the above\nr2cAB\u2212 ((\u03c1+ rcAB) cos(\u03b1))2 \u2264 ((\u03c1+ rcAB) sin(\u03b1))2+ \u03c12\u2212 2(\u03c1+ rcAB)(\u03c1) sin(\u03b1)\nr2cAB \u2264 (\u03c1+ rcAB)2 + \u03c12 \u2212 2(\u03c1+ rcAB)(\u03c1) sin(\u03b1)\nr2cAB \u2212 \u03c12 \u2264 (\u03c1+ rcAB)2 \u2212 2(\u03c1+ rcAB)(\u03c1) sin(\u03b1)\n(rcAB \u2212 \u03c1)(rcAB + \u03c1) \u2264 (\u03c1+ rcAB)2 \u2212 2(\u03c1+ rcAB)(\u03c1) sin(\u03b1)\n(rcAB \u2212 \u03c1) \u2264 (\u03c1+ rcAB)\u2212 2\u03c1 sin(\u03b1)\n0 \u2264 2\u03c1\u2212 2\u03c1 sin(\u03b1)\n0 \u2264 1\u2212 sin(\u03b1)\nwhich is obviously true, as sin never exceeds 1.\n6 Core based global k-means minimum\nIn the paper [12] we have investigated conditions under which one can ensure that the minimum of k-means cost function is related to a clustering with (wide) gaps between clusters.\nBased on the result of the preceding Section 5, we want to weaken these conditions requiring only that the big gaps exist between cluster cores and the clusters themselves are separated by much smaller gaps, equal to the size of the core.\nIn particular, let us consider the set of k clusters C = {C1, . . . , Ck} of cardinalities n1, . . . , nk and with radii of balls enclosing the clusters (with centres located at cluster centres) r1, . . . , rk. Let each of these clusters Ci have a core Ci of radius ri and cardinality ni around the cluster centre such that for p \u2208 [0, 1)\nQ({Ci})/Q({Ci}) \u2265 1\u2212 p\nWe are interested in a gap g between cluster cores C1, . . . , Ck such that it does not make sense to split each cluster Ci into subclusters Ci1, . . . , Cik and to combine them into a set of new clusters S = {S1, . . . , Sk} such that Sj = \u222aki=1Cij.\nWe seek a g such that the highest possible central sum of squares combined over the clusters Ci would be lower than the lowest conceivable combined sums of squares around respective centres of clusters Sj. Let V ar(C) be the variance of the cluster C (average squared distance to cluster gravity centre; if referring to the core of the cluster, we still compute against the cluster centre, not the core centre, so also with the Q function). Let Cij = Cij \u2229Ci be the core part of the subcluster Cij. Let rij be the distance of the centre of core subcluster Cij to the centre of cluster Ci. Let vilj be the distance of the centre of core subcluster Cij to the centre of core subcluster Clj. So the total k-means function for the set of clusters (C1, . . . , Ck) will amount to:\nQ(C) = 1 1\u2212 p Q(C) = 1 1\u2212 p k\u2211 i=1 k\u2211 j=1 (nijV ar(Cij) + nijr 2 ij) (8)\nAnd the total k-means function for the set of clusters (S1, . . . , Sk) will amount to: Q(S \u2265 k\u2211\nj=1\n(( k\u2211\ni=1\nnijV ar(Cij)\n) + (\nk\u2211 i=1 nij) ( k\u22121\u2211 i=1 k\u2211 l=i+1 nij\u2211k i=1 nij nlj\u2211k i=1 nij v2ilj )) (9)\nShould (C1, . . . , Ck) constitute the absolute minimum of the k-means target function, then Q(S) \u2265 Q(C) should hold, which is fullfilled if :\nk\u2211 j=1\n(( k\u2211\ni=1\nnijV ar(Cij)\n) + (\nk\u2211 i=1 nij) ( k\u22121\u2211 i=1 k\u2211 l=i+1 nij\u2211k i=1 nij nlj\u2211k i=1 nij v2ilj ))\n\u2265 1 1\u2212 p k\u2211 i=1 k\u2211 j=1 (nijV ar(Cij) + nijr 2 ij)\nThis implies:\nk\u2211 j=1 ( k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij v2ilj ) \u2265 1 1\u2212 p k\u2211 i=1 k\u2211 j=1 (pnijV ar(Cij) + nijr 2 ij) (10)\nNote that V ar(Cij) \u2264 r2ij, so\n1\n1\u2212 p k\u2211 i=1 k\u2211 j=1 (pnijV ar(Cij) + nijr 2 ij) \u2264 1 1\u2212 p k\u2211 i=1 k\u2211 j=1 (1 + p)nijnijr 2 ij\n= 1 + p\n1\u2212 p k\u2211 i=1 k\u2211 j=1 nijnijr 2 ij (11)\nTo maximize \u2211k\nj=1 nijr 2 ij for a single cluster Ci of enclosing ball radius\nri, note that you should set rij to ri. Let mj = argmaxj\u2208{1,...,k} nij. If we set rij = ri for all j except mj, then the maximal rimj is delimited by the relation \u2211k j=1;j 6=mj nijrij \u2265 nimjrimj . So\nk\u2211 j=1 nijr 2 ij \u2264( k\u2211 j=1;j 6=mj nij)r 2 i min(2, (1 +\n\u2211k j=1;j 6=mj nij\nnimj )) (12)\n\u22642( k\u2211\nj=1;j 6=mj\nnij)r 2 i\nSo if we can guarantee that the gap between cluster balls (of clusters from C) amounts to g then surely\nk\u2211 j=1 ( k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij v2ilj ) \u2265 g2 k\u2211 j=1 ( k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij ) (13)\nbecause in such case g \u2264 vilj for all i, l, j.\nBy combining inequalities (10), (12) and (13) we see that the global minimum is granted if the following holds:\ng2 k\u2211\nj=1 ( k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij ) \u2265 21 + p 1\u2212 p k\u2211 i=1 ( k\u2211 j=1;j 6=mj nij)r 2 i (14)\nOne can distinguish two cases: either (1) there exists a cluster St containing two subclusters Cpt, Cqt such that t = argmaxj |Cpj| and t = argmaxj |Cqj| (maximum cardinality subclasses of their respective original clusters Cp, Cq or (2) not.\nConsider the first case. Let Cp, Cq be the two clusters where Cpt and Cqt be two subclusters of highest cardinality within Cp, Cq resp. This implies that npt \u2265 1knp, nqt \u2265 1 k nq. Also this implies that for i 6= p, i 6= q nit \u2264 ni/2.\nk\u2211 j=1 k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij\n\u2265 k\u22121\u2211 i=1 k\u2211 l=i+1 nitnlt\u2211k i=1 nit\n\u2265 nptnqt\u2211k i=1 nit\n\u2265 nptnqt np/2 + nq/2 + \u2211k i=1 ni/2 = nptnqt np/2 + nq/2 + n/2\n\u2265 1 k2 npnq np/2 + nq/2 + n/2\nNote that\n2 k\u2211\ni=1\n( k\u2211\nj=1;j 6=mj\nnij)r 2 i \u2264 2 k\u2211 i=1 nir 2 i\nSo, in order to fulfil inequality (14), it is sufficient to require that\ng \u2265 \u221a\u221a\u221a\u221a 21+p1\u2212p\u2211ki=1 nir2i 1 k2 npnq np/2+nq/2+n/2\n=k \u221a np/2 + nq/2 + n/2\n\u221a 21+p 1\u2212p \u2211k i=1 nir 2 i\nnpnq\n=k \u221a np + nq + n\n\u221a 1+p 1\u2212p \u2211k i=1 nir 2 i\nnpnq (15)\nThis of course maximized over all combinations of p, q. Let us proceed to the second case. Here each cluster Sj contains a subcluster of maximum cardinality of a different cluster Ci. As the relation between Sj and Ci is unique, we can reindex Sj in such a way that actually Cj contains its maximum cardinality subcluster Cjj. Let us rewrite the inequality (14).\ng2 k\u2211\nj=1 ( k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij ) \u2212 21 + p 1\u2212 p k\u2211 i=1 ( k\u2211 j=1;j 6=mj nij)r 2 i \u2265 0\nThis is met if\ng2 k\u2211\nj=1 ( j\u22121\u2211 i=1 nijnjj\u2211k i=1 nij + k\u2211 l=j+1 njjnlj\u2211k i=1 nij ) \u2212 21 + p 1\u2212 p k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nThis is the same as:\ng2 k\u2211\nj=1 ( \u2211 i=1,...,j\u22121,j+1,...,k nijnjj\u2211k i=1 nij ) \u2212 21 + p 1\u2212 p k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nThis is fulfilled if:\ng2 k\u2211\nj=1 ( \u2211 i=1,...,j\u22121,j+1,...,k\nnijnj/k nj/2 + \u2211k i=1 ni/2\n) \u2212 21 + p\n1\u2212 p k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nLet M be the maximum over n1, . . . , nk. The above holds if\ng2 k\u2211\nj=1 ( \u2211 i=1,...,j\u22121,j+1,...,k nijnj/k M/2 + n/2 ) \u2212 21 + p 1\u2212 p k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nLet m be the minimum over n1, . . . , nk. The above holds if\ng2 k\u2211\nj=1 ( \u2211 i=1,...,j\u22121,j+1,...,k nijm/k M/2 + n/2 ) \u2212 21 + p 1\u2212 p k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nThis is the same as\ng2 m/k\nM/2 + n/2\n( k\u2211\nj=1 \u2211 i=1,...,j\u22121,j+1,...,k nij\n) \u2212 21 + p\n1\u2212 p k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\ng2 m/k\nM/2 + n/2\n( k\u2211\nj=1\n(( k\u2211\ni=1\nnij ) \u2212 njj ) \u2212 21 + p\n1\u2212 p k\u2211 i=1 (ni \u2212 nii)r2i\n) \u2265 0\ng2 m/k\nM/2 + n/2\n(( k\u2211\nj=1 k\u2211 i=1 nij\n) \u2212 (\nk\u2211 j=1 njj)\n) \u2212 21 + p\n1\u2212 p\n( k\u2211\ni=1\n(ni \u2212 nii)r2i\n) \u2265 0\ng2 m/k\nM/2 + n/2\n(( k\u2211\ni=1\nni\n) \u2212 (\nk\u2211 j=1 njj)\n) \u2212 21 + p\n1\u2212 p k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\ng2 m/k\nM/2 + n/2\n( k\u2211\ni=1\n(ni \u2212 nii) ) \u2212 21 + p\n1\u2212 p k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nk\u2211 i=1 (ni \u2212 nii) ( g2\nm/k M/2 + n/2 \u2212 21 + p 1\u2212 p r2i\n) \u2265 0\nThe above will hold, if for every i = 1, . . . , k g \u2265 ri \u221a 1 + p\n1\u2212 p 2\nm/k M/2+n/2\ng \u2265 ri \u221a k 1 + p\n1\u2212 p M + n m (16)\nSo the inequality (14) is fulfilled, if both inequality (15) and inequality (16) are held by an appropriately chosen g.\nIn summary we have shown that\nTheorem 4. Let C = {C1, . . . , Ck} be a partition of a data set into k clusters of cardinalities n1, . . . , nk and with radii of balls enclosing the clusters (with centres located at cluster centres) r1, . . . , rk. Let each of these clusters Ci have a core Ci of radius ri and cardinality ni around the cluster centre such that for p \u2208 [0, 1)\nQ({Ci})/Q({Ci}) \u2265 1\u2212 p\nThen if the gap g between cluster cores C1, . . . , Ck fulfils conditions expressed in formulas (15) and (16) then the partition C coincides with the global minimum of the k-means const function for the data set."}, {"heading": "7 Core based approach to clusterability", "text": "After the preceding preparatory work, we want to prove a theorem analogous to Theorem 1, but now allowing for smaller gaps between clusters.\nTheorem 5. (i) If the data set is well-clusterable with a gap defined by formulas (16) and (15), with ri replaced by their maxima, then with high probability k-means++ (after an appropriately chosen number of repetitions) will discover the respective clustering. (ii) If k-means++ (after an appropriately chosen number of repetitions) does not discover a clustering matching formulas (16) and (15) (with ri replaced by their maxima), then with high probability the data set is not well clusterable.\nThe rest of the current section is devoted to the proof of the claims of this theorem.\nIf we obtained the split, then for each cluster we are able to compute the cluster centre, the radius of the ball containing all the data points of the cluster but the p most distant ones, and finally we can check if the gaps between the cluster cores meet the requirement of formulas (16) and (15). So we are able to decide that we have found that the data set is well-clusterable.\nSo let us look at the claim (i). As we already know from preceding Section 6, the global minimum of k-means coincides with the separation by abovementioned gaps. Hence if there exists a positive probability, that k-means++ discovers the appropriate split, then by repeating independent runs of k-means++ and picking the split minimising k-means cost function we will increase the probability of finding the global minimum. We will show that we know the number of repetitions needed in advance, if we assume the maximum value of the quotient M/m.\nWe assume it is granted that\ng \u2265 r \u221a k 1 + p\n1\u2212 p M + n m (17)\nfor any i = 1, . . . , k\ng \u2265 kr\n\u221a 1 + p\n1\u2212 p n(np + nq + n) npnq (18)\nfor any p, q = 1, . . . , k; p 6= q, when ni, i = 1, . . . , k is the cardinality of the cluster i, M = maxi ni, m = mini ni, For an illustration of this dependence see Figure 7.\nDependence of the accurate seeding on share p of variance outside the core\nSo let us turn to k-means++ seeding. If already i distinct cluster cores were seeded, then the probability that a new cluster core will be seeded (under our assumptions) amounts to at least\n(k \u2212 i)m(1\u2212 p)g2\n(k \u2212 i)mg2 + iM 1 1\u2212pr\n2 \u2265\n(k \u2212 i)mk2(1\u2212 p)1+p 1\u2212pn(1/m+ 1/m+ n/m 2)\n(k \u2212 i)mk2 1+p 1\u2212pn(1/m+ 1/m+ n/m 2) + iM 1 1\u2212p\n= (k \u2212 i)k2(1\u2212 p)(1 + p)n(2 + n/m) (k \u2212 i)k2(1 + p)n(2 + n/m) + iM\n\u2265 k 2(1\u2212 p)(1 + p)n(2 + n/m)\nk2(1 + p)n(2 + n/m) + (k \u2212 1)M So again the probability of successful seeding will amount to at least:(\nk2(1\u2212 p)(1 + p)n(2 + n/m) k2(1 + p)n(2 + n/m) + (k \u2212 1)M\n)k\u22121\n= (1\u2212 p)k\u22121 ( 1\u2212 (k \u2212 1)M\nk2(1 + p)n(2 + n/m) + (k \u2212 1)M )k\u22121 = (1\u2212 p)k\u22121 ( 1\u2212 (k \u2212 1) 2M\nk2(1 + p)n(2 + n/m) + (k \u2212 1)M 1 k \u2212 1 )k\u22121 \u2248 (1\u2212 p)k\u22121exp ( \u2212 (k \u2212 1) 2M\nk2(1 + p)n(2 + n/m) + (k \u2212 1)M ) For an illustration of this dependence see Figure 8\nApparently in the limit the above expression lies at about (1\u2212 p)k\u22121. So to achieve the identification of the clustering with probability of at\nleast Prsucc (e.g. 95%), we will need R runs of k-means++ where\nR = log(1\u2212 Prsucc)\nlog(1\u2212 (1\u2212 p)k\u22121)\nNote that 1\u2212 (1\u2212 p)k\u22121 \u2248 1\u2212 e\u2212p(k\u22121) \u2248 1\u2212 e\u2212pk\nThe effect of doubling k is\n1\u2212 e\u2212p2k\n1\u2212 e\u2212pk = (1\u2212 e\u2212pk)(1 + e\u2212p2k) 1\u2212 e\u2212pk = 1 + e\u2212p2k\nthat is it is sublinear in the expression 1\u2212 (1\u2212 p)k\u22121, hence R grows slower than reciprocally logarithmically in k and p. For an illustration of this relation see Figure 9\nRepetitions needed to get accurate seeding on share p of variance outside the core\nSo far we have concentrated on showing that if the data is well-clusterable, then within practically reasonable number of k-means++ runs the seeding will have the property that each cluster obtains a single seed. But what about the rest of the run of k-means? As shown in Section 5, the cluster centres will never switch to balls encompassing other clusters, so that eventually the true cluster structure is detected and minimum of Q is reached. This would complete the proof of claim (i). The demonstration of claim (ii) is straight forward. If the data were well-clusterable then k-means++ would have failed to identify it with probability of at most 1\u2212 Prsucc. As the well-clusterable data are in practice extremely rare, the failure of the algorithm to identify a well-clusterable structure induces with probability of at least Prsucc that no such structure exists in the data."}, {"heading": "8 Conclusions", "text": "We have defined the notion of a well-clusterable data set from the point of view of the objective of k-means clustering algorithm and common sense in two variants - without any data points in the large gaps between clusters. The novelty introduced here, compared to other work on this topic, is that one can a posteriori (after running k-means) check if the data set is wellclusterable or not.\nLet make a comparison to the results of other investigators in the realm of well-clusterability, in particular presented in [2, 13, 6, 5, 7, 1, 10].\nIf the data is well-clusterable according to criteria of Perturbation Robustness, or -Separatedness, or (c, )-Approximation- Stability or \u03b1-Centre Stability or (1+\u03b1) Weak Deletion Stability or Perfect Separation, one can reconstruct the well-clustered structure using appropriate algorithm. But only in case of Perfect Separation or Nice Separation, you can decide that you have found the structure, if you have found it. Note that you have no warranty that you will find Nice Separation if it is there. But for none of these ways of understanding well-clusterability we are able to decide (neither a priori nor a posteriori) that the data is not well-clusterable if the well-clustered structure was not found (unless by brute force).\nThe only exception constitutes formally the method of Multi-modality Detection, which tells us a priori that the data is or is not well-clusterable. However, as we have demonstrated, data can be easily found that can foolish this method, so that it discovers well-clusterability in case when there is none.\nUnder the definitions of well-clusterability presented in this paper, we get a completely new situation. It is guaranteed that if the well-clusterable structure is there, it will be detected with high probability. A posteriori one\ncan check that the structure found is the well-clusterable structure if it is so, with 100% certainty. Furthermore if the (k-means++) algorithm did not find a well-clusterable structure then with high probability it is not there in the data.\nThe paper contains a couple of other, minor contributions. The concept of cluster cores has been introduced such that if a seed of k-means once hits each core then there is guarantee that none of the cluster centres will ever leave the cluster. It has been shown that the number of reruns of k-means++ is small when a desired probability of success in finding the well-clusterability is being targeted. Numerical examples show that several orders of magnitude smaller gaps between clusters, compared to [13], are required in order to state that the data is well clusterable, and still the probability of detecting the well clusterable structure is much higher (even close to one in a single k-means++ run).\nThe procedure elaborated for constructing a well clusterable data set, ensuring that the k-means cost function absolute minimum is reached for a predefined data partition may find applications in some testing procedures of clustering algorithms.\nOf course a number of research questions with respect to the topic of this paper remain open. First of all the issue of constructing tight (or at least tighter) bounds for estimation of required gaps between clusters. Second an investigation how the violations of these minimum values influence the capability of k-means algorithms to detect either the absolute minimum of their cost function or achieving a partition that is intuitively considered by humans as \u201dgood clustering\u201d."}], "references": [{"title": "An effective and efficient approach for clusterability evaluation", "author": ["Margareta Ackerman", "Andreas Adolfsson", "Naomi Brownstein"], "venue": "CoRR, abs/1602.06687,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Clusterability: A theoretical study", "author": ["Margareta Ackerman", "Shai Ben-David"], "venue": "Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proc. of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Stability yields a ptas for k-median and k-means clustering", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Inf. Process. Lett.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Computational feasibility of clustering under clusterability", "author": ["Shai Ben-David"], "venue": "assumptions. https://arxiv.org/abs/1501.00437,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Are stable instances easy", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "Comb. Probab. Comput.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Clusterability detection and initial seed selection in large datasets. technical report 99-6, rensselaer polytechnic institute, computer science dept., rensselaer polytechnic institute", "author": ["S. Epter", "M. Krishnamoorthy", "M. Zaki"], "venue": "troy, ny 12180,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "The dip test of unimodality", "author": ["J.A. Hartigan", "P. M . Hartigan"], "venue": "Ann. Statist.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1985}, {"title": "On the discrepancy between kleinberg\u2019s clustering axioms and k-means clustering algorithm behavior", "author": ["R. K\u0142opotek", "M. K\u0142opotek"], "venue": "https: //arxiv.org/abs/1702.04577,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "J. ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Using kernel density estimates to investigate multimodality", "author": ["B.W. Silverman"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1981}], "referenceMentions": [{"referenceID": 7, "context": "Though a number of attempts have been made to capture formally the intuition behind clusterability, none of these efforts seems to have been successful, as Ben-David exhibits in [8] in depth.", "startOffset": 178, "endOffset": 181}, {"referenceID": 0, "context": "Though a recent paper by Ackerman [1] partially eliminates these problems, but regrettably at the expense of introducing 1 ar X iv :1 70 4.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "structure of the data [2].", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "Ackerman and Ben-David in [2] studied several of these notions.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "Ben-David [8] performed an interesting investigation of the concepts of clusterability from the point of view of the capability of \u201dnot too complex\u201d algorithms to discover the cluster structure, (negatively) verifying the working hypothesis that \u201cClustering is difficult only when it does not matter\u201d (the CDNM thesis).", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "Two brands may be distinguished: additive [2] and multiplicative ones [9] (the limit of perturbation is upper-bounded either by an absolute value or by a coefficient).", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "Two brands may be distinguished: additive [2] and multiplicative ones [9] (the limit of perturbation is upper-bounded either by an absolute value or by a coefficient).", "startOffset": 70, "endOffset": 73}, {"referenceID": 12, "context": "\u2022 -Separatedness meaning that the cost of optimal clustering into k clusters is less than 2 times the cost of optimal clustering into k\u22121 clusters [13] \u2022 (c, )-Approximation- Stability [6] meaning that if the cost function values of two partitions differ by the factor c, then the distance (in some space) between the partitions is at most .", "startOffset": 147, "endOffset": 151}, {"referenceID": 5, "context": "\u2022 -Separatedness meaning that the cost of optimal clustering into k clusters is less than 2 times the cost of optimal clustering into k\u22121 clusters [13] \u2022 (c, )-Approximation- Stability [6] meaning that if the cost function values of two partitions differ by the factor c, then the distance (in some space) between the partitions is at most .", "startOffset": 185, "endOffset": 188}, {"referenceID": 4, "context": "\u2022 \u03b1-Centre Stability [5] meaning, for any centric clustering, that the distance of an element to its cluster centre is \u03b1 times smaller than the distance to any other cluster centre under optimal clustering.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "\u2022 (1+\u03b1) Weak Deletion Stability [4] meaning that given an optimal cost function value OPT for k centric clusters, then the cost function of a clustering obtained by deleting one of the cluster centres and assigning elements of that cluster to one of the remaining clusters should be bigger that (1 + \u03b1) \u00b7OPT .", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "For example, given the (c, )-Approximation- Stability [6], polynomial time clustering requires that, in the optimal clustering (beside its uniqueness), all but an -fraction of the elements, are 20 times closer to their own cluster centre than to every other cluster centre.", "startOffset": 54, "endOffset": 57}, {"referenceID": 12, "context": "-Separatedness requires that the distance to its own cluster centre must be at least 200 times closer than to every other cluster element [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 3, "context": "(1 + \u03b1) Weak Deletion Stability [4] demands distances to other clusters being log(k) times the \u201daverage radius\u201d of the own cluster.", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "The perturbational stability [2] induces exponential dependence on the sample size.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "[10] considers the data as clusterable when the minimum between - cluster separation exceeds the maximum in - cluster distance (called elsewhere \u201dperfect separation\u201d).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] proposes to consider data as clusterable if each element is closer to all elements in its cluster than to all other data (called also \u201dnice separation\u201d).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] derived a method for testing clusterability of data based on the large gap assumption.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Hence, to detect clusterability, they apply tests of multimodality, namely the Dip [11] and Silverman [14] tests.", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "Hence, to detect clusterability, they apply tests of multimodality, namely the Dip [11] and Silverman [14] tests.", "startOffset": 102, "endOffset": 106}, {"referenceID": 7, "context": "Ben-David [8] raises a further important point that it is usually (in practically all above mentioned methods except [1], which has a flaw by itself) impossible to verify apriori if the data fulfils the clusterability criterion because the conditions refer either to all possible clusterings or to optimal clustering so that we do not have the possibility to verify whether or not the data set is clusterable, before one starts clustering (but usually computing the optimum is NP-hard).", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "Ben-David [8] raises a further important point that it is usually (in practically all above mentioned methods except [1], which has a flaw by itself) impossible to verify apriori if the data fulfils the clusterability criterion because the conditions refer either to all possible clusterings or to optimal clustering so that we do not have the possibility to verify whether or not the data set is clusterable, before one starts clustering (but usually computing the optimum is NP-hard).", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "Though the criteria of [10] and [7] can clearly be applied ex post to see that in the resulting clustering the clusterability criteria hold, but these approaches lack the solving of the inverse issue: what if the clusterability criteria are not matched by the result clustering - is the data unclustrable? Could no other algorithm discover the clusterable structure? One shall note at this point that the approach in [1] is different with this respect.", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "Though the criteria of [10] and [7] can clearly be applied ex post to see that in the resulting clustering the clusterability criteria hold, but these approaches lack the solving of the inverse issue: what if the clusterability criteria are not matched by the result clustering - is the data unclustrable? Could no other algorithm discover the clusterable structure? One shall note at this point that the approach in [1] is different with this respect.", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "Though the criteria of [10] and [7] can clearly be applied ex post to see that in the resulting clustering the clusterability criteria hold, but these approaches lack the solving of the inverse issue: what if the clusterability criteria are not matched by the result clustering - is the data unclustrable? Could no other algorithm discover the clusterable structure? One shall note at this point that the approach in [1] is different with this respect.", "startOffset": 417, "endOffset": 420}, {"referenceID": 2, "context": "For details check [3].", "startOffset": 18, "endOffset": 21}, {"referenceID": 12, "context": "Note that the algorithm proposed by [13] differs from the k-means++ only by the non-uniform choice of the first cluster centre (the first pair of cluster centres should be distant, and the choice of this pair is proportional in probability to the squared distances between data elements).", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "Figure 1: Illustration of a special case where Ackerman\u2019s method [1] would falsely recognize clustering structure in the data (a) the data (b) the histogram of pai r-wise distances - two modes visible 3 Non-suitability of gap-based clusterability criteria for k-means Let us discuss more closely the relationship between the gap-based wellclusterability concepts developed in the literature and the actual optimality criterion of k-means.", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "Specifically let us consider the approaches to clusterability of [1], [10], [5] and [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "Specifically let us consider the approaches to clusterability of [1], [10], [5] and [7].", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "Specifically let us consider the approaches to clusterability of [1], [10], [5] and [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "Specifically let us consider the approaches to clusterability of [1], [10], [5] and [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 0, "context": "Let us first point to the fact that [1] may indicate a clear bimodal structure in the data where there are no gaps in the data.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "We are unaware of anybody pointing at this weakness of well-clusterability in [1]: Imagine a thin ring uniformly covered with data points.", "startOffset": 78, "endOffset": 81}, {"referenceID": 9, "context": "On the other hand, even if there are gaps between groups of data, for example those required by [10], [5] or [7], k-means optimum may not lie in the partition exhibiting gap based well-clusterability property in spite of its existence, And not only for these gaps, but also for any arbitrary many times larger ones.", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": "On the other hand, even if there are gaps between groups of data, for example those required by [10], [5] or [7], k-means optimum may not lie in the partition exhibiting gap based well-clusterability property in spite of its existence, And not only for these gaps, but also for any arbitrary many times larger ones.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "On the other hand, even if there are gaps between groups of data, for example those required by [10], [5] or [7], k-means optimum may not lie in the partition exhibiting gap based well-clusterability property in spite of its existence, And not only for these gaps, but also for any arbitrary many times larger ones.", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "As [10] is concerned, it may be considered as a special case of [7].", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "As [10] is concerned, it may be considered as a special case of [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "[5] may be viewed in turn as a strengthening of the concept of [10].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[5] may be viewed in turn as a strengthening of the concept of [10].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "We have demonstrated in [12] that under these circumstances the clustering of data into CA, CB reflects a local minimum of k-means cost function.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "As another example consider methods of estimating closeness to optimal clustering solution under some general data distributions (like for the k-means++[3]), but the guarantees are quite loose.", "startOffset": 152, "endOffset": 155}, {"referenceID": 11, "context": "In the paper [12] we have investigated conditions under which one can ensure that the minimum of k-means cost function is related to a clustering with (wide) gaps between clusters.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "But what about the rest of the run of k-means? As in all these cases g \u2265 2r, then, as shown in [12], the cluster centres will never switch to balls encompassing other clusters, so that eventually the true cluster structure is detected and minimum of Q is reached.", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "In [12] we have proven that Theorem 2.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "6 Core based global k-means minimum In the paper [12] we have investigated conditions under which one can ensure that the minimum of k-means cost function is related to a clustering with (wide) gaps between clusters.", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "Let make a comparison to the results of other investigators in the realm of well-clusterability, in particular presented in [2, 13, 6, 5, 7, 1, 10].", "startOffset": 124, "endOffset": 147}, {"referenceID": 12, "context": "Let make a comparison to the results of other investigators in the realm of well-clusterability, in particular presented in [2, 13, 6, 5, 7, 1, 10].", "startOffset": 124, "endOffset": 147}, {"referenceID": 5, "context": "Let make a comparison to the results of other investigators in the realm of well-clusterability, in particular presented in [2, 13, 6, 5, 7, 1, 10].", "startOffset": 124, "endOffset": 147}, {"referenceID": 4, "context": "Let make a comparison to the results of other investigators in the realm of well-clusterability, in particular presented in [2, 13, 6, 5, 7, 1, 10].", "startOffset": 124, "endOffset": 147}, {"referenceID": 6, "context": "Let make a comparison to the results of other investigators in the realm of well-clusterability, in particular presented in [2, 13, 6, 5, 7, 1, 10].", "startOffset": 124, "endOffset": 147}, {"referenceID": 0, "context": "Let make a comparison to the results of other investigators in the realm of well-clusterability, in particular presented in [2, 13, 6, 5, 7, 1, 10].", "startOffset": 124, "endOffset": 147}, {"referenceID": 9, "context": "Let make a comparison to the results of other investigators in the realm of well-clusterability, in particular presented in [2, 13, 6, 5, 7, 1, 10].", "startOffset": 124, "endOffset": 147}], "year": 2017, "abstractText": "We define the notion of a well-clusterable data set from the point of view of the objective of k-means clustering algorithm and common sense. The novelty introduced here is that one can a posteriori (after running k-means) check if the data set is well-clusterable or not.", "creator": "LaTeX with hyperref package"}}}