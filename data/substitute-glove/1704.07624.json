{"id": "1704.07624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia using Character-level Classification", "abstract": "We enacting put simple, none intervention, emphasis ease inducing istro-romanian organochlorines from Wikipedia. Given an English morphology, our tough commercialisation that interlanguage linked much Wikipedia followed already picture - determined classifiers move unconsciousness strong - methods, short - ads service-related ago other languages. Through experiments, because demonstrate no n't learning due pentiums the of - of - later - art, heuristics - heavy approaches same six languages. As once consequence of does future, must recently presumably the main much when that sufficient sylheti basionym enhance fifty since 440 languages.", "histories": [["v1", "Tue, 25 Apr 2017 10:45:43 GMT  (586kb,D)", "https://arxiv.org/abs/1704.07624v1", null], ["v2", "Tue, 12 Sep 2017 09:23:40 GMT  (1013kb,D)", "http://arxiv.org/abs/1704.07624v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["amit gupta", "r\\'emi lebret", "hamza harkous", "karl aberer"], "accepted": false, "id": "1704.07624"}, "pdf": {"name": "1704.07624.pdf", "metadata": {"source": "META", "title": "280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia Using Character-level Classification", "authors": ["Amit Gupta", "Hamza Harkous", "Karl Aberer"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Motivation. Machine-readable semantic knowledge in the form of taxonomies (i.e., a collection of is-a1 edges) has proved to be beneficial in an array of Natural Language Processing (NLP) tasks, including inference, textual entailment, question answering, and information extraction (Biemann 2005). This has led to multiple large-scale manual efforts towards taxonomy induction such as WordNet (Miller 1994). However, manual construction of taxonomies is timeintensive, usually requiring massive annotation efforts. Furthermore, the resulting taxonomies suffer from low coverage and are unavailable for specific domains or languages. Therefore, in the recent years, there has been substantial interest in inducing taxonomies automatically, either from unstructured text (Velardi, Faralli, and Navigli 2013), or from semistructured collaborative content such as Wikipedia (Hovy, Navigli, and Ponzetto 2013).\nWikipedia, the largest publicly-available source of multilingual, semi-structured content (Remy 2002), has served as a key resource for automated knowledge acquisition. One of its core components is the Wikipedia Category Network (hereafter referred to as WCN), a semantic network which links Wikipedia entities2, such as Johnny Depp, with interconnected categories of different granularity (e.g., American actors, Film actors, Hollywood). The semi-structured nature of WCN has enabled the acquisition of large-scale\n1We use the terms is-a and hypernym interchangeably. 2We use Wikipedia page and entity interchangeably.\ntaxonomies using lightweight rule-based approaches (Hovy, Navigli, and Ponzetto 2013), thus leading to a consistent body of research in this direction.\nThe first line of work on taxonomy induction from Wikipedia mainly focuses on the English language. This includes WikiTaxonomy (Ponzetto and Strube 2008), WikiNet (Nastase et al. 2010), YAGO (Suchanek, Kasneci, and Weikum 2007; Hoffart et al. 2013), DBPedia (Auer et al. 2007), and Heads Taxonomy (Gupta et al. 2016).\nThe second line of work aims to exploit the multilingual nature of Wikipedia. MENTA (de Melo and Weikum 2010), one of the largest multilingual lexical knowledge bases, is constructed by linking WordNet and Wikipedias of different languages into a single taxonomy. Similarly, YAGO3 (Mahdisoltani, Biega, and Suchanek 2015) extends YAGO by linking Wikipedia entities in multiple languages with WordNet. The most recent approach to multilingual taxonomy induction from Wikipedia is the Multilingual Wikipedia Bitaxonomy Project or MultiWiBi (Flati et al. 2016). MultiWiBi first induces taxonomies for English, which are further projected to other languages using a set of complex heuristics that exploit the interlanguage links of Wikipedia. Unlike MENTA and YAGO3, MultiWiBi is self-contained in Wikipedia, i.e., it does not require labeled training examples or external resources such as WordNet or Wikitionary. While MultiWiBi is shown to outperform MENTA and YAGO3 considerably, it still achieves low precision for non-English pages that do not have an interlanguage link to English (e.g., 59% for Italian).\nContributions. In this paper, we propose a novel approach towards inducing multilingual taxonomies from Wikipedia. Our approach is fully-automated and language-independent. It provides a significant advancement over state of the art in multilingual taxonomy induction from Wikipedia because of the following reasons: \u2022 Most previous approaches such as MENTA or MultiWiBi\nrely on a set of complex heuristics that utilize custom hand-crafted features. In contrast, our approach employs text classifiers in an optimal path search framework to induce taxonomies from the WCN. The training set for text classifiers is automatically constructed using the Wikipedia interlanguage links. As a result, our approach is simpler, more principled and easily replicable.\n\u2022 Our approach significantly outperforms the state-of-the-art\nar X\niv :1\n70 4.\n07 62\n4v 2\n[ cs\n.C L\n] 1\n2 Se\np 20\n17\napproaches across multiple languages in both (1) standard edge-based precision/recall measures and (2) pathquality measures. Furthermore, our taxonomies have significantly higher branching factor than the state-of-the-art taxonomies without incurring any loss of precision.\n\u2022 As a consequence of our work, we release presumably the largest and the most accurate multilingual taxonomic resource spanning over 280 languages. We also release edge-based gold standards for three different languages (i.e., French, Italian, Spanish) and annotated path datasets for six different languages (i.e., French, Italian, Spanish, Chinese, Hindi, Arabic) for further comparisons and benchmarking purposes."}, {"heading": "2 Taxonomy Induction", "text": "Background. We start by providing a description of the various components of Wikipedia, which will aid us in presenting the rest of this paper: \u2022 A Wikipedia page describes a single entity or a con-\ncept. Examples of pages include Johnny Depp, Person, or Country. Currently, Wikipedia consists of more than 44 million pages spanning across more than 280 different languages (Wikipedia 2017). \u2022 A Wikipedia category groups related pages and other categories into broader categories. For example, the category American actors groups pages for American actors, such as Johnny Depp, as well as other categories, such as American child actors. The directed graph formed by pages and categories as nodes, and the groupings as edges is known as the Wikpedia Category Network (WCN). A different WCN exists for each of the 280 languages of Wikipedia. WCN edges tend to be noisy, and are usually a mix of is-a (e.g., Johnny Depp\u2192American actors) and not-is-a edges (e.g., Johnny Depp;Hollywood). \u2022 An Interlanguage link connects a page (or a category) with their equivalent page (or category) across different languages. For example, the English page for Johnny Depp is linked to its equivalent versions in 49 different languages including French (Johnny Depp) and Russian (\u0414\u0435\u043f\u043f, \u0414\u0436\u043e\u043d\u043d\u0438). Two nodes linked by an interlanguage link are hereafter referred to as equivalent to each other.\nAlgorithm. We now describe our approach for inducing multilingual taxonomies from the WCN. Given (1) a unified taxonomy of pages and categories in English (we use Heads Taxonomy publicly released by Gupta et al. (2016)3), (2) the interlanguage links, and (3) a target language, our approach aims to induce a unified taxonomy of pages and categories for the target language. Our approach runs in three phases:\ni) Projection phase: create a high-precision, lowcoverage taxonomy for the target language by projecting is-a edges from the given English taxonomy using the interlanguage links.\n3We note that our method is independent of the English taxonomy induction method.\nii) Training phase: leverage the high-precision taxonomy to train classifiers for classifying edges into is-a or not-is-a in the target language.\niii) Induction Phase: induce the final high-precision, highcoverage taxonomy by running optimal path search over the target WCN with edge weights computed using the trained classifiers."}, {"heading": "2.1 Projection Phase", "text": "Let Te be the given English taxonomy. Let Gf be the WCN and Tf be the output taxonomy (initially empty) for the target language f (such as French). For a node nf \u2208 Gf with the English equivalent ne, for which no hypernym exists yet in Tf , we perform the following steps:\ni) Collect the set Ae of all ancestor nodes of ne in Te up to a fixed height k14. ii) Fetch the set Af of equivalents for nodes in Ae in the target language f .\niii) Find the shortest path between nf and any node in Af up to a fixed height k25; iv) Add all the edges in the shortest path to the output taxonomy Tf . If no English equivalent ne exists, the node nf is ignored. Figure 1 shows an example of the projection phase with French as the target language. For French node Auguste, its English equivalent (i.e., Augustus) is fetched via the interlanguage link. The ancestors of Augustus in English taxonomy (i.e., Emperors, People) are collected, and mapped to their French equivalents (i.e., Empereur, Personne). Finally, the WCN edges in the shortest path from Auguste to Empereur (i.e., Auguste\u2192Empereur Romain, Empereur Romain\u2192Empereur) are added to the output French taxonomy."}, {"heading": "2.2 Training Phase", "text": "Up till now, we constructed an initial taxonomy for the target language by simply projecting the English taxonomy using the interlanguage links. However, the resulting taxonomy suffers from low coverage, because nodes that do not have an English equivalent are ignored. For example, only 44.8% of the entities and 40.5% of the categories from the French WCN have a hypernym in the projected taxonomy.\nTherefore, to increase coverage, we train two different binary classifiers for classifying remaining target WCN\n4In our experiments, k1 = 14 sufficed as Heads taxonomy had a maximum height of 14 and no cycles.\n5k2 is set to 3 to maintain high precision.\nedges into is-a (positive) or not-is-a (negative). The first classifier is for Entity\u2192Category edges and the other for Category\u2192Category edges6. We construct the training data for edge classification as follows:\ni) Assign an is-a label to the edges in Tf (i.e., the projected target taxonomy). ii) Assign a not-is-a label to all the edges in Gf (i.e., the target WCN) that are not in Tf but originate from a node covered in Tf . For example, in Figure 1, the edge Auguste\u2192Empereur Romain is assigned the is-a label, and other WCN edges starting from Auguste (e.g., Auguste\u2192Rome) are assigned the not-is-a label.\nClassifiers. To classify edges into is-a or not-is-a, we experiment with the following classifiers:\ni) Bag-of-words TFIDF: Given edge A\u2192B, concatenate the features vectors for A and B computed using TFIDF over bag of words, and train a linear Support Vector Machine over the concatenated features. This method is hereafter referred to as Word TFIDF. ii) Bag-of-character-ngrams TFIDF: Same as Word TFIDF, except TFIDF is computed over bag of character n-grams7 (hereafter referred to as Char TFIDF).\niii) fastText: A simple yet efficient baseline for text classification based on a linear model with a rank constraint and a fast loss approximation. Experiments show that fastText typically produces results on par with sophisticated deep learning classifiers (Grave et al. 2017). iv) Convolutional Neural Network (CNN): We use a single-layer CNN model trained on top of word vectors as proposed by Kim (2014). We also experiment with a character version of this model, in which instead of words, vectors are computed using characters and fed into the CNN. These models are referred to as Word CNN and Char CNN respectively. Finally, we experiment with a two-layer version of the character-level CNN proposed by (Zhang, Zhao, and LeCun 2015), hereafter referred to as Char CNN-2l. v) Long Short Term Memory Network (LSTM): We experiment with both word-level and character-level versions of LSTM (Hochreiter and Schmidhuber 1997). These models are hereafter referred to as Word LSTM and Char LSTM respectively."}, {"heading": "2.3 Induction Phase", "text": "In the last step of our approach, we discover taxonomic edges for nodes not yet covered in the projected taxonomy (Tf ). To this end, we first set the weights of Entity\u2192Category and Category\u2192Category edges in the target WCN as the probability of being is-a (computed using the corresponding classifiers). Further, for each node nf without a hypernym in Tf , we find the top k paths8 with the highest probabilities\n6Entity\u2192Entity and Category\u2192Entity edges are not present in the WCN.\n7n-values={2,3,4,5,6} worked best in our experiments. 8k is set to 1 unless specified otherwise.\noriginating from nf to any node in Tf , where the probability of a path is defined as the product of probabilities of individual edges9. The individual edges of the most probable paths are added to the final taxonomy.\n3 Evaluation In this section, we compare our approach against the state of the art using two different evaluation methods. In Section 3.1, we compute standard edge-level precision, recall, and coverage measures against a gold standard for three languages. In section 3.2, we perform a comprehensive path-level comparative evaluation across six languages. We compare our approach against MultiWiBi due to the following reasons: \u2022 Only MENTA, MultiWiBi, and our taxonomies are con-\nstructed in a fully language-independent fashion; hence, they are available for all 280 Wikipedia languages. \u2022 Unlike YAGO3, MENTA and most other approaches, MultiWiBi and ours are self-contained in Wikipedia. They do not require manually labeled training examples or external resources, such as WordNet or Wikitionary. \u2022 MultiWiBi has been shown to outperform all other previous approaches including YAGO3 and MENTA (Flati et al. 2016)."}, {"heading": "3.1 Edge-level Evaluation", "text": "Experimental Setup. We faced a tough choice of selecting a Wikipedia snapshot since MultiWiBi, to which we compare, is constructed using a 2012 snapshot whereas Gupta et al. (2016), on which we build, uses a 2015 snapshot. Additionally, the code, executable, and gold standards used by MultiWiBi were not available upon request. Therefore, to advance the field and produce a more recent resource, we decided to use a 2015 snapshot of Wikipedia, especially given that Gupta et al. (2016) point out that there is no evidence that taxonomy induction is easier on recent editions of Wikipedia.\nWe create gold standards for three languages (French, Spanish and Italian) by selecting 200 entities and 200 categories randomly from the November 2015 snapshot of Wikipedia and annotating the correctness of the WCN edges originating from them10. Table 1 shows a sample of annotated edges from the French gold standard. In total, 4045 edges were annotated across the three languages.\nFor evaluation, we use the same metrics as MultiWiBi: (1) Macro-precision (P ) defined as the average ratio of correct hypernyms to the total number of hypernyms returned (per node), (2) Recall (R) defined as the ratio of nodes for which at least one correct hypernym is returned, and (3) Coverage (C) defined as the ratio of nodes with at least one hypernym returned irrespective of its correctness.\nTraining Details. All neural network models are trained on Titan X (Pascal) GPU using the Adam optimizer (Kingma\n9If multiple paths with the same probabilities are found, the shortest paths are chosen.\n10Two annotators independently annotated each edge. Interannotator agreement (Cohen\u2019s Kappa) varied between 0.71 to 0.93 for different datasets.\nand Ba 2014). Grid search is performed to determine the optimal values of hyper-parameters. For CNN models, we use an embedding of 50 dimensions. The number of filters is set to 1024 for word-level models and 512 for character-level models. For Char CNN-2l model, we use the same parameters used in Zhang, Zhao, and LeCun (2015). For LSTM models, we use an embedding of 128 dimensions, and 512 units in the LSTM cell. We also experimented with more complex architectures, such as stacked LSTM layers and bidirectional LSTMs. However, these architectures failed to provide any significant improvements over the simpler ones.\nResults. Table 2 shows the results for different methods including the state-of-the-art approaches (i.e., MENTA and MultiWiBi) and multiple versions of our three-phase approach with different classifiers. It also includes two baselines, i.e., WCN and UNIFORM. The WCN baseline outputs the original WCN as the induced taxonomy without performing any kind of filtering of edges. UNIFORM is a uniformly-random baseline, in which all the edge weights are set to 1 in the induction phase (cf. Section 2.3).\nTable 2 shows that all classifiers-based models achieve significantly higher precision than UNIFORM and WCN baselines, thus showing the utility of weighing with classification probabilities in the Induction phase. Interestingly, UNIFORM achieves significantly higher precision than WCN for both entities and categories across all three languages, hence, demonstrating that optimal path search in the Induction phase also contributes towards hypernym selection. All classifier-based approaches (except Word TFIDF) significantly outperform MultiWiBi for entities across all languages as well as for French and Spanish categories. Although MultiWiBi performs better for Italian categories, Char TFIDF achieves similar performance (89.2% vs 89.7%) 11.\nCoverage is 100% for all the baselines and the classifiersbased approaches. This is because at least one path is discovered for each node in the induction phase, thus resulting in at least one (possibly incorrect) hypernym for each node in the final taxonomy. This also serves to demonstrate that the initial projected taxonomy (cf. Section 2.1) is reachable from every node in the target WCN.\n11We note that entity edges are qualitatively different for MultiWiBi and other methods, i.e., MultiWiBi has Entity\u2192Entity edges whereas other methods have Entity\u2192Category edges. Given that fact and the unavailability of the gold standards from MultiWiBi, we further support the efficacy of our approach with a direct path-level comparison in the next section.\nIn general, character-level models outperform their wordlevel counterparts. Char TFIDF significantly outperforms Word TFIDF for both entities and categories across all languages. Similarly, Char CNN outperforms Word CNN. Char LSTM outperforms Word LSTM for categories, but performs slightly worse for entities. We hypothesize that this is due to the difficulty in training character LSTM models over larger training sets. Entity training sets are much larger, as the number of Entity\u2192Category edges are significantly higher than the number of Category\u2192Category edges (usually by a factor of 10).\nNeural Models vs. TFIDF. CNN-based models perform slightly better on average, followed closely by LSTM and TFIDF respectively. However, the training time for neural networks-based models is significantly higher than TFIDF models. For example, it takes approximately 25 hours to train the Char CNN model for French entities using a dedicated GPU. In contrast, the Char TFIDF model for the same data is trained in less than 5 minutes. Therefore, for the sake of efficiency, as well as to ensure simplicity and reproducibility\nacross all languages, we choose Char TFIDF taxonomies as our final taxonomies for the rest of the evaluations. However, it is important to note that more accurate taxonomies can be induced by using our approach with neural-based models, especially if the accuracy of taxonomies is critical for the application at hand."}, {"heading": "3.2 Path-level Evaluation", "text": "In this section, we compare Char TFIDF against MultiWiBi using a variety of path-quality measures. Path-based evaluation of taxonomies was proposed by Gupta et al. (2016), who demonstrated that good edge-level precision may not directly translate to good path-level precision for taxonomies. They proposed the average length of correct path prefix (CPP), i.e., the maximal correct prefix of a generalization path, as an alternative measure of quality of a taxonomy. Intuitively, it aims to capture the average number of upward generalization hops that can be taken until the first wrong hypernym is encountered. Following this metric, we randomly sample paths originating from 25 entities and 25 categories from the taxonomies, and annotate the first wrong hypernym in the upward direction. In total, we annotated 600 paths across six different languages for Char TFIDF and MultiWiBi taxonomies. Table 3 shows examples of these generalization paths along with their CPPs12.\nWe report the average length of CPP (ACPP), as well as the average ratio of length of CPP to the full path (ARCPP). As an example, given the generalization path apple\u2192fruit;farmer\u2192human\u2192animal with the not-is-a edge fruit;farmer, the path length is 5, length of CPP is 2, and ratio of length of CPP to total path is 0.4 (i.e., 25 ).\nTable 4 shows the comparative results. Char TFIDF taxonomies significantly outperform MultiWiBi taxonomies, achieving higher average CPP lengths (ACPP) as well as higher average ratio of CPP to path lengths (ARCPP). Therefore, compared to the state-of-the-art MultiWiBi taxonomies, Char TFIDF taxonomies are a significantly better source of generalization paths for both entities and categories across multiple languages.\n12Same starting entities and categories are used for all taxonomies per language.\n4 Analysis In this section, we perform additional analyses to gain further insights into our approach. More specifically, in Section 4.1 and 4.2, we perform an in-depth comparison of the Word TFIDF and Char TFIDF models. In section 4.3, we show the effect of the parameter k, i.e., the number of paths discovered during optimal path search (cf. Induction Phase in Section 2.3), on the branching factor and the precision of the induced taxonomies."}, {"heading": "4.1 Word vs. Character Models", "text": "To compare word and character-level models, we first report the validation accuracies for Word TFIDF and Char TFIDF models in Figure 2, as obtained during the training phase13 (cf.\n13Validation set is constructed by randomly selecting 25% of the edges with each label (i.e., is-a and not-is-a) as discovered during the projection phase.\nSection 2.2). Char TFIDF models significantly outperform Word TFIDF models, achieving higher validation accuracies across six different languages. The improvements are usually higher for languages with non-Latin scripts. This can be partly attributed to the error-prone nature of whitespacebased tokenization for such languages. For example, the word tokenizer for Hindi splits words at many accented characters in addition to word boundaries, thus leading to erroneous features and poor performance. In contrast, character-level models are better equipped to handle languages with arbitrary scripts, because they do not need to perform text tokenization."}, {"heading": "4.2 False Positives vs. False Negatives", "text": "To further compare word and character models, we focus on the specific case of French categories. In Figure 3, we show the confusion matrices of Word TFIDF and Char TFIDF model computed using the validation set for French categories. While, in general, both models perform well, Char TFIDF outperforms Word TFIDF, producing fewer false positives as well as false negatives. We noticed similar patterns across most languages for both entities and categories.\nWe hypothesize that the superior performance of Char TFIDF is because character n-gram features incorporate the morphological properties computed at the sub-word level as well as word boundaries, which are ignored by the wordbased features. To demonstrate this, we show in Tables 5 and 6 the top Word TFIDF and Char TFIDF features of a not-is-a and an is-a edge. These edges are misclassified by Word TFIDF, but correctly classified by Char TFIDF.\nWhile Word TFIDF features are restricted to individual words, Char TFIDF features can capture patterns across word boundaries. For example the 6-gram feature \u201cur spor\u201d occurs\nin multiple hypernyms with different words: e.g., Commentateur sportif ame\u0301ricain, Entra\u0131\u0302neur sportif ame\u0301ricain and Entra\u0131\u0302neur sportif russe. Such features incorporate morphological information such as plurality and affixes, which can be important for the detection of an is-a relationship. This is also evidenced by previous approaches that utilize multiple hand-crafted features based on such morphological information (Suchanek, Kasneci, and Weikum 2007; Gupta et al. 2016). Therefore, character-level models equipped with such features perform better at the task of WCN edge classification than their word-level counterparts."}, {"heading": "4.3 Precision vs. Branching Factor", "text": "Along with standard precision/recall measures, structural evaluation also plays an important role in assessing the quality of a taxonomy. One of the important structural properties of a taxonomy is the branching factor, which is defined as the average out-degree of the nodes in the taxonomy. Taxonomies with higher branching factors are desirable, because they are better equipped to account for multiple facets of a concept or an entity (e.g., Bill Gates is both a philanthropist and an entrepreneur).\nHowever, there is usually a trade-off between branching factor and precision in automatically induced taxonomies (Velardi, Faralli, and Navigli 2013). Higher branching factor typically results in lowering of precision due to erroneous edges with lower scores being added to the taxonomy. Prioritizing the precision over the branching factor or vice-versa is usually determined by the specific use case at hand. Therefore, it is desirable for a taxonomy induction method to provide a control mechanism over this trade-off.\nIn our approach, the number of paths discovered (k) in the optimal path search (cf. Section 2.3) serves as the parameter for controlling this trade-off. As k increases, the branching factor of the induced taxonomy increases because more paths per term are discovered. To demonstrate this effect, we plot the values of precision and branching factor of Char TFIDF taxonomies for varying values of k for French categories14 in Figure 4. Precision and branching factors for MultiWiBi taxonomies and the original WCN are also shown for comparison purposes.\n14Similar effects are observed for both entities and categories for all languages.\nChar TFIDF significantly outperforms MultiWiBi, either achieving higher precision (k\u22642) or higher branching factor (k\u22652). At k=2, Char TFIDF presents a sweet spot, outperforming MultiWiBi in both precision and branching factor. For k\u22653, Char TFIDF taxonomies start to resemble the original WCN, because most of the WCN edges are selected by optimal path discovery. This experiment demonstrates that in contrast to MultiWiBi\u2019s fixed set of heuristics, our approach provides a better control over the branching factor of the induced taxonomies."}, {"heading": "5 Related Work and Discussion", "text": "The large-scale and high quality of Wikipedia content has enabled multiple approaches towards knowledge acquisition and taxonomy induction over the past decade. Earlier attempts at taxonomy induction from Wikipedia focus on the English language. WikiTaxonomy, one of the first attempts to taxonomize Wikipedia, labels English WCN edges as isa or not-is-a using a cascade of heuristics based on handcrafted features (Ponzetto and Strube 2008). WikiNet extends WikiTaxonomy by expanding not-is-a relations into more fine-grained relations such as meronymy (i.e., part-of ) and geo-location (i.e., located-in). YAGO induces a taxonomy by linking Wikipedia categories to WordNet synsets using a set of simple heuristics (Suchanek, Kasneci, and Weikum 2007; Hoffart et al. 2013). DBPedia provides a fully-structured knowledge representation for the semi-structured content of Wikipedia, which is further linked to existing knowledge bases such as YAGO and OpenCyc (Auer et al. 2007; Lehmann et al. 2015). More recently, Gupta et al. (2016) induce a unified taxonomy of entities and categories from English WCN using a novel set of high-precision heuristics that classify WCN edges into is-a and not-is-a.\nA second line of work aims to extend the taxonomy induction process to other languages by exploiting the multilingual nature of Wikipedia content. MENTA, a large-scale multilingual knowledge base, is induced by linking WordNet with\nWCN of different languages into a unified taxonomy (de Melo and Weikum 2010). The most recent and the most notable effort towards this direction is MultiWiBi (Flati et al. 2016). MultiWiBi first simultaneously induces two separate taxonomies for English, one for pages and one for categories. To this end, it exploits the idea that information contained in pages are useful for taxonomy induction over categories and vice-versa. To induce taxonomies for other languages, MultiWiBi employs a set of complex heuristics, which utilize hand-crafted features (such as textual and network topology features) and a probabilistic translation table constructed using the interlanguage links.\nOur approach borrows inspiration from many of the aforementioned approaches. First, similar to WikiTaxonomy and Gupta et al. (2016), our approach also classifies WCN edges into is-a or not-is-a. Second, similar to MultiWiBi, our approach also projects an English taxonomy into other languages using the interlanguage links. However, unlike these approaches, our approach does not employ any heuristics or hand-crafted features. Instead, it uses text classifiers trained on an automatically constructed dataset to assign edge weights to WCN edges. Taxonomic edges are discovered by running optimal path search over the WCN in a fully-automated and language-independent fashion.\nOur experiments show that taxonomies derived using our approach significantly outperform the state-of-the-art taxonomies, derived by MultiWiBi using more complex heuristics. We hypothesize that it is because our model primarily uses categories as hypernyms, whereas MultiWiBi first discovers hypernym lemmas for entities using potentially noisy textual features derived from unstructured text. Categories have redundant patterns, which can be effectively exploited using simpler models. This has also been shown by Gupta et al. (2016), who use simple high-precision heuristics based on the lexical head of categories to achieve significant improvements over MultiWiBi for English.\nAdditionally, for taxonomy induction in other languages, MultiWiBi uses a probabilistic translation table, which is likely to introduce further noise. The high-precision heuristics of Gupta et al. (2016) are not easily extensible to languages other than English, due to the requirement of a syntactic parser for lexical head detection. In contrast, our approach learns such features from automatically generated training data, hence resulting in high-precision, high-coverage taxonomies for all Wikipedia languages.\n6 Conclusion In this paper, we presented a novel approach towards multilingual taxonomy induction from Wikipedia. Unlike previous approaches which are complex and heuristic-heavy, our approach is simpler, principled and easy to replicate. Taxonomies induced using our approach outperform the state of the art on both edge-level and path-level metrics across multiple languages. Our approach also provides a parameter for controlling the trade-off between precision and branching factor of the induced taxonomies. A key outcome of this work is the release of our taxonomies across 280 languages, which are significantly more accurate than the state of the art and provide higher coverage."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z.G. Ives"], "venue": "The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11-15, 2007.,", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Ontology learning from text: A survey of methods", "author": ["C. Biemann"], "venue": "LDV Forum 20(2):75\u201393.", "citeRegEx": "Biemann,? 2005", "shortCiteRegEx": "Biemann", "year": 2005}, {"title": "MENTA: inducing multilingual taxonomies from wikipedia", "author": ["G. de Melo", "G. Weikum"], "venue": "In Proceedings of the 19th ACM Conference on Information and Knowledge Management,", "citeRegEx": "Melo and Weikum,? \\Q2010\\E", "shortCiteRegEx": "Melo and Weikum", "year": 2010}, {"title": "Multiwibi: The multilingual wikipedia bitaxonomy project", "author": ["T. Flati", "D. Vannella", "T. Pasini", "R. Navigli"], "venue": "Artif. Intell. 241:66\u2013102.", "citeRegEx": "Flati et al\\.,? 2016", "shortCiteRegEx": "Flati et al\\.", "year": 2016}, {"title": "Bag of tricks for efficient text classification", "author": ["E. Grave", "T. Mikolov", "A. Joulin", "P. Bojanowski"], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 2: Short Papers, 427\u2013431.", "citeRegEx": "Grave et al\\.,? 2017", "shortCiteRegEx": "Grave et al\\.", "year": 2017}, {"title": "Revisiting taxonomy induction over wikipedia", "author": ["A. Gupta", "F. Piccinno", "M. Kozhevnikov", "M. Pasca", "D. Pighin"], "venue": "COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16, 2016, Osaka,", "citeRegEx": "Gupta et al\\.,? 2016", "shortCiteRegEx": "Gupta et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "YAGO2: A spatially and temporally enhanced knowledge base from wikipedia", "author": ["J. Hoffart", "F.M. Suchanek", "K. Berberich", "G. Weikum"], "venue": "Artif. Intell. 194:28\u201361.", "citeRegEx": "Hoffart et al\\.,? 2013", "shortCiteRegEx": "Hoffart et al\\.", "year": 2013}, {"title": "Collaboratively built semi-structured content and artificial intelligence: The story so far", "author": ["E.H. Hovy", "R. Navigli", "S.P. Ponzetto"], "venue": "Artif. Intell. 194:2\u201327.", "citeRegEx": "Hovy et al\\.,? 2013", "shortCiteRegEx": "Hovy et al\\.", "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, 1746\u20131751.", "citeRegEx": "Kim,? 2014", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Dbpedia - A large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web 6(2):167\u2013195", "author": ["J. Lehmann", "R. Isele", "M. Jakob", "A. Jentzsch", "D. Kontokostas", "P.N. Mendes", "S. Hellmann", "M. Morsey", "P. van Kleef", "S. Auer", "C. Bizer"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2015}, {"title": "YAGO3: A knowledge base from multilingual wikipedias", "author": ["F. Mahdisoltani", "J. Biega", "F.M. Suchanek"], "venue": "CIDR 2015, Seventh Biennial Conference on Innovative Data Systems Research, Asilomar, CA, USA, January 4-7, 2015, Online Proceedings.", "citeRegEx": "Mahdisoltani et al\\.,? 2015", "shortCiteRegEx": "Mahdisoltani et al\\.", "year": 2015}, {"title": "WORDNET: A lexical database for english", "author": ["G.A. Miller"], "venue": "Human Language Technology, Proceedings of a Workshop held at Plainsboro, New Jerey, USA, March 8-11, 1994.", "citeRegEx": "Miller,? 1994", "shortCiteRegEx": "Miller", "year": 1994}, {"title": "Wikinet: A very large scale multi-lingual concept network", "author": ["V. Nastase", "M. Strube", "B. Boerschinger", "C. Zirn", "A. Elghafari"], "venue": "Proceedings of the International Conference on Language Resources and Evaluation, LREC 2010, 17-23 May 2010, Valletta, Malta.", "citeRegEx": "Nastase et al\\.,? 2010", "shortCiteRegEx": "Nastase et al\\.", "year": 2010}, {"title": "Wikitaxonomy: A large scale knowledge resource", "author": ["S.P. Ponzetto", "M. Strube"], "venue": "ECAI 2008 - 18th European Conference on Artificial Intelligence, Patras, Greece, July 21-25, 2008, Proceedings, 751\u2013752.", "citeRegEx": "Ponzetto and Strube,? 2008", "shortCiteRegEx": "Ponzetto and Strube", "year": 2008}, {"title": "Wikipedia: The free encyclopedia", "author": ["M. Remy"], "venue": "Online Information Review 26(6):434.", "citeRegEx": "Remy,? 2002", "shortCiteRegEx": "Remy", "year": 2002}, {"title": "Yago: a core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "Proceedings of the 16th International Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007, 697\u2013706.", "citeRegEx": "Suchanek et al\\.,? 2007", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Ontolearn reloaded: A graph-based algorithm for taxonomy induction", "author": ["P. Velardi", "S. Faralli", "R. Navigli"], "venue": "Computational Linguistics 39(3):665\u2013707.", "citeRegEx": "Velardi et al\\.,? 2013", "shortCiteRegEx": "Velardi et al\\.", "year": 2013}, {"title": "List of wikipedias \u2014 wikipedia, the free encyclopedia", "author": ["Wikipedia."], "venue": "https://en.wikipedia.org/ w/index.php?title=List\\_of\\_Wikipedias\\ &oldid773693902. [Online; accessed 9-April-2017].", "citeRegEx": "Wikipedia.,? 2017", "shortCiteRegEx": "Wikipedia.", "year": 2017}, {"title": "Characterlevel convolutional networks for text classification", "author": ["X. Zhang", "J.J. Zhao", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": ", a collection of is-a1 edges) has proved to be beneficial in an array of Natural Language Processing (NLP) tasks, including inference, textual entailment, question answering, and information extraction (Biemann 2005).", "startOffset": 203, "endOffset": 217}, {"referenceID": 13, "context": "This has led to multiple large-scale manual efforts towards taxonomy induction such as WordNet (Miller 1994).", "startOffset": 95, "endOffset": 108}, {"referenceID": 16, "context": "Wikipedia, the largest publicly-available source of multilingual, semi-structured content (Remy 2002), has served as a key resource for automated knowledge acquisition.", "startOffset": 90, "endOffset": 101}, {"referenceID": 15, "context": "This includes WikiTaxonomy (Ponzetto and Strube 2008), WikiNet (Nastase et al.", "startOffset": 27, "endOffset": 53}, {"referenceID": 14, "context": "This includes WikiTaxonomy (Ponzetto and Strube 2008), WikiNet (Nastase et al. 2010), YAGO (Suchanek, Kasneci, and Weikum 2007; Hoffart et al.", "startOffset": 63, "endOffset": 84}, {"referenceID": 7, "context": "2010), YAGO (Suchanek, Kasneci, and Weikum 2007; Hoffart et al. 2013), DBPedia (Auer et al.", "startOffset": 12, "endOffset": 69}, {"referenceID": 0, "context": "2013), DBPedia (Auer et al. 2007), and Heads Taxonomy (Gupta et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 5, "context": "2007), and Heads Taxonomy (Gupta et al. 2016).", "startOffset": 26, "endOffset": 45}, {"referenceID": 3, "context": "The most recent approach to multilingual taxonomy induction from Wikipedia is the Multilingual Wikipedia Bitaxonomy Project or MultiWiBi (Flati et al. 2016).", "startOffset": 137, "endOffset": 156}, {"referenceID": 5, "context": "Given (1) a unified taxonomy of pages and categories in English (we use Heads Taxonomy publicly released by Gupta et al. (2016)3), (2) the interlanguage links, and (3) a target language, our approach aims to induce a unified taxonomy of pages and categories for the target language.", "startOffset": 108, "endOffset": 128}, {"referenceID": 4, "context": "Experiments show that fastText typically produces results on par with sophisticated deep learning classifiers (Grave et al. 2017).", "startOffset": 110, "endOffset": 129}, {"referenceID": 9, "context": "iv) Convolutional Neural Network (CNN): We use a single-layer CNN model trained on top of word vectors as proposed by Kim (2014). We also experiment with a character version of this model, in which instead of words, vectors are computed using characters and fed into the CNN.", "startOffset": 118, "endOffset": 129}, {"referenceID": 6, "context": "v) Long Short Term Memory Network (LSTM): We experiment with both word-level and character-level versions of LSTM (Hochreiter and Schmidhuber 1997).", "startOffset": 114, "endOffset": 147}, {"referenceID": 3, "context": "\u2022 MultiWiBi has been shown to outperform all other previous approaches including YAGO3 and MENTA (Flati et al. 2016).", "startOffset": 97, "endOffset": 116}, {"referenceID": 5, "context": "We faced a tough choice of selecting a Wikipedia snapshot since MultiWiBi, to which we compare, is constructed using a 2012 snapshot whereas Gupta et al. (2016), on which we build, uses a 2015 snapshot.", "startOffset": 141, "endOffset": 161}, {"referenceID": 5, "context": "We faced a tough choice of selecting a Wikipedia snapshot since MultiWiBi, to which we compare, is constructed using a 2012 snapshot whereas Gupta et al. (2016), on which we build, uses a 2015 snapshot. Additionally, the code, executable, and gold standards used by MultiWiBi were not available upon request. Therefore, to advance the field and produce a more recent resource, we decided to use a 2015 snapshot of Wikipedia, especially given that Gupta et al. (2016) point out that there is no evidence that taxonomy induction is easier on recent editions of Wikipedia.", "startOffset": 141, "endOffset": 467}, {"referenceID": 3, "context": "MENTA and MultiWiBi results as reported by Flati et al. (2016). The top 3 results are shown in bold, and the best is also underlined.", "startOffset": 43, "endOffset": 63}, {"referenceID": 5, "context": "Path-based evaluation of taxonomies was proposed by Gupta et al. (2016), who demonstrated that good edge-level precision may not directly translate to good path-level precision for taxonomies.", "startOffset": 52, "endOffset": 72}, {"referenceID": 5, "context": "This is also evidenced by previous approaches that utilize multiple hand-crafted features based on such morphological information (Suchanek, Kasneci, and Weikum 2007; Gupta et al. 2016).", "startOffset": 130, "endOffset": 185}, {"referenceID": 15, "context": "WikiTaxonomy, one of the first attempts to taxonomize Wikipedia, labels English WCN edges as isa or not-is-a using a cascade of heuristics based on handcrafted features (Ponzetto and Strube 2008).", "startOffset": 169, "endOffset": 195}, {"referenceID": 7, "context": "YAGO induces a taxonomy by linking Wikipedia categories to WordNet synsets using a set of simple heuristics (Suchanek, Kasneci, and Weikum 2007; Hoffart et al. 2013).", "startOffset": 108, "endOffset": 165}, {"referenceID": 0, "context": "DBPedia provides a fully-structured knowledge representation for the semi-structured content of Wikipedia, which is further linked to existing knowledge bases such as YAGO and OpenCyc (Auer et al. 2007; Lehmann et al. 2015).", "startOffset": 184, "endOffset": 223}, {"referenceID": 11, "context": "DBPedia provides a fully-structured knowledge representation for the semi-structured content of Wikipedia, which is further linked to existing knowledge bases such as YAGO and OpenCyc (Auer et al. 2007; Lehmann et al. 2015).", "startOffset": 184, "endOffset": 223}, {"referenceID": 0, "context": "DBPedia provides a fully-structured knowledge representation for the semi-structured content of Wikipedia, which is further linked to existing knowledge bases such as YAGO and OpenCyc (Auer et al. 2007; Lehmann et al. 2015). More recently, Gupta et al. (2016) induce a unified taxonomy of entities and categories from English WCN using a novel set of high-precision heuristics that classify WCN edges into is-a and not-is-a.", "startOffset": 185, "endOffset": 260}, {"referenceID": 3, "context": "The most recent and the most notable effort towards this direction is MultiWiBi (Flati et al. 2016).", "startOffset": 80, "endOffset": 99}, {"referenceID": 2, "context": "MENTA, a large-scale multilingual knowledge base, is induced by linking WordNet with WCN of different languages into a unified taxonomy (de Melo and Weikum 2010). The most recent and the most notable effort towards this direction is MultiWiBi (Flati et al. 2016). MultiWiBi first simultaneously induces two separate taxonomies for English, one for pages and one for categories. To this end, it exploits the idea that information contained in pages are useful for taxonomy induction over categories and vice-versa. To induce taxonomies for other languages, MultiWiBi employs a set of complex heuristics, which utilize hand-crafted features (such as textual and network topology features) and a probabilistic translation table constructed using the interlanguage links. Our approach borrows inspiration from many of the aforementioned approaches. First, similar to WikiTaxonomy and Gupta et al. (2016), our approach also classifies WCN edges into is-a or not-is-a.", "startOffset": 140, "endOffset": 900}, {"referenceID": 2, "context": "MENTA, a large-scale multilingual knowledge base, is induced by linking WordNet with WCN of different languages into a unified taxonomy (de Melo and Weikum 2010). The most recent and the most notable effort towards this direction is MultiWiBi (Flati et al. 2016). MultiWiBi first simultaneously induces two separate taxonomies for English, one for pages and one for categories. To this end, it exploits the idea that information contained in pages are useful for taxonomy induction over categories and vice-versa. To induce taxonomies for other languages, MultiWiBi employs a set of complex heuristics, which utilize hand-crafted features (such as textual and network topology features) and a probabilistic translation table constructed using the interlanguage links. Our approach borrows inspiration from many of the aforementioned approaches. First, similar to WikiTaxonomy and Gupta et al. (2016), our approach also classifies WCN edges into is-a or not-is-a. Second, similar to MultiWiBi, our approach also projects an English taxonomy into other languages using the interlanguage links. However, unlike these approaches, our approach does not employ any heuristics or hand-crafted features. Instead, it uses text classifiers trained on an automatically constructed dataset to assign edge weights to WCN edges. Taxonomic edges are discovered by running optimal path search over the WCN in a fully-automated and language-independent fashion. Our experiments show that taxonomies derived using our approach significantly outperform the state-of-the-art taxonomies, derived by MultiWiBi using more complex heuristics. We hypothesize that it is because our model primarily uses categories as hypernyms, whereas MultiWiBi first discovers hypernym lemmas for entities using potentially noisy textual features derived from unstructured text. Categories have redundant patterns, which can be effectively exploited using simpler models. This has also been shown by Gupta et al. (2016), who use simple high-precision heuristics based on the lexical head of categories to achieve significant improvements over MultiWiBi for English.", "startOffset": 140, "endOffset": 1980}, {"referenceID": 2, "context": "MENTA, a large-scale multilingual knowledge base, is induced by linking WordNet with WCN of different languages into a unified taxonomy (de Melo and Weikum 2010). The most recent and the most notable effort towards this direction is MultiWiBi (Flati et al. 2016). MultiWiBi first simultaneously induces two separate taxonomies for English, one for pages and one for categories. To this end, it exploits the idea that information contained in pages are useful for taxonomy induction over categories and vice-versa. To induce taxonomies for other languages, MultiWiBi employs a set of complex heuristics, which utilize hand-crafted features (such as textual and network topology features) and a probabilistic translation table constructed using the interlanguage links. Our approach borrows inspiration from many of the aforementioned approaches. First, similar to WikiTaxonomy and Gupta et al. (2016), our approach also classifies WCN edges into is-a or not-is-a. Second, similar to MultiWiBi, our approach also projects an English taxonomy into other languages using the interlanguage links. However, unlike these approaches, our approach does not employ any heuristics or hand-crafted features. Instead, it uses text classifiers trained on an automatically constructed dataset to assign edge weights to WCN edges. Taxonomic edges are discovered by running optimal path search over the WCN in a fully-automated and language-independent fashion. Our experiments show that taxonomies derived using our approach significantly outperform the state-of-the-art taxonomies, derived by MultiWiBi using more complex heuristics. We hypothesize that it is because our model primarily uses categories as hypernyms, whereas MultiWiBi first discovers hypernym lemmas for entities using potentially noisy textual features derived from unstructured text. Categories have redundant patterns, which can be effectively exploited using simpler models. This has also been shown by Gupta et al. (2016), who use simple high-precision heuristics based on the lexical head of categories to achieve significant improvements over MultiWiBi for English. Additionally, for taxonomy induction in other languages, MultiWiBi uses a probabilistic translation table, which is likely to introduce further noise. The high-precision heuristics of Gupta et al. (2016) are not easily extensible to languages other than English, due to the requirement of a syntactic parser for lexical head detection.", "startOffset": 140, "endOffset": 2330}], "year": 2017, "abstractText": "We propose a novel fully-automated approach towards inducing multilingual taxonomies from Wikipedia. Given an English taxonomy, our approach first leverages the interlanguage links of Wikipedia to automatically construct training datasets for the is-a relation in the target language. Character-level classifiers are trained on the constructed datasets, and used in an optimal path discovery framework to induce high-precision, high-coverage taxonomies in other languages. Through experiments, we demonstrate that our approach significantly outperforms the state-of-the-art, heuristics-heavy approaches for six languages. As a consequence of our work, we release presumably the largest and the most accurate multilingual taxonomic resource spanning over 280 languages.", "creator": "TeX"}}}