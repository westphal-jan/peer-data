{"id": "1704.07626", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Taxonomy Induction using Hypernym Subsequences", "abstract": "We amendments giving sequel, match - supervised this forth url etymological induction from with input tonal and 6-1 instance. Unlike only marked approaches, which typically batches direct hypernym wedge bringing terms, our approach utilizes this narrated differential framework to preferably hypernym subsequences. Taxonomy stochastic from minerals communization way cast as appears unusual whose where exceeds - cost flow problem on a carefully designed roles hypercube. Through experiments, better regardless taken understand critical outperforms state - followed - the - architectural taxonomy time-varying example on second languages. Furthermore, done show that needs strategies yet overall them on greater that noise in put feedback context.", "histories": [["v1", "Tue, 25 Apr 2017 10:49:53 GMT  (432kb,D)", "https://arxiv.org/abs/1704.07626v1", null], ["v2", "Fri, 5 May 2017 17:03:17 GMT  (432kb,D)", "http://arxiv.org/abs/1704.07626v2", null], ["v3", "Fri, 26 May 2017 14:42:59 GMT  (2114kb,D)", "http://arxiv.org/abs/1704.07626v3", null], ["v4", "Thu, 14 Sep 2017 20:34:26 GMT  (2657kb,D)", "http://arxiv.org/abs/1704.07626v4", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR", "authors": ["amit gupta", "r\\'emi lebret", "hamza harkous", "karl aberer"], "accepted": false, "id": "1704.07626"}, "pdf": {"name": "1704.07626.pdf", "metadata": {"source": "META", "title": "Taxonomy Induction Using Hypernym Subsequences", "authors": ["Amit Gupta", "R\u00e9mi Lebret", "Hamza Harkous", "Karl Aberer"], "emails": ["amit.gupta@epfl.ch", "remi.lebret@epfl.ch", "hamza.harkous@epfl.ch", "karl.aberer@epfl.ch", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Computing methodologies\u2192Artificial intelligence; Information extraction; Ontology engineering; Semantic networks;\nKEYWORDS Knowledge acquisition; taxonomy induction; term taxonomies; algorithms; flow networks; minimum-cost flow optimization;"}, {"heading": "1 INTRODUCTION", "text": "Motivation. Lexical semantic knowledge in the form of term taxonomies has been beneficial in a variety of NLP tasks, including inference, textual entailment, question answering and information extraction [3]. This widespread utility of taxonomies has led to multiple large-scale manual efforts towards taxonomy induction, such asWordNet [22] and Cyc [21]. However, such manually constructed taxonomies suffer from low coverage [15] and are unavailable for specific domains or languages. Therefore, in recent years, there has been substantial interest in extending existing taxonomies automatically or building new ones [4, 5, 19, 34, 38, 40].\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM\u201917 , November 6\u201310, 2017, Singapore, Singapore \u00a9 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-4918-5/17/11. . . $15.00 https://doi.org/10.1145/3132847.3133041\nApproaches towards automated taxonomy induction consist of two main stages:\n(1) extraction of hypernymy relations (i.e., \u201cis-a\" relations between a term and its hypernym such as apple\u2192fruit) (2) structured organization of terms into a taxonomy, i.e., a coherent tree-like hierarchy.\nExtraction of hypernymy relations has been relatively wellstudied in previous works. Its approaches can be classified into two main categories: Distributional and Pattern-based approaches.\nDistributional approaches use clustering to extract hypernymy relations from structured or unstructured text. Such approaches draw primarily on the distributional hypothesis [12], which states that semantically similar terms appear in similar contexts. The main advantage of distributional approaches is that they can discover relations not directly expressed in the text. In contrast, Pattern-based approaches utilize pre-defined rules or lexico-syntactic patterns to extract terms and hypernymy relations from text [13, 26]. Patterns are either chosen manually [13, 20] or learnt automatically via bootstrapping [35]. Pattern-based approaches usually result in higher accuracies. However, unlike the distributional approaches, which are fully unsupervised, they require a set of seed surface patterns to initiate the extraction process.\nEarly work on the second stage of taxonomy induction, namely the structured organization of terms into a taxonomy, focused on extending existing partial taxonomies such asWordNet by inserting missing terms at appropriate positions [34, 39, 40]. Another line of work focused on taxonomy induction fromWikipedia by exploiting the semi-structured nature of the Wikipedia category network [7, 10, 24, 30, 31, 36].\nSubsequent approaches to taxonomy induction focused on building lexical taxonomies entirely from scratch, i.e., from a domain corpus or the Web [1, 2, 19, 25, 28, 38].\nAutomated taxonomy induction from scratch is preferred because it can be used over arbitrary domains, including highly specific or technical domains, such as Finance or Artificial Intelligence [25]. Such domains are usually under-represented in existing taxonomic resources. For example, WordNet is limited to the most frequent and the most important nouns, adjectives, verbs, and adverbs [11, 23]. Similarly,Wikipedia is limited to popular entities [18], and its utility is further diminished by slowed growth [37].\nPast approaches to taxonomy induction from scratch either assume the availability of a clean input vocabulary [28] or employ a time-consuming manual cleaning step over a noisy input vocabulary [38]. For example, Figure 1 shows the pipeline of a typical\nar X\niv :1\n70 4.\n07 62\n6v 4\n[ cs\n.A I]\n1 4\nSe p\n20 17\ntaxonomy induction approach from a domain corpus [38]. An initial noisy vocabulary is automatically extracted from the domain corpus using a term extraction tool, such as TermExtractor [32], and is further cleaned manually to produce the final vocabulary. This requirement severely limits the applicability of such approaches in an automated setting because clean vocabularies are usually unavailable for specific domains.\nTo handle these limitations, we designed our approach to induce a taxonomy directly from a noisy input vocabulary. Consequently, it is the first work to fully automate the taxonomy induction process for arbitrary domains.\nContributions. In this paper, we present a novel, semi-supervised approach for building lexical taxonomies given an input vocabulary of (potentially noisy) seed terms. We leverage the existing work on hypernymy relations extraction and focus on the second stage, i.e. the organization of terms into a taxonomy. Our main contributions are as follows:\n\u25cf We propose a novel probabilistic framework for extracting longer hypernym subsequences from hypernymy relations, as well as a novel minimum-cost flow based optimization framework for inducing a tree-like taxonomy from a noisy hypernym graph. \u25cf We empirically show that our approach outperforms stateof-the-art taxonomy induction approaches across four different languages, while achieving >32% relative improvement in F1-measure over the Food domain. \u25cf We demonstrate that our subsequence-based model is robust to the presence of noisy terms in the input vocabulary, and achieves a 65% relative improvement in precision over an edge-based model while maintaining similar coverage. To the best of our knowledge, this is the first approach towards taxonomy induction from a noisy input vocabulary.\nThe rest of the paper is organized as follows. In Section 2, we describe our taxonomy induction approach. In Section 3, we discuss our experiments and performance results. In Section 4, we discuss related work. We conclude in Section 5."}, {"heading": "2 TAXONOMY INDUCTION", "text": "Given a potentially-noisy vocabulary1 of seed terms as an input, we define our goal as inducing a taxonomy consisting of these seed terms (and possibly other terms). This taxonomy is a directed acyclic graph with terms as the nodes and the edges indicating a\n1In this work, we use terminology and vocabulary interchangeably.\nhypernymy relationship between the terms. For our task, we assume the availability of a database of candidate hypernymy relations. Multiple such resources have been compiled and made available publicly over the years. A prominent example of such a resource is WebIsA [33], a collection of more than 400 million hypernymy relations for English, extracted from the CommonCrawl web corpus using lexico-syntactic patterns. However, such resources come with a considerable number of noisy candidate hypernyms, typically containing a mixture of relations such as hyponymy, meronymy, synonymy and co-hyponymy. For example, WebIsA has more than 12,000 hypernyms for the term apple, including noisy hypernyms such as orange, everyone and smartphone. A sample set of candidate hypernyms and their occurrence frequencies for the term apple taken from WebIsA is shown in Table 1.\nOur approach to taxonomy induction consists of three main steps:\n(1) extracting hypernym subsequences for the given seed terms (Section 2.1), (2) aggregating the extracted subsequences into an initial hypernym graph (Section 2.2), (3) pruning the hypernym graph using a minimum-cost flow approach to induce the final taxonomy (Section 2.3)."}, {"heading": "2.1 Hypernym Subsequences Extraction", "text": "Unsupervised or semi-supervised approaches to taxonomy induction typically aim to extract single hypernym edges among terms from noisy candidate hypernyms [19, 28]. In contrast, our approach consists of extracting hypernym subsequences (where a subsequence is a series of one or more individual hypernym edges).\nTomotivate this, we first note that Table 1 includes hypernyms of apple at different levels of generality, such as fruit and food. In fact, we observe this pattern in the candidate hypernyms of most terms.\nThis suggests that we can leverage such information to not only extract the direct hypernyms of apple, but to also extract longer hypernym subsequences, such as apple\u2192fruit\u2192food.\nThis becomes even more important given the result by Velardi et al. [38], who demonstrated that hypernym extraction becomes increasingly erroneous as the generality of terms increases, mainly due to the increase in term ambiguity. To further support this hypothesis, we perform an experiment where we first randomly sample 100 paths from Wordnet. For each edge a\u2192b in a sampled path, we plot the normalized frequency2 of \u201cb as a candidate hypernym for a\u201d against the height of the edge, where frequencies are computed using lexico-syntactic patterns (cf. Table 1). We also plot the average rank of b among candidate hypernyms of a, where candidate hypernyms are ranked by their normalized frequencies in a decreasing order. Results of this experiment are shown in Figure 2. Since edges in WordNet are assumed to be ground truth, it is desired that they have a higher normalized frequency and lower ranks. This small-scale experiment demonstrates that as the height of the edge increases, the normalized frequencies decrease whereas the average ranks increase. Therefore, the accuracy of patterns-based hypernymy detection decreases for more general terms that appear higher in generalization paths. Hence, for such terms, it makes sense to not solely base the hypernym selection on a noisy set of candidate hypernyms. We can potentially improve the accuracy of selected hypernyms for general terms (such as fruit) by relying on extracted subsequences starting from more specific terms (such as apple). Those subsequences would be evidenced by the less-noisy candidate hypernyms of the specific terms.\nIn sum, extracting hypernym subsequences is both possible and potentially beneficial. The remainder of this section describes our model that realizes this intuition.\nModel. We now describe our model for extracting hypernym subsequences for a given term. We begin with a general formulation using directed acyclic graphs (referred to as DAG), and we make simplifying assumptions to derive a model for hypernym subsequences. We use the following notations: \u25cf t0: a given seed term, e.g., apple; \u25cf lt : lexical head of any term t , e.g., lt=soup for t=chicken soup;\n2Normalization is performed by dividing frequency counts by the maximum.\n\u25cf E: Hypernym Evidence, i.e., the set of all the candidate hypernymy relations, in the form of 3-tuples (hyponym, hypernym, frequency); \u25cf Ek(t): Hypernym Evidence for term t , i.e., the set of top-k candidate hypernyms for term t , having the highest frequency counts (Table 1 shows a sample from Ek(t) for t=apple); \u25cf Ek(t ,m): mth ranked candidate hypernym from Ek(t), where m \u2264 k , and ranks are computed by sorting candidate hypernyms in decreasing order of frequency counts; \u25cf sim(ti , tj): A similarity measure between terms ti and tj estimated using evidence E; \u25cf Gt : a DAG consisting of generalizations for a term t (Figure 3 shows an example of a possible DAG for t=apple).\nFor a given term t0, we define the goal of this step of our taxonomy induction approach as finding a DAG G\u0302t0 , which maximizes the conditional probability ofGt0 , given the evidence Ek(t0), for a fixed k :\nG\u0302t0 = argmax Gt0 Pr(Gt0 \u22c3\ufe00Ek(t0))\n= argmax Gt0 Pr(Ek(t0)\u22c3\ufe00Gt0) \u00d7 Pr(Gt0) (1)\nDue to the combinatorial nature of the search space ofGt0 , finding an exact solution to the above equation is intractable, even for a small k . Therefore, we make the following simplifying assumptions, which facilitate an efficient search through the search space of Gt0 :\n\u25cf Gt0 can be approximated as a set of independent hypernym subsequences with possibly repeated hypernyms. In other words, Gt0 = \u22c3bi=1 Sit0 where S i t0 is the i\nth subsequence and b is a fixed constant. For example, the DAG shown in Figure 3 can be approximated as a set of three subsequences: (i) apple\u2192fruit\u2192food, (ii) apple\u2192hardware company\u2192company, and (iii) apple\u2192technology company\u2192company. This assumption intuitively derives from the fact that any DAG can be represented by a finite number of subsequences. \u25cf \u2200i , the joint events (Ek(t0),Sit0) are independent. Intuitively, this assumption implies that each subsequence independently contributes to the evidence Ek(t0). \u25cf \u2200i , the direct hypernyms of t0 in Sit0 are unique. In other words, for a candidate hypernym hc of given term t0, there is at most one subsequence with the first edge t0\u2192hc . Intuitively, this assumption implies that a candidate hypernym hc uniquely sensedisambiguates the term t0, thus resulting in a only one possible generalization subsequence.\nIn conjunction, these assumptions imply thatGt0 is composed of b hypernym subsequences, where each subsequence independently attempts to generate Ek(t0). Given these assumptions, Equation 1 transforms into:\nG\u0302t0 = argmax \u22c3bi=1 S i t0\nb \u220f i=1 Pr(Ek(t0)\u22c3\ufe00S i t0) \u00d7 Pr(S i t0) (2)\nEstimation. We now describe the estimation of Pr(Ek(t0)\u22c3\ufe00Sit0) and Pr(Sit0) for a hypernym subsequence S i t0 . In order to motivate the estimation of the conditional probability Pr(Ek(t0)\u22c3\ufe00Sit0), we start with an example. Consider a valid hypernym subsequence apple\u2192fruit\u2192food\u2192substance\u2192matter\u2192entity for the term apple (whose candidate hypernyms are in Table 1). At first sight, it might seem desirable for a candidate hypernym from Ek(t0) (e.g., fruit) to have a high similarity with as many terms in the subsequence as possible. However, since the similarity measure is based on the hypernym evidence E, it is plausible that terms such as matter and entity have a low similarity with the candidate hypernym fruit, simply because they are at a higher level of generality. To avoid penalizing such valid subsequences, we let the conditional probability Pr(Ek(t0)\u22c3\ufe00Sit0) be proportional to the maximum similarity possible between the candidate hypernym and any term in the subsequence (e.g., for the candidate hypernym fruit, the similarity is 1 as fruit is in the subsequence). We aggregate those similarity values across the candidate hypernyms. More formally, assuming subsequence Sit0 = t0\u2192hi1\u2192hi2. . .hin , where n is the length of S i t0 , we compute the conditional probability as:\nPr(Ek(t0)\u22c3\ufe00S i t0)\u221d k \u2211 m=1 (\u03bb1)m max j\u2208(\ufe001,n\u230b\ufe00 (sim(Ek(t0,m),hi j)) (3)\nwhere \u03bb1 (a fixed parameter) serves as a rank-penalty to penalize candidate hypernyms with lower frequency counts.\nWe now proceed to compute Pr(Sit0), the other constituent of Equation 2. Towards that, we assume that Sit0 is a collection of independent hypernym edges. Thus, Pr(Sit0) becomes the product of the individual edges\u2019 probabilities:\nPr(Sit0)\u221d Pre(t0,hi1) \u00d7 (\u03bb2) n n\u22121 \u220f j=1 Pre(hi j ,hi(j+1)) (4)\nwhere Pre(x1,x2) is the probability of an individual hypernym edge x1\u2192x2 between terms x1 and x2; \u03bb2 is a length penalty parameter. Finally, we estimate Pre(x1,x2) as a log-linear model using a set of features f, weighted by the learned weight vector w:\nPre(x1,x2) \u221d exp (w \u22c5 f(x1,x2)) (5) We also use this edge probability to compute the aforementioned similarity function (sim) as:\nsim(xi ,x j) = max (Pre(xi ,x j),Pre(x j ,xi)) (6)\nIntuitively, Pr(Ek(t0)\u22c3\ufe00Sit0) promotes subsequences containing a larger number of candidate hypernyms fromEk(t0)whereas Pr(Sit0) promotes subsequences consisting of individual edges with a larger probability of hypernymy.\nSubsequence Extraction. After inserting Equations 3 and 4 into Equation 2 and taking logarithm, the objective function becomes:\nG\u0302t0 = argmax \u22c3bi=1 S i t0\nb \u2211 i=1 [\ufe00 log k \u2211 m=1 (\u03bb1)m max j\u2208(\ufe001,n\u230b\ufe00 (sim(Ek(t0,m),hi j))\n+ logPre(t0,hi1) + n\u03bb2 + n\u22121 \u2211 j=1 logPre(hi j ,hi(j+1))\u2309\ufe00\nThis objective function leads to the following search algorithm for the extraction of subsequences:\n(1) For a given term t0, iterate over all candidate hypernyms in Ek(t0). (2) For each hc \u2208 Ek(t0), perform a depth-limited beam search over the space of possible subsequences by recursively exploring the candidate hypernyms of hc (i.e., Ek(hc)). (3) For each hc \u2208 Ek(t0), choose the subsequence S with the highest score (i.e., log(Pr(Ek(t0)\u22c3\ufe00S) \u00d7 Pr(S))). (4) Choose the top-b candidate hypernyms based on their corresponding subsequence scores.\nWhile, in theory, we can iterate over all candidate hypernyms in Ek(t0), in practice, we employ an alternative two-stage execution that significantly improves the running time as well as produces more meaningful subsequences:\n\u25cf Search phase: Proceed as in the aforementioned steps. However, in the special case where a candidate hypernym hc is a compound term and its lexical head lhc is also present in Ek(t0), skip hc in step (1) of the algorithm3. For example, for t0 = apple, candidate hypernyms tech company, software company and hardware company are skipped in step (1) due to the presence of company in Ek(t0) (cf. Table 1).\n\u25cf Expansion phase: In this phase, we augment the subsequences extracted in the search phase to account for skipped compound terms. We focus on the case where the lexical head of the skipped compound terms occurs in a subsequence. In that case, we expand the incoming edge of the lexical head with zero or more of those compound terms. For example, in the subsequence apple\u2192company\u2192organization, a potential expansion of the edge apple\u2192company is: apple\u2192American software company\u2192software company\u2192company. However, special attention has to be taken while generating these potential expansions. For example, the expansion apple\u2192American software company\u2192British software company\u2192company is invalid due to the co-hyponymy edge American software company\u2192British software company. In contrast, the expansion apple\u2192American software company\u2192software company\u2192 company is a valid expansion. To avoid invalid expansions, we restrict the possible expansions to the case where the set of premodifiers of a compound term is a superset of its hypernym\u2019s pre-modifiers (e.g., {American, software }\u2283{software}).\nWe generate all possible expansions for each edge and rank them by averaging a TF-IDF-style metric across the pre-modifiers of compound terms in each expansion. Our aim in the ranking is two-fold: i) promoting the pre-modifiers, which frequently appear in the evidence Ek(t0), and ii) penalizing the noisy pre-modifiers unrelated 3Lexical heads of terms have consistently played a special role in taxonomy induction [10, 31].\nto t0 that frequently occur in compound terms (e.g., several, other, etc.). Hence, we compute the TF score of a pre-modifier as its average frequency of occurrence in the candidate hypernyms Ek(t0). We compute IDF as the average frequency of occurrences of the pre-modifier in Ek(t) for a random term t . Finally, we choose the top ranked expansion per edge.\nTo illustrate the result of the previous steps, we show in Table 2 an example of extracted subsequences along with their expanded versions for the food domain. Intuitively, the two-stage execution serves to distinguish between two fundamentally different forms of generalization: (1) type-based generalization, which provides core types as\ngeneralizations (e.g., apple\u2192company\u2192organization). (2) attribute-based generalization, which enriches type-based\ngeneralization edges. For example, apple\u2192american software company\u2192software company\u2192company enriches the individual type-based edge apple\u2192company.\nIn our experiments, models that distinguished between these two different forms of generalizations consistently performed better than models, which attempted to unify them.\nFeatures. We now describe the edge features that we employ for estimating the probability of a hypernymy relation between two terms (cf. Equation 5): \u25cf Normalized Frequency Diff (nd ): Similar to [28], this feature is an asymmetric hypernymy score based on frequency counts. We compute nd(xi ,x j) by first normalizing the frequency counts obtained (i.e., the counts in Ek(xi)) for term xi as follows:nf (xi ,x j) =\nfreq(xi ,x j) max m freq(xi ,xm) , where freq(xi ,x j) is the frequency count of candidate hypernym x j in Ek(xi). Further, we subtract the score in the opposite direction to downrank synonyms and co-hyponyms: nd(xi ,x j) = nf (xi ,x j) \u2212 nf (x j ,xi). \u25cf Generality Diff (\u0434d ): We introduce a novel feature for explicitly incorporating the term generality (or abstractness) in our model. To this end, we first define the generality \u0434(t) of a term t as the log of the number of distinct hyponyms present in all candidate hypernymy relations (E); i.e., \u0434(t) = log(1+ \u22c3\ufe00x \u22c3\ufe00 x\u2192t \u2208 E\u22c3\ufe00). We define the generality of an edge as the difference in generality between the hypernym and the hyponym: \u0434e(xi ,x j) = \u0434(x j) \u2212 \u0434(xi).\nIntuitively, we aim to promote edges with the right level of generality and penalize edges, which are either too general (e.g., apple\u2192thing) or too specific (i.e., edges between synonyms or cohyponyms, such as apple\u2192orange). To realize this intuition, we first sample a random set of terms and collect the edges with highest nd for these terms (hereafter referred to as top edges). We compare\nthe distribution of generality (i.e., \u0434e ) for the top edges vs. the distribution of generality for a set of randomly sampled edges. The assumption is that it is more likely to sample the generality of a correct edge (i.e., edge at right level of generality) from the distribution of top edges as compared to random edges. Hence, given Dt and Dr as the Gaussian distributions estimated from the samples of generality for top edges and random edges respectively, we define the feature as: \u0434d(xi ,x j) = PrDt (\u0434e(xi ,x j)) \u2212 PrDr (\u0434e(xi ,x j)).\nParameter Tuning. We estimate the weights for features (w in equation 5), using a support vector machine trained on a manually annotated set of 500 edges. For beam search in the search phase, we use a beam of width 20, and limit the search to subsequences of maximum length 4. We set the rest of the parameters by running grid-search over a manually-defined range of parameters using a small validation set4. The final values of parameters are as follows: k=10, b=4, \u03bb1=\u03bb2=0.95."}, {"heading": "2.2 Aggregation of Subsequences", "text": "Up till now, we have described our methodology to generate hypernym subsequences starting from a given term. In this section, we aggregate the hypernym subsequences obtained for a set of seed terms, in order to construct an initial hypernym graph. For that, we undertake the following steps:\nDomain Filtering. Given a term t0, the usual case is that multiple hypernym subsequences corresponding to different senses of the term t0 are extracted. For example, apple can be a company or a fruit, thus resulting in subsequences apple\u2192fruit\u2192food and apple\u2192software company\u2192company. However, many of these subsequences will not pertain to the domain of interest (as determined by the seed terms). To eliminate the irrelevant ones, we estimate a smoothed unigram model5 from all extracted subsequences, and we remove those with generation probabilities below a fixed threshold.\nHypernym Graph Construction. We now aggregate the filtered subsequences into an initial hypernym graph. We construct this graph by grouping the edges with the same start and end nodes together from the filtered subsequences. The weight of each edge is computed as the sum of the scores of subsequences it belongs to (i.e., log( Pr(Ek(t)\u22c3\ufe00S) \u00d7 Pr(S))). To increase the coverage for compound seed terms that do not yet have a hypernym, we simply add an hypernym edge to their lexical head with weight=\u221e (i.e, a very large value) whenever the lexical head is already present in the hypernym graph. Finally, for each cycle in the hypernym graph, we remove the edge with the smallest weight, hence resulting in a DAG. This DAG contains many noisy terms and edges, which are pruned in the next step of our approach."}, {"heading": "2.3 Taxonomy Construction", "text": "In this step, we aim to induce a tree-like taxonomy from the hypernym DAG obtained in the previous step. We cast this as an instance of the minimum-cost flow problem (MCFP).\nMCFP is an optimization problem, which aims to find the cheapest way of sending a certain amount of flow through a flow network. 4Validation set is excluded from the test set. 5We used a weighting function (i.e., step function with cut-off at 50% of the height of the subsequence) to favor terms at lower heights as they are usually more domain-specific.\nIt has been used to find the optimal solution in applications like the transportation problem [17], where the goal is to find the cheapest paths to send commodities from a group of facilities to the customers via a transportation network. Analogously, we cast the problem of taxonomy induction as finding the cheapest way of sending the seed terms to the root terms through a carefully designed flow network F . We use the network simplex algorithm [27] to compute the optimal flow for F , and we select all edges with positive flow as part of our final taxonomy. We now describe our method for constructing the flow network F . In what follows, we refer to Figure 4 at the different steps.\nFlow Network Construction. LetV be the vocabulary of input seed terms (e.g., apple, orange, and Spain in Figure 4); H is the noisy hypernym graph constructed in Section 2.2 (cf. Figure 4(a));w(x ,y) is the weight of the edge x\u2192y in H ; Dx is the set of descendants of term x in H (e.g., apple is a descendant of food); R is the set of given roots6 (e.g., food in Figure 4). The construction of the flow network F proceeds as follows (cf. Figure 4(b)): i) For an edge x\u2192y inH , add the edge x\u2192y in F . Set the capacity (c) of the added edge as c(x ,y) = \u22c3\ufe00Dx \u2229V \u22c3\ufe00. Set the cost (a) of that edge as a(x ,y) = 1\u21d1w(x ,y). ii) Add a sentinel source node s . \u2200v \u2208 V , add an edge s\u2192v with c(s,v) = a(s,v) = 1. iii) Add a sentinel sink node t . \u2200r \u2208 R, add edge r\u2192t with c(r , t) = \u22c3\ufe00Dr \u2229V \u22c3\ufe00 and a(r , t) = 1.\nMinimum-cost Flow. Given a demand d of the total flow to be sent from s to t , the goal of MCFP is to find flow values (f ) for each edge in F that minimize the total cost of flow over all edges: \u2211\n(u,v)\u2208F a(u,v) \u22c5 f (u,v). In our construct, demand d repre-\nsents the maximum number of seed terms that can be included in the final taxonomy. Figures 4(c) and 4(d) show the minimum-cost flow for demand d=3 and d=2 respectively. In both cases, the edge apple\u2192food receives f =0 due to the presence of edges apple\u2192fruit and fruit\u2192food with lower costs. For d=2, the edge source\u2192Spain has f =0, implying that the noisy term Spain would be removed from the final taxonomy. Intuitively, demand d serves as a parameter for discarding potentially noisy terms in the input vocabulary. More formally, d can be defined as \u03b1 \u22c3\ufe00V \u22c3\ufe00, where \u03b1 , a user-defined parameter, indicates the desired coverage over seed terms. If the vocabulary contains only accurate terms, \u03b1 is set to 1. For a given \u03b1 , we run the network simplex algorithm with d=\u03b1 \u22c3\ufe00V \u22c3\ufe00 to compute 6If roots are not provided, a small set of upper terms can be used as roots [38].\nthe minimum-cost flow for F . The final taxonomy consists of all edges with flow > 0."}, {"heading": "3 EVALUATION", "text": "The aim of the empirical evaluation is to address the following questions: \u25cf How does our approach compare to the state-of-the-art ap-\nproaches under the assumption of a clean input vocabulary? \u25cf How does our approach perform on a noisy input vocabulary? \u25cf What are the benefits of extracting longer hypernym subse-\nquences compared to single hypernym edges?\nTo this end, we perform two experiments. In Section 3.1, we compare our taxonomy induction approach against the state of the art, under the simplifying assumption of a clean input vocabulary. Evaluations are performed automatically by computing standard precision, recall and F1 measures against a gold standard.\nWe then drop the simplifying assumption in Section 3.2, where we show that our taxonomy induction performs well even under the presence of significant noise in the input vocabulary. Evaluation is performed both manually as well as automatically against WordNet as the gold standard. We also demonstrate that the subsequencesbased approach significantly outperforms an edges-based variant, thus demonstrating the utility of hypernym subsequences.\nIn the remainder of this section, we use SubSeq to refer to our approach towards taxonomy induction (cf. Section 2)."}, {"heading": "3.1 Evaluation against the State of the Art", "text": "Setup. We use the setting of the SemEval 2016 task for taxonomy extraction [5]. The task provides 6 sets of input terminologies, related to three domains (food, environment and science), for four different languages (English, Dutch, French and Italian). The task requires participants to generate taxonomies for each (terminology, language) pair, which are further evaluated using a variety of techniques, including comparison against a gold standard. Except for a few restricted resources used to construct gold standard, the participants are allowed to use external corpora for hypernymy extraction and taxonomy induction. Participants are compared against each other and against a high-precision string inclusion baseline.\nWe compare SubSeq with TAXI, the system that reached the first place in all subtasks of the SemEval task [28]. TAXI harvests candidate hypernyms using substring inclusion and lexico-syntactic patterns from text corpora. It further utilizes an SVM trained with individual hypernymy edge features, such as frequency counts and\nsubstring inclusion to classify edges as positive and negative. The positive edges are added to the taxonomy. Panchenko et al. [28] also report that alternate configurations of TAXI with different term-level and edge-level features as well as different classifiers such as Logistic Regression, Gradient Boosted Trees, and Random Forest fail to provide improvements over their approach.\nIn contrast to SubSeq, which discovers new hypernyms for the seed terms, SemEval task provides the additional assumption that all the terms in the gold standard taxonomies (i.e., including leaf terms and non-leaf terms) are present in the input vocabulary. This would unfairly lower the performance of SubSeq, as SubSeq would find hypernyms, which are possibly correct but not present in the gold standard. Hence, to ensure a fair comparison, we restrict the subsequence extraction and hypernym graph construction step of SubSeq (cf. Section 2) to candidate hypernyms present in the input vocabulary. Furthermore, since candidate hypernymy extraction is orthogonal to our work, we reuse the candidate hypernymy relations made available by TAXI. As a consequence, TAXI and SubSeq are identical in input data conditions as well as evaluation metrics, and only differ in the core taxonomy induction approach.\nEvaluation Results. Table 3 shows the language-wise precision, recall and F1 values computed against the gold standard for SubSeq and TAXI. Aggregated over all domains, SubSeq outperforms TAXI for all four languages. It achieves >15% relative improvement in F1 for English and 7% improvement overall. Both methods perform significantly better for English, which can be attributed to the higher accuracy of candidate hypernymy relations for English. Figure 5 shows the performance of SubSeq compared to TAXI and the SemEval baseline across different domains and languages. SubSeq performs best for food domain, where it outperforms TAXI across all the languages. SubSeq performs best for English, where it outperforms TAXI across 3/4 domains.\nIn our experiments, we noticed that SubSeq achieves the largest improvements when a greater number of hypernym subsequences are found during the subsequence extraction step. For example, SubSeq achieves an average 32.23% relative improvement in F1 over TAXI for the food domain, where on an average 0.67 subsequences are found per term, compared to only 0.44 for the other domains. Similarly, SubSeq performs best for English datasets, where, on an average, 1.09 subsequences are found per term, compared to only 0.32 for other languages. The variation in the number of extracted subsequences per term can be attributed to two factors: (i) number\nof terms in the input vocabulary, and (ii) number of candidate hypernymy relations available. Due to the assumption that all candidate hypernyms belong to the input vocabulary, larger vocabularies of food domain make it more likely for a candidate hypernym to be found, and hence for a subsequence to be extracted. In a similar fashion, the larger set of available candidate hypernyms for English (\u223c65 million vs. < 2.2 million for other languages) makes it more likely for a subsequence to be extracted for English datasets.\nOverall this experiment shows that under the assumption of a clean input vocabulary, SubSeq is more effective that TAXI for most domains in English, and domains with large vocabularies such as food in other languages."}, {"heading": "3.2 Evaluation with Noisy Vocabulary", "text": "In the previous experiment, we performed taxonomy induction under the simplifying assumption that a clean input vocabulary of relevant domain terms is available. However, as explained in Section 1, in practice, this assumption is rarely satisfied for most domains. Hence, in this experiment, we evaluate the performance of SubSeq in the presence of significant noise in the input vocabulary.\nTAXI is inapplicable in this setting, as it assumes a clean input vocabulary consisting of both leaf and non-leaf terms. Instead, we compare SubSeq against a baseline, which is an edges-based variant of SubSeq.\nSetup. We first build a corpus of relevant documents for the food domain by collecting all English Wikipedia articles with titles matching at least one seed term (post lemmatization) in the SemEval food vocabulary. In total, 1,344 matching Wikipedia articles are found from the initial set of 1,555 seed terms. We run TermSuite [6], a state-of-the-art term extraction approach to extract an initial terminology of 12,645 terms. All terms with occurrence counts < 5 in the corpus are removed, thus resulting in a final terminology of 3,977 terms. The final terminology contains numerous noisy terms that are not food items, such as South Asia and triangular.\nWe now describe the edge-based baseline, hereafter referred to as TopEdge, which extracts individual hypernym edges for terms in the\nvocabulary. TopEdge is identical to SubSeq, except that rather than extracting hypernym subsequences, it extracts direct hypernyms for terms with the highest hypernym probability Pre(x1,x2) (cf. Equation 5). It starts with the seed terms, and recursively extracts hypernyms for terms that do not yet have a hypernym until a fixed number of iterations. The aggregation and taxonomy construction steps are identical to SubSeq (cf. Sections 2.2 and 2.3). Since the only difference between SubSeq and TopEdge is the extraction of hypernym subsequences compared to individual hypernym edges, this experiment also serves to evaluate the utility of extracting hypernym subsequences.\nEvaluation Results. We compare the quality of the taxonomies induced by TopEdge and SubSeq against the sub-hierarchy of WordNet rooted at food as the gold standard. More specifically, we compute two metrics, i.e., term precision and edge precision. Term precision of a taxonomy is computed for the set of the input vocabulary terms retained by the taxonomy as: the ratio of the number of terms in the food sub-hierarchy of WordNet to the total number of terms present in WordNet. Edge precision is computed as the ancestor precision: all nodes from the taxonomy that are not present in the WordNet are removed, and precision is computed on the hypernymy relations from the initial vocabulary to the root7.\nFigures 6 and 7 show the term precision and edge precision for TopEdge and SubSeq taxonomy induction methods for varying values of required coverage, i.e., \u03b1 (cf. Section 2.3). Both Term and edge precision scores for SubSeq are significantly higher than\n7Trivial edges t \u2192food are ignored for all terms t .\nTopEdge across all values of \u03b1 , hence demonstrating the utility of hypernym subsequences. For both methods, precision scores decrease with increase in \u03b1 . This behavior is expected, because as \u03b1 increases additional potentially-noisy seed terms are included in the output taxonomies. Figure 8 shows a section of the SubSeq taxonomy for \u03b1=0.9.\nWe also performed a manual evaluation to judge the quality of the taxonomic edges that are not present in the WordNet. Two authors independently annotated 100 such edges each of TopEdge and SubSeq taxonomies for \u03b1=0.5. The precision for SubSeq was found to be 86% compared to 52% for TopEdge, with a high inter-annotator agreement (0.68). Both evaluations show that the precision of SubSeq taxonomies is quite high, thus demonstrating the efficacy of SubSeq in inducing taxonomies from noisy terminologies.\nWhen \u03b1=1, i.e., all input terms are included in the final taxonomy, term precision is 30%, indicating that only 30% of the terms extracted by the terminology extraction algorithm belong to the WordNet food sub-hierarchy. In contrast, the term precision for the original seed terms provided by SemEval is 75.8%, hence confirming the presence of significant noise in the output of the terminology extraction approach.\nOverall, this experiment demonstrates that SubSeq is an effective approach towards taxonomy induction under the presence of significant noise in input terminologies. It also shows that extraction of hypernym subsequences is beneficial and results in significantly more accurate taxonomies.\nParameter Sensitivity. We now discuss the effect of parameters on the efficacy of subsequence extraction. To this end, we first\nconstruct a gold standard by sampling a set of 100 terms from the food domain randomly and extracting their generalization paths from WordNet. For a set of parameters, we run subsequence extraction and compute the precision and recall averaged over the top-5 paths per term. The parameters we focus on are the: subsequence length (n), number of hypernyms used (k), and rank-penalty (\u03bb1) (cf. Equations 3 and 4).\nFigure 9 shows the precision/recall values for varying values of subsequence lengths (before the expansion phase). Precision decreases and recall increases as the subsequence length increases. This can be intuitively explained by the observation that candidate hypernyms (cf. Table 1) usually only contain hypernyms up to 3/4 levels. Hence, longer subsequences would typically drift from the original term, thus causing loss of precision. Figure 10 shows the effect of the number of candidate hypernyms used (k) for subsequence extraction. As k increases, both precision and recall increase initially, but drop afterwards. This shows the benefit of utilizing lower-ranked hypernyms for subsequence extraction. However, it also illustrates the significant noise present in candidate hypernyms beyond a certain k . Figure 11 shows the effect of rank-penalty (\u03bb1), the parameter used to penalize candidate hypernyms with lower frequency counts. Both precision and recall are low for lower values of \u03bb1 and peak at \u03bb1=0.95.\nWe also evaluated the sensitivity to other parameters. We found out that subsequence extraction is fairly stable across different values of beam width and length penalty (\u03bb2). Moreover, we observed that the number of subsequences per term (b in Equation 3) is also inconsequential beyond a value of 4 as irrelevant subsequences are filtered out by domain filtering (cf. Section 2)."}, {"heading": "4 RELATEDWORK", "text": "Taxonomy induction is a well-studied task, and multiple different lines of work have been proposed in the prior literature. Early work on taxonomy induction aims to extend the existing partial taxonomies (e.g., WordNet) by inserting missing terms at appropriate positions. Widdows [39] places the missing terms in regions with most semantically-similar neighbors. Snow et al. [34] use a probabilistic model to attach novel terms in an incremental greedy fashion, such that the conditional probability of a set of relational evidence given a taxonomy is maximized. Yang and Callan [40] cluster terms incrementally using an ontology metric learnt from a set of heterogeneous features such as co-occurrence, context, and lexico-syntactic patterns.\nA different line of work aims to exploit collaboratively-built semi-structured content such as Wikipedia for inducing large-scale taxonomies. Wikipedia links millions of entities (e.g., Johnny Depp) to a network of inter-connected categories of different granularity (e.g. Hollywood Actors, Celebrities). WikiTaxonomy [29, 30] labels these links as hypernymy or non-hypernymy, using a cascade of heuristics based on the syntactic structure of Wikipedia category labels, the topology of the network and lexico-syntactic patterns for detecting subsumption and meronymy, similar to Hearst patterns [13]. WikiNet [24] extends WikiTaxonomy by expanding nonhypernymy relations into fine-grained relations such as part-of, located-in, etc. YAGO induces a taxonomy by employing heuristics linking Wikipedia categories to corresponding synsets in WordNet\n[14]. More recently, Flati et al. [7] and Gupta et al. [9] propose approaches towardsmultilingual taxonomy induction fromWikipedia, resulting in taxonomies for over 270 languages. However, as pointed out by Hovy et al. [16], these taxonomy induction approaches are non-transferable, i.e., they only work for Wikipedia, because they employ lightweight heuristics that exploit the semi-structured nature of Wikipedia content.\nAlthough taxonomy induction approaches based on external lexical resources achieve high precision, they usually suffer from incomplete coverage over specific domains. To address this issue, another line of work focuses on building lexical taxonomies automatically from a domain-specific corpus or Web. Kozareva and Hovy [19] start from an initial set of root terms and basic level terms and use hearst-like lexico-syntactic patterns recursively to harvest new terms from the Web. Hypernymy relations between terms are induced by searching theWeb again with surface patterns. The graph of extracted hypernyms is subsequently pruned using heuristics based on the out-degree of nodes and the path lengths between terms. Velardi et al. [38] extract hypernymy relations from textual definitions discovered on the Web, and further employ an optimal branching algorithm to induce a taxonomy.\nMore recently, Bordea et al. [4, 5] introduced the first shared tasks on open-domain Taxonomy Extraction, thus providing a common ground for evaluation. INRIASAC, the top system in 2015 task, uses features based on substrings and co-occurrence statistics [8] whereas TAXI, the top system in 2016 task, uses lexico-syntactic patterns, substrings and focused crawling [28].\nIn contrast to taxonomy induction approaches which use external resources, taxonomy induction approaches from a domain corpus or Web typically face two main obstacles. First, they assume the availability of a clean input vocabulary of seed terms. This requirement is not satisfied for most domains, thus requiring a time-consuming manual cleaning of noisy input vocabularies. Second, they ignore the relationship between terms and senses. For example, taxonomies induced from WordNet or Wikipedia produce different hypernyms for each sense of the term apple (e.g., apple is a fruit or a company). To tackle the second obstacle, taxonomy induction approaches from a domain corpus employ domain filtering to perform implicit sense disambiguation. This is done by removing hypernyms corresponding to domain-irrelevant senses of the terms [38]. Although taxonomies should ideally contain senses rather than terms, term taxonomies have shown significant efficacy in a variety of NLP tasks [2, 3, 38].\nTo put it in context, our approach is similar to the previous attempts at inducing taxonomies without using external resources such as WordNet or Wikipedia. One key differentiator, however, is that it is robust to the presence of significant noise in the input vocabulary, thus dealing with the first obstacle above. To deal with the second obstacle, our approach performs implicit sense disambiguation via domain filtering at two different steps: (i) domain filtering of subsequences (cf. Section 2.2); (ii) assigning lower cost for likely in-domain edges when applying the minimum-cost flow optimization (cf. Section 2.2 & 2.3)."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we proposed a novel probabilistic framework for extracting hypernym subsequences from individual hypernymy relations. We also presented a minimum cost-flow optimization approach to taxonomy induction from a noisy hypernym graph. We demonstrated that our subsequence-based approach outperforms state-of-the-art taxonomy induction approaches that utilize individual hypernymy edge features. Unlike previous approaches, our taxonomy induction approach is robust to the significant presence of noise in the input terminology. It also provides a user-defined parameter for controlling the accuracy and coverage of terms and edges in output taxonomies. As a consequence, our approach is applicable to arbitrary domains without any manual intervention, thus truly automating the process of taxonomy induction.\nAcknowledgements. This work is supported by a Sinergia Grant by the Swiss National Science Foundation (SNF 147609). The authors would like to thank Marius Pa\u015fca for helpful discussions."}], "references": [{"title": "Unsupervised learning of an is-a taxonomy from a limited domain-specific corpus", "author": ["Daniele Alfarone", "Jesse Davis"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Structured Learning for Taxonomy Induction with Belief Propagation", "author": ["Mohit Bansal", "David Burkett", "Gerard De Melo", "Dan Klein"], "venue": "ACL", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Ontology Learning from Text: A Survey of Methods", "author": ["Chris Biemann"], "venue": "LDV Forum 20,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Semeval-2015 task 17: Taxonomy Extraction Evaluation (TExEval)", "author": ["Georgeta Bordea", "Paul Buitelaar", "Stefano Faralli", "Roberto Navigli"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Semeval-2016 task 13: Taxonomy Extraction Evaluation (TExEval-2)", "author": ["Georgeta Bordea", "Els Lefever", "Paul Buitelaar"], "venue": "In Proceedings of the 10th International Workshop on Semantic Evaluation", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Termsuite: Terminology extraction with term variant detection", "author": ["Damien Cram", "B\u00e9atrice Daille"], "venue": "ACL 2016 (2016),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "MultiWiBi: The multilingual Wikipedia bitaxonomy project", "author": ["Tiziano Flati", "Daniele Vannella", "Tommaso Pasini", "Roberto Navigli"], "venue": "Artif. Intell", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "INRIASAC: Simple hypernym extraction methods", "author": ["Gregory Grefenstette"], "venue": "arXiv preprint arXiv:1502.01271", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia using Characterlevel Classification", "author": ["Amit Gupta", "R\u00e9mi Lebret", "Hamza Harkous", "Karl Aberer"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Revisiting Taxonomy Induction overWikipedia", "author": ["Amit Gupta", "Francesco Piccinno", "Mikhail Kozhevnikov", "Marius Pasca", "Daniele Pighin"], "venue": "COLING", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Expert-Built and Collaboratively Constructed Lexical Semantic Resources", "author": ["Iryna Gurevych", "Elisabeth Wolf"], "venue": "Language and Linguistics Compass 4,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["Marti A Hearst"], "venue": "In Proceedings of the 14th conference on Computational linguistics-Volume 2. Association for Computational Linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1992}, {"title": "YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia", "author": ["Johannes Hoffart", "Fabian M. Suchanek", "Klaus Berberich", "Gerhard Weikum"], "venue": "Artif. Intell", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Toward completeness in concept extraction and classification", "author": ["Eduard Hovy", "Zornitsa Kozareva", "Ellen Riloff"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Collaboratively built semi-structured content and Artificial Intelligence: The story so far", "author": ["Eduard H. Hovy", "Roberto Navigli", "Simone Paolo Ponzetto"], "venue": "Artif. Intell", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "A primal method for minimal cost flows with applications to the assignment and transportation problems", "author": ["Morton Klein"], "venue": "Management Science 14,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1967}, {"title": "Linked hypernyms dataset-generation framework and use cases. In 3rd Workshop on Linked Data in  Linguistics: Multilingual Knowledge Resources and Natural Language Processing", "author": ["Tom\u00e1\u0161 Kliegr", "V\u00e1clav Zeman", "Milan Dojchinovski"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A semi-supervised method to learn and construct taxonomies using the web", "author": ["Zornitsa Kozareva", "Eduard Hovy"], "venue": "In Proceedings of the 2010 conference on empirical methods in natural language processing. Association for Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs", "author": ["Zornitsa Kozareva", "Ellen Riloff", "Eduard H Hovy"], "venue": "In ACL,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "CYC: A large-scale investment in knowledge infrastructure", "author": ["Douglas B Lenat"], "venue": "Commun. ACM 38,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "WORDNET: A Lexical Database for English. In Human Language Technology, Proceedings of a Workshop held at Plainsboro, New Jerey", "author": ["George A. Miller"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "PATTY: a taxonomy of relational patterns with semantic types", "author": ["Ndapandula Nakashole", "Gerhard Weikum", "Fabian Suchanek"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "WikiNet: A Very Large Scale Multi-Lingual Concept Network", "author": ["Vivi Nastase", "Michael Strube", "Benjamin Boerschinger", "C\u00e4cilia Zirn", "Anas Elghafari"], "venue": "In Proceedings of the International Conference on Language Resources and Evaluation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "A graph-based algorithm for inducing lexical taxonomies from scratch", "author": ["Roberto Navigli", "Paola Velardi", "Stefano Faralli"], "venue": "In IJCAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Using Hearst\u2019s Rules for the Automatic Acquisition of Hyponyms for Mining a Pharmaceutical Corpus", "author": ["Michael P Oakes"], "venue": "In RANLP Text Mining Workshop,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "A polynomial time primal network simplex algorithm for minimum cost flows", "author": ["James B Orlin"], "venue": "Mathematical Programming 78,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "TAXI at SemEval-2016 Task 13: a taxonomy induction method based on lexico-syntactic patterns, substrings and focused crawling", "author": ["Alexander Panchenko", "Stefano Faralli", "Eugen Ruppert", "Steffen Remus", "Hubert Naets", "C\u00e9drick Fairon", "Simone Paolo Ponzetto", "Chris Biemann"], "venue": "Proceedings of SemEval (2016),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Deriving a Large Scale Taxonomy from Wikipedia", "author": ["S. Ponzetto", "M. Strube"], "venue": "In Proceedings of the 22nd National Conference on Artificial Intelligence. Vancouver, British Columbia,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "WikiTaxonomy: A Large Scale Knowledge Resource", "author": ["Simone Paolo Ponzetto", "Michael Strube"], "venue": "In ECAI 2008 - 18th European Conference on Artificial Intelligence,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Taxonomy induction based on a collaboratively built knowledge repository", "author": ["Simone Paolo Ponzetto", "Michael Strube"], "venue": "Artificial Intelligence 175,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Termextractor: a web application to learn the shared terminology of emergent web communities", "author": ["Francesco Sclano", "Paola Velardi"], "venue": "In Enterprise Interoperability II. Springer,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "A Large DataBase of Hypernymy Relations Extracted from the Web", "author": ["Julian Seitner", "Christian Bizer", "Kai Eckert", "Stefano Faralli", "Robert Meusel", "Heiko Paulheim", "Simone Paolo Ponzetto"], "venue": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016, Portoroz\u030c,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Semantic taxonomy induction from heterogenous evidence", "author": ["Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Yago: a core of semantic knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "The singularity is not near: slowing growth of Wikipedia", "author": ["Bongwon Suh", "Gregorio Convertino", "Ed H Chi", "Peter Pirolli"], "venue": "In Proceedings of the 5th International Symposium on Wikis and Open Collaboration. ACM,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "OntoLearn Reloaded: A Graph-Based Algorithm for Taxonomy Induction", "author": ["Paola Velardi", "Stefano Faralli", "Roberto Navigli"], "venue": "Computational Linguistics 39,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Unsupervised methods for developing taxonomies by combining syntactic and statistical information", "author": ["Dominic Widdows"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}, {"title": "A metric-based framework for automatic taxonomy induction", "author": ["Hui Yang", "Jamie Callan"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "Lexical semantic knowledge in the form of term taxonomies has been beneficial in a variety of NLP tasks, including inference, textual entailment, question answering and information extraction [3].", "startOffset": 192, "endOffset": 195}, {"referenceID": 20, "context": "tiple large-scale manual efforts towards taxonomy induction, such asWordNet [22] and Cyc [21].", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "tiple large-scale manual efforts towards taxonomy induction, such asWordNet [22] and Cyc [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "However, such manually constructed taxonomies suffer from low coverage [15] and are unavailable for", "startOffset": 71, "endOffset": 75}, {"referenceID": 3, "context": "Therefore, in recent years, there has been substantial interest in extending existing taxonomies automatically or building new ones [4, 5, 19, 34, 38, 40].", "startOffset": 132, "endOffset": 154}, {"referenceID": 4, "context": "Therefore, in recent years, there has been substantial interest in extending existing taxonomies automatically or building new ones [4, 5, 19, 34, 38, 40].", "startOffset": 132, "endOffset": 154}, {"referenceID": 17, "context": "Therefore, in recent years, there has been substantial interest in extending existing taxonomies automatically or building new ones [4, 5, 19, 34, 38, 40].", "startOffset": 132, "endOffset": 154}, {"referenceID": 32, "context": "Therefore, in recent years, there has been substantial interest in extending existing taxonomies automatically or building new ones [4, 5, 19, 34, 38, 40].", "startOffset": 132, "endOffset": 154}, {"referenceID": 35, "context": "Therefore, in recent years, there has been substantial interest in extending existing taxonomies automatically or building new ones [4, 5, 19, 34, 38, 40].", "startOffset": 132, "endOffset": 154}, {"referenceID": 37, "context": "Therefore, in recent years, there has been substantial interest in extending existing taxonomies automatically or building new ones [4, 5, 19, 34, 38, 40].", "startOffset": 132, "endOffset": 154}, {"referenceID": 11, "context": "In contrast, Pattern-based approaches utilize pre-defined rules or lexico-syntactic patterns to extract terms and hypernymy relations from text [13, 26].", "startOffset": 144, "endOffset": 152}, {"referenceID": 24, "context": "In contrast, Pattern-based approaches utilize pre-defined rules or lexico-syntactic patterns to extract terms and hypernymy relations from text [13, 26].", "startOffset": 144, "endOffset": 152}, {"referenceID": 11, "context": "Patterns are either chosen manually [13, 20] or learnt automatically via bootstrapping [35].", "startOffset": 36, "endOffset": 44}, {"referenceID": 18, "context": "Patterns are either chosen manually [13, 20] or learnt automatically via bootstrapping [35].", "startOffset": 36, "endOffset": 44}, {"referenceID": 32, "context": "Early work on the second stage of taxonomy induction, namely the structured organization of terms into a taxonomy, focused on extending existing partial taxonomies such asWordNet by inserting missing terms at appropriate positions [34, 39, 40].", "startOffset": 231, "endOffset": 243}, {"referenceID": 36, "context": "Early work on the second stage of taxonomy induction, namely the structured organization of terms into a taxonomy, focused on extending existing partial taxonomies such asWordNet by inserting missing terms at appropriate positions [34, 39, 40].", "startOffset": 231, "endOffset": 243}, {"referenceID": 37, "context": "Early work on the second stage of taxonomy induction, namely the structured organization of terms into a taxonomy, focused on extending existing partial taxonomies such asWordNet by inserting missing terms at appropriate positions [34, 39, 40].", "startOffset": 231, "endOffset": 243}, {"referenceID": 0, "context": "corpus or the Web [1, 2, 19, 25, 28, 38].", "startOffset": 18, "endOffset": 40}, {"referenceID": 1, "context": "corpus or the Web [1, 2, 19, 25, 28, 38].", "startOffset": 18, "endOffset": 40}, {"referenceID": 17, "context": "corpus or the Web [1, 2, 19, 25, 28, 38].", "startOffset": 18, "endOffset": 40}, {"referenceID": 23, "context": "corpus or the Web [1, 2, 19, 25, 28, 38].", "startOffset": 18, "endOffset": 40}, {"referenceID": 26, "context": "corpus or the Web [1, 2, 19, 25, 28, 38].", "startOffset": 18, "endOffset": 40}, {"referenceID": 35, "context": "corpus or the Web [1, 2, 19, 25, 28, 38].", "startOffset": 18, "endOffset": 40}, {"referenceID": 23, "context": "Automated taxonomy induction from scratch is preferred because it can be used over arbitrary domains, including highly specific or technical domains, such as Finance or Artificial Intelligence [25].", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "frequent and the most important nouns, adjectives, verbs, and adverbs [11, 23].", "startOffset": 70, "endOffset": 78}, {"referenceID": 21, "context": "frequent and the most important nouns, adjectives, verbs, and adverbs [11, 23].", "startOffset": 70, "endOffset": 78}, {"referenceID": 16, "context": "Similarly,Wikipedia is limited to popular entities [18], and its utility is further diminished by slowed growth [37].", "startOffset": 51, "endOffset": 55}, {"referenceID": 34, "context": "Similarly,Wikipedia is limited to popular entities [18], and its utility is further diminished by slowed growth [37].", "startOffset": 112, "endOffset": 116}, {"referenceID": 26, "context": "Past approaches to taxonomy induction from scratch either assume the availability of a clean input vocabulary [28] or employ a time-consuming manual cleaning step over a noisy input vocabulary [38].", "startOffset": 110, "endOffset": 114}, {"referenceID": 35, "context": "Past approaches to taxonomy induction from scratch either assume the availability of a clean input vocabulary [28] or employ a time-consuming manual cleaning step over a noisy input vocabulary [38].", "startOffset": 193, "endOffset": 197}, {"referenceID": 35, "context": "Figure 1: Traditional process for taxonomy induction from a domain-specific corpus [38].", "startOffset": 83, "endOffset": 87}, {"referenceID": 35, "context": "taxonomy induction approach from a domain corpus [38].", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": "tial noisy vocabulary is automatically extracted from the domain corpus using a term extraction tool, such as TermExtractor [32], and is further cleaned manually to produce the final vocabulary.", "startOffset": 124, "endOffset": 128}, {"referenceID": 31, "context": "A prominent example of such a resource is WebIsA [33], a collection of more than 400 million hypernymy relations for English, extracted from the CommonCrawl web corpus using lexico-syntactic patterns.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "Unsupervised or semi-supervised approaches to taxonomy induction typically aim to extract single hypernym edges among terms from noisy candidate hypernyms [19, 28].", "startOffset": 155, "endOffset": 163}, {"referenceID": 26, "context": "Unsupervised or semi-supervised approaches to taxonomy induction typically aim to extract single hypernym edges among terms from noisy candidate hypernyms [19, 28].", "startOffset": 155, "endOffset": 163}, {"referenceID": 35, "context": "[38], who demonstrated that hypernym extraction becomes increasingly erroneous as the generality of terms increases, mainly due to the increase in term ambiguity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "3Lexical heads of terms have consistently played a special role in taxonomy induction [10, 31].", "startOffset": 86, "endOffset": 94}, {"referenceID": 29, "context": "3Lexical heads of terms have consistently played a special role in taxonomy induction [10, 31].", "startOffset": 86, "endOffset": 94}, {"referenceID": 26, "context": "Equation 5): \u25cf Normalized Frequency Diff (nd ): Similar to [28], this feature is an asymmetric hypernymy score based on frequency counts.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "It has been used to find the optimal solution in applications like the transportation problem [17], where the goal is to find the cheapest paths to send commodities from a group of facilities to the customers via a transportation network.", "startOffset": 94, "endOffset": 98}, {"referenceID": 25, "context": "We use the network simplex algorithm [27] to compute the optimal flow for F , and we select all edges with positive flow as part of our final taxonomy.", "startOffset": 37, "endOffset": 41}, {"referenceID": 35, "context": "6If roots are not provided, a small set of upper terms can be used as roots [38].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "We use the setting of the SemEval 2016 task for taxonomy extraction [5].", "startOffset": 68, "endOffset": 71}, {"referenceID": 26, "context": "first place in all subtasks of the SemEval task [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 26, "context": "[28] also report that alternate configurations of TAXI with different term-level and edge-level features as well as different classifiers such as Logistic Regression, Gradient Boosted Trees, and Random Forest fail to provide improvements over their approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We run TermSuite [6], a state-of-the-art term extraction approach to extract an initial terminology of 12,645 terms.", "startOffset": 17, "endOffset": 20}, {"referenceID": 36, "context": "Widdows [39] places the missing terms in regions with most semantically-similar neighbors.", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": "[34] use a probabilistic model to attach novel terms in an incremental greedy fashion, such that the conditional probability of a set of relational evidence given a taxonomy is maximized.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Yang and Callan [40]", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "WikiTaxonomy [29, 30] labels these links as hypernymy or non-hypernymy, using a cascade of", "startOffset": 13, "endOffset": 21}, {"referenceID": 28, "context": "WikiTaxonomy [29, 30] labels these links as hypernymy or non-hypernymy, using a cascade of", "startOffset": 13, "endOffset": 21}, {"referenceID": 11, "context": "heuristics based on the syntactic structure of Wikipedia category labels, the topology of the network and lexico-syntactic patterns for detecting subsumption and meronymy, similar to Hearst patterns [13].", "startOffset": 199, "endOffset": 203}, {"referenceID": 22, "context": "WikiNet [24] extends WikiTaxonomy by expanding non-", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "YAGO induces a taxonomy by employing heuristics linking Wikipedia categories to corresponding synsets in WordNet [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "[7] and Gupta et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] propose approaches towardsmultilingual taxonomy induction fromWikipedia,", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[16], these taxonomy induction approaches are non-transferable, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Kozareva and Hovy [19] start from an initial set of root terms and basic level terms and use hearst-like lexico-syntactic patterns recursively to harvest new terms from the Web.", "startOffset": 18, "endOffset": 22}, {"referenceID": 35, "context": "[38] extract hypernymy relations from textual definitions discovered on the Web, and further employ an optimal branching algorithm to induce a taxonomy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4, 5] introduced the first shared tasks on open-domain Taxonomy Extraction, thus providing a common ground for evaluation.", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[4, 5] introduced the first shared tasks on open-domain Taxonomy Extraction, thus providing a common ground for evaluation.", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "INRIASAC, the top system in 2015 task, uses features based on substrings and co-occurrence statistics [8] whereas TAXI, the top system in 2016 task, uses lexico-syntactic patterns, substrings and focused crawling [28].", "startOffset": 102, "endOffset": 105}, {"referenceID": 26, "context": "INRIASAC, the top system in 2015 task, uses features based on substrings and co-occurrence statistics [8] whereas TAXI, the top system in 2016 task, uses lexico-syntactic patterns, substrings and focused crawling [28].", "startOffset": 213, "endOffset": 217}, {"referenceID": 35, "context": "removing hypernyms corresponding to domain-irrelevant senses of the terms [38].", "startOffset": 74, "endOffset": 78}, {"referenceID": 1, "context": "Although taxonomies should ideally contain senses rather than terms, term taxonomies have shown significant efficacy in a variety of NLP tasks [2, 3, 38].", "startOffset": 143, "endOffset": 153}, {"referenceID": 2, "context": "Although taxonomies should ideally contain senses rather than terms, term taxonomies have shown significant efficacy in a variety of NLP tasks [2, 3, 38].", "startOffset": 143, "endOffset": 153}, {"referenceID": 35, "context": "Although taxonomies should ideally contain senses rather than terms, term taxonomies have shown significant efficacy in a variety of NLP tasks [2, 3, 38].", "startOffset": 143, "endOffset": 153}], "year": 2017, "abstractText": "We propose a novel, semi-supervised approach towards domain taxonomy induction from an input vocabulary of seed terms. Unlike all previous approaches, which typically extract direct hypernym edges for terms, our approach utilizes a novel probabilistic framework to extract hypernym subsequences. Taxonomy induction from extracted subsequences is cast as an instance of the minimumcost flow problem on a carefully designed directed graph. Through experiments, we demonstrate that our approach outperforms stateof-the-art taxonomy induction approaches across four languages. Importantly, we also show that our approach is robust to the presence of noise in the input vocabulary. To the best of our knowledge, this robustness has not been empirically proven in any previous approach.", "creator": "LaTeX with hyperref package"}}}