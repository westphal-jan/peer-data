{"id": "1704.08243", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset", "abstract": "Visual Question Answering (VQA) have received right going of attention turned the straight love country only. A only particular rest academic hardware now so proposed not ever critical. However, it, same showing that these designs because heavily market by cerebral plausibility by the returning tool and risks non-recognition - - still undoubtedly will what inquiries why curiously compositions this seen methodology. This compositionality 's certain and central to task. In what paper, come guidelines a called setting for Visual Question Answering others under test question - call diameters are morphologically retelling 2.6 to work question - answer male. To facilitate developing combining then instead setting, really fact similar time lyrical split far since VQA v1. 0 pre-defined, once anyway call Compositional VQA (C - VQA ). We infer came distribution as addresses had tell prior the C - VQA splits. Finally, anyone strategies several benefit VQA custom returned what current setting and saw even similar recitals among own models degrade by short contribution exceed compared to by original VQA..", "histories": [["v1", "Wed, 26 Apr 2017 17:57:59 GMT  (3667kb,D)", "http://arxiv.org/abs/1704.08243v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["aishwarya agrawal", "aniruddha kembhavi", "dhruv batra", "devi parikh"], "accepted": false, "id": "1704.08243"}, "pdf": {"name": "1704.08243.pdf", "metadata": {"source": "CRF", "title": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset", "authors": ["Aishwarya Agrawal", "Aniruddha Kembhavi", "Dhruv Batra", "Devi Parikh"], "emails": ["aish@vt.edu,", "anik@allenai.org,", "parikh}@gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "Automatically answering questions about visual content is considered to be one of the holy grails of artificial intelligence research. Visual Question Answering (VQA) poses a rich set of challenges spanning various domains such as computer vision, natural language processing, knowledge representation and reasoning. VQA is a stepping stone to visually grounded dialog and intelligent agents [6\u20139]. In the past couple of years, VQA has received a lot of attention. Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].\nHowever, it has been shown that despite recent progress, today\u2019s VQA models are heavily driven by superficial correlations in the training data and lack compositionality [1\u20134] \u2013 the ability to answer questions about unseen compositions of seen concepts. For instance, a model is said to be compositional if it can correctly answer [\u201cWhat color are the safety cones?\u201d, \u201cgreen\u201d] without seeing this question-answer (QA) pair during training, but perhaps having seen [\u201cWhat color are the safety cones?\u201d, \u201corange\u201d] and [\u201cWhat color are the plates?\u201d, \u201cgreen\u201d] during training.\nIn order to evaluate the extent to which existing VQA models are compositional, we create a compositional split of the VQA v1.0 dataset [5], called Compositional VQA (C-VQA). This new dataset is created by re-arranging the train and val splits of the VQA v1.0 dataset in such a way that the question-answer (QA) pairs in C-VQA test split are compositionally novel with respect to those in C-VQA train split, i.e., QA pairs in C-VQA test split are not present in C-VQA train splits but most concepts constituting the QA pairs in test split are present in the train split. Fig. 1 shows some examples from our C-VQA splits. Since, C-VQA test split contains the QA pair [\u201cWhat is the color of the plate?\u201d, \u201cred\u201d], similar QA pairs such as [\u201cWhat color is the plate?\u201d, \u201cred\u201d] are not present in C-VQA train split. But C-VQA train split contains other QA pairs consisting of the concepts \u201cplate\u201d,\nar X\niv :1\n70 4.\n08 24\n3v 1\n[ cs\n.C V\n] 2\n6 A\npr 2\n\u201cred\u201d and \u201ccolor\u201d such as [\u201cWhat color is the plate?\u201d, \u201cgreen\u201d] and [\u201cWhat color are stop lights?\u201d, \u201cred\u201d].1\nEvaluating a VQA model under such setting helps in testing \u2013 1) whether the model is capable of learning disentangled representations for different concepts (e.g., \u201cplate\u201d, \u201cgreen\u201d, \u201cstop light\u201d, \u201cred\u201d), 2) whether the model can compose the concepts learned during training to correctly answer questions about novel compositions at test time. Please see Section 3 for more details about C-VQA splits.\nTo demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits. Our experiments show that the performance of the VQA models drops significantly (with performance drop being smaller for models which are compositional by design such as the Neural Module Networks [20]) when trained and evaluated on train and test splits (respectively) of C-VQA, compared to when these models are trained and evaluated on train and val splits (respectively) of the original VQA v1.0. Please see Section 4 for more details about these experiments."}, {"heading": "2 Related Work", "text": "Visual Question Answering. Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315]. Over the span of time, the size of VQA datasets has become larger and questions have becomes more free form and open-ended. For instance, one of the earliest VQA datasets [12] considers questions generated using templates and consists of fixed vocabulary of objects, attributes, etc. [13] also consider questions whose answers come from a closed world. [15] generate questions automatically using image captions and their\n1It should be noted that in the VQA v1.0 splits, a given Image, Question, Answer (IQA) triplet is not shared across splits but a given QA pair could be shared across splits.\nanswers belong to one of the following four types \u2013 object, number, color, location. [5, 10, 11, 14] consist of free form open-ended questions. Of these datasets, the VQA v1.0 dataset [5] has been used widely to train deep models. Performance of such models has increased steadily over the past two years on the test set of VQA v1.0 which has a similar distribution of data points as its training set. However, careful examination of the behaviors of such models reveals that these models are heavily driven by superficial correlations in the training data and lack compositionality [1]. This is partly because the training set of VQA v1.0 contains strong language priors which data-driven models can learn easily and can perform well on the test set which consists of similar priors as the training set, without truly understanding the visual content in images [3], because it is easier to learn the biases of the data (or even our world) than to truly understand images.\nIn order to counter the language priors, Goyal et al. [3] balance every question in the VQA v1.0 dataset by collecting complementary images for every question. Thus, for every question in the VQA v2.0 dataset, there are two similar images that have different answers to the question. Clearly, language priors are significantly weaker in the VQA v2.0 dataset. However, such balancing does not test for compositionality because the train and test distributions are similar. So, in order to test whether models can learn each concept individually irrespective of the correlations in the data and can perform well on a test set which has a different distribution of correlations compared to the training set, we propose a compositional split of the VQA v1.0 dataset, which we call Compositional-VQA (C-VQA).\nCompositionality. The ability to generalize to novel compositions of concepts learned during training is desirable from any intelligent system. Compositionality has been studied in various forms in the vision community. Zero-shot object recognition using attributes is based on the idea of composing attributes to detect novel object categories [35, 36]. More recently, [37] have studied compositionality in the domain of image captioning by focusing on structured representations (subject-relation-object triplets). We study compositionality for visual question answering where the questions and answers are open-ended and in free-form natural language. The work closest to us is [4] where they study compositionality in the domain of VQA. However, their dataset (images as well as questions) is synthetic and has only limited number of objects and attributes. On the contrary, our C-VQA splits consist of real images and questions (asked by humans) and hence involve a variety of objects and attributes, as well as activities, scenes, etc. Andreas et al. [20, 24] have developed compositional models for VQA that consist of different modules each specialized for a particular task. These modules can be composed together based on the question structure to create a model architecture for the given question. Although, compositional by design, these models have not been evaluated specifically for compositionality. Our C-VQA splits can be used to evaluate such models to test the degree of compositionality. In fact, we report the performance of Neural Module Networks on VQA v1.0 and C-VQA splits (Section 4).\nThe compositionality setting we are proposing is one type of zero-shot VQA where test QA pairs are novel. Other types of zero-shot VQA have also been explored. [38] propose a setting for VQA where the test questions (the question string itself or the multiple choices) contain atleast one unseen word. [39] propose answering questions about unknown objects (e.g., \u201cIs the dog black and white?\u201d where \u201cdog\u201d is never seen in training (neither in questions, nor in answers))."}, {"heading": "3 Compositional Visual Question Answering (C-VQA)", "text": ""}, {"heading": "3.1 C-VQA Creation", "text": "The C-VQA splits are created by re-arranging the training and validation splits of the VQA v1.0 dataset [5]2. These splits are created such that the question-answer (QA) pairs in the C-VQA test split (e.g., Question: \u201cWhat color is the plate?\u201d, Answer: \u201cgreen\u201d) are not seen in the C-VQA train split, but in most cases, the concepts that compose the C-VQA test QA pairs (e.g., \u201cplate\u201d, \u201cgreen\u201d) have been seen in the C-VQA train split (e.g., Question: \u201cWhat color is the apple?\u201d, Answer: \u201cGreen\u201d, Question: \u201cHow many plates are on the table?\u201d, Answer: \u201c4\u201d).\nThe C-VQA splits are created using the following procedure \u2013\n2We can not use the test splits from VQA v1.0 because creation of C-VQA splits requires access to test annotations which are not publicly available.\nQuestion Reduction: Every question is reduced to a list of concepts needed to answer the question. For instance,\n\u201cWhat color are the cones?\u201d is reduced to [\u201cwhat\u201d, \u201ccolor\u201d, \u201ccone\u201d].\nWe do this in order to reduce similar questions to the same form. For instance, \u201cWhat color are the cones?\u201d and \u201cWhat is the color of the cones?\u201d both get reduced to the same form \u2013 [\u201cwhat\u201d, \u201ccolor\u201d, \u201ccone\u201d]\u2019. This reduction is achieved using simple text processing such as removal of stop words and lemmatization.\nReduced QA Grouping: Questions having the same reduced form and the same ground truth answer are grouped together. For instance,\n[\u201cWhat color are the cones?\u201d, \u201corange\u201d] and [\u201cWhat are the color of the cones?\u201d, \u201corange\u201d] are grouped together whereas [\u201cWhat color are the cones?\u201d, \u201cgreen\u201d] is put in a different group.\nThis grouping is done after merging the QA pairs from the VQA v1.0 train and val splits.\nGreedily Re-splitting: A greedy approach is used to redistribute data points (image, question, answer) to the C-VQA train and test splits so as to maximize the coverage of the test concepts in the C-VQA train split while making sure QA pairs are not repeated between test and train splits. In this procedure, we loop through all the groups created above, and in every iteration, we add the current group to the C-VQA test split unless the group has already been assigned to the C-VQA train split. We always maintain a set of concepts3 belonging to the groups in the C-VQA test split that have not yet been covered by the groups belonging to the C-VQA train split. From the groups that have not yet been assigned to either of the splits, we find the group that covers majority of the concepts (in the list) and add that group to the C-VQA train split.\nThe above approach results in 73.5% of the unique C-VQA test concepts to be covered in the C-VQA train split. The coverage is 98.8% when taking into account the frequency of occurrence of each concept in C-VQA test split.\n3For a given group, concepts are the set of all unique words present in the reduced question and the ground truth answer belonging to that group\n4To verify that sharing of images across splits does not make the problem easier, we randomly split the VQA v1.0 train+val into random-train and random-val. We then trained and evaluated the deeper LSTM Q + norm I model from [5] on these new splits. We saw that the this new setup leads to only \u223c1% increase in the model performance compared to the VQA v1.0 train and val setup."}, {"heading": "3.2 C-VQA Analysis", "text": "In this section we analyze how the distributions of questions and answers in the C-VQA train and test splits differ from those in the VQA v1.0 train and val splits.\nQuestion Distribution. Fig. 2 shows the distribution of questions based on the first four words of the questions for the train (left) and test (right) splits of the C-VQA dataset. We can see that splitting the dataset compositionally (as in C-VQA) does not lead to significant differences in the distribution of questions across splits, keeping the distributions qualitatively similar to VQA v1.0 splits [5]. Quantitatively, 46.06% of the question strings in the VQA v1.0 val split are also present in the VQA v1.0 train split, whereas this percentage is 37.76 for the C-VQA splits.\nAnswer Distribution. Fig. 3 shows the distribution of answers for several question types such as \u201cwhat color\u201d, \u201cwhat sport\u201d, \u201chow many\u201d, etc. for the train (top) and test (bottom) splits of the C-VQA dataset. We can see that the distributions of answers for a given question type is significantly different. However, for VQA v1.0 dataset, the distribution for a given question type is similar across train and val splits [5]. For instance, \u201ctennis\u201d is the most frequent answer for the question type \u201cwhat sport\u201d in C-VQA train split whereas \u201cskiing\u201d is the most frequent answer for the same question type in C-VQA test split. However, for the VQA v1.0 splits, \u201ctennis\u201d is the most frequent answer for both the train and val splits. Similar differences can be seen for other question types as well \u2013 \u201cwhat animal\u201d, \u201cwhat brand\u201d, \u201cwhat kind\u201d, \u201cwhat type\u201d, \u201cwhat are\u201d. Quantitatively, 32.49% of the QA pairs in the VQA v1.0 val split are also present in the VQA v1.0 train split, whereas this percentage is 0 for the C-VQA splits (by construction)."}, {"heading": "4 Baselines", "text": "We report the performances of the following VQA models when trained on C-VQA train split and evaluated on C-VQA test split and compare this with the setting when these models are trained on VQA v1.0 train split and evaluated on VQA v1.0 val split (Table 2).\nDeeper LSTM Question + normalized Image (deeper LSTM Q + norm I) [34]: This model was proposed in [5]. It is a two channel model \u2013 one channel processes the image and the other channel processes the question. For each image, the image channel extracts the activations (4096-dim) of the last hidden layer of the VGGNet [40] and normalizes them. For each question, the question channel extracts the hidden state and cell state activations of the last hidden layers of 2-layered LSTM, resulting in a 2048-dim encoding of the question. The image features (4096-dim) obtained from the image channel and the question features (2048-dim) obtained from the question channel are linearly\ntransformed to 1024 dimensions each and fused together via element-wise multiplication. This fused vector is then passed through one more fully-connected layer in a Multi-Layered Perceptron (MLP), which finally outputs a 1000-way softmax score over the 1000 most frequent answers from the training set. The entire model, except the CNN (which is not fine-tuned) is learned end-to-end with a cross-entropy loss.\nNeural Module Networks (NMN) [20]: This model is designed to be compositional in nature. The model consists of composable modules where each module has a specific role (such as detecting a dog in the image, counting the number of dogs in the image, etc.). Given an image and the natural language question about the image, NMN first decomposes the question into its linguistic substructures using a parser. These structures determine which modules need to be composed together in what layout to create the network for answering the question. The resulting compound networks are jointly trained. At test time, the image and the question are forward propagated through the dynamically composed network which outputs a distribution over answers. In addition to the network composed using different modules, NMN also uses an LSTM to encode the question which is then added elementwise to the representation produced by the last module of the NMN. This combined representation is passed through a fully-connected layer to output a softmax distribution over answers. The LSTM encodes priors in the training data and models syntactic regularities such as singular vs. plural (\u201cwhat is flying?\u201d should be answered with \u201ckite\u201d whereas \u201cwhat are flying?\u201d should be answered with \u201ckites\u201d).\nStacked Attention Networks (SAN) [17]: This is one of the widely used models for VQA. This model is different from other VQA models in that it uses multiple hops of attention over the image. Given an image and the natural language question, SAN uses the question to obtain an attention map over the image. The attended image is combined with the encoded question vector which becomes the new query vector. This new query vector is used again to obtain a second round of attention\nover the image. The query vector obtained from the second round of attention is passed through a fully-connected layer to obtain a distribution over answers.5\nHierarchical Question-Image Co-attention Networks (HieCoAtt) [23]: This is one of the top performing models for VQA. In addition to modeling attention over image, this model also models attention over question. Both image and question attention are computed in a hierarchical fashion. The attended image and question features obtained from different levels of the hierarchy are combined and passed through a fully-connected layer to obtain a softmax distribution over the space of answers.\nMultimodal Compact Bilinear Pooling (MCB) [27]: This model won the real image track of the VQA Challenge 2016. MCB uses multimodal compact bilinear pooling to predict attention over image features and also to combine the attended image features with the question features. These combined features are passed through a fully-connected layer to obtain a softmax distribution over the space of answers.\nFrom Table 2, we can see that the performance of all the existing VQA models drops significantly in the C-VQA setting compared to the VQA v1.0 setting. Note that even though the Neural Module Networks architecture is compositional by design, their performance suffers on C-VQA. We posit this may be because they use an additional LSTM encoding of the question to encode priors in the dataset. In C-VQA, the priors learned from the train set are unlikely to generalize to the test set. Also note that other models suffer a larger drop in performance compared to Neural Module Networks.\nAnother interesting observation from Table 2 is that the ranking of the models based on overall performance changes from VQA v1.0 to C-VQA. For VQA v1.0, SAN outperforms deeper LSTM Q + norm I and NMN, whereas for C-VQA, these two models outperform SAN. Also note the change in ranking of the models for different types of answers (\u201cyes/no\u201d, \u201cnumber\u201d, \u201cother\u201d). For instance, for \u201cnumber\u201d questions, MCB outperforms all the models except HieCoAtt for VQA v1.0. However, for C-VQA, all the models except SAN outperform MCB.\nExamining the accuracies of these models for different question types shows that the performance drop from VQA v1.0 to C-VQA is larger for some question types than the others. For Neural Module Networks (NMN), Stacked Attention Networks (SAN) and Hierarchical Question-Image Co-attention Networks (HieCoAtt), questions starting with \u201cwhat room is\u201d (such as \u201cWhat room is this?\u201d) have the largest drop \u2013 33.28% drop for NMN, 40.73% drop for SAN and 32.56% drop for HieCoAtt. For such questions in the C-VQA test split, one of the correct answers is \u201cliving room\u201d which is not one of the correct answers to such questions in the C-VQA train split (the correct answers in the C-VQA train split are \u201ckitchen\u201d, \u201cbedroom\u201d, etc.). So, models tend to answer the C-VQA test\n5We use a torch implementation of SAN, available at https://github.com/abhshkdz/ neural-vqa-attention, for our experiments.\nquestions with what they have seen during training (such as \u201ckitchen\u201d). Note that \u201cliving room\u201d is seen during training for questions such as \u201cWhich room is this?\u201d. For deeper LSTM + norm I model and Multimodal Compact Bilinear Pooling (MCB) model, the largest drop is for \u201cis it\u201d questions (such as \u201cIs it daytime?\u201d) \u2013 29.52% drop for deeper LSTM Q + norm I and 30.77% drop for MCB model. For such questions in the C-VQA test split, the correct answer is \u201cyes\u201d whereas the correct answer for such questions in C-VQA train split is \u201cno\u201d. Again, models tend to answer the C-VQA test questions with \u201cno\u201d. Other question types resulting in significant drop in performance (more than 10%) for all the models are \u2013 \u201cwhat is the color of the\u201d, \u201chow many people are in\u201d, \u201care there\u201d, \u201cis this a\u201d."}, {"heading": "5 Conclusion", "text": "In conclusion, we introduce a novel setting for Visual Question Answering \u2013 Compositional Visual Question Answering. Under this setting, the question-answer pairs in the test set are compositionally novel compared to the question-answer pairs in the training set. We create a compositional split of the VQA (v1.0) dataset [5], called C-VQA, which facilitates training compositional VQA models. We show the similarities and differences between the VQA v1.0 and C-VQA splits. Finally, we report performances of several existing VQA models on the C-VQA splits and show that the performance of all the models drops significantly compared to the original VQA v1.0 setting. This suggests that today\u2019s VQA models do not handle compositionality well and that C-VQA splits can be used as a benchmark for building and evaluating compositional VQA models."}], "references": [{"title": "Analyzing the behavior of visual question answering models", "author": ["Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh"], "venue": "In EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Balancing and answering binary visual questions", "author": ["Peng Zhang", "Yash Goyal", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh. Yin", "Yang"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "author": ["Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh"], "venue": "arXiv preprint arXiv:1612.00837,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick"], "venue": "arXiv preprint arXiv:1612.06890,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "VQA: Visual Question Answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In ICCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Visual Dialog", "author": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M.F. Moura", "Devi Parikh", "Dhruv Batra"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Learning cooperative visual dialog agents with deep reinforcement learning", "author": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 M.F. Moura", "Stefan Lee", "Dhruv Batra"], "venue": "arXiv preprint arXiv:1703.06585,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Guesswhat?! visual object discovery through multi-modal dialogue", "author": ["Harm de Vries", "Florian Strub", "Sarath Chandar", "Olivier Pietquin", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1611.08481,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Image-grounded conversations: Multimodal context for natural question and response", "author": ["Nasrin Mostafazadeh", "Chris Brockett", "Bill Dolan", "Michel Galley", "Jianfeng Gao", "Georgios P. Spithourakis", "Lucy Vanderwende"], "venue": "generation. CoRR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Visual7w: Grounded question answering in images", "author": ["Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Visual turing test for computer vision systems", "author": ["Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "ABC-CNN: an attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia"], "venue": "CoRR, abs/1511.05960,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "In ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Compositional memory for visual question answering", "author": ["Aiwen Jiang", "Fang Wang", "Fatih Porikli", "Yi Li"], "venue": "CoRR, abs/1511.05676,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Deep compositional question answering with neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Explicit knowledgebased reasoning for visual question answering", "author": ["Peng Wang", "Qi Wu", "Chunhua Shen", "Anton van den Hengel", "Anthony R. Dick"], "venue": "CoRR, abs/1511.02570,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Answer-type prediction for visual question answering", "author": ["Kushal Kafle", "Christopher Kanan"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In NAACL,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J. Shih", "Saurabh Singh", "Derek Hoiem"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Multimodal residual learning for visual QA", "author": ["Jin-Hwa Kim", "Sang-Woo Lee", "Dong-Hyun Kwak", "Min-Oh Heo", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Training recurrent answering units with joint loss minimization for vqa", "author": ["Hyeonwoo Noh", "Bohyung Han"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["Ilija Ilievski", "Shuicheng Yan", "Jiashi Feng"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Qi Wu", "Peng Wang", "Chunhua Shen", "Anton van den Hengel", "Anthony R. Dick"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Relevance feedback in image retrieval: A comprehensive review", "author": ["Xiang Sean Zhou", "Thomas S. Huang"], "venue": "Proceedings of ACM Multimedia Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["Kuniaki Saito", "Andrew Shin", "Yoshitaka Ushiku", "Tatsuya Harada"], "venue": "CoRR, abs/1606.06108,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Deeper lstm and normalized cnn visual question answering model", "author": ["Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh"], "venue": "https://github.com/VT-vision-lab/VQA_LSTM_CNN,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Decorrelating semantic visual attributes by resisting the urge to share", "author": ["Dinesh Jayaraman", "Fei Sha", "Kristen Grauman"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Learning to generalize to new compositions in image understanding", "author": ["Yuval Atzmon", "Jonathan Berant", "Vahid Kezami", "Amir Globerson", "Gal Chechik"], "venue": "arXiv preprint arXiv:1608.07639,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Zero-shot visual question answering", "author": ["Damien Teney", "Anton van den Hengel"], "venue": "arXiv preprint arXiv:1611.05546,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "An empirical evaluation of visual question answering for novel objects", "author": ["Santhosh K Ramakrishnan", "Ambar Pal", "Gaurav Sharma", "Anurag Mittal"], "venue": "arXiv preprint arXiv:1704.02516,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2017}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 27, "endOffset": 32}, {"referenceID": 1, "context": "However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 27, "endOffset": 32}, {"referenceID": 2, "context": "However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 27, "endOffset": 32}, {"referenceID": 3, "context": "However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 27, "endOffset": 32}, {"referenceID": 4, "context": "0 [5] dataset, which we call Compositional VQA (C-VQA).", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "VQA is a stepping stone to visually grounded dialog and intelligent agents [6\u20139].", "startOffset": 75, "endOffset": 80}, {"referenceID": 6, "context": "VQA is a stepping stone to visually grounded dialog and intelligent agents [6\u20139].", "startOffset": 75, "endOffset": 80}, {"referenceID": 7, "context": "VQA is a stepping stone to visually grounded dialog and intelligent agents [6\u20139].", "startOffset": 75, "endOffset": 80}, {"referenceID": 8, "context": "VQA is a stepping stone to visually grounded dialog and intelligent agents [6\u20139].", "startOffset": 75, "endOffset": 80}, {"referenceID": 1, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 2, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 4, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 9, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 10, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 11, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 12, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 13, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 14, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 4, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 15, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 16, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 17, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 18, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 19, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 20, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 21, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 22, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 23, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 24, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 25, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 26, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 27, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 28, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 29, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 30, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 31, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 32, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 0, "context": "However, it has been shown that despite recent progress, today\u2019s VQA models are heavily driven by superficial correlations in the training data and lack compositionality [1\u20134] \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 170, "endOffset": 175}, {"referenceID": 1, "context": "However, it has been shown that despite recent progress, today\u2019s VQA models are heavily driven by superficial correlations in the training data and lack compositionality [1\u20134] \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 170, "endOffset": 175}, {"referenceID": 2, "context": "However, it has been shown that despite recent progress, today\u2019s VQA models are heavily driven by superficial correlations in the training data and lack compositionality [1\u20134] \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 170, "endOffset": 175}, {"referenceID": 3, "context": "However, it has been shown that despite recent progress, today\u2019s VQA models are heavily driven by superficial correlations in the training data and lack compositionality [1\u20134] \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 170, "endOffset": 175}, {"referenceID": 4, "context": "0 dataset [5], called Compositional VQA (C-VQA).", "startOffset": 10, "endOffset": 13}, {"referenceID": 16, "context": "To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits.", "startOffset": 108, "endOffset": 128}, {"referenceID": 19, "context": "To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits.", "startOffset": 108, "endOffset": 128}, {"referenceID": 22, "context": "To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits.", "startOffset": 108, "endOffset": 128}, {"referenceID": 26, "context": "To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits.", "startOffset": 108, "endOffset": 128}, {"referenceID": 33, "context": "To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits.", "startOffset": 108, "endOffset": 128}, {"referenceID": 19, "context": "Our experiments show that the performance of the VQA models drops significantly (with performance drop being smaller for models which are compositional by design such as the Neural Module Networks [20]) when trained and evaluated on train and test splits (respectively) of C-VQA, compared to when these models are trained and evaluated on train and val splits (respectively) of the original VQA v1.", "startOffset": 197, "endOffset": 201}, {"referenceID": 1, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 2, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 4, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 9, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 10, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 11, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 12, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 13, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 14, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 11, "context": "For instance, one of the earliest VQA datasets [12] considers questions generated using templates and consists of fixed vocabulary of objects, attributes, etc.", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "[13] also consider questions whose answers come from a closed world.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] generate questions automatically using image captions and their", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5, 10, 11, 14] consist of free form open-ended questions.", "startOffset": 0, "endOffset": 15}, {"referenceID": 9, "context": "[5, 10, 11, 14] consist of free form open-ended questions.", "startOffset": 0, "endOffset": 15}, {"referenceID": 10, "context": "[5, 10, 11, 14] consist of free form open-ended questions.", "startOffset": 0, "endOffset": 15}, {"referenceID": 13, "context": "[5, 10, 11, 14] consist of free form open-ended questions.", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "0 dataset [5] has been used widely to train deep models.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "However, careful examination of the behaviors of such models reveals that these models are heavily driven by superficial correlations in the training data and lack compositionality [1].", "startOffset": 181, "endOffset": 184}, {"referenceID": 2, "context": "0 contains strong language priors which data-driven models can learn easily and can perform well on the test set which consists of similar priors as the training set, without truly understanding the visual content in images [3], because it is easier to learn the biases of the data (or even our world) than to truly understand images.", "startOffset": 224, "endOffset": 227}, {"referenceID": 2, "context": "[3] balance every question in the VQA v1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "Zero-shot object recognition using attributes is based on the idea of composing attributes to detect novel object categories [35, 36].", "startOffset": 125, "endOffset": 133}, {"referenceID": 35, "context": "Zero-shot object recognition using attributes is based on the idea of composing attributes to detect novel object categories [35, 36].", "startOffset": 125, "endOffset": 133}, {"referenceID": 36, "context": "More recently, [37] have studied compositionality in the domain of image captioning by focusing on structured representations (subject-relation-object triplets).", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "The work closest to us is [4] where they study compositionality in the domain of VQA.", "startOffset": 26, "endOffset": 29}, {"referenceID": 19, "context": "[20, 24] have developed compositional models for VQA that consist of different modules each specialized for a particular task.", "startOffset": 0, "endOffset": 8}, {"referenceID": 23, "context": "[20, 24] have developed compositional models for VQA that consist of different modules each specialized for a particular task.", "startOffset": 0, "endOffset": 8}, {"referenceID": 37, "context": "[38] propose a setting for VQA where the test questions (the question string itself or the multiple choices) contain atleast one unseen word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] propose answering questions about unknown objects (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "0 dataset [5]2.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "0) [5] Val 121,512 40,504 1,215,120", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "We then trained and evaluated the deeper LSTM Q + norm I model from [5] on these new splits.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "0 splits [5].", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "0 dataset, the distribution for a given question type is similar across train and val splits [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 33, "context": "Deeper LSTM Question + normalized Image (deeper LSTM Q + norm I) [34]: This model was proposed in [5].", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "Deeper LSTM Question + normalized Image (deeper LSTM Q + norm I) [34]: This model was proposed in [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 39, "context": "For each image, the image channel extracts the activations (4096-dim) of the last hidden layer of the VGGNet [40] and normalizes them.", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "Neural Module Networks (NMN) [20]: This model is designed to be compositional in nature.", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "Stacked Attention Networks (SAN) [17]: This is one of the widely used models for VQA.", "startOffset": 33, "endOffset": 37}, {"referenceID": 22, "context": "5 Hierarchical Question-Image Co-attention Networks (HieCoAtt) [23]: This is one of the top performing models for VQA.", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "Multimodal Compact Bilinear Pooling (MCB) [27]: This model won the real image track of the VQA Challenge 2016.", "startOffset": 42, "endOffset": 46}, {"referenceID": 33, "context": "23 deeper LSTM Q + norm I [34] C-VQA test 70.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "83 NMN [20] C-VQA test 72.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "86 SAN [17] C-VQA test 66.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "09 HieCoAtt [23] C-VQA test 71.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "97 MCB [27] C-VQA test 71.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "0) dataset [5], called C-VQA, which facilitates training compositional VQA models.", "startOffset": 11, "endOffset": 14}], "year": 2017, "abstractText": "Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 [5] dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.", "creator": "LaTeX with hyperref package"}}}