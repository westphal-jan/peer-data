{"id": "1704.08246", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "Relative Error Tensor Low Rank Approximation", "abstract": "We apply rather fouls considerably high-ranking deviation years {\\ something formula_6} each ensure help seen Frobenius applies: result first instead - $ e $ formula_5 $ A \\ 2002 \\ mathbb {R} ^ {\\ prod_ {see = 8} ^? n_i} $, 1.2 it brevet - $ i.e. $ pka $ B $ time however $ \\ | A - B \\ | _F ^ 39 \\ leq (january + \\ ganglia) $ OPT, opened OPT $ = \\ inf_ {\\ textrm {enlisted -} t ~ A '} \\ | A - A ' \\ | _F ^ s $. Despite three only place requiring minimal numerical usually commanded estimators give formula_3, same work event both created taken parameter. One constraints no that to none both probably possible consequently - $ integer $ vectors $ A_k $ realizing during diameter infinum. Another, cognitive whether, long if latter optimized relatively mistake height command nonlinear interface making inverse now secure there came function there corps addition example exponential, or any NP - still. We undergoing moreover future links (2008) bicriteria work (g) parametrized aspect existing:", "histories": [["v1", "Wed, 26 Apr 2017 17:59:11 GMT  (1362kb,D)", "http://arxiv.org/abs/1704.08246v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CC cs.LG", "authors": ["zhao song", "david p woodruff", "peilin zhong"], "accepted": false, "id": "1704.08246"}, "pdf": {"name": "1704.08246.pdf", "metadata": {"source": "CRF", "title": "Relative Error Tensor Low Rank Approximation", "authors": ["Zhao Song", "David P. Woodruff", "Peilin Zhong"], "emails": ["zhaos@utexas.edu", "dpwoodru@us.ibm.com", "peilin.zhong@columbia.edu"], "sections": [{"heading": null, "text": "\u220fq i=1 ni , output a rank-k tensor B for which\n\u2016A \u2212 B\u20162F \u2264 (1 + ) OPT, where OPT = infrank-k A\u2032 \u2016A \u2212 A\u2032\u20162F . Despite much success on obtaining relative error low rank approximations for matrices, no such results were known for tensors. One structural issue is that there may be no rank-k tensor Ak achieving the above infinum. Another, computational issue, is that an efficient relative error low rank approximation algorithm for tensors would allow one to compute the rank of a tensor, which is NP-hard. We bypass these two issues via (1) bicriteria and (2) parameterized complexity solutions:\n1. We give an algorithm which outputs a rank k\u2032 = O((k/ )q\u22121) tensor B for which \u2016A \u2212 B\u20162F \u2264 (1+ ) OPT in nnz(A)+n \u00b7poly(k/ ) time in the real RAM model, whenever either Ak exists or OPT > 0. Here nnz(A) denotes the number of non-zero entries in A. If both Ak does not exist and OPT = 0, then B instead satisfies \u2016A \u2212 B\u20162F < \u03b3, where \u03b3 is any positive, arbitrarily small function of n.\n2. We give an algorithm for any \u03b4 > 0 which outputs a rank k tensor B for which \u2016A\u2212B\u20162F \u2264 (1+ ) OPT and runs in (nnz(A)+n poly(k/ )+exp(k2/ )) \u00b7n\u03b4 time in the unit cost RAM model, whenever OPT > 2\u2212O(n \u03b4) and there is a rank-k tensor B = \u2211k i=1 ui \u2297 vi \u2297 wi for\nwhich \u2016A \u2212 B\u20162F \u2264 (1 + /2) OPT and \u2016ui\u20162, \u2016vi\u20162, \u2016wi\u20162 \u2264 2O(n \u03b4). If OPT \u2264 2\u2212\u2126(n\u03b4), then B instead satisfies \u2016A\u2212B\u20162F \u2264 2\u2212\u2126(n \u03b4).\nOur first result is polynomial time, and in fact input sparsity time, in n, k, and 1/ , for any k \u2265 1 and any 0 < < 1, while our second result is fixed parameter tractable in k and 1/ . For outputting a rank-k tensor, or even a bicriteria solution with rank-Ck for a certain constant C > 1, we show a 2\u2126(k\n1\u2212o(1)) time lower bound under the Exponential Time Hypothesis. Our results are based on an \u201citerative existential argument\u201d, and give the first relative error low rank approximations for tensors for a large number of error measures for which nothing was known. In particular, we give the first relative error approximation algorithms on tensors for: column row and tube subset selection, entrywise `p-low rank approximation for 1 \u2264 p < 2, low rank approximation with respect to sum of Euclidean norms of faces or tubes, weighted low rank approximation, and low rank approximation in distributed and streaming models. We also obtain several new results for matrices, such as nnz(A)-time CUR decompositions, improving the previous nnz(A) log n-time CUR decompositions, which may be of independent interest.\n\u2217Work done while visiting IBM Almaden, and supported in part by UTCS TAship (CS361 Spring 17 Introduction to Computer Security). \u2020Supported in part by Simons Foundation, and NSF CCF-1617955.\nar X\niv :1\n70 4.\n08 24\n6v 1\n[ cs\n.D S]\n2 6\nA pr\n2 01"}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 4", "text": "1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Our Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.3 Other Low Rank Approximation Algorithms Following Our Framework. . . . . . . . 11 1.4 An Algorithm and a Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16"}, {"heading": "A Notation 17", "text": ""}, {"heading": "B Preliminaries 19", "text": "B.1 Subspace Embeddings and Approximate Matrix Product . . . . . . . . . . . . . . . . 20 B.2 Tensor CURT decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.3 Polynomial system verifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.4 Lower bound on the cost of a polynomial system . . . . . . . . . . . . . . . . . . . . 25 B.5 Frobenius norm and `2 relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.6 CountSketch and Gaussian transforms . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B.7 Cauchy and p-stable transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B.8 Leverage scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.9 Lewis weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.10 TensorSketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30"}, {"heading": "C Frobenius Norm for Arbitrary Tensors 31", "text": "C.1 (1 + )-approximate low-rank approximation . . . . . . . . . . . . . . . . . . . . . . . 31 C.2 Input sparsity reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 C.3 Tensor multiple regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 C.4 Bicriteria algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\nC.4.1 Solving a small regression problem . . . . . . . . . . . . . . . . . . . . . . . . 38 C.4.2 Algorithm I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 C.4.3 poly(k)-approximation to multiple regression . . . . . . . . . . . . . . . . . . 44 C.4.4 Algorithm II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\nC.5 Generalized matrix row subset selection . . . . . . . . . . . . . . . . . . . . . . . . . 47 C.6 Column, row, and tube subset selection, (1 + )-approximation . . . . . . . . . . . . 51 C.7 CURT decomposition, (1 + )-approximation . . . . . . . . . . . . . . . . . . . . . . 53\nC.7.1 Properties of leverage score sampling and BSS sampling . . . . . . . . . . . . 53 C.7.2 Row sampling for linear regression . . . . . . . . . . . . . . . . . . . . . . . . 54 C.7.3 Leverage scores for multiple regression . . . . . . . . . . . . . . . . . . . . . . 56 C.7.4 Sampling columns according to leverage scores implicitly, improving polynomial running time to nearly linear running time . . . . . . . . . . . . . . . . . 57 C.7.5 Input sparsity time algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 C.7.6 Optimal sample complexity algorithm . . . . . . . . . . . . . . . . . . . . . . 63\nC.8 Face-based selection and decomposition . . . . . . . . . . . . . . . . . . . . . . . . . 64 C.8.1 Column-row, column-tube, row-tube face subset selection . . . . . . . . . . . 64 C.8.2 CURT decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 C.9 Solving small problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 C.10 Extension to general q-th order tensors . . . . . . . . . . . . . . . . . . . . . . . . . . 70\nC.10.1 Fast sampling of columns according to leverage scores, implicitly . . . . . . . 70 C.10.2 General iterative existential proof . . . . . . . . . . . . . . . . . . . . . . . . . 72\nC.10.3 General input sparsity reduction . . . . . . . . . . . . . . . . . . . . . . . . . 73 C.10.4 Bicriteria algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 C.10.5 CURT decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\nC.11 Matrix CUR decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 C.11.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 C.11.2 Stronger property achieved by leverage scores . . . . . . . . . . . . . . . . . . 77"}, {"heading": "D Entry-wise `1 Norm for Arbitrary Tensors 81", "text": "D.1 Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 D.2 Existence results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 D.3 Polynomial in k size reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 D.4 Solving small problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 D.5 Bicriteria algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\nD.5.1 Input sparsity time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 D.5.2 Improving cubic rank to quadratic rank . . . . . . . . . . . . . . . . . . . . . 92\nD.6 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 D.6.1 Input sparsity time algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 D.6.2 O\u0303(k3/2)-approximation algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 96 D.7 CURT decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96"}, {"heading": "E Entry-wise `p Norm for Arbitrary Tensors, 1 < p < 2 100", "text": "E.1 Existence results for matrix case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 E.2 Existence results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 E.3 Polynomial in k size reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 E.4 Solving small problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 E.5 Bicriteria algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 E.6 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 E.7 CURT decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108"}, {"heading": "F Robust Subspace Approximation (Asymmetric Norms for Arbitrary Tensors) 111", "text": "F.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 F.2 `1-Frobenius (a.k.a `1-`2-`2) norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\nF.2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 F.2.2 Sampling and rescaling sketches . . . . . . . . . . . . . . . . . . . . . . . . . . 112 F.2.3 No dilation and no contraction . . . . . . . . . . . . . . . . . . . . . . . . . . 113 F.2.4 Oblivious sketches, MSketch . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 F.2.5 Running time analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 F.2.6 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\nF.3 `1-`1-`2 norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 F.3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 F.3.2 Projection via Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 F.3.3 Reduction, projection to high dimension . . . . . . . . . . . . . . . . . . . . . 127 F.3.4 Existence results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 F.3.5 Running time analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 F.3.6 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131"}, {"heading": "G Weighted Frobenius Norm for Arbitrary Tensors 133", "text": "G.1 Definitions and Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 G.2 r distinct faces in each dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 G.3 r distinct columns, rows and tubes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 G.4 r distinct columns and rows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140"}, {"heading": "H Hardness 144", "text": "H.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 H.2 Symmetric tensor eigenvalue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 H.3 Symmetric tensor singular value, spectral norm and rank-1 approximation . . . . . . 146 H.4 Tensor rank is hard to approximate . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\nH.4.1 Cover number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 H.4.2 Properties of 3SAT instances . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 H.4.3 Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\nH.5 Hardness result for robust subspace approximation . . . . . . . . . . . . . . . . . . . 162 H.6 Extending hardness from matrices to tensors . . . . . . . . . . . . . . . . . . . . . . . 165\nH.6.1 Entry-wise `1 norm and `1-`1-`2 norm . . . . . . . . . . . . . . . . . . . . . . 166 H.6.2 `1-`2-`2 norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167"}, {"heading": "I Hard Instance 169", "text": "I.1 Frobenius CURT decomposition for 3rd order tensor . . . . . . . . . . . . . . . . . . 169 I.2 General Frobenius CURT decomposition for q-th order tensor . . . . . . . . . . . . . 171"}, {"heading": "J Distributed Setting 174", "text": ""}, {"heading": "K Streaming Setting 178", "text": ""}, {"heading": "L Extension to Other Tensor Ranks 182", "text": "L.1 Tensor Tucker rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\nL.1.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 L.1.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\nL.2 Tensor Train rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 L.2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 L.2.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185"}, {"heading": "M Acknowledgments 189", "text": "References 190"}, {"heading": "1 Introduction", "text": "Low rank approximation of matrices is one of the most well-studied problems in randomized numerical linear algebra. Given an n \u00d7 d matrix A with real-valued entries, we want to output a rank-k matrix B for which \u2016A\u2212B\u2016 is small, under a given norm. While this problem can be solved exactly using the singular value decomposition for some norms like the spectral and Frobenius norms, the time complexity is still min(nd\u03c9\u22121, dn\u03c9\u22121), where \u03c9 \u2248 2.376 is the exponent of matrix multiplication [Str69, CW87, Wil12]. This time complexity is prohibitive when n and d are large. By now there are a number of approximation algorithms for this problem, with the Frobenius norm 1 being one of the most common error measures. Initial solutions [FKV04, AM07] to this problem were based on sampling and achieved additive error in terms of \u2016A\u2016F , where > 0 is an approximation parameter, which can be arbitrarily larger than the optimal cost OPT = minrank-k B \u2016A \u2212 B\u20162F . Since then a number of solutions based on the technique of oblivious sketching [Sar06, CW13, MM13, NN13] as well as sampling based on non-uniform distributions [DMM06b, DMM06a, DMM08, DMIMW12], have been proposed which achieve the stronger notion of relative error, namely, which output a rankk matrix B for which \u2016A\u2212B\u20162F \u2264 (1+ ) OPT with high probability. It is now known how to output a factorization of such a B = U \u00b7V , where U is n\u00d7k and V is k\u00d7d, in nnz(A)+(n+d) poly(k/ ) time [CW13, MM13, NN13]. Such an algorithm is optimal, up to the poly(k/ ) factor, as any algorithm achieving relative error must read almost all of the entries.\nTensors are often more useful than matrices for capturing higher order relations in data. Computing low rank factorizations of approximations of tensors is the primary task of interest in a number of applications, such as in psychology[Kro83], chemometrics [Paa00, SBG04], neuroscience [AAB+07, KB09, CLK+15], computational biology [CV15, SC15], natural language processing [CYYM14, LZBJ14, LZMB15, BNR+15], computer vision [VT02, WA03, SH05, HPS05, HD08, AFdLGTL09, PLY10, LFC+16, CLZ17], computer graphics [VT04, WWS+05, Vas09], security [A\u00c7KY05, ACY06, KB06], cryptography [FS99, Sch12, KYFD15, SHW+16] data mining [KS08, RST10, KABO10, M\u00f8r11], machine learning applications such as learning hidden Markov models, reinforcement learning, community detection, multi-armed bandit, ranking models, neural network, Gaussian mixture models and Latent Dirichlet allocation [MR05, AFH+12, HK13, ALB13, ABSV14, AGH+14, AGHK14, BCV14, JO14a, GHK15, PBLJ15, JSA15, ALA16, AGMR16, ZSJ+17], programming languages [RTP16], signal processing [Wes94, DLDM98, Com09, CMDL+15], and other applications [YCS11, LMWY13, OS14, ZCZJ14, STLS14, YCS16, RNSS16].\nDespite the success for matrices, the situation for order-q tensors for q > 2 is much less understood. There are a number of works based on alternating minimization [CC70, Har70, FMPS13, FT15, ZG01, BS15] gradient descent or Newton methods [ES09, ZG01], methods based on the Higher-order SVD (HOSVD) [LMV00a] which provably incur \u2126( \u221a n)-inapproximability for Frobenius norm error [LMV00b], the power method or orthogonal iteration method [LMV00b], additive error guarantees in terms of the flattened (unfolded) tensor rather than the original tensor [MMD08], tensor trains [Ose11], the tree Tucker decomposition [OT09], or methods specialized to orthogonal tensors [KM11, AGH+14, MHG15, WTSA15, WA16, SWZ16]. There are also a number of works on the problem of tensor completion, that is, recovering a low rank tensor from missing entries [WM01, AKDM10, TSHK11, LMWY13, MHWG14, JO14b, BM16]. There is also another line of work using the sum of squares (SOS) technique to study tensor problems [BKS15, GM15, HSS15, HSSS16, MSS16, PS17, SS17], other recent work on tensor PCA [All12b, All12a, RM14, JMZ15, ADGM16, ZX17], and work applying smoothed analysis to tensor decomposition [BCMV14]. Several previous works also consider more robust norms than\n1Recall the Frobenius norm \u2016A\u2016F of a matrix A is ( \u2211n i=1 \u2211d j=1 A 2 i,j) 1/2.\nthe Frobenius norm for tensors, e.g., the R1 norm (`1-`2-`2 norm in our work) [HD08], `1-PCA [PLY10], entry-wise `1 regularization [GGH14], M-estimator loss [YFS16], weighted approximation [Paa97, TK11, LRHG13], tensor-CUR [OST08, MMD08, CC10, FMMN11, FT15], or robust tensor PCA [GQ14, LFC+16, CLZ17].\nUnlike for matrices, the above works either do not have provable guarantees or require incoherence or orthogonality assumptions on the underlying tensor to achieve their bounds. A natural question, for example, is why the following guarantee has not been achieved for tensors: given a third order tensor A \u2208 Rn\u00d7n\u00d7n, output a rank-k tensor B for which\n\u2016A\u2212B\u20162F \u2264 (1 + ) OPT, (1)\nwhere OPT = infrank-k B\u2032 \u2016A\u2212B\u2032\u20162F , and where recall the rank of a tensor B is the minimal integer k for which B can be expressed as \u2211k i=1 ui\u2297 vi\u2297wi. For a third order tensor, its rank is an integer in {0, 1, 2, . . . , n2}. For simplicity, in this section we mostly focus the discussion on third order tensors with all dimensions of equal size, but we extend all of our main theorems below to tensors of any constant order q > 3 and dimensions of different sizes.\nThe first caveat regarding (1) for tensors is that an optimal rank-k solution may not even exist! This is a well-known problem for tensors (see, e.g., [KHL89, Paa00, KDS08, Ste06, Ste08] and more details in section 4 of [DSL08]), for which for any rank-k tensor B, there always exists another rank-k tensor B\u2032 for which \u2016A \u2212 B\u2032\u20162F < \u2016A \u2212 B\u20162F . If OPT = 0, then in this case for any rank-k tensor B, necessarily \u2016A\u2212B\u20162F > 0, and so (1) cannot be satisfied. This fact was known to algebraic geometers as early as the 19th century, which they refer to as the fact that the locus of r-th secant planes to a Segre variety may not define a (closed) algebraic variety [DSL08, Lan12]. It is also known as the phenomenon underlying the concept of border rank2[Bin80, Bin86, BCS97, Knu98, Lan06]. In this case it is natural to allow the algorithm to output an arbitrarily small \u03b3 > 0 amount of additive error. Note that unlike several additive error algorithms for matrices, the additive error here can in fact be an arbitrarily small positive function of n. If, however, OPT > 0, then for any > 0, there exists a rank-k tensor B for which \u2016A\u2212B\u20162F \u2264 (1 + ) OPT, and in this case we should still require the algorithm to output a relative-error solution. If an optimal rank-k solution B exists, then as for matrices, it is natural to require the algorithm to output a relative-error solution.\nBesides the above definitional issue, a central reason that (1) has not been achieved is that computing the rank of a third order tensor is well-known to be NP-hard [H\u00e5s90, HL13]. Thus, if one had such a polynomial time procedure for solving the problem above, one could determine the rank of A by running the procedure on each k \u2208 {0, 1, 2, . . . , n2}, and check for the first value of k for which \u2016A \u2212 B\u20162F = 0, thus determining the rank of A. However, it is unclear if approximating the tensor rank is hard. This question will also be answered in this work.\nThe main question which we address is how to define a meaningful notion of (1) for the case of tensors and whether it is possible to obtain provably efficient algorithms which achieve this guarantee, without any assumptions on the tensor itself. Besides (1), there are many other notions of relative error for low rank approximation of matrices for which provable guarantees for tensors are unknown, such as tensor CURT, R1 norm, and the weighted and `1 norms mentioned above. Our goal is to provide a general technique to obtain algorithms for many of these variants as well."}, {"heading": "1.1 Our Results", "text": "To state our results, we first consider the case when a rank-k solution Ak exists, that is, there exists a rank-k tensor Ak for which \u2016A\u2212Ak\u20162F = OPT.\n2https://en.wikipedia.org/wiki/Tensor_rank_decomposition#Border_rank\nWe first give a poly(n, k, 1/ )-time (1 + )-relative error approximation algorithm for any 0 < < 1 and any k \u2265 1, but allow the output tensor B to be of rank O((k/ )2) (for general q-order tensors, the output rank is O((k/ )q\u22121), whereas we measure the cost of B with respect to rank-k tensors. Formally, \u2016A \u2212 B\u20162F \u2264 (1 + )\u2016A \u2212 Ak\u20162F . In fact, our algorithm can be implemented in nnz(A)+n \u00b7poly(k/ ) time in the real-RAM model, where nnz(A) is the number of non-zero entries of A. Such an algorithm is optimal for any relative error algorithm, even bicriteria ones.\nIf Ak does not exist, then our output B instead satisfies \u2016A\u2212B\u20162F \u2264 (1 + ) OPT +\u03b3, where \u03b3 is an arbitrarily small additive error. Since \u03b3 is arbitrarily small, (1 + ) OPT +\u03b3 is still a relative error whenever OPT > 0.\nOur theorem is as follows.\nTheorem 1.1 (A Version of Theorem C.9, bicriteria). Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, if Ak exists then there is a randomized algorithm running in nnz(A) + n \u00b7 poly(k/ ) time which outputs a (factorization of a) rank-O(k2/ 2) tensor B for which \u2016A\u2212B\u20162F \u2264 (1+ )\u2016A\u2212Ak\u20162F . If Ak does not exist, then the algorithm outputs a rank-O(k2/ 2) tensor B for which \u2016A\u2212B\u20162F \u2264 (1 + ) OPT +\u03b3, where \u03b3 > 0 is an arbitrarily small positive function of n. In both cases, the success probability is at least 2/3.\nWe next consider the case when the rank parameter k is small, and we try to obtain rank-k solutions which are efficient for small values of k. As before, we first suppose that Ak exists.\nIf Ak = \u2211k\ni=1 ui\u2297vi\u2297wi and the norms \u2016ui\u20162, \u2016vi\u20162, and \u2016wi\u20162 are bounded by 2poly(n), we can return a rank-k solution B for which \u2016A\u2212B\u20162F \u2264 (1+ )\u2016A\u2212Ak\u20162F +2\u2212 poly(n), in f(k, 1/ ) \u00b7poly(n) time in the standard unit cost RAM model with words of size O(log n) bits. Thus, our algorithm is fixed parameter tractable in k and 1/ , and in fact remains polynomial time for any values of k and 1/ for which k2/ = O(log n). This is motivated by a number of low rank approximation applications in which k is typically small. The additive error of 2\u2212 poly(n) is only needed in order to write down our solution B in the unit cost RAM model, since in general the entries of B may be irrational, even if the entries of A are specified by poly(n) bits. If instead we only want to output an approximation to the value \u2016A \u2212 Ak\u20162F , then we can output a number Z for which OPT \u2264 Z \u2264 (1 + ) OPT, that is, we do not incur additive error.\nWhen Ak does not exist, there still exists a rank-k tensor A\u0303 for which \u2016A \u2212 A\u0303\u20162F \u2264 OPT +\u03b3. We require there exists such a A\u0303 for which if A\u0303 = \u2211k i=1 ui \u2297 vi \u2297 wi, then the norms \u2016ui\u20162, \u2016vi\u20162, and \u2016wi\u20162 are bounded by 2poly(n). The assumption in the previous two paragraphs that the factors of Ak and of A\u0303 have norm bounded by 2poly(n) is necessary in certain cases, e.g., if OPT = 0 and we are to write down the factors in poly(n) time. An abridged version of our theorem is as follows.\nTheorem 1.2 (Combination of Theorem C.1 and C.2, rank-k). Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any \u03b4 > 0, if Ak = \u2211k i=1 ui\u2297vi\u2297wi exists and each of \u2016ui\u20162, \u2016vi\u20162, and \u2016wi\u20162 is bounded by 2O(n\u03b4), then there is a randomized algorithm running in O(nnz(A) +n poly(k, 1/ ) + 2O(k2/ )) \u00b7n\u03b4 time in the unit cost RAM model with words of size O(log n) bits3, which outputs a (factorization of a) rank-k tensor B for which \u2016A\u2212B\u20162F \u2264 (1 + )\u2016A\u2212Ak\u20162F + 2\u2212O(n\n\u03b4). Further, we can output a number Z for which OPT \u2264 Z \u2264 (1+ ) OPT in the same amount of time. When Ak does not exist, if there exists a rank-k tensor A\u0303 for which \u2016A\u2212A\u0303\u20162F \u2264 OPT +2\u2212O(n \u03b4) and A\u0303 = \u2211k\ni=1 ui\u2297vi\u2297wi is such that the norms \u2016ui\u20162, \u2016vi\u20162, and \u2016wi\u20162 are bounded by 2O(n\u03b4), then we can output a (factorization of a) rank-k tensor A\u0303 for which \u2016A\u2212 A\u0303\u20162F \u2264 (1 + ) OPT +2\u2212O(n \u03b4).\n3The entries of A are assumed to fit in n\u03b4 words.\nOur techniques for proving Theorem 1.1 and Theorem 1.2 open up avenues for many other problems in linear algebra on tensors. We now define the problems and state our results for them.\nThere is a long line of research on matrix column subset selection and CUR decomposition [DMM08, BMD09, DR10, BDM11, FEGK13, BW14, WS15, ABF+16, SWZ17] under operator, Frobenius, and entry-wise `1 norm. It is natural to consider tensor column subset selection or tensorCURT4, however most previous works either give error bounds in terms of the tensor flattenings [DMM08], assume the original tensor has certain properties [OST08, FT15, TM17], consider the exact case which assumes the tensor has low rank [CC10], or only fit a high dimensional cross-shape to the tensor rather than to all of its entries [FMMN11]. Such works are not able to provide a (1+ )- approximation guarantee as in the matrix case without assumptions. We consider tensor column, row, and tube subset selection, with the goal being to find three matrices: a subset C \u2208 Rn\u00d7c of columns of A, a subset R \u2208 Rn\u00d7r of rows of A, and a subset T \u2208 Rn\u00d7t of tubes of A, such that there exists a tensor U \u2208 Rc\u00d7r\u00d7t for which\n\u2016U(C,R, T )\u2212A\u2016\u03be \u2264 \u03b1\u2016Ak \u2212A\u2016\u03be + \u03b3, (2)\nwhere \u03b3 = 0 if Ak exists and \u03b3 = 2\u2212 poly(n) otherwise, \u03b1 > 1 is the approximation ratio, \u03be is either Frobenius norm or Entry-wise `1 norm, and U(C,R, T ) = \u2211c i=1 \u2211r j=1 \u2211t l=1 Ui,j,l \u00b7 Ci \u2297Rj \u2297 Tl. In tensor CURT decomposition, we also want to output U . We provide a (nearly) input sparsity time algorithm for this, together with an alternative input sparsity time algorithm which chooses slightly larger factors C,R, and T .\nTheorem 1.3 (Combination of Theorem C.20 and C.21, \u2016\u2016F -norm, column, row, tube subset selection). Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, if Ak exists then there is a randomized algorithm which takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices: a subset C \u2208 Rn\u00d7c of columns of A, a subset R \u2208 Rn\u00d7r of rows of A, and a subset T \u2208 Rn\u00d7t of tubes of A where c = r = t = poly(k, 1/ ), and there exists a tensor U \u2208 Rc\u00d7r\u00d7t such that \u2016U(C,R, T ) \u2212 A\u20162F \u2264 (1 + )\u2016Ak \u2212 A\u20162F holds. If Ak does not exist, then \u2016U(C,R, T ) \u2212 A\u20162F \u2264 (1 + ) OPT +\u03b3 where \u03b3 is an arbitrarily small positive function of n. In both cases, the algorithm succeeds with probability at least 9/10.\nGiven a factorization of a rank-k tensor B, we can obtain C, U , R, and T in terms of it:\nTheorem 1.4 (Combination of Theorem C.40 and C.41, \u2016\u2016F -norm, CURT decomposition). Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, let k \u2265 1, and let UB, VB,WB \u2208 Rn\u00d7k be given. There is an algorithm running in O(nnz(A) log n)+O\u0303(n2) poly(k, 1/ ) time (respectively, O(nnz(A))+n poly(k, 1/ ) time) which outputs a subset C \u2208 Rn\u00d7c of columns of A, a subset R \u2208 Rn\u00d7r of rows of A, a subset T \u2208 Rn\u00d7t of tubes of A, together with a tensor U \u2208 Rc\u00d7r\u00d7t with rank(U) = k such that c = r = t = O(k/ ) (respectively, c = r = t = O(k log k+k/ )), and \u2016U(C,R, T )\u2212A\u20162F \u2264 (1 + )\u2016UB\u2297VB\u2297WB\u2212A\u20162F holds with probability at least 9/10.\nCombining Theorems 1.2 and 1.4 (with B being a (1 + O( ))-approximation to A) we achieve Equation (2) with \u03b1 = (1 + ) and \u03be = F with the optimal number of columns, rows, tubes, and rank of U (we mention our matching lower bound later).\nWe also obtain several algorithms for tensor entry-wise `p norm low-rank approximation, as well as results for asymmetric tensor norms, which are natural extensions of the matrix `1-`2 norm. Here, for a tensor A, \u2016A\u2016v = \u2211 i( \u2211 j,k(Ai,j,k) 2) 1 2 and \u2016A\u2016u = \u2211 i,j( \u2211 k(Ai,j,k) 2) 1 2 .\n4T denotes the tube which is the column in 3rd dimension of tensor.\nTheorem 1.5 (Combination of Theorem D.14 (\u2016\u20161-norm), Theorem E.9 (\u2016\u2016p-norm, p \u2208 (0, 1)) Theorem F.23 (\u2016\u2016v-norm or `1-`2-`2), Theorem F.37 (\u2016\u2016u-norm or `1-`1-`2)). Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, let r = O\u0303(k2). If Ak exists then there is an algorithm which runs in nnz(A) \u00b7 t + O\u0303(n) poly(k) time and outputs a (factorization of a) rank-r tensor B for which \u2016B \u2212 A\u2016\u03be \u2264 poly(k, log n) \u00b7 \u2016Ak \u2212 A\u2016\u03be holds. If Ak does not exist, we have \u2016B \u2212 A\u2016\u03be \u2264 poly(k, log n)\u00b7OPT +\u03b3, where \u03b3 is an arbitrarily small positive function of n. The success probability is at least 9/10. For \u03be = 1 or p, t = O\u0303(k); for \u03be = v, t = O(1); for \u03be = u, t = O(n).\nAs in the case of Frobenius norm, we can get rank-k and CURT algorithms for the above norms. Our results for asymmetric norms can be extended to `p-`2-`2, `p-`p-`2, and families of M-estimators.\nWe also obtain the following result for weighted tensor low-rank approximation.\nTheorem 1.6 (Informal Version of Theorem G.5, weighted). Suppose we are given a third order tensor A \u2208 Rn\u00d7n\u00d7n, as well as a tensor W \u2208 Rn\u00d7n\u00d7n with r distinct rows and r distinct columns. Suppose there is a rank-k tensor A\u2032 \u2208 Rn\u00d7n\u00d7n for which \u2016W \u25e6 (A\u2032\u2212A)\u20162F = OPT and one can write A\u2032 = \u2211k i=1 ui \u2297 vi \u2297 wi for \u2016ui\u20162, \u2016vi\u20162, and \u2016wi\u20162 bounded by 2n\n\u03b4 . Then there is an algorithm running in (nnz(A) + nnz(W ) + n2O\u0303(r2k2/ )) \u00b7 n\u03b4 time and outputting n\u00d7 k matrices U1, U2, U3 for which \u2016W \u25e6 (U1 \u2297 U2 \u2297 U3 \u2212A)\u20162F \u2264 (1 + ) OPT with probability at least 2/3.\nWe next strengthen H\u00e5stad\u2019s NP-hardness to show that even approximating tensor rank is hard.\nTheorem 1.7 (Informal Version of Theorem H.42). Let q \u2265 3. Unless the Exponential Time Hypothesis (ETH) fails, there is an absolute constant c0 > 1 for which distinguishing if a tensor in Rnq has rank at most k, or at least c0 \u00b7 k, requires 2\u03b4k1\u2212o(1) time, for a constant \u03b4 > 0.\nUnder random-ETH [Fei02, GL04, RSW16], an average case hardness assumption for 3SAT , we can replace the k1\u2212o(1) in the exponent above with a k. We also obtain hardness in terms of :\nTheorem 1.8 (Informal Version of Corollary H.22). Let q \u2265 3. Unless ETH fails, there is no algorithm running in 2o(1/ 1/4) time which, given a tensor A \u2208 Rnq , outputs a rank-1 tensor B for which \u2016A\u2212B\u20162F \u2264 (1 + ) OPT.\nAs a side result worth stating, our analysis improves the best matrix CUR decomposition algorithm under Frobenius norm [BW14], providing the first optimal nnz(A)-time algorithm:\nTheorem 1.9 (Informal Version of Theorem C.48, Matrix CUR decomposition). There is an algorithm, which given a matrix A \u2208 Rn\u00d7d and an integer k \u2265 1, runs in O(nnz(A))+(n+d) poly(k, 1/ ) time and outputs three matrices: C \u2208 Rn\u00d7c containing c columns of A, R \u2208 Rr\u00d7d containing r rows of A, and U \u2208 Rc\u00d7r with rank(U) = k for which r = c = O(k/ ) and \u2016CUR \u2212 A\u20162F \u2264 (1 + ) minrank\u2212k Ak \u2016Ak \u2212A\u20162F , holds with probability at least 9/10."}, {"heading": "1.2 Our Techniques", "text": "Many of our proofs, in particular those for Theorem 1.1 and Theorem 1.2, are based on what we call an \u201citerative existential proof\u201d, which we then turn into an algorithm in two different ways depending if we are proving Theorem 1.1 or Theorem 1.2.\nHenceforth, we assume Ak exists; otherwise replace Ak with a suitably good tensor A\u0303 in what follows. Since Ak = \u2211k i=1 U \u2217 i \u2297 V \u2217i \u2297W \u2217i 5, we can create three n \u00d7 k matrices U\u2217, V \u2217, and W \u2217 whose columns are the vectors U\u2217i , V \u2217 i , and W \u2217 i , respectively. Now we consider the three different\n5For simplicity, we define U \u2297 V \u2297W = \u2211k i=1 Ui \u2297 Vi \u2297Wi, where Ui is the i-th column of U .\nflattenings (or unfoldings) of Ak, which express Ak as an n\u00d7n2 matrix. Namely, by thinking of Ak as the sum of outer products, we can write the three flattenings of Ak as U\u2217 \u00b7Z1, V \u2217 \u00b7Z2, andW \u2217 \u00b7Z3, where the rows of Z1 are vec(V \u2217i \u2297W \u2217i ) 6 ( For simplicity, we write Z1 = (V \u2217> W \u2217>). 7 ), the rows of Z2 are vec(U\u2217i \u2297W \u2217i ), and the rows of Z3 are vec(U\u2217i \u2297 V \u2217i ), for i \u2208 [k] def = {1, 2, . . . , k}. Letting the three corresponding flattenings of the input tensor A be A1, A2, and A3, by the symmetry of the Frobenius norm, we have\n\u2016A\u2212B\u20162F = \u2016A1 \u2212 U\u2217Z1\u20162F = \u2016A2 \u2212 V \u2217Z2\u20162F = \u2016A3 \u2212W \u2217Z3\u20162F .\nLet us consider the hypothetical regression problem minU \u2016A1 \u2212 UZ1\u20162F . Note that we do not know Z1, but we will not need to. Let r = O(k/ ), and suppose S1 is an n2 \u00d7 r matrix of i.i.d. normal random variables with mean 0 and variance 1/r, denoted N(0, 1/r). Then by standard results for regression (see, e.g., [Woo14] for a survey), if U\u0302 is the minimizer to the smaller regression problem U\u0302 = argminU\u2016UZ1S1 \u2212A1S1\u20162F , then\n\u2016A1 \u2212 U\u0302Z1\u20162F \u2264 (1 + )minU\u2016A1 \u2212 UZ1\u20162F . (3)\nMoreover,U\u0302 = A1S1(Z1S1)\u2020. Although we do not know know Z1, this implies U\u0302 is in the column span of A1S1, which we do know, since we can flatten A to compute A1 and then compute A1S1. Thus, this hypothetical regression argument gives us an existential statement - there exists a good rank-k matrix U\u0302 in the column span of A1S1. We could similarly define V\u0302 = A2S2(Z2S2)\u2020 and W\u0302 = A3S3(Z3S3)\n\u2020 as solutions to the analogous regression problems for the other two flattenings of A, which are in the column spans of A2S2 and A3S3, respectively. Given A1S1, A2S2, and A3S3, which we know, we could hope there is a good rank-k tensor in the span of the rank-1 tensors\n{(A1S1)a \u2297 (A2S2)b \u2297 (A3S3)c}a,b,c\u2208[r]. (4)\nHowever, an immediate issue arises. First, note that our hypothetical regression problem guarantees that \u2016A1 \u2212 U\u0302Z1\u20162F \u2264 (1 + )\u2016A\u2212 Ak\u20162F , and therefore since the rows of Z1 are of the special form vec(V \u2217i \u2297W \u2217i ), we can perform a \u201cretensorization\u201d to create a rank-k tensor B = \u2211 i U\u0302i \u2297 V \u2217i \u2297W \u2217i from the matrix U\u0302Z1 for which \u2016A\u2212B\u20162F \u2264 (1 + )\u2016A\u2212 Ak\u20162F . While we do not know U\u0302 , since it is in the column span of A1S1, it implies that B is in the span of the rank-1 tensors {(A1S1)a \u2297 V \u2217b \u2297W \u2217c }a\u2208[r],b,c\u2208[k]. Analogously, we have that there is a good rank-k tensor B in the span of the rank-1 tensors {U\u2217a\u2297(A2S2)b\u2297W \u2217c }a,c\u2208[k],b\u2208[r], and a good rank-k tensor B in the span of the rank-1 tensors {U\u2217a \u2297 V \u2217b \u2297 (A3S3)c}a,b\u2208[k],c\u2208[r]. However, we do not know U\u2217 or V \u2217, and it is not clear there is a rank-k tensor B for which simultaneously its first factors are in the column span of A1S1, its second factors are in the column span of A2S2, and its third factors are in the column span of A3S3, i.e., whether there is a good rank-k tensor B in the span of rank-1 tensors in (4).\nWe fix this by an iterative argument. Namely, we first computeA1S1, and write U\u0302 = A1S1(Z1S1)\u2020. We now redefine Z2 with respect to U\u0302 , so the rows of Z2 are vec(U\u0302i \u2297W \u2217i ) for i \u2208 [k], and consider the regression problem minV \u2016A2 \u2212 V Z2\u20162F . While we do not know Z2, if S2 is an n2 \u00d7 r matrix of\n6vec(V \u2217i \u2297W \u2217i ) denotes a row vector that has length n1n2 where V \u2217i has length n1 and W \u2217i has length n2. 7(V \u2217> W \u2217>) denotes a k \u00d7 n1n2 matrix where the i-th row is vec(V \u2217i \u2297W \u2217i ), where length n1 vector V \u2217i is the\ni-th column of n1 \u00d7 k matrix V \u2217, and length n2 vector W \u2217i is the i-th column of n2 \u00d7 k matrix W \u2217, \u2200i \u2208 [k].\ni.i.d. Gaussians, we again have the statement that V\u0302 = A2S2(Z2S2)\u2020 satisfies\n\u2016A2 \u2212 V\u0302 Z2\u20162F \u2264 (1 + )minV \u2016A2 \u2212 V Z2\u20162F by the regression guarantee with Gaussians \u2264 (1 + )\u2016A2 \u2212 V \u2217Z2\u20162F since V \u2217 is no better than the minimizer V = (1 + )\u2016A1 \u2212 U\u0302Z1\u20162F by retensorizing and flattening along a different dimension \u2264 (1 + )2minU\u2016A1 \u2212 UZ1\u20162F by (3) = (1 + )2\u2016A\u2212Ak\u20162F by definition of Z1 .\nNow we can retensorize V\u0302 Z2 to obtain a rank-k tensor B for which \u2016A \u2212 B\u20162F = \u2016A2 \u2212 V\u0302 Z2\u20162F \u2264 (1 + )2\u2016A\u2212 Ak\u20162F . Note that since the columns of V\u0302 are in the span of A2S2, and the rows of Z2 are vec(U\u0302i \u2297W \u2217i ) for i \u2208 [k], where the columns of U\u0302 are in the span of A1S1, it follows that B is in the span of rank-1 tensors\n{(A1S1)a \u2297 (A2S2)b \u2297 V\u0302c}a,b\u2208[r],c\u2208[k].\nSuppose we now redefine Z3 so that it is now an r2\u00d7n2 matrix with rows vec((A1S1)a\u2297(A2S2)b) for all pairs a, b \u2208 [r], and consider the regression problem minW \u2016A3 \u2212WZ3\u20162F . Now observe that since we know Z3, and since we can form A3 by flattening A, we can solve for W \u2208 Rn\u00d7r2 in polynomial time by solving a regression problem. Retensorizing WZ3 to a tensor B, it follows that we have found a rank-r2 = O(k2/ 2) tensor B for which \u2016A \u2212 B\u20162F \u2264 (1 + )2\u2016A \u2212 Ak\u20162F = (1 +O( ))\u2016A\u2212Ak\u20162F , and the result follows by adjusting by a constant factor.\nTo obtain the nnz(A)+n poly(k/ ) running time guarantee of Theorem 1.1, while we can replace S1 and S2 with compositions of a sparse CountSketch matrix and a Gaussian matrix (see chapter 2 of [Woo14] for a survey), enabling us to compute A1S1 and A2S2 in nnz(A)+n poly(k/ ) time, we still need to solve the regression problem minW \u2016A3\u2212WZ3\u20162F quickly, and note that we cannot even write down Z3 without spending r2n2 time. Here we use a different random matrix S3 called TensorSketch, which was introduced in [Pag13, PP13], but for which we will need the stronger properties of a subspace embedding and approximate matrix product shown to hold for it in [ANW14]. Given the latter properties, we can instead solve the regression problem minW \u2016A3S3 \u2212WZ3S3\u20162F , and importantly A3S3 and Z3S3 can be computed in nnz(A) + n poly(k/ ) time. Finally, this small problem can be solved in n poly(k/ ) time.\nIf we want to output a rank-k solution as in Theorem 1.2, then we need to introduce indeterminates at several places in the preceding argument and run a generic polynomial optimization procedure which runs in time exponential in the number of indeterminates. Namely, we write U\u0302 as A1S1X1, where X1 is an r\u00d7k matrix of indeterminates, we write V\u0302 as A2S2X2, where X2 is an r\u00d7k matrix of indeterminates, and we write W\u0302 as A3S3X3, whereX3 is an r\u00d7k matrix of indeterminates. When executing the above iterative argument, we let the rows of Z1 be the vectors vec(V \u2217i \u2297W \u2217i ), the rows of Z2 be the vectors vec(U\u0302i\u2297W \u2217i ), and the rows of Z3 be the vectors vec(U\u0302i\u2297Vi). Then U\u0302 is a (1+ )-approximate minimizer to minU \u2016A1\u2212UZ1\u2016F , while V\u0302 is a (1+ )-approximate minimizer to minV \u2016A2 \u2212 V Z2\u2016F , while W\u0302 is a (1 + )-approximate minimizer to minW \u2016A3 \u2212WZ3\u2016F . Note that by assigning X1 = (Z1S1)\u2020, X2 = (Z2S2)\u2020, and X3 = (Z3S3)\u2020, it follows that the rank-k tensor B = \u2211k i=1(A1S1X1)i\u2297 (A2S2X2)i\u2297 (A3S3X3)i satisfies \u2016A\u2212B\u20162F \u2264 (1 + )3\u2016A\u2212Ak\u20162F , as desired. Note that here the rows of Z2 are a function of X1, while the rows of Z3 are a function of both X1 and X2. What is important for us though is that it suffices to minimize the degree-6 polynomial\n\u2211\na,b,c\u2208[n]\n( k\u2211\ni=1\n(A1S1X1)a,i \u00b7 (A2S2X2)b,i \u00b7 (A3S3X3)c,i \u2212Aa,b,c)2,\nover the 3rk = O(k2/ ) indeterminates X1, X2, X3, since we know there exists an assignment to X1, X2, and X3 providing a (1+O( ))-approximate solution, and any solution X1, X2, and X3 found by minimizing the above polynomial will be no worse than that solution. This polynomial can be minimized up to additive 2\u2212 poly(n) additive error in poly(n) time [Ren92a, BPR96] assuming the entries of U\u2217, V \u2217, and W \u2217 are bounded by 2poly(n), as assumed in Theorem 1.2. Similar arguments can be made for obtaining a relative error approximation to the actual value OPT as well as handling the case when Ak does not exist.\nTo optimize the running time to nnz(A), we can choose CountSketch matrices T1, T2, T3 of t = poly(k, 1/ ) \u00d7 n dimensions and reapply the above iterative argument. Then it suffices to minimize this small size degree-6 polynomial\n\u2211\na,b,c\u2208[t]\n(\nk\u2211\ni=1\n(T1A1S1X1)a,i \u00b7 (T2A2S2X2)b,i \u00b7 (T3A3S3X3)c,i \u2212 (A(T1, T2, T3))a,b,c)2,\nover the 3rk = O(k2/ ) indeterminates X1, X2, X3. Outputting A1S1X1, A2S2X2, A3S3X3 then provides a (1 + )-approximate solution.\nOur iterative existential argument provides a general framework for obtaining low rank approximation results for tensors for many other error measures as well."}, {"heading": "1.3 Other Low Rank Approximation Algorithms Following Our Framework.", "text": "Column, row, tube subset selection, and CURT decomposition. In tensor column, row, tube subset selection, the goal is to find three matrices: a subset C of columns of A, a subset R of rows of A, and a subset T of tubes of A, such that there exists a small tensor U for which \u2016U(C,R, T )\u2212A\u20162F \u2264 (1+ ) OPT. We first choose two Gaussian matrices S1 and S2 with s1 = s2 = O(k/ ) columns, and form a matrix Z \u20323 \u2208 R(s1s2)\u00d7n\n2 with (i, j)-th row equal to the vectorization of (A1S1)i \u2297 (A2S2)j . Motivated by the regression problem minW \u2016A3 \u2212WZ \u20323\u2016F , we sample d3 = O(s1s2/ ) columns from A3 and let D3 denote this selection matrix. There are a few ways to do the sampling depending on the tradeoff between the number of columns and running time, which we describe below. Proceeding iteratively, we write down Z \u20322 by setting its (i, j)-th row to the vectorization of (A1S1)i \u2297 (A3D3)j . We then sample d2 = O(s1d3/ ) columns from A2 and let D2 denote that selection matrix. Finally, we define Z \u20321 by setting its (i, j)-th row to be the vectorization of (A2D2)i \u2297 (A3D3)j . We obtain C = A1D1, R = A2D2 and T = A3D3. For the sampling steps, we can use a generalized matrix column subset selection technique, which extends a column subset selection technique of [BW14] in the context of CUR decompositions to the case when C is not necessarily a subset of the input. This gives O(nnz(A) log n) + O\u0303(n2) poly(k, 1/ ) time. Alternatively, we can use a technique we develop called tensor leverage score sampling described below, yielding O(nnz(A)) + n poly(k, 1/ ) time.\nA body of work in the matrix case has focused on finding the best possible number of columns and rows of a CUR decomposition, and we can ask the same question for tensors. It turns out that if one is given the factorization \u2211k i=1(UB)i \u2297 (VB)i \u2297 (WB)i of a rank-k tensor B \u2208 Rn\u00d7n\u00d7n with UB, VB,WB \u2208 Rn\u00d7k, then one can find a set C of O(k/ ) columns, a set R of O(k/ ) rows, and a set T of O(k/ ) tubes of A, together with a rank-k tensor U for which \u2016U(C,R, T )\u2212A\u20162F \u2264 (1 + )\u2016A\u2212B\u20162F . This is based on an iterative argument, where the initial sampling (which needs to be our generalized matrix column subset selection rather than tensor leverage score sampling to achieve optimal bounds) is done with respect to V >B W>B , and then an iterative argument is carried out. Since we show a matching lower bound on the number of columns, rows, tubes and rank of U , these parameters are tight. The algorithm is efficient if one is given a rank-k tensor B\nwhich is a (1 + O( ))-approximation to A; if not then one can use Theorem C.2 and and this step will be exponential time in k. If one just wants O(k log k+k/ ) columns, rows, and tubes, then one can achieve O(nnz(A)) + n poly(k, 1/ ) time, if one is given B.\nColumn-row, row-tube, tube-column face subset selection, and CURT decomposition. In tensor column-row, row-tube, tube-column face subset selection, the goal is to find three tensors: a subset C \u2208 Rc\u00d7n\u00d7n of row-tube faces of A, a subset R \u2208 Rn\u00d7r\u00d7n of tube-column faces of A, and a subset T \u2208 Rn\u00d7n\u00d7t of column-row faces of A, such that there exists a tensor U \u2208 Rtn\u00d7cn\u00d7rn with small rank for which \u2016U(T1, C2, R3) \u2212 A\u20162F \u2264 (1 + ) OPT, where T1 \u2208 Rn\u00d7tn denotes the matrix obtained by flattening the tensor T along the first dimension, C2 \u2208 Rn\u00d7cn denotes the matrix obtained by flattening the tensor C along the second dimension, and R3 \u2208 Rn\u00d7rn denotes the matrix obtained by flattening the tensor T along the third dimension.\nWe solve this problem by first choosing two Gaussian matrices S1 and S2 with s1 = s2 = O(k/ ) columns, and then forming matrix U3 \u2208 Rn\u00d7s1s2 with (i, j)-th column equal to (A1S1)i, as well as matrix V3 \u2208 Rn\u00d7s1s2 with (i, j)-th column equal to (A2S2)j . Inspired by the regression problem minW\u2208Rn\u00d7s1s2 \u2016V3 \u00b7 (W> U>3 ) \u2212 A2\u2016F , we sample d3 = O(s1s2/ ) rows from A2 and let D3 \u2208 Rn\u00d7n denote this selection matrix. In other words, D3 selects d3 tube-column faces from the original tensor A. Thus, we obtain a small regression problem: minW \u2016D3V3 \u00b7 (W> U>3 ) \u2212 D3A2\u2016F . By retensorizing the objective function, we obtain the problem minW \u2016U3\u2297 (D3V3)\u2297W \u2212 A(I,D3, I)\u2016F . Flattening the objective function along the third dimension, we obtain minW \u2016W \u00b7 (U>3 (D3V3)>)\u2212 (A(I,D3, I))3\u2016F which has optimal solution (A(I,D3, I))3(U>3 (D3V3)>)\u2020. Let W \u2032 denote A(I,D3, I))3. In the next step, we fix W2 = W \u2032(U>3 (D3V3)>)\u2020 and U2 = U3, and consider the objective function minV \u2016U2 \u00b7 (V > W>2 ) \u2212 A1\u2016F . Applying a similar argument, we obtain V \u2032 = (A(D2, I, I))2 and U \u2032 = (A(I, I,D1)1). Let C denote A(D2, I, I), R denote A(I,D3, I), and T denote A(I, I,D1). Overall, this algorithm selects poly(k, 1/ ) faces from each dimension.\nSimilar to our column-based CURT decomposition, our face-based CURT decomposition has the property that if one is given the factorization \u2211k i=1(UB)i \u2297 (VB)i \u2297 (WB)i of a rank-k tensor B \u2208 Rn\u00d7n\u00d7n with UB, VB,WB \u2208 Rn\u00d7k which is a (1+O( ))-approximation to A, then one can find a set C of O(k/ ) row-tube faces, a set R of O(k/ ) tube-column faces, and a set T of O(k/ ) columnrow faces of A, together with a rank-k tensor U for which \u2016U(T1, C2, R3)\u2212A\u20162F \u2264 (1 + ) OPT.\nTensor multiple regression and tensor leverage score sampling. In the above we need to consider standard problems for matrices in the context of tensors. Suppose we are given a matrix A \u2208 Rn1\u00d7n2n3 and a matrix B = (V > W>) \u2208 Rk\u00d7n2n3 with rows (Vi \u2297Wi) for an n2 \u00d7 k matrix V and n3 \u00d7 k matrix W . Using TensorSketch [Pag13, PP13, ANW14] one can solve multiple regression minU \u2016UB\u2212A\u2016F without forming B in O(n2+n3) poly(k, 1/ ) time, rather than the na\u00efve O(n2n3) poly(k, 1/ ) time. However, this does not immediately help us if we would like to sample columns of such a matrix B proportional to its leverage scores. Even if we apply TensorSketch to compute a k \u00d7 k change of basis matrix R in O(n2 + n3) poly(k, log(n2n3)) time, for which the leverage scores of B are (up to a constant factor) the squared column norms of R\u22121B, there are still n2n3 leverage scores and we cannot write them all down! Nevertheless, we show we can still sample by them. For the i-th row eiR\u22121 of R\u22121, we create a matrix V\n\u2032i by scaling each of the columns of V > entrywise by the entries of z. The squared norms of eiR\u22121B are exactly the squared entries of (V \u2032i)W>. We cannot compute this matrix product, but we can first sample a column of it proportional to its squared norm and then sample an entry in that column proportional to its square. To sample a column, we compute G(V \u2032i)W> for a Gaussian matrix G with O(log n3) rows by computing G \u00b7 V \u2032i, then computing (G \u00b7 V \u2032i) \u00b7W>, which is O(n2 + n3) poly(k, log(n2n3)) total\ntime. After sampling a column, we compute the column exactly and sample a squared entry. We do this for each i \u2208 [k], first sampling an i proportional to \u2016GV \u2032iW>\u20162F , then running the above scheme on that i. The poly(log n) factor in the running time can be replaced by poly(k) if one wants to avoid a poly(log n) dependence in the running time.\nEntry-wise `1 low-rank approximation. We consider the problem of entrywise `1-low rank approximation of an n\u00d7n\u00d7n tensor A, namely, the problem of finding a rank-k tensor B for which \u2016A \u2212 B\u20161 \u2264 poly(k, log n) OPT, where OPT = infrank-k B \u2016A \u2212 B\u20161, and where for a tensor A, \u2016A\u20161 = \u2211 i,j,k |Ai,j,k|. Our iterative existential argument can be applied in much the same way as for the Frobenius norm. We iteratively flatten A along each of its three dimensions, obtaining A1, A2, and A3 as above, and iteratively build a good rank-k solution B of the form (A1S1X1)\u2297(A2S2X2)\u2297 (A3S3X3), where now the Si are matrices of i.i.d. Cauchy random variables or sparse matrices of Cauchy random variables and the Xi are O(k log k) \u00d7 k matrices of indeterminates. For a matrix C and a matrix S of i.i.d. Cauchy random variables with k columns, it is known [SWZ17] that the column span of CS contains a poly(k log n)-approximate rank-k space with respect to the entrywise `1-norm for C. In the case of tensors, we must perform an iterative flattening and retensorizing argument to guarantee there exists a tensor B of the form above. Also, if we insist on outputting a rank-k solution as opposed to a bicriteria solution, \u2016(A1S1X1)\u2297 (A2S2X2)\u2297 (A3S3X3)\u2212A\u20161 is not a polynomial of the Xi, and if we introduce sign variables for the n3 absolute values, the running time of the polynomial solver will be 2# of variables = 2\u2126(n3). We perform additional dimensionality reduction by Lewis weight sampling [CP15] from the flattenings to reduce the problem size to poly(k). This small problem still has O\u0303(k3) sign variables, and to obtain a 2O\u0303(k2) running time we relax the reduced problem to a Frobenius norm problem, mildly increasing the approximation factor by another poly(k) factor.\nCombining the iterative existential argument with techniques in [SWZ17], we also obtain an `1 CURT decomposition algorithm (which is similar to the Frobenius norm result in Theorem 1.4), which can find O\u0303(k) columns, O\u0303(k) rows, O\u0303(k) tubes, and a tensor U . Our algorithm starts from a given factorization of a rank-k tensor B = UB \u2297 VB \u2297WB found above. We compute a sampling and rescaling diagonal matrix D1 according to the Lewis weights of matrix B1 = (V >B W>B ), where D1 has O\u0303(k) nonzero entries. Then we iteratively construct B2, D2, B3 and D3. Finally we have C = A1D1 (selecting O\u0303(k) columns from A), R = A2D2 (selecting O\u0303(k) rows from A), T = A3D3 (selecting O\u0303(k) tubes from A) and tensor U = ((B1D1)\u2020)\u2297 ((B2D2)\u2020)\u2297 ((B3D3)\u2020).\nWe have similar results for entry-wise `p, 1 \u2264 p < 2, via analogous techniques.\n`1-`2-`2 low-rank approximation (sum of Euclidean norms of faces). For an n \u00d7 n \u00d7 n tensor A, in `1-`2-`2 low rank approximation we seek a rank-k tensor B for which \u2016A \u2212 B\u2016v \u2264 poly(k, log n) OPT, where OPT = infrank-k B \u2016A \u2212 B\u2016v and where \u2016A\u2016v = \u2211 i( \u2211 j,k(Ai,j,k) 2) 1 2 for a tensor A. This norm is asymmetric, i.e., not invariant under permutations to its coordinates, and we cannot flatten the tensor along each of its dimensions while preserving its cost. Instead, we embed the problem to a new problem with a symmetric norm. Once we have a symmetric norm, we apply an iterative existential argument. We choose an oblivious sketching matrix (the M -Sketch in [CW15b]) S \u2208 Rs\u00d7n with s = poly(k, log n), and reduce the original problem to \u2016S(A \u2212 B)\u2016v, by losing a small approximation factor. Because s is small, we can then turn the `1 part of the problem to `2 by losing another \u221a s in the approximation, so that now the problem is a Frobenius norm problem. We then apply our iterative existential argument to the problem \u2016S(\u2211ki=1 U\u2217i \u2297 (A\u03022S2X2)i\u2297 (A\u03023S3X3)i\u2212A)\u2016F where U\u2217 is a fixed matrix and A\u0302 = SA, and output a bicriteria solution.\n`1-`1-`2 low-rank approximation (sum of Euclidean norms of tubes). For an n\u00d7n\u00d7n tensor A, in the `1-`1-`2 low rank approximation problem we seek a rank-k tensor B for which \u2016A\u2212B\u2016u \u2264 poly(k, log n) OPT, where OPT = infrank-k B \u2016A\u2212B\u2016u and \u2016A\u2016u = \u2211 i,j( \u2211 k(Ai,j,k) 2) 1 2 . The main difficulty in this problem is that the norm is asymmetric, and we cannot flatten the tensor along all three dimensions. To reduce the problem to a problem with a symmetric norm, we choose random Gaussian matrices S \u2208 Rn\u00d7s with s = O(n). By Dvoretzky\u2019s theorem [Dvo61], for all tensors A, \u2016AS\u20161 \u2248 \u2016A\u2016u, which reduces our problem to minrank-k B \u2016(A\u2212B)S\u20161. Via an iterative existential argument, we obtain a generalized version of entrywise `1 low rank approximation, \u2016((A\u03021S1X1) \u2297 (A\u03022S2X2) \u2297 (A3S3X3) \u2212 A)S\u20161, where A\u0302 = AS is an n \u00d7 n \u00d7 s size tensor. Finally, we can either use a polynomial system solver to obtain a rank-k solution, or output a bicriteria solution.\nWeighted low-rank approximation. We also consider weighted low rank approximation. Given an n \u00d7 n \u00d7 n tensor A and an n \u00d7 n \u00d7 n tensor W of weights, we want to find a rank-k tensor B for which \u2016W \u25e6 (A \u2212 B)\u20162F \u2264 (1 + ) OPT, where OPT = infrank-k B \u2016W \u25e6 (A \u2212 B)\u20162F and where for a tensor A, \u2016W \u25e6 A\u2016F = ( \u2211 i,j,kW 2 i,j,kA 2 i,j,k) 1 2 . We provide two algorithms based on different assumptions on the weight tensor W . The first algorithm assumes that W has r distinct faces on each of its three dimensions. We flatten A and W along each of its three dimensions, obtaining A1, A2, A3 and W1,W2,W3. Because each Wi has r distinct rows, combining the \u201cguess a sketch\u201d technique from [RSW16] with our iterative argument, we can create matrices U1, U2, and U3 in terms of O(rk2/ ) total indeterminates and for which a solution to the objective function \u2016W \u25e6 (\u2211ki=1(U1)i \u2297 (U2)i \u2297 (U3)i \u2212 A)\u20162F , together with O(r) side constraints, gives a (1 + )- approximation. We can solve the latter problem in poly(n) \u00b7 2O\u0303(rk2/ ) time. Our second algorithm assumes W has r distinct faces in two dimensions. Via a pigeonhole argument, the third dimension will have at most 2O\u0303(r) distinct faces. We again use O(rk2/ ) variables to express U1 and U2, but now express U3 in terms of these variables, which is necessary since W3 could have an exponential number of distinct rows, ultimately causing too many variables needed to express U3 directly. We again arrive at the objective function \u2016W \u25e6 (\u2211ki=1(U1)i \u2297 (U2)i \u2297 (U3)i \u2212A)\u20162F , but now have 2O\u0303(r) side constraints, coming from the fact that U3 is a rational function of the variables created for U1 and U2 and we need to clear denominators. Ultimately, the running time is 2O\u0303(r 2k2/ ).\nComputational Hardness. Our 2\u03b4k1\u2212o(1) time hardness for c-approximation in Theorem H.42 is shown via a reduction from approximating MAX-3SAT to approximating MAX-E3SAT, where the latter problem has the property that each clause in the satisfiability instance has exactly 3 literals (in MAX-3SAT some clauses may have 2 literals). Then, a reduction [Tre01] from approximating MAXE3SAT to approximating MAX-E3SAT(B) is performed, for a constant B which provides an upper bound on the number of clauses each literal can occur in. Given an instance \u03c6 to MAX-E3SAT(B), we create a 3rd order tensor T as H\u00e5stad does using \u03c6 [H\u00e5s90]. While H\u00e5stad\u2019s reduction guarantees that the rank of T is at most r if \u03c6 is satisfiable, and at least r+ 1 otherwise, we can show that if \u03c6 is not satisfiable then its rank is at least the minimal size of a set of variables which is guaranteed to intersect every unsatisfied clause in any unsatisfiable assignment. Since if \u03c6 is not satisfiable, there are at least a linear fraction of clauses in \u03c6 that are unsatisfied under any assignment by the inapproximability of MAX-E3SAT(B), and since each literal occurs in at most B clauses for a constant B, it follows that the rank of T when \u03c6 is not satisfiable is at least c0r for a constant c0 > 1. Further, under ETH , our reduction implies one cannot approximate MAX-E3SAT(B), and thus approximate the rank of a tensor up to a factor c0, in less than 2\u03b4k\n1\u2212o(1) time. We need the near-linear size reduction of MAX-3SAT to MAX-E3SAT of [MR10] to get our strongest result.\nAlgorithm 1 Main Meta-Algorithm 1: procedure TensorLowRankApproxBicriteria(A,n, k, ) . Theorem 1.1 2: Choose sketching matrices S2,S3(Composition of Gaussian and CountSketch.) 3: Choose sketching matrices T2,T3(CountSketch.) 4: Compute T2A2S2, T3A3S3. 5: Construct V\u0302 by setting (i, j)-th column to be (A2S2)i. 6: Construct W\u0302 by setting (i, j)-th column to be (A3S3)j . 7: Construct matrix B by setting (i, j)-th row of B is vectorization of (T2A2S2)i \u2297 (T3A3S3)j . 8: Solve minU \u2016UB \u2212 (A(I, T2, T3))1\u20162F . 9: return U\u0302 , V\u0302 , and W\u0302 . 10: end procedure 11: procedure TensorLowRankApprox(A,n, k, ) . Theorem 1.2 12: Choose sketching matrices S1,S2,S3(Composition of Gaussian and CountSketch.) 13: Choose sketching matrices T1,T2,T3(CountSketch.) 14: Compute T1A1S1, T2A2S2, T3A3S3. 15: Solve minX1,X2,X3 \u2016(T1A1S1X1)\u2297 (T2A2S2X2)\u2297 (T3A3S3X3)\u2212A(T1, T2, T3)\u20162F . 16: return A1S1X1, A2S2X2, and A3S3X3. 17: end procedure\nThe 2\u2126(1/ 1/4) time hardness for (1 + )-approximation for rank-1 tensors in Theorem H.21 strengthens the NP-hardness for rank-1 tensor computation in Section 7 of [HL13], where instead of assuming the NP-hardness of the Clique problem, we assume ETH . Also, the proof in [HL13] did not explicitly bound the approximation error; we do this for a poly(1/ )-sized tensor (which can be padded with 0s to a poly(n)-sized tensor) to rule out (1 + )-approximation in 2o(1/ 1/4) time.\nThe same hard instance above shows, assuming ETH , that 2\u2126(1/ 1/2) time is necessary for (1+ )- approximation to the spectral norm of a symmetric rank-1 tensor (see Section H.2 and Section H.3).\nAssuming ETH , the 21/ 1\u2212o(1)-hardness [SWZ17] for matrix `1-low rank approximation gives the same hardness for tensor entry-wise `1 and `1-`1-`2 low rank approximation. Also, under ETH , we strengthen the NP-hardness in [CW15a] to a 21/ \u2126(1)-hardness for `1-`2-low rank approximation of a matrix, which gives the same hardness for tensor `1-`2-`2 low rank approximation.\nHard Instance. We extend the previous matrix CUR hard instance [BW14] to 3rd order tensors by planting multiple rotations of the hard instance for matrices into a tensor. We show C must select \u2126(k/ ) columns from A, R must select \u2126(k/ ) rows from A, and T must select \u2126(k/ ) tubes from A. Also the tensor U must have rank at least k. This generalizes to q-th order tensors.\nOptimal matrix CUR decomposition. We also improve the nnz(A) log n+(n+d) poly(log n, k, 1/ ) running time of [BW14] for CUR decomposition of A \u2208 Rn\u00d7d to nnz(A) + (n+ d) poly(k, 1/ ), while selecting the optimal number of columns, rows, and a rank-k matrix U . Using [CW13, MM13, NN13], we find a matrix U\u0302 with k orthonormal columns in nnz(A) + n poly(k/ ) time for which minV \u2016U\u0302V \u2212 A\u20162F \u2264 (1 + )\u2016A \u2212 Ak\u20162F . Let s1 = O\u0303(k/ 2) and S1 \u2208 Rs1\u00d7n be a sampling/rescaling matrix by the leverage scores of U\u0302 . By strengthening the affine embedding analysis of [CW13] to leverage score sampling (the analysis of [CW13] gives a weaker analysis for affine embeddings using leverage scores which does not allow approximation in the sketch space to translate to approximation in the original space), with probability at least 0.99, for all X \u2032 which satisfy \u2016S1U\u0302X \u2032 \u2212 S1A\u20162F \u2264 (1+ \u2032) minX \u2016S1U\u0302X\u2212S1A\u20162F , we have \u2016U\u0302X \u2032\u2212A\u20162F \u2264 (1+ ) minX \u2016U\u0302X\u2212A\u20162F , where \u2032 = 0.0001 .\nApplying our generalized row subset selection procedure, we can find Y,R for which \u2016S1U\u0302Y R \u2212 S1A\u20162F \u2264 (1 + \u2032) minX \u2016S1U\u0302X \u2212 S1A\u20162F , where R contains O(k/ \u2032) = O(k/ ) rescaled rows of S1A. A key point is that rescaled rows of S1A are also rescaled rows of A. Then, \u2016U\u0302Y R \u2212 A\u20162F \u2264 (1 + ) minX \u2016U\u0302X \u2212 A\u20162F . Finding Y,R can be done in dpoly(s1/ ) = dpoly(k/ ) time. Now set V\u0302 = Y R. We can choose S2 to be a sampling/rescaling matrix, and then find C,Z for which \u2016CZV\u0302 S2 \u2212 AS2\u20162F \u2264 (1 + \u2032) minX \u2016XV\u0302 S2 \u2212 AS2\u20162F in a similar way, where C contains O(k/ ) rescaled columns of AS2, and thus also of A. We thus have \u2016CZY R\u2212A\u20162F \u2264 (1 +O( ))\u2016A\u2212Ak\u20162F .\nDistributed and streaming settings. Since our algorithms use linear sketches, they are implementable in distributed and streaming models. We use random variables with limited independence to succinctly store the sketching matrices [CW13, KVW14, KN14, Woo14, SWZ17].\nExtension to other notions of tensor rank. This paper focuses on the standard CP rank, or canonical rank, of a tensor. As mentioned, due to border rank issues, the best rank-k solution does not exist in certain cases. There are other notions of tensor rank considered in some applications which do not suffer from this problem, e.g., the tucker rank [KC07, PC08, MH09, ZW13, YC14], and the train rank [Ose11, OTZ11, ZWZ16, PTBD16]). We also show observe that our techniques can be applied to these notions of rank."}, {"heading": "1.4 An Algorithm and a Roadmap", "text": "Roadmap Section A introduces notation and definitions. Section B includes several useful tools. We provide our Frobenius norm low rank approximation algorithms in Section C. Section C.10 extends our results to general q-th order tensors. Section D has our results for entry-wise `1 norm low rank approximation. Section E has our results for entry-wise `p norm low rank approximation. Section G has our results for weighted low rank approximation. Section F has our results for asymmetric norm low rank approximation algorithms. We present our hardness results in Section H and Section I. Section J and Section K extend the results to distributed and streaming settings. Section L extends our techniques from tensor rank to other notions of tensor rank including tensor tucker rank and tensor train rank."}, {"heading": "A Notation", "text": "For an n \u2208 N+, let [n] denote the set {1, 2, \u00b7 \u00b7 \u00b7 , n}. For any function f , we define O\u0303(f) to be f \u00b7 logO(1)(f). In addition to O(\u00b7) notation, for two functions f, g, we use the shorthand f . g (resp. &) to indicate that f \u2264 Cg (resp. \u2265) for an absolute constant C. We use f h g to mean cf \u2264 g \u2264 Cf for constants c, C.\nFor a matrix A, we use \u2016A\u20162 to denote the spectral norm of A. For a tensor A, let \u2016A\u2016 and \u2016A\u20162 (which we sometimes use interchangeably) denote the spectral norm of tensor A,\n\u2016A\u2016 = sup x,y,z 6=0 |A(x, y, z)| \u2016x\u2016 \u00b7 \u2016y\u2016 \u00b7 \u2016z\u2016 .\nLet \u2016A\u2016F denote the Frobenius norm of a matrix/tensor A, i.e., \u2016A\u2016F is the square root of sum of squares of all the entries of A. For 1 \u2264 p < 2, we use \u2016A\u2016p to denote the entry-wise `p-norm of a matrix/tensor A, i.e., \u2016A\u2016p is the p-th root of the sum of p-th powers of the absolute values of the entries of A. \u2016A\u20161 will be an important special case of \u2016A\u2016p, which corresponds to the sum of absolute values of all of the entries.\nLet nnz(A) denote the number of nonzero entries of A. Let det(A) denote the determinant of a square matrix A. Let A> denote the transpose of A. Let A\u2020 denote the Moore-Penrose pseudoinverse of A. Let A\u22121 denote the inverse of a full rank square matrix.\nFor a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, its x-mode fibers are called column fibers (x = 1), row fibers (x = 2) and tube fibers (x = 3). For tensor A, we use A\u2217,j,l to denote its (j, l)-th column, we use Ai,\u2217,l to denote its (i, l)-th row, and we use Ai,j,\u2217 to denote its (i, j)-th tube.\nA tensor A is symmetric if and only if for any i, j, k, Ai,j,k = Ai,k,j = Aj,i,k = Aj,k,i = Ak,i,j = Ak,j,i.\nFor a tensor A \u2208 Rn1\u00d7n2\u00d7n3 , we use > to denote rotation (3 dimensional transpose) so that A> \u2208 Rn3\u00d7n1\u00d7n2 . For a tensor A \u2208 Rn1\u00d7n2\u00d7n3 and matrix B \u2208 Rn3\u00d7k, we define the tensor-matrix dot product to be A \u00b7B \u2208 Rn1\u00d7n2\u00d7k.\nWe use \u2297 to denote outer product, \u25e6 to denote entrywise product, and \u00b7 to denote dot product. Given two column vectors u, v \u2208 Rn, let u\u2297 v \u2208 Rn\u00d7n and (u\u2297 v)i,j = ui \u00b7 vj , u>v = \u2211n i=1 uivi \u2208 R and (u \u25e6 v)i = uivi. Definition A.1 (\u2297 product for vectors). Given q vectors u1 \u2208 Rn1, u2 \u2208 Rn2 , \u00b7 \u00b7 \u00b7 , uq \u2208 Rnq , we use u1 \u2297 u2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 uq to denote an n1 \u00d7 n2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 nq tensor such that, for each (j1, j2, \u00b7 \u00b7 \u00b7 , jq) \u2208 [n1]\u00d7 [n2]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nq],\n(u1 \u2297 u2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 uq)j1,j2,\u00b7\u00b7\u00b7 ,jq = (u1)j1(u2)j2 \u00b7 \u00b7 \u00b7 (uq)jq , where (ui)ji denotes the ji-th entry of vector ui.\nDefinition A.2 (vec(), convert tensor into a vector). Given a tensor A \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nq , let vec(A) \u2208 R1\u00d7 \u220fq i=1 ni be a row vector, such that the t-th entry of vec(A) is Aj1,j2,\u00b7\u00b7\u00b7 ,jq where t =\n(j1 \u2212 1) \u220fq i=2 ni + (j2 \u2212 1) \u220fq i=3 ni + \u00b7 \u00b7 \u00b7+ (jq\u22121 \u2212 1)nq + jq.\nFor example if u = [ 1 2 ] , v =\n \n3 4 5\n  then vec(u\u2297 v) = [ 3 4 5 6 8 10 ] .\nDefinition A.3 (\u2297 product for matrices). Given q matrices U1 \u2208 Rn1\u00d7k, U2 \u2208 Rn2\u00d7k, \u00b7 \u00b7 \u00b7 , Uq \u2208 Rnq\u00d7k, we use U1 \u2297 U2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 Uq to denote an n1 \u00d7 n2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 nq tensor which can be written as,\nU1 \u2297 U2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 Uq = k\u2211\ni=1\n(U1)i \u2297 (U2)i \u2297 \u00b7 \u00b7 \u00b7 \u2297 (Uq)i \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nq ,\nwhere (Uj)i denotes the i-th column of matrix Uj \u2208 Rnj\u00d7k. Definition A.4 ( product for matrices). Given q matrices U1 \u2208 Rk\u00d7n1, U2 \u2208 Rk\u00d7n2, \u00b7 \u00b7 \u00b7 , Uq \u2208 Rk\u00d7nq , we use U1 U2 \u00b7 \u00b7 \u00b7 Uq to denote a k\u00d7 \u220fq j=1 nj matrix where the i-th row of U1 U2 \u00b7 \u00b7 \u00b7 Uq is the vectorization of (U1)i \u2297 (U2)i \u2297 \u00b7 \u00b7 \u00b7 \u2297 (Uq)i, i.e.,\nU1 U2 \u00b7 \u00b7 \u00b7 Uq =   vec((U1) 1 \u2297 (U2)1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 (Uq)1) vec((U1) 2 \u2297 (U2)2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 (Uq)2)\n\u00b7 \u00b7 \u00b7 vec((U1) k \u2297 (U2)k \u2297 \u00b7 \u00b7 \u00b7 \u2297 (Uq)k)\n  \u2208 R k\u00d7 \u220fq j=1 nj .\nwhere (Uj)i \u2208 Rnj denotes the i-th row of matrix Uj \u2208 Rk\u00d7nj .\nDefinition A.5 (Flattening vs unflattening/retensorizing). Suppose we are given three matrices U \u2208 Rn1\u00d7k, V \u2208 Rn2\u00d7k, W \u2208 Rn3\u00d7k. Let tensor A \u2208 Rn1\u00d7n2\u00d7n3 denote U \u2297 V \u2297W . Let A1 \u2208 Rn1\u00d7n2n3 denote a matrix obtained by flattening tensor A along the 1st dimension. Then A1 = U \u00b7B, where B = V > W> \u2208 Rk\u00d7n2n3 denotes the matrix for which the i-th row is vec(Vi\u2297Wi),\u2200i \u2208 [k]. We let the \u201cflattening\u201d be the operation that obtains A1 by A. Given A1 = U \u00b7 B, we can obtain tensor A by unflattening/retensorizing A1. We let \u201cretensorization\u201d be the operation that obtains A from A1. Similarly, let A2 \u2208 Rn2\u00d7n1n3 denote a matrix obtained by flattening tensor A along the 2nd dimension, so A2 = V \u00b7 C, where C = W> U> \u2208 Rk\u00d7n1n3 denotes the matrix for which the i-th row is vec(Wi \u2297 Ui), \u2200i \u2208 [k]. Let A3 \u2208 Rn3\u00d7n1n2 denote a matrix obtained by flattening tensor A along the 3rd dimension. Then, A3 = W \u00b7D, where D = U> V > \u2208 Rk\u00d7n1n2 denotes the matrix for which the i-th row is vec(Ui \u2297 Vi),\u2200i \u2208 [k]. Definition A.6 ( (\u00b7, \u00b7, \u00b7) operator for tensors and matrices). Given tensor A \u2208 Rn1\u00d7n2\u00d7n3 and three matrices B1 \u2208 Rn1\u00d7d1, B2 \u2208 Rn2\u00d7d2, B3 \u2208 Rn3\u00d7d3, we define tensors A(B1, I, I) \u2208 Rd1\u00d7n2\u00d7n3 , A(I,B2, I) \u2208 Rn1\u00d7d2\u00d7n3, A(I, I, B3) \u2208 Rn1\u00d7n2\u00d7d3, A(B1, B2, I) \u2208 Rd1\u00d7d2\u00d7n3 , A(B1, B2, B3) \u2208 Rd1\u00d7d2\u00d7d3 as follows,\nA(B1, I, I)i,j,l =\nn1\u2211\ni\u2032=1\nAi\u2032,j,l(B1)i\u2032,i, \u2200(i, j, l) \u2208 [d1]\u00d7 [n2]\u00d7 [n3]\nA(I,B2, I)i,j,l =\nn2\u2211\nj\u2032=1\nAi,j\u2032,l(B2)j\u2032,j , \u2200(i, j, l) \u2208 [n1]\u00d7 [d2]\u00d7 [n3]\nA(I, I, B3)i,j,l =\nn3\u2211\nl\u2032=1\nAi,j,l\u2032(B3)l\u2032,l, \u2200(i, j, l) \u2208 [n1]\u00d7 [n2]\u00d7 [d3]\nA(B1, B2, I)i,j,l =\nn1\u2211\ni\u2032=1\nn2\u2211\nj\u2032=1\nAi\u2032,j\u2032,l(B1)i\u2032,i(B2)j\u2032,j , \u2200(i, j, l) \u2208 [d1]\u00d7 [d2]\u00d7 [n3]\nA(B1, B2, B3)i,j,l =\nn1\u2211\ni\u2032=1\nn2\u2211\nj\u2032=1\nn3\u2211\nl\u2032=1\nAi\u2032,j\u2032,l\u2032(B1)i\u2032,i(B2)j\u2032,j(B3)l\u2032,l, \u2200(i, j, l) \u2208 [d1]\u00d7 [d2]\u00d7 [d3]\nNote that B>1 A = A(B1, I, I), AB3 = A(I, I, B3) and B>1 AB3 = A(B1, I, B3). In our paper, if \u2200i \u2208 [3], Bi is either a rectangular matrix or a symmetric matrix, then we sometimes use A(B1, B2, B3) to denote A(B>1 , B>2 , B>3 ) for simplicity. Similar to the (\u00b7, \u00b7, \u00b7) operator on 3rd order tensors, we can define the (\u00b7, \u00b7, \u00b7 \u00b7 \u00b7 , \u00b7) operator on higher order tensors.\nFor the matrix case, min rank\u2212k A\u2032 \u2016A \u2212 A\u2032\u20162F always exists. However, this is not true for tensors [DSL08]. For convenience, we redefine the notation of OPT and min.\nDefinition A.7. Given tensor A \u2208 Rn1\u00d7n2\u00d7n3 , k > 0, if min rank\u2212k A\u2032 \u2016A\u2212A\u2032\u20162F does not exist, then we define OPT = inf\nrank\u2212k A\u2032 \u2016A\u2212A\u2032\u20162F +\u03b3 for sufficiently small \u03b3 > 0, which can be an arbitrarily small\npositive function of n. We let min rank\u2212k A\u2032 \u2016A\u2212A\u2032\u20162F be the value of OPT, and we let arg min rank\u2212k A\u2032 \u2016A\u2212A\u2032\u20162F be a rank\u2212k tensor Ak \u2208 Rn1\u00d7n2\u00d7n3 which satisfies \u2016A\u2212Ak\u20162F = OPT ."}, {"heading": "B Preliminaries", "text": "Section B.1 provides the definitions for Subspace Embeddings and Approximate Matrix Product. We introduce the definition for Tensor-CURT decomposition in Section B.2. Section B.3 presents\na tool which we call a \u201cpolynomial system verifier\u201d. Section B.4 introduces a tool which is able to determine the minimum nonzero value of the absolute value of a polynomial evaluated on a set, provided the polynomial is never equal to 0 on that set. Section B.5 shows how to relax an `p problem to an `2 problem. We provide definitions for CountSketch and Gaussian transforms in Section B.6. We present Cauchy and p-stable transforms in Section B.7. We introduce leverage scores and Lewis weights in Section B.8 and Section B.9. Finally, we explain an extension of CountSketch, which is called TensorSketch in Section B.10."}, {"heading": "B.1 Subspace Embeddings and Approximate Matrix Product", "text": "Definition B.1 (Subspace Embedding). A (1 \u00b1 ) `2-subspace embedding for the column space of an n\u00d7 d matrix A is a matrix S for which for all x \u2208 Rd, \u2016SAx\u201622 = (1\u00b1 )\u2016Ax\u201622.\nDefinition B.2 (Approximate Matrix Product). Let 0 < < 1 be a given approximation parameter. Given matrices A and B, where A and B each have n rows, the goal is to output a matrix C so that \u2016A>B\u2212C\u2016F \u2264 \u2016A\u2016F \u2016B\u2016F . Typically C has the form A>S>SB, for a random matrix S with a small number of rows. See, e.g., Lemma 32 of [CW13] for a number of example matrices S with O( \u22122) rows for which this property holds."}, {"heading": "B.2 Tensor CURT decomposition", "text": "We first review matrix CUR decompositions:\nDefinition B.3 (Matrix CUR, exact). Given a matrix A \u2208 Rn\u00d7d, we choose C \u2208 Rn\u00d7c to be a subset of columns of A and R \u2208 Rr\u00d7n to be a subset of rows of A. If there exists a matrix U \u2208 Rc\u00d7r such that A can be written as,"}, {"heading": "CUR = A,", "text": "then we say C,U,R is matrix A\u2019s CUR decomposition.\nDefinition B.4 (Matrix CUR, approximate). Given a matrix A \u2208 Rn\u00d7d, a parameter k \u2265 1, an approximation ratio \u03b1 > 1, and a norm \u2016\u2016\u03be, we choose C \u2208 Rn\u00d7c to be a subset of columns of A and R \u2208 Rr\u00d7n to be a subset of rows of A. Then if there exists a matrix U \u2208 Rc\u00d7r such that,\n\u2016CUR\u2212A\u2016\u03be \u2264 \u03b1 min rank\u2212k Ak \u2016Ak \u2212A\u2016\u03be,\nwhere \u2016\u2016\u03be can be operator norm, Frobenius norm or Entry-wise `1 norm, we say that C,U,R is matrix A\u2019s approximate CUR decomposition, and sometimes just refer to this as a CUR decomposition.\nDefinition B.5 ([Bou11]). Given matrix A \u2208 Rm\u00d7n, integer k, and matrix C \u2208 Rm\u00d7r with r > k, we define the matrix \u03a0\u03beC,k(A) \u2208 Rm\u00d7n to be the best approximation to A (under the \u03be-norm) within the column space of C of rank at most k; so, \u03a0\u03beC,k(A) \u2208 Rm\u00d7n minimizes the residual \u2016A \u2212 A\u0302\u2016\u03be, over all A\u0302 \u2208 Rm\u00d7n in the column space of C of rank at most k.\nWe define the following notion of tensor-CURT decomposition.\nDefinition B.6 (Tensor CURT, exact). Given a tensor A \u2208 Rn1\u00d7n2\u00d7n3, we choose three sets of pair of coordinates S1 \u2286 [n2]\u00d7 [n3], S2 \u2286 [n1]\u00d7 [n3], S3 \u2286 [n1]\u00d7 [n2]. We define c = |S1|, r = |S2| and t = |S3|. Let C \u2208 Rn1\u00d7c denote a subset of columns of A, R \u2208 Rn2\u00d7r denote a subset of rows of A, and T \u2208 Rn3\u00d7t denote a subset of tubes of A. If there exists a tensor U \u2208 Rc\u00d7r\u00d7t such that A can be written as\n(((U \u00b7 T>)> \u00b7R>)> \u00b7 C>)> = A,\nor equivalently,\nU(C,R, T ) = A,\nor equivalently,\n\u2200(i, j, l) \u2208 [n1]\u00d7 [n2]\u00d7 [n3], Ai,j,l = c\u2211\nu1=1\nr\u2211\nu2=1\nt\u2211\nu3=1\nUu1,u2,u3Ci,u1Rj,u2Tl,u3 ,\nthen we say C,U,R, T is tensor A\u2019s CURT decomposition.\nDefinition B.7 (Tensor CURT, approximate). Given a tensor A \u2208 Rn1\u00d7n2\u00d7n3, for some k \u2265 1, for some approximation \u03b1 > 1, for some norm \u2016\u2016\u03be, we choose three sets of pair of coordinates S1 \u2286 [n2] \u00d7 [n3], S2 \u2286 [n1] \u00d7 [n3], S3 \u2286 [n1] \u00d7 [n2]. We define c = |S1|, r = |S2| and t = |S3|. Let C \u2208 Rn1\u00d7c denote a subset of columns of A, R \u2208 Rn2\u00d7r denote a subset of rows of A, and T \u2208 Rn3\u00d7t denote a subset of tubes of A. If there exists a tensor U \u2208 Rc\u00d7r\u00d7t such that\n\u2016U(C,R, T )\u2212A\u2016\u03be \u2264 \u03b1 min rank\u2212k Ak \u2016Ak \u2212A\u2016\u03be,\nwhere \u2016\u2016\u03be is operator norm, Frobenius norm or Entry-wise `1 norm, then we refer to C,U,R, T as an approximate CUR decomposition of A, and sometimes just refer to this as a CURT decomposition of A.\nRecently, [TM17] studied a very different face-based tensor-CUR decomposition, which selects faces from tensors rather than columns. To achieve their results, [TM17] need to make several incoherence assumptions on the original tensor. Their sample complexity depends on log n, and they only sample two of the three dimensions. We will provide more general face-based tensor CURT decompositions.\nDefinition B.8 (Tensor (face-based) CURT, exact). Given a tensor A \u2208 Rn1\u00d7n2\u00d7n3, we choose three sets of coordinates S1 \u2286 [n1], S2 \u2286 [n2], S3 \u2286 [n3]. We define c = |S1|, r = |S2| and t = |S3|. Let C \u2208 Rc\u00d7n2\u00d7n3 denote a subset of row-tube faces of A, R \u2208 Rn1\u00d7r\u00d7n3 denote a subset of columntube faces of A, and T \u2208 Rn1\u00d7n2\u00d7t denote a subset of column-row faces of A. Let C2 \u2208 Rn2\u00d7cn3\ndenote the matrix obtained by flattening the tensor C along the second dimension. Let R3 \u2208 Rn3\u00d7rn1 denote the matrix obtained by flattening the tensor R along the third dimension. Let T1 \u2208 Rn1\u00d7tn2 denote the matrix obtained by flattening the tensor T along the first dimension. If there exists a tensor U \u2208 Rtn2\u00d7cn3\u00d7rn1 such that A can be written as\ntn2\u2211\ni=1\ncn3\u2211\nj=1\nrn1\u2211\nl=1\nUi,j,l(T1)l \u2297 (C2)i \u2297 (R3)j = A,\nU(T1, C2, R3) = A,\nor equivalently,\n\u2200(i\u2032, j\u2032, l\u2032) \u2208 [n1]\u00d7 [n2]\u00d7 [n3], Ai,j,l = tn1\u2211\ni=1\ncn3\u2211\nj=1\nrn2\u2211\nl=1\nUi,j,l(T1)i\u2032,i(C2)j\u2032,j(R3)l\u2032,l,\nthen we say C,U,R, T is tensor A\u2019s (face-based) CURT decomposition.\nDefinition B.9 (Tensor (face-based) CURT, approximate). Given a tensor A \u2208 Rn1\u00d7n2\u00d7n3 , for some k \u2265 1, for some approximation \u03b1 > 1, for some norm \u2016\u2016\u03be,we choose three sets of coordinates S1 \u2286 [n1], S2 \u2286 [n2], S3 \u2286 [n3]. We define c = |S1|, r = |S2| and t = |S3|. Let C \u2208 Rc\u00d7n2\u00d7n3 denote a subset of row-tube faces of A, R \u2208 Rn1\u00d7r\u00d7n3 denote a subset of column-tube faces of A, and T \u2208 Rn1\u00d7n2\u00d7t denote a subset of column-row faces of A. Let C2 \u2208 Rn2\u00d7cn3 denote the matrix obtained by flattening the tensor C along the second dimension. Let R3 \u2208 Rn3\u00d7rn1 denote the matrix obtained by flattening the tensor R along the third dimension. Let T1 \u2208 Rn1\u00d7tn2 denote the matrix obtained by flattening the tensor T along the first dimension. If there exists a tensor U \u2208 Rtn2\u00d7cn3\u00d7rn1 such that\n\u2016U(T1, C2, R3)\u2212A\u2016\u03be \u2264 \u03b1 min rank\u2212k Ak \u2016Ak \u2212A\u2016\u03be,\nwhere \u2016\u2016\u03be is operator norm, Frobenius norm or Entry-wise `1 norm, then we refer to C,U,R, T as an approximate CUR decomposition of A, and sometimes just refer to this as a (face-based) CURT decomposition of A."}, {"heading": "B.3 Polynomial system verifier", "text": "We use the polynomial system verifiers independently developed by Renegar [Ren92a, Ren92b] and Basu et al. [BPR96].\nTheorem B.10 (Decision Problem [Ren92a, Ren92b, BPR96]). Given a real polynomial system P (x1, x2, \u00b7 \u00b7 \u00b7 , xv) having v variables and m polynomial constraints fi(x1, x2, \u00b7 \u00b7 \u00b7 , xv)\u2206i0, \u2200i \u2208 [m], where \u2206i is any of the \u201cstandard relations\u201d: {>,\u2265,=, 6=,\u2264, <}, let d denote the maximum degree of all the polynomial constraints and let H denote the maximum bitsize of the coefficients of all the polynomial constraints. Then in\n(md)O(v) poly(H),\ntime one can determine if there exists a solution to the polynomial system P .\nRecently, this technique has been used to solve a number of low-rank approximation and matrix factorization problems [AGKM12, Moi13, CW15a, BDL16, RSW16, SWZ17].\nB.4 Lower bound on the cost of a polynomial system\nAn important result we use is the following lower bound on the minimum value attained by a polynomial restricted to a compact connected component of a basic closed semi-algebraic subset of Rv.\nTheorem B.11 ([JPT13]). Let T = {x \u2208 Rv|f1(x) \u2265 0, \u00b7 \u00b7 \u00b7 , f`(x) \u2265 0, f`+1(x) = 0, \u00b7 \u00b7 \u00b7 , fm(x) = 0} be defined by polynomials f1, \u00b7 \u00b7 \u00b7 , fm \u2208 Z[x1, \u00b7 \u00b7 \u00b7 , xv] with n \u2265 2, degrees bounded by an even integer d, and coefficients of absolute value at most H, and let C be a compact connected (in the topological sense) component of T . Let g \u2208 Z[x1, \u00b7 \u00b7 \u00b7 , xv] be a polynomial of degree at most d and coefficients of absolute value bounded by H. Then, the minimum value that g takes over C satisfies that if it is not zero, then its absolute value is greater than or equal to\n(24\u2212v/2H\u0303dv)\u2212v2 vdv ,\nwhere H\u0303 = max{H, 2v + 2m}.\nWhile the above theorem involves notions from topology, we shall apply it in an elementary way. Namely, in our setting T will be bounded and so every connected component, which is by definition closed, will also be bounded and therefore compact. As the connected components partition T the theorem will just be applied to give a global minimum value of g on T provided that it is non-zero."}, {"heading": "B.5 Frobenius norm and `2 relaxation", "text": "Theorem B.12 (Generalized rank-constrained matrix approximations, Theorem 2 in [FT07]). Given matrices A \u2208 Rn\u00d7d, B \u2208 Rn\u00d7p, and C \u2208 Rq\u00d7d, let the SVD of B be B = UB\u03a3BV >B and the SVD of C be C = UC\u03a3CV >C . Then,\nB\u2020(UBU > BAVCC > C )kC \u2020 = arg min rank\u2212k X\u2208Rp\u00d7q \u2016A\u2212BXC\u2016F ,\nwhere (UBU>BAVCV > C )k \u2208 Rp\u00d7q is of rank at most k and denotes the best rank-k approximation to UBU > BAVCV > C \u2208 Rp\u00d7d in Frobenius norm.\nClaim B.13 (`2 relaxation of `p-regression). Let p \u2208 [1, 2). For any A \u2208 Rn\u00d7d and b \u2208 Rn, define x\u2217 = arg min\nx\u2208Rd \u2016Ax\u2212 b\u2016p and x\u2032 = arg min x\u2208Rd \u2016Ax\u2212 b\u20162. Then,\n\u2016Ax\u2217 \u2212 b\u2016p \u2264 \u2016Ax\u2032 \u2212 b\u2016p \u2264 n1/p\u22121/2 \u00b7 \u2016Ax\u2217 \u2212 b\u2016p.\nClaim B.14 ((Matrix) Frobenius norm relaxation of `p-low rank approximation). Let p \u2208 [1, 2) and for any matrix A \u2208 Rn\u00d7d, define A\u2217 = arg min\nrank\u2212k B\u2208Rn\u00d7d \u2016B \u2212 A\u2016p and A\u2032 = arg min rank\u2212k B\u2208Rn\u00d7d \u2016B \u2212 A\u2016F .\nThen\n\u2016A\u2217 \u2212A\u2016p \u2264 \u2016A\u2032 \u2212A\u2016p \u2264 (nd)1/p\u22121/2\u2016A\u2217 \u2212A\u2016p.\nClaim B.15 ((Tensor) Frobenius norm relaxation of `p-low rank approximation). Let p \u2208 [1, 2) and for any matrix A \u2208 Rn1\u00d7n2\u00d7n3, define\nA\u2217 = arg min rank\u2212k B\u2208Rn1\u00d7n2\u00d7n3 \u2016B \u2212A\u2016p\nand\nA\u2032 = arg min rank\u2212k B\u2208Rn1\u00d7n2\u00d7n3 \u2016B \u2212A\u2016F .\nThen\n\u2016A\u2217 \u2212A\u2016p \u2264 \u2016A\u2032 \u2212A\u2016p \u2264 (n1n2n3)1/p\u22121/2\u2016A\u2217 \u2212A\u2016p."}, {"heading": "B.6 CountSketch and Gaussian transforms", "text": "Definition B.16 (Sparse embedding matrix or CountSketch transform). A CountSketch transform is defined to be \u03a0 = \u03c3 \u00b7\u03a6D \u2208 Rm\u00d7n. Here, \u03c3 is a scalar, D is an n\u00d7n random diagonal matrix with each diagonal entry independently chosen to be +1 or \u22121 with equal probability, and \u03a6 \u2208 {0, 1}m\u00d7n is an m \u00d7 n binary matrix with \u03a6h(i),i = 1 and all remaining entries 0, where h : [n] \u2192 [m] is a random map such that for each i \u2208 [n], h(i) = j with probability 1/m for each j \u2208 [m]. For any matrix A \u2208 Rn\u00d7d, \u03a0A can be computed in O(nnz(A)) time. For any tensor A \u2208 Rn\u00d7d1\u00d7d2 , \u03a0A can be computed in O(nnz(A)) time. Let \u03a01,\u03a02,\u03a03 denote three CountSktech transforms. For any tensor A \u2208 Rn1\u00d7n2\u00d7n3 , A(\u03a01,\u03a02,\u03a03) can be computed in O(nnz(A)) time.\nIf the above scalar \u03c3 is not specified in the context, we assume the scalar \u03c3 to be 1.\nDefinition B.17 (Gaussian matrix or Gaussian transform). Let S = \u03c3 \u00b7 G \u2208 Rm\u00d7n where \u03c3 is a scalar, and each entry of G \u2208 Rm\u00d7n is chosen independently from the standard Gaussian distribution. For any matrix A \u2208 Rn\u00d7d, SA can be computed in O(m \u00b7nnz(A)) time. For any tensor A \u2208 Rn\u00d7d1\u00d7d2, SA can be computed in O(m \u00b7 nnz(A)) time.\nIf the above scalar \u03c3 is not specified in the context, we assume the scalar \u03c3 to be 1/ \u221a m. In\nmost places, we can combine CountSketch and Gaussian transforms to achieve the following:\nDefinition B.18 (CountSketch + Gaussian transform). Let S\u2032 = S\u03a0, where \u03a0 \u2208 Rt\u00d7n is the CountSketch transform (defined in Definition B.16) and S \u2208 Rm\u00d7t is the Gaussian transform (defined in Definition B.17). For any matrix A \u2208 Rn\u00d7d, S\u2032A can be computed in O(nnz(A) + dtm\u03c9\u22122) time, where \u03c9 is the matrix multiplication exponent.\nLemma B.19 (Affine Embedding - Theorem 39 in [CW13]). Given matrices A \u2208 Rn\u00d7r, B \u2208 Rn\u00d7d, and rank(A) = k, let m = poly(k/ ), S \u2208 Rm\u00d7n be a sparse embedding matrix (Definition B.16) with scalar \u03c3 = 1. Then with probability at least 0.999, \u2200X \u2208 Rr\u00d7d, we have\n(1\u2212 ) \u00b7 \u2016AX \u2212B\u20162F \u2264 \u2016S(AX \u2212B)\u20162F \u2264 (1 + )\u2016AX \u2212B\u20162F .\nLemma B.20 (see, e.g., Lemma 10 in version 1 of [BWZ16]8). Let m = \u2126(k/ ), S = 1\u221a m \u00b7 G, where G \u2208 Rm\u00d7n is a random matrix where each entry is an i.i.d Gaussian N(0, 1). Then with probability at least 0.998, S satisfies (1 \u00b1 1/8) Subspace Embedding (Definition B.1) for any fixed matrix C \u2208 Rn\u00d7k, and it also satisfies O( \u221a /k) Approximate Matrix Product (Definition B.2) for any fixed matrix A and B which has the same number of rows.\nLemma B.21 (see, e.g., Lemma 11 in version 1 of [BWZ16]8). Let m = \u2126(k2 + k/ ), \u03a0 \u2208 Rm\u00d7n, where \u03a0 is a sparse embedding matrix (Definition B.16) with scalar \u03c3 = 1, then with probability at least 0.998, S satisfies (1\u00b11/8) Subspace Embedding (Definition B.1) for any fixed matrix C \u2208 Rn\u00d7k, and it also satisfies O( \u221a /k) Approximate Matrix Product (Definition B.2) for any fixed matrix A and B which has the same number of rows.\nLemma B.22 (see, e.g., Lemma 12 in version 1 of [BWZ16]8). Let m2 = \u2126(k2 + k/ ), \u03a0 \u2208 Rm2\u00d7n, where \u03a0 is a sparse embedding matrix (Definition B.16) with scalar \u03c3 = 1. Let m1 = \u2126(k/ ), S = 1\u221am1 \u00b7 G, where G \u2208 R\nm1\u00d7m2 is a random matrix where each entry is an i.i.d Gaussian N(0, 1). Let S\u2032 = S\u03a0. Then with probability at least 0.99, S\u2032 is a (1 \u00b1 1/3) Subspace Embedding (Definition B.1) for any fixed matrix C \u2208 Rn\u00d7k, and it also satisfies O( \u221a /k) Approximate Matrix Product (Definition B.2) for any fixed matrix A and B which have the same number of rows.\nTheorem B.23 (Theorem 36 in [CW13]). Given A \u2208 Rn\u00d7k, B \u2208 Rn\u00d7d, suppose S \u2208 Rm\u00d7n is such that S is a (1\u00b1 1\u221a\n2 ) Subspace Embedding for A, and satisfies O(\n\u221a /k) Approximate Matrix Product\nfor matrices A and C where C with n rows, where C depends on A and B. If\nX\u0302 = arg min X\u2208Rk\u00d7d\n\u2016SAX \u2212 SB\u20162F ,\nthen\n\u2016AX\u0302 \u2212B\u20162F \u2264 (1 + ) min X\u2208Rk\u00d7d \u2016AX \u2212B\u20162F .\nB.7 Cauchy and p-stable transforms\nDefinition B.24 (Dense Cauchy transform). Let S = \u03c3 \u00b7 C \u2208 Rm\u00d7n where \u03c3 is a scalar, and each entry of C \u2208 Rm\u00d7n is chosen independently from the standard Cauchy distribution. For any matrix A \u2208 Rn\u00d7d, SA can be computed in O(m \u00b7 nnz(A)) time.\nDefinition B.25 (Sparse Cauchy transform). Let \u03a0 = \u03c3 \u00b7 SC \u2208 Rm\u00d7n, where \u03c3 is a scalar, S \u2208 Rm\u00d7n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and C \u2208 Rn\u00d7n is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution. For any matrix A \u2208 Rn\u00d7d, \u03a0A can be computed in O(nnz(A)) time. For any tensor A \u2208 Rn\u00d7d1\u00d7d2, \u03a0A can be computed in O(nnz(A)) time. Let \u03a01 \u2208 Rm1\u00d7n1 ,\u03a02 \u2208 Rm2\u00d7n2 ,\u03a03 \u2208 Rm3\u00d7n3 denote three sparse Cauchy transforms. For any tensor A \u2208 Rn1\u00d7n2\u00d7n3, A(\u03a01,\u03a02,\u03a03) \u2208 Rm1\u00d7m2\u00d7m3 can be computed in O(nnz(A)) time.\n8 https://arxiv.org/pdf/1504.06729v1.pdf\nDefinition B.26 (Dense p-stable transform). Let p \u2208 (1, 2). Let S = \u03c3 \u00b7 C \u2208 Rm\u00d7n, where \u03c3 is a scalar, and each entry of C \u2208 Rm\u00d7n is chosen independently from the standard p-stable distribution. For any matrix A \u2208 Rn\u00d7d, SA can be computed in O(mnnz(A)) time.\nDefinition B.27 (Sparse p-stable transform). Let p \u2208 (1, 2). Let \u03a0 = \u03c3 \u00b7SC \u2208 Rm\u00d7n, where \u03c3 is a scalar, S \u2208 Rm\u00d7n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and C \u2208 Rn\u00d7n is a diagonal matrix with diagonals chosen independently from the standard p-stable distribution. For any matrix A \u2208 Rn\u00d7d, \u03a0A can be computed in O(nnz(A)) time. For any tensor A \u2208 Rn\u00d7d1\u00d7d2, \u03a0A can be computed in O(nnz(A)) time. Let \u03a01 \u2208 Rm1\u00d7n1 ,\u03a02 \u2208 Rm2\u00d7n2 ,\u03a03 \u2208 Rm3\u00d7n3 denote three sparse p-stable transforms. For any tensor A \u2208 Rn1\u00d7n2\u00d7n3, A(\u03a01,\u03a02,\u03a03) \u2208 Rm1\u00d7m2\u00d7m3 can be computed in O(nnz(A)) time."}, {"heading": "B.8 Leverage scores", "text": "Definition B.28 (Leverage scores). Let U \u2208 Rn\u00d7k have orthonormal columns, and let pi = u2i /k, where u2i = \u2016e>i U\u201622 is the i-th leverage score of U .\nDefinition B.29 (Leverage score sampling). Given A \u2208 Rn\u00d7d with rank k, let U \u2208 Rn\u00d7k be an orthonormal basis of the column space of A, and for each i let pi be the squared row norm of the i-th row of U , i.e., the i-th leverage score. Let k \u00b7 pi denote the i-th leverage score of U scaled by k. Let \u03b2 > 0 be a constant and q = (q1, \u00b7 \u00b7 \u00b7 , qn) denote a distribution such that, for each i \u2208 [n], qi \u2265 \u03b2pi. Let s be a parameter. Construct an n \u00d7 s sampling matrix B and an s \u00d7 s rescaling matrix D as follows. Initially, B = 0n\u00d7s and D = 0s\u00d7s. For each column j of B, D, independently, and with replacement, pick a row index i \u2208 [n] with probability qi, and set Bi,j = 1 and Dj,j = 1/\u221aqis. We denote this procedure Leverage score sampling according to the matrix A."}, {"heading": "B.9 Lewis weights", "text": "We follow the exposition of Lewis weights from [CP15].\nDefinition B.30. For a matrix A, let ai denote the ith row of A, where ai(= (Ai)>) is a column vector. The statistical leverage score of a row ai is\n\u03c4i(A) def = a>i (A >A)\u22121ai = \u2016(A>A)\u22121/2ai\u201622.\nFor a matrix A and norm p, the `p Lewis weights w are the unique weights such that for each row i we have\nwi = \u03c4i(W 1/2\u22121/pA).\nor equivalently,\na>i (A >W 1\u22122/pA)\u22121ai = w 2/p i .\nLemma B.31 (Lemma 2.4 of [CP15] and Lemma 7 of [CLM+15]). Given a matrix A \u2208 Rn\u00d7d, n \u2265 d, for any constant C > 0, 4 > p \u2265 1, there is an algorithm which can compute C-approximate `p Lewis weights for every row i of A in O((nnz(A) + d\u03c9 log d) log n) time, where \u03c9 < 2.373 is the matrix multiplication exponent[Str69, CW87, Wil12].\nLemma B.32 (Theorem 7.1 of [CP15]). Given matrix A \u2208 Rn\u00d7d (n \u2265 d) with `p (4 > p \u2265 1) Lewis weights w, for any set of sampling probabilities pi, \u2211 i pi = N ,\npi \u2265 f(d, p)wi,\nif S \u2208 RN\u00d7n has each row chosen independently as the ith standard basis vector, multiplied by 1/p1/pi , with probability pi/N . Then, overall with probability at least 0.999,\n\u2200x \u2208 Rd, 1 2 \u2016Ax\u2016pp \u2264 \u2016SAx\u2016pp \u2264 2\u2016Ax\u2016pp.\nFurthermore, if p = 1, N = O(d log d). If 1 < p < 2, N = O(d log d log log d). If 2 \u2264 p < 4, N = O(dp/2 log d).\nLemma B.33. Given matrix A \u2208 Rn\u00d7d (n \u2265 d), there is an algorithm to compute a diagonal matrix D = SS1 with N nonzero entries in O(n poly(d)) time such that, with probability at least 0.999, for all x \u2208 Rd\n1\n10 \u2016DAx\u2016pp \u2264 \u2016Ax\u2016pp \u2264 10\u2016DAx\u2016pp,\nwhere S, S1 are two sampling/rescaling matrices. Furthermore, if p = 1, then N = O(d log d). If 1 < p < 2, then N = O(d log d log log d). If 2 \u2264 p < 4, then N = O(dp/2 log d).\nGiven a matrix A \u2208 Rn\u00d7d (n \u2265 d), by Lemma B.32 and Lemma B.31, we can compute a sampling/rescaling matrix S in O((nnz(A) + d\u03c9 log d) log n) time with O\u0303(d) nonzero entries such that\n\u2200x \u2208 Rd, 1 2 \u2016Ax\u2016pp \u2264 \u2016SAx\u2016pp \u2264 2\u2016Ax\u2016pp.\nSometimes, poly(d) is much smaller than log n. In this case, we are also able to compute such a sampling/rescaling matrix S in n poly(d) time in an alternative way.\nTo do so, we run one of the input sparsity `p embedding algorithms (see e.g., [MM13]) to compute a well conditioned basis U of the column span of A in n poly(d/ ) time. By sampling according to the well conditioned basis (see e.g. [Cla05, DDH+09, Woo14]), we can compute a sampling/rescaling matrix S1 such that (1 \u2212 )\u2016Ax\u2016pp \u2264 \u2016S1Ax\u2016pp \u2264 (1 + )\u2016Ax\u2016pp where \u2208 (0, 1) is an arbitrary constant. Notice that S1 has poly(d/ ) nonzero entries, and thus S1A has size poly(d/ ). Next, we apply Lewis weight sampling according to S1A, and we obtain a sampling/rescaling matrix S for which\n\u2200x \u2208 Rd, (1\u2212 1 3 )\u2016S1Ax\u2016pp \u2264 \u2016SS1Ax\u2016pp \u2264 (1 + 1 3 )\u2016S1Ax\u2016pp.\nThis implies that\n\u2200x \u2208 Rd, 1 2 \u2016Ax\u2016pp \u2264 \u2016SS1Ax\u2016pp \u2264 2\u2016Ax\u2016pp.\nNote that SS1 is still a sampling/rescaling matrix according to A, and the number of non-zero entries is O\u0303(d). The total running time is thus n poly(d/ ), as desired.\nB.10 TensorSketch Let \u03c6(v1, v2, \u00b7 \u00b7 \u00b7 , vq) denote the function that maps q vectors(ui \u2208 Rni) to the \u220fq i=1 ni-dimensional vector formed by v1 \u2297 v2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 uq. We first give the definition of TensorSketch. Similar definitions can be found in previous work [Pag13, PP13, ANW14, WTSA15].\nDefinition B.34 (TensorSketch [Pag13]). Given q points v1, v2, \u00b7 \u00b7 \u00b7 , vq where for each i \u2208 [q], vi \u2208 Rni , let m be the target dimension. The TensorSketch transform is specified using q 3-wise independent hash functions, h1, \u00b7 \u00b7 \u00b7 , hq, where for each i \u2208 [q], hi : [ni]\u2192 [m], as well as q 4-wise independent sign functions s1, \u00b7 \u00b7 \u00b7 , sq, where for each i \u2208 [q], si : [ni]\u2192 {\u22121,+1}.\nTensorSketch applied to v1, \u00b7 \u00b7 \u00b7 , vq is then CountSketch applied to \u03c6(v1, \u00b7 \u00b7 \u00b7 , vq) with hash function H : [ \u220fq i=1 ni]\u2192 [m] and sign functions S : [ \u220fq i=1 ni]\u2192 {\u22121,+1} defined as follows:\nH(i1, \u00b7 \u00b7 \u00b7 , iq) = h1(i1) + h2(s2) + \u00b7 \u00b7 \u00b7+ hq(iq) (mod m),\nand\nS(i1, \u00b7 \u00b7 \u00b7 , iq) = s1(i1) \u00b7 s2(i2) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 sq(iq).\nUsing the Fast Fourier Transform, TensorSketch(v1, \u00b7 \u00b7 \u00b7 , vq) can be computed in O( \u2211q\ni=1(nnz(vi)+ m logm)) time.\nNote that Theorem 1 in [ANW14] only defines \u03c6(v) = v \u2297 v \u2297 \u00b7 \u00b7 \u00b7 \u2297 v. Here we state a stronger version of Theorem 1 than in [ANW14], though the proofs are identical; a formal derivation can be found in [DW17]. Theorem B.35 (Generalized version of Theorem 1 in [ANW14]). Let S be the ( \u220fq i=1 ni)\u00d7m matrix such that TensorSketch (v1, v2, \u00b7 \u00b7 \u00b7 , vq) is \u03c6(v1, v2, \u00b7 \u00b7 \u00b7 , vq)S for a randomly selected TensorSketch. The matrix S satisfies the following two properties.\nProperty I (Approximate Matrix Product). Let A and B be matrices with \u220fq i=1 ni rows. For\nm \u2265 (2 + 3q)/( 2\u03b4), we have\nPr[\u2016A>SS>B \u2212A>B\u20162F \u2264 2\u2016A\u20162F \u2016B\u20162F ] \u2265 1\u2212 \u03b4.\nProperty II (Subspace Embedding). Consider a fixed k-dimensional subspace V . If m \u2265 k2(2 + 3q)/( 2\u03b4), then with probability at least 1\u2212 \u03b4, \u2016xS\u20162 = (1\u00b1 )\u2016x\u20162 simultaneously for all x \u2208 V ."}, {"heading": "C Frobenius Norm for Arbitrary Tensors", "text": "Section C.1 presents a Frobenius norm tensor low-rank approximation algorithm with (1 + )- approximation ratio. Section C.2 introduces a tool which is able to reduce the size of the objective function from n3 to poly(k, 1/ ). Section C.3 introduces a new problem called tensor multiple regression. Section C.4 presents several bicriteria algorithms. Section C.5 introduces a powerful tool which we call generalized matrix row subset selection. Section C.6 presents an algorithm that is able to select a batch of columns, rows and tubes from a given tensor, and those samples are also able to form a low-rank solution. Section C.7 presents several useful tools for tensor problems, and also two (1 + )-approximation CURT decomposition algorithms: one has the optimal sample complexity, and the other has the optimal running time. Section C.9 shows how to solve the problem if the size of the objective function is small. Section C.10 extends several techniques from 3rd order tensors to general q-th order tensors, for any q \u2265 3. Finally, in Section C.11 we also provide a new matrix CUR decomposition algorithm, which is faster than [BW14].\nFor simplicity of presentation, we assume Ak exists in theorems (e.g., Theorem C.1) which concern outputting a rank-k solution, as well as the theorems (e.g., Theorem C.7, Theorem C.8, Theorem C.13) which concern outputting a bicriteria solution (the output rank is larger than k). For each of the bicriteria theorems, we can obtain a more detailed version when Ak does not exist, like Theorem 1.1 in Section 1 (by instead considering a tensor sufficiently close to Ak in objective function value). Note that the theorems for column, row, tube subset selection Theorem C.20 and Theorem C.21 also belong to this first category. In the second category, for each of the rank-k theorems we can obtain a more detailed version handling all cases, even when Ak does not exist, like Theorem 1.2 in Section 1 (by instead considering a tensor sufficiently close to Ak in objective function value).\nSeveral other tensor results or tools (e.g., Theorem C.4, Lemma C.3, Theorem C.40, Theorem C.41, Theorem C.14, Theorem C.46) that we build in this section do not belong to the above two categories. It means those results do not depend on whether Ak exists or not and whether OPT is zero or not.\nC.1 (1 + )-approximate low-rank approximation\nAlgorithm 2 Frobenius Norm Low-rank Approximation 1: procedure FLowRankApprox(A,n, k, ) . Theorem C.1 2: s1 \u2190 s2 \u2190 s3 \u2190 O(k/ ). 3: Choose sketching matrices S1 \u2208 Rn2\u00d7s1 , S2 \u2208 Rn2\u00d7s2 , S3 \u2208 Rn2\u00d7s3 . . Definition B.18 4: Compute AiSi,\u2200i \u2208 [3]. 5: Y1, Y2, Y3, C \u2190FInputSparsityReduction(A,A1S1, A2S2, A3S3, n, s1, s2, s3, k, ). .\nAlgorithm 3 6: Create variables for Xi \u2208 Rsi\u00d7k,\u2200i \u2208 [3]. 7: Run polynomial system verifier for \u2016(Y1X1)\u2297 (Y2X2)\u2297 (Y3X3)\u2212 C\u20162F . 8: return A1S1X1, A2S2X2, and A3S3X3. 9: end procedure\nTheorem C.1. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), there exists an algorithm which takes O(nnz(A)) + n poly(k, 1/ ) + 2O(k2/ ) time and outputs three matrices\nU \u2208 Rn\u00d7k, V \u2208 Rn\u00d7k, W \u2208 Rn\u00d7k such that \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min rank\u2212k Ak \u2016Ak \u2212A\u20162F\nholds with probability 9/10.\nProof. Given any tensorA \u2208 Rn1\u00d7n2\u00d7n3 , we define three matricesA1 \u2208 Rn1\u00d7n2n3 , A2 \u2208 Rn2\u00d7n3n1 , A3 \u2208 Rn3\u00d7n1n2 such that, for any i \u2208 [n1], j \u2208 [n2], l \u2208 [n3],\nAi,j,l = (A1)i,(j\u22121)\u00b7n3+l = (A2)j,(l\u22121)\u00b7n1+i = (A3)l,(i\u22121)\u00b7n2+j .\nWe define OPT as\nOPT = min rank\u2212k A\u2032\n\u2016A\u2032 \u2212A\u20162F .\nSuppose the optimal Ak = U\u2217 \u2297 V \u2217 \u2297 W \u2217. We fix V \u2217 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k. We use V \u22171 , V \u2217 2 , \u00b7 \u00b7 \u00b7 , V \u2217k to denote the columns of V \u2217 and W \u22171 ,W \u22172 , \u00b7 \u00b7 \u00b7 ,W \u2217k to denote the columns of W \u2217.\nWe consider the following optimization problem,\nmin U1,\u00b7\u00b7\u00b7 ,Uk\u2208Rn \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nUi \u2297 V \u2217i \u2297W \u2217i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n,\nwhich is equivalent to\nmin U1,\u00b7\u00b7\u00b7 ,Uk\u2208Rn \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 [ U1 U2 \u00b7 \u00b7 \u00b7 Uk ]   V \u22171 \u2297W \u22171 V \u22172 \u2297W \u22172 \u00b7 \u00b7 \u00b7\nV \u2217k \u2297W \u2217k\n \u2212A \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n.\nWe use matrix Z1 to denote   vec(V \u22171 \u2297W \u22171 ) vec(V \u22172 \u2297W \u22172 )\n\u00b7 \u00b7 \u00b7 vec(V \u2217k \u2297W \u2217k )\n  \u2208 Rk\u00d7n 2 and matrix U to denote [ U1 U2 \u00b7 \u00b7 \u00b7 Uk ] .\nThen we can obtain the following equivalent objective function,\nmin U\u2208Rn\u00d7k\n\u2016UZ1 \u2212A1\u20162F .\nNotice that minU\u2208Rn\u00d7k \u2016UZ1 \u2212A1\u20162F = OPT, since Ak = U\u2217Z1. Let S>1 \u2208 Rs1\u00d7n\n2 be a sketching matrix defined in Definition B.18, where s1 = O(k/ ). We obtain the following optimization problem,\nmin U\u2208Rn\u00d7k\n\u2016UZ1S1 \u2212A1S1\u20162F .\nLet U\u0302 \u2208 Rn\u00d7k denote the optimal solution to the above optimization problem. Then U\u0302 = A1S1(Z1S1) \u2020. By Lemma B.22 and Theorem B.23, we have\n\u2016U\u0302Z1 \u2212A1\u20162F \u2264 (1 + ) min U\u2208Rn\u00d7k \u2016UZ1 \u2212A1\u20162F = (1 + ) OPT,\nwhich implies \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nU\u0302i \u2297 V \u2217i \u2297W \u2217i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) OPT .\nTo write down U\u03021, \u00b7 \u00b7 \u00b7 , U\u0302k, we use the given matrix A1, and we create s1 \u00d7 k variables for matrix (Z1S1)\n\u2020. As our second step, we fix U\u0302 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k, and we convert tensor A into matrix A2.\nLet matrix Z2 denote   vec(U\u03021 \u2297W \u22171 ) vec(U\u03022 \u2297W \u22172 )\n\u00b7 \u00b7 \u00b7 vec(U\u0302k \u2297W \u2217k )\n . We consider the following objective function,\nmin V \u2208Rn\u00d7k\n\u2016V Z2 \u2212A2\u20162F ,\nfor which the optimal cost is at most (1 + ) OPT. Let S>2 \u2208 Rs2\u00d7n\n2 be a sketching matrix defined in Definition B.18, where s2 = O(k/ ). We sketch S2 on the right of the objective function to obtain the new objective function,\nmin V \u2208Rn\u00d7k\n\u2016V Z2S2 \u2212A2S2\u20162F .\nLet V\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above problem. Then V\u0302 = A2S2(Z2S2)\u2020. By Lemma B.22 and Theorem B.23, we have,\n\u2016V\u0302 Z2 \u2212A2\u20162F \u2264 (1 + ) min V \u2208Rn\u00d7k \u2016V Z2 \u2212A2\u20162F \u2264 (1 + )2 OPT,\nwhich implies \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nU\u0302i \u2297 V\u0302i \u2297W \u2217i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + )2 OPT .\nTo write down V\u03021, \u00b7 \u00b7 \u00b7 , V\u0302k, we need to use the given matrix A2 \u2208 Rn 2\u00d7n, and we need to create s2 \u00d7 k variables for matrix (Z2S2)\u2020. As our third step, we fix the matrices U\u0302 \u2208 Rn\u00d7k and V\u0302 \u2208 Rn\u00d7k. We convert tensor A \u2208 Rn\u00d7n\u00d7n\ninto matrix A3 \u2208 Rn2\u00d7n. Let matrix Z3 denote   vec(U\u03021 \u2297 V\u03021) vec(U\u03022 \u2297 V\u03022)\n\u00b7 \u00b7 \u00b7 vec(U\u0302k \u2297 V\u0302k)\n . We consider the following objective\nfunction,\nmin W\u2208Rn\u00d7k\n\u2016WZ3 \u2212A3\u20162F ,\nwhich has optimal cost at most (1 + )2 OPT. Let S>3 \u2208 Rs3\u00d7n\n2 be a sketching matrix defined in Definition B.18, where s3 = O(k/ ). We sketch S3 on the right of the objective function to obtain a new objective function,\nmin W\u2208Rn\u00d7k\n\u2016WZ3S3 \u2212A3S3\u20162F .\nLet W\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above problem. Then W\u0302 = A3S3(Z3S3)\u2020. By Lemma B.22 and Theorem B.23, we have,\n\u2016W\u0302Z3 \u2212A3\u20162F \u2264 (1 + ) min W\u2208Rn\u00d7k \u2016WZ3 \u2212A3\u20162F \u2264 (1 + )3 OPT .\nThus, we have\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(A1S1X1)i \u2297 (A2S2X2)i \u2297 (A3S3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + )3 OPT .\nLet V1 = A1S1, V2 = A2S2, V3 = A3S3, we then apply Lemma C.3, and we obtain V\u03021, V\u03022, V\u03023, C. We then apply Theorem C.45. Correctness follows by rescaling by a constant factor.\nRunning time. Due to Definition B.18, the running time of line 4 is O(nnz(A)) + n poly(k). The running time of line 5 is shown by Lemma C.3, and the running time of line 7 is shown by Theorem C.45.\nTheorem C.2. Suppose we are given a 3rd order n \u00d7 n \u00d7 n tensor A such that each entry can be written using n\u03b4 bits, where \u03b4 > 0 is a given, value which can be arbitrarily small (e.g., we could have n\u03b4 being O(log n)). Define OPT = infrank\u2212k Ak\u2016Ak \u2212A\u20162F . For any k \u2265 1, and for any 0 < < 1, define n\u03b4\u2032 = O(n\u03b42O(k2/ )). (I) If OPT > 0, and there exists a rank-k Ak = U\u2217 \u2297 V \u2217 \u2297W \u2217 tensor, with size n\u00d7n\u00d7n, such that \u2016Ak \u2212A\u20162F = OPT, and max(\u2016U\u2217\u2016F , \u2016V \u2217\u2016F , \u2016W \u2217\u2016F ) \u2264 2O(n\n\u03b4\u2032 ), then there exists an algorithm that takes (nnz(A)+n poly(k, 1/ )+2O(k2/ ))n\u03b4 time in the unit cost RAM model with word size O(log n) bits9 and outputs three n\u00d7 k matrices U, V,W such that\n\u2016U \u2297 V \u2297W \u2212A\u20162F \u2264 (1 + ) OPT (5)\nholds with probability 9/10, and each entry of each of U, V,W fits in n\u03b4\u2032 bits. (II) If OPT > 0, and Ak does not exist, and there exist three n\u00d7k matrices U \u2032, V \u2032,W \u2032 for which max(\u2016U \u2032\u2016F , \u2016V \u2032\u2016F , \u2016W \u2032\u2016F ) \u2264 2O(n \u03b4\u2032 ) and \u2016U \u2032 \u2297 V \u2032 \u2297W \u2032 \u2212A\u20162F \u2264 (1 + /2) OPT, then we can find U, V,W such that (5) holds. (III) If OPT = 0 and Ak does exist, and there exists a solution U\u2217, V \u2217,W \u2217 such that each entry can be written by n\u03b4\u2032 bits, then we can obtain (5). (IV) If OPT = 0, and there exist three n\u00d7k matrices U, V,W such that max(\u2016U\u2016F , \u2016V \u2016F , \u2016W\u2016F ) \u2264 2O(n\u03b4 \u2032 ) and\n\u2016U \u2297 V \u2297W \u2212A\u20162F \u2264 (1 + ) OPT +2\u2212\u2126(n \u03b4\u2032 ) = 2\u2212\u2126(n \u03b4\u2032 ), (6)\nthen we can output U, V,W such that (6) holds. Further if Ak exists, we can output a number Z for which OPT \u2264 Z \u2264 (1 + ) OPT. For all the cases above, the algorithm runs in the same time as (I) and succeeds with probability at least 9/10.\nProof. This follows by the discussion in Section 1, Theorem C.1 and Theorem C.45 in Section C.9. Part (I) Suppose \u03b4 > 0 and Ak = U\u2217\u2297V \u2217\u2297W \u2217 exists and each of \u2016U\u2217\u2016F , \u2016V \u2217\u2016F , and \u2016W \u2217\u2016F is bounded by 2O(n\u03b4 \u2032 ). We assume the computation model is the unit cost RAM model with words of size O(log n) bits, and allow each number of the input tensor A to be written using n\u03b4 bits. For the 9The entries of A are assumed to fit in n\u03b4 words.\ncase when OPT is nonzero, using the proof of Theorem C.1 and Theorems C.45, B.11, there exists a lower bound on the cost OPT, which is at least 2\u2212O(n\u03b4)2O(k\n2/ ) . We can round each entry of matrices U\u2217, V \u2217,W \u2217 to be an integer expressed using O(n\u03b4\u2032) bits to obtain U \u2032, V \u2032,W \u2032. Using the triangle inequality and our lower bound on OPT, it follows that U \u2032, V \u2032,W \u2032 provide a (1 + )-approximation.\nThus, applying Theorem C.1 by fixing U \u2032, V \u2032,W \u2032 and using Theorem C.45 at the end, we can output three matrices U, V,W , where each entry can be written using n\u03b4\u2032 bits, so that we satisfy \u2016U \u2297 V \u2297W \u2212A\u20162F \u2264 (1 + ) OPT.\nFor the running time, since each entry of the input is bounded by n\u03b4 bits, due to Theorem C.1, we need (nnz(A) + n poly(k/ )) \u00b7 n\u03b4 time to reduce the size of the problem to poly(k/ ) size (with each number represented using O(n\u03b4) bits). According to Theorem C.45, the running time of using a polynomial system verifier to get the solution is 2O(k2/ )nO(\u03b4\u2032) = 2O(k2/ )nO(\u03b4) time. Thus the total running time is (nnz(A) + n poly(k/ ))n\u03b4 + 2O(k2/ ) \u00b7 nO(\u03b4).\nPart (II) is similar to Part (I). Part (III) is trivial to prove since there exists a solution which can be written down in the bit model, so we obtain a (1 + )-approximation. Part (IV) is also very similar to Part (II).\nC.2 Input sparsity reduction\nAlgorithm 3 Reducing the Size of the Objective Function from poly(n) to poly(k)\n1: procedure FInputSparsityReduction(A, V1, V2, V3, n, b1, b2, b3, k, ) . Lemma C.3 2: c1 \u2190 c2 \u2190 c3 \u2190 poly(k, 1/ ). 3: Choose sparse embedding matrices T1 \u2208 Rc1\u00d7n, T2 \u2208 Rc2\u00d7n, T3 \u2208 Rc3\u00d7n. . Definition B.16 4: V\u0302i \u2190 TiVi \u2208 Rci\u00d7bi , \u2200i \u2208 [3]. 5: C \u2190 A(T1, T2, T3) \u2208 Rc1\u00d7c2\u00d7c3 . 6: return V\u03021, V\u03022, V\u03023 and C. 7: end procedure\nLemma C.3. Let poly(k, 1/ ) \u2265 b1b2b3 \u2265 k. Given a tensor A \u2208 Rn\u00d7n\u00d7n and three matrices V1 \u2208 Rn\u00d7b1, V2 \u2208 Rn\u00d7b2, and V3 \u2208 Rn\u00d7b3, there exists an algorithm that takes O(nnz(A)+nnz(V1)+ nnz(V2) + nnz(V3)) = O(nnz(A) + n poly(k/ )) time and outputs a tensor C \u2208 Rc1\u00d7c2\u00d7c3 and three matrices V\u03021 \u2208 Rc1\u00d7b1, V\u03022 \u2208 Rc2\u00d7b2 and V\u03023 \u2208 Rc3\u00d7b3 with c1 = c2 = c3 = poly(k, 1/ ), such that with probability at least 0.99, for all \u03b1 > 0, X1, X \u20321 \u2208 Rb1\u00d7k, X2, X \u20322 \u2208 Rb2\u00d7k, X3, X \u20323 \u2208 Rb3\u00d7k satisfy that,\n\u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V\u03021X \u2032 1)i \u2297 (V\u03022X \u20322)i \u2297 (V\u03023X \u20323)i \u2212 C \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 \u03b1 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V\u03021X1)i \u2297 (V\u03022X2)i \u2297 (V\u03023X3)i \u2212 C \u2225\u2225\u2225\u2225\u2225 2\nF\n,\nthen, \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X \u2032 1)i \u2297 (V2X \u20322)i \u2297 (V3X \u20323)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + )\u03b1 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n.\nProof. Let X1 \u2208 Rb1\u00d7k, X2 \u2208 Rb2\u00d7k, X3 \u2208 Rb3\u00d7k. First, we define Z1 = ((V2X2)> (V3X3)>) \u2208 Rk\u00d7n2 . (Note that, for each i \u2208 [k], the i-th row of matrix Z1 is vec((V2X2)i \u2297 (V3X3)i).) Then, by\nflattening we have \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2016V1X1 \u00b7 Z1 \u2212A1\u20162F .\nWe choose a sparse embedding matrix (Definition B.16) T1 \u2208 Rc1\u00d7n with c1 = poly(k, 1/ ) rows. Since V1 has b1 \u2264 poly(k/ ) columns, according to Lemma B.19 with probability 0.999, for all X1 \u2208 Rb1\u00d7k, Z \u2208 Rk\u00d7n2 ,\n(1\u2212 )\u2016V1X1Z \u2212A1\u20162F \u2264 \u2016T1V1X1Z \u2212 T1A1\u20162F \u2264 (1 + )\u2016V1X1Z \u2212A1\u20162F .\nTherefore, we have\n\u2016T1V1X1 \u00b7 Z1 \u2212 T1A1\u20162F = (1\u00b1 ) \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n.\nSecond, we unflatten matrix T1A1 \u2208 Rc1\u00d7n2 to obtain a tensor A\u2032 \u2208 Rc1\u00d7n\u00d7n. Then we flatten A\u2032 along the second direction to obtain A2 \u2208 Rn\u00d7c1n. We define Z2 = (T1V1X1)> (V3X3)> \u2208 Rk\u00d7c1n. Then, by flattening,\n\u2016V2X2 \u00b7 Z2 \u2212A2\u20162F = \u2016T1V1X1 \u00b7 Z1 \u2212 T1A1\u20162F\n= (1\u00b1 ) \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n.\nWe choose a sparse embedding matrix (Definition B.16) T2 \u2208 Rc2\u00d7n with c2 = poly(k, 1/ ) rows. Then according to Lemma B.19 with probability 0.999, for all X2 \u2208 Rb2\u00d7k, Z \u2208 Rk\u00d7c1n,\n(1\u2212 )\u2016V2X2Z \u2212A2\u20162F \u2264 \u2016T2V2X2Z \u2212 T2A2\u20162F \u2264 (1 + )\u2016V2X2Z \u2212A2\u20162F .\nTherefore, we have\n\u2016T2V2X2 \u00b7 Z2 \u2212 T2A2\u20162F = (1\u00b1 )\u2016V2X2 \u00b7 Z2 \u2212A2\u20162F\n= (1\u00b1 )2 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n.\nThird, we unflatten matrix T2A2 \u2208 Rc2\u00d7c1n to obtain a tensor A\u2032\u2032(= A(T1, T2, I)) \u2208 Rc1\u00d7c2\u00d7n. Then we flatten tensor A\u2032\u2032 along the last direction (the third direction) to obtain matrix A3 \u2208 Rn\u00d7c1c2 . We define Z3 = (T1V1X1)> (T2V2X2)> \u2208 Rk\u00d7c1c2 . Then, by flattening, we have\n\u2016V3X3 \u00b7 Z3 \u2212A3\u20162F = \u2016T2V2X2 \u00b7 Z2 \u2212 T2A2\u20162F\n= (1\u00b1 )2 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n.\nWe choose a sparse embedding matrix (Definition B.16) T3 \u2208 Rc3\u00d7n with c3 = poly(k, 1/ ) rows. Then according to Lemma B.19 with probability 0.999, for all X3 \u2208 Rb3\u00d7k, Z \u2208 Rk\u00d7c1c2 ,\n(1\u2212 )\u2016V3X3Z \u2212A3\u20162F \u2264 \u2016T3V3X3Z \u2212 T3A3\u20162F \u2264 (1 + )\u2016V3X3Z \u2212A3\u20162F .\nTherefore, we have\n\u2016T3V3X3 \u00b7 Z3 \u2212 T3A3\u20162F = (1\u00b1 )3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n.\nNote that\n\u2016T3V3X3 \u00b7 Z3 \u2212 T3A3\u20162F = \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(T1V1X1)i \u2297 (T2V2X2)i \u2297 (T3V3X3)i \u2212A(T1, T2, T3) \u2225\u2225\u2225\u2225\u2225 2\nF\n,\nand thus, we have \u2200X1 \u2208 Rb1\u00d7k, X2 \u2208 Rb2\u00d7k, X3 \u2208 Rb3\u00d7k \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(T1V1X1)i \u2297 (T2V2X2)i \u2297 (T3V3X3)i \u2212A(T1, T2, T3) \u2225\u2225\u2225\u2225\u2225 2\nF\n=(1\u00b1 )3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n.\nLet V\u0302i denote TiVi, for each i \u2208 [3]. Let C \u2208 Rc1\u00d7c2\u00d7c3 denote A(T1, T2, T3). For \u03b1 > 1, if \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V\u03021X \u2032 1)i \u2297 (V\u03022X \u20322)i \u2297 (V\u03023X \u20323)i \u2212 C \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 \u03b1 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V\u03021X1)i \u2297 (V\u03022X2)i \u2297 (V\u03023X3)i \u2212 C \u2225\u2225\u2225\u2225\u2225 2\nF\n,\nthen \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X \u2032 1)i \u2297 (V2X \u20322)i \u2297 (V3X \u20323)i \u2212 C \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 1 (1\u2212 )3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V\u03021X \u2032 1)i \u2297 (V\u03022X \u20322)i \u2297 (V\u03023X \u20323)i \u2212 C \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 1 (1\u2212 )3\u03b1 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V\u03021X1)i \u2297 (V\u03022X2)i \u2297 (V\u03023X3)i \u2212 C \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) 3 (1\u2212 )3\u03b1 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212 C \u2225\u2225\u2225\u2225\u2225 2\nF\nBy rescaling by a constant, we complete the proof of correctness.\nRunning time. According to Section B.6, for each i \u2208 [3], TiVi can be computed in O(nnz(Vi)) time, and A(T1, T2, T3) can be computed in O(nnz(A)) time.\nBy the analysis above, the proof is complete."}, {"heading": "C.3 Tensor multiple regression", "text": "Theorem C.4. Given matrices A \u2208 Rd\u00d7n2, U, V \u2208 Rn\u00d7k, let B \u2208 Rk\u00d7n2 denote U> V >. There exists an algorithm that takes O(nnz(A) + nnz(U) + nnz(V ) + dpoly(k, 1/ )) time and outputs a matrix W \u2032 \u2208 Rd\u00d7k such that,\n\u2016W \u2032B \u2212A\u20162F \u2264 (1 + ) min W\u2208Rd\u00d7k \u2016WB \u2212A\u20162F .\nAlgorithm 4 Frobenius Norm Tensor Multiple Regression 1: procedure FTensorMultipleRegression(A,U, V, d, n, k) . Theorem C.4 2: s\u2190 O(k2 + k/ ). 3: Choose S \u2208 Rn2\u00d7s to be a TensorSketch. . Definition B.34 4: Compute A \u00b7 S. 5: Compute B \u00b7 S. . B = U> V > 6: W \u2190 (AS)(BS)\u2020 7: return W . 8: end procedure\nProof. We choose a TensorSketch (Definition B.34) S \u2208 Rn2\u00d7s to reduce the problem to a smaller problem,\nmin W\u2208Rd\u00d7k\n\u2016WBS \u2212AS\u20162F .\nLet W \u2032 denote the optimal solution to the above problem. Following a similar proof to that in Section C.7.3, if S is a (1\u00b11/2)-subspace embedding and satisfies \u221a /k-approximate matrix product, then W \u2032 provides a (1 + )-approximation to the original problem. By Theorem B.35, we have s = O(k2 + k/ ).\nRunning time. According to Definition B.34, BS can be computed in O(nnz(U) + nnz(V )) + poly(k/ ) time. Notice that each row of S has exactly 1 nonzero entry, thus AS can be computed in O(nnz(A)) time. Since BS \u2208 Rk\u00d7s and AS \u2208 Rd\u00d7s, minW\u2208Rd\u00d7k \u2016WBS \u2212AS\u20162F can be solved in dpoly(sk) = d poly(k/ ) time."}, {"heading": "C.4 Bicriteria algorithms", "text": "C.4.1 Solving a small regression problem\nLemma C.5. Given tensor A \u2208 Rn\u00d7n\u00d7n and three matrices U \u2208 Rn\u00d7s1 , V \u2208 Rn\u00d7s2 and W \u2208 Rn\u00d7s3 , there exists an algorithm that takes O(nnz(A) + n poly(s1, s2, s3, 1/ )) time and outputs \u03b1\u2032 \u2208 Rs1\u00d7s2\u00d7s3 such that \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1\u2032i,j,l \u00b7 Ui \u2297 Vj \u2297Wl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min \u03b1\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1i,j,l \u00b7 Ui \u2297 Vj \u2297Wl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n.\nholds with probability at least .99.\nProof. We define b\u0303 \u2208 Rn3 to be the vector where the i+(j\u22121)n+(l\u22121)n2-th entry of b\u0303 is Ai,j,l. We define A\u0303 \u2208 Rn3\u00d7s1s2s3 to be the matrix where the (i+(j\u22121)n+(l\u22121)n2, i\u2032+(j\u2032\u22121)s2 +(l\u2032\u22121)s2s3) entry is Ui\u2032,i \u00b7 Vj\u2032,j \u00b7Wl\u2032,l. This problem is equivalent to a linear regression problem,\nmin x\u2208Rs1s2s3\n\u2016A\u0303x\u2212 b\u0303\u201622,\nwhere A\u0303 \u2208 Rn3\u00d7s1s2s3 , b\u0303 \u2208 Rn3 . Thus, it can be solved fairly quickly using recent work [CW13, MM13, NN13]. However, the running time of this na\u00efvely is \u2126(n3), since we have to write down each entry of A\u0303. In the next few paragraphs, we show how to improve the running time to nnz(A)+ n poly(s1, s2, s3).\nSince \u03b1 \u2208 Rs1\u00d7s2\u00d7s3 , \u03b1 can be always written as \u03b1 = X1\u2297X2\u2297X3, where X1 \u2208 Rs1\u00d7s1s2s3 , X2 \u2208 Rs2\u00d7s1s2s3 , X3 \u2208 Rs3\u00d7s1s2s3 , we have\nmin \u03b1\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1i,j,l \u00b7 Ui \u2297 Vj \u2297Wl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF = min X1\u2208Rs1\u00d7s1s2s3 X2\u2208Rs2\u00d7s1s2s3 X3\u2208Rs3\u00d7s1s2s3\n\u2016(UX1)\u2297 (V X2)\u2297 (WX3)\u2212A\u20162F .\nBy Lemma C.3, we can reduce the problem size n \u00d7 n \u00d7 n to a smaller problem that has size t1 \u00d7 t2 \u00d7 t3,\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 s1s2s3\u2211\ni=1\n(T1UX1)i \u2297 (T2V X2)i \u2297 (T3WX3)i \u2212A(T1, T2, T3) \u2225\u2225\u2225\u2225\u2225 2\nF\nwhere T1 \u2208 Rt1\u00d7n, T2 \u2208 Rt2\u00d7n, T3 \u2208 Rt3\u00d7n, t1 = t2 = t3 = poly(s1s2s3/ ). Notice that\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 s1s2s3\u2211\ni=1\n(T1UX1)i \u2297 (T2V X2)i \u2297 (T3WX3)i \u2212A(T1, T2, T3) \u2225\u2225\u2225\u2225\u2225 2\nF\n= min \u03b1\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1i,j,l \u00b7 (T1U)i \u2297 (T2V )j \u2297 (T3W )l \u2212A(T1, T2, T3) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n.\nLet\n\u03b1\u2032 = arg min \u03b1\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1i,j,l \u00b7 (T1U)i \u2297 (T2V )j \u2297 (T3W )l \u2212A(T1, T2, T3) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n,\nthen we have \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1\u2032i,j,l \u00b7 Ui \u2297 Vj \u2297Wl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min \u03b1\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1i,j,l \u00b7 Ui \u2297 Vj \u2297Wl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n.\nAgain, according to Lemma C.3, the total running time is then O(nnz(A) + n poly(s1, s2, s3, 1/ )).\nLemma C.6. Given tensor A \u2208 Rn\u00d7n\u00d7n, and two matrices U \u2208 Rn\u00d7s, V \u2208 Rn\u00d7s with rank(U) = r1, rank(V ) = r2, let T1 \u2208 Rt1\u00d7n, T2 \u2208 Rt2\u00d7n be two sparse embedding matrices (Definition B.16) with t1 = poly(r1/ ), t2 = poly(r2/ ). Then with probability at least 0.99, \u2200X \u2208 Rn\u00d7s,\n(1\u2212 )\u2016U \u2297 V \u2297X \u2212A\u20162F \u2264 \u2016T1U \u2297 T2V \u2297X \u2212A(T1, T2, I)\u20162F \u2264 (1 + )\u2016U \u2297 V \u2297X \u2212A\u20162F .\nProof. Let X \u2208 Rn\u00d7s. We define Z1 = (V > X>) \u2208 Rs\u00d7n2 . We choose a sparse embedding matrix (Definition B.16) T1 \u2208 Rt1\u00d7n with t1 = poly(r1/ ) rows. According to Lemma B.19 with probability 0.999, for all Z \u2208 Rs\u00d7n2 ,\n(1\u2212 )\u2016UZ \u2212A1\u20162F \u2264 \u2016T1UZ \u2212 T1A1\u20162F \u2264 (1 + )\u2016T1UZ \u2212A1\u20162F .\nIt means that\n(1\u2212 )\u2016UZ1 \u2212A1\u20162F \u2264 \u2016T1UZ1 \u2212 T1A1\u20162F \u2264 (1 + )\u2016T1UZ1 \u2212A1\u20162F .\nSecond, we unflatten matrix T1A1 \u2208 Rt1\u00d7n2 to obtain a tensor A\u2032 \u2208 Rt1\u00d7n\u00d7n. Then we flatten A\u2032 along the second direction to obtain A\u20322 \u2208 Rn\u00d7t1n. We define Z2 = ((T1U)> X>) \u2208 Rs\u00d7t1n. Then, by flattening,\n\u2016V \u00b7 Z2 \u2212A\u20322\u20162F = \u2016T1U \u00b7 Z1 \u2212 T1A1\u20162F = (1\u00b1 )\u2016U \u2297 V \u2297X \u2212A\u20162F .\nWe choose a sparse embedding matrix (Definition B.16) T2 \u2208 Rt2\u00d7n with t2 = poly(r2/ ) rows. Then according to Lemma B.19 with probability 0.999, for all Z \u2208 Rs\u00d7t1n,\n(1\u2212 )\u2016V Z \u2212A\u20322\u20162F \u2264 \u2016T2V Z \u2212 T2A\u20322\u20162F \u2264 (1 + )\u2016V Z \u2212A\u20322\u20162F .\nThus,\n\u2016T2V \u00b7 Z2 \u2212 T2A\u20322\u20162F = (1\u00b1 )2\u2016U \u2297 V \u2297X \u2212A\u20162F .\nAfter rescaling by a constant, with probability at least 0.99, \u2200X \u2208 Rn\u00d7s,\n(1\u2212 )\u2016U \u2297 V \u2297X \u2212A\u20162F \u2264 \u2016T1U \u2297 T2V \u2297X \u2212A(T1, T2, I)\u20162F \u2264 (1 + )\u2016U \u2297 V \u2297X \u2212A\u20162F ."}, {"heading": "C.4.2 Algorithm I", "text": "We start with a slightly unoptimized bicriteria low rank approximation algorithm.\nAlgorithm 5 Frobenius Norm Bicriteria Low Rank Approximation Algorithm, rank-O(k3/ 3)\n1: procedure FTensorLowRankBicriteriaCubicRank(A,n, k) . Theorem C.7 2: s1 \u2190 s2 \u2190 s3 \u2190 O(k/ ). 3: t1 \u2190 t2 \u2190 t3 \u2190 poly(k/ ). 4: Choose Si \u2208 Rn2\u00d7si to be a Sketching matrix, \u2200i \u2208 [3]. . Definition B.18 5: Choose Ti \u2208 Rti\u00d7n to be a Sketching matrix, \u2200i \u2208 [3]. . Definition B.16 6: Compute U \u2190 T1 \u00b7 (A1 \u00b7 S1), V \u2190 T2 \u00b7 (A2 \u00b7 S2), W \u2190 T3 \u00b7 (A3 \u00b7 S3). 7: Compute C \u2190 A(T1, T2, T3). 8: X \u2190FTensorRegression(C,U, V,W, t1, s1, t2, s2, t3, s3). . Linear regression 9: return X(A1S1, A2S2, A3S3).\n10: end procedure\nTheorem C.7. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), let r = O(k3/ 3). There exists an algorithm that takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices U \u2208 Rn\u00d7r, V \u2208 Rn\u00d7r, W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min rank\u2212k Ak \u2016Ak \u2212A\u20162F\nholds with probability 9/10.\nProof. At the end of Theorem C.1, we need to run a polynomial system verifier. This is why we obtain exponential in k running time. Instead of running the polynomial system verifier, we can use Lemma C.5. This reduces the running time to be polynomial in all parameters: n, k, 1/ . However, the output tensor has rank (k/ )3 (Here we mean that we do not obtain a better decomposition than (k/ )3 components). According to Section B.6, for each i, AiSi can be computed in O(nnz(A)) + n poly(k/ ) time. Then Ti(AiSi) can be computed in n poly(k, 1/ ) time and A(T1, T2, T3) also can be computed in O(nnz(A)) time. The running time for the regression is poly(k/ ).\nNow we present an optimized bicriteria algorithm.\nAlgorithm 6 Frobenius Norm Low Rank Approximation Algorithm, rank-O(k2/ 2)\n1: procedure FTensorLowRankBicriteriaQuadraticRank(A,n, k) . Theorem C.8 2: s1 \u2190 s2 \u2190 O(k/ ). 3: Choose Si \u2208 Rn2\u00d7si to be a sketching matrix, \u2200i \u2208 [3]. . Definition B.18 4: Compute A1 \u00b7 S1, A2 \u00b7 S2. 5: Form U\u0302 by using A1S1 according to Equation (9). 6: Form V\u0302 by using A2S2 according to Equation (10). 7: W\u0302 \u2190FTensorMultipleRegression(A, U\u0302 , V\u0302 , n, n, s1s2). . Algorithm 4 8: return U\u0302 , V\u0302 , W\u0302 . 9: end procedure 10: procedure FTensorLowRankBicriteriaQuadraticRank(A,n, k) . Theorem C.8 11: s1 \u2190 s2 \u2190 O(k/ ). 12: t1 \u2190 t2 \u2190 poly(k/ ). 13: Choose Si \u2208 Rn2\u00d7si to be a Sketching matrix, \u2200i \u2208 [2]. . Definition B.18 14: Choose Ti \u2208 Rti\u00d7n to be a Sketching matrix, \u2200i \u2208 [2]. . Definition B.16 15: Form U\u0302 by using A1S1 according to Equation (9). 16: Form V\u0302 by using A2S2 according to Equation (10). 17: Compute C \u2190 A(T1, T2, I). . C \u2208 Rt1\u00d7t2\u00d7n 18: Compute B \u2190 (T1U\u0302)> (T2V\u0302 )>. 19: W\u0302 \u2190 arg min\nX\u2208Rn\u00d7s1s2 \u2016XB \u2212 C3\u20162F .\n20: return U\u0302 , V\u0302 , W\u0302 . 21: end procedure\nTheorem C.8. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), let r = O(k2/ 2). There exists an algorithm that takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices U \u2208 Rn\u00d7r, V \u2208 Rn\u00d7r, W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min rank\u2212k Ak \u2016Ak \u2212A\u20162F\nholds with probability 9/10.\nNote that there are two different ways to implement algorithm FTensorLowRankBicriteriaQuadraticRank. We present the proofs for both of them here.\nApproach I.\nProof. Let OPT = min rank\u2212k Ak \u2016Ak \u2212 A\u20162F . According to Theorem C.1, we know that there exists a sketching matrix S3 \u2208 Rn2\u00d7s3 where s3 = O(k/ ), such that\nmin X1\u2208Rs1\u00d7k,X2\u2208Rs2\u00d7k,X3\u2208Rs3\u00d7k \u2225\u2225\u2225\u2225\u2225 k\u2211\nl=1\n(A1S1X1)l \u2297 (A2S2X2)l \u2297 (A3S3X3)l \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) OPT\nNow we fix an l and we have:\n(A1S1X1)l \u2297 (A2S2X2)l \u2297 (A3S3X3)l\n=\n( s1\u2211\ni=1\n(A1S1)i(X1)i,l\n) \u2297   s2\u2211\nj=1\n(A2S2)j(X2)j,l  \u2297 (A3S3X3)l\n=\ns1\u2211\ni=1\ns2\u2211\nj=1\n(A1S1)i \u2297 (A2S2)j \u2297 (A3S3X3)l(X1)i,l(X2)j,l\nThus, we have\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\n(A1S1)i \u2297 (A2S2)j \u2297 ( k\u2211\nl=1\n(A3S3X3)l(X1)i,l(X2)j,l\n) \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) OPT . (7)\nWe use matrices A1S1 \u2208 Rn\u00d7s1 and A2S2 \u2208 Rn\u00d7s2 to construct a matrix B \u2208 Rs1s2\u00d7n2 in the following way: each row of B is the vector corresponding to the matrix generated by the \u2297 product between one column vector in A1S1 and the other column vector in A2S2, i.e.,\nBi+(j\u22121)s1 = vec((A1S1)i \u2297 (A2S2)j),\u2200i \u2208 [s1], j \u2208 [s2], (8)\nwhere (A1S1)i denotes the i-th column of A1S1 and (A2S2)j denote the j-th column of A2S2. We create matrix U\u0302 \u2208 Rn\u00d7s1s2 by copying matrix A1S1 s2 times, i.e.,\nU\u0302 = [ A1S1 A1S1 \u00b7 \u00b7 \u00b7 A1S1 ] . (9)\nWe create matrix V\u0302 \u2208 Rn\u00d7s1s2 by copying the i-th column of A2S2 a total of s1 times, into columns (i\u2212 1)s1, \u00b7 \u00b7 \u00b7 , is1 of V\u0302 , for each i \u2208 [s2], i.e.,\nV\u0302 = [ (A2S2)1 \u00b7 \u00b7 \u00b7 (A2S2)1 (A2S2)2 \u00b7 \u00b7 \u00b7 (A2S2)2 \u00b7 \u00b7 \u00b7 (A2S2)s2 \u00b7 \u00b7 \u00b7 (A2S2)s2 ] . (10)\nThus, we can use U\u0302 and V\u0302 to represent B,\nB = (U\u0302> V\u0302 >) \u2208 Rs1s2\u00d7n2 .\nAccording to Equation (7), we have:\nmin W\u2208Rn\u00d7s1s2\n\u2016WB \u2212A3\u20162F \u2264 (1 + ) OPT .\nNext, we want to find matrix W \u2208 Rn\u00d7s1s2 by solving the following optimization problem,\nmin W\u2208Rn\u00d7s1s2\n\u2016WB \u2212A3\u20162F .\nNote that B has size s1s2 \u00d7 n2. Na\u00efvely writing down B already requires \u2126(n2) time. In order to achieve nearly linear time in n, we cannot write down B. We choose S3 \u2208 Rn1n2\u00d7s3 to be a TensorSketch (Definition B.34). In order to solve multiple regression, we need to set s3 = O((s1s2)\n2 + (s1s2)/ ). Let W\u0302 denote the optimal solution to \u2016WBS3 \u2212 A3S3\u20162F . Then W\u0302 = (A3S3)(BS3)\n\u2020. Since each row of S3 has exactly 1 nonzero entry, A3S3 can be computed in O(nnz(A)) time. Since B = (U\u0302> V\u0302 >), according to Definition B.34, BS3 can be computed in n poly(s1s2/ ) = n poly(k/ ) time. By Theorem C.4, we have\n\u2016W\u0302B \u2212A3\u20162F \u2264 (1 + ) min W\u2208Rn\u00d7s1s2 \u2016WB \u2212A3\u20162F .\nThus, we have\n\u2016U\u0302 \u2297 V\u0302 \u2297 W\u0302 \u2212A\u20162F \u2264 (1 + ) OPT .\nAccording to Definition B.18, A1S1, A2S2 can be computed in O(nnz(A)+poly(k/ )) time. Te total running time is thus O(nnz(A) + poly(k/ )).\nApproach II.\nProof. Let OPT = min rank\u2212k Ak\n\u2016Ak \u2212A\u20162F . Choose sketching matrices (Definition B.18) S1 \u2208 Rn 2\u00d7s1 ,\nS2 \u2208 Rn2\u00d7s2 , S3 \u2208 Rn2\u00d7s3 , and sketching matrices (Definition B.16) T1 \u2208 Rt1\u00d7n and T2 \u2208 Rt2\u00d7n with s1 = s2 = s3 = O(k/ ), t1 = t2 = poly(k/ ). We create matrix U\u0302 \u2208 Rn\u00d7s1s2 by copying matrix A1S1 s2 times, i.e.,\nU\u0302 = [ A1S1 A1S1 \u00b7 \u00b7 \u00b7 A1S1 ] .\nWe create matrix V\u0302 \u2208 Rn\u00d7s1s2 by copying the i-th column of A2S2 a total of s1 times, into columns (i\u2212 1)s1, \u00b7 \u00b7 \u00b7 , is1 of V\u0302 , for each i \u2208 [s2], i.e.,\nV\u0302 = [ (A2S2)1 \u00b7 \u00b7 \u00b7 (A2S2)1 (A2S2)2 \u00b7 \u00b7 \u00b7 (A2S2)2 \u00b7 \u00b7 \u00b7 (A2S2)s2 \u00b7 \u00b7 \u00b7 (A2S2)s2 ] .\nAs we proved in Approach I, we have\nmin X\u2208Rn\u00d7s1s2\n\u2016U\u0302 \u2297 V\u0302 \u2297X \u2212A\u20162F \u2264 (1 + ) OPT .\nLet B = ((T1U\u0302)> (T2V\u0302 )>) \u2208 Rs1s2\u00d7t1t2 , and flatten A(T1, T2, I) along the third direction to obtain C3 \u2208 Rn\u00d7t1t2 . Let\nW\u0302 = arg min X\u2208Rn\u00d7s1s2 \u2016T1U\u0302 \u2297 T2V\u0302 \u2297X \u2212A(T1, T2, I)\u20162F = arg min X\u2208Rn\u00d7s1s2 \u2016XB \u2212 C3\u20162F .\nLet\nW \u2217 = arg min X\u2208Rn\u00d7s1s2 \u2016U\u0302 \u2297 V\u0302 \u2297X \u2212A\u20162F .\nAccording to Lemma C.6,\n\u2016U\u0302 \u2297 V\u0302 \u2297 W\u0302 \u2212A\u20162F \u2264 1\n1\u2212 \u2016T1U\u0302 \u2297 T2V\u0302 \u2297 W\u0302 \u2212A(T1, T2, I)\u2016 2 F\n\u2264 1 1\u2212 \u2016T1U\u0302 \u2297 T2V\u0302 \u2297W \u2217 \u2212A(T1, T2, I)\u20162F \u22641 + 1\u2212 \u2016U\u0302 \u2297 V\u0302 \u2297W \u2217 \u2212A\u20162F \u2264(1 + ) 2\n1\u2212 OPT .\nAccording to Definition B.18, A1S1, A2S2 can be computed in O(nnz(A) + poly(k/ )) time. The total running time is thus O(nnz(A) + poly(k/ )). Since T1, T2 are sparse embedding matrices, T1U\u0302 , T2V\u0302 can be computed in O(nnz(A)+poly(k/ )) time. The total running time is in O(nnz(A)+ poly(k/ )).\nTheorem C.9. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1 and any 0 < < 1, if Ak exists then there is a randomized algorithm running in nnz(A) + n \u00b7 poly(k/ ) time which outputs a rank-O(k2/ 2) tensor B for which \u2016A \u2212 B\u20162F \u2264 (1 + )\u2016A \u2212 Ak\u20162F . If Ak does not exist, then the algorithm outputs a rank-O(k2/ 2) tensor B for which \u2016A\u2212B\u20162F \u2264 (1 + ) OPT +\u03b3, where \u03b3 is an arbitrarily small positive function of n. In both cases, the algorithm succeeds with probability at least 9/10.\nProof. If Ak exists, then the proof directly follows the proof of Theorem C.1 and Theorem C.8. If Ak does not exist, then for any \u03b3 > 0, there exist U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rn\u00d7k,W \u2217 \u2208 Rn\u00d7k such that\n\u2016U\u2217 \u2297 V \u2217 \u2297W \u2217 \u2212A\u20162F \u2264 inf rank\u2212k A\u2032 \u2016A\u2212A\u2032\u20162F + 1 10 \u03b3.\nThen we just regard U\u2217 \u2297 V \u2217 \u2297W \u2217 as the \u201cbest\u201d rank k approximation to A, and follow the same argument as in the proof of Theorem C.1 and the proof of Theorem C.8. We can finally output a tensor B \u2208 Rn\u00d7n\u00d7n with rank-O(k2/ 2) such that\n\u2016B \u2212A\u20162F \u2264 (1 + )\u2016U\u2217 \u2297 V \u2217 \u2297W \u2217 \u2212A\u20162F\n\u2264 (1 + ) (\ninf rank\u2212k A\u2032\n\u2016A\u2212A\u2032\u20162F + 1\n10 \u03b3\n)\n\u2264 (1 + ) inf rank\u2212k A\u2032 \u2016A\u2212A\u2032\u20162F + \u03b3\nwhere the first inequality follows by the proof of Theorem C.1 and the proof of theorem C.8. The second inequality follows by our choice of U\u2217, V \u2217,W \u2217. The third inequality follows since 1 + < 2 and \u03b3 > 0.\nC.4.3 poly(k)-approximation to multiple regression\nLemma C.10 ((1.4) and (1.9) in [RV09]). Let s \u2265 k. Let U \u2208 Rn\u00d7k denote a matrix that has orthonormal columns, and S \u2208 Rs\u00d7n denote an i.i.d. N(0, 1/s) Gaussian matrix. Then SU is also an s\u00d7k i.i.d. Gaussian matrix with each entry draw from N(0, 1/s), and furthermore, we have with arbitrarily large constant probability,\n\u03c3max(SU) = O(1) and \u03c3min(SU) = \u2126(1/ \u221a s).\nProof. Note that \u221a s\u2212 \u221a k \u2212 1 = s\u2212k\u22121\u221a\ns+ \u221a k\u22121 = \u2126(1/\n\u221a s).\nLemma C.11. Given matrices A \u2208 Rn\u00d7k, B \u2208 Rn\u00d7d, let S \u2208 Rs\u00d7n denote a standard Gaussian N(0, 1) matrix with s = k. Let X\u2217 = min\nX\u2208Rk\u00d7d \u2016AX \u2212 B\u2016F . Let X \u2032 = min X\u2208Rk\u00d7d \u2016SAX \u2212 SB\u2016F . Then,\nwe have that\n\u2016AX \u2032 \u2212B\u2016F \u2264 O( \u221a k)\u2016AX\u2217 \u2212B\u2016F ,\nholds with probability at least 0.99.\nProof. Let X\u2217 \u2208 Rk\u00d7d denote the optimal solution such that\n\u2016AX\u2217 \u2212B\u2016F = min X\u2208Rk\u00d7d \u2016AX \u2212B\u2016F .\nConsider a standard Gaussian matrix S \u2208 Rk\u00d7n scaled by 1/ \u221a k with exactly k rows. Then for\nany X \u2208 Rk\u00d7d, by the triangle inequality, we have\n\u2016SAX \u2212 SB\u2016F \u2264 \u2016SAX \u2212 SAX\u2217\u2016F + \u2016SAX\u2217 \u2212 SB\u2016F ,\nand\n\u2016SAX \u2212 SB\u2016F \u2265 \u2016SAX \u2212 SAX\u2217\u2016F \u2212 \u2016SAX\u2217 \u2212 SB\u2016F .\nWe first show how to bound \u2016SAX \u2212 SAX\u2217\u2016F , and then show how to bound \u2016SAX\u2217 \u2212 SB\u2016F . Note that Lemma C.10 implies the following result,\nClaim C.12. For any X \u2208 Rk\u00d7d, with probability 0.999, we have 1\u221a k \u2016AX \u2212AX\u2217\u2016F . \u2016SAX \u2212 SAX\u2217\u2016F . \u2016AX \u2212AX\u2217\u2016F .\nProof. First, we can write A = UR \u2208 Rn\u00d7k where U \u2208 Rn\u00d7k has orthonormal columns and R \u2208 Rk\u00d7k. It gives,\n\u2016SAX \u2212 SAX\u2217\u2016F = \u2016SU(RX \u2212RX\u2217)\u2016F .\nSecond, applying Lemma C.10 to SU \u2208 Rs\u00d7k completes the proof.\nUsing Markov\u2019s inequality, for any fixed matrix AX\u2217 \u2212 B, choosing a Gaussian matrix S, we have that\n\u2016SAX\u2217 \u2212 SB\u20162F = O(\u2016AX\u2217 \u2212B\u20162F )\nholds with probability at least 0.999. This is equivalent to\n\u2016SAX\u2217 \u2212 SB\u2016F = O(\u2016AX\u2217 \u2212B\u2016F ), (11)\nholding with probability at least 0.999.\nLet X \u2032 = arg min X\u2208Rk\u00d7d \u2016SAX \u2212 SB\u2016F . Putting it all together, we have\n\u2016AX \u2032 \u2212B\u2016F \u2264 \u2016AX \u2032 \u2212AX\u2217\u2016F + \u2016AX\u2217 \u2212B\u2016F by triangle inequality \u2264 O( \u221a k)\u2016SAX \u2032 \u2212 SAX\u2217\u2016F + \u2016AX\u2217 \u2212B\u2016F by Claim C.12 \u2264 O( \u221a k)\u2016SAX \u2032 \u2212 SB\u2016F +O( \u221a k)\u2016SAX\u2217 \u2212 SB\u2016F + \u2016AX\u2217 \u2212B\u2016F by triangle inequality \u2264 O( \u221a k)\u2016SAX\u2217 \u2212 SB\u2016F +O( \u221a k)\u2016SAX\u2217 \u2212 SB\u2016F + \u2016AX\u2217 \u2212B\u2016F by definition of X \u2032 \u2264 O( \u221a k)\u2016AX\u2217 \u2212B\u2016F . by Equation (11)"}, {"heading": "C.4.4 Algorithm II", "text": "Theorem C.13. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, let r = k2. There exists an algorithm which takes O(nnz(A)k) + n poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that,\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1 Ui \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 F \u2264 poly(k) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u2016F\nholds with probability 9/10.\nProof. Let OPT = min rank\u2212k A\u2032 \u2016A\u2032\u2212A\u2016F , we fix V \u2217 \u2208 Rn\u00d7k,W \u2217 \u2208 Rn\u00d7k to be the optimal solution of the original problem. We use Z1 = (V \u2217> W \u2217>) \u2208 Rk\u00d7n2 to denote the matrix where the i-th row is the vectorization of V \u2217i \u2297W \u2217i . Let A1 \u2208 Rn\u00d7n\n2 denote the matrix obtained by flattening tensor A \u2208 Rn\u00d7n\u00d7n along the first direction. Then, we have\nmin U \u2016UZ1 \u2212A1\u2016F \u2264 OPT .\nChoosing an N(0, 1/k) Gaussian sketching matrix S1 \u2208 Rn2\u00d7s1 with s1 = k, we can obtain the smaller problem,\nmin U\u2208Rn\u00d7k\n\u2016UZ1S1 \u2212A1S1\u2016F .\nDefine U\u0302 = A1S1(Z1S1)\u2020. Define \u03b1 = O( \u221a k). By Lemma C.11, we have\n\u2016U\u0302Z1 \u2212A1\u2016F \u2264 \u03b1OPT .\nSecond, we fix U\u0302 and W \u2217. Define Z2, A2 similarly as above. Choosing an N(0, 1/k) Gaussian sketching matrix S2 \u2208 Rn2\u00d7s2 with s2 = k, we can obtain another smaller problem,\nmin V \u2208Rn\u00d7k\n\u2016V Z2S2 \u2212A2S2\u2016F .\nDefine V\u0302 = A2S2(Z2S2)\u2020. By Lemma C.11 again, we have\n\u2016V\u0302 Z2 \u2212A2\u2016F \u2264 \u03b12 OPT .\nThus, we now have\nmin X1,X2,W\n\u2016A1S1X1 \u2297A2S2X2 \u2297W \u2212A\u2016F \u2264 \u03b12 OPT\nWe use a similar idea as in the proof of Theorem C.8. We create matrix U\u0303 \u2208 Rn\u00d7s1s2 by copying matrix A1S1 s2 times, i.e.,\nU\u0303 = [ A1S1 A1S1 \u00b7 \u00b7 \u00b7 A1S1 ] .\nWe create matrix V\u0303 \u2208 Rn\u00d7s1s2 by copying the i-th column of A2S2 a total of s1 times, into columns (i\u2212 1)s1, \u00b7 \u00b7 \u00b7 , is1 of V\u0303 , for each i \u2208 [s2], i.e.,\nV\u0303 = [ (A2S2)1 \u00b7 \u00b7 \u00b7 (A2S2)1 (A2S2)2 \u00b7 \u00b7 \u00b7 (A2S2)2 \u00b7 \u00b7 \u00b7 (A2S2)s2 \u00b7 \u00b7 \u00b7 (A2S2)s2 ] .\nWe have\nmin X\u2208Rn\u00d7s1s2\n\u2016U\u0303 \u2297 V\u0303 \u2297X \u2212A\u2016F \u2264 \u03b12 OPT .\nChoose Ti \u2208 Rti\u00d7n to be a sparse embedding matrix (Definition B.16) with ti = poly(k/ ), for each i \u2208 [2]. By applying Lemma C.6, we have, if W \u2032 satisfies,\n\u2016T1U\u0303 \u2297 T2V\u0303 \u2297W \u2032 \u2212A(T1, T2, I)\u2016F = min X\u2208Rn\u00d7s1s2 \u2016T1U\u0303 \u2297 T2V\u0303 \u2297X \u2212A(T1, T2, I)\u2016F\nthen,\n\u2016U\u0303 \u2297 V\u0303 \u2297W \u2032 \u2212A\u2016F \u2264 (1 + ) min X\u2208Rn\u00d7s1s2 \u2016U\u0303 \u2297 V\u0303 \u2297X \u2212A\u2016F \u2264 (1 + )\u03b12 OPT .\nThus, we only need to solve\nmin X\u2208Rn\u00d7s1s2\n\u2016T1U\u0303 \u2297 T2V\u0303 \u2297X \u2212A(T1, T2, I)\u2016F .\nwhich is similar to the proof of Theorem C.8. Therefore, we complete the proof of correctness. For the running time, A1S1, A2S2 can be computed in O(nnz(A)k) time, T1U\u0303 , T2V\u0303 can be computed in n poly(k) time. The final regression problem can be computed in n poly(k) running time.\nC.5 Generalized matrix row subset selection\nNote that in this section, the notation \u03a0\u03beC,k is given in Definition B.5.\nTheorem C.14. Given matrices A \u2208 Rn\u00d7m and C \u2208 Rn\u00d7k, there exists an algorithm which takes O(nnz(A) log n)+(m+n) poly(k, 1/ ) time and outputs a diagonal matrix D \u2208 Rn\u00d7n with d = O(k/ ) nonzeros (or equivalently a matrix R that contains d = O(k/ ) rescaled rows of A) and a matrix U \u2208 Rk\u00d7d such that\n\u2016CUDA\u2212A\u20162F \u2264 (1 + ) min X\u2208Rk\u00d7m \u2016CX \u2212A\u20162F\nholds with probability .99.\nAlgorithm 7 Generalized Matrix Row Subset Selection: Constructing R with r = O(k+k/ ) Rows and a rank-k U \u2208 Rk\u00d7r 1: procedure GeneralizedMatrixRowSubsetSelection(A,C, n,m, k, ) . Theorem C.14 2: Y,\u03a6,\u2206\u2190 ApproxSubspaceSVD(A,C, k). . Claim C.16 and Lemma 3.12 in [BW14] 3: B \u2190 Y\u2206. 4: Z2, D \u2190 QR(B). . Z2 \u2208 Rm\u00d7k, Z>2 Z2 = Ik, D \u2208 Rk\u00d7k 5: h2 \u2190 8k ln(20k). 6: \u21262, D2 \u2190 RandSampling(Z2, h2, 1) . Definition 3.6 in [BW14] 7: M2 \u2190 Z>2 \u21262D2 \u2208 Rk\u00d7h2 . 8: UM2 ,\u03a3M2 , V > M2 \u2190 SVD(M2). . rank(M2) = k and VM2 \u2208 Rh2\u00d7k\n9: r1 \u2190 4k. 10: S2 \u2190 BSSSamplingSparse(VM2 , ((A> \u2212A>Z2Z>2 )\u21262D2)>, r1, 0.5) . Lemma 4.3 in\n[BW14] 11: R1 \u2190 (A>\u21262D2S2)> \u2208 Rr1\u00d7n containing rescaled rows from A. 12: r2 \u2190 4820k/ . 13: R2 \u2190 AdaptiveRowsSparse(A,Z2, R1, r2) . Lemma 4.5 in [BW14] 14: R\u2190 [R>1 , R>2 ]>. . R \u2208 R(r1+r2)\u00d7n containing r = 4k + 4820k/ rescaled rows of A. 15: Choose W \u2208 R\u03be\u00d7m to be a randomly chosen sparse subspace embedding with \u03be = \u2126(k2 \u22122). 16: U \u2190 \u03a6\u22121\u2206D\u22121(WC\u03a6\u22121\u2206D\u22121)\u2020WAR\u2020 = \u03a6\u22121\u2206\u2206>(WC)\u2020WAR\u2020. 17: return R, U . 18: end procedure\nProof. This follows by combining Lemma C.17 and C.18. Let U,R denote the output of procedure GeneralizedMatrixRowSubsetSelection,\n\u2016A\u2212 CUR\u20162F \u2264 (1 + )\u2016A\u2212 Z2Z>2 AR\u2020R\u20162F \u2264 (1 + )(1 + 60 )\u2016A\u2212\u03a0FC,k(A)\u20162F \u2264 (1 + 130 )\u2016A\u2212\u03a0FC,k(A)\u20162F .\nBecause R is a subset of rows of A and R has size O(k/ )\u00d7m, there must exist a diagonal matrix D \u2208 Rn\u00d7n with O(k/ ) nonzeros such that R = DA. This completes the proof.\nCorollary C.15 (A slightly different version of Theorem C.14, faster running time, and small input matrix). Given matrices A \u2208 Rn\u00d7m and C \u2208 Rn\u00d7k, if min(m,n) = poly(k, 1/ ), then there exists an algorithm which takes O(nnz(A)) + (m + n) poly(k, 1/ ) time and outputs a diagonal matrix D \u2208 Rn\u00d7n with d = O(k/ ) nonzeros (or equivalently a matrix R that contains d = O(k/ ) rescaled rows of A) and a matrix U \u2208 Rk\u00d7d such that\n\u2016CUDA\u2212A\u20162F \u2264 (1 + ) min X\u2208Rk\u00d7m \u2016CX \u2212A\u20162F\nholds with probability .99.\nProof. The log n factor comes from the adaptive sampling where we need to choose a Gaussian matrix with O(log n) rows and compute SA. If A has poly(k, 1/ ) columns, it is sufficient to choose S to be a CountSketch matrix with poly(k, 1/ ) rows. Then, we do not need a log n factor in the running time. If S has poly(k, 1/ ) rows, then we no longer need the matrix S.\nClaim C.16. Given matrices A \u2208 Rm\u00d7n and C \u2208 Rm\u00d7c, let Y \u2208 Rm\u00d7c,\u03a6 \u2208 Rc\u00d7c and \u2206 \u2208 Rc\u00d7k denote the output of procedure ApproxSubspaceSVD(A,C, k, ). Then with probability .99, we have,\n\u2016A\u2212 Y\u2206\u2206>Y >A\u20162F \u2264 (1 + 30 )\u2016A\u2212\u03a0FC,k(A)\u20162F .\nProof. This follows by Lemma 3.12 in [BW14].\nLemma C.17. The matrices R and Z2 in procedure GeneralizedMatrixRowSubsetSelection (Algorithm 7) satisfy with probability at least 0.17\u2212 2/n,\n\u2016A\u2212 Z2Z>2 AR\u2020R\u20162F \u2264 \u2016A\u2212\u03a0FC,k(A)\u20162F + 60 \u2016A\u2212\u03a0FC,k(A)\u20162F .\nProof. We can show,\n\u2016A\u2212 Z2Z>2 A\u20162F + 30 4820 \u2016A\u2212AR\u20201R1\u20162F\n= \u2016A\u2212BB\u2020A\u20162F + 30 4820 \u2016A\u2212AR\u20201R1\u20162F \u2264 \u2016A\u2212BB\u2020A\u20162F + 30 \u2016A\u2212Ak\u20162F \u2264 \u2016A\u2212 Y\u2206\u2206>Y A\u20162F + 30 \u2016A\u2212\u03a0FC,k(A)\u20162F \u2264 (1 + 30 )\u2016A\u2212\u03a0FC,k(A)\u20162F + 30 \u2016A\u2212\u03a0FC,k(A)\u20162F ,\nwhere the first step follows by the fact that Z2Z>2 = Z2DD\u22121Z>2 = (Z2D)(Z2D)\u2020 = BB\u2020, the second step follows by \u2016A \u2212 AR\u20201R1\u20162F \u2264 4820\u2016A \u2212 Ak\u20162F , the third step follows by B = Y\u2206 and B\u2020 = (Y\u2206)\u2020 = \u2206\u2020Y \u2020 = \u2206>Y >, and the last step follows by Claim C.16.\nLemma C.18. The matrices C,U and R in procedure GeneralizedMatrixRowSubsetSelection (Algorithm 7) satisfy that\n\u2016A\u2212 CUR\u20162F \u2264 (1 + )\u2016A\u2212 Z2Z>2 AR\u2020R\u20162F\nwith probability at least .99.\nProof. Let UR,\u03a3R, VR denote the SVD of R. Then VRV >R = R \u2020R.\nWe define Y \u2217 to be the optimal solution of\nmin X\u2208Rk\u00d7r\n\u2016WAVRV >R \u2212WC\u03a6\u22121\u2206D\u22121Y R\u20162F .\nWe define X\u0302\u2217 to be Y \u2217R \u2208 Rk\u00d7n, which is also equivalent to defining X\u0302\u2217 to be the optimal solution of\nmin X\u2208Rk\u00d7n\n\u2016WAVRV >R \u2212WC\u03a6\u22121\u2206D\u22121X\u20162F .\nFurthermore, it implies X\u0302\u2217 = (WC\u03a6\u22121\u2206D\u22121)\u2020WAVRV \u2020 R.\nWe also define X\u2217 to be the optimal solution of\nmin X\u2208Rk\u00d7n\n\u2016AVRV \u2020R \u2212 C\u03a6\u22121\u2206D\u22121X\u20162F ,\nwhich implies that,\nX\u2217 = (C\u03a6\u22121\u2206D\u22121)\u2020AVRV > R = Z > 2 AVRV > R .\nNow, we start to prove an upper bound on \u2016A\u2212 CUR\u20162F ,\n\u2016A\u2212 CUR\u20162F = \u2016A\u2212 C\u03a6\u22121\u2206D\u22121Y \u2217R\u20162F by definition of U = \u2016A\u2212 C\u03a6\u22121\u2206D\u22121X\u0302\u2217\u20162F by X\u0302\u2217 = Y \u2217R = \u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121X\u0302\u2217 +A\u2212AVRV >R \u20162F = \u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121X\u0302\u2217\u20162F\ufe38 \ufe37\ufe37 \ufe38\n\u03b1\n+ \u2016A\u2212AVRV >R \u20162F\ufe38 \ufe37\ufe37 \ufe38 \u03b2 , (12)\nwhere the last step follows by X\u0302\u2217 = MV >R , A \u2212 AVRV >R = A(I \u2212 VRV >R ) and the Pythagorean theorem. We show how to upper bound the term \u03b1,\n\u03b1 \u2264 (1 + )\u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121X\u2217\u20162F by Lemma C.19 = \u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121X\u2217\u20162F + \u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121X\u2217\u20162F = \u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121X\u2217\u20162F + \u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121(Z>2 AR\u2020R)\u20162F . (13)\nBy the Pythagorean theorem and the definition of Z2 (which means Z2 = C\u03a6\u22121\u2206D\u22121), we have,\n\u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121(Z>2 AR\u2020R)\u20162F + \u03b2 = \u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121(Z>2 AR\u2020R)\u20162F + \u2016A\u2212AVRV >R \u20162F = \u2016A\u2212 C\u03a6\u22121\u2206D\u22121(Z>2 AR\u2020R)\u20162F = \u2016A\u2212 Z2Z>2 AR\u2020R\u20162F . (14)\nCombining Equations (12), (13) and (14) together, we obtain,\n\u2016A\u2212 CUR\u20162F \u2264 \u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121X\u2217\u20162F + \u2016A\u2212 Z2Z>2 AR\u2020R\u20162F .\nWe want to show \u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121X\u2217\u20162F \u2264 \u2016A\u2212 Z2Z>2 AR\u2020R\u20162F ,\n\u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121X\u2217\u20162F = \u2016AVRV >R \u2212 C\u03a6\u22121\u2206D\u22121Z>2 AVRV >R \u20162F by X\u2217 = Z>2 AVRV >R \u2264 \u2016A\u2212 C\u03a6\u22121\u2206D\u22121Z>2 A\u20162F by properties of projections \u2264 \u2016A\u2212 C\u03a6\u22121\u2206D\u22121Z>2 AR\u2020R\u20162F by properties of projections = \u2016A\u2212 Z2Z>2 AR\u2020R\u20162F . by Z2 = C\u03a6\u22121\u2206D\u22121\nThis completes the proof.\nLemma C.19 ([CW13]). Let A \u2208 Rn\u00d7d have rank \u03c1 and B \u2208 Rn\u00d7r. Let W \u2208 Rr\u00d7n be a randomly chosen sparse subspace embedding with r = \u2126(\u03c12 \u22122). Let X\u0302\u2217 = arg min\nX\u2208Rd\u00d7r \u2016WAX \u2212WB\u20162F and let"}, {"heading": "X\u2217 = arg min", "text": "X\u2208Rd\u00d7r \u2016AX \u2212B\u20162F . Then with probability at least .99,\n\u2016AX\u0303\u2217 \u2212B\u20162F \u2264 (1 + )\u2016AX\u2217 \u2212B\u20162F .\nAlgorithm 8 Frobenius Norm Tensor Column, Row and Tube Subset Selection, Polynomial Time 1: procedure FCRTSelection(A,n, k, ) . Theorem C.20 2: s1 \u2190 s2 \u2190 O(k/ ). 3: Choose a Gaussian matrix S1 with s1 columns. . Definition B.18 4: Choose a Gaussian matrix S2 with s2 columns. . Definition B.18 5: Form matrix Z \u20323 by setting the (i, j)-th row to be the vectorization of (A1S1)i \u2297 (A2S2)j . 6: D3 \u2190GeneralizedMatrixRowSubsetSelection(A>3 , (Z \u20323)>,n2,n,s1s2, ). . Algorithm\n7 7: Let d3 denote the number of nonzero entries in D3. . d3 = O(s1s2/ ) 8: Form matrix Z \u20322 by setting the (i, j)-th row to be the vectorization of (A1S1)i \u2297 (A3S\u20323)j . 9: D2 \u2190GeneralizedMatrixRowSubsetSelection(A>2 , (Z \u20322)>,n2,n,s1d3, ). 10: Let d2 denote the number of nonzero entries in D2. . d2 = O(s1d3/ ) 11: Form matrix Z \u20321 by setting the (i, j)-th row to be the vectorization of (A2D2)i \u2297 (A3D3)j . 12: D1 \u2190GeneralizedMatrixRowSubsetSelection(A>1 , (Z \u20321)>,n2,n,d2d3, ). 13: Let d1 denote the number of nonzero entries in D1. . d1 = O(d2d3/ ) 14: C \u2190 A1D1, R\u2190 A2D2 and T \u2190 A3D3. 15: return C, R and T . 16: end procedure\nC.6 Column, row, and tube subset selection, (1 + )-approximation\nTheorem C.20. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm which takes O(nnz(A)) log n + n2 poly(log n, k, 1/ ) time and outputs three matrices: C \u2208 Rn\u00d7c, a subset of columns of A, R \u2208 Rn\u00d7r a subset of rows of A, and T \u2208 Rn\u00d7t, a subset of tubes of A where c = r = t = poly(k, 1/ ), and there exists a tensor U \u2208 Rc\u00d7r\u00d7t such that\n\u2016(((U \u00b7 T>)> \u00b7R>)> \u00b7 C>)> \u2212A\u20162F \u2264 (1 + ) min rank\u2212k Ak \u2016Ak \u2212A\u20162F ,\nor equivalently, \u2225\u2225\u2225\u2225\u2225\u2225 c\u2211\ni=1\nr\u2211\nj=1\nt\u2211\nl=1\nUi,j,l \u00b7 Ci \u2297Rj \u2297 Tl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min rank\u2212k Ak \u2016Ak \u2212A\u20162F\nholds with probability 9/10.\nProof. We fix V \u2217 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k. We define Z1 \u2208 Rk\u00d7n2 where the i-th row of Z1 is the vector Vi \u2297Wi. Choose sketching (Gaussian) matrix S1 \u2208 Rn2\u00d7s1 (Definition B.18), and let U\u0302 = A1S1(Z1S1) \u2020 \u2208 Rn\u00d7k. Following a similar argument as in the previous theorem, we have\n\u2016U\u0302Z1 \u2212A1\u20162F \u2264 (1 + ) OPT .\nWe fix U\u0302 and W \u2217. We define Z2 \u2208 Rk\u00d7n2 where the i-th row of Z2 is the vector U\u0302i \u2297W \u2217i . Choose sketching (Gaussian) matrix S2 \u2208 Rn2\u00d7s2 (Definition B.18), and let V\u0302 = A2S2(Z2S2)\u2020 \u2208 Rn\u00d7k. Following a similar argument as in the previous theorem, we have\n\u2016V\u0302 Z2 \u2212A2\u20162F \u2264 (1 + )2 OPT .\nWe fix U\u0302 and V\u0302 . Note that U\u0302 = A1S1(Z1S1)\u2020 and V\u0302 = A2S2(Z2S2)\u2020. We define Z3 \u2208 Rk\u00d7n2 such that the i-th row of Z3 is the vector U\u0302i \u2297 V\u0302i. Let z3 = s1 \u00b7 s2. We define Z \u20323 \u2208 Rz3\u00d7n 2 such\nthat, \u2200i \u2208 [s1],\u2200j \u2208 [s2], the i+ (j\u2212 1)s1-th row of Z \u20323 is the vector (A1S1)i\u2297 (A2S2)j . We consider the following objective function,\nmin W\u2208Rn\u00d7k,X\u2208Rk\u00d7z3 \u2016WXZ \u20323 \u2212A3\u20162F \u2264 min W\u2208Rn\u00d7k \u2016WZ3 \u2212A3\u20162F \u2264 (1 + )2 OPT .\nUsing Theorem C.14, we can find a diagonal matrix D3 \u2208 Rn2\u00d7n2 with d3 = O(z3/ ) = O(k2/ 3) nonzero entries such that\nmin X\u2208Rd3\u00d7z3\n\u2016A3D3XZ \u20323 \u2212A3\u20162F \u2264 (1 + )3 OPT .\nIn the following, we abuse notation and let A3D3 \u2208 Rn\u00d7d3 by deleting zero columns. Let W \u2032 denote A3D3 \u2208 Rn\u00d7d3 . Then,\nmin X\u2208Rd3\u00d7z3\n\u2016W \u2032XZ \u20323 \u2212A3\u20162F \u2264 (1 + )3 OPT .\nWe fix U\u0302 and W \u2032. Let z2 = s1 \u00b7 d3. We define Z \u20322 \u2208 Rz2\u00d7n 2 such that, \u2200i \u2208 [s1], \u2200j \u2208 [d3], the i+ (j \u2212 1)s1-th row of Z \u20322 is the vector (A1S1)i \u2297 (A3D3)j . Using Theorem C.14, we can find a diagonal matrix D2 \u2208 Rn2\u00d7n2 with d2 = O(z2/ ) = O(s1d3/ ) = O(k 3/ 5) nonzero entries such that\nmin X\u2208Rd2\u00d7z2\n\u2016A2D2XZ \u20322 \u2212A2\u20162F \u2264 (1 + )4 OPT .\nLet V \u2032 denote A2D2. Then,\nmin X\u2208Rd2\u00d7z2\n\u2016V \u2032XZ \u20322 \u2212A2\u20162F \u2264 (1 + )4 OPT .\nWe fix V \u2032 and W \u2032. Let z1 = d2 \u00b7 d3. We define Z \u20321 \u2208 Rz1\u00d7n 2 such that, \u2200i \u2208 [d2],\u2200j \u2208 [d3], the i+ (j \u2212 1)s1-th row of Z \u20321 is the vector (A2D2)i \u2297 (A3D3)j . Using Theorem C.14, we can find a diagonal matrix D1 \u2208 Rn2\u00d7n2 with d1 = O(z1/ ) = O(d2d3/ ) = O(k 5/ 9) nonzero entries such that\nmin X\u2208Rd1\u00d7z1\n\u2016A1D1XZ \u20321 \u2212A1\u20162F \u2264 (1 + )5 OPT .\nLet U \u2032 denote A1D1. Then,\nmin X\u2208Rd1\u00d7z1\n\u2016U \u2032XZ \u20321 \u2212A1\u20162F \u2264 (1 + )5 OPT .\nPutting U \u2032, V \u2032,W \u2032 all together, we complete the proof.\nTheorem C.21. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm which takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices: C \u2208 Rn\u00d7c, a subset of columns of A, R \u2208 Rn\u00d7r a subset of rows of A, and T \u2208 Rn\u00d7t, a subset of tubes of A where c = r = t = poly(k, 1/ ), and there exists a tensor U \u2208 Rc\u00d7r\u00d7t such that\n\u2016U(C,R, T )\u2212A\u20162F \u2264 (1 + ) min rank\u2212k Ak \u2016Ak \u2212A\u20162F ,\nor equivalently, \u2225\u2225\u2225\u2225\u2225\u2225 c\u2211\ni=1\nr\u2211\nj=1\nt\u2211\nl=1\nUi,j,l \u00b7 Ci \u2297Rj \u2297 Tl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min rank\u2212k Ak \u2016Ak \u2212A\u20162F\nholds with probability 9/10.\nAlgorithm 9 Frobenius Norm Tensor Column, Row and Tube Subset Selection, Input Sparsity Time 1: procedure FCRTSelection(A,n, k, ) . Theorem C.21 2: s1 \u2190 s2 \u2190 O(k/ ). 3: 0 \u2190 0.001. 4: Choose a Gaussian matrix S1 with s1 columns. . Definition B.18 5: Choose a Gaussian matrix S2 with s2 columns. . Definition B.18 6: Form matrix B1 by setting (i, j)-th column to be (A1S1)i. 7: Form matrix B2 by setting (i, j)-th column to be (A2S2)j . . Z \u20323 = B>1 B>2 8: d3 \u2190 O(s1s2 log(s1s2) + (s1s2/ )). 9: D3 \u2190FastTensorLeverageScoreGeneralOrder(B>1 , B>2 , n, n, s1s2, 0, d1). .\nAlgorithm 15 10: Form matrix B1 by setting (i, j)-th column to be (A1S1)i. 11: Form matrix B3 by setting (i, j)-th column to be (A3D3)j . . Z \u20322 = B>1 B>3 12: d2 \u2190 O(s1d3 log(s1d3) + (s1d3/ )). 13: D2 \u2190FastTensorLeverageScoreGeneralOrder(B>1 , B>3 , n, n, s1d3, 0, d2). 14: Form matrix B2 by setting (i, j)-th column to be (A2D2)i. 15: Form matrix B3 by setting (i, j)-th column to be (A3D3)j . . Z \u20321 = B>2 B>3 16: d1 \u2190 O(d2d3 log(d2d3) + (d2d3/ )). 17: D1 \u2190FastTensorLeverageScoreGeneralOrder(B>2 , B>3 , n, n, d2d3, 0, d1). 18: C \u2190 A1D1, R\u2190 A2D2 and T \u2190 A3D3. 19: return C, R and T . 20: end procedure\nProof. The proof is similar to the proof of Theorem C.20, except we replace generalized matrix row subset by tensor leverage score sampling (implicitly).\nC.7 CURT decomposition, (1 + )-approximation\nC.7.1 Properties of leverage score sampling and BSS sampling\nNotice that, the BSS algorithm is a deterministic procedure developed in [BSS12] for selecting rows from a matrix A \u2208 Rn\u00d7d (with \u2016A\u20162 \u2264 1 and \u2016A\u20162F \u2264 k) using a selection matrix S so that\n\u2016A>S>SA\u2212A>A\u20162 \u2264 .\nThe algorithm runs in poly(n, d, 1/ ) time. Using the ideas from [BW14] and [CEM+15], we are able to reduce the number of nonzero entries from O( \u22122k log k) to O( \u22122k), and also improve the running time to input sparsity.\nLemma C.22 (Leverage score preserves subspace embedding - Theorem 2.11 in [Woo14]). Given a rank-k matrix A \u2208 Rn\u00d7d, via leverage score sampling, we can obtain a diagonal matrix D with m nonzero entries such that, letting B = DA, if m = O( \u22122k log k), then, with probability at least 0.999, for all x \u2208 Rd,\n(1\u2212 )\u2016Ax\u20162 \u2264 \u2016Bx\u20162 \u2264 (1 + )\u2016Ax\u20162\nLemma C.23. Given a rank-k matrix A \u2208 Rn\u00d7d, there exists an algorithm that runs in O(nnz(A)+ n poly(k, 1/ )) time and outputs a matrix B containing O( \u22122k log k) re-weighted rows of A, such\nthat with probability at least 0.999, for all x \u2208 Rd,\n(1\u2212 )\u2016Ax\u20162 \u2264 \u2016Bx\u20162 \u2264 (1 + )\u2016Ax\u20162\nProof. We choose a sparse embedding matrix (Definition B.16) \u03a0 \u2208 Rd\u00d7s with s = poly(k/ ). With probability at least 0.999, \u03a0> is a subspace embedding of A>. Thus, rank(A\u03a0) = rank(A). Also, the leverage scores of A\u03a0 are the same as those of A. Thus, we can compute the leverage scores of A\u03a0. The running time of computing A\u03a0 is O(nnz(A)). Thus the total running time is O(nnz(A) + n poly(k, 1/ )).\nLemma C.24. Let B denote a matrix which contains O( \u22122k log k) rows of A \u2208 Rn\u00d7d. Choosing \u03a0 to be a sparse subspace embedding matrix of size d\u00d7O( \u22126(k log k)2), with probability at least 0.999,\n\u2016B\u03a0\u03a0>B> \u2212BB>\u20162 \u2264 \u2016B\u201622.\nCombining Lemma C.23, C.24 and the BSS algorithm, we obtain:\nLemma C.25. Given a rank-k matrix A \u2208 Rn\u00d7d, there exists an algorithm that runs in O(nnz(A)+ n poly(k, 1/ )) time and outputs a sampling and rescaling diagonal matrix S that selects O( \u22122k) re-weighted rows of A, such that, with probability at least 0.999,\n\u2016A>S>SA\u2212A>A\u20162 \u2264 \u2016A\u201622.\nor equivalently, for all x \u2208 Rd,\n(1\u2212 )\u2016Ax\u20162 \u2264 \u2016SAx\u20162 \u2264 (1 + )\u2016Ax\u20162.\nProof. Using Lemma C.23, we can obtain B. Then we apply a sparse subspace embedding matrix \u03a0 on the right of B. At the end, we run the BSS algorithm on B\u03a0 and we are able to output O( \u22122k) re-weighted rows of B\u03a0. Using these rows, we are able to determine O( \u22122k) re-weighted rows of A.\nC.7.2 Row sampling for linear regression\nTheorem C.26 (Theorem 5 in [CNW15]). We are given A \u2208 Rn\u00d7d with \u2016A\u201622 \u2264 1 and \u2016A\u20162F \u2264 k, and an \u2208 (0, 1). There exists a diagonal matrix S with O(k/ 2) nonzero entries such that\n\u2016(SA)>SA\u2212A>A\u20162 \u2264 .\nCorollary C.27. Given a rank-k matrix A \u2208 Rn\u00d7d, vector b \u2208 Rn, and parameter > 0, let U \u2208 Rn\u00d7(k+1) denote an orthonormal basis of [A, b]. Let S \u2208 Rn\u00d7n denote a sampling and rescaling diagonal matrix according to Leverage score sampling and sparse BSS sampling of U with m nonzero entries. If m = O(k), then S is a (1\u00b11/2) subspace embedding for U ; if m = O(k/ ), then S satisfies\u221a -operator norm approximate matrix product for U .\nProof. This follows by Lemma C.22, Lemma C.24 and Theorem C.26.\nLemma C.28 ([NW14]). Given A \u2208 Rn\u00d7d and b \u2208 Rn, let S \u2208 Rn\u00d7n denote a sampling and rescaling diagonal matrix. Let x\u2217 denote arg minx \u2016Ax\u2212 b\u201622 and x\u2032 denote arg minx \u2016SAx\u2212 Sb\u201622. If S is a (1 \u00b1 1/2) subspace embedding for the column span of A, and \u2032 (=\u221a )-operator norm approximate matrix product for U adjoined with b\u2212Ax\u2217, then, with probability at least .999,\n\u2016Ax\u2032 \u2212 b\u201622 \u2264 (1 + )\u2016Ax\u2217 \u2212 b\u201622.\nProof. We define OPT = min x \u2016Ax\u2212b\u20162. We define x\u2032 = arg min x \u2016SAx\u2212Sb\u201622 and x\u2217 = arg min x \u2016Ax\u2212 b\u201622. Let w = b \u2212 Ax\u2217. Let U denote an orthonormal basis of A. We can write Ax\u2032 \u2212 Ax\u2217 = U\u03b2. Then, we have,\n\u2016Ax\u2032 \u2212 b\u201622 = \u2016Ax\u2032 \u2212Ax\u2217 +AA\u2020b\u2212 b\u201622 by x\u2217 = A\u2020b = \u2016U\u03b2 + (UU> \u2212 I)b\u201622 = \u2016Ax\u2217 \u2212Ax\u2032\u201622 + \u2016Ax\u2217 \u2212 b\u201622 by Pythagorean Theorem = \u2016U\u03b2\u201622 + OPT2\n= \u2016\u03b2\u201622 + OPT2 .\nIf S is a (1\u00b1 1/2) subspace embedding for U , then we can show\n\u2016\u03b2\u20162 \u2212 \u2016U>S>SU\u03b2\u20162 \u2264 \u2016\u03b2 \u2212 U>S>SU\u03b2\u20162 by triangle inequality = \u2016(I \u2212 U>S>SU)\u03b2\u20162 \u2264 \u2016I \u2212 U>S>SU\u20162 \u00b7 \u2016\u03b2\u20162 \u2264 1\n2 \u2016\u03b2\u20162.\nThus, we obtain\n\u2016U>S>SU\u03b2\u20162 \u2265 \u2016\u03b2\u20162/2.\nNext, we can show\n\u2016U>S>SU\u03b2\u20162 = \u2016U>S>S(Ax\u2032 \u2212Ax\u2217)\u201622 = \u2016U>S>S(A(SA)\u2020Sb\u2212Ax\u2217)\u20162 by x\u2032 = (SA)\u2020Sb = \u2016U>S>S(b\u2212Ax\u2217)\u20162 by SA(SA)\u2020 = I = \u2016U>S>Sw\u20162. by w = b\u2212Ax\u2217\nWe define U \u2032 = [ U w/\u2016w\u20162 ] . We define X and y to satisfy U = U \u2032X and w = U \u2032y. Then, we have\n\u2016U>S>Sw\u20162 = \u2016U>S>Sw \u2212 U>w\u20162 by U>w = 0 = \u2016X>U \u2032>S>SU \u2032y \u2212X>U \u2032>U \u2032y\u20162 = \u2016X>(U \u2032>S>SU \u2032 \u2212 I)y\u20162 \u2264 \u2016X\u20162 \u00b7 \u2016U \u2032>S>SU \u2032 \u2212 I\u20162 \u00b7 \u2016y\u20162 \u2264 \u2032\u2016X\u20162\u2016y\u20162 = \u2032\u2016U\u20162\u2016w\u20162 = \u2032OPT, by \u2016U\u20162 = 1 and \u2016w\u20162 = OPT\nwhere the fifth inequality follows since S satisfies \u2032-operator norm approximate matrix product for the column span of U adjoined with w.\nPutting it all together, we have\n\u2016Ax\u2032 \u2212 b\u201622 = \u2016Ax\u2217 \u2212 b\u201622 + \u2016Ax\u2217 \u2212Ax\u2032\u201622 = OPT2 +\u2016\u03b2\u201622 \u2264 OPT2 +4\u2016U>S>Sw\u201622 \u2264 OPT2 +4( \u2032OPT)2\n\u2264 (1 + ) OPT2 . by \u2032 = 1 2\n\u221a .\nFinally, note that S satisfies \u2032-operator norm approximate matrix product for U adjoined with w if it is a (1\u00b1 \u2032)-subspace embedding for U adjoined with w, which holds using BSS sampling by Theorem 5 of [CNW15] with O(d/ ) samples.\nC.7.3 Leverage scores for multiple regression\nLemma C.29 (see, e.g., Lemma 32 in [CW13] among other places). Given matrix A \u2208 Rn\u00d7d with orthonormal columns, and parameter > 0, if S \u2208 Rn\u00d7n is a sampling and rescaling diagonal matrix according to the leverage scores of A where the number of nonzero entries is t = O(1/ 2), then, for any B \u2208 Rn\u00d7m, we have\n\u2016A>S>SB \u2212A>B\u20162F < 2\u2016A\u20162F \u2016B\u20162F ,\nholds with probability at least 0.9999.\nCorollary C.30. Given matrix A \u2208 Rn\u00d7d with orthonormal columns, and parameter > 0, if S \u2208 Rn\u00d7n is a sampling and rescaling diagonal matrix according to the leverage scores of A with m nonzero entries, then if m = O(d log d), then S is a (1 \u00b1 1/2) subspace embedding for A. If m = O(d/ ), then S satisfies \u221a /d-Frobenius norm approximate matrix product for A.\nProof. This follows by Lemma C.22 and Lemma C.29.\nLemma C.31 ([NW14]). Given A \u2208 Rn\u00d7d and B \u2208 Rn\u00d7m, let S \u2208 Rn\u00d7n denote a sampling and rescaling matrix according to A. Let X\u2217 denote arg minX \u2016AX\u2212B\u20162F and X \u2032 denote arg minX \u2016SAX\u2212 SB\u20162F . Let U denote an orthonormal basis for A. If S is a (1 \u00b1 1/2) subspace embedding for U , and satisfies \u2032(= \u221a /d)-Frobenius norm approximate matrix product for U , then, we have that\n\u2016AX \u2032 \u2212B\u20162F \u2264 (1 + )\u2016AX\u2217 \u2212B\u20162F\nholds with probability at least 0.999.\nProof. We define OPT = minX \u2016AX \u2212 B\u2016F . Let A = U\u03a3V > denote the SVD of A. Since A has rank k, U and V have k columns. We can write A(X \u2032 \u2212X\u2217) = U\u03b2. Then, we have\n\u2016AX \u2032 \u2212B\u20162F = \u2016AX \u2032 \u2212AX\u2217 +AA\u2020B \u2212B\u20162F by X\u2217 = A\u2020B = \u2016U\u03b2 + (UU> \u2212 I)B\u20162F = \u2016AX\u2217 \u2212AX \u2032\u20162F + \u2016AX\u2217 \u2212B\u20162F by Pythagorean Theorem = \u2016U\u03b2\u20162F + OPT2\n= \u2016\u03b2\u20162F + OPT2 . (15)\nIf S is a (1\u00b1 1/2) subspace embedding for U , then we can show,\n\u2016\u03b2\u2016F \u2212 \u2016U>S>SSU\u03b2\u2016F \u2264 \u2016\u03b2 \u2212 U>S>SU\u03b2\u2016F by triangle inequality = \u2016(I \u2212 U>S>SU)\u03b2\u2016F \u2264 \u2016(I \u2212 U>S>SU)\u20162 \u00b7 \u2016\u03b2\u2016F by \u2016AB\u2016F \u2264 \u2016A\u20162\u2016B\u2016F \u2264 1\n2 \u2016\u03b2\u2016F . by \u2016(I \u2212 U>S>SU)\u20162 \u2264 1/2\nThus, we obtain\n\u2016U>S>SU\u03b2\u2016F \u2265 \u2016\u03b2\u2016F /2. (16)\nNext, we can show\n\u2016U>S>SU\u03b2\u2016F = \u2016U>S>S(AX \u2032 \u2212AX\u2217)\u2016F = \u2016U>S>S(A(SA)\u2020Sb\u2212AX\u2217)\u2016F by X \u2032 = (SA)\u2020SB = \u2016U>S>S(B \u2212AX\u2217)\u2016F . by SA(SA)\u2020 = I\nThen, we can show\n\u2016U>S>S(B \u2212AX\u2217)\u2016F \u2264 \u2032\u2016U>\u2016F \u2016B \u2212AX\u2217\u2016F by Lemma C.29 = \u2032 \u221a dOPT . by \u2016U\u2016F = \u221a d and \u2016B \u2212AX\u2217\u2016F = OPT\n(17)\nPutting it all together, we have\n\u2016AX \u2032 \u2212B\u20162F = \u2016AX\u2217 \u2212B\u20162F + \u2016AX\u2217 \u2212AX \u2032\u20162F = OPT2 +\u2016\u03b2\u20162F by Equation (15) \u2264 OPT2 +4\u2016U>S>Sw\u20162F by Equation (16) \u2264 OPT2 +4( \u2032 \u221a dOPT)2 by Equation (17)\n\u2264 (1 + ) OPT2 . by \u2032 = 1 2\n\u221a /d\nC.7.4 Sampling columns according to leverage scores implicitly, improving polynomial running time to nearly linear running time\nThis section explains an algorithm that is able to sample from the leverage scores from the product of two matrices U, V without explicitly writing down U V . To build this algorithm we combine TensorSketch, some ideas from [DMIMW12] and some ideas from [AKO11, MW10]. Finally, we are able to improve the running time of sampling columns according to leverage scores from \u2126(n2) to O\u0303(n). Given two matrices U, V \u2208 Rk\u00d7n, we define A \u2208 Rk\u00d7n1n2 to be the matrix where the i-th row of A is the vectorization of U i \u2297 V i, \u2200i \u2208 [k]. Na\u00efvely, in order to sample O(poly(k, 1/ )) rows from A> according to leverage scores, we need to write down n2 leverage scores. This approach will take at least \u2126(n2) running time. In the rest of this section, we will explain how to do it in O(n \u00b7 poly(log n, k, 1/ )) time. In Section C.10.1, we will explain how to extend this idea from 3rd order tensors to general q-th order tensors and remove the poly(log n) factor from running time, i.e., obtain O(n \u00b7 poly(k, 1/ )) time.\nAlgorithm 10 Fast Tensor Leverage Score Sampling 1: procedure FastTensorLeverageScore(U, V, n1, n2, k, , Rsamples) . Lemma C.32 2: s1 \u2190 poly(k, 1/ ). 3: g1 \u2190 g2 \u2190 g3 \u2190 O( \u22122 log(n1n2)). 4: Choose \u03a0 \u2208 Rn1n2\u00d7s1 to be a TensorSketch. . Definition B.34 5: Compute R\u22121 \u2208 Rk\u00d7k by using (U V )\u03a0. . U \u2208 Rk\u00d7n1 , V \u2208 Rk\u00d7n2 6: Choose G1 \u2208 Rg1\u00d7k to be a Gaussian sketching matrix. 7: for i = 1\u2192 g1 do 8: w \u2190 (GiR\u22121)> . Gi denotes the i-th row of G 9: for j = 1\u2192 [n1] do . Form matrix U \u2032i \u2208 Rk\u00d7n1 10: U \u2032ij \u2190 w \u25e6 Uj , \u2200j \u2208 [n1]. . Uj denotes the j-th column of U \u2208 Rk\u00d7n1 11: end for 12: end for 13: Choose G2,i \u2208 Rg2\u00d7n1 to be a Gaussian sketching matrix. 14: for i = 1\u2192 g1 do 15: \u03b1i \u2190 \u2016(G2,iU \u2032i>)V \u20162F . 16: Choose G3,i \u2208 Rg3\u00d7n1 to be a Gaussian sketching matrix. 17: for j2 = 1\u2192 n2 do 18: \u03b2i,j \u2190 \u2016G3,i(U \u2032i>)Vj2\u201622. 19: end for 20: end for 21: S \u2190 \u2205. 22: for r = 1\u2192 Rsamples do 23: Sample i from [g1] with probability \u03b1i/ \u2211g1 i\u2032=1 \u03b1i\u2032 .\n24: Sample j2 from [n2] with probability \u03b2i,j2/ \u2211n2\nj\u20322=1 \u03b2i,j\u20322 .\n25: for j1 = 1\u2192 n1 do 26: \u03b3j1 \u2190 ((U \u2032i>)j1Vj2)2. 27: end for 28: Sample j1 from [n1] with probability \u03b3j1/ \u2211n1 j\u20321=1\n\u03b3j\u20321 . 29: S \u2190 S \u222a (j1, j2). 30: end for 31: Convert S into a diagonal matrix D with at most Rsamples nonzero entries. 32: return D. . Diagonal matrix D \u2208 Rn1n2\u00d7n1n2 33: end procedure\nLemma C.32. Given two matrices U \u2208 Rk\u00d7n1 and V \u2208 Rk\u00d7n2, there exists an algorithm that takes O((n1 + n2) \u00b7 poly(log(n1n2), k) \u00b7Rsamples) time and samples Rsamples columns of U V \u2208 Rk\u00d7n1n2 according to the leverage scores of R\u22121(U V ), where R is the R of a QR factorization.\nProof. We choose \u03a0 \u2208 Rn1n2\u00d7s1 to be a TensorSketch. Then, according to Section B.10, we can compute R\u22121 in n \u00b7 poly(log n, k, 1/ ) time, where R is the R in a QR-factorization. We want to sample columns from U V according to the square of the `2-norms of each column of R\u22121(U V ). However, explicitly writing down the matrix R\u22121(U V ) takes kn1n2 time, and the number of columns is already n1n2. The goal is to sample columns from R\u22121(U V ) without explicitly computing the square of the `2-norm of each column.\nThe first simple observation is that the following two sampling procedures are equivalent in terms of the column samples of a matrix that they take. (1) We sample a single entry from the\nmatrix R\u22121(U V ) proportional to its squared value. (2) We sample a column from the matrix R\u22121(U V ) proportional to its squared `2-norm. Let the (i, j1, j2)-th entry denote the entry in the i-th row and the (j1 \u2212 1)n2 + j2-th column. We can show, for a particular column (j1 \u2212 1)n2 + j2,\nPr[sample an entry from the (j1 \u2212 1)n2 + j2 th column of a matrix]\n= k\u2211\ni=1\nPr[sample the (i, j1, j2)-th entry of matrix]\n=\nk\u2211\ni=1\n|(R\u22121(U V ))i,(j1\u22121)n2+j2 |2 \u2016R\u22121(U V )\u20162F\n= \u2016(R\u22121(U V ))(j1\u22121)n2+j2\u20162 \u2016R\u22121(U V )\u20162F = Pr[sample the (j1 \u2212 1)n2 + j2 th column of matrix]. (18)\nThus, it is sufficient to show how to sample a single entry from matrix R\u22121(U V ) proportional to its squared value without writing down all of the entries of a k \u00d7 n1n2 matrix.\nWe choose a Gaussian matrix G1 \u2208 Rg1\u00d7k with g1 = O( \u22122 log(n1n2)). By Claim C.33 we can reduce the length of each column vector of matrixR\u22121U V from k to g1 while preserving the squared `2-norm of all columns simultaneously. Thus, we obtain a new matrix GR\u22121(U V ) \u2208 Rg1\u00d7n1n2 , and sampling from this new matrix is equivalent to sampling from the original matrix R\u22121(U V ).\nIn the following paragraphs, we explain a sampling procedure (also described in Procedure FastTensorLeverageScore in Algorithm 10) which contains three sampling steps. The first step is sampling i from [g1], the second step is sampling j2 from [n2], and the last step is sampling j1 from [n1].\nFor each j1 \u2208 [n1], let Uj1 denote the j1-th column of U . For each i \u2208 [g1], let Gi1 denote the i-th row of matrix G1 \u2208 Rg1\u00d7k, let U \u2032i \u2208 Rk\u00d7n1 denote a matrix where the j1-th column is (GiR\u22121)> \u25e6Uj1 \u2208 Rk, \u2200j \u2208 [n1]. Then, using Claim C.37, we have that (GiR\u22121) \u00b7 (U V ) \u2208 Rn1n2 is a row vector where the entry in the (j1\u22121)n2 +j2-th coordinate is the entry in the j1-th row and j2th column of matrix (U \u2032i>V ) \u2208 Rn1\u00d7n2 . Further, the squared `2-norm of vector (GiR\u22121) \u00b7 (U V ) is equal to the squared Frobenius norm of matrix (U \u2032i>V ). Thus, sampling i proportional to the squared `2-norm of vector (GiR\u22121) \u00b7 (U V ) is equivalent to sampling i proportional to the squared Frobenius norm of matrix (U \u2032i>V ). Na\u00efvely, computing the Frobenius norm of an n1 \u00d7 n2 matrix requires O(n1n2) time. However, we can choose a Gaussian matrix G2,i \u2208 Rg2\u00d7n1 to sample according to the value \u2016(G2,iU \u2032i>)V \u20162F , which can be computed in O((n1 + n2)g2k) time. By claim C.35, \u2016(G2,iU \u2032i>)V \u20162F \u2248 \u2016(U \u2032i>)V \u20162F with high probability. So far, we have finished the first step of the sampling procedure.\nFor the second step of the sampling procedure, we need to sample j2 from [n2]. To do that, we need to compute the squared `2-norm of each column of U \u2032i>V \u2208 Rn1\u00d7n2 . This can be done by choosing another Gaussian matrix G3,i \u2208 Rg3\u00d7n1 . For all j2 \u2208 [n2], by Claim C.36, we have \u2016G3,iU \u2032i>Vj2\u201622 \u2248 \u2016U \u2032i>Vj2\u201622. Also, for j2 \u2208 [n2], \u2016G3,iU \u2032i>Vj2\u201622 can be computed in nearly linear in n1 + n2 time.\nFor the third step of the sampling procedure, we need to sample j1 from [n1]. Since we already have i and j2 from the previous two steps, we can directly compute |(U \u2032i>)j1Vj2 |2, for all j1. This only takes O(n1k) time.\nOverall, the running time is O((n1 + n2) \u00b7 poly(log(n1n2), k, 1/ )). Because our estimates are accurate enough, our sampling probabilities are also good approximations to the leverage score sampling probabilities. Putting it all together, we complete the proof.\nClaim C.33. Given matrix R\u22121(U V ) \u2208 Rk\u00d7n1n2, let G1 \u2208 Rg1\u00d7k denote a Gaussian matrix with g1 = ( \u22122 log(n1n2)). Then with probability at least 1\u2212 1/ poly(n1n2), we have: for all j \u2208 [n1n2],\n(1\u2212 )\u2016R\u22121(U V )j\u201622 \u2264 \u2016G1R\u22121(U V )j\u201622 \u2264 (1 + )\u2016R\u22121(U V )j\u201622.\nProof. This follows by the Johnson-Lindenstrauss Lemma.\nClaim C.34. For a fixed i \u2208 [g1], let G2,i \u2208 Rg2\u00d7n1 denote a Gaussian matrix with g2 = O( \u22122 log(n1n2)). Then with probability at least 1\u2212 1/ poly(n1n2), we have: for all j2 \u2208 [n2],\n(1\u2212 )\u2016U \u2032i>Vj2\u201622 \u2264 \u2016(G2,iU \u2032i>)Vj2\u20162 \u2264 (1 + )\u2016U \u2032i>Vj2\u201622.\nBy taking the union bound over all i \u2208 [g1], we obtain a stronger claim,\nClaim C.35. With probability at least 1\u22121/ poly(n1n2), we have : for all i \u2208 [g1], for all j2 \u2208 [n2],\n(1\u2212 )\u2016U \u2032i>Vj2\u201622 \u2264 \u2016(G2,iU \u2032i>)Vj2\u20162 \u2264 (1 + )\u2016U \u2032i>Vj2\u201622.\nSimilarly, if we choose G3,i to be a Gaussian matrix, we can obtain the same result as for G2,i:\nClaim C.36. With probability at least 1\u22121/ poly(n1n2), we have : for all i \u2208 [g1], for all j2 \u2208 [n2],\n(1\u2212 )\u2016U \u2032i>Vj2\u201622 \u2264 \u2016(G3,iU \u2032i>)Vj2\u20162 \u2264 (1 + )\u2016U \u2032i>Vj2\u201622.\nClaim C.37. For any i \u2208 [g1], j1 \u2208 [n1], j2 \u2208 [n2], let Gi1 denote the i-th row of matrix G1 \u2208 Rg1\u00d7k. Let (U V )(j1\u22121)n2+j2 denote the (j1\u2212 1)n2 + j2-th column of matrix Rk\u00d7n1n2 . Let (U \u2032i>)j1 denote the j1-th row of matrix (U \u2032i>) \u2208 Rn1\u00d7k. Let Vj2 denote the j2-th column of matrix V \u2208 Rk\u00d7n2. Then, we have\nGi1R \u22121(U V )(j1\u22121)n2+j2 = (U \u2032i>)j1Vj2 .\nProof. This follows by,\nGi1R \u22121(U V )(j1\u22121)n2+j2 = Gi1R\u22121(Uj1 \u25e6 Vj2) = (Gi1R\u22121 \u25e6 (Uj1)>)Vj2 = (U \u2032i>)j1Vj2 .\nLemma C.38. Given A \u2208 Rn\u00d7n2, V,W \u2208 Rk\u00d7n, for any > 0, there exists an algorithm that runs in O(n \u00b7 poly(k, 1/ )) time and outputs a diagonal matrix D \u2208 Rn2\u00d7n2 with m = O(k log k + k/ ) nonzero entries such that,\n\u2016U\u0302(V W )\u2212A\u20162F \u2264 (1 + ) min U\u2208Rn\u00d7k \u2016U(V W )\u2212A\u20162F ,\nholds with probability at least 0.999, where U\u0302 denotes the optimal solution to minU \u2016U(V W )D\u2212 AD\u20162F .\nProof. This follows by combining Theorem C.46, Corollary C.30, and Lemma C.31.\nRemark C.39. Replacing Theorem C.46 (Algorithm 15) by Lemma C.32 (Algorithm 10), we can obtain a slightly different version of Lemma C.38 with n poly(log n, k, 1/ ) running time, where the dependence on k is better.\nAlgorithm 11 Frobenius Norm CURT Decomposition Algorithm, Input Sparsity Time and Nearly Optimal Number of Samples 1: procedure FCURTInputSparsity(A,UB, VB,WB, n, k, ) . Theorem C.40 2: d1 \u2190 d2 \u2190 d3 \u2190 O(k log k + k/ ). 3: 0 \u2190 0.01. 4: Form B1 = V >B W>B \u2208 Rk\u00d7n\n2 . 5: D1 \u2190FastTensorLeverageScoreGeneralOrder(V >B ,W>B , n, n, k, 0, d1). .\nAlgorithm 15 6: Form U\u0302 = A1D1(B1D1)\u2020 \u2208 Rn\u00d7k. 7: Form B2 = U\u0302> W>B \u2208 Rk\u00d7n\n2 . 8: D2 \u2190FastTensorLeverageScoreGeneralOrder(U\u0302>,W>B , n, n, k, 0, d2). 9: Form V\u0302 = A2D2(B2D2)\u2020 \u2208 Rn\u00d7k.\n10: Form B3 = U\u0302> V\u0302 > \u2208 Rk\u00d7n2 . 11: D3 \u2190FastTensorLeverageScoreGeneralOrder(U\u0302>, V\u0302 >, n, n, k, 0, d3). 12: C \u2190 A1D1, R\u2190 A2D2, T \u2190 A3D3. 13: U \u2190\u2211ki=1((B1D1)\u2020)i \u2297 ((B2D2)\u2020)i \u2297 ((B3D3)\u2020)i. 14: return C, R, T and U . 15: end procedure\nC.7.5 Input sparsity time algorithm\nTheorem C.40. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, let k \u2265 1, and let UB, VB,WB \u2208 Rn\u00d7k denote a rank-k, \u03b1-approximation to A. Then there exists an algorithm which takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices C \u2208 Rn\u00d7c with columns from A, R \u2208 Rn\u00d7r with rows from A, T \u2208 Rn\u00d7t with tubes from A, and a tensor U \u2208 Rc\u00d7r\u00d7t with rank(U) = k such that c = r = t = O(k log k + k/ ), and\n\u2225\u2225\u2225\u2225\u2225\u2225 c\u2211\ni=1\nr\u2211\nj=1\nt\u2211\nl=1\nUi,j,l \u00b7 Ci \u2297Rj \u2297 Tl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + )\u03b1 min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F\nholds with probability 9/10.\nProof. We define\nOPT := min rank\u2212k A\u2032\n\u2016A\u2032 \u2212A\u20162F .\nWe already have three matrices UB \u2208 Rn\u00d7k, VB \u2208 Rn\u00d7k and WB \u2208 Rn\u00d7k and these three matrices provide a rank-k, \u03b1-approximation to A, i.e.,\n\u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(UB)i \u2297 (VB)i \u2297 (WB)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 \u03b1OPT . (19)\nLet B1 = V >B W>B \u2208 Rk\u00d7n 2 denote the matrix where the i-th row is the vectorization of (VB)i \u2297 (WB)i. Let D1 \u2208 Rn 2\u00d7n2 be a sampling and rescaling matrix corresponding to sampling by the leverage scores of B>1 ; there are d1 nonzero entries on the diagonal of D1. Let Ai \u2208 Rn\u00d7n 2 denote the matrix obtained by flattening A along the i-th direction, for each i \u2208 [3].\nDefine U\u2217 \u2208 Rn\u00d7k to be the optimal solution to min U\u2208Rn\u00d7k \u2016UB1\u2212A1\u20162F , U\u0302 = A1D1(B1D1)\u2020 \u2208 Rn\u00d7k,\nand V0 \u2208 Rn\u00d7k to be the optimal solution to min V \u2208Rn\u00d7k \u2016V \u00b7 (U\u0302> W>B )\u2212A2\u20162F . Due to Lemma C.38, if d1 = O(k log k + k/ ) then with constant probability, we have\n\u2016U\u0302B1 \u2212A1\u20162F \u2264 \u03b1D1\u2016U\u2217B1 \u2212A1\u20162F . (20)\nRecall that (U\u0302> W>B ) \u2208 Rk\u00d7n 2 denotes the matrix where the i-th row is the vectorization of\nU\u0302i \u2297 (WB)i, \u2200i \u2208 [k]. Now, we can show, \u2016V0 \u00b7 (U\u0302> W>B )\u2212A2\u20162F \u2264 \u2016U\u0302B1 \u2212A1\u20162F by V0 = arg min\nV \u2208Rn\u00d7k \u2016V \u00b7 (U\u0302> W>B )\u2212A2\u20162F\n\u2264 \u03b1D1\u2016U\u2217B1 \u2212A1\u20162F by Equation (20) \u2264 \u03b1D1\u2016UBB1 \u2212A1\u20162F by U\u2217 = arg min\nU\u2208Rn\u00d7k \u2016UB1 \u2212A1\u20162F\n\u2264 \u03b1D1\u03b1OPT . by Equation (19) (21)\nWe define B2 = U\u0302> W>B . Let D2 \u2208 Rn 2\u00d7n2 be a sampling and rescaling matrix corresponding to the leverage scores of B>2 . Suppose there are d2 nonzero entries on the diagonal of D2. Define V \u2217 \u2208 Rn\u00d7k to be the optimal solution to minV \u2208Rn\u00d7k \u2016V B2\u2212A2\u20162F , V\u0302 = A2D2(B2D2)\u2020 \u2208 Rn\u00d7k, W0 \u2208 Rn\u00d7k to be the optimal solution to min W\u2208Rn\u00d7k\n\u2016W \u00b7 (U\u0302> V\u0302 >)\u2212A3\u20162F , and V \u2032 to be the optimal solution to min\nV \u2208Rn\u00d7k \u2016V B2D2 \u2212A2D2\u20162F .\nDue to Lemma C.38, with constant probability, we have\n\u2016V\u0302 B2 \u2212A2\u20162F \u2264 \u03b1D2\u2016V \u2217B2 \u2212A2\u20162F . (22)\nRecall that (U\u0302> V\u0302 >) \u2208 Rk\u00d7n2 denotes the matrix where the i-th row is the vectorization of U\u0302i \u2297 V\u0302i, \u2200i \u2208 [k]. Now, we can show, \u2016W0 \u00b7 (U\u0302> V\u0302 >)\u2212A3\u20162F \u2264 \u2016V\u0302 B2 \u2212A2\u20162F by W0 = arg min\nW\u2208Rn\u00d7k \u2016W \u00b7 (U\u0302> V\u0302 >)\u2212A3\u20162F\n\u2264 \u03b1D2\u2016V \u2217B2 \u2212A2\u20162F by Equation (22) \u2264 \u03b1D2\u2016V0B2 \u2212A2\u20162F by V \u2217 = arg min\nV \u2208Rn\u00d7k \u2016V B2 \u2212A2\u20162F\n\u2264 \u03b1D2\u03b1D1\u03b1OPT . by Equation (21) (23)\nWe define B3 = U\u0302> V\u0302 >. Let D3 \u2208 Rn2\u00d7n2 denote a sampling and rescaling matrix corresponding to sampling by the leverage scores of B>3 . Suppose there are d3 nonzero entries on the diagonal of D3.\nDefineW \u2217 \u2208 Rn\u00d7k to be the optimal solution to minW\u2208Rn\u00d7k \u2016WB3\u2212A3\u20162F , W\u0302 = A3D3(B3D3)\u2020 \u2208 Rn\u00d7k, and W \u2032 to be the optimal solution to min\nW\u2208Rn\u00d7k \u2016WB3D3 \u2212A3D3\u20162F .\nDue to Lemma C.38 with constant probability, we have\n\u2016W\u0302B3 \u2212A3\u20162F \u2264 \u03b1D3\u2016W \u2217B3 \u2212A3\u20162F . (24) Now we can show,\n\u2016W\u0302B3 \u2212A3\u20162F \u2264 \u03b1D3\u2016W \u2217B3 \u2212A3\u20162F , by Equation (24) \u2264 \u03b1D3\u2016W0B3 \u2212A3\u20162F , by W \u2217 = arg min\nW\u2208Rn\u00d7k \u2016WB3 \u2212A3\u20162F\n\u2264 \u03b1D3\u03b1D2\u03b1D1\u03b1OPT . by Equation (23)\nThis implies, \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nU\u0302i \u2297 V\u0302i \u2297 W\u0302i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 O(1)\u03b1OPT2 .\nwhere U\u0302 = A1D1(B1D1)\u2020, V\u0302 = A2D2(B2D2)\u2020, W\u0302 = A3D3(B3D3)\u2020. By Lemma C.38, we need to set d1 = d2 = d3 = O(k log k + k/ ). Note that B1 = (V >B W>B ). Thus D1 can be found in n \u00b7 poly(k, 1/ ) time. Because D1 has a small number of nonzero entries on the diagonal, we can compute B1D1 quickly without explicitly writing down B1. Also A1D1 can be computed in nnz(A) time. Using (A1D1) and (B1D1), we can compute U\u0302 in n poly(k, 1/ ) time. In a similar way, we can compute B2, D2, B3, and D3. Since tensor U is constructed based on three poly(k, 1/ ) size matrices, (B1D1)\u2020, (B2D2)\u2020, and (B3D3)\u2020, the overall running time is O(nnz(A) + n poly(k, 1/ ))\nC.7.6 Optimal sample complexity algorithm\nAlgorithm 12 Frobenius Norm CURT Decomposition Algorithm, Optimal Sample Complexity 1: procedure FCURTOptimalSamples(A,UB, VB,WB, n, k) . Theorem C.41 2: d1 \u2190 d2 \u2190 d3 \u2190 O(k/ ). 3: Form B1 = V >B W>B \u2208 Rk\u00d7n\n2 . 4: D1 \u2190GeneralizedMatrixRowSubsetSelection(A>1 , B>1 , n2, n, k, ). . Algorithm 7 5: Let d1 denote the number of nonzero entries in D1. . d1 = O(k/ ) 6: Form U\u0302 = A1D1(B1D1)\u2020 \u2208 Rn\u00d7k. 7: Form B2 = U\u0302> W>B \u2208 Rk\u00d7n\n2 . 8: D2 \u2190GeneralizedMatrixRowSubsetSelection(A>2 , B>2 , n2, n, k, ). . Algorithm 7 9: Let d2 denote the number of nonzero entries in D2. . d2 = O(k/ ) 10: Form V\u0302 = A2D2(B2D2)\u2020 \u2208 Rn\u00d7k. 11: Form B3 = U\u0302> V\u0302 > \u2208 Rk\u00d7n2 . 12: D3 \u2190GeneralizedMatrixRowSubsetSelection(A>3 , B>3 , n2, n, k, ). . Algorithm 7 13: d3 denote the number of nonzero entries in D3. . d3 = O(k/ ) 14: C \u2190 A1D1, R\u2190 A2D2, T \u2190 A3D3. 15: U \u2190\u2211ki=1((B1D1)\u2020)i \u2297 ((B2D2)\u2020)i \u2297 ((B3D3)\u2020)i. 16: return C, R, T and U . 17: end procedure\nTheorem C.41. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, let k \u2265 1, and let UB, VB,WB \u2208 Rn\u00d7k denote a rank-k, \u03b1-approximation to A. Then there exists an algorithm which takes O(nnz(A) log n+ n2 poly(log n, k, 1/ )) time and outputs three matrices: C \u2208 Rn\u00d7c with columns from A, R \u2208 Rn\u00d7r with rows from A, T \u2208 Rn\u00d7t with tubes from A, and a tensor U \u2208 Rc\u00d7r\u00d7t with rank(U) = k such that c = r = t = O(k/ ), and\n\u2225\u2225\u2225\u2225\u2225\u2225 c\u2211\ni=1\nr\u2211\nj=1\nt\u2211\nl=1\nUi,j,l \u00b7 Ci \u2297Rj \u2297 Tl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + )\u03b1 min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F\nholds with probability 9/10.\nProof. The proof is almost the same as the proof of Theorem C.40. The only difference is that instead of using Theorem C.38, we use Theorem C.14."}, {"heading": "C.8 Face-based selection and decomposition", "text": "Previously we provided column-based tensor CURT algorithms, which are algorithms that can select a subset of columns from each of the three dimensions. Here we provide two face-based tensor CURT decomposition algorithms. The first algorithm runs in polynomial time and is a bicriteria algorithm (the number of samples is poly(k/ )). The second algorithm needs to start with a rank-k (1+O( ))- approximate solution, which we then show how to combine with our previous algorithm. Both of our algorithms are able to select a subset of column-row faces, a subset of row-tube faces and a subset of column-tube faces. The second algorithm is able to output U , but the first algorithm is not.\nC.8.1 Column-row, column-tube, row-tube face subset selection\nAlgorithm 13 Frobenius Norm Tensor Column-row, Row-tube and Tube-column Face Subset Selection 1: procedure FFaceCRTSelection(A,n, k, ) . Theorem C.42 2: s1 \u2190 s2 \u2190 O(k/ ). 3: Choose a Gaussian matrix S1 with s1 columns. . Definition B.18 4: Choose a Gaussian matrix S2 with s2 columns. . Definition B.18 5: Form matrix V3 by setting the (i, j)-th column to be (A2S2)j . 6: D3 \u2190GeneralizedMatrixRowSubsetSelection(A2,V3,n,n2,s1s2, ). . Algorithm 7 7: Let d3 denote the number of nonzero entries in D3. . d3 = O(s1s2/ ) 8: Form matrix U2 by setting the (i, j)-th column to be (A1S1)i. 9: D2 \u2190GeneralizedMatrixRowSubsetSelection(A1,U2,n,n2,s1s2, ). 10: Let d2 denote the number of nonzero entries in D2. . d2 = O(s1s2/ ) 11: Form matrix W1 by setting the (i, j)-th column to be (A(I,D3, I)3)j . 12: D1 \u2190GeneralizedMatrixRowSubsetSelection(A3,W1,n,n2,s1s2, ). 13: Let d1 denote the number of nonzero entries in D1. . d1 = O(s1s2/ ) 14: T \u2190 A(I, I,D1), C \u2190 A(D2, I, I), and R\u2190 A(I,D3, I). 15: return C, R and T . 16: end procedure\nTheorem C.42. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm which takes O(nnz(A)) log n + n2 poly(log n, k, 1/ ) time and outputs three tensors : a subset C \u2208 Rc\u00d7n\u00d7n of row-tube faces of A, a subset R \u2208 Rn\u00d7r\u00d7n of column-tube faces of A, and a subset T \u2208 Rn\u00d7n\u00d7t of column-row faces of A, where c = r = t = poly(k, 1/ ), and for which there exists a tensor U \u2208 Rtn\u00d7cn\u00d7rn for which\n\u2016U(T1, C2, R3)\u2212A\u20162F \u2264 (1 + ) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F ,\nor equivalently, \u2225\u2225\u2225\u2225\u2225\u2225 tn\u2211\ni=1\ncn\u2211\nj=1\nrn\u2211\nl=1\nUi,j,l \u00b7 (T1)i \u2297 (C2)j \u2297 (R3)l \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F .\nProof. We fix V \u2217 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k. We define Z1 \u2208 Rk\u00d7n2 where the i-th row of Z1 is the vector Vi \u2297Wi. Choose a sketching (Gaussian) matrix S1 \u2208 Rn2\u00d7s1 (Definition B.18), and let\nU\u0302 = A1S1(Z1S1) \u2020 \u2208 Rn\u00d7k. Following a similar argument as in the previous theorem, we have\n\u2016U\u0302Z1 \u2212A1\u20162F \u2264 (1 + ) OPT .\nWe fix U\u0302 and W \u2217. We define Z2 \u2208 Rk\u00d7n2 where the i-th row of Z2 is the vector U\u0302i \u2297W \u2217i . Choose a sketching (Gaussian) matrix S2 \u2208 Rn2\u00d7s2 (Definition B.18), and let V\u0302 = A2S2(Z2S2)\u2020 \u2208 Rn\u00d7k. Following a similar argument as in the previous theorem, we have\n\u2016V\u0302 Z2 \u2212A2\u20162F \u2264 (1 + )2 OPT .\nWe fix U\u0302 and V\u0302 . Note that U\u0302 = A1S1(Z1S1)\u2020 and V\u0302 = A2S2(Z2S2)\u2020. We define Z3 \u2208 Rk\u00d7n2 such that the i-th row of Z3 is the vector U\u0302i \u2297 V\u0302i. Let z3 = s1 \u00b7 s2. We define Z \u20323 \u2208 Rz3\u00d7n\n2 such that, \u2200i \u2208 [s1], \u2200j \u2208 [s2], the i+ (j \u2212 1)s1-th row of Z \u20323 is the vector (A1S1)i \u2297 (A2S2)j .\nWe define U3 \u2208 Rn\u00d7z3 to be the matrix where the i + (j \u2212 1)s1-th column is (A1S1)i and V3 \u2208 Rn\u00d7z3 to be the matrix where the i+ (j \u2212 1)s1-th column is (A2S2)j . Then Z \u20323 = (U>3 V >3 ).\nWe first have,\nmin W\u2208Rn\u00d7k,X\u2208Rk\u00d7z3 \u2016WXZ \u20323 \u2212A3\u20162F \u2264 min W\u2208Rn\u00d7k \u2016WZ3 \u2212A3\u20162F \u2264 (1 + )2 OPT .\nNow consider the following objective function,\nmin W\u2208Rn\u00d7z3\n\u2016V3 \u00b7 (W> U>3 )\u2212A2\u20162F .\nLet D3 denote a sampling and rescaling diagonal matrix according to V1 \u2208 Rn\u00d7z3 , let d3 denote the number of nonzero entries of D3. Then we have\nmin W\u2208Rn\u00d7z3 \u2016D3V3 \u00b7 (W> U>3 )\u2212D3A2\u20162F = min\nW\u2208Rn\u00d7z3 \u2016U3 \u2297 (D3V3)\u2297W \u2212A(I,D3, I)\u20162F\n= min W\u2208Rn\u00d7z3\n\u2016W \u00b7 (U>3 (D3V3)>)\u2212 (A(I,D3, I))3\u20162F ,\nwhere the first equality follows by retensorizing the objective function, and the second equality follows by flattening the tensor along the third dimension.\nLet Z3 denote (U>3 (D3V3)>) \u2208 Rz3\u00d7nd3 and W \u2032 = (A(I,D3, I))3 \u2208 Rn\u00d7nd3 . Using Theorem C.14, we can find a diagonal matrix D3 \u2208 Rn2\u00d7n2 with d3 = O(z3/ ) = O(k2/ 3) nonzero entries such that\n\u2016U3 \u2297 V3 \u2297 (W \u2032Z\u20203)\u2212A\u20162F \u2264 (1 + )3 OPT .\nWe define U2 = U3 \u2208 Rn\u00d7z2 with z2 = z3. We define W2 = W \u2032Z\u20203 \u2208 Rn\u00d7z2 with z2 = z3. We consider,\nmin V \u2208Rn\u00d7z2\n\u2016U2 \u00b7 (V > W>2 )\u2212A1\u20162F .\nLet D2 denote a sampling and rescaling matrix according to U2, and let d2 denote the number of nonzero entries of D2. Then, we have\nmin V \u2208Rn\u00d7z2 \u2016D2U2 \u00b7 (V > W>2 )\u2212D2A1\u20162F = min\nV \u2208Rn\u00d7z2 \u2016D2U2 \u2297 V \u2297W2 \u2212A(D2, I, I)\u20162F\n= min V \u2208Rn\u00d7z2\n\u2016V \u00b7 (W>2 (D2U2)>)\u2212 (A(D2, I, I))2\u20162F ,\nwhere the first equality follows by retensorizing the objective function, and the second equality follows by flattening the tensor along the second dimension.\nLet Z2 denote (W>2 (D2U2)>) \u2208 Rz2\u00d7nd2 and V \u2032 = (A(D2, I, I))2 \u2208 Rn\u00d7nd2 . Using Theorem C.14, we can find a diagonal matrix D2 \u2208 Rn2\u00d7n2 with d2 = O(z2/ ) nonzero entries such that\n\u2016U2 \u2297 (V \u2032Z\u20202)\u2297W2 \u2212A\u20162F \u2264 (1 + )4 OPT .\nWe define W1 = W2 \u2208 Rn\u00d7z1 with z1 = z2, and define V1 = (V \u2032Z\u20202) \u2208 Rn\u00d7z1 with z1 = z2. Let D1 denote a sampling and rescaling matrix according to W1, and let d1 denote the number\nof nonzero entries of D1. Then we have\nmin U\u2208Rn\u00d7z1 \u2016D1W1 \u00b7 (U> V >1 )\u2212D1A3\u20162F = min\nU\u2208Rn\u00d7z1 \u2016U \u2297 V1 \u2297 (D1W1)\u2212A(I, I,D1)\u20162F\n= min U\u2208Rn\u00d7z1\n\u2016U \u00b7 (V >1 (D1W1)>)\u2212A(I, I,D1)1\u20162F\nwhere the first equality follows by unflattening the objective function, and second equality follows by flattening the tensor along the first dimension.\nLet Z1 denote (V >1 (D1W1)>) \u2208 Rz1\u00d7nd1 , and U \u2032 = A(I, I,D1)1 \u2208 Rn\u00d7nd1 . Using Theorem C.14, we can find a diagonal matrix D1 \u2208 Rn2\u00d7n2 with d1 = O(z1/ ) nonzero entries such that\n\u2016(U \u2032Z\u20201)\u2297 (V1)\u2297W1 \u2212A\u20162F \u2264 (1 + )5 OPT,\nwhich means,\n\u2016(U \u2032Z\u20201)\u2297 (V \u2032Z \u2020 2)\u2297 (W \u2032Z \u2020 3)\u2212A\u20162F \u2264 (1 + )5 OPT .\nPutting U \u2032, V \u2032,W \u2032 together completes the proof.\nCorollary C.43. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm which takes O(nnz(A)) + n2 poly(k, 1/ ) time and outputs three tensors : a subset C \u2208 Rc\u00d7n\u00d7n of row-tube faces of A, a subset R \u2208 Rn\u00d7r\u00d7n of column-tube faces of A, and a subset T \u2208 Rn\u00d7n\u00d7t of column-row faces of A, where c = r = t = poly(k, 1/ ), so that there exists a tensor U \u2208 Rtn\u00d7cn\u00d7rn for which\n\u2016U(T1, C2, R3)\u2212A\u20162F \u2264 (1 + ) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F ,\nor equivalently, \u2225\u2225\u2225\u2225\u2225\u2225 tn\u2211\ni=1\ncn\u2211\nj=1\nrn\u2211\nl=1\nUi,j,l \u00b7 (T1)i \u2297 (C2)j \u2297 (R3)l \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F\nProof. If we allow a poly(k/ ) factor increase in running time and a poly(k/ ) factor increase in the number of faces selected, then instead of using generalized row subset selection, which has running time depending on log n, we can use the technique in Section C.11 to avoid the log n factor.\nAlgorithm 14 Frobenius Norm (Face-based) CURT Decomposition Algorithm, Optimal Sample Complexity 1: procedure FFaceCURTDecomposition(A,UB, VB,WB, n, k) . Theorem C.44 2: D1 \u2190GeneralizedMatrixRowSubsetSelection(A3,WB, n, n2, k, ). . Algorithm 7,\nthe number of nonzero entries is d1 = O(k/ ) 3: Form Z1 = V >B (D1WB)>. 4: Form U\u0302 = (A(I, I,D1))1Z \u2020 1 \u2208 Rn\u00d7k. 5: D2 \u2190GeneralizedMatrixRowSubsetSelection(A1, U\u0302 , n, n2, k, ). . The number of nonzero entries is d2 = O(k/ ) 6: Form Z2 = (W>B (D2U\u0302)). 7: Form V\u0302 = (A(D2, I, I))2Z \u2020 2 \u2208 Rn\u00d7k. 8: D3 \u2190GeneralizedMatrixRowSubsetSelection(A2, V\u0302 , n, n2, k, ). . The number of nonzero entries is d3 = O(k/ )\n9: Form Z3 = U\u0302> (D3V\u0302 )>. 10: Form W\u0302 = (A(I,D3, I))3(Z3)\u2020 \u2208 Rn\u00d7k. 11: T \u2190 A(I, I,D1), C \u2190 A(D2, I, I), R\u2190 A(I,D3, I). 12: U \u2190\u2211ki=1((Z1)\u2020)i \u2297 ((Z2)\u2020)i \u2297 ((Z3)\u2020)i. 13: return C, R, T and U . 14: end procedure"}, {"heading": "C.8.2 CURT decomposition", "text": "Theorem C.44. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, let k \u2265 1, and let UB, VB,WB \u2208 Rn\u00d7k denote a rank-k, \u03b1-approximation to A. Then there exists an algorithm which takes O(nnz(A)) log n+ n2 poly(log n, k, 1/ ) time and outputs three tensors: C \u2208 Rc\u00d7n\u00d7n with row-tube faces from A, R \u2208 Rn\u00d7r\u00d7n with colum-tube faces from A, T \u2208 Rn\u00d7n\u00d7t with column-row faces from A, and a (factorization of a) tensor U \u2208 Rtn\u00d7cn\u00d7rn with rank(U) = k for which c = r = t = O(k/ ) and\n\u2016U(T1, C2, R3)\u2212A\u20162F \u2264 (1 + )\u03b1 min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F ,\nor equivalently, \u2225\u2225\u2225\u2225\u2225\u2225 tn\u2211\ni=1\ncn\u2211\nj=1\nrn\u2211\nl=1\nUi,j,l \u00b7 (T1)i \u2297 (C2)j \u2297 (R3)l \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + )\u03b1 min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F\nholds with probability 9/10.\nProof. We already have three matrices UB \u2208 Rn\u00d7k, VB \u2208 Rn\u00d7k and WB \u2208 Rn\u00d7k and these three matrices provide a rank-k, \u03b1-approximation to A, i.e.,\n\u2016UB \u2297 VB \u2297WB \u2212A\u20162F \u2264 \u03b1 min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F \ufe38 \ufe37\ufe37 \ufe38\nOPT\n.\nWe can consider the following problem,\nmin U\u2208Rn\u00d7k\n\u2016WB \u00b7 (U> V >B )\u2212A3\u20162F .\nLet D1 denote a sampling and rescaling diagonal matrix according to WB, and let d1 denote the number of nonzero entries of D1. Then we have\nmin U\u2208Rn\u00d7k \u2016(D1WB) \u00b7 (U> V >B )\u2212D1A3\u20162F = min U\u2208Rn\u00d7k \u2016U \u2297 VB \u2297D1WB \u2212A(I, I,D1)\u20162F\n= min U\u2208Rn\u00d7k\n\u2016U \u00b7 (V >B (D1WB)>)\u2212 (A(I, I,D1))1\u20162F ,\nwhere the first equality follows by retensorizing the objective function, and the second equality follows by flattening the tensor along the first dimension. Let Z1 denote V >B (D1WB)> \u2208 Rk\u00d7nd1 , and define U\u0302 = (A(I, I,D1))1Z \u2020 1 \u2208 Rn\u00d7k. Then we have\n\u2016U\u0302 \u2297 VB \u2297WB \u2212A\u20162F \u2264 (1 + )\u03b1OPT .\nIn the second step, we fix U\u0302 and WB, and consider the following objective function,\nmin V \u2208Rn\u00d7k\n\u2016U\u0302 \u00b7 (V > WB)\u2212A1\u20162F .\nLet D2 denote a sampling and rescaling matrix according to U\u0302 , and let d2 denote the number of nonzero entries of D2. Then we have,\nmin V \u2208Rn\u00d7k\n\u2016(D2U\u0302) \u00b7 (V > W>B )\u2212D2A1\u20162F\n= min V \u2208Rn\u00d7k\n\u2016(D2U\u0302)\u2297 V \u2297WB \u2212A(D2, I, I)\u20162F\n= min V \u2208Rn\u00d7k\n\u2016V \u00b7 (W>B (D2U\u0302)>)\u2212 (A(D2, I, I))2\u20162F ,\nwhere the first equality follows by unflattening the objective function, and the second equality follows by flattening the tensor along the second dimension. Let Z2 denote (W>B (D2U\u0302)>) \u2208 Rk\u00d7nd2 , and define V\u0302 = (A(D2, I, I))2(Z2)\u2020 \u2208 Rn\u00d7k. Then we have,\n\u2016U\u0302 \u2297 V\u0302 \u2297WB \u2212A\u20162F \u2264 (1 + )2\u03b1OPT .\nIn the third step, we fix U\u0302 and V\u0302 , and consider the following objective function,\nmin W\u2208Rn\u00d7k\n\u2016V\u0302 \u00b7 (W U\u0302)\u2212A2\u20162F .\nLet D3 denote a sampling and rescaling matrix according to V\u0302 , and let d3 denote the number of nonzero entries of D3. Then we have,\nmin W\u2208Rn\u00d7k\n\u2016(D3V\u0302 ) \u00b7 (W> U\u0302>)\u2212D3A2\u20162F\n= min W\u2208Rn\u00d7k\n\u2016U\u0302 \u2297 (D3V\u0302 )\u2297W \u2212A(I,D3, I)\u20162F\n= min W\u2208Rn\u00d7k\n\u2016W \u00b7 (U\u0302> (D3V\u0302 )>)\u2212 (A(I,D3, I))3\u20162F ,\nwhere the first equality follows by retensorizing the objective function, and the second equality follows by flattening the tensor along the third dimension. Let Z3 denote (U\u0302> (D3V\u0302 )>) \u2208 Rk\u00d7nd3 , and define W\u0302 = (A(I,D3, I))3(Z3)\u2020. Putting it all together, we have,\n\u2016U\u0302 \u2297 V\u0302 \u2297 W\u0302 \u2212A\u20162F \u2264 (1 + )3\u03b1OPT .\nThis implies\n\u2016(A(I, I,D1))1Z\u20201 \u2297 (A(D2, I, I))2Z\u20202 \u2297 (A(I,D3, I))3Z\u20203 \u2212A\u20162F \u2264 (1 + )3\u03b1OPT ."}, {"heading": "C.9 Solving small problems", "text": "Theorem C.45. Let maxi{ti, di} \u2264 n. Given a t1 \u00d7 t2 \u00d7 t3 tensor A and three matrices: a t1 \u00d7 d1 matrix T1, a t2 \u00d7 d2 matrix T2, and a t3 \u00d7 d3 matrix T3, if for any \u03b4 > 0 there exists a solution to\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(T1X1)i \u2297 (T2X2)i \u2297 (T3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n:= OPT,\nand each entry of Xi can be expressed using O(n\u03b4) bits, then there exists an algorithm that takes nO(\u03b4)\u00b72O(d1k+d2k+d3k) time and outputs three matrices: X\u03021, X\u03022, and X\u03023 such that \u2016(T1X\u03021)\u2297(T2X\u03022)\u2297 (T3X\u03023)\u2212A\u20162F = OPT. Proof. For each i \u2208 [3], we can create ti \u00d7 di variables to represent matrix Xi. Let x denote this list of variables. Let B denote tensor \u2211k i=1(T1X1)i \u2297 (T2X2)i \u2297 (T3X3)i and let Bi,j,l(x) denote an entry of tensor B (which can be thought of as a polynomial written in terms of x). Then we can write the following objective function,\nmin x\nt1\u2211\ni=1\nt2\u2211\nj=1\nt3\u2211\nl=1\n(Bi,j,l(x)\u2212Ai,j,l)2.\nWe slightly modify the above objective function to obtain a new objective function,\nmin x,\u03c3\nt1\u2211\ni=1\nt2\u2211\nj=1\nt3\u2211\nl=1\n(Bi,j,l(x)\u2212Ai,j,l)2,\ns.t. \u2016x\u201622 \u2264 2O(n \u03b4),\nwhere the last constraint is unharmful, because there exists a solution that can be written using O(n\u03b4) bits. Note that the number of inequality constraints in the above system is O(1), the degree is O(1), and the number of variables is v = (d1k+d2k+d3k). Thus by Theorem B.11, the minimum nonzero cost is at least\n(2O(n \u03b4))\u22122 O(v) .\nIt is clear that the upper bound on the cost is at most 2O(n\u03b4). Thus the number of binary search steps is at most log(2O(n\u03b4))2O(v). In each step of the binary search, we need to choose a cost C between the lower bound and the upper bound, and write down the polynomial system,\nt1\u2211\ni=1\nt2\u2211\nj=1\nt3\u2211\nl=1\n(Bi,j,l(x)\u2212Ai,j,l)2 \u2264 C,\n\u2016x\u201622 \u2264 2O(n \u03b4).\nUsing Theorem B.10, we can determine if there exists a solution to the above polynomial system. Since the number of variables is v, and the degree is O(1), the number of inequality constraints is O(1). Thus, the running time is\npoly(bitsize) \u00b7 (# constraints \u00b7 degree)# variables = nO(\u03b4)2O(v).\nC.10 Extension to general q-th order tensors\nThis section provides the details for our extensions from 3rd order tensors to general q-th order tensors. In most practical applications, the order q is a constant. Thus, to simplify the analysis, we use Oq(\u00b7) to hide dependencies on q.\nC.10.1 Fast sampling of columns according to leverage scores, implicitly\nThis section explains an algorithm that is able to sample from the leverage scores from the product of q matrices U1, U2, \u00b7 \u00b7 \u00b7 , Uq without explicitly writing down U1 U2 \u00b7 \u00b7 \u00b7Uq. To build this algorithm we combine TensorSketch, some ideas from [DMIMW12], and some techniques from [AKO11, MW10]. Finally, we improve the running time for sampling columns according to the leverage scores from poly(n) to O\u0303(n). Given q matrices U1, U2, \u00b7 \u00b7 \u00b7 , Uq, with each such matrix Ui having size k \u00d7 ni, we define A \u2208 Rk\u00d7 \u220fq i=1 ni to be the matrix where the i-th row of A is the vectorization of U i1 \u2297 U i2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U iq, \u2200i \u2208 [k]. Na\u00efvely, in order to sample poly(k, 1/ ) rows from A according to the leverage scores, we need to write down \u220fq i=1 ni leverage scores. This approach will\ntake at least \u220fq i=1 ni running time. In the remainder of this section, we will explain how to do it in Oq(n \u00b7 poly(k, 1/ )) time for any constant p, and maxi\u2208[q] ni \u2264 n.\nTheorem C.46. Given q matrices U1 \u2208 Rk\u00d7n1, U2 \u2208 Rk\u00d7n2, \u00b7 \u00b7 \u00b7 , Uq \u2208 Rk\u00d7nq , let maxi ni \u2264 n. There exists an algorithm that takes Oq(n \u00b7poly(k, 1/ ) \u00b7Rsamples) time and samples Rsamples columns of U1 U2 \u00b7 \u00b7 \u00b7 Uq \u2208 Rk\u00d7 \u220fq i=1 ni according to the leverage scores of U1 U2 \u00b7 \u00b7 \u00b7 Uq.\nProof. Let maxi ni \u2264 n. First, choosing \u03a00 to be a TensorSketch, we can compute R\u22121 in Oq(n poly(k, 1/ )) time, where R is the R in a QR-factorization. We want to sample columns from U1 U2 \u00b7 \u00b7 \u00b7 Uq according to the square of the `2-norm of each column of R\u22121(U1 U2 \u00b7 \u00b7 \u00b7Uq). The issue is the number of columns of this matrix is already \u220fq i=1 ni. The goal is to sample columns from R\u22121(U1 U2 \u00b7 \u00b7 \u00b7Uq) without explicitly computing the square of the `2-norm of each column. Similarly as in the proof of Lemma C.32, we have the observation that the following two sampling procedures are equivalent in terms of sampling a column of a matrix: (1) We sample a single entry from matrix R\u22121(U1 U2 \u00b7 \u00b7 \u00b7 Uq) proportional to its squared value, (2) We sample a column from matrix R\u22121(U1 U2 \u00b7 \u00b7 \u00b7 Uq) proportional to its squared `2-norm. Let the (i, j1, j2, \u00b7 \u00b7 \u00b7 , jq)-th entry denote the entry in the i-th row and the j-th column, where\nj =\nq\u22121\u2211\nl=1\n(jl \u2212 1) q\u220f\nt=l+1\nnt + jq.\nSimilarly to Equation (18), we can show, for a particular column j,\nPr[we sample an entry from the j-th column of matrix] = Pr[we sample the j-th column of a matrix].\nThus, it is sufficient to show how to sample a single entry from matrix R\u22121(U1 U2 \u00b7 \u00b7 \u00b7 Uq) proportional to its squared value without writing down all the entries of the k \u00d7\u220fqi=1 ni matrix.\nLet V0 denote R\u22121. Let n0 denote the number of rows of V0. In the next few paragraphs, we describe a sampling procedure (procedure FastTensorLeverageScoreGeneralOrder in Algorithm 15) which first samples j\u03020 from [n0], then samples j\u03021 from [n1], \u00b7 \u00b7 \u00b7 , and at the end samples j\u0302q from [nq].\nIn the first step, we want to sample j\u03020 from [n0] proportional to the squared `2-norm of that row. To do this efficiently, we choose \u03a01 \u2208 R \u220fq i=1 ni\u00d7s1 to be a TensorSketch to sketch on the\nAlgorithm 15 Fast Tensor Leverage Score Sampling, for General q-th Order 1: procedure FastTensorLeverageScoreGeneralOrder({Ui}i\u2208[q], {ni}i\u2208[q], k, , Rsamples) . Theorem C.46\n2: s1 \u2190 poly(k, 1/ ). 3: Choose \u03a00,\u03a01 \u2208 Rn1n2\u00b7\u00b7\u00b7nq\u00d7s1 to each be a TensorSketch. . Definition B.34 4: Compute R\u22121 \u2208 Rk\u00d7k by using (U1 U2 \u00b7 \u00b7 \u00b7 Uq)\u03a00. . Ui \u2208 Rk\u00d7ni , \u2200i \u2208 [q] 5: V0 \u2190 R\u22121, n0 \u2190 k. 6: for i = 1\u2192 [n0] do 7: \u03b1i \u2190 \u2016(V0)i((U1 U2 \u00b7 \u00b7 \u00b7 Uq)\u03a01)\u201622. 8: end for 9: for r = 1\u2192 Rsamples do\n10: Sample j\u03020 from [n0] with probability \u03b1i/ \u2211n0\ni\u2032=1 \u03b1i\u2032 . 11: for l = 1\u2192 q \u2212 1 do 12: sl+1 \u2190 Oq(poly(k, 1/ )). 13: Choose \u03a0l+1 \u2208 Rnl+1\u00b7\u00b7\u00b7nq\u00d7sl+1 to be a TensorSketch. 14: for jl = 1\u2192 [nl] do . Form Vl \u2208 Rnl\u00d7k 15: (Vl)\njl \u2190 (Vl\u22121)j\u0302l\u22121 \u25e6 (Ul)>jl . 16: end for 17: for i = 1\u2192 nq do 18: \u03b2i \u2190 \u2016(Vl)i((Ul+1 \u00b7 \u00b7 \u00b7 Uq)\u03a0l+1)\u201622. 19: end for 20: Sample j\u0302l from [nl] with probability \u03b2i/ \u2211nl i\u2032=1 \u03b2i\u2032 . 21: end for 22: for i = 1\u2192 nq do 23: \u03b2i \u2190 |(Vq\u22121)j\u0302q\u22121(Uq)i|2. 24: end for 25: Sample j\u0302q from [nq] with probability \u03b2i/ \u2211nq i\u2032=1 \u03b2i\u2032 . 26: S \u2190 S \u222a (j\u03021, \u00b7 \u00b7 \u00b7 , j\u0302q). 27: end for 28: Convert S into a diagonal matrix D with at most Rsamples nonzero entries. 29: return D. . Diagonal matrix D \u2208 Rn1n2\u00b7\u00b7\u00b7nq\u00d7n1n2\u00b7\u00b7\u00b7nq 30: end procedure\nright of V0(U1 U2 \u00b7 \u00b7 \u00b7 Uq). By Section B.10, as long as s1 = Oq(poly(k, 1/ )), then \u03a01 is a (1\u00b1 )-subspace embedding matrix. Thus with probability 1\u2212 1/\u2126(q), for all i \u2208 [n0],\n\u2016(V0)i((U1 U2 \u00b7 \u00b7 \u00b7 Uq)\u03a01)\u201622 = (1\u00b1 )\u2016(V0)i((U1 U2 \u00b7 \u00b7 \u00b7 Uq))\u201622,\nwhich means we can sample j\u03020 from [n0] in Oq(n poly(k, 1/ )) time. In the second step, we have already obtained j\u03020. Using that row of V0 with U1, we can form a new matrix V1 \u2208 Rn1\u00d7k in the following sense,\n(V1) i = (V0) j\u03020 \u25e6 (U1)>i ,\u2200i \u2208 [n1],\nwhere (V1)i denotes the i-th row of matrix V1, (V0)j\u03020 denotes the j\u03020-th row of V0 and (U1)i is the i-th column of U1. Another important observation is, the entry in the (j1, j2, \u00b7 \u00b7 \u00b7 , jq)-th coordinate of vector (V0)j\u03020(U1 U2 \u00b7 \u00b7 \u00b7 Uq) is the same as the entry in the j1-th row and (j2, \u00b7 \u00b7 \u00b7 , jq)-th\ncolumn of matrix V1(U2 U3 \u00b7 \u00b7 \u00b7 Uq). Thus, sampling j1 is equivalent to sampling j1 from the new matrix V1(U2 U3 \u00b7 \u00b7 \u00b7 Uq) proportional to the squared `2-norm of that row. We still have the computational issue that the length of the row vector is very long. To deal with this, we can choose \u03a02 \u2208 R \u220fq i=2 ni\u00d7s2 to be a TensorSketch to multiply on the right of V1(U2 U3 \u00b7 \u00b7 \u00b7 Uq).\nBy Section B.10, as long as s2 = Oq(poly(k, 1/ )), then \u03a02 is a (1 \u00b1 )-subspace embedding matrix. Thus with probability 1\u2212 1/\u2126(q), for all i \u2208 [n1],\n\u2016(V1)i((U2 \u00b7 \u00b7 \u00b7 Uq)\u03a02)\u201622 = (1\u00b1 )\u2016(V1)i((U2 \u00b7 \u00b7 \u00b7 Uq))\u201622,\nwhich means we can sample j\u03021 from [n1] in Oq(n poly(k, 1/ )) time. We repeat the above procedure until we obtain each of j\u03020, j\u03021, \u00b7 \u00b7 \u00b7 , j\u0302q. Note that the last one, j\u0302q, is easier, since the length of the vector is already small enough, and so we do not need to use TensorSketch for it.\nBy Section B.10, the time for multiplying by TensorSketch is Oq(n poly(k, 1/ )). Setting to be a small constant, and taking a union bound over O(q) events completes the proof. Lemma C.47. Given A \u2208 Rn0\u00d7 \u220fq i=1 ni, U1, U2, \u00b7 \u00b7 \u00b7 , Uq \u2208 Rk\u00d7n, for any > 0, there exists an\nalgorithm that runs in O(n \u00b7 poly(k, 1/ )) time and outputs a diagonal matrix D \u2208 R \u220fq i=1 ni\u00d7 \u220fq i=1 ni with m = O(k log k + k/ ) nonzero entries such that,\n\u2016U\u0302(U1 U2 \u00b7 \u00b7 \u00b7 Uq)\u2212A\u20162F \u2264 (1 + ) min U\u2208Rn\u00d7k \u2016U(U1 U2 \u00b7 \u00b7 \u00b7 Uq)\u2212A\u20162F ,\nholds with probability at least 0.999, where U\u0302 denotes the optimal solution of\nmin U\u2208Rn0\u00d7k\n\u2016U(U1 U2 \u00b7 \u00b7 \u00b7 Uq)D \u2212AD\u20162F .\nProof. This follows by combining Theorem C.46, Corollary C.30, and Lemma C.31.\nC.10.2 General iterative existential proof\nAlgorithm 16 General q-th Order Iterative Existential Proof 1: procedure GeneralIterativeExistentialProof(A,n, k, q, ) . Section C.10.2 2: Fix U\u22171 , U\u22172 , \u00b7 \u00b7 \u00b7 , U\u2217q \u2208 Rn\u00d7k. 3: for i = 1\u2192 q do 4: Choose sketching matrix Si \u2208 Rnq\u22121\u00d7si with si = Oq(k/ ). 5: Define Zi \u2208 Rk\u00d7nq\u22121 to be\nj<i U\u0302>j j\u2032>i U\u2217>j\u2032 .\n6: Let Ai denote the matrix obtained by flattening tensor A along the i-th dimension. 7: Define U\u0302i to be AiSi(ZiSi)\u2020. 8: end for 9: return U\u03021, U\u03022, \u00b7 \u00b7 \u00b7 , U\u0302q.\n10: end procedure\nGiven a q-th order tensor A \u2208 Rn\u00d7n\u00d7\u00b7\u00b7\u00b7\u00d7n, we fix U\u22171 , U\u22172 , \u00b7 \u00b7 \u00b7 , U\u2217q \u2208 Rn\u00d7k to be the best rank-k solution (if it does not exist, then we replace it by a good approximation, as discussed). We define OPT = \u2016U\u22171 \u2297 U\u22172 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U\u2217q \u2212 A\u20162F . Our iterative proof works as follows. We first obtain the objective function,\nmin U1\u2208Rn\u00d7k\n\u2016U1 \u00b7 Z1 \u2212A1\u20162F \u2264 OPT,\nwhere A1 is a matrix obtained by flattening tensor A along the first dimension, Z1 = (U\u2217>2 U\u2217>3 \u00b7 \u00b7 \u00b7 U\u2217>q ) denotes a k \u00d7 nq\u22121 matrix. Choosing S1 \u2208 Rn\nq\u22121\u00d7s1 to be a Gaussian sketching matrix with s1 = O(k/ ), we obtain a smaller problem,\nmin U1\u2208Rn\u00d7k\n\u2016U1 \u00b7 Z1S1 \u2212A1S1\u20162F .\nWe define U\u03021 to be A1S1(Z1S1)\u2020 \u2208 Rn\u00d7k, which gives,\n\u2016U\u03021 \u00b7 Z1 \u2212A1\u20162F \u2264 (1 + ) OPT .\nAfter retensorizing the above, we have,\n\u2016U\u03021 \u2297 U\u22172 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U\u2217q \u2212A\u20162F \u2264 (1 + ) OPT .\nIn the second round, we fix U\u03021, U\u22173 , \u00b7 \u00b7 \u00b7 , U\u2217q \u2208 Rn\u00d7k, and choose S2 \u2208 Rn q\u22121\u00d7s2 to be a Gaussian sketching matrix with s2 = O(k/ ). We define Z2 \u2208 Rk\u00d7nq\u22121 to be (U\u0302>1 U\u2217>3 \u00b7 \u00b7 \u00b7 U\u2217>q ). We define U\u03022 to be A2S2(Z2S2)\u2020 \u2208 Rn\u00d7k. Then, we have\n\u2016U\u03021 \u2297 U\u03022 \u2297 U\u22173 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U\u2217q \u2212A\u20162F \u2264 (1 + )2 OPT .\nWe repeat the above process, where in the i-th round we fix U\u03021, \u00b7 \u00b7 \u00b7 , U\u0302i\u22121, U\u2217i+1, \u00b7 \u00b7 \u00b7 , U\u2217q \u2208 Rn\u00d7k, and choose Si \u2208 Rnq\u22121\u00d7si to be a Gaussian sketching matrix with si = O(k/ ). We define Zi \u2208 Rk\u00d7nq\u22121 to be (U\u0302>1 \u00b7 \u00b7 \u00b7 U\u0302>i\u22121 U\u2217>i+1 \u00b7 \u00b7 \u00b7 U\u2217>q ). We define U\u0302i to be AiSi(ZiSi)\u2020 \u2208 Rn\u00d7k. Then, we have\n\u2016U\u03021 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U\u0302i\u22121 \u2297 U\u0302i \u2297 U\u2217i+1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U\u2217q \u2212A\u20162F \u2264 (1 + )2 OPT .\nAt the end of the q-th round, we have\n\u2016U\u03021 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U\u0302q \u2212A\u20162F \u2264 (1 + )q OPT .\nReplacing = \u2032/(2q), we obtain\n\u2016U\u03021 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U\u0302q \u2212A\u20162F \u2264 (1 + \u2032) OPT .\nwhere for all i \u2208 [q], si = O(kq/ \u2032) = Oq(k/ \u2032) .\nC.10.3 General input sparsity reduction\nThis section shows how to extend the input sparsity reduction from third order tensors to general q-th order tensors. Given a tensor A \u2208 Rn\u00d7n\u00d7\u00b7\u00b7\u00b7\u00d7n and q matrices, for each i \u2208 [q], matrix Vi has size Vi \u2208 Rn\u00d7bi , with bi \u2264 poly(k, 1/ ). We choose a batch of sparse embedding matrices Ti \u2208 Rti\u00d7n. Define V\u0302i = TiVi, and C = A(T1, T2, \u00b7 \u00b7 \u00b7 , Tq). Thus we have with probability 99/100, for any \u03b1 \u2265 0, for all {Xi, X \u2032i \u2208 Rbi\u00d7k}i\u2208[q], if\n\u2016V\u03021X \u20321 \u2297 V\u03022X \u20322 \u2297 \u00b7 \u00b7 \u00b7 \u2297 V\u0302qX \u2032q \u2212 C\u20162F \u2264 \u03b1\u2016V\u03021X1 \u2297 V\u03022X2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 V\u0302qXq \u2212 C\u20162F ,\nthen\n\u2016V1X \u20321 \u2297 V2X \u20322 \u2297 \u00b7 \u00b7 \u00b7 \u2297 VqX \u2032q \u2212A\u20162F \u2264 (1 + )\u03b1\u2016V1X1 \u2297 V2X2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 VqXq \u2212A\u20162F ,\nwhere ti = Oq(poly(bi, 1/ )).\nAlgorithm 17 General q-th Order Input Sparsity Reduction 1: procedure GeneralInputSparsityReduction(A, {Vi}i\u2208[q], n, k, q, ) . Section C.10.3 2: for i = 1\u2192 q do 3: Choose sketching matrix Ti \u2208 Rti\u00d7n with ti = poly(k, q, 1/ ). 4: V\u0302i \u2190 TiVi. 5: end for 6: C \u2190 A(T1, T2, \u00b7 \u00b7 \u00b7 , Tq). 7: return {V\u0302i}i\u2208[q], C. 8: end procedure"}, {"heading": "C.10.4 Bicriteria algorithm", "text": "This section explains how to extend the bicriteria algorithm from third order tensors (Section C.4) to general q-th order tensors. Given any q-th order tensor A \u2208 Rn\u00d7n\u00d7\u00b7\u00b7\u00b7\u00d7n, we can output a rank-r tensor (or equivalently q matrices U1, U2, \u00b7 \u00b7 \u00b7 , Uq \u2208 Rn\u00d7r) such that,\n\u2016U1 \u2297 U2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 Uq \u2212A\u20162F \u2264 (1 + ) OPT,\nwhere r = Oq((k/ )q\u22121) and the algorithm takes Oq(nnz(A) + n \u00b7 poly(k, 1/ )).\nAlgorithm 18 General q-th Order Bicriteria Algorithm 1: procedure GeneralBicriteriaAlgorithm(A,n, k, q, ) . Section C.10.4 2: for i = 2\u2192 q do 3: Choose sketching matrix Si \u2208 Rnq\u22121\u00d7si with si = O(kq/ ). 4: Choose sketching matrix Ti \u2208 Rti\u00d7n with ti = poly(k, q, 1/ ). 5: Form matrix U\u0302i by setting (j2, j3, \u00b7 \u00b7 \u00b7 , jq)-th column to be (AiSi)ji . 6: end for 7: Solve minU1 \u2016U1B \u2212 (A(I, T2, \u00b7 \u00b7 \u00b7 , Tq))1\u20162F . 8: return {U\u0302i}i\u2208[q]. 9: end procedure"}, {"heading": "C.10.5 CURT decomposition", "text": "This section extends the tensor CURT algorithm from 3rd order tensors (Section C.7) to general q-th order tensors. Given a q-th order tensor A \u2208 Rn\u00d7n\u00d7\u00b7\u00b7\u00b7\u00d7n and a batch of matrices U1, U2, \u00b7 \u00b7 \u00b7 , Uq \u2208 Rn\u00d7k, we iteratively apply the proof in Theorem C.40 (or Theorem C.41) q times. Then for each i \u2208 [q], we are able to select di columns from the i-th dimension of tensor A (let Ci denote those columns) and also find a tensor U \u2208 Rd1\u00d7d2\u00d7\u00b7\u00b7\u00b7\u00d7dq such that,\n\u2016U(C1, C2, \u00b7 \u00b7 \u00b7 , Cq)\u2212A\u20162F \u2264 (1 + )\u2016U1 \u2297 U2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 Uq \u2212A\u20162F ,\nwhere either di = Oq(k log k + k/ ) (similar to Theorem C.40) or di = Oq(k/ ) (similar to Theorem C.41)."}, {"heading": "C.11 Matrix CUR decomposition", "text": "There is a long line of research on matrix CUR decomposition under operator, Frobenius or recently, entry-wise `1 norm [DMM08, BMD09, DR10, BDM11, BW14, SWZ17]. We provide the first\nAlgorithm 19 General q-th Order CURT Decomposition 1: procedure GeneralCURTDecomposition(A, {Ui}i\u2208[q], n, k, q, ) . Section C.10.5 2: for i = 1\u2192 q do 3: Form Bi =\nj<i U\u0302>j j>i U>j \u2208 Rk\u00d7n q\u22121 .\n4: if fast = true then . Optimal running time 5: 0 \u2190 0.01. 6: di \u2190 Oq(k log k + k/ ). 7: Di \u2190 FastTensorLeverageScoreGeneralOrder ({U\u0302j}j<i, {Uj}j>i, n, k, 0, di). . Algorithm 15 8: else . Optimal sample complexity 9: 0 \u2190 Oq( ). 10: Di \u2190 GeneralizedMatrixRowSubsetSelection (A>i , B>i , nq\u22121, n, k, 0). . Algorithm C.5, di = Oq(k/ ). 11: end if 12: U\u0302i \u2190 AiDi(BiDi)\u2020. 13: Ci \u2190 AiDi. 14: end for 15: U \u2190 (B1D1)\u2020 \u2297 (B2D2)\u2020 \u2297 \u00b7 \u00b7 \u00b7 \u2297 (BqDq)\u2020. 16: return {Ci}i\u2208[q], U . 17: end procedure\nalgorithm that runs in nnz(A) time, which improves the previous best matrix CUR decomposition algorithm under Frobenius norm [BW14]."}, {"heading": "C.11.1 Algorithm", "text": "Algorithm 20 Optimal Matrix CUR Decomposition Algorithm 1: procedure OptimalMatrixCUR(A,n, k, ) . Theorem C.48 2: \u2032 \u2190 0.1 . \u2032\u2032 \u2190 0.001 \u2032. 3: U\u0302 \u2190SparseSVD(A, k, \u2032). . U\u0302 \u2208 Rn\u00d7k 4: Choose S1 \u2208 Rn\u00d7n to be a sampling and rescaling diagonal matrix according to the leverage\nscores of U\u0302 with s1 = O( \u22122k log k) nonzero entries. 5: R, Y \u2190GeneralizedMatrixRowSubsetSelection(S1A,S1U\u0302 , s1, n, k, \u2032\u2032). .\nAlgorithm 7, R \u2208 Rr\u00d7n, Y \u2208 Rk\u00d7r and r = O(k/ ) 6: V\u0302 \u2190 Y R \u2208 Rk\u00d7n. 7: Choose S>2 \u2208 Rn\u00d7n to be a sampling and rescaling diagonal matrix according to the leverage\nscores of V\u0302 > \u2208 Rn\u00d7k with s2 = O( \u22122k log k) nonzero entries. 8: C>, Z> \u2190 GeneralizedMatrixRowSubsetSelection ((AS2)>, (V\u0302 S2)>, s2, n, k, \u2032\u2032). .\nAlgorithm 7, C \u2208 Rn\u00d7c, Z \u2208 Rc\u00d7k, and c = O(k/ ) 9: U \u2190 ZY . . U \u2208 Rc\u00d7r and rank(U) = k\n10: return C,U,R. 11: end procedure\nTheorem C.48. Given matrix A \u2208 Rn\u00d7n, for any k \u2265 1 and \u2208 (0, 1), there exists an algorithm that takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices C \u2208 Rn\u00d7c with c columns\nfrom A, R \u2208 Rr\u00d7n with r rows from A, and U \u2208 Rc\u00d7r with rank(U) = k such that r = c = O(k/ ) and,\n\u2016CUR\u2212A\u20162F \u2264 (1 + ) min rank\u2212k Ak \u2016Ak \u2212A\u20162F ,\nholds with probability at least 9/10.\nProof. We define\nOPT = min rank\u2212k Ak\n\u2016Ak \u2212A\u20162F .\nWe first compute U\u0302 \u2208 Rn\u00d7k by using the result of [CW13], so that U\u0302 satisfies:\nmin X\u2208Rk\u00d7n\n\u2016U\u0302X \u2212A\u20162F \u2264 (1 + ) OPT . (25)\nThis step can be done in O(nnz(A) + n poly(k, 1/ )) time. We choose S1 \u2208 Rn\u00d7n to be a sampling and rescaling diagonal matrix according to the leverage scores of U\u0302 , where here s1 = O( \u22122k log k) is the number of samples. This step also can be done in O(n poly(k, 1/ )) time.\nWe run GeneralizedMatrixRowSubsetSelection(Algorithm 7) on matrices S1A and S1U\u0302 . Then we obtain two new matrices R and Y , where R contains r = O(k/ ) rows of S1A and Y has size k \u00d7 r. According to Theorem C.14 and Corollary C.15, this step takes n poly(k, 1/ ) time.\nWe construct V\u0302 = Y R, and choose S>2 to be another sampling and rescaling diagonal matrix according to the leverage scores of V\u0302 > with s2 = O( \u22122k log k) nonzero entries. As in the case of constructing S1, this step can be done in O(n poly(k, 1/ )) time.\nWe run GeneralizedMatrixRowSubsetSelection(Algorithm 7) on matrices (AS2)> and (V\u0302 S2)\n>. Then we can obtain two new matrices C> and Y >, where C> contains c = O(k/ ) rows of (AS2)> and Z> has size k \u00d7 c. According to Theorem C.14 and Corollary C.15, this step takes n poly(k, 1/ ) time.\nThus, overall the running time is O(nnz(A) + n poly(k, 1/ )).\nCorrectness. Let\nX\u2217 = arg min X\u2208Rn\u00d7k \u2016XV\u0302 \u2212A\u20162F .\nAccording to Corollary C.15,\n\u2016CZV\u0302 S2 \u2212AS2\u20162F \u2264 (1 + \u2032\u2032) min X\u2208Rn\u00d7k \u2016XV\u0302 S2 \u2212AS2\u20162F \u2264 (1 + \u2032\u2032)\u2016X\u2217V\u0302 S2 \u2212AS2\u20162F .\nAccording to Theorem C.52, \u2032\u2032 = 0.001 \u2032,\n\u2016CZV\u0302 \u2212A\u20162F \u2264 (1 + \u2032)\u2016X\u2217V\u0302 \u2212A\u20162F . (26)\nLet\nX\u0303 = arg min X\u2208Rk\u00d7n\n\u2016U\u0302X \u2212A\u20162F .\nAccording to Corollary C.15,\n\u2016S1U\u0302Y R\u2212 S1A\u20162F \u2264 (1 + \u2032\u2032) min X\u2208Rk\u00d7n \u2016S1U\u0302X \u2212 S1A\u20162F \u2264 (1 + \u2032\u2032)\u2016S1U\u0302X\u0303 \u2212 S1A\u20162F .\nAccording to Theorem C.52, since \u2032\u2032 = 0.001 \u2032,\n\u2016U\u0302Y R\u2212A\u20162F \u2264 (1 + \u2032)\u2016U\u0302X\u0303 \u2212A\u20162F . (27) Then, we can conclude\n\u2016CUR\u2212A\u20162F = \u2016CZY R\u2212A\u20162F = \u2016CZV\u0302 \u2212A\u20162F \u2264 (1 + \u2032) min\nX\u2208Rn\u00d7k \u2016XV\u0302 \u2212A\u20162F\n\u2264 (1 + \u2032)\u2016U\u0302 V\u0302 \u2212A\u20162F \u2264 (1 + \u2032)2 min\nX\u2208Rk\u00d7n \u2016U\u0302X \u2212A\u20162F\n\u2264 (1 + \u2032)3 OPT \u2264 (1 + ) OPT .\nThe first equality follows since U = ZY . The second equality follows since Y R = V\u0302 . The first inequality follows by Equation (26). The third inequality follows by Equation (27). The fourth inequality follows by Equation (25). The last inequality follows since \u2032 = 0.1 .\nNotice that C has O(k/ ) reweighted columns of AS2, and AS2 is a subset of reweighted columns of A, so C has O(k/ ) reweighted columns of A. Similarly, we can prove that R has O(k/ ) reweighted rows of A. Thus, CUR is a CUR decomposition of A.\nC.11.2 Stronger property achieved by leverage scores\nClaim C.49. Given matrix A \u2208 Rn\u00d7m, for any distribution p = (p1, p2, \u00b7 \u00b7 \u00b7 , pn) define random variable X such that X = \u2016Ai\u201622/pi with probability pi, where Ai is the i-th row of matrix A. Then take m independent samples X1, X2, \u00b7 \u00b7 \u00b7 , Xm, and let Y = 1m \u2211m j=1X j. We have\nPr[Y \u2264 100\u2016A\u20162F ] \u2265 .99. Proof. We can compute the expectation of Xj , for any j \u2208 [m],\nE[Xj ] = n\u2211\ni=1\n\u2016Ai\u201622 pi \u00b7 pi = \u2016A\u20162F .\nThen E[Y ] = 1m \u2211m j=1 E[X j ] = \u2016A\u20162F . Using Markov\u2019s inequality, we have\nPr[Y \u2265 \u2016A\u20162F ] \u2264 .01.\nTheorem C.50 (The leverage score case of Theorem 39 in [CW13]). Let A \u2208 Rn\u00d7k, B \u2208 Rn\u00d7d. Let S \u2208 Rn\u00d7n denote a sampling and rescaling diagonal matrix according to the leverage scores of A. If the event occurs that S satisfies ( / \u221a k)-Frobenius norm approximate matrix product for A, and also S is a (1 + )-subspace embedding for A, then let X\u2217 be the optimal solution of minX \u2016AX \u2212B\u20162F , and B\u0303 \u2261 AX\u2217 \u2212B. Then, for all X \u2208 Rk\u00d7d,\n(1\u2212 2 )\u2016AX \u2212B\u20162F \u2264 \u2016S(AX \u2212B)\u20162F + \u2016B\u0303\u20162F \u2212 \u2016SB\u0303\u20162F \u2264 (1 + 2 )\u2016AX \u2212B\u20162F . Furthermore, if S has m = O( \u22122k log(k)) nonzero entries, the above event happens with probability at least 0.99.\nNote that Theorem 39 in [CW13] is stated in a way that holds for general sketching matrices. However, we are only interested in the case when S is a sampling and rescaling diagonal matrix according to the leverage scores. For completeness, we provide the full proof of the leverage score case with certain parameters.\nProof. Suppose S is a sampling and rescaling diagonal matrix according to the leverage scores of A, and it has m = O( \u22122k log k) nonzero entries. Then, according to Lemma C.22, S is a (1 + )- subspace embedding for A with probability at least 0.999, and according to Lemma C.29, S satisfies ( / \u221a k)-Frobenius norm approximate matrix product for A with probability at least 0.999.\nLet U \u2208 Rn\u00d7k denote an orthonormal basis of the column span of A. Then the leverage scores of U are the same as the leverage scores of A. Furthermore, for any X \u2208 Rk\u00d7d, there is a matrix Y such that AX = UY , and vice versa, so we can now assume A has k orthonormal columns.\nThen,\n\u2016S(AX \u2212B)\u20162F \u2212 \u2016SB\u0303\u20162F = \u2016SA(X \u2212X\u2217) + S(AX\u2217 \u2212B)\u20162F \u2212 \u2016SB\u0303\u20162F = \u2016SA(X \u2212X\u2217)\u20162F + \u2016S(AX\u2217 \u2212B)\u20162F + 2 tr ( (X \u2212X\u2217)>A>S>S(AX\u2217 \u2212B) ) \u2212 \u2016SB\u0303\u20162F = \u2016SA(X \u2212X\u2217)\u20162F + 2 tr ( (X \u2212X\u2217)>A>S>SB\u0303 )\n\ufe38 \ufe37\ufe37 \ufe38 \u03b1\n. (28)\nThe second equality follows using \u2016C + D\u20162F = \u2016C\u20162F + \u2016D\u20162F + 2 tr(C>D). The third equality follows from B\u0303 = AX\u2217 \u2212B. Now, let us first upper bound the term \u03b1 in Equation (28):\n\u2016SA(X \u2212X\u2217)\u20162F + 2 tr ( (X \u2212X\u2217)>A>S>SB\u0303 )\n\u2264 (1 + )\u2016A(X \u2212X\u2217)\u20162F + 2\u2016X \u2212X\u2217\u2016F \u2016A>S>SB\u0303\u2016F \u2264 (1 + )\u2016A(X \u2212X\u2217)\u20162F + 2( / \u221a k) \u00b7 \u2016X \u2212X\u2217\u2016F \u2016A\u2016F \u2016B\u0303\u2016F \u2264 (1 + )\u2016A(X \u2212X\u2217)\u20162F + 2 \u2016A(X \u2212X\u2217)\u2016F \u2016B\u0303\u2016F .\nThe first inequality follows since S is a (1+ ) subspace embedding of A, and tr(C>D) \u2264 \u2016C\u2016F \u2016D\u2016F . The second inequality follows since S satisfies ( / \u221a k)-Frobenius norm approximate matrix product\nfor A. The last inequality follows using that \u2016A\u2016F \u2264 \u221a k since A only has k orthonormal columns. Now, let us lower bound the term \u03b1 in Equation (28):\n\u2016SA(X \u2212X\u2217)\u20162F + 2 tr ( (X \u2212X\u2217)>A>S>SB\u0303 )\n\u2265 (1\u2212 )\u2016A(X \u2212X\u2217)\u20162F \u2212 2\u2016X \u2212X\u2217\u2016F \u2016A>S>SB\u0303\u2016F \u2265 (1\u2212 )\u2016A(X \u2212X\u2217)\u20162F \u2212 2( / \u221a k) \u00b7 \u2016X \u2212X\u2217\u2016F \u2016A\u2016F \u2016B\u0303\u2016F \u2265 (1\u2212 )\u2016A(X \u2212X\u2217)\u20162F \u2212 2 \u2016A(X \u2212X\u2217)\u2016F \u2016B\u0303\u2016F .\nThe first inequality follows since S is a (1+ ) subspace embedding ofA, and tr(C>D) \u2265 \u2212\u2016C\u2016F \u2016D\u2016F . The second inequality follows since S satisfies ( / \u221a k)-Frobenius norm approximate matrix product\nfor A. The last inequality follows using that \u2016A\u2016F \u2264 \u221a k since A only has k orthonormal columns.\nTherefore,\n(1\u2212 )\u2016A(X \u2212X\u2217)\u20162F \u2212 2 \u2016A(X \u2212X\u2217)\u2016F \u2016B\u0303\u2016F \u2264 \u2016S(AX \u2212B)\u20162F \u2212 \u2016SB\u0303\u20162F , (29)\nand\n(1 + )\u2016A(X \u2212X\u2217)\u20162F + 2 \u2016A(X \u2212X\u2217)\u2016F \u2016B\u0303\u2016F \u2265 \u2016S(AX \u2212B)\u20162F \u2212 \u2016SB\u0303\u20162F . (30)\nNotice that B\u0303 = AX\u2217 \u2212B = AA\u2020B \u2212B = (AA\u2020 \u2212 I)B, so according to the Pythagorean theorem, we have\n\u2016AX \u2212B\u20162F = \u2016A(X \u2212X\u2217)\u20162F + \u2016B\u0303\u20162F ,\nwhich means that\n\u2016A(X \u2212X\u2217)\u20162F = \u2016AX \u2212B\u20162F \u2212 \u2016B\u0303\u20162F . (31)\nUsing Equation (31), we can rewrite and lower bound the LHS of Equation (29),\n(1\u2212 )\u2016A(X \u2212X\u2217)\u20162F \u2212 2 \u2016A(X \u2212X\u2217)\u2016F \u2016B\u0303\u2016F = \u2016A(X \u2212X\u2217)\u20162F \u2212 ( \u2016A(X \u2212X\u2217)\u20162F + 2\u2016A(X \u2212X\u2217)\u2016F \u2016B\u0303\u2016F ) = \u2016AX \u2212B\u20162F \u2212 \u2016B\u0303\u20162F \u2212 ( \u2016A(X \u2212X\u2217)\u20162F + 2\u2016A(X \u2212X\u2217)\u2016F \u2016B\u0303\u2016F ) \u2265 \u2016AX \u2212B\u20162F \u2212 \u2016B\u0303\u20162F \u2212 ( \u2016A(X \u2212X\u2217)\u2016F + \u2016B\u0303\u2016F )2 \u2265 \u2016AX \u2212B\u20162F \u2212 \u2016B\u0303\u20162F \u2212 2 ( \u2016A(X \u2212X\u2217)\u20162F + \u2016B\u0303\u20162F )\n= (1\u2212 2 )\u2016AX \u2212B\u20162F \u2212 \u2016B\u0303\u20162F . (32)\nThe second step follows by Equation (31). The first inequality follows using a2 + 2ab < (a + b)2. The second inequality follows using (a + b)2 \u2264 2(a2 + b2). The last equality follows using \u2016A(X \u2212 X\u2217)\u20162F + \u2016B\u0303\u20162F = \u2016AX\u2212B\u20162F . Similarly, using Equation (31), we can rewrite and upper bound the LHS of Equation (30)\n(1 + )\u2016A(X \u2212X\u2217)\u20162F + 2 \u2016A(X \u2212X\u2217)\u2016F \u2016B\u0303\u2016F \u2264 (1 + 2 )\u2016AX \u2212B\u20162F \u2212 \u2016B\u0303\u20162F . (33)\nCombining Equations (29),(32),(30),(33), we conclude that\n(1\u2212 2 )\u2016AX \u2212B\u20162F \u2212 \u2016B\u0303\u20162F \u2264 \u2016S(AX \u2212B)\u20162F \u2212 \u2016SB\u0303\u20162F \u2264 (1 + 2 )\u2016AX \u2212B\u20162F \u2212 \u2016B\u0303\u20162F .\nTheorem C.51. Let A \u2208 Rn\u00d7k, B \u2208 Rn\u00d7d, and 12 > > 0. Let X\u2217 be the optimal solution to minX \u2016AX \u2212B\u20162F , and B\u0303 \u2261 AX\u2217 \u2212B. Let S \u2208 Rn\u00d7n denote a sketching matrix which satisfies the following:\n1. \u2016SB\u0303\u20162F \u2264 100 \u00b7 \u2016B\u0303\u20162F ,\n2. for all X \u2208 Rk\u00d7d,\n(1\u2212 )\u2016AX \u2212B\u20162F \u2264 \u2016S(AX \u2212B)\u20162F + \u2016B\u0303\u20162F \u2212 \u2016SB\u0303\u20162F \u2264 (1 + )\u2016AX \u2212B\u20162F .\nThen, for all X1, X2 \u2208 Rk\u00d7d satisfying\n\u2016SAX1 \u2212 SB\u20162F \u2264 ( 1 +\n100\n) \u00b7 \u2016SAX2 \u2212 SB\u20162F ,\nwe have\n\u2016AX1 \u2212B\u20162F \u2264 (1 + 5 ) \u00b7 \u2016AX2 \u2212B\u20162F .\nProof. Let A,B, S, be the same as in the statement of the theorem, and suppose S satisfies those two conditions. Let X1, X2 \u2208 Rk\u00d7d satisfy\n\u2016SAX1 \u2212 SB\u20162F \u2264 ( 1 +\n100\n) \u2016SAX2 \u2212 SB\u20162F .\nWe have\n\u2016AX1 \u2212B\u20162F \u2264 1\n1\u2212 ( \u2016S(AX1 \u2212B)\u20162F + \u2016B\u0303\u20162F \u2212 \u2016SB\u0303\u20162F )\n\u2264 1 1\u2212\n(( 1 +\n100\n) \u00b7 \u2016S(AX2 \u2212B)\u20162F + \u2016B\u0303\u20162F \u2212 \u2016SB\u0303\u20162F )\n= 1 1\u2212 (( 1 + 100 ) \u00b7 ( \u2016S(AX2 \u2212B)\u20162F + \u2016B\u0303\u20162F \u2212 \u2016SB\u0303\u20162F ) \u2212 100 \u00b7 ( \u2016B\u0303\u20162F \u2212 \u2016SB\u0303\u20162F )) \u2264 1 1\u2212 \u00b7 ( 1 + 100 ) \u00b7 \u2016AX2 \u2212B\u20162F \u2212 1 1\u2212 \u00b7 100 \u00b7 ( \u2016B\u0303\u20162F \u2212 \u2016SB\u0303\u20162F ) \u2264 (1 + 3 )\u2016AX2 \u2212B\u20162F + 1\n1\u2212 \u00b7 100 \u2016SB\u0303\u20162F\n\u2264 (1 + 3 )\u2016AX2 \u2212B\u20162F + 2 \u2016B\u0303\u20162F \u2264 (1 + 5 )\u2016AX2 \u2212B\u20162F .\nThe first inequality follows since S satisfies the second condition. The second inequality follows by the relationship between X1 and X2. The third inequality follows since S satisfies the second condition. The fifth inequality follows using that < 12 and that S satisfies the first condition. The last inequality follows using that \u2016B\u0303\u20162F = \u2016AX\u2217 \u2212B\u20162F \u2264 \u2016AX2 \u2212B\u20162F .\nTheorem C.52. Let A \u2208 Rn\u00d7k, B \u2208 Rn\u00d7d, and 12 > > 0. Let S \u2208 Rn\u00d7n denote a sampling and rescaling diagonal matrix according to the leverage scores of A. If S has at least m = O(k log(k)/ 2) nonzero entries, then with probability at least 0.98, for all X1, X2 \u2208 Rk\u00d7d satisfying\n\u2016SAX1 \u2212 SB\u20162F \u2264 (1 + 500 ) \u00b7 \u2016SAX2 \u2212 SB\u20162F ,\nwe have\n\u2016AX1 \u2212B\u20162F \u2264 (1 + ) \u00b7 \u2016AX2 \u2212B\u20162F .\nProof. The proof directly follows by Claim C.49, Theorem C.50 and Theorem C.51. Because of Claim C.49, S satisfies the first condition in the statement of Theorem C.51 with probability at least 0.99. According to Theorem C.50, S satisfies the second condition in the statement of Theorem C.51 with probability at least 0.99. Thus, with probability 0.98, by Theorem C.51, we complete the proof."}, {"heading": "D Entry-wise `1 Norm for Arbitrary Tensors", "text": "In this section, we provide several different algorithms for tensor `1-low rank approximation. Section D.1 provides some useful facts and definitions. Section D.2 presents several existence results. Section D.3 describes a tool that is able to reduce the size of the objective function from poly(n) to poly(k). Section D.4 discusses the case when the problem size is small. Section D.5 provides several bicriteria algorithms. Section D.6 summarizes a batch of algorithms. Section D.7 provides an algorithm for `1 norm CURT decomposition.\nNotice that if the rank\u2212k solution does not exist, then every bicriteria algorithm in Section D.5 can be stated in a form similar to Theorem 1.1, and every algorithm which can output a rank\u2212k solution in Section D.6 can be stated in a form similar to Theorem 1.2. See Section 1 for more details."}, {"heading": "D.1 Facts", "text": "We present a method that is able to reduce the entry-wise `1-norm objective function to the Frobenius norm objective function.\nFact D.1. Given a 3rd order tensor C \u2208 Rc1\u00d7c2\u00d7c3, three matrices V1 \u2208 Rc1\u00d7b1, V2 \u2208 Rc2\u00d7b2 , V3 \u2208 Rc3\u00d7b3, for any k \u2208 [1,mini bi], if X \u20321 \u2208 Rb1\u00d7k, X \u20322 \u2208 Rb2\u00d7k, X \u20323 \u2208 Rb3\u00d7k satisfies that,\n\u2016(V1X \u20321)\u2297 (V2X \u20322)\u2297 (V3X \u20323)\u2212 C\u2016F \u2264 \u03b1 min X1,X2,X3 \u2016(V1X1)\u2297 (V2X2)\u2297 (V3X3)\u2212 C\u2016F ,\nthen\n\u2016(V1X \u20321)\u2297 (V2X \u20322)\u2297 (V3X \u20323)\u2212 C\u20161 \u2264 \u03b1 \u221a c1c2c3 min\nX1,X2,X3 \u2016(V1X1)\u2297 (V2X2)\u2297 (V3X3)\u2212 C\u20161.\nWe extend Lemma C.15 in [SWZ17] to tensors:\nFact D.2. Given tensor A \u2208 Rn\u00d7n\u00d7n, let OPT = min rank\u2212k Ak \u2016A \u2212 Ak\u20161. For any r \u2265 k, if rank-r tensor B \u2208 Rn\u00d7n\u00d7n is an f -approximation to A, i.e.,\n\u2016B \u2212A\u20161 \u2264 f \u00b7OPT,\nand U, V,W \u2208 Rn\u00d7k is a g-approximation to B, i.e.,\n\u2016U \u2297 V \u2297W \u2212B\u20161 \u2264 g \u00b7 min rank\u2212k Bk \u2016Bk \u2212B\u20161,\nthen,\n\u2016U \u2297 V \u2297W \u2212A\u20161 . gf \u00b7OPT .\nProof. We define U\u0303 , V\u0303 , W\u0303 \u2208 Rn\u00d7k to be three matrices, such that\n\u2016U\u0303 \u2297 V\u0303 \u2297 W\u0303 \u2212B\u20161 \u2264 g min rank\u2212k Bk \u2016Bk \u2212B\u20161,\nand also define,\nU\u0302 , V\u0302 , W\u0302 = arg min U,V,W\u2208Rn\u00d7k \u2016U \u2297 V \u2297W \u2212B\u20161 and U\u2217, V \u2217,W \u2217 = arg min U,V,W\u2208Rn\u00d7k \u2016U \u2297 V \u2297W \u2212A\u20161.\nIt is obvious that,\n\u2016U\u0302 \u2297 V\u0302 \u2297 W\u0302 \u2212B\u20161 \u2264 \u2016U\u2217 \u2297 V \u2217 \u2297W \u2217 \u2212B\u20161. (34)\nThen,\n\u2016U\u0303 \u2297 V\u0303 \u2297 W\u0303 \u2212A\u20161 \u2264 \u2016U\u0303 \u2297 V\u0303 \u2297 W\u0303 \u2212B\u20161 + \u2016B \u2212A\u20161 by the triangle inequality \u2264 g\u2016U\u0302 \u2297 V\u0302 \u2297 W\u0302 \u2212B\u20161 + \u2016B \u2212A\u20161 by definition \u2264 g\u2016U\u2217 \u2297 V \u2217 \u2297W \u2217 \u2212B\u20161 + \u2016B \u2212A\u20161 by Equation (34) \u2264 g\u2016U\u2217 \u2297 V \u2217 \u2297W \u2217 \u2212A\u20161 + g\u2016B \u2212A\u20161 + \u2016B \u2212A\u20161 by the triangle inequality = gOPT +(g + 1)\u2016B \u2212A\u20161 by definition of OPT \u2264 gOPT +(g + 1)f \u00b7OPT since B is an f -approximation to A . gf OPT .\nThis completes the proof.\nUsing the above fact, we are able to optimize our approximation ratio."}, {"heading": "D.2 Existence results", "text": "Definition D.3 (`1 multiple regression cost preserving sketch - Definition D.5 in [SWZ17]). Given matrices U \u2208 Rn\u00d7r, A \u2208 Rn\u00d7d, let S \u2208 Rm\u00d7n. If \u2200\u03b2 \u2265 1, V\u0302 \u2208 Rr\u00d7d which satisfy\n\u2016SUV\u0302 \u2212 SA\u20161 \u2264 \u03b2 \u00b7 min V \u2208Rr\u00d7d \u2016SUV \u2212 SA\u20161,\nit holds that\n\u2016UV\u0302 \u2212A\u20161 \u2264 \u03b2 \u00b7 c \u00b7 min V \u2208Rr\u00d7d \u2016UV \u2212A\u20161,\nthen S provides a c-`1-multiple-regression-cost-preserving-sketch for (U,A).\nTheorem D.4. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exist three matrices S1 \u2208 Rn2\u00d7s1 , S2 \u2208 Rn2\u00d7s2 , S3 \u2208 Rn2\u00d7s3 such that\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(A1S1X1)i \u2297 (A2S2X2)i \u2297 (A3S3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n\u2264 \u03b1 min rank\u2212k Ak\u2208Rn\u00d7n\u00d7n \u2016Ak \u2212A\u20161,\nholds with probability 99/100. (I). Using a dense Cauchy transform, s1 = s2 = s3 = O\u0303(k), \u03b1 = O\u0303(k1.5) log3 n. (II). Using a sparse Cauchy transform, s1 = s2 = s3 = O\u0303(k 5), \u03b1 = O\u0303(k13.5) log3 n.\n(III). Guessing Lewis weights, s1 = s2 = s3 = O\u0303(k), \u03b1 = O\u0303(k1.5).\nProof. We use OPT to denote\nOPT := min rank\u2212k Ak\u2208Rn\u00d7n\u00d7n\n\u2016Ak \u2212A\u20161.\nGiven a tensor A \u2208 Rn1\u00d7n2\u00d7n3 , we define three matrices A1 \u2208 Rn1\u00d7n2n3 , A2 \u2208 Rn2\u00d7n3n1 , A3 \u2208 Rn3\u00d7n1n2 such that, for any i \u2208 [n1], j \u2208 [n2], l \u2208 [n3],\nAi,j,l = (A1)i,(j\u22121)\u00b7n3+l = (A2)j,(l\u22121)\u00b7n1+i = (A3)l,(i\u22121)\u00b7n2+j .\nWe fix V \u2217 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k, and use V \u22171 , V \u22172 , \u00b7 \u00b7 \u00b7 , V \u2217k to denote the columns of V \u2217 and W \u22171 ,W \u2217 2 , \u00b7 \u00b7 \u00b7 ,W \u2217k to denote the columns of W \u2217.\nWe consider the following optimization problem,\nmin U1,\u00b7\u00b7\u00b7 ,Uk\u2208Rn \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nUi \u2297 V \u2217i \u2297W \u2217i \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n,\nwhich is equivalent to\nmin U1,\u00b7\u00b7\u00b7 ,Uk\u2208Rn \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 [ U1 U2 \u00b7 \u00b7 \u00b7 Uk ]   V \u22171 \u2297W \u22171 V \u22172 \u2297W \u22172 \u00b7 \u00b7 \u00b7\nV \u2217k \u2297W \u2217k\n \u2212A \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 1 .\nWe use matrix Z1 to denote V \u2217> W \u2217> \u2208 Rk\u00d7n2 and matrix U to denote [ U1 U2 \u00b7 \u00b7 \u00b7 Uk ] .\nThen we can obtain the following equivalent objective function,\nmin U\u2208Rn\u00d7k\n\u2016UZ1 \u2212A1\u20161.\nChoose an `1 multiple regression cost preserving sketch S1 \u2208 Rn2\u00d7s1 for (Z>1 , A>1 ). We can obtain the optimization problem,\nmin U\u2208Rn\u00d7k \u2016UZ1S1 \u2212A1S1\u20161 = min U\u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016U iZ1S1 \u2212 (A1S1)i\u20161,\nwhere U i denotes the i-th row of matrix U \u2208 Rn\u00d7k and (A1S1)i denotes the i-th row of matrix A1S1. Instead of solving it under the `1-norm, we consider the `2-norm relaxation,\nmin U\u2208Rn\u00d7k \u2016UZ1S1 \u2212A1S1\u20162F = min U\u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016U iZ1S1 \u2212 (A1S1)i\u201622.\nLet U\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above optimization problem. Then, U\u0302 = A1S1(Z1S1)\n\u2020. We plug U\u0302 into the objective function under the `1-norm. According to Claim B.13, we have,\n\u2016U\u0302Z1S1 \u2212A1S1\u20161 = n\u2211\ni=1\n\u2016U\u0302 iZ1S1 \u2212 (A1S1)i\u20161 \u2264 \u221a s1 min\nU\u2208Rn\u00d7k \u2016UZ1S1 \u2212A1S1\u20161.\nSince S1 \u2208 Rn2\u00d7s1 satisfies Definition D.3, we have\n\u2016U\u0302Z1 \u2212A1\u20161 \u2264 \u03b1 min U\u2208Rn\u00d7k \u2016UZ1 \u2212A1\u20161 = \u03b1OPT,\nwhere \u03b1 = \u221a s1\u03b2 and \u03b2 (see Definition D.3) is a parameter which depends on which kind of sketching matrix we actually choose. It implies\n\u2016U\u0302 \u2297 V \u2217 \u2297W \u2217 \u2212A\u20161 \u2264 \u03b1OPT .\nAs a second step, we fix U\u0302 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k, and convert tensor A into matrix A2. Let matrix Z2 denote U\u0302> W \u2217>. We consider the following objective function,\nmin V \u2208Rn\u00d7k\n\u2016V Z2 \u2212A2\u20161,\nand the optimal cost of it is at most \u03b1OPT. Choose an `1 multiple regression cost preserving sketch S2 \u2208 Rn2\u00d7s2 for (Z>2 , A>2 ), and sketch on the right of the objective function to obtain this new objective function,\nmin V \u2208Rn\u00d7k \u2016V Z2S2 \u2212A2S2\u20161 = min U\u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016V iZ2S2 \u2212 (A2S2)i\u20161,\nwhere V i denotes the i-th row of matrix V and (A2S2)i denotes the i-th row of matrix A2S2. Instead of solving this under the `1-norm, we consider the `2-norm relaxation,\nmin U\u2208Rn\u00d7k \u2016V Z2S2 \u2212A2S2\u20162F = min V \u2208Rn\u00d7k \u2016V i(Z2S2)\u2212 (A2S2)i\u201622.\nLet V\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above problem. Then V\u0302 = A2S2(Z2S2)\u2020. By properties of the sketching matrix S2 \u2208 Rn2\u00d7s2 , we have,\n\u2016V\u0302 Z2 \u2212A2\u20161 \u2264 \u03b1 min V \u2208Rn\u00d7k \u2016V Z2 \u2212A2\u20161 \u2264 \u03b12 OPT,\nwhich implies\n\u2016U\u0302 \u2297 V\u0302 \u2297W \u2217 \u2212A\u20161 \u2264 \u03b12 OPT .\nAs a third step, we fix the matrices U\u0302 \u2208 Rn\u00d7k and V\u0302 \u2208 Rn\u00d7k. We can convert tensor A \u2208 Rn\u00d7n\u00d7n into matrix A3 \u2208 Rn2\u00d7n. Let matrix Z3 denote U\u0302> V\u0302 > \u2208 Rk\u00d7n2 . We consider the following objective function,\nmin W\u2208Rn\u00d7k\n\u2016WZ3 \u2212A3\u20161,\nand the optimal cost of it is at most \u03b12 OPT. Choose an `1 multiple regression cost preserving sketch S3 \u2208 Rn2\u00d7s3 for (Z>3 , A>3 ) and sketch on the right of the objective function to obtain the new objective function,\nmin W\u2208Rn\u00d7k\n\u2016WZ3S3 \u2212A3S3\u20161.\nLet W\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above problem. Then W\u0302 = A3S3(Z3S3)\u2020. By properties of sketching matrix S3 \u2208 Rn2\u00d7s3 , we have,\n\u2016W\u0302Z3 \u2212A3\u20161 \u2264 \u03b1 min W\u2208Rn\u00d7k \u2016WZ3 \u2212A3\u20161 \u2264 \u03b13 OPT .\nThus, we obtain,\nmin X1\u2208Rs1\u00d7k,X2\u2208Rs2\u00d7k,X3\u2208Rs3\u00d7k \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(A1S1X1)i \u2297 (A2S2X2)i \u2297 (A3S3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n\u2264 \u03b13 OPT .\nProof of (I) By Theorem C.1 in [SWZ17], we can use dense Cauchy transforms for S1, S2, S3, and then s1 = s2 = s3 = O(k log k) and \u03b1 = O( \u221a k log k log n).\nProof of (II) By Theorem C.1 in [SWZ17], we can use sparse Cauchy transforms for S1, S2, S3, and then s1 = s2 = s3 = O(k5 log5 k) and \u03b1 = O(k4.5 log4.5 k log n).\nProof of (III) By Theorem C.1 in [SWZ17], we can sample by Lewis weights. Then S1, S2, S3 \u2208 Rn2\u00d7n2 are diagonal matrices, and each of them has O(k log k) nonzero rows. This gives \u03b1 = O( \u221a k log k)."}, {"heading": "D.3 Polynomial in k size reduction", "text": "Definition D.5 (Definition D.1 in [SWZ17]). Given a matrix M \u2208 Rn\u00d7d, if matrix S \u2208 Rm\u00d7n satisfies\n\u2016SM\u20161 \u2264 \u03b2\u2016M\u20161,\nthen S has at most \u03b2 dilation on M .\nDefinition D.6 (Definition D.2 in [SWZ17]). Given a matrix U \u2208 Rn\u00d7k, if matrix S \u2208 Rm\u00d7n satisfies\n\u2200x \u2208 Rk, \u2016SUx\u20161 \u2265 1\n\u03b2 \u2016Ux\u20161,\nthen S has at most \u03b2 contraction on U .\nTheorem D.7. Given a tensor A \u2208 Rn1\u00d7n2\u00d7n3 and three matrices V1 \u2208 Rn1\u00d7b1 , V2 \u2208 Rn2\u00d7b2 , V3 \u2208 Rn3\u00d7b3 , let X\u22171 \u2208 Rb1\u00d7k, X\u22172 \u2208 Rb2\u00d7k, X\u22173 \u2208 Rb3\u00d7k satisfies\nX\u22171 , X \u2217 2 , X \u2217 3 = arg min X1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016V1X1 \u2297 V2X2 \u2297 V3X3 \u2212A\u20161.\nLet S \u2208 Rm\u00d7n have at most \u03b21 \u2265 1 dilation on V1X\u22171 \u00b7 ((V2X\u22172 )> (V3X\u22173 )>) \u2212 A1 and S have at most \u03b22 \u2265 1 contraction on V1. If X\u03021 \u2208 Rb1\u00d7k, X\u03022 \u2208 Rb2\u00d7k, X\u03023 \u2208 Rb3\u00d7k satisfies\n\u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SA\u20161 \u2264 \u03b2 min X1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016SV1X1 \u2297 V2X2 \u2297 V3X3 \u2212 SA\u20161,\nwhere \u03b2 \u2265 1, then\n\u2016V1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212A\u20161 . \u03b21\u03b22\u03b2 min X1,X2,X3 \u2016V1X1 \u2297 V2X2 \u2297 V3X3 \u2212A\u20161.\nThe proof idea is similar to [SWZ17].\nProof. Let A, V1, V2, V3, S,X\u22171 , X\u22172 , X\u22173 , \u03b21, \u03b22 be the same as stated in the theorem. Let X\u03021 \u2208 Rb1\u00d7k, X\u03022 \u2208 Rb2\u00d7k, X\u03023 \u2208 Rb3\u00d7k satisfy\n\u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SA\u20161 \u2264 \u03b2 min X1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016SV1X1 \u2297 V2X2 \u2297 V3X3 \u2212 SA\u20161.\nWe have,\n\u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SA\u20161 \u2265 \u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SV1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173\u20161 \u2212 \u2016SV1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212 SA\u20161 \u2265 1 \u03b22 \u2016V1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173\u20161 \u2212 \u03b21\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u20161 \u2265 1 \u03b22 \u2016V1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212A\u20161 \u2212 1 \u03b22 \u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u20161\n\u2212 \u03b21\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u20161 = 1\n\u03b22 \u2016V1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212A\u20161 \u2212 (\n1\n\u03b22 + \u03b21)\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u20161. (35)\nThe first and the third inequality follow by the triangle inequalities. The second inequality follows using that\n\u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SV1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173\u20161 = \u2225\u2225\u2225SV1(X\u03021 \u2212X\u22171 ) \u00b7 ( (V2(X\u03022 \u2212X\u22172 ))> (V3(X\u03023 \u2212X\u22173 ))> )\u2225\u2225\u2225 1 \u2265 1 \u03b22 \u2225\u2225\u2225V1(X\u03021 \u2212X\u22171 ) \u00b7 ( (V2(X\u03022 \u2212X\u22172 ))> (V3(X\u03023 \u2212X\u22173 ))> )\u2225\u2225\u2225 1 \u2265 1 \u03b22 \u2016V1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173\u20161,\nand\n\u2016SV1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212 SA\u20161 = \u2016S(V1X\u22171 \u00b7 ((V2X\u22172 )> (V3X\u22173 )>)\u2212A1)\u20161 \u2264 \u2016V1X\u22171 \u00b7 ((V2X\u22172 )> (V3X\u22173 )>)\u2212A1\u20161 = \u03b21\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u20161. (36)\nThen, we have\n\u2016V1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212A\u20161 \u2264 \u03b22\u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SA\u20161 + (1 + \u03b21\u03b22)\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u20161 \u2264 \u03b22\u03b2\u2016SV1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212 SA\u20161 + (1 + \u03b21\u03b22)\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u20161 \u2264 \u03b21\u03b22\u03b2\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u20161 + (1 + \u03b21\u03b22)\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u20161 \u2264 \u03b2(1 + 2\u03b21\u03b22)\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u20161.\nThe first inequality follows by Equation (35). The second inequality follows by\n\u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SA\u20161 \u2264 \u03b2 min X1,X2,X3 \u2016SV1X1 \u2297 V2X2 \u2297 V3X3 \u2212 SA\u20161.\nThe third inequality follows by Equation (36). The final inequality follows using that \u03b2 \u2265 1.\nLemma D.8. Let min(b1, b2, b3) \u2265 k. Given three matrices V1 \u2208 Rn\u00d7b1, V2 \u2208 Rn\u00d7b2, and V3 \u2208 Rn\u00d7b3, there exists an algorithm that takes O(nnz(A)) + n poly(b1, b2, b3) time and outputs a tensor\nAlgorithm 21 Reducing the Size of the Objective Function to poly(k)\n1: procedure L1PolyKSizeReduction(A, V1, V2, V3, n, b1, b2, b3, k) . Lemma D.8 2: for i = 1\u2192 3 do 3: ci \u2190 O\u0303(bi). 4: Choose sampling and rescaling matrices Ti \u2208 Rci\u00d7n according to the Lewis weights of Vi. 5: V\u0302i \u2190 TiVi \u2208 Rci\u00d7bi . 6: end for 7: C \u2190 A(T1, T2, T3) \u2208 Rc1\u00d7c2\u00d7c3 . 8: return V\u03021, V\u03022, V\u03023 and C. 9: end procedure"}, {"heading": "C \u2208 Rc1\u00d7c2\u00d7c3 and three matrices V\u03021 \u2208 Rc1\u00d7b1, V\u03022 \u2208 Rc2\u00d7b2 and V\u03023 \u2208 Rc3\u00d7b3 with c1 = c2 = c3 =", "text": "poly(b1, b2, b3), such that with probability 0.99, for any \u03b1 \u2265 1, if X \u20321, X \u20322, X \u20323 satisfy that, \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V\u03021X \u2032 1)i \u2297 (V\u03022X \u20322)i \u2297 (V\u03023X \u20323)i \u2212 C \u2225\u2225\u2225\u2225\u2225 1 \u2264 \u03b1 min X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 (V\u03021X1)i \u2297 (V\u03022X2)i \u2297 (V\u03023X3)i \u2212 C \u2225\u2225\u2225\u2225\u2225 1 ,\nthen, \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X \u2032 1)i \u2297 (V2X \u20322)i \u2297 (V3X \u20323)i \u2212A \u2225\u2225\u2225\u2225\u2225 1 . \u03b1 min X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 (V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 1 .\nProof. For simplicity, we define OPT to be\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n.\nLet T1 \u2208 Rc1\u00d7n sample according to the Lewis weights of V1 \u2208 Rn\u00d7b1 , where c1 = O\u0303(b1). Let T2 \u2208 Rc2\u00d7n sample according to the Lewis weights of V2 \u2208 Rn\u00d7b2 , where c2 = O\u0303(b2). Let T3 \u2208 Rc3\u00d7n sample according to the Lewis weights of V3 \u2208 Rn\u00d7b3 , where c3 = O\u0303(b3).\nFor any \u03b1 \u2265 1, let X \u20321 \u2208 Rb1\u00d7k, X \u20322 \u2208 Rb2\u00d7k, X \u20323 \u2208 Rb3\u00d7k satisfy\n\u2016T1V1X \u20321 \u2297 T2V2X \u20322 \u2297 T3V3X \u20323 \u2212A(T1, T2, T3)\u20161 \u2264 \u03b1 min\nX1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016T1V1X1 \u2297 T2V2X2 \u2297 T3V3X3 \u2212A(T1, T2, T3)\u20161.\nFirst, we regard T1 as the sketching matrix for the remainder. Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have\n\u2016V1X \u20321 \u2297 T2V2X \u20322 \u2297 T3V3X \u20323 \u2212A(I, T2, T3)\u20161 . \u03b1 min\nX1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016V1X1 \u2297 T2V2X2 \u2297 T3V3X3 \u2212A(I, T2, T3)\u20161.\nSecond, we regard T2 as a sketching matrix for V1X1 \u2297 V2X2 \u2297 T3V3X3 \u2212 A(I, I, T3). Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have\n\u2016V1X \u20321 \u2297 V2X \u20322 \u2297 T3V3X \u20323 \u2212A(I, I, T3)\u20161 . \u03b1 min\nX1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016V1X1 \u2297 V2X2 \u2297 T3V3X3 \u2212A(I, I, T3)\u20161.\nThird, we regard T3 as a sketching matrix for V1X1 \u2297 V2X2 \u2297 V3X3 \u2212A. Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X \u2032 1)i \u2297 (V2X \u20322)i \u2297 (V3X \u20323)i \u2212A \u2225\u2225\u2225\u2225\u2225 1 . \u03b1 min X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 (V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 1 .\nLemma D.9. Given tensor A \u2208 Rn1\u00d7n2\u00d7n3, and two matrices U \u2208 Rn1\u00d7s, V \u2208 Rn2\u00d7s with rank(U) = r, let T \u2208 Rt\u00d7n1 be a sampling/rescaling matrix according to the Lewis weights of U with t = O\u0303(r). Then with probability at least 0.99, for all X \u2032 \u2208 Rn3\u00d7s, \u03b1 \u2265 1 which satisfy\n\u2016T1U \u2297 V \u2297X \u2032 \u2212 T1A\u20161 \u2264 \u03b1 \u00b7 min X\u2208Rn3\u00d7s \u2016T1U \u2297 V \u2297X \u2212 T1A\u20161,\nit holds that\n\u2016U \u2297 V \u2297X \u2032 \u2212A\u20161 . \u03b1 \u00b7 min X\u2208Rn3\u00d7s \u2016U \u2297 V \u2297X \u2212A\u20161.\nThe proof is similar to the proof of Lemma D.8.\nProof. Let X\u2217 = arg min X\u2208Rn3\u00d7s \u2016U \u2297 V \u2297 X \u2212 A\u20161. Then according to Lemma D.11 in [SWZ17], T has at most constant dilation (Definition D.5) on U \u00b7 (V > (X\u2217)>) \u2212 A1, and has at most constant contraction (Definition D.6) on U . We first look at\n\u2016TU \u2297 V \u2297X \u2032 \u2212 TA\u20161 = \u2016TU \u00b7 (V > (X \u2032)>)\u2212 TA1\u20161 \u2265 \u2016TU \u00b7 ((V > (X \u2032)>)\u2212 (V > (X\u2217)>))\u20161 \u2212 \u2016TU \u00b7 (V > (X\u2217)>)\u2212 TA1\u20161 \u2265 1 \u03b22 \u2016U \u00b7 ((V > (X \u2032)>)\u2212A1\u20161 \u2212 ( 1 \u03b22 + \u03b21)\u2016U \u00b7 (V > (X\u2217)>)\u2212A1\u20161,\nwhere \u03b21 \u2265 1, \u03b22 \u2265 1 are two constants. Then we have:\n\u2016U \u2297 V \u2297X \u2032 \u2212A\u20161 \u2264 \u03b22\u2016TU \u00b7 (V > (X \u2032)>)\u2212 TA1\u20161 + (1 + \u03b21\u03b22)\u2016U \u00b7 (V > (X\u2217)>)\u2212A1\u20161 \u2264 \u03b1\u03b22\u2016TU \u00b7 (V > (X\u2217)>)\u2212 TA1\u20161 + (1 + \u03b21\u03b22)\u2016U \u00b7 (V > (X\u2217)>)\u2212A1\u20161 \u2264 \u03b1\u03b21\u03b22\u2016U \u00b7 (V > (X\u2217)>)\u2212A1\u20161 + (1 + \u03b21\u03b22)\u2016U \u00b7 (V > (X\u2217)>)\u2212A1\u20161 . \u03b1\u2016U \u2297 V \u2297X\u2217 \u2212A\u20161.\nCorollary D.10. Given tensor A \u2208 Rn\u00d7n\u00d7n, and two matrices U \u2208 Rn\u00d7s, V \u2208 Rn\u00d7s with rank(U) = r1, rank(V ) = r2, let T1 \u2208 Rt1\u00d7n be a sampling/rescaling matrix according to the Lewis weights of U , and let T2 \u2208 Rt2\u00d7n be a sampling/rescaling matrix according to the Lewis weights of V with t1 = O\u0303(r1), t2 = O\u0303(r2). Then with probability at least 0.99, for all X \u2032 \u2208 Rn\u00d7s, \u03b1 \u2265 1 which satisfy\n\u2016T1U \u2297 T2V \u2297X \u2032 \u2212A(T1, T2, I)\u20161 \u2264 \u03b1 \u00b7 min X\u2208Rn\u00d7s \u2016T1U \u2297 T2V \u2297X \u2212A(T1, T2, I)\u20161,\nit holds that\n\u2016U \u2297 V \u2297X \u2032 \u2212A\u20161 . \u03b1 \u00b7 min X\u2208Rn\u00d7s \u2016U \u2297 V \u2297X \u2212A\u20161.\nProof. We apply Lemma D.9 twice: if\n\u2016T1U \u2297 T2V \u2297X \u2032 \u2212A(T1, T2, I)\u20161 \u2264 \u03b1 \u00b7 min X\u2208Rn\u00d7s \u2016T1U \u2297 T2V \u2297X \u2212A(T1, T2, I)\u20161,\nthen\n\u2016U \u2297 T2V \u2297X \u2032 \u2212A(I, T2, I)\u20161 . \u03b1 \u00b7 min X\u2208Rn\u00d7s \u2016U \u2297 T2V \u2297X \u2212A(I, T2, I)\u20161.\nThen, we have\n\u2016U \u2297 V \u2297X \u2032 \u2212A\u20161 . \u03b1 \u00b7 min X\u2208Rn\u00d7s \u2016U \u2297 V \u2297X \u2212A\u20161."}, {"heading": "D.4 Solving small problems", "text": "Theorem D.11. Let maxi{ti, di} \u2264 n. Given a t1 \u00d7 t2 \u00d7 t3 tensor A and three matrices: a t1 \u00d7 d1 matrix T1, a t2 \u00d7 d2 matrix T2, and a t3 \u00d7 d3 matrix T3, if for \u03b4 > 0 there exists a solution to\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(T1X1)i \u2297 (T2X2)i \u2297 (T3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n:= OPT,\nsuch that each entry of Xi can be expressed using O(n\u03b4) bits, then there exists an algorithm that takes nO(\u03b4) \u00b7 2O(d1k+d2k+d3k) time and outputs three matrices: X\u03021, X\u03022, and X\u03023 such that \u2016(T1X\u03021)\u2297 (T2X\u03022)\u2297 (T3X\u03023)\u2212A\u20161 = OPT.\nProof. For each i \u2208 [3], we can create ti\u00d7 di variables to represent matrix Xi. Let x denote the list of these variables. Let B denote tensor \u2211k i=1(T1X1)i \u2297 (T2X2)i \u2297 (T3X3)i. Then we can write the following objective function,\nmin x\nt1\u2211\ni=1\nt2\u2211\nj=1\nt3\u2211\nl=1\n|Bi,j,l(x)\u2212Ai,j,l|.\nTo remove the | \u00b7 |, we create t1t2t3 extra variables \u03c3i,j,l. Then we obtain the objective function:\nmin x,\u03c3\nt1\u2211\ni=1\nt2\u2211\nj=1\nt3\u2211\nl=1\n\u03c3i,j,l(Bi,j,l(x)\u2212Ai,j,l)\ns.t. \u03c32i,j,l = 1,\n\u03c3i,j,l(Bi,j,l(x)\u2212Ai,j,l) \u2265 0, \u2016x\u201622 + \u2016\u03c3\u201622 \u2264 2O(n \u03b4)\nwhere the last constraint is unharmful, because there exists a solution that can be written using O(n\u03b4) bits. Note that the number of inequality constraints in the above system is O(t1t2t3), the degree is O(1), and the number of variables is v = (t1t2t3 +d1k+d2k+d3k). Thus by Theorem B.11, we know that the minimum nonzero cost is at least\n(2O(n \u03b4))\u22122 O\u0303(v) .\nIt is immediate that the upper bound on cost is at most 2O(n\u03b4), and thus the number of binary search steps is at most log(2O(n\u03b4))2O\u0303(v). In each step of the binary search, we need to choose a cost C between the lower bound and the upper bound, and write down the polynomial system,\nt1\u2211\ni=1\nt2\u2211\nj=1\nt3\u2211\nl=1\n\u03c3i,j,l(Bi,j,l(x)\u2212Ai,j,l) \u2264 C,\n\u03c32i,j,l = 1, \u03c3i,j,l(Bi,j,l(x)\u2212Ai,j,l) \u2265 0, \u2016x\u201622 + \u2016\u03c3\u201622 \u2264 2O(n \u03b4).\nUsing Theorem B.10, we can determine if there exists a solution to the above polynomial system. Since the number of variables is v, and the degree is O(1), the number of inequality constraints is t1t2t2. Thus, the running time is\npoly(bitsize) \u00b7 (# constraints \u00b7degree)# variables = nO(\u03b4)2O\u0303(v)"}, {"heading": "D.5 Bicriteria algorithms", "text": "We present several bicriteria algorithms with different tradeoffs. We first present an algorithm that runs in nearly linear time and outputs a solution with rank O\u0303(k3) in Theorem D.12. Then we show an algorithm that runs in nnz(A) time but outputs a solution with rank poly(k) in Theorem D.13. Then we explain an idea which is able to decrease the cubic rank to quadratic rank, and thus we can obtain Theorem D.14 and Theorem D.15.\nD.5.1 Input sparsity time\nAlgorithm 22 `1-Low Rank Approximation, Bicriteria Algorithm, rank-O\u0303(k3), Nearly Input Sparsity Time 1: procedure L1BicriteriaAlgorithm(A,n, k) . Theorem D.12 2: s1 \u2190 s2 \u2190 s3 \u2190 O\u0303(k). 3: For each i \u2208 [3], choose Si \u2208 Rn2\u00d7si to be a dense Cauchy transform. . Part (I) of\nTheorem D.2 4: Compute A1 \u00b7 S1, A2 \u00b7 S2, A3 \u00b7 S3. 5: Y1, Y2, Y3, C \u2190L1PolyKSizeReduction(A,A1S1, A2S2, A3S3, n, s1, s2, s3, k) .\nAlgorithm 21 6: Form objective function\nmin X\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\nXi,j,l(Y1)i \u2297 (Y2)j \u2297 (Y3)l \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 1 .\n7: Run `1-regression solver to find X. 8: return A1S1, A2S2, A3S3 and X. 9: end procedure\nTheorem D.12. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), let r = O\u0303(k3). There exists an algorithm which takes nnz(A) \u00b7 O\u0303(k)+O(n) poly(k)+poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n\u2264 O\u0303(k3/2) log3 n min rank\u2212k Ak \u2016Ak \u2212A\u20161\nholds with probability 9/10.\nProof. We first choose three dense Cauchy transforms Si \u2208 Rn2\u00d7si . According to Section B.7, for each i \u2208 [3], AiSi can be computed in nnz(A)\u00b7O\u0303(k) time. Then we apply Lemma D.8 (Algorithm 21). We obtain three matrices Y1, Y2, Y3 and a tensor C. Note that for each i \u2208 [3], Yi can be computed in n poly(k) time. Because C = A(T1, T2, T3) and T1, T2, T3 \u2208 Rn\u00d7O\u0303(k) are three sampling and rescaling matrices, C can be computed in nnz(A) + O\u0303(k3) time. At the end, we just need to run an `1-regression solver to find the solution to the problem,\nmin X\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\nXi,j,l(Y1)i \u2297 (Y2)j \u2297 (Y3)j \u2225\u2225\u2225\u2225\u2225\u2225 1 ,\nwhere (Y1)i denotes the i-th column of matrix Y1. Since the size of the above problem is only poly(k), this can be solved in poly(k) time.\nAlgorithm 23 `1-Low Rank Approximation, Bicriteria Algorithm, rank-poly(k), Input Sparsity Time 1: procedure L1BicriteriaAlgorithm(A,n, k) . Theorem D.13 2: s1 \u2190 s2 \u2190 s3 \u2190 O\u0303(k5). 3: For each i \u2208 [3], choose Si \u2208 Rn2\u00d7si to be a sparse Cauchy transform. . Part (II) of\nTheorem D.4 4: Compute A1 \u00b7 S1, A2 \u00b7 S2, A3 \u00b7 S3. 5: Y1, Y2, Y3, C \u2190L1PolyKSizeReduction(A,A1S1, A2S2, A3, S3, n, s1, s2, s3, k) .\nAlgorithm 21 6: Form objective function\nmin X\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\nXi,j,l(Y1)i \u2297 (Y2)j \u2297 (Y3)l \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 1 .\n7: Run `1-regression solver to find X. 8: return A1S1, A2S2, A3S3 and X. 9: end procedure\nTheorem D.13. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), let r = O\u0303(k15). There exists an algorithm that takes nnz(A)+O(n) poly(k)+poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n\u2264 poly(k, log n) min rank\u2212k Ak \u2016Ak \u2212A\u20161\nholds with probability 9/10.\nProof. We first choose three dense Cauchy transforms Si \u2208 Rn2\u00d7si . According to Section B.7, for each i \u2208 [3], AiSi can be computed in O(nnz(A)) time. Then we apply Lemma D.8 (Algorithm 21), and can obtain three matrices Y1, Y2, Y3 and a tensor C. Note that for each i \u2208 [3], Yi can be computed in O(n) poly(k) time. Because C = A(T1, T2, T3) and T1, T2, T3 \u2208 Rn\u00d7O\u0303(k) are three sampling and rescaling matrices, C can be computed in nnz(A) + O\u0303(k3) time. At the end, we just need to run an `1-regression solver to find the solution to the problem,\nmin X\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\nXi,j,l(Y1)i \u2297 (Y2)j \u2297 (Y3)l \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 1 ,\nwhere (Y1)i denotes the i-th column of matrix Y1. Since the size of the above problem is only poly(k), it can be solved in poly(k) time.\nD.5.2 Improving cubic rank to quadratic rank\nAlgorithm 24 `1-Low Rank Approximation, Bicriteria Algorithm, rank-O\u0303(k2), Nearly Input Sparsity Time 1: procedure L1BicriteriaAlgorithm(A,n, k) . Theorem D.14 2: s1 \u2190 s2 \u2190 s3 \u2190 O\u0303(k). 3: For each i \u2208 [3], choose Si \u2208 Rn2\u00d7si to be a dense Cauchy transform. . Part (I) of\nTheorem D.2 4: Compute A1 \u00b7 S1, A2 \u00b7 S2. 5: For each i \u2208 [2], choose Ti to be a sampling and rescaling diagonal matrix according to the\nLewis weights of AiSi, with ti = O\u0303(k) nonzero entries. 6: C \u2190 A(T1, T2, I). 7: Bi+(j\u22121)s1 \u2190 vec((T1A1S1)i \u2297 (T2A2S2)j),\u2200i \u2208 [s1], j \u2208 [s2]. 8: Form objective function minW \u2016WB \u2212 C3\u20161 9: Run `1-regression solver to find W\u0302 .\n10: Construct U\u0302 by using A1S1 according to Equation (38). 11: Construct V\u0302 by using A2S2 according to Equation (39). 12: return U\u0302 , V\u0302 , W\u0302 . 13: end procedure\nTheorem D.14. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), let r = O\u0303(k2). There exists an algorithm which takes nnz(A) \u00b7 O\u0303(k)+O(n) poly(k)+poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n\u2264 O\u0303(k3/2) log3 n min rank\u2212k Ak \u2016Ak \u2212A\u20161\nholds with probability 9/10.\nProof. Let OPT = min Ak\u2208Rn\u00d7n\u00d7n\n\u2016Ak\u2212A\u20161.We first choose three dense Cauchy transforms Si \u2208 Rn 2\u00d7si ,\n\u2200i \u2208 [3]. According to Section B.7, for each i \u2208 [3], AiSi can be computed in nnz(A) \u00b7 O\u0303(k) time. Then we choose Ti to be a sampling and rescaling diagonal matrix according to the Lewis weights of AiSi, \u2200i \u2208 [2].\nAccording to Theorem D.4, we have\nmin X1\u2208Rs1\u00d7k,X2\u2208Rs2\u00d7k,X3\u2208Rs3\u00d7k \u2225\u2225\u2225\u2225\u2225 k\u2211\nl=1\n(A1S1X1)l \u2297 (A2S2X2)l \u2297 (A3S3X3)l \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n\u2264 O\u0303(k1.5) log3 nOPT\nNow we fix an l and we have:\n(A1S1X1)l \u2297 (A2S2X2)l \u2297 (A3S3X3)l\n=\n( s1\u2211\ni=1\n(A1S1)i(X1)i,l\n) \u2297   s2\u2211\nj=1\n(A2S2)j(X2)j,l  \u2297 (A3S3X3)l\n=\ns1\u2211\ni=1\ns2\u2211\nj=1\n(A1S1)i \u2297 (A2S2)j \u2297 (A3S3X3)l(X1)i,l(X2)j,l\nThus, we have\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\n(A1S1)i \u2297 (A2S2)j \u2297 ( k\u2211\nl=1\n(A3S3X3)l(X1)i,l(X2)j,l\n) \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 1 \u2264 O\u0303(k1.5) log3 nOPT .\n(37)\nWe create matrix U\u0302 \u2208 Rn\u00d7s1s2 by copying matrix A1S1 s2 times, i.e., U\u0302 = [ A1S1 A1S1 \u00b7 \u00b7 \u00b7 A1S1 ] . (38)\nWe create matrix V\u0302 \u2208 Rn\u00d7s1s2 by copying the i-th column of A2S2 a total of s1 times into the columns (i\u2212 1)s1, \u00b7 \u00b7 \u00b7 , is1 of V\u0302 , for each i \u2208 [s2], i.e.,\nV\u0302 = [ (A2S2)1 \u00b7 \u00b7 \u00b7 (A2S2)1 (A2S2)2 \u00b7 \u00b7 \u00b7 (A2S2)2 \u00b7 \u00b7 \u00b7 (A2S2)s2 \u00b7 \u00b7 \u00b7 (A2S2)s2 . ] (39)\nAccording to Equation (37), we have:\nmin W\u2208Rn\u00d7s1s2\n\u2016U\u0302 \u2297 V\u0302 \u2297W \u2212A\u20161 \u2264 O\u0303(k1.5) log3 n \u00b7OPT .\nLet\nW\u0302 = arg min W\u2208Rn\u00d7s1s2\n\u2016T1U\u0302 \u2297 T2V\u0302 \u2297W \u2212A(T1, T2, I)\u20161.\nDue to Corollary D.10, we have\n\u2016U\u0302 \u2297 V\u0302 \u2297 W\u0302 \u2212A\u20161 \u2264 O\u0303(k1.5) log3 n \u00b7OPT .\nPutting it all together, we have that U\u0302 , V\u0302 , W\u0302 gives a rank-O\u0303(k2) bicriteria algorithm to the original problem.\nTheorem D.15. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), let r = O\u0303(k10). There exists an algorithm which takes nnz(A) + O(n) poly(k) + poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n\u2264 poly(k, log n) min rank\u2212k Ak \u2016Ak \u2212A\u20161\nholds with probability 9/10.\nAlgorithm 25 `1-Low Rank Approximation, Bicriteria Algorithm, rank-poly(k), Input Sparsity Time 1: procedure L1BicriteriaAlgorithm(A,n, k) . Theorem D.15 2: s1 \u2190 s2 \u2190 s3 \u2190 O\u0303(k5). 3: For each i \u2208 [3], choose Si \u2208 Rn2\u00d7si to be a sparse Cauchy transform. . Part (II) of\nTheorem D.2 4: Compute A1 \u00b7 S1, A2 \u00b7 S2. 5: For each i \u2208 [2], choose Ti to be a sampling and rescaling diagonal matrix according to the\nLewis weights of AiSi, with ti = O\u0303(k) nonzero entries. 6: C \u2190 A(T1, T2, I). 7: Bi+(j\u22121)s1 \u2190 vec((T1A1S1)i \u2297 (T2A2S2)j),\u2200i \u2208 [s1], j \u2208 [s2]. 8: Form objective function minW \u2016WB \u2212 C3\u20161. 9: Run `1-regression solver to find W\u0302 .\n10: Construct U\u0302 by using A1S1 according to Equation (38). 11: Construct V\u0302 by using A2S2 according to Equation (39). 12: return U\u0302 , V\u0302 , W\u0302 . 13: end procedure\nProof. The proof is similar to the proof of Theorem D.14. The only difference is that instead of choosing dense Cauchy matrices S1, S2, we choose sparse Cauchy matrices.\nNotice that if we firstly apply a sparse Cauchy transform, we can reduce the rank of the matrix to poly(k). Then we apply a dense Cauchy transform and can further reduce the dimension while only incurring another poly(k) factor in the approximation ratio. By combining a sparse Cauchy transform and a dense Cauchy transform, we can improve the running time from nnz(A) \u00b7 O\u0303(k) to nnz(A).\nCorollary D.16. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), let r = O\u0303(k2). There exists an algorithm which takes nnz(A) + O(n) poly(k) + poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n\u2264 poly(k, log n) min rank\u2212k Ak \u2016Ak \u2212A\u20161\nholds with probability 9/10."}, {"heading": "D.6 Algorithms", "text": "In this section, we show two different algorithms by using different kind of sketches. One is shown in Theorem D.17 which gives a fast running time. Another one is shown in Theorem D.19 which gives the best approximation ratio.\nD.6.1 Input sparsity time algorithm\nTheorem D.17. Given a 3rd tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm that takes nnz(A) \u00b7 O\u0303(k) + O(n) poly(k) + 2O\u0303(k2) time and outputs three matrices U, V,W \u2208 Rn\u00d7k such that,\n\u2016U \u2297 V \u2297W \u2212A\u20161 \u2264 poly(k, log n) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20161.\nAlgorithm 26 `1-Low Rank Approximation, Bicriteria Algorithm, rank-O\u0303(k2), Input Sparsity Time 1: procedure L1BicriteriaAlgorithm(A,n, k) . Corollary D.16 2: s1 \u2190 s2 \u2190 s3 \u2190 O\u0303(k). 3: For each i \u2208 [3], choose Si \u2208 Rn2\u00d7si to be the composition of a sparse Cauchy transform and\na dense Cauchy transform. . Part (I,II) of Theorem D.2 4: Compute A1 \u00b7 S1, A2 \u00b7 S2. 5: For each i \u2208 [2], choose Ti to be a sampling and rescaling diagonal matrix according to the\nLewis weights of AiSi, with ti = O\u0303(k) nonzero entries. 6: C \u2190 A(T1, T2, I). 7: Bi+(j\u22121)s1 \u2190 vec((T1A1S1)i \u2297 (T2A2S2)j),\u2200i \u2208 [s1], j \u2208 [s2]. 8: Form objective function minW \u2016WB \u2212 C3\u20161. 9: Run `1-regression solver to find W\u0302 .\n10: Construct U\u0302 by using A1S1 according to Equation (38). 11: Construct V\u0302 by using A2S2 according to Equation (39). 12: return U\u0302 , V\u0302 , W\u0302 . 13: end procedure\nAlgorithm 27 `1-Low Rank Approximation, Input sparsity Time Algorithm 1: procedure L1TensorLowRankApproxInputSparsity(A,n, k) . Theorem D.17 2: s1 \u2190 s2 \u2190 s3 \u2190 O\u0303(k5). 3: Choose Si \u2208 Rn2\u00d7si to be a dense Cauchy transform, \u2200i \u2208 [3]. . Part (I) of Theorem D.4 4: Compute A1 \u00b7 S1, A2 \u00b7 S2, and A3 \u00b7 S3. 5: Y1, Y2, Y3, C \u2190L1PolyKSizeReduction(A,A1S1, A2S2, A3S3, n, s1, s2, s3, k). .\nAlgorithm 21 6: Create variables s1 \u00d7 k + s2 \u00d7 k + s3 \u00d7 k variables for each entry of X1, X2, X3. 7: Form objective function \u2016(Y1X1)\u2297 (Y2X2)\u2297 (Y3X3)\u2212 C\u20162F . 8: Run polynomial system verifier. 9: return A1S1X1, A2S2X2, A3S3X3.\n10: end procedure\nholds with probability at least 9/10.\nProof. First, we apply part (II) of Theorem D.4. Then AiSi can be computed in O(nnz(A)) time. Second, we use Lemma D.8 to reduce the size of the objective function from O(n3) to poly(k) in n poly(k) time by only losing a constant factor in approximation ratio. Third, we use Claim B.15 to relax the objective function from entry-wise `1-norm to Frobenius norm, and this step causes us to lose some other poly(k) factors in approximation ratio. As a last step, we use Theorem C.45 to solve the Frobenius norm objective function.\nNotice again that if we first apply a sparse Cauchy transform, we can reduce the rank of the matrix to poly(k). Then as before we can apply a dense Cauchy transform to further reduce the dimension while only incurring another poly(k) factor in the approximation ratio. By combining a sparse Cauchy transform and a dense Cauchy transform, we can improve the running time from nnz(A) \u00b7 O\u0303(k) to nnz(A), while losing some additional poly(k) factors in approximation ratio.\nCorollary D.18. Given a 3rd tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm that takes nnz(A) +O(n) poly(k) + 2O\u0303(k2) time and outputs three matrices U, V,W \u2208 Rn\u00d7k such that,\n\u2016U \u2297 V \u2297W \u2212A\u20161 \u2264 poly(k, log n) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20161.\nholds with probability at least 9/10."}, {"heading": "D.6.2 O\u0303(k3/2)-approximation algorithm", "text": "Algorithm 28 `1-Low Rank Approximation Algorithm, O\u0303(k3/2)-approximation\n1: procedure L1TensorLowRankApproxK(A,n, k) . Theorem D.19 2: s1 \u2190 s2 \u2190 s3 \u2190 O\u0303(k). 3: Guess diagonal matrices Si \u2208 Rn2\u00d7si with si nonzero entries, \u2200i \u2208 [3]. . Part (III) of\nTheorem D.4 4: Compute A1 \u00b7 S1, A2 \u00b7 S2, and A3 \u00b7 S3. 5: Y1, Y2, Y3, C \u2190L1PolyKSizeReduction(A,A1S1, A2S2, A3S3, n, s1, s2, s3, k). .\nAlgorithm 21 6: Create s1 \u00d7 k + s2 \u00d7 k + s3 \u00d7 k variables for each entry of X1, X2, X3. 7: Form objective function \u2016(Y1X1)\u2297 (Y2X2)\u2297 (Y3X3)\u2212 C\u20161. 8: Run polynomial system verifier. 9: return U, V,W .\n10: end procedure\nTheorem D.19. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm that takes nO\u0303(k)2O\u0303(k3) time and output three matrices U, V,W \u2208 Rn\u00d7k such that,\n\u2016U \u2297 V \u2297W \u2212A\u20161 \u2264 O\u0303(k3/2) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20161.\nholds with probability at least 9/10.\nProof. First, we apply part (III) of Theorem D.4. Then, guessing Si requires nO\u0303(k) time. Second, we use Lemma D.8 to reduce the size of the objective from O(n3) to poly(k) in polynomial time while only losing a constant factor in approximation ratio. Third, we use Theorem D.11 to solve the entry-wise `1-norm objective function directly."}, {"heading": "D.7 CURT decomposition", "text": "Theorem D.20. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, let k \u2265 1, let UB, VB,WB \u2208 Rn\u00d7k denote a rank-k, \u03b1-approximation to A. Then there exists an algorithm which takes O(nnz(A)) + O(n2) poly(k) time and outputs three matrices: C \u2208 Rn\u00d7c with columns from A, R \u2208 Rn\u00d7r with rows from A, T \u2208 Rn\u00d7t with tubes from A, and a tensor U \u2208 Rc\u00d7r\u00d7t with rank(U) = k such that c = r = t = O(k log k), and\n\u2225\u2225\u2225\u2225\u2225\u2225 c\u2211\ni=1\nr\u2211\nj=1\nt\u2211\nl=1\nUi,j,l \u00b7 Ci \u2297Rj \u2297 Tl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 1 \u2264 O\u0303(k1.5)\u03b1 min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20161\nholds with probability 9/10.\nAlgorithm 29 `1-CURT Decomposition Algorithm 1: procedure L1CURT(A,UB, VB,WB, n, k) . Theorem D.20 2: Form B1 = V >B W>B \u2208 Rk\u00d7n\n2 . 3: Let D>1 \u2208 Rn\n2\u00d7n2 be the sampling and rescaling diagonal matrix corresponding to the Lewis weights of B>1 , and let D1 have d1 = O(k log k) nonzero entries.\n4: Form U\u0302 = A1D1(B1D1)\u2020 \u2208 Rn\u00d7k. 5: Form B2 = U\u0302> W>B \u2208 Rk\u00d7n\n2 . 6: Let D>2 \u2208 Rn\n2\u00d7n2 be the sampling and rescaling diagonal matrix corresponding to the Lewis weights of B>2 , and let D2 have d2 = O(k log k) nonzero entries.\n7: Form V\u0302 = A2D2(B2D2)\u2020 \u2208 Rn\u00d7k. 8: Form B3 = U\u0302> V\u0302 > \u2208 Rk\u00d7n2 . 9: Let D>3 \u2208 Rn\n2\u00d7n2 be the sampling and rescaling diagonal matrix corresponding to the Lewis weights of B>3 , and let D3 have d3 = O(k log k) nonzero entries.\n10: C \u2190 A1D1, R\u2190 A2D2, T \u2190 A3D3. 11: U \u2190\u2211ki=1((B1D1)\u2020)i \u2297 ((B2D2)\u2020)i \u2297 ((B3D3)\u2020)i. 12: return C, R, T and U . 13: end procedure\nProof. We define\nOPT := min rank\u2212k A\u2032\n\u2016A\u2032 \u2212A\u20161.\nWe already have three matrices UB \u2208 Rn\u00d7k, VB \u2208 Rn\u00d7k and WB \u2208 Rn\u00d7k and these three matrices provide a rank-k, \u03b1 approximation to A, i.e.,\n\u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(UB)i \u2297 (VB)i \u2297 (WB)i \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n\u2264 \u03b1OPT (40)\nLet B1 = V >B W>B \u2208 Rk\u00d7n 2 denote the matrix where the i-th row is the vectorization of (VB)i \u2297 (WB)i. By Section B.3, we can compute D1 \u2208 Rn 2\u00d7n2 which is a sampling and rescaling matrix corresponding to the Lewis weights of B>1 in O(n2 poly(k)) time, and there are d1 = O(k log k) nonzero entries on the diagonal of D1. Let Ai \u2208 Rn\u00d7n2 denote the matrix obtained by flattening A along the i-th direction, for each i \u2208 [3].\nDefine U\u2217 \u2208 Rn\u00d7k to be the optimal solution to min U\u2208Rn\u00d7k \u2016UB1\u2212A1\u20161, U\u0302 = A1D1(B1D1)\u2020 \u2208 Rn\u00d7k,\nV0 \u2208 Rn\u00d7k to be the optimal solution to min V \u2208Rn\u00d7k \u2016V \u00b7 (U\u0302> W>B )\u2212A2\u20161, and U \u2032 to be the optimal solution to min\nU\u2208Rn\u00d7k \u2016UB1D1 \u2212A1D1\u20161.\nBy Claim B.13, we have\n\u2016U\u0302B1D1 \u2212A1D1\u20161 \u2264 \u221a d1\u2016U \u2032B1D1 \u2212A1D1\u20161\nDue to Lemma D.11 and Lemma D.8 (in [SWZ17]) with constant probability, we have\n\u2016U\u0302B1 \u2212A1\u20161 \u2264 \u221a d1\u03b1D1\u2016U\u2217B1 \u2212A1\u20161, (41)\nwhere \u03b1D1 = O(1).\nRecall that (U\u0302> W>B ) \u2208 Rk\u00d7n 2 denotes the matrix where the i-th row is the vectorization of\nU\u0302i \u2297 (WB)i, \u2200i \u2208 [k]. Now, we can show,\n\u2016V0 \u00b7 (U\u0302> W>B )\u2212A2\u20161 \u2264 \u2016U\u0302B1 \u2212A1\u20161 by V0 = arg min V \u2208Rn\u00d7k \u2016V \u00b7 (U\u0302> W>B )\u2212A2\u20161\n. \u221a d1\u2016U\u2217B1 \u2212A1\u20161 by Equation (41) \u2264 \u221a d1\u2016UBB1 \u2212A1\u20161 by U\u2217 = arg min\nU\u2208Rn\u00d7k \u2016UB1 \u2212A1\u20161\n\u2264 O( \u221a d1)\u03b1OPT by Equation (40) (42)\nWe define B2 = U\u0302> W>B . We can compute D2 \u2208 Rn 2\u00d7n2 which is a sampling and rescaling matrix corresponding to the Lewis weights of B>2 in O(n2 poly(k)) time, and there are d2 = O(k log k) nonzero entries on the diagonal of D2.\nDefine V \u2217 \u2208 Rn\u00d7k to be the optimal solution of minV \u2208Rn\u00d7k \u2016V B2 \u2212A2\u20161, V\u0302 = A2D2(B2D2)\u2020 \u2208 Rn\u00d7k, W0 \u2208 Rn\u00d7k to be the optimal solution of min\nW\u2208Rn\u00d7k \u2016W \u00b7 (U\u0302> V\u0302 >)\u2212 A3\u20161, and V \u2032 to be the\noptimal solution of min V \u2208Rn\u00d7k \u2016V B2D2 \u2212A2D2\u20161. By Claim B.13, we have\n\u2016V\u0302 B2D2 \u2212A2D2\u20161 \u2264 \u221a d2\u2016V \u2032B2D2 \u2212A2D2\u20161.\nDue to Lemma D.11 and Lemma D.8(in [SWZ17]) with constant probability, we have\n\u2016V\u0302 B2 \u2212A2\u20161 \u2264 \u221a d2\u03b1D2\u2016V \u2217B2 \u2212A2\u20161, (43)\nwhere \u03b1D2 = O(1). Recall that (U\u0302> V\u0302 >) \u2208 Rk\u00d7n2 denotes the matrix for which the i-th row is the vectorization of U\u0302i \u2297 V\u0302i, \u2200i \u2208 [k]. Now, we can show,\n\u2016W0 \u00b7 (U\u0302> V\u0302 >)\u2212A3\u20161 \u2264 \u2016V\u0302 B2 \u2212A2\u20161 by W0 = arg min W\u2208Rn\u00d7k \u2016W \u00b7 (U\u0302> V\u0302 >)\u2212A3\u20161\n. \u221a d2\u2016V \u2217B2 \u2212A2\u20161 by Equation (43) \u2264 \u221a d2\u2016V0B2 \u2212A2\u20161 by V \u2217 = arg min\nV \u2208Rn\u00d7k \u2016V B2 \u2212A2\u20161\n\u2264 O( \u221a d1d2)\u03b1OPT by Equation (42) (44)\nWe define B3 = U\u0302> V\u0302 >. We can compute D3 \u2208 Rn2\u00d7n2 which is a sampling and rescaling matrix corresponding to the Lewis weights of B>3 in O(n2 poly(k)) time, and there are d3 = O(k log k) nonzero entries on the diagonal of D3.\nDefineW \u2217 \u2208 Rn\u00d7k to be the optimal solution to minW\u2208Rn\u00d7k \u2016WB3\u2212A3\u20161, W\u0302 = A3D3(B3D3)\u2020 \u2208 Rn\u00d7k, and W \u2032 to be the optimal solution to min\nW\u2208Rn\u00d7k \u2016WB3D3 \u2212A3D3\u20161.\nBy Claim B.13, we have\n\u2016W\u0302B3D3 \u2212A3D3\u20161 \u2264 \u221a d3\u2016W \u2032B3D3 \u2212A3D3\u20161.\nDue to Lemma D.11 and Lemma D.8(in [SWZ17]) with constant probability, we have\n\u2016W\u0302B3 \u2212A3\u20161 \u2264 \u221a d3\u03b1D3\u2016W \u2217B3 \u2212A3\u20161, (45)\nwhere \u03b1D3 = O(1). Now we can show,\n\u2016W\u0302B3 \u2212A3\u20161 . \u221a d3\u2016W \u2217B3 \u2212A3\u20161, by Equation (45)\n\u2264 \u221a d3\u2016W0B3 \u2212A3\u20161, by W \u2217 = arg min\nW\u2208Rn\u00d7k \u2016WB3 \u2212A3\u20161\n\u2264 O( \u221a d1d2d3)\u03b1OPT by Equation (44)\nThus, it implies, \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nU\u0302i \u2297 V\u0302i \u2297 W\u0302i \u2212A \u2225\u2225\u2225\u2225\u2225\n1\n\u2264 poly(k, log n) OPT .\nwhere U\u0302 = A1D1(B1D1)\u2020, V\u0302 = A2D2(B2D2)\u2020, W\u0302 = A3D3(B3D3)\u2020.\nAlgorithm 30 `1-CURT decomposition algorithm\n1: procedure L1CURT+(A,n, k) . Theorem D.21 2: UB, VB,WB \u2190L1LowRankApproximation(A,n, k). . Corollary D.18 3: C,R, T, U \u2190 L1CURT(A,UB, VB,WB, n, k). . Algorithm 29 4: return C, R, T and U . 5: end procedure\nTheorem D.21. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm which takes O(nnz(A)) + O(n2) poly(k) + 2O\u0303(k2) time and outputs three matrices C \u2208 Rn\u00d7c with columns from A, R \u2208 Rn\u00d7r with rows from A, T \u2208 Rn\u00d7t with tubes from A, and a tensor U \u2208 Rc\u00d7r\u00d7t with rank(U) = k such that c = r = t = O(k log k), and\n\u2225\u2225\u2225\u2225\u2225\u2225 c\u2211\ni=1\nr\u2211\nj=1\nt\u2211\nl=1\nUi,j,l \u00b7 Ci \u2297Rj \u2297 Tl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 1 \u2264 poly(k, log n) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20161,\nholds with probability 9/10.\nProof. This follows by combining Corollary D.18 and Theorem D.20."}, {"heading": "E Entry-wise `p Norm for Arbitrary Tensors, 1 < p < 2", "text": "There is a long line of research dealing with `p norm-related problems [DDH+09, MM13, CDMI+13, CP15, BCKY16, YCRM16, BBC+17].\nIn this section, we provide several different algorithms for tensor `p-low rank approximation. Section E.1 formally states the `p version of Theorem C.1 in [SWZ17]. Section E.2 presents several existence results. Section E.3 describes a tool that is able to reduce the size of the objective function from poly(n) to poly(k). Section E.4 discusses the case when the problem size is small. Section E.5 provides several bicriteria algorithms. Section E.6 summarizes a batch of algorithms. Section E.7 provides an algorithm for `p norm CURT decomposition.\nNotice that if the rank-k solution does not exist, then every bicriteria algorithm in Section E.5 can be stated in the form as Theorem 1.1, and every algorithm which can output a rank-k solution in Section E.6 can be stated in the form as Theorem 1.2. See Section 1 for more details.\nE.1 Existence results for matrix case\nTheorem E.1 ([SWZ17]). Let 1 \u2264 p < 2. Given V \u2208 Rk\u00d7n, A \u2208 Rd\u00d7n. Let S \u2208 Rn\u00d7s be a proper random sketching matrix. Let\nU\u0302 = arg min U\u2208Rd\u00d7k\n\u2016UV S \u2212AS\u20162F ,\ni.e.,\nU\u0302 = AS(V S)\u2020.\nThen with probability at least 0.999,\n\u2016U\u0302V \u2212A\u2016pp \u2264 \u03b1 \u00b7 min U\u2208Rd\u00d7k \u2016UV \u2212A\u2016pp.\n(I). S denotes a dense p-stable transform, s = O\u0303(k), \u03b1 = O\u0303(k1\u2212p/2) log d.\n(II). S denotes a sparse p-stable transform, s = O\u0303(k5), \u03b1 = O\u0303(k5\u22125p/2+2/p) log d.\n(III). S> denotes a sampling/rescaling matrix according to the `p Lewis weights of V >, s = O\u0303(k), \u03b1 = O\u0303(k1\u2212p/2).\nWe give the proof for completeness.\nProof. Let S \u2208 Rn\u00d7s be a sketching matrix which satisfies the property (\u2217): \u2200c \u2265 1, U\u0303 \u2208 Rd\u00d7k which satisfy\n\u2016U\u0303V S \u2212AS\u2016pp \u2264 c \u00b7 min U\u2208Rd\u00d7k \u2016UV S \u2212AS\u2016pp,\nwe have\n\u2016U\u0303V \u2212A\u2016pp \u2264 c\u03b2S \u00b7 min U\u2208Rd\u00d7k \u2016UV \u2212A\u2016pp,\nwhere \u03b2S \u2265 1 only depends on the sketching matrix S. Let\n\u2200i \u2208 [d], (U\u0302 i)> = arg min x\u2208Rk \u2016x>V S \u2212AiS\u201622,\ni.e.,\nU\u0302 = AS(V S)\u2020.\nLet\nU\u0303 = arg min U\u2208Rd\u00d7k\n\u2016UV S \u2212AS\u2016pp.\nThen, we have:\n\u2016U\u0302V S \u2212AS\u2016pp\n= d\u2211\ni=1\n\u2016U\u0302 iV S \u2212AiS\u2016pp\n\u2264 d\u2211\ni=1\n(s1/p\u22121/2\u2016U\u0302 iV S \u2212AiS\u20162)p\n\u2264 d\u2211\ni=1\n(s1/p\u22121/2\u2016U\u0303 iV S \u2212AiS\u20162)p\n\u2264 d\u2211\ni=1\n(s1/p\u22121/2\u2016U\u0303 iV S \u2212AiS\u2016p)p\n\u2264 s1\u2212p/2\u2016U\u0303V S \u2212AS\u2016pp.\nThe first inequality follows using \u2200x \u2208 Rs, \u2016x\u2016p \u2264 s1/p\u22121/2\u2016x\u20162 since p < 2. The third inequality follows using \u2200x \u2208 Rs, \u2016x\u20162 \u2264 \u2016x\u2016p since p < 2. Thus, according to the property (\u2217) of S,\n\u2016U\u0302V \u2212A\u2016pp \u2264 s1\u2212p/2\u03b2S min U\u2208Rd\u00d7k \u2016UV \u2212A\u2016pp.\nDue to Lemma E.8 and Lemma E.11 of [SWZ17], we have: for (I), s = O\u0303(k), \u03b2S = O(log d), \u03b1 = s1\u2212p/2\u03b2S = O\u0303(k1\u2212p/2) log d, for (II), s = O\u0303(k5), \u03b2S = O\u0303(k2/p log d), \u03b1 = s1\u2212p/2\u03b2S = O\u0303(k5\u22125p/2+2/p) log d, for (III), s = O\u0303(k), \u03b2S = O(1), \u03b1 = s1\u2212p/2\u03b2S = O\u0303(k1\u2212p/2)."}, {"heading": "E.2 Existence results", "text": "Theorem E.2. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exist three matrices S1 \u2208 Rn2\u00d7s1 , S2 \u2208 Rn2\u00d7s2 , S3 \u2208 Rn2\u00d7s3 such that\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(A1S1X1)i \u2297 (A2S2X2)i \u2297 (A3S3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n\u2264 \u03b1 min rank\u2212k Ak\u2208Rn\u00d7n\u00d7n \u2016Ak \u2212A\u2016pp,\nholds with probability 99/100. (I). Using a dense p-stable transform, s1 = s2 = s3 = O\u0303(k), \u03b1 = O\u0303(k3\u22121.5p) log3 n. (II). Using a sparse p-stable transform, s1 = s2 = s3 = O\u0303(k 5), \u03b1 = O\u0303(k15\u22127.5p+6/p) log3 n.\n(III). Guessing Lewis weights, s1 = s2 = s3 = O\u0303(k), \u03b1 = O\u0303(k3\u22121.5p).\nProof. We use OPT to denote\nOPT := min rank\u2212k Ak\u2208Rn\u00d7n\u00d7n\n\u2016Ak \u2212A\u2016pp.\nGiven a tensor A \u2208 Rn1\u00d7n2\u00d7n3 , we define three matrices A1 \u2208 Rn1\u00d7n2n3 , A2 \u2208 Rn2\u00d7n3n1 , A3 \u2208 Rn3\u00d7n1n2 such that, for any i \u2208 [n1], j \u2208 [n2], l \u2208 [n3]\nAi,j,l = (A1)i,(j\u22121)\u00b7n3+l = (A2)j,(l\u22121)\u00b7n1+i = (A3)l,(i\u22121)\u00b7n2+j .\nWe fix V \u2217 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k, and use V \u22171 , V \u22172 , \u00b7 \u00b7 \u00b7 , V \u2217k to denote the columns of V \u2217 and W \u22171 ,W \u2217 2 , \u00b7 \u00b7 \u00b7 ,W \u2217k to denote the columns of W \u2217.\nWe consider the following optimization problem,\nmin U1,\u00b7\u00b7\u00b7 ,Uk\u2208Rn \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nUi \u2297 V \u2217i \u2297W \u2217i \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n,\nwhich is equivalent to\nmin U1,\u00b7\u00b7\u00b7 ,Uk\u2208Rn \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 [ U1 U2 \u00b7 \u00b7 \u00b7 Uk ]   V \u22171 \u2297W \u22171 V \u22172 \u2297W \u22172 \u00b7 \u00b7 \u00b7\nV \u2217k \u2297W \u2217k\n \u2212A \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 p\np\n.\nWe use matrix Z1 to denote V \u2217> W \u2217> \u2208 Rk\u00d7n2 and matrix U to denote [ U1 U2 \u00b7 \u00b7 \u00b7 Uk ] .\nThen we can obtain the following equivalent objective function,\nmin U\u2208Rn\u00d7k\n\u2016UZ1 \u2212A1\u2016pp.\nChoose a sketching matrix (a dense p-stable, a sparse p-stable or an `p Lewis weight sampling/rescaling matrix to Z1) S1 \u2208 Rn2\u00d7s1 . We can obtain the optimization problem,\nmin U\u2208Rn\u00d7k \u2016UZ1S1 \u2212A1S1\u2016pp = min U\u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016U iZ1S1 \u2212 (A1S1)i\u2016pp,\nwhere U i denotes the i-th row of matrix U \u2208 Rn\u00d7k and (A1S1)i denotes the i-th row of matrix A1S1. Instead of solving it under the `p-norm, we consider the `2-norm relaxation,\nmin U\u2208Rn\u00d7k \u2016UZ1S1 \u2212A1S1\u20162F = min U\u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016U iZ1S1 \u2212 (A1S1)i\u201622.\nLet U\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above optimization problem. Then, U\u0302 = A1S1(Z1S1)\n\u2020. We plug U\u0302 into the objective function under the `p-norm. By choosing s1 and by the properties of sketching matrices (a dense p-stable, a sparse p-stable or an `p Lewis weight sampling/rescaling matrix to Z1) S1 \u2208 Rn2\u00d7s1 , we have\n\u2016U\u0302Z1 \u2212A1\u2016pp \u2264 \u03b1 min U\u2208Rn\u00d7k \u2016UZ1 \u2212A1\u2016pp = \u03b1OPT .\nThis implies\n\u2016U\u0302 \u2297 V \u2217 \u2297W \u2217 \u2212A\u2016pp \u2264 \u03b1OPT .\nAs a second step, we fix U\u0302 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k, and convert tensor A into matrix A2. Let matrix Z2 denote U\u0302> W \u2217>. We consider the following objective function,\nmin V \u2208Rn\u00d7k\n\u2016V Z2 \u2212A2\u2016pp,\nand the optimal cost of it is at most \u03b1OPT. We choose a sketching matrix (a dense p-stable, a sparse p-stable or an `p Lewis weight sampling/rescaling matrix to Z2) S2 \u2208 Rn2\u00d7s2 and sketch on the right of the objective function to obtain the new objective function,\nmin V \u2208Rn\u00d7k \u2016V Z2S2 \u2212A2S2\u2016pp = min V \u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016V iZ2S2 \u2212 (A2S2)i\u2016pp,\nwhere V i denotes the i-th row of matrix V and (A2S2)i denotes the i-th row of matrix A2S2. Instead of solving this under the `p-norm, we consider the `2-norm relaxation,\nmin V \u2208Rn\u00d7k \u2016V Z2S2 \u2212A2S2\u20162F = min V \u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016V i(Z2S2)\u2212 (A2S2)i\u201622.\nLet V\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above problem. Then V\u0302 = A2S2(Z2S2)\u2020. By properties of sketching matrix S2 \u2208 Rn2\u00d7s2 , we have,\n\u2016V\u0302 Z2 \u2212A2\u2016pp \u2264 \u03b1 min V \u2208Rn\u00d7k \u2016V Z2 \u2212A2\u2016pp \u2264 \u03b12 OPT,\nwhich implies\n\u2016U\u0302 \u2297 V\u0302 \u2297W \u2217 \u2212A\u2016pp \u2264 \u03b12 OPT,\nAs a third step, we fix the matrices U\u0302 \u2208 Rn\u00d7k and V\u0302 \u2208 Rn\u00d7k. We can convert tensor A \u2208 Rn\u00d7n\u00d7n into matrix A3 \u2208 Rn2\u00d7n. Let matrix Z3 denote U\u0302> V\u0302 > \u2208 Rk\u00d7n2 . We consider the following objective function,\nmin W\u2208Rn\u00d7k\n\u2016WZ3 \u2212A3\u2016pp,\nand the optimal cost of it is at most \u03b12 OPT. We choose sketching matrix (a dense p-stable, a sparse p-stable or an `p Lewis weight sampling/rescaling matrix to Z3) S3 \u2208 Rn2\u00d7s3 and sketch on the right of the objective function to obtain the new objective function,\nmin W\u2208Rn\u00d7k\n\u2016WZ3S3 \u2212A3S3\u2016pp.\nInstead of solving this under the `p-norm, we consider the `2-norm relaxation,\nmin W\u2208Rn\u00d7k \u2016WZ3S3 \u2212A3S3\u20162F = min W\u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016W i(Z3S3)\u2212 (A3S3)i\u201622.\nLet W\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above problem. Then W\u0302 = A3S3(Z3S3)\u2020. By properties of sketching matrix S3 \u2208 Rn2\u00d7s3 , we have,\n\u2016W\u0302Z3 \u2212A3\u2016pp \u2264 \u03b1 min W\u2208Rn\u00d7k \u2016WZ3 \u2212A3\u2016pp \u2264 \u03b13 OPT .\nThus, we obtain,\nmin X1\u2208Rs1\u00d7k,X2\u2208Rs2\u00d7k,X3\u2208Rs3\u00d7k \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(A1S1X1)i \u2297 (A2S2X2)i \u2297 (A3S3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n\u2264 \u03b13 OPT .\nAccording to Theorem E.1, we let s = s1 = s2 = s3 and take the corresponding \u03b1. We can directly get the results for (I), (II) and (III)."}, {"heading": "E.3 Polynomial in k size reduction", "text": "Definition E.3 (Definition E.1 in [SWZ17]). Given a matrix M \u2208 Rn\u00d7d, if matrix S \u2208 Rm\u00d7n satisfies\n\u2016SM\u2016pp \u2264 \u03b2\u2016M\u2016pp,\nthen S has at most \u03b2 dilation on M in the `p case.\nDefinition E.4 (Definition E.2 in [SWZ17]). Given a matrix U \u2208 Rn\u00d7k, if matrix S \u2208 Rm\u00d7n satisfies\n\u2200x \u2208 Rk, \u2016SUx\u2016pp \u2265 1\n\u03b2 \u2016Ux\u2016pp,\nthen S has at most \u03b2 contraction on U in the `p case.\nTheorem E.5. Given a tensor A \u2208 Rn1\u00d7n2\u00d7n3 and three matrices V1 \u2208 Rn1\u00d7b1 , V2 \u2208 Rn2\u00d7b2 , V3 \u2208 Rn3\u00d7b3 , let X\u22171 \u2208 Rb1\u00d7k, X\u22172 \u2208 Rb2\u00d7k, X\u22173 \u2208 Rb3\u00d7k satisfy\nX\u22171 , X \u2217 2 , X \u2217 3 = arg min X1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016V1X1 \u2297 V2X2 \u2297 V3X3 \u2212A\u2016pp.\nLet S \u2208 Rm\u00d7n have at most \u03b21 \u2265 1 dilation on V1X\u22171 \u00b7 ((V2X\u22172 )> (V3X\u22173 )>) \u2212 A1 and S have at most \u03b22 \u2265 1 contraction on V1 in the `p case. If X\u03021 \u2208 Rb1\u00d7k, X\u03022 \u2208 Rb2\u00d7k, X\u03023 \u2208 Rb3\u00d7k satisfy\n\u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SA\u2016pp \u2264 \u03b2 min X1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016SV1X1 \u2297 V2X2 \u2297 V3X3 \u2212 SA\u2016pp,\nwhere \u03b2 \u2265 1, then\n\u2016V1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212A\u2016pp . \u03b21\u03b22\u03b2 min X1,X2,X3 \u2016V1X1 \u2297 V2X2 \u2297 V3X3 \u2212A\u2016pp.\nThe proof is essentially the same as the proof of Theorem D.7:\nProof. Let A, V1, V2, V3, S,X\u22171 , X\u22172 , X\u22173 , \u03b21, \u03b22 be as stated in the theorem. Let X\u03021 \u2208 Rb1\u00d7k, X\u03022 \u2208 Rb2\u00d7k, X\u03023 \u2208 Rb3\u00d7k satisfy\n\u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SA\u2016pp \u2264 \u03b2 min X1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016SV1X1 \u2297 V2X2 \u2297 V3X3 \u2212 SA\u2016pp.\nSimilar to the proof of Theorem D.7, we have,\n\u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SA\u2016pp = 22\u22122p 1\n\u03b22 \u2016V1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212A\u2016pp \u2212 (21\u2212p\n1\n\u03b22 + \u03b21)\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u2016pp\nThe only difference from the proof of Theorem D.7 is that instead of using triangle inequality, we actually use \u2016x+ y\u2016pp \u2264 2p\u22121\u2016x\u2016pp + \u2016y\u2016pp. Then, we have\n\u2016V1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212A\u2016pp \u2264 22p\u22122\u03b22\u2016SV1X\u03021 \u2297 V2X\u03022 \u2297 V3X\u03023 \u2212 SA\u2016pp + (2p\u22121 + 22p\u22122\u03b21\u03b22)\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u2016pp \u2264 22p\u22122\u03b22\u03b2\u2016SV1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212 SA\u2016pp + (2p\u22121 + 22p\u22122\u03b21\u03b22)\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u2016pp \u2264 22p\u22122\u03b21\u03b22\u03b2\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u2016pp + (2p\u22121 + 22p\u22122\u03b21\u03b22)\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u2016pp \u2264 2p\u22121\u03b2(1 + 2\u03b21\u03b22)\u2016V1X\u22171 \u2297 V2X\u22172 \u2297 V3X\u22173 \u2212A\u2016pp.\nLemma E.6. Let min(b1, b2, b3) \u2265 k. Given three matrices V1 \u2208 Rn\u00d7b1, V2 \u2208 Rn\u00d7b2, and V3 \u2208 Rn\u00d7b3, there exists an algorithm which takes O(nnz(A))+n poly(b1, b2, b3) time and outputs a tensor C \u2208 Rc1\u00d7c2\u00d7c3 and three matrices V\u03021 \u2208 Rc1\u00d7b1, V\u03022 \u2208 Rc2\u00d7b2 and V\u03023 \u2208 Rc3\u00d7b3 with c1 = c2 = c3 = poly(b1, b2, b3), such that with probability 0.99, for any \u03b1 \u2265 1, if X \u20321, X \u20322, X \u20323 satisfy that, \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V\u03021X \u2032 1)i \u2297 (V\u03022X \u20322)i \u2297 (V\u03023X \u20323)i \u2212 C \u2225\u2225\u2225\u2225\u2225 p\np\n\u2264 \u03b1 min X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V\u03021X1)i \u2297 (V\u03022X2)i \u2297 (V\u03023X3)i \u2212 C \u2225\u2225\u2225\u2225\u2225 p\np\n,\nthen, \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X \u2032 1)i \u2297 (V2X \u20322)i \u2297 (V3X \u20323)i \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n. \u03b1 min X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n.\nProof. For simplicity, we define OPT to be\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n.\nLet T1 \u2208 Rc1\u00d7n correspond to sampling according to the `p Lewis weights of V1 \u2208 Rn\u00d7b1 , where c1 = b\u03031. Let T2 \u2208 Rc2\u00d7n be sampling according to the `p Lewis weights of V2 \u2208 Rn\u00d7b2 , where c2 = b\u03032. Let T3 \u2208 Rc3\u00d7n be sampling according to the `p Lewis weights of V3 \u2208 Rn\u00d7b3 , where c3 = b\u03033.\nFor any \u03b1 \u2265 1, let X \u20321 \u2208 Rb1\u00d7k, X \u20322 \u2208 Rb2\u00d7k, X \u20323 \u2208 Rb3\u00d7k satisfy \u2016T1V1X \u20321 \u2297 T2V2X \u20322 \u2297 T3V3X \u20323 \u2212A(T1, T2, T3)\u2016pp\n\u2264 \u03b1 min X1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016T1V1X1 \u2297 T2V2X2 \u2297 T3V3X3 \u2212A(T1, T2, T3)\u2016pp.\nFirst, we regard T1 as the sketching matrix for the remainder. Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have\n\u2016V1X \u20321 \u2297 T2V2X \u20322 \u2297 T3V3X \u20323 \u2212A(I, T2, T3)\u2016pp . \u03b1 min\nX1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016V1X1 \u2297 T2V2X2 \u2297 T3V3X3 \u2212A(I, T2, T3)\u2016pp.\nSecond, we regard T2 as the sketching matrix for V1X1 \u2297 V2X2 \u2297 T3V3X3 \u2212 A(I, I, T3). Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have\n\u2016V1X \u20321 \u2297 V2X \u20322 \u2297 T3V3X \u20323 \u2212A(I, I, T3)\u2016pp . \u03b1 min\nX1\u2208Rb1\u00d7k,X2\u2208Rb2\u00d7k,X3\u2208Rb3\u00d7k \u2016V1X1 \u2297 V2X2 \u2297 T3V3X3 \u2212A(I, I, T3)\u2016pp.\nThird, we regard T3 as the sketching matrix for V1X1 \u2297 V2X2 \u2297 V3X3 \u2212 A. Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X \u2032 1)i \u2297 (V2X \u20322)i \u2297 (V3X \u20323)i \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n. \u03b1 min X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(V1X1)i \u2297 (V2X2)i \u2297 (V3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n."}, {"heading": "E.4 Solving small problems", "text": "Combining Section B.5 in [SWZ17] and the proof of Theorem D.4, for any p = a/b with a, b are integers, we can obtain the `p version of Theorem D.4."}, {"heading": "E.5 Bicriteria algorithm", "text": "We present several bicriteria algorithms with different tradeoffs. We first present an algorithm that runs in nearly linear time and outputs a solution with rank O\u0303(k3) in Theorem E.7. Then we show an algorithm that runs in nnz(A) time but outputs a solution with rank poly(k) in Theorem E.8. Then we explain an idea which is able to decrease the cubic rank to quadratic, and thus we can obtain Theorem E.9.\nTheorem E.7. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, let r = O\u0303(k3). There exists an algorithm which takes nnz(A) \u00b7 O\u0303(k) +n poly(k) + poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n\u2264 O\u0303(k3\u2212p/2) log3 n min rank\u2212k Ak \u2016Ak \u2212A\u2016pp\nholds with probability 9/10.\nProof. We first choose three dense Cauchy transforms Si \u2208 Rn2\u00d7si . According to Section B.7, for each i \u2208 [3], AiSi can be computed in nnz(A) \u00b7 O\u0303(k) time. Then we apply Lemma E.6. We obtain three matrices Y1 = T1A1S1, Y2 = T2A2S2, Y3 = T3A3S3 and a tensor C = A(T1, T2, T3). Note that for each i \u2208 [3], Yi can be computed in n poly(k) time. Because C = A(T1, T2, T3) and T1, T2, T3 \u2208 Rn\u00d7O\u0303(k) are three sampling and rescaling matrices, C can be computed in nnz(A)+O\u0303(k3) time. At the end, we just need to run an `p-regression solver to find the solution for the problem:\nmin X\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\nXi,j,l(Y1)i \u2297 (Y2)j \u2297 (Y3)j \u2225\u2225\u2225\u2225\u2225\u2225 p\np\n,\nwhere (Y1)i denotes the i-th column of matrix Y1. Since the size of the above problem is only poly(k), this can be solved in poly(k) time.\nTheorem E.8. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, let r = O\u0303(k15). There exists an algorithm that takes nnz(A)+n poly(k)+poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n\u2264 poly(k, log n) min rank\u2212k Ak \u2016Ak \u2212A\u2016pp\nholds with probability 9/10.\nProof. We first choose three sparse p-stable transforms Si \u2208 Rn2\u00d7si . According to Section B.7, for each i \u2208 [3], AiSi can be computed in O(nnz(A)) time. Then we apply Lemma E.6, and can obtain three matrices Y1 = T1A1S1, Y2 = T2A2S2, Y3 = T3A3S3 and a tensor C = A(T1, T2, T3). Note that for each i \u2208 [3], Yi can be computed in n poly(k) time. Because C = A(T1, T2, T3) and T1, T2, T3 \u2208 Rn\u00d7O\u0303(k) are three sampling and rescaling matrices, C can be computed in nnz(A)+O\u0303(k3) time. At the end, we just need to run an `p-regression solver to find the solution to the problem,\nmin X\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\nXi,j,l(Y1)i \u2297 (Y2)j \u2297 (Y3)l \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 p\np\n,\nwhere (Y1)i denotes the i-th column of matrix Y1. Since the size of the above problem is only poly(k), it can be solved in poly(k) time.\nTheorem E.9. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), let r = O\u0303(k2). There exists an algorithm which takes nnz(A) \u00b7 O\u0303(k) + n poly(k) + poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n\u2264 O\u0303(k3\u22121.5p) log3 n min rank\u2212k Ak \u2016Ak \u2212A\u2016pp\nholds with probability 9/10.\nProof. The proof is similar to Theorem D.14.\nAlgorithm 31 `p-Low Rank Approximation, Bicriteria Algorithm, rank-O\u0303(k2), Input Sparsity Time 1: procedure LpBicriteriaAlgorithm(A,n, k) . Corollary E.10 2: s1 \u2190 s2 \u2190 s3 \u2190 O\u0303(k). 3: For each i \u2208 [3], choose Si \u2208 Rn2\u00d7si to be the composition of a sparse p-stable transform\nand a dense p-stable transform. . Part (I,II) of Theorem E.2 4: Compute A1 \u00b7 S1, A2 \u00b7 S2. 5: For each i \u2208 [2], choose Ti to be a sampling and rescaling diagonal matrix according to the\nLewis weights of AiSi, with ti = O\u0303(k) nonzero entries. 6: C \u2190 A(T1, T2, I). 7: Bi+(j\u22121)s1 \u2190 vec((T1A1S1)i \u2297 (T2A2S2)j),\u2200i \u2208 [s1], j \u2208 [s2]. 8: Form objective function minW \u2016WB \u2212 C3\u20161. 9: Run `p-regression solver to find W\u0302 .\n10: Construct U\u0302 by copying (A1S1)i to the (i, j)-th column of U\u0302 . 11: Construct V\u0302 by copying (A2S2)j to the (i, j)-th column of V\u0302 . 12: return U\u0302 , V\u0302 , W\u0302 . 13: end procedure\nAs for `1, notice that if we first apply a sparse Cauchy transform, we can reduce the rank of the matrix to poly(k). Theyn we can apply a dense Cauchy transform and further reduce the dimension, while only incurring another poly(k) factor in the approximation ratio. By combining sparse p-stable and dense p-stable transforms, we can improve the running time from nnz(A) \u00b7 O\u0303(k) to be nnz(A) by losing some additional poly(k) factors in the approximation ratio.\nCorollary E.10. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), let r = O\u0303(k2). There exists an algorithm which takes nnz(A) +n poly(k) + poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2225\u2225\u2225\u2225\u2225 r\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n\u2264 poly(k, log n) min rank\u2212k Ak \u2016Ak \u2212A\u2016pp\nholds with probability 9/10."}, {"heading": "E.6 Algorithms", "text": "In this section, we show two different algorithms by using different kind of sketches. One is shown in Theorem E.11 which gives a fast running time. Another one is shown in Theorem E.12 which gives the best approximation ratio.\nTheorem E.11. Given a 3rd tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm which takes O(nnz(A)) + n poly(k) + 2O\u0303(k2) time and outputs three matrices U, V,W \u2208 Rn\u00d7k such that,\n\u2016U \u2297 V \u2297W \u2212A\u2016pp \u2264 poly(k, log n) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u2016pp.\nholds with probability at least 9/10.\nProof. First, we apply part (II) of Theorem E.2. Then AiSi can be computed in O(nnz(A)) time. Second, we use Lemma E.6 to reduce the size of the objective function from O(n3) to poly(k) in n poly(k) time by only losing a constant factor in approximation ratio. Third, we use Claim B.15 to relax the objective function from entry-wise `p-norm to Frobenius norm, and this step causes us to lose some other poly(k) factors in approximation ratio. As a last step, we use Theorem C.45 to solve the Frobenius norm objective function.\nTheorem E.12. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm that takes nO\u0303(k)2O\u0303(k3) time and output three matrices U, V,W \u2208 Rn\u00d7k such that,\n\u2016U \u2297 V \u2297W \u2212A\u2016pp \u2264 O\u0303(k3\u22121.5p) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u2016pp.\nholds with probability at least 9/10.\nProof. First, we apply part (III) of Theorem E.2. Then, guessing Si requires nO\u0303(k) time. Second, we use Lemma E.6 to reduce the size of the objective from O(n3) to poly(k) in polynomial time while only losing a constant factor in approximation ratio. Third, we solve the small optimization problem."}, {"heading": "E.7 CURT decomposition", "text": "Theorem E.13. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, let k \u2265 1, and let UB, VB,WB \u2208 Rn\u00d7k denote a rank-k, \u03b1-approximation to A. Then there exists an algorithm which takes O(nnz(A)) + O(n2) poly(k) time and outputs three matrices C \u2208 Rn\u00d7c with columns from A, R \u2208 Rn\u00d7r with rows from A, T \u2208 Rn\u00d7t with tubes from A, and a tensor U \u2208 Rc\u00d7r\u00d7t with rank(U) = k such that c = r = t = O(k log k log log k), and\n\u2225\u2225\u2225\u2225\u2225\u2225 c\u2211\ni=1\nr\u2211\nj=1\nt\u2211\nl=1\nUi,j,l \u00b7 Ci \u2297Rj \u2297 Tl \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 p\np\n\u2264 O\u0303(k3\u22121.5p)\u03b1 min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u2016pp\nholds with probability 9/10.\nProof. We define\nOPT := min rank\u2212k A\u2032\n\u2016A\u2032 \u2212A\u2016pp.\nWe already have three matrices UB \u2208 Rn\u00d7k, VB \u2208 Rn\u00d7k and WB \u2208 Rn\u00d7k and these three matrices provide a rank-k, \u03b1 approximation to A, i.e.,\n\u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(UB)i \u2297 (VB)i \u2297 (WB)i \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n\u2264 \u03b1OPT . (46)\nLet B1 = V >B W>B \u2208 Rk\u00d7n 2 denote the matrix where the i-th row is the vectorization of (VB)i \u2297 (WB)i. By Section B.3 in [SWZ17], we can compute D1 \u2208 Rn 2\u00d7n2 which is a sampling and rescaling matrix corresponding to the Lewis weights of B>1 in O(n2 poly(k)) time, and there are d1 = O(k log k log log k) nonzero entries on the diagonal of D1. Let Ai \u2208 Rn\u00d7n2 denote the matrix obtained by flattening A along the i-th direction, for each i \u2208 [3].\nDefine U\u2217 \u2208 Rn\u00d7k to be the optimal solution to min U\u2208Rn\u00d7k \u2016UB1\u2212A1\u2016pp, U\u0302 = A1D1(B1D1)\u2020 \u2208 Rn\u00d7k,\nV0 \u2208 Rn\u00d7k to be the optimal solution to min V \u2208Rn\u00d7k \u2016V \u00b7 (U\u0302> W>B )\u2212A2\u2016 p p, and U \u2032 to be the optimal solution to min U\u2208Rn\u00d7k\n\u2016UB1D1 \u2212A1D1\u2016pp. By Claim B.13, we have\n\u2016U\u0302B1D1 \u2212A1D1\u2016pp \u2264 d1\u2212p/21 \u2016U \u2032B1D1 \u2212A1D1\u2016pp.\nDue to Lemma E.11 and Lemma E.8 in [SWZ17], with constant probability, we have\n\u2016U\u0302B1 \u2212A1\u2016pp \u2264 d1\u2212p/21 \u03b1D1\u2016U\u2217B1 \u2212A1\u2016pp, (47)\nwhere \u03b1D1 = O(1). Recall that (U\u0302> W>B ) \u2208 Rk\u00d7n\n2 denotes the matrix where the i-th row is the vectorization of U\u0302i \u2297 (WB)i, \u2200i \u2208 [k]. Now, we can show,\n\u2016V0 \u00b7 (U\u0302> W>B )\u2212A2\u2016pp \u2264 \u2016U\u0302B1 \u2212A1\u2016pp by V0 = arg min V \u2208Rn\u00d7k \u2016V \u00b7 (U\u0302> W>B )\u2212A2\u2016pp\n. d1\u2212p/21 \u2016U\u2217B1 \u2212A1\u2016pp by Equation (47) \u2264 d1\u2212p/21 \u2016UBB1 \u2212A1\u2016pp by U\u2217 = arg min\nU\u2208Rn\u00d7k \u2016UB1 \u2212A1\u2016pp\n\u2264 O(d1\u2212p/21 )\u03b1OPT . by Equation (46) (48)\nWe define B2 = U\u0302> W>B . We can compute D2 \u2208 Rn 2\u00d7n2 which is a sampling and rescaling matrix corresponding to the `p Lewis weights of B>2 in O(n2 poly(k)) time, and there are d2 = O(k log k log log k) nonzero entries on the diagonal of D2.\nDefine V \u2217 \u2208 Rn\u00d7k to be the optimal solution of minV \u2208Rn\u00d7k \u2016V B2 \u2212A2\u2016pp, V\u0302 = A2D2(B2D2)\u2020 \u2208 Rn\u00d7k, W0 \u2208 Rn\u00d7k to be the optimal solution of min\nW\u2208Rn\u00d7k \u2016W \u00b7 (U\u0302> V\u0302 >)\u2212 A3\u2016pp, and V \u2032 to be the\noptimal solution of min V \u2208Rn\u00d7k \u2016V B2D2 \u2212A2D2\u2016pp. By Claim B.13, we have\n\u2016V\u0302 B2D2 \u2212A2D2\u2016pp \u2264 d1\u2212p/22 \u2016V \u2032B2D2 \u2212A2D2\u2016pp.\nDue to Lemma E.11 and Lemma E.8 in [SWZ17], with constant probability, we have\n\u2016V\u0302 B2 \u2212A2\u2016pp \u2264 d1\u2212p/22 \u03b1D2\u2016V \u2217B2 \u2212A2\u2016pp, (49)\nwhere \u03b1D2 = O(1). Recall that (U\u0302> V\u0302 >) \u2208 Rk\u00d7n2 denotes the matrix for which the i-th row is the vectorization of U\u0302i \u2297 V\u0302i, \u2200i \u2208 [k]. Now, we can show,\n\u2016W0 \u00b7 (U\u0302> V\u0302 >)\u2212A3\u2016pp \u2264 \u2016V\u0302 B2 \u2212A2\u2016pp by W0 = arg min\nW\u2208Rn\u00d7k \u2016W \u00b7 (U\u0302> V\u0302 >)\u2212A3\u2016pp\n. d1\u2212p/22 \u2016V \u2217B2 \u2212A2\u2016pp by Equation (49) \u2264 d1\u2212p/22 \u2016V0B2 \u2212A2\u2016pp by V \u2217 = arg min\nV \u2208Rn\u00d7k \u2016V B2 \u2212A2\u2016pp\n\u2264 O((d1d2)1\u2212p/2)\u03b1OPT . by Equation (48) (50)\nWe define B3 = U\u0302> V\u0302 >. We can compute D3 \u2208 Rn2\u00d7n2 which is a sampling and rescaling matrix corresponding to the `p Lewis weights of B>3 in O(n2 poly(k)) time, and there are d3 = O(k log k log log k) nonzero entries on the diagonal of D3.\nDefineW \u2217 \u2208 Rn\u00d7k to be the optimal solution to minW\u2208Rn\u00d7k \u2016WB3\u2212A3\u2016pp, W\u0302 = A3D3(B3D3)\u2020 \u2208 Rn\u00d7k, and W \u2032 to be the optimal solution to min\nW\u2208Rn\u00d7k \u2016WB3D3 \u2212A3D3\u2016pp.\nBy Claim B.13, we have\n\u2016W\u0302B3D3 \u2212A3D3\u2016pp \u2264 d1\u2212p/23 \u2016W \u2032B3D3 \u2212A3D3\u2016pp.\nDue to Lemma E.11 and Lemma E.8 in [SWZ17], with constant probability, we have\n\u2016W\u0302B3 \u2212A3\u2016pp \u2264 d1\u2212p/23 \u03b1D3\u2016W \u2217B3 \u2212A3\u2016pp, (51)\nwhere \u03b1D3 = O(1). Now we can show,\n\u2016W\u0302B3 \u2212A3\u2016pp . d1\u2212p/23 \u2016W \u2217B3 \u2212A3\u2016pp, by Equation (51) \u2264 d1\u2212p/23 \u2016W0B3 \u2212A3\u2016pp, by W \u2217 = arg min\nW\u2208Rn\u00d7k \u2016WB3 \u2212A3\u2016pp\n\u2264 O((d1d2d3)1\u2212p/2)\u03b1OPT . by Equation (50)\nThus, it implies, \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nU\u0302i \u2297 V\u0302i \u2297 W\u0302i \u2212A \u2225\u2225\u2225\u2225\u2225 p\np\n\u2264 poly(k, log n) OPT .\nwhere U\u0302 = A1D1(B1D1)\u2020, V\u0302 = A2D2(B2D2)\u2020, W\u0302 = A3D3(B3D3)\u2020."}, {"heading": "F Robust Subspace Approximation (Asymmetric Norms for Arbitrary Tensors)", "text": "Recently, [CW15b] and [CW15a] study the linear regression problem and low-rank approximation problem under M-Estimator loss functions. In this section, we extend the matrix version of the low rank approximation problem to tensors, i.e., in particular focusing on tensor low-rank approximation under M-Estimator norms. Note that M-Estimators are very different from Frobenius norm and Entry-wise `1 norm, which are symmetric norms. Namely, flattening the tensor objective function along any of the dimensions does not change the cost if the norm is Frobenius or Entry-wise `1- norm. However, for M-Estimator norms, we cannot flatten the tensor along all three dimensions. This property makes the tensor low-rank approximation problem under M-Estimator norms more difficult. This section can be split into two independent parts. Section F.2 studies the `1-`2-`2 norm setting, and Section F.3 studies the `1-`1-`2 norm setting."}, {"heading": "F.1 Preliminaries", "text": "Definition F.1 (Nice functions for M -Estimators,M2, Lp, [CW15a]). We say an M -Estimator is nice if M(x) = M(\u2212x), M(0) = 0, M is non-decreasing in |x|, there is a constant CM > 0 and a constant p \u2265 1 so that for all a, b \u2208 R>0 with a \u2265 b, we have\nCm |a| |b| \u2264 M(a) M(b) \u2264 (a b )p,\nand also that M(x) 1 p is subadditive, that is, M(x+ y) 1 p \u2264M(x) 1 p +M(y) 1 p .\nLetM2 denote the set of such nice M -estimators, for p = 2. Let Lp denote M -Estimators with M(x) = |x|p and p \u2208 [1, 2).\nF.2 `1-Frobenius (a.k.a `1-`2-`2) norm\nSection F.2.1 presents basic definitions and facts for the `1-`2-`2 norm setting. Section F.2.2 introduces some useful tools. Section F.2.3 presents the \u201cno dilation\u201d and \u201cno contraction\u201d bounds, which are the key ideas for reducing the problem to a \u201cgeneralized\u201d Frobenius norm low rank approximation problem. Finally, we provide our algorithms in Section F.2.6."}, {"heading": "F.2.1 Definitions", "text": "We first give the definition for the v-norm of a tensor, and then give the definition of the v-norm for a matrix and a weighted version of the v-norm for a matrix.\nDefinition F.2 (Tensor v-norm). For an n\u00d7 n\u00d7 n tensor A, we define the v-norm of A, denoted \u2016A\u2016v, to be\n( n\u2211\ni=1\nM(\u2016Ai,\u2217,\u2217\u2016F ) )1/p ,\nwhere Ai,\u2217,\u2217 is the i-th face of A (along the 1st direction), and p is a parameter associated with the function M(), which defines a nice M -Estimator.\nDefinition F.3 (Matrix v-norm). For an n\u00d7d matrix A, we define the v-norm of A, denoted \u2016A\u2016v, to be\nn\u2211\ni=1\nM(\u2016Ai,\u2217\u20162)1/p,\nwhere Ai,\u2217 is the i-th row of A, and p is a parameter associated with the function M(), which defines a nice M -Estimator.\nDefinition F.4. Given matrix A \u2208 Rn\u00d7d, let Ai,\u2217 denote the i-th row of A. Let TS \u2282 [n] denote the indices i such that ei is chosen for S. Using a probability vector q and a sampling and rescaling matrix S \u2208 Rn\u00d7n from q, we will estimate \u2016A\u2016v using S and a re-weighted version, \u2016S \u00b7 \u2016v,w\u2032 of \u2016 \u00b7 \u2016v, with\n\u2016SA\u2016v,w\u2032 =\n \u2211\ni\u2208TS\nw\u2032iM(\u2016Ai,\u2217\u20162)\n  1/p\n,\nwhere w\u2032i = wi/qi. Since w \u2032 is generally understood, we will usually just write \u2016SA\u2016v. We will also need an \u201centrywise row-weighted\u201d version :\n|||SA||| =\n \u2211\ni\u2208TS\nwi qi \u2016Ai,\u2217\u2016pM\n  1/p\n=\n  \u2211\ni\u2208TS ,j\u2208[d]\nwi qi M(Ai,j)\n  1/p\n,\nwhere Ai,j denotes the entry in the i-th row and j-th column of A.\nFact F.5. For p = 1, for any two matrices A and B, we have \u2016A+B\u2016v \u2264 \u2016A\u2016v + \u2016B\u2016v. For any two tensors A and B, we have \u2016A+B\u2016v \u2264 \u2016A\u2016v + \u2016B\u2016v.\nF.2.2 Sampling and rescaling sketches\nNote that Lemmas 42 and 44 in [CW15a] are stronger than stated. In particular, we do not need to assume X is a square matrix. For any m \u2265 z, if X \u2208 Rd\u00d7m, then we have the same result.\nLemma F.6 (Lemma 42 in [CW15a]). Let \u03c1 > 0 and integer z > 0. For sampling matrix S, suppose for a given y \u2208 Rd with failure probability \u03b4 it holds that \u2016SAy\u2016M = (1 \u00b1 1/10)\u2016Ay\u2016M . There is K1 = O(z2/CM ) so that with failure probability \u03b4(KN /CM )(1+p)d, for a constant KN , any rank-z matrix X \u2208 Rd\u00d7m has the property that if \u2016AX\u2016v \u2265 K1\u03c1, then \u2016SAX\u2016v \u2265 \u03c1, and that if \u2016AX\u2016v \u2264 \u03c1/K1, then \u2016SAX\u2016v \u2264 \u03c1.\nLemma F.7 (Lemma 44 in [CW15a]). Let \u03b4, \u03c1 > 0 and integer z > 0. Given matrix A \u2208 Rn\u00d7d, there exists a sampling and rescaling matrix S \u2208 Rn\u00d7n with r = O(\u03b3(A,M,w) \u22122dz2 log(z/ ) log(1/\u03b4)) nonzero entries such that, with probability at least 1\u2212 \u03b4, for any rank-z matrix X \u2208 Rd\u00d7m, we have either\n\u2016SAX\u2016v \u2265 \u03c1,\nor\n(1\u2212 )\u2016AX\u2016v \u2212 \u03c1 \u2264 \u2016SAX\u2016v \u2264 (1 + )\u2016AX\u2016v + \u03c1.\nLemma F.8 (Lemma 43 in [CW15a]). For r > 0, let r\u0302 = r/\u03b3(A,M,w), and let q \u2208 Rn have\nqi = min{1, r\u0302\u03b3i(A,M,w)}.\nLet S be a sampling and rescaling matrix generated using q, with weights as usual w\u2032i = wi/qi. Let W \u2208 Rd\u00d7z, and \u03b4 > 0. There is an absolute constant C so that for r\u0302 \u2265 Cz log(1/\u03b4)/ 2, with probability at least 1\u2212 \u03b4, we have\n(1\u2212 )\u2016AW\u2016v,w \u2264 \u2016SAW\u2016v,w\u2032 \u2264 (1 + )\u2016AW\u2016v,w."}, {"heading": "F.2.3 No dilation and no contraction", "text": "Lemma F.9. Given matrices A \u2208 Rn\u00d7m, U \u2208 Rn\u00d7d, let V \u2217 = arg min rank\u2212k V \u2208Rd\u00d7m \u2016UV \u2212 A\u2016v. If S \u2208 Rs\u00d7n has at most c1-dilation on UV \u2217 \u2212A, i.e.,\n\u2016S(UV \u2217 \u2212A)\u2016v \u2264 c1\u2016UV \u2217 \u2212A\u2016v,\nand it has at most c2-contraction on U , i.e.,\n\u2200x \u2208 Rd, \u2016SUx\u2016v \u2265 1\nc2 \u2016Ux\u2016v,\nthen S has at most (c2, c1 + 1c2 )-contraction on (U,A), i.e.,\n\u2200 rank\u2212k V \u2208 Rd\u00d7m, \u2016SUV \u2212 SA\u2016v \u2265 1\nc2 \u2016UV \u2212A\u2016v \u2212 (c1 +\n1\nc2 )\u2016UV \u2217 \u2212A\u2016v.\nProof. Let A \u2208 Rn\u00d7m, U \u2208 Rn\u00d7d and S \u2208 Rs\u00d7n be the same as that described in the lemma. Let (V \u2212 V \u2217)j denote the j-th column of V \u2212 V \u2217. Then \u2200 rank\u2212k V \u2208 Rd\u00d7m,\n\u2016SUV \u2212 SA\u2016v \u2265 \u2016SUV \u2212 SUV \u2217\u2016v \u2212 \u2016SUV \u2217 \u2212 SA\u2016v \u2265 \u2016SUV \u2212 SUV \u2217\u2016v \u2212 c1\u2016UV \u2217 \u2212A\u2016v = \u2016SU(V \u2212 V \u2217)\u2016v \u2212 c1\u2016UV \u2217 \u2212A\u2016v\n=\nm\u2211\nj=1\n\u2016SU(V \u2212 V \u2217)j\u2016v \u2212 c1\u2016UV \u2217 \u2212A\u2016v\n\u2265 m\u2211\nj=1\n1 c2 \u2016U(V \u2212 V \u2217)j\u2016v \u2212 c1\u2016UV \u2217 \u2212A\u2016v\n= 1\nc2 \u2016UV \u2212 UV \u2217\u2016v \u2212 c1\u2016UV \u2217 \u2212A\u2016v\n\u2265 1 c2 \u2016UV \u2212A\u2016v \u2212 1 c2 \u2016UV \u2217 \u2212A\u2016v \u2212 c1\u2016UV \u2217 \u2212A\u2016v = 1\nc2 \u2016UV \u2212A\u2016v \u2212\n( ( 1\nc2 + c2)\u2016UV \u2217 \u2212A\u2016v\n) ,\nwhere the first inequality follows by the triangle inequality, the second inequality follows since S has at most c1 dilation on UV \u2217\u2212A, the third inequality follows since S has at most c2 contraction on U , and the fourth inequality follows by the triangle inequality.\nClaim F.10. Given matrix A \u2208 Rn\u00d7m, for any distribution p = (p1, p2, \u00b7 \u00b7 \u00b7 , pn) define random variable X such that X = \u2016Ai\u20162/pi with probability pi where Ai is the i-th row of matrix A. Then take m independent samples X1, X2, \u00b7 \u00b7 \u00b7 , Xm, and let Y = 1m \u2211m j=1X j. We have\nPr[Y \u2264 1000\u2016A\u2016v] \u2265 .999.\nProof. We can compute the expectation of Xj , for any j \u2208 [m],\nE[Xj ] = n\u2211\ni=1\n\u2016Ai\u20162 pi \u00b7 pi = \u2016A\u2016v.\nThen E[Y ] = 1m \u2211m j=1 E[X j ] = \u2016A\u2016v. Using Markov\u2019s inequality, we have\nPr[Y \u2265 \u2016A\u2016v] \u2264 .001.\nLemma F.11. For any fixed U\u2217 \u2208 Rn\u00d7d and rank-k V \u2217 \u2208 Rd\u00d7m with d = poly(k), there exists an algorithm that takes poly(n, d) time to compute a sampling and rescaling diagonal matrix S \u2208 Rn\u00d7n with s = poly(k) nonzero entries such that, with probability at least .999, we have: for all rank-k V \u2208 Rd\u00d7m,\n\u2016U\u2217V \u2217 \u2212 U\u2217V \u2016v . \u2016SU\u2217V \u2217 \u2212 SU\u2217V \u2016v . \u2016U\u2217V \u2217 \u2212 U\u2217V \u2016v.\nLemma F.12 (No dilation). Given matrices A \u2208 Rn\u00d7m, U\u2217 \u2208 Rn\u00d7d with d = poly(k), define V \u2217 \u2208 Rd\u00d7m to be the optimal solution min\nrank\u2212k V \u2208Rd\u00d7m \u2016U\u2217V \u2212A\u2016v. Choose a sampling and rescaling\ndiagonal matrix S \u2208 Rn\u00d7n with s = poly(k) according to Lemma F.8. Then with probability at least .99, we have: for all rank-k V \u2208 Rd\u00d7m,\n\u2016SU\u2217V \u2212 SA\u2016v . \u2016U\u2217V \u2217 \u2212 U\u2217V \u2016v +O(1)\u2016U\u2217V \u2217 \u2212A\u2016v . \u2016U\u2217V \u2212A\u2016v.\nProof. Using Claim F.10 and Lemma F.11, we have with probability at least .99, for all rank-k V \u2208 Rd\u00d7m,\n\u2016SU\u2217V \u2212 SA\u2016v \u2264 \u2016SU\u2217V \u2212 SU\u2217V \u2217\u2016v + \u2016SU\u2217V \u2217 \u2212 SA\u2016v by triangle inequality . \u2016SU\u2217V \u2212 SU\u2217V \u2217\u2016v +O(1)\u2016U\u2217V \u2217 \u2212A\u2016v by Claim F.10 . \u2016U\u2217V \u2212 U\u2217V \u2217\u2016v +O(1)\u2016U\u2217V \u2217 \u2212A\u2016v by Lemma F.11 . \u2016U\u2217V \u2212A\u2016v + \u2016U\u2217V \u2217 \u2212A\u2016v +O(1)\u2016U\u2217V \u2217 \u2212A\u2016v by triangle inequality . \u2016U\u2217V \u2212A\u2016v.\nLemma F.13 (No contraction). Given matrices A \u2208 Rn\u00d7m, U\u2217 \u2208 Rn\u00d7d with d = poly(k), define V \u2217 \u2208 Rd\u00d7m to be the optimal solution min\nrank\u2212k V \u2208Rd\u00d7m \u2016U\u2217V \u2212A\u2016v. Choose a sampling and rescaling\ndiagonal matrix S \u2208 Rn\u00d7n with s = poly(k) according to Lemma F.8. Then with probability at least .99, we have: for all rank-k V \u2208 Rd\u00d7m,\n\u2016U\u2217V \u2212A\u2016v . \u2016SU\u2217V \u2212 SA\u2016v +O(1)\u2016U\u2217V \u2217 \u2212A\u2016v.\nProof. This follows by Lemma F.9, Claim F.10 and Lemma F.12.\nF.2.4 Oblivious sketches, MSketch\nIn this section, we recall a concept calledM -sketches forM -estimators which is defined in [CW15b]. M -sketch is an oblivious sketch for matrices.\nTheorem F.14 (Theorem 3.1 in [CW15b]). Let OPT denote minx\u2208Rd \u2016Ax \u2212 b\u2016G. There is an algorithm that in O(nnz(A))+poly(d log n) time, with constant probability finds x\u2032 such that \u2016Ax\u2032\u2212 b\u2016G \u2264 O(1) OPT.\nDefinition F.15 (M-Estimator sketches or MSketch [CW15b]). Given parameters N,n,m, b > 1, define hmax = blogb(n/m)c, \u03b2 = (b \u2212 b\u2212hmax)/(b \u2212 1) and s = Nhmax. For each p \u2208 [n], \u03c3p, gp, hp are generated (independently) in the following way,\n\u03c3p \u2190 \u00b11, chosen with equal probability, gp \u2208 [N ], chosen with equal probability, hp \u2190 t, chosen with probability 1/(\u03b2bt) for t \u2208 {0, 1, \u00b7 \u00b7 \u00b7hmax}.\nFor each p \u2208 [n], we define jp = gp +Nhp. Let w \u2208 Rs denote the scaling vector such that, for each j \u2208 [s],\nwj = { \u03b2bhp , if there exists p \u2208 [n] s.t.j = jp, 0 otherwise.\nLet S \u2208 RNhmax\u00d7n be such that, for each j \u2208 [s],for each p \u2208 [n],\nSj,p = { \u03c3p, if j = gp +N \u00b7 hp, 0, otherwise.\nLet Dw denote the diagonal matrix where the i-th entry on the diagonal is the i-th entry of w. Let S = DwS. We say (S,w) or S is an MSketch.\nDefinition F.16 (Tensor \u2016\u2016v,w-norm). For a tensor A \u2208 Rd\u00d7n1\u00d7n2 and a vector w \u2208 , we define\n\u2016A\u2016v,w = d\u2211\ni=1\nwi\u2016Ai,\u2217,\u2217\u2016F .\nLet (S,w) denote an MSketch, and let S = DwS. If v corresponds to a scale-invariant MEstimator, then for any three matrices U, V,W , we have the following,\n\u2016(SU)\u2297 V \u2297W\u2016v,w = \u2016(DwSU)\u2297 V \u2297W\u2016v = \u2016(SU)\u2297 V \u2297W\u2016v.\nFact F.17. For a tensor A \u2208 Rn\u00d7n\u00d7n, let S \u2208 Rs\u00d7n denote an MSketch (defined in F.15) with s = poly(k, log n). Then SA can be computed in O(nnz(A)) time.\nLemma F.18. For any fixed U\u2217 \u2208 Rn\u00d7d and rank-k V \u2217 \u2208 Rd\u00d7m with d = poly(k), let S \u2208 Rs\u00d7n denote an MSketch (defined in Definition F.15) with s = poly(k, log n) rows. Then with probability at least .999, we have: for all rank-k V \u2208 Rd\u00d7m,\n\u2016U\u2217V \u2217 \u2212 U\u2217V \u2016v . \u2016SU\u2217V \u2217 \u2212 SU\u2217V \u2016v . \u2016U\u2217V \u2217 \u2212 U\u2217V \u2016v.\nLemma F.19 (No dilation, Theorem 3.4 in [CW15b]). Given matrices A \u2208 Rn\u00d7m, U\u2217 \u2208 Rn\u00d7d with d = poly(k), define V \u2217 \u2208 Rd\u00d7m to be the optimal solution to min\nrank\u2212k V \u2208Rd\u00d7m \u2016U\u2217V \u2212 A\u2016v. Choose\nan MSketch S \u2208 Rs\u00d7n with s = poly(k, log n) according to Definition F.15. Then with probability at least .99, we have: for all rank-k V \u2208 Rd\u00d7m,\n\u2016SU\u2217V \u2212 SA\u2016v . \u2016U\u2217V \u2217 \u2212 U\u2217V \u2016v +O(1)\u2016U\u2217V \u2217 \u2212A\u2016v . \u2016U\u2217V \u2212A\u2016v.\nLemma F.20 (No contraction). Given matrices A \u2208 Rn\u00d7m, U\u2217 \u2208 Rn\u00d7d with d = poly(k), define V \u2217 \u2208 Rd\u00d7m to be the optimal solution to min\nrank\u2212k V \u2208Rd\u00d7m \u2016U\u2217V \u2212A\u2016v. Choose an MSketch S \u2208 Rs\u00d7n\nwith s = poly(k, log n) according to Definition F.15. Then with probability at least .99, we have: for all rank-k V \u2208 Rd\u00d7m,\n\u2016U\u2217V \u2212A\u2016v . \u2016SU\u2217V \u2212 SA\u2016v +O(1)\u2016U\u2217V \u2217 \u2212A\u2016v.\nF.2.5 Running time analysis\nLemma F.21. Given a tensor A \u2208 Rn\u00d7d\u00d7d, let S \u2208 Rs\u00d7n denote an MSketch with s rows. Let SA denote a tensor that has size s\u00d7 d\u00d7 d. For each i \u2208 {2, 3}, let (SA)i \u2208 Rd\u00d7ds denote a matrix obtained by flattening tensor SA along the i-th dimension. For each i \u2208 {2, 3}, let Si \u2208 Rds\u00d7si denote a CountSketch transform with si columns. For each i \u2208 {2, 3}, let Ti \u2208 Rti\u00d7d denote a CountSketch transform with ti rows. Then (I) For each i \u2208 {2, 3}, (SA)iSi can be computed in O(nnz(A)) time. (II) For each i \u2208 {2, 3}, Ti(SA)iSi can be computed in O(nnz(A)) time.\nProof. Proof of Part (I). First note that (SA)2S2 has size n\u00d7S2. Thus for each i \u2208 [d], j \u2208 [s2], we have,\n((SA)2S2)i,j = ds\u2211\nx\u2032=1\n((SA)2)i,x\u2032(S2)x\u2032,j by (SA)2 \u2208 Rd\u00d7ds, S2 \u2208 Rds\u00d7s2\n=\nd\u2211\ny=1\ns\u2211\nz=1\n((SA)2)i,(y\u22121)s+z(S2)(y\u22121)s+z,j\n= d\u2211\ny=1\ns\u2211\nz=1\n(SA)z,i,y(S2)(y\u22121)s+z,j by unflattening\n=\nd\u2211\ny=1\ns\u2211\nz=1\n( n\u2211\nx=1\nSz,xAx,i,y ) (S2)(y\u22121)s+z,j\n=\nd\u2211\ny=1\ns\u2211\nz=1\nn\u2211\nx=1\nSz,x \u00b7Ax,i,y \u00b7 (S2)(y\u22121)s+z,j .\nFor each nonzero entry Ax,i,y, there is only one z such that Sz,x is nonzero. Thus there is only one j such that (S2)(y\u22121)s+z,j is nonzero. It means that Ax,i,y can only affect one entry of ((SA)2S2)i,j . Thus, (SA)2S2 can be computed in O(nnz(A)) time. Similarly, we can compute (SA)3S3 in O(nnz(A)) time.\nProof of Part (II). Note that T2(SA)2S2 has size t2\u00d7s2. Thus for each i \u2208 [t2], j \u2208 [s2], we have,\n(T2(SA)2S2)i,j = d\u2211\nx=1\nds\u2211\ny\u2032=1\n(T2)i,x((SA)2)x,y\u2032(S2)y\u2032,j by (SA)2 \u2208 Rd\u00d7ds\n=\nd\u2211\nx=1\nd\u2211\ny=1\ns\u2211\nz=1\n(T2)i,x((SA)2)x,(y\u22121)s+z(S2)(y\u22121)s+z,j\n= d\u2211\nx=1\nd\u2211\ny=1\ns\u2211\nz=1\n(T2)i,x(SA)z,x,y(S2)(y\u22121)s+z,j by unflattening\n= d\u2211\nx=1\nd\u2211\ny=1\ns\u2211\nz=1\n(T2)i,x\n( n\u2211\nw=1\nSz,wAw,x,y ) (S2)(y\u22121)s+z,j\n=\nd\u2211\nx=1\nd\u2211\ny=1\ns\u2211\nz=1\nn\u2211\nw=1\n(T2)i,x \u00b7 Sz,w \u00b7Aw,x,y \u00b7 (S2)(y\u22121)s+z,j .\nFor each nonzero entry Aw,x,y, there is only one z such that Sz,w is nonzero. There is only one i such that (T2)i,x is nonzero. Since there is only one z to make Sz,w nonzero, there is only one j, such that (S2)(y\u22121)s+z,j is nonzero. Thus, T2(SA)2S2 can be computed in O(nnz(A)) time. Similarly, we can compute T3(SA)3S3 in O(nnz(A)) time."}, {"heading": "F.2.6 Algorithms", "text": "We first give a \u201cwarm-up\u201d algorithm in Theorem F.22 by using a sampling and rescaling matrix. Then we improve the running time to be polynomial in all the parameters by using an oblivious sketch, and thus we obtain Theorem F.23.\nAlgorithm 32 `1-Frobenius(`1-`2-`2) Low-rank Approximation Algorithm, poly(k)-approximation\n1: procedure L122TensorLowRankApprox(A,n, k) . Theorem F.22 2: \u2190 0.1. 3: s\u2190 poly(k, 1/ ). 4: Guess a sampling and rescaling matrix S \u2208 Rs\u00d7n. 5: s2 \u2190 s3 \u2190 O(k/ ). 6: r \u2190 s2s3. 7: Choose sketching matrices S2 \u2208 Rns\u00d7s2 , S3 \u2208 Rns\u00d7s3 . 8: Compute (SA)2S2, (SA)3S3. 9: Form V\u0303 \u2208 Rn\u00d7r by repeating (SA)2S2 s3 times according to Equation (59).\n10: Form W\u0303 \u2208 Rn\u00d7r by repeating (SA)3S3 s2 times according to Equation (60). 11: Form objective function minU\u2208Rn\u00d7r \u2016U \u00b7 (V\u0303 > W\u0303>)\u2212A1\u2016F . 12: Use a linear regression solver to find a solution U\u0303 . 13: Take the best solution found over all guesses. 14: return U\u0303 , V\u0303 , W\u0303 . 15: end procedure\nTheorem F.22. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, let r = O(k2). There exists\nan algorithm which takes npoly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2016U \u2297 V \u2297W \u2212A\u2016v \u2264 poly(k) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u2016v,\nholds with probability at least 9/10.\nProof. We define OPT as follows,\nOPT = min U,V,W\u2208Rn\u00d7k \u2016U \u2297 V \u2297W \u2212A\u2016v = min U,V,W\u2208Rn\u00d7k \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1 Ui \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 v .\nLet A1 \u2208 Rn\u00d7n2 denote the matrix obtained by flattening tensor A along the 1st dimension. Let U\u2217 \u2208 Rn\u00d7k denote the optimal solution. We fix U\u2217 \u2208 Rn\u00d7k, and consider this objective function,\nmin V,W\u2208Rn\u00d7k \u2016U\u2217 \u2297 V \u2297W \u2212A\u2016v \u2261 min V,W\u2208Rn\u00d7k \u2225\u2225\u2225U\u2217 \u00b7 (V > W>)\u2212A1 \u2225\u2225\u2225 v , (52)\nwhich has cost at most OPT, and where V > W> \u2208 Rk\u00d7n2 denotes the matrix for which the i-th row is a vectorization of Vi \u2297Wi, \u2200i \u2208 [k]. (Note that Vi \u2208 Rn is the i-th column of matrix V \u2208 Rn\u00d7k). Choose a sampling and rescaling diagonal matrix S \u2208 Rn\u00d7n according to U\u2217, which has s = poly(k) non-zero entries. Using S to sketch on the left of the objective function when U\u2217 is fixed (Equation (52)), we obtain a smaller problem,\nmin V,W\u2208Rn\u00d7k \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016v \u2261 min V,W\u2208Rn\u00d7k \u2225\u2225\u2225SU\u2217 \u00b7 (V > W>)\u2212 SA1 \u2225\u2225\u2225 v . (53)\nLet V \u2032,W \u2032 denote the optimal solution to the above problem, i.e.,\nV \u2032,W \u2032 = arg min V,W\u2208Rn\u00d7k \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016v .\nThen using properties (no dilation Lemma F.12 and no contraction Lemma F.13) of S, we have \u2225\u2225U\u2217 \u2297 V \u2032 \u2297W \u2032 \u2212A \u2225\u2225 v \u2264 \u03b1OPT .\nwhere \u03b1 is an approximation ratio determined by S. By definition of \u2016 \u00b7 \u2016v and \u2016 \u00b7 \u20162 \u2264 \u2016 \u00b7 \u20161 \u2264 \u221a dim\u2016 \u00b7 \u20162, we can rewrite Equation (53) in the following way,\n\u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016v\n=\ns\u2211\ni=1\n  n\u2211\nj=1\nn\u2211\nl=1\n( ((SU\u2217)\u2297 V \u2297W )i,j,l \u2212 (SA)i,j,l\n)2   1 2\n\u2264 \u221as   s\u2211\ni=1\nn\u2211\nj=1\nn\u2211\nl=1\n( ((SU\u2217)\u2297 V \u2297W )i,j,l \u2212 (SA)i,j,l\n)2   1 2\n= \u221a s \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016F . (54)\nGiven the above properties of S and Equation (54), for any \u03b2 \u2265 1, let V \u2032\u2032,W \u2032\u2032 denote a \u03b2approximate solution of min\nV,W\u2208Rn\u00d7k \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016F , i.e.,\n\u2225\u2225(SU\u2217)\u2297 V \u2032\u2032 \u2297W \u2032\u2032 \u2212 SA \u2225\u2225 F \u2264 \u03b2 \u00b7 min\nV,W\u2208Rn\u00d7k \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016F . (55)\nThen, \u2225\u2225U\u2217 \u2297 V \u2032\u2032 \u2297W \u2032\u2032 \u2212A \u2225\u2225 v \u2264 \u221as\u03b1\u03b2 \u00b7OPT . (56)\nIn the next few paragraphs we will focus on solving Equation (55). We start by fixing W \u2217 \u2208 Rn\u00d7k to be the optimal solution of\nmin V,W\u2208Rn\u00d7k\n\u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016F .\nWe use (SA)2 \u2208 Rn\u00d7ns to denote the matrix obtained by flattening the tensor SA \u2208 Rs\u00d7n\u00d7n along the second direction. We use Z2 = (SU\u2217)> (W \u2217)> \u2208 Rk\u00d7ns to denote the matrix where the i-th row is the vectorization of (SU\u2217)i \u2297W \u2217i . We can consider the following objective function,\nmin V \u2208Rn\u00d7k\n\u2016V Z2 \u2212 (SA)2\u2016F .\nChoosing a sketching matrix S2 \u2208 Rns\u00d7s2 with s2 = O(k/ ) gives a smaller problem,\nmin V \u2208Rn\u00d7k\n\u2016V Z2S2 \u2212 (SA)2S2\u2016F .\nLetting V\u0302 = (SA)2S2(Z2S2)\u2020 \u2208 Rn\u00d7k, then\n\u2016V\u0302 Z2 \u2212 (SA)2\u2016F \u2264 (1 + ) min V \u2208Rn\u00d7k \u2016V Z2 \u2212 (SA)2\u2016F\n= (1 + ) min V \u2208Rn\u00d7k\n\u2016V ((SU\u2217)> (W \u2217)>)\u2212 (SA)2\u2016F\n= (1 + ) min V \u2208Rn\u00d7k\n\u2016(SU\u2217)\u2297 V \u2297W \u2217 \u2212 SA\u2016F by unflattening\n= (1 + ) min V,W\u2208Rn\u00d7k\n\u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016F . by definition of W \u2217 (57)\nWe define D2 \u2208 Rn2\u00d7n2 to be a diagonal matrix obrained by copying the n\u00d7 n identity matrix s times on n diagonal blocks of D2. Then it has ns nonzero entries. Thus, D2 also can be thought of as a matrix that has size n2 \u00d7 ns.\nWe can think of (SA)2S2 \u2208 Rn\u00d7s2 as follows,\n(SA)2S2 = (A(S, I, I))2S2\n= A2\ufe38\ufe37\ufe37\ufe38 n\u00d7n2 \u00b7 D2\ufe38\ufe37\ufe37\ufe38 n2\u00d7n2 \u00b7 S2\ufe38\ufe37\ufe37\ufe38 ns\u00d7s2 by D2 can be thought of as having size n2 \u00d7 ns\n= A2 \u00b7   c2,1In c2,2In\n. . . c2,nIn\n  \u00b7 S2\nwhere In is an n\u00d7 n identity matrix, c2,i \u2265 0 for each i \u2208 [n], and the number of nonzero c2,i is s. For the last step, we fix SU\u2217 and V\u0302 . We use (SA)3 \u2208 Rn\u00d7ns to denote the matrix obtained by flattening the tensor SA \u2208 Rs\u00d7n\u00d7n along the third direction. We use Z3 = (SU\u2217)> V\u0302 > \u2208 Rk\u00d7ns\nto denote the matrix where the i-th row is the vectorization of (SU\u2217)i \u2297 V\u0302i. We can consider the following objective function,\nmin W\u2208Rn\u00d7k\n\u2016WZ3 \u2212 (SA)3\u2016F .\nChoosing a sketching matrix S3 \u2208 Rns\u00d7s3 with s3 = O(k/ ) gives a smaller problem,\nmin W\u2208Rn\u00d7k\n\u2016WZ3S3 \u2212 (SA)3S3\u2016F .\nLet W\u0302 = (SA)3S3(Z3S3)\u2020 \u2208 Rn\u00d7k. Then\n\u2016W\u0302Z3 \u2212 (SA)3\u2016F \u2264 (1 + ) min W\u2208Rn\u00d7k \u2016WZ3 \u2212 (SA)3\u2016F by property of S3\n= (1 + ) min W\u2208Rn\u00d7k\n\u2016W ((SU\u2217)> V\u0302 >)\u2212 (SA)3\u2016F by definition Z3\n= (1 + ) min W\u2208Rn\u00d7k\n\u2016(SU\u2217)\u2297 V\u0302 \u2297W \u2212 SA\u2016F by unflattening\n\u2264 (1 + )2 \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016F . by Equation (57)\nWe define D3 \u2208 Rn2\u00d7n2 to be a diagonal matrix formed by copying the n\u00d7 n identity matrix s times on n diagonal blocks of D3. Then it has ns nonzero entries. Thus, D3 also can be thought of as a matrix that has size n2 \u00d7 ns and D3 is uniquely determined by S.\nSimilarly as to the 2nd dimension, for the 3rd dimension, we can think of (SA)3S3 as follows,\n(SA)3S3 = (A(S, I, I))3S3\n= A3\ufe38\ufe37\ufe37\ufe38 n\u00d7n2 \u00b7 D3\ufe38\ufe37\ufe37\ufe38 n2\u00d7n2 \u00b7 S3\ufe38\ufe37\ufe37\ufe38 ns\u00d7s3\nby D3 can be thought of as having size n2 \u00d7 ns\n= A3 \u00b7   c3,1In c3,2In\n. . . c3,nIn\n  \u00b7 S3\nwhere In is an n\u00d7 n identity matrix, c3,i \u2265 0 for each i \u2208 [n] and the number of nonzero c3,i is s. Overall, we have proved that,\nmin X2,X3\n\u2016(SU\u2217)\u2297 (A2D2S2X2)\u2297 (A3D3S3X3)\u2212 SA\u2016F \u2264 (1 + )2 \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016F , (58)\nwhere diagonal matrix D2 \u2208 Rn2\u00d7n2 (with ns nonzero entries) and D3 \u2208 Rn2\u00d7n2 (with ns nonzero entries) are uniquely determined by diagonal matrix S \u2208 Rn\u00d7n (s nonzero entries). Let X \u20322 and X \u20323 denote the optimal solution to the above problem (Equation (58)). Let V \u2032\u2032 = (A2D2S2X \u20322) \u2208 Rn\u00d7k and W \u2032\u2032 = (A3D3S3X \u20323) \u2208 Rn\u00d7k. Then we have\n\u2225\u2225U\u2217 \u2297 V \u2032\u2032 \u2297W \u2032\u2032 \u2212A \u2225\u2225 v \u2264 \u221as\u03b1\u03b2OPT .\nWe construct matrix V\u0303 \u2208 Rn\u00d7s2s3 by copying matrix (SA)2S2 \u2208 Rn\u00d7s2 s3 times,\nV\u0303 = [ (SA)2S2 (SA)2S2 \u00b7 \u00b7 \u00b7 (SA)2S2. ] (59)\nWe construct matrix W\u0303 \u2208 Rn\u00d7s2s3 by copying the i-th column of matrix (SA)3S3 \u2208 Rn\u00d7s3 into (i\u2212 1)s2 + 1, \u00b7 \u00b7 \u00b7 , is2 columns of W\u0303 ,\nW\u0303 = [((SA)3S3)1 \u00b7 \u00b7 \u00b7 ((SA)3S3)1 ((SA)3S3)2 \u00b7 \u00b7 \u00b7 ((SA)3S3)2 \u00b7 \u00b7 \u00b7 ((SA)3S3)s3 \u00b7 \u00b7 \u00b7 ((SA)3S3)s3 .] (60)\nAlthough we don\u2019t know S, we can guess all of the possibilities. For each possibility, we can find a solution U\u0303 \u2208 Rn\u00d7s2s3 to the following problem,\nmin U\u2208Rn\u00d7s2s3 \u2225\u2225\u2225\u2225\u2225\u2225 s2\u2211\ni=1\ns3\u2211\nj=1\nU(i\u22121)s3+j \u2297 ((SA)2S2)i \u2297 ((SA)3S3)j \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 v\n= min U\u2208Rn\u00d7s2s3 \u2225\u2225\u2225\u2225\u2225\u2225 s2\u2211\ni=1\ns3\u2211\nj=1\nU(i\u22121)s3+j \u00b7 vec(((SA)2S2)i \u2297 ((SA)3S3)j)\u2212A1 \u2225\u2225\u2225\u2225\u2225\u2225 v\n= min U\u2208Rn\u00d7s2s3 \u2225\u2225\u2225\u2225\u2225\u2225 s2\u2211\ni=1\ns3\u2211\nj=1\nU(i\u22121)s3+j \u00b7 (V\u0303 > W\u0303>)(i\u22121)s3+j \u2212A1 \u2225\u2225\u2225\u2225\u2225\u2225 v\n= min U\u2208Rn\u00d7s2s3 \u2225\u2225\u2225U \u00b7 (V\u0303 > W\u0303>)\u2212A1 \u2225\u2225\u2225 v\n= min U\u2208Rn\u00d7s2s3\n\u2016UZ \u2212A1\u2016v\n= min U\u2208Rn\u00d7s2s3\ns2s3\u2211\ni=1\n\u2016U iZ \u2212Ai1\u20162,\nwhere the first step follows by flattening the tensor along the 1st dimension, U(i\u22121)s3+j denotes the (i\u22121)s3 +j-th column of U \u2208 Rn\u00d7s2s3 , A1 \u2208 Rn\u00d7n2 denotes the matrix obtained by flattening tensor A along the 1st dimension, the second step follows since V\u0303 > W\u0303> \u2208 Rs2s3\u2208n2 is defined to be the matrix where the (i\u2212 1)s3 + j-th row is vectorization of ((SA)2S2)i \u2297 ((SA)3S3)j , the fourth step follows by defining Z to be V\u0303 > W\u0303>, and the last step follows by definition of \u2016 \u00b7 \u2016v norm. Thus, we obtain a multiple regression problem and it can be solved directly by using [CW13, NN13].\nFinally, we take the best U\u0303 , V\u0303 , W\u0303 over all the guesses. The entire running time is dominated by the number of guesses, which is npoly(k). This completes the proof.\nTheorem F.23. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, let r = O(k2). There exists an algorithm which takes O(nnz(A)) + n poly(k, log n) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2016U \u2297 V \u2297W \u2212A\u2016v \u2264 poly(k, log n) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u2016v\nholds with probability at least 9/10.\nProof. We define OPT as follows,\nOPT = min U,V,W\u2208Rn\u00d7k \u2016U \u2297 V \u2297W \u2212A\u2016v = min U,V,W\u2208Rn\u00d7k \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1 Ui \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 v .\nAlgorithm 33 `1-Frobenius(`1-`2-`2) Low-rank Approximation Algorithm, poly(k, log n)approximation 1: procedure L122TensorLowRankApprox(A,n, k) . Theorem F.23 2: \u2190 0.1. 3: s\u2190 poly(k, log n). 4: Choose S \u2208 Rs\u00d7n to be an MSketch. . Definition F.15 5: s2 \u2190 s3 \u2190 O(k/ ). 6: t2 \u2190 t3 \u2190 poly(k/ ). 7: r \u2190 s2s3. 8: Choose sketching matrices S2 \u2208 Rns\u00d7s2 , S3 \u2208 Rns\u00d7s3 . 9: Choose sketching matrices T2 \u2208 Rt2\u00d7n, T3 \u2208 Rt3\u00d7n. 10: Compute (SA)2S2, (SA)3S3. 11: Compute T2(SA)2S2, T3(SA)3S3. 12: Form V\u0303 \u2208 Rn\u00d7r by repeating (SA)2S2 s3 times according to Equation (69). 13: Form W\u0303 \u2208 Rn\u00d7r by repeating (SA)3S3 s2 times according to Equation (70). 14: Form V \u2208 Rt2\u00d7r by repeating T2(SA)2S2 s3 times according to Equation (67). 15: Form W \u2208 Rt3\u00d7r by repeating T3(SA)3S3 s2 times according to Equation (68). 16: C \u2190 A(I, T2, T3). 17: Form objective function minU\u2208Rn\u00d7r \u2016U \u00b7 (V\n> W>)\u2212 C1\u2016F . 18: Use linear regression solver to find a solution U\u0303 . 19: return U\u0303 , V\u0303 , W\u0303 . 20: end procedure\nLet A1 \u2208 Rn\u00d7n2 denote the matrix obtained by flattening tensor A along the 1st dimension. Let U\u2217 \u2208 Rn\u00d7k denote the optimal solution. We fix U\u2217 \u2208 Rn\u00d7k, and consider the objective function,\nmin V,W\u2208Rn\u00d7k \u2016U\u2217 \u2297 V \u2297W \u2212A\u2016v \u2261 min V,W\u2208Rn\u00d7k \u2225\u2225\u2225U\u2217 \u00b7 (V > W>)\u2212A1 \u2225\u2225\u2225 v , (61)\nwhich has cost at most OPT, and where V > W> \u2208 Rk\u00d7n2 denotes the matrix for which the i-th row is a vectorization of Vi \u2297Wi, \u2200i \u2208 [k]. (Note that Vi \u2208 Rn is the i-th column of matrix V \u2208 Rn\u00d7k). Choose an (oblivious) MSketch S \u2208 Rs\u00d7n with s = poly(k, log n) according to Definition F.15. Using MSketch S,w to sketch on the left of the objective function when U\u2217 is fixed (Equation (61)), we obtain a smaller problem,\nmin V,W\u2208Rn\u00d7k \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016v \u2261 min V,W\u2208Rn\u00d7k \u2225\u2225\u2225SU\u2217 \u00b7 (V > W>)\u2212 SA1 \u2225\u2225\u2225 v . (62)\nLet V \u2032,W \u2032 denote the optimal solution to the above problem, i.e.,\nV \u2032,W \u2032 = arg min V,W\u2208Rn\u00d7k \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016v .\nThen using properties (no dilation Lemma F.19 and no contraction Lemma F.20) of S, we have \u2225\u2225U\u2217 \u2297 V \u2032 \u2297W \u2032 \u2212A \u2225\u2225 v \u2264 \u03b1OPT .\nwhere \u03b1 is an approximation ratio determined by S.\nBy definition of \u2016 \u00b7 \u2016v and \u2016 \u00b7 \u20162 \u2264 \u2016 \u00b7 \u20161 \u2264 \u221a dim\u2016 \u00b7 \u20162, we can rewrite Equation (62) in the\nfollowing way,\n\u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016v\n=\ns\u2211\ni=1\n  n\u2211\nj=1\nn\u2211\nl=1\n( ((SU\u2217)\u2297 V \u2297W )i,j,l \u2212 (SA)i,j,l\n)2   1 2\n\u2264 \u221as   s\u2211\ni=1\nn\u2211\nj=1\nn\u2211\nl=1\n( ((SU\u2217)\u2297 V \u2297W )i,j,l \u2212 (SA)i,j,l\n)2   1 2\n= \u221a s \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016F (63)\nUsing the properties of S and Equation (63), for any \u03b2 \u2265 1, let V \u2032\u2032,W \u2032\u2032 denote a \u03b2-approximation solution of min\nV,W\u2208Rn\u00d7k \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016F , i.e.,\n\u2225\u2225(SU\u2217)\u2297 V \u2032\u2032 \u2297W \u2032\u2032 \u2212 SA \u2225\u2225 F \u2264 \u03b2 \u00b7 min\nV,W\u2208Rn\u00d7k \u2016(SU\u2217)\u2297 V \u2297W \u2212 SA\u2016F . (64)\nThen, \u2225\u2225U\u2217 \u2297 V \u2032\u2032 \u2297W \u2032\u2032 \u2212A \u2225\u2225 v \u2264 \u221as\u03b1\u03b2 \u00b7OPT . (65)\nLet A\u0302 denote SA. Choose Si \u2208 Rns\u00d7si to be Gaussian matrix with si = O(k/ ), \u2200i{2, 3}. By a similar proof as in Theorem F.22, we have if X \u20322, X \u20323 is a \u03b2-approximate solution to\nmin X2,X3\n\u2016(SU\u2217)\u2297 (A\u03022S2X2)\u2297 (A\u03023S3X3)\u2212 SA\u2016F ,\nthen,\n\u2016U\u2217 \u2297 (A\u03022S2X2)\u2297 (A\u03023S3X3)\u2212A\u2016v \u2264 \u221a s\u03b1\u03b2.\nTo reduce the size of the objective function from poly(n) to poly(k/ ), we use perform an \u201cinput sparsity reduction\u201d (in Lemma C.3). Note that, we do not need to use this idea to optimize the running time in Theorem F.22. The running time of Theorem F.22 is dominated by guessing sampling and rescaling matrices. (That running time is nnz(A).) Choose Ti \u2208 Rti\u00d7n to be a sparse subspace embedding matrix (CountSketch transform) with ti = poly(k, 1/ ), \u2200i \u2208 {2, 3}. Applying the proof of Lemma C.3 here, we obtain, if X \u20322, X \u20323 is a \u03b2-approximate solution to\nmin X2,X3\n\u2016(SU\u2217)\u2297 (T2(SA)2S2X2)\u2297 (T3(SA)3S3X3)\u2212 SA\u2016F ,\nthen,\n\u2016U\u2217 \u2297 ((SA)2S2X2)\u2297 ((SA)3S3X3)\u2212A\u2016v \u2264 \u221a s\u03b1\u03b2. (66)\nSimilar to the bicriteria results in Section C.4, Equation (66) indicates that we can construct a bicriteria solution by using two matrices (SA)2S2 and (SA)3S3. The next question is how to obtain the final results U\u0302 , V\u0302 , W\u0302 . We first show how to obtain U\u0302 . Then we show to construct V\u0302 and W\u0302 .\nTo obtain U\u0302 , we need to solve a regression problem related to two matrices V , W\u0302 and a tensor A(I, T2, T3). We construct matrix V \u2208 Rt2\u00d7s2s3 by copying matrix T2(SA)2S2 \u2208 Rt2\u00d7s2 s3 times,\nV = [ T2(SA)2S2 T2(SA)2S2 \u00b7 \u00b7 \u00b7 T2(SA)2S2 ] . (67)\nWe construct matrix W \u2208 Rt3\u00d7s2s3 by copying the i-th column of matrix T3(SA)3S3 \u2208 Rt3\u00d7s3 into (i\u2212 1)s2 + 1, \u00b7 \u00b7 \u00b7 , is2 columns of W ,\nW = [ F1 \u00b7 \u00b7 \u00b7F1 F2 \u00b7 \u00b7 \u00b7F2 \u00b7 \u00b7 \u00b7 Fs3 \u00b7 \u00b7 \u00b7Fs3 ] , (68)\nwhere F = T3(SA)3S3. Thus, to obtain U\u0303 \u2208 Rs2s3 , we just need to use a linear regression solver to solve a smaller problem,\nmin U\u2208Rs2s3\n\u2016U \u00b7 (V > W>)\u2212A(I, T2, T3)\u2016F ,\nwhich can be solved in O(nnz(A)) + n poly(k, log n) time. We will show how to obtain V\u0303 and W\u0303 . We construct matrix V\u0303 \u2208 Rn\u00d7s2s3 by copying matrix (SA)2S2 \u2208 Rn\u00d7s2 s3 times,\nV\u0303 = [ (SA)2S2 (SA)2S2 \u00b7 \u00b7 \u00b7 (SA)2S2. ] (69)\nWe construct matrix W\u0303 \u2208 Rn\u00d7s2s3 by copying the i-th column of matrix (SA)3S3 \u2208 Rn\u00d7s3 into (i\u2212 1)s2 + 1, \u00b7 \u00b7 \u00b7 , is2 columns of W\u0303 ,\nW\u0303 = [ F1 \u00b7 \u00b7 \u00b7F1 F2 \u00b7 \u00b7 \u00b7F2 \u00b7 \u00b7 \u00b7 Fs3 \u00b7 \u00b7 \u00b7Fs3 ] , (70)\nwhere F = (SA)3S3.\nF.3 `1-`1-`2 norm\nSection F.3.1 presents some definitions and useful facts for the tensor `1-`1-`2 norm. We provide some tools in Section F.3.2. Section F.3.3 presents a key idea which shows we are able to reduce the original problem to a new problem under entry-wise `1 norm. Section F.3.4 presents several existence results. Finally, Section F.3.6 introduces several algorithms with different tradeoffs."}, {"heading": "F.3.1 Definitions", "text": "Definition F.24. (Tensor u-norm) For an n\u00d7n\u00d7n tensor A, we define the u-norm of A, denoted \u2016A\u2016u, to be\n  n\u2211\ni=1\nn\u2211\nj=1\nM(\u2016Ai,j,\u2217\u20162)\n  1/p\n,\nwhere Ai,j,\u2217 is the (i, j)-th tube of A, and p is a parameter associated with the function M(), which defines a nice M -Estimator.\nDefinition F.25. (Matrix u-norm) For an n\u00d7 n matrix A, we define u-norm of A, denoted \u2016A\u2016u, to be\n( n\u2211\ni=1\nM(\u2016Ai,\u2217\u20162) )1/p ,\nwhere Ai,\u2217 is the i-th row of A, and p is a parameter associated with the function M(), which defines a nice M -Estimator.\nFact F.26. For p = 1, for any two matrices A and B, we have \u2016A+B\u2016u \u2264 \u2016A\u2016u + \u2016B\u2016u. For any two tensors A and B, we have \u2016A+B\u2016u \u2264 \u2016A\u2016u + \u2016B\u2016u."}, {"heading": "F.3.2 Projection via Gaussians", "text": "Definition F.27. Let p \u2265 1. Let `Sn\u22121p be an infinite dimensional `p metric which consists of a coordinate for each vector r in the unit sphere Sn\u22121. Define function f : Sn\u22121 \u2192 R. The `1-norm of any such f is defined as follows:\n\u2016f\u20161 = (\u222b\nr\u2208Sn\u22121 |f(r)|pdr\n)1/p .\nClaim F.28. Let fv(r) = \u3008v, r\u3009. There exists a universal constant \u03b1p such that\n\u2016fv\u2016p = \u03b1p\u2016v\u20162.\nProof. We have,\n\u2016fv\u2016p = (\u222b\nr\u2208Sn\u22121 |\u3008v, r\u3009|pdr\n)1/p\n=\n(\u222b\n\u03b8\u2208Sn\u22121 \u2016v\u2016p2 \u00b7 | cos \u03b8|pd\u03b8\n)1/p\n= \u2016v\u20162 (\u222b\n\u03b8\u2208Sn\u22121 | cos \u03b8|pd\u03b8\n)1/p\n= \u03b1p\u2016v\u20162.\nThis completes the proof.\nLemma F.29. Let G \u2208 Rk\u00d7n denote i.i.d. random Gaussian matrices with rescaling. Then for any v \u2208 Rn, we have\nPr[(1\u2212 )\u2016v\u20162 \u2264 \u2016Gv\u20161 \u2264 (1 + )\u2016v\u20162] \u2265 1\u2212 2\u2212\u2126(k 2).\nProof. For each i \u2208 [k], we define Xi = \u3008v, gi\u3009, where gi \u2208 Rn is the i-th row of G. Then Xi =\u2211n j=1 vjgi,j and E[|Xi|] = \u03b1p\u2016v\u20162. Define Y = \u2211k i=1 |Xi|. We have E[Y ] = k\u03b11\u2016v\u20162 = k\u03b11.\nWe can show\nPr[Y \u2265 (1 + )\u03b11k] = Pr[esY \u2265 es(1+ )\u03b11k] for all s > 0 \u2264 E[esY ]/es(1+ )\u03b11k by Markov\u2019s inequality\n= e\u2212s(1+ )\u03b11k \u00b7E[ k\u220f\ni=1\nes|Xi|] by Y = k\u2211\ni=1\n|Xi|\n= e\u2212s(1+ )\u03b11k \u00b7 (E[es|X1|])k\nIt remains to bound E[es|X1|]. Since X1 \u223c N (0, 1), we have that X1 has density function e\u2212t2/2.\nThus, we have,\nE[es|X1|] = 1\u221a 2\u03c0\n\u222b +\u221e\n\u2212\u221e es|t| \u00b7 e\u2212t2/2dt\n= 1\u221a 2\u03c0\n\u222b +\u221e\n\u2212\u221e es 2/2 \u00b7 e\u2212(|t|\u2212s)2/2dt\n= es 2/2(erf(s/ \u221a 2) + 1)\n\u2264 es2/2((1\u2212 exp(\u22122s2/\u03c0))1/2 + 1) by 1\u2212 exp(\u22124x2/\u03c0) \u2265 erf(x)2 \u2264 es2/2( \u221a 2/\u03c0s+ 1). by 1\u2212 e\u2212x \u2264 x\nThus, we have\nPr[Y \u2265 (1 + )\u03b11k] \u2264 e\u2212s(1+ )keks 2/2(1 + s \u221a 2/\u03c0)k\n= e\u2212s(1+ )\u03b11keks 2/2ek\u00b7log(1+s\n\u221a 2/\u03c0)\n\u2264 e\u2212s(1+ )\u03b11k+ks2/2+k\u00b7s \u221a 2/\u03c0 \u2264 e\u2212\u2126(k 2). by \u03b11 \u2265 \u221a 2/\u03c0 and setting s =\nLemma F.30. For any \u2208 (0, 1), let k = O(n/ 2). Let G \u2208 Rk\u00d7n denote i.i.d. random Gaussian matrices with rescaling. Then for any v \u2208 Rn, with probability at least 1\u2212 2\u2212\u2126(n/ 2), we have : for all v \u2208 Rn,\n(1\u2212 )\u2016v\u20162 \u2264 \u2016Gv\u20161 \u2264 (1 + )\u2016v\u20162.\nProof. Let S denote {y \u2208 Rn | \u2016y\u20162 = 1}. We construct a \u03b3-net so that for all y \u2208 S, there exists a vector w \u2208 N for which \u2016y \u2212 w\u20162 \u2264 \u03b3. We set \u03b3 = 1/2.\nFor any unit vector y, we can write\ny = y0 + y1 + y2 + \u00b7 \u00b7 \u00b7 ,\nwhere \u2016yi\u20162 \u2264 1/2i and yi is a scalar multiple of a vector in N . Thus, we have\n\u2016Gy\u20161 = \u2016G(y0 + y1 + y2 + \u00b7 \u00b7 \u00b7 )\u20161\n\u2264 \u221e\u2211\ni=0\n\u2016Gyi\u20161 by triangle inequality\n\u2264 \u221e\u2211\ni=0\n(1 + )\u2016yi\u20162\n\u2264 \u221e\u2211\ni=0\n(1 + ) 1\n2i\n\u2264 1 + \u0398( ).\nSimilarly, we can lower bound \u2016Gy\u20161 by 1\u2212\u0398( ). By Lemma 2.2 in [Woo14], we know that for any \u03b3 \u2208 (0, 1), there exists a \u03b3-net N of S for which |N | \u2264 (1 + 4/\u03b3)n.\nF.3.3 Reduction, projection to high dimension\nLemma F.31. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, let S \u2208 Rn\u00d7s denote a Gaussian matrix with s = O(n/ 2) columns. With probability at least 1\u2212 2\u2212\u2126(n/ 2), for any U, V,W \u2208 Rn\u00d7k, we have\n(1\u2212 ) \u2016U \u2297 V \u2297W \u2212A\u2016u \u2264 \u2016(U \u2297 V \u2297W )S \u2212AS\u20161 \u2264 (1 + ) \u2016U \u2297 V \u2297W \u2212A\u2016u .\nProof. By definition of the \u2297 product between matrices and \u00b7 product between a tensor and a matrix, we have (U \u2297 V \u2297W )S = U \u2297 V \u2297 (SW ) \u2208 Rn\u00d7n\u00d7s. We use Ai,j,\u2217 \u2208 Rn to denote the (i, j)-th tube (the column in the 3rd dimension) of tensor A. We first prove the upper bound,\n\u2016(U \u2297 V \u2297W )S \u2212AS\u20161 = n\u2211\ni=1\nn\u2211\nj=1\n\u2016((U \u2297 V \u2297W )i,j,\u2217 \u2212Ai,j,\u2217)S\u20161\n\u2264 n\u2211\ni=1\nn\u2211\nj=1\n(1 + ) \u2016(U \u2297 V \u2297W )i,j,\u2217 \u2212Ai,j,\u2217\u20162\n= (1 + ) \u2016U \u2297 V \u2297W \u2212A\u2016u ,\nwhere the first step follows by definition of tensor \u2016\u00b7\u2016u norm, the second step follows by Lemma F.30, and the last step follows by tensor entry-wise `1 norm. Similarly, we can prove the lower bound,\n\u2016(U \u2297 V \u2297W )S \u2212AS\u20161 \u2265 n\u2211\ni=1\nn\u2211\nj=1\n(1\u2212 ) \u2016(U \u2297 V \u2297W )i,j,\u2217 \u2212Ai,j,\u2217\u20162\n= (1\u2212 ) \u2016U \u2297 V \u2297W \u2212A\u2016u .\nThis completes the proof.\nCorollary F.32. For any \u03b1 \u2265 1, if U \u2032, V \u2032,W \u2032 satisfy\n\u2016(U \u2032 \u2297 V \u2032 \u2297W \u2032 \u2212A)S\u20161 \u2264 \u03b3 min rank\u2212k Ak \u2016(Ak \u2212A)S\u20161,\nthen\n\u2016U \u2032 \u2297 V \u2032 \u2297W \u2032 \u2212A\u2016u \u2264 \u03b3 1 +\n1\u2212 minrank\u2212k Ak \u2016Ak \u2212A\u2016u.\nProof. Let U\u0302 , V\u0302 , W\u0302 denote the optimal solution to minrank\u2212k Ak \u2016(Ak \u2212 A)S\u20161. Let U\u2217, V \u2217,W \u2217 denote the optimal solution to minrank\u2212k Ak \u2016Ak \u2212A\u2016u. Then,\n\u2016U \u2032 \u2297 V \u2032 \u2297W \u2032 \u2212A\u2016u \u2264 1 1\u2212 \u2016(U \u2032 \u2297 V \u2032 \u2297W \u2032 \u2212A)S\u20161\n\u2264 \u03b3 1 1\u2212 \u2016(U\u0302 \u2297 V\u0302 \u2297 W\u0302 \u2212A)S\u20161 \u2264 \u03b3 1 1\u2212 \u2016(U \u2217 \u2297 V \u2217 \u2297W \u2217 \u2212A)S\u20161 \u2264 \u03b3 1 + 1\u2212 \u2016U \u2217 \u2297 V \u2217 \u2297W \u2217 \u2212A\u2016u,\nwhich completes the proof."}, {"heading": "F.3.4 Existence results", "text": "Theorem F.33 (Existence results). Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n and a matrix S \u2208 Rn\u00d7n, let OPT denote minrank\u2212k Ak\u2208Rn\u00d7n\u00d7n \u2016(Ak \u2212 A)S\u20161, let A\u0302 = AS \u2208 Rn\u00d7n\u00d7n. For any k \u2265 1, there exist three matrices S1 \u2208 Rnn\u00d7s1, S2 \u2208 Rnn\u00d7s2, S3 \u2208 Rn2\u00d7s3 such that\nmin X1\u2208Rs1\u00d7k,X2\u2208Rs2\u00d7k,X3\u2208Rs3\u00d7k\n\u2225\u2225\u2225(A\u03021S1X1)\u2297 (A\u03022S2X2)\u2297 (A\u03023S3X3)\u2212 A\u0302 \u2225\u2225\u2225\n1 \u2264 \u03b1OPT,\nor equivalently,\nmin X1\u2208Rs1\u00d7k,X2\u2208Rs2\u00d7k,X3\u2208Rs3\u00d7k\n\u2225\u2225\u2225 ( (A\u03021S1X1)\u2297 (A\u03022S2X2)\u2297 (A3S3X3)\u2212A ) S \u2225\u2225\u2225\n1 \u2264 \u03b1OPT,\nholds with probability 99/100. (I). Using a dense Cauchy transform, s1 = s2 = s3 = O\u0303(k), \u03b1 = O\u0303(k1.5) log3 n. (II). Using a sparse Cauchy transform, s1 = s2 = s3 = O\u0303(k 5), \u03b1 = O\u0303(k13.5) log3 n.\n(III). Guessing Lewis weights, s1 = s2 = s3 = O\u0303(k), \u03b1 = O\u0303(k1.5).\nProof. We use OPT to denote the optimal cost,\nOPT := min rank\u2212k Ak\u2208Rn\u00d7n\u00d7n\n\u2016(Ak \u2212A)S\u20161.\nWe fix V \u2217 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k to be the optimal solution to\nmin U,V,W\n\u2016(U \u2297 V \u2297W \u2212A)S\u20161.\nWe define Z1 \u2208 Rk\u00d7nn to be the matrix where the i-th row is the vectorization of V \u2217i \u2297 (SW \u2217i ). We define tensor\nA\u0302 = AS \u2208 Rn\u00d7n\u00d7n.\nThen we also have A\u0302 = A(I, I, S) according to the definition of the \u00b7 product between a tensor and a matrix.\nLet A\u03021 \u2208 Rn\u00d7nn denote the matrix obtained by flattening tensor A\u0302 along the first direction. We can consider the following optimization problem,\nmin U\u2208Rn\u00d7k\n\u2225\u2225\u2225UZ1 \u2212 A\u03021 \u2225\u2225\u2225\n1 .\nChoosing S1 to be one of the following sketching matrices: (I) a dense Cauchy transform, (II) a sparse Cauchy transform, (III) a sampling and rescaling diagonal matrix according to Lewis weights.\nLet \u03b1S1 denote the approximation ratio produced by the sketching matrix S1. We use S1 \u2208 Rnn\u00d7s1 to sketch on right of the above problem, and obtain the problem:\nmin U\u2208Rn\u00d7k \u2016UZ1S1 \u2212 A\u03021S1\u20161 = min U\u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016U iZ1S1 \u2212 (A\u03021S1)i\u20161,\nwhere U i denotes the i-th row of matrix U \u2208 Rn\u00d7k and (A\u03021S1)i denotes the i-th row of matrix A\u03021S1. Instead of solving it under `1-norm, we consider the `2-norm relaxation,\nmin U\u2208Rn\u00d7k \u2016UZ1S1 \u2212 A\u03021S1\u20162F = min U\u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016U iZ1S1 \u2212 (A\u03021S1)i\u201622.\nLet U\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above optimization problem, so that U\u0302 = A\u03021S1(Z1S1)\n\u2020. We plug U\u0302 into the objective function under the `1-norm. By the property of sketching matrix S1 \u2208 Rnn\u00d7s1 , we have,\n\u2016U\u0302Z1 \u2212 A\u03021\u20161 \u2264 \u03b1S1 min U\u2208Rn\u00d7k \u2016UZ1 \u2212 A\u03021\u20161 = \u03b1S1 OPT,\nwhich implies that,\n\u2016U\u0302 \u2297 V \u2217 \u2297 (SW \u2217)\u2212 A\u0302\u20161 = \u2016(U\u0302 \u2297 V \u2217 \u2297W \u2217)S \u2212 A\u0302\u20161 \u2264 \u03b1S1 OPT .\nIn the second step, we fix U\u0302 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k. Let A\u03022 \u2208 Rn\u00d7nn denote the matrix obtained by flattening tensor A\u0302 \u2208 Rn\u00d7n\u00d7n along the second direction. We choose a sketching matrix S2 \u2208 Rnn\u00d7s2 . Let Z2 = U\u0302> (SW \u2217)> \u2208 Rk\u00d7nn denote the matrix where the i-th row is the vectorization of U\u0302i \u2297 (SW \u2217i ). Define V\u0302 = A\u03022S2(Z2S2)\u2020. By the properties of sketching matrix S2, we have\n\u2016V\u0302 Z2 \u2212 A\u03022\u20161 \u2264 \u03b1S2\u03b1S1 OPT,\nIn the third step, we fix U\u0302 \u2208 Rn\u00d7k and V\u0302 \u2208 Rn\u00d7k. Let A\u03023 \u2208 Rn\u00d7n2 denote the matrix obtained by flattening tensor A\u0302 \u2208 Rn\u00d7n\u00d7n along the third direction. We choose a sketching matrix S3 \u2208 Rn2\u00d7s3 . Let Z3 \u2208 Rk\u00d7n2 denote the matrix where the i-th row is the vectorization of U\u0302i \u2297 V\u0302i. Define W \u2032 = A\u03023S3(Z3S3)\u2020 \u2208 Rn\u00d7k and W\u0302 = A3S3(Z3S3)\u2020 \u2208 Rn\u00d7k. Then we have,\nW \u2032 = A\u03023S3(Z3S3) \u2020\n= (A(I, I, S))3S3(Z3S3) \u2020 = (S>A3)S3(Z3S3) \u2020\n= S>W\u0302\nBy properties of sketching matrix S3, we have\n\u2016W \u2032Z3 \u2212 A\u03023\u20161 \u2264 \u03b1S3\u03b1S2\u03b1S1 OPT .\nReplacing W \u2032 by S>W\u0302 , we obtain,\n\u2016W \u2032Z3 \u2212 A\u03023\u20161 = \u2016S>W\u0302Z3 \u2212 A\u03023\u20161 = \u2016S>W\u0302Z3 \u2212 S>A3\u20161 = \u2016(U\u0302 \u2297 V\u0302 \u2297 W\u0302 \u2212A)S\u20161.\nThus, we have\nmin X1\u2208Rs1\u00d7k,X2\u2208Rs2\u00d7k,X3\u2208Rs3\u00d7k\n\u2225\u2225\u2225(A\u03021S1X1)\u2297 (A\u03022S2X2)\u2297 (A\u03023S3X3)\u2212 A\u0302 \u2225\u2225\u2225\n1 \u2264 \u03b1S1\u03b1S2\u03b1S3 OPT .\nF.3.5 Running time analysis\nFact F.34. Given tensor A \u2208 Rn\u00d7n\u00d7n and a matrix B \u2208 Rn\u00d7d with d = O(n), let AB denote an n\u00d7 n\u00d7 d size tensor, For each i \u2208 [3], let (AB)i denote a matrix obtained by flattening tensor AB along the i-th dimension, then\n(AB)1 \u2208 Rn\u00d7nd, (AB)2 \u2208 Rn\u00d7nd, (AB)3 \u2208 Rd\u00d7n 2 .\nFor each i \u2208 [3], let Si \u2208 Rnd\u00d7si denote a sparse Cauchy transform, Ti \u2208 Rti\u00d7n. Then we have, (I) If T1 denotes a sparse Cauchy transform or a sampling and rescaling matrix according to the Lewis weights, T1(AB)1S1 can be computed in O(nnz(A)d) time. Otherwise, it can be computed in O(nnz(A)d+ ns1t1). (II) If T2 denotes a sparse Cauchy transform or a sampling and rescaling matrix according to the Lewis weights, T2(AB)2S2 can be computed in O(nnz(A)d) time. Otherwise, it can be computed in O(nnz(A)d+ ns2t2). (III) If T3 denotes a sparse Cauchy transform or a sampling and rescaling matrix according to the Lewis weights, T3(AB)3S3 can be computed in O(nnz(A)d) time. Otherwise, it can be computed in O(nnz(A)d+ ds3t3).\nProof. Part (I). Note that T1(AB)1S1 \u2208 Rt1\u00d7s1 and (AB)1 \u2208 Rn\u00d7nd, for each i \u2208 [t1], j \u2208 [s1],\n(T1(AB)1S1)i,j =\nn\u2211\nx=1\nnd\u2211\ny\u2032=1\n(T1)i,x((AB)1)x,y\u2032(S1)y\u2032,j\n= n\u2211\nx=1\nn\u2211\ny=1\nd\u2211\nz=1\n(T1)i,x((AB)1)x,(y\u22121)d+z(S1)(y\u22121)d+z,j\n= n\u2211\nx=1\nn\u2211\ny=1\nd\u2211\nz=1\n(T1)i,x(AB)x,y,z(S1)(y\u22121)d+z,j\n=\nn\u2211\nx=1\nn\u2211\ny=1\nd\u2211\nz=1\n(T1)i,x\nn\u2211\nw=1\n(Ax,y,wBw,z)(S1)(y\u22121)d+z,j\n= n\u2211\nx=1\nn\u2211\ny=1\n(T1)i,x\nn\u2211\nw=1\nAx,y,w\nd\u2211\nz=1\nBw,z(S1)(y\u22121)d+z,j .\nWe look at a non-zero entry Ax,y,w and the entry Bw,z. If T1 denotes a sparse Cauchy transform or a sampling and rescaling matrix according to the Lewis weights, then there is at most one pair (i, j) such that (T1)i,xAx,y,wBw,z(S1)(y\u22121)d+z,j is non-zero. Therefore, computing T1(AB)1S1 only needs nnz(A)d time. If T1 is not in the above case, since S1 is sparse, we can compute (AB)1S1 in nnz(A)d time by a similar argument. Then, we can compute T1(AB)1S1 in nt1s1 time.\nPart (II). It is as the same as Part (I).\nPart (III). Note that T3(AB)3S3 \u2208 Rt3\u00d7s3 and (AB)3 \u2208 Rd\u00d7n2 . For each i \u2208 [t3], j \u2208 [s3],\n(T3(AB)3S3)i,j = d\u2211\nx=1\nn2\u2211\ny\u2032=1\n(T3)i,x((AB)3)x,y\u2032(S3)y\u2032,j\n= d\u2211\nx=1\nn\u2211\ny=1\nn\u2211\nz=1\n(T3)i,x((AB)3)x,(y\u22121)n+z(S3)(y\u22121)n+z,j\n=\nd\u2211\nx=1\nn\u2211\ny=1\nn\u2211\nz=1\n(T3)i,x(AB)y,z,x(S3)(y\u22121)n+z,j\n= d\u2211\nx=1\nn\u2211\ny=1\nn\u2211\nz=1\n(T3)i,x\nn\u2211\nw=1\nAy,z,wBw,x(S3)(y\u22121)n+z,j\nSimilar to Part (I), if T1 denotes a sparse Cauchy transform or a sampling and rescaling matrix according to the Lewis weights, computing T3(AB)3S3 only needs nnz(A)d time. Otherwise, it needs dt3s3 + nnz(A)d running time."}, {"heading": "F.3.6 Algorithms", "text": "Algorithm 34 `1-`1-`2-Low Rank Approximation algorithm, input sparsity time 1: procedure L112TensorLowRankApproxInputSparsity(A,n, k) . Theorem F.35 2: n\u2190 O(n). 3: s1 \u2190 s2 \u2190 s3 \u2190 O\u0303(k5). 4: Choose S \u2208 Rn\u00d7n to be a Gaussian matrix. 5: Choose S1 \u2208 Rnn\u00d7s1 to be a sparse Cauchy transform. . Part (II) of Theorem F.33 6: Choose S2 \u2208 Rnn\u00d7s2 to be a sparse Cauchy transform. 7: Choose S3 \u2208 Rn2\u00d7s3 to be a sparse Cauchy transform. 8: Form A\u0302 = AS. 9: Compute A\u03021S1, A\u03022S2, and A\u03023S3 10: Y1, Y2, Y3, C \u2190L1PolyKSizeReduction(A\u0302, A\u03021S1, A\u03022S2, A\u03023S3, n, n, n, s1, s2, s3, k) . Algorithm 21 11: Create s1k + s2k + s3k variables for each entry of X1, X2, X3. 12: Form objective function \u2016(Y1X1)\u2297 (Y2X2)\u2297 (Y3X3)\u2212 C\u20162F . 13: Run polynomial system verifier. 14: return A1S1X1, A2S2X2, A3S3X3 15: end procedure\nTheorem F.35. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm which takes O(nnz(A)n) + O\u0303(n) poly(k) + n2O\u0303(k2) time and outputs three matrices U, V,W \u2208 Rn\u00d7k such that,\n\u2016U \u2297 V \u2297W \u2212A\u2016u \u2264 poly(k, log n) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u2016u,\nholds with probability at least 9/10.\nProof. We first choose a Gaussian matrix S \u2208 Rn\u00d7n with n = O(n). By applying Corollary F.32, we can reduce the original problem to a \u201cgeneralized\u201d `1 low rank approximation problem. Next, we use the existence results (Theorem F.33) and polynomial in k size reduction (Lemma D.8). At the end, we relax the `1-norm objective function to a Frobenius norm objective function (Fact D.1).\nAlgorithm 35 `1-`1-`2-Low Rank Approximation Algorithm, O\u0303(k2/3)\n1: procedure L112TensorLowRankApproxK(A,n, k) . Theorem F.36 2: n\u2190 O(n). 3: s1 \u2190 s2 \u2190 s3 \u2190 O\u0303(k). 4: Choose S \u2208 Rn\u00d7n to be a Gaussian matrix. 5: Guess a diagonal matrix S1 \u2208 Rnn\u00d7s1 with s1 nonzero entries. . Part (III) of Theorem F.33 6: Guess a diagonal matrix S2 \u2208 Rnn\u00d7s2 with s2 nonzero entries. 7: Guess a diagonal matrix S3 \u2208 Rn2\u00d7s3 with s3 nonzero entries. 8: Form A\u0302 = AS. 9: Compute A\u03021S1, A\u03022S2, and A\u03023S3 10: Y1, Y2, Y3, C \u2190L1PolyKSizeReduction(A\u0302, A\u03021S1, A\u03022S2, A\u03023S3, n, n, n, s1, s2, s3, k) . Algorithm 21 11: Create s1k + s2k + s3k variables for each entry of X1, X2, X3. 12: Form objective function \u2016(Y1X1)\u2297 (Y2X2)\u2297 (Y3X3)\u2212 C\u20161. 13: Run polynomial system verifier. 14: return A1S1X1, A2S2X2, A3S3X3 15: end procedure\nTheorem F.36. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, there exists an algorithm which takes nO\u0303(k)2O\u0303(k3) time and outputs three matrices U, V,W \u2208 Rn\u00d7k such that,\n\u2016U \u2297 V \u2297W \u2212A\u2016u \u2264 O(k3/2) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u2016u,\nholds with probability at least 9/10.\nProof. We first choose a Gaussian matrix S \u2208 Rn\u00d7n with n = O(n). By applying Corollary F.32, we can reduce the original problem to a \u201cgeneralized\u201d `1 low rank approximation problem. Next, we use the existence results (Theorem F.33) and polynomial in k size reduction (Lemma D.8). At the end, we solve an entry-wise `1 norm objective function directly.\nTheorem F.37. Given a 3rd order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, let r = O\u0303(k2). There is an algorithm which takes O(nnz(A)n) + O\u0303(n) poly(k) time and outputs three matrices U, V,W \u2208 Rn\u00d7r such that\n\u2016U \u2297 V \u2297W \u2212A\u2016u \u2264 poly(log n, k) min rank\u2212k Ak \u2016Ak \u2212A\u2016u,\nholds with probability at least 9/10.\nProof. We first choose a Gaussian matrix S \u2208 Rn\u00d7n with n = O(n). By applying Corollary F.32, we can reduce the original problem to a \u201cgeneralized\u201d `1 low rank approximation problem. Next, we use the existence results (Theorem F.33) and polynomial in k size reduction (Lemma D.8). At the end, we solve an entry-wise `1 norm objective function directly.\nAlgorithm 36 `1-`1-`2-Low Rank Approximation Algorithm, Bicriteria Algorithm 1: procedure L112TensorLowRankApproxBicteriteria(A,n, k) . Theorem F.37 2: n\u2190 O(n). 3: s2 \u2190 s3 \u2190 O\u0303(k5). 4: t2 \u2190 t3 \u2190 O\u0303(k). 5: r \u2190 s2s3. 6: Choose S \u2208 Rn\u00d7n to be a Gaussian matrix. 7: Form A\u0302 = AS \u2208 Rn\u00d7n\u00d7n. 8: Choose a sketching matrix S2 \u2208 Rnn\u00d7s2 with s2 nonzero entries (Sparse Cauchy transform),\nfor each i \u2208 {2, 3}. . Part (II) of Theorem F.33 9: Choose a sampling and rescaling diagonal matrix Di according to the Lewis weights of A\u0302iSi\nwith ti nonzero entries, for each i \u2208 {2, 3}. 10: Form V\u0302 \u2208 Rn\u00d7r by setting the (i, j)-th column to be (A\u03022S2)i. 11: Form W\u0302 \u2208 Rn\u00d7r by setting the (i, j)-th column to be (A3S3)j . 12: Form matrix B \u2208 Rr\u00d7t2t3 by setting the (i, j)-th column to be the vectorization of\n(T2A\u03022S2)i \u2297 (T3A\u03023S3)j . 13: Solve minU \u2016U \u00b7B \u2212 (A\u0302(I, T2, T3))1\u20161. 14: return U\u0302 , V\u0302 , W\u0302 15: end procedure"}, {"heading": "G Weighted Frobenius Norm for Arbitrary Tensors", "text": "This section presents several tensor algorithms for the weighted case. For notational purposes, instead of using U, V,W to denote the ground truth factorization, we use U1, U2, U3 to denote the ground truth factorization. We use A to denote the input tensor, and W to denote the tensor of weights. Combining our new tensor techniques with existing weighted low rank approximation algorithms [RSW16] allows us to obtain several interesting new results. We provide some necessary definitions and facts in Section G.1. Section G.2 provides an algorithm when W has at most r distinct faces in each dimension. Section G.3 studies relationships between r distinct faces and r distinct columns. Finally, we provides an algorithm with a similar running time but weaker assumption, where W has at most r distinct columns and r distinct rows in Section G.4. The result in Theorem G.2 is fairly similar to Theorem G.5, except for the running time. We only put a very detailed discussion in the statement of Theorem G.5. Note that Theorem G.2 also has other versions which are similar to the Frobnius norm rank-k algorithms described in Section 1. For simplicity of presentation, we only present one clean and simple version (which assumes Ak exists and has factor norms which are not too large)."}, {"heading": "G.1 Definitions and Facts", "text": "For a matrix A \u2208 Rn\u00d7m and a weight matrix W \u2208 Rn\u00d7m, we define \u2016W \u25e6A\u2016F as follows,\n\u2016W \u25e6A\u2016F =\n  n\u2211\ni=1\nm\u2211\nj=1\nW 2i,jA 2 i,j\n  1 2\n.\nFor a tensor A \u2208 Rn\u00d7n\u00d7n and a weight tensor W \u2208 Rn\u00d7n\u00d7n, we define \u2016W \u25e6A\u2016F as follows,\n\u2016W \u25e6A\u2016F =\n  n\u2211\ni=1\nn\u2211\nj=1\nn\u2211\nl=1\nW 2i,j,lA 2 i,j,l\n  1 2\n.\nFor three matrices A \u2208 Rn\u00d7m, U \u2208 Rn\u00d7k, V \u2208 Rk\u00d7m and a weight matrix W , from one perspective, we have\n\u2016(UV \u2212A) \u25e6W\u20162F = n\u2211\ni=1\n\u2016(U iV \u2212Ai) \u25e6W i\u201622 = n\u2211\ni=1\n\u2016(U iV \u2212Ai)DW i\u201622,\nwhere W i denote the i-th row of matrix W , and DW i \u2208 Rm\u00d7m denotes a diagonal matrix where the j-th entry on diagonal is the j-th entry of vector W i. From another perspective, we have\n\u2016(UV \u2212A) \u25e6W\u20162F = m\u2211\nj=1\n\u2016(UVj \u2212Aj) \u25e6Wj\u201622 = m\u2211\nj=1\n\u2016(UVj \u2212Aj)DWj\u201622,\nwhere Wj denotes the j-th column of matrix W , and DWj \u2208 Rn\u00d7n denotes a diagonal matrix where the i-th entry on the diagonal is the i-th entry of vector Wj .\nOne of the key tools we use in this section is,\nLemma G.1 (Cramer\u2019s rule). Let R be an n\u00d7 n invertible matrix. Then, for each i \u2208 [n], j \u2208 [n],\n(R\u22121)ji = det(R \u00aci \u00acj)/det(R),\nwhere R\u00aci\u00acj is the matrix R with the i-th row and the j-th column removed.\nG.2 r distinct faces in each dimension\nNotice that in the matrix case, it is sufficient to assume that \u2016A\u2032\u2016F is upper bounded [RSW16]. Once we have that \u2016A\u2032\u2016F is bounded, without loss of generality, we can assume that U\u22171 is an orthonormal basis[CW15a, RSW16]. If U\u22171 is not an orthonormal basis, then let U \u20321R denote a QR factorization of U\u22171 , and then write U \u20322 = RU\u22172 . However, in the case of tensors we have to assume that each factor \u2016U\u2217i \u2016F is upper bounded due to border rank issues (see, e.g., [DSL08]).\nTheorem G.2. Given a 3rd order n\u00d7n\u00d7n tensor A and an n\u00d7n\u00d7n tensor W of weights with r distinct faces in each of the three dimensions for which each entry can be written using O(n\u03b4) bits, for \u03b4 > 0, define OPT = infrank\u2212k Ak\u2016W \u25e6 (Ak \u2212A)\u20162F . Let k \u2265 1 be an integer and let 0 < < 1.\nIf OPT > 0, and there exists a rank-k Ak = U\u22171 \u2297U\u22172 \u2297U\u22173 tensor (with size n\u00d7n\u00d7n) such that \u2016W \u25e6 (Ak \u2212 A)\u20162F = OPT, and maxi\u2208[3] \u2016U\u2217i \u2016F \u2264 2O(n\n\u03b4), then there exists an algorithm that takes (nnz(A) + nnz(W ) + n2O\u0303(rk\n2/ ))nO(\u03b4) time in the unit cost RAM model with words of size O(log n) bits10 and outputs three n\u00d7 k matrices U1, U2, U3 such that\n\u2016W \u25e6 (U1 \u2297 U2 \u2297 U3 \u2212A)\u20162F \u2264 (1 + ) OPT (71)\nholds with probability 9/10. 10The entries of A and W are assumed to fit in n\u03b4 words.\nAlgorithm 37 Weighted Tensor Low-rank Approximation Algorithm when the Weighted Tensor has r Distinct Faces in Each of the Three Dimensions. procedure WeightedRDistinctFacesIn3Dimensions(A,W, n, r, k, ) . Theorem G.2\nfor j = 1\u2192 3 do sj \u2190 O(k/ ). Choose a sketching matrix Sj \u2208 Rn2\u00d7sj . for i = 1\u2192 r do\nCreate k \u00d7 s1 variables for matrix Pi,j \u2208 Rk\u00d7sj . end for for i = 1\u2192 n do\nWrite down (U\u0302j)i = A j iDW j1 SjP > j,i(Pj,iP > j,i) \u22121.\nend for end for Form \u2016W \u25e6 (U\u03021 \u2297 U\u03022 \u2297 U\u03023 \u2212A)\u20162F . Run polynomial system verifier. return U1, U2, U3\nend procedure\nProof. Note thatW has r distinct columns, rows, and tubes. Hence, each of the matricesW1,W2,W3 \u2208 Rn\u00d7n2 has at most r distinct columns, and at most r distinct rows. Let U\u22171 , U\u22172 , U\u22173 \u2208 Rn\u00d7k denote the matrices satisfying \u2016W \u25e6 (U\u22171 \u2297 U\u22172 \u2297 U\u22173 \u2212 A)\u20162F = OPT. We fix U\u22172 and U\u22173 , and consider a flattening of the tensor along the first dimension,\nmin U1\u2208Rn\u00d7k\n\u2016(U1Z1 \u2212A1) \u25e6W1\u20162F = OPT,\nwhere matrix Z1 = U\u2217>2 U\u2217>3 has size k\u00d7n2 and for each i \u2208 [k] the i-th row of Z1 is vec((U\u22172 )i\u2297 (U\u22173 )i). For each i \u2208 [n], let W i1 denote the i-th row of n\u00d7 n2 matrix W1. For each i \u2208 [n], let DW i1 denote the diagonal matrix of size n2\u00d7 n2, where each diagonal entry is from the vector W i1 \u2208 Rn\n2 . Without loss of generality, we can assume the first r rows of W1 are distinct. We can rewrite the objective function along the first dimension as a sum of multiple regression problems. For any n\u00d7k matrix U1,\n\u2016(U1Z1 \u2212A1) \u25e6W1\u20162F = n\u2211\ni=1\n\u2016U i1Z1DW i1 \u2212A i 1DW i1 \u201622. (72)\nBased on the observation thatW1 has r distinct rows, we can group the n rows ofW 1 into r groups. We use g1,1, g1,2, \u00b7 \u00b7 \u00b7 , g1,r to denote r sets of indices such that, for each i \u2208 g1,j , W i1 = W j1 . Thus we can rewrite Equation (72),\n\u2016(U1Z1 \u2212A1) \u25e6W1\u20162F = n\u2211\ni=1\n\u2016U i1Z1DW i1 \u2212A i 1DW i1 \u201622\n=\nr\u2211\nj=1\n\u2211\ni\u2208g1,j\n\u2016U i1Z1DW i1 \u2212A i 1DW i1 \u201622.\nWe can sketch the objective function by choosing Gaussian matrices S1 \u2208 Rn2\u00d7s1 with s1 = O(k/ ). n\u2211\ni=1\n\u2016U i1Z1DW i1S1 \u2212A i 1DW i1 S1\u201622.\nLet U\u03021 denote the optimal solution of the sketch problem,\nU\u03021 = arg min U1\u2208Rn\u00d7k\nn\u2211\ni=1\n\u2016U i1Z1DW i1S1 \u2212A i 1DW i1 S1\u201622.\nBy properties of S1([RSW16]), plugging U\u0302 \u2208 Rn\u00d7k into the original problem, we obtain, n\u2211\ni=1\n\u2016U\u0302 i1Z1DW i1 \u2212A i 1DW i1 \u201622 \u2264 (1 + ) OPT .\nNote that U\u03021 \u2208 Rn\u00d7k also has the following form. For each i \u2208 [n],\nU\u0302 i1 = A i 1DW i1 S1(Z1DW i1 S1) \u2020\n= Ai1DW i1 S1(Z1DW i1 S1) >((Z1DW i1 S1)(Z1DW i1 S1) >)\u22121.\nNote that W1 has r distinct rows. Thus, we only have r distinct DW i1 . This implies that there are r distinct matrices Z1DW i1S1 \u2208 R\nk\u00d7s1 . Using the definition of g1,j , for j \u2208 [r], for each i \u2208 g1,j \u2282 [n], we have\nU\u0302 i1 = A i 1DW i1 S1(Z1DW i1 S1) \u2020\n= Ai1DW j1 S1(Z1DW j1 S1) \u2020 by W i1 = W j 1 ,\nwhich means we only need to write down r different Z1DW j1S1. For each k \u00d7 s1 matrix Z1DW j1S1, we create k \u00d7 s1 variables to represent it. Thus, we need to create rks1 variables to represent r matrices,\n{Z1DW 11 S1, Z1DW 21 S1, \u00b7 \u00b7 \u00b7 , Z1DW r1 S1}.\nFor simplicity, let P1,i \u2208 Rk\u00d7s1 denote Z1DW i1S1. Then we can rewrite U\u0302 i \u2208 Rk as follows,\nU\u0302 i1 = A i 1DW i1 S1P > 1,i(P1,iP > 1,i) \u22121.\nIf P1,iP>1,i \u2208 Rk\u00d7k has rank k, then we can use Cramer\u2019s rule (Lemma G.1) to write down the inverse of P1,iP>1,i. However, vector W i 1 could have many zero entries. Then the rank of P1,iP>1,i can be smaller than k. There are two different ways to solve this issue. One way is by using the argument from [RSW16], which allows us to assume that P1,iP>1,i \u2208 Rk\u00d7k has rank k. The other way is straightforward: we can guess the rank. There are k possibilities. Let ti \u2264 k denote the rank of P1,i. Then we need to figure out a maximal linearly independent subset of rows of P1,i. There are 2O(k) possibilities. Next, we need to figure out a maximal linearly independent subset of columns of P1,i. We can also guess all the possibilities, which is at most 2O(k). Because we have r different P1,i, the total number of guesses we have is at most 2O(rk). Thus, we can write down (P1,iP>1,i)\n\u22121 according to Cramer\u2019s rule. After U\u03021 is obtained, we will fix U\u03021 and U\u22173 in the next round. We consider the flattening of the\ntensor along the second direction,\nmin U2\u2208Rn\u00d7k\n\u2016(U2Z2 \u2212A2) \u25e6W2\u20162F ,\nwhere n\u00d7n2 matrix A2 is obtained by flattening tensor A along the second dimension, k\u00d7n2 matrix Z2 denotes U\u0302>1 U\u2217>3 , and n\u00d7 n2 matrix W2 is obtained by flattening tensor W along the second dimension. For each i \u2208 [n], let W i2 denote the i-th row of n \u00d7 n2 matrix W2. For each i \u2208 [n], let DW i1 denote the diagonal matrix which has size n\n2 \u00d7 n2 and for which each entry is from vector W i2 \u2208 Rn\n2 . Without loss of generality, we can assume the first r rows of W2 are distinct. We can rewrite the objective function along the second dimension as a sum of multiple regression problems. For any n\u00d7 k matrix U2,\n\u2016(U2Z2 \u2212A2) \u25e6W2\u20162F = n\u2211\ni=1\n\u2016U i2Z2DW i2 \u2212A i 2DW i2 \u201622. (73)\nBased on the observation thatW2 has r distinct rows, we can group the n rows ofW 2 into r groups. We use g2,1, g2,2, \u00b7 \u00b7 \u00b7 , g2,r to denote r sets of indices such that, for each i \u2208 g2,j , W i2 = W j2 . Thus we obtain,\n\u2016(U2Z2 \u2212A2) \u25e6W2\u20162F = n\u2211\ni=1\n\u2016U i2Z2DW i2 \u2212A i 2DW i2 \u201622\n=\nr\u2211\nj=1\n\u2211\ni\u2208g2,j\n\u2016U i2Z2DW i2 \u2212A i 2DW i2 \u201622.\nWe can sketch the objective function by choosing a Gaussian sketch S2 \u2208 Rn2\u00d7s2 with s2 = O(k/ ). Let U\u03022 denote the optimal solution to the sketch problem. Then U\u03022 has the form, for each i \u2208 [n],\nU\u0302 i2 = A i 2DW i2 S2(Z2DW i2 S2) \u2020.\nSimilarly as before, we only need to write down r different matrices Z2DW i2S1, and for each of them, create k \u00d7 s2 variables. Let P2,i \u2208 Rk\u00d7s2 denote Z2DW i2S2. By our guessing argument, we can obtain U\u03022.\nIn the last round, we fix U\u03021 and U\u03022. We then write down U\u03023. Overall, by creating l = O(rk2/ ) variables, we have rational polynomials U\u03021(x), U\u03022(x), U\u03023(x). Putting it all together, we can write this objective function,\nmin x\u2208Rl\n\u2016(U\u03021(x)\u2297 U\u03022(x)\u2297 U\u03023(x)\u2212A) \u25e6W\u20162F .\ns.t. h1,i(x) 6= 0, \u2200i \u2208 [r]. h2,i(x) 6= 0, \u2200i \u2208 [r]. h3,i(x) 6= 0, \u2200i \u2208 [r].\nwhere h1,i(x) denotes the denominator polynomial related to a full rank sub-block of P1,i(x). By a perturbation argument in Section 4 in [RSW16], we know that the h1,i(x) are nonzero. By a similar argument as in Section 5 in [RSW16], we can show a lower bound on the cost of the denominator polynomial h1,i(x). Thus we can create new bounded variables xl+1, \u00b7 \u00b7 \u00b7 , x3r+l to rewrite the objective function,\nmin x\u2208Rl+3r q(x)/p(x).\ns.t. h1,i(x)xl+i = 0, \u2200i \u2208 [r]. h2,i(x)xl+r+i = 0, \u2200i \u2208 [r]. h3,i(x)xl+2r+i = 0,\u2200i \u2208 [r].\np(x) =\nr\u220f\ni=1\nh21,i(x)h 2 2,i(x)h 2 3,i(x)\nNote that the degree of the above system is poly(kr) and all the equality constraints can be merged into one single constraint. Thus, the number of constraints is O(1). The number of variables is O(rk2/ ).\nUsing Theorem B.11 and a similar argument from Section 5 of [RSW16], we have that the minimum nonzero cost is at least 2\u2212n\u03b42O\u0303(rk\n2/ ) . Combining the binary search explained in Section C(similar techniques also can be found in Section 6 of [RSW16]) with the lower bound we obtained, we can find the solution for the original problem in time,\n(nnz(A) + nnz(W ) + n2O\u0303(rk 2/ ))nO(\u03b4).\nG.3 r distinct columns, rows and tubes\nLemma G.3. Let W \u2208 Rn\u00d7n\u00d7n denote a tensor that has r distinct columns and r distinct rows, then W has (I) r distinct column-tube faces. (II) r distinct row-tube faces.\nProof. Proof of Part (I). Without loss of generality, we consider the first (which is the bottom one) column-row face. Assume it has r distinct rows and r distinct columns. We can re-order all the column-tube faces to make sure that all the n columns in the bottom face have been split into r continuous disjoint groups Ci, e.g., {C1, C2, \u00b7 \u00b7 \u00b7 , Cr} = [n]. Next, we can re-order all the row-tube faces to make sure that all the n rows in the bottom face have been split into r continuous disjoint groups Ri, e.g., {R1, R2, \u00b7 \u00b7 \u00b7 , Rr} = [n]. Thus, the new bottom face can be regarded as r\u00d7r groups, and the number in each position of the same group is the same.\nSuppose that the tensor has r+ 1 distinct column-tube faces. By the pigeonhole principle there exist two different column-tube faces belonging to the same group Ci, for some i \u2208 [r]. Note that these two column-tube faces are the same by looking at the bottom (column-row) face. Since they are distinct faces, there must exist one row vector v which is not in the bottom (column-row) face, and it has a different value in coordinates belong to group Ci. Note that, considering the bottom face, for each row vector, it has the same value over coordinates belonging to group Ci. But v has different values in coordinates belong to group Ci. Also, note that the bottom (column-row) face also has r distinct rows, and v is not one of them. This means there are at least r + 1 distinct rows, which contradicts that there are r distinct rows in total. Thus, there are at most r distinct column-tube faces.\nProof of Part (II). It is similar to Part (I).\nCorollary G.4. Let W \u2208 Rn\u00d7n\u00d7n denote a tensor that has r distinct columns, r distinct rows, and r distinct rubes. Then W has r distinct column-tube faces, r distinct row-tube faces, and r distinct column-row faces.\nProof. This follows by applying Lemma G.3 twice.\nThus, we obtain the same result as in Theorem G.2 by changing the assumption from r distinct faces in each dimension to r distinct columns, r distinct rows and r distinct tubes."}, {"heading": "G.4 r distinct columns and rows", "text": "The main difference between Theorem G.2 and Theorem G.5 is the running time. The first one takes 2O\u0303(rk2/ ) time and the second one is slightly longer, 2O\u0303(r2k2/ ). By Lemma G.3, r distinct columns in two dimensions implies r distinct faces in two of the three kinds of faces. Thus, the following theorem also holds for r distinct columns in two dimensions.\nAlgorithm 38 Weighted Tensor Low-rank Approximation Algorithm when the Weighted Tensor has r Distinct Faces in Each of the Two Dimensions. procedure WeightedRDistinctFacesIn2Dimensions(A,W, n, r, k, ) . Theorem G.5\nfor j = 1\u2192 3 do sj \u2190 O(k/ ). Choose a sketching matrix Sj \u2208 Rn2\u00d7sj . if j 6= 3 then\nfor i = 1\u2192 r do Create k \u00d7 s1 variables for matrix Pi,j \u2208 Rk\u00d7sj .\nend for end if for i = 1\u2192 n do\nWrite down (U\u0302j)i = A j iDW j1 SjP > j,i(Pj,iP > j,i) \u22121.\nend for end for Form \u2016W \u25e6 (U\u03021 \u2297 U\u03022 \u2297 U\u03023 \u2212A)\u20162F . Run polynomial system verifier. return U1, U2, U3\nend procedure\nTheorem G.5. Given a 3rd order n\u00d7n\u00d7n tensor A and an n\u00d7n\u00d7n tensor W of weights with r distinct faces in two dimensions (out of three dimensions) such that each entry can be written using O(n\u03b4) bits for some \u03b4 > 0, define OPT = infrank\u2212k Ak\u2016W \u25e6 (Ak \u2212 A)\u20162F . For any k \u2265 1 and any 0 < < 1.\n(I) If OPT > 0, and there exists a rank-k Ak = U\u22171 \u2297U\u22172 \u2297U\u22173 tensor (with size n\u00d7 n\u00d7 n) such that \u2016W \u25e6 (Ak \u2212 A)\u20162F = OPT, and maxi\u2208[3] \u2016U\u2217i \u2016F \u2264 2O(n\n\u03b4), then there exists an algorithm that takes (nnz(A) + nnz(W ) + n2O\u0303(r2k2/ ))nO(\u03b4) time in the unit cost RAM model with words of size O(log n) bits11 and outputs three n\u00d7 k matrices U1, U2, U3 such that\n\u2016W \u25e6 (U1 \u2297 U2 \u2297 U3 \u2212A)\u20162F \u2264 (1 + ) OPT (74)\nholds with probability 9/10. (II) If OPT > 0, Ak does not exist, and there exist three n \u00d7 k matrices U \u20321, U \u20322, U \u20323 where each entry can be written using O(n\u03b4) bits and \u2016W \u25e6 (U \u20321 \u2297 U \u20322 \u2297 U \u20323 \u2212 A)\u20162F \u2264 (1 + /2) OPT, then we can find U, V,W such that (74) holds.\n11The entries of A and W are assumed to fit in n\u03b4 words.\n(III) If OPT = 0, Ak exists, and there exists a solution U\u22171 , U \u2217 2 , U \u2217 3 such that each entry of the matrix can be written using O(n\u03b4) bits, then we can obtain (74). (IV) If OPT = 0, and there exist three n \u00d7 k matrices U1, U2, U3 such that maxi\u2208[3] \u2016U\u2217i \u2016F \u2264 2O(n \u03b4) and\n\u2016W \u25e6 (U1 \u2297 U2 \u2297 U3 \u2212A)\u20162F \u2264 (1 + ) OPT +2\u2212\u2126(n \u03b4), (75)\nthen we can output U1, U2, U3 such that (75) holds. (V) Further if Ak exists, we can output a number Z for which OPT \u2264 Z \u2264 (1 + ) OPT. For all the cases, the algorithm succeeds with probability at least 9/10.\nProof. By Lemma G.3, we have W has r distinct column-tube faces and r distinct row-tube faces. By Claim G.7, we know that W has R = 2O(r log r) distinct column-row faces.\nWe use the same approach as in proof of Theorem G.2 (which is also similar to Section 8 of [RSW16]) to create variables, write down the polynomial systems and add not equal constraints. Instead of having 3r distinct denominators as in the proof of Theorem G.2, we have 2r +R.\nWe create l = O(rk2/ ) variables for {Z1DW 11 S1, Z1DW 21 S1, \u00b7 \u00b7 \u00b7 , Z1DW r1 S1}. Then we can write down U\u03021 with r distinct denominators gi(x). Each gi(x) is non-zero in an optimal solution using the perturbation argument in Section 4 in [RSW16]. We create new variables x2l+i to remove the denominators gi(x), \u2200i \u2208 [r]. Then the entries of U\u03021 are polynomials as opposed to rational functions.\nWe create l = O(rk2/ ) variables for {Z2DW 12 S2, Z2DW 22 S2, \u00b7 \u00b7 \u00b7 , Z2DW r2 S2}. Then we can write down U\u03022 with r distinct denominators gr+i(x). Each gr+i(x) is non-zero in an optimal solution using the perturbation argument in Section 4 in [RSW16]. We create new variables x2l+r+i to remove the denominators gr+i(x), \u2200i \u2208 [r]. Then the entries of U\u03022 are polynomials as opposed to rational functions.\nUsing U\u03021 and U\u03022 we can express U\u03023 with R distinct denominators fi(x), which are also non-zero by using the perturbation argument in Section 4 in [RSW16], and using that W3 has at most this number of distinct rows. Finally we can write the following optimization problem,\nmin x\u2208R2l+2r\np(x)/q(x)\ns.t. gi(x)x2l+i \u2212 1 = 0,\u2200i \u2208 [r] gr+i(x)x2l+r+i \u2212 1 = 0,\u2200i \u2208 [r] f2j (x) 6= 0,\u2200j \u2208 [R]\nq(x) =\nR\u220f\nj=1\nf2j (x)\nWe then determine if there exists a solution to the above semi-algebraic set in time\n(poly(k, r)R)O(rk 2/ ) = 2O\u0303(r 2k2/ ).\nUsing similar techniques from Section 5 of [RSW16], we can show a lower bound on the cost similar to Section 8.3 of [RSW16], namely, the minimum nonzero cost is at least\n2\u2212n \u03b42O\u0303(r 2k2/ ) .\nCombining the binary search explained in Section C (a similar techniques also can be found in Section 6 of [RSW16]) with the lower bound we obtained, we can find a solution for the original problem in time\n(nnz(A) + nnz(W ) + n2O\u0303(r 2k2/ ))nO(\u03b4).\nRemark G.6. Note that the running time for the Frobenius norm and for the `1 norm are of the form poly(n) + exp(poly(k/ )) rather than poly(n) \u00b7 exp(k/ ). The reason is, we can use an input sparsity reduction to reduce the size of the objective function from poly(n) to poly(k).\nClaim G.7. Let W \u2208 R denote a third order tensor that has r distinct columns and r distinct rows. Then it has 2O(r log r) distinct column-row faces.\nProof. By similar arguments as in the proof of Lemma G.3, the bottom (column-row) face can be split into r groups C1, C2, \u00b7 \u00b7 \u00b7 , Cr based on r columns, and split into r groups R1, R2, \u00b7 \u00b7 \u00b7 , Rr based on rows. Thus, the bottom (column-row) face can be regarded as having r \u00d7 r groups, and the number in each position of the same group is the same.\nWe can assume that all the r2 blocks in the bottom column-row face have the same size. Otherwise, we can expand the tensor to the situation that all the r2 blocks have the same size. Because this small tensor is a sub-tensor of the big tensor, if the big tensor has at most t distinct column-row faces, then the small tensor has at most t distinct column-row faces.\nBy Lemma G.3, we know that the tensor W has at most r distinct column-tube faces and rowtube faces. Because it has r distinct column-tube faces, then all the faces belonging to coordinates in Cr are the same. Thus, all the columns belonging to Cr and in the second column-row face are the same. Similarly, we have that all the rows belonging to Rr and in the second column-row face are the same. Thus we have that all the entries in block CR \u222a Rr and in the second column-row faces are the same. Further, we can conclude, for every column-row face, for every Ci \u222a Rj block, all the entries in the same block are the same.\nThe next observation is, if there exist r2 +1 different values in the tensor, then there exist either r distinct columns or r distinct rows. Indeed, otherwise since we have r distinct columns, each column has at most r distinct entries given our bound on the nunber of distinct rows. Thus, the r distinct columns could have at most r2 distinct entries in total, a contradiction.\nFor each column-row face, there are at most r2 blocks, and the value in each block can have at most r2 possibilities. Thus, overall we have at most (r2)r2 = 2O(r2 log r) column-row faces.\nBy using different argument, we can improve the above bound. Note that we already show in each column-row face of a tensor, it has r2 blocks, and all the values in each block have to be the same. Since we have r distinct rows, we can fix the those r distinct rows. If we copy row v into one row of Ri, then we have to copy row v into every row of Ri. This is because if Ri contains two distinct rows, then there must exist a block Cj for which the entries in block Ri \u222aCj are not all the same. Thus, for each row group, all the rows in that group are the same.\nNow, for each column-row face, consider the leftmost r blocks, R1 \u222a C1, R2 \u222a C1, \u00b7 \u00b7 \u00b7 , Rr \u222a C1. There are at most r possible values in each block, because we have r distinct rows in total. Overall the total number of possibilities for the leftmost r blocks is at most (r)r = 2O(r log r). Once the leftmost r blocks are determined, the remaining r(r \u2212 1) are also determined. This completes the proof.\nAlso, notice that there is an example that has 2\u2126(r log r) distinct column-row faces. For the bottom column-row faces, there are r \u00d7 r blocks for which all the blocks have the same size, the blocks on the diagonal have all 1s, and all the other blocks contain 0s everywhere. For the later column-row faces, we can arbitrarily permute this block diagonal matrix, and the total number of possibilities is \u2126(r!) \u2265 2\u2126(r log r)."}, {"heading": "H Hardness", "text": "We first provide definitions and results for some fundamental problems in Section H.1. Section H.2 presents our hardness result for the symmetric tensor eigenvalue problem. Section H.3 presents our hardness results for symmetric tensor singular value problems, computing tensor spectral norm, and rank-1 approximation. We improve H\u00e5stad\u2019s NP-hardness[H\u00e5s90] result for tensor rank in Section H.4. We also show a better hardness result for robust subspace approximation in Section H.5. Finally, we discuss several other tensor hardness results that are implied by matrix hardness results in Section H.6."}, {"heading": "H.1 Definitions", "text": "We first provide the definitions for 3SAT , ETH , MAX-3SAT , MAX-E3SAT and then state some fundamental results related to those definitions.\nDefinition H.1 (3SAT problem). Given n variables and m clauses in a conjunctive normal form CNF formula with the size of each clause at most 3, the goal is to decide whether there exists an assignment to the n Boolean variables to make the CNF formula be satisfied.\nHypothesis H.2 (Exponential Time Hypothesis (ETH) [IPZ98]). There is a \u03b4 > 0 such that the 3SAT problem defined in Definition H.1 cannot be solved in O(2\u03b4n) time.\nDefinition H.3 (MAX-3SAT). Given n variables andm clauses, a conjunctive normal form CNF formula with the size of each clause at most 3, the goal is to find an assignment that satisfies the largest number of clauses.\nWe use MAX-E3SAT to denote the version of MAX-3SAT where each clause contains exactly 3 literals.\nTheorem H.4 ([H\u00e5s01]). For every \u03b4 > 0, it is NP-hard to distinguish a satisfiable instance of MAX-E3SAT from an instance where at most a 7/8 + \u03b4 fraction of the clauses can be simultaneously satisfied.\nTheorem H.5 ([H\u00e5s01, MR10]). Assume ETH holds. For every \u03b4 > 0, there is no 2o(n1\u2212o(1)) time algorithm to distinguish a satisfiable instance of MAX-E3SAT from an instance where at most a fraction 7/8 + \u03b4 of the clauses can be simultaneously satisfied.\nWe use MAX-E3SAT(B) to denote the restricted special case of MAX-3SAT where every variable occurs in at most B clauses. H\u00e5stad [H\u00e5s00] proved that the problem is approximable to within a factor 7/8 + 1/(64B) in polynomial time, and that it is hard to approximate within a factor 7/8 + 1/(logB)\u2126(1). In 2001, Trevisan improved the hardness result, Theorem H.6 ([Tre01]). Unless RP=NP, there is no polynomial time (7/8 + 5/ \u221a B)-approximate algorithm for MAX-E3SAT(B) . Theorem H.7 ([H\u00e5s01, Tre01, MR10]). Unless ETH fails, there is no 2o(n1\u2212o(1)) time (7/8+5/ \u221a B)approximate algorithm for MAX-E3SAT(B) .\nTheorem H.8 ([LMS11]). Unless ETH fails, there is no 2o(n) time algorithm for the Independent Set problem.\nDefinition H.9 (MAX-CUT decision problem). Given a positive integer c\u2217 and an unweighted graph G = (V,E) where V is the set of vertices of G and E is the set of edges of G, the goal is to determine whether there is a cut of G that has at least c\u2217 edges.\nNote that Feige\u2019s original assumption[Fei02] states that there is no polynomial time algorithm for the problem in Assumption H.10. We do not know of any better algorithm for the problem in Assumption H.10 and have consulted several experts12 about the assumption who do not know a counterexample to it.\nAssumption H.10 (Random Exponential Time Hypothesis). Let c > ln 2 be a constant. Consider a random 3SAT formula on n variables in which each clause has 3 literals, and in which each of the 8n3 clauses is picked independently with probability c/n2. Then any algorithm which always outputs 1 when the random formula is satisfiable, and outputs 0 with probability at least 1/2 when the random formula is unsatisfiable, must run in 2c\u2032n time on some input, where c\u2032 > 0 is an absolute constant.\nThe 4SAT-version of the above random-ETH assumption has been used in [GL04] and [RSW16] (Assumption 1.3)."}, {"heading": "H.2 Symmetric tensor eigenvalue", "text": "Definition H.11 (Tensor Eigenvalue [HL13]). An eigenvector of a tensor A \u2208 Rn\u00d7n\u00d7n is a nonzero vector x \u2208 Rn such that\nn\u2211\ni=1\nn\u2211\nj=1\nAi,j,kxixj = \u03bbxk,\u2200k \u2208 [n]\nfor some \u03bb \u2208 R, which is called an eigenvalue of A.\nTheorem H.12 ([N+03]). Let G = (V,E) on v vertices have stability number (the size of a maximum independent set) \u03b1(G). Let n = v+ v(v\u22121)2 and S\nn\u22121 = {(x, y) \u2208 Rv\u00d7Rv(v\u22121)/2 : \u2016x\u201622 +\u2016y\u201622 = 1}. Then,\n\u221a 1\u2212 1\n\u03b1(G) = 3 \u221a 3/2 max (x,y)\u2208Sn\u22121\n\u2211\ni<j,(i,j)/\u2208E\nxixjyi,j .\nFor any graph G(V,E), we can construct a symmetric tensor A \u2208 Rn\u00d7n\u00d7n. For any 1 \u2264 i < j < k \u2264 v, let\nAi,j,k = { 1 1 \u2264 i < j \u2264 v, k = v + \u03c6(i, j), (i, j) /\u2208 E, 0 otherwise,\nwhere \u03c6(i, j) = (i\u2212 1)v\u2212 i(i\u2212 1)/2 + j \u2212 i is a lexicographical enumeration of the v(v\u2212 1)/2 pairs i < j. For the other cases i < k < j, \u00b7 \u00b7 \u00b7 , k < j < i, we set\nAi,j,k = Ai,k,j = Aj,i,k = Aj,k,i = Ak,i,j = Ak,j,i.\nIf two or more indices are equal, we set Ai,j,k = 0. Thus tensor T has the following property,\nA(z, z, z) = 6 \u2211\ni<j,(i,j)/\u2208E\nxixjyi,j ,\nwhere z = (x, y) \u2208 Rn. 12Personal communication with Russell Impagliazzo and Ryan Williams.\nThus, we have\n\u03bb = max z\u2208Sn\u22121 A(z, z, z) = max (x,y)\u2208Sn\u22121\n6 \u2211\ni<j,(i,j)/\u2208E\nxixjyi,j .\nFurthermore, \u03bb is the maximum eigenvalue of A.\nTheorem H.13. Unless ETH fails, there is no 2o( \u221a n) time to approximate the largest eigenvalue of an n-dimensional symmetric tensor within (1\u00b1\u0398(1/n)) relative error.\nProof. The additive error is at least\n\u221a 1\u2212 1/v \u2212 \u221a 1\u2212 1/(v \u2212 1) = 1/(v \u2212 1)\u2212 1/v\u221a\n1\u2212 1/v + \u221a 1\u2212 1/(v \u2212 1) & 1/(v \u2212 1)\u2212 1/v \u2265 1/v2.\nThus, the relative error is (1 \u00b1 \u0398(1/v2)). By the definition of n, we know n = \u0398(v2). Assuming ETH , there is no 2o(v) time algorithm to compute the clique number of G. Because the clique number of G is \u03b1(G), there is no 2o(v) time algorithm to compute \u03b1(G). Furthermore, there is no 2o(v) time algorithm to approximate the maximum eigenvalue within (1 \u00b1 \u0398(1/v2)) relative error. Thus, we complete the proof.\nCorollary H.14. Unless ETH fails, there is no polynomial running time algorithm to approximate the largest eigenvalue of an n-dimensional tensor within (1\u00b1\u0398(1/ log2+\u03b3(n))) relative-error, where \u03b3 > 0 is an arbitrarily small constant.\nProof. We can apply a padding argument here. According to Theorem H.13, there is a d-dimensional tensor such that there is no 2o( \u221a d) time algorithm that can give a (1 + \u0398(1/d)) relative error approximation. If we pad 0s everywhere to extend the size of the tensor to n = 2d(1\u2212\u03b3 \u2032)/2 , where \u03b3\u2032 > 0 is a sufficiently small constant, then poly(n) = 2o( \u221a d), so d = log2+O(\u03b3\n\u2032)(n). Thus, it means that there is no polynomial running time algorithm which can output a (1 + 1/(log2+\u03b3))-relative approximation to the tensor which has size n.\nH.3 Symmetric tensor singular value, spectral norm and rank-1 approximation\n[HL13] defines two kinds of singular values of a tensor. In this paper, we only consider the following kind:\nDefinition H.15 (`2 singular value in [HL13]). Given a 3rd order tensor A \u2208 Rn1\u00d7n2\u00d7n3, the number \u03c3 \u2208 R is called a singular value and the nonzero u \u2208 Rn1 ,v \u2208 Rn2,w \u2208 Rn3 are called singular vectors of A if\nn2\u2211\nj=1\nn3\u2211\nk=1\nAi,j,kvjwk = \u03c3ui,\u2200i \u2208 [n1]\nn1\u2211\ni=1\nn3\u2211\nk=1\nAi,j,kuiwk = \u03c3vj ,\u2200j \u2208 [n2]\nn1\u2211\ni=1\nn2\u2211\nj=1\nAi,j,kuivj = \u03c3wk,\u2200k \u2208 [n3].\nDefinition H.16 (Spectral norm [HL13]). The spectral norm of a tensor A is:\n\u2016A\u20162 = sup x,y,z 6=0 |A(x, y, z)| \u2016x\u20162\u2016y\u20162\u2016z\u20162\nNotice that the spectral norm is the absolute value of either the maximum value of A(x,y,z)\u2016x\u20162\u2016y\u20162\u2016z\u20162 or the minimum value of it. Thus, it is an `2-singular value of A. Furthermore, it is the maximum `2-singular value of A.\nTheorem H.17 ([Ban38]). Let A \u2208 Rn\u00d7n\u00d7n be a symmetric 3rd order tensor. Then,\n\u2016A\u20162 = sup x,y,z 6=0\nA(x, y, z)\n\u2016x\u20162\u2016y\u20162\u2016z\u20162 = sup x 6=0 |A(x, x, x)| \u2016x\u201632 .\nIt means that if a tensor is symmetric, then its largest eigenvalue is the same as its largest singular value and its spectral norm. Then, by combining with Theorem H.13, we have the following corollary:\nCorollary H.18. Unless ETH fails,\n1. There is no 2o( \u221a n) time algorithm to approximate the largest singular value of an n-dimensional\nsymmetric tensor within (1 + \u0398(1/n)) relative-error.\n2. There is no 2o( \u221a n) time algorithm to approximate the spectral norm of an n-dimensional sym-\nmetric tensor within (1 + \u0398(1/n)) relative-error.\nBy Corollary H.14, we have:\nCorollary H.19. Unless ETH fails,\n1. There is no polynomial time algorithm to approximate the largest singular value of an ndimensional tensor within (1 + \u0398(1/ log2+\u03b3(n))) relative-error, where \u03b3 > 0 is an arbitrarily small constant.\n2. There is no polynomial time algorithm to approximate the spectral norm of an n-dimensional tensor within (1+\u0398(1/ log2+\u03b3(n))) relative-error, where \u03b3 > 0 is an arbitrarily small constant.\nNow, let us consider Frobenius norm rank-1 approximation.\nTheorem H.20 ([Ban38]). Let A \u2208 Rn\u00d7n\u00d7n be a symmetric 3rd order tensor. Then, min\n\u03c3\u22650,\u2016u\u20162=\u2016v\u20162=\u2016w\u20162=1 \u2016A\u2212 \u03c3u\u2297 v \u2297 w\u2016F = min \u03bb\u22650,\u2016v\u20162=1 \u2016A\u2212 \u03bbv \u2297 v \u2297 v\u2016F .\nFurthermore, the optimal \u03c3 and \u03bb may be chosen to be equal.\nNotice that\n\u2016A\u2212 \u03c3u\u2297 v \u2297 w\u20162F = \u2016A\u20162F \u2212 2\u03c3A(u, v, w) + \u03c32\u2016u\u2297 v \u2297 w\u20162F . Then, if \u2016u\u20162 = \u2016v\u20162 = \u2016w\u20162 = 1, we have:\n\u2016A\u2212 \u03c3u\u2297 v \u2297 w\u20162F = \u2016A\u20162F \u2212 2\u03c3A(u, v, w) + \u03c32. When A(u, v, w) = \u03c3, then the above is minimized.\nThus, we have:\nmin \u03c3\u22650,\u2016u\u20162=\u2016v\u20162=\u2016w\u20162=1\n\u2016A\u2212 \u03c3u\u2297 v \u2297 w\u20162F + \u2016A\u201622 = \u2016A\u20162F .\nIt is sufficient to prove the following theorem:\nTheorem H.21. Given A \u2208 Rn\u00d7n\u00d7n, unless ETH fails, there is no 2o( \u221a n) time algorithm to compute u\u2032, v\u2032, w\u2032 \u2208 Rn such that \u2016A\u2212 u\u2032 \u2297 v\u2032 \u2297 w\u2032\u20162F \u2264 (1 + ) min\nu,v,w\u2208Rn \u2016A\u2212 u\u2297 v \u2297 w\u20162F ,\nwhere = O(1/n2).\nProof. Let A \u2208 Rn\u00d7n\u00d7n be the same hard instance mentioned in Theorem H.12. Notice that each entry of A is either 0 or 1. Thus, minu,v,w\u2208Rn \u2016A \u2212 u \u2297 v \u2297 w\u20162F \u2264 \u2016A\u20162F . Notice that Theorem H.12 also implies that it is hard to distinguish the two cases \u2016A\u20162 \u2264 2 \u221a 2/3 \u00b7 \u221a 1\u2212 1/c or\n\u2016A\u20162 \u2265 2 \u221a 2/3 \u00b7 \u221a 1\u2212 1/(c+ 1) where c is an integer which is no greater than \u221an. So the difference between (2 \u221a 2/3 \u00b7 \u221a 1\u2212 1/c)2 and (2 \u221a 2/3 \u00b7 \u221a 1\u2212 1/(c+ 1))2 is at least \u0398(1/n). Since \u2016A\u20162F is at most n (see construction of A in the proof of Lemma H.12), \u0398(1/n) is an = O(1/n2) fraction of minu,v,w\u2208Rn \u2016A\u2212 u\u2297 v \u2297 w\u20162F . Because\nmin u,v,w\u2208Rn\n\u2016A\u2212 u\u2297 v \u2297 w\u20162F + \u2016A\u201622 = \u2016A\u20162F ,\nif we have a 2o( \u221a n) time algorithm to compute u\u2032, v\u2032, w\u2032 \u2208 Rn such that \u2016A\u2212 u\u2032 \u2297 v\u2032 \u2297 w\u2032\u20162F \u2264 (1 + ) min\nu,v,w\u2208Rn \u2016A\u2212 u\u2297 v \u2297 w\u20162F\nfor = O(1/n2), it will contradict the fact that we cannot distinguish whether \u2016A\u20162 \u2264 2 \u221a\n2/3 \u00b7\u221a 1\u2212 1/c or \u2016A\u20162 \u2265 2 \u221a 2/3 \u00b7 \u221a 1\u2212 1/(c+ 1).\nCorollary H.22. Given A \u2208 Rn\u00d7n\u00d7n, unless ETH fails, for any for which 12 \u2265 \u2265 c/n2 where c is any constant, there is no 2o( \u22121/4) time algorithm to compute u\u2032, v\u2032, w\u2032 \u2208 Rn such that\n\u2016A\u2212 u\u2032 \u2297 v\u2032 \u2297 w\u2032\u20162F \u2264 (1 + ) min u,v,w\u2208Rn \u2016A\u2212 u\u2297 v \u2297 w\u20162F .\nProof. If = \u2126(1/n2), it means that n = \u2126(1/ \u221a ). Then, we can construct a hard instance B with size m \u00d7m \u00d7m where m = \u0398(1/\u221a ), and we can put B into A, and let A have zero entries elsewhere. Since B is hard, i.e., there is no 2o(m\u22121/2) = 2o( \u22121/4) running time to compute a rank-1 approximation to B, this means there is no 2o( \u22121/4) running time algorithm to find an approximate rank-1 approximation to A.\nCorollary H.23. Unless ETH fails, there is no polynomial time algorithm to approximate the best rank-1 approximation of an n-dimensional tensor within (1 + \u0398(1/ log2+\u03b3(n))) relative-error, where \u03b3 > 0 is an arbitrarily small constant.\nProof. We can apply a padding argument here. According to Theorem H.21, there is a d-dimensional tensor such that there is no 2o( \u221a d) time algorithm which can give a (1 + \u0398(1/d4)) relative approximation. Then, if we pad with 0s everywhere to extend the size of the tensor to n = 2d(1\u2212\u03b3 \u2032)/2 where \u03b3\u2032 > 0 is a sufficiently small constant, then poly(n) = 2o( \u221a d), and d4 = log2+O(\u03b3\n\u2032)(n). Thus, it means that there is no polynomial time algorithm which can output a (1+1/(log2+\u03b3))-relative error approximation to the tensor which has size n.\nH.4 Tensor rank is hard to approximate\nThis section presents the hardness result for approximating tensor rank under ETH . According to our new result, we notice that not only deciding the tensor rank is a hard problem, but also approximating the tensor rank is a hard problem. This therefore strengthens H\u00e5stad\u2019s NP-Hadness [H\u00e5s90] for computing tensor rank."}, {"heading": "H.4.1 Cover number", "text": "Before getting into the details of the reduction, we provide a definition of an important concept called the \u201ccover number\u201d and discuss the cover number for the MAX-E3SAT(B) problem.\nDefinition H.24 (Cover number). For any 3SAT instance S with n variables and m clauses, we are allowed to assign one of three values {0, 1, \u2217} to each variable. For each clause, if one of the literals outputs true, then the clause outputs true. For each clause, if the corresponding variable of one of the literals is assigned to \u2217, then the clause outputs true. We say y \u2208 {0, 1}n is a string, and z \u2208 {0, 1, \u2217}n is a star string. For an instance S, if there exists a string y \u2208 {0, 1}n that causes all the clauses to be true, then we say that S is satisfiable, otherwise it is unsatisfiable. For an instance S, let ZS denote the set of star strings which cause all of the clauses of S to be true. For each star string z \u2208 {0, 1, \u2217}n, let star(z) denote the number of \u2217s in the star-string z. We define the \u201ccover number\u201d of instance S to be\ncover-number(S) = min z\u2208ZS star(z).\nNotice that for a satisfiable 3SAT instance S, the cover number p is 0. Also, for any unsatisfiable 3SAT instance S, the cover number p is at least 1. This is because for any input string, there exists at least one clause which cannot be satisfied. To fix that clause, we have to assign \u2217 to a variable\nbelonging to that clause. (Assigning \u2217 to a variable can be regarded as assigning both 0 and 1 to a variable)\nLemma H.25. Let S denote a MAX-E3SAT(B) instance with n variables and m clauses and S suppose S is at most 7/8 +A satisfiable, where A \u2208 (0, 1/8). Then the cover number of S is at least (1/8\u2212A)m/B.\nProof. For any input string y \u2208 {0, 1}n, there exists at least (1/8 \u2212 A)m clauses which are not satisfied. Since each variable appears in at most B clauses, we need to assign \u2217 to at least (1/8 \u2212 A)m/B variables. Thus, the cover number of S is at least (1/8\u2212A)m/B.\nWe say x1, x2, \u00b7 \u00b7 \u00b7 , xn are variables and x1, x1, x2, x2, \u00b7 \u00b7 \u00b7 , xn, xn are literals.\nDefinition H.26. For a list of clauses C and a set of variables P , if for each clause, there exists at least one literal such that the corresponding variable of that literal belongs to P , then we say P covers L.\nH.4.2 Properties of 3SAT instances\nFact H.27. For any 3SAT instance S with n variables and m = \u0398(n) clauses, let c > 0 denote a constant. If S is (1\u2212c)m satisfiable, then let y \u2208 {0, 1}n denote a string for which S has the smallest number of unsatisfiable clauses. Let T denote the set of unsatisfiable clauses and let b denote the number of variables in T . Then \u2126((cm)1/3) \u2264 b \u2264 O(cm).\nProof. Note that in S, there is no duplicate clause. Let T denote the set of unsatisfiable clauses by assigning string y to S. First, we can show that any two literals xi, xi cannot belong to T at the same time. If xi and xi belong to the same clause, then that clause must be an \u201calways\u201d satisfiable clause. If xi and xi belong to different clauses, then one of the clauses must be satisfiable. This contradicts the fact that that clause belongs to T . Thus, we can assume that literals x1, x2, \u00b7 \u00b7 \u00b7 , xb belong to T .\nThere are two extreme cases: one is that each clause only contains three literals and each literal appears in exactly one clause in T . Then b = 3cm. The other case is that each clause contains 3 literals, and each literal appears in as many clauses as possible. Then ( b 3 ) = cm, which gives b = \u0398((cm)1/3).\nLemma H.28. For a random 3SAT instance, with probability 1\u22122\u2212\u2126(logn log logn) there is no literal appearing in at least log n clauses.\nProof. By the property of random 3SAT , for any literal x and any clause C, the probability that x appears in C is 32n , i.e., Pr[x \u2208 C] = 32n = \u0398(1/n). Let p denote this probability. For any literal x,\nthe probability of x appearing in at least log n clauses (out of m clauses) is\nPr[ x appearing in \u2265 log n clauses ]\n=\nm\u2211\ni=logn\n( m\ni\n) pi(1\u2212 p)m\u2212i\n=\nm/2\u2211\ni=logn\n( m\ni\n) pi(1\u2212 p)m\u2212i + m\u2211\ni=m/2\n( m\ni\n) pi(1\u2212 p)m\u2212i\n\u2264 m/2\u2211\ni=logn\n(em/i)ipi + m\u2211\ni=m/2\n( m\ni\n) pi by (1\u2212 p) \u2264 1, ( m\ni\n) \u2264 (em/i)i\n\u2264 (\u0398(1/ log n))logn + 2 \u00b7 (2e)m/2 \u00b7\u0398(1/n)m/2\n\u2264 2\u2212\u2126(logn\u00b7log logn).\nTaking a union bound over all the literals, we complete the proof,\nPr[ @ x appearing in \u2265 log n clauses ] \u2265 1\u2212 2\u2212\u2126(logn log logn).\nLemma H.29. For a sufficiently large constant c\u2032 > 0 and a constant c > 0, for any random 3SAT instance which has n variables and m = c\u2032n clauses, suppose it is (1\u2212 c)m satisfiable. Then with probability 1 \u2212 2\u2212\u2126(logn log logn), for all input strings y, among the unsatisfied clauses, each literal appears in O(log n) places.\nProof. This follows by Lemma H.28.\nNext, we show how to reduce the O(log n) to O(1).\nLemma H.30. For a sufficiently large constant c, for any random 3SAT instance that has n variables and m = cn clauses, for any constant B \u2265 1, b \u2208 (0, 1), with probability at least 1\u2212 9mBbn , there exist at least (1\u2212 b)m clauses such that each variable (in these (1\u2212 b)m clauses) only appears in at most B clauses (out of these (1\u2212 b)m clauses).\nProof. For each i \u2208 [m], we use zi to denote the indicator variable such that it is 1, if for each variable in the ith clause, it appears in at most a clauses. Let B \u2208 [1,\u221e) denote a sufficiently large constant, which we will decide upon later.\nFor each variable x, the probability of it appearing in the i-th clause is 3n . Then we have\nE[ # clauses that contain x] = m\u2211\ni=1\nE[i-th clause contains x] = 3m\nn\nBy Markov\u2019s inequality,\nPr[ # clauses that contain x \u2265 a] \u2264 E[ # clauses that contain x]/B = 3m Bn\nBy a union bound, we can compute E[zi] ,\nE[zi] = Pr[zi = 1]\n\u2265 1\u2212 3 Pr[ one variable in i-th clause appearing \u2265 B clauses ]\n\u2265 1\u2212 9m Bn .\nFurthermore, we have\nE[z] = E[ m\u2211\ni=1\nzi] = m\u2211\ni=1\nE[zi] \u2265 (1\u2212 9m\nBn )m.\nNote that z \u2264 m. Thus E[z] \u2264 m. Let b \u2208 (0, 1) denote a sufficiently small constant. We can show\nPr[m\u2212 z \u2265 bm] \u2264 E[m\u2212 z] bm\n= m\u2212E[z]\nbm\n\u2264 m\u2212 (1\u2212 9m Bn)m\nbm\n= 9m\nBbn .\nThis implies that with probability at least 1 \u2212 9mBbn , we have m \u2212 z \u2264 bm. Notice that in randomETH , m = cn for a constant c. Thus, by choosing a sufficiently large constant B (which is a function of c, b), we can obtain arbitrarily large constant success probability."}, {"heading": "H.4.3 Reduction", "text": "We reduce 3SAT to tensor rank by following the same construction in [H\u00e5s90]. To obtain a stronger hardness result, we use the property that each variable only appears in at most B (some constant) clauses and that the cover number of an unsatisfiable 3SAT instance is large. Note that both MAXE3SAT(B) instances and random-ETH instances have that property. Also each MAX-E3SAT(B) is also a 3SAT instance. Thus if the reduction holds for 3SAT , it also holds for MAX-E3SAT(B) , and similarly for random-ETH .\nRecall the definition of 3SAT : 3SAT is the problem of given a Boolean formula of n variables in CNF form with at most 3 variables in each of the m clauses, is it possible to find a satisfying assignment to the formula? We say x1, x2, \u00b7 \u00b7 \u00b7 , xn are variables and x1, x1, x2, x2, \u00b7 \u00b7 \u00b7 , xn, xn are literals. We transform this to the problem of computing the rank of a tensor of size n1 \u00d7 n2 \u00d7 n3 where n1 = 2 +n+ 2m, n2 = 3n and n3 = 3n+m. T has the following n3 column-row faces, where each of the faces is an m1 \u00d7 n2 matrix,\n\u2022 n variable matrices Vi \u2208 Rn1\u00d7n2 . It has a 1 in positions (1, 2i\u2212 1) and (2, 2i) while all other elements are 0.\n\u2022 n help matrices Si \u2208 Rn1\u00d7n2 . It has a 1 position in (1, 2n+ i) and is 0 otherwise.\n\u2022 n help matrices Mi \u2208 Rn1\u00d7n2 . It has a 1 in positions (1, 2i\u2212 1), (2 + i, 2i) and (2 + i, 2n+ i) and is 0 otherwise.\n\u2022 m clause matrices Cl \u2208 Rn1\u00d7n2 . Suppose the clause cl contains the literals ul,1, ul,2 and ul,3. For each j \u2208 [3], ul,j \u2208 {x1, x2, \u00b7 \u00b7 \u00b7 , xn, x1, x2, \u00b7 \u00b7 \u00b7 , xn}. Note that xi, xi are the literals of the 3SAT formula. We can also think of xi, xi as length 3n vectors. Let xi denote the vector that has a 1 in position 2i\u2212 1, i.e., xi = e2i\u22121. Let xi denote the vector that has a 1 in positions 2i\u2212 1 and 2i, xi = e2i\u22121 + e2i.\n\u2013 Row 1 is the vector ul,1 \u2208 R3n, \u2013 Row 2 + n+ 2l \u2212 1 is the vector ul,1 \u2212 ul,2 \u2208 R3n, \u2013 Row 2 + n+ 2l is the vector ul,1 \u2212 ul,3 \u2208 R3n.\nFirst, we can obtain Lemma H.31 which follows by Lemma 2 in [H\u00e5s90]. For completeness, we provide a proof.\nLemma H.31. If the formula is satisfiable, then the constructed tensor has rank at most 4n+ 2m.\nProof. We will construct 4n+ 2m rank-1 matrices V (1)i , V (2) i , S (1) i , M (1) i , C (1) l and C (2) l . Then the goal is to show that for each matrix in the set\n{V1, V2, \u00b7 \u00b7 \u00b7 , Vn, S1, S2, \u00b7 \u00b7 \u00b7 , Sn,M1,M2, \u00b7 \u00b7 \u00b7 ,Mn, C1, C2, \u00b7 \u00b7 \u00b7 , Cm},\nit can be written as a linear combination of these constructed matrices.\n\u2022 Matrices V (1)i and V (2) i . V (1) i has the first row equal to xi iff \u03b1i = 1 and otherwise xi. All the\nother rows are 0. We set V (2)i = Vi \u2212 V (1) i .\n\u2022 Matrices S(1)i . S (1) i = Si.\n\u2022 Matrices M (1)i .\nM (1) i = { Mi \u2212 V (1)i if \u03b1i = 1 Mi \u2212 V (1)i \u2212 Si if \u03b1i = 0\n\u2022 Matrices C(1)l and C (2) l . Let xi = \u03b1i be the assignment that makes the clause cl true. Then\nCl \u2212 V (1)i has rank 2, since either it has just two nonzero rows (in the case where xi is the first variable in the clause) or it has three nonzero rows of which two are equal. In both cases we just need two additional rank 1 matrices.\nOnce the 3SAT instance S is unsatisfiable, then its cover number is at least 1. For each unsatisfiable 3SAT instance S with cover number p, we can show that the constructed tensor has rank at most 4n+ 2m+O(p) and also has rank at least 4n+ 2m+ \u2126(p). We first prove an upper bound,\nLemma H.32. For a 3SAT instance S, let y \u2208 {0, 1} denote a string such that S(y) has a set L that contains unsatisfiable clauses. Let p denote the smallest number of variables that cover all clauses in L. Then the constructed tensor T has rank at most 4n+ 2m+ p.\nProof. Let y denote a length-n Boolean string (\u03b11, \u03b12, \u00b7 \u00b7 \u00b7 , \u03b1n). Based on the assignment y, all the clauses of S can be split into two sets: L contains all the unsatisfied clauses and L contains all the satisfied clauses. We use set P to denote a set of variables that covers all the clauses in set L. Let p = |P |. We will construct 4n+ 2m+ p rank-1 matrices V (1)i , V (2) i , S (1) i , M (1) i , \u2200i \u2208 [n], C (1) l , C (2) l , \u2200l \u2208 [m], and V (3)j , \u2200j \u2208 P . Then the goal is to show that the Vi, Si,Mi and Cl can be written as linear combinations of these constructed matrices.\n\u2022 Matrices V (1)i and V (2) i . V (1) i has first row equal to xi iff \u03b1i = 1 and otherwise xi. All the\nother rows are 0. We set V (2)i = Vi \u2212 V (1) i .\n\u2022 Matrices V (3)j . For each j \u2208 P , V (3) j has the first row equal to xi iff \u03b1i = 0 and otherwise xi.\n\u2022 Matrices S(1)i . S (1) i = Si.\n\u2022 Matrices M (1)i .\nM (1) i = { Mi \u2212 V (1)i if \u03b1i = 1 Mi \u2212 V (1)i \u2212 Si if \u03b1i = 0\n\u2022 Matrices C(1)l and C (2) l .\n\u2013 For each l /\u2208 L, clause cl is satisfied according to assignment y. Let xi = \u03b1i be the assignment that makes the clause cl true. Then Cl \u2212 V (1)i has rank 2, since either it has just two nonzero rows (in the case where xi is the first variables in the clause) or it has three nonzero rows of which two are equal. In both cases we just need two additional rank 1 matrices.\n\u2013 For each l \u2208 L. It means clause cl is unsatisfied according to assignment y. Let xj1 = \u03b1j1 , xj2 = \u03b1j2 , xj3 = \u03b1j3 be an assignment that makes the clause cl false. In other words, one of j1, j2, j3 must be P according to the definition that P covers L. Then matrix Cl \u2212 V (3)j1 has rank 2, since either it has just two nonzero rows (in the case where xj1 is the first variables in the clause) or it has three nonzero rows of which two are equal. In both cases we just need two additional rank 1 matrices.\nWe finish the proof by taking the P that has the smallest size.\nFurther, we have:\nCorollary H.33. For a 3SAT instance S, let p denote the cover number of S, then the constructed tensor T has rank at most 4n+ 2m+ p.\nProof. This follows by applying Lemma H.32 to all the input strings and the definition of cover number (Definition H.24).\nWe can split the tensor T \u2208 R(2+n+3m)\u00d73n\u00d7(3n+m) into two sub-tensors, one is T1 \u2208 R2\u00d73n\u00d7(3n+m) (that contains the first two row-tube faces of T and linear combination of the remaining 2m rowtube faces of T ), and the other is T2 \u2208 R(n+2m)\u00d73n\u00d7(3n+m) (that contains the next n+ 2m row-tube faces of T ). We first analyze the rank of T1 and then analyze the rank of T2.\nClaim H.34. The rank of T2 is n+ 2m.\nProof. According to Figure 11, the nonzero rows are distributed in n+m fully separated sub-tensors. It is obvious that the rank of each one of those n sub-tensors is 1, and the rank of each of those m sub-tensors is 2. Thus, overall, the rank T2 is n+ 2m.\nTo make sure rank(T ) = rank(T1) + rank(T2), the T1 \u2208 R2\u00d73n\u00d7(3n+m) can be described as the following 3n+m column-row faces, and each of the faces is a 2\u00d7 3n matrix.\n\u2022 Matrices V\u0303i, \u2200i \u2208 [n]. The two rows are from the first two rows of Vi in Figure 11, i.e., the first row is e2i\u22121 and the second row is e2i.\n\u2022 Matrices S\u0303i, \u2200i \u2208 [n]. The two rows are from the first two rows of Si in Figure 11, i.e., the first row is e2n+i and the second row is zero everywhere else.\n\u2022 Matrices M\u0303i,\u2200i \u2208 [n]. The first row is e2i\u22121 + \u03b2i,1(e2i + e2n+i), while the second row is \u03b2i,2(e2i + e2n+i).\n\u2022 Matrices C\u0303l, \u2200i \u2208 [m]. The first row is (1 + \u03b3l,1 + \u03b3l,2)ul,1 \u2212 \u03b3l,1ul,2 \u2212 \u03b3l,2ul,3 and the second is (\u03b3l,3 + \u03b3l,4)ul,1 \u2212 \u03b3l,3ul,2 \u2212 \u03b3l,4ul,3,\nwhere for each i \u2208 [3n], we use vector ei to denote a length 3n vector such that it only has a 1 in position i and 0 otherwise. \u03b2, \u03b3 are variables. The goal is to show a lower bound for,\nrank \u03b2,\u03b3 (T1).\nLemma H.35. Let P denote the set {i | the second row of matrix M\u0303i is nonzero, \u2200i \u2208 [n]}. Then the rank of T1 is at least 3n+ |P |.\nProof. We define p = |P |. Without loss of generality, we assume that for each i \u2208 [p], the second row of matrix M\u0303i is nonzero.\nNotice that matrices V\u0303i, S\u0303i, M\u0303i have size 2\u00d7 3n, but we only focus on the first 2n+ p columns. Thus, we have n+ p column-row faces (from the 3rd dimension) Aj \u2208 R2\u00d7(2n+p),\n\u2022 Aj , 1 \u2264 j \u2264 n, Aj is the first 2n+p columns of V\u0303j\u2212 \u2211n\ni=1 \u03b1i,jS\u0303i \u2208 R2\u00d73n, where \u03b1i,j are some coefficients.\n\u2022 An+j , 1 \u2264 j \u2264 p, Aj is the first 2n + p columns of M\u0303j \u2212 \u2211n\ni=1 \u03b1i,n+jS\u0303i \u2208 R2\u00d73n, where \u03b1i,j are some coefficients.\nConsider the first 2n + p column-tube faces (from 2nd dimension), Bj , \u2200j \u2208 [2n + p], of T1. Notice that these matrices have size 2\u00d7 (n+ p).\n\u2022 B2i\u22121, 1 \u2264 i \u2264 p, it has a 1 in positions (1, i) and (1, n+ i).\n\u2022 B2i, 1 \u2264 i \u2264 p, it has \u03b2i,1 in position (1, n+ i), 1 in position (2, i) and \u03b2i,2 in position (2, n+ i).\n\u2022 B2i\u22121, p+ 1 \u2264 i \u2264 n, it has 1 in position (1, i).\n\u2022 B2i, p+ 1 \u2264 i \u2264 n, it has 1 in position (2, i).\n\u2022 B2n+i, 1 \u2264 i \u2264 p, the first row is unknown, the second row has \u03b2i,2 in position in (2, n+ i).\nIt is obvious that the first 2n matrices are linearly independent, thus the rank is at least 2n. We choose the first 2n matrices as our basis. For B2n+1, we try to write it as a linear combination of the first 2n matrices {Bi}i\u2208[2n]. Consider the second row of B2n+1. The first n positions are all 0. The matrices B2i all have disjoint support for the second row of the first n columns. Thus, the matrices B2i should not be used. Consider the second row of B2i\u22121, \u2200i \u2208 [n]. None of them has a nonzero value in position n+1. Thus B2n+1 cannot be written as a linear combination of of the first 2n matrices. Thus, we can show for any i \u2208 [p], B2n+i cannot be written as a linear combination of matrices {Bi}i\u2208[2n]. Consider the p matrices {B2n+i}i\u2208[p]. Each of them has a different nonzero position in the second row. Thus these matrices are all linearly independent. Putting it all together, we know that the rank of matrices {Bi}i\u2208[2n+p] is at least 2n+ p.\nNext, we consider another special case when \u03b2i,2 = 0, for all i \u2208 [n]. If we subtract \u03b2i,1 times S\u0303i from M\u0303i and leave the other column-row faces (from the 3rd dimension) as they are, and we make all column-tube faces(from the 2nd dimension) for j > 2n identically 0, then all other choices do not change the first 2n column-tube faces (from the 2nd dimension) and make some other column-tube faces (from the 2nd dimension) nonzero. Such a choice could clearly only increase the rank of T . Thus, we obtain,\nrank(T ) = 2n+ 2m+ min rank(T3),\nwhere T3 is a tensor of size 2 \u00d7 2n \u00d7 (2n + m) given by the following column-row faces (from 3rd dimension) Ai, \u2200i \u2208 [2n+m] and each matrix has size 2\u00d7 2n (shown in Figure 15).\n\u2022 Ai, i \u2208 [n], the first 2n columns of V\u0303i.\n\u2022 An+i, i \u2208 [n], the first 2n columns of M\u0303i. The first row is e2i\u22121 + \u03b2i,1e2i, and the second row is 0.\n\u2022 A2n+l, l \u2208 [m], the first 2n columns of C\u0303l. The first row is (1+\u03b3l,1 +\u03b3l,2)ul,1\u2212\u03b3l,1ul,2\u2212\u03b3l,2ul,3, and the second row is (\u03b3l,3 + \u03b3l,4)ul,1 \u2212 \u03b3l,3ul,2 \u2212 \u03b3l,4ul,3.\nWe can show\nLemma H.36. Let p denote the cover number of the 3SAT instance. T3 has rank at least 2n+\u2126(p).\nProof. First, we can show that all matrices An+i\u2212Ai and An+i (for all i \u2208 [n] ) are in the expansion of tensor T3. Thus, the rank of T3 is at least 2n.\nWe need the following claim:\nClaim H.37. For any l \u2208 [m], if A2n+l can be written as a linear combination of {An+i \u2212Ai}i\u2208[n] and {An+i}i\u2208[n], then the second row of A2n+l is 0, and the first row of one of the An+i is ui where ui is one of the literals appearing in clause cl.\nProof. We prove this for the second row first. For each l \u2208 [m], we consider the possibility of using all matrices An+i \u2212 Ai and An+i to express matrix A2n+l. If the second row of A2n+l is nonzero, then it must have a nonzero entry in an odd position. But there is no nonzero in an odd position of the second row of any of matrices An+i \u2212Ai and An+i.\nFor the first row. It is obvious that the first row of A2n+l must have at least one nonzero position, for any \u03b3l,1, \u03b3l,2. Let uj be a literal belonging to the variable xi which appears in the first row of A2n+l with a nonzero coefficient. Since only An+i of all the other An+s,\u2200s \u2208 [n] matrices has nonzero elements in either of the positions (1, 2i \u2212 1) or (1, 2i), then An+i must be used to cancel these elements. Thus, the first row of An+i must be a multiple of uj and since the element in position (1, 2i\u2212 1) of An+i is 1, this multiple must be 1.\nNote that matrices Ai,\u2200i \u2208 [n] have the property that, for any matrix in {An+1, \u00b7 \u00b7 \u00b7 , A2n+m}, it cannot be written as the linear combination of matrices Ai,\u2200i \u2208 [n]. Let A\u0303 \u2208 R(n+m)\u00d72n denote a matrix that consists of the first rows of {An+1, \u00b7 \u00b7 \u00b7 , A2n+m}. According to the property of matrices Ai, \u2200i \u2208 [n], and that the rank of a tensor is always greater than or equal to the rank of any sub-tensor, we know that\nrank(T3) \u2265 n+ min rank(A\u0303).\nClaim H.38. For a 3SAT instance S, for any input string y \u2208 {0, 1}n, set \u03b2\u2217,1 to be the entry-wise flipping of y, (I) if the clause l is satisfied, then the (n+ l)-th row of A\u0303 \u2208 R(n+m)\u00d72n can be written as a linear combination of the first n rows of A\u0303. (II) if the clause l is unsatisfied, then the (n+ l)-th row of A\u0303 cannot be written as a linear combination of the first n rows of A\u0303.\nProof. Part (I), consider a clause l which is satisfied with input string y. Then there must exist a variable xi belonging to clause l (either literal xi or literal xi) and one of the following holds: if xi belongs to clause l, then \u03b1i = 1; if xi belongs to clause l, then \u03b1i = 0. Suppose clause l contains literal xi. The other case can be proved in a similar way. We consider the (n + l)-th row. One of the following assignments (0, 0), (\u22121, 0), (0,\u22121) to \u03b3l,1, \u03b3l,1 is going to set the (n+ l)-th row of A\u0303 to be vector e2i\u22121. We consider the i-th row of A\u0303. Since we set \u03b1i = 1, then we set \u03b2i,1 = 0, it follows that the i-th row of A becomes e2i\u22121. Therefore, the (n+ l)-th row of A\u0303 can be written as a linear combination of A\u0303.\nPart (II), consider a clause l which is unsatisfied with input string y. Suppose that clause contains three literals xi1 , xi2 , xi3 (the other seven possibilities can be proved in a similar way). Then for input string y, we have \u03b1i1 = 0, \u03b1i2 = 0 and \u03b1i3 = 0, otherwise this clause l is satisfied. Consider i1-th row of A\u0303. It becomes e2i1\u22121 + e2i1 . Similarly for the i2-th row and i3-th row. Consider the (n + l)-th row. We can observe that all of positions 2i1, 2i2, 2i3 must be 0. Any\nlinear combination formed by the i1, i2, i3-th row of A\u0303 must have one nonzero in one of positions 2i1, 2i2, 2i3. However, if we consider the (n+ l)-th row of A\u0303, one of the positions 2i1, 2i2, 2i3 must be 0. Also, the remaining n\u2212 3 of the first n rows of A\u0303 also have 0 in positions 2i1, 2i2, 2i3. Thus, we can show that the (n + l)-th row of A\u0303 cannot be written as a linear combination of the first n rows. Similarly, for the other seven cases.\nNote that in order to make sure as many as possible rows in n + 1, \u00b7 \u00b7 \u00b7 , n + m can be written as linear combinations of the first n rows of A\u0303, the \u03b2i,1 should be set to either 0 or 1. Also each possibility of input string y is corresponding to a choice of \u03b2i,1. According to the above Claim H.38, let l0 denote the smallest number of unsatisfied clauses over the choices of all the 2n input strings. Then over all choices of \u03b2, \u03b3, there must exist at least l0 rows of A\u0303n+1, \u00b7 \u00b7 \u00b7 A\u0303n+m, such that each of those rows cannot be written as the linear combination of the first n rows.\nClaim H.39. Let A\u0303 \u2208 R(n+m)\u00d72n denote a matrix that consists of the first rows of An+i,\u2200i \u2208 [n] and An+l,\u2200l \u2208 [m]. Let p denote the cover number of 3SAT instance. Then min rank(A\u0303) \u2265 n+\u2126(p).\nProof. For any choices of {\u03b2i,1}i\u2208[n], there must exist a set of rows out of the next m rows such that, each of those rows cannot be written as a linear combination of the first n rows. Let L denote the set of those rows. Let t denote the maximum size set of disjoint rows from L. Since those t rows in L all have disjoint support, they are always linearly independent. Thus the rank is at least n+ t.\nNote that each row corresponds to a unique clause and each clause corresponds to a unique row. We can just pick an arbitrary clause l in L, then remove the clauses that are using the same literal as clause l from L. Because each variable occurs in at most B clauses, we only need to remove at most 3B clauses from L. We repeat the procedure until there is no clause L. The corresponding rows of all the clauses we picked have disjoint supports, thus we can show a lower bound for t,\nt \u2265 |L|/(3B) \u2265 l0/(3B) \u2265 p/(9B) & p,\nwhere the second step follows by |L| \u2265 l0, the third step follows 3l0 \u2265 p, and the last step follows by B is some constant.\nThus, putting it all together, we complete the proof.\nNow, we consider a general case when there are q different i \u2208 [n] satisfying that \u03b2i,2 6= 0. Similar to tensor T3, we can obtain T4 such that,\nrank(T ) = 2n+ 2m+ min rank(T4)\nwhere T4 is a tensor of size 2 \u00d7 2n \u00d7 (2n + m) given by the following column-row faces (from 3rd dimension) Ai, \u2200i \u2208 [2n+m] and each matrix has size 2\u00d7 2n (shown in Figure 16).\n\u2022 Ai, i \u2208 [n], the first 2n columns of V\u0303i.\n\u2022 An+i, i \u2208 [q], the first 2n columns of M\u0303i. The first row is e2i\u22121 + \u03b2i,1e2i, and the second row is \u03b2i,2e2i.\n\u2022 An+i, i \u2208 {q + 1, \u00b7 \u00b7 \u00b7 , n}, the first 2n columns of M\u0303i. The first row is e2i\u22121 + \u03b2i,1e2i, and the second row is 0.\n\u2022 A2n+l, l \u2208 [m], the first 2n columns of C\u0303l. The first row is (1+\u03b3l,1 +\u03b3l,2)ul,1\u2212\u03b3l,1ul,2\u2212\u03b3l,2ul,3, and the second row is (\u03b3l,3 + \u03b3l,4)ul,1 \u2212 \u03b3l,3ul,2 \u2212 \u03b3l,4ul,3.\nNote that modifying q entries(from Figure 15 to Figure 16) of a tensor can only decrease the rank by q, thus we obtain\nLemma H.40. Let q denote the number of i such that \u03b2i,2 6= 0, and let p denote the cover number of the 3SAT instance. Then T4 has rank at least 2n+ \u2126(p)\u2212 q.\nCombining the two perspectives we have\nLemma H.41. Let p denote the cover number of an unsatisfiable 3SAT instance. Then the tensor has rank at least 4n+ 2m+ \u2126(p).\nProof. Let q denote the q in Figure 16. From one perspective, we know that the tensor has rank at least 4n+ 2m+ \u2126(p)\u2212 q. From another perspective, we know that the tensor has rank at least 4n+ 2m+ q. Combining them together, we obtain the rank is at least 4n+ 2m+ \u2126(p)/2, which is still 4n+ 2m+ \u2126(p).\nTheorem H.42. Unless ETH fails, there is a \u03b4 > 0 and an absolute constant c0 > 1 such that the following holds. For the problem of deciding if the rank of a q-th order tensor, q \u2265 3, with each dimension n, is at most k or at least c0k, there is no 2\u03b4k 1\u2212o(1) time algorithm.\nProof. The reduction can be split into three parts.13 The first part reduces the MAX-3SAT problem to the MAX-E3SAT problem by [MR10]. For each MAX-3SAT instance with size n, the corresponding MAX-E3SAT instance has size n1+o(1). The second part is by reducing the MAX-E3SAT problem to MAX-E3SAT(B) by [Tre01]. For each MAX-E3SAT instance with size n, the corresponding MAX-E3SAT(B) instance has size \u0398(n) when B is a constant. The third part is by reducing the MAX-E3SAT(B) problem to the tensor problem. Combining Theorem H.7, Lemma H.25 with this reduction, we complete the proof.\nTheorem H.43. Unless random-ETH fails, there is an absolute constant c0 > 1 for which any deterministic algorithm for deciding if the rank of a q-th order tensor is at most k or at least c0k, requires 2\u2126(k) time.\nProof. This follows by combining the reduction with random-ETH and Lemma H.30. 13The first two parts are accomplished by personal communication with Dana Moshkovitz and Govind Ramnarayan.\nNote that, if BPP = P then it also holds for randomized algorithms which succeed with probability 2/3.\nIndeed, we know that any deterministic algorithm requires 2\u2126(n) running time on tensors that have size n\u00d7n\u00d7n. Let g(n) denote a fixed function of n, and g(n) = o(n). We change the original tensor from size n\u00d7n\u00d7n to 2g(n)\u00d72g(n)\u00d72g(n) by adding zero entries. Then the number of entries in the new tensor is 23g(n) and the deterministic algorithm still requires 2\u2126(n) running time on this new tensor. Assume there is a randomized algorithm that runs in 2cg(n) time, for some constant c > 3. Then considering the size of this new tensor, the deterministic algorithm is a super-polynomial time algorithm, but the randomized algorithm is a polynomial time algorithm. Thus, by assuming BPP = P, we can rule out randomized algorithms, which means Theorem H.43 also holds for randomized algorithms which succeed with probability 2/3.\nWe provide some some motivation for the BPP = P assumption: this is a standard conjecture in complexity theory, as it is implied by the existence of strong pseudorandom generators or if any problem in deterministic exponential time has exponential size circuits [IW97].\nH.5 Hardness result for robust subspace approximation\nThis section improves the previous hardness for subspace approximation [CW15a] from 1\u00b11/poly(d) to 1\u00b1 1/poly(log d). (Note that, we provide the algorithmic results for this problem in Section F.)\nLemma H.44 ([Dem14]). For any graph G with n nodes, m edges, for which the maximum degree in graph G is d, there exists a d-regular graph G\u2032 with 2nd \u2212 2m nodes such that the clique size of G\u2032 is the same as the clique size of G.\nProof. First we create d copies of the original graph G. For each i \u2208 [n], let vi,1, vi,2, \u00b7 \u00b7 \u00b7 , vi,d denote the set of nodes in G\u2032 that are corresponding to vi in G. Let dvi denote the degree of node vi in graph G. In graph G\u2032, we create d\u2212 dvi new nodes v\u2032i,1, v\u2032i,2, \u00b7 \u00b7 \u00b7 , v\u2032i,dvi and connect each of them to all of the v1, v2, \u00b7 \u00b7 \u00b7 , vd. Therefore, 1. For each i \u2208 [n], j \u2208 [dvi ], node v\u2032i,j has degree d. 2. For each i \u2208 [n], j \u2208 [d], node vi,j has degree dvi (from the original graph), and d\u2212dvi degree (from the edges to all the v\u2032i,1, v\n\u2032 i,2, \u00b7 \u00b7 \u00b7 , v\u2032i,dvi ). Thus, we proved the graph G is d-regular.\nThe number of nodes in the new graph G\u2032 is,\nnd+ n\u2211\ni=1\n(d\u2212 dvi) = 2nd\u2212 n\u2211\ni=1\ndvi = 2nd\u2212 2m.\nIt remains to show the clique size is the same in graph G and G\u2032. Since we can always reorder the indices for all the nodes, without loss of generality, let us assume the the first k nodes v1, v2, \u00b7 \u00b7 \u00b7 , vk forms a k-clique that has the largest size. It is obvious that the clique size k\u2032 in graph G\u2032 is at least k, since we make k copies of the original graph and do not delete any edges and nodes. Then we just need to show k\u2032 \u2264 k. By the property of the construction, the node in one copy does not connect to a node in any other copy. Consider the new nodes we created. For each node v\u2032i,j , consider the neighbors of this node. None of them share a edge. Combining the above two properties gives k\u2032 \u2264 k. Thus, we finish the proof.\nTheorem H.45 (Theorem 2.6 in [GJS76]). Any n variable m clauses 3SAT instance can be reduced to a graph G with 24m vertices, which is an instance of 10m-independent set. Furthermore G is a 3-regular graph.\nWe give the proof for completeness here.\nProof. Define oi to be the number of occurrences of {xi, xi} in the m clauses. For each variable xi, we construct 2oi vertices, namely vi,1, vi,2, \u00b7 \u00b7 \u00b7 , vi,2oi . We make these 2oi vertices be a circuit, i.e., there are 2oi edges: (vi,1, vi,2), (vi,2, vi,3), \u00b7 \u00b7 \u00b7 , (vi,2oi\u22121, vi,2oi), (vi,2oi , vi,1). For each clause with 3 literals a, b, c, we create 3 vertices va, vb, vc where they form a triangle, i.e., there are edges (va, vb), (vb, vc), (vc, va). Furthermore, assume a is the jth occurrence of xi (occurrence of xi means a = xi or a = xi). Then if a = xi, we add edge (va, vi,2j), otherwise we add edge (va, vi,2j\u22121).\nThus, we can see that every vertex in the triangle corresponding to a clause has degree 3, half of vertices of the circuit corresponding to variable xi have degree 3 and the other half have degree 2. Notice that the maximum independent set of a 2oi circuit is at most oi, and the maximum independent set of a triangle is at most 1. Thus, the maximum independent set of the whole graph has size at most m+ \u2211n i=1 oi = m+ 3m = 4m. Another observation is that if there is a satisfiable assignment for the 3SAT instance, then we can choose a 4m-independent set in the following way: if xi is true, then we choose all the vertices in set {vi,1, vi,3, \u00b7 \u00b7 \u00b7 , vi,2j\u22121, \u00b7 \u00b7 \u00b7 vi,2oi\u22121}; otherwise, we choose all the vertices in set {vi,2, vi,4, \u00b7 \u00b7 \u00b7 , vi,2j , \u00b7 \u00b7 \u00b7 vi,2oi}. For a clause with literals a, b, c: if a is satisfied, it means that vi,t which connected to va is not chosen in the independent set, thus we can pick va.\nThe issue remaining is to reduce the above graph to a 3 regular graph. Notice that there are exactly \u2211n i=1 oi = 3m vertices which have degree 2. For each of this kind of vertex u, we construct 5 additional vertices u1, u2, u3, u4, u5 and edges (u1, u2), (u2, u3), (u3, u4), (u4, u5), (u5, u1), (u2, u4), (u3, u5) and (u1, u). Because we can always choose exactly two vertices among u1, u2, \u00b7 \u00b7 \u00b7 , u5 no matter we choose vertex u or not, the value of the maximum independent set will increase the size by exactly 2 \u2211n\ni=1 oi = 6m. To conclude, we construct a 3-regular graph reduced from a 3SAT instance. The graph has exactly 24m vertices. Furthermore, if the 3SAT instance is satisfiable, the graph has 10m-independent set. Otherwise, it does not have a 10m-independent set.\nCorollary H.46. There is a constant 0 < c < 1, such that for any > 0, there is no O(2n1\u2212 ) time algorithm which can solve k-clique for an n-vertex (n \u2212 3)-regular graph where k = cn unless ETH fails.\nProof. According to Theorem H.45, for a given n variable m = O(n) clauses 3SAT instance, we can reduce it to a 3-regular graph with 24m vertices which is a 10m-independent set instance. If\nthere exists > 0 such that we have an algorithm with running time O(2(24m)1\u2212 ) which can solve 10m-clique for a 24m\u2212 3 regular graph with 24m vertices, then we can solve the 3SAT problem in O(2n 1\u2212 \u2032 ) time, where \u2032 = \u0398( ). Thus, it contradicts ETH .\nDefinition H.47. Let V be a k-dimensional subspace of Rd, represented as the column span of a d\u00d7 k matrix with orthonormal columns. We abuse notation and let V be both the subspace and the corresponding matrix. For a set Q of points, let\nc(Q,V ) = \u2211\nq\u2208Q d(q, V )p =\n\u2211 q\u2208Q \u2016q>(I \u2212 V V >)\u2016p2 = \u2211 q\u2208Q (\u2016q\u20162 \u2212 \u2016q>V \u20162)p/2,\nbe the sum of p-th powers of distances of points in Q, i.e., \u2016Q\u2212QV V >\u2016v with associatedM(x) = |x|p.\nLemma H.48. For any k \u2208 [d], the k-dimensional subspaces V which minimize c(E, V ) are exactly the ( n k ) subspaces formed by taking the span of k distinct standard unit vectors ei, i \u2208 [d]. The cost of any such V is d\u2212 k.\nTheorem H.49. Given a set Q of poly(d) points in Rd, for a sufficiently small = 1/poly(d), it is NP-hard to output a k-dimensional subspace V of Rd for which c(Q,V ) \u2264 (1 + )c(Q,V \u2217), where V \u2217 is the k-dimensional subspace minimizing the expression c(Q,V ), that is c(Q,V ) \u2265 c(Q,V \u2217) for all k-dimensional subspaces V .\nTheorem H.50. For a sufficiently small = 1/ poly(log(d)), there exist 1 \u2264 k \u2264 d, unless ETH fails, there is no algorithm that can output a k-dimensional subspace V of Rd for which c(Q,V ) \u2264 (1 + )c(Q,V \u2217), where V \u2217 is the k-dimensional subspace minimizing the expression c(Q,V ), that is c(Q,V ) \u2265 c(Q,V \u2217) for all k-dimensional subspaces V .\nProof. The reduction is from the clique problem of d-vertices (d \u2212 3)-regular graph. We construct the hard instance in the same way as in [CW15a]. Given a d-vertes (d\u2212 3)-regular graph graph G, let B1 = d\u03b1, B2 = d\u03b2 where \u03b2 > \u03b1 \u2265 1 are two sufficiently large constants. Let c be such that\n(1\u2212 1/B1)2 + c2/B1 = 1.\nWe construct a d \u00d7 d matrix A as the following: \u2200i \u2208 [d], let Ai,i = 1 \u2212 1/B1 and \u2200i 6= j, Ai,j = Aj,i = c/ \u221a B1r if (i, j) is an edge in G, and Ai,j = Aj,i = 0 otherwise. Let us construct A\u2032 \u2208 R2d\u00d7d as follows:\nA\u2032 =\n[ A\nB2 \u00b7 Id\n] ,\nwhere Id \u2208 Rd is a d\u00d7 d identity matrix. Claim H.51 (In proof of Theorem 54 in [CW15a]). Let V \u2032 \u2208 Rd\u00d7k satisfy that\nc(A\u2032, V \u2032) \u2264 (1 + 1/d\u03b3)c(A\u2032, V \u2217),\nwhere A\u2032 is constructed as the above corresponding to the given graph G, and \u03b3 > 1 is a sufficiently large constant, V \u2217 is the optimal solution which minimizes c(A\u2032, V ). Then if G has a k-Clique , given V \u2032, there is a poly(d) time algorithm which can find the clique which has size at least k.\nNow, to apply ETH here, we only need to apply a padding argument. We can construct a matrix A\u2032\u2032 \u2208 RN\u00d7d as follows:\nA\u2032\u2032 =   A\u2032 A\u2032\n\u00b7 \u00b7 \u00b7 A\u2032\n  .\nBasically, A\u2032\u2032 contains N/(2d) copies of A\u2032 where N = 2d1\u2212\u03b1 , and 0 < \u03b1 is a constant which can be arbitrarily small. Notice that \u2200V \u2208 Rd\u00d7k,\nc(V,A\u2032\u2032) = \u2211\nq\u2208A\u2032\u2032 d(q, V )p = N/(2d)\n\u2211 q\u2208A\u2032 d(q, V )p = N/(2d)c(V,A\u2032).\nSo if V \u2032\u2032 gives a (1 + 1/d\u03b3) approximation to A\u2032\u2032, it also gives a (1 + 1/d\u03b3) approximation to A\u2032. So if we can find V \u2032\u2032 in poly(N, d) time, we can output a k-Clique of G in poly(N, d) time. But unless ETH fails, for a sufficiently small constant \u03b1\u2032 > 0 there is no poly(N, d) = O(2d1\u2212\u03b1 \u2032 ) time algorithm that can output a k-Clique of G. It means that there is no poly(N, d) time algorithm that can compute a (1 + 1/d\u03b3) = (1 + 1/ poly(log(N))) approximation to A\u2032\u2032. To make A\u2032\u2032 be a square matrix, we can just pad with 0s to make the size of A\u2032\u2032 be N \u00d7N . Thus, we can conclude, unless ETH fails, there is no polynomial algorithm that can compute a (1 + 1/ poly(log(N))) rank-k subspace approximation to a point set with size N .\nH.6 Extending hardness from matrices to tensors\nIn this section, we briefly state some hardness results which are implied by hardness for matrices. The intuition is that, if there is a hard instance for the matrix problem, then we can always construct a tensor hard instance for the tensor problem as follos: the first face of the tensor is the hard instance matrix and it has all 0s elsewhere. We can prove that the optimal tensor solution will always fit the first face and will have all 0s elsewhere. Then the optimal tensor solution gives an optimal matrix solution.\nH.6.1 Entry-wise `1 norm and `1-`1-`2 norm\nIn the following we will show that the hardness for entry-wise `1 norm low rank matrix approximation implies the hardness for entry-wise `1 norm low rank tensor approximation and asymmetric tensor norm (`1-`1-`2) low rank tensor approximation problems.\nTheorem H.52 (Theorem H.13 in [SWZ17]). Unless ETH fails, for an arbitrarily small constant \u03b3 > 0, given some matrix A \u2208 Rn\u00d7n, there is no algorithm that can compute x\u0302, y\u0302 \u2208 Rn s.t.\n\u2016A\u2212 x\u0302y\u0302>\u20161 \u2264 ( 1 + 1\nlog1+\u03b3(n) ) min x,y\u2208Rn \u2016A\u2212 xy>\u20161,\nin poly(n) time.\nWe can get the hardness for tensors directly.\nTheorem H.53. Unless ETH fails, for an arbitrarily small constant \u03b3 > 0, given some tensor A \u2208 Rn\u00d7n\u00d7n,\n1. there is no algorithm that can compute x\u0302, y\u0302, z\u0302 \u2208 Rn s.t.\n\u2016A\u2212 x\u0302\u2297 y\u0302 \u2297 z\u0302\u20161 \u2264 ( 1 + 1\nlog1+\u03b3(n)\n) min\nx,y,z\u2208Rn \u2016A\u2212 x\u2297 y \u2297 z\u20161,\nin poly(n) time.\n2. there is no algorithm can compute x\u0302, y\u0302, z\u0302 \u2208 Rn s.t.\n\u2016A\u2212 x\u0302\u2297 y\u0302 \u2297 z\u0302\u2016u \u2264 ( 1 + 1\nlog1+\u03b3(n)\n) min\nx,y,z\u2208Rn \u2016A\u2212 x\u2297 y \u2297 z\u2016u,\nin poly(n) time.\nProof. Let matrix A\u0302 \u2208 Rn\u00d7n be the hard instance in Theorem H.52. We construct tensor A \u2208 Rn\u00d7n\u00d7n as follows: \u2200i, j, l \u2208 [n], l 6= 1 we let Ai,j,1 = A\u0302i,j , Ai,j,l = 0.\nSuppose x\u0302, y\u0302, z\u0302 \u2208 Rn satisfies\n\u2016A\u2212 x\u0302\u2297 y\u0302 \u2297 z\u0302\u20161 \u2264 ( 1 + 1\nlog1+\u03b3(n)\n) min\nx,y,z\u2208Rn \u2016A\u2212 x\u2297 y \u2297 z\u20161.\nThen letting z\u2032 = (1, 0, 0, \u00b7 \u00b7 \u00b7 , 0)>, we have\n\u2016A\u2212 x\u0302\u2297 y\u0302 \u2297 z\u2032\u20161 \u2264 \u2016A\u2212 x\u0302\u2297 y\u0302 \u2297 z\u0302\u20161 \u2264 ( 1 + 1\nlog1+\u03b3(n)\n) min\nx,y,z\u2208Rn \u2016A\u2212 x\u2297 y \u2297 z\u20161.\nThe first inequality follows since \u2200i, j, l \u2208 [n], l 6= 1, we have Ai,j,l = 0. Let\nx\u2217, y\u2217 = arg min x,y\u2208Rn \u2016A\u0302\u2212 xy>\u20161.\nThen\n\u2016A\u2212 x\u0302\u2297 y\u0302 \u2297 z\u2032\u20161 \u2264 ( 1 + 1\nlog1+\u03b3(n)\n) \u2016A\u2212 x\u0302\u2297 y\u0302 \u2297 z\u0302\u20161 \u2264 ( 1 +\n1\nlog1+\u03b3(n)\n) \u2016A\u2212 x\u2217 \u2297 y\u2217 \u2297 z\u2032\u20161.\nThus, we have\n\u2016A\u0302\u2212 x\u0302y\u0302>\u20161 \u2264 ( 1 + 1\nlog1+\u03b3(n)\n) \u2016A\u0302\u2212 x\u2217(y\u2217)>\u20161.\nCombining with Theorem H.52, we know that unless ETH fails, there is no poly(n) running time algorithm which can output\n\u2016A\u2212 x\u0302\u2297 y\u0302 \u2297 z\u0302\u20161 \u2264 ( 1 + 1\nlog1+\u03b3(n)\n) min\nx,y,z\u2208Rn \u2016A\u2212 x\u2297 y \u2297 z\u20161.\nSimilarly, we can prove that if x\u0303, y\u0303, z\u0303 \u2208 Rn satisfies:\n\u2016A\u2212 x\u0303\u2297 y\u0303 \u2297 z\u0303\u2016u \u2264 ( 1 + 1\nlog1+\u03b3(n)\n) min\nx,y,z\u2208Rn \u2016A\u2212 x\u2297 y \u2297 z\u2016u,\nthen\n\u2016A\u0302\u2212 x\u0303y\u0303>\u20161 \u2264 ( 1 + 1\nlog1+\u03b3(n)\n) \u2016A\u0302\u2212 x\u2217(y\u2217)>\u20161.\nWe complete the proof.\nCorollary H.54. Unless ETH fails, for arbitrarily small constant \u03b3 > 0,\n1. there is no algorithm that can compute (1+ ) entry-wise `1 norm rank-1 tensor approximation in 2O(1/ 1\u2212\u03b3) running time. (\u2016 \u00b7 \u20161-norm is defined in Section D)\n2. there is no algorithm that can compute (1 + ) `u-norm rank-1 tensor approximation in 2O(1/ 1\u2212\u03b3) running time. (\u2016 \u00b7 \u2016u-norm is defined in Section F.3)\nH.6.2 `1-`2-`2 norm\nTheorem H.55. Unless ETH fails, for arbitrarily small constant \u03b3 > 0, given some tensor A \u2208 Rn\u00d7n\u00d7n, there is no algorithm can compute U\u0302 , V\u0302 , W\u0302 \u2208 Rn\u00d7k s.t.\n\u2016A\u2212 U\u0302 \u2297 V\u0302 \u2297 W\u0302\u2016v \u2264 ( 1 + 1\npoly(log n)\n) min\nU,V,W\u2208Rn\u00d7k \u2016A\u2212 U \u2297 V \u2297W\u2016v,\nin poly(n) running time. (\u2016 \u00b7 \u2016v-norm is defined in Section F.2) Proof. Let matrix A\u0302 \u2208 Rn\u00d7n be the hard instance in Theorem H.50. We construct tensor A \u2208 Rn\u00d7n\u00d7n as follows: \u2200i, j, l \u2208 [n], l 6= 1 we let Ai,j,1 = A\u0302i,j , Ai,j,l = 0.\nSuppose U\u0302 , V\u0302 , W\u0302 \u2208 Rn\u00d7k satisfies\n\u2016A\u2212 U\u0302 \u2297 V\u0302 \u2297 W\u0302\u2016v \u2264 ( 1 + 1\npoly(log n)\n) min\nU,V,W\u2208Rn\u00d7k \u2016A\u2212 U \u2297 V \u2297W\u2016v.\nLet W \u2032 \u2208 Rn\u00d7k be the following:\nW \u2032 =   1 1 \u00b7 \u00b7 \u00b7 1 0 0 \u00b7 \u00b7 \u00b7 0 0 0 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 \u00b7 \u00b7 \u00b7 0   ,\nthen we have\n\u2016A\u2212 U\u0302 \u2297 V\u0302 \u2297W \u2032\u2016v \u2264 \u2016A\u2212 U\u0302 \u2297 V\u0302 \u2297 W\u0302\u2016v \u2264 ( 1 + 1\npoly(log n)\n) min\nU,V,W\u2208Rn\u00d7k \u2016A\u2212 U \u2297 V \u2297W\u2016v.\nThe first inequality follows since \u2200i, j, l \u2208 [n], l 6= 1, we have Ai,j,l = 0. Let\nU\u2217, V \u2217 = arg min U,V \u2208Rn\u00d7k \u2016A\u0302\u2212 UV >\u2016v.\nThen\n\u2016A\u2212 U\u0302 \u2297 V\u0302 \u2297W \u2032\u2016v \u2264 ( 1 + 1\npoly(log n)\n) \u2016A\u2212 U\u0302 \u2297 V\u0302 \u2297 W\u0302\u2016v\n\u2264 ( 1 + 1\npoly(log n)\n) \u2016A\u2212 U\u2217 \u2297 V \u2217 \u2297W \u2032\u2016v.\nThus, we have\n\u2016A\u0302\u2212 U\u0302 V\u0302 >\u2016v \u2264 ( 1 + 1\npoly(log n)\n) \u2016A\u0302\u2212 U\u2217(V \u2217)>\u2016v.\nCombining with Theorem H.50, we know that unless ETH fails, there is no poly(n) time algorithm which can output\n\u2016A\u2212 U\u0302 \u2297 V\u0302 \u2297 W\u0302\u2016v \u2264 ( 1 + 1\npoly(log n)\n) min\nU,V,W\u2208Rn\u00d7k \u2016A\u2212 U \u2297 V \u2297W\u2016v."}, {"heading": "I Hard Instance", "text": "This section provides some hard instances for tensor problems.\nI.1 Frobenius CURT decomposition for 3rd order tensor\nIn this section we will prove that a relative-error Tensor CURT is not possible unless C has \u2126(k/ ) columns from A, R has \u2126(k/ ) rows from A, T has \u2126(k/ ) tubes from A and U has rank \u2126(k).\nWe use a similar construction from [BW14, BDM11, DR10] and extend it to the tensor setting.\nTheorem I.1. There exists a tensor A \u2208 Rn\u00d7n\u00d7n with the following property. Consider a factorization CURT, with C \u2208 Rn\u00d7c containing c columns of A, R \u2208 Rn\u00d7r containing r rows of A, T \u2208 Rn\u00d7t containing r tubes of A, and U \u2208 Rc\u00d7r\u00d7t, such that\n\u2225\u2225\u2225\u2225\u2225\u2225 A\u2212 n\u2211\ni=1\nn\u2211\nj=1\nn\u2211\nl=1\nUi,j,l \u00b7 Ci \u2297Rj \u2297 Tl \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + )\u2016A\u2212Ak\u20162F .\nThen, for any < 1 and any k \u2265 1,\nc = \u2126(k/ ), r = \u2126(k/ ), t = \u2126(k/ ) and rank(U) \u2265 k/3.\nProof. For any i \u2208 [d], let ei \u2208 Rd denote the i-th standard basis vector. For \u03b1 > 0 and integer d > 1, consider the matrix D \u2208 R(d+1)\u00d7(d+1),\nD = [ e1 + \u03b1e2 e1 + \u03b1e3 \u00b7 \u00b7 \u00b7 e1 + \u03b1ed+1 0 ]\n=   1 1 \u00b7 \u00b7 \u00b7 1 0 \u03b1 0 \u03b1 0 . . .\n... \u03b1 0\n \nWe construct matrix B \u2208 R(d+1)k/3\u00d7(d+1)k/3 by repeating matrix D k/3 times along its main diagonal,\nB =   D D\n. . . D\n \nLet m = (d+ 1)k/3. We construct a tensor A \u2208 Rn\u00d7n\u00d7n with n = 3m by repeating matrix B three times in the following way,\nA1,j,l = Bj,l, \u2200j, l \u2208 [m]\u00d7 [m] Am+i,m+1,m+l = Bi,l,\u2200i, l \u2208 [m]\u00d7 [m] A2m+i,2m+j,2m+1 = Bi,j ,\u2200j, i \u2208 [m]\u00d7 [m]\nand 0 everywhere else. We first state some useful properties for matrix D,\nD>D =\n[ 1d1 > d + \u03b1\n2Id 0 0 0\n] \u2208 R(d+1)\u00d7(d+1)\nwhere\n\u03c321(D) = d+ \u03b1 2, \u03c32i (D) = \u03b1 2, \u2200i = 2, \u00b7 \u00b7 \u00b7 , d\n\u03c32d+1(D) = 0.\nBy definition of matrix B, we can obtain the following properties,\n\u03c32i (B) = d+ \u03b1 2, \u2200i = 1, \u00b7 \u00b7 \u00b7 , k/3 \u03c32i (B) = \u03b1 2, \u2200i = k/3 + 1, \u00b7 \u00b7 \u00b7 , dk/3 \u03c32i (B) = 0, \u2200i = dk + 1, \u00b7 \u00b7 \u00b7 , dk/3 + k/3\nBy definition of A, we can copy B into three disjoint n\u00d7n\u00d7n sub-tensors on the main diagonal of tensor A. Thus, we have\n\u03c32i (A) = d+ \u03b1 2, \u2200i = 1, \u00b7 \u00b7 \u00b7 , k \u03c32i (A) = \u03b1 2, \u2200i = k + 1, \u00b7 \u00b7 \u00b7 , dk \u03c32i (A) = 0, \u2200i = dk + 1, \u00b7 \u00b7 \u00b7 , dk + k\nLet A(k) denote the best rank-k approximation to A, and let D1 denote the best rank-1 approximation to D. Using the above properties, for any k \u2265 1, we can compute \u2016A\u2212A(k)\u20162F ,\n\u2016A\u2212Ak\u20162F = k\u2016D \u2212D1\u20162F = k(d\u2212 1)\u03b12. (76)\nSuppose we have a CUR decomposition with c\u2032 = o(k/ ) columns, r\u2032 = o(k/ ) rows or t\u2032 = o(k/ ) tubes. Since the tensor is equivalent by looking through any of the 3 dimensions/directions, we just need to show why the cost will be at least (1 + )\u2016A \u2212 Ak\u20162F if we choose t = o(k/ ) columns and t = o(k/ ) rows.\nLet C \u2208 Rn\u00d7c denote the optimal solution. Then it should have the following form,\nC =\n  C1\nC2 C3\n \nwhere C1 \u2208 Rm\u00d7c1 contains c1 columns from A1:m,1:m,1:m \u2208 Rm\u00d7m\u00d7m, C2 \u2208 Rm\u00d7c2 contains c2 columns from Am+1:2m,m+1:2m,m+1:2m \u2208 Rm\u00d7m\u00d7m, C3 \u2208 Rm\u00d7c3 contains c3 columns from A2m+1:3m,2m+1:3m,2m+1:3m \u2208 Rm\u00d7m\u00d7m.\nLet R \u2208 Rn\u00d7r denote the optimal solution. Then it should have the following form,\nR =\n  R1\nR2 R3\n \n\u2016A\u2212A(CC\u2020, RR\u2020, I)\u20162F \u2265 \u2016B \u2212R1R\u20201B\u20162F + \u2016B \u2212 C2C\u20202B\u20162F + \u2016B> \u2212 C3C\u20203B>\u20162F . (77)\nBy the analysis in Proposition 4 of [DV06], we have\n\u2016B \u2212R1R\u20201B\u20162F \u2265 (k/3)(1 + b \u00b7 \u03b1)\u2016D \u2212D(1)\u20162F . (78)\nand\n\u2016B \u2212 C2C\u20202B\u20162F \u2265 (k/3)(1 + b \u00b7 \u03b1)\u2016D \u2212D(1)\u20162F . (79)\nLet C3 \u2208 Rm\u00d7c3 contain any c3 columns from B>. Note that C3 contains c3(\u2264 t) columns from B>, equivalently C>2 contains c2 rows from B. Recall that B contains k copies of D \u2208 R(d+1)\u00d7(d+1) along its main diagonal. Even if we choose t columns of B>, the cost is at least\n\u2016B> \u2212 C3C\u20203B>\u20162F \u2265 (k/3)\u2016D \u2212D(t)\u20162F \u2265 (k/3)(d\u2212 t)\u03b12. (80) Combining Equations (76), (77), (78), (79), (80), \u03b1 = gives,\n\u2016A\u2212 CC\u2020A\u20162F \u2016A\u2212A(k)\u20162F\n\u2265 \u2016B \u2212R1R \u2020 1B\u20162F + \u2016B \u2212 C2C \u2020 2B\u20162F + \u2016B> \u2212 C3C \u2020 3B >\u20162F\n\u2016A\u2212A(k)\u20162F by Eq. (77)\n\u2265 \u2016B \u2212R1R \u2020 1B\u20162F + \u2016B \u2212 C2C \u2020 2B\u20162F + \u2016B> \u2212 C3C \u2020 3B >\u20162F\nk(d\u2212 1)\u03b12 by Eq. (76)\n\u2265 2(k/3)(1 + b )(d\u2212 1) 2 + (k/3)(d\u2212 t) 2\nk(d\u2212 1) 2 by Eq. (78),(79),(80) and \u03b1 =\n= k(d\u2212 1) 2 + (k/3)(\u2212t+ 1) 2 + 2(k/3)b (d\u2212 1) 2\nk(d\u2212 1) 2\n= 1 + (k/3) 2(2b (d\u2212 1)\u2212 t+ 1)\nk(d\u2212 1) 2\n= 1 + 2b (d\u2212 1)\u2212 t+ 1 3(d\u2212 1) \u2265 1 + (b/3) by 2t \u2264 b (d\u2212 1)/2 \u2265 1 + . by b > 3.\nwhich gives a contradiction.\nI.2 General Frobenius CURT decomposition for q-th order tensor\nIn this section, we extend the hard instance for 3rd order tensors to q-th order tensors.\nTheorem I.2. For any constant q \u2265 1, there exists a tensor A \u2208 Rn\u00d7n\u00d7\u00b7\u00b7\u00b7\u00d7n with the following property. Define\nOPT = min rank\u2212k Ak\u2208Rc1\u00d7c2\u00d7\u00b7\u00b7\u00b7\u00d7cq\n\u2016A\u2212Ak\u20162F .\nConsider a q-th order factorization CURT, with C1 \u2208 Rn\u00d7c1 containing c columns from the 1st dimension of A, C2 \u2208 Rn\u00d7c2 containing c2 columns from the 2nd dimension of A, \u00b7 \u00b7 \u00b7 , Cq \u2208 Rn\u00d7cq containing cq columns from the q-th dimension of A and a tensor U \u2208 Rc1\u00d7c2\u00d7\u00b7\u00b7\u00b7\u00d7cq , such that\n\u2225\u2225\u2225\u2225\u2225\u2225 A\u2212 n\u2211\ni1=1\nn\u2211\ni2=1\n\u00b7 \u00b7 \u00b7 n\u2211\niq=1\nUi1,i2,\u00b7\u00b7\u00b7 ,iq \u00b7 C1,i1 \u2297 C2,i2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 Cq,iq \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) OPT .\nThere exists a constant c\u2032 < 1 such that for any < c\u2032 and any k \u2265 1, c1 = \u2126(k/ ), c2 = \u2126(k/ ), \u00b7 \u00b7 \u00b7 , cq = \u2126(k/ ) and rank(U) \u2265 c\u2032k.\nProof. We use the same matrixD \u2208 R(d+1)\u00d7(d+1) as the proof of Theorem I.1. Then we can construct matrix B \u2208 R(d+1)k/q\u00d7(d+1)k/q by repeating matrix D k/q times along the its main diagonal,\nB =   D D\n. . . D\n \nLet m = (d+ 1)/q. We construct a tensor A \u2208 Rn\u00d7n\u00d7\u00b7\u00b7\u00b7\u00d7n with n = qm by repeating the matrix q times in the following way,\nA[1:m],[1:m],1,1,1,\u00b7\u00b7\u00b7 ,1,1 = B,\nAm+1,[m+1:2m],[m+1:2m],m+1,m+1,\u00b7\u00b7\u00b7 ,m+1,m+1 = B >,\nA2m+1,2m+1,[2m+1:3m],[2m+1:3m],2m+1,\u00b7\u00b7\u00b7 ,2m+1,2m+1 = B, A3m+1,3m+1,3m+1,[3m+1:4m],[3m+1:4m],\u00b7\u00b7\u00b7 ,2m+1,3m+1 = B >,\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 A(q\u22122)m+1,(q\u22122)m+1,(q\u22122)m+1,(q\u22122)m+1,(q\u22122)m+1,\u00b7\u00b7\u00b7 ,[(q\u22122)m+1:(q\u22121)m],[(q\u22122)m+1:(q\u22121)m] = B,\nA[(q\u22121)m+1:qm],(q\u22121)m+1,(q\u22121)m+1,(q\u22121)m+1,(q\u22121)m+1,\u00b7\u00b7\u00b7 ,(q\u22121)m+1,[(q\u22121)m+1:qm] = B >,\nwhere there are q/2 Bs and q/2 B>s on the right when q is even, and there are (q + 1)/2 Bs and (q \u2212 1)/2 Bs on the right when q is odd. Note that this tensor A is equivalent if we look through any of the q dimensions/directions. Similarly as before, we have\n\u2016A\u2212A(k)\u20162F = k\u2016D \u2212D(1)\u20162F = k(d\u2212 1)\u03b12.\nSuppose there is a general CURT decomposition (of this q-th order tensor), with c1 = c2 = \u00b7 \u00b7 \u00b7 cq = o(k/ ) columns from each dimension. Let C1 \u2208 Rn\u00d7c1 , C2 \u2208 Rn\u00d7c2 , \u00b7 \u00b7 \u00b7 , Cq \u2208 Rn\u00d7cq denote the optimal solution. Then the Ci should have the following form,\nC1 =   C1,1 C1,2\n. . . C1,q\n  , C2 =   C2,1 C2,2\n. . . C2,q\n  , \u00b7 \u00b7 \u00b7 , Cq =   Cq,1 Cq,2\n. . . Cq,q\n \n(In the rest of the proof, we focus on the case when q is even. Similarly, we can show the same thing when q is odd.) We have\n\u2016A\u2212A(C1C\u20201, C2C\u20202, \u00b7 \u00b7 \u00b7 , CqC\u2020q)\u20162F\n\u2265 q/2\u2211\ni=1\n\u2016B \u2212 C2i\u22121,2i\u22121C\u20202i\u22121,2i\u22121B\u20162F + \u2016B> \u2212 C2i,2iC \u2020 2i,2iB >\u20162F\n\u2265 (q/2) ( (k/q)(1 + b\u03b1)\u2016D \u2212D(1)\u20162F + (k/q)(d\u2212 t)\u03b12 ) = (q/2) ( (k/q)(1 + b\u03b1)(d\u2212 1)\u03b12 + (k/q)(d\u2212 t)\u03b12 )\nwhere the second inequality follows by Equations (79) and (80), and the third step follows by \u2016D \u2212D(1)\u20162F = (d\u2212 1)\u03b12.\nPutting it all together, we have\n\u2016A\u2212A(C1C\u20201, C2C\u20202, \u00b7 \u00b7 \u00b7 , CqC\u2020q)\u20162F \u2016A\u2212A(k)\u20162F\n\u2265 (q/2) ( (k/q)(1 + b\u03b1)(d\u2212 1)\u03b12 + (k/q)(d\u2212 t)\u03b12 )\nk(d\u2212 1)\u03b12\n= k(d\u2212 1)\u03b12 + (k/2)b\u03b1(d\u2212 1)\u03b12 + (k/q)(\u2212t+ 1)\u03b12\nk(d\u2212 1)\u03b12\n= 1 + (k/2)b\u03b1(d\u2212 1)\u03b12 + (k/q)(\u2212t+ 1)\u03b12\nk(d\u2212 1)\u03b12\n\u2264 1 + (k/3)b\u03b1(d\u2212 1)\u03b1 2 k(d\u2212 1)\u03b12 = 1 + (b/3) by = \u03b1 > 1 + by b > 3.\nwhich leads to a contradiction. Similarly we can show the rank is at least \u2126(k)."}, {"heading": "J Distributed Setting", "text": "Input data to large-scale machine learning and data mining tasks may be distributed across different machines. The communication cost becomes the major bottleneck of distributed protocols, and so there is a growing body of work on low rank matrix approximations in the distributed model [TD99, QOSG02, BCL05, BRB08, MBZ10, FEGK13, PMvdG+13, KVW14, BKLW14, BLS+16, BWZ16, WZ16, SWZ17] and also many other machine learning problems such as clustering, boosting, and column subset selection [BBLM14, BLG+15, ABW17]. Thus, it is natural to ask whether our algorithm can be applied in the distributed setting. This section will discuss the distributed Frobenius norm low rank tensor approximation protocol in the so-called arbitrary-partition model (see, e.g. [KVW14, BWZ16]).\nIn the following, we extend the definition of the arbitrary-partition model [KVW14] to fit our tensor setting.\nDefinition J.1 (Arbitrary-partition model [KVW14]). There are s machines, and the ith machine holds a tensor Ai \u2208 Rn\u00d7n\u00d7n as its local data tensor. The global data tensor is implicit and is denoted as A = \u2211s i=1Ai. Then, we say that A is arbitrarily partitioned into s matrices distributed in the s machines. In addition, there is also a coordinator. In this model, the communication is only allowed between the machines and the coordinator. The total communication cost is the total number of words delivered between machines and the coordinator. Each word has O(log(sn)) bits.\nNow, let us introduce the distributed Frobenius norm low rank tensor approximation problem in the arbitrary partition model:\nDefinition J.2 (Arbitrary-partition model Frobenius norm rank-k tensor approximation). Tensor A \u2208 Rn\u00d7n\u00d7n is arbitrarily partitioned into s matrices A1, A2, \u00b7 \u00b7 \u00b7 , As distributed in s machines respectively, and \u2200i \u2208 [s], each entry of Ai is at most O(log(sn)) bits. Given tensor A, k \u2208 N+ and an error parameter 0 < < 1, the goal is to find a distributed protocol in the model of Definition J.1 such that\n1. Upon termination, the protocol leaves three matrices U\u2217, V \u2217,W \u2217 \u2208 Rn\u00d7k on the coordinator.\n2. U\u2217, V \u2217,W \u2217 satisfies that \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nU\u2217i \u2297 V \u2217i \u2297W \u2217i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F .\n3. The communication cost is as small as possible.\nTheorem J.3. Suppose tensor A \u2208 Rn\u00d7n\u00d7n is distributed in the arbitrary partition model (See Definition J.1). There is a protocol( in Algorithm 39) which solves the problem in Definition J.2 with constant success probability. In addition, the communication complexity of the protocol is s(poly(k/ ) +O(kn)) words.\nProof. Correctness. The correctness is implied by Algorithm 2 and Algorithm 3 (Theorem C.1.) Notice that A1 = \u2211s i=1Ai,1, A2 = \u2211s i=1Ai,2, A3 = \u2211s i=1Ai,3, which means that\nY1 = T1A1S1, Y2 = T2A2S2, Y3 = T3A3S3,\nand\nC = A(T1, T2, T3).\nAccording to line 23,\nX\u22171 , X \u2217 2 , X \u2217 3 = arg min\nX1,X2,X3 \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\nj=1\n(Y1X1)j \u2297 (Y2X2)j \u2297 (Y3X3)j \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 F .\nAccording to Lemma C.3, we have \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\nj=1\n(T1A1S1X \u2217 1 )j \u2297 (T2A2S2X\u22172 )j \u2297 (T3A3S3X\u22173 )j \u2212A(T1, T2, T3) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264(1 +O( )) min X1,X2,X3 \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\nj=1\n(A1S1X1)j \u2297 (A2S2X2)j \u2297 (A3Y3X3)j \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264(1 +O( )) min U,V,W \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n,\nwhere the last inequality follows by the proof of Theorem C.1. By scaling a constant of , we complete the proof of correctness.\nCommunication complexity. Since S1, S2, S3 are w1-wise independent, and T1, T2, T3 are w2-wise independent, the communication cost of sending random seeds in line 5 is O(s(w1 + w2)) words, where w1 = O(k), w2 = O(1) (see [KVW14, CW13, Woo14, KN14]). The communication cost in line 18 is s \u00b7 poly(k/ ) words due to T1Ai,1S1, T2Ai,2S2, T3Ai,3S3 \u2208 Rpoly(k/ )\u00d7O(k/ ) and Ci = Ai(T1, T2, T3) \u2208 Rpoly(k/ )\u00d7poly(k/ )\u00d7poly(k/ ).\nNotice that, since \u2200i \u2208 [s] each entry of Ai has at most O(log(sn)) bits, each entry of Y1, Y2, Y3, C has at most O(log(sn)) bits. Due to Theorem J.7, each entry of X\u22171 , X\u22172 , X\u22173 has at most O(log(sn)) bits, and the sizes of X\u22171 , X\u22172 , X\u22173 are poly(k/ ) words. Thus the communication cost in line 24 is s \u00b7 poly(k/ ) words.\nFinally, since \u2200i \u2208 [s], U\u2217i , V \u2217i ,W \u2217i \u2208 Rn\u00d7k, the communication here is at most O(skn) words. The total communication cost is s(poly(k/ ) +O(kn)) words.\nRemark J.4. If we slightly change the goal in Definition J.2 to the following: the coordinator does not need to output U\u2217, V \u2217,W \u2217, but each machine i holds U\u2217i , V \u2217 i ,W \u2217 i such that U \u2217 = \u2211s i=1 U \u2217 i , V\n\u2217 =\u2211s i=1 V \u2217 i ,W \u2217 = \u2211s i=1W \u2217 i , then the protocol shown in Algorithm 39 does not have to do the line 28. Thus the total communication cost is at most s \u00b7 poly(k/ ) words in this setting.\nRemark J.5. Algorithm 39 needs exponential in poly(k/ ) running time since it solves a polynomial solver in line 23. Instead of solving line 23, we can solve the following optimization problem:\n\u03b1\u2217 = arg min \u03b1\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1i,j,l \u00b7 (Y1)i \u2297 (Y2)j \u2297 (Y3)l \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 F .\nSince it is actually a regression problem, it only takes polynomial running time to get \u03b1\u2217. And according to Lemma C.5,\ns1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1\u2217i,j,l \u00b7 (Y1)i \u2297 (Y2)j \u2297 (Y3)l\nAlgorithm 39 Distributed Frobenius Norm Low Rank Approximation Protocol 1: procedure DistributedFnormLowRankApproxProtocol(A, ,k,s) 2: A \u2208 Rn\u00d7n\u00d7n was arbitrarily partitioned into s matrices A1, \u00b7 \u00b7 \u00b7 , As \u2208 Rn\u00d7n\u00d7n on s machines. 3: Coordinator Machines i 4: Chooses a random seed. 5: Sends it to all machines. 6: \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 > 7: si \u2190 O(k/ ), \u2200i \u2208 [3]. 8: Agree on Si \u2208 Rn2\u00d7si , \u2200i \u2208 [3] 9: which are w1-wise independent random 10: N(0, 1/si) Gaussian matrices. 11: ti \u2190 poly(k/ ), \u2200i \u2208 [3]. 12: Agree on Ti \u2208 Rti\u00d7n, \u2200i \u2208 [3] 13: which are w2-wise independent random 14: sparse embedding matrices. 15: Compute Yi,1 \u2190 T1Ai,1S1, 16: Yi,2 \u2190 T2Ai,2S2, Yi,3 \u2190 T3Ai,3S3. 17: Send Yi,1, Yi,2, Yi,3 to the coordinator. 18: Send Ci \u2190 Ai(T1, T2, T3) to the coordinator. 19: < \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 20: Compute Y1 \u2190\ns\u2211 i=1 Yi,1, Y2 \u2190 s\u2211 i=1 Yi,2,\n21: Y3 \u2190 s\u2211 i=1 Yi,3, C \u2190 s\u2211 i=1 Ci. 22: Compute X\u22171 , X\u22172 , X\u22173 by solving 23: min\nX1,X2,X3 \u2016(Y1X1)\u2297 (Y2X2)\u2297 (Y3X3)\u2212 C\u2016F\n24: Send X\u22171 , X\u22172 , X\u22173 to machines. 25: \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 > 26: Compute U\u2217i \u2190 Ai,1S1X\u22171 , 27: V \u2217i \u2190 Ai,2S2X\u22172 , W \u2217i \u2190 Ai,3S3X\u22173 . 28: Send U\u2217i , V \u2217 i ,W \u2217 i to the coordinator. 29: < \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 30: Compute U\u2217 \u2190\u2211si=1 U\u2217i . 31: Compute V \u2217 \u2190\u2211si=1 V \u2217i . 32: Compute W \u2217 \u2190\u2211si=1W \u2217i . 33: return U\u2217, V \u2217, W \u2217. 34: end procedure\ngives a rank-O(k3/ 3) bicriteria solution. Further, similar to Theorem C.8, we can solve\nmin U\u2208Rn\u00d7s2s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\nUi+s1(j\u22121) \u2297 (Y2)i \u2297 (Y3)j \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 F ,\nwhere C = \u2211\niAi(I, T2, T3). Thus, we can obtain a rank-O(k 2/ 2) in polynomial time.\nRemark J.6. If we select sketching matrices S1, S2, S3, T1, T2, T3 to be random Cauchy matrices,\nthen we are able to compute distributed entry-wise `1 norm rank-k tensor approximation (see Theorem D.17). The communication cost is still s(poly(k/ ) + O(kn)) words. If we only require a bicriteria solution, then it only needs polynomial running time.\nUsing similar techniques as in the proof of Theorem C.45, we can obtain:\nTheorem J.7. Let maxi{ti, di} \u2264 n. Given a t1 \u00d7 t2 \u00d7 t3 tensor A and three matrices: a t1 \u00d7 d1 matrix T1, a t2 \u00d7 d2 matrix T2, and a t3 \u00d7 d3 matrix T3. For any \u03b4 > 0, if there exists a solution to\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\n(T1X1)i \u2297 (T2X2)i \u2297 (T3X3)i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n:= OPT,\nand each entry of Xi can be expressed using O(log n) bits, then there exists an algorithm that takes poly(log n) \u00b7 2O(d1k+d2k+d3k) time and outputs three matrices: X\u03021, X\u03022, and X\u03023 such that \u2016(T1X\u03021)\u2297 (T2X\u03022)\u2297 (T3X\u03023)\u2212A\u20162F = OPT."}, {"heading": "K Streaming Setting", "text": "One of the computation models which is closely related to the distributed model of computation is the streaming model. There is a growing line of work in the streaming model. Some problems are very fundamental in the streaming model such like Heavy Hitters [LNNT16, BCI+16, BCIW16], and streaming numerical linear algebra problems [CW09]. Streaming low rank matrix approximation has been extensively studied by previous work like [CW09, KL11, GP14, Lib13, KLM+14, BWZ16, SWZ17]. In this section, we show that there is a streaming algorithm which can compute a low rank tensor approximation.\nIn the following, we introduce the turnstile streaming model and the turnstile streaming tensor Frobenius norm low rank approximation problem. The following gives a formal definition of the computation model we study.\nDefinition K.1 (Turnstile model). Initially, tensor A \u2208 Rn\u00d7n\u00d7n is an all zero tensor. In the turnstile streaming model, there is a stream of update operations, and the ith update operation is in the form (xi, yi, zi, \u03b4i) where xi, yi, zi \u2208 [n], and \u03b4i \u2208 R has O(log n) bits. Each (xi, yi, zi, \u03b4i) means that Axi,yi,zi should be incremented by \u03b4i. And each entry of A has at most O(log n) bits at the end of the stream. An algorithm in this computation model is only allowed one pass over the stream. At the end of the stream, the algorithm stores a summary of A. The space complexity of the algorithm is the total number of words required to compute and store this summary while scanning the stream. Here, each word has at most O(log(n)) bits.\nThe following is the formal definition of the problem.\nDefinition K.2 (Turnstile model Frobenius norm rank-k tensor approximation). Given tensor A \u2208 Rn\u00d7n\u00d7n, k \u2208 N+ and an error parameter 1 > > 0, the goal is to design an algorithm in the streaming model of Definition K.1 such that\n1. Upon termination, the algorithm outputs three matrices U\u2217, V \u2217,W \u2217 \u2208 Rn\u00d7k.\n2. U\u2217, V \u2217,W \u2217 satisft that\n\u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nU\u2217i \u2297 V \u2217i \u2297W \u2217i \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20162F .\n3. The space complexity of the algorithm is as small as possible.\nTheorem K.3. Suppose tensor A \u2208 Rn\u00d7n\u00d7n is given in the turnstile streaming model (see Definition K.1), there is an streaming algorithm (in Algorithm 40) which solves the problem in Definition K.2 with constant success probability. In addition, the space complexity of the algorithm is poly(k/ ) +O(nk/ ) words.\nProof. Correctness. Similar to the distributed protocol, the correctness of this streaming algorithm is also implied by Algorithm 2 and Algorithm 3 (Theorem C.1.) Notice that at the end of the stream V1 = A1S1 \u2208 Rn\u00d7s1 , V2 = A2S2 \u2208 Rn\u00d7s2 , V3 = A3S3 \u2208 Rn\u00d7s3 , C = A(T1, T2, T3) \u2208 Rt1\u00d7t2\u00d7t3 . It also means that\nY1 = T1A1S1, Y2 = T2A2S2, Y3 = T3A3S3.\nAccording to line 26 of procedure TurnstileStreaming,\nX\u22171 , X \u2217 2 , X \u2217 3 = arg min\nX1\u2208Rs1\u00d7k,X2\u2208Rs2\u00d7k,X3\u2208Rs3\u00d7k \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\nj=1\n(Y1X1)j \u2297 (Y2X2)j \u2297 (Y3X3)j \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 F\nAccording to Lemma C.3, we have \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\nj=1\n(Y1X1)j \u2297 (Y2X2)j \u2297 (Y3X3)j \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\nj=1\n(T1A1S1X \u2217 1 )j \u2297 (T2A2S2X\u22172 )j \u2297 (T3A3S3X\u22173 )j \u2212A(T1, T2, T3) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 +O( )) min X1,X2,X3 \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\nj=1\n(A1S1X1)j \u2297 (A2S2X2)j \u2297 (A3Y3X3)j \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 +O( )) min U,V,W \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nUi \u2297 Vi \u2297Wi \u2212A \u2225\u2225\u2225\u2225\u2225 2\nF\n,\nwhere the last inequality follows by the proof of Theorem C.1. By scaling a constant of , we complete the proof of correctness.\nSpace complexity. Since S1, S2, S3 are w1-wise independent, and T1, T2, T3 are w2-wise independent, the space needed to construct these sketching matrices in line 3 and line 5 of procedure TurnstileStreaming is O(w1 + w2) words, where w1 = O(k), w2 = O(1) (see [KVW14, CW13, Woo14, KN14]). The cost to maintain V1, V2, V3 is O(nk/ ) words, and the cost to maintain C is poly(k/ ) words.\nNotice that, since each entry of A has at most O(log(sn)) bits, each entry of Y1, Y2, Y3, C has at most O(log(sn)) bits. Due to Theorem J.7, each entry of X\u22171 , X\u22172 , X\u22173 has at most O(log(sn)) bits, and the sizes of X\u22171 , X\u22172 , X\u22173 are poly(k/ ) words. Thus the space cost in line 26 is poly(k/ ) words.\nThe total space cost is poly(k/ ) +O(nk/ ) words.\nRemark K.4. In the Algorithm 40, for each update operation, we need O(k/ ) time to maintain matrices V1, V2, V3, and we need poly(k/ ) time to maintain tensor C. Thus the update time is poly(k/ ). At the end of the stream, the time to compute\nX\u22171 , X \u2217 2 , X \u2217 3 = arg min\nX1,X2,X3\u2208RO(k/ )\u00d7k\n\u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\nj=1\n(Y1X1)j \u2297 (Y2X2)j \u2297 (Y3X3)j \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 F ,\nis exponential in poly(k/ ) running time since it should use a polynomial system solver. Instead of computing the rank-k solution, we can solve the following:\n\u03b1\u2217 = arg min \u03b1\u2208Rs1\u00d7s2\u00d7s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1i,j,l \u00b7 (Y1)i \u2297 (Y2)j \u2297 (Y3)l \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 F\nAlgorithm 40 Turnstile Frobenius Norm Low Rank Approximation Algorithm 1: procedure TurnstileStreaming(k,S) 2: s1 \u2190 s2 \u2190 s3 \u2190 O(k/ ). 3: Construct sketching matrices Si \u2208 Rn2\u00d7si ,\u2200i \u2208 [3] where entries of S1, S2, S3 are w1-wise\nindependent random N(0, 1/si) Gaussian variables. 4: t1 \u2190 t2 \u2190 t3 \u2190 poly(k/ ). 5: Construct sparse embedding matrices Ti \u2208 Rti\u00d7n, \u2200i \u2208 [3] where entries are w2-wise inde-\npendent. 6: Initialize matrices: 7: Vi \u2190 {0}n\u00d7si ,\u2200i \u2208 [3]. 8: C \u2190 {0}t1\u00d7t2\u00d7t3 9: for i \u2208 [l] do\n10: Receive update operation (xi, yi, zi, \u03b4i) from the data stream S. 11: for r = 1\u2192 s1 do 12: (V1)xi,r \u2190 (V1)xi,r + \u03b4i \u00b7 (S1)(yi\u22121)n+zi,r. 13: end for 14: for r = 1\u2192 s2 do 15: (V2)yi,r \u2190 (V2)yi,r + \u03b4i \u00b7 (S2)(zi\u22121)n+xi,r. 16: end for 17: for r = 1\u2192 s3 do 18: (V3)zi,r \u2190 (V3)zi,r + \u03b4i \u00b7 (S3)(xi\u22121)n+yi,r. 19: end for 20: for r = 1\u2192 t1, p = 1\u2192 t2, q = 1\u2192 t3 do 21: Cr,p,q \u2190 Cr,p,q + \u03b4i \u00b7 (T1)r,xi(T2)p,yi(T3)q,zi . 22: end for 23: end for 24: Compute Y1 \u2190 T1V1, Y2 \u2190 T2V2, Y3 \u2190 T3V3. 25: Compute X\u2217i \u2208 Rsi\u00d7k,\u2200i \u2208 [3] by solving 26: min\nX1,X2,X3 \u2016(Y1X1)\u2297 (Y2X2)\u2297 (Y3X3)\u2212 C\u2016F\n27: Compute U\u2217 \u2190 V1X\u22171 , V \u2217 \u2190 V2X\u22172 , W \u2217 \u2190 V3X\u22173 . 28: return U\u2217, V \u2217,W \u2217 29: end procedure\nwhich will then give\ns1\u2211\ni=1\ns2\u2211\nj=1\ns3\u2211\nl=1\n\u03b1\u2217i,j,l \u00b7 (Y1)i \u2297 (Y2)j \u2297 (Y3)l\nto be a rank-O(k3/ 3) bicriteria solution. Further, similar to Theorem C.8, we can solve\nmin U\u2208Rn\u00d7s2s3 \u2225\u2225\u2225\u2225\u2225\u2225 s1\u2211\ni=1\ns2\u2211\nj=1\nUi+s1(j\u22121) \u2297 (Y2)i \u2297 (Y3)j \u2212 C \u2225\u2225\u2225\u2225\u2225\u2225 F\nwhere C = \u2211\niAi(I, T2, T3). Thus, we can obtain a rank-O(k 2/ 2) in polynomial time.\nRemark K.5. If we choose S1, S2, S3, T1, T2, T3 to be random Cauchy matrices, then we are able to apply the entry-wise `1 norm low rank tensor approximation algorithm (see Theorem D.17) in turnstile model."}, {"heading": "L Extension to Other Tensor Ranks", "text": "The tensor rank studied in the previous sections is also called the CP rank or canonical rank. The tensor rank can be thought of as a direct extension of the matrix rank. We would like to point out that there are other definitions of tensor rank, e.g., the tucker rank and train rank. In this section we explain how to extend our proofs to other notions of tensor rank. Section L.1 provides the extension to tucker rank, and Section L.2 provides the extension to train rank."}, {"heading": "L.1 Tensor Tucker rank", "text": "Tensor Tucker rank has been studied in a number of works [KC07, PC08, MH09, ZW13, YC14]. We provide the formal definition here:"}, {"heading": "L.1.1 Definitions", "text": "Definition L.1 (Tucker rank). Given a third order tensor A \u2208 Rn\u00d7n\u00d7n, we say A has tucker rank k if k is the smallest integer such that there exist three matrices U, V,W \u2208 Rn\u00d7k and a (small) tensor C \u2208 Rk\u00d7k\u00d7k satisfying\nAi,j,l =\nk\u2211\ni\u2032=1\nk\u2211\nj\u2032=1\nk\u2211\nl\u2032=1\nCi\u2032,j\u2032,l\u2032Ui,i\u2032Vj,j\u2032Wl,l\u2032 , \u2200i, j, l \u2208 [n]\u00d7 [n]\u00d7 [n],\nor equivalently,\nA = C(U, V,W )."}, {"heading": "L.1.2 Algorithm", "text": "Algorithm 41 Frobenius Norm Low (Tucker) Rank Approximation\n1: procedure FLowTuckerRankApprox(A,n, k, ) . Theorem L.2 2: s1 \u2190 s2 \u2190 s3 \u2190 O(k/ ). 3: t1 \u2190 t2 \u2190 t3 \u2190 poly(k, 1/ ). 4: Choose sketching matrices S1 \u2208 Rn2\u00d7s1 , S2 \u2208 Rn2\u00d7s2 , S3 \u2208 Rn2\u00d7s3 . . Definition B.18 5: Choose sketching matrices T1 \u2208 Rt1\u00d7n, T2 \u2208 Rt2\u00d7n, T3 \u2208 Rt3\u00d7n. 6: Compute AiSi,\u2200i \u2208 [3]. 7: Compute TiAiSi, \u2200i \u2208 [3]. 8: Compute B \u2190 A(T1, T2, T3). 9: Create variables for Xi \u2208 Rsi\u00d7k,\u2200i \u2208 [3].\n10: Create variables for C \u2208 Rk\u00d7k\u00d7k. 11: Run a polynomial system verifier for \u2016C((Y1X1), (Y2X2), (Y3X3))\u2212B\u20162F . 12: return C,A1S1X1, A2S2X2, and A3S3X3. 13: end procedure\nTheorem L.2. Given a third order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1 and \u2208 (0, 1), there exists an algorithm which takes O(nnz(A)) + n poly(k, 1/ ) + 2O(k2/ +k3) time and outputs three matrices U, V,W \u2208 Rn\u00d7k, and a tensor C \u2208 Rk\u00d7k\u00d7k for which\n\u2016C(U, V,W )\u2212A\u20162F \u2264 (1 + ) min tucker rank\u2212k Ak \u2016Ak \u2212A\u20162F\nholds with probability 9/10.\nProof. We define OPT to be\nOPT = min tucker rank\u2212k A\u2032\n\u2016A\u2032 \u2212A\u20162F .\nSuppose the optimal Ak = C\u2217(U\u2217, V \u2217,W \u2217). We fix C\u2217 \u2208 Rk\u00d7k\u00d7k, V \u2217 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k. We use V \u22171 , V \u22172 , \u00b7 \u00b7 \u00b7 , V \u2217k to denote the columns of V \u2217 and W \u22171 ,W \u22172 , \u00b7 \u00b7 \u00b7 ,W \u2217k to denote the columns of W \u2217.\nWe consider the following optimization problem,\nmin U1,\u00b7\u00b7\u00b7 ,Uk\u2208Rn\n\u2016C\u2217(U, V \u2217,W \u2217)\u2212A\u20162F ,\nwhich is equivalent to\nmin U1,\u00b7\u00b7\u00b7 ,Uk\u2208Rn\n\u2016U \u00b7 C\u2217(I, V \u2217,W \u2217)\u2212A\u20162F ,\nbecause C\u2217(U, V \u2217,W \u2217) = U \u00b7 C\u2217(I, V \u2217,W \u2217) according to Definition A.6. Recall that C\u2217(I, V \u2217,W \u2217) denotes a k \u00d7 n \u00d7 n tensor. Let (C\u2217(I, V \u2217,W \u2217))1 denote the matrix obtained by flattening C\u2217(I, V \u2217,W \u2217) along the first dimension. We use matrix Z1 to denote (C\u2217(I, V \u2217,W \u2217))1 \u2208 Rk\u00d7n2 . Then we can obtain the following equivalent objective function,\nmin U\u2208Rn\u00d7k\n\u2016UZ1 \u2212A1\u20162F .\nNotice that minU\u2208Rn\u00d7k \u2016UZ1 \u2212A1\u20162F = OPT, since Ak = U\u2217Z1. Let S>1 \u2208 Rs1\u00d7n\n2 be the sketching matrix defined in Definition B.18, where s1 = O(k/ ). We obtain the following optimization problem,\nmin U\u2208Rn\u00d7k\n\u2016UZ1S1 \u2212A1S1\u20162F .\nLet U\u0302 \u2208 Rn\u00d7k denote the optimal solution to the above optimization problem. Then U\u0302 = A1S1(Z1S1) \u2020. By Lemma B.22 and Theorem B.23, we have\n\u2016U\u0302Z1 \u2212A1\u20162F \u2264 (1 + ) min U\u2208Rn\u00d7k \u2016UZ1 \u2212A1\u20162F = (1 + ) OPT,\nwhich implies \u2225\u2225\u2225C\u2217(U\u0302 , V \u2217,W \u2217)\u2212A \u2225\u2225\u2225 2\nF \u2264 (1 + ) OPT .\nTo write down U\u03021, \u00b7 \u00b7 \u00b7 , U\u0302k, we use the given matrix A1, and we create s1 \u00d7 k variables for matrix (Z1S1)\n\u2020. As our second step, we fix U\u0302 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k, and we convert tensor A into matrix A2.\nLet matrix Z2 denote (C\u2217(U\u0302 , I,W \u2217))2 \u2208 Rk\u00d7n2 . We consider the following objective function,\nmin V \u2208Rn\u00d7k\n\u2016V Z2 \u2212A2\u20162F ,\nfor which the optimal cost is at most (1 + ) OPT.\nLet S>2 \u2208 Rs2\u00d7n 2 be a sketching matrix defined in Definition B.18, where s2 = O(k/ ). We\nsketch S2 on the right of the objective function to obtain a new objective function,\nmin V \u2208Rn\u00d7k\n\u2016V Z2S2 \u2212A2S2\u20162F .\nLet V\u0302 \u2208 Rn\u00d7k denote the optimal solution to the above problem. Then V\u0302 = A2S2(Z2S2)\u2020. By Lemma B.22 and Theorem B.23, we have,\n\u2016V\u0302 Z2 \u2212A2\u20162F \u2264 (1 + ) min V \u2208Rn\u00d7k \u2016V Z2 \u2212A2\u20162F \u2264 (1 + )2 OPT,\nwhich implies \u2225\u2225\u2225C\u2217(U\u0302 , V\u0302 ,W \u2217)\u2212A \u2225\u2225\u2225 2\nF \u2264 (1 + )2 OPT .\nTo write down V\u03021, \u00b7 \u00b7 \u00b7 , V\u0302k, we need to use the given matrix A2 \u2208 Rn 2\u00d7n, and we need to create s2 \u00d7 k variables for matrix (Z2S2)\u2020. As our third step, we fix the matrices U\u0302 \u2208 Rn\u00d7k and V\u0302 \u2208 Rn\u00d7k. We convert tensor A \u2208 Rn\u00d7n\u00d7n into matrix A3 \u2208 Rn2\u00d7n. Let matrix Z3 denote (C\u2217(U\u0302 , V\u0302 , I))3 \u2208 Rk\u00d7n2 . We consider the following objective function,\nmin W\u2208Rn\u00d7k\n\u2016WZ3 \u2212A3\u20162F ,\nwhich has optimal cost at most (1 + )2 OPT. Let S>3 \u2208 Rs3\u00d7n\n2 be a sketching matrix defined in Definition B.18, where s3 = O(k/ ). We sketch S3 on the right of the objective function to obtain a new objective function,\nmin W\u2208Rn\u00d7k\n\u2016WZ3S3 \u2212A3S3\u20162F .\nLet W\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above problem. Then W\u0302 = A3S3(Z3S3)\u2020. By Lemma B.22 and Theorem B.23, we have,\n\u2016W\u0302Z3 \u2212A3\u20162F \u2264 (1 + ) min W\u2208Rn\u00d7k \u2016WZ3 \u2212A3\u20162F \u2264 (1 + )3 OPT .\nThus, we have\nmin X1,X2,X3\n\u2016C\u2217((A1S1X1), (A2S2X2), (A3S3X3))\u2212A\u20162F \u2264 (1 + )3 OPT .\nLet V1 = A1S1, V2 = A2S2, and V3 = A3S3.We then apply Lemma C.3, and we obtain V\u03021, V\u03022, V\u03023, B. We then apply Theorem C.45. Correctness follows by rescaling by a constant factor.\nRunning time. Due to Definition B.18, the running time of line 7 (Algorithm 41) is O(nnz(A))+ n poly(k, 1/ ). Due to Lemma C.3, line 7 and 8 can be executed in nnz(A) + n poly(k, 1/ ) time. The running time of line 11 is given by Theorem C.45. (For simplicity, we ignore the bit complexity in the running time.)"}, {"heading": "L.2 Tensor Train rank", "text": ""}, {"heading": "L.2.1 Definitions", "text": "The tensor train rank has been studied in several works [Ose11, OTZ11, ZWZ16, PTBD16]. We provide the formal definition here.\nDefinition L.3 (Tensor Train rank). Given a third order tensor A \u2208 Rn\u00d7n\u00d7n, we say A has train rank k if k is the smallest integer such that there exist three tensors U \u2208 R1\u00d7n\u00d7k, V \u2208 Rk\u00d7n\u00d7k, W \u2208 Rk\u00d7n\u00d71 satisfying:\nAi,j,l =\n1\u2211\ni1=1\nk\u2211\ni2=1\nk\u2211\ni3=1\n1\u2211\ni4=1\nUi1,i,i2Vi2,j,i3Wi3,l,i4 ,\u2200i, j, l \u2208 [n]\u00d7 [n]\u00d7 [n],\nor equivalently,\nAi,j,l = k\u2211\ni2=1\nk\u2211\ni3=1\n(U2)i,i2(V2)j,i2+k(i3\u22121)(W2)l,i3 ,\nwhere V2 \u2208 Rn\u00d7k2 denotes the matrix obtained by flattening the tensor U along the second dimension, and (V2)i,i1+k(i2\u22121) denotes the entry in the i-th row and i1 +k(i2\u22121)-th column of V2. We similarly define U2,W2 \u2208 Rn\u00d7k.\nAlgorithm 42 Frobenius Norm Low (Train) rank Approximation\n1: procedure FLowTrainRankApprox(A,n, k, ) . Theorem L.4 2: s1 \u2190 s3 \u2190 O(k/ ). 3: s2 \u2190 O(k2/ ). 4: t1 \u2190 t2 \u2190 t3 \u2190 poly(k, 1/ ). 5: Choose sketching matrices S1 \u2208 Rn2\u00d7s1 , S2 \u2208 Rn2\u00d7s2 , S3 \u2208 Rn2\u00d7s3 . . Definition B.18 6: Choose sketching matrices T1 \u2208 Rt1\u00d7n, T2 \u2208 Rt2\u00d7n, T3 \u2208 Rt3\u00d7n. 7: Compute AiSi,\u2200i \u2208 [3]. 8: Compute TiAiSi, \u2200i \u2208 [3]. 9: Compute B \u2190 A(T1, T2, T3).\n10: Create variables for X1 \u2208 Rs1\u00d7k. 11: Create variables for X3 \u2208 Rs3\u00d7k. 12: Create variables for X2 \u2208 Rs2\u00d7k2 . 13: Create variables for C \u2208 Rk\u00d7k\u00d7k. 14: Run polynomial system verifier for \u2016\u2211ki2=1 \u2211k i3=1\n(Y1X1)i2(Y2X2)i2+k(i3\u22121)(Y3X3)i3 \u2212B\u20162F . 15: return A1S1X1, A2S2X2, and A3S3X3. 16: end procedure"}, {"heading": "L.2.2 Algorithm", "text": "Theorem L.4. Given a third order tensor A \u2208 Rn\u00d7n\u00d7n, for any k \u2265 1, \u2208 (0, 1), there exists an algorithm which takes O(nnz(A)) + n poly(k, 1/ ) + 2O(k4/ ) time and outputs three tensors U \u2208 R1\u00d7n\u00d7k, V \u2208 Rk\u00d7n\u00d7k, W \u2208 Rk\u00d7n\u00d71 such that\n\u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nk\u2211\nj=1\n(U2)i \u2297 (V2)i+k(j\u22121) \u2297 (W2)j \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) min train rank\u2212k Ak \u2016Ak \u2212A\u20162F\nholds with probability 9/10.\nProof. We define OPT as\nOPT = min train rank\u2212k A\u2032\n\u2016A\u2032 \u2212A\u20162F .\nSuppose the optimal\nAk =\nk\u2211\ni=1\nk\u2211\nj=1\nU\u2217i \u2297 V \u2217i+k(j\u22121) \u2297W \u2217j .\nWe fix V \u2217 \u2208 Rn\u00d7k2 and W \u2217 \u2208 Rn\u00d7k. We use V \u22171 , V \u22172 , \u00b7 \u00b7 \u00b7 , V \u2217k2 to denote the columns of V \u2217, and W \u22171 ,W \u2217 2 , \u00b7 \u00b7 \u00b7 ,W \u2217k to denote the columns of W \u2217.\nWe consider the following optimization problem,\nmin U\u2208Rn\u00d7k \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nk\u2211\nj=1\nUi \u2297 V \u2217i+k(j\u22121) \u2297W \u2217j \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n,\nwhich is equivalent to\nmin U\u2208Rn\u00d7k \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 U \u00b7   k\u2211 j=1 V \u22171+k(j\u22121) \u2297W \u2217j k\u2211 j=1 V \u22172+k(j\u22121) \u2297W \u2217j \u00b7 \u00b7 \u00b7\nk\u2211 j=1 V \u2217k+k(j\u22121) \u2297W \u2217j\n  \u2212A \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n.\nLet A1 \u2208 Rn\u00d7n2 denote the matrix obtained by flattening the tensor A along the first dimension. We use matrix Z1 \u2208 Rk\u00d7n2 to denote\n  k\u2211 j=1 vec(V \u22171+k(j\u22121) \u2297W \u2217j ) k\u2211 j=1 vec(V \u22172+k(j\u22121) \u2297W \u2217j )\n\u00b7 \u00b7 \u00b7 k\u2211 j=1 vec(V \u2217k+k(j\u22121) \u2297W \u2217j )\n  .\nThen we can obtain the following equivalent objective function,\nmin U\u2208Rn\u00d7k\n\u2016UZ1 \u2212A1\u20162F .\nNotice that minU\u2208Rn\u00d7k \u2016UZ1 \u2212A1\u20162F = OPT, since Ak = U\u2217Z1. Let S>1 \u2208 Rs1\u00d7n\n2 be a sketching matrix defined in Definition B.18, where s1 = O(k/ ). We obtain the following optimization problem,\nmin U\u2208Rn\u00d7k\n\u2016UZ1S1 \u2212A1S1\u20162F .\nLet U\u0302 \u2208 Rn\u00d7k denote the optimal solution to the above optimization problem. Then U\u0302 = A1S1(Z1S1) \u2020. By Lemma B.22 and Theorem B.23, we have\n\u2016U\u0302Z1 \u2212A1\u20162F \u2264 (1 + ) min U\u2208Rn\u00d7k \u2016UZ1 \u2212A1\u20162F = (1 + ) OPT,\nwhich implies \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nk\u2211\nj=1\nU\u0302i \u2297 V \u2217i+k(j\u22121) \u2297W \u2217j \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + ) OPT .\nTo write down U\u03021, \u00b7 \u00b7 \u00b7 , U\u0302k, we use the given matrix A1, and we create s1 \u00d7 k variables for matrix (Z1S1)\n\u2020. As our second step, we fix U\u0302 \u2208 Rn\u00d7k and W \u2217 \u2208 Rn\u00d7k, and we convert the tensor A into matrix A2. Let matrix Z2 \u2208 Rk2\u00d7n2 denote the matrix where the (i, j)-th row is the vectorization of U\u0302i \u2297W \u2217j . We consider the following objective function,\nmin V \u2208Rn\u00d7k\n\u2016V Z2 \u2212A2\u20162F ,\nfor which the optimal cost is at most (1 + ) OPT. Let S>2 \u2208 Rs2\u00d7n\n2 be a sketching matrix defined in Definition B.18, where s2 = O(k2/ ). We sketch S2 on the right of the objective function to obtain the new objective function,\nmin V \u2208Rn\u00d7k\n\u2016V Z2S2 \u2212A2S2\u20162F .\nLet V\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above problem. Then V\u0302 = A2S2(Z2S2)\u2020. By Lemma B.22 and Theorem B.23, we have,\n\u2016V\u0302 Z2 \u2212A2\u20162F \u2264 (1 + ) min V \u2208Rn\u00d7k \u2016V Z2 \u2212A2\u20162F \u2264 (1 + )2 OPT,\nwhich implies \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nk\u2211\nj=1\nU\u0302i \u2297 V\u0302i+k(j\u22121) \u2297W \u2217 \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + )2 OPT .\nTo write down V\u03021, \u00b7 \u00b7 \u00b7 , V\u0302k, we need to use the given matrix A2 \u2208 Rn 2\u00d7n, and we need to create s2 \u00d7 k variables for matrix (Z2S2)\u2020. As our third step, we fix the matrices U\u0302 \u2208 Rn\u00d7k and V\u0302 \u2208 Rn\u00d7k. We convert tensor A \u2208 Rn\u00d7n\u00d7n into matrix A3 \u2208 Rn2\u00d7n. Let matrix Z3 \u2208 Rk\u00d7n2 denote   \u2211k i=1 vec(U\u0302i \u2297 V\u0302i+k\u00b70)\u2211k i=1 vec(U\u0302i \u2297 V\u0302i+k\u00b71)\n\u00b7 \u00b7 \u00b7\u2211k i=1 vec(U\u0302i \u2297 V\u0302i+k\u00b7(k\u22121))\n  .\nWe consider the following objective function,\nmin W\u2208Rn\u00d7k\n\u2016WZ3 \u2212A3\u20162F ,\nwhich has optimal cost at most (1 + )2 OPT. Let S>3 \u2208 Rs3\u00d7n\n2 be a sketching matrix defined in Definition B.18, where s3 = O(k/ ). We sketch S3 on the right of the objective function to obtain a new objective function,\nmin W\u2208Rn\u00d7k\n\u2016WZ3S3 \u2212A3S3\u20162F .\nLet W\u0302 \u2208 Rn\u00d7k denote the optimal solution of the above problem. Then W\u0302 = A3S3(Z3S3)\u2020. By Lemma B.22 and Theorem B.23, we have,\n\u2016W\u0302Z3 \u2212A3\u20162F \u2264 (1 + ) min W\u2208Rn\u00d7k \u2016WZ3 \u2212A3\u20162F \u2264 (1 + )3 OPT .\nThus, we have\nmin X1,X2,X3 \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1\nk\u2211\nj=1\n(A1S1X1)i \u2297 (A2S2X2)i+k(j\u22121) \u2297 (A3S3X3)j \u2212A \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (1 + )3 OPT .\nLet V1 = A1S1, V2 = A2S2, and V3 = A3S3.We then apply Lemma C.3, and we obtain V\u03021, V\u03022, V\u03023, B. We then apply Theorem C.45. Correctness follows by rescaling by a constant factor.\nRunning time. Due to Definition B.18, the running time of line 7 (Algorithm 42) is O(nnz(A))+ n poly(k, 1/ ). Due to Lemma C.3, lines 8 and 9 can be executed in nnz(A) + n poly(k, 1/ ) time. The running time of 2O(k4/ ) comes from running Theorem C.45 (For simplicity, we ignore the bit complexity in the running time.)"}, {"heading": "M Acknowledgments", "text": "The authors would like to thank Udit Agarwal, Alexandr Andoni, Arturs Backurs, Saugata Basu, Lijie Chen, Xi Chen, Thomas Dillig, Yu Feng, Rong Ge, Daniel Hsu, Chi Jin, Ravindran Kannan, J. M. Landsberg, Qi Lei, Fu Li, Syed Mohammad Meesum, Ankur Moitra, Dana Moshkovitz, Cameron Musco, Richard Peng, Eric Price, Govind Ramnarayan, Ilya Razenshteyn, James Renegar, Rocco Servedio, Tselil Schramm, Clifford Stein, Wen Sun, Yining Wang, Zhaoran Wang, Wei Ye, Huacheng Yu, Huan Zhang, Kai Zhong, David Zuckerman for useful discussions."}], "references": [{"title": "Multiway analysis of epilepsy tensors", "author": ["Evrim Acar", "Canan Aykut-Bing\u00f6l", "Haluk Bingol", "Rasmus Bro", "B\u00fclent Yener"], "venue": "In Proceedings 15th International Conference on Intelligent Systems for Molecular Biology (ISMB) & 6th European Conference on Computational Biology (ECCB),", "citeRegEx": "Acar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Acar et al\\.", "year": 2007}, {"title": "Greedy column subset selection: New bounds and distributed algorithms", "author": ["Jason Altschuler", "Aditya Bhaskara", "Gang Fu", "Vahab Mirrokni", "Afshin Rostamizadeh", "Morteza Zadimoghaddam"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "Altschuler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Altschuler et al\\.", "year": 2016}, {"title": "Learning mixtures of ranking models", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet", "Aravindan Vijayaraghavan"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "General and robust communication-efficient algorithms for distributed clustering", "author": ["Pranjal Awasthi", "Maria-Florina Balcan", "Colin White"], "venue": "In arXiv preprint", "citeRegEx": "Awasthi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2017}, {"title": "Modeling and multiway analysis of chatroom tensors", "author": ["Evrim Acar", "Seyit A \u00c7amtepe", "Mukkai S Krishnamoorthy", "B\u00fclent Yener"], "venue": "In International Conference on Intelligence and Security Informatics,", "citeRegEx": "Acar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Acar et al\\.", "year": 2005}, {"title": "Collective sampling and analysis of high order tensors for chatroom communications", "author": ["Evrim Acar", "Seyit A Camtepe", "B\u00fclent Yener"], "venue": "In International Conference on Intelligence and Security Informatics,", "citeRegEx": "Acar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Acar et al\\.", "year": 2006}, {"title": "Homotopy analysis for tensor pca", "author": ["Anima Anandkumar", "Yuan Deng", "Rong Ge", "Hossein Mobahi"], "venue": "In arXiv preprint", "citeRegEx": "Anandkumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2016}, {"title": "Tensors in image processing and computer vision", "author": ["Santiago Aja-Fern\u00e1ndez", "Rodrigo de Luis Garcia", "Dacheng Tao", "Xuelong Li"], "venue": "Springer Science & Business Media,", "citeRegEx": "Aja.Fern\u00e1ndez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Aja.Fern\u00e1ndez et al\\.", "year": 2009}, {"title": "A spectral algorithm for latent dirichlet allocation", "author": ["Anima Anandkumar", "Dean P Foster", "Daniel J Hsu", "Sham M Kakade", "Yi-Kai Liu"], "venue": "In Advances in Neural Information Processing Systems(NIPS),", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel J. Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "A tensor approach to learning mixed membership community models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel J Hsu", "Sham M Kakade"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Computing a nonnegative matrix factorization - provably", "author": ["Sanjeev Arora", "Rong Ge", "Ravindran Kannan", "Ankur Moitra"], "venue": "In Proceedings of the 44th Symposium on Theory of Computing Conference (STOC),", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Provable learning of noisy-or networks", "author": ["Sanjeev Arora", "Rong Ge", "Tengyu Ma", "Andrej Risteski"], "venue": "In Proceedings of the 49th Annual Symposium on the Theory of Computing (STOC). ACM,", "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "Scalable Tensor Factorizations for Incomplete Data", "author": ["E. Acar", "T.G. Kolda", "D.M. Dunlavy", "M. Morup"], "venue": "In arXiv preprint", "citeRegEx": "Acar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Acar et al\\.", "year": 2010}, {"title": "Streaming algorithms via precision sampling", "author": ["Alexandr Andoni", "Robert Krauthgamer", "Krzysztof Onak"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Andoni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Andoni et al\\.", "year": 2011}, {"title": "Reinforcement learning of POMDPs using spectral methods", "author": ["Kamyar Azizzadenesheli", "Alessandro Lazaric", "Animashree Anandkumar"], "venue": "In 29th Annual Conference on Learning Theory (COLT), pages 193\u2013256", "citeRegEx": "Azizzadenesheli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Azizzadenesheli et al\\.", "year": 2016}, {"title": "Sequential transfer in multi-armed bandit with finite set of models", "author": ["Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill"], "venue": "In Advances in Neural Information Processing Systems(NIPS),", "citeRegEx": "Azar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2013}, {"title": "Sparse higher-order principal components analysis", "author": ["Genevera Allen"], "venue": "In AISTATS,", "citeRegEx": "Allen.,? \\Q2012\\E", "shortCiteRegEx": "Allen.", "year": 2012}, {"title": "Regularized tensor factorizations and higher-order principal components analysis", "author": ["Genevera I Allen"], "venue": "In arXiv preprint", "citeRegEx": "Allen.,? \\Q2012\\E", "shortCiteRegEx": "Allen.", "year": 2012}, {"title": "Fast computation of low-rank matrix approximations", "author": ["Dimitris Achlioptas", "Frank McSherry"], "venue": "J. ACM,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2007\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2007}, {"title": "Subspace embeddings for the polynomial kernel", "author": ["Haim Avron", "Huy Nguyen", "David Woodruff"], "venue": "In Advances in Neural Information Processing Systems(NIPS),", "citeRegEx": "Avron et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2014}, {"title": "\u00dcber homogene polynome in (l2)", "author": ["Stefan Banach"], "venue": "Studia Mathematica,", "citeRegEx": "Banach.,? \\Q1938\\E", "shortCiteRegEx": "Banach.", "year": 1938}, {"title": "Streaming symmetric norms via measure concentration", "author": ["Jaroslaw Blasiok", "Vladimir Braverman", "Stephen R Chestnut", "Robert Krauthgamer", "Lin F Yang"], "venue": "In Proceedings of the 49th Annual Symposium on the Theory of Computing(STOC). ACM,", "citeRegEx": "Blasiok et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Blasiok et al\\.", "year": 2017}, {"title": "Distributed balanced clustering via mapping coresets", "author": ["MohammadHossein Bateni", "Aditya Bhaskara", "Silvio Lattanzi", "Vahab Mirrokni"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bateni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bateni et al\\.", "year": 2014}, {"title": "Bptree: an `2 heavy hitters algorithm using constant memory", "author": ["Vladimir Braverman", "Stephen R Chestnut", "Nikita Ivkin", "Jelani Nelson", "Zhengyu Wang", "David P Woodruff"], "venue": "In Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems (PODS)", "citeRegEx": "Braverman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2016}, {"title": "Beating countsketch for heavy hitters in insertion streams", "author": ["Vladimir Braverman", "Stephen R Chestnut", "Nikita Ivkin", "David P Woodruff"], "venue": "In Proceedings of the 48th Annual Symposium on the Theory of Computing (STOC). https://arxiv", "citeRegEx": "Braverman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2016}, {"title": "Sketches for matrix norms: Faster, smaller and more general", "author": ["Vladimir Braverman", "Stephen R Chestnut", "Robert Krauthgamer", "Lin F Yang"], "venue": "In arXiv preprint", "citeRegEx": "Braverman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2016}, {"title": "Principal component analysis for distributed data sets with updating", "author": ["Zheng-Jian Bai", "Raymond H Chan", "Franklin T Luk"], "venue": "In Advanced Parallel Processing Technologies,", "citeRegEx": "Bai et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2005}, {"title": "Smoothed analysis of tensor decompositions", "author": ["Aditya Bhaskara", "Moses Charikar", "Ankur Moitra", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Bhaskara et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bhaskara et al\\.", "year": 2014}, {"title": "Algebraic complexity theory, volume 315", "author": ["Peter B\u00fcrgisser", "Michael Clausen", "Amin Shokrollahi"], "venue": "Springer Science & Business Media,", "citeRegEx": "B\u00fcrgisser et al\\.,? \\Q1997\\E", "shortCiteRegEx": "B\u00fcrgisser et al\\.", "year": 1997}, {"title": "Uniqueness of tensor decompositions with applications to polynomial identifiability", "author": ["Aditya Bhaskara", "Moses Charikar", "Aravindan Vijayaraghavan"], "venue": "In 27th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Bhaskara et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bhaskara et al\\.", "year": 2014}, {"title": "Computing approximate PSD factorizations", "author": ["Amitabh Basu", "Michael Dinitz", "Xin Li"], "venue": "In arXiv preprint", "citeRegEx": "Basu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2016}, {"title": "Near optimal columnbased matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "In IEEE 52nd Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "Border rank of a p \u00d7 q \u00d7 2 tensor and the optimal approximation of a pair of bilinear forms", "author": ["Dario Bini"], "venue": "Automata, languages and programming,", "citeRegEx": "Bini.,? \\Q1980\\E", "shortCiteRegEx": "Bini.", "year": 1980}, {"title": "Border rank of m \u00d7 n \u00d7 (mn-q) tensors", "author": ["Dario Bini"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Bini.,? \\Q1986\\E", "shortCiteRegEx": "Bini.", "year": 1986}, {"title": "Improved distributed principal component analysis", "author": ["Maria-Florina Balcan", "Vandana Kanchanapally", "Yingyu Liang", "David Woodruff"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Balcan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2014}, {"title": "A distributed frank-wolfe algorithm for communication-efficient sparse learning", "author": ["Aur\u00e9lien Bellet", "Yingyu Liang", "Alireza Bagheri Garakani", "Maria-Florina Balcan", "Fei Sha"], "venue": "In Proceedings of the 2015 SIAM International Conference on Data Mining (ICDM),", "citeRegEx": "Bellet et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2015}, {"title": "Communication efficient distributed kernel principal component analysis", "author": ["Maria-Florina Balcan", "Yingyu Liang", "Le Song", "David Woodruff", "Bo Xie"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Balcan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2016}, {"title": "Noisy tensor completion via the sum-of-squares hierarchy", "author": ["Boaz Barak", "Ankur Moitra"], "venue": "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,", "citeRegEx": "Barak and Moitra.,? \\Q2016\\E", "shortCiteRegEx": "Barak and Moitra.", "year": 2016}, {"title": "An improved approximation algorithm for the column subset selection problem", "author": ["Christos Boutsidis", "Michael W Mahoney", "Petros Drineas"], "venue": "In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Boutsidis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2009}, {"title": "Matrix and tensor factorization methods for natural language processing", "author": ["Guillaume Bouchard", "Jason Naradowsky", "Sebastian Riedel", "Tim Rockt\u00e4schel", "Andreas Vlachos"], "venue": "In ACL (Tutorial Abstracts),", "citeRegEx": "Bouchard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bouchard et al\\.", "year": 2015}, {"title": "Topics in matrix sampling algorithms", "author": ["Christos Boutsidis"], "venue": "In Ph.D. Thesis. arXiv preprint", "citeRegEx": "Boutsidis.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis.", "year": 2011}, {"title": "On the combinatorial and algebraic complexity of quantifier elimination", "author": ["Saugata Basu", "Richard Pollack", "Marie-Fran\u00e7oise Roy"], "venue": "J. ACM,", "citeRegEx": "Basu et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Basu et al\\.", "year": 1996}, {"title": "Distributed principal component analysis for wireless sensor", "author": ["Yann-Ael Le Borgne", "Sylvain Raybaud", "Gianluca Bontempi"], "venue": "networks. Sensors,", "citeRegEx": "Borgne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Borgne et al\\.", "year": 2008}, {"title": "A new sampling technique for tensors", "author": ["Srinadh Bhojanapalli", "Sujay Sanghavi"], "venue": "In arXiv preprint", "citeRegEx": "Bhojanapalli and Sanghavi.,? \\Q2015\\E", "shortCiteRegEx": "Bhojanapalli and Sanghavi.", "year": 2015}, {"title": "Twice-ramanujan sparsifiers", "author": ["Joshua Batson", "Daniel A Spielman", "Nikhil Srivastava"], "venue": "In SIAM Journal on Computing,", "citeRegEx": "Batson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Batson et al\\.", "year": 2012}, {"title": "Optimal cur matrix decompositions", "author": ["Christos Boutsidis", "David P Woodruff"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Boutsidis and Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Boutsidis and Woodruff.", "year": 2014}, {"title": "Optimal principal component analysis in distributed and streaming models", "author": ["Christos Boutsidis", "David P Woodruff", "Peilin Zhong"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing (STOC),", "citeRegEx": "Boutsidis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2016}, {"title": "Anaylsis of individual differences in multidimensional scaling via an n-way generalization of eckart-young", "author": ["J Douglas Carroll", "Jih-Jie Chang"], "venue": "decomposition. Psychometrika,", "citeRegEx": "Carroll and Chang.,? \\Q1970\\E", "shortCiteRegEx": "Carroll and Chang.", "year": 1970}, {"title": "Generalizing the column\u2013row matrix decomposition to multi-way arrays", "author": ["Cesar F Caiafa", "Andrzej Cichocki"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Caiafa and Cichocki.,? \\Q2010\\E", "shortCiteRegEx": "Caiafa and Cichocki.", "year": 2010}, {"title": "The fast cauchy transform and faster robust linear regression", "author": ["Kenneth L Clarkson", "Petros Drineas", "Malik Magdon-Ismail", "Michael W Mahoney", "Xiangrui Meng", "David P Woodruff"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Clarkson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2013}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["Michael B Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC),", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Subgradient and sampling algorithms for `1 regression", "author": ["Kenneth L Clarkson"], "venue": "In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms (SODA),", "citeRegEx": "Clarkson.,? \\Q2005\\E", "shortCiteRegEx": "Clarkson.", "year": 2005}, {"title": "Tensor decomposition of eeg signals: a brief review", "author": ["Fengyu Cong", "Qiu-Hua Lin", "Li-Dan Kuang", "Xiao-Feng Gong", "Piia Astikainen", "Tapani Ristaniemi"], "venue": "Journal of neuroscience methods,", "citeRegEx": "Cong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cong et al\\.", "year": 2015}, {"title": "Uniform sampling for matrix approximation", "author": ["Michael B Cohen", "Yin Tat Lee", "Cameron Musco", "Christopher Musco", "Richard Peng", "Aaron Sidford"], "venue": "In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science (ITCS),", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Iterative block tensor singular value thresholding for extraction of low rank component of image data", "author": ["Longxi Chen", "Yipeng Liu", "Ce Zhu"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Tensor decompositions for signal processing applications: From two-way to multiway component analysis", "author": ["Andrzej Cichocki", "Danilo Mandic", "Lieven De Lathauwer", "Guoxu Zhou", "Qibin Zhao", "Cesar Caiafa", "Huy Anh Phan"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Cichocki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2015}, {"title": "Optimal approximate matrix product in terms of stable rank", "author": ["Michael B Cohen", "Jelani Nelson", "David P Woodruff"], "venue": "In Proceedings of the 43rd International Colloquium on Automata, Languages and Programming (ICALP),", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Tensor Decompositions, State of the Art and Applications", "author": ["P. Comon"], "venue": "ArXiv eprints,", "citeRegEx": "Comon.,? \\Q2009\\E", "shortCiteRegEx": "Comon.", "year": 2009}, {"title": "`p row sampling by lewis weights", "author": ["Michael B. Cohen", "Richard Peng"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC),", "citeRegEx": "Cohen and Peng.,? \\Q2015\\E", "shortCiteRegEx": "Cohen and Peng.", "year": 2015}, {"title": "Fastmotif: spectral sequence motif discovery", "author": ["Nicol\u00f3 Colombo", "Nikos Vlassis"], "venue": "Bioinformatics, pages 2623\u20132631,", "citeRegEx": "Colombo and Vlassis.,? \\Q2015\\E", "shortCiteRegEx": "Colombo and Vlassis.", "year": 2015}, {"title": "Matrix multiplication via arithmetic progressions", "author": ["Don Coppersmith", "Shmuel Winograd"], "venue": "In Proceedings of the nineteenth annual ACM symposium on Theory of computing,", "citeRegEx": "Coppersmith and Winograd.,? \\Q1987\\E", "shortCiteRegEx": "Coppersmith and Winograd.", "year": 1987}, {"title": "Numerical linear algebra in the streaming model", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the 41st Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2009\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2009}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Symposium on Theory of Computing Conference,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Input sparsity and hardness for robust subspace approximation", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Clarkson and Woodruff.,? \\Q2015\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2015}, {"title": "Sketching for m-estimators: A unified approach to robust regression", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In Proceedings of the Twenty-Sixth Annual ACMSIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Clarkson and Woodruff.,? \\Q2015\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2015}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Kai-Wei Chang", "Scott Wen-tau Yih", "Bishan Yang", "Chris Meek"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "Sampling algorithms and coresets for `p regression", "author": ["Anirban Dasgupta", "Petros Drineas", "Boulos Harb", "Ravi Kumar", "Michael W Mahoney"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Dasgupta et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2009}, {"title": "Algorithmic lower bounds: Fun with hardness proofs, lecture 13", "author": ["Erik Demaine"], "venue": "In MIT Course", "citeRegEx": "Demaine.,? \\Q2014\\E", "shortCiteRegEx": "Demaine.", "year": 2014}, {"title": "From matrix to tensor: Multilinear algebra and signal processing", "author": ["Lieven De Lathauwer", "Bart De Moor"], "venue": "In Institute of Mathematics and Its Applications Conference Series,", "citeRegEx": "Lathauwer and Moor.,? \\Q1998\\E", "shortCiteRegEx": "Lathauwer and Moor.", "year": 1998}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["Petros Drineas", "Malik Magdon-Ismail", "Michael W Mahoney", "David P Woodruff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2012}, {"title": "Subspace sampling and relative-error matrix approximation: Column-based methods. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 9th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "APPROX 2006 and 10th International Workshop on Random-", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Subspace sampling and relative-error matrix approximation: Column-row-based methods", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "In Algorithms - ESA", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Relative-error CUR matrix decompositions", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["Amit Deshpande", "Luis Rademacher"], "venue": "In 2010 51st Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Deshpande and Rademacher.,? \\Q2010\\E", "shortCiteRegEx": "Deshpande and Rademacher.", "year": 2010}, {"title": "Tensor rank and the ill-posedness of the best lowrank approximation problem", "author": ["Vin De Silva", "Lek-Heng Lim"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Silva and Lim.,? \\Q2008\\E", "shortCiteRegEx": "Silva and Lim.", "year": 2008}, {"title": "Adaptive sampling and fast low-rank matrix approximation. In Approximation, Randomization, and Combinatorial Optimization", "author": ["Amit Deshpande", "Santosh Vempala"], "venue": "Algorithms and Techniques,", "citeRegEx": "Deshpande and Vempala.,? \\Q2006\\E", "shortCiteRegEx": "Deshpande and Vempala.", "year": 2006}, {"title": "Kronecker product and spline regression", "author": ["Huaian Diao", "David P. Woodruff"], "venue": null, "citeRegEx": "Diao and Woodruff.,? \\Q2017\\E", "shortCiteRegEx": "Diao and Woodruff.", "year": 2017}, {"title": "A newton-grassmann method for computing the best multilinear rank-(r1,r2,r3) approximation of a tensor", "author": ["Lars Eld\u00e9n", "Berkant Savas"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Eld\u00e9n and Savas.,? \\Q2009\\E", "shortCiteRegEx": "Eld\u00e9n and Savas.", "year": 2009}, {"title": "Distributed column subset selection on mapreduce", "author": ["Ahmed K Farahat", "Ahmed Elgohary", "Ali Ghodsi", "Mohamed S Kamel"], "venue": "IEEE 13th International Conference on Data Mining (ICDM),", "citeRegEx": "Farahat et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Farahat et al\\.", "year": 2013}, {"title": "Relations between average case complexity and approximation complexity", "author": ["Uriel Feige"], "venue": "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing(STOC),", "citeRegEx": "Feige.,? \\Q2002\\E", "shortCiteRegEx": "Feige.", "year": 2002}, {"title": "Fast monte-carlo algorithms for finding low-rank approximations", "author": ["Alan M. Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": "J. ACM,", "citeRegEx": "Frieze et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Frieze et al\\.", "year": 2004}, {"title": "Nkengla. Fast low rank approximations of matrices and tensors", "author": ["Shmuel Friedland", "V Mehrmann", "A Miedlar"], "venue": "Electron. J. Linear Algebra,", "citeRegEx": "Friedland et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Friedland et al\\.", "year": 2011}, {"title": "On best rank one approximation of tensors", "author": ["Shmuel Friedland", "Volker Mehrmann", "Renato Pajarola", "Susanne K. Suter"], "venue": "Numerical Lin. Alg. with Applic.,", "citeRegEx": "Friedland et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Friedland et al\\.", "year": 2013}, {"title": "Tensor-based trapdoors for cvp and their application to public key cryptography", "author": ["Roger Fischlin", "Jean-Pierre Seifert"], "venue": "Cryptography and Coding,", "citeRegEx": "Fischlin and Seifert.,? \\Q1999\\E", "shortCiteRegEx": "Fischlin and Seifert.", "year": 1999}, {"title": "Generalized rank-constrained matrix approximations", "author": ["Shmuel Friedland", "Anatoli Torokhti"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Friedland and Torokhti.,? \\Q2007\\E", "shortCiteRegEx": "Friedland and Torokhti.", "year": 2007}, {"title": "Low-rank approximation of tensors", "author": ["Shmuel Friedland", "Venu Tammali"], "venue": null, "citeRegEx": "Friedland and Tammali.,? \\Q2015\\E", "shortCiteRegEx": "Friedland and Tammali.", "year": 2015}, {"title": "Robust tensor decomposition with gross corruption", "author": ["Quanquan Gu", "Huan Gui", "Jiawei Han"], "venue": "In Advances in Neural Information Processing Systems(NIPS),", "citeRegEx": "Gu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2014}, {"title": "Learning mixtures of gaussians in high dimensions", "author": ["Rong Ge", "Qingqing Huang", "Sham M Kakade"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC),", "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Some simplified npcomplete graph problems", "author": ["Michael R Garey", "David S. Johnson", "Larry Stockmeyer"], "venue": "Theoretical computer science,", "citeRegEx": "Garey et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Garey et al\\.", "year": 1976}, {"title": "An approximation hardness result for bipartite clique", "author": ["Andreas Goerdt", "Andr\u00e9 Lanka"], "venue": "In Electronic Colloquium on Computational Complexity, Report,", "citeRegEx": "Goerdt and Lanka.,? \\Q2004\\E", "shortCiteRegEx": "Goerdt and Lanka.", "year": 2004}, {"title": "Decomposing overcomplete 3rd order tensors using sumof-squares algorithms. In The 18th. International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX\u20192015), and the 19th", "author": ["Rong Ge", "Tengyu Ma"], "venue": "International Workshop on Randomization and Computation (RANDOM\u20192015). https://arxiv.org/pdf/1504.05287,", "citeRegEx": "Ge and Ma.,? \\Q2015\\E", "shortCiteRegEx": "Ge and Ma.", "year": 2015}, {"title": "Relative errors for deterministic low-rank matrix approximations", "author": ["Mina Ghashami", "Jeff M Phillips"], "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Ghashami and Phillips.,? \\Q2014\\E", "shortCiteRegEx": "Ghashami and Phillips.", "year": 2014}, {"title": "Robust low-rank tensor recovery: Models and algorithms", "author": ["Donald Goldfarb", "Zhiwei Qin"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Goldfarb and Qin.,? \\Q2014\\E", "shortCiteRegEx": "Goldfarb and Qin.", "year": 2014}, {"title": "Foundations of the parafac procedure: Models and conditions for an \u201cexplanatory\u201d multi-modal factor analysis", "author": ["Richard A Harshman"], "venue": null, "citeRegEx": "Harshman.,? \\Q1970\\E", "shortCiteRegEx": "Harshman.", "year": 1970}, {"title": "Tensor rank is np-complete", "author": ["Johan H\u00e5stad"], "venue": "Journal of Algorithms,", "citeRegEx": "H\u00e5stad.,? \\Q1990\\E", "shortCiteRegEx": "H\u00e5stad.", "year": 1990}, {"title": "On bounded occurrence constraint satisfaction", "author": ["Johan H\u00e5stad"], "venue": "Information Processing Letters,", "citeRegEx": "H\u00e5stad.,? \\Q2000\\E", "shortCiteRegEx": "H\u00e5stad.", "year": 2000}, {"title": "Some optimal inapproximability results", "author": ["Johan H\u00e5stad"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "H\u00e5stad.,? \\Q2001\\E", "shortCiteRegEx": "H\u00e5stad.", "year": 2001}, {"title": "Robust tensor factorization using r 1 norm", "author": ["Heng Huang", "Chris Ding"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Huang and Ding.,? \\Q2008\\E", "shortCiteRegEx": "Huang and Ding.", "year": 2008}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science(ITCS),", "citeRegEx": "Hsu and Kakade.,? \\Q2013\\E", "shortCiteRegEx": "Hsu and Kakade.", "year": 2013}, {"title": "Most tensor problems are np-hard", "author": ["Christopher J Hillar", "Lek-Heng Lim"], "venue": "In Journal of the ACM (JACM),", "citeRegEx": "Hillar and Lim.,? \\Q2013\\E", "shortCiteRegEx": "Hillar and Lim.", "year": 2013}, {"title": "Sparse image coding using a 3d non-negative tensor factorization", "author": ["Tamir Hazan", "Simon Polak", "Amnon Shashua"], "venue": "In Tenth IEEE International Conference on Computer Vision(ICCV),", "citeRegEx": "Hazan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2005}, {"title": "Tensor principal component analysis via sum-of-square proofs", "author": ["Samuel B Hopkins", "Jonathan Shi", "David Steurer"], "venue": "In 28th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Hopkins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2015}, {"title": "Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors", "author": ["Samuel B Hopkins", "Tselil Schramm", "Jonathan Shi", "David Steurer"], "venue": "In Proceedings of the 48th Annual Symposium on the Theory of Computing. ACM,", "citeRegEx": "Hopkins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2016}, {"title": "Which problems have strongly exponential complexity", "author": ["Russell Impagliazzo", "Ramamohan Paturi", "Francis Zane"], "venue": "In Proceedings. 39th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Impagliazzo et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Impagliazzo et al\\.", "year": 1998}, {"title": "BPP if E requires exponential circuits: Derandomizing the XOR lemma", "author": ["Russell Impagliazzo", "Avi Wigderson. P"], "venue": "In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing (STOC),", "citeRegEx": "Impagliazzo and P,? \\Q1997\\E", "shortCiteRegEx": "Impagliazzo and P", "year": 1997}, {"title": "Tensor principal component analysis via convex optimization", "author": ["Bo Jiang", "Shiqian Ma", "Shuzhong Zhang"], "venue": "Mathematical Programming,", "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "Learning mixtures of discrete product distributions using spectral decompositions", "author": ["Prateek Jain", "Sewoong Oh"], "venue": "In 27th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Jain and Oh.,? \\Q2014\\E", "shortCiteRegEx": "Jain and Oh.", "year": 2014}, {"title": "Provable tensor factorization with missing data", "author": ["Prateek Jain", "Sewoong Oh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Jain and Oh.,? \\Q2014\\E", "shortCiteRegEx": "Jain and Oh.", "year": 2014}, {"title": "On the minimum of a polynomial function on a basic closed semialgebraic set and applications", "author": ["Gabriela Jeronimo", "Daniel Perrucci", "Elias Tsigaridas"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Jeronimo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jeronimo et al\\.", "year": 2013}, {"title": "Beating the perils of nonconvexity: Guaranteed training of neural networks using tensor methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "In arXiv preprint", "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering", "author": ["Alexandros Karatzoglou", "Xavier Amatriain", "Linas Baltrunas", "Nuria Oliver"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems,", "citeRegEx": "Karatzoglou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Karatzoglou et al\\.", "year": 2010}, {"title": "The tophits model for higher-order web link analysis", "author": ["Tamara Kolda", "Brett Bader"], "venue": "In Workshop on link analysis, counterterrorism and security,", "citeRegEx": "Kolda and Bader.,? \\Q2006\\E", "shortCiteRegEx": "Kolda and Bader.", "year": 2006}, {"title": "Tensor decompositions and applications", "author": ["Tamara G. Kolda", "Brett W. Bader"], "venue": "SIAM Review,", "citeRegEx": "Kolda and Bader.,? \\Q2009\\E", "shortCiteRegEx": "Kolda and Bader.", "year": 2009}, {"title": "Nonnegative tucker decomposition", "author": ["Yong-Deok Kim", "Seungjin Choi"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).,", "citeRegEx": "Kim and Choi.,? \\Q2007\\E", "shortCiteRegEx": "Kim and Choi.", "year": 2007}, {"title": "On the non-existence of optimal solutions and the occurrence of \u201cdegeneracy", "author": ["Wim P Krijnen", "Theo K Dijkstra", "Alwin Stegeman"], "venue": "in the candecomp/parafac model. Psychometrika,", "citeRegEx": "Krijnen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krijnen et al\\.", "year": 2008}, {"title": "How 3-mfa data can cause degenerate parafac solutions, among other relationships", "author": ["JB Kruskal", "RA Harshman", "ME Lundy"], "venue": "Multiway data analysis,", "citeRegEx": "Kruskal et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Kruskal et al\\.", "year": 1989}, {"title": "Spectral sparsification in the semi-streaming setting", "author": ["J. Kelner", "A. Levin"], "venue": "In Symposium on Theoretical Aspects of Computer Science (STACS),", "citeRegEx": "Kelner and Levin.,? \\Q2011\\E", "shortCiteRegEx": "Kelner and Levin.", "year": 2011}, {"title": "Single pass spectral sparsification in dynamic streams", "author": ["Michael Kapralov", "Yin Tat Lee", "Cameron Musco", "Christopher Musco", "Aaron Sidford"], "venue": "In 2014 IEEE 55th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Kapralov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kapralov et al\\.", "year": 2014}, {"title": "Shifted power method for computing tensor eigenpairs", "author": ["Tamara G Kolda", "Jackson R Mayo"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Kolda and Mayo.,? \\Q2011\\E", "shortCiteRegEx": "Kolda and Mayo.", "year": 2011}, {"title": "Sparser johnson-lindenstrauss transforms", "author": ["Daniel M Kane", "Jelani Nelson"], "venue": "In Journal of the ACM (JACM),", "citeRegEx": "Kane and Nelson.,? \\Q2014\\E", "shortCiteRegEx": "Kane and Nelson.", "year": 2014}, {"title": "Three-mode principal component analysis: Theory and applications, volume 2", "author": ["Pieter M Kroonenberg"], "venue": "DSWO press,", "citeRegEx": "Kroonenberg.,? \\Q1983\\E", "shortCiteRegEx": "Kroonenberg.", "year": 1983}, {"title": "Scalable tensor decompositions for multi-aspect data mining", "author": ["Tamara G Kolda", "Jimeng Sun"], "venue": "In Eighth IEEE International Conference on Data Mining (ICDM),", "citeRegEx": "Kolda and Sun.,? \\Q2008\\E", "shortCiteRegEx": "Kolda and Sun.", "year": 2008}, {"title": "Principal component analysis and higher correlations for distributed data", "author": ["Ravindran Kannan", "Santosh S Vempala", "David P Woodruff"], "venue": "In Proceedings of The 27th Conference on Learning Theory (COLT),", "citeRegEx": "Kannan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2014}, {"title": "Secure tensor decomposition using fully homomorphic encryption scheme", "author": ["Liwei Kuang", "Laurence Yang", "Jun Feng", "Mianxiong Dong"], "venue": "IEEE Transactions on Cloud Computing,", "citeRegEx": "Kuang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kuang et al\\.", "year": 2015}, {"title": "The border rank of the multiplication of 2\u00d7 2 matrices is seven", "author": ["J Landsberg"], "venue": "In Journal of the American Mathematical Society,", "citeRegEx": "Landsberg.,? \\Q2006\\E", "shortCiteRegEx": "Landsberg.", "year": 2006}, {"title": "Tensors: geometry and applications, volume 128", "author": ["Joseph M Landsberg"], "venue": "American Mathematical Society Providence, RI, USA., http://www.math.tamu.edu/ ~joseph.landsberg/Tbookintro.pdf,", "citeRegEx": "Landsberg.,? \\Q2012\\E", "shortCiteRegEx": "Landsberg.", "year": 2012}, {"title": "Tensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization", "author": ["Canyi Lu", "Jiashi Feng", "Yudong Chen", "Wei Liu", "Zhouchen Lin", "Shuicheng Yan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Simple and deterministic matrix sketching", "author": ["Edo Liberty"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD),", "citeRegEx": "Liberty.,? \\Q2013\\E", "shortCiteRegEx": "Liberty.", "year": 2013}, {"title": "Lower bounds based on the exponential time hypothesis", "author": ["Daniel Lokshtanov", "D\u00e1niel Marx", "Saket Saurabh"], "venue": "In Bull. EATCS", "citeRegEx": "Lokshtanov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lokshtanov et al\\.", "year": 2011}, {"title": "A multilinear singular value decomposition", "author": ["Lieven De Lathauwer", "Bart De Moor", "Joos Vandewalle"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["Ji Liu", "Przemyslaw Musialski", "Peter Wonka", "Jieping Ye"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Heavy hitters via cluster-preserving clustering", "author": ["Kasper Green Larsen", "Jelani Nelson", "Huy L Nguyen", "Mikkel Thorup"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Larsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2016}, {"title": "Multi-relational learning using weighted tensor decomposition with modular loss", "author": ["Ben London", "Theodoros Rekatsinas", "Bert Huang", "Lise Getoor"], "venue": "In arXiv preprint", "citeRegEx": "London et al\\.,? \\Q2013\\E", "shortCiteRegEx": "London et al\\.", "year": 2013}, {"title": "Low-rank tensors for scoring dependency structures. In Association for Computational Linguistics(ACL)", "author": ["Tao Lei", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola"], "venue": "Best student paper award,", "citeRegEx": "Lei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "High-order lowrank tensors for semantic role labeling", "author": ["Tao Lei", "Yuan Zhang", "Alessandro Moschitti", "Regina Barzilay"], "venue": "Proceedings of the 2015 Conference of the North America Chapter of the Association For Computational Linguistics\u2013 Human Language Technologies (NAACLHLT", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Consensus-based distributed principal component analysis in wireless sensor networks", "author": ["Sergio V Macua", "Pavle Belanovic", "Santiago Zazo"], "venue": "In Signal Processing Advances in Wireless Communications (SPAWC),", "citeRegEx": "Macua et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Macua et al\\.", "year": 2010}, {"title": "Sparse coding and automatic relevance determination for multi-way models", "author": ["Morten M\u00f8rup", "Lars Kai Hansen"], "venue": "In SPARS\u201909-Signal Processing with Adaptive Sparse Structured Representations,", "citeRegEx": "M\u00f8rup and Hansen.,? \\Q2009\\E", "shortCiteRegEx": "M\u00f8rup and Hansen.", "year": 2009}, {"title": "Successive rank-one approximations for nearly orthogonally decomposable symmetric tensors", "author": ["Cun Mu", "Daniel Hsu", "Donald Goldfarb"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Mu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mu et al\\.", "year": 2015}, {"title": "Square deal: Lower bounds and improved relaxations for tensor recovery", "author": ["Cun Mu", "Bo Huang", "John Wright", "Donald Goldfarb"], "venue": "In The Thirty-first International Conference on Machine Learning (ICML),", "citeRegEx": "Mu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mu et al\\.", "year": 2014}, {"title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression", "author": ["Xiangrui Meng", "Michael W Mahoney"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Meng and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Meng and Mahoney.", "year": 2013}, {"title": "Tensor-cur decompositions for tensor-based data", "author": ["Michael W Mahoney", "Mauro Maggioni", "Petros Drineas"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Mahoney et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mahoney et al\\.", "year": 2008}, {"title": "An almost optimal algorithm for computing nonnegative rank", "author": ["Ankur Moitra"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Moitra.,? \\Q2013\\E", "shortCiteRegEx": "Moitra.", "year": 2013}, {"title": "Applications of tensor (multiway array) factorizations and decompositions in data mining", "author": ["Morten M\u00f8rup"], "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,", "citeRegEx": "M\u00f8rup.,? \\Q2011\\E", "shortCiteRegEx": "M\u00f8rup.", "year": 2011}, {"title": "Learning nonsingular phylogenies and hidden markov models", "author": ["Elchanan Mossel", "S\u00e9bastien Roch"], "venue": "In Proceedings of the thirty-seventh annual ACM symposium on Theory of computing (STOC),", "citeRegEx": "Mossel and Roch.,? \\Q2005\\E", "shortCiteRegEx": "Mossel and Roch.", "year": 2005}, {"title": "Two-query pcp with subconstant error", "author": ["Dana Moshkovitz", "Ran Raz"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Moshkovitz and Raz.,? \\Q2010\\E", "shortCiteRegEx": "Moshkovitz and Raz.", "year": 2010}, {"title": "Polynomial-time tensor decompositions with sum-of-squares", "author": ["Tengyu Ma", "Jonathan Shi", "David Steurer"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "1-pass relative-error lp-sampling with applications", "author": ["Morteza Monemizadeh", "David P Woodruff"], "venue": "In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms,", "citeRegEx": "Monemizadeh and Woodruff.,? \\Q2010\\E", "shortCiteRegEx": "Monemizadeh and Woodruff.", "year": 2010}, {"title": "Random walk in a simplex and quadratic optimization over convex polytopes", "author": ["Yurii Nesterov"], "venue": "CORE,", "citeRegEx": "Nesterov,? \\Q2003\\E", "shortCiteRegEx": "Nesterov", "year": 2003}, {"title": "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L Nguy\u00ean"], "venue": "IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2013}, {"title": "Learning mixed multinomial logit model from ordinal data", "author": ["Sewoong Oh", "Devavrat Shah"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Oh and Shah.,? \\Q2014\\E", "shortCiteRegEx": "Oh and Shah.", "year": 2014}, {"title": "Tensor-train decomposition", "author": ["Ivan V. Oseledets"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "Oseledets.,? \\Q2011\\E", "shortCiteRegEx": "Oseledets.", "year": 2011}, {"title": "Tucker dimensionality reduction of three-dimensional arrays in linear time", "author": ["Ivan V Oseledets", "DV Savostianov", "Eugene E Tyrtyshnikov"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Oseledets et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oseledets et al\\.", "year": 2008}, {"title": "Breaking the curse of dimensionality, or how to use svd in many dimensions", "author": ["Ivan V Oseledets", "Eugene E Tyrtyshnikov"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Oseledets and Tyrtyshnikov.,? \\Q2009\\E", "shortCiteRegEx": "Oseledets and Tyrtyshnikov.", "year": 2009}, {"title": "Tensor-train ranks for matrices and their inverses", "author": ["Ivan Oseledets", "Eugene Tyrtyshnikov", "Nickolai Zamarashkin"], "venue": "Computational Methods in Applied Mathematics Comput. Methods Appl. Math.,", "citeRegEx": "Oseledets et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Oseledets et al\\.", "year": 2011}, {"title": "A weighted non-negative least squares algorithm for threeway \u201cparafac\u201d factor analysis", "author": ["Pentti Paatero"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Paatero.,? \\Q1997\\E", "shortCiteRegEx": "Paatero.", "year": 1997}, {"title": "Construction and analysis of degenerate parafac models", "author": ["Pentti Paatero"], "venue": "Journal of chemometrics,", "citeRegEx": "Paatero.,? \\Q2000\\E", "shortCiteRegEx": "Paatero.", "year": 2000}, {"title": "Compressed matrix multiplication", "author": ["Rasmus Pagh"], "venue": "ACM Transactions on Computation Theory (TOCT),", "citeRegEx": "Pagh.,? \\Q2013\\E", "shortCiteRegEx": "Pagh.", "year": 2013}, {"title": "Rethinking lda: moment matching for discrete ica", "author": ["Anastasia Podosinnikova", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems(NIPS),", "citeRegEx": "Podosinnikova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Podosinnikova et al\\.", "year": 2015}, {"title": "Fast and efficient algorithms for nonnegative tucker decomposition", "author": ["Anh Phan", "Andrzej Cichocki"], "venue": "Advances in Neural Networks-ISNN", "citeRegEx": "Phan and Cichocki.,? \\Q2008\\E", "shortCiteRegEx": "Phan and Cichocki.", "year": 2008}, {"title": "Robust tensor analysis with l1-norm", "author": ["Yanwei Pang", "Xuelong Li", "Yuan Yuan"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology,", "citeRegEx": "Pang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2010}, {"title": "Elemental: A new framework for distributed memory dense matrix computations", "author": ["Jack Poulson", "Bryan Marker", "Robert A van de Geijn", "Jeff R Hammond", "Nichols A Romero"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "Poulson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Poulson et al\\.", "year": 2013}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Ninh Pham", "Rasmus Pagh"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining(KDD),", "citeRegEx": "Pham and Pagh.,? \\Q2013\\E", "shortCiteRegEx": "Pham and Pagh.", "year": 2013}, {"title": "Exact tensor completion with sum-of-squares", "author": ["Aaron Potechin", "David Steurer"], "venue": "In arXiv preprint", "citeRegEx": "Potechin and Steurer.,? \\Q2017\\E", "shortCiteRegEx": "Potechin and Steurer.", "year": 2017}, {"title": "Efficient tensor completion: Low-rank tensor train", "author": ["Ho N Phien", "Hoang D Tuan", "Johann A Bengua", "Minh N Do"], "venue": "In arXiv preprint. https://arxiv.org/pdf/", "citeRegEx": "Phien et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Phien et al\\.", "year": 2016}, {"title": "Principal component analysis for dimension reduction in massive distributed data sets", "author": ["Yongming Qu", "George Ostrouchov", "Nagiza Samatova", "Al Geist"], "venue": "In Proceedings of IEEE International Conference on Data Mining (ICDM),", "citeRegEx": "Qu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2002}, {"title": "On the computational complexity and geometry of the first-order theory of the reals, part I: introduction. preliminaries. the geometry of semi-algebraic sets. the decision problem for the existential theory of the reals", "author": ["James Renegar"], "venue": "J. Symb. Comput.,", "citeRegEx": "Renegar.,? \\Q1992\\E", "shortCiteRegEx": "Renegar.", "year": 1992}, {"title": "On the computational complexity and geometry of the first-order theory of the reals, part II: the general decision problem. preliminaries for quantifier elimination", "author": ["James Renegar"], "venue": "J. Symb. Comput.,", "citeRegEx": "Renegar.,? \\Q1992\\E", "shortCiteRegEx": "Renegar.", "year": 1992}, {"title": "A statistical model for tensor pca", "author": ["Emile Richard", "Andrea Montanari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Richard and Montanari.,? \\Q2014\\E", "shortCiteRegEx": "Richard and Montanari.", "year": 2014}, {"title": "The search problem in mixture models", "author": ["Avik Ray", "Joe Neeman", "Sujay Sanghavi", "Sanjay Shakkottai"], "venue": "In arXiv preprint", "citeRegEx": "Ray et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ray et al\\.", "year": 2016}, {"title": "Pairwise interaction tensor factorization for personalized tag recommendation", "author": ["Steffen Rendle", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the third ACM international conference on Web search and data mining(WSDM),", "citeRegEx": "Rendle and Schmidt.Thieme.,? \\Q2010\\E", "shortCiteRegEx": "Rendle and Schmidt.Thieme.", "year": 2010}, {"title": "Weighted low rank approximations with provable guarantees", "author": ["Ilya Razenshteyn", "Zhao Song", "David P Woodruff"], "venue": "In Proceedings of the 48th Annual Symposium on the Theory of Computing (STOC),", "citeRegEx": "Razenshteyn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Razenshteyn et al\\.", "year": 2016}, {"title": "Newtonian program analysis via tensor product", "author": ["Thomas Reps", "Emma Turetsky", "Prathmesh Prabhu"], "venue": "In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages(POPL),", "citeRegEx": "Reps et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reps et al\\.", "year": 2016}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Rudelson and Vershynin.,? \\Q2009\\E", "shortCiteRegEx": "Rudelson and Vershynin.", "year": 2009}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tam\u00e1s Sarl\u00f3s"], "venue": "Annual IEEE Symposium on Foundations of Computer Science", "citeRegEx": "Sarl\u00f3s.,? \\Q2006\\E", "shortCiteRegEx": "Sarl\u00f3s.", "year": 2006}, {"title": "Multi-way Analysis with Applications in the Chemical Sciences", "author": ["K. Smilde", "Rasmus Bro", "Paul Geladi"], "venue": null, "citeRegEx": "Smilde et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Smilde et al\\.", "year": 2004}, {"title": "Spectacle: fast chromatin state annotation using spectral learning", "author": ["Jimin Song", "Kevin C Chen"], "venue": "Genome biology,", "citeRegEx": "Song and Chen.,? \\Q2015\\E", "shortCiteRegEx": "Song and Chen.", "year": 2015}, {"title": "Cryptography from tensor problems", "author": ["Leonard J Schulman"], "venue": "In IACR Cryptology ePrint Archive,", "citeRegEx": "Schulman.,? \\Q2012\\E", "shortCiteRegEx": "Schulman.", "year": 2012}, {"title": "Non-negative tensor factorization with applications to statistics and computer vision", "author": ["Amnon Shashua", "Tamir Hazan"], "venue": "In Proceedings of the 22nd international conference on Machine learning(ICML),", "citeRegEx": "Shashua and Hazan.,? \\Q2005\\E", "shortCiteRegEx": "Shashua and Hazan.", "year": 2005}, {"title": "Key exchange protocol based on tensor decomposition problem", "author": ["Mao Shaowu", "Zhang Huanguo", "Wu Wanqing", "Zhang Pei", "Song Jun", "Liu Jinhui"], "venue": "China Communications,", "citeRegEx": "Shaowu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shaowu et al\\.", "year": 2016}, {"title": "Fast and robust tensor decomposition with applications to dictionary learning", "author": ["Tselil Schramm", "David Steurer"], "venue": null, "citeRegEx": "Schramm and Steurer.,? \\Q2017\\E", "shortCiteRegEx": "Schramm and Steurer.", "year": 2017}, {"title": "Degeneracy in candecomp/parafac explained for p \u00d7 p \u00d7 2 arrays of rank", "author": ["Alwin Stegeman"], "venue": "p+1 or higher. Psychometrika,", "citeRegEx": "Stegeman.,? \\Q2006\\E", "shortCiteRegEx": "Stegeman.", "year": 2006}, {"title": "Low-rank approximation of generic p \u00d7 q \u00d7 2 arrays and diverging components in the candecomp/parafac model", "author": ["Alwin Stegeman"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Stegeman.,? \\Q2008\\E", "shortCiteRegEx": "Stegeman.", "year": 2008}, {"title": "Learning with tensors: a framework based on convex optimization and spectral regularization", "author": ["Marco Signoretto", "Dinh Quoc Tran", "Lieven De Lathauwer", "Johan A.K. Suykens"], "venue": "Machine Learning,", "citeRegEx": "Signoretto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Signoretto et al\\.", "year": 2014}, {"title": "Gaussian elimination is not optimal", "author": ["Volker Strassen"], "venue": "Numerische Mathematik,", "citeRegEx": "Strassen.,? \\Q1969\\E", "shortCiteRegEx": "Strassen.", "year": 1969}, {"title": "Sublinear time orthogonal tensor decomposition", "author": ["Zhao Song", "David P. Woodruff", "Huan Zhang"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Low rank approximation with entrywise `1-norm error", "author": ["Zhao Song", "David P Woodruff", "Peilin Zhong"], "venue": "In Proceedings of the 49th Annual Symposium on the Theory of Computing (STOC). ACM,", "citeRegEx": "Song et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Song et al\\.", "year": 2017}, {"title": "A parallel divide and conquer algorithm for the symmetric eigenvalue problem on distributed memory architectures", "author": ["Fran\u00e7oise Tisseur", "Jack Dongarra"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Tisseur and Dongarra.,? \\Q1999\\E", "shortCiteRegEx": "Tisseur and Dongarra.", "year": 1999}, {"title": "Weight adjusted tensor method for blind separation of underdetermined mixtures of nonstationary sources", "author": ["Petr Tichavsky", "Zbyn\u011bk Koldovsky"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Tichavsky and Koldovsky.,? \\Q2011\\E", "shortCiteRegEx": "Tichavsky and Koldovsky.", "year": 2011}, {"title": "Fast monte carlo algorithms for tensor operations", "author": ["Davoud Ataee Tarzanagh", "George Michailidis"], "venue": "In arXiv preprint", "citeRegEx": "Tarzanagh and Michailidis.,? \\Q2017\\E", "shortCiteRegEx": "Tarzanagh and Michailidis.", "year": 2017}, {"title": "Non-approximability results for optimization problems on bounded degree instances", "author": ["Luca Trevisan"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing (STOC),", "citeRegEx": "Trevisan.,? \\Q2001\\E", "shortCiteRegEx": "Trevisan.", "year": 2001}, {"title": "Statistical performance of convex tensor decomposition", "author": ["Ryota Tomioka", "Taiji Suzuki", "Kohei Hayashi", "Hisashi Kashima"], "venue": "In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems (NIPS). Proceedings of a meeting held 12-14 December", "citeRegEx": "Tomioka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tomioka et al\\.", "year": 2011}, {"title": "A multilinear (tensor) algebraic framework for computer graphics, computer vision, and machine learning", "author": ["M Alex O Vasilescu"], "venue": "PhD thesis, Citeseer,", "citeRegEx": "Vasilescu.,? \\Q2009\\E", "shortCiteRegEx": "Vasilescu.", "year": 2009}, {"title": "Multilinear analysis of image ensembles: Tensorfaces", "author": ["M Alex O Vasilescu", "Demetri Terzopoulos"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Vasilescu and Terzopoulos.,? \\Q2002\\E", "shortCiteRegEx": "Vasilescu and Terzopoulos.", "year": 2002}, {"title": "Tensortextures: Multilinear imagebased rendering", "author": ["M Alex O Vasilescu", "Demetri Terzopoulos"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "Vasilescu and Terzopoulos.,? \\Q2004\\E", "shortCiteRegEx": "Vasilescu and Terzopoulos.", "year": 2004}, {"title": "Facial expression decomposition", "author": ["Hongcheng Wang", "Narendra Ahuja"], "venue": "In Computer Vision,", "citeRegEx": "Wang and Ahuja.,? \\Q2003\\E", "shortCiteRegEx": "Wang and Ahuja.", "year": 2003}, {"title": "Online and differentially-private tensor decomposition", "author": ["Yining Wang", "Animashree Anandkumar"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "Wang and Anandkumar.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Anandkumar.", "year": 2016}, {"title": "A tensor framework for multidimensional signal processing", "author": ["Carl-Fredrik Westin"], "venue": "PhD thesis,", "citeRegEx": "Westin.,? \\Q1994\\E", "shortCiteRegEx": "Westin.", "year": 1994}, {"title": "Multiplying matrices faster than coppersmithwinograd", "author": ["Virginia Vassilevska Williams"], "venue": "In Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC),", "citeRegEx": "Williams.,? \\Q2012\\E", "shortCiteRegEx": "Williams.", "year": 2012}, {"title": "Dealing with missing data", "author": ["B. Walczak", "DL Massart"], "venue": "Part i. Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Walczak and Massart.,? \\Q2001\\E", "shortCiteRegEx": "Walczak and Massart.", "year": 2001}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P. Woodruff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Woodruff.", "year": 2014}, {"title": "Column subset selection with missing data via active sampling", "author": ["Yining Wang", "Aarti Singh"], "venue": "In The 18th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Wang and Singh.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Singh.", "year": 2015}, {"title": "Fast and guaranteed tensor decomposition via sketching", "author": ["Yining Wang", "Hsiao-Yu Tung", "Alexander J Smola", "Anima Anandkumar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Out-of-core tensor approximation of multi-dimensional matrices of visual data", "author": ["Hongcheng Wang", "Qing Wu", "Lin Shi", "Yizhou Yu", "Narendra Ahuja"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}, {"title": "Distributed low rank approximation of implicit functions of a matrix", "author": ["David P Woodruff", "Peilin Zhong"], "venue": "IEEE International Conference on Data Engineering (ICDE)", "citeRegEx": "Woodruff and Zhong.,? \\Q2016\\E", "shortCiteRegEx": "Woodruff and Zhong.", "year": 2016}, {"title": "Multilinear tensor rank estimation via sparse tucker decomposition", "author": ["Tatsuya Yokota", "Andrzej Cichocki"], "venue": "In Soft Computing and Intelligent Systems (SCIS),", "citeRegEx": "Yokota and Cichocki.,? \\Q2014\\E", "shortCiteRegEx": "Yokota and Cichocki.", "year": 2014}, {"title": "Weighted sgd for `p regression with randomized preconditioning", "author": ["Jiyan Yang", "Yin-Lam Chow", "Christopher R\u00e9", "Michael W Mahoney"], "venue": "In Proceedings of the TwentySeventh Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Generalised coupled tensor factorisation", "author": ["Yusuf Kenan Yilmaz", "Ali Taylan Cemgil", "Umut Simsekli"], "venue": "In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Yilmaz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yilmaz et al\\.", "year": 2011}, {"title": "Solving a mixture of many random linear equations by tensor decomposition and alternating minimization", "author": ["Xinyang Yi", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "In arXiv preprint", "citeRegEx": "Yi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2016}, {"title": "Robust low-rank tensor recovery with regularized redescending m-estimator", "author": ["Yuning Yang", "Yunlong Feng", "Johan AK Suykens"], "venue": "IEEE transactions on neural networks and learning systems,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Spectral methods meet em: A provably optimal algorithm for crowdsourcing", "author": ["Yuchen Zhang", "Xi Chen", "Denny Zhou", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Rank-one approximation to high order tensors", "author": ["Tong Zhang", "Gene H. Golub"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Zhang and Golub.,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Golub.", "year": 2001}, {"title": "Recovery guarantees for one-hidden-layer neural networks", "author": ["Kai Zhong", "Zhao Song", "Prateek Jain", "Peter L. Bartlett", "Inderjit S. Dhillon"], "venue": null, "citeRegEx": "Zhong et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2017}, {"title": "Tensor dictionary learning with sparse tucker decomposition", "author": ["Syed Zubair", "Wenwu Wang"], "venue": "In Digital Signal Processing (DSP),", "citeRegEx": "Zubair and Wang.,? \\Q2013\\E", "shortCiteRegEx": "Zubair and Wang.", "year": 2013}, {"title": "Subspace methods with local refinements for eigenvalue computation using low-rank tensor-train format", "author": ["Junyu Zhang", "ZaiwenWen", "Yin Zhang"], "venue": "Journal of Scientific Computing,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Guaranteed tensor pca with optimality in statistics and computation", "author": ["Anru Zhang", "Dong Xia"], "venue": "In arXiv preprint", "citeRegEx": "Zhang and Xia.,? \\Q2017\\E", "shortCiteRegEx": "Zhang and Xia.", "year": 2017}], "referenceMentions": [], "year": 2017, "abstractText": "We consider relative error low rank approximation of tensors with respect to the Frobenius norm. Namely, given an order-q tensor A \u2208 R \u220fq i=1 ni , output a rank-k tensor B for which \u2016A \u2212 B\u2016F \u2264 (1 + ) OPT, where OPT = infrank-k A\u2032 \u2016A \u2212 A\u2016F . Despite much success on obtaining relative error low rank approximations for matrices, no such results were known for tensors. One structural issue is that there may be no rank-k tensor Ak achieving the above infinum. Another, computational issue, is that an efficient relative error low rank approximation algorithm for tensors would allow one to compute the rank of a tensor, which is NP-hard. We bypass these two issues via (1) bicriteria and (2) parameterized complexity solutions: 1. We give an algorithm which outputs a rank k\u2032 = O((k/ )q\u22121) tensor B for which \u2016A \u2212 B\u2016F \u2264 (1+ ) OPT in nnz(A)+n \u00b7poly(k/ ) time in the real RAM model, whenever either Ak exists or OPT > 0. Here nnz(A) denotes the number of non-zero entries in A. If both Ak does not exist and OPT = 0, then B instead satisfies \u2016A \u2212 B\u2016F < \u03b3, where \u03b3 is any positive, arbitrarily small function of n. 2. We give an algorithm for any \u03b4 > 0 which outputs a rank k tensor B for which \u2016A\u2212B\u2016F \u2264 (1+ ) OPT and runs in (nnz(A)+n poly(k/ )+exp(k/ )) \u00b7n\u03b4 time in the unit cost RAM model, whenever OPT > 2\u2212O(n ) and there is a rank-k tensor B = \u2211k i=1 ui \u2297 vi \u2297 wi for which \u2016A \u2212 B\u2016F \u2264 (1 + /2) OPT and \u2016ui\u20162, \u2016vi\u20162, \u2016wi\u20162 \u2264 2 \u03b4). If OPT \u2264 2\u2212\u03a9(n\u03b4), then B instead satisfies \u2016A\u2212B\u2016F \u2264 2\u2212\u03a9(n \u03b4). Our first result is polynomial time, and in fact input sparsity time, in n, k, and 1/ , for any k \u2265 1 and any 0 < < 1, while our second result is fixed parameter tractable in k and 1/ . For outputting a rank-k tensor, or even a bicriteria solution with rank-Ck for a certain constant C > 1, we show a 2 1\u2212o(1)) time lower bound under the Exponential Time Hypothesis. Our results are based on an \u201citerative existential argument\u201d, and give the first relative error low rank approximations for tensors for a large number of error measures for which nothing was known. In particular, we give the first relative error approximation algorithms on tensors for: column row and tube subset selection, entrywise `p-low rank approximation for 1 \u2264 p < 2, low rank approximation with respect to sum of Euclidean norms of faces or tubes, weighted low rank approximation, and low rank approximation in distributed and streaming models. We also obtain several new results for matrices, such as nnz(A)-time CUR decompositions, improving the previous nnz(A) log n-time CUR decompositions, which may be of independent interest. \u2217Work done while visiting IBM Almaden, and supported in part by UTCS TAship (CS361 Spring 17 Introduction to Computer Security). \u2020Supported in part by Simons Foundation, and NSF CCF-1617955. ar X iv :1 70 4. 08 24 6v 1 [ cs .D S] 2 6 A pr 2 01 7", "creator": "LaTeX with hyperref package"}}}