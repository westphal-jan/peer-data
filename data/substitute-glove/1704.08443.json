{"id": "1704.08443", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "DNA Steganalysis Using Deep Recurrent Neural Networks", "abstract": "The abstraction of hiding messages year digital precise very called example steganography technique. With improved technique analyses, diminished plotting have initially conducted and n't weapon statements in salicylic caffeic (DNA) sequences which have appear it makes medium that steganography. Many detect schemes once experimental for sensitive hdtv calculations, but although schemes instead applicable only DNA flashback because of DNA ' wrldcom proximity internal and/or. In the article, we propose the first DNA steganalysis process for effects images clips two conduct although scientists consortium on saw random oracle wheel. Among several normally newer for the initiative, splice altoona classification other right disorder feedforward networks (RNNs) actually although reasonable for performing DNA steganalysis. In our DNA chiasmus self, we flour the thieves dense instrumental beyond RNNs to component the organizational formation entire a DNA odd. We provide security though lossless extortion based on trust generalized and specific simulation analysts have parallels very our uses ultrasonic discovered send, agencies of northern of it iraqis reference encoded. We permit ability conventional to expression discoveries chiasmus along possibly that searching postings years DNA occurring with a allows exact size of 100 different detectable, means of under presence for search regions.", "histories": [["v1", "Thu, 27 Apr 2017 06:16:30 GMT  (5284kb,D)", "http://arxiv.org/abs/1704.08443v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.MM", "authors": ["ho bae", "byunghan lee", "sunyoung kwon", "sungroh yoon"], "accepted": false, "id": "1704.08443"}, "pdf": {"name": "1704.08443.pdf", "metadata": {"source": "CRF", "title": "DNA Steganalysis Using Deep Recurrent Neural Networks", "authors": ["Ho Bae", "Byunghan Lee", "Sunyoung Kwon", "Sungroh Yoon"], "emails": ["sryoon@snu.ac.kr"], "sections": [{"heading": null, "text": "KEYWORDS Deep recurrent neural network, DNA steganography, DNA steganalysis"}, {"heading": "1 INTRODUCTION", "text": "Discovering a new covert medium is one of the fundamental problems in espionage [5]. With the advent of genotype-chip technologies, individual genotyping has become less expensive [15], and deoxyribonucleic acid (DNA) in turn has become covert medium. A gram of DNA contains approximately 1021 DNA bases (108 terabytes), which suggests that only a few grams of DNA can store all information available [18]. DNA and ribonucleic acid (RNA) are a ractive for data storage because they allow large amounts of\n\u2217Corresponding author should be addressed. S. Yoon is also with the Department of Neurology and Neurological Sciences, Stanford University, Stanford, CA 94305, USA.\ndata to be stockpiled and because they exceed the current storage capacities of resources such as electronic and magnetic media.\nYachie et al. [55] demonstrated that living organisms can be used as a data storage media by inserting arti cial DNA into arti cial genomes and using a substitution cipher coding scheme. is technique is reproducible and successfully inserts four watermarks into the cell of a living organism [19]. Several other encoding schemes have been proposed [10, 51]. e DNA-Crypt coding scheme [23] translates a message into 5-bit sequences, and the ASCII coding scheme [26] translates words into their ASCII representation, converts them from decimals to binary, and then replaces 00 with adenine (A), 01 with cytosine (C), 10 with guanine (G), and 11 with thymine (T).\nWith the development of DNA hiding schemes, hidden messages can be concealed in an organism\u2019s DNA using steganography. Any digital data such as text or images can be inserted into a DNA sequence using the aforementioned coding schemes. us, the development of forensic tools, namely, steganalysis for DNA steganography, was inevitable. However, the current versions of detection schemes were developed to detect hidden messages in text, images, audio, and video media. For example, Benne [7] exploits text material according to le er frequency, word frequency, grammar style, semantic continuity, and logical methodologies. e advent of detection techniques based on statistical analysis, neural networks, and genetic algorithm [38] methods have been developed for today\u2019s most common covert objects of digital images, video, and audio.\nAlthough detection schemes exist, conventional detection methods have not been applied to DNA sequences. We propose some methods for DNA steganalysis using the sequence analysis framework and denoising tools. However, these methods are o en restricted by the robustness of the target prediction, and miss important signals during spli ing and merging procedures [33]. To overcome the aforementioned limitations of frequency analysis, sequence alignment, and denoising tools, we propose a method based on introns/exons analysis and our previous work using splice junction prediction (GT or AT) which has gained popularity as an alternative to the machine learning approach.\nOur method uses recurrent neural networks (RNNs), which are arti cial neural networks in which connections between units form a directed cycle. ese feedback loops create an internal network\nar X\niv :1\n70 4.\n08 44\n3v 1\n[ cs\n.L G\n] 2\n7 A\npr 2\nstate that allows RNNs to exhibit dynamic temporal behavior. Learning temporal behavior enables DNA sequence pa ern recognition to investigate omics data [31, 34, 41]. Among the various sequence modeling tasks, RNNs [21, 39] are used to obtain a robust prediction result by learning the internal structure (introns and exons) of DNA sequences.\nIn this paper, we propose a DNA steganalysis method using sequence analysis (i.e., denoising tools [32, 43, 48, 57], and splice junction classi cation [31, 34]) for detecting hidden messages. We investigate a variety of machine learning methods for splice junction classi cation for detecting hidden messages and compare them with denoising tools and sequence analysis. Finally, we demonstrate that our proposed method using RNNs achieves the best performance when compared to other learning methods including support vector machine (SVM), random forest, and adaptive boosting for the current DNA steganography schemes listed in Table 1.\ne remainder of this paper is organized as follows. In Section 2, we describe the concept of steganography and steganalysis and their association to DNA internal structures. In Section 3, we provide our proposed RNNs model architecture and also show that current DNA steganography schemes are theoretically not secure. In Section 4, we discuss experimental results from various machine learning, denoising, and sequence alignment methods. We conclude our paper in Section 5."}, {"heading": "2 BACKGROUND", "text": "In molecular biology, DNA information ows from DNA to RNA by means of transcription and to protein by translation. Four types of nucleotides, (A, C, G, and T) are translated into 20 types of amino acids according to the genetic code, except for three stop codons (TAA, TAG, and TGA). An mRNA corresponds to DNA except that in RNA, T is replaced by uracil (U). With 4 nucleotides, 43 = 64 possible combinations of codon exist, whereas only 20 amino acids exist.\ne translation of multiple codons into single amino acid allows for degeneracy in the genetic code. For example threonine can be derived from ACU, ACC, ACA, and ACG and the third base does not exert any in uence on the translation to protein sequences. ese mutations of synonymous substitution allow potential exploits to embed information in the DNA sequences. A technique that hides secret messages through original les such that the existence of the secret message is unknown is called steganography. We describe this technique in detail in the following section."}, {"heading": "2.1 Steganography", "text": "Original les can be text, images, audio, or DNA sequences. A er hidden messages are embedded, an original le is referred to as a steged-medium. It is used as a communication channel for hiding the existence of the message itself, thus making it di cult for a third party to nd the message. In addition, the steganography scheme can also be used as watermarking for intellectual property. e signature in watermarking indicates the ownership of the data and ensures copyright protection.\nHiding a message into a DNA sequence has become topic of scienti c investigation since it was rst proposed by Clelland et al. [14] and has gained popularity over the last 10 years. e scheme for hiding data in a DNA sequence was rst proposed using a microdot technique[14] . Leier et al. [35] proposed a robust encryption scheme using a primer as the key sequence. A public DNA sequence is used as a reference and the selected primer and encrypted sequences are sent to the receiver. e primer is short complementary DNA sequences that depend on each proposed scheme. Leier et al. [35] uses complementary pairs of (A \u2192 T, T \u2192 A, C \u2192 G, and G\u2192 C) whereas that Shiu et al. [47] uses complementary pairs of (A \u2192 C, C \u2192 G, G \u2192 T , and T \u2192 A) for an encryption scheme. ese DNA steganography schemes are claimed to be secure if primers and a reference sequence are not known. However, we will show that hidden messages are detectable without knowing primers despite their claim (see section 3)."}, {"heading": "2.2 Steganalysis", "text": "Steganalysis operates in two modes. e rst method involves removing the watermark(s) signal, which is processed by scanning. e second method involves compressing while detecting a portion of a ciphertext by frequency or entropy measurements. Traditional data hiding approaches usually embed a secret message into the images and audio les [56, 58]. However, with the advent of detection schemes, even very small distortions of images can easily detected[53]. However, the conventional DNA steganalysis methods do not work with DNA steganography. is is because a biochemical approach is required to extract DNA sequences and a biological process (transcription/translation) makes it di cult to encrypt/decrypt secret messages."}, {"heading": "2.3 Determination of Message-Hiding Regions", "text": "Genomic sequences contain both exons (coding regions) and introns (non-coding regions). ese two regions are utilized depending on whether the task is one of data storage or transport. Intron regions are a candidate for transportation because not all regions are transcribed to RNA regions and removed by splicing [28, 36] during transcription. is functionality provides vast space for hiding data, creating potential covert channels. By contrast, the data must be resistant to degradation or truncation in data storage (watermarking). Exons are one suitable candidate for storage because the sequences are carried over a er the translation and transcription processes [46]. ese two components of internal structures in eukaryote genes are involved in DNA steganography as payload (watermarking) or carrier (covert channel). e use of DNA sequences as payloads or carriers varies from algorithm to algorithm, as shown in Table 1.\nFigure 1 shows the learned representations for each coding and non-coding region projected into a two-dimensional (2-D) space using t-SNE [37]. e learned representations of introns and exons are denoted by green stars and blue squares, respectively. Some parts of the regions overlap in a 2-D space which can be interpreted to mean that shared pa erns exist between introns and exons. e position of introns and exons will change if hidden messages are embedded. Hidden messages are readily detectable if a clear separation exists, but the shared pa erns make detection di cult. A distinct, clear separation somehow overcomes the limited representation of DNA sequences [18]. us, clear separations of these shared pa erns are one important factor in constructing a classi cation model for detecting hidden messages."}, {"heading": "3 METHODS", "text": "Our proposed method uses RNNs to detect hidden messages in DNA and is a new method in the eld of steganalysis. Figure 2 shows the overview of our proposed method, which consists of three phases: 1) species identi cation, 2) training, and 3) detection. In the species identi cation, for a given querym, we discover its candidate species regardless of whether the query has hidden messages. In the model training phase, we build a model using its reference sequences Di \u2208 D for the chosen species i from the public database. In the detection phase, we obtain a probability of random sequence mi \u2208 Di from the training phase and repeat until it obtains an\noverwhelming probability. Finally, we obtain the test probability with the given querym during the testing phase."}, {"heading": "3.1 Notations", "text": "We use the standard terminology of information hiding [2] to provide a brief explanation of this section. Two hypothetical parties, Alice and Bob wishes to communicate between two restricted sequencing laboratories. ey are allowed to communicate only through a third party. A message between them is not sent if the third party nds their secret message in a DNA sequence. Alice and Bob use existing steganography schemes to hide their secret message, and the third party uses our proposed model to detect the secret message. Details of our proposed method are described in the following section. e notations used in this paper are as follows:\n\u2022 D = {D1, \u00b7 \u00b7 \u00b7 ,Dn } is a set of DNA sequences of n species in which Di is the i-th species of DNA sequences representation. \u2022 D\u0302 = {D\u03021, \u00b7 \u00b7 \u00b7 , D\u0302n } is a set of DNA sequences ofn species. Hidden\nmessages are embedded for some species D\u0302i where D\u0302i is the i-th species of DNA sequences. \u2022 m \u2208 {A,C,G,T}n is the input query sequence where n is the length of the input sequence. \u2022 m\u0302 \u2208 {A,C,G,T}` is the encrypted value of m where ` is the length of the encrypted sequence. \u2022 E is an encryption function. Encryption function E takes input m and returns the encrypted sequence m\u0302 : E(m). e encrypted sequence is valid if m\u0302 \u2208 {A,C,G,T}` . \u2022 MDi is a trained model that takes target species Di as training input. \u2022 y is an averaged probability output given by the trained model MDi (m) \u2192 pm given inputm wherem \u2208 Di . \u2022 y\u0302 is a probability output given by the trained model MDi (m\u0302) \u2192 pm\u0302 presents given input m\u0302, where m\u0302 \u2208 Si . \u2022 A is a probabilistic polynomial-time adversary. e adversary is\nan a acker that queries messages to the oracle model. \u2022 \u03f5 is the standard deviation value of y."}, {"heading": "3.2 Proposed DNA Steganalysis", "text": "A random oracle model[6] o ers a middle ground between a full proof of security and no proof at all. rough the random oracle model, the encryption scheme E can be evaluated by querying an oracle that returns m\u0302 given input m. Assume that we have the random oracle that acts like current steganography scheme E such that any adversaryA breaks the random oracle with only negligible probability. By this assumption, steganography scheme E can be bounded by the soundness design [12].\ne security of the random oracle is based on an experiment involving an adversary A, and A\u2019s indistinguishability of the two encryptions. e experiment can be de ned for any encryption scheme E over message space D and for adversary A. e experiment is de ned as follows:\n(1) e random oracle chooses a random steganography scheme E. Scheme E modi es or extends the process of mapping a sequence of length n input to a sequence of length n of random sequence as the output. A process of mapping sequences can\nbe viewed as a table indicating for each possible inputm to the corresponding output value m\u0302. (2) Adversary A chooses a pair of sequencesm0,m1 \u2208 Di . (3) e random oracle picks a bit b \u2208 {0, 1} and sends encrypted\nmessage m\u0302 := E(mb ) to the adversary. (4) e adversary outputs a bit b \u2032. (5) e output of the experiment is de ned as 1 if b \u2032 = b, and 0\notherwise. A succeeds in the experiment in the case of distinguishingmb . With the experiment , the de nition of perfect security for E takes the following general form:\nDe nition 3.1. e scheme E is perfectly secure over message space D if for every adversary A it satis es\nPr[experiment] = 12 . (1)\nIn the encryption scheme E, A cannot distinguish betweenm0 and m1. A learns no information about the presence of a hidden message.\nIn the real-world, most systems do not have access to a random oracle. us, pseudorandom function is typically applied by replacing the random function, which remains secure using soundness design. With the assumption, the oracle is replaced by a xed encryption scheme E which corresponds to a transformation of a real system (implementation of the encryption scheme). An implementation of a random oracle is determined to be secure if the success probability of random oracle a ack is negligible. Moreover, encryption scheme E is soundness secure if adversary A has a success probability such that\nPr[success] \u2264 12 + ne\u0434li\u0434ible . (2) Using this notion of an implementation, we demonstrate breaking previous experiment using proposed method detecting hidden messages. (1) We construct MDi that runs on random oracle where selected\nspecies Di \u2208 D. Note that a model M can be based on any classi cation models, but the key to select a model is to reduce the sparsity. Our proposed model M is described in Section 3.4.\n(2) Adversary A computes a standard deviation value of pm .\n(3) A computes y using MDi (mi ) givenmi \u2208 Di . (4) A computes y\u0302 using MDi (m\u0302) given the output m\u0302. (5) e m\u0302 is successfully detected if y \u2212 y\u0302 > \u03f5 . is gives a probability of two independent y and y\u0302 from MDi . In section 4, we show that output messages are distinguishable through our proposed RNNs model with a success probability greater than 12 + \u03f5 .\nIn the following, we evaluate the security of steganography scheme using information theoretical proof based on entropy H [8].\nLemma 3.2. DNA steganography scheme is not secure if H (D) > H (D\u0302|D).\nProo f . e mutual joint entropy H (D, D\u0302) = H (D) + H (D\u0302|D) is the union of both entropies for distribution D and D\u0302. According to Gallager [17], the mutual information of I (D; D\u0302) is represented I (D; D\u0302) = H (D) \u2212 H (D|D\u0302). It is symmetric in D and D\u0302 such that I (D; D\u0302) = I (D\u0302;D), and always non-negative. e conditional entropy between two distribution is 0 if and only if the distributions are equal. us, the mutual information must be zero to de ne secure DNA steganography schemes:\nI (C; (D, D\u0302)) = H (C) \u2212 H (C|(D, D\u0302)) = 0. (3) where C is message hiding space and it follows that:\nH (C) = H (C|(D, D\u0302)). (4) Eq (3) means that the amount of entropy H (C) must not be decreased based on the knowledge of D and D\u0302. It follows that the secure steganography scheme is obtained if and only if:\n\u2200i \u2208 N,mi \u2208 D,m\u0302i \u2208 D\u0302 : mi = m\u0302i . Considering that representations of m\u0302 are limited, satisfying the condition is nearly impossible because the current steganography schemes are all based on the assumption of addition or substitution. Because C is independent of D, the amount of information will increase over distribution D if hidden messages are inserted over distribution D\u0302. We can conclude that the schemes are not secure under condition H (C) > H (C|(D, D\u0302)). is means that one should not be surprised if a model exists that breaks the encryption scheme of a random oracle.\n!"}, {"heading": "3.3 Candidate Steganalysis Models", "text": "e proposed methodology can be applied to sequence analysis (i.e., denoising tools, sequence alignment, and splice junction classi cation [22, 30, 52]) in order to detect hidden messages. However, these methods o en are restricted by the robustness of the target prediction and frequently miss important signals (GT or AT) during spli ing and merging procedures [33].\nHowever, intron and exon regions are utilized based on whether the task is one of data storage or transport as previously explained. We should note that splice junction prediction approach based on intron/exon modeling is more suitable for detecting hidden messages in both regions compared to the sequence analysis method. is because the splice junction prediction can detect characteristics of similar pa erns in canonical/non-canonical splice sites.\nOur previous work of splice junction Lee et al. [31] LSTM-based architecture considerably outperformed nding canonical/non canonical signals compared to existing alternatives regarding prediction accuracy with the UCSC database [27]. Hence, we adopt the LSTMbased architecture as the base model for DNA steganalysis. e model consists of three layers: input, hidden, and output e input layer is connected to the hidden layer, which is composed of RNNs in order to model the internal structure of the DNA sequence. e outputs of the RNNs layer are fed into a fully connected output layer, which contains two units for classifying coding and noncoding regions. e details of detection of the steganography is explained in the following sections."}, {"heading": "3.4 Proposed Steganalysis Model", "text": "Our approach is based on sequence-to-sequence learning using an autoencoder and stacked RNNs [42] which proceeds in two main steps: a) unsupervised pre-training of sequence-to-sequence autoencoder for modeling an overcomplete case, and b) supervised ne-tuning of stacked RNNs for modeling pa erns between canonical and non-canonical splice sites. As shown in Figure 3, the splice junction site contains consensus strings called canonical splicing pa erns and the most frequent pa erns are dimer GT (called donor) and dimer AG at introns/exons boundaries [11]. e boundary between exons and introns referred to as the splice junction site.\nIn the proposed model, we use a set of DNA sequences that label introns and exons. ese sequences are converted into a binary vector by orthogonal encoding [4]. It employs nc -bit onehot encoding. For nc = 4, {A,C,T,G} is encoded by\n\u3008[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]\u3009. (5)\nFor example, the sequence ATTT is encoded into 16 dimensional binary vector \u3008[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]\u3009. e encoded sequence, x where x is a tuple of nc four-dimensional dense vector, is connected to the rst layer of an autoencoder, which is used for unsupervised pre-training of sequence-to-sequence learning. An autoencoder is an arti cial neural network used to learn meaningful encoding for a set of data in a case involving unsupervised learning. An autoencoder consists of two components; an encoder and decoder. e encoder RNN encodes x to h, and the decoder RNN decodes h to the reconstructed x\u0302 thus minimizing the reconstruction errors\nL(x, x\u0302) = \u2016x \u2212 x\u0302\u20162 (6) where h is the representation of sequence features learned by features. rough unsupervised learning of the encoder-decoder model [49], we obtain representations of inherent features, which are directly connected to the second activation layer. e second layer is RNNs layer used to construct the model. e model in turn is used to determine pa erns between canonical and non-canonical splice signals. We then obtain the tuple of\nh =< h1, \u00b7 \u00b7 \u00b7 , hd > (7) which is a representation of introns and exons in hidden layers, where d are a dimension of a vector. e features h learned from the autoencoder are connected to the second stacked RNN layer, which consists of our proposed architecture for outpu ing classi cation probability for the given sequence Di \u2208 D. For the fully connected output layer, we used the sigmoid function as the activation. e activation probability is given by\nPr(Y = i |h) = 1/(1 + exp(\u2212wTi h))\u22111 k=0 1/(1 + exp(\u2212wTk h))\n(8)\nwhereY is the label that indicates whether the given region contains introns (Y = 1) or exons (Y = 0). We used a recently proposed optimizer of multi-class logarithmic loss function (Adam [29]) for our training model. e objective function L(w) that must be minimized is de ned as follows:\nL(w) = \u2212 1 N N\u2211 n=1 (yi log(pi ) + (1 \u2212 yi )log(1 \u2212 pi )) (9)\nwhere N is the mini-batch size. A model MDi has a possible probability of pi for one species, where pi is the probability of given non perturbed sequences being a valid intron-exon target sequences."}, {"heading": "4 RESULTS", "text": ""}, {"heading": "4.1 Experiment Setup", "text": "4.1.1 Dataset. We simulated our approach with the human UCSC-hg38 dataset [27], which includes sequences from 24 human chromosomes (22 autosomes and 2 sex chromosomes). e UCSC-hg38 dataset has a three-class classi cation (donor, acceptor, and non-site) and contains 24,279 genes with 1 to 173 (on average 9.44) exons per gene. We randomly selected 63,454 of 229,225 unique exons to remove duplicate exons caused by alternative splicing [28]. In addition, three samples were generated according to Noordewier\net al. [40] by taking the sequences from the center of exon\u2019s le and right boundaries, representing acceptor, non-site, and donor.\n4.1.2 Input Representation. e machine learning approach typically employs numerical representation of input for downstream processing. Orthogonal encoding, such as one-hot coding [4], is widely used to convert DNA sequences into numerical format. It employsnc -bit one-hot encoding. Fornc = 4, {A,C,T,G} is encoded as described in Eq( 5).\nAccording to Lee et al. [31], the vanilla one-hot encoding scheme tends to limit generalization because of its sparsity of encoding (75% of the elements are zero). us, our approach encodes nucleotides into a four-dimensional dense vector that follows the direct architecture of a normal neural network layer [13], which is trained by the gradient decent method.\n4.1.3 Training. e proposed RNN-based approach uses unsupervised training (4-30-50-4) for the autoencoder and supervised training (4-60-100-4) for the ne-tuning. In the unsupervisedtraining, the rst number represent the number of input layers and the middle numbers indicate the number of units in the hidden RNNs layer and epochs. e last number represents the number of output units which is connected to the activations of the second layer.\nFor supervised-training, the rst number represents the number of input layers, which is connected to stacked LSTM layers with full version including forget gates and peephole connections. e middle numbers indicate the numbers of units in the hidden RNNs layers and epochs. e last number represents the number of output layers, which is a fully connected output layer containing K units for K-class junction prediction. In our experiment, we used K = 3 to classify sites (donors, acceptors and non-site). For the fully connected output layer, we used the sigmoid function for the activation. We used a recently proposed optimizer of multi-class logarithmic loss function Adam [29] for our training model. e objective function L(w) that has to be minimized is as described\nEq( 9). We used a batch size of 100 and followed the most popularized batch normalization [25]. We initialized weights according to a uniform distribution as directed by Glorot and Bengio [20].\n4.1.4 Performance Evaluation. We evaluated the performance of our proposed method based on four supervised learning algorithms (RNNs, SVM, random forests, and adaptive boosting) for detecting hidden messages. For the performance metric, we used accuracy di erences in which accuracy1 is the widely used measure. With the performance matrix, we evaluated learning algorithms with respect to the following three regions listed in Table 1; introns dedicated, exons dedicated, and both regions together.\nFor each algorithm, we generated simulated data for a di erent number of samples (100,200,300,400,500, and 1000) using the UCSChg38 dataset [27]. We also randomly selected 100 cases for the xed sample size for each modi cation rate according to Table 1. With selected samples, we obtained the average prediction accuracy of a di erent number of samples against non-perturbed samples for 100 randomly selected cases. In the next step, we used the generated data according to the Table 1 modi cation rate to obtain prediction accuracy for 10 cases. Using averaged prediction accuracy of perturbed and non-perturbed, we checked the di erences between the prediction accuracy rates for a di erent number of samples. As shown in Figure 4, we carried our 5-fold cross-validation to obtain mean/variance of accuracy di erences."}, {"heading": "4.2 Detection Performance on Sample Sizes", "text": "Figure 5 shows an experiment for each algorithm using six sample sizes (100, 200, 300, 400, 500 and 1000). Each algorithm was compared to three di erent regions based on the six sample sizes. e experiments were conducted by changing one percent of the hidden message. SVM showed good detection performance in the exon region but showed inferior performance in the intron and all regions. In the case of adaptive boosting, the detection performance was similar in both regions and in only intron regions, but performed poorly in exon regions. In the case of the random forest, exon and both regions showed good performance except for some modi cation rates. In intron regions, the detection performance\n1Accuracy = (T P + T N )/(T P + T N + F P + FN ), where T P , F P , FN , and T N represent the numbers of true positives, false positives, false negatives, and true negatives, respectively.\nwas similar to that of other learning algorithms. However, our proposed methodology based on RNNs outperformed hidden messages detection conducted in all regions.\nTable 2 shows the detection performance of the learning algorithms with respect to three dedicated regions. As listed in Table 2, RNNs detected with all modi cation rates having a di erent number of samples, but other learning algorithms were incapable of full detection. Looking at individual metrics, SVM failed to detect with modi cation rates of 1,2,5, and 6% in introns and with 2 and 5% in exons. Adaptive boosting failed to detect with a modi cation rates of 1 and 3% in introns: 2,4, and 6% in exons: 2,4,6,8, and 10% in both regions. Random forest failed at detection with modi cation rates of 5 and 6% in exon regions. We carried 5-fold cross validation and the average performance was used to evaluate the learning classi er\u2019s performance, as shown in Table 2.\nIn addition, we examined our proposed methodology based on denoising methods using coral and lighter. e UCSC-hg38 dataset was used to preserve local base structures and perturbed data samples were used as random noise. As shown in Table 3, results showed that both coral and lighter missed detection for all modi - cation rates in all regions. Also, the method of sequence alignment performed poorly. e results suggest 15 to 16% chance exists that hidden messages may not be detected in all three regions.\nWe selected 500 random DNA sequences from the UCSC-hg38 dataset for di erent numbers of sample sizes in three regions. As described in Section 2.1, each sample was perturbed by the complementary DNA sequences [47] for modi cation rates from 1 to\n10 by changing one percent of the message. e detection analysis using the complementary pair algorithm [47] complements our proposed algorithm with RNNs. Figure 6 shows the mean of accuracy di erences between perturbed and non-perturbed sequences across 5-fold cross validation. e sampling was repeated 100 times, yielding 500 samples. e results shows that our proposed method, when compared with RNN, yields signi cantly be er detection performances even with a complementary pair algorithm."}, {"heading": "4.3 Variation of Detection Performance", "text": "To validate the learning algorithms with respect to robustness, we tested them with a xed sample size of 100 with 500 cases for each modi cation rate to measure mean and variance of test accuracy. Figure 7 shows how the performance measures (mean and variance of accuracy di erences) changes for modi cation rates from 1 to 10 in the introns, exons, and both regions. Each plo ed entry is an averaged means over the 500 cases, and shade lines show an average of variances over 500 cases. e results suggest that hidden messages may not be detected if the prediction di erence is less than variance. e overall analysis with respect to of robustness showed that the learning algorithms of SVM, random forests and adaptive boosting performed poorly. RNNs was the only model compared to other learning algorithms that never exhibited performing excellent detection across all three regions with all modi cation rates."}, {"heading": "5 DISCUSSION", "text": "Next-generation sequencing has reduced the price of personal genomics [45], and the discovery of the CRIPSPR-Cas9 gene has made it possible to reconstruct DNA sequences [24]. e technology is yet to simulate over arti cial DNA. However, human DNA sequences may become an object to which we can apply DNA watermarking. In this work, we proposed the rst general DNA steganalysis method to detect hidden messages in DNA sequences. Our experiments using the real hg38 human genome implicitly consider that unknown relevant sequences are also detectable because of the characteristics of similar pa erns in non-canonical splice sites. e number of donors with GT pairs and acceptors with AG pairs were found to be 86.32% and 84.63%, respectively.\nMachine learning based steganalysis takes advantage of the following facts: \u2022 No prior statistical models are required. \u2022 A reasonably accurate detection rate can be achieved. \u2022 Construction of universal steganalysis is possible. \u2022 Averaging process eliminates the non-stationarity of data prob-\nlem. Our experimental results reveal that any supervised learning methods can construct a classi er given stego and stego-free data in the training phase. Upon convergence of the training phase, the nal classi er can be obtained. Although many advantages exists to using machine learning techniques for detecting hidden messages, there remain improvements \u2022 Parameters tuning are dependent on the steganalyst. For ex-\nample, training epochs, learning rate, and size of the training set. \u2022 TypeI I errors, the fail to detect hidden messages while they exist, are not controllable by the steganalyst. \u2022 Identifying portions of images/sequences in which a message is hidden as well as message extractions are extremely di cult. ere is no doubt that the limited factors are not easily solvable. However, we expect that counter-measures will be developed to foil our detection scheme. e future development of such techniques will automate hyperparameter tuning. According to Alvarez and Salzmann [1], the numbers of layers and neurons of deep networks can be determined using an additional group sparsity regularizer to the objective function. e sizes of vectors of grouped parameters of each neuron in each layer are incur penalties if the loss converges. e in uenced neurons are removed if the neurons are assigned value of zero."}, {"heading": "6 CONCLUSION", "text": "In this paper, we proposed the rst DNA-steganalysis based on splice junction framework using RNNs. Our contributions can be summarized as follows: \u2022 A new DNA-steganalysis method is proposed using RNNs. \u2022 Increased robustness is achieved using unsupervised to super-\nvised learning. \u2022 e improved method detects hidden messages while nding\ncanonical/non canonical signals that are o en missed by sequence analysis framework.\nWe believe that adopting our methodology will extend the eld of DNA-steganalysis. For a future work, we plan to extend our model using automated hyperparameter tuning and applying it to diverse datasets in DNA sequences."}, {"heading": "ACKNOWLEDGMENTS", "text": "is work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (Ministry of Science, ICT and Future Planning) [No. 2014M3C9A3063541 and No. 2014M3A9E2064434], in part by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare [HI15C3224], and in part by the Brain Korea 21 Plus Project in 2017."}], "references": [{"title": "Learning the Number of Neurons in Deep Networks", "author": ["Jose M Alvarez", "Mathieu Salzmann"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Secret signatures inside genomic DNA", "author": ["Masanori Arita", "Yoshiaki Ohashi"], "venue": "Biotechnology progress 20,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Bioinformatics: the machine learning approach", "author": ["Pierre Baldi", "S\u00f8ren Brunak"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Finding data in DNA: computer forensic investigations of living organisms", "author": ["Marc B Beck", "Eric C Rouchka", "Roman V Yampolskiy"], "venue": "In International Conference on Digital Forensics and Cyber Crime", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Random oracles are practical: A paradigm for designing e\u0081cient protocols", "author": ["Mihir Bellare", "Phillip Rogaway"], "venue": "In Proceedings of the 1st ACM conference on Computer and communications security", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "Linguistic steganography: Survey, analysis, and robustness concerns for hiding information in text", "author": ["Krista Benne"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Principles and practice of information theory", "author": ["Richard E Blahut"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "Analysis of canonical and noncanonical splice sites in mammalian genomes", "author": ["M Burset", "IA Seledtsov", "VV Solovyev"], "venue": "Nucleic acids research 28,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "\u008ce random oracle methodology, revisited", "author": ["Ran Cane\u008ai", "Oded Goldreich", "Shai Halevi"], "venue": "Journal of the ACM (JACM) 51,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Keras: Deep learning library for theano and tensor\u0083ow", "author": ["Fran\u00e7ois Chollet"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Hiding messages in DNA microdots", "author": ["Catherine Taylor Clelland", "Viviana Risca", "Carter Bancro"], "venue": "Nature 399,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Genetic association studies", "author": ["Heather J Cordell", "David G Clayton"], "venue": "\u008ae Lancet 366,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Search and clustering orders of magnitude faster than BLAST", "author": ["Robert C Edgar"], "venue": "Bioinformatics 26,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Information theory and reliable communication", "author": ["Robert G Gallager"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1968}, {"title": "DNA-based cryptography", "author": ["Ashish Gehani", "\u008comas LaBean", "John Reif"], "venue": "In Aspects of Molecular Computing", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Understanding the di\u0081culty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "DNA-based watermarks using the DNA-Crypt algorithm", "author": ["Dominik Heider", "Angelika Barnekow"], "venue": "BMC bioinformatics 8,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Development and applications of CRISPR-Cas9 for genome engineering", "author": ["Patrick D Hsu", "Eric S Lander", "Feng Zhang"], "venue": "Cell 157,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shi\u0089", "author": ["Sergey Io\u0082e", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Code for encryption hiding data into genomic DNA of living organisms", "author": ["Shuhong Jiao", "Robert Gou\u008ae"], "venue": "In 2008 9th International Conference on Signal Processing", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "\u008ce human genome browser at UCSC", "author": ["W James Kent", "Charles W Sugnet", "Terrence S Furey", "Krishna M Roskin", "Tom H Pringle", "Alan M Zahler", "David Haussler"], "venue": "Genome research 12,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Alternative splicing and evolution: diversi\u0080cation, exon de\u0080nition and function", "author": ["Hadas Keren", "Galit Lev-Maor", "Gil Ast"], "venue": "Nature Reviews Genetics 11,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "DNA- Level Splice Junction Prediction using Deep Recurrent Neural Networks", "author": ["Byunghan Lee", "Taehoon Lee", "Byunggook Na", "Sungroh Yoon"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "DUDE- Seq: Fast, \u0083exible, and robust denoising of nucleotide sequences", "author": ["Byunghan Lee", "Taesup Moon", "Sungroh Yoon", "Tsachy Weissman"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions", "author": ["Taehoon Lee", "Sungroh Yoon"], "venue": "In ICML", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions", "author": ["Taehoon Lee", "Sungroh Yoon"], "venue": "In ICML", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Cryptography with DNA binary strands", "author": ["Andr\u00e9 Leier", "Christoph Richter", "Wolfgang Banzhaf", "Hilmar Rauhe"], "venue": "Biosystems 57,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}, {"title": "Genomics, gene expression and DNA arrays", "author": ["David J Lockhart", "Elizabeth A Winzeler"], "venue": "Nature 405,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geo\u0082rey Hinton"], "venue": "Journal of Machine Learning Research 9,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "Digital steganalysis: Review on recent approaches", "author": ["Indra Kanta Maitra"], "venue": "Journal of Global Research in Computer Science 2,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Deep learning in bioinformatics", "author": ["Seonwoo Min", "Byunghan Lee", "Sungroh Yoon"], "venue": "Brie\u0080ngs in Bioinformatics (2016),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Training knowledge-based neural networks to recognize genes in DNA sequences. Advances in neural information processing systems", "author": ["Michiel O Noordewier", "Geo\u0082rey G Towell", "Jude W Shavlik"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1991}, {"title": "deep- MiRGene: deep neural network based precursor microRNA prediction", "author": ["Seunghyun Park", "Seonwoo Min", "Hyunsoo Choi", "Sungroh Yoon"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Common features of microRNA target prediction tools", "author": ["Sarah M Peterson", "Je\u0082rey A \u008compson", "Melanie L U\u0088in", "Pradeep Sathyanarayana", "Lucy Liaw", "Clare Bates Congdon"], "venue": "Frontiers in genetics", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Correction of sequencing errors in a mixed set of reads", "author": ["Leena Salmela"], "venue": "Bioinformatics 26,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Next-generation sequencing transforms today\u2019s biology", "author": ["Stephan C Schuster"], "venue": "Nature methods 5,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2008}, {"title": "Hiding data in DNA", "author": ["Boris Shimanovsky", "Jessica Feng", "Miodrag Potkonjak"], "venue": "In International Workshop on Information Hiding", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2002}, {"title": "Data hiding methods based upon DNA sequences", "author": ["HJ Shiu", "Ka-Lok Ng", "Jywe-Fei Fang", "Richard CT Lee", "Chien-Hung Huang"], "venue": "Information Sciences 180,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "Lighter: fast and memorye\u0081cient sequencing error correction without counting", "author": ["Li Song", "Liliana Florea", "Ben Langmead"], "venue": "Genome biology 15,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Unsupervised Learning of Video Representations using LSTMs", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "In ICML", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "Least squares support vector machine classi\u0080ers", "author": ["Johan AK Suykens", "Joos Vandewalle"], "venue": "Neural processing le\u0088ers 9,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1999}, {"title": "Public-key system using DNA as a one-way function for key distribution", "author": ["Kazuo Tanaka", "Akimitsu Okamoto", "Isao Saito"], "venue": "Biosystems 81,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2005}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2008}, {"title": "Using Non-invertible Data Transformations to Build Adversary-Resistant", "author": ["Qinglong Wang", "Wenbo Guo", "II Ororbia", "G Alexander", "Xinyu Xing", "Lin Lin", "C Lee Giles", "Xue Liu", "Peng Liu", "Gang Xiong"], "venue": "Deep Neural Networks. arXiv preprint arXiv:1610.01934", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Organic data memory using the DNA approach", "author": ["Pak Chung Wong", "Kwong-kwok Wong", "Harlan Foote"], "venue": "Commun. ACM 46,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2003}, {"title": "Alignment-Based Approach for Durable Data Storage into Living Organisms", "author": ["Nozomu Yachie", "Kazuhide Sekiyama", "Junichi Sugahara", "Yoshiaki Ohashi", "Masaru Tomita"], "venue": "Biotechnology progress 23,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2007}, {"title": "Improved Separable Reversible Data Hiding in Encrypted Image Based on Neighborhood Prediction", "author": ["Shu Yan", "Fan Chen", "Hongjie He"], "venue": "In International Conference on Cloud Computing and Security", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "A survey of errorcorrection methods for next-generation sequencing", "author": ["Xiao Yang", "Sriram P Chockalingam", "Srinivas Aluru"], "venue": "Brie\u0080ngs in bioinformatics 14,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2013}, {"title": "Lossless and reversible data hiding in encrypted images with public-key cryptography", "author": ["Xinpeng Zhang", "Jing Long", "Zichi Wang", "Hang Cheng"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology 26,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Discovering a new covert medium is one of the fundamental problems in espionage [5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 11, "context": "With the advent of genotype-chip technologies, individual genotyping has become less expensive [15], and deoxyribonucleic acid (DNA) in turn has become covert medium.", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "A gram of DNA contains approximately 1021 DNA bases (108 terabytes), which suggests that only a few grams of DNA can store all information available [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 48, "context": "[55] demonstrated that living organisms can be used as a data storage media by inserting arti\u0080cial DNA into arti\u0080cial genomes and using a substitution cipher coding scheme.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "Several other encoding schemes have been proposed [10, 51].", "startOffset": 50, "endOffset": 58}, {"referenceID": 17, "context": "\u008ce DNA-Crypt coding scheme [23] translates a message into 5-bit sequences, and the ASCII coding scheme [26] translates words into their ASCII representation, converts them from decimals to binary, and then replaces 00 with adenine (A), 01 with cytosine (C), 10 with guanine (G), and 11 with thymine (T).", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "\u008ce DNA-Crypt coding scheme [23] translates a message into 5-bit sequences, and the ASCII coding scheme [26] translates words into their ASCII representation, converts them from decimals to binary, and then replaces 00 with adenine (A), 01 with cytosine (C), 10 with guanine (G), and 11 with thymine (T).", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "For example, Benne\u008a [7] exploits text material according to le\u008aer frequency, word frequency, grammar style, semantic continuity, and logical methodologies.", "startOffset": 20, "endOffset": 23}, {"referenceID": 32, "context": "\u008ce advent of detection techniques based on statistical analysis, neural networks, and genetic algorithm [38] methods have been developed for today\u2019s most common covert objects of digital images, video, and audio.", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "However, these methods are o\u0089en restricted by the robustness of the target prediction, and miss important signals during spli\u008aing and merging procedures [33].", "startOffset": 153, "endOffset": 157}, {"referenceID": 31, "context": "D) space using t-SNE [37].", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "Learning temporal behavior enables DNA sequence pa\u008aern recognition to investigate omics data [31, 34, 41].", "startOffset": 93, "endOffset": 105}, {"referenceID": 28, "context": "Learning temporal behavior enables DNA sequence pa\u008aern recognition to investigate omics data [31, 34, 41].", "startOffset": 93, "endOffset": 105}, {"referenceID": 35, "context": "Learning temporal behavior enables DNA sequence pa\u008aern recognition to investigate omics data [31, 34, 41].", "startOffset": 93, "endOffset": 105}, {"referenceID": 33, "context": "Among the various sequence modeling tasks, RNNs [21, 39] are used to obtain a robust prediction result by learning the internal structure (introns and exons) of DNA sequences.", "startOffset": 48, "endOffset": 56}, {"referenceID": 26, "context": ", denoising tools [32, 43, 48, 57], and splice junction classi\u0080cation [31, 34]) for detecting hidden messages.", "startOffset": 18, "endOffset": 34}, {"referenceID": 37, "context": ", denoising tools [32, 43, 48, 57], and splice junction classi\u0080cation [31, 34]) for detecting hidden messages.", "startOffset": 18, "endOffset": 34}, {"referenceID": 41, "context": ", denoising tools [32, 43, 48, 57], and splice junction classi\u0080cation [31, 34]) for detecting hidden messages.", "startOffset": 18, "endOffset": 34}, {"referenceID": 50, "context": ", denoising tools [32, 43, 48, 57], and splice junction classi\u0080cation [31, 34]) for detecting hidden messages.", "startOffset": 18, "endOffset": 34}, {"referenceID": 25, "context": ", denoising tools [32, 43, 48, 57], and splice junction classi\u0080cation [31, 34]) for detecting hidden messages.", "startOffset": 70, "endOffset": 78}, {"referenceID": 28, "context": ", denoising tools [32, 43, 48, 57], and splice junction classi\u0080cation [31, 34]) for detecting hidden messages.", "startOffset": 70, "endOffset": 78}, {"referenceID": 10, "context": "Substitution based on primers of microdots [14] Introns & Exons 20 % DNA cryptosystem with one-time pads [35] Introns & Exons \u2264 9% Insert to arti\u0080cial DNA strand [54] Introns & Exons 20% Improved DNA cryptosystem with one-time pads [18] Introns & Exons Insertion based algorithm namely Arita [3] Exons \u2264 5% Applied existing encryption algorithms to DNA [23] Exons \u2264 8% Substitution and insertion based algorithm [47] Introns & Exons -", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "Substitution based on primers of microdots [14] Introns & Exons 20 % DNA cryptosystem with one-time pads [35] Introns & Exons \u2264 9% Insert to arti\u0080cial DNA strand [54] Introns & Exons 20% Improved DNA cryptosystem with one-time pads [18] Introns & Exons Insertion based algorithm namely Arita [3] Exons \u2264 5% Applied existing encryption algorithms to DNA [23] Exons \u2264 8% Substitution and insertion based algorithm [47] Introns & Exons -", "startOffset": 105, "endOffset": 109}, {"referenceID": 47, "context": "Substitution based on primers of microdots [14] Introns & Exons 20 % DNA cryptosystem with one-time pads [35] Introns & Exons \u2264 9% Insert to arti\u0080cial DNA strand [54] Introns & Exons 20% Improved DNA cryptosystem with one-time pads [18] Introns & Exons Insertion based algorithm namely Arita [3] Exons \u2264 5% Applied existing encryption algorithms to DNA [23] Exons \u2264 8% Substitution and insertion based algorithm [47] Introns & Exons -", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "Substitution based on primers of microdots [14] Introns & Exons 20 % DNA cryptosystem with one-time pads [35] Introns & Exons \u2264 9% Insert to arti\u0080cial DNA strand [54] Introns & Exons 20% Improved DNA cryptosystem with one-time pads [18] Introns & Exons Insertion based algorithm namely Arita [3] Exons \u2264 5% Applied existing encryption algorithms to DNA [23] Exons \u2264 8% Substitution and insertion based algorithm [47] Introns & Exons -", "startOffset": 232, "endOffset": 236}, {"referenceID": 1, "context": "Substitution based on primers of microdots [14] Introns & Exons 20 % DNA cryptosystem with one-time pads [35] Introns & Exons \u2264 9% Insert to arti\u0080cial DNA strand [54] Introns & Exons 20% Improved DNA cryptosystem with one-time pads [18] Introns & Exons Insertion based algorithm namely Arita [3] Exons \u2264 5% Applied existing encryption algorithms to DNA [23] Exons \u2264 8% Substitution and insertion based algorithm [47] Introns & Exons -", "startOffset": 292, "endOffset": 295}, {"referenceID": 17, "context": "Substitution based on primers of microdots [14] Introns & Exons 20 % DNA cryptosystem with one-time pads [35] Introns & Exons \u2264 9% Insert to arti\u0080cial DNA strand [54] Introns & Exons 20% Improved DNA cryptosystem with one-time pads [18] Introns & Exons Insertion based algorithm namely Arita [3] Exons \u2264 5% Applied existing encryption algorithms to DNA [23] Exons \u2264 8% Substitution and insertion based algorithm [47] Introns & Exons -", "startOffset": 353, "endOffset": 357}, {"referenceID": 40, "context": "Substitution based on primers of microdots [14] Introns & Exons 20 % DNA cryptosystem with one-time pads [35] Introns & Exons \u2264 9% Insert to arti\u0080cial DNA strand [54] Introns & Exons 20% Improved DNA cryptosystem with one-time pads [18] Introns & Exons Insertion based algorithm namely Arita [3] Exons \u2264 5% Applied existing encryption algorithms to DNA [23] Exons \u2264 8% Substitution and insertion based algorithm [47] Introns & Exons -", "startOffset": 412, "endOffset": 416}, {"referenceID": 10, "context": "[14] and has gained popularity over the last 10 years.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "\u008ce scheme for hiding data in a DNA sequence was \u0080rst proposed using a microdot technique[14] .", "startOffset": 88, "endOffset": 92}, {"referenceID": 29, "context": "[35] proposed a robust encryption scheme using a primer as the key sequence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[35] uses complementary pairs of (A \u2192 T, T \u2192 A, C \u2192 G, and G\u2192 C) whereas that Shiu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[47] uses complementary pairs of (A \u2192 C, C \u2192 G, G \u2192 T , and T \u2192 A) for an encryption scheme.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "Traditional data hiding approaches usually embed a secret message into the images and audio \u0080les [56, 58].", "startOffset": 97, "endOffset": 105}, {"referenceID": 51, "context": "Traditional data hiding approaches usually embed a secret message into the images and audio \u0080les [56, 58].", "startOffset": 97, "endOffset": 105}, {"referenceID": 46, "context": "However, with the advent of detection schemes, even very small distortions of images can easily detected[53].", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "Intron regions are a candidate for transportation because not all regions are transcribed to RNA regions and removed by splicing [28, 36] during transcription.", "startOffset": 129, "endOffset": 137}, {"referenceID": 30, "context": "Intron regions are a candidate for transportation because not all regions are transcribed to RNA regions and removed by splicing [28, 36] during transcription.", "startOffset": 129, "endOffset": 137}, {"referenceID": 39, "context": "Exons are one suitable candidate for storage because the sequences are carried over a\u0089er the translation and transcription processes [46].", "startOffset": 133, "endOffset": 137}, {"referenceID": 31, "context": "Figure 1 shows the learned representations for each coding and non-coding region projected into a two-dimensional (2-D) space using t-SNE [37].", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "A distinct, clear separation somehow overcomes the limited representation of DNA sequences [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "A random oracle model[6] o\u0082ers a middle ground between a full proof of security and no proof at all.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "By this assumption, steganography scheme E can be bounded by the soundness design [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 6, "context": "In the following, we evaluate the security of steganography scheme using information theoretical proof based on entropy H [8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 13, "context": "According to Gallager [17], the mutual information of I (D; D\u0302) is represented I (D; D\u0302) = H (D) \u2212 H (D|D\u0302).", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "ical donor and acceptor slice sites, respectively [31].", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": ", denoising tools, sequence alignment, and splice junction classi\u0080cation [22, 30, 52]) in order to detect hidden messages.", "startOffset": 73, "endOffset": 85}, {"referenceID": 24, "context": ", denoising tools, sequence alignment, and splice junction classi\u0080cation [22, 30, 52]) in order to detect hidden messages.", "startOffset": 73, "endOffset": 85}, {"referenceID": 45, "context": ", denoising tools, sequence alignment, and splice junction classi\u0080cation [22, 30, 52]) in order to detect hidden messages.", "startOffset": 73, "endOffset": 85}, {"referenceID": 27, "context": "prediction and frequently miss important signals (GT or AT) during spli\u008aing and merging procedures [33].", "startOffset": 99, "endOffset": 103}, {"referenceID": 25, "context": "[31] LSTM-based architecture considerably outperformed \u0080nding canonical/non canonical signals compared to existing alternatives regarding prediction accuracy with the UCSC database [27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[31] LSTM-based architecture considerably outperformed \u0080nding canonical/non canonical signals compared to existing alternatives regarding prediction accuracy with the UCSC database [27].", "startOffset": 181, "endOffset": 185}, {"referenceID": 36, "context": "Our approach is based on sequence-to-sequence learning using an autoencoder and stacked RNNs [42] which proceeds in two main steps: a) unsupervised pre-training of sequence-to-sequence autoencoder for modeling an overcomplete case, and b) supervised \u0080ne-tuning of stacked RNNs for modeling pa\u008aerns between canonical and non-canonical splice sites.", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "As shown in Figure 3, the splice junction site contains consensus strings called canonical splicing pa\u008aerns and the most frequent pa\u008aerns are dimer GT (called donor) and dimer AG at introns/exons boundaries [11].", "startOffset": 207, "endOffset": 211}, {"referenceID": 2, "context": "\u008cese sequences are converted into a binary vector by orthogonal encoding [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "\u3008[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]\u3009.", "startOffset": 1, "endOffset": 13}, {"referenceID": 0, "context": "\u3008[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]\u3009.", "startOffset": 15, "endOffset": 27}, {"referenceID": 0, "context": "\u3008[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]\u3009.", "startOffset": 29, "endOffset": 41}, {"referenceID": 0, "context": "\u3008[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]\u3009.", "startOffset": 43, "endOffset": 55}, {"referenceID": 0, "context": "(5) For example, the sequence ATTT is encoded into 16 dimensional binary vector \u3008[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]\u3009.", "startOffset": 81, "endOffset": 93}, {"referenceID": 0, "context": "(5) For example, the sequence ATTT is encoded into 16 dimensional binary vector \u3008[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]\u3009.", "startOffset": 95, "endOffset": 107}, {"referenceID": 0, "context": "(5) For example, the sequence ATTT is encoded into 16 dimensional binary vector \u3008[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]\u3009.", "startOffset": 109, "endOffset": 121}, {"referenceID": 0, "context": "(5) For example, the sequence ATTT is encoded into 16 dimensional binary vector \u3008[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]\u3009.", "startOffset": 123, "endOffset": 135}, {"referenceID": 42, "context": "\u008crough unsupervised learning of the encoder-decoder model [49], we obtain representations of inherent features, which are directly connected to the second activation layer.", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "We used a recently proposed optimizer of multi-class logarithmic loss function (Adam [29]) for our training model.", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "We simulated our approach with the human UCSC-hg38 dataset [27], which includes sequences from 24 human chromosomes (22 autosomes and 2 sex chromosomes).", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "We randomly selected 63,454 of 229,225 unique exons to remove duplicate exons caused by alternative splicing [28].", "startOffset": 109, "endOffset": 113}, {"referenceID": 34, "context": "[40] by taking the sequences from the center of exon\u2019s le\u0089 and right boundaries, representing acceptor, non-site, and donor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Orthogonal encoding, such as one-hot coding [4], is widely used to convert DNA sequences into numerical format.", "startOffset": 44, "endOffset": 47}, {"referenceID": 25, "context": "[31], the vanilla one-hot encoding scheme tends to limit generalization because of its sparsity of encoding (75% of the elements are zero).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "\u008cus, our approach encodes nucleotides into a four-dimensional dense vector that follows the direct architecture of a normal neural network layer [13], which is trained by the gradient decent method.", "startOffset": 145, "endOffset": 149}, {"referenceID": 23, "context": "We used a recently proposed optimizer of multi-class logarithmic loss function Adam [29] for our training model.", "startOffset": 84, "endOffset": 88}, {"referenceID": 43, "context": "RNN(proposed) O O O O O O SVM [50] X O X O O O Adaptive boosting [44] X O X O X O Random forest[9] O O X O O O Coral[43] X X X X X X Lighter[48] X X X X X X Sequence-alignment[16] X O X O X O", "startOffset": 30, "endOffset": 34}, {"referenceID": 37, "context": "RNN(proposed) O O O O O O SVM [50] X O X O O O Adaptive boosting [44] X O X O X O Random forest[9] O O X O O O Coral[43] X X X X X X Lighter[48] X X X X X X Sequence-alignment[16] X O X O X O", "startOffset": 116, "endOffset": 120}, {"referenceID": 41, "context": "RNN(proposed) O O O O O O SVM [50] X O X O O O Adaptive boosting [44] X O X O X O Random forest[9] O O X O O O Coral[43] X X X X X X Lighter[48] X X X X X X Sequence-alignment[16] X O X O X O", "startOffset": 140, "endOffset": 144}, {"referenceID": 12, "context": "RNN(proposed) O O O O O O SVM [50] X O X O O O Adaptive boosting [44] X O X O X O Random forest[9] O O X O O O Coral[43] X X X X X X Lighter[48] X X X X X X Sequence-alignment[16] X O X O X O", "startOffset": 175, "endOffset": 179}, {"referenceID": 19, "context": "We used a batch size of 100 and followed the most popularized batch normalization [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "We initialized weights according to a uniform distribution as directed by Glorot and Bengio [20].", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "For each algorithm, we generated simulated data for a di\u0082erent number of samples (100,200,300,400,500, and 1000) using the UCSChg38 dataset [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 12, "context": "94 Sequence-alignment[16] 84.", "startOffset": 21, "endOffset": 25}, {"referenceID": 37, "context": "00 Coral [43] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 41, "context": "00 Lighter [48] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 40, "context": "1, each sample was perturbed by the complementary DNA sequences [47] for modi\u0080cation rates from 1 to 10 by changing one percent of the message.", "startOffset": 64, "endOffset": 68}, {"referenceID": 40, "context": "\u008ce detection analysis using the complementary pair algorithm [47] complements our proposed algorithm with RNNs.", "startOffset": 61, "endOffset": 65}, {"referenceID": 38, "context": "Next-generation sequencing has reduced the price of personal genomics [45], and the discovery of the CRIPSPR-Cas9 gene has made it possible to reconstruct DNA sequences [24].", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "Next-generation sequencing has reduced the price of personal genomics [45], and the discovery of the CRIPSPR-Cas9 gene has made it possible to reconstruct DNA sequences [24].", "startOffset": 169, "endOffset": 173}, {"referenceID": 0, "context": "According to Alvarez and Salzmann [1], the numbers of layers and neurons of deep networks can be determined using an additional group sparsity regularizer to the objective function.", "startOffset": 34, "endOffset": 37}], "year": 2017, "abstractText": "\u008ce technique of hiding messages in digital data is called a steganography technique. With improved sequencing techniques, increasing a\u008aempts have been conducted to hide hidden messages in deoxyribonucleic acid (DNA) sequences which have been become a medium for steganography. Many detection schemes have developed for conventional digital data, but these schemes not applicable to DNA sequences because of DNA\u2019s complex internal structures. In this paper, we propose the \u0080rst DNA steganalysis framework for detecting hidden messages and conduct an experiment based on the random oracle model. Among the suitable models for the framework, splice junction classi\u0080cation using deep recurrent neural networks (RNNs) is most appropriate for performing DNA steganalysis . In our DNA steganography approach, we extract the hidden layer composed of RNNs to model the internal structure of a DNA sequence. We provide security for steganography schemes based on mutual entropy and provide simulation results that illustrate how our model detects hidden messages, independent of regions of a targeted reference genome. We apply our method to human genome datasets and determine that hidden messages in DNA sequences with a minimum sample size of 100 are detectable, regardless of the presence of hidden regions.", "creator": "LaTeX with hyperref package"}}}