{"id": "1704.08533", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "EEG-Based User Reaction Time Estimation Using Riemannian Geometry Features", "abstract": "Riemannian euclidean already taken intended not in far virus - computer interface (BCI) classification problems included demonstrated superior quality. In this paper, a along four first, should rather technical to BCI regression have, whose certain s that BCI information. More similar, we oppose a include similar sedimentation balanced both Electroencephalogram (EEG) and BCI stochastic however: end spatial coating yet first variety move income only signal plenty related the EEG participating took only to reduce itself heptatonic of the covariance formula_8, and then Riemannian tangent location features as pellets. We contradict from performance made for to ambitious while unlikely this calculations started EEG frequencies measured in a especially - bringing sharp - critical psychomotor utmost establish, made drama that figure with the primarily chatterer audio, several unbounded space features can reduce the so-called even sits estimation problem by fourth. 30 - second. 30% , of due from estimation corresponding coefficient by 6. 61 - 11. eight%.", "histories": [["v1", "Thu, 27 Apr 2017 12:30:05 GMT  (456kb)", "http://arxiv.org/abs/1704.08533v1", "arXiv admin note: text overlap witharXiv:1702.02914"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1702.02914", "reviews": [], "SUBJECTS": "cs.HC cs.LG", "authors": ["dongrui wu", "brent j lance", "vernon j lawhern", "stephen gordon", "tzyy-ping jung", "chin-teng lin"], "accepted": false, "id": "1704.08533"}, "pdf": {"name": "1704.08533.pdf", "metadata": {"source": "CRF", "title": "EEG-Based User Reaction Time Estimation Using Riemannian Geometry Features", "authors": ["Dongrui Wu"], "emails": ["drwu09@gmail.com,", "brent.j.lance.civ@mail.mil,", "vernon.j.lawhern.civ@mail.mil,", "sgordon@dcscorp.com,", "jung@sccn.ucsd.edu,", "Chin-Teng.Lin@uts.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n08 53\n3v 1\n[ cs\nIndex Terms\u2014Brain-computer interface, EEG, reaction time estimation, Riemannian geometry, spatial filtering\nI. INTRODUCTION\nBrain-computer interfaces (BCIs) can use brain signals such as the scalp electroencephalogram (EEG) to enable people to communicate or control external devices [23], [36]. Thus, they can help people with devastating neuromuscular disorders such as amyotrophic lateral sclerosis, brainstem stroke, cerebral palsy, and spinal cord injury [53]. However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [29], and the availability of highperformance and robust algorithms for signal analysis and interpretation [24], [33], [34], [47]. This paper focuses on the latter, particularly, feature extraction for EEG-based BCIs.\nRiemannian geometry (RG) [3], [10], [27], [41], [45] is a very useful mathematical tool in machine learning and signal/image processing, due to its utility in generating smooth\nmanifolds from intrinsically nonlinear data spaces. Recently it has also been introduced into the BCI community and demonstrated superior performance in a number of applications [5]\u2013 [8], [14], [25], [28], [33], [39], [48], [59].\nFor example, Li, Wong, and de Bruin [28] used RG of the EEG power spectral density matrices for sleep pattern classification. They also proposed a closed-form weighting matrix for the power spectral density matrices to minimize the distance between similar features and to maximize the distance between dissimilar features, and demonstrated better performance than the Euclidian distance and the KullbackLeibler distance. Barachant et al. [5] proposed two RG approaches for motor imagery classification. The first uses the spatial covariance matrices of the EEG signal as features and RG to directly classify them in the manifold of symmetric and positive definite (SPD) matrices. The second maps the covariance matrices onto the Riemannian tangent space, which is a Euclidean space, and then performs variable selection and classification. They achieved comparable or better performance than a multiclass Common Spatial Pattern (CSP) plus Linear Discriminant Analysis (LDA) approach. In [14], Congedo, Barachant, and Andreev further used RG to build calibrationless BCI systems for applications based on eventrelated potentials, sensorimotor (mu) rhythms, and steadystate evoked potential. It outperformed several state-of-theart approaches, including xDAWN, stepwise LDA, CSP+LDA, and blind source separation plus logistic regression. Barachant [7] also proposed a spatial filter to increase the signal to signalplus-noise ratio of magnetoencephalography (MEG) signals before constructing a special form of a covariance matrix for RG feature extraction, and a k-means clustering like unsupervised learning algorithm in the Riemannian manifold to improve the offline classification performance. This approach outperformed 266 other approaches and won the Kaggle\n2 \u201cDecMeg2014 \u2013 Decoding the Human Brain\u201d competition1, which aimed to predict visual stimuli from MEG recordings of human brain activity. Kalunga et al. [25] proposed an online classification approach in the Riemannian space and showed that it outperformed Canonical Correlation Analysis in SteadyState Visually Evoked Potential classification. Yger, Lotte, and Sugiyama [59] empirically compared several covariance matrix averaging methods for EEG signal classification. They showed that RG for averaging covariance matrices improved performances for small dimensional problems, but as the dimensionality of the covariance matrix increased, RG became less efficient. Lotte [33] also proposed a framework to combine transfer learning, ensemble learning, and RG for calibration time reduction, which outperformed CSP+LDA. The Riemannian distance was used in regularization to emphasize auxiliary users whose covariance matrices are close to the target user. Navarro-Sune et al. [39] proposed a BCI to automatically detect patient-ventilator disharmony from EEG signals. RG of EEG covariance matrices was used in semi-supervised learning for effective classification of respiratory state, and it outperformed the Euclidean distance. Waytowich et al. [48] proposed an approach to integrate RG with transfer learning and spectral meta-learner [40], an offline ensemble fusion approach, for user-independent BCI, and demonstrated in single-trial event-related potential classification that it can significantly outperform existing calibration-free techniques and traditional within-subject calibration techniques when limited data is available.\nAll above approaches focused on EEG classification problems in BCI, whereas BCI regression problems have been largely overlooked. In theory a regression problem is equivalent to a classification problem with infinitely many classes, and hence the output has much finer granularity than a traditional two-class or multi-class classification problem, which provides richer information in decision making. There are at least two types of BCI regression problems in the literature and practice. The first type is behavioral or cognitive status prediction, e.g., estimating the continuous value of a driver\u2019s drowsiness from the EEG [30]\u2013[32], [49], [54], [56]\u2013[58], and estimating a subject\u2019s response speed in a psychomotor vigilance task (PVT) from the EEG [55]. The second type is direct control applications, e.g., controlling the movement of a mouse cursor using BCI [13], [21], [35], [51], [52], and controlling the continuous movement of a hand in the 3D space using EEG [12].\nOnce the EEG signal is acquired, the regression problem involves three steps: 1) signal processing to increase the signal-to-noise ratio. Frequency domain filters, such as band pass filters and notch filters [12], [13], and spatial filters, such as independent component analysis [30] and CSP [55], are frequently used here. 2) feature extraction to construct meaningful predictors, e.g., standardized difference of the EEG voltage [12], [13], and EEG power band features [54], [55], [57], [58]. 3) regression algorithms to estimate the continuous output, e.g., ordinary linear regression [12], [13], ridge regression [54], LASSO [55], k-nearest neighbors (kNN)\n1https://www.kaggle.com/c/decoding-the-human-brain.\n[55], fuzzy neural networks [32], transfer learning [49], [56], active learning [57], etc.\nIn this paper, we apply RG and tangent space features to supervised BCI regression problems. To overcome the limitation pointed out by Yger, Lotte, and Sugiyama [59], i.e., RG is less efficient when the dimensionality of the covariance matrix is large, we adopt an approach similar to what Barachant used in [7]: we first use a spatial filter proposed in [55] to reduce the dimensionality of the covariance matrices and also to increase the EEG signal quality, and then extract the RG features in the Riemannian tangent space. We validate the performance of the proposed approach in reaction time (RT) estimation from EEG signals measured in a large-scale sustained-attention PVT [16], which collected 143 sessions of data from 17 subjects in a 5- month period. To our knowledge, this is the first time that RG has been used in BCI regression problems.\nThe remainder of this paper is organized as follows: Section II describes the spatial filter we proposed earlier for supervised BCI regression problems. Section III introduces RG and the tangent space features for BCI regression problems. Section IV describes the experimental setup, RT and EEG data preprocessing techniques, and the procedure to evaluate the performances of different feature extraction methods. Section V presents the results of the comparative studies. Section VI provides parameter sensitivity analysis and additional discussions. Finally, Section VII draws conclusions and outlines a future research direction."}, {"heading": "II. SPATIAL FILTERING FOR SUPERVISED BCI REGRESSION PROBLEMS", "text": "Recently we [55] proposed two spatial filters for supervised BCI regression problems, which were extended from the common spatial pattern (CSP) algorithm for supervised classification problems. They have similar performance and computational cost. One of them, CSP for regression - one versus the rest (CSPR-OVR), is briefly introduced in this section, as the RG features are better extracted from the spatially filtered EEG data than the raw EEG data.\nLet Xn \u2208 RC\u00d7S (n = 1, ..., N ) denote the nth EEG trial in the training data, where C is the number of channels and S the number of time samples. We assume that the mean of each channel measurement has been removed, which is usually performed by band-pass filtering. Let yn \u2208 R be the corresponding RT of the nth trial. CSPR-OVR first constructs K fuzzy sets [60], which partition the training samples into K fuzzy classes. To do that, it partitions the interval [0, 100] into K +1 equal intervals, and denotes the partition points as {pk}k=1,...,K . It is easy to obtain that\npk = 100 \u00b7 k K + 1 , k = 1, ...,K (1)\nFor each pk, CSPR-OVR then finds the corresponding pk percentile value of all training yn and denotes it as Pk. Next we define K fuzzy classes from them, as shown in Fig. 1. Then, for each fuzzy class, CSPR-OVR computes its mean spatial covariance matrix as:\n\u03a3\u0304k =\n\u2211N\nn=1 \u00b5k(yn)XnX T n\n\u2211N n=1 \u00b5k(yn) , k = 1, ...,K (2)\n3\nwhere \u00b5k(yn) is the membership degree of yn in Fuzzy Class k. Next CSPR-OVR designs a spatial filtering matrix W\u2217k \u2208 R C\u00d7F , where F is the number of individual vector filters, to maximize the variance difference between Fuzzy Class k and the rest, i.e.,\nW \u2217 k = arg max\nW\u2208RC\u00d7F\nTr(WT \u03a3\u0304kW)\nTr[WT ( \u2211 i6=k \u03a3\u0304i)W] (3)\nwhere Tr(\u00b7) is the trace of a matrix. (3) is a generalized Rayleigh quotient [22], and the solution W\u2217k is the concatenation of the F eigenvectors associated with the F largest eigenvalues of the matrix ( \u2211\ni6=k \u03a3\u0304i) \u22121 \u03a3\u0304k.\nThe final spatial filtering matrix W\u2217 \u2208 RC\u00d7KF is the concatenation of all W\u2217k, i.e.,\nW \u2217 = [W\u2217 1 , . . . , W\u2217K ] (4)\nand the spatially filtered trial for Xn is:\nX \u2032 n = W \u2217T Xn, n = 1, ..., N. (5)\nIn summary, the complete CSPR-OVR algorithm for super-\nvised BCI regression problems is shown in Algorithm 1.\nAlgorithm 1: The CSPR-OVR spatial filter for supervised BCI regression problems [55].\nInput: EEG training examples (Xn, yn), where Xn \u2208 RC\u00d7S , n = 1, ..., N ; K , the number of fuzzy classes for yn; F , the number of spatial filters for each fuzzy class. Output: Spatially filtered EEG trials X\u2032n \u2208 RKF\u00d7S . Band-pass filter each Xn to remove the mean of each\nchannel;\nCompute {pk}k=1,...,K in (1); Compute the corresponding percentile values\n{Pk}k=1,...,K for yn; Construct the K fuzzy classes as shown in Fig. 1; Compute \u03a3\u0304k by (2); Compute W\u2217k by (3); Construct W\u2217 by (4); Return X\u2032n by (5)"}, {"heading": "III. RG AND THE TANGENT SPACE FEATURES", "text": "This section introduces the basics of RG, and an approach\nto extract the Riemannian tangent space features."}, {"heading": "A. Riemannian Geometry", "text": "The RG approach for BCI works on the covariance matrices of EEG trials, which are symmetric positive-definite and form a differentiable Riemannian manifold M [20] with dimensionality R(R + 1)/2, where R is the number of rows (columns) of the covariance matrices. As a result, we need to use Riemannian metrics, instead of the traditional Euclidean metrics, which are more appropriate for flat spaces of vectors. Particularly, we are interested in the distance measure between two covariance matrices, as many machine learning methods rely on such distances.\nThe Riemannian distance \u03b4(\u03a3\u0304,\u03a3n) between two covariance matrices \u03a3\u0304 \u2208 RR\u00d7R and \u03a3n \u2208 RR\u00d7R, called the geodesic, is the minimum length of a curve connecting them on the manifold M. It can be computed as [4], [37]:\n\u03b4(\u03a3\u0304,\u03a3n) = \u2225 \u2225log ( \u03a3\u0304 \u22121 \u03a3n )\u2225 \u2225\nF =\n[\nR \u2211\nr=1\nlog2 \u03bbr\n]\n1 2\n(6)\nwhere the subscript F denotes the Frobenius norm, and \u03bbr, r = 1, ..., R, are the real eigenvalues of \u03a3\u0304\u22121\u03a3n. At \u03a3\u0304 \u2208 M, a scalar product can be defined in the associated tangent space T\u03a3\u0304M. This tangent space is Euclidean and locally homomorphic to the manifold. So, Riemannian distance computations in the manifold can be approximated by Euclidean distance computations in the tangent space [6].\nThe logarithmic map projects locally a \u03a3n \u2208 M onto the tangent space T\u03a3\u0304M of \u03a3\u0304 by:\n\u03a3\u0302n = Log\u03a3\u0304(\u03a3n) = \u03a3\u0304 1\n2 logm ( \u03a3\u0304 \u2212 1 2\u03a3n\u03a3\u0304 \u2212 1 2 ) \u03a3\u0304 1 2 (7)\nwhere logm(\u00b7) denotes the logarithm of a matrix [10]. The logarithm of a diagonalizable matrix A = VDV\u22121 is defined as logm(A) = VD\u2032V\u22121, where D\u2032 is a diagonal matrix with elements D\u2032i,i = log(Di,i).\nThe exponential map projects an element \u03a3\u0302n on the tangent\nspace T\u03a3\u0304M back to the manifold M by:\n\u03a3n = Exp\u03a3\u0304(\u03a3\u0302n) = \u03a3\u0304 1\n2 expm ( \u03a3\u0304 \u2212 1 2\u03a3n\u03a3\u0304 \u2212 1 2 ) \u03a3\u0304 1 2 (8)\nwhere expm(\u00b7) denotes the exponential of a matrix [10]. The exponential of a diagonalizable matrix A = VDV\u22121 is defined as expm(A) = VD\u2032V\u22121, where D\u2032 is a diagonal matrix with elements D\u2032i,i = exp(Di,i). Fig. 2 illustrates a Riemannian manifold M, the tangent space T\u03a3\u0304M at \u03a3\u0304, the geodesic between \u03a3\u0304 and \u03a3n, and the corresponding logarithmic and exponential maps.\nThe Riemannian distance \u03b4(\u03a3\u0304,\u03a3n) between two covariance matrices \u03a3\u0304 and \u03a3n on the manifold M can also be computed by a Euclidean distance in the tangent space around \u03a3\u0304, i.e. [5],\n\u03b4(\u03a3\u0304,\u03a3n) = \u2016Log\u03a3\u0304(\u03a3n)\u2016\u03a3\u0304 = \u2225 \u2225 \u2225 upper ( \u03a3\u0304 \u2212 1 2 \u03a3\u0302n\u03a3\u0304 \u2212 1 2 )\u2225 \u2225 \u2225\n2\n= \u2225 \u2225 \u2225 upper ( logm ( \u03a3\u0304 \u2212 1 2\u03a3n\u03a3\u0304 \u2212 1 2 )) \u2225 \u2225 \u2225\n2\n(9)\nwhere the upper(\u00b7) operator keeps the upper triangular part of a symmetric matrix and vectorizes it by applying weight 1 for the diagonal elements and weight \u221a 2 for the out-of-diagonal elements [45].\n4\nThe RG mean [42], or the intrinsic mean [19], of N covariance matrices is defined as the matrix minimizing the sum of the squared Riemannian distances, i.e.,\n\u03a3\u0304 \u2261 G(\u03a31, ...,\u03a3N ) = argmin \u03a3\nN \u2211\nn=1\n\u03b42(\u03a3,\u03a3n) (10)\nThere is no closed-form expression for the RG mean, but an iterative gradient descent algorithm (see Algorithm 2 [19]) can be used to find the solution. Note that Algorithm 2 makes heavy use of the logarithmic and exponential maps. In this paper we used the implementation in the Matlab Covariance Toolbox2.\nAlgorithm 2: The gradient descent algorithm for computing the RG (intrinsic) mean [19].\nInput: \u03a3n \u2208 RR\u00d7R, n = 1, ..., N ; \u01eb > 0. Output: The RG (intrinsic) mean \u03a3\u0304 \u2208 RR\u00d7R. Initialize \u03a3\u03040 = 0 \u2208 RR\u00d7R, the zero matrix; Initialize \u03a3\u0304 = I \u2208 RR\u00d7R, the identify matrix; repeat\n\u03a3\u03040 = \u03a3\u0304; \u03a3\u0302 = 1 N \u2211N n=1 Log\u03a3\u03040(\u03a3n);\n\u03a3\u0304 = Exp\u03a3\u03040(\u03a3\u0302). until \u2225 \u2225\u03a3\u0304\u2212 \u03a3\u03040 \u2225\n\u2225 < \u01eb; Return \u03a3\u0304"}, {"heading": "B. Tangent Space Features for BCI Regression Problems", "text": "To use the tangent space features for BCI regression problems, we first spatially filter each Xn to obtain X \u2032 n in (5), and then estimate its spatial covariance matrix \u03a3n \u2208 RKF\u00d7KF (note that each row of X\u2032n has zero mean):\n\u03a3n = 1\nS X\n\u2032 nX \u2032T n , n = 1, ..., N (11)\n2https://github.com/alexandrebarachant/covariancetoolbox.\nNext, we compute the Riemannian mean \u03a3\u0304 of all \u03a3n by Algorithm 2, and take the KF (KF + 1)/2 upper triangular part of logm ( \u03a3\u0304 \u2212 1 2\u03a3n\u03a3\u0304 \u2212 1 2 ) as our features. Note that we need to assign weight 1 to the diagonal elements of logm ( \u03a3\u0304 \u2212 1 2\u03a3n\u03a3\u0304 \u2212 1 2 ) and weight \u221a 2 to the out-of-diagonal elements so that their Euclidean norm is equal to the Riemannian distance between \u03a3\u0304 and \u03a3k. The weights do not have an effect when regression methods like LASSO are used, but are very important for distance based regression methods like kNN regression.\nThe complete tangent space feature extraction procedure for\nBCI regression problems is summarized in Algorithm 3.\nAlgorithm 3: The Riemannian tangent space feature extraction procedure for BCI regression problems.\nInput: Spatially filtered EEG trial X\u2032n \u2208 RKF\u00d7S , n = 1, ..., N . Output: KF (KF + 1)/2 tangent space features for each trial. Compute \u03a3n by (11); Compute \u03a3\u0304 by Algorithm 2; Construct the KF (KF + 1)/2 tangent space features for\nX \u2032 n from logm\n(\n\u03a3\u0304 \u2212 1 2\u03a3n\u03a3\u0304 \u2212 1 2\n)\n."}, {"heading": "IV. EXPERIMENTS AND THE PERFORMANCE EVALUATION PROCESS", "text": "This section introduces a PVT experiment that was used to evaluate the performances of the proposed tangent space feature extraction method, and the corresponding RT and EEG data preprocessing procedures."}, {"heading": "A. Experiment Setup", "text": "Seventeen university students (13 males; average age 22.4, standard deviation 1.6) from National Chiao Tung University (NCTU) in Taiwan volunteered to support the data-collection efforts over a 5-month period to study EEG correlates of attention and performance changes under specific conditions of real-world fatigue [26], as determined by the percent effectiveness score of Readiband [43]. The Institutional Review Board of NCTU approved the experimental protocol.\nThe customer-designed daily sampling system consists of a smartphone, actigraph, sleep diary, subjective scales of fatigue and stress, and software for recording, storing, transmitting, and analyzing data acquired from individuals in their natural environments on a daily basis. Each participant was provided a wrist-worn actigraph (Fatigue Science Readiband, Vancouver, BC), and was instructed to complete several subjective report scales and enter the percent effectiveness score from the actigraph approximately 30-60 minutes upon awakening each morning and to be available for experiment testing approximately once every 1-3 weeks over a 5-month period for a total of nine repeated sessions. Data recorded by the daily sampling system included electronically-adapted visual analog scales of fatigue and stress, the Karolinska Sleepiness Scale [1], and\n5 the Pittsburgh Sleep Diary [38]. The daily sampling data were automatically uploaded from the smartphone to a designated secure server at NCTU on a daily basis. In this way we could track and identify periods when the participants were currently exhibiting low, normal, or high levels of fatigue based on the percent effectiveness score values (>90%, 70 \u2212 90%, <70%, respectively). The goal was to examine the participants during experiment sessions three times within each of the three fatigue levels. Most participants finished all nine sessions.\nWhen the participants reported to the laboratory, we measured their fatigue level on site again right before the experiment to make sure it was close to the fatigue state reported via the smartphone. Upon completion of the related questionnaires and the informed consent form, subjects performed a PVT, a dynamic attention-shifting task, a lane-keeping task, and selected surveys preceding each condition. EEG data were recorded at 1000 Hz using a 64-channel NeuroScan Quik-Cap system (62 EEG channels and 1 electrocardiogram channel). The ground was between FPZ and FZ, and the reference channels were A1 and A2 at the mastoids.\nIn this paper we focus on the PVT [15], which is a sustained-attention task that uses RT to measure the speed with which a subject responds to a visual stimulus. It is widely used, particularly by NASA, for its ease of scoring, simple metrics, convergent validity, and free of learning effects. In our experiment, the PVT was presented on a smartphone with each trial initiated as an empty solid white circle centered on the touchscreen that began to fill in red displayed as a clockwise sweeping motion like the hand of a clock. The sweeping motion was programmed to turn solid red in one second or terminate upon a response by the participants, which required them to tap the touchscreen with the thumb of their dominant hand. The RT was computed as the elapsed time between the appearance of the empty solid white circle and the participant\u2019s response. Following completion of each trial, the circle went back to solid white until the next trial. Inter-trial intervals consisted of random intervals between 2-10 seconds.\n143 sessions of PVT data were collected from the 17 subjects, and each session lasted 10 minutes. Our goal is to predict the RT using a short EEG trial immediately before it."}, {"heading": "B. Performance Evaluation Process", "text": "The following procedure was used to evaluate the perfor-\nmances of different feature extraction methods:\n1) RT data preprocessing to remove outliers.\nThe number of trials and the mean RTs for the 17 subjects are shown in Table I. Subject 17 may have data recording issues, because many of his RTs were longer than 5 seconds, which are highly unlikely in practice, and his mean RT was more than two times larger than the largest mean RT from other subjects. So we excluded him from consideration in this paper, and only used Subjects 1-16. The RTs were very noisy, and there were obvious outliers. It is very important to suppress the outliers and noise so that the performances of different algorithms can be more accurately compared. We employed the following 2-step procedure for RT data preprocessing:\na) Outlier removal, which aimed to remove abnor-\nmally large RTs. First, a threshold \u03b8 = my + 3\u03c3y was computed for each subject, where my is the mean RT from all sessions of that subject, and \u03c3y is the corresponding standard deviation. Then, all RTs larger than \u03b8 were removed. Note that the threshold was different for different subjects. b) Moving average smoothing, which replaced each\nRT by the average RT during a 60 seconds moving window centered at the onset of the corresponding PVT to suppress the noise.\n2) EEG data preprocessing to remove or suppress artifacts\nand noise. Generally raw EEG data recorded from the scalp contain many artifacts (e.g., head motion, blinks, eye movements, etc.) and noise (e.g., power-line noise, noise caused by changes in electrode impedances, etc.) [11], [46], so it is very important to remove or suppress them to increase the signal-to-noise ratio before a machine learning algorithm is applied. This paper used the standardized early-stage EEG processing pipeline (PREP) [11], which consists of three steps: a) remove line-noise, b) determine and remove a robust reference signal, and, c) interpolate the bad channels (channels with a low recording signal-to-noise ratio). The preprocessed EEG signals coming out of PREP were downsampled to 250 Hz. They were then epoched to 5-second trials according to the onset of the PVTs: if a PVT started at t, then the 62-channel EEG trial in [t \u2212 5, t] seconds was used to predict the RT, i.e., Xn \u2208 R62\u00d71250. Each trial was then individually filtered by a [1, 20] Hz finite impulse response band-pass filter to make each channel zero-mean and to remove nonrelevant high frequency components. 3) 5-fold cross-validation to compute the regression perfor-\nmance for each combination of feature set and regression method. We first randomly partitioned the trials into five folds; then, used four folds for supervised spatial filtering and regression model training, and the remaining fold for testing. We repeated this five times so that every fold was used in testing. Finally we computed the regression performances in terms of root mean square error (RMSE) and correlation coefficient (CC). We extracted the following three different feature sets for each preprocessed EEG trial:\n\u2022 Feature Set 1 (FS1): Theta and Alpha powerband\nfeatures from the band-pass filtered EEG trials. We computed the average power spectral density in the Theta band (4-8 Hz) and Alpha band (8-13 Hz) for each channel using Welch\u2019s method [50], and converted these 62 \u00d7 2 = 124 band powers to dBs as our features. \u2022 Feature Set 2 (FS2): Theta and Alpha powerband\nfeatures from EEG trials filtered by Algorithm 1. This procedure was almost identical to the above one, except that the band-pass filtered EEG trials\n6\nwere also spatially filtered by Algorithm 1 before the powerband features were computed. We used 3 fuzzy sets for the RTs, and 10 spatial filters for each fuzzy class, so that the spatially filtered EEG trials had dimensionality 30 \u00d7 1250, and FS2 had 60 dimensions. \u2022 Feature Set 3 (FS3): Riemannian tangent space\nfeatures from EEG trials filtered by Algorithm 1. That is, we first band-pass filtered the raw EEG signals, then spatially filtered them by Algorithm 1 (K = 10 and F = 3), and further applied Algorithm 3 to extract the tangent space features, which\nhad 30\u00d7 31/2 = 465 dimensions. Two regression methods were used on each feature set: LASSO [44], and kNN regression [2]. For labeled training data {xn, yn}n=1,...,N , LASSO solves the following minimization problem to find a sparse linear regression model:\nmin \u03b20,\u03b2\n[\n1\n2N\nN \u2211\nn=1\n( yn \u2212 \u03b20 \u2212 \u03b2Txn )2 + \u03bb \u2016\u03b2\u2016 1\n]\n(12)\nwhere \u03bb > 0 is an adjustable parameter, which was optimized by an inner 5-fold cross-validation on the training dataset in this paper. Once \u03b20 and \u03b2 are identified, the final LASSO regression model is:\ny\u0302n = \u03b20 + \u03b2 T xn (13)\nWe used k = 5 in kNN. Once the five nearest neighbors {xi, yi}i=1,...,5 to the new trial xn are identified, the regression output is computed as a weighted average:\ny\u0302n =\n\u2211\n5\ni=1 wiyi \u2211\n5 i=1 wi (14)\nwhere the weights are the inverses of the feature distances:\nwi = 1\n\u2016xn \u2212 xi\u20162 (15)\n4) Repeat Step 3 10 times and compute the average regres-\nsion performance."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "This section compares the informativeness of the features in FS1, FS2 and FS3, and presents the regression performances.\nA. Informativeness of the Features\nBefore studying the regression performance, it is important to check if the extracted features in FS1, FS2 and FS3 are indeed meaningful.\nIn this first study, we computed the CC between the RT and powerband features in FS1 at different channel locations for each of the 16 subjects, and then averaged them. The corresponding topoplot is shown in Fig. 3. Both theta and alpha band powers show higher correlation at the central and central-frontal regions of the brain; however, generally the CC is small. This indicates that FS1 features are not very informative.\nIn the second study, we picked a typical subject, partitioned his data randomly into 50% training and 50% testing, and extracted the powerband features FS1. We then designed the spatial filters using Algorithm 1 on the training data, and extracted the corresponding powerband features FS2, and the Riemannian tangent space features FS3 using Algorithm 3. For each feature set, we identified the top three features that had the maximum CCs with the RT using the training data, and also computed the corresponding CCs for the testing data. The results are shown in Fig. 4, where in each panel the data on the left of the black dotted line were used for training, and the right for testing. The top thick curve is the RT, and the bottom three curves are the maximally correlated features identified from the training data. The training and testing CCs are shown on the left and right of the corresponding feature, respectively. For FS1, we also show the corresponding channel labels and powerband names. For FS2, we only show the powerband names of the top three features, as a channel here does not have a specific label (each channel in FS2 is a weighted combination of all 62 physical electrodes). Fig. 4 shows that FS2 gave much smoother features than FS1, and also achieved much larger CCs to the RT, both in training and testing, suggesting that spatial filtering by Algorithm 1 can indeed increase the signal quality. FS3 further achieved larger training and testing CCs to the RT than FS2, suggesting that the tangent space features are more informative than the powerband features.\n7 FS1, powerband features from band-pass filtered EEG\nRT FCz, theta, 0.24 C2, theta, 0.23 F7, alpha, 0.21\n0.24 0.22 0.24\nFS2, powerband features from bandpass and spatially filtered EEG\nRT theta, 0.48 theta, 0.48 theta, 0.42\n0.46 0.5 0.41\nFS3, tangent space features from bandpass and spatially filtered EEG\nRT 0.58 0.57 0.52\n0.6 0.62 0.59\nFig. 4. Features from different feature extraction methods, and the corresponding training and testing CCs with the RT."}, {"heading": "B. Estimation Performance Comparison", "text": "The RMSEs and CCs of LASSO and kNN using three different feature sets are shown in Fig. 5 for the 16 subjects. Recall that for each subject the feature extraction methods were run 10 times, each with randomly partitioned training and testing data, and the average regression performances are shown here. The average RMSEs and CCs across all subjects are also shown in the last group of each panel.\nFig. 5 shows that regardless of which regression method was used, generally FS2 resulted in smaller RMSEs and larger CCs than FS1, suggesting that the spatial filtering approach can indeed improve the regression performance. Fig. 5 also shows that FS3 further achieved better RMSEs and CCs than FS2, suggesting that the tangent space features were more effective than the powerband features. Finally, LASSO had better performance than kNN on FS1, but kNN became better on FS2 and FS3. The RMSEs for Subjects 4, 9 and 11 in Fig. 5 are much larger than others, because, as shown in Table I, these three subjects have much larger RTs than others.\nTo illustrate the performance differences among the three feature extraction methods from another viewpoint, Fig. 6 shows the corresponding percentage performance improvements of LASSO and kNN using the three feature sets, where the legend \u201cLASSO,FS2/FS1\u201d means the percentage performance improvement of LASSO on FS2 over LASSO on FS1, and other legends should be understood in a similar manner. For LASSO, on average FS3 had 4.30% smaller\nRMSE than FS2, and 6.59% larger CC. For kNN, on average FS3 had 8.30% smaller RMSE than FS2, and 11.13% larger CC. These results again demonstrated that the tangent space features are more effective than the traditional powerband features.\nWe also performed a two-way Analysis of Variance (ANOVA) for different regression algorithms to check if the raw RMSE and CC differences among the three feature sets (FS1, FS2, and FS3) were statistically significant, by setting the subjects as a random effect. The results are shown in Table II as \u201cp for raw values.\u201d Study results showed that there were statistically significant differences (at 5% level) in raw CCs among different feature sets for both LASSO and kNN, but not for raw RMSEs.\nHowever, because the RTs from different subjects had significantly different magnitudes, an ANOVA on the raw RMSEs and CCs may be unfair for those subjects with small RTs. So, we also performed a two-way ANOVA for different algorithms and feature sets on the ratios. For example, to compute the RMSE ratios for LASSO, we replaced all RMSEs for FS1 by 1, the RMSEs for FS2 by the ratios of the corresponding RMSEs from FS2 over those from FS1, and the RMSEs for FS3 by the ratios of the corresponding RMSEs from FS3 over those from FS1. In this way the RMSEs were normalized, and hence different subjects were treated equally. The corresponding ANOVA test results are shown in Table II as \u201cp for ratios.\u201d Observe that there were statistically significant differences (at 5% level) in both RMSE ratios and CC ratios among different feature sets for both LASSO and kNN.\nThen, non-parametric multiple comparison tests based on Dunn\u2019s procedure [17], [18] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [9]. The p-values for the raw values are shown in Table III, and the p-values for the ratios are shown in\n8 Table IV, where the statistically significant ones are marked in bold. Table III shows that the raw RMSE difference between FS3 and FS1 was statistically significant when kNN was used. Furthermore, the raw CC differences between all pairs of feature sets were statistically significant. Table IV shows that the ratio differences between all pairs of feature sets were statistically significant, for both LASSO and kNN."}, {"heading": "VI. DISCUSSIONS", "text": "additional discussions."}, {"heading": "A. Parameter Sensitivity Analysis", "text": "Tangent space feature extraction relies on the spatial filter in Algorithm 1, which has two adjustable parameters: K , the number of fuzzy classes for the RTs, and F , the number of spatial filters for each fuzzy class. The filtering performance is robust to K but changes noticeably when F changes [55]. As a result, the performance of the tangent space features also varies as F changes. In this subsection we study the sensitivity of the regression performance to F . The regression performances for F = {5, 10, 15, 20} (K was fixed to be 3) are shown in Fig. 7. Algorithms 1 and 3 were repeated five times, each time with a random partition of training and testing data, and the average regression results are shown. Note that F cannot be too large because of three constraints: 1) F cannot exceed the number of channels (C) in the original EEG data, because \u03a3\u0304k\u03a3\u0304\n\u22121 \u2208 RC\u00d7C in (3) has at most C eigenvectors; 2) the tangent space features have dimensionality KF (KF +1)/2, which increases rapidly with F ; so, a large F can easily result in over-fitting; and, 3) there may be numerical difficulties in computing the RG mean when F is large, e.g., for Subjects 5, 8 and 15 in Fig. 7 when F = 20. Fig. 7 shows that the regression performance increased when F increased from 5 to 15, but decreased when F further increased to 20. For the PVT experiment, F \u2208 [10, 15] seemed to achieve a good compromise between performance and computational cost.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Avg Subject\n0\n0.2\n0.4\nR M\nSE\nF=5 F=10 F=15 F=20\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Avg Subject\n0.4\n0.6\n0.8\nC C\nF=5 F=10 F=15 F=20\nAdditionally, in the previous subsection we used 5-second EEG trials to estimate the corresponding RT, and it is also interesting to study how the estimation performance changes with different trial lengths. The results are shown in Fig. 8 for trial lengths of {1, 3, 5, 7, 9} seconds. In general, as trial length increased, the estimation performance improved. However, a longer trial means heavier computational cost and larger delay in estimation. Furthermore, a trial cannot be arbitrary long, as then it cannot capture the up-to-date RT. These effects should be taken into consideration when choosing the right trial length."}, {"heading": "B. Regression Performance versus the Number of Features", "text": "Recall from Section IV-B that FS1 has 124 features, FS2 has 60 features, and FS3 has 465 features, i.e., FS3 has much more features than FS1 and FS2. So, FS3\u2019s superior performance may be due to its increased number of features. In this subsection we investigate the relationship between the regression performance and the number of useful features.\nBecause LASSO automatically selects the most useful features, whereas kNN always uses all the features, in this study we focus only on LASSO. For each subject and each feature set, we used all data in LASSO training, and recorded the number of selected features, as well as the corresponding training RMSEs and CCs. The results are shown in Fig. 9. On average LASSO selected 58.6 features from FS1, 30.6 features from FS2, and 69.1 features from FS3. Although the selected FS2 subset was only about half the size of the selected FS1 subset, they resulted in similar overall training\n9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Avg Subject 0 0.2 0.4 R M SE 1s 3s 5s 7s 9s\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Avg Subject\n0.4\n0.6\n0.8\nC C\n1s 3s 5s 7s 9s\n(a)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Avg Subject\n0\n0.2\n0.4\nR M\nSE\n1s 3s 5s 7s 9s\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Avg Subject\n0.4\n0.6\n0.8\nC C\n1s 3s 5s 7s 9s\n(b)\nFig. 8. RMSEs and CCs of (a) LASSO and (b) kNN with respect to the trial length.\nRMSEs and CCs. Connecting this observation with that in the previous subsection, i.e., FS2 had much better testing RMSEs and CCs than FS1, we can conclude that the CSPR-OVR spatial filter can aggregate the most useful information into just a small number of features, which reduces overfitting and improves the generalization performance. Fig. 9 also shows that the selected FS3 subset was slightly larger than the selected FS1 subset, but the FS3 subset resulted in much better training performance, and also much better testing performance, as presented in the previous subsection. These observations together suggest that the Riemannian geometry approach can indeed extract some novel informative features, which improve both the training and the testing performances."}, {"heading": "C. Computational cost", "text": "The training of our feature extraction method (FS3) consists of three steps: 1) design the CSPR-OVR filter by Algorithm 1; 2) compute the RG mean \u03a3\u0304 by Algorithm 2; and, 3) map the spatially filtered EEG trials to the Riemannian tangent space by Algorithm 3. Once the training is done, feature extraction for a testing trial can be performed very efficiently: a matrix multiplication (5) is first used to spatially filter it, and then another matrix multiplication (11) is used to compute its spatial covariance matrix \u03a3n; finally, compute logm ( \u03a3\u0304 \u2212 1 2\u03a3n\u03a3\u0304 \u2212 1 2 ) and take its upper triangular part as the features. Note that \u03a3\u0304 has been obtained in training, so \u03a3\u0304\u2212 1 2 can be pre-computed, and hence \u03a3\u0304\u2212 1 2\u03a3n\u03a3\u0304 \u2212 1 2 is also a simple matrix multiplication. So, in this subsection we focus on the training computational cost only.\nNumber of selected features\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Avg\nSubject\n0\n50"}, {"heading": "100 LASSO, FS1 LASSO, FS2", "text": "LASSO, FS3\nRMSE in training\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Avg\nSubject\n0\n0.2 0.4 LASSO, FS1 LASSO, FS2 LASSO, FS3\nCC in training\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Avg\nSubject\n0.6\n0.8 LASSO, FS1 LASSO, FS2 LASSO, FS3\nFig. 9. The nubmer of features selected by LASSO, and the corresponding training RMSEs and CCs.\nLet N be the number of training samples. Then, the actual training time increased linearly with N , as shown in Fig. 10. The platform was a Dell XPS15 laptop (Intel i7-6700HQ CPU @2.60GHz, 16 GB memory) running Windows 10 Pro 64-bit and Matlab 2016b. A least squares curve fit shows that the training time is 0.0261+ 0.0030N seconds, which should not be a problem for a practical N ."}, {"heading": "D. RT versus Fatigue State", "text": "We also studied the relationship between the RT and the fatigue state. Our conjecture is that as the fatigue level goes up, the RT should be larger. Boxplots of the RT in different sessions for two typical subjects are shown in Fig. 11, where \u201cL\u201d, \u201cN\u201d and \u201cH\u201d mean low, normal, and high fatigue, respectively. Fig. 11 shows that the mean RT of a high fatigue sessions is generally larger than that of a low or normal fatigue session, and the former also has more extreme values and a larger variance. The difference between a low fatigue session and a normal fatigue session is not obvious. These observations suggest that although the fatigue state contains some useful information, it may be too coarse for accurate RT prediction. That\u2019s why it was not used in this paper.\n10"}, {"heading": "VII. CONCLUSIONS AND FUTURE RESEARCH", "text": "In this paper, we have proposed a new feature extraction approach for EEG-based BCI regression problems: a spatial filter is first used to increase the EEG trial signal quality and also to reduce the dimensionality of the covariance matrix, and then Riemannian tangent space features are extracted. We validated the performance of the proposed approach in RT estimation from EEG signals measured in a large-scale sustainedattention PVT experiment, and showed that compared with the traditional powerband features, the tangent space features can reduce the estimation RMSE by 4.30-8.30%, and increase the estimation CC by 6.59-11.13%. To our knowledge, this is the first time that RG has been used in BCI regression problems.\nOur future research will focus on reducing the dimensionality of the tangent space features. As shown in Algorithm 3, the tangent space features have dimensionality KF (KF + 1)/2, where K is the number of fuzzy classes for the RTs, and F is the number of spatial filters for each fuzzy class. So, the feature dimensionality increases quadratically with respect to both K and F , which quickly results in overwhelming computational cost, overfitting, and numerical problems. We will investigate effective dimensionality reduction approaches for the tangent space features to reduce the computational cost while maintaining or even improving the regression performance."}, {"heading": "ACKNOWLEDGEMENT", "text": "Research was sponsored by the U.S. Army Research Laboratory and was accomplished under Cooperative Agreement Numbers W911NF-10-2-0022 and W911NF-10-D-0002/TO 0023. The views and the conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army Research Laboratory or the U.S Government. This work was also partially supported by the Australian Research Council (ARC) under discovery grant DP150101645."}], "references": [{"title": "Subjective and objective sleepiness in the active individual", "author": ["T. Akerstedt", "M. Gillberg"], "venue": "International Journal of Neuroscience, vol. 52, no. 1-2, pp. 29\u201337, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "An introduction to kernel and nearest-neighbor nonparametric regression", "author": ["N.S. Altman"], "venue": "The American Statistician, vol. 46, no. 3, pp. 175\u2013185, 1992.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "Geometric means in a novel vector space structure on symmetric positive-definite matrices", "author": ["V. Arsigny", "P. Fillard", "X. Pennec", "N. Ayache"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 29, no. 1, pp. 328\u2013347, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiclass braincomputer interface classification by Riemannian geometry", "author": ["A. Barachant", "S. Bonnet", "M. Congedo", "C. Jutten"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 59, no. 4, pp. 920\u2013928, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification of covariance matrices using a Riemannian-based kernel for BCI applications", "author": ["A. Barachant", "S. Bonnet", "M. Congedo", "C. Jutten"], "venue": "Neurocomputing, vol. 112, pp. 172\u2013178, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "MEG decoding using Riemannian geometry and unsupervised classification. Accessed: 8/17/2016", "author": ["A. Barachant"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A plug & play P300 BCI using information geometry", "author": ["A. Barachant", "M. Congedo"], "venue": "arXiv: 1409.0107, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Controlling the false discovery rate: A practical and powerful approach to multiple testing", "author": ["Y. Benjamini", "Y. Hochberg"], "venue": "Journal of the Royal Statistical Society, Series B (Methodological), vol. 57, pp. 289\u2013 300, 1995.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "A Panoramic View of Riemannian Geometry", "author": ["M. Berger"], "venue": "New York, NY: Springer,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "The PREP pipeline: standardized preprocessing for large-scale EEG analysis", "author": ["N. Bigdely-Shamlo", "T. Mullen", "C. Kothe", "K.-M. Su", "K.A. Robbins"], "venue": "Frontiers in Neuroinformatics, vol. 9, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Reconstructing three-dimensional hand movements from noninvasive electroencephalographic signals", "author": ["T.J. Bradberry", "R.J. Gentili", "J.L. Contreras-Vidal"], "venue": "Journal of Neuroscience, vol. 30, no. 9, pp. 3432\u20133437, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast attainment of computer cursor control with noninvasively acquired brain signals", "author": ["T.J. Bradberry", "R.J. Gentili", "J.L. Contreras-Vidal"], "venue": "Journal of Neural Engineering, vol. 8, no. 3, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "A new generation of braincomputer interface based on Riemannian geometry", "author": ["M. Congedo", "A. Barachant", "A. Andreev"], "venue": "arXiv: 1310.8115, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Microcomputer analyses of performance on a portable, simple visual RT task during sustained operations", "author": ["D.F. Dinges", "J.W. Powell"], "venue": "Behavior research methods, instruments, & computers, vol. 17, no. 6, pp. 652\u2013655, 1985.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1985}, {"title": "The neural basis of the psychomotor vigilance task", "author": ["S.P. Drummond", "A. Bischoff-Grethe", "D.F. Dinges", "L. Ayalon", "S.C. Mednick", "M. Meloy"], "venue": "Sleep, vol. 28, no. 9, pp. 1059\u20131068, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiple comparisons among means", "author": ["O. Dunn"], "venue": "Journal of the American Statistical Association, vol. 56, pp. 62\u201364, 1961.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1961}, {"title": "Multiple comparisons using rank sums", "author": ["O. Dunn"], "venue": "Technometrics, vol. 6, pp. 214\u2013252, 1964.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1964}, {"title": "Principal geodesic analysis on symmetric spaces: Statistics of diffusion tensors", "author": ["P.T. Fletcher", "S. Joshi"], "venue": "Computer Vision and Mathematical Methods in Medical and Biomedical Image Analysis, pp. 87\u201398, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "A metric for covariance matrices", "author": ["W. Forstner", "B. Moonen"], "venue": "Dept. of Geodesy and Geoinformatics, Stuttgart University, Tech. Rep., 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "A comparison of regression techniques for a two-dimensional sensorimotor rhythm-based brain-computer interface", "author": ["J. Fruitet", "D.J. McFarland", "J.R. Wolpaw"], "venue": "Journal of Neural Engineering, vol. 7, no. 1, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Matrix Computation, 3rd ed", "author": ["G.H. Golub", "C.F.V. Loan"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Noninvasive brain-computer interfaces based on sensorimotor rhythms", "author": ["B. He", "B. Baxter", "B.J. Edelman", "C.C. Cline", "W.W. Ye"], "venue": "Proc. of the IEEE, vol. 103, no. 6, pp. 907\u2013925, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer learning in brain-computer interfaces", "author": ["V. Jayaram", "M. Alamgir", "Y. Altun", "B. Scholkopf", "M. Grosse- Wentrup"], "venue": "IEEE Computational Intelligence Magazine, vol. 11, no. 1, pp. 20\u201331, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Online SSVEP-based BCI using Riemannian geometry", "author": ["E.K. Kalunga", "S. Chevallier", "K. Djouani", "E. Monacelli", "Y. Hamam"], "venue": "Neurocomputing, vol. 191, pp. 55\u201368, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Inter- and intra-individual variations in sleep, subjective fatigue, and vigilance task performance of students in their real-world environments over extended periods", "author": ["S. Kerick", "C.-H. Chuang", "J.-T. King", "T.-P. Jung", "J. Brooks", "B.T. Files", "K. McDowell", "C.-T. Lin"], "venue": "2016, submitted.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Introduction to Smooth Manifolds", "author": ["J. Lee"], "venue": "New York, NY: Springer,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Electroencephalogram signals classification for sleep state decision \u2013 a Riemannian geometry approach", "author": ["Y. Li", "K. Wong", "H. de Bruin"], "venue": "IET Signal Processing, vol. 6, no. 4, pp. 288\u2013299, 2012.  11", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Biosensor technologies for augmented brain-computer interfaces in the next decades", "author": ["L.-D. Liao", "C.-T. Lin", "K. McDowell", "A. Wickenden", "K. Gramann", "T.-P. Jung", "L.-W. Ko", "J.-Y. Chang"], "venue": "Proc. of the IEEE, vol. 100, no. 2, pp. 1553\u20131566, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "EEG-based drowsiness estimation for safety driving using independent component analysis", "author": ["C.T. Lin", "R.C. Wu", "S.F. Liang", "T.Y. Huang", "W.H. Chao", "Y.J. Chen", "T.P. Jung"], "venue": "IEEE Trans. on Circuits and Systems, vol. 52, pp. 2726\u20132738, 2005.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Development of wireless brain computer interface with embedded multitask scheduling and its application on real-time driver\u2019s drowsiness detection and warning", "author": ["C.-T. Lin", "Y.-C. Chen", "T.-Y. Huang", "T.-T. Chiu", "L.-W. Ko", "S.-F. Liang", "H.-Y. Hsieh", "S.-H. Hsu", "J.-R. Duann"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 55, no. 5, pp. 1582\u20131591, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Adaptive EEG-based alertness estimation system by using ICA-based fuzzy neural networks", "author": ["C.-T. Lin", "L.-W. Ko", "I.-F. Chung", "T.-Y. Huang", "Y.-C. Chen", "T.-P. Jung", "S.-F. Liang"], "venue": "IEEE Trans. on Circuits and Systems-I, vol. 53, no. 11, pp. 2469\u20132476, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Signal processing approaches to minimize or suppress calibration time in oscillatory activity-based brain-computer interfaces", "author": ["F. Lotte"], "venue": "Proc. of the IEEE, vol. 103, no. 6, pp. 871\u2013890, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolving signal processing for brain-computer interfaces", "author": ["S. Makeig", "C. Kothe", "T. Mullen", "N. Bigdely-Shamlo", "Z. Zhang", "K. Kreutz-Delgado"], "venue": "Proc. of the IEEE, vol. 100, no. Special Centennial Issue, pp. 1567\u20131584, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Design and operation of an EEG-based brain-computer interface with digital signal processing technology", "author": ["D.J. McFarland", "A.T. Lefkowicz", "J.R. Wolpaw"], "venue": "Behavior Research Methods, Instruments, & Computers, vol. 29, no. 3, pp. 337\u2013345, 1997.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1997}, {"title": "Electroencephalographic (EEG) control of three-dimensional movement", "author": ["D.J. McFarland", "W.A. Sarnacki", "J.R. Wolpaw"], "venue": "Journal of Neural Engineering, vol. 7, no. 3, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "A differential geometric approach to the geometric mean of symmetric positive-definite matrices", "author": ["M. Moakher"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 26, no. 3, pp. 735\u2013747, 2005.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "The Pittsburgh sleep diary", "author": ["T. Monk", "C. Reynolds", "D. Kupfer", "D. Buysse", "P. Coble", "A. Hayes", "M. Machen", "S. Petrie", "A. Ritenour"], "venue": "Journal of Sleep Research, vol. 3, pp. 111\u2013120, 1994.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1994}, {"title": "Riemannian geometry applied to detection of respiratory states from EEG signals: the basis for a brain-ventilator interface", "author": ["X. Navarro-Sune", "A.L. Hudson", "F.D.V. Fallani", "J. Martinerie", "A. Witon", "P. Puget", "M. Raux", "T. Similowski", "M. Chavez"], "venue": "IEEE Trans. on Biomedical Engineering, 2016, in press.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Ranking and combining multiple predictors without labeled data", "author": ["F. Parisi", "F. Strino", "B. Nadler", "Y. Kluger"], "venue": "Proc. National Academy of Science (PNAS), vol. 111, no. 4, pp. 1253\u20131258, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements", "author": ["X. Pennec"], "venue": "Journal of Mathematical Imaging and Vision, vol. 25, no. 1, pp. 127\u2013154, 2006.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "A Riemannian framework for tensor computing", "author": ["X. Pennec", "P. Fillard", "N. Ayache"], "venue": "Int\u2019l Journal of Computer Vision, vol. 66, no. 1, pp. 41\u201366, 2006.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Validation of the fatigue science readiband actigraph and associated sleep/wake classification algorithms. Accessed: 08/11/2016", "author": ["C. Russell", "J. Caldwell", "D. Arand", "L. Myers", "P. Wubbels", "H. Downs"], "venue": "Validation.pdf", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, vol. 58, no. 1, pp. 267\u2013288, 1996.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1996}, {"title": "Pedestrian detection via classification on Riemannian manifolds", "author": ["O. Tuzel", "F. Porikli", "P. Meer"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 30, no. 10, pp. 1713\u20131727, 2008.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "EEG artifact removal \u2013 state-ofthe-art and guidelines", "author": ["J.A. Uriguen", "B. Garcia-Zapirain"], "venue": "Journal of Neural Engineering, vol. 12, no. 3, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "A review on transfer learning for brain-computer interface classification", "author": ["P. Wang", "J. Lu", "B. Zhang", "Z. Tang"], "venue": "Prof. 5th Int\u2019l Conf. on Information Science and Technology (IC1ST), Changsha, China, April 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral transfer learning using Information Geometry for a user-independent brain-computer interface", "author": ["N.R. Waytowich", "V.J. Lawhern", "A.W. Bohannon", "K.R. Ball", "B.J. Lance"], "venue": "Frontiers in Neuroscience, vol. 10, p. 430, 2016.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Selective transfer learning for EEG-based drowsiness detection", "author": ["C.-S. Wei", "Y.-P. Lin", "Y.-T. Wang", "T.-P. Jung", "N. Bigdely-Shamlo", "C.- T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Hong Kong, October 2015, pp. 3229\u20133232.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "The use of fast Fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms", "author": ["P. Welch"], "venue": "IEEE Trans. on Audio Electroacoustics, vol. 15, pp. 70\u2013 73, 1967.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1967}, {"title": "An EEG-based brain-computer interface for cursor control", "author": ["J.R. Wolpaw", "D.J. McFarland", "G.W. Neat", "C.A. Forneris"], "venue": "Electroencephalography and Clinical Neurophysiology, vol. 78, no. 3, pp. 252\u2013 259, 1991.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1991}, {"title": "Brain-computer interface research at the Wadsworth Center", "author": ["J.R. Wolpaw", "D.J. McFarland", "T.M. Vaughan"], "venue": "IEEE Trans. on Rehabilitation Engineering, vol. 8, no. 2, pp. 222\u2013226, 2000.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2000}, {"title": "Brain-computer interfaces for communication and control", "author": ["J. Wolpaw", "N. Birbaumer"], "venue": "Textbook for Neural Repair and Repair and Rehabilitation, M. Selzer, L. Cohen, F. Gage, S. Clarke, and P. Duncan, Eds. Cambridge University Press, 2006, pp. 602\u2013614.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Online driver\u2019s drowsiness estimation using domain adaptation with model fusion", "author": ["D. Wu", "C.-H. Chuang", "C.-T. Lin"], "venue": "Proc. Int\u2019l Conf. on Affective Computing and Intelligent Interaction, Xi\u2019an, China, September 2015, pp. 904\u2013910.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Spatial filtering for EEG-based regression problems in brain-computer interface (BCI)", "author": ["D. Wu", "J.-T. King", "C.-H. Chuang", "C.-T. Lin", "T.-P. Jung"], "venue": "IEEE Trans. on Fuzzy Systems, 2017, accepted. [Online]. Available: https://arxiv.org/abs/1702.02914", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2017}, {"title": "Driver drowsiness estimation from EEG signals using online weighted adaptation regularization for regression (OwARR)", "author": ["D. Wu", "V.J. Lawhern", "S. Gordon", "B.J. Lance", "C.-T. Lin"], "venue": "IEEE Trans. on Fuzzy Systems, 2016, in press.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2016}, {"title": "Offline EEG-based driver drowsiness estimation using enhanced batch-mode active learning (EBMAL) for regression", "author": ["D. Wu", "V.J. Lawhern", "S. Gordon", "B.J. Lance", "C.-T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Budapest, Hungary, October 2016, pp. 730\u2013736.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral meta-learner for regression (SMLR) model aggregation: Towards calibrationless brain-computer interface (BCI)", "author": ["D. Wu", "V.J. Lawhern", "S. Gordon", "B.J. Lance", "C.-T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Budapest, Hungary, October 2016, pp. 743\u2013749.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Averaging covariance matrices for EEG signal classification based on the CSP: An empirical study", "author": ["F. Yger", "F. Lotte", "M. Sugiyama"], "venue": "Proc. 23rd European Signal Processing Conference (EUSIPCO), Nice, France, August 2015, pp. 2721\u20132725.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2015}, {"title": "Fuzzy sets", "author": ["L.A. Zadeh"], "venue": "Information and Control, vol. 8, pp. 338\u2013353, 1965.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1965}], "referenceMentions": [{"referenceID": 21, "context": "Brain-computer interfaces (BCIs) can use brain signals such as the scalp electroencephalogram (EEG) to enable people to communicate or control external devices [23], [36].", "startOffset": 160, "endOffset": 164}, {"referenceID": 34, "context": "Brain-computer interfaces (BCIs) can use brain signals such as the scalp electroencephalogram (EEG) to enable people to communicate or control external devices [23], [36].", "startOffset": 166, "endOffset": 170}, {"referenceID": 51, "context": "Thus, they can help people with devastating neuromuscular disorders such as amyotrophic lateral sclerosis, brainstem stroke, cerebral palsy, and spinal cord injury [53].", "startOffset": 164, "endOffset": 168}, {"referenceID": 27, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [29], and the availability of highperformance and robust algorithms for signal analysis and interpretation [24], [33], [34], [47].", "startOffset": 179, "endOffset": 183}, {"referenceID": 22, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [29], and the availability of highperformance and robust algorithms for signal analysis and interpretation [24], [33], [34], [47].", "startOffset": 286, "endOffset": 290}, {"referenceID": 31, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [29], and the availability of highperformance and robust algorithms for signal analysis and interpretation [24], [33], [34], [47].", "startOffset": 292, "endOffset": 296}, {"referenceID": 32, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [29], and the availability of highperformance and robust algorithms for signal analysis and interpretation [24], [33], [34], [47].", "startOffset": 298, "endOffset": 302}, {"referenceID": 45, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [29], and the availability of highperformance and robust algorithms for signal analysis and interpretation [24], [33], [34], [47].", "startOffset": 304, "endOffset": 308}, {"referenceID": 8, "context": "Riemannian geometry (RG) [3], [10], [27], [41], [45] is a very useful mathematical tool in machine learning and signal/image processing, due to its utility in generating smooth manifolds from intrinsically nonlinear data spaces.", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": "Riemannian geometry (RG) [3], [10], [27], [41], [45] is a very useful mathematical tool in machine learning and signal/image processing, due to its utility in generating smooth manifolds from intrinsically nonlinear data spaces.", "startOffset": 36, "endOffset": 40}, {"referenceID": 39, "context": "Riemannian geometry (RG) [3], [10], [27], [41], [45] is a very useful mathematical tool in machine learning and signal/image processing, due to its utility in generating smooth manifolds from intrinsically nonlinear data spaces.", "startOffset": 42, "endOffset": 46}, {"referenceID": 43, "context": "Riemannian geometry (RG) [3], [10], [27], [41], [45] is a very useful mathematical tool in machine learning and signal/image processing, due to its utility in generating smooth manifolds from intrinsically nonlinear data spaces.", "startOffset": 48, "endOffset": 52}, {"referenceID": 3, "context": "Recently it has also been introduced into the BCI community and demonstrated superior performance in a number of applications [5]\u2013 [8], [14], [25], [28], [33], [39], [48], [59].", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "Recently it has also been introduced into the BCI community and demonstrated superior performance in a number of applications [5]\u2013 [8], [14], [25], [28], [33], [39], [48], [59].", "startOffset": 131, "endOffset": 134}, {"referenceID": 12, "context": "Recently it has also been introduced into the BCI community and demonstrated superior performance in a number of applications [5]\u2013 [8], [14], [25], [28], [33], [39], [48], [59].", "startOffset": 136, "endOffset": 140}, {"referenceID": 23, "context": "Recently it has also been introduced into the BCI community and demonstrated superior performance in a number of applications [5]\u2013 [8], [14], [25], [28], [33], [39], [48], [59].", "startOffset": 142, "endOffset": 146}, {"referenceID": 26, "context": "Recently it has also been introduced into the BCI community and demonstrated superior performance in a number of applications [5]\u2013 [8], [14], [25], [28], [33], [39], [48], [59].", "startOffset": 148, "endOffset": 152}, {"referenceID": 31, "context": "Recently it has also been introduced into the BCI community and demonstrated superior performance in a number of applications [5]\u2013 [8], [14], [25], [28], [33], [39], [48], [59].", "startOffset": 154, "endOffset": 158}, {"referenceID": 37, "context": "Recently it has also been introduced into the BCI community and demonstrated superior performance in a number of applications [5]\u2013 [8], [14], [25], [28], [33], [39], [48], [59].", "startOffset": 160, "endOffset": 164}, {"referenceID": 46, "context": "Recently it has also been introduced into the BCI community and demonstrated superior performance in a number of applications [5]\u2013 [8], [14], [25], [28], [33], [39], [48], [59].", "startOffset": 166, "endOffset": 170}, {"referenceID": 57, "context": "Recently it has also been introduced into the BCI community and demonstrated superior performance in a number of applications [5]\u2013 [8], [14], [25], [28], [33], [39], [48], [59].", "startOffset": 172, "endOffset": 176}, {"referenceID": 26, "context": "For example, Li, Wong, and de Bruin [28] used RG of the EEG power spectral density matrices for sleep pattern classification.", "startOffset": 36, "endOffset": 40}, {"referenceID": 3, "context": "[5] proposed two RG approaches for motor imagery classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "In [14], Congedo, Barachant, and Andreev further used RG to build calibrationless BCI systems for applications based on eventrelated potentials, sensorimotor (mu) rhythms, and steadystate evoked potential.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Barachant [7] also proposed a spatial filter to increase the signal to signalplus-noise ratio of magnetoencephalography (MEG) signals before constructing a special form of a covariance matrix for RG feature extraction, and a k-means clustering like unsupervised learning algorithm in the Riemannian manifold to improve the offline classification performance.", "startOffset": 10, "endOffset": 13}, {"referenceID": 23, "context": "[25] proposed an online classification approach in the Riemannian space and showed that it outperformed Canonical Correlation Analysis in SteadyState Visually Evoked Potential classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "Yger, Lotte, and Sugiyama [59] empirically compared several covariance matrix averaging methods for EEG signal classification.", "startOffset": 26, "endOffset": 30}, {"referenceID": 31, "context": "Lotte [33] also proposed a framework to combine transfer learning, ensemble learning, and RG for calibration time reduction, which outperformed CSP+LDA.", "startOffset": 6, "endOffset": 10}, {"referenceID": 37, "context": "[39] proposed a BCI to automatically detect patient-ventilator disharmony from EEG signals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[48] proposed an approach to integrate RG with transfer learning and spectral meta-learner [40], an offline ensemble fusion approach, for user-independent BCI, and demonstrated in single-trial event-related potential classification that it can significantly outperform existing calibration-free techniques and traditional within-subject calibration techniques when limited data is available.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[48] proposed an approach to integrate RG with transfer learning and spectral meta-learner [40], an offline ensemble fusion approach, for user-independent BCI, and demonstrated in single-trial event-related potential classification that it can significantly outperform existing calibration-free techniques and traditional within-subject calibration techniques when limited data is available.", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": ", estimating the continuous value of a driver\u2019s drowsiness from the EEG [30]\u2013[32], [49], [54], [56]\u2013[58], and estimating a subject\u2019s response speed in a psychomotor vigilance task (PVT) from the EEG [55].", "startOffset": 72, "endOffset": 76}, {"referenceID": 30, "context": ", estimating the continuous value of a driver\u2019s drowsiness from the EEG [30]\u2013[32], [49], [54], [56]\u2013[58], and estimating a subject\u2019s response speed in a psychomotor vigilance task (PVT) from the EEG [55].", "startOffset": 77, "endOffset": 81}, {"referenceID": 47, "context": ", estimating the continuous value of a driver\u2019s drowsiness from the EEG [30]\u2013[32], [49], [54], [56]\u2013[58], and estimating a subject\u2019s response speed in a psychomotor vigilance task (PVT) from the EEG [55].", "startOffset": 83, "endOffset": 87}, {"referenceID": 52, "context": ", estimating the continuous value of a driver\u2019s drowsiness from the EEG [30]\u2013[32], [49], [54], [56]\u2013[58], and estimating a subject\u2019s response speed in a psychomotor vigilance task (PVT) from the EEG [55].", "startOffset": 89, "endOffset": 93}, {"referenceID": 54, "context": ", estimating the continuous value of a driver\u2019s drowsiness from the EEG [30]\u2013[32], [49], [54], [56]\u2013[58], and estimating a subject\u2019s response speed in a psychomotor vigilance task (PVT) from the EEG [55].", "startOffset": 95, "endOffset": 99}, {"referenceID": 56, "context": ", estimating the continuous value of a driver\u2019s drowsiness from the EEG [30]\u2013[32], [49], [54], [56]\u2013[58], and estimating a subject\u2019s response speed in a psychomotor vigilance task (PVT) from the EEG [55].", "startOffset": 100, "endOffset": 104}, {"referenceID": 53, "context": ", estimating the continuous value of a driver\u2019s drowsiness from the EEG [30]\u2013[32], [49], [54], [56]\u2013[58], and estimating a subject\u2019s response speed in a psychomotor vigilance task (PVT) from the EEG [55].", "startOffset": 199, "endOffset": 203}, {"referenceID": 11, "context": ", controlling the movement of a mouse cursor using BCI [13], [21], [35], [51], [52], and controlling the continuous movement of a hand in the 3D space using EEG [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": ", controlling the movement of a mouse cursor using BCI [13], [21], [35], [51], [52], and controlling the continuous movement of a hand in the 3D space using EEG [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 33, "context": ", controlling the movement of a mouse cursor using BCI [13], [21], [35], [51], [52], and controlling the continuous movement of a hand in the 3D space using EEG [12].", "startOffset": 67, "endOffset": 71}, {"referenceID": 49, "context": ", controlling the movement of a mouse cursor using BCI [13], [21], [35], [51], [52], and controlling the continuous movement of a hand in the 3D space using EEG [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 50, "context": ", controlling the movement of a mouse cursor using BCI [13], [21], [35], [51], [52], and controlling the continuous movement of a hand in the 3D space using EEG [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": ", controlling the movement of a mouse cursor using BCI [13], [21], [35], [51], [52], and controlling the continuous movement of a hand in the 3D space using EEG [12].", "startOffset": 161, "endOffset": 165}, {"referenceID": 10, "context": "Frequency domain filters, such as band pass filters and notch filters [12], [13], and spatial filters, such as independent component analysis [30] and CSP [55], are frequently used here.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Frequency domain filters, such as band pass filters and notch filters [12], [13], and spatial filters, such as independent component analysis [30] and CSP [55], are frequently used here.", "startOffset": 76, "endOffset": 80}, {"referenceID": 28, "context": "Frequency domain filters, such as band pass filters and notch filters [12], [13], and spatial filters, such as independent component analysis [30] and CSP [55], are frequently used here.", "startOffset": 142, "endOffset": 146}, {"referenceID": 53, "context": "Frequency domain filters, such as band pass filters and notch filters [12], [13], and spatial filters, such as independent component analysis [30] and CSP [55], are frequently used here.", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": ", standardized difference of the EEG voltage [12], [13], and EEG power band features [54], [55], [57], [58].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": ", standardized difference of the EEG voltage [12], [13], and EEG power band features [54], [55], [57], [58].", "startOffset": 51, "endOffset": 55}, {"referenceID": 52, "context": ", standardized difference of the EEG voltage [12], [13], and EEG power band features [54], [55], [57], [58].", "startOffset": 85, "endOffset": 89}, {"referenceID": 53, "context": ", standardized difference of the EEG voltage [12], [13], and EEG power band features [54], [55], [57], [58].", "startOffset": 91, "endOffset": 95}, {"referenceID": 55, "context": ", standardized difference of the EEG voltage [12], [13], and EEG power band features [54], [55], [57], [58].", "startOffset": 97, "endOffset": 101}, {"referenceID": 56, "context": ", standardized difference of the EEG voltage [12], [13], and EEG power band features [54], [55], [57], [58].", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": ", ordinary linear regression [12], [13], ridge regression [54], LASSO [55], k-nearest neighbors (kNN)", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": ", ordinary linear regression [12], [13], ridge regression [54], LASSO [55], k-nearest neighbors (kNN)", "startOffset": 35, "endOffset": 39}, {"referenceID": 52, "context": ", ordinary linear regression [12], [13], ridge regression [54], LASSO [55], k-nearest neighbors (kNN)", "startOffset": 58, "endOffset": 62}, {"referenceID": 53, "context": ", ordinary linear regression [12], [13], ridge regression [54], LASSO [55], k-nearest neighbors (kNN)", "startOffset": 70, "endOffset": 74}, {"referenceID": 53, "context": "[55], fuzzy neural networks [32], transfer learning [49], [56], active learning [57], etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[55], fuzzy neural networks [32], transfer learning [49], [56], active learning [57], etc.", "startOffset": 28, "endOffset": 32}, {"referenceID": 47, "context": "[55], fuzzy neural networks [32], transfer learning [49], [56], active learning [57], etc.", "startOffset": 52, "endOffset": 56}, {"referenceID": 54, "context": "[55], fuzzy neural networks [32], transfer learning [49], [56], active learning [57], etc.", "startOffset": 58, "endOffset": 62}, {"referenceID": 55, "context": "[55], fuzzy neural networks [32], transfer learning [49], [56], active learning [57], etc.", "startOffset": 80, "endOffset": 84}, {"referenceID": 57, "context": "To overcome the limitation pointed out by Yger, Lotte, and Sugiyama [59], i.", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": ", RG is less efficient when the dimensionality of the covariance matrix is large, we adopt an approach similar to what Barachant used in [7]: we first use a spatial filter proposed in [55] to reduce the dimensionality of the covariance matrices and also to increase the EEG signal quality, and then extract the RG features in the Riemannian tangent space.", "startOffset": 137, "endOffset": 140}, {"referenceID": 53, "context": ", RG is less efficient when the dimensionality of the covariance matrix is large, we adopt an approach similar to what Barachant used in [7]: we first use a spatial filter proposed in [55] to reduce the dimensionality of the covariance matrices and also to increase the EEG signal quality, and then extract the RG features in the Riemannian tangent space.", "startOffset": 184, "endOffset": 188}, {"referenceID": 14, "context": "We validate the performance of the proposed approach in reaction time (RT) estimation from EEG signals measured in a large-scale sustained-attention PVT [16], which collected 143 sessions of data from 17 subjects in a 5month period.", "startOffset": 153, "endOffset": 157}, {"referenceID": 53, "context": "Recently we [55] proposed two spatial filters for supervised BCI regression problems, which were extended from the common spatial pattern (CSP) algorithm for supervised classification problems.", "startOffset": 12, "endOffset": 16}, {"referenceID": 58, "context": "CSPR-OVR first constructs K fuzzy sets [60], which partition the training samples into K fuzzy classes.", "startOffset": 39, "endOffset": 43}, {"referenceID": 20, "context": "(3) is a generalized Rayleigh quotient [22], and the solution W k is the concatenation of the F eigenvectors associated with the F largest eigenvalues of the matrix ( \u2211", "startOffset": 39, "endOffset": 43}, {"referenceID": 53, "context": "Algorithm 1: The CSPR-OVR spatial filter for supervised BCI regression problems [55].", "startOffset": 80, "endOffset": 84}, {"referenceID": 18, "context": "The RG approach for BCI works on the covariance matrices of EEG trials, which are symmetric positive-definite and form a differentiable Riemannian manifold M [20] with dimensionality R(R + 1)/2, where R is the number of rows (columns) of the covariance matrices.", "startOffset": 158, "endOffset": 162}, {"referenceID": 2, "context": "It can be computed as [4], [37]:", "startOffset": 22, "endOffset": 25}, {"referenceID": 35, "context": "It can be computed as [4], [37]:", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "So, Riemannian distance computations in the manifold can be approximated by Euclidean distance computations in the tangent space [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 8, "context": "where logm(\u00b7) denotes the logarithm of a matrix [10].", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "where expm(\u00b7) denotes the exponential of a matrix [10].", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "[5],", "startOffset": 0, "endOffset": 3}, {"referenceID": 43, "context": "where the upper(\u00b7) operator keeps the upper triangular part of a symmetric matrix and vectorizes it by applying weight 1 for the diagonal elements and weight \u221a 2 for the out-of-diagonal elements [45].", "startOffset": 195, "endOffset": 199}, {"referenceID": 40, "context": "The RG mean [42], or the intrinsic mean [19], of N covariance matrices is defined as the matrix minimizing the sum of the squared Riemannian distances, i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "The RG mean [42], or the intrinsic mean [19], of N covariance matrices is defined as the matrix minimizing the sum of the squared Riemannian distances, i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "There is no closed-form expression for the RG mean, but an iterative gradient descent algorithm (see Algorithm 2 [19]) can be used to find the solution.", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "Algorithm 2: The gradient descent algorithm for computing the RG (intrinsic) mean [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "(NCTU) in Taiwan volunteered to support the data-collection efforts over a 5-month period to study EEG correlates of attention and performance changes under specific conditions of real-world fatigue [26], as determined by the percent effectiveness score of Readiband [43].", "startOffset": 199, "endOffset": 203}, {"referenceID": 41, "context": "(NCTU) in Taiwan volunteered to support the data-collection efforts over a 5-month period to study EEG correlates of attention and performance changes under specific conditions of real-world fatigue [26], as determined by the percent effectiveness score of Readiband [43].", "startOffset": 267, "endOffset": 271}, {"referenceID": 0, "context": "Data recorded by the daily sampling system included electronically-adapted visual analog scales of fatigue and stress, the Karolinska Sleepiness Scale [1], and", "startOffset": 151, "endOffset": 154}, {"referenceID": 36, "context": "the Pittsburgh Sleep Diary [38].", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "In this paper we focus on the PVT [15], which is a sustained-attention task that uses RT to measure the speed with which a subject responds to a visual stimulus.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": ") [11], [46], so it is very important to remove or suppress them to increase the signal-to-noise ratio before a machine learning algorithm is applied.", "startOffset": 2, "endOffset": 6}, {"referenceID": 44, "context": ") [11], [46], so it is very important to remove or suppress them to increase the signal-to-noise ratio before a machine learning algorithm is applied.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "This paper used the standardized early-stage EEG processing pipeline (PREP) [11], which consists of three steps: a) remove line-noise, b) determine and remove a robust reference signal, and, c) interpolate the bad channels (channels with a low recording signal-to-noise ratio).", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "Each trial was then individually filtered by a [1, 20] Hz finite impulse response band-pass filter to make each channel zero-mean and to remove nonrelevant high frequency components.", "startOffset": 47, "endOffset": 54}, {"referenceID": 18, "context": "Each trial was then individually filtered by a [1, 20] Hz finite impulse response band-pass filter to make each channel zero-mean and to remove nonrelevant high frequency components.", "startOffset": 47, "endOffset": 54}, {"referenceID": 48, "context": "We computed the average power spectral density in the Theta band (4-8 Hz) and Alpha band (8-13 Hz) for each channel using Welch\u2019s method [50], and converted these 62 \u00d7 2 = 124 band powers to dBs as our features.", "startOffset": 137, "endOffset": 141}, {"referenceID": 42, "context": "Two regression methods were used on each feature set: LASSO [44], and kNN regression [2].", "startOffset": 60, "endOffset": 64}, {"referenceID": 1, "context": "Two regression methods were used on each feature set: LASSO [44], and kNN regression [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 15, "context": "Then, non-parametric multiple comparison tests based on Dunn\u2019s procedure [17], [18] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [9].", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "Then, non-parametric multiple comparison tests based on Dunn\u2019s procedure [17], [18] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [9].", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "Then, non-parametric multiple comparison tests based on Dunn\u2019s procedure [17], [18] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [9].", "startOffset": 251, "endOffset": 254}, {"referenceID": 53, "context": "The filtering performance is robust to K but changes noticeably when F changes [55].", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "For the PVT experiment, F \u2208 [10, 15] seemed to achieve a good compromise between performance and computational cost.", "startOffset": 28, "endOffset": 36}, {"referenceID": 13, "context": "For the PVT experiment, F \u2208 [10, 15] seemed to achieve a good compromise between performance and computational cost.", "startOffset": 28, "endOffset": 36}], "year": 2017, "abstractText": "Riemannian geometry has been successfully used in many brain-computer interface (BCI) classification problems and demonstrated superior performance. In this paper, for the first time, it is applied to BCI regression problems, an important category of BCI applications. More specifically, we propose a new feature extraction approach for Electroencephalogram (EEG) based BCI regression problems: a spatial filter is first used to increase the signal quality of the EEG trials and also to reduce the dimensionality of the covariance matrices, and then Riemannian tangent space features are extracted. We validate the performance of the proposed approach in reaction time estimation from EEG signals measured in a large-scale sustained-attention psychomotor vigilance task, and show that compared with the traditional powerband features, the tangent space features can reduce the root mean square estimation error by 4.30-8.30%, and increase the estimation correlation coefficient by 6.59-11.13%.", "creator": "LaTeX with hyperref package"}}}