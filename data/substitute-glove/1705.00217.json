{"id": "1705.00217", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2017", "title": "Extending and Improving Wordnet via Unsupervised Word Embeddings", "abstract": "This even aspects it unsupervised innovative besides stronger WordNet some builds except recent struggled now document and brings certain networks non-linguistic notation. We specify keeping particular to tunnels Wordnets early French. Russian, languages called both lack better setup monumental. 13 These are conclusive on two new 1,000 - word cricket double five every - to - synset padded making found up vital greatly however synset announced, outperforming be best portable Wordnets also F - score. Our methods require very rarely linguistic creating, must being applicable next Wordnet structures having increase - maintain surnames, and for further none technique their sense clustering on three Wordnet enhancements.", "histories": [["v1", "Sat, 29 Apr 2017 17:50:02 GMT  (306kb,D)", "http://arxiv.org/abs/1705.00217v1", "17 pages, 3 figures, In Submission"]], "COMMENTS": "17 pages, 3 figures, In Submission", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["mikhail khodak", "rej risteski", "christiane fellbaum", "sanjeev arora"], "accepted": false, "id": "1705.00217"}, "pdf": {"name": "1705.00217.pdf", "metadata": {"source": "CRF", "title": "Linguistic Issues in Language Technology \u2013 LiLT", "authors": ["Mikhail Khodak", "Andrej Risteski", "Christiane Fellbaum", "Sanjeev Arora"], "emails": [], "sections": [{"heading": null, "text": "Linguistic Issues in Language Technology \u2013 LiLT Volume 10, Issue 4 Sep 2017"}, {"heading": "Extending and Improving Wordnet", "text": "via Unsupervised Word"}, {"heading": "Embeddings", "text": ""}, {"heading": "Mikhail Khodak, Andrej Risteski, Christiane", "text": "Fellbaum, Sanjeev Arora\nar X\niv :1\n70 5.\n00 21\n7v 1\n[ cs\n.C L\n] 2\n9 A\npr 2\n01 7\n(IN SUBMISSION) LiLT volume 10, issue 4 Sep 2017"}, {"heading": "Extending and Improving Wordnet via", "text": ""}, {"heading": "Unsupervised Word Embeddings", "text": ""}, {"heading": "Mikhail Khodak, Andrej Risteski, Christiane Fellbaum,", "text": "Sanjeev Arora, Princeton University\nThis work presents an unsupervised approach for improving WordNet that builds upon recent advances in document and sense representation via distributional semantics. We apply our methods to construct Wordnets in French and Russian, languages which both lack good manual constructions.1 These are evaluated on two new 600-word test sets for word-to-synset matching and found to improve greatly upon synset recall, outperforming the best automated Wordnets in F-score. Our methods require very few linguistic resources, thus being applicable for Wordnet construction in low-resources languages, and may further be applied to sense clustering and other Wordnet improvements."}, {"heading": "1.1 Introduction", "text": "Since the development of the Princeton WordNet (PWN) and its successful application to computational linguistics and information retrieval (Fellbaum, 1998), there have been many efforts to extend it to other languages and improve its synset relations and sense associations. Doing this by hand is difficult and resource-intensive, making automated methods desirable. However, these are often tailored to a specific language structure or depend heavily on resource availability, complicating application to many languages. We develop an unsuper-\n1 Available at http://nlp.cs.princeton.edu/PAWN/.\n1\n(IN SUBMISSION) LiLT Volume 10, Issue 4, Sep 2017. Extending and Improving Wordnet via Unsupervised Word Embeddings.\nvised approach for synset representation and word-sense induction and apply it to automated Wordnet construction for French and Russian. The method requires only an unannotated corpus in the target language and machine translation (MT) between that language and English.\nThe basis of our work is the use of word embeddings: representations of words as vectors, typically real and low-dimensional (Turney and Pantel, 2010). Although many vector representations of synsets have been proposed, most already depend on Wordnet, limiting their use for building it in new languages. We instead represent translated synset information directly using recent work on document representations (Arora et al., 2017). We also apply a method for linear algebraic wordsense induction (WSI) to develop a sense-clustering procedure that can be further used to improve Wordnet construction (Arora et al., 2016a).\nOur further contribution is the application of these representations to the extend approach for automated Wordnet construction (Vossen, 1998). This framework assumes that synset relations are invariant across languages and generates a set of candidate synsets for each word w in the target language by using a set of English translations of w to query PWN (we refer to this as MT+PWN). As the number of candidate synsets produced may be quite large, we need to select those synsets that are its appropriate senses. Here a simple word embedding approach is to use a cutoff on the average similarity between a word and the synset\u2019s lemmas. We find that using our synset representations improve greatly upon this baseline and outperforms other languageindependent methods as well as language-specific approaches such as WOLF, the French Wordnet used by the Natural Language ToolKit (Sagot and Fi\u030cser, 2008, Bond and Foster, 2013, Bird et al., 2009).\nA further contribution is two 600-word test sets in French and Russian that are the largest and most comprehensive available, containing 200 each of nouns, verbs, and adjectives. We construct them by presenting native speakers with all candidate synsets produced by MT+PWN and treating the senses picked as \u201cground truth\u201d for measuring accuracy. Besides its size, our data sets also have the advantage of being separated by part-of-speech (POS), making evident differences in performance across POS. With these test sets, we hope to address the difficulties in evaluating non-English Wordnets from the use of different and unreported data, incompatible metrics (e.g. matching synsets vs. retrieving synset lemmas), and differing cross-lingual dictionaries."}, {"heading": "1.2 Related Work", "text": "Much past work on automated Wordnets has focused on languagespecific approaches \u2014 using resources or properties specific to a language or language family. Efforts for Korean (Lee et al., 2000), French (Sagot and Fi\u030cser, 2008, Pradet et al., 2013), and Persian (Montazery and Faili, 2010), have found success in using bilingual corpora, expert knowledge, or Wordnets in related languages on top of an MT+PWN step. We compare to the Wordnet Libre du Franc\u0327ais (WOLF), which leverages multiple European Wordnets (Sagot and Fi\u030cser, 2008); in our evaluation an embedding method outperforms the approach in F-score while having far fewer resource requirements. Wordnet du Franc\u0327ais (WoNeF), an extension of WOLF that combined linguistic models via a voting scheme (Pradet et al., 2013), was found to have performance generally below WOLF\u2019s, so we compare to the earlier database.\nThere have also been recent vector approaches for Wordnet construction, specifically for an Arabic Wordnet (Tarouti and Kalita, 2016) and a Bengali Wordnet (Nasiruddin et al., 2014). The small size of these Wordnets (below 1000 synsets for high-F-score versions) underscores the difficulty of extracting sense information from unsupervised representations. In particular, we found that stronger sense-induction methods, specifically sparse coding, than those presented in Nasiruddin et al. (2014) were needed to distinguish word-senses well.\nAnother approach is to leverage and expand upon existing resources. Two multi-lingual Wordnets thus constructed are the Extended Open Multilingual Wordnet (OMW) Bond and Foster (2013), which scraped Wiktionary, and the Universal Multilingual Wordnet (UWN) (de Melo and Weikum, 2009), which used multiple translations to match wordsenses. Through evaluation we found that the approach leads to high precision/low recall Wordnets. This method is also used for BabelNet, which extends Wordnet and Wikipedia (Navigli and Ponzetto, 2012).\nExisting ontologies are also frequently used for sense representations; these include efforts using Wordnet (Rothe and Schu\u0308tze, 2015) and BabelNet (Iacobacci et al., 2015). The approach often uses unsupervised embeddings for initialization and attains state-of-the-art on standard NLP tasks (Camacho-Collados et al., 2016). However, such representations depend on existing ontologies and so are difficult to apply to Wordnet construction. We instead use unsupervised embeddings, shown empirically (Mikolov et al., 2013) and under a generative model (Arora et al., 2016b) to recover word-similarity and analogies from word-cooccurrences. We use the latter paper\u2019s Squared-Norm (SN) vectors, which are similar in form to GloVe (Pennington et al., 2014)."}, {"heading": "1.3 Distributed Synset Representation", "text": ""}, {"heading": "MT + PWN Synset", "text": "We introduce an unsupervised method for representing PWN synsets in non-English languages needing only a large corpus and machine translation. For a vocabulary V of target language words with ddimensional2 unit vectors vw \u2208 Rd, the representation of a synset S will also be a vector uS \u2208 Rd. The construction of uS will be motivated by the following score-threshold procedure, illustrated in Figure 1, for automated Wordnet construction. Given a target word w, we use a bilingual dictionary to get its translations in English and let its set of candidate synsets be all PWN senses of the translations (MT+PWN). We then assign a score uS \u00b7vw to each S and accept as correct all synsets with score above a threshold \u03b1; if no synset is above the cutoff we accept only the highest-scoring synset. Thus we want synset representations uS whose inner product with vw is high if S is a matching synset of w and low otherwise. We present a simple baseline representation and then a more involved approach using embeddings of glosses."}, {"heading": "1.3.1 Baseline: Average Embedding", "text": "Given a candidate synset S, define TS \u2282 V as the set of translations of its lemmas from English to the target language. Then represent S as\nuS = 1 |TS | \u2211 w\u2208TS vw\nIn this case the synset score in the score-threshold procedure is equivalent to the average cosine similarity between w and the translations of the lemmas of S. Although straightforward, this representation is quite noisy and does not use all synset information provided by PWN.\n2 d |V |, e.g. d = 300 for vocabulary size |V | = 50000."}, {"heading": "1.3.2 Synset Representation Method", "text": "We now add synset relation and gloss information into the representation uS . Recalling the set TS of translations of lemmas of synset S, define RS to be the union over all synsets S\n\u2032 related to S of lemmatranslation sets TS\u2032 . Then the lemma embedding and related-synset embedding of S are (before normalization) the element-wise sums\nv (SUM) TS = \u2211 w\u2208TS vw and v (SUM) RS = \u2211 w\u2208RS vw\nWhile both gloss translations and the translated lemmas have mistakes from translation noise and polysemy, glosses also have irrelevant words (both stopwords and otherwise). As we would like to downweight these, we use the sentence embedding formula of Arora et al. (2017), a smooth inverse frequency (SIF) weighted average. Given a list L of words w \u2208 V with corpus frequency fw, the SIF-embedding is\nv (SIF ) L = \u2211 w\u2208L a a+ fw vw\nwhere a is a parameter (commonly set to 10\u22124). Note that weight aa+fw is low for high frequency words and so is similar to TF-IDF (Salton and Buckley, 1988). Through experiments with word-synset matching data, we found that simple sums work for representing the lemmas and related synsets of S but SIF-embeddings are better for gloss representations. Defining DS to be the translated synset definition of S and ES to be the set of translated example sentences of S, we set the definition embedding of S to be v\u0302 (SIF ) DS and the example-sentence embedding to be\n1 |ES | \u2211 E\u2208ES v\u0302 (SIF ) E\nThe representation uS of synset S is then an average of all four (lemma, related-synset, definition, example-sentence3) of these embeddings."}, {"heading": "1.4 Cluster-Based Sense Representation", "text": "The representations above work well for automated Wordnets but make no use of the polysemous structure found to be encoded in embeddings themselves by Arora et al. (2016a). Here we describe their Linear WordSense Induction (Linear-WSI) model and introduce a sense purification procedure to represent each induced sense as a word-cluster. Finally, we discuss an application to PWN sense clustering. We again assume a vocabulary V with each word w represented by a unit vector vw \u2208 Rd. 3 If S has no example sentences this is not included in the average."}, {"heading": "1.4.1 Summary of Linear-WSI Model", "text": "Arora et al. (2016a) posit that a vector of a word can be linearly decomposed into vectors associated to its senses. Thus w = tie \u2014 which can be an article of clothing, a drawn match, and so on \u2014 would be vw \u2248 avw-clothing + bvw-match + . . . for a, b \u2208 R. Learning such finegrained sense-vectors is difficult4, but one expects some words to have related sense-vectors, e.g. the vector vtie-clothing would be close to the vector vbow-clothing. Thus Linear-WSI hypothesizes that for k > d there exist unit basis vectors, or atoms, a1, . . . , ak \u2208 Rd such that \u2200 w \u2208 V\nvw = k\u2211 i=1 Rw,iai + \u03b7w (1.1)\nwhere \u03b7w is a noise vector and at most s coefficientsRw,i are nonzero. vw is thus modeled as a sparse linear combination of s vectors ai, with the hope that the sense-vectors vtie-clothing and vbow-clothing are both close to a clothing-related atom ai. (1.1) is signals problem, sparse coding, that can be approximated by K-SVD (Aharon et al., 2006). For k = 2000 and s = 5 Arora et al. (2016a) report that that the solution represents English word-senses as well as a competent non-native speaker and significantly better than clustering methods for WSI."}, {"heading": "1.4.2 Sense Purification", "text": "Though effective for WSI, the model produces comparatively few senses ai relative to the total number of synsets in WordNet; indeed, if k is set to be more than a few thousand the senses become repetitive. For finer-grained representations we develop a sense purification procedure that views each sense as a pair (w, ai), where ai is a sense-vector s.t. Rw,i > 0, and represents it as a cluster of words C \u2282 V .\nFor each word-sense pair (w, ai), sense purification finds a cluster C of words whose embeddings are close to each other, to vw, and to ai. The hope is that these words are used in contexts of w in which the sense used is ai. Explicitly, given a word w, one of its senses ai, and a fixed set-size n, we find C as the arg max of:\nmaximize C\u2282V \u2032,C3w,|C|=n\n\u03b3\nsubject to \u03b3 \u2264 Median{vx \u00b7 vw\u2032 : w\u2032 \u2208 C\\{x}} \u2200 x \u2208 C \u03b3 \u2264 Median{ai \u00b7 vw\u2032 : w\u2032 \u2208 C} (1.2)\nThe constraints on the objective ensure that in order to maximize it the words w\u2032 \u2208 C must have high average cosine similarity with each\n4 Indeed this is a standard criticism of unsupervised approaches to WSI.\nother, with w, and with ai. For computational purposes we find C approximately using a greedy algorithm that starts with C = {w} and repeatedly adds to it the word w \u2208 V \\C that results in the highest objective value \u03b3 of the new cluster. Processing time is further reduced by restricting our search-space to be a subset of words in V whose embeddings have cosine similarity of at least .2 with vw and ai.\nA depiction of the senses recovered via sense-purification is shown in Figure 2. Despite the difficulty of recovering small sense distinctions by distributional algorithms (partly due to Zipf\u2019s Law holding for wordsenses), the algorithm is still able to distinguish very fine difference such as TV station Fox News vs. film corporation 20th Century Fox."}, {"heading": "1.4.3 Synset Clusters and Sense Clustering", "text": "Thus far our work with word-senses has been entirely unsupervised, based only upon the polysemous structure of word embeddings. We now consider an application of Linear-WSI and sense purification to the problem of sense-clustering \u2014 reducing the granularity of Wordnet\u2019s sense distinctions by merging closely related senses of different words, This is a well-studied but difficult problem in NLP that is useful for applications requiring a much coarser set of senses for each word than that provided by PWN (Agirre and Lacalle, 2003, Snow et al., 2007).\nTo define our approach, we first specify a cluster similarity metric and a method for finding synset-atoms/synset-clusters for each wordsynset pair w, S using the atoms in the sparse representation of vw. This similarity condition (1.3) and the synset-atom/synset-cluster pairs (aS , CS) will also be useful in improving Wordnet construction performance in the next section.\nFirst, we take any two word-clusters C1, C2 \u2282 V and define a cluster similarity function\n\u03c1(C1, C2) = Median{vx \u00b7 vy : x \u2208 C1, y \u2208 C2} We then declare C1 and C2 to be similar if \u03c1(C1, C2) \u2265 min{\u03c1(C1, C1), \u03c1(C2, C2)} (1.3) i.e. if their cluster similarity with each other exceeds either one\u2019s cluster similarity with itself. Next, given a synset S we define the set VS \u2282 V to be the union of all sets of translated lemmas of synsets related to S. Then for any word-synset pair w, S we let their synset-atom be the sense ai from all ai s.t. Rw,i > 0 for which sense-purification using V\n\u2032 = VS as the search-space produces the synset-cluster CS with maximal objective value (see Equation 1.2). This can be done by running purification on each atom and choosing the best resulting cluster.\nAs formalized in Algorithm 1, the sense-clustering algorithm merges synsets that share a sense ai in the sparse representation of vw and whose clusters share similar words. Here the atoms ai s.t. Rw,i > 0 represent the coarse set of senses of w and each synset S of w is assumed to be related to one of them; therefore merging synsets sharing an atom clusters those synsets together.\nAlgorithm 1: Sense Clustering\nData: w \u2208 V , its PWN synsets S, and atoms ai s.t. Rw,i > 0 for candidate synset pairs (S, S\u2032) \u2208 S \u00d7 S do\ncompute synset-atoms aS , aS\u2032 and synset-clusters CS , CS\u2032 if aS = aS\u2032 and CS , CS\u2032 are similar (1.3) then merge the senses of w associated with synsets S and S\u2032"}, {"heading": "1.5 Methods for Automated Wordnet Construction", "text": "Our basis for automated Wordnet construction is the score-threshold procedure described in Section 1.3, where a candidate synset S is matched to a word w if uS \u00b7 vw \u2265 \u03b1 for synset representation uS and a threshold \u03b1. The representation described in Section 1.3.2 performs well compared to previous methods and our baseline; however, through examination we identified two cases in which the method performs poorly:\n1. w has no candidate synset S with score uS \u00b7 vw that clears the score-threshold \u03b1.\n2. w has multiple closely related synsets that are all correct matches but some have a much lower score than the others.\nIn this section we discuss how to improve performance in these cases by addressing a cause of noise in representing synset S in the target language \u2014 that due to polysemy many translated lemmas of S and related synsets are irrelevant. As seen before, sense-purification addresses a similar problem of Linear-WSI \u2014 that each sense ai has too many related words \u2014 by extracting a cluster of words related to both w and ai. Thus synset clusters produced via purification as in Section 1.4.3 may also lead to more useful representations of synsets than simply uS .\nPreviously, given a word w \u2208 V we constructed a synset cluster CS and associated sense aS by using VS , the union of the sets of all lemmas of synsets related to S, as the search-space V \u2032 in sense-purification. Since we now want synset clusters in the target vocabulary, we simply replace VS its translations. Then for each candidate synset S of w we obtain an associated sense aS and cluster CS as in Section 1.4.3."}, {"heading": "1.5.1 A Better Threshold Using the Purification Objective", "text": "The first failure case of the score-threshold procedure \u2014 no candidate synset scores above the cutoff \u03b1 \u2014 often occurs when synsets have little information in their glosses. Letting f(C) : 2V 7\u2192 [0, 1] be the objective function in Equation 1.2, the synset-clusters CS obtained as above allow f(CS) to be used as another measure of relevance of S with w, as an incorrect candidate synset S likely has fewer translated related lemmas sharing a context with w to put in the search-space VS for sense-purification and thus a lower objective value.\nTo exploit this, define S\u2217 = arg max f(CS) as the synset whose cluster has maximal objective value. Then replace \u03b1 by a new cutoff \u03b1w = min{\u03b1, uS\u2217 \u00b7vw} and match all candidate synsets S with score uS \u00b7vw \u2265 \u03b1w. This ensures that if no synset\u2019s score is above \u03b1, the synset S\u2217 with the best synset-cluster is matched to w and, if it is polysemous, so are any candidate synsets S of w with score uS \u00b7 vw \u2265 uS\u2217 \u00b7 vw."}, {"heading": "1.5.2 Recovering Similar Synsets Using Synset Clusters", "text": "The second failure case of the score-threshold procedure \u2014 many similar candidate synsets of w are correct but some have scores below the cutoff \u2014 occurs for words with fine sense distinctions. Thus we address the issue similarly to the sense clustering algorithm in Section 1.4.3.\nAlgorithm 2: Synset Recovery\nData: w \u2208 V , its PWN synsets S, and atoms ai s.t. Rw,i > 0 \u2200 S \u2208 S compute a synset-atom aS and a synset-cluster CS for each atom ai do\nlet Mi \u2282 S be candidates S s.t. aS = ai and score uS \u00b7 vw \u2265 \u03b1w for each S s.t. aS = ai and \u03b2 \u2264 uS \u00b7 vw < \u03b1w do\nif CS and CS\u2032 are similar (1.3) \u2200 S\u2032 \u2208Mi then match synset S to word w\nGiven a word w, first run the score-threshold procedure with modified cutoff \u03b1w. Then for a fixed low cutoff \u03b2 \u2264 \u03b1 run Algorithm 2, allowing a candidate S unmatched by the score-threshold procedure to be compared to \u03b2 \u2264 \u03b1 if its synset-atom aS is the same as that of a matched synset S\u2032 and their clusters CS , CS\u2032 are similar. This exploits the fact that similar synsets are likely associated with the same sense ai of w and have similar words from which to construct their synsetclusters. The final improvement of the score-threshold method with a w-dependent objective \u03b1w and sense-recovery is outlined in Figure 3."}, {"heading": "1.6 Evaluation of Automated Wordnet Construction", "text": "We evaluate our methods by constructing automated French and Russian Wordnets. For word embeddings we train 300-dimensional SN vectors on |V | \u2248 50000 words, restricted to those with 1000 occurrences or having candidate PWN synsets and 100 occurrences in the lemmatized Wikipedia corpus (Arora et al., 2016b). We use sparsity s = 4 and basissize k = 2000 for Linear-WSI and set-size n = 5 for sense-purification. Translation dictionaries were built from Google and Microsoft Translate and the dictionary of the translation company ECTACO; Microsoft was used for the sentence-length MT needed for translating synset glosses."}, {"heading": "1.6.1 Test Sets for Word-Synset Matching", "text": "A natural method of evaluation is by using a manually constructed Wordnet as a source of \u201cground truth\u201d senses. However, the ELRA French Wordnet5 is private while Russian Wordnets are too small and unlinked with PWN6 or gotten by direct translation from PWN7.\nWe instead construct and release test sets for each language by randomly choosing 200 each of adjectives, nouns, and verbs from words whose English translations appear in the synsets of the Core WordNet, a semi-automatically selected set of about 5000 most-used synsets in PWN (Fellbaum, 1998). Choosing words from Core synset lemmas makes the evaluation more difficult since common words are more polysemous, with more synsets to retrieve; this is reflected in the lower performance of WOLF relative to (Sagot and Fi\u030cser, 2008, Table 4).\n5 http://catalog.elra.info/product_info.php?products_id=550 6 http://project.phil.spbu.ru/RussNet/ 7 http://wordnet.ru/\nThe \u201cground truth\u201d senses are picked by native speakers asked to match synsets to a word given a set of candidates synsets generated by MT+PWN. For example, the French word foie has one translation, liver, with four PWN synsets: 1-\u201cglandular organ\u201d; 2-\u201cliver used as meat\u201d; 3-\u201cperson with a special life style\u201d; 4-\u201csomeone living in a place.\u201d Only the first two align with senses of foie, so the expert marks the first two as good and the others as negative.\nTwo native speakers of each language were trained by a conversant author with knowledge of WordNet; the latter also resolved discrepancies. We get 600 words and \u223c 12000 candidate word-synset pairs in each language, with adjectives and nouns having on average about 15 candidates and verbs having about 30. This comprises a very large data set compared to previous efforts. Accuracy compared to this ground truth estimates how well an algorithm does compared to humans.\nOne property of this test set is its dependence on the translation we use to get candidate synsets, which can leave out correct synset matches if they are not in the bilingual dictionaries. However, providing both correct and incorrect candidates allows future work to focus on selecting senses and not worry about finding the best dictionary. This dictionary-independent evaluation is an important feature since translation systems used by many authors are often not provided in full. When comparing our performance to previous work, we do not penalize word-synset matches in which the synset is not among the candidates generated for that word, reducing the loss of precision incurred by other methods due to the use of different dictionaries. We also do not penalize other Wordnets for test words they do not contain.\nIn addition to precision and recall, we report coverage as the proportion of synsets in the Core WordNet that are matched to. While an imperfect metric given different sense usage by language, the synsets are universal-enough for it to be a good indicator of usability."}, {"heading": "1.6.2 Experimental Results", "text": "We report results in Tables 2 & 3. Parameters \u03b1 and \u03b2 are tuned to maximize micro-average F.5-score 1.25\u00b7Precision\u00b7Recall .25\u00b7Precision+Recall , used instead of F1 to prioritize precision (often more important for applications). Our synset representations (Section 1.3.2) outperform the baseline by 6% in F.5-score for French and 10% for Russian; in French it is competitive with (WOLF) and in both it exceeds both multi-lingual Wordnets. Linear-WSI heuristics further improve F.5-score by 1% in Russian and 2% for French, exceeding WOLF in F.5-score across POS while having similar coverage. Notably, OMW consistently achieves best precision, although it and UWN have low recall and coverage.\nAcross POS, we do best on nouns and worst on verbs, a standard result likely exacerbated in this case due to the greater polysemy of verbs. Comparing between languages, we see slightly better performance on Russian adjectives, slightly worse performance on Russian nouns, and much worse performance on Russian verbs. The latter can be explained by a difference in treating the reflexive case and aspectual variants due to the grammatical complexity of Russian verbs. In French, making a verb reflexive requires adding a word while in Russian the verb itself changes, e.g. to wash\u2192to wash oneself is laver\u2192se laver in French but \u043c\u044b\u0442\u044c\u2192\u043c\u044b\u0442\u044c\u0441\u044f in Russian. Thus we do not distinguish them for French as the token is the same but for Russian we do, so both \u043c\u044b\u0442\u044c and \u043c\u044b\u0442\u044c\u0441\u044f may appear and have distinct synset matches. Matching Russian verbs is thus harder as the reflexive usage is often contextually similar to the non-reflexive usage. Aspectual verb pairs are another complication; for Russian, to do has aspects (\u0434\u0435\u043b\u0430\u0442\u044c, \u0441\u0434\u0435\u043b\u0430\u0442\u044c) that are treated as distinct while in French these are just tenses of faire.\nOverall the word embedding method seems robust to the language\u2019s closeness to English, with similar noun and adjective performance and a verb-performance discrepancy stemming from an intrinsic quality rather than language dissimilarity. Such a claim can be further examined by constructing Wordnets for non-European languages."}, {"heading": "1.7 Conclusion", "text": "We have introduced unsupervised synset and sense representations via word vectors that can be used to improve WordNet and extend it to other languages. These methods outperform language-specific and resource-heavy approaches, enabling the construction of automated Wordnets in low-resource languages. We also release two large POSsplit test sets for automated Wordnets for French and Russian that give a more accurate picture of a method\u2019s strengths and weaknesses. In future work these methods may be improved upon by incorporating other language representation methods such as multi-lingual embeddings (Faruqui and Dyer, 2014). Furthermore, the sense-purification procedure we introduce has direct applications to word-sense induction, clustering, and disambiguation."}, {"heading": "Arora, Sanjeev, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski.", "text": "2016a. Linear algebraic structure of word sense, with applications to polysemy."}, {"heading": "Arora, Sanjeev, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski.", "text": "2016b. Rand-walk: A latent variable model approach to word embeddings. Transactions of the Association for Computational Linguistics 4:385\u2013399.\nArora, Sanjeev, Yingyu Liang, and Tengyu Ma. 2017. A simple but toughto-beat baseline for sentence embeddings. In International Conference on Learning Representations. To Appear.\nBird, Steven, Edward Loper, and Ewan Klein. 2009. Natural Language Processing with Python. O\u2019Reilly Media Inc.\nBond, Francis and Ryan Foster. 2013. Linking and extending an open multilingual wordnet. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics."}, {"heading": "Camacho-Collados, Jose\u0301, Mohammad Taher Pilehvar, and Roberto Navigli.", "text": "2016. Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities. Artificial Intelligence .\nde Melo, Gerard and Gerhard Weikum. 2009. Towards a universal wordnet by learning from combined evidence. In Proceedings of the 18th ACM Conference on Information and Knowledge Management .\nFaruqui, Manaal and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.\nFellbaum, Christiane. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.\nIacobacci, Ignacio, Mohammad Taher Pilehvar, and Roberto Navigli. 2015. Sensembed: Learning sense embeddings for word and relational similarity. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics."}, {"heading": "Lee, Changki, Geunbae Lee, and Seo JungYun. 2000. Automated wordnet", "text": "mapping using word sense disambiguation. In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora."}, {"heading": "Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient", "text": "estimation of word representations in vector space.\nMontazery, Mortaza and Heshaam Faili. 2010. Automatic persian wordnet construction. In Proceedings of the 23rd International Conference on Computational Linguistics."}, {"heading": "Nasiruddin, Mohammad, Didier Schwab, and Andon Tchechmedjiev. 2014.", "text": "Induction de sens pour enrichir des ressources lexicales. In 21e\u0301me Traitement Automatique des Langues Naturelles."}, {"heading": "Navigli, Roberto and Simone Paolo Ponzetto. 2012. Babelnet: The automatic", "text": "construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence 193."}, {"heading": "Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014.", "text": "Glove: Global vectors for word representation. In Proceedings of Empirical Methods in Natural Language Processing ."}, {"heading": "Pradet, Quentin, Gae\u0308l de Chalendar, and Jeanne Baguenier Desormeaux.", "text": "2013. Wonef, an improved, expanded and evaluated automatic french translation of wordnet. In Proceedings of the Seventh Global Wordnet Conference.\nRothe, Sascha and Hinrich Schu\u0308tze. 2015. Autoextend: Extending word embeddings to embeddings for synsets and lexemes. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.\nSagot, Beno\u0302\u0131t and Darja Fi\u030cser. 2008. Building a free french wordnet from multilingual resources. In Proceedings of the Sixth International Language Resources and Evaluation Conference."}, {"heading": "Salton, Gerald and Christopher Buckley. 1988. Term-weighting approaches", "text": "in automatic text retrieval. Information Processing & Management 24(5)."}, {"heading": "Snow, Rion, Sushant Prakash, Daniel Jurafsky, and Andrew Y. Ng. 2007.", "text": "Learning to merge word senses. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning .\nTarouti, Feras Al and Jugal Kalita. 2016. Enhancing automatic wordnet construction using word embeddings. In Proceedings of the Workshop on Multilingual and Cross-lingual Methods in NLP .\nTurney, Peter D. and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics 37.\nVossen, Piek. 1998. EuroWordNet: a multilingual databse with lexical semantic networks for European Languages. Dordrecht, the Netherlands: Kluwer."}], "references": [{"title": "Clustering wordnet word senses", "author": ["Agirre", "Eneko", "Oier Lopez De Lacalle."], "venue": "Proceedings of Recent Advances in Natural Language Processing , pages 123\u2013130.", "citeRegEx": "Agirre et al\\.,? 2003", "shortCiteRegEx": "Agirre et al\\.", "year": 2003}, {"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["Aharon", "Michal", "Michael Elad", "Alfred Bruckstein."], "venue": "IEEE Transactions on Signal Processing 54(11).", "citeRegEx": "Aharon et al\\.,? 2006", "shortCiteRegEx": "Aharon et al\\.", "year": 2006}, {"title": "Linear algebraic structure of word sense, with applications to polysemy", "author": ["Arora", "Sanjeev", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "Rand-walk: A latent variable model approach to word embeddings", "author": ["Arora", "Sanjeev", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."], "venue": "Transactions of the Association for Computational Linguistics 4:385\u2013399.", "citeRegEx": "Arora et al\\.,? 2016b", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "A simple but toughto-beat baseline for sentence embeddings", "author": ["Arora", "Sanjeev", "Yingyu Liang", "Tengyu Ma."], "venue": "International Conference on Learning Representations. To Appear.", "citeRegEx": "Arora et al\\.,? 2017", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Natural Language Processing with Python", "author": ["Bird", "Steven", "Edward Loper", "Ewan Klein."], "venue": "O\u2019Reilly Media Inc.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Linking and extending an open multilingual wordnet", "author": ["Bond", "Francis", "Ryan Foster."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Bond et al\\.,? 2013", "shortCiteRegEx": "Bond et al\\.", "year": 2013}, {"title": "Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities", "author": ["Camacho-Collados", "Jos\u00e9", "Mohammad Taher Pilehvar", "Roberto Navigli."], "venue": "Artificial Intelligence .", "citeRegEx": "Camacho.Collados et al\\.,? 2016", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2016}, {"title": "Towards a universal wordnet by learning from combined evidence", "author": ["de Melo", "Gerard", "Gerhard Weikum"], "venue": "In Proceedings of the 18th ACM Conference on Information and Knowledge Management", "citeRegEx": "Melo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Melo et al\\.", "year": 2009}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Manaal", "Chris Dyer."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.", "citeRegEx": "Faruqui et al\\.,? 2014", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "WordNet: An Electronic Lexical Database", "author": ["Fellbaum", "Christiane."], "venue": "Cambridge, MA: MIT Press.", "citeRegEx": "Fellbaum and Christiane.,? 1998", "shortCiteRegEx": "Fellbaum and Christiane.", "year": 1998}, {"title": "Sensembed: Learning sense embeddings for word and relational similarity", "author": ["Iacobacci", "Ignacio", "Mohammad Taher Pilehvar", "Roberto Navigli."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Iacobacci et al\\.,? 2015", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2015}, {"title": "Automated wordnet mapping using word sense disambiguation", "author": ["Lee", "Changki", "Geunbae Lee", "Seo JungYun."], "venue": "Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.", "citeRegEx": "Lee et al\\.,? 2000", "shortCiteRegEx": "Lee et al\\.", "year": 2000}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Automatic persian wordnet construction", "author": ["Montazery", "Mortaza", "Heshaam Faili."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics.", "citeRegEx": "Montazery et al\\.,? 2010", "shortCiteRegEx": "Montazery et al\\.", "year": 2010}, {"title": "Induction de sens pour enrichir des ressources lexicales", "author": ["Nasiruddin", "Mohammad", "Didier Schwab", "Andon Tchechmedjiev."], "venue": "21\u00e9me Traitement Automatique des Langues Naturelles.", "citeRegEx": "Nasiruddin et al\\.,? 2014", "shortCiteRegEx": "Nasiruddin et al\\.", "year": 2014}, {"title": "Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network", "author": ["Navigli", "Roberto", "Simone Paolo Ponzetto."], "venue": "Artificial Intelligence 193.", "citeRegEx": "Navigli et al\\.,? 2012", "shortCiteRegEx": "Navigli et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of Empirical Methods in Natural Language Processing .", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Wonef, an improved, expanded and evaluated automatic french translation of wordnet", "author": ["Pradet", "Quentin", "Ga\u00ebl de Chalendar", "Jeanne Baguenier Desormeaux."], "venue": "Proceedings of the Seventh Global Wordnet Conference.", "citeRegEx": "Pradet et al\\.,? 2013", "shortCiteRegEx": "Pradet et al\\.", "year": 2013}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sascha", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Rothe et al\\.,? 2015", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "Building a free french wordnet from multilingual resources", "author": ["Sagot", "Ben\u00f4\u0131t", "Darja F\u01d0ser."], "venue": "Proceedings of the Sixth International Language Resources and Evaluation Conference.", "citeRegEx": "Sagot et al\\.,? 2008", "shortCiteRegEx": "Sagot et al\\.", "year": 2008}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Salton", "Gerald", "Christopher Buckley."], "venue": "Information Processing & Management 24(5).", "citeRegEx": "Salton et al\\.,? 1988", "shortCiteRegEx": "Salton et al\\.", "year": 1988}, {"title": "Learning to merge word senses", "author": ["Snow", "Rion", "Sushant Prakash", "Daniel Jurafsky", "Andrew Y. Ng."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning .", "citeRegEx": "Snow et al\\.,? 2007", "shortCiteRegEx": "Snow et al\\.", "year": 2007}, {"title": "Enhancing automatic wordnet construction using word embeddings", "author": ["Tarouti", "Feras Al", "Jugal Kalita."], "venue": "Proceedings of the Workshop on Multilingual and Cross-lingual Methods in NLP .", "citeRegEx": "Tarouti et al\\.,? 2016", "shortCiteRegEx": "Tarouti et al\\.", "year": 2016}, {"title": "From frequency to meaning: Vector space models of semantics 37", "author": ["Turney", "Peter D", "Patrick Pantel"], "venue": null, "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "EuroWordNet: a multilingual databse with lexical semantic networks for European Languages", "author": ["Vossen", "Piek."], "venue": "Dordrecht, the Netherlands: Kluwer.", "citeRegEx": "Vossen and Piek.,? 1998", "shortCiteRegEx": "Vossen and Piek.", "year": 1998}], "referenceMentions": [{"referenceID": 4, "context": "We instead represent translated synset information directly using recent work on document representations (Arora et al., 2017).", "startOffset": 106, "endOffset": 126}, {"referenceID": 12, "context": "Efforts for Korean (Lee et al., 2000), French (Sagot and F\u01d0ser, 2008, Pradet et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 18, "context": "Wordnet du Fran\u00e7ais (WoNeF), an extension of WOLF that combined linguistic models via a voting scheme (Pradet et al., 2013), was found to have performance generally below WOLF\u2019s, so we compare to the earlier database.", "startOffset": 102, "endOffset": 123}, {"referenceID": 15, "context": "There have also been recent vector approaches for Wordnet construction, specifically for an Arabic Wordnet (Tarouti and Kalita, 2016) and a Bengali Wordnet (Nasiruddin et al., 2014).", "startOffset": 156, "endOffset": 181}, {"referenceID": 15, "context": "There have also been recent vector approaches for Wordnet construction, specifically for an Arabic Wordnet (Tarouti and Kalita, 2016) and a Bengali Wordnet (Nasiruddin et al., 2014). The small size of these Wordnets (below 1000 synsets for high-F-score versions) underscores the difficulty of extracting sense information from unsupervised representations. In particular, we found that stronger sense-induction methods, specifically sparse coding, than those presented in Nasiruddin et al. (2014) were needed to distinguish word-senses well.", "startOffset": 157, "endOffset": 497}, {"referenceID": 11, "context": "Existing ontologies are also frequently used for sense representations; these include efforts using Wordnet (Rothe and Sch\u00fctze, 2015) and BabelNet (Iacobacci et al., 2015).", "startOffset": 147, "endOffset": 171}, {"referenceID": 7, "context": "The approach often uses unsupervised embeddings for initialization and attains state-of-the-art on standard NLP tasks (Camacho-Collados et al., 2016).", "startOffset": 118, "endOffset": 149}, {"referenceID": 13, "context": "We instead use unsupervised embeddings, shown empirically (Mikolov et al., 2013) and under a generative model (Arora et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 3, "context": ", 2013) and under a generative model (Arora et al., 2016b) to recover word-similarity and analogies from word-cooccurrences.", "startOffset": 37, "endOffset": 58}, {"referenceID": 17, "context": "We use the latter paper\u2019s Squared-Norm (SN) vectors, which are similar in form to GloVe (Pennington et al., 2014).", "startOffset": 88, "endOffset": 113}, {"referenceID": 2, "context": "As we would like to downweight these, we use the sentence embedding formula of Arora et al. (2017), a smooth inverse frequency (SIF) weighted average.", "startOffset": 79, "endOffset": 99}, {"referenceID": 2, "context": "The representations above work well for automated Wordnets but make no use of the polysemous structure found to be encoded in embeddings themselves by Arora et al. (2016a). Here we describe their Linear WordSense Induction (Linear-WSI) model and introduce a sense purification procedure to represent each induced sense as a word-cluster.", "startOffset": 151, "endOffset": 172}, {"referenceID": 1, "context": "1) is signals problem, sparse coding, that can be approximated by K-SVD (Aharon et al., 2006).", "startOffset": 72, "endOffset": 93}, {"referenceID": 1, "context": "1) is signals problem, sparse coding, that can be approximated by K-SVD (Aharon et al., 2006). For k = 2000 and s = 5 Arora et al. (2016a) report that that the solution represents English word-senses as well as a competent non-native speaker and significantly better than clustering methods for WSI.", "startOffset": 73, "endOffset": 139}, {"referenceID": 3, "context": "For word embeddings we train 300-dimensional SN vectors on |V | \u2248 50000 words, restricted to those with 1000 occurrences or having candidate PWN synsets and 100 occurrences in the lemmatized Wikipedia corpus (Arora et al., 2016b).", "startOffset": 208, "endOffset": 229}], "year": 2017, "abstractText": null, "creator": "TeX"}}}