{"id": "1705.00403", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Dependency Parsing with Dilated Iterated Graph CNNs", "abstract": "Dependency submunition different an means way to inject sociocultural knowledge into many connects tasks, and many clinics think then utilize parse execution at related. Recent advances in GPU manufactured most enabled morphological networks return ensure actual surging both the 11 all similar, though models thought fail well willingness GPUs ' ability it of parallelism. without for certificate include variables techniques. three reprieve. In direct, keep agenda Dilated Iterated Graph Convolutional Neural Networks (DIG - CNNs) made tangent - in criminalization parser, while graph co-axial renaissance that allows for affordable end - once - end GPU annotation. In feasibility leaving is English Penn TreeBank benchmark, still scene been DIG - CNNs rehearse on parred with that bringing first best somatic stations dacs.", "histories": [["v1", "Mon, 1 May 2017 02:39:00 GMT  (34kb,D)", "https://arxiv.org/abs/1705.00403v1", "Preliminary workshop draft"], ["v2", "Fri, 21 Jul 2017 21:55:04 GMT  (33kb,D)", "http://arxiv.org/abs/1705.00403v2", "2nd Workshop on Structured Prediction for Natural Language Processing (at EMNLP '17)"]], "COMMENTS": "Preliminary workshop draft", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["emma strubell", "andrew mccallum"], "accepted": false, "id": "1705.00403"}, "pdf": {"name": "1705.00403.pdf", "metadata": {"source": "CRF", "title": "Dependency Parsing with Dilated Iterated Graph CNNs", "authors": ["Emma Strubell", "Andrew McCallum"], "emails": ["mccallum}@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "By vastly accelerating and parallelizing the core numeric operations for performing inference and computing gradients in neural networks, recent developments in GPU hardware have facilitated the emergence of deep neural networks as state-ofthe-art models for many NLP tasks, such as syntactic dependency parsing. The best neural dependency parsers generally consist of two stages: First, they employ a recurrent neural network such as a bidirectional LSTM to encode each token in context; next, they compose these token representations into a parse tree. Transition based dependency parsers (Nivre, 2009; Chen and Manning, 2014; Andor et al., 2016) produce a well-formed tree by predicting and executing a series of shiftreduce actions, whereas graph-based parsers (Mc-\nDonald et al., 2005; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) generally employ attention to produce marginals over each possible edge in the graph, followed by a dynamic programming algorithm to find the most likely tree given those marginals.\nBecause of their dependency on sequential processing of the sentence, none of these architectures fully exploit the massive parallel processing capability that GPUs possess. If we wish to maximize GPU resources, graph-based dependency parsers are more desirable than their transitionbased counterparts since attention over the edgefactored graph can be parallelized across the entire sentence, unlike the transition-based parser which must sequentially predict and perform each transition. By encoding token-level representations with\nar X\niv :1\n70 5.\n00 40\n3v 2\n[ cs\n.C L\n] 2\n1 Ju\nl 2 01\n7\nan Iterated Dilated CNN (ID-CNN) (Strubell et al., 2017), we can also remove the sequential dependencies of the RNN layers. Unlike Strubell et al. (2017) who use 1-dimensional convolutions over the sentence to produce token representations, our network employs 2-dimensional convolutions over the adjacency matrix of the sentence\u2019s parse tree, modeling attention from the bottom up. By training with an objective that encourages our model to predict trees using only simple matrix operations, we additionally remove the additional computational cost of dynamic programming inference. Combining all of these ideas, we present Dilated Iterated Graph CNNs (DIG-CNNs): a combined convolutional neural network architecture and training objective for efficient, end-to-end GPU graph-based dependency parsing.\nWe demonstrate the efficacy of these models in experiments on English Penn TreeBank, in which our models perform similarly to the state-of-theart."}, {"heading": "2 Dilated Convolutions", "text": "Though common in other areas such as computer vision, 2-dimensional convolutions are rarely used in NLP since it is usually unclear how to process text as a 2-dimensional grid. However, 2- dimensional convolutional layers are a natural model for embedding the adjacency matrix of a sentence\u2019s parse.\nA 2-dimensional convolutional neural network layer transforms each input element, in our case an edge in the dependency graph, as a linear function of the width rw and height rh window of surrounding input elements (other possible edges in the dependency graph). In this work we assume square convolutional windows: rh = rw.\nDilated convolutions perform the same operation, except rather than transforming directly adjacent inputs, the convolution is defined over a wider input window by skipping over \u03b4 inputs at a time, where \u03b4 is the dilation width. A dilated convolution of width 1 is equivalent to a simple convolution. Using the same number of parameters as a simple convolution with the same radius, the \u03b4 > 1 dilated convolution incorporates broader context into the representation of a token than a simple convolution."}, {"heading": "2.1 Iterated Dilated CNNs", "text": "Stacking many dilated CNN layers can easily incorporate information from a whole sentence. For example, with a radius of 1 and 4 layers of dilated convolutions, the effective input window size for each token is width 31, which exceeds the average sentence length (23) in the Penn TreeBank corpus. However, simply increasing the depth of the CNN can cause considerable over-fitting when data is sparse relative to the growth in model parameters. To address this, we employ Iterated Dilated CNNs (ID-CNNs) (Strubell et al., 2017), which instead apply the same small stack of dilated convolutions repeatedly, each time taking the result of the last stack as input to the current iteration. Applying the parameters recurrently in this way increases the size of the window of context incorporated into each token representation while allowing the model to generalize well. Their training objective additionally computes a loss for the output of each application, encouraging parameters that allow subsequent stacks to resolve dependency violations from their predecessors."}, {"heading": "3 Dilated Iterated Graph CNNs", "text": "We describe how to extend ID-CNNs (Strubell et al., 2017) to 2-dimensional convolutions over the adjacency matrix of a sentence\u2019s parse tree, allowing us to model the parse tree through the whole network, incorporating evidence about nearby head-dependent relationships in every layer of the network, rather than modeling at the token level followed by a single layer of attention to produce head-dependent compatibilities between tokens. ID-CNNs allow us to efficiently incorporate evidence from the entire tree without sacrificing generalizability."}, {"heading": "3.1 Model architecture", "text": "Let x = [x1, . . . , xT ] be our input text1 Let y = [y1, . . . , yT ] be labels with domain size D for the edge between each token xi and its head xj . We predict the most likely y, given a conditional model P (y|x) where the tags are conditionally independent given some features for x:\nP (y|x) = T\u220f t=1 P (yt|F (x)), (1)\n1In practice, we include a dummy root token at the beginning of the sentence which serves as the head of the root. We do not predict a head for this dummy token.\nThe local conditional distributions of Eqn. (1) come from a straightforward extension of IDCNNs (Strubell et al., 2017) to 2-dimensional convolutions. This network takes as input a sequence of T vectors xt, and outputs a T \u00d7 T matrix of per-class scores hij for each pair of tokens in the sentence.\nWe denote the kth dilated convolutional layer of dilation width \u03b4 as D(k)\u03b4 . The first layer in the network transforms the input to a graph by concatenating all pairs of vectors in the sequence xi,xj and applying a 2-dimensional dilation-1 convolution D(0)1 to form an initial edge representation c (0) ij for each token pair:\ncij (0) = D (0) 1 [xi;xj] (2)\nWe denote vector concatenation with [\u00b7; \u00b7]. Next, Lc layers of dilated convolutions of exponentially increasing dilation width are applied to cij(0), folding in increasingly broader context into the embedded representation of eij at each layer. Let r() denote the ReLU activation function (Glorot et al., 2011). Beginning with ct(0) = it we define the stack of layers with the following recurrence:\ncij (k) = r ( D (k\u22121) 2Lc\u22121 ct (k\u22121) ) (3)\nand add a final dilation-1 layer to the stack:\ncij (Lc+1) = r ( D (Lc) 1 ct (Lc) )\n(4)\nWe refer to this stack of dilated convolutions as a block B(\u00b7), which has output resolution equal to its input resolution. To incorporate even broader context without over-fitting, we avoid making B deeper, and instead iteratively apply B Lb times, introducing no extra parameters. Starting with bt (1) = B (it), we define the output of block m:\nbij (m) = B ( bt (m\u22121) )\n(5)\nWe apply a simple affine transformationWo to this final representation to obtain label scores for each edge eij:\nhij (Lb) =Wobt (Lb) (6)\nWe can obtain the most likely head (and its label) for each dependent by computing the argmax over all labels for all heads for each dependent:\nht = argmax j\nhij (Lb) (7)"}, {"heading": "3.2 Training", "text": "Our main focus is to apply the DIG-CNN as feature extraction for the conditional model described in Sec. 3.1, where tags are conditionally independent given deep features, since this will enable prediction that is parallelizable across all possible edges. Here, maximum likelihood training is straightforward because the likelihood decouples into the sum of the likelihoods of independent logistic regression problems for every edge, with natural parameters given by Eqn. (6):\n1\nT T\u2211 t=1 logP (yt | ht) (8)\nWe could also use the DIG-CNN as input features for an MST parser, where the partition function and its gradient are computed using Kirchhoffs Matrix-Tree Theorem (Tutte, 1984), but our aim is to approximate inference in a treestructured graphical model using greedy inference and expressive features over the input in order to perform inference as efficiently as possible on a GPU.\nTo help bridge the gap between these two techniques, we use the training technique described in (Strubell et al., 2017). The tree-structured graphical model has preferable sample complexity and accuracy since prediction directly reasons in the space of structured outputs. Instead, we compile some of this reasoning in output space into DIG-CNN feature extraction. Instead of explicit reasoning over output labels during inference, we train the network such that each block is predictive of output labels. Subsequent blocks learn to correct dependency violations of their predecessors, refining the final sequence prediction.\nTo do so, we first define predictions of the model after each of the Lb applications of the block. Let ht(m) be the result of applying the matrix Wo from (6) to bt(m), the output of block m. We minimize the average of the losses for each application of the block:\n1\nLb Lb\u2211 k=1 1 T T\u2211 t=1 logP (yt | ht(m)). (9)\nBy rewarding accurate predictions after each application of the block, we learn a model where later blocks are used to refine initial predictions. The loss also helps reduce the vanishing gradient problem (Hochreiter, 1998) for deep architectures.\nWe apply dropout (Srivastava et al., 2014) to the raw inputs xij and to each block\u2019s output bt(m) to help prevent overfitting."}, {"heading": "4 Related work", "text": "Currently, the most accurate parser in terms of labeled and unlabeled attachment scores is the neural network graph-based dependency parser of Dozat and Manning (2017). Their parser builds token representations with a bidirectional LSTM over word embeddings, followed by head and dependent MLPs. Compatibility between heads and dependents is then scored using a biaffine model, and the highest scoring head for each dependent is selected.\nPreviously, (Chen and Manning, 2014) pioneered neural network paring with a transitionbased dependency parser which used features from a fast feed-forward neural network over word, token and label embeddings. Many improved upon this work by increasing the size of the network and using a structured training objective (Weiss et al., 2015; Andor et al., 2016). (Kiperwasser and Goldberg, 2016) were the first to present a graph-based neural network parser, employing an MLP with bidirectional LSTM inputs to score arcs and labels. (Cheng et al., 2016) propose a similar network, except with additional forward and backward encoders to allow for conditioning on previous predictions. (Kuncoro et al., 2016) take a different approach, distilling a consensus of many LSTM-based transition-based parsers into one graph-based parser. (Ma and Hovy, 2017) employ a similar model, but add a CNN over characters as an additional word representation and perform structured training using the Matrix-Tree Theorem. Hashimoto et al. (2017) train a large network which performs many NLP tasks including part-of-speech tagging, chunking, graph-based parsing, and entailment, observing benefits from multitasking with these tasks.\nDespite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a label for each token) Kim (2014); Kalchbrenner et al. (2014); Zhang et al. (2015); Toutanova et al. (2015). As described above, CNNs have also been used to encode token representations from embeddings of their characters, which similarly\nperform a pooling operation over characters. Lei et al. (2015) present a CNN variant where convolutions adaptively skip neighboring words. While the flexibility of this model is powerful, its adaptive behavior is not well-suited to GPU acceleration.\nMore recently, inspired by the success of deep dilated CNNs for image segmentation in computer vision (Yu and Koltun, 2016; Chen et al., 2015), convolutional neural networks have been employed as fast models for tagging, speech generation and machine translation. (van den Oord et al., 2016) use dilated CNNs to efficiently generate speech, and Kalchbrenner et al. (2016) describes an encoder-decoder model for machine translation which uses dilated CNNs over bytes in both the encoder and decoder. Strubell et al. (2017) first described the one-dimensional IDCNN architecture which is the basis for this work, demonstrating its success as a fast and accurate NER tagger. Gehring et al. (2017) report state-ofthe-art results and much faster training from using many CNN layers with gated activations as encoders and decoders for a sequence-to-sequence model. While our architecture is similar to the encoder architecture of these models, ours is differentiated by (1) being tailored to smaller-data regimes such as parsing via our iterated architecture and loss, and (2) employing two-dimensional convolutions to model the adjacency matrix of the parse tree. We are the first to our knowledge to use dilated convolutions for parsing, or to use twodimensional dilated convolutions for NLP."}, {"heading": "5 Experimental Results", "text": ""}, {"heading": "5.1 Data and Evaluation", "text": "We train our parser on the English Penn TreeBank on the typical data split: training on sections 2\u201321, testing on section 23 and using section 22 for development. We convert constituency trees to dependencies using the Stanford dependency framework v3.5 (de Marneffe and Manning, 2008), and use part-of-speech tags from the Stanford left3words part-of-speech tagger. As is the norm for this dataset, our evaluation excludes punctuation. Hyperparameters that resulted in the best performance on the validation set were selected via grid search. A more detailed description of optimization and data pre-processing can be found in the Appendix.\nCheng et al. (2016) 94.10 91.49 Hashimoto et al. (2017) 94.67 92.90 Dozat and Manning (2017) 95.74 94.08 DIG-CNN 93.70 91.72 DIG-CNN + Eisner 94.03 92.00"}, {"heading": "5.2 English PTB Results", "text": "We compare our models labeled and unlabeled attachment scores to the neural network graph-based dependency parsers described in Sec. 4. Without enforcing trees at test time, our model performs just under the LSTM-based parser of Kiperwasser and Goldberg (2016), and a few points lower than the state-of-the-art. When we post-process our model\u2019s outputs into trees, like all the other models in our table, our results increase to perform slightly above Kiperwasser and Goldberg (2016).\nWe believe our model\u2019s relatively poor performance compared to existing models is due to its limited incorporation of context from the entire sentence. While each bidirectional LSTM token representation observes all tokens in the sentence, our reported model observes a relatively small window, only 9 tokens. We hypothesize that this window is not sufficient for producing accurate parses. Still, we believe this is a promising architecture for graph-based parsing, and with further experimentation could meet or exceed the stateof-the-art while running faster by better leveraging GPU architecture."}, {"heading": "6 Conclusion", "text": "We present DIG-CNNs, a fast, end-to-end convolutional architecture for graph-based dependency parsing. Future work will experiment with deeper CNN architectures which incorporate broader sentence context in order to increase accuracy without sacrificing speed."}, {"heading": "Acknowledgments", "text": "We thank Patrick Verga and David Belanger for helpful discussions. This work was supported in\npart by the Center for Intelligent Information Retrieval, in part by DARPA under agreement number FA8750-13-2-0020, in part by Defense Advanced Research Agency (DARPA) contract number HR0011-15-2-0036, in part by the National Science Foundation (NSF) grant number DMR1534431, and in part by the National Science Foundation (NSF) grant number IIS-1514053. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of the 54th Annual Meeting of the Associa-", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L. Yuille."], "venue": "ICLR.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Bi-directional attention with agreement for dependency parsing", "author": ["Hao Cheng", "Hao Fang", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "EMNLP.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "The stanford typed dependencies representation", "author": ["Marie-Catherine de Marneffe", "Christopher D. Manning."], "venue": "COLING 2008 Workshop on Crossframework and Cross-domain Parser Evaluation.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "ICLR.", "citeRegEx": "Dozat and Manning.,? 2017", "shortCiteRegEx": "Dozat and Manning.", "year": 2017}, {"title": "Convolutional sequence to sequence learning", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin."], "venue": "arXiv preprint: arXiv:1705.03122 .", "citeRegEx": "Gehring et al\\.,? 2017", "shortCiteRegEx": "Gehring et al\\.", "year": 2017}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "AISTATS.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "A joint many-task model: Growing a neural network for multiple nlp tasks", "author": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher."], "venue": "arXiv preprint: arXiv:1611.01587 .", "citeRegEx": "Hashimoto et al\\.,? 2017", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2017}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter."], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 6(02):107\u2013116.", "citeRegEx": "Hochreiter.,? 1998", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu."], "venue": "arXiv preprint arXiv:1610.10099 .", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "EMNLP.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Distilling an ensemble of greedy dependency parsers into one mst parser", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith."], "venue": "EMNLP.", "citeRegEx": "Kuncoro et al\\.,? 2016", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2016}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Empirical Methods in Natural Language Processing .", "citeRegEx": "Lei et al\\.,? 2015", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Neural probabilistic model for non-projective mst parsing", "author": ["Xuezhe Ma", "Eduard Hovy."], "venue": "arXiv preprint: 1701.00874 .", "citeRegEx": "Ma and Hovy.,? 2017", "shortCiteRegEx": "Ma and Hovy.", "year": 2017}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Hajic."], "venue": "Proc. Human Language Technology Conf. and Conf. Empirical Methods Natural Language Pro-", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre."], "venue": "Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP.", "citeRegEx": "Nivre.,? 2009", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Fast and accurate sequence labeling with iterated dilated convolutions", "author": ["Emma Strubell", "Patrick Verga", "David Belanger", "Andrew McCallum."], "venue": "arXiv preprint: arXiv:1702.02098 .", "citeRegEx": "Strubell et al\\.,? 2017", "shortCiteRegEx": "Strubell et al\\.", "year": 2017}, {"title": "Representing text for joint embedding of text", "author": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": null, "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Graph theory, volume 11", "author": ["William Thomas Tutte."], "venue": "Addison-Wesley Menlo Park.", "citeRegEx": "Tutte.,? 1984", "shortCiteRegEx": "Tutte.", "year": 1984}, {"title": "Wavenet: A generative model for raw audio", "author": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu."], "venue": "arXiv preprint arXiv:1609.03499 .", "citeRegEx": "Oord et al\\.,? 2016", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Yu and Koltun.,? 2016", "shortCiteRegEx": "Yu and Koltun.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems 28 (NIPS).", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Transition based dependency parsers (Nivre, 2009; Chen and Manning, 2014; Andor et al., 2016) produce a well-formed tree by predicting and executing a series of shiftreduce actions, whereas graph-based parsers (Mc[ro ot] My dog als o like s eat ing sau sag e", "startOffset": 36, "endOffset": 93}, {"referenceID": 1, "context": "Transition based dependency parsers (Nivre, 2009; Chen and Manning, 2014; Andor et al., 2016) produce a well-formed tree by predicting and executing a series of shiftreduce actions, whereas graph-based parsers (Mc[ro ot] My dog als o like s eat ing sau sag e", "startOffset": 36, "endOffset": 93}, {"referenceID": 0, "context": "Transition based dependency parsers (Nivre, 2009; Chen and Manning, 2014; Andor et al., 2016) produce a well-formed tree by predicting and executing a series of shiftreduce actions, whereas graph-based parsers (Mc[ro ot] My dog als o like s eat ing sau sag e", "startOffset": 36, "endOffset": 93}, {"referenceID": 20, "context": "an Iterated Dilated CNN (ID-CNN) (Strubell et al., 2017), we can also remove the sequential dependencies of the RNN layers.", "startOffset": 33, "endOffset": 56}, {"referenceID": 20, "context": "an Iterated Dilated CNN (ID-CNN) (Strubell et al., 2017), we can also remove the sequential dependencies of the RNN layers. Unlike Strubell et al. (2017) who use 1-dimensional convolutions over the sentence to produce token representations, our network employs 2-dimensional convolutions over the adjacency matrix of the sentence\u2019s parse tree, modeling attention from the bottom up.", "startOffset": 34, "endOffset": 154}, {"referenceID": 20, "context": "To address this, we employ Iterated Dilated CNNs (ID-CNNs) (Strubell et al., 2017), which instead apply the same small stack of dilated convolutions repeatedly, each time taking the result of the last stack as input to the current iteration.", "startOffset": 59, "endOffset": 82}, {"referenceID": 20, "context": "We describe how to extend ID-CNNs (Strubell et al., 2017) to 2-dimensional convolutions over the adjacency matrix of a sentence\u2019s parse tree, allowing us to model the parse tree through the whole network, incorporating evidence about nearby head-dependent relationships in every layer of the network, rather than modeling at the token level followed by a single layer of attention to produce head-dependent compatibilities between tokens.", "startOffset": 34, "endOffset": 57}, {"referenceID": 20, "context": "(1) come from a straightforward extension of IDCNNs (Strubell et al., 2017) to 2-dimensional convolutions.", "startOffset": 52, "endOffset": 75}, {"referenceID": 7, "context": "Let r() denote the ReLU activation function (Glorot et al., 2011).", "startOffset": 44, "endOffset": 65}, {"referenceID": 22, "context": "We could also use the DIG-CNN as input features for an MST parser, where the partition function and its gradient are computed using Kirchhoffs Matrix-Tree Theorem (Tutte, 1984), but our aim is to approximate inference in a treestructured graphical model using greedy inference and expressive features over the input in order to perform inference as efficiently as possible on a GPU.", "startOffset": 163, "endOffset": 176}, {"referenceID": 20, "context": "To help bridge the gap between these two techniques, we use the training technique described in (Strubell et al., 2017).", "startOffset": 96, "endOffset": 119}, {"referenceID": 9, "context": "The loss also helps reduce the vanishing gradient problem (Hochreiter, 1998) for deep architectures.", "startOffset": 58, "endOffset": 76}, {"referenceID": 19, "context": "We apply dropout (Srivastava et al., 2014) to the raw inputs xij and to each block\u2019s output bt to help prevent overfitting.", "startOffset": 17, "endOffset": 42}, {"referenceID": 5, "context": "Currently, the most accurate parser in terms of labeled and unlabeled attachment scores is the neural network graph-based dependency parser of Dozat and Manning (2017). Their parser builds token representations with a bidirectional LSTM over word embeddings, followed by head and dependent MLPs.", "startOffset": 143, "endOffset": 168}, {"referenceID": 1, "context": "Previously, (Chen and Manning, 2014) pioneered neural network paring with a transitionbased dependency parser which used features from a fast feed-forward neural network over word, token and label embeddings.", "startOffset": 12, "endOffset": 36}, {"referenceID": 24, "context": "Many improved upon this work by increasing the size of the network and using a structured training objective (Weiss et al., 2015; Andor et al., 2016).", "startOffset": 109, "endOffset": 149}, {"referenceID": 0, "context": "Many improved upon this work by increasing the size of the network and using a structured training objective (Weiss et al., 2015; Andor et al., 2016).", "startOffset": 109, "endOffset": 149}, {"referenceID": 13, "context": "(Kiperwasser and Goldberg, 2016) were the first to present a graph-based neural network parser, employing an MLP with bidirectional LSTM inputs to score arcs and labels.", "startOffset": 0, "endOffset": 32}, {"referenceID": 3, "context": "(Cheng et al., 2016) propose a similar network, except with additional forward and backward encoders to allow for conditioning on previous predictions.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "(Kuncoro et al., 2016) take a different approach, distilling a consensus of many LSTM-based transition-based parsers into one graph-based parser.", "startOffset": 0, "endOffset": 22}, {"referenceID": 16, "context": "(Ma and Hovy, 2017) employ a similar model, but add a CNN over characters as an additional word representation and perform structured training using the Matrix-Tree Theorem.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": ", 2015; Andor et al., 2016). (Kiperwasser and Goldberg, 2016) were the first to present a graph-based neural network parser, employing an MLP with bidirectional LSTM inputs to score arcs and labels. (Cheng et al., 2016) propose a similar network, except with additional forward and backward encoders to allow for conditioning on previous predictions. (Kuncoro et al., 2016) take a different approach, distilling a consensus of many LSTM-based transition-based parsers into one graph-based parser. (Ma and Hovy, 2017) employ a similar model, but add a CNN over characters as an additional word representation and perform structured training using the Matrix-Tree Theorem. Hashimoto et al. (2017) train a large network which performs many NLP tasks including part-of-speech tagging, chunking, graph-based parsing, and entailment, observing benefits from multitasking with these tasks.", "startOffset": 8, "endOffset": 695}, {"referenceID": 10, "context": "Despite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a label for each token) Kim (2014); Kalchbrenner et al.", "startOffset": 251, "endOffset": 262}, {"referenceID": 10, "context": "Despite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a label for each token) Kim (2014); Kalchbrenner et al. (2014); Zhang et al.", "startOffset": 263, "endOffset": 290}, {"referenceID": 10, "context": "Despite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a label for each token) Kim (2014); Kalchbrenner et al. (2014); Zhang et al. (2015); Toutanova et al.", "startOffset": 263, "endOffset": 311}, {"referenceID": 10, "context": "Despite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a label for each token) Kim (2014); Kalchbrenner et al. (2014); Zhang et al. (2015); Toutanova et al. (2015). As described above, CNNs have also been used to encode token representations from embeddings of their characters, which similarly perform a pooling operation over characters.", "startOffset": 263, "endOffset": 336}, {"referenceID": 10, "context": "Despite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a label for each token) Kim (2014); Kalchbrenner et al. (2014); Zhang et al. (2015); Toutanova et al. (2015). As described above, CNNs have also been used to encode token representations from embeddings of their characters, which similarly perform a pooling operation over characters. Lei et al. (2015) present a CNN variant where convolutions adaptively skip neighboring words.", "startOffset": 263, "endOffset": 530}, {"referenceID": 25, "context": "More recently, inspired by the success of deep dilated CNNs for image segmentation in computer vision (Yu and Koltun, 2016; Chen et al., 2015), convolutional neural networks have been employed as fast models for tagging, speech generation and machine translation.", "startOffset": 102, "endOffset": 142}, {"referenceID": 2, "context": "More recently, inspired by the success of deep dilated CNNs for image segmentation in computer vision (Yu and Koltun, 2016; Chen et al., 2015), convolutional neural networks have been employed as fast models for tagging, speech generation and machine translation.", "startOffset": 102, "endOffset": 142}, {"referenceID": 2, "context": "More recently, inspired by the success of deep dilated CNNs for image segmentation in computer vision (Yu and Koltun, 2016; Chen et al., 2015), convolutional neural networks have been employed as fast models for tagging, speech generation and machine translation. (van den Oord et al., 2016) use dilated CNNs to efficiently generate speech, and Kalchbrenner et al. (2016) describes an encoder-decoder model for machine translation which uses dilated CNNs over bytes in both the encoder and decoder.", "startOffset": 124, "endOffset": 372}, {"referenceID": 2, "context": "More recently, inspired by the success of deep dilated CNNs for image segmentation in computer vision (Yu and Koltun, 2016; Chen et al., 2015), convolutional neural networks have been employed as fast models for tagging, speech generation and machine translation. (van den Oord et al., 2016) use dilated CNNs to efficiently generate speech, and Kalchbrenner et al. (2016) describes an encoder-decoder model for machine translation which uses dilated CNNs over bytes in both the encoder and decoder. Strubell et al. (2017) first described the one-dimensional IDCNN architecture which is the basis for this work, demonstrating its success as a fast and accurate NER tagger.", "startOffset": 124, "endOffset": 522}, {"referenceID": 2, "context": "More recently, inspired by the success of deep dilated CNNs for image segmentation in computer vision (Yu and Koltun, 2016; Chen et al., 2015), convolutional neural networks have been employed as fast models for tagging, speech generation and machine translation. (van den Oord et al., 2016) use dilated CNNs to efficiently generate speech, and Kalchbrenner et al. (2016) describes an encoder-decoder model for machine translation which uses dilated CNNs over bytes in both the encoder and decoder. Strubell et al. (2017) first described the one-dimensional IDCNN architecture which is the basis for this work, demonstrating its success as a fast and accurate NER tagger. Gehring et al. (2017) report state-ofthe-art results and much faster training from using many CNN layers with gated activations as encoders and decoders for a sequence-to-sequence model.", "startOffset": 124, "endOffset": 694}, {"referenceID": 3, "context": "9 Cheng et al. (2016) 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "9 Cheng et al. (2016) 94.10 91.49 Kuncoro et al. (2016) 94.", "startOffset": 2, "endOffset": 56}, {"referenceID": 3, "context": "9 Cheng et al. (2016) 94.10 91.49 Kuncoro et al. (2016) 94.3 92.1 Hashimoto et al. (2017) 94.", "startOffset": 2, "endOffset": 90}, {"referenceID": 3, "context": "9 Cheng et al. (2016) 94.10 91.49 Kuncoro et al. (2016) 94.3 92.1 Hashimoto et al. (2017) 94.67 92.90 Ma and Hovy (2017) 94.", "startOffset": 2, "endOffset": 121}, {"referenceID": 3, "context": "9 Cheng et al. (2016) 94.10 91.49 Kuncoro et al. (2016) 94.3 92.1 Hashimoto et al. (2017) 94.67 92.90 Ma and Hovy (2017) 94.9 93.0 Dozat and Manning (2017) 95.", "startOffset": 2, "endOffset": 156}, {"referenceID": 13, "context": "Without enforcing trees at test time, our model performs just under the LSTM-based parser of Kiperwasser and Goldberg (2016), and a few points lower than the state-of-the-art.", "startOffset": 93, "endOffset": 125}, {"referenceID": 13, "context": "Without enforcing trees at test time, our model performs just under the LSTM-based parser of Kiperwasser and Goldberg (2016), and a few points lower than the state-of-the-art. When we post-process our model\u2019s outputs into trees, like all the other models in our table, our results increase to perform slightly above Kiperwasser and Goldberg (2016).", "startOffset": 93, "endOffset": 348}], "year": 2017, "abstractText": "Dependency parses are an effective way to inject linguistic knowledge into many downstream tasks, and many practitioners wish to efficiently parse sentences at scale. Recent advances in GPU hardware have enabled neural networks to achieve significant gains over the previous best models, these models still fail to leverage GPUs\u2019 capability for massive parallelism due to their requirement of sequential processing of the sentence. In response, we propose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for graphbased dependency parsing, a graph convolutional architecture that allows for efficient end-to-end GPU parsing. In experiments on the English Penn TreeBank benchmark, we show that DIG-CNNs perform on par with some of the best neural network parsers.", "creator": "LaTeX with hyperref package"}}}