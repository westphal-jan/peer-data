{"id": "1705.02131", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Joint RNN Model for Argument Component Boundary Detection", "abstract": "Argument Component Boundary Detection (ACBD) takes same important sub - handling held positivism venture; it ensure saw proof the exactly real-time both constitute argument product, own is usually considered changed the first asian - operations opened of legalistic steel pipelines. Existing ACBD analytical heavily depend time mission - related knowledge, though require vast serious efforts on feature - pioneer. To tackle often inability, time exactly work, understand assess ACBD taken followed exact labeling very but finalize takes use of Recurrent Neural Network (RNN) similar purely, same ca either applied cctld specific make beadwork video rest its relative position significant the sentence at the document. In of, doing propose well memoirs sponsored RNN model they allow predict saying incarceration are argumentative use now, under usually the predicted advance to nearly thought detect the argument component redrawing. We findings our useful on which corpora from previous different incorporating; possible similar far my separate RNN system obtain the to - of - the - taught playing while unlike cnvs.", "histories": [["v1", "Fri, 5 May 2017 08:49:14 GMT  (457kb,D)", "http://arxiv.org/abs/1705.02131v1", "6 pages, 3 figures, submitted to IEEE SMC 2017"]], "COMMENTS": "6 pages, 3 figures, submitted to IEEE SMC 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["minglan li", "yang gao", "hui wen", "yang du", "haijing liu", "hao wang"], "accepted": false, "id": "1705.02131"}, "pdf": {"name": "1705.02131.pdf", "metadata": {"source": "CRF", "title": "Joint RNN Model for Argument Component Boundary Detection", "authors": ["Minglan Li", "Yang Gao", "Hui Wen", "Yang Du", "Haijing Liu", "Hao Wang"], "emails": ["wanghao}@iscas.ac.cn"], "sections": [{"heading": "1. Introduction", "text": "Argumentation mining aims at automatically extracting arguments from natural language texts [19]. An argument is a basic unit people use to persuade their audiences to accept a particular state of affairs [3], and it usually consists of one or more argument components, for example a claim and some premises offered in support of the claim. As a concrete example, consider the essay excerpt below (obtained from the essay corpus in [25]):\nExample 1: Furthermore, 1\u00a9[[putting taxpayers\u2019 money on building theaters or sports stadiums is unfair to those who cannot use them]]. That is the reason why 2\u00a9[[sectors such as medical care and education deserve more governmental support]], because 3\u00a9[[they are accessed by every individual in our society on a daily basis]].\nThe above example includes three argument components ([[ ]] give their boundaries): one claim (in bold face) and two premises (underlined). Premises 1\u00a9 and 3\u00a9 support the claim 2\u00a9. As argumentation mining reveals the discourse relations between clauses, it can be potentially used in applications like decision making, document summarising, essay scoring, etc., and thus receives growing research interests in recent years (see, e.g. [16]).\nA typical argumentation mining pipeline consists of three consecutive subtasks [24]: i) separating argument components from non-argumentative texts, ii) classifying the type (e.g. claim or premise or others) of argument components; and iii) predicting the relations (e.g. support or attack) between argument components. The first subtask is also known as argument component boundary detection (ACBD); it aims at finding the exact boundary of a consecutive token subsequence that constitutes an argument component, thus separating it from non-argumentative texts. In this work, we focus on the ACBD subtask, because ACBD\u2019s performance significantly influences downstream argumentation mining subtasks\u2019 performances, but there exist relatively little research working on ACBD.\nMost existing ACBD techniques require sophisticated hand-crafted features (e.g. syntactic, structural and lexical features) and domain-specific resources (e.g. indicator gazetteers), resulting in their poor cross-domain applicabilities. To combat these problems, in this work, we consider ACBD as a sequence labeling task at the token level and propose some novel neural network based ACBD methods, so that no domain specific or hand-crafted features beyond the relative location of sentences are used. Although neural network based approaches have been recently used in some related Natural Language Processing (NLP) tasks, such as linguistic sequence labelling [9] and named entity recognition (NER) [13], applying neural network to ACBD is challenging because the length of an argument component is much longer than that of a name/location in NER: [23] reports that an argument component includes 24.25 words in average, while a name/location usually consists of only 2 to 5 words. In fact, it has been reported in [14], [25] that separating argumentative and non-argumentative texts is often subtle even to human annotators.\nIn particular, our neural network models are designed to capture two intuitions. First, since an argument component often consists of considerable number of words, it is essential to jointly considering multiple words\u2019 labels so as to detect argument components\u2019 boundaries; hence, we propose a bidirectional Recurrent Neural Network (RNN) [4] with a Conditional Random Field (CRF) [11] layer above it, as both RNN and CRF are widely recognized as effective methods for considering contextual information. Second, we believe that if the argumentative-or-not information of each\nar X\niv :1\n70 5.\n02 13\n1v 1\n[ cs\n.C L\n] 5\nM ay\n2 01\n7\nsentence1 is provided a priori, the performance of ACBD can be substantially improved. As such, we propose a novel joint RNN model that can predict a sentence\u2019s argumentative status and use the predicted status to detect boundaries.\nThe contributions of this work are threefold: i) we present the first deep-learning based ACBD technique, so that the feature-engineering demand is greatly reduced and the technique\u2019s cross-domain applicability is significantly improved; ii) we propose a novel joint RNN model that can classify the argumentative status of sentences and separating argument components from non-argumentative texts simultaneously, which can significantly improve the performance of ACBD; and iii) we test our ACBD methods on two different text genres, and results suggest that our approach outperforms the state-of-the-art techniques in both domains."}, {"heading": "2. Related Work", "text": "In this section, we first review ACBD techniques, and then review works that apply RNN to applications related to ACBD, e.g. sequence labeling and text classification."}, {"heading": "2.1. Boundary Detection", "text": "Most existing ACBD methods consist of two consecutive subtasks: identifying argumentative sentences (i.e. sentences that include some argument components) and detecting the component boundaries [16]. Levy et al. [14] identify context-dependent claims in Wikipedia articles by using a cascade of classifiers. They first use logistic regression to identify sentences containing topic-related claims (the topic is provide a priori), and then detect the boundaries of claims and rank the candidate boundaries, so as to identify the most relevant claims for the topic. However, the importance of topic information is questionable, as Lippi and Torroni [15] achieve a similar result on the first subtask without using the topic information. Goudas et al. [5] propose a ACBD technique and test it on a corpus constructed from social media texts. They first use a variety of classifiers to perform the first subtask, and then employ a feature-rich CRF to perform the second subtask.\nBesides the two-stage model presented above, some works consider ACBD as a sequence labeling task at token level. Stab and Gurevych [25] employ a CRF model with four kinds of hand-craft features (structural, syntactic, lexical and probability features) to perform ACBD on their persuasive essay corpus. Unlike texts in Wikipedia, persuasive essays are organised by structurally and the percentage of argumentative sentences are much higher (77.1% sentences in persuasive essays include argument component). The performance of this ACBD technique (in terms of macro F1) is .867.\n1. In this work, we assume that an argument component cannot span across multiple sentences. This assumption is valid in most existing argumentation corpora, e.g. [25]."}, {"heading": "2.2. RNN on Similar Tasks", "text": "RNN techniques, especially Long Short-Term Memory (LSTM) [8], have recently been successfully applied to sequence labeling and text classification tasks in various NLP problems. Graves et al. [6] propose a bidirectional RNN for speech recognition, which takes the context on both sides of each word into account. However, in sequential labeling tasks with strong dependencies between output labels, the performance of RNN is not ideal. To tackle this problem, instead of modeling tagging decisions independently, Huang et al. [9] and Lample et al. [13] apply a sequential CRF to jointly decode labels for the whole sequence.\nRNN has also be successfully used in text classification. Lai et al. [12] propose a Recurrent Convolutional Neural Network, which augments a max-pooling layer after bidirectional RNN. The purpose of the pooling layer is to capture the most important latent semantic factors in the document. Then the softmax function is used to predict the classification distribution. Results shows the effectiveness on the text classification tasks.\nSome RNN-based techniques are developed for the spoken language understanding task, in which both text classification and sequence labeling are involved: intent detection is a classification problem, while slot filling is a sequence labeling problem. Liu and Lane [17] propose an attentionbased bidirectional RNN model to perform these two tasks simultaneously; the method achieves state-of-the-art performance on both tasks."}, {"heading": "3. Models", "text": "We consider a sentence in the document as a sequence of tokens/words and label the argument boundaries using the IOB-tagset: a word is labelled as \u201cB\u201d if it is the first token in an argument component, \u201cI\u201d if it constitutes, but not leads, an argument component, and \u201cO\u201d if it is not included in any argument components. In this section, we first review some widely used techniques for sequence labeling , then we present our joint RNN model, which can distinguish argumentative and non-argumentative sentences and use this information to detect boundaries, in Sect. 3.5."}, {"heading": "3.1. Bi-LSTM", "text": "RNN [4] is a neural architecture designed for dealing with sequential data. RNN takes as input a vector X = [xt]T1 and returns a feature vector sequence ~h = [ht]T1 at every time step.2 A primary goal of RNN is to capture longdistance dependencies. However, in many real applications, standard RNN are often biased towards their most recent inputs in the sequence [2] . LSTM [8] alleviates this problem by using a memory cell and three gates (an input gate i, a forget gate f, and an output gate o) to tradeoff between the influence of the new input state and the previous state on\n2. In this work, we let [x]T1 be the short-hand notation for vector [x1, \u00b7 \u00b7 \u00b7 , xT ], where T \u2208 N\u2217 is the length of the vector.\nthe memory cell. The computation operations of a memory cell ct and hidden states ht (of size H), at time step t, are as follows:\nit = \u03c3(Wixt + Uiht\u22121 + bi), ft = \u03c3(Wfxt + Ufht\u22121 + bf ), ot = \u03c3(Woxt + Uoht\u22121 + bo), gt = tanh(W\ngxt + Ught\u22121 + bg), ct = ft ct\u22121 + it gt, ht = ot tanh(ct),\nwhere by is the bias vector for gate y (where y can be i, f , o or g), \u03c3 is the element-wise sigmoid function, and is the element-wise multiplication operator. W \u2208 RH\u00d7d, U \u2208 RH\u00d7H and b \u2208 RH\u00d71 are the network parameters.\nThe LSTM presented above is known as single direction LSTM, because it only considers the preceding states, ignoring the states following the current state; thus, it fails to consider the \u201cfuture\u201d information. Bidirectional LSTM (Bi-LSTM) [7] is proposed to combat this problem. BiLSTM includes a forward LSTM and a backward LSTM, thus can capture both past and future information. Then the final output of Bi-LSTM is the product of concatenating the past and future context representations: ht = [\n\u2212\u2192 ht ; \u2190\u2212 ht ], where\u2212\u2192\nht and \u2190\u2212 ht are the forward and backward LSTM, resp."}, {"heading": "3.2. CRF", "text": "CRF [11] is widely used in sequence labeling tasks. For a given sequence X and its labels y, CRF gives a real-valued score as follows:\nscore(X, y) = T\u2211\nt=2\n\u03c8(yt\u22121, yt) + T\u2211\nt=1\n\u03c6(yt),\nwhere \u03c6(yt) is the unary potential for the label at position t and \u03c8(yt\u22121, yt) is the pairwise potential of labels at t and t \u2212 1. The probability of y given X can be obtained from the score:\np(y|X) = 1 Z exp(score(X, y)). (1)\nGiven a new input Xnew, the goal of CRF is find a label y\u2217 for Xnew, whose conditional probability is maximised:\ny\u2217 = argmax y ( \u2211 i log(p(y|Xnew))). (2)\nThe process for obtaining the optimal label is termed decoding. For a linear chain CRF described above that only models bigram interactions between outputs, both training (Eq. (1)) and decoding (Eq. (2)) can be solved efficiently by dynamic programming."}, {"heading": "3.3. Bi-LSTM-CRF", "text": "In sequential labeling tasks where there exist strong dependencies between neighbouring labels, the performance of Bi-LSTM is not ideal. To tackle this problem, Huang et al.\n[9] propose the Bi-LSTM-CRF method, which augments a CRF layer after the output of Bi-LSTM, so as to explicitly model the dependencies between the output labels. Fig. 1 illustrates the structure of Bi-LSTM-CRF networks.\nFor a given input sentence X = [xt]T1 , h = [ \u2212\u2192 hL; \u2190\u2212 hR] is\nthe output of Bi-LSTM, where \u2212\u2192 hL and \u2190\u2212 hR are the output of the forward and backward LSTM, resp. The connection layer C is used for connecting the structure features s and the output of Bi-LSTM, namely the feature representations h. Note that s is the relative position of the input sentence in document, and is not shown in Fig. 1. The output of C is a matrix of scores, denoted by P. P is of size T \u00d7 k, where k is the number of distinct tags, and Pij corresponds to the score of the jth tag of ith word in a sentence. The score s of a sentence X = [xt]T1 along with a path of tags y = [yi]T1 is then defined as follows:\nscore(X, y) = T\u2211\ni=0\nAyi,yi+1 + T\u2211\ni=1\nPi,yi ,\nwhere A is the transition matrix, which gives the transition scores between tags such that Ai,j is the score of a transition from the tag i to tag j. We add two special tags at the beginning and end of the sequence so that A is a squared matrix of size k+2. The conditional probability for a label sequence y given a sentence X thus can be obtained as follows:\np(y|X) = e score(X,y)\u2211\ny\u0303\u2208YX e score(X,\u0303y) ,\nwhere YX represents all possible tag sequences for a input sentence X. The network is trained by minimizing the negative log-probability of the correct tag sequence y. Dynamic programming techniques can be used to efficiently compute the transition matrix A and the optimal tag sequence y\u2217 for inference."}, {"heading": "3.4. Attention based RNN for Classification", "text": "Besides in sequence labeling, RNN is also widely used in text classification tasks. Lai et al. [12] combine the word embeddings and representation output by Bi-LSTM as the\nfeature representation for text classification, weighting each input word equally. However, as the importances of words differ, such a equal-weighting strategy fails to highlight the truly important information. The attention mechanism [21] is proposed to tackle this problem. As the name suggests, the attention mechanism computes a weight vector to measure the importance of each word and aggregates those informative words to form a sentence vector. Specifically,\nf(xt) = tanh(Waxt + ba), \u03b1t = exp(Uaf(xt))\u2211 i exp(U af(xi)) ,\na = \u2211 t \u03b1t \u00b7 xt.\nThe sentence vector a is the weighted sum of the word embeddings xt, weighted by \u03b1t. Vector a gives additional supporting information, especially the information that requires longer term dependencies; this information can hardly be fully captured by the hidden states.\nThe architecture of the RNN for classification is illustrated in Fig 2. Remind that, Bi-LSTM can capture both the past and the future context information and can convert the tokens comprising each document into a feature representation h = [ht]T1 . Max-pooling operation is used to extract maximum values over the time-step dimensions of h to obtain a sentence level representation r. Then the sentence\u2019s argumentative status is predicted by the concatenating of context feature r, weighted sentence vector a and structure feature s (relative location of the sentence):\nHc = softmax(Wc[r; a; s] + bc)\nwhere Hc is the output of softmax function, which represents the probabilities of sentence\u2019s argumentative status."}, {"heading": "3.5. Joint RNN Model", "text": "The joint model for argumentative sentence classification and sequence labeling in boundary detection is shown in Fig 3. In the proposed model, a Bi-LSTM reads the source sentence in both forward and backward directions and creates the hidden states h = [ht]T1 . For sentence argumentative classification, as we mentioned in Sect. 3.4, an attention mechanism aggregate the input words into a sentence vector a. The max-pooling operation is applied to capture the key components of the latent information. the sentence\u2019s argumentative status p is then predicted by the combination of vector a, vector r (output by max-pooling operation) and relative location feature s.\nFor sequence labeling in boundary detection, we reuse the pre-computed hidden states h of the Bi-LSTM. At each time-step, we combine each hidden state ht with the relative location feature s and the sentence\u2019s predicted argumentative status p created by the above mentioned classification operation: h \u2032\nt = [ht; s; p]. Then the Hs will be the scores matrix P described in Sec. 3.3 which will be given to the CRF layer.\nHst = tanh(Wsh \u2032 t + b s)\nThe sequence labeling operation is as same as Sect. 3.3. The network of joint model is trained to find the parameters that minimize the cross-entropy of the predicted and true argumentative status for sentence and the negative logprobability of the sentence\u2019s labels jointly."}, {"heading": "4. Experiments", "text": "We first present the argumentation corpora on which we test our techniques in Sect. 4.1, introduce our experimental settings in Sect. 4.2, and present and analyse the empirical results in Sect. 4.3."}, {"heading": "4.1. Datasets", "text": "We evaluate the neural network based ACBD techniques on two different corpora: the persuasive essays corpus [25] and the Wikipedia corpus [1]. The persuasive essay corpus has three types of argument components: major claims, claims and premises. The corpus contains 402 English essays on a variety of topics, consisting of 7116 sentences and 147271 tokens (words). The Wikipedia corpus contains 315 Wikipedia articles grouped into 33 topics, and 1392 context-dependent claims have been annotated in total. A context-dependent claim is \u201ca general, concise statement that directly supports or contests the given topic\u201d, thus claims that do not support/attack the claim are not annotated. Note that the Wikipedia corpus is very imbalanced: only 2% sentences are argumentative (i.e. contain some argument components)."}, {"heading": "4.2. Experiment Settings", "text": "On the persuasive essay corpus, in line with [25], we use precision (P), recall (R) and macro-F1 as evaluation metrics, and use the same train/test split ratio: 322 essays are used in training, and the remaining 80 essays are used in test. On the Wikipedia corpus, in line with [14], a predicted claim is considered as True Positive if and only if it precisely matches a labeled claim. For all the articles across 33 topics, we randomly select 1/33 of all the sentences to serve as the test set, and the remaining sentences are used in training. As the corpus is very imbalanced, we apply a random undersampling on the training set so ensure that the ratio between non-argumentative and argumentative sentences is 4:1.\nIn experiments on both corpora, we randomly select 10% data in the training set to serve as the validation set. Training occurs for 200 epochs. Only the models that perform best (in terms of F1) on the validation set are tested on the test set. The RNN-based methods read in texts sentence by sentence, and each sentence is represented by concatenating all its component words\u2019 embeddings. All RNN are trained using the Adam update rule [10] with initial learning rate 0.001. We let the batch size be 50 and the attention hidden size be 150. To mitigate overfitting, we apply both the dropout method [22] and L2 weight regularization. We let dropout rate be 0.5 throughout our experiments for all dropout layers on both the input and the output vectors of Bi-LSTM. The regularization weight of the parameters is 0.001. Some hyper-parameter settings of the RNNs may depend on the dataset being used. In experiments on persuasive essays, we use Google\u2019s word2vec [18] 300-dimensional as the pretrained word embeddings, and set the hidden size to 150 and use only one hidden layer in LSTM; on Wikipedia articles, we use glove\u2019s 300-dimensional embeddings [20], and let the hidden size of LSTM be 80."}, {"heading": "4.3. Results and Discussion", "text": "The performance of different methods on the persuasive essays are presented in Table 1. Note that the performance of\nCRF is obtained from [25]. Bi-LSTM achieves .825 macro F1 thanks to the context information captured by the LSTM layer. Adding a CRF layer to Bi-LSTM can significantly improve the performance and can achieve comparable results with the CRF method that uses a number of handcrafted features. The third row in Table 1 gives the performance of Bi-LSTM-CRF with ground-truth argumentativeor-not information for each sentence, i.e. the feature p in Figure 3 are ground-truth labels; surprisingly, this method even outperforms the \u201chuman upperbound\u201d performance3 reported in [25], validating our assumption that the sentences\u2019 argumentative-or-not information is helpful for the ACBD task. This is validated again by the outperformance of our joint model against Bi-LSTM-CRF (row 5 and 6 in Table 1). Note that, among all methods that do not use the ground-truth argumentative-or-not information, our joint model achieves the highest performance.\nThe performances on the Wikipedia articles are presented in Table 2 and Table 3. The upper part of these two tables give the performances of some existing ACBD methods, and we can see that the performance metrics used for existing methods and for our RNN-based methods are different: our RNN-based methods output a unique boundary and component type for the input sentence, thus the performance metric is P/R/F1; however, existing ACBD methods produce a ranked list of candidate argument component boundries, thus their performance metrics are, e.g., precision@200, i.e. the probability that the true boundary is included in\n3. The human upperbound performance is obtained by by averaging the evaluation scores of all three annotator pairs on test data. Note that sentences\u2019 argumentative-or-not information are not used in obtaining the human upperbound performance.\nthe top 200 predicted boundaries (definitions of recall@200 and F1@200 can be obtained similarly). Also note that, the results reported in [14] are obtained from a slightly older version of the dataset, containing only 32 topics (instead of 33) and 976 claims (instead of 1332).\nFrom Table 2, we find that for argumentative sentence classification, our joint model significantly outperforms all the other techniques. From Table 3, we find that the joint RNN model prevails over the other Bi-LSTM based models, again confirms that the argumentative-or-not information can further improve the boundary detection performance. Note that, performances on Wikipedia corpus are not that high in general. One of the reasons is that the length of the argument component is long and the performance metrics we use are strict. In addition, only topic-dependent claims are annotated in the Wikipedia corpus; our RNN-based approaches do not consider the topic information, thus identify some topicirrelevant claims, which are treated as False Positive. Similar observations are also made in [15]."}, {"heading": "5. Conclusion", "text": "In this work, we present the first deep-learning based family of algorithms for the argument component boundary detection (ACBD) task. In particular, we propose a novel joint model that combines an attention-based classification RNN to predict the argumentative-or-not information and a Bi-LSTM-CRF network to identify the exact boundary. We empirically compare the joint model with Bi-LSTM, BiLSTM-CRF and some state-of-the-art ACBD methods on two benchmark corpora; results suggest that our joint model outperforms all the other methods, suggesting that our joint model can effectively use the argumentative-or-not information to improve the boundary detection performance. As for the future work, a natural next step is to apply deep learning techniques to other sub-tasks of argumentation mining; in addition, a deep-learning-based end-to-end argumentation mining tool is also worthy of further investigation."}], "references": [{"title": "A benchmark dataset for automatic detection of claims and evidence in the context of controversial topics", "author": ["Ehud Aharoni", "Anatoly Polnarov", "Tamar Lavee", "Daniel Hershcovich", "Ran Levy", "Ruty Rinott", "Dan Gutfreund", "Noam Slonim"], "venue": "In Proceedings of the First Workshop on Argumentation Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning longterm dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "On the role of discourse markers for discriminating claims and premises in argumentative discourse", "author": ["Judith Eckle-Kohler", "Roland Kluge", "Iryna Gurevych"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler"], "venue": "In Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Argument extraction from news, blogs, and social media", "author": ["Theodosis Goudas", "Christos Louizos", "Georgios Petasis", "Vangelis Karkaletsis"], "venue": "In Hellenic Conference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the eighteenth international conference on machine learning, ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "arXiv preprint arXiv:1603.01360,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Context dependent claim detection", "author": ["Ran Levy", "Yonatan Bilu", "Daniel Hershcovich", "Ehud Aharoni", "Noam Slonim"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Context-independent claim detection for argument mining", "author": ["Marco Lippi", "Paolo Torroni"], "venue": "In IJCAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Argumentation mining: State of the art and emerging trends", "author": ["Marco Lippi", "Paolo Torroni"], "venue": "ACM Transactions on Internet Technology (TOIT),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Attention-based recurrent neural network models for joint intent detection and slot filling", "author": ["Bing Liu", "Ian Lane"], "venue": "arXiv preprint arXiv:1609.01454,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Argumentation mining: Where are we now, where do we want to be and how do we get there", "author": ["Marie-Francine Moens"], "venue": "In Post- Proceedings of the 4th and 5th Workshops of the Forum for Information Retrieval Evaluation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1929}, {"title": "Annotating argument components and relations in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych"], "venue": "In COLING,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Identifying argumentative discourse structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych"], "venue": "In EMNLP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Parsing argumentation structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych"], "venue": "arXiv preprint arXiv:1604.07370,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Argumentation mining aims at automatically extracting arguments from natural language texts [19].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "An argument is a basic unit people use to persuade their audiences to accept a particular state of affairs [3], and it usually consists of one or more argument components, for example a claim and some premises offered in support of the claim.", "startOffset": 107, "endOffset": 110}, {"referenceID": 24, "context": "As a concrete example, consider the essay excerpt below (obtained from the essay corpus in [25]):", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "[16]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "A typical argumentation mining pipeline consists of three consecutive subtasks [24]: i) separating argument components from non-argumentative texts, ii) classifying the type (e.", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "Although neural network based approaches have been recently used in some related Natural Language Processing (NLP) tasks, such as linguistic sequence labelling [9] and named entity recognition (NER) [13], applying neural network to ACBD is challenging because the length of an argument component is much longer than that of a name/location in NER: [23] reports that an argument component includes 24.", "startOffset": 160, "endOffset": 163}, {"referenceID": 12, "context": "Although neural network based approaches have been recently used in some related Natural Language Processing (NLP) tasks, such as linguistic sequence labelling [9] and named entity recognition (NER) [13], applying neural network to ACBD is challenging because the length of an argument component is much longer than that of a name/location in NER: [23] reports that an argument component includes 24.", "startOffset": 199, "endOffset": 203}, {"referenceID": 22, "context": "Although neural network based approaches have been recently used in some related Natural Language Processing (NLP) tasks, such as linguistic sequence labelling [9] and named entity recognition (NER) [13], applying neural network to ACBD is challenging because the length of an argument component is much longer than that of a name/location in NER: [23] reports that an argument component includes 24.", "startOffset": 348, "endOffset": 352}, {"referenceID": 13, "context": "In fact, it has been reported in [14], [25] that separating argumentative and non-argumentative texts is often subtle even to human annotators.", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "In fact, it has been reported in [14], [25] that separating argumentative and non-argumentative texts is often subtle even to human annotators.", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "First, since an argument component often consists of considerable number of words, it is essential to jointly considering multiple words\u2019 labels so as to detect argument components\u2019 boundaries; hence, we propose a bidirectional Recurrent Neural Network (RNN) [4] with a Conditional Random Field (CRF) [11] layer above it, as both RNN and CRF are widely recognized as effective methods for considering contextual information.", "startOffset": 259, "endOffset": 262}, {"referenceID": 10, "context": "First, since an argument component often consists of considerable number of words, it is essential to jointly considering multiple words\u2019 labels so as to detect argument components\u2019 boundaries; hence, we propose a bidirectional Recurrent Neural Network (RNN) [4] with a Conditional Random Field (CRF) [11] layer above it, as both RNN and CRF are widely recognized as effective methods for considering contextual information.", "startOffset": 301, "endOffset": 305}, {"referenceID": 15, "context": "sentences that include some argument components) and detecting the component boundaries [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "[14] identify context-dependent claims in Wikipedia articles by using a cascade of classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "However, the importance of topic information is questionable, as Lippi and Torroni [15] achieve a similar result on the first subtask without using the topic information.", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "[5] propose a ACBD technique and test it on a corpus constructed from social media texts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Stab and Gurevych [25] employ a CRF model with four kinds of hand-craft features (structural, syntactic, lexical and probability features) to perform ACBD on their persuasive essay corpus.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "RNN techniques, especially Long Short-Term Memory (LSTM) [8], have recently been successfully applied to sequence labeling and text classification tasks in various NLP problems.", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "[6] propose a bidirectional RNN for speech recognition, which takes the context on both sides of each word into account.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] and Lample et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] apply a sequential CRF to jointly decode labels for the whole sequence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] propose a Recurrent Convolutional Neural Network, which augments a max-pooling layer after bidirectional RNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Liu and Lane [17] propose an attentionbased bidirectional RNN model to perform these two tasks simultaneously; the method achieves state-of-the-art performance on both tasks.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "RNN [4] is a neural architecture designed for dealing with sequential data.", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "However, in many real applications, standard RNN are often biased towards their most recent inputs in the sequence [2] .", "startOffset": 115, "endOffset": 118}, {"referenceID": 7, "context": "LSTM [8] alleviates this problem by using a memory cell and three gates (an input gate i, a forget gate f, and an output gate o) to tradeoff between the influence of the new input state and the previous state on", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "Bidirectional LSTM (Bi-LSTM) [7] is proposed to combat this problem.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "CRF [11] is widely used in sequence labeling tasks.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "[9] propose the Bi-LSTM-CRF method, which augments a CRF layer after the output of Bi-LSTM, so as to explicitly model the dependencies between the output labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] combine the word embeddings and representation output by Bi-LSTM as the", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The attention mechanism [21] is proposed to tackle this problem.", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "We evaluate the neural network based ACBD techniques on two different corpora: the persuasive essays corpus [25] and the Wikipedia corpus [1].", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "We evaluate the neural network based ACBD techniques on two different corpora: the persuasive essays corpus [25] and the Wikipedia corpus [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 24, "context": "On the persuasive essay corpus, in line with [25], we use precision (P), recall (R) and macro-F1 as evaluation metrics, and use the same train/test split ratio: 322 essays are used in training, and the remaining 80 essays are used in test.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "On the Wikipedia corpus, in line with [14], a predicted claim is considered as True Positive if and only if it precisely matches a labeled claim.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "All RNN are trained using the Adam update rule [10] with initial learning rate 0.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "To mitigate overfitting, we apply both the dropout method [22] and L2 weight regularization.", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "In experiments on persuasive essays, we use Google\u2019s word2vec [18] 300-dimensional as the pretrained word embeddings, and set the hidden size to 150 and use only one hidden layer in LSTM; on Wikipedia articles, we use glove\u2019s 300-dimensional embeddings [20], and let the hidden size of LSTM be 80.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "In experiments on persuasive essays, we use Google\u2019s word2vec [18] 300-dimensional as the pretrained word embeddings, and set the hidden size to 150 and use only one hidden layer in LSTM; on Wikipedia articles, we use glove\u2019s 300-dimensional embeddings [20], and let the hidden size of LSTM be 80.", "startOffset": 253, "endOffset": 257}, {"referenceID": 24, "context": "[25]) 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "16 TK [15] 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "168 TK + Topic [15] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "[14] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "CRF is obtained from [25].", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "the feature p in Figure 3 are ground-truth labels; surprisingly, this method even outperforms the \u201chuman upperbound\u201d performance3 reported in [25], validating our assumption that the sentences\u2019 argumentative-or-not information is helpful for the ACBD task.", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "Also note that, the results reported in [14] are obtained from a slightly older version of the dataset, containing only 32 topics (instead of 33) and 976 claims (instead of 1332).", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "Similar observations are also made in [15].", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "Argument Component Boundary Detection (ACBD) is an important sub-task in argumentation mining; it aims at identifying the word sequences that constitute argument components, and is usually considered as the first sub-task in the argumentation mining pipeline. Existing ACBD methods heavily depend on task-specific knowledge, and require considerable human efforts on feature-engineering. To tackle these problems, in this work, we formulate ACBD as a sequence labeling problem and propose a variety of Recurrent Neural Network (RNN) based methods, which do not use domain specific or handcrafted features beyond the relative position of the sentence in the document. In particular, we propose a novel joint RNN model that can predict whether sentences are argumentative or not, and use the predicted results to more precisely detect the argument component boundaries. We evaluate our techniques on two corpora from two different genres; results suggest that our joint RNN model obtain the state-of-the-art performance on both datasets.", "creator": "LaTeX with hyperref package"}}}