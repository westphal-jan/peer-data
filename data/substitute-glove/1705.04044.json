{"id": "1705.04044", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "End-to-end Recurrent Neural Network Models for Vietnamese Named Entity Recognition: Word-level vs. Character-level", "abstract": "This glass sincerity full - without - end connective network microcontroller while Vietnamese established principle inclusion. Our movie model known the enough of one-out Long Short - Term Memory (Bi - LSTM ), Convolutional Neural Network (CNN ), Conditional Random Field (CRF ), require new - entered not elysium as usage, or wholeness means F1 points where 76. 81% when a guidance combination set. Our problem goes wo before emphasis on larger success to the two - assigned system including made VLSP turning right example indeed arithmetic or hand - straightforward display. We on opportunity provided extensive empirical educational by using particular deep taught sizes year Vietnamese NER, the and it through plays highest.", "histories": [["v1", "Thu, 11 May 2017 07:31:39 GMT  (148kb)", "http://arxiv.org/abs/1705.04044v1", "6 pages, submitted to PACLING 2017"], ["v2", "Tue, 11 Jul 2017 16:49:03 GMT  (218kb)", "http://arxiv.org/abs/1705.04044v2", "14 pages, 5 figures, 7 tables, accepted to PACLING 2017"], ["v3", "Fri, 21 Jul 2017 00:04:32 GMT  (218kb)", "http://arxiv.org/abs/1705.04044v3", "14 pages, 5 figures, 7 tables, accepted to PACLING 2017, fix CRF formular"]], "COMMENTS": "6 pages, submitted to PACLING 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thai-hoang pham", "phuong le-hong"], "accepted": false, "id": "1705.04044"}, "pdf": {"name": "1705.04044.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Phuong Le-Hong"], "emails": ["phamthaihoang.hn@gmail.com", "phuonglh@vnu.edu.vn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n04 04\n4v 1\n[ cs\n.C L\n] 1\n1 M\nay 2\nKeywords-Vietnamese, named entity recognition, end-to-end\nI. INTRODUCTION\nNamed entity recognition (NER) is a fundamental task in natural language processing and information extraction. It involves identifying noun phrases and classifying each of them into a predefined class. In 1995, the 6th Message Understanding Conference (MUC)1 started evaluating NER systems for English, and in subsequent shared tasks of CoNLL 20022 and CoNLL 20033 conferences, language independent NER systems were evaluated. In these evaluation tasks, four named entity types were considered, including names of persons, organizations, locations, and names of miscellaneous entities that do not belong to these three types. More recently, the Vietnamese Language and Speech Processing (VLSP)4 community has organized an evaluation campaign to systematically compare NER systems for the Vietnamese language. Similar to the CoNLL 2003 share task, four named entity types are evaluated: persons (PER), organizations (ORG), locations (LOC), and miscellaneous entities (MISC). The data are collected from electronic newspapers published on the web. In this paper, we present a state-of-the-art NER system for the Vietnamese language without using any hand-crafted features. Our system is competitive with the first-rank system\n1http://cs.nyu.edu/faculty/grishman/muc6.html 2http://www.cnts.ua.ac.be/conll2002/ner/ 3http://www.cnts.ua.ac.be/conll2003/ner/ 4http://vlsp.org.vn/\nof the VLSP campaign that used many syntactic and handcrafted features. In summary, the overall F1 score of our system is 88.59% on the standard test set provided by the organizing committee of the evaluation campaign5. The contributions of this work include:\n\u2022 We propose a truly end-to-end deep learning model which gives the state-of-the-art performance on a standard NER data set for Vietnamese. Our best model is the combination of Bi-LSTM, CNN, and CRF models, which achieves an F1 score of 88.59%. \u2022 We give an extensive empirical study on using common deep learning models for Vietnamese NER, at both word and character level. These models are also comparable to conventional sequence labeling models, including Maximum Entropy Markov Models (MEMMs) and CRFs. \u2022 We make our NER system open source for research purpose, which is believed to be a good contribution to the future development of Vietnamese NER in particular and Vietnamese language processing research in general.\nThe remainder of this paper is structured as follows. Section II summarizes related work on NER. Section III describes end-to-end models used in our system. Section IV gives experimental results and discussions. Finally, Section V concludes the paper."}, {"heading": "II. RELATED WORK", "text": "Within the large body of research on NER which have been published in the last two decades, we identify two main approaches. The first approach is characterized by the use of well-established sequence labeling models such as conditional random field (CRF), hidden markov model, support vector machine, maximum entropy and so on. The performance of these models is heavily dependent on handcrafted features. In particular, most of the participants at CoNLL-2003 shared task attempted to use information other than the available training data such as gazetteers and\n5The first-rank system of the VLSP 2016 NER evaluation campaign has F1 =88.78% on the test set.\nunannotated data. The best system at CoNLL-2003 shared task is the work of [1] which achieved an F1 score of 88.76%. After that, [2] surpassed them by using phrase features extracted from an external database. Moreover, training NER models jointly with related tasks helps improve their performance. For instance, [3] trained a CRF model for joint-learning three tasks, including coreference resolution, entity linking, and NER, and achieved the state-of-the-art result on OntoNotes dataset. With a similar approach, [4] gained the best performance on CoNLL-2003 shared task dataset.\nWith a recent resurgence of the deep learning approach, several neural architectures have been proposed for NER task. These methods have a long story, but they have been focused only recently by the advance of computational power and high-quality word embeddings. The first neural network model is the work of [5] that used a feed-forward neural network with one hidden layer. This model achieved the state-of-the-art result on the MUC6 dataset. After that, [6] used a long short-term memory network for this problem. Recently, [7] used a convolution neural network over a sequence of word embeddings with a conditional random field on the top. This model achieved near state-of-theart results on some sequence labeling tasks such as POS tagging, chunking, and NER. From 2015 until now, the long short-term memory model has been the best approach for many sequence labeling tasks. [8] used bidirectional LSTM with CRF layer for joint decoding. Instead of using hand-crafted feature as [8], [9] proposed a hybrid model that combined bidirectional LSTM with convolutional neural networks (CNN) to learn both character-level and word-level representations. Unlike [9], [10] used bidirectional LSTM to model both character and word-level information. The work of [11] proposed a truly end-to-end model that used only word embeddings for detecting entities. This model is the combination of CNN, bidirectional LSTM, and CRF models. Approaching this problem at the character-level sequence, the LSTM-CRF model of [12] achieved the nearly state-ofthe-art results in seven languages."}, {"heading": "III. METHODOLOGY", "text": ""}, {"heading": "A. Long Short-Term Memory", "text": "Long short-term memory (LSTM) [13] is a variant of RNN which is designed to deal with these gradient vanishing and exploding problems [14], [15] when learning with long-range sequences. LSTM networks are the same as RNN, except that the hidden layer updates are replaced by memory cells. Basically, a memory cell unit is composed of three multiplicative gates that control the proportions of information to forget and to pass on to the next time step. As a result, it is better for exploiting long-range dependency\ndata. The memory cell is computed as follows:\nit = \u03c3(Wiht\u22121 + Uixt + bi)\nft = \u03c3(Wfht\u22121 + Ufxt + bf )\nct = ft \u2299 ct\u22121 + it \u2299 tanh(Wcht\u22121 + Ucxt + bc)\not = \u03c3(Woht\u22121 + Uoxt + bo)\nht = ot \u2299 tanh(ct)\nwhere \u03c3 is the element-wise sigmoid function and \u2299 is the element-wise product, i, f, o and c are the input gate, forget gate, output gate and cell vector respectively. Ui,Uf ,Uc,Uo are connection weight matrices between input x and gates, and Ui,Uf ,Uc,Uo are connection weight matrices between gates and hidden state h. bi, bf , bc, bo are the bias vectors."}, {"heading": "B. Bidirectional Long Short-Term Memory", "text": "The original LSTM uses only previous contexts for prediction. For many sequence labeling tasks, it is advisable when taking the contexts from two directions. Thus, we utilize the bidirectional LSTM (Bi-LSTM) [16], [17] for both word and character-level systems."}, {"heading": "C. Convolutional Neural Network for Character-Embedding", "text": "Convolutional neural network (CNN) is a type of feedforward neural networks that that uses many identical copies of the same neuron. This characteristic of CNN permits this network to have lots of neurons and, therefore, express computationally large models while keeping the number of actual parameters relativity small. For NLP tasks, previous works have shown that CNN is likely to extract morphological features such as prefix and suffix effectively [18], [9], [11]. For this reason, we incorporate the CNN to the word-level model to get richer information from both word and character features. The CNN we use in this paper is described in Figure 1."}, {"heading": "D. Conditional Random Field", "text": "Conditinal Random Field (CRF) [19] is a type of graphical model designed for labeling sequence of data. Although the LSTM is likely to handle the sequence of the input data by learning the dependencies between the input at each time step but it predicts the outputs independently. The CRF, therefore, is beneficial to explore the correlations between outputs and jointly decode the best sequence of labels. In NER task, we implement the CRF on the top of Bi-LSTM instead of the softmax layer and take outputs of Bi-LSTM as the inputs of this model. The parameter of the CRF is the transition matrix A where Ai,j represents the transition score from tag i to tag j. The score of the input sentence x along with the sequence of tags y is computed as follow:\nS(x, y, \u03b8 \u222aAi,j) =\nT \u2211\ni=1\n(Ait\u22121,it + f\u03b8(it,t))\nwhere \u03b8 is the parameters of Bi-LSTM, f\u03b8 is the score outputed by Bi-LSTM, and T is the number of time steps. Then the tag-sequence likelihood is computed by the softmax equation:\np(y|x, A) = exp(S(x, y, \u03b8 \u222a Ai,j)) \u2211\ny \u2032 \u2208Y exp(S(x, y\n\u2032\n, \u03b8 \u222a Ai,j))\nwhere Y is the set of all possible output sequences. In the training stage, we maximize the log-likelihood function:\nL =\nN \u2211\ni=1\nlog p(yi|xi;A)\nwhere N is the number of training samples. In the inference stage, the Viterbi algorithm is used to find the output sequence y\u2217 that maximize the conditional probability:\ny\u2217 = argmax y\u2208Y p(y|x, A)"}, {"heading": "E. Word-level Model", "text": "We construct the CRF on the top of Bi-LSTM layer for the word-level model. The input for this model is the concatenation of word embeddings and character-level features learnt from CNN layer. To create word embeddings for Vietnamese, we train a skip-gram model using the word2vec6 tool on a dataset consisting of 7.3GB of text from 2 million articles collected through a Vietnamese news portal.7 The text is first normalized to lower case and all special characters are removed. The common symbols such as the comma, the semicolon, the colon, the full stop and the percentage sign are replaced with the special token punct, and all numeral sequences are\n6https://code.google.com/archive/p/word2vec/ 7http://www.baomoi.com\nreplaced with the special token number. Each word in the Vietnamese language may consist of more than one syllables with spaces in between, which could be regarded as multiple words by the unsupervised models. Hence it is necessary to replace the spaces within each word with underscores to create full word tokens. The tokenization process follows the method described in [20]. For words that appear in VLSP corpus but not appear in word embeddings set, we create random vectors for these words by uniformly sampling from the range [\u2212 \u221a 3\ndim ,+\n\u221a\n3\ndim ] where dim is the dimension\nof embeddings. Figure 2 describes the architecture of the word-level model."}, {"heading": "F. Character-level Model", "text": "We also utilize the Bi-LSTM-CRF architecture for the character-level model but the input is the sequence of characters instead of the sequence of words. The training data is designed for word-level sequence labeling. In particular, the output is the sequence of labels where each label belongs to one specific word rather than a character. For this reason, it is necessary to convert the dataset from word-level sequences to character-level sequences. We use a simple method in which all characters of a word are labeled with the same tag. For example, the label of all characters of a person named entity is P. Similarly, all characters of location, organization, and miscellaneous tokens are labelled with letters L, G, and M respectively. The characters of other words and spaces are labelled by O. Figure 3 shows the transformation from word-level to character-level of an example sentence Anh r\u1eddi EU h\u00f4m qua (UK left EU yesterday). Because the size of Vietnamese character set is relatively small, our data set is sufficient to learn distributed representations for these characters. We initialize random vectors for these characters by uniformly sampling from the\nrange [\u2212 \u221a 3\ndim ,+\n\u221a\n3\ndim ] where dim is the dimension of\nembeddings. These character vectors are then learnt during training together with the parameters of the models."}, {"heading": "IV. RESULTS AND DISCUSSIONS", "text": "A. VLSP Corpus\nWe evaluate our system on the VSLP NER shared task 2016 corpus. This corpus consists of electronic newspapers published on the web. There are four named entity types in this corpus, names of person, location, organization and other named entities. Four types of NEs are compatible with their descriptions in the CoNLL shared task 2003. The examples of each entity type are described in Table I\nData have been preprocessed with word segmentation and POS tagging. Because POS tags and chunking tags are determined automatically by public tools, they may contain mistakes. The format of this corpus follows that of the CoNLL 2003 shared task. It consists of five columns. The order of these columns are word, POS tag, chunking tag, named entity label, and nested named entity label. Our system focuses on only named entity without nesting, so we do not use the fifth column. Named entity labels are annotated using the IOB notation as in the CoNLL shared tasks. There are 9 labels: B-PER and I-PER are used for persons, B-ORG and I-ORG are used for organizations, BLOC and I-LOC are used for locations, B-MISC and IMISC are used for other named entities and O is used for other elements. Table II shows the quantity of named entity annotated in the training set and the test set. Because our systems are end-to-end architecture, we focus only on the word and named entity label columns. To alleviate the data sparseness, we perform the following preprocessing for our system:\n\u2022 All tokens containing digit number are replaced by a special token number. \u2022 All punctuations are replaced by a special token punct.\nMoreover, we take one part of training data for validation. The detail of each data set is described in Table III."}, {"heading": "B. Evaluation Method", "text": "The performance is measured with F1 score:\nF1 = 2 \u2217 precision \u2217 recall\nprecision+ recall\nPrecision is the percentage of named entities found by the learning system that are correct. Recall is the percentage of named entities present in the corpus that are found by the system. A named entity is correct only if it is an exact match of the corresponding entity in the data file. For characterlevel model, after predicting label for each character, we convert these outputs back to the word-level sequence to evaluate. The performance of our system is evaluated by the automatic evaluation script of the CoNLL 2003 shared task.8."}, {"heading": "C. Results", "text": "Word-level Model vs. Character-level Model: In the first experiment, we compare the effectiveness of word and character-level approaches without using any external corpus. For this reason, in this experiment, we temporarily do not use any pre-trained word embeddings. We initialize both word and character embeddings randomly and learn them during training. Both of the two systems take embeddings as inputs of Bi-LSTM and predict outputs by the CRF top layer. Table IV presents the performance of these systems. We see that the character-level model outperforms the word-level model by about 4%. It is because the size of the character set is much smaller than that of word set. The VLSP corpus, therefore, is enough for learning effectively character embeddings. For word embeddings, we need a bigger corpus to learn useful word vectors.\n8http://www.cnts.ua.ac.be/conll2003/ner/\nEffect of Word Embeddings: It is beneficial to use the external corpus to learn the word embeddings. In the second experiment, we use pre-trained word embeddings on a large text corpus as described in Section III. The improvement by using pre-trained word embeddings for the word-level model is shown in Table V.\nBy using pre-trained word embeddings, the performance of word-level model increases by about 4%, to 80.85%. This accuracy is equivalent to that of the character-level model. It proves the effectiveness of using good embeddings for both words and characters in the Bi-LSTM-CRF model.\nEffect of Character-level Features: In the third experiment, we evaluate the improvement when adding the character-level features learned from CNN to the wordlevel feature. These features are concatenated with the pretrained word embeddings as input of the Bi-LSTM layer. The performance when exploiting character-level features is described in Table VI.\nWe observe a significant improvement of performance when character features learnt by CNN are integrated with pre-trained word embeddings. This model achieves an overall F1 score of 88.38%.\nComparison with Previous Systems: In VLSP 2016 workshop, several different systems have been proposed for Vietnamese NER. In this campaign, they have evaluated over three entities types LOC, ORG, PER. In all fairness,\nwe also evaluate our performances over these tags on the same training and test set. The accuracy of our best model over three entity types is 88.59%, which is competitive with the best participating system [21] in that shared task. That system, however, used many hand-crafted features to improve the performance of maximum entropy classifier (ME) while our system is truly end-to-end model that takes only word sequences as inputs. Most approaches in VLSP 2016 used the CRF and ME models, whose performance is heavily dependent on feature engineering. Table VII shows those models and their performance.\nThere is one work [23] that applied deep learning approach for this task. They used the implementation provided by [10]. There are two types of LSTM models in this open source software: Bi-LSTM-CRF and Stack-LSTM. The model that is most similar to ours is Bi-LSTM-CRF. The accuracy of this system is 83.25%. Our system outperforms this model due to some possible reasons. First, they used random vectors as word embeddings and update them during the training stage. The VLSP corpus size is relatively small so it is not good enough for learning word representations. Our word embeddings are trained on a collection of Vietnamese newspapers that is much larger and more abundant than the VLSP corpus. Second, they used LSTM to model characterlevel features, while we used CNN in our model. Previous works have shown that CNN is very useful to extract these features [18], [9], [11]."}, {"heading": "V. CONCLUSION", "text": "In this work, we have investigated a variety of end-toend recurrent neural network architectures at both word and character-level for Vietnamese named entity recognition. Our best end-to-end system is the combination of Bi-LSTM, CNN, and CRF models, and uses pre-trained word embeddings as input, which achieves an F1 score of 88.59% on the standard test corpus published recently by the Vietnamese Language and Speech community. Our system is competitive with the first-rank system of the related NER shared task without using any hand-crafted features."}], "references": [{"title": "Named entity recognition through classifier combination", "author": ["R. Florian", "A. Ittycheriah", "H. Jing", "T. Zhang"], "venue": "Proceedings of CoNLL-2003, W. Daelemans and M. Osborne, Eds. Edmonton, Canada, 2003, pp. 168\u2013171. 9This team provided a system without the technical report.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Phrase clustering for discriminative learning", "author": ["D. Lin", "X. Wu"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, vol. 2. Association for Computational Linguistics, 2009, pp. 1030\u20131038.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "A joint model for entity analysis: Coreference, typing, and linking", "author": ["G. Durrett", "D. Klein"], "venue": "Transactions of the Association for Computational Linguistics, vol. 2, pp. 477\u2013490, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint entity recognition and disambiguation", "author": ["G. Luo", "Z.N. Xiaojiang Huang", "Chin-Yew Lin"], "venue": "Proceedings of the 2015 Conference on Empirical Methods on Natural Language Processing. Association for Computational Linguistics, 2015, pp. 879\u2013888.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Symbolic and neural learning for named-entity recognition", "author": ["G. Petasis", "S. Petridis", "G. Paliouras", "V. Karkaletsis", "S. Perantonis", "C. Spyropoulos"], "venue": "Symposium on Computational Intelligence and Learning. Chios, Greece: Citeseer, 2000, pp. 58\u201366.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Named entity recognition with long shortterm memory", "author": ["J. Hammerton"], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL, vol. 4. Association for Computational Linguistics, 2003, pp. 172\u2013175.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Z. Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1991}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["J.P. Chiu", "E. Nichols"], "venue": "Transactions of the Association for Computational Linguistics, vol. 4, pp. 357\u2013370, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "arXiv preprint arXiv:1603.01360, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end sequence labeling via bidirectional lstm-cnns-crf", "author": ["X. Ma", "E. Hovy"], "venue": "arXiv preprint arXiv:1603.01354, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Charner: Characterlevel named entity recognition", "author": ["O. Kuru", "O.A. Can", "D. Yuret"], "venue": "Proceedings of The 26th International Conference on Computational Linguistics, 2016, pp. 911\u2013921.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning longterm dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE transactions on neural networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "The 30th International Conference on Machine Learning, vol. 28, Atlanta, USA, 2013, pp. 1310\u20131318.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Proceedings of 2005 IEEE International Joint Conference on Neural Networks, vol. 4. Montreal, QC, Canada: IEEE, 2005, pp. 2047\u20132052.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. rahmand Mohamed", "G. Hinton"], "venue": "Proceedings of 2013 IEEE international conference on acoustics, speech and signal processing. Vancouver, BC, Canada: IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["C. dos Santos", "V. Guimaraes", "a. R. d. J. RJ Niter\u00f3i"], "venue": "Proceedings of NEWS 2015 The Fifth Named Entities Workshop, 2015, pp. 25\u201333.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of The Eighteenth International Conference on Machine Learning, vol. 1, 2001, pp. 282\u2013289.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "A hybrid approach to word segmentation of Vietnamese texts", "author": ["P. Le-Hong", "T.M.H. Nguyen", "A. Roussanaly", "T.V. Ho"], "venue": "Language and Automata Theory and Applications, ser. Lecture Notes in Computer Science. Springer Berlin Heidelberg, 2008, vol. 5196, pp. 240\u2013249.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Vietnamese named entity recognition using token regular expressions and bidirectional inference", "author": ["P. Le-Hong"], "venue": "Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Processing, Hanoi, Vietnam, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Dsktlab-ner: Nested named entity recognition in vietnamese text", "author": ["T.C.V. Nguyen", "T.S. Pham", "T.H. Vuong", "N.V. Nguyen", "M.V. Tran"], "venue": "Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Processing, Hanoi, Vietnam, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Vietnamese named entity recognition at vlsp 2016 evaluation campaign", "author": ["T.S. Nguyen", "L.M. Nguyen", "X.C. Tran"], "venue": "Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Processing, Hanoi, Vietnam, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Named entity recognition in vietnamese text", "author": ["T.H. Le", "T.T.T. Nguyen", "T.H. Do", "X.T. Nguyen"], "venue": "Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Processing, Hanoi, Vietnam, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The best system at CoNLL-2003 shared task is the work of [1] which achieved an F1 score of 88.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "After that, [2] surpassed them by using phrase features extracted from an external database.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "For instance, [3] trained a CRF model for joint-learning three tasks, including coreference resolution, entity linking, and NER, and achieved the state-of-the-art result on OntoNotes dataset.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "With a similar approach, [4] gained the best performance on CoNLL-2003 shared task dataset.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "The first neural network model is the work of [5] that used a feed-forward neural network with one hidden layer.", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "After that, [6] used a long short-term memory network for this problem.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "Recently, [7] used a convolution neural network over a sequence of word embeddings with a conditional random field on the top.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "[8] used bidirectional LSTM with CRF layer for joint decoding.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Instead of using hand-crafted feature as [8], [9] proposed a hybrid model that combined bidirectional LSTM with convolutional neural networks (CNN) to learn both character-level and word-level representations.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "Instead of using hand-crafted feature as [8], [9] proposed a hybrid model that combined bidirectional LSTM with convolutional neural networks (CNN) to learn both character-level and word-level representations.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "Unlike [9], [10] used bidirectional LSTM to model both character and word-level information.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "Unlike [9], [10] used bidirectional LSTM to model both character and word-level information.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "The work of [11] proposed a truly end-to-end model that used only word embeddings for detecting entities.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "Approaching this problem at the character-level sequence, the LSTM-CRF model of [12] achieved the nearly state-ofthe-art results in seven languages.", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Long short-term memory (LSTM) [13] is a variant of RNN which is designed to deal with these gradient vanishing and exploding problems [14], [15] when learning with long-range sequences.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "Long short-term memory (LSTM) [13] is a variant of RNN which is designed to deal with these gradient vanishing and exploding problems [14], [15] when learning with long-range sequences.", "startOffset": 134, "endOffset": 138}, {"referenceID": 14, "context": "Long short-term memory (LSTM) [13] is a variant of RNN which is designed to deal with these gradient vanishing and exploding problems [14], [15] when learning with long-range sequences.", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "Thus, we utilize the bidirectional LSTM (Bi-LSTM) [16], [17] for both word and character-level systems.", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "Thus, we utilize the bidirectional LSTM (Bi-LSTM) [16], [17] for both word and character-level systems.", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "For NLP tasks, previous works have shown that CNN is likely to extract morphological features such as prefix and suffix effectively [18], [9], [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 8, "context": "For NLP tasks, previous works have shown that CNN is likely to extract morphological features such as prefix and suffix effectively [18], [9], [11].", "startOffset": 138, "endOffset": 141}, {"referenceID": 10, "context": "For NLP tasks, previous works have shown that CNN is likely to extract morphological features such as prefix and suffix effectively [18], [9], [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "Conditinal Random Field (CRF) [19] is a type of graphical model designed for labeling sequence of data.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "The tokenization process follows the method described in [20].", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "59%, which is competitive with the best participating system [21] in that shared task.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "Teams Models Performances [21] ME 88.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "62 [22] ME 84.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "08 [23] LSTM 83.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "80 [24] CRF 78.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "There is one work [23] that applied deep learning approach for this task.", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "They used the implementation provided by [10].", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "Previous works have shown that CNN is very useful to extract these features [18], [9], [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "Previous works have shown that CNN is very useful to extract these features [18], [9], [11].", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": "Previous works have shown that CNN is very useful to extract these features [18], [9], [11].", "startOffset": 87, "endOffset": 91}], "year": 2017, "abstractText": "This paper demonstrates end-to-end neural network architectures for Vietnamese named entity recognition. Our best model is the combination of bidirectional Long ShortTerm Memory (Bi-LSTM), Convolutional Neural Network (CNN), Conditional Random Field (CRF), using pre-trained word embeddings as input, which achieves an F1 score of 88.59% on a standard test set. Our system is able to achieve a comparable performance to the first-rank system of the VLSP campaign without using any syntactic or hand-crafted features. We also give an extensive empirical study on using common deep learning models for Vietnamese NER, at both word and character level. Keywords-Vietnamese, named entity recognition, end-to-end", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}