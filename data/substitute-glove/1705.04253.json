{"id": "1705.04253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Sketching Word Vectors Through Hashing", "abstract": "We propose a setting caught word 3-dimensional mastered turn symplectic types. The combinations unlike without derandomization more from new meaning fact random surpluses: By arbitrariness the classic constraint supplied 2003 designing random deficits (everyone. blog. , sanctity pairwise distances days rather particular pesa larger ), our framework superhero definitely sparse terms - negative dna analysts. Our discovering showing even entered proposal method can fulfill compete results, range they processes invariant practical large-scale, however, with only a fraction of the optimisation complexity while same specifically. While the request derandomization musicality although probabilistic made storage practical result their requires, the change of applying inflate utilized such or expect three-fold mutual information (PPMI) to our models after their construction (other, a reduced gargantuan) imparts long middle blatantly power any time related pgl. Obviously, example procedure comes with separate which benefits of random projection - an purposes sometimes rather curb of update.", "histories": [["v1", "Thu, 11 May 2017 15:53:00 GMT  (203kb)", "http://arxiv.org/abs/1705.04253v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["behrang qasemizadeh", "laura kallmeyer", "peyman passban"], "accepted": false, "id": "1705.04253"}, "pdf": {"name": "1705.04253.pdf", "metadata": {"source": "CRF", "title": "Sketching Word Vectors Through Hashing", "authors": ["Behrang QasemiZadeh", "Laura Kallmeyer", "Peyman Passban"], "emails": ["zadeh@phil.hhu.de", "kallmeyer@phil.hhu.de", "ppassban@computing.dcu.ie"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n04 25\n3v 1\n[ cs\n.C L\n] 1\n1 M\nay 2"}, {"heading": "1 Introduction", "text": "Word embedding techniques (i.e., using distributional frequencies to produce word vectors of reduced dimensionality) have become a cornerstone of modern natural language processing and information retrieval systems. These quantifications of words are often justified by Harris\u2019 Distributional Hypothesis [Harris, 1954]. It is hypothesized that words of comparable linguistic properties appear with/within a similar set of \u2018contexts\u2019. For instance, words of similar meaning co-occur with a similar set of context words {c1, . . . cn}. This hypothesis implies that if this set of context words are partitioned randomly into m buckets, e.g. {{c1 . . . cx}1, . . . , {cy, . . . cn}m}, then co-related words still co-occur with similar set of buckets. We design a new random projection to exploit this presumption, and accordingly, propose a new hash-based word embedding technique.\nIn the remaining of this paper, we propose the method in Section 2. Section 3 provides a brief overview of related\nwork. Section 4 reports results from a number of experiments, followed by a conclusion in Section 5."}, {"heading": "2 The Proposed Embedding Technique", "text": "Lets assume that we build an m-dimensional embedding for a word w that is co-occurred with a number of context words c. To build an embedding for each w (denoted by ~w), the following steps are taken:\n1: Initialize a zerom-dimensional vector ~w 2: for each c co-occurred with w do 3: d\u2190 abs(hash(c) %m) 4: ~wd = ~wd + 1\nreturn ~w\nAbove, wd is the dth component of ~w; the hash function assigns a (ideally unique, and independently and identically distributed) hash code (e.g., an integer) to each context word (object). The abs function returns the absolute value of an input number; % is the modulus operator and it gives the remainder of the division of the generated hash code by the chosen value m. We choose Jenkins hash function in our implementation since it shows a low collision rate for short words (bytes sequences):1\nint hash(byte[] key) { int i = 0; int hash = 0; while (i != key.length) { hash += key[i++]; hash += hash << 10; hash ^= hash >> 6;\n} hash += hash << 3; hash ^= hash >> 11; hash += hash << 15; return hash;\n}\nOnce these embeddings are constructed, similarities between them are given by an appropriate correlation measure such as Goodman and Kruskal\u2019s \u03b3 coefficient [Goodman and Kruskal, 1954] (or, alternatively, Kenall\u2019s \u03c4b [Kendall, 1938]). This choice is motivated by the fact that the resulting embeddings are multinomial categorical variables\n1Designed by Bob Jenkins (2006); see http://www.burtleburtle.net/bob/hash/doobs.html.\n(of a discrete distribution) with non-Gaussian long-tailed distribution (which are, in many cases, sparse). To compute \u03b3, concordant and discordant pairs must be counted. Given any pairs such as (xi, yi) and (xj , yj) from two m-dimensional vectors ~x and ~y and the value v = (xi \u2212 xj)(yi \u2212 yj), these two pairs are concordant if v > 0 and discordant if v < 0. If v = 0, the pair is neither concordant nor discordant. Let p and q be the number of concordant and discordant pairs, then \u03b3 is given by [Chen and Popovich, 2002, p. 86]: \u03b3 = p\u2212q\np+q .\nAs with other statistical models, normalizing the counted frequencies in the obtained embeddings using a \u2018weighting process\u2019 enhances results. Evidently, a weighting process can eliminate uninformative and irrelevant frequencies and boost the impact of informative and discriminatory ones, e.g., through normalizing raw frequencies by the expected and marginal frequencies, such as the popular PPMI weighting [Turney, 2001; Bouma, 2009]. If Wp\u00d7m is the matrix consisting of p row vectors ~w (i.e., the set of obtained embeddings in our model), the PPMI weight for a component wxy inW is:\nppmi(wxy) = max(0, log wxy \u00d7\n\u2211p\ni=1\n\u2211m\nj=1 wij \u2211p\ni=1 wiy \u00d7 \u2211m j=1 wxj ).\nFor the weighted vectors (which often have rectified Gaussian distribution), we expect a correlation measure such Pearson be a more suitable mean to compute similarities between vectors.\nNote that the procedure suggested above can be serialized in various ways to meet the requirements of applications for which the method is used. For example, when processing text streams (or, sequential scan of very large corpora), a new vector can be added or an existing one can be updated on the fly. Obviously, these vectors can be used at any time since no actual training phase is involved; not to mention that the process can be easily carried out in parallel. To facilitate the weighting process, one m-dimensional vector that holds the sum of coordinates of all vectors (or a random subset of them) can reside in memory at all time."}, {"heading": "2.1 Method\u2019s Justification", "text": "The proposed method can be explained mathematically using the principles of dimensionality reduction using random projections.\nLet Cp\u00d7n denotes the set of p word vectors obtained by counting the co-occurrences of each target word ~wi (1 \u2264 i \u2264 p) with each context element cj (1 \u2264 j \u2264 n). In most applications, n is a very large number in a way that it hinders processes (and causes the so-called curse of dimensionality problem). To address this problem, Cp\u00d7n undergoes a set of \u2018transformations\u2019 T to map the high-dimensional space Cp\u00d7n onto a space of reduced dimensionality Wp\u00d7m (m \u226a n): Cp\u00d7n \u00d7 Tn\u00d7m = Wp\u00d7m. In our proposed method, Tn\u00d7m is a sparse randomly generated matrix, in which tij elements of T has the following distribution:\ntij =\n{\n0 with probability m\u22121 m 1 with probability 1 m , (1)\nsuch that each row vector of T has exactly one component that has value 1, and these non-zero values of T have independent and identical distribution. It can be verified that the procedure proposed in the previous section computes the desired embeddings (i.e., W) by (a) de-randomization of T using a hash function and the modulus operator, and (b) serializing the involved multiplication for computingC\u00d7T to a set of addition operations (based on the distributive property of multiplication over addition).\nAs an expert can identify, the major novelty of our method is in the way that we compute T. Previously, randomprojection-based methods compute T with the goal of having the least distortions in pairwise (\u03b1-normed) distances while mapping from C onto W\u2014e.g., the well-known lemmas proposed in [Johnson and Lindenstrauss, 1984] (for \u21132normed spaces), [Indyk, 2000] (for \u21131-normed spaces), and their subsequent refinements and generalizations such as [Li et al., 2006]. In contrast to these research, we disregard this classic desiderata of preserving distances and the goal of minimum-distortion correspondence. In return, we motivate our proposed random projection directly using the implications that Harris\u2019 Distributional Hypothesis bears."}, {"heading": "3 Related Work", "text": "Random projections and hash kernels have been vibrant research areas in artificial intelligence and theoretical computer science. These methods have been employed to provide viable solutions for a range of problems requiring a notion of (approximate) nearest neighbor search, particularly in information retrieval tasks, e.g., identification of duplicate and near duplicate documents [Manku et al., 2007] and string matching [Dalvi et al., 2013; Michael, 1981], semantic labeling [Yang et al., 2016], and cross-media retrieval [Wang et al., 2015] to name a few. Naturally, these methods have been also applied to the problem of word embedding, either as a dimensionality reduction method [Bingham and Mannila, 2001; Kaski, 1998], or in the form of an incremental vector space construction technique in which the random projection-based dimensionality reduction process is merged with the process of collecting word co-occurrence frequencies, e.g., [Kanerva et al., 2000; Geva and De Vries, 2011; Q. Zadeh and Handschuh, 2014].\nKanerva et al. employed sparse Gaussian random projections [QasemiZadeh, 2015] to build word vectors directly at a reduced dimensionality and showed that their proposed \u2018random indexing\u2019 technique yields results comparable to the latent semantic analysis technique of [Deerwester et al., 1990], which employs truncation using singular value decomposition for dimensionality reduction. By the same token, Geva and De Vries and Q. Zadeh and Handschuh extend the idea proposed by Kanerva et al. for the comparison of textual similarities using the Hamming and Manhattan distances, respectively. However, when it comes to comparing semantic similarities between words, these methods fail to compete with the more recent neural-based embedding techniques, e.g., word2vec [Mikolov et al., 2013b] and GloVe [Pennington et al., 2014]. It is known that crude distances between words are not discriminatory enough to address the\nso-called semantic relatedness tasks, and a weighting process such as PPMI is crucial for achieving a high score. Unfortunately, since these methods use projections with zero expectation,2 the sum of components in their resulting vectors is always zero. Thus, weighting techniques such as PPMI cannot be applied to these models after their construction (simply, due to the problem of division by zero). Compared to these techniques, the method proposed in this paper has a better computational complexity (thanks to its sparser projections) and yields better results in semantic relatedness tasks given the possibility of applyingweighting techniques after the construction of randomly projected spaces."}, {"heading": "4 Empirical Evaluation", "text": "We report results from empirical evaluations in both intrinsic and extrinsic setups. Besides an intrinsic evaluation using word relatedness tests, we report results when the method is used for training a neural-based part-of-speech tagger."}, {"heading": "4.1 Intrinsic Evaluation", "text": "Datasets and Material: We evaluate our method over a number of word relatedness tests. Each relatedness test encompasses a set of word pairs, associated with similarity/relatedness ratings obtained from human annotators. For each test, evaluation takes the form of calculating the harmonic mean of Pearson and Spearman\u2019s correlations between the list sorted by the human-induced scores and the list sorted by the scores assigned to word pairs by the system. Relatedness tests in our experiments are WS353 [Finkelstein et al., 2001], WSS and WSR by [Agirre et al., 2009], the classic tests of MC30 [Miller and Charles, 1991] and RG65 [Rubenstein and Goodenough, 1965], the Stanford Rare Word dataset RW [Luong et al., 2013], M287 by [Radinsky et al., 2011], M771 [Halawi et al., 2012], SL [Hill et al., 2015], YP130 verb relatedness [Yang and Powers, 2006], and MEN [Bruni et al., 2014]. As input corpus, we build embeddings from a recent dump of Wikiepda (January 1st, 2017; 2.9 billion tokens).\nBaselines: For this input, as one baseline, we train a word2vec CBOW model [Mikolov et al., 2013b], one of the most popular word embedding techniques. To train this model, we use parameters that are commonly known to give best performances across the chosen tasks in our experiments (i.e., 10 negative samples, 1e-5 for subsampling, and contextwindows that extend 7 tokens around target words). We set the dimensionality of this model to 500 (put aside its impact on computational complexity, for our input corpus and the targeted tests, we observed that using dimensionality larger than 500 has an adverse effect on word2vec\u2019s CBOW algorithm and deteriorates its performance). We use W2C-CBOW to denote the obtained set of embeddings from this trial. As an additional baseline, we report the performance of a classic unweighted and PPMI-weighted high-dimensional\n2Note that for these methods, projections with zero expectation is essential for achieving an acceptable performance when computing projected spaces (i.e., to guarantee the sparsity of projection matrices).\nmodel trained over the same input corpus. In this case, the dimensionality of the classic model soared to 10.671 million. To confirm our earlier statement on sparse Gaussian random projections, we run Kanerva et al.\u2019s random indexing method (using projection vectors of dimensionality 2000 with 8 nonzero components, of which four are set to +1 and another four to -1); as expected, we witness that the obtained results (averaged over 5 independent runs of the system) are almost identical to those reported for unweighted high-dimensional classic model. In the experiments above, we use cosine as the similarity estimator between vectors.\nResult: We build word vectors of dimension m = 500, 1000, and 2000, and 4000 using our proposed hash-based embedding technique. Similar to the classic baseline method, except a white space tokenization, we do not use any additional heuristics or information (such as subsampling criteria used in word2vec). Table 1 summarizes the observed results, as well as the average of performances across these tasks. As shown, disregardingm and on average, our unweighted and PPMI-weighted models outperform their counterpart classic high-dimensional models. Moreover, for large values ofm, weighted models tend to perform as well as word2vec\u2019s CBOW model. Interestingly, increasing m hurts the taskbased performances for unweighted models, while it has a positive impact on the PPMI-weightedmodels. Overall, when it comes to choosing a neural embedding learning method versus our method for addressing a semantic similarity task, our experiments suggest that the hash-based method is more suitable for similarity identification than relatedness (in the sense that described in [Agirre et al., 2009]), and for tasks that involve comparison of frequent and infrequent words (due to the nature of PPMI weighting). Naturally, the hashbased method is a better choice than a neural embedding learning technique when dealing with frequently updated text data and streams,3 and when computational resources for training neural nets are insufficient or the learning process is constrained by hard time limits (note that hash-based embeddings can be fetched at any time during the construction of vectors). In our experiments and using the specific configurations, building word vectors using the hash-based method took almost one-third of the time for building the CBOW model (using 16 threads on a dual CPU machine). Note that the computational complexity of our method, in contrast to word2vec, is independent of m. For m = 500, our method requires half of the memory used by word2vec using a sparse vector representation model (note that we use unsigned integers for collecting co-occurrence frequencies and postpone using float number arithmetic until we weight vectors); this is not surprising given the very long tail of co-occurrence frequencies. Word2vec\u2019s CBOW-induced vectors (like other neural embeddings) are dense: in contrary to our method, even for words of low frequencies, all the elements of neural-based embeddings are non-zero. We would like to emphasize that\n3And, in scenarios in which vectors are built using a strategy other than offline scanning large text corpora, e.g., as explained in [Turney, 2001] by querying a Web search engine.\nthe real computational advantages of the proposedmethod become apparent when building vectors for real large corpora and vocabularies. At any rate, a neural embedding method such as word2vec requires two passes over an input corpus: one pass to make a frequency profile of words (which is used, e.g., for subsampling), and the second pass to adjust and optimise weights (i.e., the actual training phase). This requirement, however, is not necessary for our method (although a two-pass strategy for subsampling and the selection of a subset of co-occurrence events can be employed to enhance discriminatory power of the hash-based embeddings).\nRegarding the classic method, compared to the hash-based method, building vectors and frequency profiles may require slightly less amount of processes (given that, theoretically,4 no hash code must be computed); however, compared to our method, storing these vectors demands much more amount of memory. Moreover, the process of weighting these large vectors is computationally intense and time-consuming. In comparison, using the proposed hash-based method dramatically reduces the time required for the weighting process. For instance, in our experiments, PPMI weighting of highdimensional vectors took almost two hours while this process was carried out in only a few minutes using the hash-based method.\nBoosting Performances in Relatedness Tasks\nThrough several means, the performance of the proposed hash-based embedding method for semantic similarly tasks can be improved. The easiest method is, perhaps, using models of higher dimensionality ( as results reported in the previous section imply), or/and to enlarge the size of input corpora.\nAs reported in Table 1, we observe that increasing the dimensionality together with PPMI weighting of models enhance task-based performances. Obviously, increasing the dimensionality does not affect the computational complexity of building a model; however, it increases the cost of similarity\n4We say \u2018theoretically\u2019 since an inverted index of context elements (often using hashing) is necessary for building classic models.\ncomputation. Figure 1a plots changes in the method\u2019s performance (i.e., the arithmetic average of performances across tasks in our experiments) when the dimensionality of models are increased (the evaluation setup, i.e., the input corpus and context window size, remains the same as those reported earlier). As shown, up to m = 13000, the performance mostly improves. We admit that this solution may not look desirable at first sight. However, we notice that these models of higher dimensionality (e.g., m = 4000) can be compressed (e.g., to m = 250) using matrix factorization techniques without hurting task-based performances. Hence, we suggest that the proposed hash-based solution can safely replace methods of constructing classic PPMI-weighted high-dimensional models, for which the obtained models are compressed using matrix factorization methods. To show the effect of enlarging the size of input corpus on the method\u2019s performance, in addition to the Wikipedia corpus that we used initially, we feed another 4 billion tokens of web crawled text data [Schafer and Bildhauer, 2012] to our models. As shown in Figure 1b, disregarding the dimension of models (here 500, and 2000), using larger input corpora enhances the method\u2019s performance in relatedness tasks.\nSecond, like many embedding techniques, our method can be tuned for each task, e.g., by choosing an appropriate context window size, and in general, by making (let\u2019s call it) linguistically more informed decisions. For instance, we could boost the performance in the MEN relatedness from 0.71 to 0.76 using larger context window (i.e., 13+11 instead of 5+5). Similarly, we could enhance the obtained performance for the YP130 verb relatedness from 0.39 to 0.72 (for m = 500) through filtering context words using dependency parses (context elements were limited to those words with direct syntactic relationships to the targeted verbs). Apart from these, our method allows for heterogeneous context types: For example, in addition to context words, we use documentlevel co-occurrences to boost performance in the WS353 test from 0.67 to 0.72. For each w appeared in document di, we pass to the hash function di\u2019s identifier in our input and continue updating ~w as instructed in the algorithm. Note that there is no limit on the type of context elements that can be fed to our method as long as these elements can be converted to byte sequences and subsequently to a hash code. In comparison, this is not that straightforward with neural embedding techniques: often the objective function of neural nets (which defines their underlying optimization goal) must be modified and adapted for new context types.\nApart from methods discussed above, \u2018retrofitting\u2019 (refining word embeddings using lexical relations available in lexical resources [Faruqui et al., 2015]) is another methodology for improving results. Our method can be easily adapted for retrofitting. We implement a notion of retrofitting by simply treating lexical resources like any other text files; however, instead of scanning these resources using a sliding context window, we make sure that we encode co-occurrence information about all related items in each entry of an input lexical resource. For example, given a synset S of words in WordNet (i.e, S = {w1 . . . wn}), for each wi \u2208 S, we consider all the remaining words in S as context element and respectively update ~wi with these context elements. To\nshow the impact of this \u2018retrofitting\u2019 method, we take vectors of dimensionality m = 500 from our earlier experiment, and update them by reading WordNet [Miller, 1995] and PPDB [Ganitkevitch et al., 2013] (i.e., in addition to the Wikipedia corpus, WordNet and PPDB are also fed to our algorithm). Table 2 reports our results. As discussed in [Faruqui et al., 2015], encoding this knowledge into neural embeddingmodels is not straightforward, and it demands certain modification in their learning algorithm. For instance, we observe that retraining the CBOW algorithm with WordNet and PPDB as additional inputs leads to a negligible improvement on task-based performances (an improvement of 0.002 on average), whereas performance gain in retrofitted hash-based model is considerably large (from 0.417 to 0.436 for the unweighted model, and from 0.570 to 0.647 for the PPMI-weighted model). In this experiment, we provide the word2vec\u2019s CBOW algorithm with the same input used for building our hash-based model (i.e., in addition to the Wikipedia corpus, we list the set of all generated tuples from the WordNet and PPDB as input)."}, {"heading": "4.2 Extrinsic Evaluation", "text": "When developing models for upstream NLP tasks, an important application of embedding learning methods is to provide a numerical representation of words for employed (neural) learning algorithms. Likewise relatedness tasks, the word2vec family of embeddings methods [Mikolov et al., 2013a; Le and Mikolov, 2014] are a popular choice for this pre-training phase. Hence, we provide a comparison between the embeddings obtained using the proposed method in this paper and those of the word2vec model in a part-of-speech tagging task. We witness that vectors produced using our hash-based technique are more suitable than word2vec embeddings, at least, in our part-of-speech tagging experiment using Long Short Term Memory (LSTM) units [Hochreiter and Schmidhuber, 1997]. The configuration used for obtaining the embeddings is similar to one explained in the previous section, and the hashbased embeddings are weighted using PPMI prior to feeding them to the network. However, to make sure that the evaluation setup is realistic, we set the size of embeddings to 300 for both of the hash-based and word2vec models. For evaluation, we use the tagged version of the Brown corpus [Francis and Kucera, 1979], which has 57,067 sentences and 313 different tags. Note that although the tag set consists of only 85 categories, the tagged Brown corpus contains combined tags for contracted words; e.g., the sequence of we\u2019ll is tagged as PPSS+MD (which means, personal pronoun followed by modal auxiliary); hence, the number of tags in our experiment increases to 313. We randomly select 2000 sentences as the validation set, another 2000 as the test set, and the remaining sentences are used for training. The neural tagger has a straightforward architecture and consists of four layers, of which the first layer is a look-up table. This look-up table is a trainable weight matrix whose values are updated during training. Embeddings reside in this look-up table, which are retrieved at each forward pass. The second and third layers include LSTM units of size 300. The last Softmax layer scales values of the network to the range of\n[0,1] in order to make the prediction probability over classes, i.e., part-of-speech categories in our problem. We keep our neural architecture as simple as possible and avoid usingmore sophisticated models for two reasons. Firstly, we believe this makes it easier to reproduce results reported in this section. Secondly, we believe this property enables us to focus more on the quality of the employed embeddings rather than the network power, and thus to attribute performance gains to the embeddings themselves, not the neural architecture.\nWe design two evaluation settings. In the first setting, we freeze the look-up table and the network does not manipulate values of our embeddings. They are kept unchanged and other network parameters are tuned to make the best prediction. In the second setting, embeddings are treated as part of the network, and, similar to other network parameters, and they are updated during the training phase. Table 3 reports the observed results. In both settings, we observe better performance (accuracy) using the proposed hash-based embedding technique. We interpret the result from the first setting (frozen look-up table) as evidence that, by nature, the hash-based embeddings preserve richer and more suitable information for our sequence labelling task. Results observed in the second setting reconfirms our claim that the hash-based embeddings are a better choice than word2vec embeddings, at least, for this task: Hash-based embeddings not only yield a better performance in our sequence labelling task, but they also can be obtained faster and using less computational resources than word2vec."}, {"heading": "5 Conclusion", "text": "A method for sketching word vectors using a hash-based algorithm is proposed. The proposed method is fast, and it requires a small amount of computational resources to build a model; yet, as shown empirically, it can deliver an acceptable performance in both intrinsic and extrinsic evaluation setups. Thanks to the random projection that this method implements, developing embeddings requires no offline training time: vectors can be used, updated, added, and removed at any stage during a system\u2019s life cycle. The method can be particularly useful when building embeddings from text streams or very very large corpora, or when the training of\nembeddings is constrained by time."}], "references": [{"title": "In HLT-NAACL \u201909", "author": ["EnekoAgirre", "EnriqueAlfonseca", "Keith Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa. A study on similarity", "relatedness using distributional", "wordnet-based approaches"], "venue": "pages 19\u2013 27, Stroudsburg, PA, USA,", "citeRegEx": "Agirre et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Random projection in dimensionality reduction: Applications to image and text data", "author": ["Bingham", "Mannila", "2001] Ella Bingham", "Heikki Mannila"], "venue": "In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201901,", "citeRegEx": "Bingham et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bingham et al\\.", "year": 2001}, {"title": "Normalized (pointwise) mutual information in collocation extraction", "author": ["Gerlof Bouma"], "venue": "Proceedings of the Biennial GSCL Conference,", "citeRegEx": "Bouma. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "andMarco Baroni", "author": ["Elia Bruni", "Nam Khanh Tran"], "venue": "Multimodal distributional semantics. J. Artif. Int. Res., 49(1):1\u201347, January", "citeRegEx": "Bruni et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Correlation: Parametric and Nonparametric Measures", "author": ["Peter Y. Chen", "Paula M. Popovich"], "venue": "Quantitative Applications in the Social Sciences. Sage Publications,", "citeRegEx": "Chen and Popovich. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "In WWW \u201913", "author": ["Nilesh Dalvi", "Vibhor Rastogi", "Anirban Dasgupta", "Anish Das Sarma", "Tamas Sarlos. Optimal hashing schemes for entity matching"], "venue": "pages 295\u2013306, New York, NY, USA,", "citeRegEx": "Dalvi et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "JASIS", "author": ["Scott C. Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman. Indexing by latent semantic analysis"], "venue": "41(6):391\u2013407,", "citeRegEx": "Deerwester et al.. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "In NAACL-HLT \u201915", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy", "Noah A. Smith. Retrofitting word vectors to semantic lexicons"], "venue": "pages 1606\u20131615,", "citeRegEx": "Faruqui et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Placing search in context: the concept revisited", "author": ["Lev Finkelstein", "Gabrilovich Evgenly", "Matias Yossi", "Rivlin Ehud", "Solan Zach", "Wolfman Gadi", "Ruppin Eytan"], "venue": "WWW \u201910,", "citeRegEx": "Finkelstein et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Technical report", "author": ["W. Nelson Francis", "Henry Kucera. Brown corpus manual"], "venue": "Brown University,", "citeRegEx": "Francis and Kucera. 1979", "shortCiteRegEx": null, "year": 1979}, {"title": "PPDB: The paraphrase", "author": ["Ganitkevitch et al", "2013] Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Topsig: Topology preserving document signatures", "author": ["Shlomo Geva", "Christopher M. De Vries"], "venue": "CIKM \u201911, pages 333\u2013338, New York, NY, USA,", "citeRegEx": "Geva and De Vries. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Journal of the American Statistical Association", "author": ["Leo A. Goodman", "William H. Kruskal. Measures of association for cross classifications"], "venue": "49(268):732\u2013764,", "citeRegEx": "Goodman and Kruskal. 1954", "shortCiteRegEx": null, "year": 1954}, {"title": "In KDD \u201912", "author": ["Guy Halawi", "Gideon Dror", "Evgeniy Gabrilovich", "Yehuda Koren. Large-scale learning of word relatedness with constraints"], "venue": "pages 1406\u20131414, New York, NY, USA,", "citeRegEx": "Halawi et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Word", "author": ["Zellig S Harris. Distributional structure"], "venue": "10(2-3):146\u2013162,", "citeRegEx": "Harris. 1954", "shortCiteRegEx": null, "year": 1954}, {"title": "SimLex-999: Evaluating semantic models with genuine similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "Comput. Linguist., 41(4):665\u2013 695, December", "citeRegEx": "Hill et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "pseudorandom generators", "author": ["Piotr Indyk. Stable distributions"], "venue": "embeddings and data stream computation. In 41st Annual Symposium on Foundations of Computer Science, pages 189\u2013197,", "citeRegEx": "Indyk. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "volume 26", "author": ["William Johnson", "Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. In Conference in modern analysis", "probability"], "venue": "pages 189\u2013206. AMS,", "citeRegEx": "Johnson and Lindenstrauss. 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "Random indexing of text samples for latent semantic analysis", "author": ["P. Kanerva", "J. Kristofersson", "A. Holst"], "venue": "Proceedings of the 22nd Annual Conference of the Cognitive Science Society, volume 1036", "citeRegEx": "Kanerva et al.. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Dimensionality reduction by random mapping: fast similarity computation for clustering", "author": ["S. Kaski"], "venue": "IEEE International Joint Conference on Neural Networks Proceedings, volume 1, pages 413\u2013418 vol.1, May", "citeRegEx": "Kaski. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Biometrika", "author": ["Maurice G. Kendall. A newmeasure of rank correlation"], "venue": "30(1-2):81\u201393,", "citeRegEx": "Kendall. 1938", "shortCiteRegEx": null, "year": 1938}, {"title": "volume 14", "author": ["Quoc V Le", "Tomas Mikolov. Distributed representations of sentences", "documents. In ICML"], "venue": "pages 1188\u20131196,", "citeRegEx": "Le and Mikolov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In KDD \u201906", "author": ["Ping Li", "Trevor J. Hastie", "Kenneth W. Church. Very sparse random projections"], "venue": "pages 287\u2013296, New York, NY, USA,", "citeRegEx": "Li et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "pages 104\u2013113", "author": ["Thang Luong", "Richard Socher", "Christopher D Manning. Better word representations with recursive neural networks for morphology. In CoNLL"], "venue": "ACL,", "citeRegEx": "Luong et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Detecting near-duplicates for web", "author": ["Manku et al", "2007] Gurmeet Singh Manku", "Arvind Jain", "Anish Das Sarma"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Technical report", "author": ["Michael. Fingerprinting by random polynomials"], "venue": "Center of Research in Computer Technology,", "citeRegEx": "Michael. 1981", "shortCiteRegEx": null, "year": 1981}, {"title": "CoRR", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean. Efficient estimation of word representations in vector space"], "venue": "abs/1301.3781,", "citeRegEx": "Mikolov et al.. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "In NIPS 26", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "Language and Cognitive Processes", "author": ["George A. Miller", "Walter G. Charles. Contextual correlates of semantic similarity"], "venue": "6(1):1\u201328,", "citeRegEx": "Miller and Charles. 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller"], "venue": "Commun. ACM, 38(11):39\u201341, November", "citeRegEx": "Miller. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "EMNLP \u201914, pages 1532\u20131543, Doha, Qatar,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Random manhattan integer indexing: Incremental l1 normed vector space construction", "author": ["Behrang Q. Zadeh", "Siegfried Handschuh"], "venue": "EMNLP, pages 1713\u20131723, Doha, Qatar,", "citeRegEx": "Q. Zadeh and Handschuh. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In NLDB", "author": ["Behrang QasemiZadeh. Random indexing revisited"], "venue": "pages 437\u2013442,", "citeRegEx": "QasemiZadeh. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A word at a time: Computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch"], "venue": "WWW \u201911, pages 337\u2013346, New York, NY, USA,", "citeRegEx": "Radinsky et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "ACM", "author": ["Herbert Rubenstein", "John B. Goodenough. Contextual correlates of synonymy. Commun"], "venue": "8(10):627\u2013633,", "citeRegEx": "Rubenstein and Goodenough. 1965", "shortCiteRegEx": null, "year": 1965}, {"title": "In LREC\u201912", "author": ["Roland Schafer", "Felix Bildhauer. Building large corpora from the web using a new efficient tool chain"], "venue": "pages 486\u2013493,", "citeRegEx": "Schafer and Bildhauer. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining the web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["Peter D. Turney"], "venue": "EMCL \u201901, pages 491\u2013502, London, UK, UK,", "citeRegEx": "Turney. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "In IJCAI\u201915", "author": ["DiWang", "Xinbo Gao", "XiumeiWang", "Lihuo He. Semantic topic multimodal hashing for crossmedia retrieval"], "venue": "pages 3890\u20133896. AAAI Press,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In GWC-06", "author": ["Dongqiang Yang", "David M.W. Powers. Verb similarity on the taxonomy of wordnet"], "venue": "Jeju Island, Korea,", "citeRegEx": "Yang and Powers. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Poisketch: Semantic place labeling over user activity streams", "author": ["Dingqi Yang", "Bin Li", "Philippe Cudr\u00e9Mauroux"], "venue": "IJCAI 2016, pages 2697\u20132703,", "citeRegEx": "Yang et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "These quantifications of words are often justified by Harris\u2019 Distributional Hypothesis [Harris, 1954].", "startOffset": 88, "endOffset": 102}, {"referenceID": 12, "context": "Once these embeddings are constructed, similarities between them are given by an appropriate correlation measure such as Goodman and Kruskal\u2019s \u03b3 coefficient [Goodman and Kruskal, 1954] (or, alternatively, Kenall\u2019s \u03c4b [Kendall, 1938]).", "startOffset": 157, "endOffset": 184}, {"referenceID": 21, "context": "Once these embeddings are constructed, similarities between them are given by an appropriate correlation measure such as Goodman and Kruskal\u2019s \u03b3 coefficient [Goodman and Kruskal, 1954] (or, alternatively, Kenall\u2019s \u03c4b [Kendall, 1938]).", "startOffset": 217, "endOffset": 232}, {"referenceID": 37, "context": ", through normalizing raw frequencies by the expected and marginal frequencies, such as the popular PPMI weighting [Turney, 2001; Bouma, 2009].", "startOffset": 115, "endOffset": 142}, {"referenceID": 2, "context": ", through normalizing raw frequencies by the expected and marginal frequencies, such as the popular PPMI weighting [Turney, 2001; Bouma, 2009].", "startOffset": 115, "endOffset": 142}, {"referenceID": 18, "context": ", the well-known lemmas proposed in [Johnson and Lindenstrauss, 1984] (for l2normed spaces), [Indyk, 2000] (for l1-normed spaces), and their subsequent refinements and generalizations such as [Li et al.", "startOffset": 36, "endOffset": 69}, {"referenceID": 17, "context": ", the well-known lemmas proposed in [Johnson and Lindenstrauss, 1984] (for l2normed spaces), [Indyk, 2000] (for l1-normed spaces), and their subsequent refinements and generalizations such as [Li et al.", "startOffset": 93, "endOffset": 106}, {"referenceID": 23, "context": ", the well-known lemmas proposed in [Johnson and Lindenstrauss, 1984] (for l2normed spaces), [Indyk, 2000] (for l1-normed spaces), and their subsequent refinements and generalizations such as [Li et al., 2006].", "startOffset": 192, "endOffset": 209}, {"referenceID": 5, "context": ", 2007] and string matching [Dalvi et al., 2013; Michael, 1981], semantic labeling [Yang et al.", "startOffset": 28, "endOffset": 63}, {"referenceID": 26, "context": ", 2007] and string matching [Dalvi et al., 2013; Michael, 1981], semantic labeling [Yang et al.", "startOffset": 28, "endOffset": 63}, {"referenceID": 40, "context": ", 2013; Michael, 1981], semantic labeling [Yang et al., 2016], and cross-media retrieval [Wang et al.", "startOffset": 42, "endOffset": 61}, {"referenceID": 38, "context": ", 2016], and cross-media retrieval [Wang et al., 2015] to name a few.", "startOffset": 35, "endOffset": 54}, {"referenceID": 20, "context": "Naturally, these methods have been also applied to the problem of word embedding, either as a dimensionality reduction method [Bingham and Mannila, 2001; Kaski, 1998], or in the form of an incremental vector space construction technique in which the random projection-based dimensionality reduction process is merged with the process of collecting word co-occurrence frequencies, e.", "startOffset": 126, "endOffset": 166}, {"referenceID": 19, "context": ", [Kanerva et al., 2000; Geva and De Vries, 2011; Q. Zadeh and Handschuh, 2014].", "startOffset": 2, "endOffset": 79}, {"referenceID": 11, "context": ", [Kanerva et al., 2000; Geva and De Vries, 2011; Q. Zadeh and Handschuh, 2014].", "startOffset": 2, "endOffset": 79}, {"referenceID": 32, "context": ", [Kanerva et al., 2000; Geva and De Vries, 2011; Q. Zadeh and Handschuh, 2014].", "startOffset": 2, "endOffset": 79}, {"referenceID": 33, "context": "employed sparse Gaussian random projections [QasemiZadeh, 2015] to build word vectors directly at a reduced dimensionality and showed that their proposed \u2018random indexing\u2019 technique yields results comparable to the latent semantic analysis technique of [Deerwester et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 6, "context": "employed sparse Gaussian random projections [QasemiZadeh, 2015] to build word vectors directly at a reduced dimensionality and showed that their proposed \u2018random indexing\u2019 technique yields results comparable to the latent semantic analysis technique of [Deerwester et al., 1990], which employs truncation using singular value decomposition for dimensionality reduction.", "startOffset": 253, "endOffset": 278}, {"referenceID": 28, "context": ", word2vec [Mikolov et al., 2013b] and GloVe [Pennington et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 31, "context": ", 2013b] and GloVe [Pennington et al., 2014].", "startOffset": 19, "endOffset": 44}, {"referenceID": 8, "context": "Relatedness tests in our experiments are WS353 [Finkelstein et al., 2001], WSS and WSR by [Agirre et al.", "startOffset": 47, "endOffset": 73}, {"referenceID": 0, "context": ", 2001], WSS and WSR by [Agirre et al., 2009], the classic tests of MC30 [Miller and Charles, 1991] and RG65 [Rubenstein and Goodenough, 1965], the Stanford Rare Word dataset RW [Luong et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 29, "context": ", 2009], the classic tests of MC30 [Miller and Charles, 1991] and RG65 [Rubenstein and Goodenough, 1965], the Stanford Rare Word dataset RW [Luong et al.", "startOffset": 35, "endOffset": 61}, {"referenceID": 35, "context": ", 2009], the classic tests of MC30 [Miller and Charles, 1991] and RG65 [Rubenstein and Goodenough, 1965], the Stanford Rare Word dataset RW [Luong et al.", "startOffset": 71, "endOffset": 104}, {"referenceID": 24, "context": ", 2009], the classic tests of MC30 [Miller and Charles, 1991] and RG65 [Rubenstein and Goodenough, 1965], the Stanford Rare Word dataset RW [Luong et al., 2013], M287 by [Radinsky et al.", "startOffset": 140, "endOffset": 160}, {"referenceID": 34, "context": ", 2013], M287 by [Radinsky et al., 2011], M771 [Halawi et al.", "startOffset": 17, "endOffset": 40}, {"referenceID": 13, "context": ", 2011], M771 [Halawi et al., 2012], SL [Hill et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 15, "context": ", 2012], SL [Hill et al., 2015], YP130 verb relatedness [Yang and Powers, 2006], and MEN [Bruni et al.", "startOffset": 12, "endOffset": 31}, {"referenceID": 39, "context": ", 2015], YP130 verb relatedness [Yang and Powers, 2006], and MEN [Bruni et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 3, "context": ", 2015], YP130 verb relatedness [Yang and Powers, 2006], and MEN [Bruni et al., 2014].", "startOffset": 65, "endOffset": 85}, {"referenceID": 28, "context": "Baselines: For this input, as one baseline, we train a word2vec CBOW model [Mikolov et al., 2013b], one of the most popular word embedding techniques.", "startOffset": 75, "endOffset": 98}, {"referenceID": 0, "context": "Overall, when it comes to choosing a neural embedding learning method versus our method for addressing a semantic similarity task, our experiments suggest that the hash-based method is more suitable for similarity identification than relatedness (in the sense that described in [Agirre et al., 2009]), and for tasks that involve comparison of frequent and infrequent words (due to the nature of PPMI weighting).", "startOffset": 278, "endOffset": 299}, {"referenceID": 37, "context": ", as explained in [Turney, 2001] by querying a Web search engine.", "startOffset": 18, "endOffset": 32}, {"referenceID": 36, "context": "To show the effect of enlarging the size of input corpus on the method\u2019s performance, in addition to the Wikipedia corpus that we used initially, we feed another 4 billion tokens of web crawled text data [Schafer and Bildhauer, 2012] to our models.", "startOffset": 204, "endOffset": 233}, {"referenceID": 7, "context": "Apart from methods discussed above, \u2018retrofitting\u2019 (refining word embeddings using lexical relations available in lexical resources [Faruqui et al., 2015]) is another methodology for improving results.", "startOffset": 132, "endOffset": 154}, {"referenceID": 30, "context": "To show the impact of this \u2018retrofitting\u2019 method, we take vectors of dimensionality m = 500 from our earlier experiment, and update them by reading WordNet [Miller, 1995] and PPDB [Ganitkevitch et al.", "startOffset": 156, "endOffset": 170}, {"referenceID": 7, "context": "As discussed in [Faruqui et al., 2015], encoding this knowledge into neural embeddingmodels is not straightforward, and it demands certain modification in their learning algorithm.", "startOffset": 16, "endOffset": 38}, {"referenceID": 27, "context": "Likewise relatedness tasks, the word2vec family of embeddings methods [Mikolov et al., 2013a; Le and Mikolov, 2014] are a popular choice for this pre-training phase.", "startOffset": 70, "endOffset": 115}, {"referenceID": 22, "context": "Likewise relatedness tasks, the word2vec family of embeddings methods [Mikolov et al., 2013a; Le and Mikolov, 2014] are a popular choice for this pre-training phase.", "startOffset": 70, "endOffset": 115}, {"referenceID": 16, "context": "We witness that vectors produced using our hash-based technique are more suitable than word2vec embeddings, at least, in our part-of-speech tagging experiment using Long Short Term Memory (LSTM) units [Hochreiter and Schmidhuber, 1997].", "startOffset": 201, "endOffset": 235}, {"referenceID": 9, "context": "For evaluation, we use the tagged version of the Brown corpus [Francis and Kucera, 1979], which has 57,067 sentences and 313 different tags.", "startOffset": 62, "endOffset": 88}], "year": 2017, "abstractText": "We propose a new fast word embedding technique using hash functions. The method is a derandomization of a new type of random projections: By disregarding the classic constraint used in designing random projections (i.e., preserving pairwise distances in a particular normed space), our solution exploits extremely sparse non-negative random projections. Our experiments show that the proposed method can achieve competitive results, comparable to neural embedding learning techniques, however, with only a fraction of the computational complexity of these methods. While the proposed derandomization enhances the computational and space complexity of our method, the possibility of applying weighting methods such as positive pointwise mutual information (PPMI) to our models after their construction (and at a reduced dimensionality) imparts a high discriminatory power to the resulting embeddings. Obviously, this method comes with other known benefits of random projection-based techniques such as ease of update.", "creator": "LaTeX with hyperref package"}}}