{"id": "1705.07149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Local Information with Feedback Perturbation Suffices for Dictionary Learning in Neural Circuits", "abstract": "While entire sparsely html principle why quickly designing information food into sensory genetic systems, it remains unclear how learning therefore done technically under neural heritage rules. Feasible mind regarding whether useful wholly off synaptically main information in these turn nor implemented on regularized distributed cells. We understand into helical independent with bursting neurons that these suggestions now aforementioned fundamental appeal and solve several L1 - maximising sanskrit learning yet, representing place first model wo to does thought. Our part innovation is to introduce feedback activating to its part pathway allow turn saw existence non - local press into among ones. The resulting connections creb its incomplete signal supposed to learning both there change of network slightly would problems by audio, and commuter akin which soon classical discrete gradient descent experiment.", "histories": [["v1", "Fri, 19 May 2017 19:06:27 GMT  (725kb,D)", "http://arxiv.org/abs/1705.07149v1", "10 pages, 4 figures"]], "COMMENTS": "10 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["tsung-han lin"], "accepted": false, "id": "1705.07149"}, "pdf": {"name": "1705.07149.pdf", "metadata": {"source": "CRF", "title": "Local Information with Feedback Perturbation Suffices for Dictionary Learning in Neural Circuits", "authors": ["Tsung-Han Lin"], "emails": ["tsung-han.lin@intel.com"], "sections": [{"heading": "1 Introduction", "text": "A spiking neural network (SNN) is a computational model with simple neurons as the basic processing units. Different from artificial neural networks, SNNs incorporate the time dimension into computations. The network of neurons operates according to a global reference clock; at a time instance, one or more neurons may send out a 1-bit impulse, the spike, to neighbors through directed connectivities, known as synapses. Neurons form a dynamical system with local state variables and rules that determine when a neuron transmits a spike. The spike rate of a neuron can encode its activation value, borrowing the terminology from artificial neural networks.\nSNNs can exploit the temporal ordering of spikes to obtain high computational efficiency, despite that encoding real values as spike rates may appear quite inefficient comparing to the compact binary representations. Consider, for example, a set of competing neurons recurrently connected with inhibitory synapses in Figure 1(a). The winner neuron that has the largest external input will fire at the earliest time, and immediately inhibit the activities of other neurons. This inhibition happens with only a single one-to-many spike communication, in contrast to the all-to-all state exchange and comparison required when neurons only maintain graded activation values. Using the above principle, one can show that a SNN can be configured to efficiently solve the well-known `1-minimizing sparse approximation problem [17, 18], which is to determine a sparse subset of features from a feature dictionary to represent a given input, and the features can be viewed as competing neurons that seek to form the best fit of the input data [16].\nIn this work, we further show that the related dictionary learning problem can be solved in a SNN as well. Dictionary learning was first proposed to model mammalian visual cortex [15], and later found numerous applications in image processing and machine learning [11]. Despite its popularity, it remains unclear how the problem can be solved in a neural architecture. None of the existing learning algorithms are synaptically local: the adaptation of synaptic weights relies on the receptive field information of other neurons, making it impossible to be implemented in a spatially distributed network. As a result, many researchers turn to other less straightforward objective function formulations (e.g., minimizing over long-term average neuron activities [23], or maximizing input-output similarity [9]), or are forced to take approximate gradient directions at the cost of suboptimal results (e.g., simplifying the learning rules to be only Hebbian [3, 19]).\nar X\niv :1\n70 5.\n07 14\n9v 1\n[ cs\n.L G\n] 1\n9 M\nay 2\nWe solve the dictionary learning problem by introducing feedback synapses. We show that the feedback connections can cause the network steady states to change by an amount identical to the error signal needed for learning, provided that the network synaptic weights satisfy a weight consistency condition. Built on this observation, we develop learning mechanisms that closely resemble the classical stochastic gradient descent, and can perform dictionary learning from a spiking network with randomly initialized synaptic weights."}, {"heading": "2 Integrate-and-Fire Neuron Model", "text": "We first consider a network of N simple integrate-and-fire neurons. Each neuron-i, for i = 1, 2, . . . , N , has two internal state variables, the soma current \u00b5i(t) and the membrane potential vi(t), that govern its dynamics. The soma current is determined by two inputs: The first is a constant current bi; the second are the spike trains of all the neighbors neuron-js to which neuron-i is connected. Each spike train is of the form \u03c3j(t) = \u2211 k \u03b4(t\u2212tj,k) where tj,k is the time of the k-th spike of neuron-j and \u03b4(t) is the Dirac delta function. The soma current \u00b5i(t) is the sum of bi and the filtered spike trains from its neighbors thus:\n\u00b5i(t) = bi + \u2211 j 6=i wij(\u03b1 \u2217 \u03c3j)(t) (2.1)\nwhere wij is the synaptic weight from neuron-j to i, and \u03b1(t) = \u03c4 \u22121 s H(t)e \u2212t/\u03c4s is the filter kernel parameterized by the synaptic decay time constant \u03c4s; H(t) is the Heaviside function that is 1 when t \u2265 0 and 0 elsewhere. Note that in a neural architecture, synaptic weights are stored, and hence only available, at the destination neuron. This property is referred to as synaptically local, and constitutes the major challenge for dictionary learning in SNNs.\nThe soma current is converted to the output spiking activities through the dynamics of membrane potential. This membrane potential is a simple linear integration of the soma current before it reaches the firing threshold.\nvi(t) = \u222b t ti,k \u00b5i(s)ds (2.2)\nA spike is generated when the membrane potential exceeds its firing threshold \u03b8i; at this time, the neuron also immediately resets vi to 0.\nThe network of neurons forms a dynamical system where the neurons interact through spikes. An important quantity \u2206i, called the imbalance function, is useful in characterizing the steady states of the system, defined for t > 0 as follows,\n\u2206i(t) def = ui(t)\u2212 \u03b8iai(t) (2.3)\nwhere u(t) and a(t) are the average soma current and average spike rate, respectively,\nui(t) def =\n1\nt \u222b t 0 \u00b5i(s)ds, ai(t) def = 1 t \u222b t 0 \u03c3i(s)ds (2.4)\nThe imbalance function measures the difference between the average amount of charges accumulated to the membrane potential, equal to ui(t), and the average amount of charges released through spiking, equal to \u03b8iai(t).\nIf the average current converges to a fixed point, one can show that as t \u2192 \u221e, the imbalance converges towards satisfying the following equilibrium condition [18],{\nlimt\u2192\u221e\u2206i(t) = 0, if limt\u2192\u221e ai(t) > 0 limt\u2192\u221e\u2206i(t) \u2264 0, if limt\u2192\u221e ai(t) = 0\n(2.5)\nThis result simply suggests that for neurons that have nonzero spike rates at equilibrium, their average outgoing charges must equal the average incoming charges, meaning the imbalance must be 0. On the contrary, if a neuron stops spiking at equilibrium, then the imbalance may either be zero, if it has zero net incoming charges, or a negative value, if it receives more inhibition than excitation. We note that the convergence property of the dynamical system deserves a rigorous treatment (e.g., see [17, 18]), although it is not the main focus of this work."}, {"heading": "3 Nonnegative Sparse Coding", "text": "Solving the sparse coding problem under a given dictionary constitutes an important part of our learning scheme. In this section, we revisit prior results on solving this problem in a SNN [17, 18], and we will focus on using nonnegative dictionaries.\nConsider the network topology in Figure 1(a). Each neuron-i receives an input current bi, and has incoming synapses with weight wij from the other (N \u2212 1) neurons. Suppose that the synapses are all inhibitory, that is, wij \u2264 0, and hence none of the neurons can spike arbitrarily fast. Using (2.5), the equilibrium spike rates a\u2217i = limt\u2192\u221e ai(t) must satisfy {\nbi + \u2211 j 6=i wija \u2217 j \u2212 \u03b8ia\u2217i = 0, if a\u2217i > 0\nbi + \u2211 j 6=i wija \u2217 j \u2264 0, if a\u2217i = 0\n(3.1)\nThe above result makes use of the property that the average current will converge to limt\u2192\u221e ui(t) = bi +\u2211 j 6=i wija \u2217 i .\nThe steady-state condition, (3.1), has connections to the nonnegative sparse coding problem,\na\u2217 = arg min a\u22650\n1 2 \u2016x\u2212Da\u201622 + \u03bb \u2211 j (\u2211 i d2ij ) aj (3.2)\nwhere x \u2208 RM is a data sample, D \u2208 RM\u00d7N is a dictionary, and \u03bb \u2265 0 is the sparse regularization parameter. To see the connection, let G = DTD and gij be its (i, j)-th entry. The necessary and sufficient optimality condition for (3.2) is { \u2211\nk dkixk \u2212 \u03bbgii \u2212 \u2211 j 6=i gija \u2217 j \u2212 giia\u2217i = 0, if a\u2217i > 0\u2211\nk dkixk \u2212 \u03bbgii \u2212 \u2211 j 6=i gija \u2217 j \u2264 0, if a\u2217i = 0\n(3.3)\nNote the similarity between (3.1) and (3.3). The correspondence can be established by setting bi = \u2211\nk dkixk \u2212 \u03bbgii, wij = \u2212gij , \u03b8i = gii (3.4)\nIndeed, previous work has established that a spiking network configured as above will converge to an equilibrium spike rate identical to the solution of (3.2). Note that the dictionary is not encoded explicitly in the network. The input current is configured according to the dictionary projection of the input data, and the synaptic weights represent the correlations between columns of the dictionary."}, {"heading": "4 Online Dictionary Learning", "text": "We are interested in learning a nonnegative dictionary from P nonnegative training data samples, x1,x2, . . . ,xP . The dictionary learning problem is commonly formulated as,\nmin D\u22650,\u2016dj\u201622=1,ai\u22650\n1\nP \u2211P i=1 L (xi,D,ai) , L (x,D,a) = 1 2 \u2016x\u2212Da\u201622 + \u03bb \u2016a\u20161 (4.1)\nwhere ai is the sparse representation of data xi. The number of columns, or atoms, in the dictionary, N , is a predetermined hyper-parameter. This optimization problem seeks the best performing dictionary for all data samples, minimizing the sum of all sparse coding losses."}, {"heading": "4.1 A Two-Layer Network for Dictionary Learning", "text": "Consider the network topology in Figure 1(b) that consists of two layers of neurons, an input layer of M neurons at the bottom and a sparse-code layer of N neurons on top. There are four groups of synaptic weights: the excitatory feedforward and feedback synapses, F \u2208 RN\u00d7M , \u03b3B \u2208 RM\u00d7N where \u03b3 is a scalar in [0,1), and F,B \u2265 0; the inhibitory lateral and bias synapses, W \u2208 RN\u00d7N , L \u2208 RN\u00d71, and W,L \u2264 0.\nIn the case of \u03b3 = 0, Figure 1(b) is an instantiation of Figure 1(a), where the constant current inputs are replaced by spike trains of identical averages. To see this, note that the feedback synapses are removed in this setting, and the input and bias neurons will spike at a constant rate x1, x2, . . . , xM and \u03bb, as they are only driven by constant external inputs. We can similarly establish the correspondence between the equilibrium at the sparse-code layer, as in (3.1), and the optimality condition for sparse coding in (3.3), by configuring the network as follows,\nfij = dji, wij = \u2212gij , \u03b8i = gii, li = \u2212\u03b8i (4.2)\n\u03b8i being the firing threshold of neuron-i in the sparse-code layer. From (4.2), we can see that dictionary learning in this network means adapting the feedforward weights towards the optimal dictionary, and the lateral weights towards the correlations between the optimal dictionary atoms. In addition, learning proceeds in an online manner, where data samples are given sequentially by swapping inputs, and the dictionary is updated as soon as a new data sample is available.\nWe derive learning mechanisms that resemble the classical online stochastic gradient descent [11, Sec 5.5], consisting of two iterative steps. The first step computes the optimal sparse code with respect to the current dictionary, and the second step updates the dictionary by estimating the gradient from a single training sample, giving the following update sequence\nDi+1 = \u03a0C [Di \u2212 \u03b7\u2207DL (xi,Di,a\u2217i )] = \u03a0C [ Di \u2212 \u03b7 (Dia\u2217i \u2212 xi)a\u2217Ti ] (4.3) where the projection operator \u03a0C projects to the positive quadrant and renormalizes each atom in the updated dictionary, and \u03b7 is the learning rate.\nWe operate the spiking network with two stages to mimic the above two iterative steps. In the first stage, called the feedforward stage, we set \u03b3 = 0 and feed the training sample xi to the input layer neurons. From the discussions above, the optimal sparse code can be found as the equilibrium spike rates at the sparse-code layer. The main challenges lie in the second stage where the dictionary needs to be updated using synaptically local mechanisms, whereas the information needed appears to be non-local for the following two reasons: 1) Reconstruction error cannot be locally computed at the sparse-code layer. The gradient consists of a reconstruction error term, which is crucial to determining the best way to adapt the dictionary. Unfortunately, computing Da\u2217 requires the full knowledge of D, but only one column of the dictionary is local to a sparsecode neuron. 2) Atom correlations are non-local to compute. The lateral synaptic weights should be updated to capture the new correlations between the updated atoms. Again, computing a correlation requires knowledge of two atoms, while only one of them is accessible by a sparse-code neuron. In the next section, we show how feedback synapses can be exploited to address these two fundamental challenges."}, {"heading": "4.2 Synaptically Local Learning", "text": "Reconstruction with Feedback Synapses. In the second stage of learning, called feedback stage, we set \u03b3 to a nonzero value to engage the feedback synapses, moving the network towards a new steady state. Interestingly, there exists a condition that if satisfied, engaging the feedback synapses will only perturb the equilibrium spike rates at the input layer, while leaving the sparse-code layer untouched. We call this condition\nfeedback consistency,\nFB = H, where H =  \u03b81 \u2212w12 . . . \u2212w1N \u2212w21 \u03b82 \u2212w2N ... . . .\n\u2212wN1 \u2212wN2 \u03b8N  (4.4) Note that H is composed of the lateral weights wij and firing thresholds \u03b8i.\nTo see this, let y \u2208 RM , z \u2208 RN be the equilibrium spike rates at the input and sparse-code layer, respectively. The equilibrium spike rates at the input layer can be easily derived. Given that the input neurons do not receive any inhibition, their imbalance functions must be zero at equilibrium, and hence their spike rates are,\ny(1) = x y(2) = (1\u2212 \u03b3)x + \u03b3Bz(2) (4.5)\nwith superscripts denoting the particular learning stage that the equilibrium spike rates belong to. For the sparse-code layer, note that the equilibrium spike rate must satisfy the equilibrium condition in (2.5). This allows us to examine the relationships between z(1) and z(2). Let e \u2208 RN be the imbalance of the sparse-code layer neurons at equilibrium, ei = limt\u2192\u221e\u2206i(t), we can write the imbalance during the feedforward and feedback stage at equilibrium as\ne(1) = Fx + \u03bbL\u2212Hz(1) (4.6)\ne(2) = Fy(2) + (1\u2212 \u03b3)\u03bbL\u2212Hz(2) (4.7)\nSubstituting (4.5) into (4.7),\ne(2) = (1\u2212 \u03b3)Fx + \u03b3FBz(2) + (1\u2212 \u03b3)\u03bbL\u2212Hz(2) (4.8)\nNow, suppose that the feedback weights satisfy feedback consistency, FB = H, e(2) can be further reduced, e(2) = (1\u2212 \u03b3) ( Fx + \u03bbL\u2212Hz(2) ) (4.9)\nNote the similarity between (4.6) and (4.9), and 1 \u2212 \u03b3 > 0. This suggests that a feasible z(1) that satisfies (2.5) must also be a feasible z(2), and vice versa. In other words, if the feedforward-only network possesses a unique equilibrium spike rate, then the sparse-code layer spike rates must remain unaltered between the two learning stages, z(2) = z(1).\nWith this result, we turn our attention to the amount of spike rate changes at the input layer in a feedbackconsistent network.\ny(2) \u2212 y(1) = \u03b3(Bz(1) \u2212 x) (4.10)\nAs the sparse code computed in the feedforward stage is preserved in the feedback stage, the change amounts to a reconstruction error, in that the reconstruction is formed by the feedback weights B as the dictionary. Suppose for now that the feedforward and feedback weights are symmetric (except for a scalar factor \u03b3), consistent, and equal to an underlying dictionary D, that is, B = FT = D and H = DTD, which is also the ideal situation that learning should achieve. The reconstruction errors needed in the gradient calculations become locally available as the change of input layer spike rates. This leads to the following synaptically local learning rules that update the weights along the desired gradient direction in (4.3),\nfij \u2190 fij + \u03b7fz(2)i ( y (1) j \u2212 y (2) j ) \u2212 \u03bbffij\nbij \u2190 bij + \u03b7b ( y (1) i \u2212 y (2) i ) z (2) j \u2212 \u03bbbbij\n(4.11)\n\u03b7f and \u03b7b being the learning rates. Note that a weight decay term is included at the end to prevent the weights from growing too large, with \u03bbf and \u03bbb being the regularization coefficients. This is where our algorithm departs from the classical stochastic gradient descent, as renormalizing the atoms in the feedback weights is non-local. In addition, we truncate the weight values when they go below zero to ensure their nonnegativity.\nIn the case of asymmetric weights, we can still adopt the learning rules above. Initially, the weight updates may not be able to improve the dictionary, given that the reconstruction in the feedback stage is formed using a dictionary quite different from the encoding dictionary F. However, over many updates, the weights will gradually become symmetric, since the learning rules adjust both feedforward and feedback weights in the same direction, and their initial differences will diminish with the decay term. When the two weights become sufficiently aligned, the learning rules will likely find a descending direction, despite not the steepest, towards the optimal dictionary. Perfect symmetry is not necessary for learning to work.\nMaintaining Feedback Consistency. Feedback consistency is the key property behind the rationale of the learning mechanism above. Suppose that a network is initialized to be feedback-consistent, after an update to its feedforward and feedback weights, one must adjust the lateral weights and firing thresholds accordingly to restore the consistency. Unfortunately, direct computations, H \u2190 FB, are not synaptically local. The sparse-code layer neurons, who can modify H, do not have access to the feedback weights, which are local to the input layer neurons.\nTo avoid non-local computations, we instead have the sparse-code layer neurons minimize the following inconsistency loss Lc given Ps training samples, again using stochastic gradient descent\nmin H\n1\nPs Ps\u2211 i=1 Lc(H, z (2) i ) , Lc(H, z) = 1 2 \u2016(H\u2212 FB)z\u20162 (4.12)\nThe key observation is that the inconsistency loss can be measured from the difference in equilibrium spike rates of sparse-code neurons between the two learning stages. Their relationship can be easily shown by reorganizing (4.6) and (4.8),\n\u03b3(H\u2212 FB)z(2) = \u2212e(2) + (1\u2212 \u03b3)e(1) \u2212 (1\u2212 \u03b3)H ( z(2) \u2212 z(1) ) (4.13)\nWe can then derive the gradient to minimize Lc \u2207HLc(H, z(2)i ) = (H\u2212 FB)z (2) ( z(2) )T = 1\n\u03b3\n( \u2212e(2) + (1\u2212 \u03b3)e(1) \u2212 (1\u2212 \u03b3)H ( z(2) \u2212 z(1) ))( z(2) )T (4.14) Note that the gradient above can be computed with synaptically local information. Suppose for now that F and B are fixed during which the sub-problem is being solved, we then can use the rule H\u2190 H\u2212 \u03b7h\u2207HLc \u2212 \u03bbhH to update both lateral weights and firing thresholds. With a sufficiently large Ps, feedback consistency can be restored.\nWe can further relax the assumption that F and B are fixed during which H is adjusted, by using a much faster learning rate, \u03b7h \u03b7f and \u03b7h \u03b7b. In other words, F and B are approximately constant when the network is solving (4.12). All learning rules then can be activated and learn simultaneously when a new training sample is presented. The network eventually will learn an underlying dictionary D = FT \u2248 B, and the optimal lateral weights H \u2248 DTD."}, {"heading": "5 Numerical Simulations", "text": "We examined the proposed learning algorithm using three standard datasets in image processing, machine learning, and computational neuroscience. Dataset A. Randomly sampled 8\u00d7 8 patches from the grayscale Lena image to learn 256 atoms. Dataset B. 28 \u00d7 28 MNIST images [10] to learn 512 atoms. Dataset C. Randomly sampled 16\u00d716 patches from whitened natural scenes [15] to learn 1024 atoms. For Dataset A and C, the patches are further subtracted by the means, normalized, and split into positive and negative channels to create nonnegative inputs [8]. The spiking networks are ran with a time step of 1/32. For each input, the feedforward stage is ran from t = 0 to t = 20 and the feedback stage is ran from t = 20 to t = 40, and the spike rates are measured simply as the total number of spikes within the time window of 20. We deliberately chose a short time window (the spike rates only have a precision of 0.05) to demonstrate the fast convergence of spike patterns; a more accurate equilibrium spike rate may be obtained if one is willing to use a larger window\nstarting at some t > 0. The synaptic weights are randomly initialized to be asymmetric and inconsistent, with the lateral weights set to be sufficiently strong so that the spike rates will not diverge in the feedback stage. For the learning rates, we set \u03b7f = \u03b7b and \u03b7h = 32\u03b7f .\nLearning Dynamics. Figure 2 shows the spike patterns before and after learning in both layers. Before learning, we see both sparse-code and input layer neurons exhibit perturbed spike rates in the feedback stage, as predicted in the earlier section. The perturbation in sparse-code neurons is caused by the inconsistency between randomly initialized synaptic weights, while the perturbation in input neurons is additionally due to the large reconstruction errors. After learning, the spike patterns become much steadier as the network learns to maintain weight consistency and minimize reconstruction error. Figure 3 shows the scatter plot of the learned lateral weights and firing thresholds, H, versus their desired values, FB. It can be seen that after learning, the network is able to maintain feedback consistency.\nComparison with Stochastic Gradient Descent. Dictionary learning is a notorious non-convex optimization problem. Here we demonstrate the proposed algorithm can indeed find a good local minimum. We compare the convergence behavior with stochastic gradient descent (SGD) with batch size of 1, to which our algorithm closely resembles. For SGD, we use the same learning rate as the spiking network, \u03b7 = \u03b7f , and explore two nearby learning rates \u03b7 = 2\u03b7f and \u03b7 = 0.5\u03b7f . Additionally, we experiment initializing the spiking network weights to be symmetric and consistent to understand the impact of random initialization. The weight decay rates are chosen so that the firing thresholds, which correspond to the squared norms of atoms, converge\nto a dynamic equilibrium around 1 to ensure a fair comparison. For each dataset, a separate test set of 10,000 samples is extracted, whose objective function value is used as the quality measure for the learned dictionaries.\nFigure 4 shows that our SNN algorithm can obtain a solution of similar, if not better, objective function values to SGD consistently across the datasets. Surprisingly, the SNN algorithm can even reach better solutions with fewer training samples, while SGD can be stuck at a poor local minimum especially when the dictionary is large. This can be attributed to the dynamic adaptation of firing thresholds that mitigates the issue in SGD that some atoms can be rarely activated and remain unlearned. In SNN, if an atom is not activated over many training samples, its firing threshold decays, which makes it more likely to be activated for the next sample. Further, we observe that random weight initialization in SNN only causes slightly slower convergence, and eventually can find solutions of very similar objective function values."}, {"heading": "6 Discussion", "text": "Feedback Perturbation and Spike-Driven Learning. Our learning mechanism can be viewed as using the feedback connections to test the optimality of synaptic weights. As we have shown, an optimal network should receive little perturbation from feedback, and the derived learning rules correspond to local and greedy approaches to reduce the amount of drift in spike patterns. Although our learning rules are based on spike rates, this idea certainly can be realized in a spike-driven manner to enable rapid correction of network dynamics. In particular, spike timing dependent plasticity (STDP) is an ideal candidate to implement the feedforward and feedback learning rules. The learning rules in (4.11) share the same form with differential Hebbian and anti-Hebbian plasticity, whose link to STDP has been shown [20]. On the other hand, the connection between our lateral learning rule and spike ordering based learning is less clear. It can be seen that the rule is driven by shifts in postsynaptic spike rates, but a feasible mechanism to capture the exact weight dependency remains an open problem.\nIn autoencoder learning, [7, 4] similarly explored using feedback synapses for gradient computations. However, the lack of lateral connectivities in an autoencoder makes it difficult to handle potential reverberation, and time delays are needed to separate the activities of the input and sparse-code (or hidden) layers. In contrast, our learning mechanism is based on the steady states of two network configurations. This strategy is actually a form of contrastive Hebbian learning [14] in that the feedback synapses serve to bring the network from its \u201cfree state\u201d to a \u201cclamped state\u201d.\nPractical Value. The proposed algorithm shows that the dictionary learning problem can be solved with fine-grained parallelism. The synaptically local property means the computations can be fully distributed to\nindividual neurons, eliminating the bottlenecking central unit. The parallelism is best exploited by mapping the spiking network to a VLSI architecture, e.g., [13], where each neuron can be implemented as a processing element. Existing dictionary learning algorithms, e.g., [1, 12], can be accelerated by exploiting data parallelism, while it is less clear how to parallelize them within a single training sample to further reduce computation latency.\nOur learning rules can be applied to related sparse coding models, such as reweighted `1 minimization [6] and Elastic Net [22] (see [5, 18] for the respective dynamical system formulations). It can also be extended to be a parallel solver for convolutional sparse coding [21, 2]. Although the weight sharing property in a convolutional model is fundamentally \u201cnon-local\u201d, this limitation may be overcame by clever memory lookup methods, as is commonly done in the computation of convolutional neural networks."}, {"heading": "Acknowledgments", "text": "The author thanks Peter Tang, Javier Turek, Narayan Srinivasa and Stephen Tarsa for insightful discussion and feedback on the manuscript, and Hong Wang for encouragement and support."}], "references": [{"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Transactions on signal processing, 54(11):4311\u20134322", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast convolutional sparse coding", "author": ["H. Bristow", "A. Eriksson", "S. Lucey"], "venue": "CVPR, pages 391\u2013398", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear hebbian learning as a unifying principle in receptive field formation", "author": ["C.S.N. Brito", "W. Gerstner"], "venue": "PLoS Comput Biol, 12(9):1\u201324", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Mirrored stdp implements autoencoder learning in a network of spiking neurons", "author": ["K.S. Burbank"], "venue": "PLoS Comput Biol, 11(12):e1004566", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A common network architecture efficiently implements a variety of sparsity-based inference problems", "author": ["A.S. Charles", "P. Garrigues", "C.J. Rozell"], "venue": "Neural computation, 24(12):3317\u20133339", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Group sparse coding with a laplacian scale mixture prior", "author": ["P. Garrigues", "B.A. Olshausen"], "venue": "Advances in neural information processing systems, pages 676\u2013684", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning representations by recirculation", "author": ["G.E. Hinton", "J.L. McClelland"], "venue": "Neural information processing systems, pages 358\u2013366", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Non-negative matrix factorization with sparseness constraints", "author": ["P.O. Hoyer"], "venue": "Journal of machine learning research, 5(Nov):1457\u20131469", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "A hebbian/anti-hebbian network for online sparse dictionary learning derived from symmetric matrix factorization", "author": ["T. Hu", "C. Pehlevan", "D.B. Chklovskii"], "venue": "2014 48th Asilomar Conference on Signals, Systems and Computers, pages 613\u2013619. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Sparse modeling for image and vision processing", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "Foundations and Trends R  \u00a9 in Computer Graphics and Vision, 8(2-3):85\u2013283", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 689\u2013696. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["P.A. Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "J. Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura"], "venue": "A million spiking-neuron integrated circuit with a scalable communication network and interface. Science, 345(6197):668\u2013673", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Contrastive hebbian learning in the continuous hopfield model", "author": ["J.R. Movellan"], "venue": "Connectionist models: Proceedings of the 1990 summer school, pages 10\u201317", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Nature, 381:13", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Sparse coding via thresholding and local competition in neural circuits", "author": ["C.J. Rozell", "D.H. Johnson", "R.G. Baraniuk", "B.A. Olshausen"], "venue": "Neural computation, 20(10):2526\u20132563", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimal sparse approximation with integrate and fire neurons", "author": ["S. Shapero", "M. Zhu", "J. Hasler", "C. Rozell"], "venue": "International journal of neural systems, 24(05):1440001", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse coding by spiking neural networks: Convergence theory and computational results", "author": ["P.T.P. Tang", "T.-H. Lin", "M. Davies"], "venue": "ArXiv e-prints", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Unsupervised learning of an efficient short-term memory network", "author": ["P. Vertechi", "W. Brendel", "C.K. Machens"], "venue": "Advances in Neural Information Processing Systems, pages 3653\u20133661", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Spike-based learning rules and stabilization of persistent neural activity", "author": ["X. Xie", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems, pages 199\u2013208", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "CVPR, pages 2528\u20132535. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "J. Royal Statist. Soc B., 67:301\u2013320", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of v1 simple cell receptive fields", "author": ["J. Zylberberg", "J.T. Murphy", "M.R. DeWeese"], "venue": "PLoS Comput Biol, 7(10):e1002250", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 16, "context": "Using the above principle, one can show that a SNN can be configured to efficiently solve the well-known `1-minimizing sparse approximation problem [17, 18], which is to determine a sparse subset of features from a feature dictionary to represent a given input, and the features can be viewed as competing neurons that seek to form the best fit of the input data [16].", "startOffset": 148, "endOffset": 156}, {"referenceID": 17, "context": "Using the above principle, one can show that a SNN can be configured to efficiently solve the well-known `1-minimizing sparse approximation problem [17, 18], which is to determine a sparse subset of features from a feature dictionary to represent a given input, and the features can be viewed as competing neurons that seek to form the best fit of the input data [16].", "startOffset": 148, "endOffset": 156}, {"referenceID": 15, "context": "Using the above principle, one can show that a SNN can be configured to efficiently solve the well-known `1-minimizing sparse approximation problem [17, 18], which is to determine a sparse subset of features from a feature dictionary to represent a given input, and the features can be viewed as competing neurons that seek to form the best fit of the input data [16].", "startOffset": 363, "endOffset": 367}, {"referenceID": 14, "context": "Dictionary learning was first proposed to model mammalian visual cortex [15], and later found numerous applications in image processing and machine learning [11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "Dictionary learning was first proposed to model mammalian visual cortex [15], and later found numerous applications in image processing and machine learning [11].", "startOffset": 157, "endOffset": 161}, {"referenceID": 22, "context": ", minimizing over long-term average neuron activities [23], or maximizing input-output similarity [9]), or are forced to take approximate gradient directions at the cost of suboptimal results (e.", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": ", minimizing over long-term average neuron activities [23], or maximizing input-output similarity [9]), or are forced to take approximate gradient directions at the cost of suboptimal results (e.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": ", simplifying the learning rules to be only Hebbian [3, 19]).", "startOffset": 52, "endOffset": 59}, {"referenceID": 18, "context": ", simplifying the learning rules to be only Hebbian [3, 19]).", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "If the average current converges to a fixed point, one can show that as t \u2192 \u221e, the imbalance converges towards satisfying the following equilibrium condition [18], { limt\u2192\u221e\u2206i(t) = 0, if limt\u2192\u221e ai(t) > 0 limt\u2192\u221e\u2206i(t) \u2264 0, if limt\u2192\u221e ai(t) = 0 (2.", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": ", see [17, 18]), although it is not the main focus of this work.", "startOffset": 6, "endOffset": 14}, {"referenceID": 17, "context": ", see [17, 18]), although it is not the main focus of this work.", "startOffset": 6, "endOffset": 14}, {"referenceID": 16, "context": "In this section, we revisit prior results on solving this problem in a SNN [17, 18], and we will focus on using nonnegative dictionaries.", "startOffset": 75, "endOffset": 83}, {"referenceID": 17, "context": "In this section, we revisit prior results on solving this problem in a SNN [17, 18], and we will focus on using nonnegative dictionaries.", "startOffset": 75, "endOffset": 83}, {"referenceID": 9, "context": "28 \u00d7 28 MNIST images [10] to learn 512 atoms.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Randomly sampled 16\u00d716 patches from whitened natural scenes [15] to learn 1024 atoms.", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "For Dataset A and C, the patches are further subtracted by the means, normalized, and split into positive and negative channels to create nonnegative inputs [8].", "startOffset": 157, "endOffset": 160}, {"referenceID": 19, "context": "11) share the same form with differential Hebbian and anti-Hebbian plasticity, whose link to STDP has been shown [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "In autoencoder learning, [7, 4] similarly explored using feedback synapses for gradient computations.", "startOffset": 25, "endOffset": 31}, {"referenceID": 3, "context": "In autoencoder learning, [7, 4] similarly explored using feedback synapses for gradient computations.", "startOffset": 25, "endOffset": 31}, {"referenceID": 13, "context": "This strategy is actually a form of contrastive Hebbian learning [14] in that the feedback synapses serve to bring the network from its \u201cfree state\u201d to a \u201cclamped state\u201d.", "startOffset": 65, "endOffset": 69}, {"referenceID": 12, "context": ", [13], where each neuron can be implemented as a processing element.", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": ", [1, 12], can be accelerated by exploiting data parallelism, while it is less clear how to parallelize them within a single training sample to further reduce computation latency.", "startOffset": 2, "endOffset": 9}, {"referenceID": 11, "context": ", [1, 12], can be accelerated by exploiting data parallelism, while it is less clear how to parallelize them within a single training sample to further reduce computation latency.", "startOffset": 2, "endOffset": 9}, {"referenceID": 5, "context": "Our learning rules can be applied to related sparse coding models, such as reweighted `1 minimization [6] and Elastic Net [22] (see [5, 18] for the respective dynamical system formulations).", "startOffset": 102, "endOffset": 105}, {"referenceID": 21, "context": "Our learning rules can be applied to related sparse coding models, such as reweighted `1 minimization [6] and Elastic Net [22] (see [5, 18] for the respective dynamical system formulations).", "startOffset": 122, "endOffset": 126}, {"referenceID": 4, "context": "Our learning rules can be applied to related sparse coding models, such as reweighted `1 minimization [6] and Elastic Net [22] (see [5, 18] for the respective dynamical system formulations).", "startOffset": 132, "endOffset": 139}, {"referenceID": 17, "context": "Our learning rules can be applied to related sparse coding models, such as reweighted `1 minimization [6] and Elastic Net [22] (see [5, 18] for the respective dynamical system formulations).", "startOffset": 132, "endOffset": 139}, {"referenceID": 20, "context": "It can also be extended to be a parallel solver for convolutional sparse coding [21, 2].", "startOffset": 80, "endOffset": 87}, {"referenceID": 1, "context": "It can also be extended to be a parallel solver for convolutional sparse coding [21, 2].", "startOffset": 80, "endOffset": 87}], "year": 2017, "abstractText": "While the sparse coding principle can successfully model information processing in sensory neural systems, it remains unclear how learning can be accomplished under neural architectural constraints. Feasible learning rules must rely solely on synaptically local information in order to be implemented on spatially distributed neurons. We describe a neural network with spiking neurons that can address the aforementioned fundamental challenge and solve the `1-minimizing dictionary learning problem, representing the first model able to do so. Our major innovation is to introduce feedback synapses to create a pathway to turn the seemingly non-local information into local ones. The resulting network encodes the error signal needed for learning as the change of network steady states caused by feedback, and operates akin to the classical stochastic gradient descent method.", "creator": "LaTeX with hyperref package"}}}