{"id": "1705.08063", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge", "abstract": "Citation texts directly sometimes need very candid or moved some reasons inaccurate by having; much done the moreover emphasis half the appearing paper supposed reflect its parameters contributions. To review except worse, anyone require charge unsupervised newest however uses distributed singular from words as well as defines insights. extract be requiring reflects soon after explicit paper. Evaluation results talking but effectiveness of our modified have significantly stagnated also officials - mainly - the - contemporary. We determined effectiveness how an effective bestower use negative in improving citation - example summarization of seen scientific commentaries.", "histories": [["v1", "Tue, 23 May 2017 02:55:56 GMT  (106kb,D)", "http://arxiv.org/abs/1705.08063v1", "SIGIR 2017"]], "COMMENTS": "SIGIR 2017", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["arman cohan", "nazli goharian"], "accepted": false, "id": "1705.08063"}, "pdf": {"name": "1705.08063.pdf", "metadata": {"source": "META", "title": "Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge", "authors": ["Arman Cohan", "Nazli Goharian"], "emails": ["arman@ir.cs.georgetown.edu", "nazli@ir.cs.georgetown.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "KEYWORDS Text Summarization, Scienti c Text, Information Retrieval"}, {"heading": "1 INTRODUCTION", "text": "In scienti c literature, related work is often referenced along with a short textual description regarding that work which we call citation text. Citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper. Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16]).\nWhile useful, citation texts might lack the appropriate context from the reference article [4, 5, 18]. For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned. Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form. Hence, sometimes the citation text is not su ciently informative or in other cases, even inaccurate [17]. This problem is more serious in life sciences where accurate dissemination of knowledge has direct impact on human lives.\nWe present an approach for addressing such concerns by adding the appropriate context from the reference article to the citation texts. Enriching the citation texts with relevant context from the reference paper helps the reader to better understand the context for the ideas, methods or ndings stated in the citation text.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR \u201917, Shinjuku, Tokyo, Japan \u00a9 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080740\nA challenge in citation contextualization is the discourse and terminology variations between the citing and the referenced authors. Hence, traditional IR models that rely on term matching for nding the relevant information are ine ective.\nWe propose to address this challenge by a model that utilizes word embeddings and domain speci c knowledge. Speci cally, our approach is a retrieval model for nding the appropriate context of citations, aimed at capturing terminology variations and paraphrasing between the citation text and its relevant reference context.\nWe perform two sets of experiments to evaluate the performance of our system. First, we evaluate the relevance of extracted contexts intrinsically. Then we evaluate the e ect of citation contextualization on the application of scienti c summarization. Experimental results on TAC 2014 benchmark show that our approach signi cantly outperforms several strong baselines in extracting the relevant contexts. We furthermore, demonstrate that our contextualization models can enhance summarizing scienti c articles."}, {"heading": "2 CONTEXTUALIZING CITATIONS", "text": "Given a citation text, our goal is to extract the most relevant context to it in the reference article. These contexts are essentially certain textual spans within the reference article. Throughout, colloquially, we refer to the citation text as query and reference spans in the reference article as documents. Our approach extends Language Models for IR (LM) by incorporating word embeddings and domain ontology to address shortcomings of LM for this research purpose. The goal in LM is to rank a document d according to the conditional probability p(d |q) \u221d p(q |d) =\u220fqi \u2208q p(qi |d) where qi shows the tokens in the query q. Estimating p(qi |d) is often achieved by maximum likelihood estimate from term frequencies with some sort of smoothing. Using Dirichlet smoothing [21], we have:\np(qi |d) = f (qi ,d) + \u00b5 p(qi |C)\u2211\nw \u2208V f (w,d) + \u00b5 (1)\nwhere f (qi ,d) shows the frequency of term qi in document d , C is the entire collection, V is the vocabulary and \u00b5 the Dirichlet smoothing parameter. In the citation contextualization problem, (i) the target reference sentences are short documents and (ii) there exist terminology variations between the citing author and the referenced author. Hence, the citation terms usually do not appear in the documents and relying only on the frequencies of citation terms in the documents (f (qi ,d)) for estimating p(qi |d) yields an almost uniform smoothed distribution that is unable to decisively distinguish between the documents.\nEmbeddings. Distributed representation (embedding) of a word w in a eld F is a mapping w \u2192 Fn where words semantically similar to w will be ideally located close to it. Given a query q, we rank the documents d according to the following scoring function\nar X\niv :1\n70 5.\n08 06\n3v 1\n[ cs\n.C L\n] 2\n3 M\nay 2\n01 7\nwhich leverages this property:\np(qi |d) = fsem (qi ,d) + \u00b5 p(qi |C)\u2211\nw \u2208V fsem (w,d) + \u00b5 (2)\nwhere fsem is a function that measures semantic relatedness of the query term qi to the document d and is de ned as: fsem (qi ,d) =\u2211 dj \u2208d s(qi ,dj ); where dj \u2019s are document terms and s(qi ,dj ) is the relatedness between the the query term and document term which is calculated by applying a similarity function to the distributed representations of qi and dj . We use a transformation (\u03d5) of dot products between the unit vectors e(qi ) and e(dj ) corresponding to the embeddings of the terms qi and dj for the similarity function:\ns(qi ,dj ) = { \u03d5(e(qi ).e(dj )); if e(qi ).e(dj ) > \u03c4 0; otherwise\nWe rst explain the role of \u03c4 and then the reason for considering the function \u03d5 instead of raw dot product. \u03c4 is a parameter that controls the noise introduced by less similar words. Many unrelated word vectors have non-zero similarity scores and adding them up introduces noise to the model and reduces the performance. \u03c4 \u2019s function is to set the similarity between unrelated words to zero instead of a positive number. To identify an appropriate value for \u03c4 , we select a random set of words from the embedding model and calculate the average and standard deviation of pointwise absolute value of similarities between terms from these two samples. We then select the threshold \u03c4 to be two standard deviations larger than the average to only consider very high similarity values (this choice was empirically justi ed).\nExamining term similarity values between words shows that there are many terms with high similarities associated with each term and these values are not highly discriminative. We apply a transfer function \u03d5 to the dot product e(qi ).e(dj ) to dampen the e ect of less similar words. In other words, we only want highly related words to have high similarity values and similarity should quickly drop as we move to less related words. We use the logit function for \u03d5 to achieve this dampening e ect:\n\u03d5(x) = log( x 1 \u2212 x )\nFigure 1 shows this e ect. The purple line is the normalized dot product of a sample word with the most similar words in the model. As illustrated, the similarity score di erences among top words is not very discriminative. However, applying the logit function (green line) causes the less similar words to have lower similarity values to the target word.\nDomain knowledge. Successful word embedding methods have previously shown to be e ective in capturing syntactic and semantic relatedness between terms. These co-occurrence based models are data driven. On the other hand, domain ontologies and lexicons that are built by experts include some information that might not be captured by embedding methods [8]. Therefore, using domain\nknowledge can further help the embedding based retrieval model; we incorporate it in our model in the following ways: 1) Retro tting: Faruqui et al. [6] proposed a model that uses the constraints on WordNet lexicon to modify the word vectors and pull synonymous words closer to each other. To inject the domain knowledge in the embeddings, we apply this model on two domain speci c ontologies, namely, Mesh and Protein Ontologies (PO)1. We chose these two biomedical domain ontologies because they are in the same domain as the articles in the TAC dataset. Mesh is a broad ontology that consists of biomedical terms and PO is a more focused ontology related to biology of proteins and genes. 2) Interpolating in the LM: We also directly incorporate the domain knowledge in the retrieval model; we modify the LM into the following interpolated LM with parameter \u03bb: p(qi |d) = \u03bbp1(qi |d) + (1 \u2212 \u03bb)p2(qi |d) where p1 is estimated using Eq. 2 and p2 is similar to p1 except that we replace fsem with the function font which considers domain ontology in calculating similarities:\nfont (qi ,d)= \u2211 dj \u2208d s2(qi ,dj ); s2(qi ,dj )= { 1, if qi=dj \u03b3 , if qi\u2248dj 0, o.w.\nwhere \u03b3 \u2208 [0, 1] is a parameter and qi \u2248 dj shows that there is an is-synonym relation in ontology between qi and dj 2."}, {"heading": "3 EXPERIMENTS", "text": "Data. We use the TAC 2014 Biomedical Summarization benchmark3. This dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by 4 experts.\nBaselines. To our knowledge, the only published results on TAC 2014 is [4], where the authors utilized query reformulation (QR) based on UMLS ontology. In addition to [4], we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM : Vector Space Model that was used in [4]; 3) DESM : Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10]. All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.\nOur methods. We rst report results based on training the embeddings on Wikipedia (WEWiki). Since TAC dataset is in biomedical domain, many of the biomedical terms might be either outof-vocabulary or not captured in the correct context using general embeddings, therefore we also train biomedical embeddings (WEBio)4. In addition, we report results for biomedical embeddings with retro tting (WEBio+rtrft), as well as interpolating domain knowledge (WEBio+dmn)"}, {"heading": "3.1 Intrinsic Evaluation", "text": "First, we analyze the e ectiveness of our proposed approach for contextualization intrinsically. That is, we evaluate the quality of the 1https://www.nlm.nih.gov/mesh/; http://pir.georgetown.edu/pro/ 2The values of the parameters \u03b3 and \u03bb were selected empirically by grid search 3http://www.nist.gov/tac/2014/BiomedSumm/ 4We train biomedical embeddings on TREC Genomics 2004 and 2006 collections (both Wikipedia and Genomics embeddings were trained using gensim implementation of Word2Vec, negative sampling, window size of 5 and 300 dimensions.\nextracted citation contexts using our contextualization methods in terms of how accurate they are with respect to human annotations.\nEvaluation. We consider the following evaluation metrics for assessing the quality of the retrieved contexts for each citation from multiple aspects: (i) Character o set overlaps of the retrieved contexts with human annotations in terms of precision (c-P), recall (c-R) and F-score (c-F). These are the recommended metrics for the task per TAC5. (ii) nDCG: we treat any partial overlaps with the gold standard as a correct context and then calculate the nDCG scores. (iii) Rouge-N scores: To also consider the content similarity of the retrieved contexts with the gold standard, we calculate the Rouge scores between them. (iv) Character precision at K (c-P@K): Since we are usually interested in the top retrieved spans, we consider character o set precision only for the top K spans and we denote it with \u201cc-P@K\u201d.\nResults. The results of intrinsic evaluation of contextualization are presented in Table 1. Our models (last 4 rows of table 1) achieve signi cant improvements over the baselines consistently across most of the metrics. This shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines. The best baseline performance is the query reformulation (QR) method by [4] which improves over other baselines.\nWe observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WEwiki and QR in the Table). However, using the domain speci c embeddings (WEBio ) results in 10% c-F improvement over the best baseline. This is expected since word relations in the biomedical context are better captured with biomedical embeddings. In Table 2 an illustrative word \u201cexpression\u201d gives better intuition why is that the case. As shown, using general embeddings (left column in the table), the most similar words to \u201cexpression\u201d are those related to 5https://tac.nist.gov/2014/BiomedSumm/guidelines.html\nthe general meaning of it. However, many of these related words are not relevant in the biomedical context. In the biomedical context, \u201cexpression\u201d refers to \u201cthe appearance in a phenotype attributed to a particular gene\u201d. As shown on the right column, the domain speci c embeddings (Bio) trained on genomics data are able to capture this meaning. This further con rms the inferior performance of the out-of-domain word embeddings in capturing correct word-level semantics [13]. Last two rows in Table 1 show incorporating the domain knowledge in the model which results in signi cant improvement over the best baseline in terms of most metrics (e.g. 14% and 16% c-F improvements). This shows that domain ontologies provide additional information that the domain trained embeddings may not contain. While both our methods of incorporating domain ontologies prove to be e ective, interpolating domain knowledge directly (WEBio+dmn) has the edge over retro tting (WEBio+rtrft). This is likely due to the direct e ect of ontology on the interpolated language model, whereas in retro tting, the ontology rst a ects the embeddings and then the context extraction model.\nTo analyze the performance of our system more closely, we took the context identi ed by 1 annotator as the candidate and the other 3 as gold standard and evaluated the precision to obtain an estimate of human performance on each citation. We then divided the citations based on human performance to 4 groups by quartiles. Table 3 shows our system\u2019s performance on each of these groups. We observe that, when human precision is higher (upper quartiles in the table), our system also performs better and with more con dence (lower std). Therefore, the system errors correlate well with human disagreement on the correct context for the citations. Averaged over the 4 annotators for each citation, the mean precision was 56.7% (note that this translates to our c-P@1 metric). In Table 1, we observe that our best method (c-P@1 of 56.1%) is comparable with average human precision score (c-P@1 of 56.7%) which further demonstrates the e ectiveness of our model."}, {"heading": "3.2 External evaluation", "text": "Citation-based summarization can e ectively capture various contributions and aspects of the paper by utilizing citation texts [15]. However; as argued in section 1, citation texts do not always accurately re ect the original paper. We show how adding context from the original paper can address this concern, while keeping the bene ts of citation-based summarization. Speci cally, we compare how using no contextualization, versus various proposed contextualization approaches a ect the quality of summarization. We apply the following well-known summarization algorithms on the set of citation texts, and the retrieved citation-contexts: LexRank, LSAbased, SumBasic, and KL-Divergence (For space constraints, we will not explain these approaches here; refer to [14] for details). We then compare the e ect of our proposed contextualization methods using the standard Rouge-N summarization evaluation metrics.\nResults. The results of external evaluation are illustrated in Table 4. The rst row (\u201cNo context\u201d) shows the performance of each\nsummarization approach solely on the citations without any contextualization. The next 5 rows show the baselines and last 4 rows are our proposed contextualization methods. As shown, e ective contextualization positively impacts the generated summaries. For example, our best method is \u201cWEBio + dmn\u201d which signi cantly improves the quality of generated summaries in terms of Rouge over the ones without any context. We observe that two low-performing baseline methods for contextualization according to Table 1 (\u201cVSM\u201d and \u201cBM25\u201d) also do not result in any improvements for summarization. Therefore, the intrinsic quality of citation contextualization has direct impact on the quality of generated summaries. These results further demonstrate that e ective contextualization is helpful for scienti c citation-based summarization."}, {"heading": "4 RELATEDWORK", "text": "Related work has mostly focused on extracting the citation text in the citing article (e.g. [1]). In this work, given the citation texts, we focus on extracting its relevant context from the reference paper. Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20]. Our proposed model utilizes word embeddings and the domain knowledge. Embeddings have been recently used in general information retrieval models. Vuli\u0107 and Moens [19] proposed an architecture for learning word embeddings in multilingual settings and used them in document and query representation. Mitra et al. [12] proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms. Ganguly et al. [7] used embeddings to transform term weights in a translation model for retrieval. Their model uses embeddings to expand documents and use co-occurrences for estimation. Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model. The most relevant prior work to ours is [4] where the authors approached the problem using a vector space model similarity ranking and query reformulations."}, {"heading": "5 CONCLUSIONS", "text": "Citation texts are textual spans in a citing article that explain certain contributions of a reference paper. We presented an e ective model for contextualizing citation texts (associating them with the\nappropriate context from the reference paper). We obtained statistically signi cant improvements in multiple evaluation metrics over several strong baseline, and we matched the human annotators precision. We showed that incorporating embeddings and domain knowledge in the language modeling based retrieval is e ective for situations where there are high terminology variations between the source and the target (such as citations and their reference context). Citation contextualization not only can help the readers to better understand the citation texts but also as we demonstrated, they can improve other downstream applications such as scienti c document summarization. Overall, our results show that citation contextualization enables us to take advantage of the bene ts of citation texts, while ensuring accurate dissemination of the claims, ideas and ndings of the original referenced paper."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank the three anonymous reviewers for their helpful comments and suggestions. This work was partially supported by National Science Foundation (NSF) through grant CNS-1204347."}], "references": [{"title": "Reference scope identi cation in citing sentences", "author": ["Amjad Abu-Jbara", "Dragomir Radev"], "venue": "In NAACL-HLT. ACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Scienti c Article Summarization Using Citation-Context and Article\u2019s Discourse Structure", "author": ["Arman Cohan", "Nazli Goharian"], "venue": "In EMNLP", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Scienti c document summarization via citation contextualization and scienti c discourse", "author": ["Arman Cohan", "Nazli Goharian"], "venue": "International Journal on Digital Libraries", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Matching Citation Text and Cited Spans in Biomedical Literature: a Search-Oriented Approach", "author": ["Arman Cohan", "Luca Soldaini", "Nazli Goharian"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Epistemic modality and knowledge attribution in scienti c discourse: A taxonomy of types and overview of features", "author": ["Anita de Waard", "Henk Pander Maat"], "venue": "In Workshop on Detecting Structure in Scholarly Discourse", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Retro tting Word Vectors to Semantic Lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Kumar Sujay Jauhar", "Chris Dyer", "Eduard Hovy", "A. Noah Smith"], "venue": "In NAACL-HLT. Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Word Embedding Based Generalized Language Model for Information Retrieval", "author": ["Debasis Ganguly", "Dwaipayan Roy", "Mandar Mitra", "Gareth J.F. Jones"], "venue": "In SIGIR", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Simlex-999: Evaluating semantic models with similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Overview of the CL-SciSumm 2016 Shared Task", "author": ["Kokil Jaidka", "Muthu Kumar Chandrasekaran", "Sajal Rustagi", "Min-Yen Kan"], "venue": "In BIRNDL@ JCDL", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "A simple enhancement for ad-hoc information retrieval via topic modelling", "author": ["Fanghong Jian", "Jimmy Xiangji Huang", "Jiashu Zhao", "Tingting He", "Po Hu"], "venue": "In SIGIR", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Generating Impact-Based Summaries for Scienti c Literature", "author": ["Qiaozhu Mei", "ChengXiang Zhai"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "A dual embedding space model for document ranking", "author": ["Bhaskar Mitra", "Eric Nalisnick", "Nick Craswell", "Rich Caruana"], "venue": "CoRR arXiv:1602.01137", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Classify or Select: Neural Architectures for Extractive Document Summarization", "author": ["Ramesh Nallapati", "Bowen Zhou", "Mingbo Ma"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "A survey of text summarization techniques", "author": ["Ani Nenkova", "Kathleen McKeown"], "venue": "In Mining text data", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Scienti c Paper Summarization Using Citation Summary Networks (COLING", "author": ["Vahed Qazvinian", "Dragomir R. Radev"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Comparing Citation Contexts for Information Retrieval", "author": ["Anna Ritchie", "Stephen Robertson", "Simone Teufel"], "venue": "In CIKM", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Identifying claimed knowledge updates in biomedical research articles", "author": ["\u00c1gnes S\u00e1ndor", "Anita De Waard"], "venue": "In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Summarizing scienti c articles: experiments with relevance and rhetorical status", "author": ["Simone Teufel", "Marc Moens"], "venue": "Computational linguistics", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Whetting the appetite of scientists: Producing summaries tailored to the citation context", "author": ["Stephen Wan", "C\u00e9cile Paris", "Robert Dale"], "venue": "JCDL", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["Chengxiang Zhai", "John La erty"], "venue": "TOIS 22,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "[2, 15, 16]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 14, "context": "[2, 15, 16]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 15, "context": "[2, 15, 16]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 3, "context": "While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18].", "startOffset": 91, "endOffset": 101}, {"referenceID": 4, "context": "While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18].", "startOffset": 91, "endOffset": 101}, {"referenceID": 17, "context": "While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18].", "startOffset": 91, "endOffset": 101}, {"referenceID": 16, "context": "the citation text is not su ciently informative or in other cases, even inaccurate [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "Using Dirichlet smoothing [21], we have:", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "On the other hand, domain ontologies and lexicons that are built by experts include some information that might not be captured by embedding methods [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "[6] proposed a model that uses the constraints on WordNet lexicon to modify the word vectors and pull synonymous words closer to each other.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "where \u03b3 \u2208 [0, 1] is a parameter and qi \u2248 dj shows that there is an is-synonym relation in ontology between qi and dj 2.", "startOffset": 10, "endOffset": 16}, {"referenceID": 3, "context": "To our knowledge, the only published results on TAC 2014 is [4], where the authors utilized query reformulation (QR) based on UMLS ontology.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "In addition to [4], we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM : Vector Space Model that was used in [4]; 3) DESM : Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10].", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "In addition to [4], we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM : Vector Space Model that was used in [4]; 3) DESM : Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10].", "startOffset": 173, "endOffset": 176}, {"referenceID": 11, "context": "In addition to [4], we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM : Vector Space Model that was used in [4]; 3) DESM : Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10].", "startOffset": 265, "endOffset": 269}, {"referenceID": 9, "context": "In addition to [4], we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM : Vector Space Model that was used in [4]; 3) DESM : Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10].", "startOffset": 401, "endOffset": 405}, {"referenceID": 3, "context": "VSM [4] 20.", "startOffset": 4, "endOffset": 7}, {"referenceID": 11, "context": "2 DESM [12] 20.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "5 LMD-LDA [10] 22.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "7 QR [4] 22.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "by [4] which improves over other baselines.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "This further con rms the inferior performance of the out-of-domain word embeddings in capturing correct word-level semantics [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "Citation-based summarization can e ectively capture various contributions and aspects of the paper by utilizing citation texts [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "based, SumBasic, and KL-Divergence (For space constraints, we will not explain these approaches here; refer to [14] for details).", "startOffset": 111, "endOffset": 115}, {"referenceID": 3, "context": "7 VSM [4] 35.", "startOffset": 6, "endOffset": 9}, {"referenceID": 11, "context": "6 DESM [12] 36.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "9 LMD-LDA [10] 38.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "9 QR [4] 39.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 2, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 8, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 10, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 14, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 19, "context": "Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20].", "startOffset": 108, "endOffset": 129}, {"referenceID": 18, "context": "Vuli\u0107 and Moens [19] proposed an architecture for learning word embeddings in multilingual settings and used them in document and query representation.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "[12] proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] used embeddings to transform term weights in a translation model for retrieval.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The most relevant prior work to ours is [4] where the authors approached the problem using a vector space model similarity ranking and query reformulations.", "startOffset": 40, "endOffset": 43}], "year": 2017, "abstractText": "Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to re ect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the e ectiveness of our model by signi cantly outperforming the state-of-the-art. We furthermore demonstrate how an e ective contextualization method results in improving citation-based summarization of the scienti c articles.", "creator": "LaTeX with hyperref package"}}}