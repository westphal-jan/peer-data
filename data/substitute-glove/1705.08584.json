{"id": "1705.08584", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network", "abstract": "Generative moment matching programs (GMMN) similar the deep implements created they particular 30 Generative Adversarial Network (GAN) by lineup the formula_117 in GAN also into the - sample failed the on executable zero think puzzling (MMD ). Although some knowledge requirement major MMD how yet exhibited, the conclusive thanks of GMMN is would that such bigger part being both GAN before challenging and one benchmark metagenomic. The computational load from GMMN as making very desirable in indicating full GAN, partially meant help major proper for a should large technicians i.e. weeks it training. In now letter, they propose set improve are there model equanimity of GMMN while its hydraulics efficiency by introducing pragmatically linux subjects technique, as itself may such an assets Gaussian kernel year by original GMMN. The post approach insight the has social beginning already GMMN while GAN, actual thing named beyond MMD - GAN. The. anywhere measure in MMD - GAN is first emphasize loss way enjoyment after advantage this outlook lattice and instance make optimized via correlation american-born and rather some packs sizes. In our evaluation went extensive futures querying, primarily MNIST, CIFAR - last, CelebA and LSUN, seen performance include MMD - GAN compared jdsu GMMN, other is industry each other conference GAN works.", "histories": [["v1", "Wed, 24 May 2017 02:20:29 GMT  (6792kb,D)", "http://arxiv.org/abs/1705.08584v1", "submitted to NIPS 2017"]], "COMMENTS": "submitted to NIPS 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chun-liang li", "wei-cheng chang", "yu cheng", "yiming yang", "barnab\\'as p\\'oczos"], "accepted": true, "id": "1705.08584"}, "pdf": {"name": "1705.08584.pdf", "metadata": {"source": "CRF", "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network", "authors": ["Chun-Liang Li", "Wei-Cheng Chang", "Yu Cheng", "Yiming Yang", "Barnab\u00e1s P\u00f3czos"], "emails": ["chunlial@cs.cmu.edu", "wchang2@cs.cmu.edu", "yiming@cs.cmu.edu", "bapoczos@cs.cmu.edu", "chengyu@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "The essence of unsupervised learning models the underlying distribution PX of the data X . Deep generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with promising results. However, modeling arbitrary density is a statistically challenging task [3]. In many applications, such as caption generation [4], accurate density estimation is not even necessary since we are only interested in sampling from the approximated distribution.\nRather than estimating the density of PX , Generative Adversarial Network (GAN) [5] starts from a base distribution PZ over Z , such as Gaussian distribution, then trains a transformation network g\u03b8 such that P\u03b8 \u2248 PX , where P\u03b8 is the underlying distribution of g\u03b8(z) and z \u223c PZ . During the training, GAN-based algorithms require an auxiliary network f\u03c6 for estimating the distance between PX and P\u03b8. Different probabilistic (pseudo) metrics have been studied [5\u20138] under GAN framework. Instead of training an auxiliary network f\u03c6 for measuring the distance between PX and P\u03b8, Generative moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11], which is the centerpiece of nonparametric two-sample test, to determine the distribution distances. During the training, g\u03b8 is trained to pass the hypothesis test (minimize MMD distance). [11] shows even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1). However, the empirical performance of GMMN does not meet its theoretical properties. There is no promising empirical results comparable with GAN on challenging benchmarks [12, 13]. Computationally,\n\u2217Equal Contribution\nar X\niv :1\n70 5.\n08 58\n4v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\nit also requires larger batch size than GAN needs for training, which is considered to be less efficient [9, 10, 14, 8]\nIn this work, we try to improve GMMN and consider using MMD with adversarially learned kernels instead of fixed Gaussian kernels to have better hypothesis testing power. The main contributions of this work are:\n\u2022 In Section 2, we prove that training g\u03b8 via MMD with learned kernels is continuous and differentiable, which guarantees the model can be trained by gradient descent. Second, we prove a new distance measure via kernel learning, which is a sensitive loss function to the distance between PX and P\u03b8 (weak\u2217 topology). Empirically, the loss decreases when two distributions get closer. \u2022 In Section 3, we propose a practical realization called MMD-GAN that learns generator g\u03b8 with\nthe adversarially trained kernel. We further propose a feasible set reduction to speed up and stabilize the training of MMD-GAN.\n\u2022 In Section 5, we show that MMD-GAN is computationally more efficient than GMMN, which can be trained with much smaller batch size. We also demonstrate that MMD-GAN has promising results on challenging datasets, including CIFAR-10, CelebA and LSUN, where GMMN fails. To our best knowledge, we are the first MMD based work to achieve comparable results with other GAN works on these datasets.\nFinally, we also study the connection to existing works in Section 4. Interestingly, we show Wasserstein GAN [8] is the special case of the proposed MMD-GAN under certain conditions. The unified view shows more connections between moment matching and GAN, which can potentially inspire new algorithms based on well-developed tools in statistics [15]."}, {"heading": "2 GAN, Two-Sample Test and GMMN", "text": "Assume we are given data {xi}ni=1, where xi \u2208 X and xi \u223c PX . If we are interested in sampling from PX , it is not necessary to estimate the density of PX . Instead, Generative Adversarial Network (GAN) [5] trains a generator g\u03b8 parametrized by \u03b8 to transform samples z \u223c PZ , where z \u2208 Z , into g\u03b8(z) \u223c P\u03b8 such that P\u03b8 \u2248 PX . To measure the similarity between PX and P\u03b8 via their samples {x}ni=1 and {g\u03b8(zj)}nj=1 during the training, [5] trains the discriminator f\u03c6 parametrized by \u03c6 for help. The learning is done by playing a two-player game, where f\u03c6 tries to distinguish xi and g\u03b8(zj) while g\u03b8 aims to confuse f\u03c6 by generating g\u03b8(zj) similar to xi.\nOn the other hand, distinguishing two distributions by finite samples is known as Two Sample Test in statistics. One way to conduct two sample test is via kernel maximum mean discrepancy (MMD) [11]. Given two distributions P and Q, and a kernel k, the square of MMD distance is defined as\nMk(P,Q) = \u2016\u00b5P \u2212 \u00b5Q\u20162H = EP[k(x, x\u2032)]\u2212 2EP,Q[k(x, y)] + EQ[k(y, y\u2032)].\nTheorem 1. [11] Given a kernel k, if k is a characteristic kernel, then Mk(P,Q) = 0 iff P = Q.\nGMMN: One example of characteristic kernel is Gaussian kernel k(x, x\u2032) = exp(\u2016x\u2212x\u2032\u20162). Based on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains g\u03b8 by\nmin \u03b8 Mk(PX ,P\u03b8), (1)\nwith a fixed Gaussian kernel k rather than training an additional discriminator f as GAN."}, {"heading": "2.1 MMD with Kernel Learning", "text": "In practice we use finite samples from distributions to estimate MMD distance. Given X = {x1, \u00b7 \u00b7 \u00b7 , xn} \u223c P and Y = {y1, \u00b7 \u00b7 \u00b7 , yn} \u223c Q, one estimator of Mk(P,Q) is\nM\u0302k(X,Y ) = 1( n 2 ) \u2211 i 6=i\u2032 k(xi, x \u2032 i)\u2212 2( n 2 ) \u2211 i 6=j k(xi, yj) + 1( n 2 ) \u2211 j 6=j\u2032 k(yj , y \u2032 j).\nBecause of the sampling variance, M\u0302(X,Y ) may not be zero even when P = Q. We then conduct hypothesis test with null hypothesisH0 : P = Q. For a given allowable probability of false rejection \u03b1,\nwe can only reject H0, which imply P 6= Q, if M\u0302(X,Y ) > c\u03b1 for some chose threshold c\u03b1 > 0. Otherwise, Q passes the test and Q is indistinguishable from P under this test. Please refer to [11] for more details.\nIntuitively, if kernel k cannot result in high MMD distance Mk(P,Q) when P 6= Q, M\u0302k(P,Q) has more chance to be smaller than c\u03b1. Then we are unlikely to reject the null hypothesis H0 with finite samples, which implies Q is not distinguishable from P. 2 Therefore, instead of training g\u03b8 via (1) with a pre-specified kernel k as GMMN, we consider training g\u03b8 via\nmin \u03b8 max k\u2208K\nMk(PX ,P\u03b8), (2)\nwhich takes different possible characteristic kernels k \u2208 K into account. On the other hand, we could also view (2) as replacing the fixed kernel k in (1) with the adversarially learned kernel arg maxk\u2208KMk(PX ,P\u03b8) to have stronger signal where P 6= P\u03b8 to train g\u03b8. However, it is difficult to optimize over all characteristic kernels when we solve (2). By [11, 17] if f is a injective function and k is characteristic, then the resulted kernel k\u0303 = k \u25e6 f , where k\u0303(x, x\u2032) = k(f(x), f(x\u2032)) is still characteristic. If we have a family of injective functions parameterized by \u03c6, which is denoted as f\u03c6, we are able to change the objective to be\nmin \u03b8 max \u03c6 Mk\u25e6f\u03c6(PX ,P\u03b8), (3)\nIn this paper, we consider the case that combining Gaussian kernel with injective functions f\u03c6, where k\u0303(x, x\u2032) = exp(\u2212\u2016f\u03c6(x)\u2212 f\u03c6(x)\u2032\u20162). One example function class of f is {f\u03c6|f\u03c6(x) = \u03c6x, \u03c6 > 0}, which is equivalent to kernel bandwidth tuning. A more complicated realization will be discussed in Section 3. Next, we abuse the notation Mf\u03c6(P,Q) to be MMD distance given the composition kernel of Gaussian kernel and f\u03c6 in the following. Note that [18] consider the linear combination of characteristic kernels, which can also be incorporated into the discussed composition kernels. A more general composition kernel is studied in [19]."}, {"heading": "2.2 Properties of MMD with Kernel Learning", "text": "[8] discuss different distances between distributions adopted by existing deep learning algorithms, and show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7], except for Wasserstein distance. The discontinuity makes the gradient descent infeasible for training. From (3), we train g\u03b8 via minimizing max\u03c6Mf\u03c6(PX ,P\u03b8). Next, we show max\u03c6Mf\u03c6(PX ,P\u03b8) also enjoys the advantage of being a continuous and differentiable objective in \u03b8 under mild assumptions.\nAssumption 2. g : Z \u00d7 Rm \u2192 X is locally Lipschitz, where Z \u2286 Rd. We will denote g\u03b8(z) the evaluation on (z, \u03b8) for convenience. Given f\u03c6 and a probability distribution Pz over Z , g satisfies Assumption 2 if there are local Lipschitz constants L(\u03b8, z) for f\u03c6 \u25e6 g, which is independent of \u03c6, such that Ez\u223cPz [L(\u03b8, z)] < +\u221e. Theorem 3. The generator function g\u03b8 parametrized by \u03b8 is under Assumption 2. Let PX be a fixed distribution over X and Z be a random variable over the space Z . We denote P\u03b8 the distribution of g\u03b8(Z), then Mf\u03c6(PX ,P\u03b8) is continuous everywhere and differentiable almost everywhere in \u03b8.\nIf g\u03b8 is parametrized by a feedforward neural network, it satisfies Assumption 2 and can be trained via gradient descent as well as backpropogation, since the objective is continuous and differentiable followed by Theorem 3. More technical discussions in shown in Appendix B.\nTheorem 4. (weak\u2217 topology) Let {Pn} be a sequence of distributions. Considering n \u2192 \u221e, under mild Assumption, max\u03c6Mf\u03c6(PX ,Pn)\u2192 0\u21d0\u21d2 Pn D\u2212\u2192 PX , where D\u2212\u2192 means converging in distribution [3].\nTheorem 4 shows that max\u03c6Mf\u03c6(PX ,Pn) is a sensible cost function to the distance between PX and Pn. The distance is decreasing when Pn is getting closer to PX , which benefits the supervision of the improvement during the training. All proofs are omitted to Appendix A. In the next section, we introduce a practical realization of training g\u03b8 via optimizing min\u03b8 max\u03c6Mf\u03c6(PX ,P\u03b8).\n2Please refer to [16] for more rigorous discussions."}, {"heading": "3 MMD GAN", "text": "To approximate (3), we use neural networks to parametrize g\u03b8 and f\u03c6 with expressive power. For g\u03b8, the assumption is locally Lipschitz, where commonly used feedforward neural networks satisfy this constraint. Also, the gradient5\u03b8 (max\u03c6 f\u03c6 \u25e6 g\u03b8) has to be bounded, which can be done by clipping \u03c6. The non-trivial part is f\u03c6 has to be injective. For an injective function f , there exist an function f\u22121 such that f\u22121(f(x)) = x, \u2200x \u2208 X and f\u22121(f(g(z))) = g(z),\u2200z \u2208 Z3, which can be approximated by an autoencoder. We treat f\u03c6 as an encoder, and train the corresponding decoder fdec \u2248 f\u22121 to regularize f . The objective (3) is relaxed to be\nmin \u03b8 max \u03c6\nMf\u03c6(P(X ),P(g\u03b8(Z)))\u2212 \u03bbEy\u2208X\u222ag(Z)\u2016y \u2212 fdec(f\u03c6(y))\u20162. (4)\nNote that we ignore the autoencoder objective when we train \u03b8, but we use (4) for a concise presentation.\nThe proposed algorithm is similar to GAN [5], which aims to optimize two neural networks g\u03b8 and f\u03c6 in a minmax formulation, while the meaning of the objective is different. In [5], f\u03c6 is a discriminator (binary) classifier to distinguish two distributions. In the proposed algorithm, distinguishing two distribution is still done by two-sample test via MMD, but with an adversarially learned kernel parametrized by f\u03c6. g\u03b8 is then trained to pass the hypothesis test. More connection and difference with related works is discussed in Section 4. Because of the similarity of GAN, we call the proposed algorithm MMD GAN. The algorithm is described in Algorithm 1.\nAlgorithm 1: MMD GAN, our proposed algorithm. input :\u03b1 the learning rate, c the clipping parameter, B the batch size, nc the number of iterations of discriminator per generator update. initialize generator parameter \u03b8 and discriminator parameter \u03c6; while \u03b8 has not converged do\nfor t = 1, . . . , nc do Sample a minibatches {xi}Bi=1 \u223c P(X ) and {zj}Bj=1 \u223c P(Z) g\u03c6 \u2190 \u2207\u03c6Mf\u03c6(P(X ),P(g\u03b8(Z)))\u2212 \u03bbEy\u2208X\u222ag(Z)\u2016y \u2212 fdec(f\u03c6(y))\u20162 \u03c6\u2190 \u03c6+ \u03b1 \u00b7 RMSProp(\u03c6, g\u03c6) \u03c6\u2190 clip(\u03c6,\u2212c, c)\nSample a minibatches {xi}Bi=1 \u223c P(X ) and {zj}Bj=1 \u223c P(Z) g\u03b8 \u2190 \u2207\u03b8Mf\u03c6(P(X ),P(g\u03b8(Z))) \u03b8 \u2190 \u03b8 \u2212 \u03b1 \u00b7 RMSProp(\u03b8, g\u03b8)\nEncoding Prespective of MMD GAN: Besides from using kernel selection to explain MMD GAN, the other way to see the proposed MMD GAN is viewing f\u03c6 as a feature transformation function, and the kernel two sample test is performed on this transformed feature space (i.e. the code space of the autoencoder). The optimization is finding a manifold with stronger signals for MMD two-sample test. From this perspective, [9] is the special case of MMD-GAN if f\u03c6 is the identity mapping function. In such circumstance, the kernel two sample test is conducted on the original data space."}, {"heading": "3.1 Feasible Set Reduction", "text": "Theorem 5. For any f\u03c6, theres exist f \u2032\u03c6 such that Mf\u03c6(Pr,P\u03b8) = Mf \u2032\u03c6(Pr,P\u03b8) and Ex[f\u03c6(x)] Ez[f\u03c6\u2032(g\u03b8(z))].\nWith Theorem 5, we could reduce the feasible set of \u03c6 during the optimization by solving min\u03b8 max\u03c6Mf\u03c6(Pr,P\u03b8) s.t. E[f\u03c6(x)] E[f\u03c6(g\u03b8(z))]\nwhich the optimal solution is still equivalent to solving (2).\nHowever, it is hard to solve the constrained optimization problem with backpropogation. We relax the constraint by ordinal regression [20] to be\nmin \u03b8 max \u03c6\nMf\u03c6(Pr,P\u03b8) + \u03bbmin ( E[f\u03c6(x)]\u2212 E[f\u03c6(g\u03b8(z))], 0 ) ,\n3Note that injective is not necessary invertible.\nwhich only penalizes the objective when the constraint is violated. In practice, we observe that reducing the feasible set makes the training faster and stabler."}, {"heading": "4 Related Works", "text": "There has been a recent surge on improving GAN [5]. We review some related works here.\nConnection with WGAN: If we compoiste f\u03c6 with linear kernel instead of Gaussian kernel, and we restrict the output dimension h to be 1, we then have the objective\nmin \u03b8 max \u03c6 \u2016E[f\u03c6(x)]\u2212 E[f\u03c6(g\u03b8(z))]\u20162. (5)\nParametrizing f\u03c6 and g\u03b8 with nerual networks and assuming \u2203\u03c6\u2032 \u2208 \u03a6 such f \u2032\u03c6 = \u2212f\u03c6,\u2200\u03a6, recovers Wasserstein GAN (WGAN) [8] 4. If we treat f\u03c6(x) as the data transform function, WGAN can be treated as first-order moment matching (linear kernel) while MMD GAN aims to match infinite order of moments with Gaussian kernel form Taylor expansion [9]. Theoretically, Wasserstein distance has similar theoretically guarantee as Theorem 1, 3 and 4. In practice, [21] show neural networks does not have enough capacity to approximate Wasserstein distance. In Section 5, we demonstrate matching high-order moments benefits the results. [22] also propose McGAN that matches second order moment from the primal-dual norm perspective. However, the proposed algorithm requires matrix (tensor) decompositions because of exact moment matching [23], which is hard to scale to higher order moment matching. On the other hand, by giving up exact moment matching, MMD GAN can match high-order moments with kernel tricks. More detailed discussions are in Appendix B.3.\nDifference from Other Works with Autoencoders: Energy-based GAN (EBGAN) [7] also utilizes the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes the reconstruction error of real samples x while maximize the reconstruction error of generated samples g\u03b8(z). In contrast, MMD GAN uses AE to approximate invertible functions by minimizing the reconstruction errors of both real samples x and generated samples g\u03b8(z). Also, [8] show EBGAN approximates total variation, with the drawback of discontinuity, while MMD GAN optimize MMD distance. The other line of works include [2, 24, 9], which aims to match the AE codespace f(x), and utilize the decoder fdec(\u00b7). [2, 24] match the distribution of f(x) and z via different distribution distances and generate data (e.g. image) by fdec(z). [9] use MMD to match f(x) and g(z), and generate data via fdec(g(z)). The proposed MMD GAN matches the f(x) and f(g(z)), and generates data via g(z) directly as GAN. [25] is similar to MMD GAN but it considers KL-divergence without showing continuity and weak\u2217 topology guarantee as we prove in Section 2.\nOther GAN Works: In addition to the discussed works, there are several extended works of GAN. [26] propose use linear kernel to match first moment of its discriminator\u2019s latent features. [14] consider the variance of empirical MMD score during the training. However, the cost of computing the variance of empirical MMD distance is cubic of the batch size. Also, [14] only improve the latent feature matching in [26] by using kernel MMD, instead of proposing an adversarial training framework as we studied in Section 2. [27] use Wasserstein distance to match the distribution of autoencoder loss instead of data. One can consider to extend [27] to higher order matching based on the proposed MMD-GAN."}, {"heading": "5 Experiment", "text": "We train MMD GAN for image generation on the MNIST [28], CIFAR-10 [29], CelebA [13], and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M respectively. All the samples images are generated from a fixed noise random vectors and are not cherry-picked.\nNetwork architecture: In our experiments, we follow the architecture of DCGAN [30] to design g\u03b8 by its generator and f\u03c6 by its discriminator except for expanding the output layer of f\u03c6 to be h dimensions.\n4Theoretically, they are not equivalent but the practical neural network approximation results in the same algorithm.\nKernel designs: The loss function of MMD GAN is implicitly associated with a family of characteristic kernels. Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF kernels k(x, x\u2032) = \u2211K q=1 k\u03c3q (x, x\n\u2032) where k\u03c3q is a Gaussian kernel with bandwidth parameter \u03c3q. Tuning kernel bandwidth \u03c3q optimally still remains an open problem. In this works, we fixed K = 5 and \u03c3q to be {1, 2, 4, 8, 16} and left the f\u03c6 to learn the kernel (feature representation) under these \u03c3q . Hyper-parameters: We use RMSProp [31] with learning rate of 0.00005 for a fair comparison with WGAN as suggested in its original paper [8]. We ensure the boundedness of model parameters of discriminator by clipping the weights pointwisely to the range [\u22120.01, 0.01] as required by Assumption 2. The dimensionality h of the latent space is manually set according to the complexity of the dataset. We thus use h = 16 for MNIST, h = 64 for CelebA, and h = 128 for CIFAR-10 and LSUN bedrooms. The batch size is set to be B = 64 for all datasets."}, {"heading": "5.1 Qualitative Analysis", "text": "We start with comparing MMD-GAN with GMMN on two standard benchmarks, MNIST and CIFAR10. We consider two variants for GMMN. The first one is original GMMN, which trains the generator by minimizing the MMD distance on the original data space. We call it as GMMN-D. To compare with MMD-GAN, we also pretrain an autoencoder for projecting data to a manifold, then fix the autoencoder as a featuer transformation, and train the generator by minimizing the MMD distance on the code space. We call it as GMMN-C.\nThe results are pictured in Figure 1. Both GMMN-D and GMMN-C are able to generate meaningful digits on MNIST because of the simple data structure. By a closer look, onetheless, the boundary and shape of the digits in Figure 1a and 1b are often irregular and non-smooth. In contrast, the sample digits in Figure 1c are more natural with smooth outline and sharper strike. For CIFAR-10 dataset, both GMMNN variants fail to generate meaningful images, but resulting some low level visual features. We observe similar cases in other complex large-scale datasets such as CelebA and LSUN bedrooms, thus results are omitted. On the other hand, the proposed MMD-GAN successfully outputs natural images with sharp boundary and high diversity. The results in Figure 1 confirms the\nsuccess of the proposed adversarial learned kernels to enrich statistical testing power, which is the key difference between GMMN and MMD-GAN.\nIf we increase the batch size of GMMN to 1024, the image quality is improved, however, it is still not competitive to MMD-GAN with B = 64. The images are put in Appendix C. This results proves the proposed MMD-GAN can be trained more efficiently than GMMN with smaller batch size.\nComparisons with GANs: There are several representative extension of GANs. We consider recent state-of-art WGAN [8] based on DCGAN structure [30], because of the connection with MMD-GAN discussed in Section 4. The results are shown in Figure 2. For MNIST, the digits generated from WGAN in Figure 2a are more unnatural with peculiar strikes. In Contrary, the digits from MMD-GAN in Figure 2d enjoy smoother contour. Furthermore, both WGAN and MMD-GAN generate diversified digits, avoiding the mode collapse problems appeared in traditional literature of training GANs. For CelebA, we can see the difference of generated samples from WGAN and MMD-GAN on CelebA dataset. We observe varied poses, expressions, genders, skin colors and light exposure in Figure 2b and 2e. By a closer look (view on-screen with zooming in), we observe that faces from WGAN have higher chances to be blurry and twisted while faces from MMD-GAN are more spontaneous with sharp and acute outline of faces. As for LSUN dataset, we could not distinguish salient difference between the samples generated from MMD-GAN and WGAN."}, {"heading": "5.2 Quantitative Analysis", "text": "To quantitatively measure the quality and diversity of generated samples, we compute the inception score [26] on CIFAR-10 images. The inception score is used for GANs to measure samples quality and diversity on the pretrained inception model [26]. Models that generate collapsed samples have relatively low score. Table 1 lists the results for 50K samples generated by various unsupervised generative models trained on CIFAR-10 dataset. The inception scores of [32, 33, 26] are directly derived from the corresponding references.\nAlthough both WGAN and MMD-GAN can generate sharp images as we show in Section 5.1, our score is better than other GAN techniques except for DFM [32]. This seems to confirm empirically\nthat higher order of moment matching between the real data and fake sample distribution benefits generating more diversified sample images. Also note DFM appears compatible with our method and combing training techniques in DFM is a possible avenue for future work."}, {"heading": "5.3 Stability of MMD GAN", "text": "We further illustrates how the MMD distance correlates well with the quality of the generated samples. Figure 4 plots the evolution of the MMD-GAN estimate the MMD distance during training for MNIST, CelebA and LSUN datasets. We report the average of the M\u0302f\u03c6(PX ,P\u03b8) with moving average to smooth the graph to reduce the variance caused by mini-batch stochastic training. We observe during the whole training process, samples generated from the same noise vector across iterations, remain similar in nature (face identity, bedroom style, while details and background will evolve. This qualitative observation indicates valuable stability of the training process. The decreasing curve with the improving quality of images supports the weak\u2217 topology shown in Theorem 4. Also, We can also see from the plot that the model converges very quickly. In Figure 4b, for example, it converges shortly after ten of thousands of generator iterations on CelebA dataset."}, {"heading": "5.4 Computation Issue", "text": "We conduct time complexity analysis with respective to the batch size B. The time complexity of each iteration is O(B) for WGAN and O(KB2) for our proposed MMD-GAN with a mixture of K RBF kernels. The quadratic complexity O(B2) of MMD-GAN is introduced by computing kernel matrix, which is sometimes criticized to be inapplicable with large batch size in practice. However, we first point that there are several recent works, such as EBGAN [7], also matching pairwise relation between samples of batch size, leading to O(B2) complexity as well.\nEmpirically , we find that under GPU environment, the highly parallelized matrix operation tremendously alleviated the quadratic time to almost linear time with modest B. Figure 3 compares the computational time per generator iterations versus different B on Titan X. When B = 64, which is adopted for training MMD-GAN in our experiments setting, the time per iteration of WGAN and MMD-GAN is 0.268 and 0.676 seconds, respectively. When B = 1024, which is used for training\nGMMN in its references [9], the time per iteration becomes 4.431 and 8.565 seconds, respectively. These results coheres our argument that the empirical computational time for MMD-GAN is not quadratically expensive compared to WGAN with powerful GPU parallel computation."}, {"heading": "6 Discussion", "text": "We introduce a new deep generative model trained via MMD with adversarially learned kernels. We further study its theoretical properties and propose a practical realization MMD-GAN, which can be trained with much smaller batch size than GMMN and has competitive performance with state-of-theart GANs. We can view MMD-GAN as the first practical step forward connecting moment matching network and GAN. One important direction is applying developed tools in moment matching [15] on general GAN works based the connections shown by MMD-GAN. Also, in Section 4, we connect WGAN and MMD-GAN by first-order and infinite-order moment matching. [23] shows finite-order moment matching (\u223c 5) achieves the best performance on domain adaption. One could extend MMD-GAN to this by using polynomial kernels."}, {"heading": "A Technical Proof", "text": "A.1 Proof of Theorem 3\nProof. Since MMD is a probabilistic metric [11], we have the triangular inequality for every Mf . Therefore, |max\n\u03c6 Mf\u03c6(PX ,P\u03b8)\u2212max \u03c6 Mf\u03c6(PX ,P\u03b8\u2032)| \u2264 max \u03c6 |Mf\u03c6(PX ,P\u03b8)\u2212Mf\u03c6(PX ,P\u03b8\u2032)| (6)\n= |Mf\u03c6\u2217 (PX ,P\u03b8)\u2212Mf\u03c6\u2217 (PX ,P\u03b8\u2032)| \u2264 Mf\u03c6\u2217 (P\u03b8,P\u03b8\u2032), (7)\nwhere \u03c6\u2217 is the solution of (6).\nBy definition, given any \u03c6 \u2208 \u03a6, define h\u03b8 = f\u03c6 \u25e6 g\u03b8, the MMD distance Mf\u03c6(P\u03b8,P\u03b8\u2032) = Ez,z\u2032 [k(h\u03b8(z), h\u03b8(z\u2032))\u2212 2k(h\u03b8(z), h\u03b8\u2032(z\u2032)) + k(h\u03b8\u2032(z), h\u03b8\u2032(z\u2032))] \u2264 Ez,z\u2032 [|k(h\u03b8(z), h\u03b8(z\u2032))\u2212 k(h\u03b8(z), h\u03b8\u2032(z\u2032))|] + Ez,z\u2032 [|k(h\u03b8\u2032(z), h\u03b8\u2032(z\u2032))\u2212 k(h\u03b8(z), h\u03b8\u2032(z\u2032))|] (8) In this, we consider Gaussian kerenl k, therefore |k(h\u03b8(z), h\u03b8(z\u2032))\u2212k(h\u03b8(z), h\u03b8\u2032(z\u2032))| = | exp(\u2212\u2016h\u03b8(z)\u2212h\u03b8(z\u2032)\u20162)\u2212exp(\u2212\u2016h\u03b8(z)\u2212h\u03b8\u2032(z\u2032)\u20162)| \u2264 1, for all (\u03b8, \u03b8\u2032) and (z, z\u2032). Similarly, |k(h\u03b8\u2032(z), h\u03b8\u2032(z\u2032)) \u2212 k(h\u03b8(z), h\u03b8\u2032(z\u2032))| \u2264 1. Combining the above claim with (7) and bounded convergence theorem, we have\n|max \u03c6 Mf\u03c6(PX ,P\u03b8)\u2212max \u03c6\nMf\u03c6(PX ,P\u03b8\u2032)| \u03b8\u2192\u03b8\u2032\u2212\u2212\u2212\u2192 0,\nwhich proves the continuity of maxf MMDf (PX ,P\u03b8).\nBy Mean Value Theorem, |e\u2212x2 \u2212 e\u2212y2 | \u2264 maxz |2ze\u2212z 2 | \u00d7 |x \u2212 y| \u2264 |x \u2212 y|. Incorporating it with (8) and triangular inequality, we have Mf\u03c6(P\u03b8,P\u03b8\u2032) \u2264 Ez,z\u2032 [\u2016h\u03b8(z\u2032)\u2212 h\u03b8\u2032(z\u2032)\u2016+ \u2016h\u03b8(z)\u2212 h\u03b8\u2032(z)\u2016] = 2Ez[\u2016h\u03b8(z)\u2212 h\u03b8\u2032(z)\u2016] Now let h be locally Lipschitz. For a given pair (\u03b8, z) there is a constant L(\u03b8, z) and an open set U\u03b8 such that for every (\u03b8\u2032, z) \u2208 U\u03b8 we have \u2016h\u03b8(z)\u2212 h\u03b8\u2032(z)\u2016 \u2264 L(\u03b8, z)(\u2016\u03b8 \u2212 \u03b8\u2032\u2016) Under Assumption 2, we then achieve |Mf\u03c6(PX ,P\u03b8)\u2212Mf\u03c6(PX ,P\u2032\u03b8)| \u2264Mf\u03c6(P\u03b8,P\u03b8\u2032) \u2264 2Ez[L(\u03b8, z)]\u2016\u03b8 \u2212 \u03b8\u2032\u2016. (9) Combining (7) and (9) implies max\u03c6Mf\u03c6(PX ,P\u03b8) is locally Lipschitz and continuous everywhere. Last, applying Radamacher\u2019s theorem proves max\u03c6Mf\u03c6(PX ,P\u03b8) is differentiable almost everywhere, which completes the proof.\nA.2 Proof of Theorem 4\nProof. The proof utilizes parts of results from [34].\n(\u21d2) If max\u03c6Mf\u03c6(Pn,P)\u2192 0, there exists \u03c6 \u2208 \u03a6 such that Mf\u03c6(P,Pn)\u2192 0 since Mf\u03c6(P,Pn) is non-negative. By [34], for any characteristic kernel k,\nMk(Pn,P)\u2192 0\u21d0\u21d2 Pn D\u2212\u2192 P.\nTherefore, if max\u03c6Mf\u03c6(Pn,P)\u2192 0, Pn D\u2212\u2192 P.\n(\u21d0) By [34], given a characteristic kernel k, if supx,x\u2032 k(x, x\u2032) \u2264 1, \u221a Mk(P,Q) \u2264 W (P,Q), where W (P,Q) is Wasserstein distance. In this paper, we consider the kernel k(x, x\u2032) = exp(\u2212\u2016f\u03c6(x) \u2212 f\u03c6(x\u2032)\u20162) \u2264 1. By above, \u221a Mf\u03c6(P,Q) \u2264 W (P,Q),\u2200\u03c6. By [8], if Pn\nD\u2212\u2192 P, W (P,Pn)\u2192 0. Combining all of them, we get\nPn D\u2212\u2192 P =\u21d2W (P,Pn)\u2192 0 =\u21d2 max\n\u03c6 Mf\u03c6(P,Pn)\u2192 0.\nA.3 Proof of Theorem 5\nProof. The proof assumes f\u03c6(x) is scalar, but the vector case can be proved with the same sketch. First, if E[f\u03c6(x)] > E[f\u03c6(g\u03b8(z))], then \u03c6 = \u03c6\u2032. If E[f\u03c6(x)] < E[f\u03c6(g\u03b8(z))], we let f = \u2212f\u03c6, then E[f(x)] > E[f(g\u03b8(z))] and flipping sign does not change the MMD distance. If we parametrize f\u03c6 by a neural network, which has a linear output layer, \u03c6\u2032 can realized by flipping the sign of the weights of the last layer."}, {"heading": "B Property of MMD with Fixed and Learned Kernels", "text": "B.1 Continuity and Differentiability\nOne can simplify Theorem 3 and its proof for standard MMD distance to show MMD is also continuous and differentiable almost everywhere. In [8], they propose a counterexample to show the discontinuity of MMD by assumingH = L2. However, it is known that L2 is not in RKHS, so the discussed counterexample is not appropriate.\nB.2 IPM Framework\nFrom integral probability metrics (IPM), the probabilitic distance can be defined as \u2206(P,Q) = sup\nf\u2208F Ex\u223cP[f(x)]\u2212 Ey\u223cQ[f(y)]. (10)\nBy changing the function class F , we can recover several distances, such as total variation, Wasserstein distance and MMD distance. From [8], the discriminator f\u03c6 in different existing works of GAN can be explained to be used to solve different probabilistic metrics based on (10). For MMD, the function class F is {\u2016f\u2016Hk \u2264 1}, whereH is RKHS associated with kernel k. Different form many distances, such as total variation and Wasserstein distance, there is an analytical representation [11] as we show in Section 2, which is\n\u2206(P,Q) = MMD(P,Q)\u2016\u00b5P \u2212 \u00b5Q\u2016H = \u221a\nEP[k(x, x\u2032)]\u2212 2EP,Q[k(x, y)] + EQ[k(y, y\u2032)]. Because of the analytical representation of (10), GMMN does not need an additional network f\u03c6 for estimating the distance.\nHere we also provide an explanation of the proposed MMD with adversarially learned kernel under IPM framework. The MMD distance with adversarially learned kernel is represented as\nmax k\u2208K\nMMD(P,Q),\nThe corresponding IPM formulation is \u2206(P,Q) = max\nk\u2208K MMD(P,Q) = sup f\u2208Hk1\u222a\u00b7\u00b7\u00b7\u222aHkn Ex\u223cP[f(x)]\u2212 Ey\u223cQ[f(y)],\nwhere ki \u2208 K,\u2200i. From this perspective, the proposed MMD distance with adversarially learned kernel is still defined by IPM but with a larger function class.\nB.3 MMD is an Efficient Moment Matching\nWe consider the example of matching first and second moments of P and Q. The `1 objective in McGAN [22] is \u2016\u00b5P \u2212 \u00b5Q\u20161 + \u2016\u03a3P \u2212 \u03a3Q\u2016\u2217, where \u2016 \u00b7 \u2016\u2217 is the trace norm. The objective can be changed to be general `q norm. In MMD, with polynomial kernel k(x, y) = 1 + x>y, the MMD distance is\n2\u2016\u00b5P \u2212 \u00b5Q\u20162 + \u2016\u03a3P \u2212 \u03a3Q + \u00b5P\u00b5>P \u2212 \u00b5Q\u00b5>Q\u20162F , which is inexact moment matching because the second term contains the quadratic of the first moment. It is difficult to match high-order moments, because we have to deal with high order tensors directly. On the other hand, MMD can easily match high-order moments (even infinite order moments by using Gaussian kernel) with kernel tricks, and enjoys strong theoretical guarantee.\nC Sheets of Samples"}], "references": [{"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey Hinton"], "venue": "In AISTATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "In ICLR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "All of statistics: a concise course in statistical inference", "author": ["Larry Wasserman"], "venue": "Springer Science & Business Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Energy-based Generative Adversarial Network", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "In ICLR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Richard Zemel"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["Gintare Karolina Dziugaite", "Daniel M. Roy", "Zoubin Ghahramani"], "venue": "In UAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte J. Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["Fisher Yu", "Ari Seff", "Yinda Zhang", "Shuran Song", "Thomas Funkhouser", "Jianxiong Xiao"], "venue": "arXiv preprint arXiv:1506.03365,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Generative models and model criticism via optimized maximum mean discrepancy", "author": ["Dougal J. Sutherland", "Hsiao-Yu Fish Tung", "Heiko Strathmann", "Soumyajit De", "Aaditya Ramdas", "Alexander J. Smola", "Arthur Gretton"], "venue": "In ICLR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Kernel mean embedding of distributions: A review and beyonds", "author": ["Krikamol Muandet", "Kenji Fukumizu", "Bharath Sriperumbudur", "Bernhard Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1605.09522,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Kernel choice and classifiability for rkhs embeddings of probability distributions", "author": ["Kenji Fukumizu", "Arthur Gretton", "Gert R Lanckriet", "Bernhard Sch\u00f6lkopf", "Bharath K Sriperumbudur"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["A. Gretton", "B. Sriperumbudur", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["Arthur Gretton", "Dino Sejdinovic", "Heiko Strathmann", "Sivaraman Balakrishnan", "Massimiliano Pontil", "Kenji Fukumizu", "Bharath K Sriperumbudur"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Deep kernel learning", "author": ["Andrew Gordon Wilson", "Zhiting Hu", "Ruslan Salakhutdinov", "Eric P Xing"], "venue": "In AISTATS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Support vector learning for ordinal regression", "author": ["Ralf Herbrich", "Thore Graepel", "Klaus Obermayer"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Generalization and equilibrium in generative adversarial nets (gans)", "author": ["Sanjeev Arora", "Rong Ge", "Yingyu Liang", "Tengyu Ma", "Yi Zhang"], "venue": "arXiv preprint arXiv:1703.00573,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Mcgan: Mean and covariance feature matching gan", "author": ["Youssef Mroueh", "Tom Sercu", "Vaibhava Goel"], "venue": "arxiv pre-print", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Central moment discrepancy (cmd) for domain-invariant representation learning", "author": ["Werner Zellinger", "Thomas Grubinger", "Edwin Lughofer", "Thomas Natschl\u00e4ger", "Susanne Saminger-Platz"], "venue": "arXiv preprint arXiv:1702.08811,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Adversarial generator-encoder networks", "author": ["Dmitry Ulyanov", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1704.02304,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Began: Boundary equilibrium generative adversarial networks", "author": ["David Berthelot", "Tom Schumm", "Luke Metz"], "venue": "arXiv preprint arXiv:1703.10717,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "In ICLR,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Improving generative adversarial networks with denoising feature matching", "author": ["D Warde-Farley", "Y Bengio"], "venue": "In ICLR,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2017}, {"title": "Adversarially learned inference", "author": ["Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Alex Lamb", "Martin Arjovsky", "Olivier Mastropietro", "Aaron Courville"], "venue": "In ICLR,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Deep generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with promising results.", "startOffset": 22, "endOffset": 28}, {"referenceID": 1, "context": "Deep generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with promising results.", "startOffset": 22, "endOffset": 28}, {"referenceID": 2, "context": "However, modeling arbitrary density is a statistically challenging task [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "In many applications, such as caption generation [4], accurate density estimation is not even necessary since we are only interested in sampling from the approximated distribution.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "Rather than estimating the density of PX , Generative Adversarial Network (GAN) [5] starts from a base distribution PZ over Z , such as Gaussian distribution, then trains a transformation network g\u03b8 such that P\u03b8 \u2248 PX , where P\u03b8 is the underlying distribution of g\u03b8(z) and z \u223c PZ .", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "Different probabilistic (pseudo) metrics have been studied [5\u20138] under GAN framework.", "startOffset": 59, "endOffset": 64}, {"referenceID": 5, "context": "Different probabilistic (pseudo) metrics have been studied [5\u20138] under GAN framework.", "startOffset": 59, "endOffset": 64}, {"referenceID": 6, "context": "Different probabilistic (pseudo) metrics have been studied [5\u20138] under GAN framework.", "startOffset": 59, "endOffset": 64}, {"referenceID": 7, "context": "Instead of training an auxiliary network f\u03c6 for measuring the distance between PX and P\u03b8, Generative moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11], which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.", "startOffset": 132, "endOffset": 139}, {"referenceID": 8, "context": "Instead of training an auxiliary network f\u03c6 for measuring the distance between PX and P\u03b8, Generative moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11], which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.", "startOffset": 132, "endOffset": 139}, {"referenceID": 9, "context": "Instead of training an auxiliary network f\u03c6 for measuring the distance between PX and P\u03b8, Generative moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11], which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.", "startOffset": 183, "endOffset": 187}, {"referenceID": 9, "context": "[11] shows even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "There is no promising empirical results comparable with GAN on challenging benchmarks [12, 13].", "startOffset": 86, "endOffset": 94}, {"referenceID": 11, "context": "There is no promising empirical results comparable with GAN on challenging benchmarks [12, 13].", "startOffset": 86, "endOffset": 94}, {"referenceID": 7, "context": "it also requires larger batch size than GAN needs for training, which is considered to be less efficient [9, 10, 14, 8] In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels instead of fixed Gaussian kernels to have better hypothesis testing power.", "startOffset": 105, "endOffset": 119}, {"referenceID": 8, "context": "it also requires larger batch size than GAN needs for training, which is considered to be less efficient [9, 10, 14, 8] In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels instead of fixed Gaussian kernels to have better hypothesis testing power.", "startOffset": 105, "endOffset": 119}, {"referenceID": 12, "context": "it also requires larger batch size than GAN needs for training, which is considered to be less efficient [9, 10, 14, 8] In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels instead of fixed Gaussian kernels to have better hypothesis testing power.", "startOffset": 105, "endOffset": 119}, {"referenceID": 13, "context": "The unified view shows more connections between moment matching and GAN, which can potentially inspire new algorithms based on well-developed tools in statistics [15].", "startOffset": 162, "endOffset": 166}, {"referenceID": 4, "context": "Instead, Generative Adversarial Network (GAN) [5] trains a generator g\u03b8 parametrized by \u03b8 to transform samples z \u223c PZ , where z \u2208 Z , into g\u03b8(z) \u223c P\u03b8 such that P\u03b8 \u2248 PX .", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "To measure the similarity between PX and P\u03b8 via their samples {x}i=1 and {g\u03b8(zj)}j=1 during the training, [5] trains the discriminator f\u03c6 parametrized by \u03c6 for help.", "startOffset": 106, "endOffset": 109}, {"referenceID": 9, "context": "One way to conduct two sample test is via kernel maximum mean discrepancy (MMD) [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "[11] Given a kernel k, if k is a characteristic kernel, then Mk(P,Q) = 0 iff P = Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Based on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains g\u03b8 by", "startOffset": 20, "endOffset": 27}, {"referenceID": 8, "context": "Based on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains g\u03b8 by", "startOffset": 20, "endOffset": 27}, {"referenceID": 9, "context": "Please refer to [11] for more details.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "By [11, 17] if f is a injective function and k is characteristic, then the resulted kernel k\u0303 = k \u25e6 f , where k\u0303(x, x\u2032) = k(f(x), f(x\u2032)) is still characteristic.", "startOffset": 3, "endOffset": 11}, {"referenceID": 15, "context": "By [11, 17] if f is a injective function and k is characteristic, then the resulted kernel k\u0303 = k \u25e6 f , where k\u0303(x, x\u2032) = k(f(x), f(x\u2032)) is still characteristic.", "startOffset": 3, "endOffset": 11}, {"referenceID": 16, "context": "Note that [18] consider the linear combination of characteristic kernels, which can also be incorporated into the discussed composition kernels.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "A more general composition kernel is studied in [19].", "startOffset": 48, "endOffset": 52}, {"referenceID": 4, "context": "[8] discuss different distances between distributions adopted by existing deep learning algorithms, and show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7], except for Wasserstein distance.", "startOffset": 175, "endOffset": 178}, {"referenceID": 6, "context": "[8] discuss different distances between distributions adopted by existing deep learning algorithms, and show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7], except for Wasserstein distance.", "startOffset": 199, "endOffset": 202}, {"referenceID": 2, "context": "Considering n \u2192 \u221e, under mild Assumption, max\u03c6Mf\u03c6(PX ,Pn)\u2192 0\u21d0\u21d2 Pn D \u2212\u2192 PX , where D \u2212\u2192 means converging in distribution [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 14, "context": "Please refer to [16] for more rigorous discussions.", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "The proposed algorithm is similar to GAN [5], which aims to optimize two neural networks g\u03b8 and f\u03c6 in a minmax formulation, while the meaning of the objective is different.", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "In [5], f\u03c6 is a discriminator (binary) classifier to distinguish two distributions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "From this perspective, [9] is the special case of MMD-GAN if f\u03c6 is the identity mapping function.", "startOffset": 23, "endOffset": 26}, {"referenceID": 18, "context": "We relax the constraint by ordinal regression [20] to be min \u03b8 max \u03c6 Mf\u03c6(Pr,P\u03b8) + \u03bbmin ( E[f\u03c6(x)]\u2212 E[f\u03c6(g\u03b8(z))], 0 ) ,", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "There has been a recent surge on improving GAN [5].", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "If we treat f\u03c6(x) as the data transform function, WGAN can be treated as first-order moment matching (linear kernel) while MMD GAN aims to match infinite order of moments with Gaussian kernel form Taylor expansion [9].", "startOffset": 214, "endOffset": 217}, {"referenceID": 19, "context": "In practice, [21] show neural networks does not have enough capacity to approximate Wasserstein distance.", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "[22] also propose McGAN that matches second order moment from the primal-dual norm perspective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "However, the proposed algorithm requires matrix (tensor) decompositions because of exact moment matching [23], which is hard to scale to higher order moment matching.", "startOffset": 105, "endOffset": 109}, {"referenceID": 6, "context": "Difference from Other Works with Autoencoders: Energy-based GAN (EBGAN) [7] also utilizes the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes the reconstruction error of real samples x while maximize the reconstruction error of generated samples g\u03b8(z).", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "The other line of works include [2, 24, 9], which aims to match the AE codespace f(x), and utilize the decoder fdec(\u00b7).", "startOffset": 32, "endOffset": 42}, {"referenceID": 7, "context": "The other line of works include [2, 24, 9], which aims to match the AE codespace f(x), and utilize the decoder fdec(\u00b7).", "startOffset": 32, "endOffset": 42}, {"referenceID": 1, "context": "[2, 24] match the distribution of f(x) and z via different distribution distances and generate data (e.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[9] use MMD to match f(x) and g(z), and generate data via fdec(g(z)).", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[25] is similar to MMD GAN but it considers KL-divergence without showing continuity and weak\u2217 topology guarantee as we prove in Section 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] propose use linear kernel to match first moment of its discriminator\u2019s latent features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] consider the variance of empirical MMD score during the training.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Also, [14] only improve the latent feature matching in [26] by using kernel MMD, instead of proposing an adversarial training framework as we studied in Section 2.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "Also, [14] only improve the latent feature matching in [26] by using kernel MMD, instead of proposing an adversarial training framework as we studied in Section 2.", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "[27] use Wasserstein distance to match the distribution of autoencoder loss instead of data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "One can consider to extend [27] to higher order matching based on the proposed MMD-GAN.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "We train MMD GAN for image generation on the MNIST [28], CIFAR-10 [29], CelebA [13], and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M respectively.", "startOffset": 51, "endOffset": 55}, {"referenceID": 26, "context": "We train MMD GAN for image generation on the MNIST [28], CIFAR-10 [29], CelebA [13], and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M respectively.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "We train MMD GAN for image generation on the MNIST [28], CIFAR-10 [29], CelebA [13], and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M respectively.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "We train MMD GAN for image generation on the MNIST [28], CIFAR-10 [29], CelebA [13], and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M respectively.", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "Network architecture: In our experiments, we follow the architecture of DCGAN [30] to design g\u03b8 by its generator and f\u03c6 by its discriminator except for expanding the output layer of f\u03c6 to be h dimensions.", "startOffset": 78, "endOffset": 82}, {"referenceID": 8, "context": "Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF kernels k(x, x\u2032) = \u2211K q=1 k\u03c3q (x, x \u2032) where k\u03c3q is a Gaussian kernel with bandwidth parameter \u03c3q.", "startOffset": 40, "endOffset": 51}, {"referenceID": 7, "context": "Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF kernels k(x, x\u2032) = \u2211K q=1 k\u03c3q (x, x \u2032) where k\u03c3q is a Gaussian kernel with bandwidth parameter \u03c3q.", "startOffset": 40, "endOffset": 51}, {"referenceID": 12, "context": "Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF kernels k(x, x\u2032) = \u2211K q=1 k\u03c3q (x, x \u2032) where k\u03c3q is a Gaussian kernel with bandwidth parameter \u03c3q.", "startOffset": 40, "endOffset": 51}, {"referenceID": 28, "context": "Hyper-parameters: We use RMSProp [31] with learning rate of 0.", "startOffset": 33, "endOffset": 37}, {"referenceID": 27, "context": "We consider recent state-of-art WGAN [8] based on DCGAN structure [30], because of the connection with MMD-GAN discussed in Section 4.", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "To quantitatively measure the quality and diversity of generated samples, we compute the inception score [26] on CIFAR-10 images.", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "The inception score is used for GANs to measure samples quality and diversity on the pretrained inception model [26].", "startOffset": 112, "endOffset": 116}, {"referenceID": 29, "context": "The inception scores of [32, 33, 26] are directly derived from the corresponding references.", "startOffset": 24, "endOffset": 36}, {"referenceID": 30, "context": "The inception scores of [32, 33, 26] are directly derived from the corresponding references.", "startOffset": 24, "endOffset": 36}, {"referenceID": 23, "context": "The inception scores of [32, 33, 26] are directly derived from the corresponding references.", "startOffset": 24, "endOffset": 36}, {"referenceID": 29, "context": "1, our score is better than other GAN techniques except for DFM [32].", "startOffset": 64, "endOffset": 68}, {"referenceID": 29, "context": "20 DFM [32] 7.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "72 ALI [33] 5.", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "34 Improved GANs [26] 4.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "However, we first point that there are several recent works, such as EBGAN [7], also matching pairwise relation between samples of batch size, leading to O(B) complexity as well.", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "GMMN in its references [9], the time per iteration becomes 4.", "startOffset": 23, "endOffset": 26}], "year": 2017, "abstractText": "Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD-GAN. The new distance measure in MMD-GAN is a meaningful loss that enjoys the advantage of weak\u2217 topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.", "creator": "LaTeX with hyperref package"}}}