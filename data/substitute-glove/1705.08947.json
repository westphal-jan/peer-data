{"id": "1705.08947", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech", "abstract": "We implemented as synthetic for augmenting vascular manuscript - one - echoed (TTS) to lowdimensional polemically speaker hedgerow to benefit examples angry last end fifth unique. As each starting point, n't appeared completion four with from post - ofthe - medieval approaches than album - congressman neural TTS: Deep Voice 2009 of Tacotron. We introduce Deep Voice 2, instance is included a turn similar pipeline with Deep Voice 1, but two-storey with sector performing inside section and aspect a most audio quality increased face Deep Voice 1. We emphasis Tacotron by adoption a post - grain neural vocoder, addition demonstrate part likely downloads products improvement. We immediately demonstrate come methods with includes - republican echoing fission that many Deep Voice 1 one Tacotron on same holds - deputy TTS time-series. We there called a run neural TTS system want learn reportedly of well expressing from some even half from p.m. such data per speaker, coming achieves high data example synthetic while preserving later appointee identify almost seems.", "histories": [["v1", "Wed, 24 May 2017 19:53:13 GMT  (1150kb,D)", "http://arxiv.org/abs/1705.08947v1", "Submitted to NIPS 2017"], ["v2", "Wed, 20 Sep 2017 21:43:18 GMT  (1145kb,D)", "http://arxiv.org/abs/1705.08947v2", "Accepted in NIPS 2017"]], "COMMENTS": "Submitted to NIPS 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sercan arik", "gregory diamos", "rew gibiansky", "john miller", "kainan peng", "wei ping", "jonathan raiman", "yanqi zhou"], "accepted": true, "id": "1705.08947"}, "pdf": {"name": "1705.08947.pdf", "metadata": {"source": "CRF", "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech", "authors": ["Sercan \u00d6. Ar\u0131k", "Gregory Diamos", "Kainan Peng", "Yanqi Zhou"], "emails": ["sercanarik@baidu.com", "gregdiamos@baidu.com", "gibianskyandrew@baidu.com", "millerjohn@baidu.com", "pengkainan@baidu.com", "pingwei01@baidu.com", "jonathanraiman@baidu.com", "zhouyanqi@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "Artificial speech synthesis, commonly known as text-to-speech (TTS), has a variety of applications in technology interfaces, accessibility, media, and entertainment. Most TTS systems are built with a single speaker voice, and multiple speaker voices are provided by having distinct speech databases or model parameters. As a result, developing a TTS system with support for multiple voices requires much more data and development effort than a system which only supports a single voice.\nIn this work, we demonstrate that we can build all-neural multi-speaker TTS systems which share the vast majority of parameters between different speakers. We show that not only can a single model generate speech from multiple different voices, but also that significantly less data is required per speaker than when training single-speaker systems.\nConcretely, we make the following contributions:\n1. We present Deep Voice 2, an improved architecture based on Deep Voice 1 (Arik et al., 2017).\n2. We introduce a WaveNet-based (Oord et al., 2016) spectrogram-to-audio neural vocoder, and use it with Tacotron (Wang et al., 2017) as a replacement for Griffin-Lim audio generation.\nSubmitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 5.\n08 94\n7v 1\n[ cs\n.C L\n3. Using these two single-speaker models as a baseline, we demonstrate multi-speaker neural speech synthesis by introducing trainable speaker embeddings into Deep Voice 2 and Tacotron.\nWe organize the rest of this paper as follows. Section 2 discusses related work and what makes the contributions of this paper distinct from prior work. In Section 3, we present Deep Voice 2 and highlight the differences from Deep Voice 1. Section 4 presents our speaker embedding technique for neural TTS models and shows multi-speaker variants of the Deep Voice 2 and Tacotron architectures. We quantify the improvement for single speaker TTS through a mean opinion score (MOS) evaluation in Section 5.1 and Section 5.2 measures the synthesized audio quality of multi-speaker Deep Voice 2 and Tacotron via both MOS evaluation and a multi-speaker discriminator accuracy metric. Section 6 concludes with a discussion of the results and potential future work."}, {"heading": "2 Related Work", "text": "We discuss the related work relevant to each of our claims in Section 1 in order, starting from single-speaker neural speech synthesis and moving on to multi-speaker speech synthesis and metrics for generative model quality.\nWith regards to single-speaker speech synthesis, deep learning has been used for a variety of subcomponents, including duration prediction (Zen et al., 2016), fundamental frequency prediction (Ronanki et al., 2016), acoustic modeling (Zen and Sak, 2015), and more recently autoregressive sample-bysample audio waveform generation (e.g., Oord et al., 2016; Mehri et al., 2016). Our contributions build upon recent work in entirely neural TTS systems, including Deep Voice 1 (Arik et al., 2017), Tacotron (Wang et al., 2017), and Char2Wav (Sotelo et al., 2017). While these works focus on building single-speaker TTS systems, our paper focuses on extending neural TTS systems to handle multiple speakers with less data per speaker.\nOur work is not the first to attempt a multi-speaker TTS system. For instance, in traditional HMMbased TTS synthesis (e.g., Yamagishi et al., 2009), an average voice model is trained using multiple speakers\u2019 data, which is then adapted to different speakers. DNN-based systems (e.g., Yang et al., 2016) have also been used to build average voice models, with i-vectors representing speakers as additional inputs and separate output layers for each target speaker. Similarly, Fan et al. (2015) uses a shared hidden representation among different speakers with speaker-dependent output layers predicting vocoder parameters (e.g., line spectral pairs, aperiodicity parameters, etc.). For further context, Wu et al. (2015) empirically studies DNN-based multi-speaker modeling. More recently, speaker adaptation has been tackled with generative adversarial networks (GANs) (Hsu et al., 2017).\nWe instead use trainable speaker embeddings for multi-speaker TTS, which is a novel technique. Unlike prior work which depends on i-vectors, the speaker embeddings used in this work are trained jointly with the rest of the model from scratch, and thus can directly learn the features relevant to the speech synthesis task. In addition, this work does not rely on per-speaker output layers or average voice modeling, which leads to higher-quality synthesized samples and lower data requirements (as there are fewer unique parameters per speaker to learn).\nIn order to evaluate the distinctiveness of the generated voices in an automated way, we propose using the classification accuracy of a speaker discriminator. Similar metrics such as an \u201cInception score\u201d have been used for quantitative quality evaluations of GANs for image synthesis (e.g., Salimans et al., 2016). Speaker classification has been studied with both traditional GMM-based methods (e.g., Reynolds et al., 2000) and more recently with deep learning approaches (e.g., Li et al., 2017)."}, {"heading": "3 Single-Speaker Deep Voice 2", "text": "In this section, we present Deep Voice 2, a neural TTS system based on Deep Voice 1 (Arik et al., 2017). We keep the general structure of the Deep Voice 1 (Arik et al., 2017), as depicted in Fig. 1 (the corresponding training pipeline is depicted in Appendix A). Our primary motivation for presenting an improved single-speaker model is to use it as the starting point for a high-quality multi-speaker model.\nOne major difference between Deep Voice 2 and Deep Voice 1 is the separation of the phoneme duration and frequency models. Deep Voice 1 has a single model to jointly predict phoneme duration\nConv-BN-Res\nPhoneme 1 Phoneme n Stacked Bi-GRU Bi-GRU FC ! + F0 Filter-Bank FC ! FC Voiced Speaker \u2026 FC softsign)+)1 + \u2a09 FC softsign)+)1 f FC\nPhoneme 1 Phoneme n Stacked Bi-GRU Durations (bucketed) Speaker \u2026 FC FC concat concat MLP CRF \u2a09\u03bcF \u03bc !F\nMel 1 Mel m\u2026\nSpeaker\nDuration Frequency Vocal Speaker\nFC\nsoftsign\nConvFC\nConv + BN\n\u2a09\nReLu6\n+\nConv-BN-Res \u2026 Stacked Bi-GRU Dropout softsign FC Dropout CTC Phoneme pairs \u03c9\nText Synthesized Speech\nEncoder CBFG\nChar 1 Char n\u2026 Taco\nSpeaker\nFC\nBi-GRU Highway Layers\nMLP\nmax9pool Filter-Bank + BN + ReLu Filter-Bank + BN + ReLu\n+\nFC\nsoftsign\ntile softsign\nMLP\nMel iTaco Speaker\nAttention\nFC\nsoftsign\nFC\nStacked Residual GRUsoftsign FC Decoder CBFG FC Spectrogram Vocal Audio\nVocal Speaker\nGrif<in9Lim \u22c1\nMel i-1 Mel i+1\n\u2026 \u2026\nGRU i-1 GRU i-1 GRU i+1\u2026 \u2026 Mel i+1 Audio\nPronunciation Dictionary Phonemes upsample\nF0\nUpsampled PhonemesPhonemes\nFigure 1: Inference system diagram: first text-phonemes dictionary conversion, second predict phoneme durations, third upsample and generate F0, finally feed F0 and phonemes to vocal model.\nand frequency profile (voicedness and time-dependent fundamental frequency, F0). In Deep Voice 2, the phoneme durations are predicted first and then are used as inputs to the frequency model.\nIn the subsequent subsections, we present the models used in Deep Voice 2. All hyperparameters are specified in Appendix B. We will provide a quantitative comparison of Deep Voice 1 and Deep Voice 2 in Section 5.1."}, {"heading": "3.1 Segmentation model", "text": "Estimation of phoneme locations is treated as an unsupervised learning problem in Deep Voice 2, similar to Deep Voice 1. The segmentation model is convolutional-recurrent architecture with connectionist temporal classification (CTC) loss applied to classify phoneme pairs, which are then used to extract the boundaries between them. The major architecture changes in Deep Voice 2 are the addition of batch normalization and residual connections in the convolutional layers. Specifically, Deep Voice 1\u2019s segmentation model computes the output of each layer as\nh(l) = relu ( W (l) \u2217 h(l\u22121) + b(l) ) , (1)\nwhere h(l) is the output of the lth layer, W (l) is the convolution filterbank, b(l) is the bias bias vector, and \u2217 is the convolution operator. In contrast, Deep Voice 2\u2019s segmentation model layers instead compute\nh(l) = relu ( h(l\u22121) +BN ( W (l) \u2217 h(l\u22121) )) , (2)\nwhere BN is batch normalization (Ioffe and Szegedy, 2015). In addition, we find that the segmentation model often makes mistakes for boundaries between silence phonemes and other phonemes, which can significantly reduce segmentation accuracy on some datasets. We introduce a small post-processing step to correct these mistakes: whenever the segmentation model decodes a silence boundary, we adjust the location of the boundary with a silence detection heuristic.1"}, {"heading": "3.2 Duration Model", "text": "In Deep Voice 2, instead of predicting a continuous-valued duration, we formulate duration prediction as a sequence labeling problem. We discretize the phoneme duration into log-scaled buckets, and assign to each input phoneme the bucket label corresponding to its duration. We model the sequence by a conditional random field (CRF) with pairwise potentials at output layer (Lample et al., 2016). During inference, we decode from the CRF using the Viterbi forward-backward algorithm. We find that quantizing the duration prediction and introducing the pairwise dependence implied by the CRF improves synthesis quality."}, {"heading": "3.3 Frequency Model", "text": "After decoding from the duration model, the input features are upsampled using the predicted phoneme durations from a per-phoneme input to a per-frame input.2 Deep Voice 2 frequency model\n1We compute the smoothed normalized audio power as p[n] = (x[n]2/xmax2) \u2217 g[n], where x[n] is the audio signal, g[n] is the impulse response of a Gaussian filter, xmax is the maximum value of x[n] and \u2217 is one-dimensional convolution operation. We assign the silence phoneme boundaries when p[n] exceeds a fixed threshold. The optimal parameter values for the Gaussian filter and the threshold depend on the dataset and audio sampling rate.\n2Each frame is ensured to be 10 milliseconds. E.g. if a phoneme lasts 20 milliseconds, the input features corresponding to that phoneme will be repeated in 2 frames. If it lasts less than 10 milliseconds, it is extend to a single frame.\nconsists of multiple layers: firstly, bidirectional gated recurrent unit (GRU) layers (Cho et al., 2014) generate hidden states from the input features. From these hidden states, an affine projection followed by a sigmoid nonlinearity produces the probability that each frame is voiced. Hidden states are also used to make two separate normalized F0 predictions. The first prediction, fGRU, is made with a single-layer bidirectional GRU followed by an affine projection. The second prediction, fconv, is made by adding up the contributions of multiple convolutions with varying convolution widths and a single output channel. Finally, the hidden state is used with an affine projection and a sigmoid nonlinearity to predict a mixture ratio \u03c9, which is used to weigh the two normalized frequency predictions and combine them into f = \u03c9 \u00b7 fGRU + (1\u2212 \u03c9) \u00b7 fconv. (3) The normalized prediction f is then converted to the true frequency F0 prediction via\nF0 = \u00b5F0 + \u03c3F0 \u00b7 f, (4) where \u00b5F0 and \u03c3F0 are, respectively, the mean and standard deviation of F0 for the speaker the model is trained on. We find that predicting F0 with a mixture of convolutions and a recurrent layer performs better than predicting with either one individually. We attribute this to the hypothesis that including the wide convolutions reduces the need for the recurrent layers to maintain state over a large number of input frames, while processing the entire context information efficiently."}, {"heading": "3.4 Vocal Model", "text": "The Deep Voice 2 vocal model is based on a WaveNet architecture (Oord et al., 2016) with a two-layer bidirectional QRNN (Bradbury et al., 2016) conditioning network, similar to Deep Voice 1. However, we remove the 1\u00d7 1 convolution between the gated tanh nonlinearity and the residual connection. In addition, we use the same conditioner bias for every layer of the WaveNet, instead of generating a separate bias for every layer as was done in Deep Voice 1.3"}, {"heading": "4 Multi-Speaker Models with Trainable Speaker Embeddings", "text": "In order to synthesize speech from multiple speakers, we augment each of our models with a single low-dimensional speaker embedding vector per speaker. Unlike previous work, our approach does not rely on per-speaker weight matrices or layers. Speaker-dependent parameters are stored in a very low-dimensional vector and thus there is near-complete weight sharing between speakers. We use speaker embeddings to produce recurrent neural network (RNN) initial states, nonlinearity biases, and multiplicative gating factors, used throughout the networks. Speaker embeddings are initialized randomly with a uniform distribution over [\u22120.1, 0.1] and trained jointly via backpropagation; each model has its own set of speaker embeddings.\nTo encourage each speaker\u2019s unique voice signature to influence the model, we incorporate the speaker embeddings into multiple portions of the model. Empirically, we find that simply providing the speaker embeddings to the input layers does not work as well for any of the presented models besides the vocal model, possibly due to the high degree of residual connections present in the WaveNet and due to the difficulty of learning high-quality speaker embeddings. We observed that several patterns tend to yield high performance:\n\u2022 Site-Specific Speaker Embeddings: For every use site in the model architecture, transform the shared speaker embedding to the appropriate dimension and form through an affine projection and a nonlinearity.\n\u2022 Recurrent Initialization: Initialize recurrent layer hidden states with site-specific speaker embeddings.\n\u2022 Input Augmentation: Concatenate a site-specific speaker embedding to the input at every timestep of a recurrent layer.\n\u2022 Feature Gating: Multiply layer activations elementwise with a site-specific speaker embedding to render adaptable information flow. 4\n3We find that these changes reduce model size by a factor of \u223c7 and speed up inference by \u223c25%, while yielding no perceptual change in quality. However, we do not focus on demonstrating these claims in this paper.\n4We hypothesize that feature gating lets the model learn the union of all necessary features while allowing speaker embeddings to determine what features are used for each speaker and how much influence they will have on the activations.\nConv-BN-Res\nPhoneme 1 Phoneme n\nStacked Bi-GRU\nBi-GRU\nFC ! + F0\nFilter-Bank\nFC ! FC Voiced\nSpeaker \u2026\nFC\nsoftsign)+)1 + \u2a09\nFC\nsoftsign)+)1 f\nFC\nPhoneme 1 Phoneme n\nStacked Bi-GRU Durations (bucketed)\nSpeaker\n\u2026 FC\nFC\nconcat concat\nMLP\nCRF \u2a09\u03bcF \u03bc !F\nEncoder CBFG\nChar 1 Char n\u2026Speaker\nFC\nBi-GRU\nHighway Layers\nMLP\nmax/pool\nFilter-Bank + BN + ReLu\nFilter-Bank + BN + ReLu\n+\nFC\nsoftsign\ntile\nsoftsign\nMLP\nMel 1 Mel m\u2026Speaker\ntile\nGRU + Attention\nFC\nsoftsign\nFC\nStacked Residual GRUsoftsign\nFC Mel\nDecoder CBFG\nFC Log-Mel\nMel 1 Mel m\u2026\nSpeaker\nDuration Frequency Vocal Speaker\nFC\nsoftsign\nConvFC\nConv + BN\n\u2a09\nReLu6\n+\nConv-BN-Res \u2026\nStacked Bi-GRU\nDropout\nsoftsign\nFC\nDropout\nCTC Phoneme pairs\n(a)\nPhoneme 1 Phoneme n\nStacked Bi-GRU\nBi-GRU\nFC\n!\n+\nF0\nFilter-Bank\nFC\n!\nFC\nVoiced\nSpeaker \u2026\nFC\nsoftsign)+)1\n+ \u2a09\nFC\nsoftsign)+)1\n\u03bc\nstd\nf\nFC\nPhoneme 1 Phoneme n\nStacked Bi-GRU\nDurations (bucketed)\nSpeaker\n\u2026 FC\nFC\nconcat concat\nMLP\nCRF\n(b)\nConv-BN-Res\nPhoneme 1 Phoneme n\nStacked Bi-GRU\nBi-GRU\nFC\n!\n+\nF0\nFilter-Bank\nFC\n!\nFC\nVoiced\nSpeaker \u2026\nFC\nsoftsign)+)1\n+ \u2a09\nFC\nsoftsign)+)1\nf\nFC\nPhoneme 1 Phoneme n\nStacked Bi-GRU\nDurations (bucketed)\nSpeaker\n\u2026 FC\nFC\nconcat concat\nMLP\nCRF\n\u2a09\u03bcF \u03bc\n!F\nEncoder CBFG\nChar 1 Char n\u2026Speaker\nFC\nBi-GRU\nHighway Layers\nMLP\nmax/pool\nFilter-Bank + BN + ReLu\nFilter-Bank + BN + ReLu\n+\nFC\nsoftsign\ntile\nsoftsign\nMLP\nMel 1 Mel m\u2026Speaker\ntile\nGRU + Attention\nFC\nsoftsign\nFC\nStacked Residual GRUsoftsign\nFC Mel\nDecoder CBFG\nFC Log-Mel\nMel 1 Mel m\u2026\nSpeaker\nDuration Frequency Vocal Speaker\nFC\nsoftsign\nConvFC\nConv + BN\n\u2a09\nReLu6\n+\nConv-BN-Res \u2026\nStacked Bi-GRU\nDropout\nsoftsign\nFC\nDropout\nCTC Phoneme pairs\n\u03c9\n(c)\nFigure 2: Architecture for the multi-speaker (a) segmentation, (b) duration, and (c) frequency model.\nNext, we describe how speaker embeddings are used in each architecture."}, {"heading": "4.1 Multi-Speaker Deep Voice 2", "text": "The Deep Voice 2 models have separate speaker embeddings for each model. Yet, they can be viewed as chunks of a larger speaker embedding, which are trained independently."}, {"heading": "4.1.1 Segmentation Model", "text": "In multi-speaker segmentation model, we use feature gating in the residual connections of the convolution layers. Instead of Eq. 2, we multiply the batch-normalized activations by a site-specific speaker embedding:\nh(l) = relu ( h(l\u22121) +BN ( W \u2217 h(l\u22121) ) \u00b7 gs ) , (5)\nwhere gs is a site-specific speaker embedding. The same site-specific embedding is used for all the convolutional layers. In addition, we initialize each of the recurrent layers with a second site specific embedding. Similarly, each layer uses the same site-specific embedding, rather than having a separate embedding per layer."}, {"heading": "4.1.2 Duration Model", "text": "The multi-speaker duration model uses speaker-dependent recurrent initialization and input augmentation. A site-specific embedding is used to initialize RNN hidden states, and another site-specific embedding is provided as input to the first RNN layer by concatenating it to the feature vectors."}, {"heading": "4.1.3 Frequency Model", "text": "The multi-speaker frequency model uses recurrent initialization, which initializes the recurrent layers (except for the recurrent output layer) with a single site-specific speaker-embedding. As described in Section 3.3, the recurrent and convolutional output layers in the single-speaker frequency model predict a normalized frequency, which is then converted into the true F0 by a fixed linear transformation. The linear transformation depends on the mean and standard deviation of observed F0 for the speaker. These values vary greatly between speakers: male speakers, for instance, tend to have a much lower mean F0. To better adapt to these variations, we make the mean and standard deviation trainable model parameters and multiply them by scaling terms which depend on the speaker embeddings. Specifically, instead of Eq. 4, we compute the F0 prediction\nF0 = \u00b5F0 \u00b7 ( 1 + softsign ( V\u00b5 T gf )) + \u03c3F0 \u00b7 ( 1 + softsign ( V\u03c3 T gf )) \u00b7 f, (6)\nwhere gf is a site-specific speaker embedding, \u00b5F0 and \u03c3F0 are trainable scalar parameters initialized to the F0 mean and standard deviation on the dataset, and V\u00b5 and V\u03c3 are trainable parameter vectors.\nConv-BN-Res\nPhoneme 1 Phoneme n\nStacked Bi-GRU\nBi-GRU\nFC\n!\n+ F0\nFilter-Bank\nFC\n!\nFC\nVoiced\nSpeaker \u2026\nFC\nsoftsign)+)1 + \u2a09\nFC\nsoftsign)+)1\nf\nFC\nPhoneme 1 Phoneme n\nStacked Bi-GRU\nDurations (bucketed)\nSpeaker\n\u2026 FC\nFC\nconcat concat\nMLP\nCRF \u2a09\u03bcF \u03bc !F\nMel 1 Mel m\u2026\nSpeaker\nDuration Frequency Vocal Speaker\nFC\nsoftsign\nConvFC\nConv + BN\n\u2a09\nReLu6\n+\nConv-BN-Res \u2026\nStacked Bi-GRU\nDropout\nsoftsign\nFC\nDropout\nCTC Phoneme pairs\n\u03c9\nText Synthesized Speech\nEncoder CBFG\nChar 1 Char n\u2026 Tacotron Speaker\nFC\nBi-GRU\nHighway Layers\nMLP\nmax9pool\nFilter-Bank + BN + ReLu\nFilter-Bank + BN + ReLu\n+\nFC\nsoftsign\ntile\nsoftsign\nMLP\nMel iTacotron Speaker\nAttention\nFC\nsoftsign\nFC\nStacked Residual GRUsoftsign\nFC\nDecoder CBFG\nFC Spectrogram\nVocal\nAudio\nVocal Speaker\nGrif<in9Lim \u22c1\nMel i-1 Mel i+1\n\u2026 \u2026\nGRU i-1 GRU i-1 GRU i+1\u2026 \u2026\nMel i+1\nAudio\nPronunciation Dictionary Phonemes upsample\nF0\nUpsampled PhonemesPhonemesFigure 3: Tacotron with speaker conditioni g in the Encoder CBHG module and decoder with two ways to convert spectrogram to audio: Griffin-Lim or our speaker-conditioned Vocal model."}, {"heading": "4.1.4 Vocal Model", "text": "The multi-speaker vocal model uses only input augmentation, with the site-specific speaker embedding concatenated onto each input frame of the conditioner. This differs from the global conditioning suggested in Oord et al. (2016) and allows the speaker embedding to influence the local conditioning network as well.\nWithout speaker embeddings, the vocal model is still able to generate somewhat distinct-sounding voices because of the disctinctive features provided by the frequency and duration models. Yet, having speaker embeddings in the vocal model increases the audio quality. We indeed observe that the embeddings converge to a meaningful latent space."}, {"heading": "4.2 Multi-Speaker Tacotron", "text": "In addition to extending Deep Voice 2 with speaker embeddings, we also extend Tacotron (Wang et al., 2017), a sequence-to-sequence character-to-waveform model. When training multi-speaker Tacotron variants, we find that model performance is highly dependent on model hyperparameters, and that some models often fail to learn attention mechanisms for a small subset of speakers. We also find that if the speech in each audio clip does not start at the same timestep, the models are much less likely to converge to a meaningful attention curve and recognizable speech; thus, we trim all initial and final silence in each audio clip. Due to the sensitivity of the model to hyperparameters and data preprocessing, we believe that additional tuning may be necessary to obtain maximal quality. Thus, our work focuses on demonstrating that Tacotron, like Deep Voice 2, is capable of handling multiple speakers through speaker embeddings, rather than comparing the quality of the two architectures."}, {"heading": "4.2.1 Character-to-Spectrogram Model", "text": "The Tacotron character-to-spectrogram architecture consists of a convolution-bank-highway-GRU (CBHG) encoder, an attentional decoder, and a CBHG post-processing network. Due to the complexity of the architecture, we leave out a complete description and instead focus on our modifications.\nWe find that incorporating speaker embeddings into the CBHG post-processing network degrades output quality, whereas incorporating speaker embeddings into the character encoder is necessary. Without a speaker-dependent CBHG encoder, the model is incapable of learning its attention mechanism and cannot generate meaningful output (see Appendix C.2 for speaker-dependent attention visualizations). In order to condition the encoder on the speaker, we use one site-specific embedding as an extra input to each highway layer at each timestep and initialize the CBHG RNN state with a second site-specific embedding.\nWe also find that augmenting the decoder with speaker embeddings is helpful. We use one site-specific embedding as an extra input to the decoder pre-net, one extra site-specific embedding as the initial attention context vector for the attentional RNN, one site-specific embedding as the initial decoder GRU hidden state, and one site-specific embedding as a bias to the tanh in the content-based attention mechanism."}, {"heading": "4.2.2 Spectrogram-to-Waveform Model", "text": "The original Tacotron implementation in (Wang et al., 2017) uses the Griffin-Lim algorithm to convert spectrograms to time-domain audio waveforms by iteratively estimating the unknown phases.5 We observe that minor noise in the input spectrogram causes noticeable estimation errors in the GriffinLim algorithm and the generated audio quality is degraded. To produce higher quality audio using Tacotron, instead of using Griffin-Lim, we train a WaveNet-based neural vocoder to convert from linear spectrograms to audio waveforms. The model used is equivalent to the Deep Voice 2 vocal model, but takes linear-scaled log-magnitude spectrograms instead of phoneme identity and F0 as input. The combined Tacotron-WaveNet model is shown in Fig. 3. As we will show in 5.1, WaveNet-based neural vocoder indeed significantly improves single-speaker Tacotron as well."}, {"heading": "5 Results", "text": "In this section, we will present the results on both single-speaker and multi-speaker speech synthesis using the described architectures. All model hyperparameters are presented in Appendix B."}, {"heading": "5.1 Single-Speaker Speech Synthesis", "text": "We train Deep Voice 1, Deep Voice 2, and Tacotron on an internal English speech database containing approximately 20 hours of single-speaker data. The intermediate evaluations of models in Deep Voice 1 and Deep Voice 2 can be found in Table 3 within Appendix A. We run an MOS evaluation using the crowdMOS framework (Ribeiro et al., 2011) to compare the quality of samples (Table 1). The results show conclusively that the architecture improvements in Deep Voice 2 yield significant gains in quality over Deep Voice 1. They also demonstrate that converting Tacotron-generated spectrograms to audio using WaveNet is preferable to using the iterative Griffin-Lim algorithm."}, {"heading": "5.2 Multi-Speaker Speech Synthesis", "text": "We train all the aforementioned models on the VCTK dataset with 44 hours of speech, which contains 108 speakers with approximately 400 utterances each. We also train all models on an internal dataset of audiobooks, which contains 477 speakers with 30 minutes of audio each (for a total of \u223c238 hours). The consistent sample quality observed from our models indicates that our architectures can easily learn hundreds of distinct voices with a variety of different accents and cadences. We also observe that the learned embeddings lie in a meaningful latent space (see Fig. 4 as an example and Appendix C for more details).\nIn order to evaluate the quality of the synthesized audio, we run MOS evaluations using the crowdMOS framework, and present the results in Table 2. We purposefully include ground truth samples in the set being evaluated, because the accents in datasets are likely to be unfamiliar to our North American crowdsourced raters and will thus be rated poorly due to the accent rather than due to the model quality. By including ground truth samples, we are able to compare the MOS of the models with the ground truth MOS and thus evaluate the model quality rather than the data quality; however, the resulting MOS may be lower, due to the implicit comparison with the ground truth samples. Overall, we observe that the Deep Voice 2 model can approach an MOS value that is very close to the ground truth, when low sampling rate and companding/expanding taken into account.\n5Estimation of the unknown phases is done by repeatedly converting between frequency and time domain representations of the signal using the short-time Fourier transform and its inverse, substituting the magnitude of each frequency component to the predicted magnitude at each step.\nA multi-speaker TTS system with high sample quality but indistinguishable voices would result in high MOS, but fail to meet the desired objective of reproducing the input voices accurately. To show that our models not only generate high quality samples, but also generate distinguishable voices, we also measure the classification accuracy of a speaker discriminative model on our generated samples. The speaker discriminative is a convolutional network trained to classify utterances based on their speaker, trained on the same dataset as the TTS systems themselves. If the voices were indistinguishable (or the audio quality was low), the classification accuracy would be much lower for synthesized samples than it is for the ground truth samples. As we demonstrate in Table 2, classification accuracy demonstrates that samples generated from our models are as distinguishable as the ground truth samples (see Appendix D for more details). The classification accuracy is only significantly lower for Tacotron with WaveNet, and we suspect that generation errors in the spectrogram is exacerbated by the WaveNet, as it is trained with ground truth spectrograms."}, {"heading": "6 Conclusion", "text": "In this work, we explore how entirely-neural speech synthesis pipelines may be extended to multispeaker text-to-speech via low-dimensional trainable speaker embeddings. We start by presenting Deep Voice 2, an improved single-speaker model. Next, we demonstrate the applicability of our technique by training both multi-speaker Deep Voice 2 and multi-speaker Tacotron models, and evaluate their quality through MOS. In conclusion, we use our speaker embedding technique to create high quality text-to-speech systems and conclusively show that neural speech synthesis models can learn effectively from small amounts of data spread among hundreds of different speakers.\nThe results presented in this work suggest many directions for future research. Future work may test the limits of this technique and explore how many speakers these models can generalize to, how little data is truly required per speaker for high quality synthesis, whether new speakers can be added to a system by fixing model parameters and solely training new speaker embeddings, and whether the speaker embeddings can be used as a meaningful vector space, as is possible with word embeddings."}, {"heading": "B Model Hyperparameters", "text": "All hyperparameters of the models used in this paper are provided in Table 4.\nTo speed up the training of character-to-spectrogram model in Tacotron in our experiments, we added a penalty term in the form CTC loss (obtained from the attention hidden states) to the overall loss function. We do not have a clear conclusive evidence that it improves the overall audio quality but we observed faster convergence in some cases.\nLearning rate is presented as a triple ` \u2013 r \u2013 s, which means that the initial learning rate of ` was decayed by a factor of r every s iterations. All models use the Adam optimization technique (Kingma and Ba, 2014) with \u03b21 = 0.9, \u03b22 = 0.99, and \u03b5 = 10\u22128.\nConvolutional layers are presented as l\u00d7, o, h\u00d7 w filters, which means that there are l convolutional layers, and o (output channels) filters in each layer. The filter size is h \u00d7 w, where height h is in frequency bins and width w is in time frames.\nAny unspecified details or hyperparameters for Deep Voice 2 are identical to those from the best models used in the original implementation of Deep Voice 1 (Arik et al., 2017). Similarly, any unspecified details or hyperparameters for our Tacotron implementation are identical to those from the best models used in the original implementation of Tacotron (Wang et al., 2017).\nC Interpretation of Learned Embeddings\nIn this section, we explore the consequences of speaker-dependent models on intermediate model outputs, model activations, and the distributions of the learned embeddings.\nC.1 Speaker-Dependent Fundamental Frequency Profiles\nFigure 5. Time-dependent fundamental frequency values generated by the model when the inputs and embeddings correspond to Speaker S6 or S24. Speaker S6 is a 23 year-old female with a Southern England accent and Speaker S24 is a 24 year-old male with an Indian accent. The pronounced sentence is \u201cSix spoons of fresh snow peas five thick slabs of blue cheese and maybe a snack for her brother Bob\u201d.\nTo demonstrate the significance of the speaker embeddings, we run inference for the frequency model with the speaker embedding vectors corresponding to the actual speaker and a different speaker. As shown in Fig. 6, while the input phoneme features are dominant in determining the overall shape of the fundamental frequency profiles, the actual values are highly speaker dependent. For example, when the speaker embedding vector of a male is substituted with the speaker embedding vector of a female, the output frequency profile would cause generation of a female voice despite all the other input features correspond to the correct male speaker.\nC.2 Speaker-Dependent Attention Plots\nFig. 7 shows the learned attention plots for three different speakers who talk at different speeds. It demonstrates that the modifications in the Tacotron encoder architecture are highly effective making the attention model speaker dependent such that different portions of the input text can be focused depending on the speech features of the speaker.\nC.3 Principal Components of the Embeddings\nWe explore the latent space of the learned speaker embeddings by visualizing them in a lower dimensional space. Fig. 8 and Fig. 9 show the first two principal components of the learned embeddings of the vocal model and character-to-spectrogram model respectively. Although they are initialized randomly and completely trained based on a loss function related to the generative quality, we can observe discriminative patterns in the learned embeddings. Gender of the speaker is the most apparent discriminative pattern in these learned embeddings that even a linear classifier fit on the shown twodimensional space can classify the gender with a very high accuracy. Besides, we observe apparent discriminative patterns for the region of the speaker. 6 In the two-dimensional space, especially Great Britain and North America regions seem highly separable.\n6The regions are determined according to https://en.wikipedia.org/wiki/Regional_accents_of_ English"}, {"heading": "D Speaker Discriminative Model", "text": "To compute multi-speaker classification accuracy, we use a speaker discriminative model trained on the ground truth data set of multiple speakers. Although using another discriminator model such as Deep Speaker (Li et al., 2017) or other methods would also suffice, we choose to create our own deep learning based discriminative model. We note that our accuracy results on the test set are on par with the state-of-the-art speaker classification methods in the literature.\nOur architecture is depicted in Fig. 10. We use mel-frequency cepstral coefficients (MFCCs) computed after resampling the input to a constant sampling frequency. Then, we employ twodimensional convolutional layers convolving over both time and cepstral frequency bands, with a relu nonlinearity clipped to a maximum of six after each convolutional layer. The last convolutional layer is followed by max-pool layer. We then mean-pool over time for all utterance timesteps and apply a fully connected layer with a relu nonlinearity followed by a fully connected output layer with a softmax nonlinearity and cross-entropy classification loss. In order to avoid overfitting to the dataset, we apply dropout after every relu nonlinearity.\nIn order to demonstrate that the classification results are not sensitive to the choice of the hyperparameters of the discriminative model, we demonstrate the classification accuracy for other choices in this section. Hyperparameters for all the discriminator models are available in Table 6. Only the results for the models, D3 and D8, are presented in Table 2, as they yielded the highest validation set accuracy."}], "references": [{"title": "Deep voice: Real-time neural text-to-speech", "author": ["S.O. Arik", "M. Chrzanowski", "A. Coates", "G. Diamos", "A. Gibiansky", "Y. Kang", "X. Li", "J. Miller", "J. Raiman", "S. Sengupta", "M. Shoeybi"], "venue": null, "citeRegEx": "Arik et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arik et al\\.", "year": 2017}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis", "author": ["Y. Fan", "Y. Qian", "F.K. Soong", "L. He"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Fan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2015}, {"title": "Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks", "author": ["C.-C. Hsu", "H.-T. Hwang", "Y.-C. Wu", "Y. Tsao", "H.-M. Wang"], "venue": null, "citeRegEx": "Hsu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2017}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "K. Kawakami", "S. Subramanian", "C. Dyer"], "venue": "In Proc. NAACL-HLT,", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Deep speaker: an end-to-end neural speaker embedding system", "author": ["C. Li", "X. Ma", "B. Jiang", "X. Li", "X. Zhang", "X. Liu", "Y. Cao", "A. Kannan", "Z. Zhu"], "venue": "arXiv preprint arXiv:1705.02304,", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "SampleRNN: An unconditional end-to-end neural audio generation model", "author": ["S. Mehri", "K. Kumar", "I. Gulrajani", "R. Kumar", "S. Jain", "J. Sotelo", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Mehri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mehri et al\\.", "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["A. v. d. Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Speaker verification using adapted gaussian mixture models", "author": ["D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn"], "venue": "Digital signal processing,", "citeRegEx": "Reynolds et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Reynolds et al\\.", "year": 2000}, {"title": "Crowdmos: An approach for crowdsourcing mean opinion score studies", "author": ["F. Ribeiro", "D. Flor\u00eancio", "C. Zhang", "M. Seltzer"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Ribeiro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2011}, {"title": "Median-based generation of synthetic speech durations using a non-parametric approach", "author": ["S. Ronanki", "O. Watts", "S. King", "G.E. Henter"], "venue": null, "citeRegEx": "Ronanki et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ronanki et al\\.", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Char2wav: End-to-end speech synthesis", "author": ["J. Sotelo", "S. Mehri", "K. Kumar", "J.F. Santos", "K. Kastner", "A. Courville", "Y. Bengio"], "venue": "In ICLR2017 workshop submission,", "citeRegEx": "Sotelo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sotelo et al\\.", "year": 2017}, {"title": "Tacotron: Towards end-to-end speech synthesis", "author": ["Y. Wang", "R. Skerry-Ryan", "D. Stanton", "Y. Wu", "R.J. Weiss", "N. Jaitly", "Z. Yang", "Y. Xiao", "Z. Chen", "S. Bengio"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "A study of speaker adaptation for DNN-based speech synthesis", "author": ["Z. Wu", "P. Swietojanski", "C. Veaux", "S. Renals", "S. King"], "venue": "In INTERSPEECH,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Robust speaker-adaptive hmm-based text-to-speech synthesis", "author": ["J. Yamagishi", "T. Nose", "H. Zen", "Z.-H. Ling", "T. Toda", "K. Tokuda", "S. King", "S. Renals"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Yamagishi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yamagishi et al\\.", "year": 2009}, {"title": "On the training of dnn-based average voice model for speech synthesis", "author": ["S. Yang", "Z. Wu", "L. Xie"], "venue": "In Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2016 Asia-Pacific,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis", "author": ["H. Zen", "H. Sak"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Zen and Sak.,? \\Q2015\\E", "shortCiteRegEx": "Zen and Sak.", "year": 2015}, {"title": "Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices", "author": ["H. Zen", "Y. Agiomyrgiannakis", "N. Egberts", "F. Henderson", "P. Szczepaniak"], "venue": null, "citeRegEx": "Zen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zen et al\\.", "year": 2016}, {"title": "Discriminative Model To compute multi-speaker classification accuracy, we use a speaker discriminative model trained on the ground truth data set of multiple speakers. Although using another discriminator model such as Deep Speaker (Li et al., 2017) or other methods would also suffice, we choose to create our own deep learning based discriminative model", "author": ["D Speaker"], "venue": null, "citeRegEx": "Speaker,? \\Q2017\\E", "shortCiteRegEx": "Speaker", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "We present Deep Voice 2, an improved architecture based on Deep Voice 1 (Arik et al., 2017).", "startOffset": 72, "endOffset": 91}, {"referenceID": 9, "context": "We introduce a WaveNet-based (Oord et al., 2016) spectrogram-to-audio neural vocoder, and use it with Tacotron (Wang et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 15, "context": ", 2016) spectrogram-to-audio neural vocoder, and use it with Tacotron (Wang et al., 2017) as a replacement for Griffin-Lim audio generation.", "startOffset": 70, "endOffset": 89}, {"referenceID": 20, "context": "With regards to single-speaker speech synthesis, deep learning has been used for a variety of subcomponents, including duration prediction (Zen et al., 2016), fundamental frequency prediction (Ronanki et al.", "startOffset": 139, "endOffset": 157}, {"referenceID": 12, "context": ", 2016), fundamental frequency prediction (Ronanki et al., 2016), acoustic modeling (Zen and Sak, 2015), and more recently autoregressive sample-bysample audio waveform generation (e.", "startOffset": 42, "endOffset": 64}, {"referenceID": 19, "context": ", 2016), acoustic modeling (Zen and Sak, 2015), and more recently autoregressive sample-bysample audio waveform generation (e.", "startOffset": 27, "endOffset": 46}, {"referenceID": 8, "context": ", 2016), acoustic modeling (Zen and Sak, 2015), and more recently autoregressive sample-bysample audio waveform generation (e.g., Oord et al., 2016; Mehri et al., 2016).", "startOffset": 123, "endOffset": 168}, {"referenceID": 0, "context": "Our contributions build upon recent work in entirely neural TTS systems, including Deep Voice 1 (Arik et al., 2017), Tacotron (Wang et al.", "startOffset": 96, "endOffset": 115}, {"referenceID": 15, "context": ", 2017), Tacotron (Wang et al., 2017), and Char2Wav (Sotelo et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 14, "context": ", 2017), and Char2Wav (Sotelo et al., 2017).", "startOffset": 22, "endOffset": 43}, {"referenceID": 3, "context": "More recently, speaker adaptation has been tackled with generative adversarial networks (GANs) (Hsu et al., 2017).", "startOffset": 95, "endOffset": 113}, {"referenceID": 0, "context": "Our contributions build upon recent work in entirely neural TTS systems, including Deep Voice 1 (Arik et al., 2017), Tacotron (Wang et al., 2017), and Char2Wav (Sotelo et al., 2017). While these works focus on building single-speaker TTS systems, our paper focuses on extending neural TTS systems to handle multiple speakers with less data per speaker. Our work is not the first to attempt a multi-speaker TTS system. For instance, in traditional HMMbased TTS synthesis (e.g., Yamagishi et al., 2009), an average voice model is trained using multiple speakers\u2019 data, which is then adapted to different speakers. DNN-based systems (e.g., Yang et al., 2016) have also been used to build average voice models, with i-vectors representing speakers as additional inputs and separate output layers for each target speaker. Similarly, Fan et al. (2015) uses a shared hidden representation among different speakers with speaker-dependent output layers predicting vocoder parameters (e.", "startOffset": 97, "endOffset": 846}, {"referenceID": 0, "context": "Our contributions build upon recent work in entirely neural TTS systems, including Deep Voice 1 (Arik et al., 2017), Tacotron (Wang et al., 2017), and Char2Wav (Sotelo et al., 2017). While these works focus on building single-speaker TTS systems, our paper focuses on extending neural TTS systems to handle multiple speakers with less data per speaker. Our work is not the first to attempt a multi-speaker TTS system. For instance, in traditional HMMbased TTS synthesis (e.g., Yamagishi et al., 2009), an average voice model is trained using multiple speakers\u2019 data, which is then adapted to different speakers. DNN-based systems (e.g., Yang et al., 2016) have also been used to build average voice models, with i-vectors representing speakers as additional inputs and separate output layers for each target speaker. Similarly, Fan et al. (2015) uses a shared hidden representation among different speakers with speaker-dependent output layers predicting vocoder parameters (e.g., line spectral pairs, aperiodicity parameters, etc.). For further context, Wu et al. (2015) empirically studies DNN-based multi-speaker modeling.", "startOffset": 97, "endOffset": 1072}, {"referenceID": 0, "context": "In this section, we present Deep Voice 2, a neural TTS system based on Deep Voice 1 (Arik et al., 2017).", "startOffset": 84, "endOffset": 103}, {"referenceID": 0, "context": "We keep the general structure of the Deep Voice 1 (Arik et al., 2017), as depicted in Fig.", "startOffset": 50, "endOffset": 69}, {"referenceID": 4, "context": "In contrast, Deep Voice 2\u2019s segmentation model layers instead compute h = relu ( h(l\u22121) +BN ( W (l) \u2217 h(l\u22121) )) , (2) where BN is batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 150, "endOffset": 175}, {"referenceID": 6, "context": "We model the sequence by a conditional random field (CRF) with pairwise potentials at output layer (Lample et al., 2016).", "startOffset": 99, "endOffset": 120}, {"referenceID": 1, "context": "consists of multiple layers: firstly, bidirectional gated recurrent unit (GRU) layers (Cho et al., 2014) generate hidden states from the input features.", "startOffset": 86, "endOffset": 104}, {"referenceID": 9, "context": "4 Vocal Model The Deep Voice 2 vocal model is based on a WaveNet architecture (Oord et al., 2016) with a two-layer bidirectional QRNN (Bradbury et al.", "startOffset": 78, "endOffset": 97}, {"referenceID": 9, "context": "This differs from the global conditioning suggested in Oord et al. (2016) and allows the speaker embedding to influence the local conditioning network as well.", "startOffset": 55, "endOffset": 74}, {"referenceID": 15, "context": "2 Multi-Speaker Tacotron In addition to extending Deep Voice 2 with speaker embeddings, we also extend Tacotron (Wang et al., 2017), a sequence-to-sequence character-to-waveform model.", "startOffset": 112, "endOffset": 131}, {"referenceID": 15, "context": "2 Spectrogram-to-Waveform Model The original Tacotron implementation in (Wang et al., 2017) uses the Griffin-Lim algorithm to convert spectrograms to time-domain audio waveforms by iteratively estimating the unknown phases.", "startOffset": 72, "endOffset": 91}, {"referenceID": 11, "context": "We run an MOS evaluation using the crowdMOS framework (Ribeiro et al., 2011) to compare the quality of samples (Table 1).", "startOffset": 54, "endOffset": 76}], "year": 2017, "abstractText": "We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.", "creator": "LaTeX with hyperref package"}}}