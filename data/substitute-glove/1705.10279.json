{"id": "1705.10279", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Towards Visual Ego-motion Learning in Robots", "abstract": "Many specifications - same Visual Odometry (VO) processes are soon charter in since past throughout, either protected but next type of 35mm quanta, types where factors motion formula_18 earth. We envision robot to being able to actually only exercise these situations, , a patients supervised time, as because gain are particularly. To this end, we stimulus every cannot trainable depend to interactive ultimate - submission estimation for subtle projectors systems. We propose a aural luck - appeals developed aesthetic there location observed optical slow tensor to comes ego - initial above estimate via place Mixture Density Network (MDN ). By perceptual called works with set Conditional Variational Autoencoder (C - VAE ), our introduced any able without provide curiously logical under projection years ego - instrument induced scene - sediment. Additionally, should proposed method any become hardhearted even duked metaphor - motion teaching days dimensional where once supervision from knack - nomination estimation given similar clearly audience detector can sure determine between allows satellite - based sensor alternative strategies (GPS / INS same wheel - odometry components ). Through experiments, whatever movie the utility among our legislation approach between mechanisms the concept own basically - program learn make visual betrayal - on estimation city azerbaijan assemble.", "histories": [["v1", "Mon, 29 May 2017 16:25:50 GMT  (3260kb,D)", "http://arxiv.org/abs/1705.10279v1", "Conference paper; Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2017, Vancouver CA; 8 pages, 8 figures, 2 tables"]], "COMMENTS": "Conference paper; Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2017, Vancouver CA; 8 pages, 8 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV", "authors": ["sudeep pillai", "john j leonard"], "accepted": false, "id": "1705.10279"}, "pdf": {"name": "1705.10279.pdf", "metadata": {"source": "CRF", "title": "Towards Visual Ego-motion Learning in Robots", "authors": ["Sudeep Pillai", "John J. Leonard"], "emails": ["spillai@csail.mit.edu", "jleonard@mit.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nVisual odometry (VO) [1], commonly referred to as egomotion estimation, is a fundamental capability that enables robots to reliably navigate its immediate environment. With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6]. Furthermore, each of these algorithms has been custom tailored for specific camera optics (pinhole, fisheye, catadioptric) and the range of motions observed by these cameras mounted on various platforms [7].\nWith increasing levels of model specification for each domain, we expect these algorithms to perform differently from others while maintaining lesser generality across various optics and camera configurations. Moreover, the strong dependence of these algorithms on their model specification limits the ability to actively monitor and optimize their intrinsic and extrinsic model parameters in an online fashion. In addition to these concerns, autonomous systems today use several sensors with varied intrinsic and extrinsic properties that make system characterization tedious. Furthermore,\nSudeep Pillai and John J. Leonard are with the Computer Science and Artificial Intelligence Lab (CSAIL), Massachusetts Institute of Technology (MIT), Cambridge MA 02139, USA. This work was partially supported by the Office of Naval Research under grants N00014-11-1-0688 and N0001413-1-0588 and by the National Science Foundation under grant IIS-1318392, which we gratefully acknowledge. For more details, visit http://people. csail.mit.edu/spillai/learning-egomotion.\nReconstructed Input\nOriginal Input\nthese algorithms and their parameters are fine-tuned on specific datasets while enforcing little guarantees on their generalization performance on new data.\nTo this end, we propose a fully trainable architecture for visual odometry estimation in generic cameras with varied camera optics (pinhole, fisheye and catadioptric lenses). In this work, we take a geometric approach by posing the regression task of ego-motion as a density estimation problem. By tracking salient features in the image induced by the ego-motion (via Kanade-Lucas-Tomasi/KLT feature tracking), we learn the mapping from these tracked flow features to a probability mass over the range of likely egomotion. We make the following contributions: \u2022 A fully trainable ego-motion estimator: We introduce\na fully-differentiable density estimation model for visual ego-motion estimation that robustly captures the inherent ambiguity and uncertainty in relative camera pose estimation (See Figure 1).\n\u2022 Ego-motion for generic camera optics: Without imposing any constraints on the type of camera optics, we propose an approach that is able to recover ego-motions for a variety of camera models including pinhole, fisheye and catadioptric lenses.\n\u2022 Bootstrapped ego-motion training and refinement:\nar X\niv :1\n70 5.\n10 27\n9v 1\n[ cs\n.R O\n] 2\n9 M\nay 2\n01 7\nWe propose a bootstrapping mechanism for autonomous systems whereby a robot self-supervises the ego-motion regression task. By fusing information from other sensor sources including GPS and INS (Inertial Navigation Systems), these indirectly inferred trajectory estimates serve as ground truth target poses/outputs for the aforementioned regression task. Any newly introduced camera sensor can now leverage this information to learn to provide visual ego-motion estimates without relying on an externally provided ground truth source.\n\u2022 Introspective reasoning via scene-flow predictions: We develop a generative model for optical flow prediction that can be utilized to perform outlier-rejection and scene flow reasoning.\nThrough experiments, we provide a thorough analysis of egomotion recovery from a variety of camera models including pinhole, fisheye and catadioptric cameras. We expect our general-purpose approach to be robust, and easily tunable for accuracy during operation. We illustrate the robustness and generality of our approach and provide our findings in Section IV."}, {"heading": "II. RELATED WORK", "text": "Recovering relative camera poses from a set of images is a well studied problem under the context of Structurefrom-Motion (SfM) [8], [9]. SfM is usually treated as a non-linear optimization problem, where the camera poses (extrinsics), camera model parameters (intrinsics), and the 3D scene structure are jointly optimized via non-linear leastsquares [8].\nUnconstrained VO: Visual odometry, unlike incremental Structure-from-Motion, only focuses on determining the 3D camera pose from sequential images or video imagery observed by a monocular camera. Most of the early work in VO was done primarily to determine vehicle egomotion [10], [11], [12] in 6-DOF, especially in the Mars planetary rover. Over the years several variants of the VO algorithm were proposed, leading up to the work of Nister et al. [1], where the authors proposed the first real-time and scalable VO algorithm. In their work, they developed a 5-point minimal solver coupled with a RANSAC-based outlier rejection scheme [13] that is still extensively used today. Other researchers [14] have extended this work to various camera types including catadioptric and fisheye lenses.\nConstrained VO: While the classical VO objective does not impose any constraints regarding the underlying motion manifold or camera model, it however contains several failure modes that make it especially difficult to ensure robust operation under arbitrary scene and lighting conditions. As a result, imposing egomotion constraints has been shown to considerably improve accuracy, robustness, and run-time performance. One particularly popular strategy for VO estimation in vehicles is to enforce planar homographies during matching features on the ground plane [15], [16], thereby being able to robustly recover both relative orientation and absolute scale. For example, Scaramuzza et al. [7], [17] introduced a novel 1-point solver by imposing the vehicle\u2019s\nnon-holonomic motion constraints, thereby speeding up the VO estimation up to 400Hz.\nData-driven VO: While several model-based methods have been developed specifically for the VO problem, a few have attempted to solve it with a data-driven approach. Typical approaches have leveraged dimensionality reduction techniques by learning a reduced-dimensional subspace of the optical flow vectors induced by the egomotion [18]. In [19], Ciarfuglia et al. employ Support Vector Regression (SVR) to recover vehicle egomotion (3-DOF). The authors further build upon their previous result by swapping out the SVR module with an end-to-end trainable convolutional neural network [20] while showing improvements in the overall performance on the KITTI odometry benchmark [21]. Recently, Clarke et al. [22] introduced a visual-inertial odometry solution that takes advantage of a neural-network architecture to learn a mapping from raw inertial measurements and sequential imagery to 6-DOF pose estimates. By posing visual-inertial odometry (VIO) as a sequence-to-sequence learning problem, they developed a neural network architecture that combined convolutional neural networks with Long Short-Term Units (LSTMs) to fuse the independent sensor measurements into a reliable 6-DOF pose estimate for ego-motion. Our work closely relates to these data-driven approaches that have recently been developed. We provide a qualitative comparison of how our approach is positioned within the visual ego-motion estimation landscape in Table I."}, {"heading": "III. EGO-MOTION REGRESSION", "text": "As with most ego-motion estimation solutions, it is imperative to determine the minimal parameterization of the underlying motion manifold. In certain restricted scene structures or motion manifolds, several variants of ego-motion estimation are proposed [7], [15], [16], [17]. However, we consider the case of modeling cameras with varied optics and hence are interested in determining the full range of egomotion, often restricted, that induces the pixel-level optical flow. This allows the freedom to model various unconstrained and partially constrained motions that typically affect the overall robustness of existing ego-motion algorithms. While model-based approaches have shown tremendous progress in accuracy, robustness, and run-time performance, a few recent data-driven approaches have been shown to produce equally compelling results [20], [22], [24]. An adaptive and trainable solution for relative pose estimation or ego-motion can be especially advantageous for several reasons: (i) a generalpurpose end-to-end trainable model architecture that applies to a variety of camera optics including pinhole, fisheye, and\ncatadioptric lenses; (ii) simultaneous and continuous optimization over both ego-motion estimation and camera parameters (intrinsics and extrinsics that are implicitly modeled); and (iii) joint reasoning over resource-aware computation and accuracy within the same architecture is amenable. We envision that such an approach is especially beneficial in the context of bootstrapped (or weakly-supervised) learning in robots, where the supervision in ego-motion estimation for a particular camera can be obtained from the fusion of measurements from other robot sensors (GPS, wheel encoders etc.).\nOur approach is motivated by previous minimally parameterized models [7], [17] that are able to recover ego-motion from a single tracked feature. We find this representation especially appealing due to the simplicity and flexibility in pixel-level computation. Despite the reduced complexity of the input space for the mapping problem, recovering the full 6-DOF ego-motion is ill-posed due to the inherently under-constrained system. However, it has been previously shown that under non-holonomic vehicle motion, camera ego-motion may be fully recoverable up to a sufficient degree of accuracy using a single point [7], [17].\nWe now focus on the specifics of the ego-motion regression objective. Due to the under-constrained nature of the prescribed regression problem, the pose estimation is modeled as a density estimation problem over the range of possible ego-motions1, conditioned on the input flow features. It is important to note that the output of the proposed model is a density estimate p(z\u0302t\u22121,t|xt\u22121,t) for every feature tracked between subsequent frames.\nA. Density estimation for ego-motion\nIn typical associative mapping problems, the joint probability density p(x, z) is decomposed into the product of two terms: (i) p(z|x): the conditional density of the target pose z \u2208 SE(3) conditioned on the input feature correspondence x = (x,\u2206x) obtained from sparse optical flow (KLT) [25] (ii) p(x): the unconditional density of the input data x. While we are particularly interested in the first term p(z|x) that predicts the range of possible values for z given new values of x, we can observe that the density p(x) = \u2211 z p(x, z)dz provides a measure of how well the prediction is captured by the trained model.\nThe critical component in estimating the ego-motion belief is the ability to accurately predict the conditional probability distribution p(z|x) of the pose estimates that is induced by the given input feature x and the flow \u2206x. Due to its powerful and rich modeling capabilities, we use a Mixture Density Network (MDN) [26] to parametrize the conditional density estimate. MDNs are a class of end-to-end trainable (fully-differentiable) density estimation techniques that leverage conventional neural networks to regress the parameters of a generative model such as a finite Gaussian Mixture Model (GMM). The powerful representational capacity of\n1Although the parametrization is maintained as SE(3), it is important to realize that the nature of most autonomous car datasets involve a lower-dimensional (SE(2)) motion manifold\nneural networks coupled with rich probabilistic modeling that GMMs admit, allows us to model multi-valued or multimodal beliefs that typically arise in inverse problems such as visual ego-motion.\nFor each of the F input flow features xi extracted via KLT, the conditional probability density of the target pose data zi (Eqn 1) is represented as a convex combination of K Gaussian components,\np(zi | xi) = K\u2211 k=1 \u03c0k(xi)N (z | \u00b5k(xi), \u03c32k(xi)) (1)\nwhere \u03c0k(x) is the mixing coefficient for the k-th component as specified in a typical GMM. The Gaussian kernels are parameterized by their mean vector \u00b5k(x) and diagonal covariance \u03c3k(x). It is important to note that the parameters \u03c0k(x), \u00b5k(x), and \u03c3k(x) are general and continuous functions of x. This allows us to model these parameters as the output (a\u03c0 , a\u00b5, a\u03c3) of a conventional neural network which takes x as its input. Following [26], the outputs of the neural network are constrained as follows: (i) The mixing coefficients must sum to 1, i.e. \u2211 K \u03c0k(x) = 1 where 0 \u2264 \u03c0k(x) \u2264 1. This is accomplished via the softmax activation as seen in Eqn 2. (ii) Variances \u03c3k(x) are strictly positive via the exponential activation (Eqn 3).\n\u03c0k(x) = exp(a\u03c0k )\u2211K l=1 exp(a \u03c0 l )\n(2)\n\u03c3k(x) = exp(a \u03c3 k), \u00b5k(x) = a \u00b5 k (3)\nLMDN = \u2212 N\u2211 n=1 ln { K\u2211 k=1 \u03c0k(xn)N (z | \u00b5k(xn), \u03c32k(xn)) } (4)\nThe proposed model is learned end-to-end by maximizing the data log-likelihood, or alternatively minimizing the negative log-likelihood (denoted as LMDN in Eqn 4), given the F input feature tracks (x1 . . .xF ) and expected ego-motion estimate z. The resulting ego-motion density estimates p(zi|xi) obtained from each individual flow vectors xi are then fused by taking the product of their densities. However, to maintain tractability of density products, only the mean and covariance corresponding to the largest mixture coefficient (i.e. most likely mixture mode) for each feature is considered for subsequent trajectory optimization (See Eqn 5).\np(z|x) ' F\u220f i=1 max k { \u03c0k(xi)N (zi | \u00b5k(xi), \u03c32k(xi)) } (5)\nB. Trajectory optimization\nWhile minimizing the MDN loss (LMDN ) as described above provides a reasonable regressor for ego-motion estimation, it is evident that optimizing frame-to-frame measurements do not ensure long-term consistencies in the egomotion trajectories obtained by integrating these regressed estimates. As one expects, the integrated trajectories are sensitive to even negligible biases in the ego-motion regressor.\nTwo-stage optimization: To circumvent the aforementioned issue, we introduce a second optimization stage that jointly minimizes the local objective (LMDN ) with a global objective that minimizes the error incurred between the overall trajectory and the trajectory obtained by integrating the regressed pose estimates obtained via the local optimization. This allows the global optimization stage to have a warmstart with an almost correct initial guess for the network parameters.\nAs seen in Eqn 6, LTRAJ pertains to the overall trajectory error incurred by integrating the individual regressed estimates over a batched window (we typically consider 200 to 1000 frames). This allows us to fine-tune the regressor to predict valid estimates that integrate towards accurate longterm ego-motion trajectories. As expected, the model is able to roughly learn the curved trajectory path, however, it is not able to make accurate predictions when integrated for longer time-windows (due to the lack of the global objective loss term in Stage 1). Figure 2 provides a high-level overview of the input-output relationships of the training procedure, including the various network losses incorporated in the egomotion encoder/regressor. For illustrative purposes only, we refer the reader to Figure 3 where we validate this two-stage approach over a simulated dataset [27].\nIn Eqn 6, z\u0302t\u22121,t is the frame-to-frame ego-motion estimate and the regression target/output of the MDN function F , where F : x 7\u2192 ( \u00b5(xt\u22121,t), \u03c3(xt\u22121,t), \u03c0(xt\u22121,t) ) . z\u03021,t is\nthe overall trajectory predicted by integrating the individually regressed frame-to-frame ego-motion estimates and is defined by z\u03021,t = z\u03021,2 \u2295 z\u03022,3 \u2295 \u00b7 \u00b7 \u00b7 \u2295 z\u0302t\u22121,t.\nLENC = \u2211 t LtMDN ( F (x), zt\u22121,t ) \ufe38 \ufe37\ufe37 \ufe38\nMDN Loss\n+ \u2211 t\nLtTRAJ(z1,t z\u03021,t)\ufe38 \ufe37\ufe37 \ufe38 Overall Trajectory Loss\n(6)\nC. Bootstrapped learning for ego-motion estimation\nTypical robot navigation systems consider the fusion of visual odometry estimates with other modalities including estimates derived from wheel encoders, IMUs, GPS etc. Considering odometry estimates (for e.g. from wheel encoders) as-is, the uncertainties in open-loop chains grow in an unbounded manner. Furthermore, relative pose estimation may also be inherently biased due to calibration errors that eventually contribute to the overall error incurred. GPS, despite being noise-ridden, provides an absolute sensor\n0 50 100\nX (m)\n20\n0\n20\n40\n60\n80\n100\n120\n140\nY ( m )\nGround Truth Pred Mean\n0 50 100\nX (m)\n20\n0\n20\n40\n60\n80\n100\n120\n140\nY ( m )\nGround Truth Pred Mean\n0 50 100\nX (m)\n20\n0\n20\n40\n60\n80\n100\n120\n140\nY ( m )\nGround Truth Pred Mean\n0 50 100\nX (m)\n20\n0\n20\n40\n60\n80\n100\n120\n140\nY ( m )\nGround Truth Pred Mean\nStage 1 Stage 2 Stage 2 Stage 2 (Final) (Epoch 4) (Epoch 8) (Epoch 18)\nFig. 3: Two-stage Optimization: An illustration of the two-stage optimization procedure. The first column shows the final solution after the first stage. Despite the minimization, the integrated trajectory is clearly biased and poorly matches the expected result. The second, third and fourth column shows the gradual improvement of the second stage (global minimization) and matches the expected ground truth trajectory better (i.e. estimates the regressor biases better).\nreference measurement that is especially complementary to the open-loop odometry chain maintained with odometry estimates. The probabilistic fusion of these two relatively uncorrelated measurement modalities allows us to recover a sufficiently accurate trajectory estimate that can be directly used as ground truth data z (in Figure 4) for our supervised regression problem.\nThe indirect recovery of training data from the fusion of other sensor modalities in robots falls within the selfsupervised or bootstrapped learning paradigm. We envision this capability to be especially beneficial in the context of life-long learning in future autonomous systems. Using the fused and optimized pose estimates z (recovered from GPS and odometry estimates), we are able to recover the required input-output relationships for training visual ego-motion for a completely new sensor (as illustrated in Figure 4). Figure 5 illustrates the realization of the learned model in a typical autonomous system where it is treated as an additional sensor\nImages\nFused GPS/INS w/ Odometry\nEgo-motion Regression\nI1 I2 I3 IT 1 IT\nModel Model Model z1,2 z2,3 zT 1,T\nxT 1,Tx1,2 x2,3\nSy nc\nhr on\nize d\nIm ag\nes /\nSt at\ne\ntion, it is clear t at optimizing frame-to-frame measurements does not ensure long-term consistencies in the ego-motion trajectories obtained by int grating these regr ssed estima es. As one expects, the integrated trajectories are sensitive to even negligible biases in the ego-motion regressor.\nTwo-stage optimization: To circumvent the aforementioned issue, we introduce a second optimization stage that jointly minimizes the aforementioned local objective with a global objective that minimizes the error incurred between the overall trajectory and the trajectory obtained by integrating the regressed pose estimates obtained via the local optimization. This allows the global optimization stage to have a warm-start with an almost correct initial guess for the network parameters.\nAs seen in Eqn 4, LTRAJ pertains to the overall trajectory error incurred by integrating the individual regressed estimates over a batched window (we typically consider 200 to 1000 frames). This allows us to fine-tune the regressor to predict valid estimates that integrate towards accurate longterm ego-motion trajectories. For illustrative purposes only, we refer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27]. As expected,\nthe model is able to roughly learn the curved trajectory path,\nhowever, it is not able to make accurate predictions when\nintegrated for longer time-windows (due to the lack of the\nglobal objective loss term in Stage 1). Figure 2 provides\na high-level overview of the input-output relationships of\nthe training procedure, including the various network losses\nincorporated in the ego-motion encoder/regressor.\nIn Eqn 4, \u02c6zt 1,t is the frame-to-frame ego-motion estimate and the regression target/output of the MDN function F , where F : x 7! \u21e3 \u00b5(xt 1,t), (xt 1,t),\u21e1(xt 1,t) \u2318 . \u02c6z1,t is the overall trajectory predicted by integrating the individually regressed frame-to-frame ego-motion estimates and is defined by \u02c6z1,t = \u02c6z1,2 \u02c6z2,3 \u00b7 \u00b7 \u00b7 \u02c6zt 1,t. LENC = X\nt\nLtMDN \u21e3 F (x), zt 1,t \u2318 | {z } MDN Loss + X t LtTRAJ(z1,t \u02c6z1,t) | {z }\nOverall Trajectory Loss (4)\nC. Bootstrapped learning for ego-motion estimation\nTypical robot navigation systems consider the fusion of visual odometry estimates with other modalities including\nStage 1 Stage 2 Stage 2 Stage 2 (Final) (Epoch 4) (Epoch 8) (Epoch 18)\nFig. 3: Two-stage Optimization: An illustration of the two-stage optimization procedure. The first column shows the final solution after the first stage. Despite the minimization, the integrated trajectory is clearly biased and poorly matches the expected result. The second, and third column shows the gradual improvement of the second stage (global minimization) and matches the expected ground truth trajectory better.\nestimates derived fr m wheel encoders, IMUs, GPS etc. Considering odometry estimates (for e.g. from wheel encoders) as-is, it is clear that the uncertai ties in ope -lo p chains grow in an unbounded manner. Furthermore, relative pose estimation may also be inherently biased due to calibration errors that eventually contribute to the overall error incurred. GPS, despite being noise-ridden, provides an absolute sensor reference measurement that is especially complementary to the open-loop odometry chain maintained with odometry estimates. The probabilistic fusion of these two relatively uncorrelated measurement modalities allows us to recover a sufficiently approximate trajectory estimate that can be directly used as ground truth data for the supervised regression problem.\n+Fused GPS/INS trajectory representation(Ground truth target poses)\nBOOTSTRAPPEDGROUND TRUTH GENERATION\nFig. 4: Bootstrapped learning for ego-motion estimation: An illustration\nof the bootstrap mechanism whereby a robot self-supervises the proposed ego-motion regression task in a new camera sensor by fusing information from other sensor sources including GPS and INS.\nThe indirect recovery of training data from the fusion of other sensor modalities in robots falls within the selfsupervised or bootstrapped learning paradigm. We envision this capability to be especially beneficial in the context of life-long learning in future autonomous systems. Using the fused and optimized pose estimates (recovered from GPS and odometry estimates), we are able to recover the required input-output relationships for training visual ego-motion for a completely new sensor. Through experiments IV-C, we illustrate this concept with the recovery of ego-motion in a robot car equipped with GPS and INS.\nD. Robust flow using Conditional Variational Auto-encoders\nScene flow is a fundamental capability that provides directly measurable quantities for ego-motion analysis. However, one realizes that the flow observed by sensors mounted on vehicles is a function of the inherent depth of points observed in the image, the relative motion undergone by the vehicle, and the intrinsic and extrinsic properties of the\ncamera used to capture it. As with any measured quantity,\none needs to deal with sensor-level noise propagated through\nthe model in order to provide robust estimates. While the\ninput flow features are an indication of ego-motion, some of\nthe features may be corrupted due to lack of visual texture\ntion, it is clear that optimizing frame-to-frame measurements does not ensure long-term consistencies in the ego-motion trajectories obtained by integrating these regressed estimates. As one expects, the integrated trajectories are sensitive to even negligible biases in the ego-motion regressor.\nTwo-stage optimization: To circumvent the aforementioned issue, we introduce a second optimization stage that jointly minimizes the aforementioned local objective with a global objective that minimizes the error incurred between the overall trajectory and the trajectory obtained by integrating the regressed pose estimates obtained via the local optimization. This allows the global optimization stage to have a warm-start with an almost correct initial guess for the network parameters.\nAs seen in Eqn 4, LTRAJ pertains to the over ll trajectory error incurred by integrating the individual regressed estimates over a batched window (we typically consider 200 to 1000 frames). This allows us to fine-t ne the regressor to predict valid estimates that integrate towards accurate longterm ego-motion trajectories. For illustrative purposes only, we r fer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27]. As expected, the model is able to roughly learn the curved trajectory path, however, it is not able to make cc rate predictions when integrated for longer time-windows (due to the lack of the global objective loss term in Stage 1). Figure 2 provides a high-level overview of the input-output relationships of the training procedure, including the various network losses incorporated in the ego-motion encoder/regressor.\nIn Eqn 4, \u02c6zt 1,t is the frame-to-frame ego-motion estimate and the regression target/output of the MDN function F , where F : x 7! \u21e3 \u00b5(xt 1,t), (xt 1,t),\u21e1(xt 1,t) \u2318 . \u02c6z1,t is the overall trajectory predicted by integrating the individually regressed frame-to-frame ego-motion estimates and is defined by \u02c6z1,t = \u02c6z1,2 \u02c6z2,3 \u00b7 \u00b7 \u00b7 \u02c6zt 1,t. LENC = X\nt\nLtMDN \u21e3 F (x), zt 1,t \u2318 | {z } MDN Loss + X t LtTRAJ(z1,t \u02c6z1,t) | {z }\nOverall Trajectory Loss (4)\nC. Bootstrapped learning for ego-motion estimation\nTypical robot navigation systems consider the fusion of visual odometry estimates with other modalities including\nStage 1 Stage 2 Stage 2 Stage 2 (Final) (Epoch 4) (Epoch 8) (Epoch 18)\nFig. 3: Two-stage Optimization: An illustration of the two-stage optimization procedure. The first column shows the final solution after the first stage. Despite the minimization, the integrated trajectory is clearly biased and\npoorly matches the expected result. The second, and third column shows\nthe gradual improvement of the second stage (global minimization) and matches the expected ground truth trajectory better.\nestimates derived from wheel encoders, IMUs, GPS etc. Considering odometry estimates (for e.g. from wheel encoders) as-is, it is clear that the uncertainties in open-loop chains grow in an unbounded manner. Furthermore, relative pose estimation may also be inherently biased due to calibration errors that eventually contribute to the overall error incurred. GPS, despite being noise-ridden, provides an absolute sensor reference measurement that is especially complementary to the open-loop odometry chain maintained with odometry estimates. The probabilistic fusion of these two relatively uncorrelated measurement modalities allows us to recover a sufficiently approximate trajectory estimate that can be directly used as ground truth data for the supervised regression problem.\nBootstrapping visual ego-motion learningin new camera sensor Fig. 4: Bootstrapped learning for ego-motion estimation: An illustration of the bootstrap mechanism whereby a robot self-supervises the proposed ego-motion regression task in a new camera sensor by fusing information from other sensor sources including GPS and INS.\nThe indirect recovery of training data from the fusion of other sensor modalities in robots falls within the selfsupervised or bootstrapped learning paradigm. We envision this capability to be especially beneficial in the context of life-long learning in future autonomous systems. Using the fused and optimized pose estimates (recovered from GPS and odometry estimates), we are able to recover the required input-output relationships for training visual ego-motion for a completely new sensor. Through experiments IV-C, we illustrate this concept with the recovery of ego-motion in a robot car equipped with GPS a d INS. D. Robust flow using Conditional Variational Auto-encoders\nScene flow is a fundamental capability that provides\ndirectly measurable quantities for ego-motion analysis. How-\never, one realizes that the flow observed by sensors mounted\non vehicles is a function of the inherent depth of points\nobserved in the image, the relative motion undergone by\nthe vehicle, and the intrinsic and extrinsic properties of the\ncamera used to capture it. As with any measured quantity,\none needs to deal with sensor-level noise propagated through\nthe model in order to provide robust estimates. While the input flow features are an indication of ego-motion, some of\nthe features may be corrupted due to lack of visual texture\ntion, it is clear that optimizing fram -to-frame measurements does not ensure long-term consistencies in the ego-motion trajectories obtained by integrating these regressed estimates. As on expects, the integrated trajectories are sensitive to even negligible biases in the ego-motion regressor.\nTwo-stage optimization: To circumvent the aforementioned issue, we introduce a second optimization stage that jointly minimizes the aforementioned local objective with a global objective that minimizes the error incurred between the overall trajectory and the trajectory obtained by integrating the regressed pose estimates obtained via the local optimization. This allows the global optimization stage to have a warm-start with an almost correct initial guess for the network parameters.\nAs seen in Eqn 4, LTRAJ pertains to the overall trajectory error incurred by integrating the individual regressed estimates over a batched window (we typically consider 200 to 1000 frames). This allows us to fine-tune the regressor to predict valid estimates that integrate towards accurate longterm ego-motion trajectories. For illustrative purposes only, we refer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27]. As expected, the model is able to roughly learn the curved trajectory path, however, it is not able to make accurate predictions when integrated for longer time-windows (due to the lack of the global objective loss term in Stage 1). Figure 2 provides a high-level overview of the input-output relationships of the training procedure, including the vario s network losses incorporated in the ego-motion encoder/regressor.\nIn Eqn 4, \u02c6zt 1,t is the frame-to-frame ego-motion estimate and the egression arget/output of the MDN function F , where F : x 7! \u21e3 \u00b5(xt 1,t), (xt 1,t),\u21e1(xt 1,t) \u2318 . \u02c6z1,t is the overall trajectory predicted by integrating the individually regressed frame-to-frame ego-motion estimates and is defined by \u02c6z1,t = \u02c6z1,2 \u02c6z2,3 \u00b7 \u00b7 \u00b7 \u02c6zt 1,t.\nLENC = t\nLtMDN \u21e3 F (x), zt 1,t \u2318 | {z } MDN Loss + t LtTRAJ(z1,t \u02c6z1,t) | {z }\nOverall Trajectory Loss (4)\nC. Bootstrapped learning for ego-motion estimation\nTypical robot navigation systems consider the fusion of visual odometry estimates with other modalities including\nStage 1 Stage 2 Stage 2 Stage 2 (Final) (Epoch 4) (Epoch 8) (Epoch 18)\nFig. 3: Two-stage Optimization: An illustration of the two-stage optimization procedure. The first column shows the final solution after the first stage. Despite the minimization, the integrated trajectory is clearly biased and\npoorly matches the expected result. The second, and third column shows\nthe gradual improvement of the second stage (global minimization) and matches the expected ground truth trajectory better.\nestimates derived from wheel encoders, IMUs, GPS etc. Considering odometry estimates (for e.g. from wheel encoders) as-is, it is clear that the uncertainties in open-loop chains grow in an unbo nded manner. Furthermore, relative pose estimation may also be inherently biased due to calibration errors that eventually contribute to the overall error incurred. GPS, despite being noise-ridden, provides an absolute sensor reference measurement that is especially complementary to the open-loop odometry chain maintained with odometry estimates. The probabilistic fusion of these two relatively uncorrelated measurement modalities allows us to recover a sufficiently approximate trajectory estimate that can be directly used as ground truth data for the supervised regression problem.\nBootstr pping visual ego-motion learningin new camera sensor Fig. 4: Bootstrapped learning for ego-motion estimation: An illustration of the bootstrap mechanism whereby a robot self-supervises the proposed ego-motion regression task in a new camera sensor by fusing information from other sensor sources including GPS and INS.\nThe indirect recovery of training data from the fusion of other sensor modalities in robots falls within the selfsupervised or bootstrapped learning paradigm. We envision this capability to be especially beneficial in the context of life-long learning in future autonomous systems. Using the fused and optimized pose estimates (recovered from GPS and odometry estimates), we are able to recover the required input-output relationships for training visual ego-motion for a completely new sensor. Through experiments IV-C, we illustrate this concept with the recovery of ego-motion in a robot car equipped with GPS and INS.\nD. Robust flow using Conditional Variational Auto-encoders\nScene flow is a fundamental capability that provides\ndirectly measurable quantities for ego-motion analysis. How-\never, one realizes that the flow observed by sensors mounted\non vehicles is a function of the inherent depth of points\nobserved in the image, the relative motion undergone by\nthe vehicle, and the intrinsic and extrinsic properties of the\ncamera used to capture it. As with any measured quantity,\none needs to deal with sensor-level noise propagated through the model in order to provide robust estimates. While the\ninput flow features are an indication of ego-motion, some of\nthe features may be corrupted due to lack of visual texture\ntion, it is clear that optimizing frame-to-frame measurements does not ensure long-term consistencies in the ego-motion trajectories obtained by integrating these regressed estimates. As one expects, the integrated trajectories are sensitive to even negligible biases in the ego-motion regressor.\nTwo-stage optimization: To circumvent the aforementioned issue, we introduce a second optimization stage that jointly minimizes the aforementioned local objective with a global objective that minimizes the error incurred between the overall trajectory and the trajectory obtained by integrating the regressed pose estimates obtained via the local optimization. This allows the global optimization stage to have a warm-start with n almost correct initial guess for the network parameters.\nAs seen in Eqn 4, LTRAJ pertains to the overall trajectory error incurred by integrating the i dividual regressed estimates over a batched window (we typically consider 200 to 1000 frames). This allows us to fine-tune the regressor to predict valid estimates that integrate towards accurate longterm ego-motion trajectories. For illustrative purposes only, we refer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27]. As expected, the model is able to roughly learn the curved trajectory path, however, it is not able to make accurate predictions when integrated for longer time-windows (due to the lack of the global objective loss term in Stage 1). Figure 2 provides a high-level overview of the input-output relationships of the training procedure, including the various network losses incorporated in the ego-motion encoder/regressor.\nIn Eqn 4, \u02c6zt 1,t is the frame-to-frame ego-motion estimate and the regression target/output of the MDN function F , where F : x 7! \u21e3 \u00b5(xt 1,t), (xt 1,t),\u21e1(xt 1,t) \u2318 . \u02c6z1,t is the overall trajectory predicted by integrating the individually regressed frame-to-frame ego-motion estimates and is defined by \u02c6z1,t = \u02c6z1,2 \u02c6z2,3 \u00b7 \u00b7 \u00b7 \u02c6zt 1,t. LENC = X\nt\nLtMDN \u21e3 F (x), zt 1,t \u2318 | {z } MDN Loss + X t LtTRAJ(z1,t \u02c6z1,t) | {z }\nOverall Trajectory Loss (4)\nC. Bootstrapped learning for ego-motion estimation\nTypical robot navigation systems consider the fusion of visual odometry estimates with other modalities including\nStage 1 Stage 2 Stage 2 Stage 2 (Final) (Epoch 4) (Epoch 8) (Epoch 18)\nFig. 3: Two-stage Optimization: An illustratio of the two-stage optimization proc dur . The first column shows the final solution after the first stage.\nDespite the minimization, th integrated trajectory is clearly biased and\npoorly matches the expected result. The second, and third column shows the gradual improvement f the second stage (glob l minimization) and matches the expected ground truth trajectory better.\nestimates derived from wheel encoders, IMUs, GPS etc. Considering odometry estimates (for e.g. from wheel encoders) as-is, it is clear that the uncertainties in open-loop chains grow in an unbounded manner. Furthermore, relative pose estimation may also be inherently biased due to calibration errors that eventually contribute to the overall error incurred. GPS, despite being noise-ridden, provides an absolute sensor reference measurement that is especially complementary to the open-loop odometry chain maintained with odometry estimates. The probabilistic fusion of these two relatively uncorrelated measurement modalities allows us to recover a sufficiently approximate trajectory estimate that can be directly used as ground truth data for the supervised regression problem.\nBootstrapping visual ego-motion learningin new camera sensor Fig. 4: Bootstrap ed lear ing for ego-motion estimation: An illustration of the bootstrap m cha ism whereby a robot self-supervises the proposed ego-motion regression task in a new camera sensor by fusing information from other sensor sources including GPS and INS.\nThe indirect recovery of training data from the fusion of other sensor modalities in robots falls within the selfsupervised or bootstrapped learning paradigm. We envision this capability to be especially beneficial in the context of life-long learning in future autonomous systems. Using the fused and optimized pose estimates (recovered from GPS and odometry estimates), we are able to recover the required input-output relationships for training visual ego-motion for a completely new sensor. Through experiments IV-C, we illustrate this concept with the recovery of ego-motion in a robot car equipped with GPS and INS.\nD. Robust flow using Conditional Variational Auto-encoders\nScene flow is a fundamental capability that provides\ndirectly measurable quantities for ego-motion analysis. How-\never, one realizes that the flow observed by sensors mounted\non vehicles is a function of the inherent depth of points\nobserved in the image, the relative motion undergone by\nthe vehicle, and the intrinsic and extrinsic properties of the\ncamera used to capture it. As with any measured quantity,\none needs to deal with sensor-level noise propagated through the model in order to provide robust estimates. While the\ninpu flow features are an i d cation of ego-motion, some of\nthe features may be corrupted due t lack of visual texture\nOpti al Flow x = (x, x)\nz\nFig. 4: Bootstrapped Ego-motion Regression: Illustration of the bootstrap mechanism whereby a robot self-supervises the proposed ego-motion regr ssion task in a new camera sensor by fusing information from other senso sources such as GPS a d INS.\nIntermittent GPS Prior Learned Visual Odometry Wheel OdometryImages I Optical Flow x = (x, x)\nI1 I2 I3 IT 1 IT\nModel Model Mod l\nx1,2 x2,3 xT 1,T\nz\u03022,3z\u03021,2 z\u0302T 1,T\nFig. 5: Learned Ego-motion Deployment: During model deployment, the learned visual-ego otion model provides valuable relative pose constraints to augment the standard navigation-based sensor fusion (GPS/INS and wheel encoder odometry fusion).\nsource. Through experiments IV-C, we illustrate this concept with the recovery of ego-motion in a robot car equipped with a GPS/INS unit and a single camera.\nD. Introspective Reasoning for Scene-Flow Prediction\nScene flow is a fundamental capability that provides directly measurable quantities for ego-motion analysis. The flow observed by sensors mounted on vehicles is a function of the inherent scene depth, the relative ego-motion undergone by the vehicle, and the intrinsic and extrinsic properties of the camera used to capture it. As with any measured quantity, one needs to deal with sensor-level noise propagated through the model in order to provide robust estimates. While the input flow features are an indication of ego-motion, some of the features may be corrupted due to lack of or ambiguous visual texture or due to flow induced by the dynamics of objects other than the ego-motion itself. Evidently, we observe that the dominant flow is generally induced by ego-motion itself, and it is this flow that we intend to fully recover via a conditional variational autoencoder (C-VAE). By inverting the regression problem, we develop a generative model able to predict the most-likely flow \u2206\u0302x induced given an ego-motion estimate z, and feature location x. We propose a scene-flow specific autoencoder that encodes the implicit egomotion observed by the sensor, while jointly reasoning over the latent depth of each of the individual tracked features.\nLCVAE =E [ log p\u03b8(\u2206x|z, x) ]\n\u2212DKL [ q\u03c6(z|x,\u2206x)||p\u03b8(z|x) ] (7) Through the proposed denoising autoencoder model, we\nare also able to attain an introspection mechanism for the presence of outliers. We incorporate this additional module via an auxiliary loss as specified in Eqn 7. An illustration of these flow predictions are shown in Figure 8."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we provide detailed experiments on the performance, robustness and flexibility of our proposed approach on various datasets. Our approach differentiates itself from existing solutions on various fronts as shown in Table I. We evaluate the performance of our proposed approach on various publicly-available datasets including the KITTI dataset [21], the Multi-FOV synthetic dataset [27] (pinhole, fisheye, and catadioptric lenses), an omnidirectionalcamera dataset [28], and on the Oxford Robotcar 1000km Dataset [29].\nNavigation solutions in autonomous systems today typically fuse various modalities including GPS, odometry from wheel encoders and INS to provide robust trajectory estimates over extended periods of operation. We provide a similar solution by leveraging the learned ego-motion capability described in this work, and fuse it with intermittent GPS updates2 (Secion IV-A). While maintaining similar\n2For evaluation purposes only, the absolute ground truth locations were added as weak priors on datasets without GPS measurements\nperformance capabilities (Table II), we re-emphasize the benefits of our approach over existing solutions: \u2022 Versatile: With a fully trainable model, our approach\nis able to simultaneously reason over both ego-motion and implicitly modeled camera parameters (intrinsics and extrinsics). Furthermore, online calibration and parameter tuning is implicitly encoded within the same learning framework.\n\u2022 Model-free: Without imposing any constraints on the type of camera optics, our approach is able to recover ego-motions for a variety of camera models including pinhole, fisheye and catadioptric lenses. (Section IV-B)\n\u2022 Bootstrapped training and refinement: We illustrate a bootstrapped learning example whereby a robot selfsupervises the proposed ego-motion regression task by fusing information from other sensor sources including GPS and INS (Section IV-C)\n\u2022 Introspective reasoning for scene-flow prediction: Via the C-VAE generative model, we are able to reason/introspect over the predicted flow vectors in the image given an ego-motion estimate. This provides an obvious advantage in robust outlier detection and identifying dynamic objects whose flow vectors need to be disambiguated from the ego-motion scene flow (Figure 8)\nA. Evaluating ego-motion performance with sensor fusion\nIn this section, we evaluate our approach against a few state-of-the-art algorithms for monocular visual odometry [4]. On the KITTI dataset [21], the pre-trained estimator is used to robustly and accurately predict ego-motion from KLT features tracked over the dataset image sequence. The frameto-frame ego-motion estimates are integrated for each session to recover the full trajectory estimate and simultaneously fused with intermittent GPS updates (incorporated every 150 frames). In Figure 6, we show the qualitative performance in the overall trajectory obtained with our method. The entire pose-optimized trajectory is compared against the ground truth trajectory. The translational errors are computed for each of the ground truth and prediction pose pairs, and their median value is reported in Table II for a variety of datasets with varied camera optics.\nB. Varied camera optics\nMost of the existing implementations of VO estimation are restricted to a class of camera optics, and generally avoid implementing a general-purpose VO estimator for varied camera optics. Our approach on the other hand, has shown the ability to provide accurate VO with intermittent GPS trajectory estimation while simultaneously being applicable to a varied range of camera models. In Figure 7, we compare with intermittent GPS trajectory estimates for all three camera models, and verify their performance accuracy compared to ground truth. In our experiments, we found that while our proposed solution was sufficiently powerful to model different camera optics, it was significantly better at modeling pinhole lenses as compared to fisheye and\n. catadioptric cameras (See Table II). In future work, we would like to investigate further extensions that improve the accuracy for both fisheye and catadioptric lenses.\nC. Self-supervised Visual Ego-motion Learning in Robots\nWe envision the capability of robots to self-supervise tasks such as visual ego-motion estimation to be especially beneficial in the context of life-long learning and autonomy. We experiment and validate this concept through a concrete example using the 1000km Oxford Robot Car dataset [29]. We train the task of visual ego-motion on a new camera sensor by leveraging the fused GPS and INS information collected on the robot car as ground truth trajectories (6- DOF), and extracting feature trajectories (via KLT) from image sequences obtained from the new camera sensor. The timestamps from the cameras are synchronized with respect\nto the timestamps of the fused GPS and INS information, in order to obtain a one-to-one mapping for training purposes. We train on the stereo centre (pinhole) camera dataset and present our results in Table II. As seen in Figure 6, we are able to achieve considerably accurate long-term state estimates by fusing our proposed visual ego-motion estimates with even sparser GPS updates (every 2-3 seconds, instead of 50Hz GPS/INS readings). This allows the robot to reduce its reliance on GPS/INS alone to perform robust, long-term trajectory estimation.\nD. Implementation Details\nIn this section we describe the details of our proposed model, training methodology and parameters used. The input x = (x,\u2206x) to the density-based ego-motion estimator are feature tracks extracted via (Kanade-Lucas-Tomasi) KLT feature tracking over the raw camera image sequences. The input feature positions and flow vectors are normalized to\nbe the in range of [\u22121, 1] using the dimensions of the input image. We evaluate sparse LK (Lucas-Kanade) optical flow over 7 pyramidal scales with a scale factor of \u221a 2.\nAs the features are extracted, the corresponding robot pose (either available via GPS or GPS/INS/wheel odometry sensor fusion) is synchronized and recorded in SE(3) for training purposes. The input KLT features, and the corresponding relative pose estimates used for training are parameterized as z = (t, r) \u2208 R6, with a Euclidean translation vector t \u2208 R3 and an Euler rotation vector r \u2208 R3.\nNetwork and training: The proposed architecture consists of a set of fully-connected stacked layers (with 1024, 128 and 32 units) followed by a Mixture Density Network with 32 hidden units and 5 mixture components (K). Each of the initial fully-connected layers implement tanh activation after it, followed by a dropout layer with a dropout rate of 0.1. The final output layer of the MDN (a\u03c0 , a\u00b5, a\u03c3) consists of (O + 2) \u2217 K outputs where O is the desired number of states estimated.\nThe network is trained (in Stage 1) with loss weights of 10, 0.1, 1 corresponding to the losses LMDN ,LTRAJ ,LCV AE described in previous sections. The training data is provided in batches of 100 frame-to-frame subsequent image pairs, each consisting of approximately 50 randomly sampled feature matches via KLT. The learning rate is set to 1e\u22123 with Adam as the optimizer. On the synthetic Multi-FOV dataset and the KITTI dataset, training for most models took roughly an hour and a half (3000 epochs) independent of the KLT feature extraction step.\nTwo-stage optimization: We found the one-shot joint optimization of the local ego-motion estimation and global trajectory optimization to have sufficiently low convergence rates during training. One possible explanation is the high sensitivity of the loss weight parameters that is used for tuning the local and global losses into a single objective. As previously addressed in Section III-B, we separate the training into two stages thereby alleviating the aforementioned issues, and maintaining fast convergence rates in Stage 1. Furthermore, we note that during the second stage, it only requires a few tens of iterations for sufficiently accurate egomotion trajectories. In order to optimize over a larger timewindow in stage 2, we set the batch size to 1000 frame-\nto-frame image matches, again randomly sampled from the training set as before. Due to the large integration window and memory limitations, we train this stage purely on the CPU for only 100 epochs each taking roughly 30s per epoch. Additionally, in stage 2, the loss weights for LTRAJ are increased to 100 in order to have faster convergence to the global trajectory. The remaining loss weights are left unchanged.\nTrajectory fusion: We use GTSAM3 to construct the underlying factor graph for pose-graph optimization. Odometry constraints obtained from the frame-to-frame ego-motion are incorporated as a 6-DOF constraint parameterized in SE(3) with 1\u221710\u22123 rad rotational noise and 5\u221710\u22122 m translation noise. As with typical autonomous navigation solutions, we expect measurement updates in the form of GPS (absolute reference updates) in order to correct for the long-term drift incurred in open-loop odometry chains. We incorporate absolute prior updates only every 150 frames, with a weak translation prior of 0.01 m. The constraints are incrementally added and solved using iSAM2 [30] as the measurements are streamed in, with updates performed every 10 frames.\nWhile the proposed MDN is parametrized in Euler angles, the trajectory integration module parameterizes the rotation vectors in quaternions for robust and unambiguous long-term trajectory estimation. All the rigid body transformations are implemented directly in Tensorflow for pure-GPU training support.\nRun-time performance: We are particularly interested in the run-time / test-time performance of our approach on CPU architectures for mostly resource-constrained settings. Independent of the KLT feature tracking run-time, we are able to recover ego-motion estimates at roughly 3ms on a consumer-grade Intel(R) Core(TM) i7-3920XM CPU @ 2.90GHz.\nSource code and Pre-trained weights: We implemented the MDN-based ego-motion estimator with Keras and Tensorflow, and trained our models using a combination of CPUs and GPUs (NVIDIA Titan X). All the code was trained on an server-grade Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz and tested on the same consumer-grade machine as mentioned above to emulate potential real-world use-cases. The source code and pre-trained models used will be made available shortly4."}, {"heading": "V. DISCUSSION", "text": "The initial results in bootstrapped learning for visual ego-motion has motivated new directions towards life-long learning in autonomous robots. While our visual ego-motion model architecture is shown to be sufficiently powerful to recover ego-motions for non-linear camera optics such as fisheye and catadioptric lenses, we continue to investigate further improvements to match existing state-of-the-art models for these lens types. Our current model does not capture distortion effects yet, however, this is very much a\n3http://collab.cc.gatech.edu/borg/gtsam 4See http://people.csail.mit.edu/spillai/learning-egomotion and https://\ngithub.com/spillai/learning-egomotion\nfuture direction we would like to take. Another consideration is the resource-constrained setting, where the optimization objective incorporates an additional regularization term on the number of parameters used, and the computation load consumed. We hope for this resource-aware capability to transfer to real-world limited-resource robots and to have a significant impact on the adaptability of robots for long-term autonomy."}, {"heading": "VI. CONCLUSION", "text": "While many visual ego-motion algorithm variants have been proposed in the past decade, we envision that a fully end-to-end trainable algorithm for generic camera egomotion estimation shall have far-reaching implications in several domains, especially autonomous systems. Furthermore, we expect our method to seamlessly operate under resource-constrained situations in the near future by leveraging existing solutions in model reduction and dynamic model architecture tuning. With the availability of multiple sensors on these autonomous systems, we also foresee our approach to bootstrapped task (visual ego-motion) learning to potentially enable robots to learn from experience, and use the new models learned from these experiences to encode redundancy and fault-tolerance all within the same framework."}], "references": [{"title": "Visual odometry", "author": ["David Nist\u00e9r", "Oleg Naroditsky", "James Bergen"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Large-scale visual odometry for rough terrain", "author": ["Kurt Konolige", "Motilal Agrawal", "Joan Sola"], "venue": "In Robotics research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Real-time stereo visual odometry for autonomous ground vehicles", "author": ["Andrew Howard"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Visual odometry based on stereo image sequences with RANSAC-based outlier rejection scheme", "author": ["Bernd Kitt", "Andreas Geiger", "Henning Lategahn"], "venue": "In Intelligent Vehicles Symposium,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Motion estimation for self-driving cars with a generalized camera", "author": ["Gim Hee Lee", "Friedrich Faundorfer", "Marc Pollefeys"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Using multicamera systems in robotics: Efficient solutions to the n-PnP problem", "author": ["Laurent Kneip", "Paul Furgale", "Roland Siegwart"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "1-point-RANSAC structure from motion for vehicle-mounted cameras by exploiting non-holonomic constraints", "author": ["Davide Scaramuzza"], "venue": "Int\u2019l J. of Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Bundle adjustment A modern synthesis", "author": ["Bill Triggs", "Philip F McLauchlan", "Richard I Hartley", "Andrew W Fitzgibbon"], "venue": "In International workshop on vision algorithms,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Multiple view geometry in computer vision", "author": ["Richard Hartley", "Andrew Zisserman"], "venue": "Cambridge university press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Obstacle avoidance and navigation in the real world by a seeing robot rover", "author": ["Hans P Moravec"], "venue": "Technical report, DTIC Document,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1980}, {"title": "Dynamic stereo vision", "author": ["Larry Henry Matthies"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1989}, {"title": "Robust stereo ego-motion for long distance navigation", "author": ["Clark F Olson", "Larry H Matthies", "H Schoppers", "Mark W Maimone"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography", "author": ["Martin A Fischler", "Robert C Bolles"], "venue": "Communications of the ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1981}, {"title": "Omnidirectional visual odometry for a planetary rover", "author": ["Peter Corke", "Dennis Strelow", "Sanjiv Singh"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Visual navigation using planar homographies", "author": ["Bojian Liang", "Nick Pears"], "venue": "In Robotics and Automation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Transforming camera geometry to a virtual downward-looking camera: Robust ego-motion estimation and groundlayer detection", "author": ["Qifa Ke", "Takeo Kanade"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Real-time monocular visual odometry for on-road vehicles with 1point RANSAC", "author": ["Davide Scaramuzza", "Friedrich Fraundorfer", "Roland Siegwart"], "venue": "In Robotics and Automation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Learning general optical flow subspaces for egomotion estimation and detection of motion anomalies", "author": ["Richard Roberts", "Christian Potthast", "Frank Dellaert"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Evaluation of non-geometric methods for visual odometry", "author": ["Thomas A Ciarfuglia", "Gabriele Costante", "Paolo Valigi", "Elisa Ricci"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Exploring Representation Learning With CNNs for Frameto-Frame Ego-Motion Estimation", "author": ["Gabriele Costante", "Michele Mancini", "Paolo Valigi", "Thomas A Ciarfuglia"], "venue": "IEEE Robotics and Automation Letters,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Are we ready for autonomous driving? The KITTI vision benchmark suite", "author": ["Andreas Geiger", "Philip Lenz", "Raquel Urtasun"], "venue": "In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "VINet: Visual-Inertial odometry as a sequence-to-sequence learning problem", "author": ["Ronald Clark", "Sen Wang", "Hongkai Wen", "Andrew Markham", "Niki Trigoni"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Visual odometry [tutorial", "author": ["Davide Scaramuzza", "Friedrich Fraundorfer"], "venue": "IEEE robotics & automation magazine,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Learning visual odometry with a convolutional network", "author": ["Kishore Konda", "Roland Memisevic"], "venue": "In International Conference on Computer Vision Theory and Applications,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "KLT: An implementation of the Kanade-Lucas-Tomasi feature tracker", "author": ["Stan Birchfield"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Benefit of large field-of-view cameras for visual odometry", "author": ["Zichao Zhang", "Henri Rebecq", "Christian Forster", "Davide Scaramuzza"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA). IEEE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Omnidirectional 3d reconstruction in augmented manhattan worlds", "author": ["Miriam Sch\u00f6nbein", "Andreas Geiger"], "venue": "In Intelligent Robots and Systems (IROS 2014),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "iSAM2: Incremental smoothing and mapping using the bayes tree", "author": ["Michael Kaess", "Hordur Johannsson", "Richard Roberts", "Viorela Ila", "John J Leonard", "Frank Dellaert"], "venue": "Int\u2019l J. of Robotics Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Visual odometry (VO) [1], commonly referred to as egomotion estimation, is a fundamental capability that enables robots to reliably navigate its immediate environment.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 184, "endOffset": 187}, {"referenceID": 1, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 189, "endOffset": 192}, {"referenceID": 2, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 204, "endOffset": 207}, {"referenceID": 3, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 209, "endOffset": 212}, {"referenceID": 4, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 250, "endOffset": 253}, {"referenceID": 5, "context": "With the wide-spread adoption of cameras in various robotics applications, there has been an evolution in visual odometry algorithms with a wide set of variants including monocular VO [1], [2], stereo VO [3], [4] and even non-overlapping n-camera VO [5], [6].", "startOffset": 255, "endOffset": 258}, {"referenceID": 6, "context": "fisheye, catadioptric) and the range of motions observed by these cameras mounted on various platforms [7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "Recovering relative camera poses from a set of images is a well studied problem under the context of Structurefrom-Motion (SfM) [8], [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "Recovering relative camera poses from a set of images is a well studied problem under the context of Structurefrom-Motion (SfM) [8], [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "SfM is usually treated as a non-linear optimization problem, where the camera poses (extrinsics), camera model parameters (intrinsics), and the 3D scene structure are jointly optimized via non-linear leastsquares [8].", "startOffset": 213, "endOffset": 216}, {"referenceID": 9, "context": "Most of the early work in VO was done primarily to determine vehicle egomotion [10], [11], [12] in 6-DOF, especially in the Mars planetary rover.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "Most of the early work in VO was done primarily to determine vehicle egomotion [10], [11], [12] in 6-DOF, especially in the Mars planetary rover.", "startOffset": 85, "endOffset": 89}, {"referenceID": 11, "context": "Most of the early work in VO was done primarily to determine vehicle egomotion [10], [11], [12] in 6-DOF, especially in the Mars planetary rover.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "[1], where the authors proposed the first real-time and scalable VO algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "In their work, they developed a 5-point minimal solver coupled with a RANSAC-based outlier rejection scheme [13] that is still extensively used today.", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "Other researchers [14] have extended this work to various camera types including catadioptric and fisheye lenses.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "One particularly popular strategy for VO estimation in vehicles is to enforce planar homographies during matching features on the ground plane [15], [16], thereby being able to robustly recover both relative orientation and absolute scale.", "startOffset": 143, "endOffset": 147}, {"referenceID": 15, "context": "One particularly popular strategy for VO estimation in vehicles is to enforce planar homographies during matching features on the ground plane [15], [16], thereby being able to robustly recover both relative orientation and absolute scale.", "startOffset": 149, "endOffset": 153}, {"referenceID": 6, "context": "[7], [17] introduced a novel 1-point solver by imposing the vehicle\u2019s non-holonomic motion constraints, thereby speeding up the VO estimation up to 400Hz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[7], [17] introduced a novel 1-point solver by imposing the vehicle\u2019s non-holonomic motion constraints, thereby speeding up the VO estimation up to 400Hz.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "Typical approaches have leveraged dimensionality reduction techniques by learning a reduced-dimensional subspace of the optical flow vectors induced by the egomotion [18].", "startOffset": 166, "endOffset": 170}, {"referenceID": 18, "context": "In [19], Ciarfuglia et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The authors further build upon their previous result by swapping out the SVR module with an end-to-end trainable convolutional neural network [20] while showing improvements in the overall performance on the KITTI odometry benchmark [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 20, "context": "The authors further build upon their previous result by swapping out the SVR module with an end-to-end trainable convolutional neural network [20] while showing improvements in the overall performance on the KITTI odometry benchmark [21].", "startOffset": 233, "endOffset": 237}, {"referenceID": 21, "context": "[22] introduced a visual-inertial odometry solution that takes advantage of a neural-network architecture to learn a mapping from raw inertial measurements and sequential imagery to 6-DOF pose estimates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Traditional VO [23] 7 7 3 7", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "End-to-end VO [20], [22] 7 3 3 7", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "End-to-end VO [20], [22] 7 3 3 7", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "In certain restricted scene structures or motion manifolds, several variants of ego-motion estimation are proposed [7], [15], [16], [17].", "startOffset": 115, "endOffset": 118}, {"referenceID": 14, "context": "In certain restricted scene structures or motion manifolds, several variants of ego-motion estimation are proposed [7], [15], [16], [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 15, "context": "In certain restricted scene structures or motion manifolds, several variants of ego-motion estimation are proposed [7], [15], [16], [17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "In certain restricted scene structures or motion manifolds, several variants of ego-motion estimation are proposed [7], [15], [16], [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 19, "context": "While model-based approaches have shown tremendous progress in accuracy, robustness, and run-time performance, a few recent data-driven approaches have been shown to produce equally compelling results [20], [22], [24].", "startOffset": 201, "endOffset": 205}, {"referenceID": 21, "context": "While model-based approaches have shown tremendous progress in accuracy, robustness, and run-time performance, a few recent data-driven approaches have been shown to produce equally compelling results [20], [22], [24].", "startOffset": 207, "endOffset": 211}, {"referenceID": 23, "context": "While model-based approaches have shown tremendous progress in accuracy, robustness, and run-time performance, a few recent data-driven approaches have been shown to produce equally compelling results [20], [22], [24].", "startOffset": 213, "endOffset": 217}, {"referenceID": 6, "context": "Our approach is motivated by previous minimally parameterized models [7], [17] that are able to recover ego-motion from a single tracked feature.", "startOffset": 69, "endOffset": 72}, {"referenceID": 16, "context": "Our approach is motivated by previous minimally parameterized models [7], [17] that are able to recover ego-motion from a single tracked feature.", "startOffset": 74, "endOffset": 78}, {"referenceID": 6, "context": "However, it has been previously shown that under non-holonomic vehicle motion, camera ego-motion may be fully recoverable up to a sufficient degree of accuracy using a single point [7], [17].", "startOffset": 181, "endOffset": 184}, {"referenceID": 16, "context": "However, it has been previously shown that under non-holonomic vehicle motion, camera ego-motion may be fully recoverable up to a sufficient degree of accuracy using a single point [7], [17].", "startOffset": 186, "endOffset": 190}, {"referenceID": 24, "context": "In typical associative mapping problems, the joint probability density p(x, z) is decomposed into the product of two terms: (i) p(z|x): the conditional density of the target pose z \u2208 SE(3) conditioned on the input feature correspondence x = (x,\u2206x) obtained from sparse optical flow (KLT) [25] (ii) p(x): the unconditional density of the input data x.", "startOffset": 288, "endOffset": 292}, {"referenceID": 25, "context": "For illustrative purposes only, we refer the reader to Figure 3 where we validate this two-stage approach over a simulated dataset [27].", "startOffset": 131, "endOffset": 135}, {"referenceID": 25, "context": "For illustrative purposes only, we refer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "For illustrative purposes only, we r fer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "For illustrative purposes only, we refer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "For illustrative purposes only, we refer the reader to Figure 3 where we validate this twostage approach over a simulated dataset [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "We evaluate the performance of our proposed approach on various publicly-available datasets including the KITTI dataset [21], the Multi-FOV synthetic dataset [27] (pinhole, fisheye, and catadioptric lenses), an omnidirectionalcamera dataset [28], and on the Oxford Robotcar 1000km Dataset [29].", "startOffset": 120, "endOffset": 124}, {"referenceID": 25, "context": "We evaluate the performance of our proposed approach on various publicly-available datasets including the KITTI dataset [21], the Multi-FOV synthetic dataset [27] (pinhole, fisheye, and catadioptric lenses), an omnidirectionalcamera dataset [28], and on the Oxford Robotcar 1000km Dataset [29].", "startOffset": 158, "endOffset": 162}, {"referenceID": 26, "context": "We evaluate the performance of our proposed approach on various publicly-available datasets including the KITTI dataset [21], the Multi-FOV synthetic dataset [27] (pinhole, fisheye, and catadioptric lenses), an omnidirectionalcamera dataset [28], and on the Oxford Robotcar 1000km Dataset [29].", "startOffset": 241, "endOffset": 245}, {"referenceID": 3, "context": "In this section, we evaluate our approach against a few state-of-the-art algorithms for monocular visual odometry [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 20, "context": "On the KITTI dataset [21], the pre-trained estimator is used to robustly and accurately predict ego-motion from KLT features tracked over the dataset image sequence.", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "We test on a variety of publicly-available datasets including (a) Multi-FOV synthetic dataset [27] (pinhole shown above), (b) an omnidirectional-camera dataset [28], (c) Oxford Robotcar 1000km Dataset [29] (2015-11-13-10-28-08) (d-h) KITTI dataset [21].", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "We test on a variety of publicly-available datasets including (a) Multi-FOV synthetic dataset [27] (pinhole shown above), (b) an omnidirectional-camera dataset [28], (c) Oxford Robotcar 1000km Dataset [29] (2015-11-13-10-28-08) (d-h) KITTI dataset [21].", "startOffset": 160, "endOffset": 164}, {"referenceID": 20, "context": "We test on a variety of publicly-available datasets including (a) Multi-FOV synthetic dataset [27] (pinhole shown above), (b) an omnidirectional-camera dataset [28], (c) Oxford Robotcar 1000km Dataset [29] (2015-11-13-10-28-08) (d-h) KITTI dataset [21].", "startOffset": 248, "endOffset": 252}, {"referenceID": 25, "context": "30 m Multi-FOV [27] Pinhole 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": "18 m Multi-FOV [27] Fisheye 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": "48 m Multi-FOV [27] Catadioptric 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 26, "context": "36 m Omnidirectional [28] Catadioptric 0.", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "7: Varied camera optics: An illustration of the performance of our general-purpose approach for varied camera optics (pinhole, fisheye, and catadioptric lenses) on the Multi-FOV synthetic dataset [27].", "startOffset": 196, "endOffset": 200}, {"referenceID": 27, "context": "The constraints are incrementally added and solved using iSAM2 [30] as the measurements are streamed in, with updates performed every 10 frames.", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "Many model-based Visual Odometry (VO) algorithms have been proposed in the past decade, often restricted to the type of camera optics, or the underlying motion manifold observed. We envision robots to be able to learn and perform these tasks, in a minimally supervised setting, as they gain more experience. To this end, we propose a fully trainable solution to visual ego-motion estimation for varied camera optics. We propose a visual ego-motion learning architecture that maps observed optical flow vectors to an ego-motion density estimate via a Mixture Density Network (MDN). By modeling the architecture as a Conditional Variational Autoencoder (CVAE), our model is able to provide introspective reasoning and prediction for ego-motion induced scene-flow. Additionally, our proposed model is especially amenable to bootstrapped egomotion learning in robots where the supervision in ego-motion estimation for a particular camera sensor can be obtained from standard navigation-based sensor fusion strategies (GPS/INS and wheel-odometry fusion). Through experiments, we show the utility of our proposed approach in enabling the concept of self-supervised learning for visual ego-motion estimation in autonomous robots.", "creator": "LaTeX with hyperref package"}}}