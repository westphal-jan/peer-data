{"id": "1705.10385", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Collaborative Deep Learning for Speech Enhancement: A Run-Time Model Selection Method Using Autoencoders", "abstract": "We show adding first Modular Neural Network (MNN) n't whisk various speech enhancement playable, different significant which where end Deep Neural Network (DNN) engineering put next rather adequate need. Differently and well living musical useful that averages variations under prototype, the propose MNN fox-affiliated although movie transponder for place heretofore qualifying signal only most a disreputable composer. We no it came Collaborative Deep Learning (CDL ), when it or reuse similar for - assistants DNN configurations without they further metallurgical. In only proposed MNN selecting first terrific module during then though takes though. To comes still, we employ only repeated AutoEncoder (AE) that an arbitrator, own amplifier along output even trained well come when the as come mean its mechanisms name dirty on. Therefore, is AE can narrow-gauge the quality followed the compartment - practical denoised result subsequently mind creation AE creation error, call. r. short correction means that since kvant-1 output an result to them speech. We propose an MNN thus with include modules but places laboratories saturday issues with entire analysis noise or, gender, to required Signal - to - Noise Ratio (SNR) value, have empirically seemed no but about true library better likely with subordinate selected DNN installation work entirely as maybe even means ibm result.", "histories": [["v1", "Mon, 29 May 2017 20:30:24 GMT  (287kb,D)", "http://arxiv.org/abs/1705.10385v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["minje kim"], "accepted": false, "id": "1705.10385"}, "pdf": {"name": "1705.10385.pdf", "metadata": {"source": "CRF", "title": "Collaborative Deep Learning for Speech Enhancement: A Run-Time Model Selection Method Using Autoencoders", "authors": ["Minje Kim"], "emails": ["minje@indiana.edu"], "sections": [{"heading": null, "text": "Keywords: Speech Enhancement, Source Separation, Deep Learning, Modular Neural Networks, Autoencoders"}, {"heading": "1 Introduction", "text": "Deep learning has become one of the most popular frameworks for speech enhancement. The basic strategy of applying a Deep Neural Network (DNN) for the enhancement job is to learn a network that approximates the mapping function from a contaminated speech signal to its cleaned-up version. Various input and output features have been proposed. Xu et al. introduced a pre-training based Speech Denoising AutoEncoder (SDAE), which uses magnitudes of Fourier coefficients for both input and output [1]. Ideal Binary Masks (IBM) [2] and Ideal Ratio Masks (IRM) [3] are another common target representations. Huang et al.\u2019s Deep Recurrent Neural Networks (DRNN) added the recurrent structure to SDAE, along with a discriminative term, too [4]. More specialized speech features showed state-of-the-art performances such as cochleagrams [5], Mel-Frequency Cepstrum Coefficients (MFCC) [3], and their combinations. Structural variations have been also investigated in the literature: deep clustering based on the independence of speakers [6], Long Short-Term Memory (LSTM) to handle long-term dependency of time-structured speech signals [7], deep unfolding networks to substitute the iterations in some estimation algorithms with a number of hidden layers [8], etc.\nAlthough it is common to adapt the model for the unseen noise types in the dictionary-based approaches, in the deep learning-based models the adaptation largely relied on the generalization power of the already trained network. For example, in the semi-supervised source separation scenario, the system can learn the unseen noise dictionary from the noise source mixed in the test signal during run time along with the ordinary speech dictionary [9, 10, 11]1. As shown in [10], this semi-supervised technique is prone to overfitting due to the lack of the knowledge about the noise source. Meanwhile, Liu et al. performed some experiments to see the generalization power of a SDAE [14]. If the network was not exposed to a specific noise type during training, its performance degrades for the mixtures with that particular noise. Similar tests confirmed a suboptimal performance for unseen speakers and mixing weights as well.\nRecently, there have been DNNs that adapt to the unknown noise type by refining an already trained SDAE during run time. Kim and Smaragdis proposed an Adaptive SDAE (ASDAE) system, which is a vertical concatenation of two\n\u2217This work is copyrighted by the IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.\n1Nonnegative Matrix Factorization (NMF) [12, 13] and its variations are a common choice for the dictionary learning algorithm.\nar X\niv :1\n70 5.\n10 38\n5v 1\n[ cs\n.S D\n] 2\n9 M\nay 2\nAEs: the bottom SDAE trained from known mixtures of speech and noise (to denoise them) and the top AE trained only from pure speech [15]. It is based on the assumption that a properly trained AE for a source of interest (i.e. speech) can be used to judge the similarity between its input signal and the source, because the AE\u2019s reconstruction error will be low if the AE\u2019s input is speech as well, while for a non-speech signal the autoencoding performance is not guaranteed. Therefore, for a test mixture the bottom SDAE first estimates a cleaned-up version, which is subsequently fed to the top AE to calculate AE\u2019s reconstruction error as a measure of the denoising quality. Then, this reconstruction error is used to fine-tune the bottom SDAE through an additional backpropagation step. Another primitive refining scheme was proposed earlier by Williamson et al. [2], where an NMF speech dictionary was used to further clean up the DNN results, although the use of NMF was limited to smoothing the results rather than fine-tune the main DNN. More recently, separable deep autoencoder showed promising denoising performance by having two AEs that model speech and noise separately. In there, the speech AE and an embedded NMF dictionary for an additional speech modeling are trained in advance, while the noise AE is trained from the test signal [16]. Those adaptive DNN models for speech denoising have focused only on adapting to unseen noise types. However, in practice we face a lot more variations such as in the ratio of sources\u2019 contributions, the frequency response of microphones, amount of reverberations, etc.\nThe proposed Modular Neural Network (MNN) assumes that it is easier to learn a smaller specialized DNN that work better for a particular enhancement job than a larger DNN for the general speech enhancement task as partially shown in [17]. Similarly, an MNN consists of local experts that provide various outputs for a test sample and a gating network that chooses the best module [18]. The proposed MNN also includes the specialized speech enhancement modules as experts, while it uses the speech AE as its arbitrator. As the AE can be learned without any information about the participating modules, the proposed MNN is more scalable than the cases that need to learn the discriminative gating network for the selection. This can be seen as a Collaborative Deep Learning (CDL) system as well, because now we can invite any already-trained DNNs with different properties, and then the proposed selection scheme produces a greedy ensemble of them as the optimal result for the current test signal. The use of AE to determine the speech enhancement quality of another module is similar to the use in ASDAE, but the proposed MNN differs from ASDAE in that it accepts the best modular output instead of refining the modules so that it can prevent overfitting."}, {"heading": "2 DNN for Supervised and Unsupervised Learning", "text": "In this section we review two basic DNN systems for supervised speech denoising and unsupervised speech modeling, which are then combined to build the proposed system in Section 3."}, {"heading": "2.1 DNN for Supervised Speech Denoising", "text": "We start from a D-dimensional complex-valued Fourier spectrum at t-th time frame as an instantaneous mixture of a clean speech and noise spectra: xt = st + nt 2. Usually the input x (sometimes along with its consecutive frames as well) goes through a feature extraction procedure to construct the input feature vector x\u0304 \u2208 RK (1)\n. Now the goal of the training job is to learn the mapping function FDNN to produce an output vector y \u2208 RD, which is either an estimation for the original speech features or a mask that can be later used to recover the speech. For the latter case, the feedforward and masking procedures work as follows:\ny = FDNN (x\u0304), s\u0302 = y x, (1)\nwhere stands for an element-wise multiplication and s\u0302 is an estimation of s. For training, we can calculate the magnitude ratio as the masking vector m = |s||s+n| , and use them to prepare the training pairs (x\u0304,m). Hence, the training objective for a DNN with L hidden layers is to minimize the sum of errors between the target masking vectors and the estimated ones:\narg min W (1),\u00b7\u00b7\u00b7 ,W (L+1) \u2211 t E ( mt \u2225\u2225FDNN (x\u0304t)), (2)\nwhere W (l) \u2208 RK (l+1)\u00d7(K(l)+1) holds the network parameters at l-th layer, which participates in the feedforward procedure as follows:\nFDNN (x\u0304) = z(L+2), z(1) = x\u0304, z(l+1) = g(l) ( W (l) \u00b7 [ ( z(l) )> , 1]> ) . (3)\nNote that z(l) \u2208 RK (l)\nis a vector of K(l) hidden unit outputs. There are a lot of choices for the activation function g(l), but the logistic function is commonly used for the last layer to ensure the soft masks between 0 and 1. Note\n2From now on we drop the frame index for the notational convenience.\nalso that the proposed model selection scheme works on any choice of the target representation of the participating DNNs if they can estimate a speech approximation s\u0302.\nThe mapping function FDNN could have been trained to perform well only on a subset of infinitely many mixing scenarios. For example, it can target on denoising only a particular person\u2019s noisy speech. On the other hand, the DNN might work for only a particular noise type, e.g. airplane noise. Finally, the DNN might have been trained only for a few choices of Signal-to-Noise Ratio (SNR) between the time domain signals s and n with sample index \u03c4 ,\ne.g. SNR = 10 log10\n\u2211 \u03c4 s(\u03c4)\n2\u2211 \u03c4 n(\u03c4) 2 . In theory, there can be a very large and deep network that has been trained on all\npossible mixing cases. However, it is of our interest whether there is a systematic way to combine all the specialized models and to make the best out of them."}, {"heading": "2.2 Autoencoders for Unsupervised Speech Modeling", "text": "AEs are another kind of neural networks whose target variables are set to be the same with the input, E ( s\u0304 \u2225\u2225FAE(s\u0304)). (4)\nTherefore, a straightforward AE that models a source, e.g. speech, can be trained by using clean speech spectra for both input and target. Magnitudes of the complex-valued Fourier coefficients, s\u0304 = |s|, can serve as the features for our purpose.\nIn the deep learning literature, a DAE has been also used to provide a greedy layer-wise feature learning [19, 20], where the input vector goes through random perturbations such as masking noise:\ns\u0303 = |s| \u03bd, \u03bdi \u223c Bernoulli(p), (5) E ( s\u0304 \u2225\u2225FDAE(s\u0303)), (6)\nwith p as the parameter for the Bernoulli distribution. Since the DAE has to produce the clean example from the corrupted inputs, the learned features are more robust and representative for the later use. Although the input and target are not exactly same, this kind of DAEs can still be seen as an unsupervised modeling because the corruption is done randomly without any supervision.\nA similar concept can be found in the dropout technique, too [21]. During the feedforward process, dropout randomly turns off a certain number of units with the layer-wise Bernourlli random variable, \u03bd(l) \u223c Bernoulli(p(l)), as its masking value:\nz(l+1) = g(l) ( W (l) \u00b7 [ ( \u03bd(l) z(l) )> , 1]> ) , (7)\nwhich is a procedure having a similar effect of averaging multiple thinned versions of the network. AEs with the dropout feature can also be seen as a DAE since not only their hidden units, but their input units are corrupted with masking noise.\nA clarification for SDAE: As we have reviewed in Section 1, SDAEs have been actively used to directly approximate the mapping from the contaminated speech to the clean ones in the context of supervised learning [1, 14]. For these supervised SDAEs, the objective is somewhat similar to that of an unsupervised AE in (4) because their target variables are the clean speech features, too. However, it is different in the sense that it directly involves a few specific types of noise known in advance to perturb the input:\nE ( s\u0304 \u2225\u2225FSDAE(x\u0304t)). (8)\nHence, those SDAEs are not one of the unsupervised speech modeling techniques. Instead, it can serve as one of the participating enhancement modules in the proposed MNN system for CDL."}, {"heading": "3 The Proposed Modular Neural Network for Collaborative Deep Learning", "text": ""}, {"heading": "3.1 The Proposed Architecture", "text": "FDNNj is one of the J participating DNN modules in the MNN, which has been trained on only a subset of all the possible types of corruption. The structure of the modules can also vary in their number of layers and hidden units, choice of activation functions, use of recurrence and convolution, etc. Once each of them estimates a clean speech signal s\u0302DNNj , it is fed to the model selector FDAE in the form of a magnitude spectrum, |s\u0302DNNj |. FDAE is trained in advance to produce a clean magnitude speech spectrum for its input of the same kind, while for the robustness to the various imperfection of s\u0302DNNj we choose to use a dropout-based DAE as in (6) rather than an AE trained on clean speech. The key assumption is that a properly trained DAE will keep its clean speech spectra input intact, while its behavior for an unseen non-speech spectrum is not guaranteed. Consequently, the DAE error E ( s\u0302DNNj \u2225\u2225FDAE(|s\u0302DNNj | \u03bd(1))) measures how much the input and speech are alike. Fig. 1 depicts this run-time\nprocess when J = 3. \u02c6\u0302sDNNj = FDAE(|s\u0302DNNj | \u03bd(1)) denotes the run-time DAE output. The final output of the proposed MNN system is the DNN module\u2019s output whose subsequent DAE error is the lowest:\nFCDL(x\u0304) = s\u0302DNNj\u2217 , (9)\nj\u2217 = arg min j\u2208{1,\u00b7\u00b7\u00b7 ,J}\nE(|s\u0302DNNj | \u2225\u2225\u2225|\u02c6\u0302sDNNj |) (10)\nAlternatively, SNR can capture the discrepancy in time domain, too:\n10 log10 (\u2211 \u03c4 s\u0302 2 DNNj (\u03c4) )/(\u2211 \u03c4 ( s\u0302DNNj (\u03c4)\u2212 \u02c6\u0302sDNNj (\u03c4) )2) . (11)"}, {"heading": "3.2 Computational and Spatial Complexity", "text": "The run-time computational and spatial complexity is clearly an issue with the proposed MNN for CDL method as every participating DNN needs to run a feedforward step. There are some promising approaches to compressing DNNs such as a low-rank approximation of the weight matrices [22] and networks that operate using bit logics and binary variables [23, 24]. Since the compressed networks claim their efficiency during run time, they can substitute the comprehensive ones for the model selection purpose. After the AE selection is done, we finally run the the best comprehensive DNN. We leave this network compression issue to future work."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 The Speech AE", "text": "Randomly chosen 400 utterances from the TIMIT training set are used for training (20 speakers \u00d7 2 genders \u00d7 10 utterances). Short-time Fourier transform with an 1024-point frame size and a 75% overlap is used for the time-frequency conversion. Resilient backpropagation (Rprop) [25] technique is employed, and their parameters are found through a validation with additional four speakers: 0.5, 1.5, 10\u22127 and 10\u22121 for backtracking, acceleration, and minimum and maximum step sizes, respectively. We choose a modified Rectified Linear Unit (ReLU) as proposed in [14] for the activation function. Dropout parameters p(l) are all set to be 0.8. The sum of the squared error is minimized during training. The batch size was set to be 1,000. They all converge in 5, 000 iterations.\nTwo DAEs with different network topologies model this speech data set: FDAE128 and FDAE2048\u00d72. FDAE128 is with a single hidden layer of 128 units. A 513-dimensional magnitude spectrum works as its input and target. FDAE2048\u00d72 is with two hidden layers, each of which has 2048 units. ForFDAE2048\u00d72, we concatenate three spectra\n\u2223\u2223[s>t\u22121, s>t , s>t+1]>\u2223\u2223 to take the temporal dynamics of the signal into account, while its target is still a single spectrum, i.e. |st|. The two DAEs are compared to see if the larger and more complicated DAE measures the speech quality more correctly than the smaller one."}, {"heading": "4.2 Experiment 1: Variations in the Noise Types", "text": "For training we prepare 60 clean utterances per a noise type: (6 speakers)\u00d7(2 genders)\u00d7(5 utterances). They are then mixed up with one of three noise types chosen from {\u201cBirds\u201d, \u201cTyping\u201d, \u201cMotorcycle\u201d} [11] at 0 dB SNR. We train one 512\u00d72 DNN per one of the three noisy speech datasets as described in Section 4.1, except some facts that (a) three input frames are always concatenated to form an input vector (b) the target is a masking vector of the center frame. The logistic function ensures the soft masking at the final layer. As for evaluation, we use both Signal-to-Distortion Ratio (SDR) [26] and Short-Time Objective Intelligibility (STOI) [27].\nThree test datasets are from 20 gender-balanced clean utterances (5 from each of 4 speakers) mixed with different parts of the same three noise types. As shown in Table 1, if a DNN is trained and tested for the same kind of mixture, it performs the best: 12.12, 12.74 and 10.15 dB for the SDR and 0.8501, 0.8581, and 0.8260 for STOI. On the other hand, if we randomly choose one of the three trained systems at every time, the performance is a lot worse (the \u201cChance\u201d column). A truly optimal case is when we know the best module for each test sample (\u201cOracle\u201d column), although in this experiment the system trained on the same noise type is always the best choice.\nBoth selection metrics proposed in (10) and (11) assess the quality of all three participating DNNs\u2019 results, and then we average the SDR or STOI values of the selected results for the 20 test utterances. For all cases, the proposed method is better than chance. When one of the systems is absolutely better than the others (\u201cBirds\u201d and \u201cTyping\u201d) the proposed system reaches the oracle case. The shallow and deep AEs performs similarly in general, except the \u201cMotorcycle\u201d case. We conjecture that a small AE is good enough when the participating DNNs are very specialized on a noise type like this."}, {"heading": "4.3 Experiment 2: Variations in Gender", "text": "Next, we construct two datasets, each of which is from either 12 male or 12 female speakers. This time all ten noise types used in [11] are mixed with the 12\u00d7 5 clean utterances, totalling 600 per gender. Two gender-specific 2048\u00d72 DNNs are trained from these datasets, respectively. For testing we collect 10\u00d75 utterances per gender and mix them with the same ten noise types.\nThe module trained from the same gender performs better on the test set with the same gender than the wrong choice: 10.38 vs 8.15 and 11.00 vs 7.58 dB in SDR (Table 2), although their gap is smaller than Table 1. It might be because there can be a male test speaker whose voice is more similar to a female training speaker and vice versa. Similarly, the oracle case is better than the correct choice of DNN. Consequently, in this experiment the AEs\u2019 decision is not perfect, yet nearing the oracle case and showcasing much better results than chance. Note that FDAE2048\u00d72 performs better than FDAE128 ."}, {"heading": "4.4 Experiment 3: Variations in the Input SNR", "text": "For the final experiment, we randomly choose gender-balanced 12 speakers and their five utterances for training. Then, all ten noise types are mixed to build a set of 600 noisy utterances. For each set of 600 signals, we fix the loudness of the noise source to make the mixture has one of three SNR values, -5, 0, and +5. We train three 2048\u00d72 DNN modules on these. Table 3 shows that the difference between the DNN modules is minute. For example, for the test signals with -5 dB SNR, the DNN system trained on 0 dB mixtures performs better than the correct choice in terms of SDR (7.00 vs 6.89 dB), because the correct DNN separated out the interfering noise too much, while introducing more artifacts which in turn decreased the overall separation quality. Yet, the DNN system trained on the -5 dB samples performs the best in terms of STOI. The proposed systems work better than chance most of the time (except the STOI value for -5 dB input case). FDAE2048\u00d72 works better than FDAE128 in distinguishing the well denoised results that are only slightly different from each other (0 and +5 dB inputs). Note that this generic DNN with 0 dB mixture (9.87 dB) performs worse than the smaller noise-specific ones in Table 1, so it empirically shows that the correctly chosen specialized module outperforms the large generic network."}, {"heading": "5 Conclusion", "text": "We proposed a collaborative deep learning method where multiple specialized DNN modules participate in the denoising job to produce various intermediate results. A DAE trained from clean speech judged the quality of the intermediate denoised results and chose the best one. The proposed MNN method showed better performance than the average of the candidate results in general. A shallow DAE was enough for most of the jobs, while the other deeper and larger DAE was more suitable for confusing high quality cases. The system was tested with variations in the noise type, gender, and input SNR. As future work, we plan to investigate more variations, e.g. reverberations and LSTMs for both DNN modules and DAEs."}], "references": [{"title": "An experimental study on speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65\u201368, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Reconstruction techniques for improving the perceptual quality of binary masked speech", "author": ["D.S. Williamson", "Y. Wang", "D.L. Wang"], "venue": "Journal of the Acoustical Society of America, vol. 136, pp. 892\u2013902, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Ideal ratio mask estimation using deep neural networks for robust speech recognition", "author": ["A. Narayanan", "D.L. Wang"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), May 2013, pp. 7092\u20137096.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint optimization of masks and deep recurrent neural networks for monaural source separation", "author": ["P. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 12, pp. 2136\u20132147, Dec 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards scaling up classification-based speech separation", "author": ["Y. Wang", "D.L. Wang"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 7, pp. 1381\u20131390, July 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["J.R. Hershey", "Z. Chen", "J. Le Roux", "S. Watanabe"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Mar. 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr", "author": ["F. Weninger", "H. Erdogan", "S. Watanabe", "E. Vincent", "J. Le Roux", "J.R. Hershey", "B. Schuller"], "venue": "Proceedings of the International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA), Aug. 2015. 6", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep NMF for speech separation", "author": ["J. Le Roux", "J.R. Hershey", "F. Weninger"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Apr. 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised and semi-supervised separation of sounds from singlechannel mixtures", "author": ["P. Smaragdis", "B. Raj", "M. Shashanka"], "venue": "Proceedings of the International Conference on Independent Component Analysis and Blind Signal Separation (ICA), London, UK, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Stopping criteria for non-negative matrix factorization based supervised and semi-supervised source separation", "author": ["F.G. Germain", "G.J. Mysore"], "venue": "IEEE Signal Processing Letters, vol. 21, no. 10, pp. 1284\u20131288, Oct 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Online PLCA for real-time semi-supervised source separation", "author": ["Z. Duan", "G.J. Mysore", "P. Smaragdis"], "venue": "Proceedings of the International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA), 2012, pp. 34\u201341.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, pp. 788\u2013791, 1999.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems (NIPS). 2001, vol. 13, MIT Press.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Experiments on deep learning for speech denoising", "author": ["D. Liu", "P. Smaragdis", "M. Kim"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), Sep 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive denoising autoencoders: A fine-tuning scheme to learn from test mixtures", "author": ["M. Kim", "P. Smaragdis"], "venue": "Proceedings of the International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA), August 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Unseen noise estimation using separable deep auto encoder for speech enhancement", "author": ["M. Sun", "X. Zhang", "H. Van hamme", "T.F. Zheng"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 1, pp. 93\u2013104, Jan 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech intelligibility potential of general and specialized deep neural network based speech enhancement systems", "author": ["M. Kolb\u00e6k", "Z.H. Tan", "J. Jensen"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 1, pp. 153\u2013167, Jan 2017.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Adaptive mixtures of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton"], "venue": "Neural Computation, vol. 3, no. 1, pp. 79\u201387, Mar. 1991.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1991}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), 2008, pp. 1096\u20131103.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, Dec. 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, Jan. 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1929}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["J. Xue", "J. Li", "Y. Gong"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2013, pp. 2365\u20132369.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Bitwise neural networks", "author": ["M. Kim", "P. Smaragdis"], "venue": "International Conference on Machine Learning (ICML) Workshop on Resource-Efficient Machine Learning, Jul 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2015, pp. 3105\u20133113.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "A direct adaptive method for faster backpropagation learning: the rprop algorithm", "author": ["M. Riedmiller", "H. Braun"], "venue": "IEEE International Conference on Neural Networks, 1993, pp. 586\u2013591 vol.1.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1993}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "C. Fevotte", "R. Gribonval"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 4, pp. 1462\u20131469, 2006. 7", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "A short-time objective intelligibility measure for timefrequency weighted noisy speech", "author": ["C.H. Taal", "R.C. Hendriks", "R. Heusdens", "J. Jensen"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2010. 8", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "introduced a pre-training based Speech Denoising AutoEncoder (SDAE), which uses magnitudes of Fourier coefficients for both input and output [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 1, "context": "Ideal Binary Masks (IBM) [2] and Ideal Ratio Masks (IRM) [3] are another common target representations.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Ideal Binary Masks (IBM) [2] and Ideal Ratio Masks (IRM) [3] are another common target representations.", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "\u2019s Deep Recurrent Neural Networks (DRNN) added the recurrent structure to SDAE, along with a discriminative term, too [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 4, "context": "More specialized speech features showed state-of-the-art performances such as cochleagrams [5], Mel-Frequency Cepstrum Coefficients (MFCC) [3], and their combinations.", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": "More specialized speech features showed state-of-the-art performances such as cochleagrams [5], Mel-Frequency Cepstrum Coefficients (MFCC) [3], and their combinations.", "startOffset": 139, "endOffset": 142}, {"referenceID": 5, "context": "Structural variations have been also investigated in the literature: deep clustering based on the independence of speakers [6], Long Short-Term Memory (LSTM) to handle long-term dependency of time-structured speech signals [7], deep unfolding networks to substitute the iterations in some estimation algorithms with a number of hidden layers [8], etc.", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "Structural variations have been also investigated in the literature: deep clustering based on the independence of speakers [6], Long Short-Term Memory (LSTM) to handle long-term dependency of time-structured speech signals [7], deep unfolding networks to substitute the iterations in some estimation algorithms with a number of hidden layers [8], etc.", "startOffset": 223, "endOffset": 226}, {"referenceID": 7, "context": "Structural variations have been also investigated in the literature: deep clustering based on the independence of speakers [6], Long Short-Term Memory (LSTM) to handle long-term dependency of time-structured speech signals [7], deep unfolding networks to substitute the iterations in some estimation algorithms with a number of hidden layers [8], etc.", "startOffset": 342, "endOffset": 345}, {"referenceID": 8, "context": "For example, in the semi-supervised source separation scenario, the system can learn the unseen noise dictionary from the noise source mixed in the test signal during run time along with the ordinary speech dictionary [9, 10, 11].", "startOffset": 218, "endOffset": 229}, {"referenceID": 9, "context": "For example, in the semi-supervised source separation scenario, the system can learn the unseen noise dictionary from the noise source mixed in the test signal during run time along with the ordinary speech dictionary [9, 10, 11].", "startOffset": 218, "endOffset": 229}, {"referenceID": 10, "context": "For example, in the semi-supervised source separation scenario, the system can learn the unseen noise dictionary from the noise source mixed in the test signal during run time along with the ordinary speech dictionary [9, 10, 11].", "startOffset": 218, "endOffset": 229}, {"referenceID": 9, "context": "As shown in [10], this semi-supervised technique is prone to overfitting due to the lack of the knowledge about the noise source.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "performed some experiments to see the generalization power of a SDAE [14].", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Nonnegative Matrix Factorization (NMF) [12, 13] and its variations are a common choice for the dictionary learning algorithm.", "startOffset": 39, "endOffset": 47}, {"referenceID": 12, "context": "Nonnegative Matrix Factorization (NMF) [12, 13] and its variations are a common choice for the dictionary learning algorithm.", "startOffset": 39, "endOffset": 47}, {"referenceID": 14, "context": "AEs: the bottom SDAE trained from known mixtures of speech and noise (to denoise them) and the top AE trained only from pure speech [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 1, "context": "[2], where an NMF speech dictionary was used to further clean up the DNN results, although the use of NMF was limited to smoothing the results rather than fine-tune the main DNN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In there, the speech AE and an embedded NMF dictionary for an additional speech modeling are trained in advance, while the noise AE is trained from the test signal [16].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "The proposed Modular Neural Network (MNN) assumes that it is easier to learn a smaller specialized DNN that work better for a particular enhancement job than a larger DNN for the general speech enhancement task as partially shown in [17].", "startOffset": 233, "endOffset": 237}, {"referenceID": 17, "context": "Similarly, an MNN consists of local experts that provide various outputs for a test sample and a gating network that chooses the best module [18].", "startOffset": 141, "endOffset": 145}, {"referenceID": 18, "context": "In the deep learning literature, a DAE has been also used to provide a greedy layer-wise feature learning [19, 20], where the input vector goes through random perturbations such as masking noise: s\u0303 = |s| \u03bd, \u03bdi \u223c Bernoulli(p), (5) E ( s\u0304 \u2225\u2225FDAE(s\u0303)), (6)", "startOffset": 106, "endOffset": 114}, {"referenceID": 19, "context": "In the deep learning literature, a DAE has been also used to provide a greedy layer-wise feature learning [19, 20], where the input vector goes through random perturbations such as masking noise: s\u0303 = |s| \u03bd, \u03bdi \u223c Bernoulli(p), (5) E ( s\u0304 \u2225\u2225FDAE(s\u0303)), (6)", "startOffset": 106, "endOffset": 114}, {"referenceID": 20, "context": "A similar concept can be found in the dropout technique, too [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "A clarification for SDAE: As we have reviewed in Section 1, SDAEs have been actively used to directly approximate the mapping from the contaminated speech to the clean ones in the context of supervised learning [1, 14].", "startOffset": 211, "endOffset": 218}, {"referenceID": 13, "context": "A clarification for SDAE: As we have reviewed in Section 1, SDAEs have been actively used to directly approximate the mapping from the contaminated speech to the clean ones in the context of supervised learning [1, 14].", "startOffset": 211, "endOffset": 218}, {"referenceID": 21, "context": "There are some promising approaches to compressing DNNs such as a low-rank approximation of the weight matrices [22] and networks that operate using bit logics and binary variables [23, 24].", "startOffset": 112, "endOffset": 116}, {"referenceID": 22, "context": "There are some promising approaches to compressing DNNs such as a low-rank approximation of the weight matrices [22] and networks that operate using bit logics and binary variables [23, 24].", "startOffset": 181, "endOffset": 189}, {"referenceID": 23, "context": "There are some promising approaches to compressing DNNs such as a low-rank approximation of the weight matrices [22] and networks that operate using bit logics and binary variables [23, 24].", "startOffset": 181, "endOffset": 189}, {"referenceID": 24, "context": "Resilient backpropagation (Rprop) [25] technique is employed, and their parameters are found through a validation with additional four speakers: 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "We choose a modified Rectified Linear Unit (ReLU) as proposed in [14] for the activation function.", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "They are then mixed up with one of three noise types chosen from {\u201cBirds\u201d, \u201cTyping\u201d, \u201cMotorcycle\u201d} [11] at 0 dB SNR.", "startOffset": 99, "endOffset": 103}, {"referenceID": 25, "context": "As for evaluation, we use both Signal-to-Distortion Ratio (SDR) [26] and Short-Time Objective Intelligibility (STOI) [27].", "startOffset": 64, "endOffset": 68}, {"referenceID": 26, "context": "As for evaluation, we use both Signal-to-Distortion Ratio (SDR) [26] and Short-Time Objective Intelligibility (STOI) [27].", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "This time all ten noise types used in [11] are mixed with the 12\u00d7 5 clean utterances, totalling 600 per gender.", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "We show that a Modular Neural Network (MNN) can combine various speech enhancement modules, each of which is a Deep Neural Network (DNN) specialized on a particular enhancement job. Differently from an ordinary ensemble technique that averages variations in models, the propose MNN selects the best module for the unseen test signal to produce a greedy ensemble. We see this as Collaborative Deep Learning (CDL), because it can reuse various already-trained DNN models without any further refining. In the proposed MNN selecting the best module during run time is challenging. To this end, we employ a speech AutoEncoder (AE) as an arbitrator, whose input and output are trained to be as similar as possible if its input is clean speech. Therefore, the AE can gauge the quality of the module-specific denoised result by seeing its AE reconstruction error, e.g. low error means that the module output is similar to clean speech. We propose an MNN structure with various modules that are specialized on dealing with a specific noise type, gender, and input Signal-to-Noise Ratio (SNR) value, and empirically prove that it almost always works better than an arbitrarily chosen DNN module and sometimes as good as an oracle result.", "creator": "LaTeX with hyperref package"}}}