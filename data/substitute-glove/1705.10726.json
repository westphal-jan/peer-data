{"id": "1705.10726", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Strength Factors: An Uncertainty System for a Quantified Modal Logic", "abstract": "We present came working applied S more handling speculation for a logically modal deductive (first - order modal rational ). The forms has its saturday both analogous logic have clear implies. The system means derived but Chisholm ' comes kantian. We concretize Chisholm ') system by flight his resolvable and patterns (things. card. foundational) concept of reasonablenes in infinitesimal while proof molecular. S ca except useful in systems might have return differentiated addition diseases out provide impositions for having uncertainty. As after demonstration related the system, we provide the code move provide only principle whether the lottery dualism. Another advantage another also problem is example there example there supplied to provide underscored values provided counterfactual denied. Counterfactuals much issued concerned once spy knows for 'd are allegedly. Among other evidence, noisome are describe from based were to might one actions to cellphones. Uncertainties making shoddiness pushed time otherwise same take system.", "histories": [["v1", "Tue, 30 May 2017 16:24:18 GMT  (174kb,D)", "http://arxiv.org/abs/1705.10726v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["naveen sundar govindarajulu", "selmer bringsjord"], "accepted": false, "id": "1705.10726"}, "pdf": {"name": "1705.10726.pdf", "metadata": {"source": "CRF", "title": "Strength Factors: An Uncertainty System for a Quantified Modal Logic", "authors": ["Naveen Sundar Govindarajulu", "Selmer Bringsjord"], "emails": ["naveensundarg@gmail.com", "selmer.bringsjord@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "We introduce a new system S for talking about uncertainty of iterated beliefs in a quantified modal logic with belief operators. The quantified modal logic we use is based on the deontic cognitive event calculus (DCEC ), which belongs to the\nfamily of cognitive calculi that have been used in modeling complex cognition. Here, we use a subset of DCEC that we term micro cognitive calculus (\u00b5C ). Specifically, we add a system of uncertainty derived from Chisholm\u2019s epistemology [Chisholm, 1987].1 The system S is a work in progress and hence the presentation here will be abstract in nature.\nOne of our primary motivations is to design a system of uncertainty that is easy to use in end-user facing systems. There have been many studies that show that laypeople have difficulty understanding raw probability values (e.g. see [Kaye and Koehler, 1991]); and we believe that our approach borrowed from philosophy can pave the way for systems that can present uncertain statements in a more understandable format to lay users."}, {"heading": "2 Prior Work", "text": "Members in the family of cognitive calculi have been used to formalize and automate highly intensional reasoning processes.2 More recently, using DCEC we have presented an automation of the doctrine of double effect in [Govindarajulu and Bringsjord, 2017].3 The doctrine of double effect is an ethical principle that has been shown to be used by both untrained laypeople and experts when faced with moral dilemmas; and it plays a central role in many legal systems.\nOther tasks automated by cognitive calculi include the false-belief task [Arkoudas and Bringsjord, 2008] and akrasia (succumbing to temptation to violate moral principles) [Bringsjord et al., 2014].4 Each cognitive calculus is a sorted (i.e. typed) quantified modal logic (also known as sorted firstorder modal logic). Each calculus has a well-defined syntax and proof calculus. The proof calculus is based on natural\n1See the SEP entry on Chisholm for a quick overview of Chisholm\u2019s epistemology: https://plato.stanford.edu/entries/ chisholm/#EpiIEpiTerPriFou.\n2By \u201cintensional processes\u201d, we roughly mean processes that take into account knowledge, beliefs, desires, intentions, etc. of agents. This is opposed to extensional systems such as pure firstorder systems that do not take into account states of minds of other agents. This is not be confused with \u201cintentional\u201d systems which would be modeled with intensional systems. See [Zalta, 1988] for a detailed treatment of intensionality.\n3This work will be presented at IJCAI 2017. 4Arkoudas and Bringsjord [2008] introduced the general family\nof cognitive event calculi to which DCEC belongs.\nar X\niv :1\n70 5.\n10 72\n6v 1\n[ cs\n.A I]\n3 0\nM ay\n2 01\n7\ndeduction [Gentzen, 1935], and includes all the introduction and elimination rules for first-order logic, as well as inference schemata for the modal operators and related structures.\nOn the uncertainty and probability front, there have been many logics of probability, see [Demey et al., 2016] for an overview. Since our system builds upon probabilities, our approach could use a variety of such systems. As we noted above, there has been very little work in uncertainty systems for first-order modal logics. Among first-order systems, the seminal work in [Halpern, 1990] presents a first-order logic with modified semantics to handle probabilistic statements. We can use such a system as the foundation for our work, and use it to define the base probability function Pr used below. (Note that we leave Pr unspecified for now.)"}, {"heading": "3 The Formal System", "text": "The formal system \u00b5C is a modal extension of the the event calculus. The event calculus is a multi-sorted first-order logic with a family of axiom sets. The exact axiom set is not important. The primary sorts in the system are shown below.\nSort Description\nAgent Human and non-human actors. Moment or Time Time points and intervals. E.g. simple, such as ti, or complex, such as birthday(son( jack)). Event Used for events in the domain. ActionType Action types are abstract actions. They are instantiated at particular times by actors. E.g.: \u201ceating\u201d vs. \u201cjack eats.\u201d Action A subtype of Event for events that occur as actions by agents. Fluent Used for representing states of the world in the event calcu-\nlus.\nFull DCEC has a suite of modal operators and inference schemata. Here we focus on just two: an operator for belief B and an operator for perception P. The syntax of and inference schemata of the system are shown below. S is the set of all sorts, f are the core function symbols, t shows the set of terms, and \u03c6 is the syntax for the formulae.\nSyntax\nS ::=\n{ Object \u2223\u2223 Agent \u2223\u2223 Self @ Agent \u2223\u2223 ActionType \u2223\u2223 Actionv Event \u2223\u2223 Moment \u2223\u2223 Formula \u2223\u2223 Fluent \u2223\u2223 Numeric\nf ::=  action : Agent\u00d7ActionType\u2192 Action initially : Fluent\u2192 Formula holds : Fluent\u00d7Moment\u2192 Formula happens : Event\u00d7Moment\u2192 Formula clipped : Moment\u00d7Fluent\u00d7Moment\u2192 Formula initiates : Event\u00d7Fluent\u00d7Moment\u2192 Formula terminates : Event\u00d7Fluent\u00d7Moment\u2192 Formula prior : Moment\u00d7Moment\u2192 Formula\nt ::= x : S \u2223\u2223 c : S \u2223\u2223 f (t1, . . . , tn)\n\u03c6 ::=\n{ t :Formula \u2223\u2223 \u00ac\u03c6 \u2223\u2223 \u03c6\u2227\u03c8 \u2223\u2223 \u03c6\u2228\u03c8 \u2223\u2223 P(a, t,\u03c6)\n\u2223\u2223 B(a, t,\u03c6) The above calculus lets us formalize statements of the form \u201cJohn believes now that Mary perceived that it was raining yesterday.\u201d One formalization could be:\n\u2203t < now : B ( john,now,P ( mary, t,holds(raining, t) ))\nThe figure below shows the inference schemata for DCEC . RP captures that perceptions get turned into beliefs. RB is an inference schema that lets us model idealized agents that have their beliefs closed under the \u00b5C proof theory. While normal humans are not deductively closed, this lets us model more closely how deliberate agents such as organizations and more strategic actors reason. Assume that there is a background set of axioms \u0393 we are working with.\nInference Schemata\nP(a, t1,\u03c61), \u0393 ` t1 < t2 B(a, t2,\u03c6) [RP]\nB(a, t1,\u03c61), . . . ,B(a, tm,\u03c6m), {\u03c61, . . . ,\u03c6m} ` \u03c6, \u0393 ` ti < t B(a, t,\u03c6) [RB]"}, {"heading": "4 The Uncertainty System S", "text": "In the uncertainty system, we augment the belief modal operators with a discrete set of uncertainty factors termed as strength factors. The factors are not arbitrary and are based on how derivable a proposition is for a given agent.\nChisholm\u2019s epistemology has a primitive undefined binary relation that he terms reasonableness with which he defines a scale of strengths for beliefs one might have in a proposition. Note that Chisholm\u2019s system is agent free while ours is agentbased. Let \u03c6 at \u03c8 denote that \u03c6 is more reasonable than \u03c8 to an agent a at time t. For convenience, we define a new operator, the withholding operator W, defined below:\nW(a, t,\u03c6)\u2261 \u00acB(a, t,\u03c6)\u2227\u00acB(a, t,\u00ac\u03c6)\nWe now reproduce Chisholm\u2019s system below. Note the formula used in the definitions below are meta-formula and not strictly in \u00b5C .\nStrength Factor Definitions\nAcceptable An agent a at time t finds \u03c6 acceptable iff withholding \u03c6 is not more reasonable than believing in \u03c6.\nB1(a, t,\u03c6)\u21d4 W(a, t,\u03c6) 6 a t B(a, t,\u03c6); or( \u00acB(a, t,\u03c6)\u2227\u00acB(a, t,\u00ac\u03c6) ) 6 at B(a, t,\u03c6)\nSome Presumption in Favor An agent a at time t has some presumption in favor of \u03c6 iff believing \u03c6 at t is more reasonable than believing \u00ac\u03c6 at time t:\nB2(a, t,\u03c6)\u21d4 ( B(a, t,\u03c6) ta B(a, t,\u00ac\u03c6) )\nBeyond Reasonable Doubt An agent a at time t has beyond reasonable doubt in \u03c6 iff believing \u03c6 at t is more reasonable than withholding \u03c6 at time t:\nB3(a, t,\u03c6)\u21d4  B(a, t,\u03c6) t a W(a, t,\u03c6); or( B(a, t,\u03c6) at ( \u00acB(a, t,\u03c6)\u2227\u00acB(a, t,\u00ac\u03c6)\n) Evident A formula \u03c6 is evident to an agent a at time t iff \u03c6 is\nbeyond reasonable doubt and if there is a \u03c8 such that believing \u03c8 is more reasonable for a at time t than believing \u03c6, then a is certain about \u03c8 at time t.\nB4(a, t,\u03c6)\u21d4  B3(a, t,\u03c6)\u2227 \u2203\u03c8 : [ B(a, t,\u03c8) at B(a, t,\u03c6)\n\u21d2B5(a, t,\u03c8)\n]\nCertain An agent a at time t is certain about \u03c6 iff \u03c6 is beyond reasonable doubt and there is no \u03c8 such that believing \u03c8 is more reasonable for a at time t than believing \u03c6.\nB5(a, t,\u03c6)\u21d4\n{ B3(a, t,\u03c6)\u2227\n\u00ac\u2203\u03c8 : B(a, t,\u03c8) at B(a, t,\u03c6)\nThe above definitions are from Chisholm but more rigorously formalized in \u00b5C . The definitions almost give us S except for the fact that at is undefined. While Chisholm gives a careful and informal analysis of the relation, he does not provide a more precise definition. Such a definition is needed for automation. We provide a three clause defintion that is based on both probabilities and proof theory.\nThere are many probability logics that allow us to define probabilities over formulae. They are well studied and understood for propositional and first-order logics. Let L be the set of all formulae in \u00b5C . Let Lp be a pure first-order subset of L . Assume that we have the following partial probability function defined over Lp5:\nPr : Agent\u00d7Moment\u00d7Formula 7\u2192 R\nThen we have the first clause of our definition for at . Clause I. Defining\nIf Pr(a, t,\u03c6) and Pr(a, t,\u03c8) are defined then:( B ( a, t,\u03c6 ) at B ( a, t,\u03c6 )) \u21d4 ( Pr(a, t,\u03c6)> Pr(a, t,\u03c8) )\nWe might not always have meaningful probabilities for all propositions. For example, consider propositions of the form \u201cI believe that Jack believes that \u03c6.\u201d It is hard to get precise numbers for such statements. In such situations, we might look at the ease of derivation of such statements given a knowledge base \u0393. Given two competing statements \u03c6 and \u03c8, we can say one is more reasonable than the other if we can easily derive one more than the other from \u0393. This assumes that we can derive \u03c6 and \u03c8 from \u0393. We assume we have a cost function \u03c1 : Proof 7\u2192R+ that lets us compute costs of proofs.\n5Something similar to the system in [Halpern, 1990] that accounts for probabilities as statistical information or degrees of belief can work.\nThere are many ways of specifying such functions. Possible candidates are length of the proof, time for computing the proof, depth vs breadth of the proof, unique symbols used in the proof etc. We leave this choice unspecified but any such function could work here. Let `a,t denote provability w.r.t. to agent a at time t.\nClause II. Defining\nIf one of Pr(a, t,\u03c6) and Pr(a, t,\u03c8) is not defined, but if \u0393 `a,t \u03c6 and \u0393 `a,t \u03c8:(\n\u03c6 at \u03c8 ) \u21d4 ( \u03c1 ( \u0393 `a,t \u03c6 ) < \u03c1 ( \u0393 `a,t \u03c8 )) Clauses I and II might not always hold. A more common case could be when we cannot derive the propositions of interest from our background set of axioms \u0393. For example, if we are interested in the uncertainty values for statements that we know are false, then it should be the case that they be not derivable from our background set of axioms. In this situation, we look at \u0393 and see what minimal changes we can make to it to let us derive the proposition of interest. Trivially, if we cannot derive \u03c6 from \u0393, we can add it to \u0393 to derive it, as \u0393+\u03c6 ` \u03c6. This is not desirable for two reasons.\nFirst, simply adding to \u0393 might result in a contradiction. In such cases we would be looking at removing a minimal set of statements \u039b from \u0393. Second, we might prefer to add a more simpler set of propositions \u0398 to \u0393 rather than \u03c6 itself to derive \u03c6. Recapping, we go from (1) to (2) below:\n\u0393 6` \u03c6 (1) \u0393\u222a\u0398\u2212\u039b ` \u03c6 (2)\nWhen we go from (1) to (2) we would like to modify the background axioms as minimally as possible. Assume that we have a similarity function \u03c0 for sets of formulae. We then choose \u0398 and \u039b as given below (Con[S] denotes that S is consistent):\n\u3008\u0398,\u039b\u3009= argmin \u3008\u0398,\u039b\u3009 \u03c0(\u0393,\u0393\u222a\u0398\u2212\u039b); such that { \u0393\u222a\u0398\u2212\u039b ` \u03c6; and Con [ \u0393\u222a\u0398\u2212\u039b\n] Consider a statement such as \u201cIt rained last week\u201d when it did not actually rain last week, and another statement such as \u201cThe moon is made of cheese.\u201d Both statements denote things that did not happen, but intuitively it seems that former should be more easier to accept from what we know than the latter. There are many similarity measures which can help convey this. Analogical reasoning is one such possible measure of similarity. If the new formulae are structurally similar to existing formulae, then we might be more justified in accepting such formulae. For example, one such measure could be the analogical measure used by us in [Licato et al., 2013].\nNow we have the formal mechanism in place for defining the final clause in our definition for our reasonableness. Let \u03b4at (\u0393,\u03c6) be the distance between \u0393 and closest consistent set under \u03c0 that lets us prove \u03c6:\n\u03b4at (\u0393,\u03c6)\u2261 min\u3008\u0398,\u039b\u3009\n{ \u03c0 ( \u0393,\u0393\u222a\u0398\u2212\u039b )\u2223\u2223\u2223\u2223\u2223 ( \u0393\u222a\u0398\u2212\u039b ) `at \u03c8; and Con [ \u0393\u222a\u0398\u2212\u039b ] }\nClause III. Defining\nIf one of Pr(a, t,\u03c6) and Pr(a, t,\u03c8) is not defined, and one of \u0393 `a,t \u03c6 and \u0393 `a,t \u03c8 does not hold, then(\n\u03c6 at \u03c8 ) \u21d4 [ \u03b4at (\u0393,\u03c6)< \u03b4 a t (\u0393,\u03c8) ]\nThe final piece of S is inference rules for belief propagation with uncertainty values. This is quite straightforward. Inferences propagate uncertainty values from the premises with the lowest strength factor; and inferences happen only with beliefs that are close in their uncertainty values, with maximum difference being parametrized by u, with default u = 2.\nInference Schemata for S\nP(a, t1,\u03c61), \u0393 ` t1 < t2 B5(a, t2,\u03c6) [RsP]\nBs1(a, t1,\u03c61), . . . ,Bsm(a, tm,\u03c6m),{\u03c61, . . . ,\u03c6m} ` \u03c6,\u0393 ` ti < t Bmin(s1,...,sm)(a, t,\u03c6) [RsB]\nwith max({s1, . . . ,sm})\u2212min({s1, . . . ,sm})\u2264 u"}, {"heading": "5 Usage", "text": "In this section, we illustrate S by applying it solve problems of foundational interest such as the lottery paradox [Kyburg Jr, 1961, p. 197] and a toy version of a more real life example, a murder mystery example (following in the traditions of logic pedagogy). Finally, we very briefly sketch abstract scenarios in which S can be used to generate uncertainty values for counterfactual statements and to generate explanations for actions."}, {"heading": "5.1 Paradoxes: Lottery Paradox", "text": "In the lottery paradox, we have a situation in which an agent a comes to believe \u03c6 and \u00ac\u03c6 from a seemingly consistent set of premises \u0393L describing a lottery. Our solution to the paradox is that the agent simply has different strengths of beliefs in the proposition and its negation. We first go over the paradox formalized in \u00b5C and then present the solution.\nLet \u0393L be a meticulous and perfectly accurate description of a 1,000,000,000,000-ticket lottery, of which rational agent a is fully apprised. Assume that from \u0393L it can be proved that either ticket 1 will win or ticket 2 will win or . . . or ticket 1,000,000,000,000 will win. Lets write this (exclusive) disjunction as follows (here \u2295 is an exclusive disjunction):\n\u0393L ` win(t1)\u2295win(t2)\u2295 . . .\u2295win(t1,000,000,000,000)\nThe paradox has two strands of reasoning. The first strand yields B(a,now,\u03c6) and the second strand yields B(a,now,\u00ac\u03c6) with \u03c6\u2261 \u2203t : win(t). Strand 1: Since a believes all propositions in \u0393L, a can then deduce from this the belief that there is at least one ticket that will win, a proposition represented as:\nS1 B ( a,now,\u2203t : win ( t ))\nStrand 2: From \u0393L it can be proved that the probability of a particular ticket ti winning is 10\u221212.[\nPr ( a,now,win ( t1 )) = 10\u221212 ] \u2227 [ Pr ( a,now,win ( t2 )) = 10\u221212 ]\n\u2227 . . .\u2227 [ Pr ( a,now,win ( t1T )) = 10\u221212 ]\nFor the next step, note that the probability of ticket t1 winning is lower than, say, the probability that if you walk outside a minute from now, you will be flattened on the spot by a door from a 747 that falls from a jet of that type cruising at 35,000 feet. Since you, the reader, have the rational belief that death won\u2019t ensue if you go outside (and have this belief precisely because you believe that the odds of your sudden demise in this manner are vanishingly small), the inference to the rational belief on the part of a that t1 won\u2019t win sails through \u2014 and this of course works for each ticket. Hence we have as a valid belief (though not derivable in \u00b5C from \u0393L):\nB ( a,now,\u00acwin ( t1) ) \u2227B ( a,now,\u00acwin ( t2) ) \u2227 . . .\n\u2227B ( a,now,\u00acwin ( t1T ) ) From RB and above, we get:\nB ( a,now,\u00acwin ( t1)\u2227\u00acwin ( t2)\u2227 . . .\u2227\u00acwin ( t1T ) ) Applying RB to the above and \u0393L, we get:\nS2 B ( a,now,\u00ac\u2203t : win(t) )\nThe two strands are complete, and we have derived contradictory beliefs labeled S1 and S2. Our solution consists of two new uncertainty infused strands that result in beliefs of sufficiently varying strengths that block inferences that could combine them. Strand 3: Assume that a is certain of all propositions in \u0393L, then using RsB, we have:\nS3 B5 ( a,now,\u2203t : win(t) ) Strand 4: Since Pr(a,now,win(ti)) < Pr(a,now,\u00acwin(ti)), using Clause I and the strength factor definitions, we have now that for all ti\nB2 ( a,now,\u00acwin(ti) ) Using the reasoning similar to that in Strand 2, we get:\nS4 B2 ( a,now,\u00ac\u2203t : win(t) )\nStrands 3 and 4 resolve the paradox. Note that RsB cannot be applied to S3 and S4 and churn out arbitrary propositions, as the default value of the u parameter in RsB requires beliefs to be no more than 2 levels apart."}, {"heading": "5.2 Application: Solving a Murder", "text": "We look at a toy example in which an agent s has to solve a murder that happened at time t3. s believes that either Alice or Bob is the murderer. The agent knows that there is a gun gun involved in the murder and that the owner of the gun at t3 committed the murder. s also knows that Alice is the owner of the gun initially at time t0.\nPresumption in Favor of Alice Being the Murderer\nFrom just these facts, the agent has some presumption for believing that Alice is the murderer.\nProof Sketch: All the above statements can be taken as certain beliefs B5 of s. For convenience, we consider the formulae directly without the belief operators.\nIn order to prove the above, we need to prove that it is easier for the agent to derive that Alice is the murderer than to derive that Alice is not the murderer. First, to prove the former, the agent just has to assume that Alice\u2019s ownership of the gun did not change from t0 to t3. Second, in order for the agent to believe that Alice did not commit the murder but Bob committed it, the agent must be willing to admit that something happened to change Alice\u2019s ownership of the gun from time t0 to t3 that results in Bob owning the gun. One possibility is that Alice simply sold the gun to Bob. Both the scenarios are shown as proofs in the Slate theorem proving workspace [Bringsjord et al., 2008] in the Appendix. Figure 1 shows a proof modulo belief operators of B(s,now,Murderer(Alice)) from \u0393\u222a\u03981 and Figure 2 shows a proof of B(s,now,\u00acMurderer(Alice)) from \u0393\u222a\u03982.\nIf we assume that \u03981 and \u03982 exhaust the space of allowed additions, then it easy to see how syntactic measures of complexity will yield that \u03b4at ( \u0393,\u0393\u222a\u03981 ) < \u03b4at ( \u0393,\u0393\u222a\u03982 ) as \u03982 is more complex than \u03981. This lets us derive that s has some presumption in favor of Murderer(Alice).\nWhat happens if the agent knows or has a belief with certainty that Alice\u2019s ownership of the gun did not change from t0 to t3?\nBeyond Reasonable Doubt that Alice is the Murderer\nIf the agent is certain that Alice\u2019s ownership of the gun did not change from t0 till t3, the agent has beyond reasonable doubt that she is the murderer.\nProof Sketch: In this case we directly have that: \u0393 ` B(s,now,Murderer(Alice)) \u0393 6 ` \u00acB(s,now,Murderer(Alice)) \u0393 6 ` \u00acB(s,now,\u00acMurderer(Alice))\nIn order to flip the last two statements above, we need to modify \u0393, but we can derive that Alice is the murderer without any modifications, and since \u03b4at (\u0393,\u0393) = 0, it easier to believe Alice is the murderer than to withhold that Alice is the murderer."}, {"heading": "5.3 Counterfactuals", "text": "At time t, assume that an agent a believes in a set of propositions \u0393 and is interested in propositions holds( f , t \u2032) and holds(g, t \u2032) with t \u2032 < t and:\n\u0393 ` \u00acholds( f , t \u2032)\u2227\u00acholds(g, t \u2032)\nWe may need non-trivial uncertainty values, but in this case, Pr will assign a trivial value of 0 to both the propositions. We can then look at closest consistent sets to \u0393 under \u03b4:\n\u03931 ` holds( f , t \u2032) \u03932 ` holds(g, t \u2032)\nClause III from the definition for reasonableness gives us: B ( a, t,holds( f , t \u2032) ) at B ( a, t,holds(g, t \u2032) ) \u21d4\n\u03b4at (\u0393,\u03931)< \u03b4 a t (\u0393,\u03932)"}, {"heading": "5.4 Explanations", "text": "The definitions of the strength factors and reasonableness above can be used to generate high-level schemas for explanations. These schemas can be used instead of simply presenting raw probability values to end-users. While we have not fleshed out such explanation schemas, we illustrate one possible schema. Say an agent performs an action \u03b1 on the basis of \u03c6. In this case, the agent could generate an explanation that at the highest level simply says that it is more reasonable for the agent to believe \u03c6 than for the agent to believe in \u00ac\u03c6. The agent could then further explain why it was reasonable for it by using one of the three clauses in the reasonableness definition."}, {"heading": "6 Inference Algorithm Sketch", "text": "Describing the inference algorithm in detail is beyond the scope of this paper, but we provide a high-level sketch here.6 Our proof calculus is simply an extension of standard firstorder proof calculus under different modal contexts. For example, if a believes that b believes in a set of propositions \u0393 and \u0393 `FOL \u03c8, then a believes that b believes \u03c8. We convert B(a, ta,B(b, tb,Q)) into the pure first-order formula Q(context(a, ta,b, tb)) and use a first-order prover. The conversion process is a bit more nuanced as we have to handle negations, properly handle substitutions of equalities, uncertainties and transform compound formulae within iterated beliefs."}, {"heading": "7 Conclusion and Future Work", "text": "We have presented initial steps in building a system of uncertainty that is both probability and proof theory based that could lend itself to (1) solving foundational problems; (2) being useful in applications; (3) generating uncertainty values for counterfactuals; and (4) building understandable explanations.\nMany challenges remain, some relatively easy and some quite hard. Among the easy challenges are defining and experimenting with different candidates for Pr, \u03c1, \u03c0 and \u03b4. On the more difficult side, we have to come up with tractable computational mechanisms for computing the min\u3008\u0398,\u039b\u3009 in the definition for \u03b4. Also on the difficult side, is the challenge of coming up efficient reasoning schemes. While we have an exact inference algorithm, we believe that an approximate algorithm that selectively discards beliefs in a large knowledge base during reasoning will be more useful.\n6More details can be found here: https://goo.gl/2Vz2nJ"}, {"heading": "8 Acknowledgements", "text": "We are grateful to the Office of Naval Research for their funding of research that enabled the research presented in this paper."}, {"heading": "A Appendix: Slate Proofs", "text": "The figures below are vector graphics and can be zoomed to more easily read the contents.\nFigure 1: Alice is the murder: B ( s, t,Murderer(Alice) )\nBeliefs of s\n\u21e51 FOL \u22a2 \u2713\n\u0393 6. \u2200x (Murderer(x) \u2194 Holds(Owns(x,gun),t3)) {\u0393 6} Assume \u2713\n\u0393 4. \u2200x,t ((Holds(Owns(x,gun),t) \u2227 holds(Owns(y,gun),t)) \u2192 (x = y)) {\u0393 4} Assume \u2713\n\u0393 3. Murderer(Bob) \u2192 \u00acMurderer(Alice) {\u0393 3} Assume \u2713\n\u0393 1. Murderer(Alice) \u2228 Murderer(Bob) {\u0393 1} Assume \u2713\n\u0393 5. Alice \u2260 Bob {\u0393 5} Assume \u2713\n\u0393 2. Murderer(Alice) \u2192 \u00acMurderer(Bob) {\u0393 2} Assume \u2713\n\u0393 8. \u2200f,t ((Initially(f) \u2227 \u00acClipped(t0,f,t)) \u2192 Holds(f,t)) {\u0393 8} Assume \u2713\n\u0393 7. Initially(Owns(Alice,gun)) {\u0393 7} Assume \u2713\n\u0398 1. \u00acClipped(t0,Owns(Alice,gun),t3) {\u0398 1} Assume \u2713\n10. Murderer(Alice) {\u0393 1,\u0393 2,\u0393 3,\u0393 4,\u0393 5,\u0393 6,\u0393 7,\u0393 8,\u0398 1}\n( s, t,\u00acMurderer(Alice) )"}], "references": [{"title": "Toward Formalizing Common-Sense Psychology: An Analysis of the False-Belief Task", "author": ["Arkoudas", "Bringsjord", "2008] Konstantine Arkoudas", "Selmer Bringsjord"], "venue": "Proceedings of the Tenth Pacific Rim International Conference on Artificial Intelligence (PRICAI", "citeRegEx": "Arkoudas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Arkoudas et al\\.", "year": 2008}, {"title": "Slate: An Argument-Centered Intelligent Assistant to Human Reasoners", "author": ["Bringsjord et al", "2008] Selmer Bringsjord", "Joshua Taylor", "Andrew Shilliday", "Micah Clark", "Konstantine Arkoudas"], "venue": "Proceedings of the 8th International Workshop", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "In Proceedings of ETHICS \u2022 2014 (2014 IEEE Symposium on Ethics in Engineering", "author": ["Selmer Bringsjord", "Naveen Sundar Govindarajulu", "Daniel Thero", "Mei Si. Akratic Robots", "the Computational Logic Thereof"], "venue": "Science, and Technology), pages 22\u201329, Chicago, IL,", "citeRegEx": "Bringsjord et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Theory of Knowledge 3rd ed", "author": ["Roderick Chisholm"], "venue": "Prentice-Hall, Englewood Cliffs, NJ,", "citeRegEx": "Chisholm. 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "editor", "author": ["Lorenz Demey", "Barteld Kooi", "Joshua Sack. Logic", "Probability. In Edward N. Zalta"], "venue": "The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, winter 2016 edition,", "citeRegEx": "Demey et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "editor", "author": ["Gerhard Gentzen. Investigations into Logical Deduction. In M.E. Szabo"], "venue": "The Collected Papers of Gerhard Gentzen, pages 68\u2013131. North-Holland, Amsterdam, The Netherlands,", "citeRegEx": "Gentzen. 1935", "shortCiteRegEx": null, "year": 1935}, {"title": "On Automating the Doctrine of Double Effect", "author": ["Naveen Sundar Govindarajulu", "Selmer Bringsjord"], "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI 2017),", "citeRegEx": "Govindarajulu and Bringsjord. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Artificial intelligence", "author": ["Joseph Y Halpern. An Analysis of First-order Logics of Probability"], "venue": "46(3):311\u2013350,", "citeRegEx": "Halpern. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Can Jurors Understand Probabilistic Evidence? Journal of the Royal Statistical Society", "author": ["D.H. Kaye", "Jonathan J. Koehler"], "venue": "Series A (Statistics in Society), 154(1):75\u201381,", "citeRegEx": "Kaye and Koehler. 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "Middletown", "author": ["Henry E Kyburg Jr. Probability", "the Logic of Rational Belief. Wesleyan University Press"], "venue": "CT,", "citeRegEx": "Kyburg Jr. 1961", "shortCiteRegEx": null, "year": 1961}, {"title": "Analogico-Deductive Generation of G\u00f6del\u2019s First Incompleteness Theorem from the Liar Paradox", "author": ["Licato et al", "2013] John Licato", "Naveen Sundar Govindarajulu", "Selmer Bringsjord", "Michael Pomeranz", "Logan Gittelson"], "venue": "Proceedings of the 23rd International Joint Conference on Arti-", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Cambridge", "author": ["Edward N Zalta. Intensional Logic", "the Metaphysics of Intentionality. MIT Press"], "venue": "MA,", "citeRegEx": "Zalta. 1988", "shortCiteRegEx": null, "year": 1988}], "referenceMentions": [{"referenceID": 3, "context": "Specifically, we add a system of uncertainty derived from Chisholm\u2019s epistemology [Chisholm, 1987].", "startOffset": 82, "endOffset": 98}, {"referenceID": 8, "context": "see [Kaye and Koehler, 1991]); and we believe that our approach borrowed from philosophy can pave the way for systems that can present uncertain statements in a more understandable format to lay users.", "startOffset": 4, "endOffset": 28}, {"referenceID": 6, "context": "2 More recently, using DCEC we have presented an automation of the doctrine of double effect in [Govindarajulu and Bringsjord, 2017].", "startOffset": 96, "endOffset": 132}, {"referenceID": 2, "context": "Other tasks automated by cognitive calculi include the false-belief task [Arkoudas and Bringsjord, 2008] and akrasia (succumbing to temptation to violate moral principles) [Bringsjord et al., 2014].", "startOffset": 172, "endOffset": 197}, {"referenceID": 11, "context": "See [Zalta, 1988] for a detailed treatment of intensionality.", "startOffset": 4, "endOffset": 17}, {"referenceID": 5, "context": "deduction [Gentzen, 1935], and includes all the introduction and elimination rules for first-order logic, as well as inference schemata for the modal operators and related structures.", "startOffset": 10, "endOffset": 25}, {"referenceID": 4, "context": "On the uncertainty and probability front, there have been many logics of probability, see [Demey et al., 2016] for an overview.", "startOffset": 90, "endOffset": 110}, {"referenceID": 7, "context": "Among first-order systems, the seminal work in [Halpern, 1990] presents a first-order logic with modified semantics to handle probabilistic statements.", "startOffset": 47, "endOffset": 62}, {"referenceID": 7, "context": "5Something similar to the system in [Halpern, 1990] that accounts for probabilities as statistical information or degrees of belief can work.", "startOffset": 36, "endOffset": 51}], "year": 2017, "abstractText": "We present a new system S for handling uncertainty in a quantified modal logic (first-order modal logic). The system is based on both probability theory and proof theory. The system is derived from Chisholm\u2019s epistemology. We concretize Chisholm\u2019s system by grounding his undefined and primitive (i.e. foundational) concept of reasonableness in probability and proof theory. S can be useful in systems that have to interact with humans and provide justifications for their uncertainty. As a demonstration of the system, we apply the system to provide a solution to the lottery paradox. Another advantage of the system is that it can be used to provide uncertainty values for counterfactual statements. Counterfactuals are statements that an agent knows for sure are false. Among other cases, counterfactuals are useful when systems have to explain their actions to users (If I had not done \u03b1, then \u03c6 would have happened). Uncertainties for counterfactuals fall out naturally from our system. Efficient reasoning in just simple first-order logic is a hard problem. Resolution-based first-order reasoning systems have made significant progress over the last several decades in building systems that have solved non-trivial tasks (even unsolved conjectures in mathematics). We present a sketch of a novel algorithm for reasoning that extends firstorder resolution. Finally, while there have been many systems of uncertainty for propositional logics, first-order logics and propositional modal logics, there has been very little work in building systems of uncertainty for first-order modal logics. The work described below is in progress; and once finished will address this lack.", "creator": "LaTeX with hyperref package"}}}