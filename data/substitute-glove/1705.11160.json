{"id": "1705.11160", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Learning When to Attend for Neural Machine Translation", "abstract": "In part past less when, attention mechanisms have become comes indispensable component between its - giving - row cortex invented script applications. However, pre attention product seems refer way some presence: when predicting a target word, from confirming with the extent we often considering words once saying component essential words. Motivated by all shuttle, mean propose means seminal continuing designed that neither part mobility where determining put up decoder should attend. source expressions and put something must not. Experimental results on NIST Chinese - English mentioned tasks though although place making model validation another ensuring of piacenza. 21 BLEU score four a arizona - is - the - art stretch.", "histories": [["v1", "Wed, 31 May 2017 16:05:49 GMT  (124kb)", "http://arxiv.org/abs/1705.11160v1", "5 pages, 2 figures"]], "COMMENTS": "5 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["junhui li", "muhua zhu"], "accepted": false, "id": "1705.11160"}, "pdf": {"name": "1705.11160.pdf", "metadata": {"source": "CRF", "title": "Learning When to Attend for Neural Machine Translation", "authors": ["Junhui Li"], "emails": ["lijunhui@suda.edu.cn", "muhuazhu@tencent.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n11 16\n0v 1\n[ cs\n.C L\n] 3\n1 M\nay 2\n01 7\nanisms have become an indispensable component of end-to-end neural machine translation models. However, previous attention models always refer to some source words when predicting a target word, which contradicts with the fact that some target words have no corresponding source words. Motivated by this observation, we propose a novel attention model that has the capability of determining when a decoder should attend to source words and when it should not. Experimental results on NIST Chinese-English translation tasks show that the new model achieves an improvement of 0.8 BLEU score over a state-of-the-art baseline."}, {"heading": "1 Introduction", "text": "The past several years have witnessed rapid progress of end-to-end neural machine translation (NMT)models, most of which are built on the base of encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a). In addition, attention mechanisms have become an indispensable component in state-of-the-art NMT systems (Luong et al., 2015b; Tu et al., 2016b). The idea of attention mechanisms is to guide a translation decoder to selectively focus on a local window of source words that are used to generate the current target word. Previous studies have demonstrated necessity and effectiveness of such attention mechanisms.\nHowever, previous attention models are all dedicated to solving the problem of where to attend. They take no account of when to attend. In fact, target words in a translation are not always generated according to the source sentence. Take the\nChinese-English translation in Figure 1 as an example, where words are manually aligned. The English words to, enjoys, that, a are not translated from any source words. Instead, it is appropriate to use a language model to predict the words by conditioning on their preceding words. To show how prevalent the phenomenon is, we analyze a set of 900 Chinese-English sentence pairs with manual word alignments (Yang and Maosong, 2015), and find that 25.3% (21,244/28,433) English words are not translations from Chinese words. Thus, an attention mechanism should distinguish between target words that are generated referring to the source sentence and the words that are generated according to a language model.\nTo this end, we propose a novel attention mechanism that is equipped with a component named attention sentinel, on which a decoder can fall back when it chooses not to attend to the source sentence. Hereafter, the improved attention mechanism is referred to as adaptive attention since it can choose adaptively between relying on a regular attention component and falling back on the attention sentinel. We build a new NMT system by integrating an adaptive attention model into the NMT system described in (Bahdanau et al., 2015). To show the effectiveness of adaptive attention, we conduct experiments on Chinese-English translation tasks with standard NIST datasets. Results show that the proposed adaptive attention model achieves an improvement of 0.8 BLEU score. To the best of our knowledge, the adaptive attention method discussed here has not been used before for NMT, although the problem we intend to attack is not new (Tu et al., 2016a).\nThe rest of the paper is structured as follows. In Section 2 we first introduce a conventional attention mechanism for NMT (Section 2.1), then present a detailed description of adaptive attention (Section 2.2). Experiments are presented in Sec-\ntion 3. After comparing with related work in Section 4, we finally conclude our work in Section 5."}, {"heading": "2 Method", "text": ""}, {"heading": "2.1 Attention-based NMT", "text": "We start by describing an NMT model, which builds on the base of an RNN encoder-decoder framework (Sutskever et al., 2014) and attention mechanisms. Given a source sentence X = {x1, . . . , xJ} and the corresponding target sentence Y = {y1, . . . , yK}, the model seeks to maximize an objective function that is defined as loglikelihood:\n\u03b8\u2217 = argmax \u2211\n(X,Y )\nlogP (Y |X; \u03b8) (1)\nwhere \u03b8 are the parameters of the model.\nRegarding the decoding phase, the model produces a translation by choosing a target word yi at each time step i. 1 The probability of word prediction is conditioned on the source sentence X and previously generated words yi, . . . , yi\u22121, as defined as the following softmax function:\nP (yi|y<i,X) = softmax(g(yi\u22121, ti, ci)) (2)\nwhere g(\u00b7) is a non-linear function, and ti is a decoding state for the time step i, which is initialized with an encoding vector of X and is computed by\nti = f(ti\u22121, xi\u22121, ci) (3)\nThe activation function f(\u00b7) can be a vanilla RNN (Boden, 2002) or sophisticated units such as Gated Recurrent Unit (GRU) (Cho et al., 2014) and Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997). For the model in this paper we choose to use\n1This greedy search process can be extended with beam search in a straightforward way.\nGRU.\nci in Eq. 2 and Eq. 3 are the attention model from the source sentence, which can be defined as:\nci = J\u2211\nj=1\n\u03b1i,j \u00b7 hj (4)\nwhere hj = [ \u2212\u2192 h Tj ; \u2190\u2212 h Tj ] T represents the annotation vector of the source word xj generated by a bi-directional RNN (Schuster and Paliwal, 1997), and the weight \u03b1i,j is calculated as follows:\n\u03b1i,j = ei,j\u2211J j=1 ei,j\n(5)\nHere ei,j measures the similarity between the target word yi and the source word xj , which can be calculated with diverse methods (Luong et al., 2015b). In this paper we specifically utilize the following one:\nei,j = a(ti\u22121, hj) = V Ta tanh(Wati\u22121 + Uahj)\n(6)\nwhere V Ta , Wa, and Ua are parameters to be learned. The architecture of the attention model described above is depicted in Figure 2(a)."}, {"heading": "2.2 Adaptive Attention Model", "text": "Although the attention model introduced above has shown its effectiveness in NMT, it cannot tell when a decoder should use the attention model information and when the decoder should not. Motivated from the work in (Merity et al., 2016; Lu et al., 2016), we introduce the concept of attention sentinel, which is a latent representation of what a decoder already knows. A decoder can fall back on the attention sentinel when it chooses to \u201comit\u201d the source sentence for some time steps.\nAttention sentinel: A decoder\u2019s memory stores information from both the source sentence and the\ntarget-size language model. From the state we learn a new component that can be used when the decoder chooses not to attend to the source sentence. Such a component is called the attention sentinel. For a decoder that uses GRU-RNN, the attention sentinel vector si is defined by Eq. 7 and Eq. 8. 2\ngi = \u03c3(Wxxi +Wtti\u22121) (7)\nsi = gi \u2299 tanh(Wsti) (8)\nwhere Wx, Wt, and Ws are parameters of the attention sentiel, xi is the input to GRU at the time step i, \u03c3 represents a sigmoid activation function, and \u2299 means an element-wise product. Based on the attention sentinel, we can propose our adaptive attention model (depicted in Figure 2(b)). The new model has a context vector c+i , defined as a linear combination of si and ci:\nc+i = \u03b2isi + (1\u2212 \u03b2)ci (9)\nwhere \u03b2i is a sentinel gate at time step i, which always takes a scalar value between 0 and 1. A value of 1 means that only attention sentinel information is used. To learn the parameter \u03b2i, we extend the vector ei = [ei1, . . . , eiJ ] with a new element\ne\u0302i = [ei;W T h tanh(Wssi + Uahi)] (10)\nwhere [.; .] indicates vector concatenation, andWs and Wh are parameters to be learned. Now the weights for the adaptive attention model are computed by:\n\u03b1\u0302i = softmax(e\u0302i) (11)\n2The attention sentinel for LSTM-RNN can be defined in a similar way; Readers interested can refer to (Lu et al., 2016) for a detailed description.\nHere \u03b1\u0302i is a vector with J + 1 dimensions. We take the last element of the vector as the sentinel gate value: \u03b2i = \u03b1\u0302i[J + 1]. Decoder prediction: The prediction over a vocabulary is a standard softmax function with an extended attention mechanism:\npi = softmax(Wp(c + i + ti)) (12)\nwhereWp are parameter to be learned."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Setup", "text": "We conducted experiments on NIST ChineseEnglish translation tasks. Our training data consists of 1.25M sentence pairs extracted from LDC corpora, 3 which contain 27.9M Chinese words and 34.5M English words, respectively. In all the experiments, we used the NIST 2006 dataset (1,664 sentence pairs) for system development and tested the system on the NIST 2003, 2004, 2005 datasets (919, 1,788, 1,082 sentence pairs, respectively). We used the case-insensitive 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric.\nFor efficient training of neural networks, we utilized sentences of length up to 50 words in the training data. Moreover, we limited the source and target vocabularies to the 16K most frequent words, which cover 95.8% and 98.2%word tokens of source and target sentences, respectively. All the out-of-vocabulary (OOV) words were mapped to the special token UNK. The word embedding dimension is set to 620 and the size of a hidden layer is set to 1,000. The beam size for translation is set to 10. All the other settings are the same as in (Bahdanau et al., 2015).\nWe compared our system with two representative translation systems, one for conventional statistical machine translation (SMT) and the other for NMT.\n\u2022 cdec (Dyer et al., 2010): an open-source hierarchical phrase-based translation sys-\ntem (Chiang, 2007) with default configuration and 4-gram language model trained on the target sentences of training data. 4\n3The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06.\n4 https://github.com/redpony/cdec\n\u2022 RNNSearch: a re-implementation of the attention-based neural machine transla-\ntion system (Bahdanau et al., 2015) with slight changes from dl2mt tutorial. 5 RNNSearch uses GRU as the activation function of an RNN and incorporates dropout (Srivastava et al., 2012) on the output layer. We use AdaDelta (Zeiler, 2012) to optimize model parameters. For translation, the beam size is also set to 10."}, {"heading": "3.2 Main Results", "text": "The main results are shown in Table 1, where the parameter size, training speed, and performance of each system are presented. From the results we can see that the best adaptive attention model achieved an improvement of 0.7 BLEU score over RNNSearch on the development set. We then evaluated the same model on the test sets, and a significant improvement of 0.8 BLEU score was achieved over RNNSearch (the improvement over cdec is 2.0 BLEU score). On the other hand, we find that adaptive attention model incurs more parameters than RNNSearch (60.6M vs. 70.6M). And more training time is required (153minutes/epoch vs. 207minutes/epoch)."}, {"heading": "3.3 Analysis", "text": "It is interesting to examine what kind of words tends not to attend to source sentences. To this end, we translated the set of Chinese sentences extracted from NIST 2003, 2004, and 2005 datasets, and recorded all the predicted target words that have a sentinel gate value greater than or equal to 0.9. From the resulted word list, we present the top 15 most frequent words and their frequency counts in Table 2.\nFrom the table we can see that the translation system is inclined to rely on the attention sentinel to generate auxiliary words, such as the and to. This observation is consistent with our intuition. Regarding the token UNK, recall that the symbol is a mapping from OOV words, whose lexical information is lost due to the mapping. Thus, resorting to the attention sentinel to predict UNK is an appropriate choice. Finally, states appears in the top word list because this word, most of the time, occurs immediately after the word united in our data. Thus, states can be predicted without referring to the source sentence when united appears as the\n5 https://github.com/nyu-dl/dl4mt-tutorial\npreceding word. Inspired by the observation, we further conclude that the adaptive attention model can help predict words in named entities and collocations, in addition to unaligned words."}, {"heading": "4 Related Work", "text": "Attention mechanism have become a standard component of state-of-the-art neural NMT systems in some sense. Bahdanau et al. (2015) propose a model to jointly align and translate words. Luong et al. (2015b) propose and compare diverse attention models. Tu et al. (2016b) propose to extend attention models with a coverage vector in order to attack the problem of under-translation and over-translation. All the previous attention models work well, but they cannot tell when not to attend to source sentences.\nOur work is inspired by Lu et al. (2016), which propose an adaptive attention model for the task of image captioning. The main difference is that they build their adaptive attention model on the base of a spatial attention model, which is different from conventional attention models for NMT. Moreover, our adaptive attention model uses GRU as the RNN activation function while Lu et al. (Lu et al., 2016) adopt LSTM. Regarding the literature of NMT, the most related work is Tu et al. (2016a), which utilize a context gate to trade off the source-side and target-side context. In this paper, we instead focus on designing a new attention mechanism."}, {"heading": "5 Conclusion", "text": "In this paper, we addressed the problem of learning when to attend to source sentence. We introduced a new component named attention sentinel, based which we built an adaptive attention model. Experiments on NIST Chinese-English translation tasks show that the model achieved a significant improvement of 0.8 BLEU score."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A guide to recurrent neural networks and back-propagation", "author": ["Mikael Boden."], "venue": "the Dallas project.", "citeRegEx": "Boden.,? 2002", "shortCiteRegEx": "Boden.", "year": 2002}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Computational Linguistics 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of EMNLP. pages 1724\u2013", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Adam Lopz", "Juri Ganitkevitch", "Jonathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of EMNLP. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2014", "shortCiteRegEx": "Koehn.", "year": 2014}, {"title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning", "author": ["Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Socher Richard."], "venue": "arXiv preprint arxiv:1612.01887.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL. pages 11\u201319.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of EMNLP. pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Socher Richard."], "venue": "arXiv preprint arXiv:1609.07843.", "citeRegEx": "Merity et al\\.,? 2016", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Bleu: A method for automatic evaluation", "author": ["Kishore Papineni", "Salim Roukos", "ToddWard", "andWeiJing Zhu"], "venue": "In Proceedings of ACL", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "IEEE Transactions on Signal Processing 45(1):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov", "Geoffrey E. Hinto."], "venue": "arXiv preprint arXiv:1207.0580.", "citeRegEx": "Srivastava et al\\.,? 2012", "shortCiteRegEx": "Srivastava et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Proceedings of NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Context gates for neural machine translation", "author": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li."], "venue": "TACL 5:87\u201399.", "citeRegEx": "Tu et al\\.,? 2016a", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of ACL. pages 76\u201385.", "citeRegEx": "Tu et al\\.,? 2016b", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Contrastive unsupervised word alignment with non-local features", "author": ["Liu Yang", "Sun Maosong."], "venue": "Proceedings of AAAI. pages 857\u2013868.", "citeRegEx": "Yang and Maosong.,? 2015", "shortCiteRegEx": "Yang and Maosong.", "year": 2015}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 14, "context": "The past several years have witnessed rapid progress of end-to-end neural machine translation (NMT)models, most of which are built on the base of encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 172, "endOffset": 240}, {"referenceID": 0, "context": "The past several years have witnessed rapid progress of end-to-end neural machine translation (NMT)models, most of which are built on the base of encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 172, "endOffset": 240}, {"referenceID": 8, "context": "The past several years have witnessed rapid progress of end-to-end neural machine translation (NMT)models, most of which are built on the base of encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 172, "endOffset": 240}, {"referenceID": 9, "context": "In addition, attention mechanisms have become an indispensable component in state-of-the-art NMT systems (Luong et al., 2015b; Tu et al., 2016b).", "startOffset": 105, "endOffset": 144}, {"referenceID": 16, "context": "In addition, attention mechanisms have become an indispensable component in state-of-the-art NMT systems (Luong et al., 2015b; Tu et al., 2016b).", "startOffset": 105, "endOffset": 144}, {"referenceID": 17, "context": "To show how prevalent the phenomenon is, we analyze a set of 900 Chinese-English sentence pairs with manual word alignments (Yang and Maosong, 2015), and find that 25.", "startOffset": 124, "endOffset": 148}, {"referenceID": 0, "context": "We build a new NMT system by integrating an adaptive attention model into the NMT system described in (Bahdanau et al., 2015).", "startOffset": 102, "endOffset": 125}, {"referenceID": 15, "context": "the best of our knowledge, the adaptive attention method discussed here has not been used before for NMT, although the problem we intend to attack is not new (Tu et al., 2016a).", "startOffset": 158, "endOffset": 176}, {"referenceID": 14, "context": "We start by describing an NMT model, which builds on the base of an RNN encoder-decoder framework (Sutskever et al., 2014) and attention mechanisms.", "startOffset": 98, "endOffset": 122}, {"referenceID": 1, "context": "The activation function f(\u00b7) can be a vanilla RNN (Boden, 2002) or sophisticated units such as Gated Recurrent Unit (GRU) (Cho et al.", "startOffset": 50, "endOffset": 63}, {"referenceID": 3, "context": "The activation function f(\u00b7) can be a vanilla RNN (Boden, 2002) or sophisticated units such as Gated Recurrent Unit (GRU) (Cho et al., 2014) and Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 122, "endOffset": 140}, {"referenceID": 5, "context": ", 2014) and Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 42, "endOffset": 76}, {"referenceID": 12, "context": "where hj = [ \u2212\u2192 h j ; \u2190\u2212 h j ] T represents the annotation vector of the source word xj generated by a bi-directional RNN (Schuster and Paliwal, 1997), and the weight \u03b1i,j is calculated as follows:", "startOffset": 122, "endOffset": 150}, {"referenceID": 9, "context": "Here ei,j measures the similarity between the target word yi and the source word xj , which can be calculated with diverse methods (Luong et al., 2015b).", "startOffset": 131, "endOffset": 152}, {"referenceID": 10, "context": "Motivated from the work in (Merity et al., 2016; Lu et al., 2016), we introduce the concept of attention sentinel, which is a latent representation of what a decoder already knows.", "startOffset": 27, "endOffset": 65}, {"referenceID": 7, "context": "Motivated from the work in (Merity et al., 2016; Lu et al., 2016), we introduce the concept of attention sentinel, which is a latent representation of what a decoder already knows.", "startOffset": 27, "endOffset": 65}, {"referenceID": 7, "context": "The attention sentinel for LSTM-RNN can be defined in a similar way; Readers interested can refer to (Lu et al., 2016) for a detailed description.", "startOffset": 101, "endOffset": 118}, {"referenceID": 11, "context": "We used the case-insensitive 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric.", "startOffset": 52, "endOffset": 75}, {"referenceID": 0, "context": "All the other settings are the same as in (Bahdanau et al., 2015).", "startOffset": 42, "endOffset": 65}, {"referenceID": 4, "context": "\u2022 cdec (Dyer et al., 2010): an open-source hierarchical phrase-based translation system (Chiang, 2007) with default configuration and 4-gram language model trained on the target sentences of training data.", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": ", 2010): an open-source hierarchical phrase-based translation system (Chiang, 2007) with default configuration and 4-gram language model trained on the target sentences of training data.", "startOffset": 69, "endOffset": 83}, {"referenceID": 0, "context": "\u2022 RNNSearch: a re-implementation of the attention-based neural machine translation system (Bahdanau et al., 2015) with slight changes from dl2mt tutorial.", "startOffset": 90, "endOffset": 113}, {"referenceID": 13, "context": "5 RNNSearch uses GRU as the activation function of an RNN and incorporates dropout (Srivastava et al., 2012) on the output layer.", "startOffset": 83, "endOffset": 108}, {"referenceID": 18, "context": "We use AdaDelta (Zeiler, 2012) to optimize model parameters.", "startOffset": 16, "endOffset": 30}, {"referenceID": 7, "context": "(Lu et al., 2016) adopt LSTM.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "Bahdanau et al. (2015) propose a model to jointly align and translate words.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bahdanau et al. (2015) propose a model to jointly align and translate words. Luong et al. (2015b) propose and compare diverse attention models.", "startOffset": 0, "endOffset": 98}, {"referenceID": 0, "context": "Bahdanau et al. (2015) propose a model to jointly align and translate words. Luong et al. (2015b) propose and compare diverse attention models. Tu et al. (2016b) propose to extend attention models with a coverage vector in order to attack the problem of under-translation and over-translation.", "startOffset": 0, "endOffset": 162}, {"referenceID": 0, "context": "Bahdanau et al. (2015) propose a model to jointly align and translate words. Luong et al. (2015b) propose and compare diverse attention models. Tu et al. (2016b) propose to extend attention models with a coverage vector in order to attack the problem of under-translation and over-translation. All the previous attention models work well, but they cannot tell when not to attend to source sentences. Our work is inspired by Lu et al. (2016), which propose an adaptive attention model for the task of image captioning.", "startOffset": 0, "endOffset": 441}, {"referenceID": 0, "context": "Bahdanau et al. (2015) propose a model to jointly align and translate words. Luong et al. (2015b) propose and compare diverse attention models. Tu et al. (2016b) propose to extend attention models with a coverage vector in order to attack the problem of under-translation and over-translation. All the previous attention models work well, but they cannot tell when not to attend to source sentences. Our work is inspired by Lu et al. (2016), which propose an adaptive attention model for the task of image captioning. The main difference is that they build their adaptive attention model on the base of a spatial attention model, which is different from conventional attention models for NMT. Moreover, our adaptive attention model uses GRU as the RNN activation function while Lu et al. (Lu et al., 2016) adopt LSTM. Regarding the literature of NMT, the most related work is Tu et al. (2016a), which utilize a context gate to trade off the source-side and target-side context.", "startOffset": 0, "endOffset": 894}], "year": 2017, "abstractText": "In the past few years, attention mechanisms have become an indispensable component of end-to-end neural machine translation models. However, previous attention models always refer to some source words when predicting a target word, which contradicts with the fact that some target words have no corresponding source words. Motivated by this observation, we propose a novel attention model that has the capability of determining when a decoder should attend to source words and when it should not. Experimental results on NIST Chinese-English translation tasks show that the new model achieves an improvement of 0.8 BLEU score over a state-of-the-art baseline.", "creator": "LaTeX with hyperref package"}}}