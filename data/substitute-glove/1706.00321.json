{"id": "1706.00321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Using of heterogeneous corpora for training of an ASR system", "abstract": "The packaging 120-page the development country seen LVCSR internal built changed a and one the Pashto discussion - biographical essentially evening made SCALE (Summer Camp for Applied Language Exploration) '98 preparation weeks \" Speech - put - book - translation for current - sufficient distinguishes \". The Pashto language had take this into i \" posts \" significantly - depends shorthand, reproductions connections extremes a make the echoed - recognition and example criticism - soon - manuscripts - introduction transmission stability put.", "histories": [["v1", "Thu, 1 Jun 2017 14:30:19 GMT  (35kb,D)", "http://arxiv.org/abs/1706.00321v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan trmal", "gaurav kumar", "vimal manohar", "sanjeev khudanpur", "matt post", "paul mcnamee"], "accepted": false, "id": "1706.00321"}, "pdf": {"name": "1706.00321.pdf", "metadata": {"source": "CRF", "title": "USING OF HETEROGENEOUS CORPORA FOR TRAINING OF AN ASR SYSTEM", "authors": ["Jan Trmal", "Gaurav Kumar", "Vimal Manohar", "Sanjeev Khudanpur", "Matt Post", "Paul McNamee"], "emails": [], "sections": [{"heading": null, "text": "Even when the amount of data is seemingly sufficient, given the fact that the data originates from multiple sources, the preliminary experiments reveal that there is little to no benefit in merging (concatenating) the corpora and more elaborate ways of making use of all of the data must be worked out.\nThis paper concentrates only on the LVCSR part and presents a range of different techniques that were found to be useful in order to benefit from multiple different corpora\nIndex Terms\u2014 speech translation; pashto; babel; multiple corpora; neural networks; discriminative training;"}, {"heading": "1. INTRODUCTION", "text": "Pashto belongs to the southeastern Iranian branch of IndoIranian languages. It has three main variants: Northern and Central (both spoken mainly in Pakistan) and Southern (spoken mainly in Afghanistan). Each of these variants has a number of dialectal varieties. It is estimated that Pashto has 66 million speakers across the world[1]. While written Pashto has existed since the 16th century, standardization of the writing system is still in progress. There are a substantial number of words that have more than one publicly accepted way of being written (cf. English adviser vs. advisor). Other issues include present/missing spacing after certain characters (especially those, which belong to non-connecting arabic characters, and frequent substitution of visually similar (and similar sounding) characters. The last problem is emphasized by the fact that for writing Pashto, several different keyboard layouts are used \u201cin the wild\u201d. There is, of course the official Pashto keyboard\nThe work was supported by the NSF CRI Grant No 1513128, by the DARPA LORELEI Contract No HR0011-15-2-0024 and by IARPA BABEL Contract No 2012-12050800010. \u201dThe U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation hereon.\nlayout although the Arabic and Urdu layouts are used as well. The alternative layouts have a majority of the Pashto characters (and people freely substitute those which are missing with visually similar characters). Also, different fonts can have small deficiencies in rendering of glyphs, especially during kerning or joining of characters and the users often try to fix this by substituting a different character that looks better (i.e. closer to the expected shape) in the given context.\nA direct impact of this is that a word as a sequence of glyphs (visual representations of characters) can be represented as multiple sequences of unicode codepoints (numerical codes of the characters). One example was the pair of unicode codepoints ARABIC LETTER FARSI YEH and ARABIC LETTER ALEF MAKSURA (see Figure 1) whose glyphs are rendered with no visible differences, despite the fact that the codepoints are different.\nThis also causes problems when looking only for visual differences (for example, during debugging of problems with the lexicon). We mention this somewhat anecdotal evidence to show the readers who are only familiar with languages which use romanized scripts, that there are a number of hidden peculiarities that are really surprising when met for the first time."}, {"heading": "2. SPEECH CORPORA AVAILABLE", "text": "In this section, we give a short overview of the different corpora we worked with. We acquired two corpora before the start of the workshop and one additional during the course of the workshop. As the latter is substantially different and not completely suitable for our task (LVCSR and machine translation), we did not achieve any gain from using it and we are reporting the numbers here just for the sake of completeness."}, {"heading": "2.1. Lila consortium Appen Pashto (A)", "text": "The Appen Pashto (dataset \u201cA\u201d) contains approximately 90 hours of conversational telephone (8 kHz) speech. As the\nar X\niv :1\n70 6.\n00 32\n1v 1\n[ cs\n.C L\n] 1\nJ un\n2 01\n7\ntrain/dev partitions were not externally defined in the Appen data release, we partitioned the data into 85 hours for training and 5 hours as dev sets1. The lexicon was included with the corpus and the lexical entries included vowelized representations and the romanized forms of words. Moreover, the lexicon contained four dialectal variants of pronunciation of each word (we assume that these were generated automatically)."}, {"heading": "2.2. IARPA Babel Pashto (B)", "text": "Babel Pashto (dataset \u201cB\u201d) is the Full Language Pack2 in Babel program terminology. Simply explained, it is a dataset of 80 hours of 8 kHz sampled telephone speech data and the associated lexicon and transcripts. We used the development set defined in the corpus. Although our description of the corpora \u201cA\u201d and \u201cB\u201d may indicate that the datasets are similar, our observations demonstrated that this was not the case. We observed that dataset \u201cB\u201d was, on overall, prepared and transcribed more carefully than dataset \u201cA\u201d."}, {"heading": "2.3. TransTac Pashto (T)", "text": "The TransTac Pashto is significantly different when compared with the two aforementioned corpora. By nature, it is more scripted speech, albeit with high level of spontaneity. It is also professionally recorded (in a recording room) and hence we had to down-sample it to 8 kHz for use with corpora \u201cA\u201d and \u201cB\u201d. The dataset was created as follows. The participating individuals were given one out of a limited set of scenarios and they were then asked to re-enact that scenario. We were unable to obtain the definitions of the train/dev partitions for this dataset that were used in the DARPA TransTac ([2]) project. Because of this and the fact that we used the down-sampled speech, our performance is not directly comparable to the previously reported results for this dataset. We started work on this corpus relatively late in the course of the workshop and hence a majority of the reported experiments done on the \u201cmerged\u201d training data only used the A+B training set. Also, our primary aim was not to achieve the best result on this corpus but to establish whether this dataset could be used in the 8 kHz telephone speech scenario."}, {"heading": "2.4. Corpus preparation", "text": "As mentioned in the introduction, there is significant variability in the process used to transcribe corpora \u201cA\u201d and \u201cB\u201d. Our first efforts targeted at making the transcriptions from both corpora more consistent in their use of characters. This was motivated by our inspection of the lexicons from the corpora. Starting with the lexicons and then removing the vowel marks from all words, the overlap (defined as the number of words the lexicons share) was only 15 %. In an attempt to determine the minimal number of character changes that would increase lexicon overlap, we developed a simple algorithm that allowed a language expert to determine the edit rules. 1. Take 1000 most frequent words from each lexicon (based\non the associated transcripts). We chose this number so as to cover over 90 % of the frequency mass in each corpus.\n1These splits are part of the Kaldi recipe 2release-IARPA-babel104b-v04.bY\n2. Find the best word pairs in terms of the least character distance, one word from each lexicon, based on (a) Character Edit Distance (b) Phone Edit Distance: We used the Festvox toolkit3\nto generate the phone sequence for each word in the lexicon.\n3. Generate count based statistics for character based substitution and deletion rules based on the previous step. 4. Get an expert to verify the most frequent rules. 5. Use the rules to modify the lexicons and transcripts. 6. Go to Step 1.\nTo elaborate, step 2 (the edit character computations) can provide highly informative observations \u2013 for each edit operation, we keep track of the global effect of applying such rules. We choose the minimal set of rules that would lead to maximal corpus overlap. These most frequent operations can provide insights to discover character-substitution or deletion rules. See Table 1 for an example of these automatically discovered (and expert-confirmed) rules. After about four iteration of this process, we increased the overlap of the most frequent 1000 words across corpora \u201cA\u201d and \u201cB\u201d to approximately 70 %. After the fourth iteration, we didn\u2019t find any additional systematic differences, so we stopped there."}, {"heading": "3. BASELINE ACOUSTIC SYSTEM", "text": "In this section, we provide a high level description of how we trained the baseline system. As the objective was to develop a sufficiently simple, single pass, minimum delay system, we opted for training a deep neural network system (or more precisely, a TDNN, as described in [3]). We used the Kaldi\n3http://festvox.org\ntoolkit [4] for training the ASR system. There are a couple of details worth mentioning which follow."}, {"heading": "3.1. Speed perturbation", "text": "We utilized augmentation of the data via speed perturbation (as described in [5]) during training. We used sox4 to obtain two copies of the training data (the first copy used a speed factor 1.1 and the second 0.9). Our experience confirms this improves overall robustness of the resulting models. The network was trained in a parallel fashion using model averaging, as described in [6]."}, {"heading": "3.2. Estimation of pronunciation probabilities", "text": "The pronunciation lexicon rarely contains the probabilities of the individual pronunciation variants. It is however possible to estimate these probabilities from the alignments of the training data. Moreover, it is possible to model word-dependent silence probabilities, in addition to modeling of the probability of silence to estimate (and suitably smooth) the probability of each word appearing after silence. See [7] for a detailed analysis of this and several related ideas.\nIn our experience, it is also beneficial to re-estimate these probabilities iteratively several times during the training process. We saw reasonable gains (given the fact that this phase itself is not computationally expensive) from using the probabilistic pronunciation lexicon even during training."}, {"heading": "3.3. Sequence discriminative training", "text": "For the sequence training we used the sMBR method [8, 9] which is reported [10] to give best performance (measured with respect to WER). Also, we found it beneficial to adjust the prior probabilities \u2013 used during decoding for converting TDNN posterior probabilites into likelihoods - after finishing the discriminative training.\nHistorically, the priors are computed from alignment. As mentioned in [11], marginalizing of the DNN posteriors over all acoustic vectors gives better performance, especially when the data is noisy. Only a limited subset of the full training data is usually needed, so again, this improvement comes relatively cheap."}, {"heading": "3.4. Duration model rescoring", "text": "After the final lattices were generated, we used the duration modeling rescoring as described in [12]. We used the software the author of the paper provided.5 We find the improvements fairly consistent, albeit lower, than the numbers reported in the original paper."}, {"heading": "3.5. Overall performance of the baseline system", "text": "The overall performance of the resulting baseline system is reported in Table 2. Please note that a separate baseline system\n4http://sox.sourceforge.net/ 5https://github.com/alumae/kaldi-nnet-dur-model\nis trained for each test set \u2013 Appen, Babel and TransTac \u2013 using only speech data from the corresponding training set."}, {"heading": "4. RELATED WORK", "text": "This comparison is not completely straightforward. There was a substantial amount of work reported as a part of the Babel Pashto project, but the reported numbers are generally from a combination of multiple (sometimes of huge number of) systems. As our team participated (as a part of team Radical) in this project, we are presenting a comparison of our current system with respect to our best-performing Babel system (hybrid DNN system) from two years ago (see Table 3). The comparison has to be made carefully, as we (in order to unify the corpora at hand) applied the rule set mentioned in section 2.4. From Table 3 it can be seen that the new training procedure (Scale B-train only without normalization) gives us \u223c6% absolute gain over the older system and the normalization rules provide another \u223c3% gain. Note, as an aside, that simply adding the Appen data to the Babel trainng set degrades WER by about 1.2 %."}, {"heading": "5. JOINT MULTI-CORPUS TRAINING", "text": "During the course of the workshop, it became apparent that the three corpora actually do not combine well. The corpora \u201cA\u201d and \u201cB\u201d are closest, but even their combination for training did not produce better results \u2013 see Table 4.\nAnother piece of evidence can be gathered from Table 5. The language model created from the training data of the \u201cT\u201d dataset was not useful for the language model interpolation.\nAs the diversity of the data proved to be too high to allow for the training of a single model on all of the data that would function well, we decided to train data-set specific models, i.e. train three models, each of which would be specialized to that given dataset. Moreover, we tried to find out if there was a way to benefit from the fact that we had multiple (similar) corpora. The method we used to exploit this fact was the sharing (i.e.\ntraining jointly) of the hidden layers and only having the last and the first layers be dataset-specific. The reason for doing this was two-fold. First, it allowed us to train a larger neural network with potentially better performance. Secondly, the shared layers would hopefully learn more general/robust hyperplane separations. See Figure 2 for an illustration of this method.\nWe experimented with different sharing strategies. The best performance was obtained when the first and the last layer were shared. Sharing less or more layers (than the first and last one) has made the performance worse.\nThe training procedure was similar to the training of a single network. First, in each step, for each dataset, a new, updated network was obtained using model averaging. After that, these corpus-specific networks were averaged and the shared layers of this final network were copied back to the corpus-specific network. This represents one iteration of the joint-multi-corpora training.\nUsing the described approach, we were able to train a 7-layer p-norm network with a hidden layer dimension of 4500 and p-norm pooling of 1:10. The best network trainable using only the native data was a 7-layer p-norm network with a hidden layer dimension of 2500 and p-norm pooling of 1:10."}, {"heading": "6. MAIN RESULTS", "text": "A key lesson from Table 6 is that relative to single corpus training, straightforward pooling of multiple corpora (A and B) to train a single acoustic model results in significant degradation of WER on 2 out of 3 test sets (Babel and TransTac). Only the Appen test set benefits from training on the Babel data.\nThe main new result, by contrast, is that training separate TDNN acoustic models for each corpus while sharing the internal layers \u2013 an idea akin to the training of multilingual acoustic models \u2013 followed by some LM and duration model optimization results in significant improvements in WER.\nThe Appen WER reduces from 51.9 % to 50.4 % (3 % relative), while the Babel WER reduces from 46.8 % to 44.3 % (5 % relative).\nThe TransTac corpus is too different from the Appen and Babel corpora to see any benefit from data pooling. However, it is also remarkable that acoustic models trained on only the Appen and Babel data with the new method attain the same performance as models trained exclusively on TransTac data: 24.8 %. By contrast, acoustic models trained on Appen and Babel using traditional data pooling degrade WER to 53 %. This illustrates the cross-corpus robustness of the new method."}, {"heading": "7. CONCLUSION", "text": "This paper presents an overview of Pashto low-resource ASR system built during the SCALE\u201915 workshop. Initially, we developed a single pass LVCSR system for Pashto language. This system, trained using a corpora obtained by concatenating two different corpora (called \u201cA\u201d and \u201cB\u201d in this paper) outperformed significantly the performance of systems developed as a part of the Babel program. Our single-pass system achieved comparable, if not better, performance with respect to very complex systems (multiple passes + combination of multiple systems).\nWhile achieving as good WER as possible was important, the body of the work done during the duration of the workshop and hence described in the paper concentrates on providing insight on how to combine training data for multiple different sources. Pashto is a good proxy to demonstrate various issues which can be seen \u201cin the wild\u201d. While the aspects of having different audio channels (i.e. sampling frequencies, additive line noise, room impulse response) are generally fully appreciated, the issues stemming from an inner complexities of the language are largely overlooked. On Pashto, we are demonstrating some issues native to languages with not mature-enough computerimplemented writing system together with our take on how to deal with them in order to obtain data homogeneous enough to be useful for ASR system training."}, {"heading": "8. REFERENCES", "text": "[1] M. Paul Lewis, Gary F. Simons, and Charles D. Fenning, Eds., Ethnologue: Languages of the World, SIL International, Dallas, Texas, eighteenth edition, 2015.\n[2] Gregory A. Sanders, Brian A. Weiss, Craig Schlenoff, Michelle P. Steves, and Sherri Condon, \u201cEvaluation methodology and metrics employed to assess the TransTac two-way, speech-to-speech translation systems,\u201d Computer Speech & Language, vol. 27, no. 2, pp. 528 \u2013 553, 2013, Special Issue on Speech-speech translation.\n[3] Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, \u201cA time delay neural network architecture for efficient modeling of long temporal contexts,\u201d in Proceedings of Interspeech. 2015, ISCA.\n[4] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely, \u201cThe Kaldi speech recognition toolkit,\u201d in IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. Dec. 2011, IEEE Signal Processing Society.\n[5] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, \u201cAudio augmentation for speech recognition,\u201d in Proceedings of Interspeech. 2015, ISCA.\n[6] Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur, \u201cParallel training of deep neural networks with natural gradient and parameter averaging,\u201d CoRR, vol. abs/1410.7455, 2014.\n[7] Guoguo Chen, Hainan Xu, Minhua Wu, Daniel Povey, and Sanjeev Khudanpur, \u201cPronunciation and silence probability modeling for ASR,\u201d in Proceedings of Interspeech. 2015, ISCA.\n[8] Janez Kaiser, Bogomir Horvat, and Zdravko Kacic, \u201cA novel loss function for the overall risk criterion based discriminative training of HMM models,\u201d in Sixth International Conference on Spoken Language Processing. 2000, ISCA.\n[9] Matthew Gibson and Thomas Hain, \u201cHypothesis spaces for minimum Bayes risk training in large vocabulary speech recognition.,\u201d in Proceedings of Interspeech. 2006, ISCA.\n[10] Karel Vesely\u0301, Arnab Ghoshal, Luka\u0301s\u030c Burget, and Daniel Povey, \u201cSequence-discriminative training of deep neural networks.,\u201d in Proceedings of Interspeech. 2013, pp. 2345\u20132349, ISCA.\n[11] Vimal Manohar, Daniel Povey, and Sanjeev Khudanpur, \u201cSemi-supervised maximum mutual information training of deep neural network acoustic models,\u201d in Proceedings of Interspeech. 2015, ISCA.\n[12] Tanel Aluma\u0308e, \u201cNeural network phone duration model for speech recognition,\u201d in Proceedings of Interspeech. 2014, ISCA."}], "references": [{"title": "Evaluation methodology and metrics employed to assess the TransTac two-way, speech-to-speech translation systems", "author": ["Gregory A. Sanders", "Brian A. Weiss", "Craig Schlenoff", "Michelle P. Steves", "Sherri Condon"], "venue": "Computer Speech & Language, vol. 27, no. 2, pp. 528 \u2013 553, 2013, Special Issue on Speech-speech translation.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "A time delay neural network architecture for efficient modeling of long temporal contexts", "author": ["Vijayaditya Peddinti", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "Proceedings of Interspeech. 2015, ISCA.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "The Kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. Dec. 2011, IEEE Signal Processing Society.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio augmentation for speech recognition", "author": ["Tom Ko", "Vijayaditya Peddinti", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "Proceedings of Interspeech. 2015, ISCA.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["Daniel Povey", "Xiaohui Zhang", "Sanjeev Khudanpur"], "venue": "CoRR, vol. abs/1410.7455, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Pronunciation and silence probability modeling for ASR", "author": ["Guoguo Chen", "Hainan Xu", "Minhua Wu", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "Proceedings of Interspeech. 2015, ISCA.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "A novel loss function for the overall risk criterion based discriminative training of HMM models", "author": ["Janez Kaiser", "Bogomir Horvat", "Zdravko Kacic"], "venue": "Sixth International Conference on Spoken Language Processing. 2000, ISCA.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Hypothesis spaces for minimum Bayes risk training in large vocabulary speech recognition", "author": ["Matthew Gibson", "Thomas Hain"], "venue": "Proceedings of Interspeech. 2006, ISCA.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["Karel Vesel\u00fd", "Arnab Ghoshal", "Luk\u00e1\u0161 Burget", "Daniel Povey"], "venue": "Proceedings of Interspeech. 2013, pp. 2345\u20132349, ISCA.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-supervised maximum mutual information training of deep neural network acoustic models", "author": ["Vimal Manohar", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "Proceedings of Interspeech. 2015, ISCA.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network phone duration model for speech recognition", "author": ["Tanel Alum\u00e4e"], "venue": "Proceedings of Interspeech. 2014, ISCA.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "We were unable to obtain the definitions of the train/dev partitions for this dataset that were used in the DARPA TransTac ([2]) project.", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "As the objective was to develop a sufficiently simple, single pass, minimum delay system, we opted for training a deep neural network system (or more precisely, a TDNN, as described in [3]).", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "toolkit [4] for training the ASR system.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "We utilized augmentation of the data via speed perturbation (as described in [5]) during training.", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "The network was trained in a parallel fashion using model averaging, as described in [6].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "See [7] for a detailed analysis of this and several related ideas.", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "For the sequence training we used the sMBR method [8, 9] which is reported [10] to give best performance (measured with respect to WER).", "startOffset": 50, "endOffset": 56}, {"referenceID": 7, "context": "For the sequence training we used the sMBR method [8, 9] which is reported [10] to give best performance (measured with respect to WER).", "startOffset": 50, "endOffset": 56}, {"referenceID": 8, "context": "For the sequence training we used the sMBR method [8, 9] which is reported [10] to give best performance (measured with respect to WER).", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "As mentioned in [11], marginalizing of the DNN posteriors over all acoustic vectors gives better performance, especially when the data is noisy.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "After the final lattices were generated, we used the duration modeling rescoring as described in [12].", "startOffset": 97, "endOffset": 101}], "year": 2017, "abstractText": "The paper summarizes the development of the LVCSR system built as a part of the Pashto speech-translation system at the SCALE (Summer Camp for Applied Language Exploration) 2015 workshop on \u201cSpeech-to-text-translation for low-resource languages\u201d. The Pashto language was chosen as a good \u201cproxy\u201d low-resource language, exhibiting multiple phenomena which make the speech-recognition and and speech-to-text-translation systems development hard. Even when the amount of data is seemingly sufficient, given the fact that the data originates from multiple sources, the preliminary experiments reveal that there is little to no benefit in merging (concatenating) the corpora and more elaborate ways of making use of all of the data must be worked out. This paper concentrates only on the LVCSR part and presents a range of different techniques that were found to be useful in order to benefit from multiple different corpora", "creator": "LaTeX with hyperref package"}}}