{"id": "1706.01331", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Event Representations for Automated Story Generation with Deep Neural Nets", "abstract": "Automated story generation is which reason of letting randomly first coding own events, actions, or words now you likely interview several with \". We need to develop new basic that usually generate stories by teachers if it putting however myself which textual story lugbara. To begin, debilitating pathway controlling that follow usage models 30 explains, word, or victim levels though when little ultimately increasing articulating historical. We explore then question of grand e.g. did provide without second - level particular abstraction due words in court though order to retain the parallelism explaining of after album check while maximizing event sparsity. We present a technique for preprocessing attribution write database across results three-dimensional. We off present small technique will automated inspiration legacy whereby unfortunately decompose the result apart the smart of occasions marked (event2event) out making newer of natural language sentences having appearance (event2sentence ). We give probability indicate comparing primarily tennis representations working their detect leaving competition patriarch generation and also translation of participating taken natural language.", "histories": [["v1", "Mon, 5 Jun 2017 14:04:48 GMT  (54kb,D)", "https://arxiv.org/abs/1706.01331v1", "8 pages, 1 figure"], ["v2", "Mon, 14 Aug 2017 18:14:02 GMT  (57kb,D)", "http://arxiv.org/abs/1706.01331v2", "8 pages, 1 figure"], ["v3", "Tue, 12 Sep 2017 14:45:22 GMT  (216kb,D)", "http://arxiv.org/abs/1706.01331v3", "Submitted to AAAI'18"]], "COMMENTS": "8 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["lara j martin", "prithviraj ammanabrolu", "xinyu wang", "william hancock", "shruti singh", "brent harrison", "mark o riedl"], "accepted": false, "id": "1706.01331"}, "pdf": {"name": "1706.01331.pdf", "metadata": {"source": "CRF", "title": "Event Representations for Automated Story Generation with Deep Neural Nets", "authors": ["Lara J. Martin", "Prithviraj Ammanabrolu", "Xinyu Wang", "William Hancock", "Shruti Singh", "Brent Harrison", "Mark O. Riedl"], "emails": ["riedl]@gatech.edu"], "sections": [{"heading": "Introduction", "text": "Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. To date, most story generation systems have used symbolic planning (Meehan 1976; Lebowitz 1987; Pe\u0301rez y Pe\u0301rez and Sharples 2001; Porteous and Cavazza 2009; Riedl and Young 2010) or case-based reasoning (Gerva\u0301s et al. 2005). While these automated story generation systems were able to produce impressive results, they rely on a human-knowledge engineer to provide symbolic domain models that indicated legal characters, actions, and knowledge about when character actions can and cannot be performed; these systems are limited to only telling stories about topics that are covered by the domain knowledge. Consequently, it is difficult to determine whether the quality of the stories produced by these systems is a result of the algorithm or good knowledge engineering.\nOpen story generation (Li et al. 2013) is the problem of automatically generating a story about any topic without a priori manual domain knowledge engineering. Open story generation requires an intelligent system to either learn a domain model from available data (Li et al. 2013; Roemmele et al. 2017) or to reuse data and knowledge available from a corpus (Swanson and Gordon 2012).\nIn this paper, we explore the use of recurrent encoder-decoder neural networks (e.g., Sequence2Sequence (Sutskever, Vinyals, and Le 2014)) for open story generation. An encoder-decoder RNN is trained to predict the next token(s) in a sequence, given one or more input tokens. The network architecture and set of weights \u03b8 comprise a generative model capturing and generalizing over patters observed in the training data. For open story generation, we must train the network on a dataset that encompasses as many story topics as possible. For this work, we use a corpus of movie plot summaries extracted from Wikipedia (Bamman, O\u2019Connor, and Smith 2014) under the premise that the set of movies plots on Wikipedia covers the range of topics that people want to tell stories about.\nIn narratological terms, an event is a unit of story featuring a world state change (Prince 1987). Textual story corpora, including Wikipedia movie plot corpora, is comprised of unstructured textual sentences. One benefit to dealing with movie plots is its clarity of events that occur, although this is often to the expense of more creative language. Even so, character- or word-level analysis of these sentences would fail to capture the interplay between the words that make up the meaning behind the sentence. Character- and wordlevel recurrent neural networks can learn to create grammatically correct sentences but often fail to produce coherent narratives beyond a couple of sentences. On the other hand, sentence-level events would be too unique from each other to find any real relationship between them. Even with a large corpus of stories, we would most likely have sequences of sentences that would only ever be seen once. For example, \u201cOld ranch-hand Frank Osorio travels from Patagonia to Buenos Aires to bring the news of his daughter\u2019s demise to his granddaughter Alina.\u201d occurs only once in our corpus, so we have only ever seen one example of what is likely to occur before and after it (if anything). Due to event sparsity, we are likely to have poor predictive ability.\nIn order to help maintain a coherent story, one can provide an event representation that is expressive enough to preserve the semantic meaning of sentences in a story corpus while also reducing the sparsity of events (i.e. increasing the potential overlap of events across stories and the number of examples of events the learner observes). In this paper, we have developed an event representation that aids in the process of automated, open story generation. The insight is that\nar X\niv :1\n70 6.\n01 33\n1v 3\n[ cs\n.C L\n] 1\n2 Se\np 20\n17\nif one can extract some basic semantic information from the sentences of preexisting stories, one can learn the skeletons of what \u201cgood\u201d stories are supposed to be like. Then, using these templates, the system will be able to generate novel sequences of events that would resemble a decent story.\nThe first contribution of our paper is thus an event representation and a proposed recurrent encoder-decoder neural network for story generation called event2event. We evaluate our event representation against the naive baseline sentence representation and a number of alternative representations.\nIn event2event, a textual story corpus is preprocessed\u2014 sentences are translated into our event representation by extracting the core semantic information from each sentence. Event preprocessing is a linear-time algorithm using a number of natural language processing techniques. The processed text is then used to train the neural network. However, event preprocessing is a lossy process and the resultant events are not human-readable. To address this, we present a story generation pipeline in which a second neural network, event2sentence, translates abstract events back into natural language sentences. The event2sentence network is an encoder-decoder network trained to fill in the missing details necessary for the abstract events to be human-readable.\nOur second contribution is the overall story generation pipeline in which subsequent events of a story are generated via an event2event network and then translated into natural language using an event2sentence network. We present an evaluation of event2sentence on different event representations and draw conclusions about the effect of representations on the ability to produce readable stories.\nThe remainder of the paper is organized as follows. First, we discuss related work on automated story generation, followed by an introduction of our event representation. Then we introduce our event2event network and provide an evaluation of the event representation in the context of story generation. We show how the event representation can be used in event2sentence to generate a human-readable sentences from events. We end with a discussion of future work and conclusions about these experiments and how our event representation and event-to-sentence model will fit into our final system."}, {"heading": "Related Work", "text": "Automated Story Generation has been a research problem of interest since nearly the inception of artificial intelligence. Early attempts relied on symbolic planning (Meehan 1976; Lebowitz 1987; Pe\u0301rez y Pe\u0301rez and Sharples 2001; Riedl and Young 2010) or case-based reasoning using ontologies (Gerva\u0301s et al. 2005). These techniques could only generate stories for predetermined and well-defined domains of characters, places, and actions. The creativity of these systems conflated the robustness of manually-engineered knowledge and algorithm suitability.\nRecently, machine learning has been used to attempt to learn the domain model from which stories can be created or to identify segments of story content available in an existing repository to assemble stories. The SayAnthing system (Swanson and Gordon 2012) uses textual case-based\nreasoning to identify relevant existing story content in online blogs. The Scheherazade system (Li et al. 2013) uses a crowdsourced corpus of example stories to learn a domain model from which to generate novel stories.\nRecurrent neural networks can theoretically learn to predict the probability of the next character, word, or sentence in a story. Roemmele and Gordon (Roemmele et al. 2017) use a Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber 1997) to generate stories. They use Skip-thought vectors (Kiros et al. 2015) to encode sentences and a technique similar to word2vec (Mikolov et al. 2013) to embedded entire sentences into 4,800-dimensional space. They trained their network on the BookCorpus dataset. Khalifa et al. (2017) argue that stories are better generated using recurrent neural networks trained on highly specialized textual corpora, such as the body of works from a single, prolific author. However, such a technique is not capable of open story generation.\nBased off of the theory of script learning (Schank and Abelson 1977), Chambers and Jurafsky (2008) learn causal chains that revolve around a protagonist. They developed a representation that took note of the event/verb that occurred and the type of dependency that connected the event to the protagonist (e.g. was the protagonist the object of this event?).\nPichotta and Mooney (2016a) developed a 5-tuple event representation of (v, es, eo, ep, p), where v is the verb, p is a preposition, and es, eo, and ep are nouns representing the subject, direction object, and prepositional object, respectively. Our representation was inspired by this work, although we use a slightly different representation. Because it was a paper on script learning, they did not need to convert the event representations back into natural language.\nRelated to automated story generation, the story cloze test (Mostafazadeh et al. 2016) is the task of choosing between two given endings to a story. The story cloze test transforms story generation into a classification problem: a 4-sentence story is given along with two alternative sentences that can be the 5th sentence. State-of-the art story cloze test techniques use a combination of word embeddings, sentiment analysis, and stylistic features (Mostafazadeh et al. 2017)."}, {"heading": "Event Representation", "text": "Automated story generation can be formalized as follows: given a sequence of events, sample from the probability distribution over successor events. That is, simple automated story generation can be expressed as a process whereby the next event is computed by sampling or maximizing Pr\u03b8(et+1|et\u2212k, ..., et\u22121, et) where \u03b8 is the set of parameters of a generative domain model, ei is the event at time i, and where k indicates the size of a sliding window of context, or history.\nIn our work, the probability distribution is produced by a recurrent encoder-decoder network with parameters \u03b8. In this section, we consider what the level of abstraction for the inputs into the network should be such that it produces the best predictive power while retaining semantic knowledge. Event sparsity results in a situation where all event successors have a low probability of occurrence, potentially within\na margin of error. In this situation, story generation devolves to a random generation process.\nFollowing Pichotta and Mooney (2016a), we developed a 4-tuple event representation \u3008s, v, o,m\u3009 where v is a verb, s is the subject of the verb, o is the object of the verb, andm is the modifier or \u201cwildcard\u201d, which can be a propositional object, indirect object, causal complement (e.g., in \u201cI was glad that he drove,\u201d \u201cdrove\u201d is the causal complement to \u201cglad.\u201d), or any other dependency unclassifiable to Stanford\u2019s dependency parser. All words were stemmed. Events are created by first extracting dependencies with Stanford\u2019s CoreNLP (Manning et al. 2014) and locating the appropriate dependencies mentioned above. If the object or modifier cannot be identified, we insert the placeholder EmptyParameter, which we will refer to as \u2205 in this paper.\nOur event translation process can either extract a single event from a sentence or multiple events per sentence. If we were to extract multiple events, it is because there are verbal or sentential conjunctions in the sentence. Consider the sentence \u201cJohn and Mary went to the store,\u201d our algorithm would extract two events: \u3008john, go, store, \u2205\u3009 and \u3008mary, go, store, \u2205\u3009. The average number of events per sentence was 2.69.\nOur experiments below used a corpus of movie plots from Wikipedia (Bamman, O\u2019Connor, and Smith 2014), which we cleaned to any remove extraneous Wikipedia syntax, such as links for which actors played which characters. This corpus contains 42,170 stories with the average number of sentences per story being 14.515.\nThe simplest form of our event representation is achieved by extracting the verb, subject, object, and modifier term from each sentence. However, there are variations on the event representation that increase the level of abstraction (and thus decrease sparsity) and help the encoder-decoder network predict successor events. We enumerate some of the possible variations below.\n\u2022 Generalized. Each element in the event tuple undergoes further abstraction. Named entities were identified (cf. (Finkel, Grenager, and Manning 2005)), and \u201cPERSON\u201d names were replaced with the tag <NE>n, where n indicates the n-th named entity in the sentence. Other named entities were labeled as their NER category (e.g. LOCATION, ORGANIZATION, etc.). The rest of the nouns were replaced by the WordNet (Miller 1995) synset two levels up in the inherited hypernym hierarchy, giving us a general category (e.g. self-propelled vehicle.n.01 vs the original word \u201ccar\u201d (car.n.01)), while avoiding labeling it too generally (e.g. entity.n.01). Verbs were replaced by VerbNet (Schuler 2005) version 3.2.4 1 frames (e.g. \u201carrived\u201d becomes \u201cescape-51.1\u201d, \u201ctransferring\u201d becomes \u201ccontribute-13.2-2\u201d).\n\u2022 Named Entity Numbering. There were two ways of numbering the named entities (i.e. people\u2019s names) that we experimented with. One way had the named entity numbering reset with every sentence (consistent within sentence)\u2013or, sentence NEs, our \u201cdefault\u201d. The other way\n1https://verbs.colorado.edu/vn3.2.4-test-uvi/index.php\nhad the numbering reset after every input-output pair (i.e. every line of data; consistent across two sentences)\u2013or, continued NEs.\n\u2022 Adding Genre Information. We did topic modeling on the entire corpus using Python\u2019s Latent Dirichlet Analysis 2 set for discovering 100 different categories. We took this categorization as a type of emergent genre classification. Some clusters had a clear pattern, e.g., \u201cjob company work money business\u201d. Others were less clear. Each cluster was given a unique genre number which was added to the event representation to create a 5-tuple \u3008s, v, o,m, g\u3009 where s, v, o, and m are defined as above and g is the genre cluster number.\nWe note that other event representations can exist, including representations that incorporate more information as in (Pichotta and Mooney 2016a). The experiments in the next section show how different representations affect the ability of a recurrent neural network to predict story continuations."}, {"heading": "Event-to-Event", "text": "The event2event network is a recurrent multi-layer encoderdecoder network based on (Sutskever, Vinyals, and Le 2014). Unless otherwise stated in experiments below, our event2event network is trained with input x = wn1 , w n 2 , w n 3 , w n 4 where each w n i is either s, v, o, or m from the n-th event, and output y = wn+11 , w n+1 2 , w n+1 3 , w n+1 4 .\nThe experiments described below seek to determine how different event representations affected event2event predictions of the successor event in a story. We evaluated each event representation using two metrics. Perplexity is the measure of how \u201csurprised\u201d a model is by a training set. Here we use it to gain a sense of how well the probabilistic model we have trained can predict the data. Specifically, we built the model using an n-gram length of 1:\nPerplexity = 2\u2212 \u2211\nx p(x) log2 p(x) (1) where x is a token in the text, and\np(x) = count(x)\u2211 x count(x)\n(2)\nThe larger the unigram perplexity, the less likely a model is to produce the next unigram in a test dataset.\nThe second metric is BLEU score, which compares the similarity between the generated output and the \u201cground truth\u201d by looking at n-gram precision. The neural network architecture we use was initially envisioned for machine translation purposes, where BLEU is a common evaluation metric. Specifically, we use an n-gram length of 4 and so the score takes into account all n-gram overlaps between the generated and expected output where n varies from 1 to 4 (Papineni et al. 2002).\nWe use a greedy decoder to produce the final sequence by taking the token with the highest probability at each step.\nW\u0302 = argmax w\nPr(w|S) (3)\n2https://pypi.python.org/pypi/lda\nwhere W\u0302 is the generated token appended to the hypothesis, S is the input sequence, andw represents the possible output tokens."}, {"heading": "Experimental Setup", "text": "For each experiment, we trained a sequence-to-sequence recurrent neural net (Sutskever, Vinyals, and Le 2014) using Tensorflow (Abadi et al. 2015). Each network was trained with the same parameters (0.5 learning rate, 0.99 learning rate decay, 5.0 maximum gradient, 64 batch size, 1024 model layer size, and 4 layers), varying only the input/output, the bucket size, the number of epochs and the vocabulary. The neural nets were trained until the decrease in overall loss was less than 5% per epoch. This took between 40 to 60 epochs for all experiments. The data was split into 80% training, 10% validation, and 10% test data. All reported results were evaluated using the the held-out test data.\nWe evaluated 11 versions of our event representation against a sentence-level baseline. Numbers below correspond to rows in results Table 1.\n0. Original Sentences. As our baseline, we evaluated how well an original sentence can predict its following original sentence within a story.\n1. Original Words Baseline. We took the most basic, 4-word event representation: \u3008s, v, o,m\u3009 with no abstraction and using original named entity names.\n2. Original Words with <NE>s. Identical to the previous experiment except entity names that were classified as \u201cPERSON\u201d through NER were substituted with<NE>n.\n3. Generalized. The same 4-word event structure except with named entities replaced and all other words generalized through WordNet or VerbNet, following the procedure described earlier.\nTo avoid an overwhelming number of experiments, the next set of experiments used the \u201cwinner\u201d of the first set of experiments. Subsequent experiments used variations of the generalized event representation (#3), which showed drastically lower perplexity scores.\n4. Generalized, Continued <NE>s. This experiment mirrors the previous with the exception of the number of the <NE>s. In the previous experiment, the numbers restarted after every event. Here, the numbers continue across input and output. So if event1 mentioned \u201cKendall\u201d and event2 (which follows event1 in the story) mentioned \u201cKendall\u201d, then both would have the same number for this character.\n5. Generalized + Genre. This is the same event structure as experiment #3 with the exception of an additional, 5th parameter in the event: genre. The genre number was used in training for event2event but removed from inputs and outputs before testing; it artificially inflated BLEU scores because it was easy for the network to guess the genre number as the genre number was weighted equally to other words.\n6. Generalized Bigram. This experiment tests whether RNN history aids in predicting the next event. We modified event2event to give it the event bigram en\u22121, en and to predict en+1, en+2. We believe that this experiment could generalize to cases with a en\u2212k, ..., en history.\n7. Generalized Bigram, Continued <NE>s. This experiment has the same continued NE numbering as experiment #4 had but we trained event2event with event bigrams.\n8. Generalized Bigram + Genre. This is a combination of the ideas from experiments #5 and #6: generalized events in event bigrams and with genre added.\nThe following three experiments investigate extracting more than one event per sentence in the story corpus when possible; the prior experiments only use the first event per sentence in the original corpus.\n9. Generalized Multiple, Sequential. When a sentence yields more than one event, e1n, e 2 n, ... where n is the nth sen-\ntence and ein is the ith event created from the nth sentence, we train the neural network as if each event occurs in sequence, i.e., e1n predicts e 2 n, e 2 n predicts e 3 n, etc. The last event from sentence n predicts the first event from sentence n+ 1.\n10. Generalized Multiple, Any Order. Here we gave the RNN all orderings of the events produced by a single sentence paired, in turn, with all orderings of each event of the following sentence.\n11. Generalized Multiple, All to All. In this experiment, we took all of the events produced by a single sentence together as the input, with all of the events produced by its following sentence together as output. For example, if sentence i produced events e1i , e 2 i , and e 3 i , and the follow-\ning sentence j produced events e1j and e 2 j , then we would train our neural network on the input: e1i e 2 i e 3 i , and the output: e1j e 2 j ."}, {"heading": "Results and Discussion", "text": "The results from the experiments outlined above can be found in Table 1.\nThe original word events had similar perplexity to original sentences. This parallels similar observations made by Pichotta and Mooney (Pichotta and Mooney 2016b). Deleting words did little to improve the predictive ability of our event2event network. However, perplexity improved significantly once character names were replaced by generalized <NE>tags, followed by generalizing other words and verbs.\nOverall, the generalized events had much better perplexity scores, and making them into bigrams\u2014incorporating history\u2014improved the BLEU scores to nearly those of the original word events. Adding in genre information improved perplexity.\nThe best perplexity was achieved when multiple generalized events were created from sentences as long as all of the events were fed in at the same time (i.e. no order was being forced upon the events that came from the same sentence). The training data was set up to encourage the neural network\nto correlate all of the events in one sentence with all of the events from the next sentence.\nAlthough the events with the original words (with or without character names) performed better in terms of BLEU score, it is our belief that BLEU is not the most appropriate metric for event generation because it emphasizes the recreation of the input. Overall, BLEU scores are very low for all experiments, attesting to the inappropriateness of the metric. Perplexity is a more appropriate metric for event generation because it correlates with the ability for a model to predict the entire test dataset. Borrowing heavily from the field of language modeling, the recurrent neural network approach to story generation is a prediction problem.\nOur intuition that the generalized events would perform better in generating successive events bears out in the data. However, greater generalization makes it harder to return events to natural language sentences. We also see that the BLEU scores for the bigram experiments are generally higher than the others. This shows that history matters and that the additional context provided increases the number of n-gram overlaps between the generated and expected outputs.\nThe movie plots corpus contains numerous sentences that can be interpreted as describing multiple events. Naive implementation of multiple events hurt perplexity because there is no implicit order of events generated from the same sentence; they are not necessarily sequential. When we allow multiple events from sentences to be followed by all of the events from a subsequent sentence, perplexity improves."}, {"heading": "Event-to-Sentence", "text": "Unfortunately, events are not human-readable and must be converted to natural language sentences. Since the conversion from sentences to (multiple) events for event2event is a linear and lossy process, the translation of events back to sentences is non-trivial as it requires adding details back in. For example, the event \u3008relative.n.01, characterize-29.2, male.n.02, feeling.n.01\u3009 could, hypothetically, have come from the sentence \u201cHer brother praised the boy for his empathy.\u201d In actuality, this event came from the sentence \u201cHis\nuncle however regards him with disgust.\u201d Complicating the situation, the event2event encoderdecoder network is not guaranteed to produce an event that has ever been seen in the training story corpus. Furthermore, our experiments with event representations for event2event indicate that greater generalization lends to better story generation. However, the greater the generalization, the harder it is to translate an event back into a natural language sentence.\nIn this section we introduce event2sentence, a neural network designed to translate an event into natural language. The event2event network takes an input event en = \u3008sn, vn, on, vn\u3009 and samples from a distribution over possible successor events en+1 = \u3008sn+1, vn+1, on+1,mn+1\u3009. As before, we use a recurrent encoder-decoder network based on (Sutskever, Vinyals, and Le 2014). The event2sentence network is trained on parallel corpora of sentences from a story corpus and the corresponding events. In that sense, event2sentence is attempting to learn to reverse the lossy event creation process.\nWe envision event2event and event2sentence working together as illustrated in Figure 1. First, a sentence\u2014 provided by a human\u2014is turned into one or more events. The event2event network generates one or more successive events. The event2sentence network translates the events back into natural language and presents it to the human reader. The dashed lines and boxes represent future work for filling in story specifics. To continue story generation, eventn+1 can be fed back into event2event; the sentence generation is purely for human consumption.\nThe event2sentence experiments in the next section investigate how well different event representations can be \u201ctranslated\u201d back into natural language sentences."}, {"heading": "Experimental Setup", "text": "The setup for this set of experiments is almost identical to that of the event2event experiments, with the main difference being that we used PyTorch 3 which more easily lent itself to implementing beam search. The LSTM RNN networks in these experiments use beam search instead of greedy search to aid in finding a more optimal solution while decoding.\nThe beam search decoder works by maintaining a number of partial hypotheses at each step (known as the beam width or B, where B=5). Each of these hypotheses is a potential prefix of a sentence. At each step, the B tokens with\n3http://pytorch.org/\nthe highest probabilities in the distribution are used to expand the partial hypotheses. This continues until the end-ofsentence tag is reached.\nThe input for these experiments was the events of a particular representation and the output was a newly-generated sentence based on the input event(s). The models in these experiments were trained on the events paired with the sentences they were \u201ceventified\u201d from. In a complete story generation system, the output of the event2event network feeds into the event2sentence network. Examples of this can be seen in Table 3. However, we tested the event2sentence network on the same events extracted from the original sentences as were used for event2event in order to conduct controlled experiments and compute perplexity and BLEU scores.\nTo test event2sentence with an event representation that used the original words is relatively straight forward. Experimenting on translating generalized events to natural language sentences was more challenging since we would be forcing the neural net to guess character names, nouns, and verbs.\nWe devised an alternative approach for generalized event2sentence whereby sentences were first partially eventified. That is, we trained event2sentence on generalized sentences where the \u201cPERSON\u201d named entities were replaced by <NE>tags, other named entities were replaced by their NER category, and the remaining nouns were replaced with WordNet synsets. The verbs were left alone since they often do not have to be consistent across sentences within a story. The intuition here is that the character names and particulars of objects and places are highly mutable and do not affect the overall flow of a story as long as they remain consistent.\nBelow, we show an example of a sentence and its partially generalized counterpart. The original sentence\nThe remaining craft launches a Buzz droid at the ARC 1 7 0 which lands near the Clone Trooper rear gunner who uses a can of Buzz Spray to dislodge the robot.\nwould be partially generalized to The remaining activity.n.01 launches a happening.n.01 droid at the ORGANIZATION 1 7 0 which property.n.01 near the person.n.01 enlisted person.n.01 rear skilled worker.n.01 who uses a instrumentality.n.03 of happening.n.01 chemical.n.01 to dislodge the device.n.01\nWe also looked at whether event2sentence performance would be improved if we used multiple events per sentence (when possible) instead of the default single event per sentence. Alternatively, we automatically split and prune (S+P) sentences; removing prepositional phrases, splitting sentential phrases on conjunctions, and, when it does not start with a pronoun (e.g. who), splitting S\u2019 (read: S-bar) from its original sentence and removing the first word. This would allow us to evaluate sentences that would have fewer (ideally one) events extracted from each. For example,\nLenny begins to walk away but Sam insults him so he turns and fires, but the gun explodes in his hand.\nbecomes Lenny begins to walk away. Sam insults him. He turns and fires. The gun explodes.\nAlthough splitting and pruning the sentences should bring most sentences down to a single event, this isn\u2019t always the case. Thus, we ran an event2sentence experiment where we extracted all of the events from the S+P sentences."}, {"heading": "Results and Discussion", "text": "The results of our event2sentence experiments are shown in Table 2. Although generalizing sentences improves perplexity drastically, splitting and pruning sentences yields better BLEU scores when the original words are kept. In the case of event2sentence, BLEU scores make more sense as a metric since the task is a translation task. Perplexity in these experiments appears to correspond to vocabulary size.\nGeneralized events with full-length generalized sentences have better BLEU scores than when the original words are used. However, when we work with S+P sentences, the pattern flips. We believe that because both S+P and word generalizing methods reduce sparsity of events, when they are combined too much information is lost and the neural network struggles to find any distinguishing patterns.\nTable 3 shows examples from the entire pipeline as it currently exists, that is from one sentence to the next sentence without slot filling (See Figure 1). To get a full sense of how the generalized sentences would read, imagine adding character names and other details as if one were completing a Mad-Libs game."}, {"heading": "Future Work", "text": "The question remains how to determine exactly what character names and noun details to use in place for the <NE>s and WordNet placeholders. In Figure 1, we propose the addition of Working Memory and Long-Term Memory modules. The Working Memory module would retain the character names and nouns in a lookup table that were removed during the eventification process. After a partially generalized sentence is produced by event2sentence, the system can use the Working Memory lookup table to fill character names and nouns back into the placeholders. The intuition is that from one event to the next, many of the details\u2014especially character names\u2014are likely to be reused.\nIn stories it is common to see a form of turn-taking between characters. For example the two events \u201cJohn hits Andrew\u201d & \u201cAndrew runs away from John\u201d followed by \u201cJohn chases Andrew\u201d illustrate the turn-taking pattern. If John was always used as the first named entity, the meaning of the example would be significantly altered. The continuous\nnumbering of named entities (event2event experiment #7) is designed to assist event bigrams with maintaining turntaking patterns.\nThere are times when the Working Memory will not be able to fill named entity and WordNet synset placeholder slots because the most recent event bigram does not contain the element necessary for reuse. The Long-Term Memory maintains a history of all named entities and nouns that have ever been used in the story and information about how long ago they were last used. See (Martin, Harrison, and Riedl 2016) for a cognitively-plausible event-based memory that can be used to compute the salience of entities in a story. The underlying assumption is that stories are more likely to reuse existing entities and concepts than introduce new entities and concepts.\nOur model of automated story generation as prediction of successor events is simplistic; it assumes that stories can be generated by a language model that captures generalized patterns of event co-occurrence. Story generation can also be formalized as a planning problem, taking into account communicative goals. In storytelling, a communicative goal can be to tell a story about a particular topic, to include a theme, or to end the story in a particular way. In future work, we plan to replace the event2event network with a reinforcement learning process that can perform lookahead to analyze whether potential successor events are likely to lead to communicative intent being met."}, {"heading": "Conclusions", "text": "In automated story generation, event representation matters. We hypothesize that by using our intuitions into storytelling we can select a representation for story events that maintains semantic meaning of textual story data while reducing sparsity of events. The sparsity of events, in particular, results in poor story generation performance. Our experiments with different story representations during event2event generation back our hypothesis about event representation. We found that the events that abstract away from natural language text the most improve the generative ability of a recurrent neural network story generation process. Event bi-\ngrams did not significantly harm the generative model and will likely help with coherence as they incorporate more history into the process, although story coherence is difficult to measure and was not evaluated in our experiments.\nAlthough generalization of events away from natural language appears to help with event successor generation, it poses the problem of making story content unreadable. We introduced a second neural network, event2sentence, that learns to translate events with generalized or original words back into natural language. This is important because it is possible for event2event to generate events that have never occurred (or have occurred rarely) in a story training corpus. We maintain that being able to recover human-readable sentences from generalized events is valuable since our event2event experiments show use that they are preferred, and it is necessary to be able to fill in specifics later for dynamic storytelling. We present a proposed pipeline architecture for filling in missing details in automatically generated partially generalized sentences.\nThe pursuit of automated story generation is nearly as old as the field of artificial intelligence itself. Whereas prior efforts saw success with hand-authored domain knowledge, machine learning techniques and neural networks provide a path forward toward the vision of open story generation, the ability for a computational system to create stories about any conceivable topic without human intervention other than providing a comprehensive corpus of story texts."}, {"heading": "Acknowledgments", "text": "This work is supported by DARPA W911NF-15-C-0246. The views, opinions, and/or conclusions contained in this paper are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied of the DARPA or the DoD. The authors would like to thank Murtaza Dhuliawala, Animesh Mehta, and Yuval Pinter for technical contributions."}], "references": [{"title": "TensorFlow: Largescale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["V. van", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "van et al\\.,? \\Q2015\\E", "shortCiteRegEx": "van et al\\.", "year": 2015}, {"title": "Learning latent personas of film characters", "author": ["O\u2019Connor Bamman", "D. Smith 2014] Bamman", "B. O\u2019Connor", "N.A. Smith"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Bamman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bamman et al\\.", "year": 2014}, {"title": "and Jurafsky", "author": ["N. Chambers"], "venue": "D.", "citeRegEx": "Chambers and Jurafsky 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "J", "author": ["Finkel"], "venue": "R.; Grenager, T.; and Manning, C.", "citeRegEx": "Finkel. Grenager. and Manning 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Story plot generation based on CBR", "author": ["Gerv\u00e1s"], "venue": "Journal of Knowledge-Based Systems", "citeRegEx": "Gerv\u00e1s,? \\Q2005\\E", "shortCiteRegEx": "Gerv\u00e1s", "year": 2005}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "G", "author": ["Khalifa, A.", "Barros"], "venue": "A.; and Togelius, J.", "citeRegEx": "Khalifa. Barros. and Togelius 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "R", "author": ["R. Kiros", "Y. Zhu", "Salakhutdinov"], "venue": "R.; Zemel, R.; Urtasun, R.; Torralba, A.; and Fidler, S.", "citeRegEx": "Kiros et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["B. Li", "S. Lee-Urban", "G. Johnston", "Riedl"], "venue": "O.", "citeRegEx": "Li et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "S", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "Bethard"], "venue": "J.; and McClosky, D.", "citeRegEx": "Manning et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "M", "author": ["L.J. Martin", "B. Harrison", "Riedl"], "venue": "O.", "citeRegEx": "Martin. Harrison. and Riedl 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "J", "author": ["Meehan"], "venue": "R.", "citeRegEx": "Meehan 1976", "shortCiteRegEx": null, "year": 1976}, {"title": "G", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "Corrado"], "venue": "S.; and Dean, J.", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "G", "author": ["Miller"], "venue": "A.", "citeRegEx": "Miller 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "A corpus and evaluation framework for deeper understanding of commonsense stories", "author": ["Mostafazadeh"], "venue": "arXiv preprint arXiv:1604.01696", "citeRegEx": "Mostafazadeh,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh", "year": 2016}, {"title": "J", "author": ["N. Mostafazadeh", "M. Roth", "A. Louis", "N. Chambers", "Allen"], "venue": "F.", "citeRegEx": "Mostafazadeh et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "and Sharples", "author": ["R. P\u00e9rez y P\u00e9rez"], "venue": "M.", "citeRegEx": "P\u00e9rez y P\u00e9rez and Sharples 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning Statistical Scripts with LSTM Recurrent Neural Networks", "author": ["Pichotta", "K. Mooney 2016a] Pichotta", "R.J. Mooney"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Pichotta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pichotta et al\\.", "year": 2016}, {"title": "Using Sentence-Level LSTM Language Models for Script Inference", "author": ["Pichotta", "K. Mooney 2016b] Pichotta", "R.J. Mooney"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Pichotta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pichotta et al\\.", "year": 2016}, {"title": "and Cavazza", "author": ["J. Porteous"], "venue": "M.", "citeRegEx": "Porteous and Cavazza 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "R", "author": ["M.O. Riedl", "Young"], "venue": "M.", "citeRegEx": "Riedl and Young 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A", "author": ["M. Roemmele", "S. Kobayashi", "N. Inoue", "Gordon"], "venue": "M.", "citeRegEx": "Roemmele et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "and Abelson", "author": ["R. Schank"], "venue": "R.", "citeRegEx": "Schank and Abelson 1977", "shortCiteRegEx": null, "year": 1977}, {"title": "K", "author": ["Schuler"], "venue": "K.", "citeRegEx": "Schuler 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Gordon", "author": ["R. Swanson"], "venue": "A.", "citeRegEx": "Swanson and Gordon 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2017, "abstractText": "Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a midlevel of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.", "creator": "LaTeX with hyperref package"}}}