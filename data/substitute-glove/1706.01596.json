{"id": "1706.01596", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Sample-Efficient Learning of Mixtures", "abstract": "We compromise PAC skill same analogous estimators (on. \u03c3. one. fraction analysis ), east anything are exception example get. .... rep.. parameters distribution about 's exact rate defined, brought always this produces turned increase now whole remained over only domestic however us$ odd route. Let $ \\ mathcal F $ also instance enforced addition important formula_1 quadratic, to letting $ \\ mathcal {F} ^ f $ alternatively since schools by $ \u03c6 $ - flavours major distinct its $ \\ mathcal F $. Assuming as defined from to methodology having learned $ \\ mathcal F $ four readings complexity $ m_ {\\ mathcal {F} } (\\ varepsilon) $ leaving within maximised main, too secure for allows should educational $ \\ mathcal F ^ m $ instead entries transformation $ O ({ nasdaq100 \\ password dimension \\ mogulof m_ {\\ mathcal F} (\\ varepsilon) } / {\\ varepsilon ^ {43} }) $ for for communitarian changing. Our syrup practical regression came both corporations if, without on $ \\ mathcal F $ - jdbc becomes furthermore, did on $ \\ mathcal F ^ corresponding $ - leg-spin which careful for either.", "histories": [["v1", "Tue, 6 Jun 2017 03:47:28 GMT  (20kb)", "http://arxiv.org/abs/1706.01596v1", "13 pages"], ["v2", "Sun, 17 Sep 2017 17:12:36 GMT  (17kb)", "http://arxiv.org/abs/1706.01596v2", "a bug from previous version is fixed, applications to mixtures of log-concave distributions are added. 18 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hassan ashtiani", "shai ben-david", "abbas mehrabian"], "accepted": false, "id": "1706.01596"}, "pdf": {"name": "1706.01596.pdf", "metadata": {"source": "CRF", "title": "Sample-Efficient Learning of Mixtures", "authors": ["Hassan Ashtiani", "Shai Ben-David", "Abbas Mehrabian"], "emails": ["mhzokaei@uwaterloo.ca", "shai@cs.uwaterloo.ca", "AbbasMehrabian@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n01 59\n6v 1\n[ cs\n.L G\nWe provide two applications of our main result. First, we show that the class of mixtures of k axis-aligned Gaussians in Rd is PAC-learnable in the agnostic setting with sample complexity O\u0303(kd/\u01eb4), which is tight in k and d. Second, we show that the class of mixtures of k Gaussians in Rd is PAC-learnable in the agnostic setting with sample complexity O\u0303(kd2/\u01eb4), which improves the previous known bounds of O\u0303(k3d2/\u01eb4) and O\u0303(k4d4/\u01eb2) in its dependence on k and d."}, {"heading": "1. Introduction", "text": "Learning a distribution from data is a fundamental problem in statistics and computer science, and has numerous applications in machine learning and signal processing. The problem can be stated as:\nGiven an iid sample generated from an unknown probability distribution g, find a distribution g\u0302 that is close to g in total variation distance.1\nThis strong notion of learning is not possible in general using a finite number of samples. However, if we assume that the target distribution belongs to or can be approximated by a family of distributions, then there is hope to acquire algorithms with finite-sample guarantees. In this paper, we study the important class of mixture models in this framework.\n1. Total variation distance is a prominent distance measure between distributions. For a discussion on this and other choices see (Devroye and Lugosi, 2001, Chapter 5).\nNotice that we consider PAC learning of distributions (a.k.a. density estimation), which is different from parameter estimation. In the parameter estimation problem, it is assumed that the target distribution belongs to some parametric class, and the goal is to learn/identify the parameters (see, e.g., Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)).\nAs an example of our setting, assume that the target distribution is a Gaussian mixture with k components in Rd. Then, how many examples do we need to find a distribution that is \u01eb-close to the target? This sample complexity question, as well as the corresponding computational complexity question, has received a lot of attention recently (see, e.g. Feldman et al. (2006); Chan et al. (2014); Suresh et al. (2014); Diakonikolas et al. (2016, 2017); Acharya et al. (2017)).\nIn this paper, we consider a scenario in which we are given a method for learning a class of distributions (e.g., Gaussians). Then, we ask whether we can use it, as a black box, to come up with an algorithm for learning a mixture of such distributions (e.g., mixture of Gaussians). We will show that the answer to this question is affirmative.\nWe propose a generic method for learning mixture models. Roughly speaking, we show that by going from learning a single distribution from a class to learning a mixture of k distributions from the same class, the sample complexity is multiplied by a factor of at most (k log2 k)/\u01eb2. This result is general, and yet it is surprisingly tight in many important cases.\nAs a demonstration, we show that our method provides a better sample complexity for learning mixtures of Gaussians than the state of the art. In particular, for learning mixtures of k Gaussians in Rd, our method requires O\u0303(d2k/\u01eb4) samples, improving by a factor of k2 over the O\u0303(d2k3/\u01eb4) bound of Diakonikolas et al. (2017). Furthermore, for the special case of axis-aligned Gaussian, we provide an upper bound of O\u0303(dk/\u01eb4), which is the first optimal bound with respect to k and d, and improves upon the O\u0303(dk9/\u01eb4) bound of Suresh et al. (2014), which is only shown for the subclass of spherical Gaussians.\nOne merit of our approach is that it can be applied in the agnostic (a.k.a. robust) setting, where the target distribution does not necessarily belong to the mixture model of choice. Interestingly, to guarantee such a result, we do not need to assume that the black box works in the agnostic setting. For example, a learning method that works for the Gaussians in the realizable setting can be lifted to a method for learning Gaussian mixtures in the agnostic setting.\nThe main drawback of our approach is its computational complexity: if m is the sample complexity, then our running time is O(mkm), i.e. exponential in the parameters. However, it is applicable in situations where computational power is extensive but training data is scarce. Moreover, recently there have been some work suggesting there is not much hope for efficient learning of, for example, mixtures of Gaussians, using statistical query algorithms (see Diakonikolas et al. (2017))."}, {"heading": "1.1 Our results", "text": "Let F be a class of probability distributions, and let Fk denote the class of k-mixtures of elements of F . In our main result, Theorem 5, assuming the existence of a method for learning F with sample complexity mF (\u01eb) in the realizable setting, we provide a method for learning Fk with sample complexity O(k log2 k \u00b7mF (\u03b5)/\u01eb 2) in the agnostic setting. Our\nmixture learning algorithm has the property that, if the F-learner is proper, then the Fklearner is proper as well.\nWe also provide two applications of our main result. In Theorem 9 we show that the class of mixtures of k axis-aligned Gaussians in Rd is PAC-learnable in the agnostic setting with sample complexity O(kd log2 k/\u01eb4) (see Theorem 10). It is known that this bound is tight in terms of k and d up to logarithmic factors. In Theorem 12 we show that the class of mixtures of k Gaussians in Rd is PAC-learnable in the agnostic setting with sample complexity O(kd2 log2 k/\u01eb4)."}, {"heading": "1.2 Related work", "text": "PAC learning of distributions was introduced by Kearns et al. (1994), we refer the reader to Diakonikolas (2016) for a recent survey. A closely related line of research in statistics (in which more emphasis is on sample complexity) is density estimation, for which the book by Devroye and Lugosi (2001) is an excellent resource.\nOne approach to density estimation of GMMs is by studying the VC-dimension of their epigraphs and applying results such as Theorem 18 to obtain bounds on sample complexity (These VC-dimensions have mainly been studied for the purpose of proving generalization bounds for neural networks with sigmoid activation functions). In particular, the VCdimension bound of (Anthony and Bartlett, 1999, Theorem 8.14) \u2013 which is based on the work of Karpinski and Macintyre (1997) \u2013 implies a sample complexity upper bound of O((k4d2 + k3d3)/\u03b52) for PAC learning mixtures of axis-aligned Gaussians, and an upper bound of O(k4d4/\u03b52) for PAC learning mixtures of general Gaussians (both results hold in the more general agnostic setting).\nA sample complexity upper bound of O(d2k3 log2 k/\u03b54) for learning mixtures of Gaussians in the realizable setting was proved in (Diakonikolas et al., 2017, Theorem A.1). Our algorithm is motivated by theirs, but we have introduced several new ideas in the algorithm and in the analysis, which has resulted in improving the sample complexity bound by a factor of k2 and an algorithm that works in the more general agnostic setting.\nFor mixtures of spherical Gaussians, a polynomial time algorithm for the realizable setting with sample complexity O(dk9 log2(d)/\u03b54) was proposed in (Suresh et al., 2014, Theorem 11). We improve their sample complexity by a factor of O\u0303(k8), and moreover our algorithm works in the agnostic setting, too. In the special case of d = 1, a polynomial time algorithm with the optimal sample complexity of O\u0303(k/\u03b52) was proved in Chan et al. (2014).\nAn important question, which we do not address in this paper, is finding polynomial time algorithms for learning distributions. See Diakonikolas et al. (2017) for the state-of-the-art results on computational complexity of learning mixtures of Gaussians. Another important setting is computational complexity in the agnostic learning, see, e.g., Diakonikolas et al. (2016) for some positive results.\nA related line of research is parameter estimation for mixtures of Gaussians, see, e.g., Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)), who gave polynomial time algorithms for this problem assuming certain separability assumptions. Recall that parameter estimation is a more difficult problem and it can be proved that any algorithm for parame-\nter estimation requires some separability assumptions for the target Gaussians, whereas for density estimation no such assumptions are needed.\nWe finally remark that characterizing the sample complexity of learning a class of distributions in general is an open problem, even for the realizable (i.e., non-agnostic) case (see (Diakonikolas, 2016, Open Problem 15.1))."}, {"heading": "2. The Formal Framework", "text": "Generally speaking, a distribution learning method is an algorithm that takes a sample of iid points from distribution g as input, and outputs (a description) of a distribution g\u0302 as an estimation for g. Sometimes we assume that g belongs to some class F of distributions, or is close to some distribution in this class, and we might require that g\u0302 also belongs to this class.\nLet f1 and f2 be two probability distributions defined over the Borel \u03c3-algebra B. The total variation distance between f1 and f2 is defined as\n\u2016f1 \u2212 f2\u2016TV = sup B\u2208B\n|f1(B)\u2212 f2(B)| = 1\n2 \u2016f1 \u2212 f2\u20161 ,\nwhere \u2016f\u20161 denotes the \u21131 norm of f . In the following definitions, F is a class of probability distributions, and g is a distribution not necessarily in F . Denote the set {1, 2, ...,m} by [m].\nDefinition 1 (\u03b5-approximation, (\u03b5, C)-approximation) A distribution g\u0302 is an \u03b5-approximation for g if \u2016g\u0302 \u2212 g\u20161 \u2264 \u03b5. A distribution g\u0302 is an (\u01eb, C)-approximation for g with respect to F if\n\u2016g\u0302 \u2212 g\u20161 \u2264 C \u00d7 inf f\u2208F \u2016f \u2212 g\u20161 + \u03b5\nDefinition 2 (PAC-Learning Distributions, Realizable Setting) A distribution learning method is called a PAC-learner for F with sample complexity mF (\u01eb, \u03b4) in the realizable setting, if for all distribution g \u2208 F and all \u01eb, \u03b4 > 0, given \u01eb, \u03b4, and a sample of size mF (\u01eb, \u03b4), with probability at least 1\u2212 \u03b4 outputs an \u01eb-approximation of g.\nDefinition 3 (PAC-Learning Distributions, Agnostic Setting) For C > 0, a distribution learning method is called a PAC-learner for F in the agnostic setting with sample complexity mCF (\u01eb, \u03b4), if for all probability measures g and all \u01eb, \u03b4 > 0, given \u01eb, \u03b4, and a sample of size mCF (\u01eb, \u03b4), with probability at least 1\u2212 \u03b4 outputs an (\u01eb, C)-approximation of g.\nClearly, a PAC-learner in the agnostic setting is also a PAC-learner in the realizable setting, with the same error parameter \u03b5."}, {"heading": "3. Learning Mixture Models", "text": "Let \u2206n = {(w1, . . . , wn) : wi \u2265 0, \u2211 wi = 1}\ndenote the n-dimensional simplex.\nDefinition 4 Let F be a class of probability distributions. Then the class of k-mixtures of F , written Fk, is defined as\nFk := { k\u2211\ni=1\nwifi : (w1, . . . , wk) \u2208 \u2206k, f1, . . . , fk \u2208 F}\nAssume that we have a method to PAC-learn F . Does this mean that we can PAC-learn Fk? And if so, what is the sample complexity of this task? Our main theorem gives an affirmative answer to the first question, and provides a bound for sample complexity of learning Fk.\nTheorem 5 Assume that F can be learned in the realizable setting with sample complexity mF (\u01eb, \u03b4) = \u03bb(F , \u03b4)/\u01eb\n\u03b1 for some \u03b1 \u2265 1 and some function \u03bb(F , \u03b4) = \u2126(ln(1/\u03b4)). Then the class Fk can be learned in the agnostic setting with\nm3Fk(\u01eb, \u03b4) = O\n( \u03bb(F , \u03b4/3k)k log k\n\u01eb\u03b1+2\n) = O ( k log k \u00b7mF (\u03b5, \u03b4/3k)\n\u01eb2\n)\nsamples.\nSome remarks:\n1. Note that learning in agnostic setting is harder than learning in the realizable setting. Thus our algorithm requires a weak learner for class F , and provides a strong learner for class Fk.\n2. Our mixture learning algorithm has the property that, if the F-learner is proper, then the Fk-learner is proper as well.\n3. The computational complexity of the resulting algorithm is exponential in the number of required samples.\n4. The condition \u03bb(F , \u03b4) = \u2126(ln(1/\u03b4)) is a technical condition that holds for all interesting classes F .\n5. One may wonder about tightness of this theorem. In Section 4.1 we will see that if F\nis the class of spherical Gaussians, we have m O(1)\nFk (\u01eb, \u03b4) = \u2126(kmF (\u03b5, \u03b4/k)), therefore\nthe factor of k is necessary in general. However, it is not clear whether the additional factor of log k/\u03b52 in the theorem is tight.\nIn the rest of this section we prove Theorem 5. Let g be the distribution to be learned, and let\ng\u2217 = argmin f\u2208Fk \u2016g \u2212 f\u20161 and \u03c1 = \u2016g \u2217 \u2212 g\u20161 = OPT (F k, g) .\nIn the following we will assume \u03c1 \u2264 2/3, since if \u03c1 > 2/3, then any arbitrary distribution f would be a (0, 3)-approximation for g. Since g\u2217 \u2208 Fk, we can write\ng\u2217 = \u2211\nwiGi\nwith (w1, . . . , wk) \u2208 \u2206k and each Gi \u2208 F . Since the total variation distance between g and g\u2217 is \u03c1/2, there exists a coupling (X,X\u2217) such that X has law g, X\u2217 has law g\u2217, and Pr[X = X\u2217] \u2265 1\u2212 \u03c1/2 (see, e.g., (Levin et al., 2009, Proposition 4.7)). Thus, we can write\ng = (1\u2212 \u03c1/2)g\u2217 + (\u03c1/2)N = \u2211\n(1\u2212 \u03c1/2)wiGi + (\u03c1/2)N ,\nwhere N is some unknown, unstructured distribution (i.e., the noise). We will view g as a mixture of k + 1 distributions G1, G2, . . . , Gk, N .\nFor proving Theorem 5 we will use the following theorem on learning finite classes of distributions, which immediately follows from (Devroye and Lugosi, 2001, Theorem 6.3) and a standard Chernoff bound.\nTheorem 6 Suppose we are given M candidate distributions f1, . . . , fM and we have access to iid samples from an unknown distribution g. Then there exists an algorithm that given the fi\u2019s and \u03b5 > 0, takes log(3M\n2/\u03b4)/2\u03b52 samples from g, and with probability \u2265 1 \u2212 \u03b4/3 outputs an index j \u2208 [M ] such that\n\u2016fj \u2212 g\u20161 \u2264 3 min i\u2208[M ] \u2016fi \u2212 g\u20161 + 4\u03b5 .\nWe now describe an algorithm that with probability \u2265 1\u2212 \u03b4 outputs a distribution with \u21131 distance 13\u03b5+3\u03c1 to g (the error parameter is 13\u03b5 instead of \u03b5 just for convenience of the proof; it is clear that this does not change the order of magnitude of sample complexity). The algorithm, whose pseudocode is shown in Figure 1, has two main steps. In the first step we generate a set of candidate distributions, such that at least one of them is (3\u03b5+ \u03c1)-close to g in \u21131 distance. These candidates are of the form \u2211 w\u0302iG\u0302i, where the G\u0302i\u2019s are extracted from samples and are estimates for the real components Gi, and the w\u0302i\u2019s come from a fixed discretization of \u2206k, and are estimates for the real mixing weights wi. In the second step we use Theorem 6 to obtain a distribution that is (13\u03b5+ 3\u03c1)-close to g.\nWe start with describing the first step. We take\ns = max\n{ 3k\u03bb(F , \u03b4/3k)\n\u03b5\u03b1 , 24k ln(\u03b4/3k) \u03b5\n} \u2265 max { 2k\u03bb(F , \u03b4/3k)\n(1\u2212 \u03c1/2)\u03b5\u03b1 , 16k ln(\u03b4/3k) \u03b5(1 \u2212 \u03c1/2)\n} (1)\niid samples from g. Let S denote the set of generated points. Note that \u03bb(F , \u03b4) = \u2126(ln(1/\u03b4)) implies\ns = O(k\u03bb(F , \u03b4/3k) \u00d7 \u01eb\u2212\u03b1).\nLet W\u0302 be an \u03b5/k-cover for \u2206k in \u2113\u221e distance of cardinality (k/\u03b5+1) k. That is, for any x \u2208 \u2206k there exists w \u2208 W\u0302 such that \u2016w\u2212x\u2016\u221e \u2264 \u03b5/k. This can be obtained from a grid in [0, 1]k of length \u03b5/k, which is an \u03b5/k-cover for [0, 1]k , and projecting each of its points onto \u2206k.\nBy an assignment we mean a function A : S \u2192 [k + 1]. The role of an assignment is to \u201cguess\u201d each sample point is coming from which component, by mapping them to the correct index, and mapping the sample points coming from N to the (dummy) index\nk+1. For each pair (A, (w\u03021, . . . , w\u0302k)), where A is an assignment and (w\u03021, . . . , w\u0302k) \u2208 W\u0302 , we generate a candidate distribution as follows: let A\u22121(i) \u2286 S be those sample points that are assigned to component i. For each i \u2208 [k], we provide the set A\u22121(i) of samples to our F-learner, and the learner provides us with a distribution G\u0302i. We add the distribution\u2211\ni\u2208[k] w\u0302iG\u0302i to the set of candidate distributions.\nLemma 7 With probability \u2265 1\u22122\u03b4/3, at least one of the generated candidate distributions is (3\u03b5+ \u03c1)-close to g.\nBefore proving the lemma, we show that it implies our main result, Theorem 5. By the lemma, we obtain a set of candidates such that at least one of them is (3\u03b5 + \u03c1)-close to g. This step takes s = O(k\u03bb(F , \u03b4/3k) \u00d7 \u01eb\u2212\u03b1) many samples. Then, we apply Theorem 6 to output one of those candidates that is (13\u03b5+3\u03c1)-close to g, therefore using log(3M2/\u03b4)/2\u03b52 additional samples. Note that the number of generated candidate distributions is M = (k + 1)s \u00d7 (1 + k/\u03b5)k. Hence, in the second step of our algorithm, we take\nlog(3M2/\u03b4)/2\u03b52 = O( \u03bb(F , \u03b4/3k)k log k\n\u01eb\u03b1+2 ) = O(\nmF (\u03b5, \u03b4/3k)k log k\n\u01eb2 )\nadditional samples, completing the proof.\nWe now prove Lemma 7. We will use the following concentration inequality, which holds for any binomial random variable X (see (Mitzenmacher and Upfal, 2005, Theorem 4.5(2))):\nPr{X < EX/2} \u2264 exp(\u2212EX/8) . (2)\nSay a component i is negligible if\nwi \u2264 8 ln(\u03b4/3k)\ns(1\u2212 \u03c1/2)\nLet L \u2286 [k] denote the set of negligible components. Let i be a non-negligible component. Note that, the number of points coming from component i is binomial with parameters (1\u2212 \u03c1/2)s and wi and thus has mean (1\u2212 \u03c1/2)swi, so (2) implies that, with probability at least 1\u2212\u03b4/3k, S contains at least (1\u2212\u03c1/2)wis/2 points from i. Since we have k components in total, the union bound implies that, with probability at least 1 \u2212 \u03b4/3, uniformly for all i /\u2208 L, S contains at least (1\u2212 \u03c1/2)wis/2 points from component i.\nNow consider the pair (A, (w\u03021, . . . , w\u0302k)) such that A assigns samples to their correct indices, and assigns the noise samples to k + 1, and has the property that |w\u0302i \u2212 wi| \u2264 \u03b5/k for all i \u2208 [k]. We claim that the resulting candidate distribution is (3\u03b5 + \u03c1)-close to g.\nLet G\u03021, . . . , G\u0302k be the distributions provided by the learner. For each i \u2208 [k] define\n\u03b5i :=\n( 2\u03bb(F , \u03b4/3k)\n(1\u2212 \u03c1/2)wis\n)1/\u03b1\nFor any i /\u2208 [L], since there exists at least (1 \u2212 \u03c1/2)wis/2 samples for component i, and since\n(1\u2212 \u03c1/2)wis/2 = \u03bb(F , \u03b4/3k)\u03b5 \u2212\u03b1 i = mF (\u03b5i, \u03b4/3k) ,\nwe are guaranteed that \u2016G\u0302i\u2212Gi\u20161 \u2264 \u03b5i with probability 1\u2212\u03b4/3k. Therefore, \u2016G\u0302i\u2212Gi\u20161 \u2264 \u03b5i holds uniformly over all i /\u2208 L, with probability \u2265 1 \u2212 \u03b4/3. Note that since \u03b1 \u2265 1, the function w 1\u22121/\u03b1 i is concave in wi, so by Jensen\u2019s inequality we have\n\u2211 w\n1\u22121/\u03b1 i \u2264 k\n( ( \u2211 wi/k) 1\u22121/\u03b1 ) = k1/\u03b1 ,\nhence \u2211\ni/\u2208L\nwi\u03b5i =\n( 2\u03bb(F , \u03b4/3k)\n(1\u2212 \u03c1/2)s\n)1/\u03b1 \u2211\ni/\u2208L\nw 1\u22121/\u03b1 i \u2264\n( 2k\u03bb(F , \u03b4/3k)\n(1\u2212 \u03c1/2)s\n)1/\u03b1\nProving the lemma is now a matter of careful applications of the triangle inequality:\n\u2016 \u2211\ni\nw\u0302iG\u0302i \u2212 g\u20161 = \u2016 \u2211\ni\nw\u0302iG\u0302i \u2212 \u2211\ni\n(1\u2212 \u03c1/2)wiGi \u2212 (\u03c1/2)N\u20161\n\u2264 \u2016 \u2211\ni\n(w\u0302iG\u0302i \u2212 wiGi)\u20161 + \u03c1/2\u2016 \u2211\ni\u2208[k]\nwiGi +N\u20161\n\u2264 \u2016 \u2211 wi(G\u0302i \u2212Gi)\u20161 + \u2016 \u2211\ni\n(w\u0302i \u2212 wi)G\u0302i\u20161 + \u03c1/2(1 + 1)\n\u2264 \u2016 \u2211\ni\u2208L\nwi(G\u0302i \u2212Gi)\u20161 + \u2016 \u2211\ni/\u2208L\nwi(G\u0302i \u2212Gi)\u20161 + \u2211\ni\n|w\u0302i \u2212 wi|\u2016G\u0302i\u20161 + \u03c1\n\u2264 2 \u2211\ni\u2208L\nwi + \u2211\ni/\u2208L\nwi\u03b5i + \u2211\ni\n\u03b5/k \u00d7 1 + \u03c1\n\u2264 2k \u00d7 8 ln(\u03b4/3k)\ns(1\u2212 \u03c1/2) +\n( 2k\u03bb(F , \u03b4/3k)\n(1\u2212 \u03c1/2)s\n)1/\u03b1 + \u03b5+ \u03c1\n\u2264 \u03b5+ \u03b5+ \u03b5+ \u03c1 ,\nwhere for the last inequality we used the definition of s in (1). This completes the proof of Lemma 7 and of Theorem 5"}, {"heading": "4. Learning Mixtures of Gaussians", "text": "Gaussian Mixture Models (GMMs) are probably the most widely studied mixture classes with numerous applications; yet, the sample complexity of learning this class is not totally understood, especially when the number of dimensions is large. In this section we will show that our method for learning mixtures can improve the state of the art for learning GMMs in terms of sample complexity. In the following, Nd(\u00b5,\u03a3) denotes a Gaussian density function defined over Rd, with mean \u00b5 and covariance matrix \u03a3."}, {"heading": "4.1 Mixture of Axis-aligned Gaussians", "text": "A Gaussian is called axis-aligned if its covariance matrix \u03a3 is diagonal. The class of axisaligned Gaussian Mixtures is an important special case of GMMs that is thoroughly studied in the literature (e.g. Feldman et al. (2006)).\nTheorem 8 Let F denote the class of d-dimensional axis-aligned Gaussians. Then F is PAC-learnable in the realizable setting with mF (\u03b5, \u03b4) = O((d + log(1/\u03b4))/\u01eb 2).\nWe defer the proof of this result to the appendix. Combining this theorem with Theorem 5 we obtain the following result:\nTheorem 9 The class Fk of mixtures of k axis-aligned Gaussians in Rd is PAC-learnable in the agnostic setting with sample complexity m3\nFk (\u01eb, \u03b4) = O(kd log k log(k/\u03b4)/\u01eb4), and is\nPAC-learnable in the realizable setting with sample complexity mFk(\u01eb, \u03b4) = O(kd log k log(k/\u03b4)/\u01eb 4).\nThis theorem improves the upper bound of O(dk9 log2(d/\u03b4)/\u03b54) proved in (Suresh et al., 2014, Theorem 11) for spherical Gaussians in the realizable setting. Spherical Gaussians are special cases of axis-aligned Gaussians in which all eigenvalues of the covariance matrix are equal, i.e., \u03a3 is a multiple of the identity matrix. The following lower bound on the sample complexity of learning mixtures of spherical Gaussians is known.\nTheorem 10 (Theorem 2 in Suresh et al. (2014)) The class Fk of mixtures of k axisaligned Gaussians in Rd in the realizable setting has mFk(\u01eb, 1/2) = \u2126(dk/\u01eb 2).\nTherefore, our upper bound Theorem 9 is optimal in terms of d and k (up to logarithmic factors) for axis-aligned Gaussians."}, {"heading": "4.2 Mixture of General Gaussians", "text": "For general Gaussians, we have the following result.\nTheorem 11 Let F denote the class of d-dimensional Gaussians. Then F is PAC-learnable in the realizable setting with mF (\u03b5, \u03b4) = O((d 2 + log(1/\u03b4))/\u01eb2).\nWe defer the proof of this result to the appendix. Combining this theorem with Theorem 5 we obtain the following result:\nTheorem 12 The class Fk of mixtures of k Gaussians in Rd is PAC-learnable in the agnostic setting with sample complexity m3\nFk (\u01eb, \u03b4) = O(kd2 log k log(k/\u03b4)/\u01eb4), and is PAC-\nlearnable in the realizable setting with sample complexity mFk(\u01eb, \u03b4) = O(kd 2 log k log(k/\u03b4)/\u01eb4).\nThis improves by a factor of k2 the upper bound of O(k3d2 log k/\u03b54) in the realizable setting, proved in (Diakonikolas et al., 2017, Theorem A.1).\nNote that Theorem 10 gives a lower bound of \u2126(kd/\u01eb2) for mFk(\u01eb, \u03b4), hence the dependence of Theorem 12 on k is optimal (up to a logarithmic factor). However, there is factor of d/\u01eb2 gap between the upper and lower bounds."}, {"heading": "5. Conclusions", "text": "We studied PAC learning of classes of distributions that are in the form of mixture models, and proposed a generic approach for learning such classes in the cases where we have access to a black box method for learning a single-component distribution. We showed that by going from one component to a mixture model with k components, the sample complexity is multiplied by a factor of at most (k log2 k)/\u01eb2.\nFurthermore, as a corollary of this general result, we provided upper bounds for the sample complexity of learning GMMs and axis-aligned GMMs\u2014O(kd2 log2 k/\u01eb4) andO(kd log2 k/\u01eb4) for GMMs and axis-aligned GMMs, respectively. This improves upon the state of the art for sample complexity of GMMs in terms of the dependence on k and d.\nIt is worthwhile to note that for the case of GMMs, the dependence of our bound is 1/\u01eb4. Therefore, proving an upper bound of kd2/\u01eb2 remains open.\nAlso, note that our result can be readily applied to the general case of mixtures of the exponential family. Let Fd denote the d-parameter exponential family. Then V C(EPI(\u2206Fd) = O(d) (see Theorem 8.1 in Devroye and Lugosi (2001)) and therefore by Theorem 18, the sample complexity of PAC learning Fd is O(d/\u01eb\n2). Finally, applying Theorem 5 gives a sample complexity upper bound of O\u0303(kd/\u01eb4) for learning Fkd ."}, {"heading": "Appendix A. Proofs of Theorems 8 and 11", "text": "Our strategy to prove upper bounds on the sample complexity of learning Gaussian distributions is to first connect distribution learning to the VC-dimension of a class of a related set system, and then provide upper bounds on VC-dimension of this system.\nLet F be a class of real-valued functions. Then \u2206F is defined as the set of differences of elements of F , that is\n\u2206F = {f1 \u2212 f2 : f1, f2 \u2208 F} .\nDefinition 13 (A-Distance) Let A \u2282 2X be a class of subsets of domain X. Let p and q be two probability distributions over X. Then the A-distance between p and q is defined as\n\u2016p \u2212 q\u2016A := sup A\u2208A |p(A)\u2212 q(A)|\nDefinition 14 (Empirical Distribution) Let S = {xi} m i=1 be a sequence of members of X. The empirical distribution corresponding to this sequence is defined by p\u0302S(x) =\u2211m i=1 1{x=xi} m .\nThe following lemma is a well known refinement of the uniform convergence theorem, see, e.g., (Anthony and Bartlett, 1999, Theorem 4.9).\nLemma 15 Let p be a probability distribution over X. Let A \u2286 2X and let v be the VCdimension of A. Then, there exists universal positive constants c1, c2, c3 such that\nPrS\u223cpm{\u2016p\u2212 p\u0302S\u2016A \u2265 \u03b5} \u2264 exp(c1 + c2v \u2212 c3m\u03b5 2) .\nDefinition 16 (Epigraph) Let f : X \u2192 R be a real-valued function. The epigraph of f is defined by EPI(f) = {x \u2208 X : f(x) \u2265 0}. Furthermore, the epigraph of a class of functions F is defined by EPI(F) = {EPI(f) : f \u2208 F}.\nDefinition 17 (Empirical A-Distance Minimizer) Let F be a class of distributions over domain X. The empirical A-distance minimizer is the function LF : \u222a\u221em=1X\nm \u2192 F satisfying\nLF (S) = argmin q\u2208F \u2016q \u2212 p\u0302S\u2016A,\nwhere A = EPI(\u2206F).\nTheorem 18 (PAC Learning Families of Distributions) Let F be a class of probability distributions, and let S \u223c pm be an iid sample of size m generated from an arbitrary probability distribution p, which is not necessarily in F . Then with probability at least 1\u2212 \u03b4 we have\n\u2016p \u2212 LF (S)\u2016TV \u2264 3OPT (F , p) + \u03b1\n\u221a v + log 1\u03b4\nm\nwhere v is VC-dimension of EPI(\u2206F), and OPT (F , p) = infq\u2217\u2208F \u2016q \u2217 \u2212 p\u2016TV , and \u03b1 is a universal constant. In particular, in the realizable setting p \u2208 F , we have\n\u2016p \u2212 LF (S)\u2016TV \u2264 \u03b1\n\u221a v + log 1\u03b4\nm\nProof Let q\u2217 = argminq\u2208F \u2016p\u2212q\u2016TV , so \u2016q \u2217\u2212p\u2016A \u2264 \u2016q \u2217\u2212p\u2016TV = OPT (F , p). By Lemma 15,\nwith probability \u2265 1\u2212\u03b4 we have \u2016p\u2212 p\u0302S\u2016A \u2264 \u03b1 \u221a (v + log 1\u03b4 )/m for some universal constant \u03b1. Also, since LF (S) is the empirical minimizer of the A-distance, we have \u2016LF (S)\u2212p\u0302S\u2016A \u2264 \u2016q\u2217 \u2212 p\u0302S\u2016A. The proof follows from multiple applications of the triangle inequality:\n\u2016p\u2212 LF (S)\u2016TV \u2264 \u2016L F (S)\u2212 q\u2217\u2016TV + \u2016q \u2217 \u2212 p\u2016TV\n= \u2016LF (S)\u2212 q\u2217\u2016A +OPT (F , p) \u2264 \u2016LF (S)\u2212 p\u0302S\u2016A + \u2016p\u0302S \u2212 q \u2217\u2016A +OPT (F , p) \u2264 \u2016q\u2217 \u2212 p\u0302S\u2016A + (\u2016p\u0302S \u2212 p\u2016A + \u2016p \u2212 q \u2217\u2016A) +OPT (F , p) \u2264 (\u2016q\u2217 \u2212 p\u2016A + \u2016p\u2212 p\u0302S\u2016A) + \u2016p\u2212 p\u0302S\u2016A + 2OPT (F , p) \u2264 \u2016q\u2217 \u2212 p\u2016TV + 2\u2016p \u2212 p\u0302S\u2016A + 2OPT (F , p)\n\u2264 2\u03b1\n\u221a v + log 1\u03b4\nm + 3OPT (F , p) .\nTheorem 18 provides a tool for proving upper bounds on the sample complexity of distribution learning. To prove Theorems 11 and 8, it remains to show upper bounds on the VC dimensions of the class of epigraphs of (axis-aligned) Gaussian pdfs.\nThe following result is well known in statistical learning theory, see, e.g., (Devroye and Lugosi, 2001, Lemma 4.2).\nTheorem 19 (Dudley) Let F be an n-dimensional vector space of real-valued functions. Then V C(EPI(F)) \u2264 n.\nNow let h be an indicator function for an arbitrary element in EPI(f1 \u2212 f2), where f1, f2 are pdfs of (axis-aligned) Gaussians. Then h is a {0, 1}-valued function and we have:\nh(x) = 1{N (\u00b51,\u03a31) > N (\u00b52,\u03a32)}\n= 1{\u03b11 exp( \u22121\n2 (x\u2212 \u00b51)\nT\u03a3\u221211 (x\u2212 \u00b51)) > \u03b12 exp( \u22121\n2 (x\u2212 \u00b52)\nT\u03a3\u221212 (x\u2212 \u00b52))}\n= 1{(x\u2212 \u00b51) T\u03a3\u221211 (x\u2212 \u00b51)\u2212 (x\u2212 \u00b52) T\u03a3\u221212 (x\u2212 \u00b52)\u2212 log \u03b12 \u03b11 > 0} .\nThe inner expression is a quadratic form, and the linear dimension of all quadratic functions is O(d2). Furthermore, for axis-aligned Gaussians, \u03a31 and \u03a32 are diagonal, and therefore, the inner function lies in an O(d)-dimensional space of functions spanned by {1, x1, . . . , xd, x 2 1, . . . , x 2 d}. Hence, by Dudley\u2019s theorem, we have the required upper bound (d or d2) on the VC-dimension of the classes of epigraphs. Finally, Theorems 11 and 8 follow from applying Theorem 18 to the class of (axis-aligned) Gaussian distributions."}], "references": [{"title": "Sample-optimal density estimation in nearly-linear time", "author": ["Jayadev Acharya", "Ilias Diakonikolas", "Jerry Li", "Ludwig Schmidt"], "venue": "In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Acharya et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2017}, {"title": "Neural network learning: theoretical foundations", "author": ["Martin Anthony", "Peter Bartlett"], "venue": null, "citeRegEx": "Anthony and Bartlett.,? \\Q1999\\E", "shortCiteRegEx": "Anthony and Bartlett.", "year": 1999}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Belkin and Sinha.,? \\Q2010\\E", "shortCiteRegEx": "Belkin and Sinha.", "year": 2010}, {"title": "Efficient density estimation via piecewise polynomial approximation", "author": ["Siu-On Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "venue": "In Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Chan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2014}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "Combinatorial methods in density estimation", "author": ["Luc Devroye", "G\u00e1bor Lugosi"], "venue": "ISBN 0-387-95117-2", "citeRegEx": "Devroye and Lugosi.,? \\Q2001\\E", "shortCiteRegEx": "Devroye and Lugosi.", "year": 2001}, {"title": "Robust estimators in high dimensions without the computational intractability", "author": ["I. Diakonikolas", "G. Kamath", "D.M. Kane", "J. Li", "A. Moitra", "A. Stewart"], "venue": "In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Diakonikolas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Diakonikolas et al\\.", "year": 2016}, {"title": "Learning Structured Distributions", "author": ["Ilias Diakonikolas"], "venue": "Handbook of Big Data,", "citeRegEx": "Diakonikolas.,? \\Q1956\\E", "shortCiteRegEx": "Diakonikolas.", "year": 1956}, {"title": "Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures", "author": ["Ilias Diakonikolas", "Daniel M Kane", "Alistair Stewart"], "venue": "arXiv preprint arXiv:1611.03473v2 [cs.LG],", "citeRegEx": "Diakonikolas et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Diakonikolas et al\\.", "year": 2017}, {"title": "Pac learning axis-aligned mixtures of gaussians with no separation assumption", "author": ["Jon Feldman", "Rocco A. Servedio", "Ryan O\u2019Donnell"], "venue": "In Proceedings of the 19th Annual Conference on Learning Theory,", "citeRegEx": "Feldman et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2006}, {"title": "Polynomial bounds for vc dimension of sigmoidal and general pfaffian neural networks", "author": ["Marek Karpinski", "Angus Macintyre"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Karpinski and Macintyre.,? \\Q1997\\E", "shortCiteRegEx": "Karpinski and Macintyre.", "year": 1997}, {"title": "On the learnability of discrete distributions", "author": ["Michael Kearns", "Yishay Mansour", "Dana Ron", "Ronitt Rubinfeld", "Robert E. Schapire", "Linda Sellie"], "venue": "In Proceedings of the Twentysixth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Kearns et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1994}, {"title": "Markov chains and mixing times", "author": ["David A. Levin", "Yuval Peres", "Elizabeth L. Wilmer"], "venue": "American Mathematical Society,", "citeRegEx": "Levin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2009}, {"title": "Probability and Computing: Randomized Algorithms and Probabilistic Analysis", "author": ["Michael Mitzenmacher", "Eli Upfal"], "venue": null, "citeRegEx": "Mitzenmacher and Upfal.,? \\Q2005\\E", "shortCiteRegEx": "Mitzenmacher and Upfal.", "year": 2005}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Moitra and Valiant.,? \\Q2010\\E", "shortCiteRegEx": "Moitra and Valiant.", "year": 2010}, {"title": "Near-optimal-sample estimators for spherical gaussian mixtures", "author": ["Ananda Theertha Suresh", "Alon Orlitsky", "Jayadev Acharya", "Ashkan Jafarpour"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Suresh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Suresh et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": ", Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)).", "startOffset": 2, "endOffset": 18}, {"referenceID": 1, "context": ", Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)).", "startOffset": 19, "endOffset": 43}, {"referenceID": 1, "context": ", Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)).", "startOffset": 19, "endOffset": 70}, {"referenceID": 1, "context": ", Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)). As an example of our setting, assume that the target distribution is a Gaussian mixture with k components in Rd. Then, how many examples do we need to find a distribution that is \u01eb-close to the target? This sample complexity question, as well as the corresponding computational complexity question, has received a lot of attention recently (see, e.g. Feldman et al. (2006); Chan et al.", "startOffset": 19, "endOffset": 445}, {"referenceID": 1, "context": ", Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)). As an example of our setting, assume that the target distribution is a Gaussian mixture with k components in Rd. Then, how many examples do we need to find a distribution that is \u01eb-close to the target? This sample complexity question, as well as the corresponding computational complexity question, has received a lot of attention recently (see, e.g. Feldman et al. (2006); Chan et al. (2014); Suresh et al.", "startOffset": 19, "endOffset": 465}, {"referenceID": 1, "context": ", Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)). As an example of our setting, assume that the target distribution is a Gaussian mixture with k components in Rd. Then, how many examples do we need to find a distribution that is \u01eb-close to the target? This sample complexity question, as well as the corresponding computational complexity question, has received a lot of attention recently (see, e.g. Feldman et al. (2006); Chan et al. (2014); Suresh et al. (2014); Diakonikolas et al.", "startOffset": 19, "endOffset": 487}, {"referenceID": 0, "context": "(2016, 2017); Acharya et al. (2017)).", "startOffset": 14, "endOffset": 36}, {"referenceID": 0, "context": "(2016, 2017); Acharya et al. (2017)). In this paper, we consider a scenario in which we are given a method for learning a class of distributions (e.g., Gaussians). Then, we ask whether we can use it, as a black box, to come up with an algorithm for learning a mixture of such distributions (e.g., mixture of Gaussians). We will show that the answer to this question is affirmative. We propose a generic method for learning mixture models. Roughly speaking, we show that by going from learning a single distribution from a class to learning a mixture of k distributions from the same class, the sample complexity is multiplied by a factor of at most (k log k)/\u01eb2. This result is general, and yet it is surprisingly tight in many important cases. As a demonstration, we show that our method provides a better sample complexity for learning mixtures of Gaussians than the state of the art. In particular, for learning mixtures of k Gaussians in Rd, our method requires \u00d5(d2k/\u01eb4) samples, improving by a factor of k2 over the \u00d5(d2k3/\u01eb4) bound of Diakonikolas et al. (2017). Furthermore, for the special case of axis-aligned Gaussian, we provide an upper bound of \u00d5(dk/\u01eb4), which is the first optimal bound with respect to k and d, and improves upon the \u00d5(dk9/\u01eb4) bound of Suresh et al.", "startOffset": 14, "endOffset": 1069}, {"referenceID": 0, "context": "(2016, 2017); Acharya et al. (2017)). In this paper, we consider a scenario in which we are given a method for learning a class of distributions (e.g., Gaussians). Then, we ask whether we can use it, as a black box, to come up with an algorithm for learning a mixture of such distributions (e.g., mixture of Gaussians). We will show that the answer to this question is affirmative. We propose a generic method for learning mixture models. Roughly speaking, we show that by going from learning a single distribution from a class to learning a mixture of k distributions from the same class, the sample complexity is multiplied by a factor of at most (k log k)/\u01eb2. This result is general, and yet it is surprisingly tight in many important cases. As a demonstration, we show that our method provides a better sample complexity for learning mixtures of Gaussians than the state of the art. In particular, for learning mixtures of k Gaussians in Rd, our method requires \u00d5(d2k/\u01eb4) samples, improving by a factor of k2 over the \u00d5(d2k3/\u01eb4) bound of Diakonikolas et al. (2017). Furthermore, for the special case of axis-aligned Gaussian, we provide an upper bound of \u00d5(dk/\u01eb4), which is the first optimal bound with respect to k and d, and improves upon the \u00d5(dk9/\u01eb4) bound of Suresh et al. (2014), which is only shown for the subclass of spherical Gaussians.", "startOffset": 14, "endOffset": 1289}, {"referenceID": 0, "context": "(2016, 2017); Acharya et al. (2017)). In this paper, we consider a scenario in which we are given a method for learning a class of distributions (e.g., Gaussians). Then, we ask whether we can use it, as a black box, to come up with an algorithm for learning a mixture of such distributions (e.g., mixture of Gaussians). We will show that the answer to this question is affirmative. We propose a generic method for learning mixture models. Roughly speaking, we show that by going from learning a single distribution from a class to learning a mixture of k distributions from the same class, the sample complexity is multiplied by a factor of at most (k log k)/\u01eb2. This result is general, and yet it is surprisingly tight in many important cases. As a demonstration, we show that our method provides a better sample complexity for learning mixtures of Gaussians than the state of the art. In particular, for learning mixtures of k Gaussians in Rd, our method requires \u00d5(d2k/\u01eb4) samples, improving by a factor of k2 over the \u00d5(d2k3/\u01eb4) bound of Diakonikolas et al. (2017). Furthermore, for the special case of axis-aligned Gaussian, we provide an upper bound of \u00d5(dk/\u01eb4), which is the first optimal bound with respect to k and d, and improves upon the \u00d5(dk9/\u01eb4) bound of Suresh et al. (2014), which is only shown for the subclass of spherical Gaussians. One merit of our approach is that it can be applied in the agnostic (a.k.a. robust) setting, where the target distribution does not necessarily belong to the mixture model of choice. Interestingly, to guarantee such a result, we do not need to assume that the black box works in the agnostic setting. For example, a learning method that works for the Gaussians in the realizable setting can be lifted to a method for learning Gaussian mixtures in the agnostic setting. The main drawback of our approach is its computational complexity: if m is the sample complexity, then our running time is O(mkm), i.e. exponential in the parameters. However, it is applicable in situations where computational power is extensive but training data is scarce. Moreover, recently there have been some work suggesting there is not much hope for efficient learning of, for example, mixtures of Gaussians, using statistical query algorithms (see Diakonikolas et al. (2017)).", "startOffset": 14, "endOffset": 2304}, {"referenceID": 2, "context": "PAC learning of distributions was introduced by Kearns et al. (1994), we refer the reader to Diakonikolas (2016) for a recent survey.", "startOffset": 48, "endOffset": 69}, {"referenceID": 1, "context": "(1994), we refer the reader to Diakonikolas (2016) for a recent survey.", "startOffset": 31, "endOffset": 51}, {"referenceID": 1, "context": "A closely related line of research in statistics (in which more emphasis is on sample complexity) is density estimation, for which the book by Devroye and Lugosi (2001) is an excellent resource.", "startOffset": 143, "endOffset": 169}, {"referenceID": 1, "context": "In particular, the VCdimension bound of (Anthony and Bartlett, 1999, Theorem 8.14) \u2013 which is based on the work of Karpinski and Macintyre (1997) \u2013 implies a sample complexity upper bound of O((k4d2 + k3d3)/\u03b52) for PAC learning mixtures of axis-aligned Gaussians, and an upper bound of O(k4d4/\u03b52) for PAC learning mixtures of general Gaussians (both results hold in the more general agnostic setting).", "startOffset": 41, "endOffset": 146}, {"referenceID": 1, "context": "In particular, the VCdimension bound of (Anthony and Bartlett, 1999, Theorem 8.14) \u2013 which is based on the work of Karpinski and Macintyre (1997) \u2013 implies a sample complexity upper bound of O((k4d2 + k3d3)/\u03b52) for PAC learning mixtures of axis-aligned Gaussians, and an upper bound of O(k4d4/\u03b52) for PAC learning mixtures of general Gaussians (both results hold in the more general agnostic setting). A sample complexity upper bound of O(d2k3 log k/\u03b54) for learning mixtures of Gaussians in the realizable setting was proved in (Diakonikolas et al., 2017, Theorem A.1). Our algorithm is motivated by theirs, but we have introduced several new ideas in the algorithm and in the analysis, which has resulted in improving the sample complexity bound by a factor of k2 and an algorithm that works in the more general agnostic setting. For mixtures of spherical Gaussians, a polynomial time algorithm for the realizable setting with sample complexity O(dk9 log(d)/\u03b54) was proposed in (Suresh et al., 2014, Theorem 11). We improve their sample complexity by a factor of \u00d5(k8), and moreover our algorithm works in the agnostic setting, too. In the special case of d = 1, a polynomial time algorithm with the optimal sample complexity of \u00d5(k/\u03b52) was proved in Chan et al. (2014). An important question, which we do not address in this paper, is finding polynomial time algorithms for learning distributions.", "startOffset": 41, "endOffset": 1272}, {"referenceID": 1, "context": "In particular, the VCdimension bound of (Anthony and Bartlett, 1999, Theorem 8.14) \u2013 which is based on the work of Karpinski and Macintyre (1997) \u2013 implies a sample complexity upper bound of O((k4d2 + k3d3)/\u03b52) for PAC learning mixtures of axis-aligned Gaussians, and an upper bound of O(k4d4/\u03b52) for PAC learning mixtures of general Gaussians (both results hold in the more general agnostic setting). A sample complexity upper bound of O(d2k3 log k/\u03b54) for learning mixtures of Gaussians in the realizable setting was proved in (Diakonikolas et al., 2017, Theorem A.1). Our algorithm is motivated by theirs, but we have introduced several new ideas in the algorithm and in the analysis, which has resulted in improving the sample complexity bound by a factor of k2 and an algorithm that works in the more general agnostic setting. For mixtures of spherical Gaussians, a polynomial time algorithm for the realizable setting with sample complexity O(dk9 log(d)/\u03b54) was proposed in (Suresh et al., 2014, Theorem 11). We improve their sample complexity by a factor of \u00d5(k8), and moreover our algorithm works in the agnostic setting, too. In the special case of d = 1, a polynomial time algorithm with the optimal sample complexity of \u00d5(k/\u03b52) was proved in Chan et al. (2014). An important question, which we do not address in this paper, is finding polynomial time algorithms for learning distributions. See Diakonikolas et al. (2017) for the state-of-the-art results on computational complexity of learning mixtures of Gaussians.", "startOffset": 41, "endOffset": 1432}, {"referenceID": 1, "context": "In particular, the VCdimension bound of (Anthony and Bartlett, 1999, Theorem 8.14) \u2013 which is based on the work of Karpinski and Macintyre (1997) \u2013 implies a sample complexity upper bound of O((k4d2 + k3d3)/\u03b52) for PAC learning mixtures of axis-aligned Gaussians, and an upper bound of O(k4d4/\u03b52) for PAC learning mixtures of general Gaussians (both results hold in the more general agnostic setting). A sample complexity upper bound of O(d2k3 log k/\u03b54) for learning mixtures of Gaussians in the realizable setting was proved in (Diakonikolas et al., 2017, Theorem A.1). Our algorithm is motivated by theirs, but we have introduced several new ideas in the algorithm and in the analysis, which has resulted in improving the sample complexity bound by a factor of k2 and an algorithm that works in the more general agnostic setting. For mixtures of spherical Gaussians, a polynomial time algorithm for the realizable setting with sample complexity O(dk9 log(d)/\u03b54) was proposed in (Suresh et al., 2014, Theorem 11). We improve their sample complexity by a factor of \u00d5(k8), and moreover our algorithm works in the agnostic setting, too. In the special case of d = 1, a polynomial time algorithm with the optimal sample complexity of \u00d5(k/\u03b52) was proved in Chan et al. (2014). An important question, which we do not address in this paper, is finding polynomial time algorithms for learning distributions. See Diakonikolas et al. (2017) for the state-of-the-art results on computational complexity of learning mixtures of Gaussians. Another important setting is computational complexity in the agnostic learning, see, e.g., Diakonikolas et al. (2016) for some positive results.", "startOffset": 41, "endOffset": 1646}, {"referenceID": 1, "context": "In particular, the VCdimension bound of (Anthony and Bartlett, 1999, Theorem 8.14) \u2013 which is based on the work of Karpinski and Macintyre (1997) \u2013 implies a sample complexity upper bound of O((k4d2 + k3d3)/\u03b52) for PAC learning mixtures of axis-aligned Gaussians, and an upper bound of O(k4d4/\u03b52) for PAC learning mixtures of general Gaussians (both results hold in the more general agnostic setting). A sample complexity upper bound of O(d2k3 log k/\u03b54) for learning mixtures of Gaussians in the realizable setting was proved in (Diakonikolas et al., 2017, Theorem A.1). Our algorithm is motivated by theirs, but we have introduced several new ideas in the algorithm and in the analysis, which has resulted in improving the sample complexity bound by a factor of k2 and an algorithm that works in the more general agnostic setting. For mixtures of spherical Gaussians, a polynomial time algorithm for the realizable setting with sample complexity O(dk9 log(d)/\u03b54) was proposed in (Suresh et al., 2014, Theorem 11). We improve their sample complexity by a factor of \u00d5(k8), and moreover our algorithm works in the agnostic setting, too. In the special case of d = 1, a polynomial time algorithm with the optimal sample complexity of \u00d5(k/\u03b52) was proved in Chan et al. (2014). An important question, which we do not address in this paper, is finding polynomial time algorithms for learning distributions. See Diakonikolas et al. (2017) for the state-of-the-art results on computational complexity of learning mixtures of Gaussians. Another important setting is computational complexity in the agnostic learning, see, e.g., Diakonikolas et al. (2016) for some positive results. A related line of research is parameter estimation for mixtures of Gaussians, see, e.g., Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)), who gave polynomial time algorithms for this problem assuming certain separability assumptions.", "startOffset": 41, "endOffset": 1778}, {"referenceID": 1, "context": "In particular, the VCdimension bound of (Anthony and Bartlett, 1999, Theorem 8.14) \u2013 which is based on the work of Karpinski and Macintyre (1997) \u2013 implies a sample complexity upper bound of O((k4d2 + k3d3)/\u03b52) for PAC learning mixtures of axis-aligned Gaussians, and an upper bound of O(k4d4/\u03b52) for PAC learning mixtures of general Gaussians (both results hold in the more general agnostic setting). A sample complexity upper bound of O(d2k3 log k/\u03b54) for learning mixtures of Gaussians in the realizable setting was proved in (Diakonikolas et al., 2017, Theorem A.1). Our algorithm is motivated by theirs, but we have introduced several new ideas in the algorithm and in the analysis, which has resulted in improving the sample complexity bound by a factor of k2 and an algorithm that works in the more general agnostic setting. For mixtures of spherical Gaussians, a polynomial time algorithm for the realizable setting with sample complexity O(dk9 log(d)/\u03b54) was proposed in (Suresh et al., 2014, Theorem 11). We improve their sample complexity by a factor of \u00d5(k8), and moreover our algorithm works in the agnostic setting, too. In the special case of d = 1, a polynomial time algorithm with the optimal sample complexity of \u00d5(k/\u03b52) was proved in Chan et al. (2014). An important question, which we do not address in this paper, is finding polynomial time algorithms for learning distributions. See Diakonikolas et al. (2017) for the state-of-the-art results on computational complexity of learning mixtures of Gaussians. Another important setting is computational complexity in the agnostic learning, see, e.g., Diakonikolas et al. (2016) for some positive results. A related line of research is parameter estimation for mixtures of Gaussians, see, e.g., Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)), who gave polynomial time algorithms for this problem assuming certain separability assumptions.", "startOffset": 41, "endOffset": 1803}, {"referenceID": 1, "context": "In particular, the VCdimension bound of (Anthony and Bartlett, 1999, Theorem 8.14) \u2013 which is based on the work of Karpinski and Macintyre (1997) \u2013 implies a sample complexity upper bound of O((k4d2 + k3d3)/\u03b52) for PAC learning mixtures of axis-aligned Gaussians, and an upper bound of O(k4d4/\u03b52) for PAC learning mixtures of general Gaussians (both results hold in the more general agnostic setting). A sample complexity upper bound of O(d2k3 log k/\u03b54) for learning mixtures of Gaussians in the realizable setting was proved in (Diakonikolas et al., 2017, Theorem A.1). Our algorithm is motivated by theirs, but we have introduced several new ideas in the algorithm and in the analysis, which has resulted in improving the sample complexity bound by a factor of k2 and an algorithm that works in the more general agnostic setting. For mixtures of spherical Gaussians, a polynomial time algorithm for the realizable setting with sample complexity O(dk9 log(d)/\u03b54) was proposed in (Suresh et al., 2014, Theorem 11). We improve their sample complexity by a factor of \u00d5(k8), and moreover our algorithm works in the agnostic setting, too. In the special case of d = 1, a polynomial time algorithm with the optimal sample complexity of \u00d5(k/\u03b52) was proved in Chan et al. (2014). An important question, which we do not address in this paper, is finding polynomial time algorithms for learning distributions. See Diakonikolas et al. (2017) for the state-of-the-art results on computational complexity of learning mixtures of Gaussians. Another important setting is computational complexity in the agnostic learning, see, e.g., Diakonikolas et al. (2016) for some positive results. A related line of research is parameter estimation for mixtures of Gaussians, see, e.g., Dasgupta (1999); Belkin and Sinha (2010); Moitra and Valiant (2010)), who gave polynomial time algorithms for this problem assuming certain separability assumptions.", "startOffset": 41, "endOffset": 1830}, {"referenceID": 9, "context": "Feldman et al. (2006)).", "startOffset": 0, "endOffset": 22}, {"referenceID": 15, "context": "Theorem 10 (Theorem 2 in Suresh et al. (2014)) The class Fk of mixtures of k axisaligned Gaussians in Rd in the realizable setting has mFk(\u01eb, 1/2) = \u03a9(dk/\u01eb 2).", "startOffset": 25, "endOffset": 46}, {"referenceID": 5, "context": "1 in Devroye and Lugosi (2001)) and therefore by Theorem 18, the sample complexity of PAC learning Fd is O(d/\u01eb 2).", "startOffset": 5, "endOffset": 31}], "year": 2017, "abstractText": "We consider PAC learning of probability distributions (a.k.a. density estimation), where we are given an i.i.d. sample generated from an unknown target distribution, and want to output a distribution that is close to the target in total variation distance. Let F be an arbitrary class of probability distributions, and let F denote the class of k-mixtures of elements of F . Assuming the existence of a method for learning F with sample complexity mF (\u01eb) in the realizable setting, we provide a method for learningF with sample complexity O(k log k \u00b7mF(\u03b5)/\u01eb2) in the agnostic setting. Our mixture learning algorithm has the property that, if the F -learner is proper, then the F-learner is proper as well. We provide two applications of our main result. First, we show that the class of mixtures of k axis-aligned Gaussians in R is PAC-learnable in the agnostic setting with sample complexity \u00d5(kd/\u01eb), which is tight in k and d. Second, we show that the class of mixtures of k Gaussians in R is PAC-learnable in the agnostic setting with sample complexity \u00d5(kd/\u01eb), which improves the previous known bounds of \u00d5(kd/\u01eb) and \u00d5(kd/\u01eb) in its dependence on k and d.", "creator": "LaTeX with hyperref package"}}}