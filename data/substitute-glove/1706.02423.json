{"id": "1706.02423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Seamless Integration and Coordination of Cognitive Skills in Humanoid Robots: A Deep Learning Approach", "abstract": "This noted examining explain resources coordination include in different cognitive concepts following put snakelike robot want simply developed through continues - without - next school followed direct factors of visuomotor flowed. We propose a deep particular ganglia network model laid on new evolving vision network, little passenger generation focus, and a gain - level network. The scheme current took designed it process and decided integrate making perception is dynamic visuomotor patterns recently another stratification uses consequence took different spatial and temporal complexity slapped on each balance. We presented products lander lab and which a canine learned allow reading human ' s supposed into observing the purposeful all so only generate the hence goal - lucas intentions. Results verify neither the implement model is able allow everything the tutors agility between to generalize taken to shakespeare unfortunately. The comparison showed synergic stability however expression, appeal took decision without, and it framework or responsible when another of analytical tool including visual desire, intention giving, prompted switching, working memory, action preparation besides rape 2003 took seamless rational. Analysis revelation that mechanism pressing iconographic emerged first each positive it the hierarchy. Higher - level independent reflecting actional nor 1950s including that large maximum transition as with bank - level visuo - sidetracks stream.", "histories": [["v1", "Thu, 8 Jun 2017 01:15:00 GMT  (1791kb)", "http://arxiv.org/abs/1706.02423v1", "Accepted in the IEEE Transactions on Cognitive and Developmental Systems (TCDS), 2017"]], "COMMENTS": "Accepted in the IEEE Transactions on Cognitive and Developmental Systems (TCDS), 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.RO", "authors": ["jungsik hwang", "jun tani"], "accepted": false, "id": "1706.02423"}, "pdf": {"name": "1706.02423.pdf", "metadata": {"source": "CRF", "title": "Seamless Integration and Coordination of Cognitive Skills in Humanoid Robots: A Deep Learning Approach", "authors": ["Jungsik Hwang", "Jun Tani"], "emails": ["jungsik.hwang@gmail.com).", "tani1216jp@gmail.com)."], "sections": [{"heading": null, "text": "Seamless Integration and Coordination of Cognitive Skills in Humanoid Robots: A Deep\nLearning Approach\nJungsik Hwang and Jun Tani\nAbstract\nThis study investigates how adequate coordination among the different cognitive processes of a humanoid robot can be developed through end-to-end learning of direct perception of visuomotor stream. We propose a deep dynamic neural network model built on a dynamic vision network, a motor generation network, and a higher-level network. The proposed model was designed to process and to integrate direct perception of dynamic visuomotor patterns in a hierarchical model characterized by different spatial and temporal constraints imposed on each level. We conducted synthetic robotic experiments in which a robot learned to read human's intention through observing the gestures and then to generate the corresponding goal-directed actions. Results verify that the proposed model is able to learn the tutored skills and to generalize them to novel situations. The model showed synergic coordination of perception, action and decision making, and it integrated and coordinated a set of cognitive skills including visual perception, intention reading, attention switching, working memory, action preparation and execution in a seamless manner. Analysis reveals that coherent internal representations emerged at each level of the hierarchy. Higher-level representation reflecting actional intention developed by means of continuous integration of the lower-level visuo-proprioceptive stream.\nIndex Terms\u2014Neurorobotics, deep learning, sensorimotor learning\nThis work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) [No. 2014R1A2A2A01005491]. Jungsik Hwang is with the school of Electrical Engineering in Korea Advanced Institute of Science and Technology, Daejeon, South Korea (e-mail: jungsik.hwang@gmail.com). Jun Tani is a corresponding author and he is a professor at the school of Electrical Engineering in Korea Advanced Institute of Science and Technology, Daejeon, South Korea and an adjunct professor at Okinawa Institute of Science and Technology, Okinawa, Japan. (e-mail: tani1216jp@gmail.com).\nAccepted in the IEEE Transactions on Cognitive and Developmental Systems (TCDS), 2017\nI. INTRODUCTION\nIt would be desirable if a robot could learn to generate complex goal-directed behaviors from its own sensorimotor experience, as human beings do. One challenge in reaching this goal is that such complex behaviors require an agent to coordinate multiple cognitive processes. For instance, imagine a robot conducting an object manipulation task with a human partner, such as reaching for and grasping an object. The human partner indicates a target objects located on the workspace through a gesture. Then, the robot observes the workspace, finds the indicated object by combining the perceived gesture information as well as the perceived object\u2019s properties, switches its attention to the object, prepares an action and executes it. Even this simple task is complex, involving diverse cognitive skills such as visual perception, intention reading, working memory, decision-making, action preparation and execution. It is essential to link these skills with synergy by developing spatio-temporal coordination among them. Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].\nIn this study, we employed a deep learning approach to build a robotic system which can directly and autonomously learn from its own visuomotor experience. Deep learning is a fast-growing field in machine learning and artificial intelligence with remarkable advances, such as text recognition, speech recognition, image recognition and many others (See [10-12] for recent reviews of deep learning). One of the most important characteristics of deep learning is that deep networks can autonomously extract task-related features in high-dimensional data, such as images and action sequences, without the necessity of hand-engineered feature extraction methods [1, 10, 12]. So, deep learning provides an important tool for robotics, because through deep learning a robot can learn directly from its huge-dimensional sensorimotor data acquired through dynamic interaction with the environment [1]. A few recent studies [13-16] have demonstrated the plausibility of deep learning in the field of robotics. However, several challenges in adapting deep learning schemes to robotics remain. For example, a robotic system must process spatio-temporally dynamic patterns, whereas deep learning schemes have generally been designed to process static patterns. In addition, robotic tasks typically incorporate multiple sensory modalities, such as vision, proprioception and audition, while most deep learning applications attend to single modalities, e.g. visual facial recognition. Also, Sigaud and Droniou [1] have pointed out that it is still unclear how higher-level representations can be built by stacking several networks.\nIn this paper, we propose a dynamic deep neural network model called the Visuo-Motor Deep Dynamic Neural\nNetwork (VMDNN) which can learn to generate goal-directed behaviors by coordinating multiple cognitive processes including visual perception, intention reading 1 , attention switching, memorization and retrieval with working memory, action preparation and generation. The model was designed to process and to integrate dynamic visuomotor patterns directly perceived through the robot\u2019s interaction with its environment. The VMDNN is composed of three different types of subnetwork: the Multiple Spatio-Temporal scales Neural Network (MSTNN) [17], the Multiple Timescales Recurrent Neural Network (MTRNN) [18] and the PFC (Prefrontal Cortex) subnetworks. The MSTNN has demonstrated an ability to recognize dynamic visual scenes [17], and the MTRNN to learn compositional actions [18]. In the VMDNN model, these two subnetworks are tightly coupled via the PFC subnetwork, enabling the system to process dynamic visuomotor patterns simultaneously. In other words, the problems of perceiving visual patterns and generating motor patterns were regarded as inseparable problem and therefore the visual perception and the motor generation were performed simultaneously in a single VMDNN model in the current study. This approach is based on the previous studies which emphasized the importance of perception-action coupling in robotic manipulation [13] as well as in developmental robotics [40]. Each part of the architecture is imposed with different spatial and temporal constraints to enable the hierarchical computation of visuomotor processing in the lower-level and its abstraction in the higher-level. We conducted a set of synthetic robotics experiments to examine the proposed model and also to gain insight into mechanisms involved in learning goal-directed actions in biological systems. Here, it is worth noting that artificial neural networks are meant to model the essential features of the nervous system, not its detailed implementation [19-21].\nIn our experiments, a humanoid robot learned goal-directed actions from huge-dimensional visuomotor data acquired from repeated tutoring during which the robot\u2019s actions were guided by the experimenter. We investigated how a set of cognitive skills can be integrated and coordinated in a seamless manner to generate goal-directed sequential behaviors of the robot. Particularly, we focused on identifying the human gestures, reading intention underlying them, and generating corresponding sequential behaviors of the humanoid robot in a reaching-and-grasping task. In our experiment, the robot was trained to recognize human gestures and to grasp the target object indicated by the gestures. This task thus required a set of cognitive skills such as visual perception, intention reading, working memory, action preparation and execution. Recognizing gestures is more challenging than recognizing static images\n1Here, the term of \u201creading\u201d or \u201ccategorizing\u201d is used rather than \u201crecognizing\u201d because recognition may involve reconstruction of patterns but categorization in\nthe current study does not.\nsince both spatial and temporal information in the gesture need to be identified. Moreover, reading intention of others by observing their behavior has been considered as one of the core abilities required for social cognition [5, 22-24]. In addition, reaching and grasping are fundamental skills that have significant influences on the development of perceptual and cognitive abilities [5]. The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29]. In a robotic context, they require robust perception and action systems as well as simultaneous coordination of a set of cognitive skills, making hand-designing features demanding and time-consuming [14]. Moreover, the visuomotor task in our experiment was not explicitly segmented into the different sub-tasks such as gesture classification and action generation by the experimenter. Therefore, the task requires the robot to adapt to different task phases autonomously by coordinating a set of cognitive skills in a seamless manner throughout the task. In addition, this task requires the robot to have working memory capability to keep the contextual information such that the robot could compare the categorized human intention with perceived object properties. This implies that simply mapping perception to action cannot perform the task successfully since it lacks contextual dynamics. We expected that the synergic coordination of the above mentioned cognitive skills would arise through end-to-end learning of the tightly coupled structure in which the multiple subnetworks densely interact.\nIn the learning stage, the robot learned a task in the supervised end-to-end manner. In the testing stage, we examined the model\u2019s learning and generalization capabilities. Furthermore, the robot was examined under a visual occlusion experimental paradigm in which the visual input to the model was unexpectedly and completely occluded. This was to verify whether the proposed model was equipped with a sort of memory capability for maintaining task-related information. In addition, we analyzed the neuronal activation in order to clarify the internal dynamics and representations emerging in the model during different phases of operation.\nThe remaining part of the paper is organized as follows. In Section II, we review several previous studies employing deep learning schemes in the robotic context. In Section III, we introduce the proposed model in detail. Section IV and V are devoted to the experimental settings and the results respectively. Several key aspects and the implications of the proposed model are discussed in Section VI. Finally, we conclude our paper and indicate current and future research directions in Section VII."}, {"heading": "II. RELATED WORKS", "text": "Due to the remarkable success of deep learning in various fields, recent studies have attempted to employ deep learning in the field of robotics (See [1] for a recent review). For instance, Di Nuovo, et al. [15] employed a deep neural network architecture in order to study number cognition in a humanoid robot. The robot was trained to classify numbers by learning the association between an auditory signal and corresponding finger counting activity. They found that their model was quicker and more accurate when both modalities were associated. Similarly, Droniou, et al. [2] introduced a deep network architecture which could learn from different sensory modalities, including vision, audition and proprioception. During experiments, their model was trained to classify handwritten digits, and they demonstrated that learning across multiple modalities significantly improved classification performance. Yu and Lee [30] employed a deep learning approach on reading human intention. A supervised MTRNN model was employed in their experiments, and they showed that their model could successfully recognize human intention through observing a set of motions. Lenz, et al. [14] proposed a two-stage cascaded detection system to detect robotic grasps in an RGB-D view of a scene and conducted experiments on different robotic platforms. They found that their deep learning method performed significantly better than one with well-designed hand-engineered features. Pinto and Gupta [31] also addressed the problem of detecting robotic grasps, adopting a convolutional neural network (CNN) model to predict grasp location and angle.\nAlthough these studies demonstrate the utility of deep learning in a robotic context, they most focused on robotic perception and robots were not directly controlled by deep learning schemes. A few recent studies have attempted to utilize deep learning to control a robot. Wahlstr\u00f6m, et al. [16], addressed the pixels-to-torques problem by introducing a reinforcement learning algorithm that enabled their agent to learn control policy from pixel information. Deep auto-encoders and a multi-layer feedforward neural network were employed in their model, and a 1-link robotic pendulum was used as a testing platform. Their model was able to learn closed-loop policies in continuous state-action spaces directly from pixel information. Noda, et al. [32] introduced a deep auto-encoder-based computational framework designed to integrate sensorimotor data. A humanoid robot was used in their object manipulation experiments, and they showed that their model was able to form higher-level multimodal representations by learning sensorimotor information including joint angles, RGB images and audio data. Levine, et al. [13] proposed a deep neural network model which learned a control policy that linked raw image percepts to motor torques of the robot. They\nshowed that the robot was able to conduct various object manipulation tasks by learning perception and control together in an end-to-end manner. Park and Tani [33] investigated how a robot could infer the underlying intention of human gestures and generate corresponding behaviors of a humanoid robot. In their work, an MTRNN model was employed and the robot could successfully achieve the task by extracting the compositional semantic rules latent in the various combinations of human gestures.\nSeveral problems confront ongoing deep-learning research in robotics applications, as represented by limitations in existing studies such as relatively simple testing platform [16, 33], separate processing of individual modalities [32], and inability to handle temporal information [13]. In the current study, we aim to address these challenges with a deep dynamic neural network model for a humanoid robot which can process and integrate spatio-temporal dynamic visuomotor patterns in an end-to-end manner."}, {"heading": "III. THE DEEP NEURAL NETWORK MODEL", "text": "In this section, we describe the Visuo-Motor Deep Dynamic Neural Network (VMDNN) in detail. The proposed model was designed to process and to integrate direct perception of visuomotor patterns in a hierarchical structure characterized by different spatio-temporal constraints imposed on each part of the hierarchy. It has several distinctive characteristics. First, the model can perform low-level visuomotor processing without hand-engineered feature extraction methods by means of deep learning schemes. Second, the model processes dynamic visuomotor patterns in a hierarchical structure essential to cortical computation [15, 34]. Third, perception and action are tightly intertwined within the system, enabling the model to form multimodal representations across sensory modalities. The VMDNN model consists of three types of subnetworks: (1) MSTNN subnetworks for processing dynamic visual images, (2) MTRNN subnetworks for controlling the robot\u2019s action and attention and (3) a prefrontal cortex (PFC) subnetwork located on top of these two subnetworks which dynamically integrates them (Fig. 1)."}, {"heading": "A. MSTNN Subnetwork", "text": "In our study, we employed Multiple Spatio-Temporal Scales Neural Network (MSTNN) to process dynamic visual images perceived by a robot conducting a visuomotor task. The MSTNN is an extended Convolutional Neural Network (CNN) [35] employing leaky integrator neural units with different time constants [17, 36]. Although conventional CNN models have been shown the ability to process spatial data such as static images, they lack the ability to process spatio-temporally dynamic patterns. To successfully conduct a visuomotor task, the robot needs to extract both spatial and temporal features latent in the sequential observations. Unlike conventional CNN models that utilize spatial constraints only, the MSTNN model has been shown that it can process both spatial and temporal patterns by imposing multiple spatio-temporal scales constraints on local neural activity [17, 36]. Consequently, the MSTNN can extract the task-related visual features latent in the dynamic visual images while the robot was tutored for the task iteratively.\nThe MSTNN subnetwork was composed of three layers imposed with different spatio-temporal constraints: the MSTNN-input (VI) layer containing a current visual scene, the MSTNN-fast (VF) layer with shorter-distance connectivity and smaller time constants, and the MSTNN-slow (VS) layer with longer-distant connectivity and larger time constants. Each MSTNN layer organizes into a specific set of feature maps retaining spatial information of visual input. They are connected successively from VI to VS."}, {"heading": "B. MTRNN Subnetwork", "text": "In the current study, we employed Multiple Timescales Recurrent Neural Network (MTRNN) for generating the robot\u2019s behavior and controlling the attention of the robot. The MTRNN is a hierarchical neural network model consisting of a multiple continuous time recurrent neural networks with leaky integrator neurons [18]. MTRNN has been shown superior performance in modeling robot\u2019s sequential action by utilizing its temporal hierarchy. To be more specific, the lower level in the MTRNN has a smaller time constant showing fast dynamics whereas the higher level has a bigger time constant exhibiting slow dynamics. Due to this temporal hierarchy, an MTRNN can learn compositional action sequences as a meaningful functional hierarchy emerges within the system [18, 34, 37]. Consequently, the entire behavior of the robot including reaching and grasping as well as visual attention control can be decomposed into a set of primitives for their flexible recombination adapting to various situations [18].\nIn our model, the MTRNN subnetwork is composed of three layers characterized by different temporal constraints: the MTRNN-slow (MS) showing slow dynamics with the larger time constant, the MTRNN-fast (MF) showing fast dynamics with smaller and the MTRNN-output (MO) layer with the smallest time constant. Neurons in the MS and MF layers are asymmetrically connected to each other and to themselves. The MO layer is composed of groups of softmax neurons indicating the sparse representation of the model\u2019s output. The MO layer receives inputs from the MF layer and generates behavior outputs as well as attention control signals."}, {"heading": "C. PFC Subnetwork", "text": "At the top, the PFC (Prefrontal Cortex) layer tightly couples the MSTNN and MTRNN subnetworks for achieving tight association between the visual perception and the motor generation. The PFC layer is a recurrent neural network consisting of a set of leaky-integrator neurons equipped with recurrent loops in order to process abstract sequential information. The PFC layer receives inputs from the VS layer in the MSTNN subnetwork as well as from the MS layer in the MTRNN subnetwork, meaning that both abstracted visual information (VS) and proprioceptive information (MS) are integrated in the PFC layer. The PFC layer also has a forward connection to the MS layer to control the robot\u2019s behavior and attention.\nThe PFC layer can be characterized by several key features. First, neurons in the PFC layer are assigned the largest time constant. As a result, the PFC subnetwork exhibits the slowest-scale dynamics and this enables the PFC subnetwork to carry more information about a situation [38]. Second, neurons in the PFC layer are equipped with\nrecurrent connections which are essential to handle dynamic sequential data [1, 9, 39]. Third, perception and action are coupled via the PFC layer which integrates two monomodal subnetworks (MSTNN and MTRNN) and builds higher-level multimodal representations from abstracted visuomotor information in each pathway. Lungarella and Metta [40] argued that perception and action are not separated but tightly coupled, with this coupling is gradually getting refined during developmental process."}, {"heading": "D. Problem Formulation", "text": "Fig. 1 illustrates the structure of the VMDNN model. The input to the model (\ud835\udc3c\ud835\udc61) is an observation of the world at time step t which can be obtained from the robot\u2019s camera at the beginning to the end of the task. The observation is a pixel-level image represented as a matrix H \u00d7 W, where H is a height and W is a width of the image. Robot\u2019s behavior outputs as well as attention control signals are generated at the output layer MO at each time step t. Let \ud835\udc66 \ud835\udc61 =\n[\ud835\udc661 \ud835\udc61 , \ud835\udc662 \ud835\udc61 , \u2026 , \ud835\udc66\ud835\udc5b \ud835\udc61 ] denotes the output of the model at the time step t where n is the number of neurons at the output layer\nMO.\nIn forward dynamics computation, the problem is defined as to the behavior output and the attention signal (\ud835\udc66\ud835\udc61) at time step t given a visual observation \ud835\udc3c\ud835\udc61 and the model parameters \u03b8 such as kernels, weights, and biases. In the training phase, the problem is defined as to optimize the model\u2019s parameters \u03b8 in order to minimize the error E at the output layer MO represented by the Kullback-Leibler divergence between the teaching signal \ud835\u0305\udc66 \ud835\udc61 and the network\u2019s output \ud835\udc66\ud835\udc61. The training data was visuomotor sequences obtained from the repeated tutoring prior to the training phase. The detailed description about the forward dynamics and the training phase are described in the following sections."}, {"heading": "E. Forward Dynamics for Action Generation", "text": "The internal states of all neural units were initialized with neutral values at the onset of the action generation mode. Then, a pixel image \ud835\udc3c\ud835\udc61 in grayscale obtained from the robot\u2019s camera was given to the vision input layer (VI) and neural unit\u2019s internal states and activations were successively computed in every layer from the input layer (VI) to the output layer (MO). The neuron activation at the MO layer was transformed to analog values using the softmax activation to control the robot\u2019s joints and attention. A detailed description of the computational procedure follows.\nAt each time step t, the internal state ui txy and the dynamic activation vi txy of the neural unit located at the (x, y) position in the ith feature map in the MSTNN layers (i \u2208 VF \u02c5 VS) are computed according to the following formulas:\n\ud835\udc62\ud835\udc56 \ud835\udc61\ud835\udc65\ud835\udc66\n= (1 \u2212 1\n\ud835\udf0f\ud835\udc56 )\ud835\udc62\ud835\udc56\n(\ud835\udc61\u22121)\ud835\udc65\ud835\udc66 + 1\n\ud835\udf0f\ud835\udc56 [\u2211(\ud835\udc58\ud835\udc56\ud835\udc57 \u2217 \ud835\udc63\ud835\udc57\n\ud835\udc61) \ud835\udc65\ud835\udc66\n\ud835\udc57\u2208\ud835\udc49\ud835\udc57\n+ \ud835\udc4f\ud835\udc56] (1)\nv\ud835\udc56 \ud835\udc61\ud835\udc65\ud835\udc66\n= 1.7159 \u00d7 tanh ( 2\n3 \ud835\udc62\ud835\udc56 \ud835\udc61\ud835\udc65\ud835\udc66 ) (2)\n\u03c4 is the time constant, Vj is the feature maps in the previous layer (if i \u2208 VF, then Vj = VI and if i \u2208 VS, then Vj = VF), * is the convolution operator, kij is the kernel connecting jth feature map in Vj with the ith feature map in the current layer, and b is the bias. Please note that the hyperbolic tangent recommended in [41] was used as an activation function to enhance convergence.\nFrom the PFC layer to the MO layer, the internal state ui t and the dynamic activation yi t of the ith neuron in the PFC\nand MTRNN layers (i \u2208 PFC \u02c5 MS \u02c5 MF \u02c5 MO) are determined by the following equations:\n\ud835\udc62\ud835\udc56 \ud835\udc61 = (1 \u2212\n1 \ud835\udf0f\ud835\udc56 )\ud835\udc62\ud835\udc56 \ud835\udc61\u22121 +\n{\n1 \ud835\udf0f\ud835\udc56 [\u2211 \ud835\udc58\ud835\udc56\ud835\udc57 \u2217 \ud835\udc63\ud835\udc57 \ud835\udc61 \ud835\udc57\u2208\ud835\udc49\ud835\udc46 + \u2211 \ud835\udc64\ud835\udc56\ud835\udc58\ud835\udc66\ud835\udc58 \ud835\udc61\u22121 + \ud835\udc4f\ud835\udc56\ud835\udc58\u2208\ud835\udc40\ud835\udc46\u02c5\ud835\udc43\ud835\udc39\ud835\udc36 ] \ud835\udc56\ud835\udc53 \ud835\udc56 \u2208 \ud835\udc43\ud835\udc39\ud835\udc36\n1 \ud835\udf0f\ud835\udc56 [\u2211 \ud835\udc64\ud835\udc56\ud835\udc57\ud835\udc66\ud835\udc57 \ud835\udc61 +\ud835\udc57\u2208\ud835\udc43\ud835\udc39\ud835\udc36 \u2211 \ud835\udc64\ud835\udc56\ud835\udc58\ud835\udc66\ud835\udc58 \ud835\udc61\u22121 + \ud835\udc4f\ud835\udc56\ud835\udc58\u2208\ud835\udc40\ud835\udc39\u02c5\ud835\udc40\ud835\udc46 ] \ud835\udc56\ud835\udc53 \ud835\udc56 \u2208 \ud835\udc40\ud835\udc46 1 \ud835\udf0f\ud835\udc56 [\u2211 \ud835\udc64\ud835\udc56\ud835\udc57\ud835\udc66\ud835\udc57 \ud835\udc61\u22121 + \ud835\udc4f\ud835\udc56\ud835\udc57\u2208\ud835\udc40\ud835\udc46\u02c5\ud835\udc40\ud835\udc39 ] \ud835\udc56\ud835\udc53 \ud835\udc56 \u2208 \ud835\udc40\ud835\udc39\n1 \ud835\udf0f\ud835\udc56 [\u2211 \ud835\udc64\ud835\udc56\ud835\udc57\ud835\udc66\ud835\udc57 \ud835\udc61 + \ud835\udc4f\ud835\udc56\ud835\udc57\u2208\ud835\udc40\ud835\udc39 ] \ud835\udc56\ud835\udc53 \ud835\udc56 \u2208 \ud835\udc40\ud835\udc42\n(3)\ny\ud835\udc56 \ud835\udc61 =\n{\n1.7159 \u00d7 tanh (\n2 3 \ud835\udc62\ud835\udc56 \ud835\udc61) \ud835\udc56\ud835\udc53 \ud835\udc56 \u2208 \ud835\udc43\ud835\udc39\ud835\udc36\u02c5\ud835\udc40\ud835\udc46\u02c5\ud835\udc40\ud835\udc39\nexp (\ud835\udc62\ud835\udc56 \ud835\udc61)\n\u2211 exp(\ud835\udc62\ud835\udc57 \ud835\udc61)\ud835\udc57\u2208\ud835\udc40\ud835\udc42 \ud835\udc56\ud835\udc53 \ud835\udc56 \u2208 \ud835\udc40\ud835\udc42\n(4)\nwij are the weight from the jth neural unit to the ith neural unit. In sum, the image obtained from the robot\u2019s camera was sent to the input layer (VI) of the model at each time step of the action generation mode. Then, the internal states and activations were successively computed from the input layer (VI) to the output layer (MO). The robot was operated based on the output of the model including the joint position values. After the execution of an action, the image acquired from the robot\u2019s camera was sent to the input layer (VI) of the model again in the next time step. In this sense, an image given to the model can be considered as a visual feedback since it reflects the effect of the robot\u2019s action in the preceding time step."}, {"heading": "F. Training Phase", "text": "The model was trained in a supervised end-to-end learning fashion. The training data consisted of raw visuomotor sequences obtained from repeated tutoring during which the robot was manually operated by the experimenter. From\nthe beginning to the end of tutoring, a visual image perceived from the robot\u2019s camera (visual observation) was jointly collected with the encoder values of the robot\u2019s joint positions as well as the level of grasping and the level of foveation at each time step. The model was trained to abstract and associate visual perception with proprioceptive information using these visuomotor sequence patterns. Backpropagation through time (BPTT) [42] was employed in learning values of parameters, such as the kernels, weights and biases of the model.\nPrior to end-to-end training, learnable parameters in the visual pathway (from VI to PFC) were initialized by means of pre-training. Studies have demonstrated that pre-training is an efficient method for initializing network parameters [10, 13, 43]. The pre-training method in our study is similar to that of [13] in which the visual part of the model was pre-trained prior to the end-to-end learning phase of the experiment. During pre-training in our study, the softmax output layer was connected to the PFC layer and the connections between the MTRNN subnetworks and the PFC layer, as well as the recurrent connections within the PFC layer were removed. In this condition, the system operates as an MSTNN model, and it was trained as a typical classifier using BPTT as described in [36]. Then, the values of the parameters in the visual pathway acquired during pre-training were used as initial values for the parameters in the same pathways during the end-to-end training phases of the experiment.\nAfter the pre-training, end-to-end training was conducted from the VI layer to the MO layer. During end-to-end training, the model\u2019s entire learnable parameters (k, w and b) were updated to minimize the error E at the MO layer represented by Kullback-Leibler divergence between the teaching signal y\u0305i t and the network\u2019s output yi t.\n\ud835\udc38 = \u2211 \u2211 \ud835\u0305\udc66\ud835\udc56 \ud835\udc61\ud835\udc59\ud835\udc5c\ud835\udc54\n\ud835\u0305\udc66\ud835\udc56 \ud835\udc61 \ud835\udc66\ud835\udc56 \ud835\udc61\n\ud835\udc56\u2208\ud835\udc40\ud835\udc42\ud835\udc61\n(5)\nA stochastic gradient descent method was applied during end-to-end training and the entire learnable parameters\nwere updated when each training data was presented as follows.\n\ud835\udc64\ud835\udc56\ud835\udc57(\ud835\udc5b + 1) = \ud835\udc64\ud835\udc56\ud835\udc57(\ud835\udc5b) \u2212 \ud835\udf02 (\n\ud835\udf15\ud835\udc38\n\ud835\udf15\ud835\udc64\ud835\udc56\ud835\udc57 + 0.0005\ud835\udc64\ud835\udc56\ud835\udc57(\ud835\udc5b)) (6)\n\ud835\udc58\ud835\udc56\ud835\udc57(\ud835\udc5b + 1) = \ud835\udc58\ud835\udc56\ud835\udc57(\ud835\udc5b) \u2212 \ud835\udf02 (\n\ud835\udf15\ud835\udc38\n\ud835\udf15\ud835\udc58\ud835\udc8a\ud835\udc8b + 0.0005\ud835\udc58\ud835\udc56\ud835\udc57(\ud835\udc5b)) (7)\nb\ud835\udc56(\ud835\udc5b + 1) = b\ud835\udc56(\ud835\udc5b) \u2212 \ud835\udf02 (\n\ud835\udf15\ud835\udc38 \ud835\udf15\ud835\udc4f\ud835\udc56 ) (8)\nn is an index of the learning step and \u03b7 is the learning rate. The weight decay method was used to prevent overfitting\n[35] with the weight decay rate of 0.0005.\nIV. EXPERIMENT SETTINGS"}, {"heading": "A. Robotic Platform", "text": "iCub [44] is a child-like humanoid robot consisting of 53 degrees of freedom (DOFs) distributed in the body. We used a simulation of the iCub [45] in our experiments. The iCub simulator accurately models the actual robot\u2019s physical interaction with the environment, making it an adequate research platform for studying developmental robotics [15, 45]. Simulation of the iCub is shown in (Fig. 2). In our simulation environment, a screen was located in front of the robot to display human gestures, the task table was placed between the screen and the robot and the two objects were placed on the task table.\nAt each time step, the interfacing program captured the image perceived from the robot\u2019s camera, preprocessed the captured image, sent it to the VMDNN model, received the output of the VMDNN model and then operated the robot based on the output of the model. To be more specific, the pixel image obtained from the robot\u2019s camera embedded in its left eye was passed to the vision input layer (VI) of the model at each time step. The obtained image was preprocessed by resizing to 64 (w) \u00d7 48 (h), converting to grayscale, and normalizing to -1 to 1. The softmax values of the output layer (MO) were converted into the values of the corresponding joint positions (7 DOFs in the right arm, 2 DOFs in the neck) as well as into grasping and attention control signals. Then, the interfacing program operated the robot based on the VMDNN model\u2019s outputs using the motor controller provided in the iCub software package [45]. Then, the interfacing program captured an image from the robot\u2019s camera and sent it back to the VMDNN model in the next time step.\nRegarding the output for the robot\u2019s behavior, we used the robot\u2019s right arm consisting of 7 DOFs (shoulder\u2019s pitch, roll, yaw, elbow, wrist\u2019s pronosupination, pitch, and yaw). In addition, the network output the level of extension or flexion of finger joints in order to control grasping similar to [20]. The level varied from 1 (not grasping) to 10 (fully grasping) and it was used to control the 9 DOFs of the right hand. In our experiments, the VMDNN model also controlled visual attention which is essential to generate adequate robot\u2019s behavior [37]. Jeong, et al. [37] demonstrated that MTRNN could seamlessly coordinate visual attention and motor behaviors. In their work, the MTRNN model outputted the category of the object to be attended to, and the external visual guiding system localized the position of the specified object based on the network\u2019s output and the retina image so that the robot\u2019s camera could fixate the object. In contrast, no external module was employed in our study, and the VMDNN model itself controlled robot\u2019s visual attention. More specifically, two attention control mechanisms were employed in our study: attention shift and foveation. First, the robot located the object of attention at the center of the visual scene by orienting its head (2 DOFs \u2013 neck\u2019s pitch and yaw). Second, the robot controlled the resolution of the visual scene given to the network by controlling the level of foveation from 1 (minimum foveation) to 10 (maximum foveation). For instance, the level of foveation increased while the robot\u2019s hand was closely approaching to the attended object. Consequently, images\ncontaining the target object and the robot\u2019s hand were given to the network with higher resolution so that the model could more clearly perceive the object\u2019s properties including orientation and the location of the hand."}, {"heading": "B. Network Configuration", "text": "The VMDNN model was composed of 7 layers: the VI, VF, VS layers in the MSTNN subnetwork, the PFC layer, and the MS, MF, MO layers in the MTRNN subnetwork. The structure of the VMDNN model used in this study was found empirically in our preliminary experiments [46]. Note that the structure of the VMDNN model including the number of layers in each subnetwork can be extended depending on the complexity of the task since the \u2018deeper\u2019 structure can enhance learning of complex functions in visuomotor patterns [11]. For instance, more number of layers in the MSTNN subnetwork can be employed to process more complex visual images [17]. Similarly, more complex robot\u2019s behavior can be learned by employing more number of layers in the MTRNN subnetwork as reported in [33].\nEach MSTNN layer consisted of a set of feature maps retaining the spatial information of the visual input. The number and size of feature maps varied between layers. The vision input layer (VI) had a single feature map containing the current visual scene (64 (w) \u00d7 48 (h)) obtained from the left eye of the robot. The VF layer consisted of 4 feature maps with sized 15 \u00d7 11 and the VS layer consisted of 8 feature maps with sized 5 \u00d7 3. The size of kernels in the VF and VS layers were set to 8 \u00d7 8 and 7 \u00d7 7 respectively. The sampling factors denoting the amount of shift of the kernel in a convolution operation were set to 4 and 2 for the VF and VS respectively. The PFC layer was composed of 20 neurons and the kernel size was 5 \u00d7 3 with the sampling factor set to 1. The numbers of neurons employed in the MTRNN layers (MS, MF and MO) were 30, 50 and 110 respectively. The 110 neurons in the MO layer were comprised of 11 groups of softmax neurons representing the 11 categories of the model\u2019s output: the joint position values of the robot\u2019s neck (2), the joint position values of the right arm (7), the level of grasping (1) and the level of foveation (1). The softmax output values in each group at the MO layer were inversely transformed to analog values that directly set the joint angles of the robots, the level of grasping and the level of foveation.\nRegarding the time scale properties, we compared two different types of visual pathway: CNN and MSTNN. In the CNN condition, the time constants of both the VF and the VS layers were set to 1, resulting in no temporal hierarchy. On the other hand, in the MSTNN condition, the time constants of the VF and the VS layers were set to 1 and 15 respectively, resulting in a temporal hierarchy in the visual pathway. We also compared two different temporal scales (fast and slow) in the PFC layer. The time constant was set to 1 in the fast PFC condition whereas it was set to 150 in the\nslow PFC condition. In sum, there were 4 different network conditions examined in our experiments. Throughout the experiments, the time constants of the MS, MF and MO layers were fixed to 70, 2 and 1 respectively. The proper values for the time constant at each level of the model were found heuristically in our preliminary study [46].\nPrior to end-to-end learning, learnable parameters were initialized with the values acquired from the pre-training stage to enhance learning capability. During the pre-training stage, the model was trained to grasp an object without human gestures. Also, the visual pathway was additionally pre-trained to classify the four types of human gestures as described in Section III. During the end-to-end learning, the network was trained for 13,000 epochs with a learning rate of 0.01."}, {"heading": "C. Task", "text": "The objective of the task was to grasp the target object indicated by the human gesture displayed on the screen at the beginning of the task. The overall task flow was as follows. At the beginning of the task, the robot was set to the home position, orienting its head toward the screen in front of the robot and stretching both arms sideways. From the home position, the robot observed a human gesture being displayed on the screen (Fig. 2). After observing the gesture, the robot oriented its head to the task table on which two objects consisting of a long and a tall object were placed. While observing the task space, the robot figured out the target object indicated by the human gesture and oriented its head to the target object. Then, the robot reached out its right arm and grasped the target object. For example, when the human gesture indicated the tall object, the robot had to pick out the tall object from among the two available along with information about its orientation. Similarly, when the human gesture indicated the right side, the robot had to figure out the type and orientation of the object on the right side (see the supplementary video). This task, therefore, inherently required the robot to have working memory capability to maintain the human gesture information throughout the task phases and dynamically combine it with the perceived object properties.\nThe visuomotor sequence for each training trial consisted of images perceived from the robot\u2019s camera (visual observation), the values of robot\u2019s joint positions and the values of the grasping and foveation signals and they were collected simultaneously from the beginning to the end of tutoring. Consequently, the images in the visuomotor sequences included the ones that were perceived while observing the human gestures on the screen as well as the ones perceived while completing the behavioral task. Similarly, the values of the robot\u2019s joint positions in the visuomotor sequences included the ones that were recorded while observing the human gestures as well as the ones recorded while\nacting in the task space after the observation. The robot was trained with 200 trials consisting of varying object configurations and human gestures. Two objects consisting of one tall and one long object were presented in each trial and the location and the orientation of the two objects were controlled so that the way of presenting the two objects did not bias the robot\u2019s behavior toward certain reaching and grasping behaviors and away from others. Regarding human gestures, a target object was specified by one of four different human gestures indicating the location (either left or right) or the type (either tall or long) of the target object (Fig. 2 (a)). A 40-frame video clip of the human gesture was displayed on the screen in front of the robot. We collected several gesture trials from 7 human subjects and pseudo-randomly selected from amongst them so that each type of gesture appeared the same number of times in the training dataset. Throughout the experiments, two different types of box-shape objects were used: a tall object with a size of 2.8cm \u00d7 5cm \u00d7 10cm and a long object with a size of 2.8cm \u00d7 10cm \u00d7 5cm. Objects were placed with 5 different orientations (-45\u00b0, -22.5\u00b0, 0\u00b0, 22.5\u00b0, 45\u00b0) at 10 positions symmetrically distributed on the XY-plane of the task space (Fig. 2 (c)).\nDuring the testing stage, we evaluated the model\u2019s performance with respect to the learned trials (TR) as well as with a set of novel situations in order to examine the model\u2019s generalization capability. First, we tested the model with 60 trials in which two objects were randomly located (OBJ) to examine whether the robot was able to generalize reaching and grasping skills to unlearned object positions and orientations. We also examined the model\u2019s generalization capability with respect to human gestures, using 200 training trials with the gestures of a novel subject (SUB). Then, we examined the model\u2019s generalization capability with 60 cases of the most novel situation, in which randomly located objects were indicated by gestures of the novel subject (OBJ \u00d7 SUB). It should be noted that main focus was on the model\u2019s ability to coordinate cognitive skills such as gesture recognition and working memory rather than solely on gesture recognition, since the latter is more concerned with typical perceptual classification tasks. In addition, we evaluated the model under a visual occlusion experimental paradigm in which the vision input to the network was completely occluded. The main focus of this evaluation was to verify whether the network was equipped with a sort of internal memory, enabling it to show robust behavior even the visual information was unexpectedly and completely occluded. From the training trials (TR) and the testing trials (OBJ \u00d7 SUB), we selected 80 and 60 trials respectively. During the visual occlusion experiment, vision input to the network was occluded at the onset of observing the task space (t = 51), at the onset of attending to the target object (t = 55), at the onset of observing the target object (t = 61), at\nthe onset of reaching (t = 66), and during reaching (t = 86). Prior to the testing, the learnable parameters were initialized to the ones obtained from the training.\nV. RESULTS"}, {"heading": "A. Generalization Performances", "text": "Table 1 shows the success rate of each network condition. Each trial was evaluated as \u201csuccessful\u201d if the robot grasped and lifted an object and \u201cfailure\u201d otherwise. In general, the MSTNN vision with the slow PFC condition showed the better performance than did other conditions. In this condition, the model successfully learned the training trials (TR = 98.5%) and was able to generalize learned skills to different testing conditions. This model was able to generalize reaching and grasping skills when the objects were randomly located (OBJ = 85.0%), when the gestures of the novel subject were displayed (SUB = 96.5%), and when the randomly located objects were specified by the novel subject (OBJ \u00d7 SUB = 83.3%). It is worth noting that the model demonstrated relatively low success rates when in the fast PFC conditions, especially CNN vision with the fast PFC condition. The task required the robot to maintain human gesture information displayed at the beginning of the trial throughout all task phases. In the end, the model equipped with the temporal hierarchy performed significantly better than did those without it.\nWe analyzed failure cases in training trials (TR) and in the most novel trials (OBJ \u00d7 SUB). Particularly, we focused on task failure caused by apparent confusion between gesture-indicated and other objects. In these cases, the robot simply grasped the incorrect object. Table 2 shows the percentage of the error caused by confusion in the four network conditions. The model showed the least confusion errors in the MSTNN with slow PFC condition. Moreover, the error caused by confusion was more pronounced in fast PFC than in slow PFC conditions. This result implies that the model was able to maintain correct and stable representations by means of slow dynamics in the PFC layer.\nFig. 3 illustrates the success rate for each network condition with respect to (a) training trials (TR) and (b) testing trials (OBJ \u00d7 SUB) with different occlusion timings. When vision input to the model was occluded, performance of the\nfour different network conditions differed significantly. As expected, the model\u2019s performance in all conditions generally degraded when vision input was occluded during earlier task phases. Especially, the model showed relatively worse performance in the CNN vision with the fast PFC condition than other conditions. Therefore, it can be inferred that memory capability achieved by the slow dynamics subnetwork at the higher level plays an important role when vision input is occluded. That is, by means of memory, the robot was able to maintain information about human gestures as well as information about the target objects including position, type and orientation throughout the all phases of task execution. This result shows the importance of the internal contextual dynamics of the proposed model and highlights a difference between the proposed model and the previous study [13] which was prone to occlusion due to the lack of capability of keeping memory."}, {"heading": "B. Development of Internal Representation", "text": "In order to reveal the model\u2019s self-organized internal representation, we analyzed neural activation during training trials using the t-Distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction algorithm [47]. During t-SNE analysis, the parameters for the initial dimensions and the perplexity were set to 10 and 30 respectively. Fig. 4 depicts the internal representation emerging at each layer during the three different task phases. Each point indicates a single training trial and distances between those points represent relative similarity between trials. The color and the shape of each point denote the type of human gesture and the type of the object respectively. The number next to each point indicates the object\u2019s position.\nIn MSTNN layers (VF and VS), sequential visual images were abstracted in both temporal and spatial dimensions through hierarchical processing. For instance, where the robot begins observation of the task space, five clusters can be observed in the VF layer (b). These clusters correspond to the five possible pair locations employed during training.\nObjects appearing in the same position appeared in the same cluster regardless of the type of object and type of human gesture. In the VS layer, those representations were further separated, with the type of target object differentiated within the clusters reflecting the relative locations of the two objects (e). Similarly, in the mid-reaching phase, the VF layer (c) and the VS layer (f) encoded both the type and the location of the target object, but the distinction with respect to the object\u2019s type was less clear in the VF layer.\nTransitions of representations from those reflecting types of human gesture to those reflecting specific target objects organized in the PFC layer. After observing the gesture (g), four clusters reflecting the type of gesture appeared, suggesting that human gestures were successfully recognized. Then, those internal representations started to develop progressively, and smaller clusters indicating specific target objects emerged in the mid-reaching phase regardless of the presented human gesture (i). Our interpretation is that the higher-level of the model read the human\u2019s intention and translated it into robot\u2019s own intention to reach and to grasp the specified object. To be more specific, lower-level visual images containing human gestures were processed hierarchically by MSTNN subnetworks and the abstracted representation underlying one of the four human\u2019s intentions appeared in the higher-level (the PFC layer). Then, the robot simultaneously incorporated the perceived object\u2019s properties including location, type and orientation, and it formed its own intention for reaching and grasping the target object. In the end, this result suggests that the robot\u2019s intention was not explicitly mapped but arose dynamically from perceived information.\nIn the MS layer, four clusters representing each type of gesture appeared at the end of human gesture observation (j) and at the onset of target object observation (k). This implies that higher-level proprioception was calibrated based on the perceived human gestures before the robot exhibited different behaviors depending on the target object. At mid-reach (l), the MS layer encoded both the type and the location of the target object, but differences with respect to the object\u2019s location were less clear than those appearing in the PFC layer. In the MF layer, the internal representations developed similarly, but less clearly than those in the MS layer. For instance, the MF layer encoded both the type and the location of the object during mid-reach (o), but representations were less clear than those of the PFC (i) and MS (l).\nWe further analyzed the development of internal representations for exemplar cases in the PFC, MS and MF layers using the t-SNE algorithm (Fig. 5). A total number of 8 representative training trials consisting of 4 different target objects indicated by 2 different human gestures were compared. In the t-SNE analysis, the number of the initial dimensions and the perplexity were set to 10 and 50 respectively. Fig. 5 illustrates that the proposed model dynamically integrated perception and computed motor plans. The PFC layer developed higher-level task-related representations encompassing both visual and motor information whenever available. For instance, the PFC layer identified the type of gesture even before the end of human gesture presentation. While the robot was observing the gestures, the representations at the PFC layer already started to develop differently according to the type of gesture. When the robot started observing the task space, representations developed differently depending on the object\u2019s features, such as\nlocation and type. Analysis shows that the MS layer was calibrated based on the perceived human gesture. For example, when the robot started observing the task space, representations in the MS layer began to differentiate themselves depending on the perceived human gesture. The implication here is that higher-level proprioception was calibrated based on perceived visual information. The representations of the MF layer developed similarly to those of the MS layer, showing very similar development of representations for the same target object, but they were more closely related the robot\u2019s current action. In sum, the MS layer mainly encoded higher-level motor actions and mediated between higher-level cognition (PFC) and the lower-level motor actions (MF).\nIn order to clarify differences in the internal dynamics of the model when vision input was occluded, we compared two different timings in the two diametrically opposed cases: not occluded and occluded at the onset of reaching, in the CNN with the fast PFC condition and in the MSTNN with the slow PFC condition. The development of the first three principal components (PCs) of each layer for the same target object is depicted from the beginning to the end of the task in Fig. 6. In the CNN with fast PFC condition, the robot failed to grasp the target object when visual input was completely occluded at the onset of reaching. However, it was able to grasp the target object when the visual input was not occluded. Analysis clearly reveals that PCs in the PFC, MS and MF layers developed differently in both cases, implying that the model cannot form consistent representations. In the MSTNN with the slow PFC condition, the robot was able to grasp the target object in both cases. Analysis reveals that the development of PCs of the PFC, MS and MF layers in occluded and non-occluded cases were similar. Especially, when vision input was occluded at the onset of reaching, the activation of MS and MF developed similarly with those in the non-occluded case, implying the development of the coherent proprioceptive representations regardless of occlusion conditions. In sum, the PFC layer\nas well as MTRNN layers developed consistent representations even when visual information was lost at the initiation of a reaching action, resulting in successful task performance."}, {"heading": "VI. DISCUSSION", "text": "Throughout the experiments, we verified several key aspects of the proposed model. In this section, we discuss them\nin detail."}, {"heading": "A. Self-organized Coordinated Dynamic Structure", "text": "The proposed model developed a coordinated dynamic mechanism in the whole network, enabling the robot to learn goal-directed behaviors through the seamless coordination of cognitive skills. In terms of downward causation, spatio-temporal constraints imposed on the each level of the hierarchy and end-to-end learning performed on the tightly coupled structure are the two key factors in achieving the coordinated dynamic mechanism.\nFirst, multiple scales of spatio-temporal constraint enabled the model to dynamically compose conceptual representations in the higher level by assembling perceptual features in the lower level, suggesting the emergence of different cognitive functionalities in its hierarchy. Sporns [48] argues that cognitive functions develop in human brains through anatomical spatio-temporal constraints including connectivity and timescales among local regions. Similarly, different spatio-temporal constraints imposed on different parts of the proposed model lead to the development of different cognitive functionalities at each level of the hierarchy. For example, the PFC layer mainly encoded higher-level task-related information whereas the MTRNN layers encoded information related to the robot\u2019s current action. Specifically, lower-level proprioception layer (MF) developed the representation closely reflecting the robot\u2019s current action whereas higher-level proprioception layer (MS) played the role of mediating between higher-level cognition (PFC) and lower-level proprioception (MF). This result is analogous to findings in [49] that higher-level task-related information was encoded in the PFC and the lower-level arm movements were encoded in the primary motor cortex of monkeys.\nSecond, end-to-end learning performed on a tightly coupled structure enabled the model to form coordinated representations without explicitly encapsulating each perceptual modality, i.e. perception, action, and decision making. On the present model, the coupling of perception and action is achieved by connecting two different deep networks through the PFC layer. This coupling enabled the model to generate motor plans dynamically in response to visual\nperceptual flow. This finding is in line with the previous studies [13, 40] in which the importance of coupling perception and action was emphasized. Analysis of neural activation shows that this dynamic motor plan generation involved continuous integration of the visual perceptual flow, rather than being directly mapped from visual perception. This dynamic transformation of visual information into behavioral information was also observed in the experiments with macaque monkey\u2019s brains [50].\nMultimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52]. Ultimately, the present model was able to develop multimodal representations by abstracting and associating visual perception with proprioceptive information in different pathways. In other words, higher level representations were based not only on visual information about and object and corresponding human gesture, but also on information about movement trajectories resulting in the successful grasp of the target object."}, {"heading": "B. Memory and Pre-planning Capability", "text": "Results indicate that the proposed model is capable of developing and employing working memory. We found that the robot was able to maintain task-related information in higher levels throughout the task phases, and dynamically combining it with object percepts. For instance, the robot maintained human intention categorized at the beginning of the task and combined it with the object percept, so that it could reach and grasp the target object. Furthermore, the proposed model performed robustly even when visual input was completely and unexpectedly occluded. This memory capability was achieved by the temporal hierarchy of the model as well as by the recurrent connections of the PFC layer. Particularly, when the time constants of the higher-level were larger than those of the lower-level, the model showed the most robust performance in the various circumstances including the experiments with the novel object configurations as well as the ones with unexpected visual occlusion. Although the suitable values of time constant at each level might differ depending on the task, the finding of our study suggests that the progressively larger time constants from the low-level to the higher-level of the architecture played an important role to form the coordinated dynamic structure including a memory capability. This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN. This internal contextual dynamics of the proposed model highlights a key difference between the proposed model and the previous study [13] which lacked capability for keeping memory.\nFurthermore, the model was able to pre-plan an action prior to action execution. More specifically, neuronal activation in the higher-level proprioception layer (MS) was calibrated based on the perceived visual images prior to action execution. This pre-planning capability played a particularly important role in the visual occlusion experiment, enabling the robot to reach and grasp the target object even without monitoring the target object and the hand during reaching. This result is analogous to findings in [53] which reported that F5 neurons in the brain of the macaque monkey encoded grip-specific information even when no movement was intended."}, {"heading": "VII. CONCLUSION", "text": "The current study introduced the Visuo-Motor Deep Dynamic Neural Network (VMDNN) model which can learn to read human intention and generate corresponding behaviors in robots by coordinating multiple cognitive processes including visual recognition, attention switching, memorizing and retrieving with working memory, action preparation and generation in a seamless manner. The simulation study on the model using the iCub simulator revealed that the robot could categorize human intention through observing gestures, preserve the contextual information, and execute corresponding goal-directed actions. The analysis further showed that synergic coordination among these cognitive processes can be developed when end-to-end learning of tutored experience is performed on the whole network, allowing dense interaction between subnetworks. In conclusion, the aforementioned cognitive mechanism can be developed by means of downward causation in terms of spatio-temporal scale differentiation among local subnetworks, topological connectivity among them, and the way of interacting through sensory-motor coupling.\nThere are several research directions suggested by this study. First, experiments incorporating other cognitive skills as well as other sensory modalities need to be conducted for a better understanding of the mechanisms of learning goal-directed actions in human and other biological neural systems. Second, the scalability of the proposed model will need to be examined in experiments with a real robot and with a larger variety of objects. The complexity of the task that the model can perform seems proportional to the size of the learnable parameters including the number of layers in each subnetwork and the number of neurons in each layer. Third, the robot\u2019s sensorimotor experience in our study was acquired from tutoring by the experimenter. In some cases, tutoring may be impossible, so it would be worth investigating how such experience can be obtained autonomously."}], "references": [{"title": "Towards Deep Developmental Learning", "author": ["O. Sigaud", "A. Droniou"], "venue": "IEEE Transactions on Cognitive and Developmental Systems, vol. 8, pp. 99-114, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep unsupervised network for multimodal perception, representation and classification", "author": ["A. Droniou", "S. Ivaldi", "O. Sigaud"], "venue": "Robotics and Autonomous Systems, vol. 71, pp. 83-98, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A Survey of the Ontogeny of Tool Use: From Sensorimotor Experience to Planning", "author": ["F. Guerin", "N. Kruger", "D. Kraft"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 5, pp. 18-45, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Developmental robotics: a survey", "author": ["M. Lungarella", "G. Metta", "R. Pfeifer", "G. Sandini"], "venue": "Connection Science, vol. 15, pp. 151-190, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Developmental robotics: From babies to robots", "author": ["A. Cangelosi", "M. Schlesinger", "L.B. Smith"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Epigenetic robotics\u2014modelling cognitive development in robotic systems", "author": ["L. Berthouze", "T. Ziemke"], "venue": "Connection Science, vol. 15, pp. 147-150, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Cognitive developmental robotics as a new paradigm for the design of humanoid robots", "author": ["M. Asada", "K.F. MacDorman", "H. Ishiguro", "Y. Kuniyoshi"], "venue": "Robotics and Autonomous Systems, vol. 37, pp. 185-193, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Symbol emergence in robotics: a survey", "author": ["T. Taniguchi", "T. Nagai", "T. Nakamura", "N. Iwahashi", "T. Ogata", "H. Asoh"], "venue": "Advanced Robotics, vol. 30, pp. 706-728, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Self-Organization and Compositionality in Cognitive Brains: A Neurorobotics Study", "author": ["J. Tani"], "venue": "Proceedings of the IEEE, vol. 102, pp. 586-605, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, pp. 1798-1828, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1828}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, pp. 436-444, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, vol. 61, pp. 85-117, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "The International Journal of Robotics Research, vol. 34, pp. 705-724, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A Deep Learning Neural Network for Number Cognition: A bi-cultural study with the iCub", "author": ["A. Di Nuovo", "V.M. De La Cruz", "A. Cangelosi"], "venue": "2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob), 2015, pp. 320-325.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "From pixels to torques: Policy learning with deep dynamical models", "author": ["N. Wahlstr\u00f6m", "T.B. Sch\u00f6n", "M.P. Deisenroth"], "venue": "arXiv preprint arXiv:1502.02251, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-Organization of Spatio-Temporal Hierarchy via Learning of Dynamic Visual Image Patterns on Action Sequences", "author": ["M. Jung", "J. Hwang", "J. Tani"], "venue": "PLoS ONE, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Emergence of functional hierarchy in a multiple timescale neural network model: a humanoid robot experiment", "author": ["Y. Yamashita", "J. Tani"], "venue": "PLoS Computational Biology, vol. 4, p. e1000220, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Incremental Learning in a 14 DOF Simulated iCub Robot: Modeling Infant Reach/Grasp Development", "author": ["P. Savastano", "S. Nolfi"], "venue": "Biomimetic and Biohybrid Systems: First International Conference, Living Machines 2012, Barcelona, Spain, July 9-12, 2012. Proceedings, T. J. Prescott, N. F. Lepora, A. Mura, and P. F. M. J. Verschure, Eds., ed Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 250-261.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "A Robotic Model of Reaching and Grasping Development", "author": ["P. Savastano", "S. Nolfi"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 5, pp. 326-336, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Letting structure emerge: connectionist and dynamical systems approaches to cognition", "author": ["J.L. McClelland", "M.M. Botvinick", "D.C. Noelle", "D.C. Plaut", "T.T. Rogers", "M.S. Seidenberg"], "venue": "Trends in Cognitive Sciences, vol. 14, pp. 348-356, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "The basis of shared intentions in human and robot cognition", "author": ["P.F. Dominey", "F. Warneken"], "venue": "New Ideas in Psychology, vol. 29, pp. 260-274, 12// 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding and sharing intentions: the origins of cultural cognition", "author": ["M. Tomasello", "M. Carpenter", "J. Call", "T. Behne", "H. Moll"], "venue": "Behav Brain Sci, vol. 28, pp. 675-91; discussion 691-735, Oct 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "The Role of Intention in Cognitive Robotics", "author": ["D. Vernon", "S. Thill", "T. Ziemke"], "venue": "Toward Robotic Socially Believable Behaving Systems - Volume I : Modeling Emotions, A. Esposito and C. L. Jain, Eds., ed Cham: Springer International Publishing, 2016, pp. 15-27.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "How infants use vision for grasping objects", "author": ["M.E. McCarty", "R.K. Clifton", "D.H. Ashmead", "P. Lee", "N. Goubet"], "venue": "Child Dev, vol. 72, pp. 973-87, Jul-Aug 2001.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "The effect of visual feedback of the hand on the reaching and retrieval behavior of young infants", "author": ["R.E. Lasky"], "venue": "Child Dev, vol. 48, pp. 112-7, Mar 1977.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1977}, {"title": "Infant grasp learning: a computational model", "author": ["E. Oztop", "N.S. Bradley", "M.A. Arbib"], "venue": "Exp Brain Res, vol. 158, pp. 480-503, Oct 2004.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Self-discovery of motor primitives and learning grasp affordances", "author": ["E. Ugur", "E. \u015eahin", "E. Oztop"], "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012, pp. 3260-3267.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Staged Development of Robot Skills: Behavior Formation, Affordance Learning and Imitation with Motionese", "author": ["E. Ugur", "Y. Nagai", "E. Sahin", "E. Oztop"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 7, pp. 119-139, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Human motion based intent recognition using a deep dynamic neural model", "author": ["Z. Yu", "M. Lee"], "venue": "Robotics and Autonomous Systems, vol. 71, pp. 134-149, 9// 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours", "author": ["L. Pinto", "A. Gupta"], "venue": "arXiv preprint arXiv:1509.06825, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal integration learning of robot behavior using deep neural networks", "author": ["K. Noda", "H. Arie", "Y. Suga", "T. Ogata"], "venue": "Robotics and Autonomous Systems, vol. 62, pp. 721-736, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Development of Compositional and Contextual Communication of Robots by Using the Multiple Timescales Dynamic Neural Network", "author": ["G. Park", "J. Tani"], "venue": "presented at the The 5th Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics, Rhode Island, USA, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Development of hierarchical structures for actions and motor imagery: a constructivist view from synthetic neuro-robotics study", "author": ["R. Nishimoto", "J. Tani"], "venue": "Psychol Res, vol. 73, pp. 545-58, Jul 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097-1105.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple spatio-temporal scales neural network for contextual visual recognition of human actions", "author": ["M. Jung", "J. Hwang", "J. Tani"], "venue": "2014 Joint IEEE International Conferences on Development and Learning and Epigenetic Robotics (ICDL-Epirob), 2014, pp. 235-241.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Neuro-robotics study on integrative learning of proactive visual attention and motor behaviors", "author": ["S. Jeong", "H. Arie", "M. Lee", "J. Tani"], "venue": "Cognitive Neurodynamics, vol. 6, pp. 43-59, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent Slow Feature Analysis for Developing Object Permanence in Robots", "author": ["H. Celikkanat", "E. Sahin", "S. Kalkan"], "venue": "presented at the IROS 2013 Workshop on Neuroscience and Robotics, Tokyo, Japan, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Neurological basis of language and sequential cognition: evidence from simulation, aphasia, and ERP studies", "author": ["P.F. Dominey", "M. Hoen", "J.M. Blanc", "T. Lelekov-Boissard"], "venue": "Brain Lang, vol. 86, pp. 207-25, Aug 2003.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2003}, {"title": "Beyond Gazing, Pointing, and Reaching: A Survey of Developmental Robotics", "author": ["M. Lungarella", "G. Metta"], "venue": "vol. 101, C. G. Prince, L. Berthouze, H. Kozima, D. Bullock, G. Stojanov, and C. Balkenius, Eds., ed: Lund University Cognitive Studies, 2003, pp. 81-89.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural Networks: Tricks of the Trade. vol. 7700, G. Montavon, G. B. Orr, and K.-R. M\u00fcller, Eds., ed: Springer Berlin Heidelberg, 2012, pp. 9-48.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems, vol. 19, p. 153, 2007.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "iCub: the design and realization of an open humanoid platform for cognitive and neuroscience research", "author": ["N.G. Tsagarakis", "G. Metta", "G. Sandini", "D. Vernon", "R. Beira", "F. Becchi"], "venue": "Advanced Robotics, vol. 21, pp. 1151-1175, 2007.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "An open-source simulator for cognitive robotics research: the prototype of the iCub humanoid robot simulator", "author": ["V. Tikhanoff", "A. Cangelosi", "P. Fitzpatrick", "G. Metta", "L. Natale", "F. Nori"], "venue": "presented at the Proceedings of the 8th Workshop on Performance Metrics for Intelligent Systems, Gaithersburg, Maryland, 2008.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Achieving \"synergy\" in cognitive behavior of humanoids via deep learning of dynamic visuo-motor-attentional coordination", "author": ["J. Hwang", "M. Jung", "N. Madapana", "J. Kim", "M. Choi", "J. Tani"], "venue": "2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids), 2015, pp. 817-824.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, p. 85, 2008.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}, {"title": "Networks of the Brain", "author": ["O. Sporns"], "venue": "Cambridge, MA: MIT press,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "Activity in the lateral prefrontal cortex reflects multiple steps of future events in action plans", "author": ["H. Mushiake", "N. Saito", "K. Sakamoto", "Y. Itoyama", "J. Tanji"], "venue": "Neuron, vol. 50, pp. 631-41, May 18 2006.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}, {"title": "Linking Objects to Actions: Encoding of Target Object and Grasping Strategy in Primate Ventral Premotor Cortex", "author": ["C.E. Vargas-Irwin", "L. Franquemont", "M.J. Black", "J.P. Donoghue"], "venue": "The Journal of Neuroscience, vol. 35, pp. 10888-10897, 2015.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Convergence and divergence in a neural architecture for recognition and memory", "author": ["K. Meyer", "A. Damasio"], "venue": "Trends Neurosci, vol. 32, pp. 376-82, Jul 2009.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "ECA: An enactivist cognitive architecture based on sensorimotor modeling", "author": ["O.L. Georgeon", "J.B. Marshall", "R. Manzotti"], "venue": "Biologically Inspired Cognitive Architectures, vol. 6, pp. 46-57, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Decoding the activity of grasping neurons recorded from the ventral premotor area F5 of the macaque monkey", "author": ["J. Carpaneto", "M.A. Umilta", "L. Fogassi", "A. Murata", "V. Gallese", "S. Micera"], "venue": "Neuroscience, vol. 188, pp. 80-94, Aug 11 2011.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 1, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 2, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 3, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 4, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 5, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 6, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 7, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 8, "context": "Furthermore, these skills ideally arise from the robot\u2019s experience (of reaching for and grasping objects, for example), rather than from hand-engineered features reflecting a human engineer\u2019s understanding of what any given task may require [1-9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 9, "context": "Deep learning is a fast-growing field in machine learning and artificial intelligence with remarkable advances, such as text recognition, speech recognition, image recognition and many others (See [10-12] for recent reviews of deep learning).", "startOffset": 197, "endOffset": 204}, {"referenceID": 10, "context": "Deep learning is a fast-growing field in machine learning and artificial intelligence with remarkable advances, such as text recognition, speech recognition, image recognition and many others (See [10-12] for recent reviews of deep learning).", "startOffset": 197, "endOffset": 204}, {"referenceID": 11, "context": "Deep learning is a fast-growing field in machine learning and artificial intelligence with remarkable advances, such as text recognition, speech recognition, image recognition and many others (See [10-12] for recent reviews of deep learning).", "startOffset": 197, "endOffset": 204}, {"referenceID": 0, "context": "One of the most important characteristics of deep learning is that deep networks can autonomously extract task-related features in high-dimensional data, such as images and action sequences, without the necessity of hand-engineered feature extraction methods [1, 10, 12].", "startOffset": 259, "endOffset": 270}, {"referenceID": 9, "context": "One of the most important characteristics of deep learning is that deep networks can autonomously extract task-related features in high-dimensional data, such as images and action sequences, without the necessity of hand-engineered feature extraction methods [1, 10, 12].", "startOffset": 259, "endOffset": 270}, {"referenceID": 11, "context": "One of the most important characteristics of deep learning is that deep networks can autonomously extract task-related features in high-dimensional data, such as images and action sequences, without the necessity of hand-engineered feature extraction methods [1, 10, 12].", "startOffset": 259, "endOffset": 270}, {"referenceID": 0, "context": "So, deep learning provides an important tool for robotics, because through deep learning a robot can learn directly from its huge-dimensional sensorimotor data acquired through dynamic interaction with the environment [1].", "startOffset": 218, "endOffset": 221}, {"referenceID": 12, "context": "A few recent studies [13-16] have demonstrated the plausibility of deep learning in the field of robotics.", "startOffset": 21, "endOffset": 28}, {"referenceID": 13, "context": "A few recent studies [13-16] have demonstrated the plausibility of deep learning in the field of robotics.", "startOffset": 21, "endOffset": 28}, {"referenceID": 14, "context": "A few recent studies [13-16] have demonstrated the plausibility of deep learning in the field of robotics.", "startOffset": 21, "endOffset": 28}, {"referenceID": 15, "context": "A few recent studies [13-16] have demonstrated the plausibility of deep learning in the field of robotics.", "startOffset": 21, "endOffset": 28}, {"referenceID": 0, "context": "Also, Sigaud and Droniou [1] have pointed out that it is still unclear how higher-level representations can be built by stacking several networks.", "startOffset": 25, "endOffset": 28}, {"referenceID": 16, "context": "The VMDNN is composed of three different types of subnetwork: the Multiple Spatio-Temporal scales Neural Network (MSTNN) [17], the Multiple Timescales Recurrent Neural Network (MTRNN) [18] and the PFC (Prefrontal Cortex) subnetworks.", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "The VMDNN is composed of three different types of subnetwork: the Multiple Spatio-Temporal scales Neural Network (MSTNN) [17], the Multiple Timescales Recurrent Neural Network (MTRNN) [18] and the PFC (Prefrontal Cortex) subnetworks.", "startOffset": 184, "endOffset": 188}, {"referenceID": 16, "context": "The MSTNN has demonstrated an ability to recognize dynamic visual scenes [17], and the MTRNN to learn compositional actions [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "The MSTNN has demonstrated an ability to recognize dynamic visual scenes [17], and the MTRNN to learn compositional actions [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "This approach is based on the previous studies which emphasized the importance of perception-action coupling in robotic manipulation [13] as well as in developmental robotics [40].", "startOffset": 133, "endOffset": 137}, {"referenceID": 39, "context": "This approach is based on the previous studies which emphasized the importance of perception-action coupling in robotic manipulation [13] as well as in developmental robotics [40].", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "Here, it is worth noting that artificial neural networks are meant to model the essential features of the nervous system, not its detailed implementation [19-21].", "startOffset": 154, "endOffset": 161}, {"referenceID": 19, "context": "Here, it is worth noting that artificial neural networks are meant to model the essential features of the nervous system, not its detailed implementation [19-21].", "startOffset": 154, "endOffset": 161}, {"referenceID": 20, "context": "Here, it is worth noting that artificial neural networks are meant to model the essential features of the nervous system, not its detailed implementation [19-21].", "startOffset": 154, "endOffset": 161}, {"referenceID": 4, "context": "Moreover, reading intention of others by observing their behavior has been considered as one of the core abilities required for social cognition [5, 22-24].", "startOffset": 145, "endOffset": 155}, {"referenceID": 21, "context": "Moreover, reading intention of others by observing their behavior has been considered as one of the core abilities required for social cognition [5, 22-24].", "startOffset": 145, "endOffset": 155}, {"referenceID": 22, "context": "Moreover, reading intention of others by observing their behavior has been considered as one of the core abilities required for social cognition [5, 22-24].", "startOffset": 145, "endOffset": 155}, {"referenceID": 23, "context": "Moreover, reading intention of others by observing their behavior has been considered as one of the core abilities required for social cognition [5, 22-24].", "startOffset": 145, "endOffset": 155}, {"referenceID": 4, "context": "In addition, reaching and grasping are fundamental skills that have significant influences on the development of perceptual and cognitive abilities [5].", "startOffset": 148, "endOffset": 151}, {"referenceID": 24, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 81, "endOffset": 89}, {"referenceID": 25, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 81, "endOffset": 89}, {"referenceID": 18, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 113, "endOffset": 128}, {"referenceID": 19, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 113, "endOffset": 128}, {"referenceID": 26, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 113, "endOffset": 128}, {"referenceID": 27, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 113, "endOffset": 128}, {"referenceID": 28, "context": "The reaching-and-grasping task has been extensively studied in child development [25, 26] as well as in robotics [19, 20, 27-29].", "startOffset": 113, "endOffset": 128}, {"referenceID": 13, "context": "In a robotic context, they require robust perception and action systems as well as simultaneous coordination of a set of cognitive skills, making hand-designing features demanding and time-consuming [14].", "startOffset": 199, "endOffset": 203}, {"referenceID": 0, "context": "Due to the remarkable success of deep learning in various fields, recent studies have attempted to employ deep learning in the field of robotics (See [1] for a recent review).", "startOffset": 150, "endOffset": 153}, {"referenceID": 14, "context": "[15] employed a deep neural network architecture in order to study number cognition in a humanoid robot.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] introduced a deep network architecture which could learn from different sensory modalities, including vision, audition and proprioception.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "Yu and Lee [30] employed a deep learning approach on reading human intention.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "[14] proposed a two-stage cascaded detection system to detect robotic grasps in an RGB-D view of a scene and conducted experiments on different robotic platforms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Pinto and Gupta [31] also addressed the problem of detecting robotic grasps, adopting a convolutional neural network (CNN) model to predict grasp location and angle.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "[16], addressed the pixels-to-torques problem by introducing a reinforcement learning algorithm that enabled their agent to learn control policy from pixel information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] introduced a deep auto-encoder-based computational framework designed to integrate sensorimotor data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed a deep neural network model which learned a control policy that linked raw image percepts to motor torques of the robot.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Park and Tani [33] investigated how a robot could infer the underlying intention of human gestures and generate corresponding behaviors of a humanoid robot.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "Several problems confront ongoing deep-learning research in robotics applications, as represented by limitations in existing studies such as relatively simple testing platform [16, 33], separate processing of individual modalities [32], and inability to handle temporal information [13].", "startOffset": 176, "endOffset": 184}, {"referenceID": 32, "context": "Several problems confront ongoing deep-learning research in robotics applications, as represented by limitations in existing studies such as relatively simple testing platform [16, 33], separate processing of individual modalities [32], and inability to handle temporal information [13].", "startOffset": 176, "endOffset": 184}, {"referenceID": 31, "context": "Several problems confront ongoing deep-learning research in robotics applications, as represented by limitations in existing studies such as relatively simple testing platform [16, 33], separate processing of individual modalities [32], and inability to handle temporal information [13].", "startOffset": 231, "endOffset": 235}, {"referenceID": 12, "context": "Several problems confront ongoing deep-learning research in robotics applications, as represented by limitations in existing studies such as relatively simple testing platform [16, 33], separate processing of individual modalities [32], and inability to handle temporal information [13].", "startOffset": 282, "endOffset": 286}, {"referenceID": 14, "context": "Second, the model processes dynamic visuomotor patterns in a hierarchical structure essential to cortical computation [15, 34].", "startOffset": 118, "endOffset": 126}, {"referenceID": 33, "context": "Second, the model processes dynamic visuomotor patterns in a hierarchical structure essential to cortical computation [15, 34].", "startOffset": 118, "endOffset": 126}, {"referenceID": 34, "context": "The MSTNN is an extended Convolutional Neural Network (CNN) [35] employing leaky integrator neural units with different time constants [17, 36].", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "The MSTNN is an extended Convolutional Neural Network (CNN) [35] employing leaky integrator neural units with different time constants [17, 36].", "startOffset": 135, "endOffset": 143}, {"referenceID": 35, "context": "The MSTNN is an extended Convolutional Neural Network (CNN) [35] employing leaky integrator neural units with different time constants [17, 36].", "startOffset": 135, "endOffset": 143}, {"referenceID": 16, "context": "Unlike conventional CNN models that utilize spatial constraints only, the MSTNN model has been shown that it can process both spatial and temporal patterns by imposing multiple spatio-temporal scales constraints on local neural activity [17, 36].", "startOffset": 237, "endOffset": 245}, {"referenceID": 35, "context": "Unlike conventional CNN models that utilize spatial constraints only, the MSTNN model has been shown that it can process both spatial and temporal patterns by imposing multiple spatio-temporal scales constraints on local neural activity [17, 36].", "startOffset": 237, "endOffset": 245}, {"referenceID": 17, "context": "The MTRNN is a hierarchical neural network model consisting of a multiple continuous time recurrent neural networks with leaky integrator neurons [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "Due to this temporal hierarchy, an MTRNN can learn compositional action sequences as a meaningful functional hierarchy emerges within the system [18, 34, 37].", "startOffset": 145, "endOffset": 157}, {"referenceID": 33, "context": "Due to this temporal hierarchy, an MTRNN can learn compositional action sequences as a meaningful functional hierarchy emerges within the system [18, 34, 37].", "startOffset": 145, "endOffset": 157}, {"referenceID": 36, "context": "Due to this temporal hierarchy, an MTRNN can learn compositional action sequences as a meaningful functional hierarchy emerges within the system [18, 34, 37].", "startOffset": 145, "endOffset": 157}, {"referenceID": 17, "context": "Consequently, the entire behavior of the robot including reaching and grasping as well as visual attention control can be decomposed into a set of primitives for their flexible recombination adapting to various situations [18].", "startOffset": 222, "endOffset": 226}, {"referenceID": 37, "context": "As a result, the PFC subnetwork exhibits the slowest-scale dynamics and this enables the PFC subnetwork to carry more information about a situation [38].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "recurrent connections which are essential to handle dynamic sequential data [1, 9, 39].", "startOffset": 76, "endOffset": 86}, {"referenceID": 8, "context": "recurrent connections which are essential to handle dynamic sequential data [1, 9, 39].", "startOffset": 76, "endOffset": 86}, {"referenceID": 38, "context": "recurrent connections which are essential to handle dynamic sequential data [1, 9, 39].", "startOffset": 76, "endOffset": 86}, {"referenceID": 39, "context": "Lungarella and Metta [40] argued that perception and action are not separated but tightly coupled, with this coupling is gradually getting refined during developmental process.", "startOffset": 21, "endOffset": 25}, {"referenceID": 40, "context": "Please note that the hyperbolic tangent recommended in [41] was used as an activation function to enhance convergence.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "Studies have demonstrated that pre-training is an efficient method for initializing network parameters [10, 13, 43].", "startOffset": 103, "endOffset": 115}, {"referenceID": 12, "context": "Studies have demonstrated that pre-training is an efficient method for initializing network parameters [10, 13, 43].", "startOffset": 103, "endOffset": 115}, {"referenceID": 41, "context": "Studies have demonstrated that pre-training is an efficient method for initializing network parameters [10, 13, 43].", "startOffset": 103, "endOffset": 115}, {"referenceID": 12, "context": "The pre-training method in our study is similar to that of [13] in which the visual part of the model was pre-trained prior to the end-to-end learning phase of the experiment.", "startOffset": 59, "endOffset": 63}, {"referenceID": 35, "context": "In this condition, the system operates as an MSTNN model, and it was trained as a typical classifier using BPTT as described in [36].", "startOffset": 128, "endOffset": 132}, {"referenceID": 34, "context": "[35] with the weight decay rate of 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "iCub [44] is a child-like humanoid robot consisting of 53 degrees of freedom (DOFs) distributed in the body.", "startOffset": 5, "endOffset": 9}, {"referenceID": 43, "context": "We used a simulation of the iCub [45] in our experiments.", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "The iCub simulator accurately models the actual robot\u2019s physical interaction with the environment, making it an adequate research platform for studying developmental robotics [15, 45].", "startOffset": 175, "endOffset": 183}, {"referenceID": 43, "context": "The iCub simulator accurately models the actual robot\u2019s physical interaction with the environment, making it an adequate research platform for studying developmental robotics [15, 45].", "startOffset": 175, "endOffset": 183}, {"referenceID": 43, "context": "Then, the interfacing program operated the robot based on the VMDNN model\u2019s outputs using the motor controller provided in the iCub software package [45].", "startOffset": 149, "endOffset": 153}, {"referenceID": 19, "context": "In addition, the network output the level of extension or flexion of finger joints in order to control grasping similar to [20].", "startOffset": 123, "endOffset": 127}, {"referenceID": 36, "context": "In our experiments, the VMDNN model also controlled visual attention which is essential to generate adequate robot\u2019s behavior [37].", "startOffset": 126, "endOffset": 130}, {"referenceID": 36, "context": "[37] demonstrated that MTRNN could seamlessly coordinate visual attention and motor behaviors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "The structure of the VMDNN model used in this study was found empirically in our preliminary experiments [46].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Note that the structure of the VMDNN model including the number of layers in each subnetwork can be extended depending on the complexity of the task since the \u2018deeper\u2019 structure can enhance learning of complex functions in visuomotor patterns [11].", "startOffset": 243, "endOffset": 247}, {"referenceID": 16, "context": "For instance, more number of layers in the MSTNN subnetwork can be employed to process more complex visual images [17].", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Similarly, more complex robot\u2019s behavior can be learned by employing more number of layers in the MTRNN subnetwork as reported in [33].", "startOffset": 130, "endOffset": 134}, {"referenceID": 44, "context": "The proper values for the time constant at each level of the model were found heuristically in our preliminary study [46].", "startOffset": 117, "endOffset": 121}, {"referenceID": 12, "context": "This result shows the importance of the internal contextual dynamics of the proposed model and highlights a difference between the proposed model and the previous study [13] which was prone to occlusion due to the lack of capability of keeping memory.", "startOffset": 169, "endOffset": 173}, {"referenceID": 45, "context": "In order to reveal the model\u2019s self-organized internal representation, we analyzed neural activation during training trials using the t-Distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction algorithm [47].", "startOffset": 221, "endOffset": 225}, {"referenceID": 46, "context": "Sporns [48] argues that cognitive functions develop in human brains through anatomical spatio-temporal constraints including connectivity and timescales among local regions.", "startOffset": 7, "endOffset": 11}, {"referenceID": 47, "context": "This result is analogous to findings in [49] that higher-level task-related information was encoded in the PFC and the lower-level arm movements were encoded in the primary motor cortex of monkeys.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "This finding is in line with the previous studies [13, 40] in which the importance of coupling perception and action was emphasized.", "startOffset": 50, "endOffset": 58}, {"referenceID": 39, "context": "This finding is in line with the previous studies [13, 40] in which the importance of coupling perception and action was emphasized.", "startOffset": 50, "endOffset": 58}, {"referenceID": 48, "context": "This dynamic transformation of visual information into behavioral information was also observed in the experiments with macaque monkey\u2019s brains [50].", "startOffset": 144, "endOffset": 148}, {"referenceID": 2, "context": "Multimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "Multimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52].", "startOffset": 185, "endOffset": 199}, {"referenceID": 3, "context": "Multimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52].", "startOffset": 185, "endOffset": 199}, {"referenceID": 49, "context": "Multimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52].", "startOffset": 185, "endOffset": 199}, {"referenceID": 50, "context": "Multimodal representation helps with distinguishing object type and orientation [3] and competency for multimodal information integration is considered essential for embodied cognition [2, 4, 51, 52].", "startOffset": 185, "endOffset": 199}, {"referenceID": 8, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 16, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 17, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 32, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 33, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 35, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 36, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 44, "context": "This finding is also in consistency with the other previous studies [9, 17-18, 33-34, 36-37, 46] that have shown the importance of a similar temporal hierarchy in the multiple timescales neural network such as MSTNN and MTRNN.", "startOffset": 68, "endOffset": 96}, {"referenceID": 12, "context": "This internal contextual dynamics of the proposed model highlights a key difference between the proposed model and the previous study [13] which lacked capability for keeping memory.", "startOffset": 134, "endOffset": 138}, {"referenceID": 51, "context": "This result is analogous to findings in [53] which reported that F5 neurons in the brain of the macaque monkey encoded grip-specific information even when no movement was intended.", "startOffset": 40, "endOffset": 44}], "year": 2017, "abstractText": "This study investigates how adequate coordination among the different cognitive processes of a humanoid robot can be developed through end-to-end learning of direct perception of visuomotor stream. We propose a deep dynamic neural network model built on a dynamic vision network, a motor generation network, and a higher-level network. The proposed model was designed to process and to integrate direct perception of dynamic visuomotor patterns in a hierarchical model characterized by different spatial and temporal constraints imposed on each level. We conducted synthetic robotic experiments in which a robot learned to read human's intention through observing the gestures and then to generate the corresponding goal-directed actions. Results verify that the proposed model is able to learn the tutored skills and to generalize them to novel situations. The model showed synergic coordination of perception, action and decision making, and it integrated and coordinated a set of cognitive skills including visual perception, intention reading, attention switching, working memory, action preparation and execution in a seamless manner. Analysis reveals that coherent internal representations emerged at each level of the hierarchy. Higher-level representation reflecting actional intention developed by means of continuous integration of the lower-level visuo-proprioceptive stream.", "creator": "Microsoft\u00ae Word 2010"}}}