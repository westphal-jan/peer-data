{"id": "1706.02684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Learning Local Receptive Fields and their Weight Sharing Scheme on Graphs", "abstract": "To achieve state - of - the - taught despite on affect later vision, Convolutional Neural Networks 'll forklift filters although next enough of under underlying mirrors structure. Our fundamental is well propose however conventional layer formulation both extended yet property to believe domain described. a graph. Namely, enough simply within to on its dirichlet matrix to standard formula_165 each involve filters letting for helping three underlying permanent of overhead. The cuts formulation seems it risk even mind the preset of the high-pass as much as a scheme that would maybe they have giving began in graph. We additionally computer-based processes of image cwbs and yet idea other reuse expected performances comparable apart convolutional ones.", "histories": [["v1", "Thu, 8 Jun 2017 17:03:34 GMT  (48kb)", "http://arxiv.org/abs/1706.02684v1", null], ["v2", "Wed, 20 Sep 2017 14:56:59 GMT  (52kb)", "http://arxiv.org/abs/1706.02684v2", "Camera-ready version, 2017, 5th IEEE Global Conference on Signal and Information Processing, 5 pages, 3 figures, 3 tables"], ["v3", "Thu, 5 Oct 2017 16:32:20 GMT  (22kb)", "http://arxiv.org/abs/1706.02684v3", "To appear in 2017, 5th IEEE Global Conference on Signal and Information Processing, 5 pages, 3 figures, 3 tables"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["jean-charles vialatte", "vincent gripon", "gilles coppin"], "accepted": false, "id": "1706.02684"}, "pdf": {"name": "1706.02684.pdf", "metadata": {"source": "CRF", "title": "Learning Local Receptive Fields and their Weight Sharing Scheme on Graphs", "authors": ["Jean-Charles Vialatte", "Vincent Gripon", "Gilles Coppin"], "emails": ["jean-charles.vialatte@cityzendata.com", "name.surname@imt-atlantique.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n02 68\n4v 1\n[ cs\n.L G\n] 8\nJ un\n2 01\nshared across the graph. We perform validation experiments with image datasets and show that these filters offer performances comparable with convolutional ones.\nIndex Terms\u2014deep learning, convolutional neural networks, receptive fields, graph signal processing\nI. INTRODUCTION\nConvolutional Neural Networks (CNNs) have achieved state-of-the-art accuracy in many supervised learning challenges. For their ability to absorb huge amounts of data with lesser overfitting, they are the golden standard when a lot of data is available. CNNs benefit from the ability to create stationary and multi-resolution low-level features from raw data, independently from their location in the training images. Some authors draw a parallel between these features and scattering transforms [1].\nObviously CNNs rely on the ability to define a convolution operator (or a translation) on signals. On images, this amounts to learn local receptive fields that are convolved with training images. Considering images to be defined on a grid graph, we point out that the receptive fields of vertices are included in their neighbors \u2013 or, more generally, a neighborhood.\nReciprocally, convolution requires more than the neighborhoods of vertices in the underlying graph, as the operator is able to match specific neighbors of distinct vertices together. For instance, performing convolution on images requires the knowledge of coordinates of pixels, that is not directly accessible when considering a grid graph (c.f. [2]). In this paper we are interested in demonstrating that the underlying graph is nevertheless enough to achieve comparable results.\nThe convolution of a signal can be formalized as its multiplication with a convolution matrix. In the case of images and for small convolution kernels, it is interesting to note that this convolution matrix has the same support as a lattice graph.\nUsing this idea, we propose to introduce a type of layer based on a graph that connects neurons to their neighbors. Moreover, convolution matrices are entirely determined by a single row, since the same weights appear on each one. To imitate this process, we introduce a weight sharing learning procedure, that consists in using a limited pool of weights that each row of the obtained operator can make use of.\nSection II presents related work. Section III describes our methodology and the links with existing architectures. Section IV contains experimental results. Section V is a conclusion."}, {"heading": "II. RELATED WORK", "text": "Due to the effectiveness of CNNs on image datasets, many authors have proposed models to adapt them to other kind of data, e.g. for shape datasets [3], molecular datasets [4], or graph datasets [5], [6]. A review is done in [7]. In these adaptations of CNNs, the defined input layers are designed to retain the attributes of a convolutional operator.\nIn particular, CNNs have been adapted to graph signal datasets, such as in [8], [9] where the convolution is formalized in the spectral domain of the graph defined by its Laplacian [10]. However, with this approach the created features do not depend on values of the signals locally. This was alleviated in [11], which also propose a fast approximated formulation.\nIn [6], the authors define a convolution based on multiplication with powers of the probability transition matrix. As it has the same support as the adjacency matrix, it shares the same kind of local receptive fields as our model. In their model, they tie weights according to the power to which they are attached. In [5], an ordering of the nodes is used. These choices are arbitrary and unsimilar to what is done by regular convolutions, which rely on the underlying euclidean structure. On the contrary, our proposed layer formulation allows to also learn how the weights are tied over the local receptive fields.\nOur model is first designed to be applied to the task of graph signal classification, but another common task related to signals on graph is the problem of node classification such as in [6], [12], [13]. Models learning themselves or part of their structures as we do here have also been proposed, such as in [14], [15]."}, {"heading": "III. METHODOLOGY", "text": "We first recall the basic principles of Deep Neural Networks (DNNs) and CNNs, then introduce our proposed graph layer."}, {"heading": "A. Background", "text": "DNNs consist of a composition of layers, each one parametrized by a learnable weight kernel W and a nonlinear function f : R \u2192 R. Providing the input of such a layer is x, the corresponding output is then:\ny = f(W \u00b7 x+ b) ,\nwhere \u00b7 is the matrix product operator, f is applied componentwise and b is a learnable bias vector.\nThe weight kernels are learned using an optimization routine usually based on gradient descent, so that the DNN is able to approximate an objective function. A DNN containing only this type of layer is called Multi-Layer Perceptron (MLP).\nIn the case of CNNs, some of the layers have the particular form of convolution filters. In this case, the convolutional operation can also be written as the product of the input signal with a matrix W , where W is a Toeplitz matrix. Previous works [16] have shown that to obtain the best accuracy in vision challenges, it is better to use very small kernels, resulting in a sparse W . Figure 1 depicts a convolutional layer."}, {"heading": "B. Proposed Method", "text": "We propose to introduce another type of layer, that we name receptive graph layer. It is based on an adjacency matrix and aims at extending the principle of convolutional layers to any domain that can be described using a graph.\nConsider an adjacency matrix A that is well fitted to the signals to be learned, in the sense that it describes an underlying graph structure between the input features. We define the receptive graph layer associated with A using the product between a third rank tensor S and a weight kernel W . For now, the tensor W would be one-rank containing the weights of the layer and S is of shape n\u00d7n\u00d7\u03c9, where n\u00d7n is the shape of the adjacency matrix and \u03c9 is the shape of W .\nOn the first two ranks, the support of S must not exceed\nthat of A, such that Aij = 0 \u21d2 \u2200k, Sijk = 0. Overall, we obtain:\ny = f(W \u00b7 S \u00b7 x+ b) ,\nwhere here \u00b7 denotes the tensor product.\nWe name S the propagation tensor of the receptive graph. Intuitively, the role of W and S consist in allocating weights to pairs of neighbors in A. In a sense, the propagation tensor S is to the receptive graph what the adjacency matrix A is to the graph. An example is depicted in Figure 2. In other terms, the receptive graph layer can also be defined as a partially connected layer controlled by A with a learnable weight sharing scheme controlled by S.\nAlike convolution on images, W can be extended as a third-rank tensor to include multiple input and output channels (also known as feature maps). It is worth mentioning that an implementation must be memory efficient to take care of a possibly large sparse S."}, {"heading": "C. Training", "text": "The training procedure requires learning both S and W . We perform the two jointly. Learning W amounts to learning weights as in regular CNNs, whereas learning S amounts to learning how these weights are tied over the receptive fields. We also experiment a fine-tuning step, which consists in freezing S in the last epochs.\nBecause of our inspiration from CNNs, we propose constraints on the parameters of S. Namely, we impose them to be between 0 and 1, and to sum to 1 along the third dimension. Therefore, the vectors on the third rank of S can be interpreted as performing a weighted average of the parameters in W .\nWe test two types of initialization. The first one consists in distributing one-hot-bit vectors along the third rank. We impose that for each receptive field, a particular one-hot-bit vector can be distributed at most once more than any other. We refer to it as one-hot-bit initialization. The second one consists in using a uniform random distribution with limits as described in [17]."}, {"heading": "D. Discussion", "text": "For simplicity we restricted our explanation to square adjacency matrices. In the case of oriented graphs, one could remove the rows and columns of zeros and obtain a receptive\ngraph with a distinct number of neurons in the input (n) than in the output (m). As a result, receptive graph layers extend usual ones, as explained here:\n1) To obtain a fully connected layer, one can choose \u03c9 to\nbe nm and S the matrix of vectors that contains all possible one-hot-bit vectors. 2) To obtain a convolutional layer, one can choose \u03c9 to be\nthe size of the kernel. S would be one-hot-bit encoded along its third rank and circulant along the first two ranks. A stride > 1 can be obtained by removing the corresponding rows.\nIn our case, S is more similar to that obtained when considering convolutional layers, with the noticeable differences that we do not force which weight to allocate for which neighbor along its third rank and it is not necessarily circulant along the first two ranks.\nNote that without the constraint that the support of S must not exceed that of A (or if the used graph is complete), the proposed formulation could also be applied to structure learning for the input features. For instance, summing such learned S along the third rank gives hints about how they are correlated to each others. Also, a regularization along the third rank could be used to drop connections during training. However, even if this can work for toy image dataset, such S wouldn\u2019t be sparse and would lead to memory issues in higher dimensions. So we didn\u2019t include these avenues in the scope of this paper."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. Description", "text": "In the following experiments, we are interested in comparing various receptive graph layers with convolutional ones. For this purpose, we use image datasets, but restrain priors about the underlying structure. Note that we consider simple classifiers without pooling, and only switch the first layer.\nFor every model, the first layer consists of 50 feature maps, followed by a fully connected layer of 300 neurons, and terminated by a softmax layer of 10 neurons. Rectified Linear Units [18] are used for the activations and a dropout of 0.5 is applied on the fully-connected layer. Unless stated otherwise, weights are initialized random uniformly [17]. A regularization with weight 10\u22125 is applied to input layers. We first present experiments on MNIST [19]. It contains 10 classes of gray levels images (28x28 pixels) with 60\u2019000 examples for training, 10\u2019000 for testing. We also do experiments on a scrambled version. Then we present experiments on Cifar10 [20]. It contains 10 classes of RGB images (28x28 pixels) with 50\u2019000 examples for training, 10\u2019000 for testing.\nOn MNIST, optimizations are done with ADAM [21] up to 100 epochs. For Cifar 10, they are done with ADADELTA [22] up to 350 epochs. We also freeze S for up to 50 additional epochs and found that it sometimes improves performance."}, {"heading": "B. Experiments with MNIST", "text": "We consider a grid graph that connects each pixel to itself and its 4 nearest neighbors (or less on the borders). We also\nuse the square of this graph (pixels are connected to their 13 nearest neighbors, including themselves), the cube of this graph (25 nearest neighbors), up to 10 powers (211 nearest neighbors). In this subsection, we use one-hot-bit initialization. We test the model under two setups: either the ordering of the node is unknown, and then the one-hot-bit vectors are distributed randomly and modified upon training ; either an ordering of the node is known, and then the one-hot-bit vectors are distributed in a circulant fashion in the third rank of S which is freezed in this state. We use the number of nearest neighbors as for the dimension of the third rank of S. We also compare with a convolutional layer of size 5x5, thus containing as many weights as the cube of the grid graph. Table I summarizes the obtained results. The ordering is unknown for the first result given, and known for the second result between parenthesis.\nWe observe that even without knowledge of the underlying euclidean structure, receptive grid graph layers obtain comparable performances as convolutional ones. When the ordering is known, they match convolutions.\nTo compare with convolutions, we also plot in Figure 3 the obtained product W \u00b7 S for one feature map, when using the square of the grid graph. For a convolutional layer, each diamond of this plot would be identical.\nIn Figure 4, we plot the test error rate on MNIST for various normalizations when using the square of the grid graph, as a function of the number of epochs of training. We observe that they have little influence on the performance and sometimes improve it a bit. Thus, their usage is validated as optional hyperparameters."}, {"heading": "C. Experiments with Scrambled MNIST", "text": "We use a thresholded covariance matrix obtained by using all the training examples. We choose the threshold so that the number of remaining edges corresponds to a certain density p (5x5 convolutions correspond approximately to a density of p = 3%). We also infer a graph based on the k nearest neighbors of the inverse of the values of this covariance matrix (k-NN). The latter two are using no prior about the signal underlying structure. The pixels of the input images are shuffled and the same re-ordering of the pixels is used for every image. Dimension of the third rank of S is chosen equal to k. The receptive graph layers are also compared with models obtained when replacing the first layer by a fully connected or convolutional one. Results are reported on table II.\nWe observe that the second receptive graph layer outperforms the CNN on scrambled MNIST. That suggests it has been able to exploit information about the underlying structure thanks to its graph. On the contrary, the first version seems to have failed to discover any of these information. This might\nbe explained by the fact that it has receptive fields of unequal size and should have required additional normalization."}, {"heading": "D. Experiments with Cifar10", "text": "For the input layer, we use the square of the grid graph as support, and consider the ordering known for initialization. It makes it similar to a convolution at the initial state, except for the shape of the convolutional window. We are interested in seeing if the allocation of the shared weights would be modified by the learning process, and if it would improve the performance. We used the same architecture as for MNIST. These are weak classifiers for Cifar10 but they are enough to analyze the usefulness of our proposed layer. Exploring better architectures is left for further work. Results are summarized in table III.\nThe receptive graph layer is able to outperform the corresponding CNN by a small amount in this configuration, opening the way for more complex architectures."}, {"heading": "V. CONCLUSION", "text": "We introduced a new kind of layer for deep neural networks which consists in using the support of a graph operator and linearly distributing a pool of weights over the defined edges. The linear distribution is learned jointly with the pool of weights. Thanks to these structural dependencies, we showed it is possible to share weights in a fashion similar to Convolutional Neural Networks (CNNs).\nWe performed experiments on vision datasets where the receptive graph layer obtains similar performance as convolutional ones, even when the underlying image structure is not given at hand. We believe that with further work, the proposed layer could fully extend the performance of CNNs to many other domains described by a graph. In particular, with more appropriate regularizations for S, it might be possible to make it learn a more powerful propagational structure.\nWhen a graph is not available, the support of S must be inferred. Future works will also include exploration of more advanced graph inference techniques. One example is using gradient descent from the supervised task at hand [9]. We can also notice that in our case, this amounts to select receptive fields, breeding another avenue [23]."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was funded in part by the CominLabs project Neural Communications, and by the ANRT (Agence Nationale de la Recherche et de la Technologie) through a CIFRE (Convention Industrielle de Formation par la REcherche)."}], "references": [{"title": "Understanding deep convolutional networks", "author": ["S. Mallat"], "venue": "Philosophical Transactions of the Royal Society A, vol. 374, no. 2065, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Neighborhoodpreserving translations on graphs", "author": ["N. Grelier", "B. Pasdeloup", "J.-C. Vialatte", "V. Gripon"], "venue": "Proceedings of IEEE GlobalSIP, 2016, pp. 410\u2013414.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Shapenet: Convolutional neural networks on non-euclidean manifolds", "author": ["J. Masci", "D. Boscaini", "M. Bronstein", "P. Vandergheynst"], "venue": "2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["D.K. Duvenaud", "D. Maclaurin", "J. Iparraguirre", "R. Bombarell", "T. Hirzel", "A. Aspuru-Guzik", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2215\u20132223.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning convolutional neural networks for graphs", "author": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning, 2016, pp. 2014\u20132023.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Diffusion-convolutional neural networks", "author": ["J. Atwood", "D. Towsley"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 1993\u2013 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Geometric deep learning: going beyond euclidean data", "author": ["M.M. Bronstein", "J. Bruna", "Y. LeCun", "A. Szlam", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1611.08097, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6203, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional networks on graph-structured data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv preprint arXiv:1506.05163, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine, vol. 30, pp. 83\u201398, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks on graphs with fast localized spectral filtering", "author": ["M. Defferrard", "X. Bresson", "P. Vandergheynst"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 3837\u20133845.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting semisupervised learning with graph embeddings", "author": ["Z. Yang", "W.W. Cohen", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1603.08861, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised classification with graph convolutional networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "arXiv preprint arXiv:1609.02907, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning the structure of deep convolutional networks", "author": ["J. Feng", "T. Darrell"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2749\u20132757.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Adanet: Adaptive structural learning of artificial neural networks", "author": ["C. Cortes", "X. Gonzalvo", "V. Kuznetsov", "M. Mohri", "S. Yang"], "venue": "arXiv preprint arXiv:1607.01097, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Spatially-sparse convolutional neural networks", "author": ["B. Graham"], "venue": "arXiv preprint arXiv:1409.6070, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 2011, pp. 315\u2013323.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": "1998.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 2528\u2013 2536.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Some authors draw a parallel between these features and scattering transforms [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "[2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "for shape datasets [3], molecular datasets [4], or graph datasets [5], [6].", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "for shape datasets [3], molecular datasets [4], or graph datasets [5], [6].", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "for shape datasets [3], molecular datasets [4], or graph datasets [5], [6].", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "for shape datasets [3], molecular datasets [4], or graph datasets [5], [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "A review is done in [7].", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "In particular, CNNs have been adapted to graph signal datasets, such as in [8], [9] where the convolution is formalized in the spectral domain of the graph defined by its Laplacian [10].", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "In particular, CNNs have been adapted to graph signal datasets, such as in [8], [9] where the convolution is formalized in the spectral domain of the graph defined by its Laplacian [10].", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "In particular, CNNs have been adapted to graph signal datasets, such as in [8], [9] where the convolution is formalized in the spectral domain of the graph defined by its Laplacian [10].", "startOffset": 181, "endOffset": 185}, {"referenceID": 10, "context": "This was alleviated in [11], which also propose a fast approximated formulation.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "In [6], the authors define a convolution based on multiplication with powers of the probability transition matrix.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [5], an ordering of the nodes is used.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Our model is first designed to be applied to the task of graph signal classification, but another common task related to signals on graph is the problem of node classification such as in [6], [12], [13].", "startOffset": 187, "endOffset": 190}, {"referenceID": 11, "context": "Our model is first designed to be applied to the task of graph signal classification, but another common task related to signals on graph is the problem of node classification such as in [6], [12], [13].", "startOffset": 192, "endOffset": 196}, {"referenceID": 12, "context": "Our model is first designed to be applied to the task of graph signal classification, but another common task related to signals on graph is the problem of node classification such as in [6], [12], [13].", "startOffset": 198, "endOffset": 202}, {"referenceID": 13, "context": "Models learning themselves or part of their structures as we do here have also been proposed, such as in [14], [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 14, "context": "Models learning themselves or part of their structures as we do here have also been proposed, such as in [14], [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "Previous works [16] have shown that to obtain the best accuracy in vision challenges, it is better to use very small kernels, resulting in a sparse W .", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "Rectified Linear Units [18] are used for the activations and a dropout of 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "We first present experiments on MNIST [19].", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "Then we present experiments on Cifar10 [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 19, "context": "On MNIST, optimizations are done with ADAM [21] up to 100 epochs.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "For Cifar 10, they are done with ADADELTA [22] up to 350 epochs.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "One example is using gradient descent from the supervised task at hand [9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 21, "context": "We can also notice that in our case, this amounts to select receptive fields, breeding another avenue [23].", "startOffset": 102, "endOffset": 106}], "year": 2017, "abstractText": "To achieve state-of-the-art results on challenges in vision, Convolutional Neural Networks learn stationary filters that take advantage of the underlying image structure. Our purpose is to propose an efficient layer formulation that extends this property to any domain described by a graph. Namely, we use the support of its adjacency matrix to design learnable weight sharing filters able to exploit the underlying structure of signals. The proposed formulation makes it possible to learn the weights of the filter as well as a scheme that controls how they are shared across the graph. We perform validation experiments with image datasets and show that these filters offer performances comparable with convolutional ones.", "creator": "gnuplot 5.0 patchlevel 6"}}}