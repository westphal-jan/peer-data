{"id": "1706.02792", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "The FastMap Algorithm for Shortest Path Computations", "abstract": "We for put new postprocessing algorithm provided relational there protein with, so edge - barrel plasticity graph but every Euclidean space. In because module, the Euclidean one continued nothing pair hippocampus time-period early reaching well the shortest apart line them recently into may logarithmic. Later, started runtime, piece lifespan path between same two peripheral if be formula_11 devices A * allows made in Euclidean distances as heuristics surveyed. Our 13-track graphs, originally FastMap, that success by the Data Mining computation of the same origin and two a tunnel - geometries set. Hence, FastMap is u.s. form magnitude depend perhaps competing shaping that produce that Euclidean isometric instance Semidefinite Programming. Our FastMap approximated some exclusively hearsay however satisfied heuristics and therefore commitments with generation of optimal paths. Moreover, FastMap works on was fddi vertices put both same traditional diagrammatic, more brought the Manhattan Distance serological, are actually lot well terms. Empirically too, simply commitment same the FastMap non-intrusive is challenging over also legislature - a - the - art formalisms now the Differential heuristic.", "histories": [["v1", "Thu, 8 Jun 2017 23:29:05 GMT  (2119kb,D)", "http://arxiv.org/abs/1706.02792v1", null], ["v2", "Sat, 21 Oct 2017 19:11:06 GMT  (2119kb,D)", "http://arxiv.org/abs/1706.02792v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["liron cohen", "t k satish kumar", "tansel uras", "sven koenig"], "accepted": false, "id": "1706.02792"}, "pdf": {"name": "1706.02792.pdf", "metadata": {"source": "CRF", "title": "The FastMap Algorithm for Shortest Path Computations", "authors": ["Liron Cohen", "T.K. Satish Kumar", "Tansel Uras", "Sven Koenig"], "emails": [], "sections": [{"heading": "Introduction and Related Work", "text": "Shortest path problems commonly occur in the inner procedures of many AI programs. In video games, for example, a large fraction of CPU cycles are spent on shortest path computations [Uras and Koenig, 2015]. Many other tasks in AI, including motion planning [LaValle, 2006], temporal reasoning [Dechter, 2003], and decision making [Russell and Norvig, 2009], also involve finding and reasoning about shortest paths. While Dijkstra\u2019s algorithm [Dijkstra, 1959] can be used to compute shortest paths in polynomial time, faster computations bear important implications on the time-efficiency of solving the aforementioned tasks. One way to boost shortest path computations is to use the A* search framework with an informed heuristic [Hart et al., 1968].\nA perfect heuristic is one that returns the true shortest path distance between any two nodes in a given graph. In this graph, A* with such a heuristic and proper tie-breaking is guaranteed to expand nodes only on an optimal path between\nthe specified start and goal nodes. In general, computing the perfect heuristic value between two nodes is as hard as computing the shortest path between them. Hence, A* search can benefit from a perfect heuristic only if it is computed offline. However, precomputing all pairwise shortest path distances is not only time-intensive but also requires a prohibitive O(N2) memory where N is the number of nodes.\nMany methods for preprocessing a given graph (without precomputing all pairwise shortest path distances) have been studied before and can be grouped into several categories. Hierarchical abstractions that yield suboptimal paths have been used to reduce the size of the search space by abstracting groups of vertices [Botea et al., 2004; Sturtevant and Buro, 2005]. More informed heuristics [Bjo\u0308rnsson and Halldo\u0301rsson, 2006; Cazenave, 2006; Sturtevant et al., 2009] guide the searches better to expand fewer states. Hierarchies can also be used to derive heuristics during search [Leighton et al., 2008; Holte et al., 1994]. Dead-end detection and other pruning methods [Bjo\u0308rnsson and Halldo\u0301rsson, 2006; Goldenberg et al., 2010; Pochter et al., 2010] identify areas of the graph that do not need to be searched to find shortest paths. Search with contraction hierarchies [Geisberger et al., 2008] is an optimal and extremely hierarchical method, as every level of the hierarchy contains only a single node. It has been shown to be effective on road networks but seems to be less effective on graphs with higher branching factors, such as gridbased game maps [Storandt, 2013]. Another approach is that of N-level graphs [Uras and Koenig, 2014] constructed from undirected graphs by partitioning the nodes into levels. The hierarchy allows significant pruning during search.\nA different approach that does not rely on preprocessing the graph makes use of some notion of a geometric distance between two nodes as a heuristic estimate of the shortest path distance between them. One such common heuristic that is used in gridworlds is the Manhattan Distance heuristic.1 For many gridworlds, A* search with the Manhattan Distance heuristic outperforms Dijkstra\u2019s algorithm. However, in complicated 2D/3D gridworlds like mazes, the Manhattan Distance heuristic may not be informed enough to efficiently guide A* search. Another issue associated with Manhattan\n1In a 4-connected 2D gridworld, for example, the Manhattan Distance between two cells (x1, y1) and (x2, y2) is |x1\u2212x2|+|y1\u2212y2|. Similar generalizations exist for 3D and 8-connected gridworlds.\nar X\niv :1\n70 6.\n02 79\n2v 1\n[ cs\n.A I]\n8 J\nun 2\n01 7\nDistance-like heuristics is that they are not well defined for general graphs.2 For a graph that cannot be conceived in a geometric space, there is no closed-form formula for a \u201cgeometric\u201d heuristic estimate for the distance between two nodes because there are no coordinates associated with them.\nFor a graph that does not already have a geometric embedding in Euclidean space, a preprocessing algorithm can be used to generate one. As described before, at runtime, A* search would then use the Euclidean distance between any two nodes in this space as an estimate for the length of the shortest path between them in the given graph. One such approach is presented in [Rayner et al., 2011]. This approach guarantees admissiblility and consistency of the heuristic and therefore generates optimal paths. However, it requires solving a Semidefinite Program (SDP) in its preprocessing phase. SDPs can be solved in polynomial time [Vandenberghe and Boyd, 1996]; and in this case, additional structure is leveraged to solve them in cubic time [Rayner et al., 2011]. Still, a cubic preprocessing time limits the size of the graphs that are amenable to this approach.\nThe Differential heuristic is another state-of-the-art approach that has the benefits of near-linear preprocessing time. However, unlike the approach in [Rayner et al., 2011], it does not produce an explicit Euclidean embedding. In the preprocessing phase of the Differential heuristic approach, some nodes of the graph are chosen as pivot nodes. The shortest path distances between each pivot node and every other node are precomputed and stored [Sturtevant et al., 2009]. At runtime, the heuristic distance between two nodes, a and b, is given by maxp |d(a, p)\u2212 d(p, b)| where p is a pivot node and d(, ) is the precomputed distance. The preprocessing time is linear in the number of pivots times the size of the graph. The required space is linear in the number of pivots times the number of nodes, although a more succinct representation is presented in [Goldenberg et al., 2011]. Similar preprocessing techniques are used in Portal-Based True Distance heuristics [Goldenberg et al., 2010].\nIn this paper, we present a new preprocessing algorithm that produces an explicit Euclidean embedding while running in near-linear time. It therefore has the benefits of the Differential heuristic\u2019s preprocessing time as well as that of producing an embedding from which heuristic estimates can be quickly computed using closed-form formulas. Our preprocessing algorithm, dubbed FastMap, is inspired by the Data Mining algorithm of the same name [Faloutsos and Lin, 1995]. It is orders of magnitude faster than SDP-based approaches for producing Euclidean embeddings. FastMap also produces admissible and consistent heuristics and therefore guarantees the generation of optimal paths.\nIn comparison to other heuristics derived from closed-form formulas, like the Manhattan or the Octile Distance heuristics, the FastMap heuristic has several advantages. First, it is defined for general undirected graphs (even if they are not gridworlds). Second, we observe empirically that even in gridworlds, A* with the FastMap heuristic outperforms A* with the Manhattan or the Octile Distance heuristic. In com-\n2Henceforth, whenever we refer to a graph, we mean an edgeweighted undirected graph unless stated otherwise.\nparison to the Differential heuristic with the same memory resources, the FastMap heuristic is not only competitive with it on some graphs but even outperforms it on some others. This performance of FastMap is encouraging given that it produces an explicit Euclidean embedding that has other representational benefits like recovering the underlying manifolds of the graph and/or visualizing it. Moreover, we observe that the FastMap and the Differential heuristics have complementary strengths and can be easily combined to generate a more informed heuristic."}, {"heading": "The Origin of FastMap", "text": "The FastMap algorithm [Faloutsos and Lin, 1995] was introduced in the Data Mining community for automatically generating geometric embeddings of abstract objects. For example, if we are given objects in the form of long DNA strings, multimedia datasets such as voice excerpts or images, or medical datasets such as ECGs or MRIs, there is no geometric space in which these objects can be naturally visualized. However, in many of these domains, there is still a well defined distance function between every pair of objects. For example, given two DNA strings, the edit distance between them3 is well defined although an individual DNA string cannot be conceptualized in a geometric space.\nClustering techniques, such as the k-means algorithm, are well studied in Machine Learning [Alpaydin, 2010]; but they cannot be applied directly to domains with abstract objects as described above. This is because these algorithms assume that the objects are described as points in a geometric space. FastMap revives the applicability of these clustering techniques by first creating an artificial Euclidean embedding for the abstract objects. The Euclidean embedding is such that the pairwise distances are approximately preserved. Such an embedding would also help in the visualization of the abstract objects. This visualization, for example, can aid physicians in identifying correlations between symptoms or other patterns from medical records.\nWe are given a complete undirected edge-weighted graph G = (V,E). Each vertex vi \u2208 V represents an abstract object Oi. Between any two vertices, vi and vj , there is an edge (vi, vj) \u2208 E with weight D(Oi, Oj). Here, D(Oi, Oj) is the given pairwise distance between the objects Oi and Oj . A Euclidean embedding assigns to each object Oi a Kdimensional point pi \u2208 RK . A good Euclidean embedding is one in which the Euclidean distance between any two points, pi and pj , closely approximates D(Oi, Oj).\nOne of the early approaches for generating such an embedding was based on the idea of multi-dimensional scaling (MDS) [Torgerson, 1952]. Here, overall distortion of the pairwise distances is measured in terms of the \u201cenergy\u201d stored in \u201csprings\u201d connecting each pair of objects. MDS, however, requires O(N2) time (N = |V |) and hence does not scale well in practice. On the other hand, FastMap [Faloutsos and Lin, 1995] requires only linear time. Both methods embed the objects in a K-dimensional space for a user-specified K.\n3The edit distance between two strings is the minimum number of insertions, deletions or substitutions that are needed to transform one to the other.\nFastMap works as follows. In the very first iteration, it heuristically identifies the farthest pair of objects Oa and Ob in linear time. It does this by initially choosing a random object Ob and then choosing Oa to be the farthest object away from Oa. It then reassigns Ob to be the farthest object away from Oa. Once Oa and Ob are determined, every other object Oi defines a triangle with sides of lengths dai = D(Oa, Oi), dab = D(Oa, Ob) and dib = D(Oi, Ob). Figure 1 shows this triangle. Since the sides of the triangle define its entire geometry, the length xi = (d2ai+d 2 ab\u2212d2ib)/(2dab). We set the first coordinate of pi, the embedding of the object Oi, to be xi. In particular, the first coordinate of pa is xa = 0 and of pb is xb = dab. We note that computing the first coordinates of all objects takes only linear time since the distance between any two objects Oi and Oj for i, j /\u2208 {a, b} is never computed.\nIn the subsequentK\u22121 iterations, the same procedure follows for computing the remaining K \u2212 1 coordinates of each object. However, the distance function is adapted for different iterations. For example, after the first iteration, Oa and Ob have their first coordinates as 0 and dab respectively. Because this fully explains the true distance between them, from the second iteration onwards, the rest of pa and pb\u2019s coordinates should be identical. Intuitively, this means that the second iteration should mimic the first one on a hyperplane that is perpendicular to the line OaOb. Figure 2 explains this intuition. Although the hyperplane is never constructed explicitly, its conceptualization implies that the distance function for the second iteration should be changed in the following way: D2new(O \u2032 i, O \u2032 j) = D\n2(Oi, Oj) \u2212 (xi \u2212 xj)2. Here, O\u2032i and O\u2032j are the projections of Oi and Oj , respectively, onto this hyperplane; and Dnew is the new distance function."}, {"heading": "FastMap for Shortest Path Computations", "text": "In this section, we provide the high-level ideas for how to adapt the Data Mining FastMap algorithm to the context of shortest path computations. In the shortest path computation problem, we are given an edge-weighted undirected graph G = (V,E) along with a start node vs and a goal node vg . As a preprocessing technique, we can embed the vertices of G in a Euclidean space. During A* search for a shortest path from vs to vg , the Euclidean distances from v \u2208 V to vg can be used as heuristic estimates to vg . The number of nodes expanded by the A* search depends on the informedness of the heuristic which, in turn, depends on the ability of the embedding to preserve pairwise distances.\nThe general idea is to view the nodes of G as the objects to be embedded in Euclidean space. As such, the Data Mining FastMap algorithm cannot be used directly for generating an embedding in linear time. This is because the Data Mining FastMap algorithm assumes that given two objects, Oi and Oj , computing the distance dij between them can be done in constant time, i.e., it does not depend on the number of objects. This assumption does not hold for our domain because computing the shortest path distance between two nodes depends on the size of the graph. Another problem that arises in this context is that the Euclidean distances may not satisfy important properties such as admissibility and/or consistency. Admissibility guarantees the generation of optimal paths in A* while consistency allows us to avoid re-expansions of nodes as well.\nThe first issue of having to retain (near-)linear time complexity can be addressed as follows. In each iteration, after we identify the farthest pair of objects Oa and Ob (which are nodes in G), the distances dai and dib need to be computed for all other objects Oi. Computing dai and dib for any single Oi can no longer be done in constant time but requiresO(|E|+|V | log |V |) time instead [Tarjan and Fredman, 1984]. However, since we need to compute these distances for allOi, computing two shortest path trees rooted atOa andOb yields all the necessary distances. The complexity of doing so is also O(|E| + |V | log |V |), which is only linear in the size of the graph.4 The amortized complexity for computing dai and dib for any singleOi is therefore near-constant time. This revives the applicability of the FastMap algorithm.\nThe second issue of having to generate admissible and consistent heuristics is formally addressed in Theorem 1. The basic idea is to use L1 distances instead of L2 distances in each iteration of the FastMap algorithm. The mathematical properties of the L1 distance function can be used to prove that admissibility and consistency hold irrespective of the dimensionality of the embedding, K.\nAlgorithm 1 presents the FastMap algorithm adapted to the shortest path problem. The input to this algorithm is an edgeweighted undirected graph G = (V,E,w) along with two user-specified parameters Kmax and . Kmax is the maximum number of dimensions allowed in the Euclidean embedding. It bounds the amount of memory needed to store the Euclidean embedding of any node. is a threshold pa-\n4unless |E| = O(|V |), in which case the complexity of doing so is near-linear because of the log(|V |) factor\nAlgorithm 1: Shows the FastMap algorithm. G = (V,E,w) is the edge-weighted undirected graph; Kmax is the user-specified upper bound on the dimensionality; is a user-specified threshold; K \u2264 Kmax is the dimensionality of the computed embedding; pi is the Euclidean embedding of node vi \u2208 V . Line 11 is equivalent to w\u2032(u, v) = w(u, v)\u2212 \u2016pu \u2212 pv\u20161.\nInput: G = (V,E,w), Kmax and . Output: K and pi \u2208 RK for all vi \u2208 V .\n1 w\u2032 = w; K = 0; 2 while Kmax > 0 do 3 Let G\u2032 = (V,E,w\u2032); 4 (na, nb)\u2190 GetFarthestPair(G\u2032); 5 Compute shortest path trees rooted at na and nb on G\u2032 to obtain dab, dai and dib for all vi \u2208 V ; 6 if dab < then 7 Break; 8 for each vi \u2208 V do 9 [pi]K = (dai \u2212 dib + dab)/2 ; // Kth coord.\n10 for each edge (u, v) \u2208 E do 11 w\u2032(u, v) = w\u2032(u, v)\u2212 |[pu]K \u2212 [pv]K |; 12 K = K + 1; Kmax = Kmax \u2212 1;\nrameter that marks a point of diminishing returns when the distance between the farthest pair of nodes becomes negligible. The output of this algorithm is an embedding pi for each node vi \u2208 V . pi \u2208 RK is a K-dimensional point, where K \u2264 Kmax.\nThe algorithm maintains a working graph G\u2032 = (V,E,w\u2032) initialized to G. The nodes and edges of G\u2032 are identical to those of G but the weights on the edges of G\u2032 change with every iteration. In each iteration, the farthest pair of nodes, na and nb, in G\u2032 is heuristically identified in near-linear time (line 4). The Kth coordinate, [pi]K , of each node vi is computed using a formula similar to that for xi in Figure 1. However, that formula is modified to (dai\u2212dib +dab)/2 to ensure admissibility and consistency of the heuristic. In each iteration, the weight of each edge is decremented to resemble the update rule for Dnew in Figure 2 (line 11). However, that update rule is modified again to use the L1 distances instead of the L2 distances. Theorem 1 shows that doing so ensures admissibility and consistency of the heuristic.\nThe method GetFarthestPair(G\u2032) in line 4 computes shortest path trees inG\u2032 a small constant number of times, denoted by \u03c4 .5 It therefore runs in near-linear time. In the first iteration, we assign na to be a random node. A shortest path tree rooted at na is computed to identify the farthest node from it. nb is assigned to be this farthest node. In the next iteration, a shortest path tree rooted at nb is computed to identify the farthest node from it. na is reassigned to be this farthest node. Subsequent iterations follow the same switching rule for na and nb. The final assignments of nodes to na and nb are returned after \u03c4 iterations. This entire process of starting from a\n5\u03c4 = 10 in our experiments.\nrandomly chosen node can be repeated a small constant number of times.6\nFigure 3 shows the working of our algorithm on a small gridworld example."}, {"heading": "Proof of Consistency", "text": "In this subsection, we prove the consistency of the FastMap heuristic. Since consistency implies admissibility, this also proves that A* with the FastMap heuristic returns optimal paths. We use the following notation in the proofs: wixy is the weight on the edge between nodes x and y in the ith iteration; dixy is the shortest path distance between nodes x and y in the ith iteration (using the weights wi); px is the vector of coordinates produced for node x and [px]j is its jth coordinate;7 hixy is the FastMap heuristic value between nodes x and y after i iterations. Note that hixy := \u2016pix \u2212 piy\u20161 =\u2211i\nj=1 |[pix]j \u2212 [piy]j |. We also define \u2206i+1xy := dixy \u2212 di+1xy . In the following proofs, we use the fact that |A|+ |B| \u2265 |A+B| and |A| \u2212 |B| \u2264 |A\u2212B|. Lemma 1. For all x, y and i, dixy \u2265 0.\nProof. We prove by induction that in any iteration i, wiuv \u2265 0 for all (u, v) \u2208 E. This would mean that the weight of each edge in the ith iteration is non-negative and therefore dixy \u2265 0 for all x, y. For the base case, w1uv = w(u, v) \u2265 0. We assume wiuv \u2265 0 and show that wi+1uv \u2265 0. Let na and nb be the farthest pair of nodes identified in the ith iteration. From lines 9 and 11, wi+1uv = w i uv\u2212|(diau\u2212diav)/2+(divb\u2212diub)/2|. To show that wi+1uv \u2265 0 we show that wiuv \u2265 |(diau \u2212 diav)/2 + (divb\u2212diub)/2|. From triangle inequality, for any node l, diuv+ min(diul, d i lv) \u2265 max(diul, dilv). Therefore diuv \u2265 |dilv \u2212 diul|. This means that diuv \u2265 |diau\u2212diav|/2 + |divb\u2212diub|/2. Therefore, diuv \u2265 |(diau\u2212diav)/2 + (divb\u2212diub)/2|. This concludes the proof since wiuv \u2265 diuv .\nLemma 2. For all x, y and i, \u2206i+1xy \u2265 |[px]i \u2212 [py]i|.\nProof. Let \u3008u1 = x, . . . , um = y\u3009 be the shortest path from x to y in iteration i. By definition, dixy = \u2211m\u22121 j=1 w i ujuj+1 and di+1xy \u2264 \u2211m\u22121 j=1 w i+1 ujuj+1 . From line 11, w i+1 ujuj+1 = wiujuj+1\u2212|[puj ]i\u2212[puj+1 ]i|. Therefore, \u2206 i+1 xy = d\ni xy\u2212di+1xy \u2265\u2211m\u22121 j=1 |[puj ]i \u2212 [puj+1 ]i|. This concludes the proof since\u2211m\u22121 j=1 |[puj ]i \u2212 [puj+1 ]i| \u2265 | \u2211m\u22121 j=1 [puj ]i \u2212 [puj+1 ]i| = |[px]i \u2212 [py]i|.\nLemma 3. For all x, y, g and i, d1xy + hiyg \u2212 hixg \u2265 di+1xy .\nProof. We prove by induction on i. The base case for i = 1 is implied by Lemma 2. We assume d1xy+h i yg\u2212hixg \u2265 di+1xy and show d1xy+h i+1 yg \u2212hi+1xg \u2265 di+2xy . We know that hi+1yg \u2212hi+1xg = hiyg\u2212hixg\u2212 (|[px]i+1\u2212 [pg]i+1|\u2212 |[py]i+1\u2212 [pg]i+1|). Since |[px]i+1\u2212 [pg]i+1|\u2212|[py]i+1\u2212 [pg]i+1| \u2264 |[px]i+1\u2212 [py]i+1|, we have hi+1yg \u2212hi+1xg \u2265 hiyg\u2212hixg\u2212|[px]i+1\u2212[py]i+1|. Hence,\n6This constant is also 10 in our experiments. 7The ith iteration sets the value of [px]i.\nd1xy +h i+1 yg \u2212hi+1xg \u2265 (d1xy +hiyg\u2212hixg)\u2212|[px]i+1\u2212 [py]i+1|. Using the inductive assumption, we get d1xy +h i+1 yg \u2212hi+1xg \u2265 di+1xy \u2212|[px]i+1\u2212[py]i+1|. By definition, di+1xy = \u2206i+2xy +di+2xy . Substituting for di+1xy , we get d 1 xy + h i+1 yg \u2212 hi+1xg \u2265 di+2xy + (\u2206i+2xy \u2212 |[px]i+1 \u2212 [py]i+1|). Lemma 2 shows that \u2206i+2xy \u2265 |[px]i+1 \u2212 [py]i+1| which concludes the proof.\nTheorem 1. The FastMap heuristic is consistent.\nProof. From Lemma 3, we know d1xy + h i yg \u2212 hixg \u2265 di+1xy . From Lemma 1, we have di+1xy \u2265 0. Put together, we have d1xy + h i yg \u2265 hixg for any x, y, g and i.\nTheorem 2. The informedness of the FastMap heuristic increases monotonically with the number of dimensions.\nProof. This follows from the fact that for any two nodes x and g, hi+1xg = h i xg + |[px]i+1 \u2212 [pg]i+1| \u2265 hixg ."}, {"heading": "Experimental Results", "text": "We set up experiments on many benchmark maps from [Sturtevant, 2012]. Figure 4 presents representative results. The FastMap heuristic (FM) and the Differential heuristic (DH) with equal memory resources8 are compared against each other. In addition, we include the Octile heuristic (OCT) as a baseline heuristic that also uses a closed-form formula for heuristic computations.\nWe observe that as the number of dimensions increases, (a) FM and DH perform better than OCT; (b) in accordance with Theorem 2, the median number of FM\u2019s expansions decreases; and (c) FM\u2019s MADs decrease. When FM\u2019s MADs are high, the variabilities can possibly be exploited in future work using Rapid Randomized Restart strategies.\n8The dimensionality of the Euclidean embedding for FM matches the number of pivots in DH.\nFastMap also gives us a framework to identify a point of diminishing returns with increasing dimensionality. This happens when the distance between the farthest pair of nodes stops being \u201csignificant\u201d. For example, such a point is observed in Figure 4(f) around dimensionality 5.9\nIn mazes, such as in Figure 4(g), DH outperforms FM. This leads us to believe that FM provides good heuristic guidance in domains that can be approximated with a low-dimensional manifold. This observation also motivates us to create a hybrid FM+DH heuristic by taking the max of the two. Some relevant results are shown in Table 1. Here, all heuristics have equal memory resources. We observe that FM(5)+DH(5) always performs second best compared to FM(10) and DH(10). On the one hand, this decreases the percentages of instances on which it expands the least number of nodes. But, on the other hand, its performance is not far from that of the best technique in each breakdown."}, {"heading": "Conclusions", "text": "In this paper, we presented a near-linear time preprocessing algorithm, dubbed FastMap, for producing a Euclidean embedding of a general edge-weighted undirected graph. At runtime, these Euclidean distances were used as heuristic estimates by A* for shortest path computations. We proved that the FastMap heuristic is admissible and consistent, thereby generating optimal paths. FastMap is significantly faster than competing approaches for producing Euclidean embeddings with optimality guarantees. We also showed that it is competitive with other state-of-the-art heuristics derived in nearlinear preprocessing time. However, our method has the combined benefits of requiring only near-linear preprocessing time as well as producing explicit Euclidean embeddings that try to recover the underlying manifolds of the given graphs.\n9The farthest pair distances, computed in line 4 of Algorithm 1, for the first 10 dimensions are: \u3008581, 36, 22, 15, 14, 10, 6, 6, 5, 4\u3009."}], "references": [{"title": "The MIT Press", "author": ["Ethem Alpaydin. Introduction to Machine Learning"], "venue": "2nd edition,", "citeRegEx": "Alpaydin. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of the Sixth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment", "author": ["Yngv Bj\u00f6rnsson", "K\u00e1ri Halld\u00f3rsson. Improved heuristics for optimal path-finding on game maps"], "venue": "pages 9\u201314,", "citeRegEx": "Bj\u00f6rnsson and Halld\u00f3rsson. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Journal of Game Development", "author": ["Adi Botea", "Martin M\u00fcller", "Jonathan Schaeffer. Near optimal hierarchical path-finding"], "venue": "1:7\u201328,", "citeRegEx": "Botea et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimizations of data structures", "author": ["T. Cazenave"], "venue": "heuristics and algorithms for path-finding on maps. In Proceedings of the 2006 IEEE Symposium on Computational Intelligence and Games, pages 27\u201333", "citeRegEx": "Cazenave. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "The Morgan Kaufmann Series in Artificial Intelligence", "author": ["Rina Dechter. Constraint processing"], "venue": "Elsevier,", "citeRegEx": "Dechter. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Numerische Mathematik", "author": ["Edsger W. Dijkstra. A note on two problems in connexion with graphs"], "venue": "1(1):269\u2013271,", "citeRegEx": "Dijkstra. 1959", "shortCiteRegEx": null, "year": 1959}, {"title": "Fastmap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets", "author": ["Faloutsos", "Lin", "1995] Christos Faloutsos", "King-Ip Lin"], "venue": "In Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Faloutsos et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Faloutsos et al\\.", "year": 1995}, {"title": "Contraction hierarchies: Faster and simpler hierarchical routing in road networks", "author": ["R. Geisberger", "P. Sanders", "D. Schultes", "D. Delling"], "venue": "Proceedings of the 7th International Conference on Experimental Algorithms", "citeRegEx": "Geisberger et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Portal-based true-distance heuristics for path finding", "author": ["M. Goldenberg", "A. Felner", "N. Sturtevant", "J. Schaeffer"], "venue": "Proceedings of the Third Annual Symposium on Combinatorial Search", "citeRegEx": "Goldenberg et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence", "author": ["Meir Goldenberg", "Nathan Sturtevant", "Ariel Felner", "Jonathan Schaeffer. The compressed differential heuristic"], "venue": "pages 24\u201329,", "citeRegEx": "Goldenberg et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Cybernetics", "author": ["Peter E. Hart", "Nils J. Nilsson", "Bertram Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems", "Science"], "venue": "SSC-4(2):100\u2013107,", "citeRegEx": "Hart et al.. 1968", "shortCiteRegEx": null, "year": 1968}, {"title": "Searching with abstractions: A unifying framework and new highperformance algorithm", "author": ["R.C. Holte", "C. Drummond", "M.B. Perez", "R.M. Zimmer", "A.J. Macdonald"], "venue": "Proceedings of the 10th Canadian Conference on Artificial Intelligence", "citeRegEx": "Holte et al.. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "NY", "author": ["Steven LaValle. Planning Algorithms. Cambridge University Press", "New York"], "venue": "USA,", "citeRegEx": "LaValle. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Search space reduction using swamp hierarchies", "author": ["N. Pochter", "A. Zohar", "J. Rosenschein", "A. Felner"], "venue": "Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, pages 155\u2013160", "citeRegEx": "Pochter et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence", "author": ["Chris Rayner", "Michael Bowling", "Nathan Sturtevant. Euclidean heuristic optimization"], "venue": "pages 81\u201386,", "citeRegEx": "Rayner et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["Stuart J. Russell", "Peter Norvig"], "venue": "Prentice Hall,", "citeRegEx": "Russell and Norvig. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Contraction hierarchies on grid graphs", "author": ["S. Storandt"], "venue": "Proceedings of the 36th Annual Conference on Artificial Intelligence", "citeRegEx": "Storandt. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the Twentieth AAAI Conference on Artificial Intelligence", "author": ["Nathan Sturtevant", "Michael Buro. Partial pathfinding using map abstraction", "refinement"], "venue": "pages 1392\u20131397,", "citeRegEx": "Sturtevant and Buro. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Ariel Felner", "author": ["N. Sturtevant"], "venue": "Max Barrer, Jonathan Schaeffer, and Neil Burch. Memorybased heuristics for explicit state spaces. In Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence, pages 609\u2013614", "citeRegEx": "Sturtevant et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Transactions on Computational Intelligence and AI in Games", "author": ["Nathan Sturtevant. Benchmarks for gridbased pathfinding"], "venue": "4(2):144 \u2013 148,", "citeRegEx": "Sturtevant. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Fibonacci heaps and their uses in improved network optimization algorithms", "author": ["R.E. Tarjan", "M.L. Fredman"], "venue": "2013 IEEE 54th Annual Symposium on Foundations of Computer Science", "citeRegEx": "Tarjan and Fredman. 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "Multidimensional scaling: I", "author": ["Warren S. Torgerson"], "venue": "theory and method. Psychometrika, 17(4):401\u2013 419,", "citeRegEx": "Torgerson. 1952", "shortCiteRegEx": null, "year": 1952}, {"title": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence", "author": ["Tansel Uras", "Sven Koenig. Identifying hierarchies for fast optimal search"], "venue": "pages 878\u2013884,", "citeRegEx": "Uras and Koenig. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "editor", "author": ["Tansel Uras", "Sven Koenig. Subgoal graphs for fast optimal pathfinding. In Steve Rabin"], "venue": "Game AI Pro 2: Collected Wisdom of Game AI Professionals, chapter 15, pages 145\u2013160. A K Peters/CRC Press,", "citeRegEx": "Uras and Koenig. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "SIAM REVIEW", "author": ["Lieven Vandenberghe", "Stephen Boyd. Semidefinite programming"], "venue": "38:49\u201395,", "citeRegEx": "Vandenberghe and Boyd. 1996", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 23, "context": "In video games, for example, a large fraction of CPU cycles are spent on shortest path computations [Uras and Koenig, 2015].", "startOffset": 100, "endOffset": 123}, {"referenceID": 12, "context": "Many other tasks in AI, including motion planning [LaValle, 2006], temporal reasoning [Dechter, 2003], and decision making [Russell and Norvig, 2009], also involve finding and reasoning about shortest paths.", "startOffset": 50, "endOffset": 65}, {"referenceID": 4, "context": "Many other tasks in AI, including motion planning [LaValle, 2006], temporal reasoning [Dechter, 2003], and decision making [Russell and Norvig, 2009], also involve finding and reasoning about shortest paths.", "startOffset": 86, "endOffset": 101}, {"referenceID": 15, "context": "Many other tasks in AI, including motion planning [LaValle, 2006], temporal reasoning [Dechter, 2003], and decision making [Russell and Norvig, 2009], also involve finding and reasoning about shortest paths.", "startOffset": 123, "endOffset": 149}, {"referenceID": 5, "context": "While Dijkstra\u2019s algorithm [Dijkstra, 1959] can be used to compute shortest paths in polynomial time, faster computations bear important implications on the time-efficiency of solving the aforementioned tasks.", "startOffset": 27, "endOffset": 43}, {"referenceID": 10, "context": "One way to boost shortest path computations is to use the A* search framework with an informed heuristic [Hart et al., 1968].", "startOffset": 105, "endOffset": 124}, {"referenceID": 2, "context": "Hierarchical abstractions that yield suboptimal paths have been used to reduce the size of the search space by abstracting groups of vertices [Botea et al., 2004; Sturtevant and Buro, 2005].", "startOffset": 142, "endOffset": 189}, {"referenceID": 17, "context": "Hierarchical abstractions that yield suboptimal paths have been used to reduce the size of the search space by abstracting groups of vertices [Botea et al., 2004; Sturtevant and Buro, 2005].", "startOffset": 142, "endOffset": 189}, {"referenceID": 1, "context": "More informed heuristics [Bj\u00f6rnsson and Halld\u00f3rsson, 2006; Cazenave, 2006; Sturtevant et al., 2009] guide the searches better to expand fewer states.", "startOffset": 25, "endOffset": 99}, {"referenceID": 3, "context": "More informed heuristics [Bj\u00f6rnsson and Halld\u00f3rsson, 2006; Cazenave, 2006; Sturtevant et al., 2009] guide the searches better to expand fewer states.", "startOffset": 25, "endOffset": 99}, {"referenceID": 18, "context": "More informed heuristics [Bj\u00f6rnsson and Halld\u00f3rsson, 2006; Cazenave, 2006; Sturtevant et al., 2009] guide the searches better to expand fewer states.", "startOffset": 25, "endOffset": 99}, {"referenceID": 11, "context": "Hierarchies can also be used to derive heuristics during search [Leighton et al., 2008; Holte et al., 1994].", "startOffset": 64, "endOffset": 107}, {"referenceID": 1, "context": "Dead-end detection and other pruning methods [Bj\u00f6rnsson and Halld\u00f3rsson, 2006; Goldenberg et al., 2010; Pochter et al., 2010] identify areas of the graph that do not need to be searched to find shortest paths.", "startOffset": 45, "endOffset": 125}, {"referenceID": 8, "context": "Dead-end detection and other pruning methods [Bj\u00f6rnsson and Halld\u00f3rsson, 2006; Goldenberg et al., 2010; Pochter et al., 2010] identify areas of the graph that do not need to be searched to find shortest paths.", "startOffset": 45, "endOffset": 125}, {"referenceID": 13, "context": "Dead-end detection and other pruning methods [Bj\u00f6rnsson and Halld\u00f3rsson, 2006; Goldenberg et al., 2010; Pochter et al., 2010] identify areas of the graph that do not need to be searched to find shortest paths.", "startOffset": 45, "endOffset": 125}, {"referenceID": 7, "context": "Search with contraction hierarchies [Geisberger et al., 2008] is an optimal and extremely hierarchical method, as every level of the hierarchy contains only a single node.", "startOffset": 36, "endOffset": 61}, {"referenceID": 16, "context": "It has been shown to be effective on road networks but seems to be less effective on graphs with higher branching factors, such as gridbased game maps [Storandt, 2013].", "startOffset": 151, "endOffset": 167}, {"referenceID": 22, "context": "Another approach is that of N-level graphs [Uras and Koenig, 2014] constructed from undirected graphs by partitioning the nodes into levels.", "startOffset": 43, "endOffset": 66}, {"referenceID": 14, "context": "One such approach is presented in [Rayner et al., 2011].", "startOffset": 34, "endOffset": 55}, {"referenceID": 24, "context": "SDPs can be solved in polynomial time [Vandenberghe and Boyd, 1996]; and in this case, additional structure is leveraged to solve them in cubic time [Rayner et al.", "startOffset": 38, "endOffset": 67}, {"referenceID": 14, "context": "SDPs can be solved in polynomial time [Vandenberghe and Boyd, 1996]; and in this case, additional structure is leveraged to solve them in cubic time [Rayner et al., 2011].", "startOffset": 149, "endOffset": 170}, {"referenceID": 14, "context": "However, unlike the approach in [Rayner et al., 2011], it does not produce an explicit Euclidean embedding.", "startOffset": 32, "endOffset": 53}, {"referenceID": 18, "context": "The shortest path distances between each pivot node and every other node are precomputed and stored [Sturtevant et al., 2009].", "startOffset": 100, "endOffset": 125}, {"referenceID": 9, "context": "The required space is linear in the number of pivots times the number of nodes, although a more succinct representation is presented in [Goldenberg et al., 2011].", "startOffset": 136, "endOffset": 161}, {"referenceID": 8, "context": "Similar preprocessing techniques are used in Portal-Based True Distance heuristics [Goldenberg et al., 2010].", "startOffset": 83, "endOffset": 108}, {"referenceID": 0, "context": "Clustering techniques, such as the k-means algorithm, are well studied in Machine Learning [Alpaydin, 2010]; but they cannot be applied directly to domains with abstract objects as described above.", "startOffset": 91, "endOffset": 107}, {"referenceID": 21, "context": "One of the early approaches for generating such an embedding was based on the idea of multi-dimensional scaling (MDS) [Torgerson, 1952].", "startOffset": 118, "endOffset": 135}, {"referenceID": 20, "context": "Computing dai and dib for any single Oi can no longer be done in constant time but requiresO(|E|+|V | log |V |) time instead [Tarjan and Fredman, 1984].", "startOffset": 125, "endOffset": 151}, {"referenceID": 19, "context": "We set up experiments on many benchmark maps from [Sturtevant, 2012].", "startOffset": 50, "endOffset": 68}], "year": 2017, "abstractText": "We present a new preprocessing algorithm for embedding the nodes of a given edge-weighted undirected graph into a Euclidean space. In this space, the Euclidean distance between any two nodes approximates the length of the shortest path between them in the given graph. Later, at runtime, a shortest path between any two nodes can be computed using A* search with the Euclidean distances as heuristic estimates. Our preprocessing algorithm, dubbed FastMap, is inspired by the Data Mining algorithm of the same name and runs in nearlinear time. Hence, FastMap is orders of magnitude faster than competing approaches that produce a Euclidean embedding using Semidefinite Programming. Our FastMap algorithm also produces admissible and consistent heuristics and therefore guarantees the generation of optimal paths. Moreover, FastMap works on general undirected graphs for which many traditional heuristics, such as the Manhattan Distance heuristic, are not always well defined. Empirically too, we demonstrate that the FastMap heuristic is competitive with other stateof-the-art heuristics like the Differential heuristic. Introduction and Related Work Shortest path problems commonly occur in the inner procedures of many AI programs. In video games, for example, a large fraction of CPU cycles are spent on shortest path computations [Uras and Koenig, 2015]. Many other tasks in AI, including motion planning [LaValle, 2006], temporal reasoning [Dechter, 2003], and decision making [Russell and Norvig, 2009], also involve finding and reasoning about shortest paths. While Dijkstra\u2019s algorithm [Dijkstra, 1959] can be used to compute shortest paths in polynomial time, faster computations bear important implications on the time-efficiency of solving the aforementioned tasks. One way to boost shortest path computations is to use the A* search framework with an informed heuristic [Hart et al., 1968]. A perfect heuristic is one that returns the true shortest path distance between any two nodes in a given graph. In this graph, A* with such a heuristic and proper tie-breaking is guaranteed to expand nodes only on an optimal path between the specified start and goal nodes. In general, computing the perfect heuristic value between two nodes is as hard as computing the shortest path between them. Hence, A* search can benefit from a perfect heuristic only if it is computed offline. However, precomputing all pairwise shortest path distances is not only time-intensive but also requires a prohibitive O(N) memory where N is the number of nodes. Many methods for preprocessing a given graph (without precomputing all pairwise shortest path distances) have been studied before and can be grouped into several categories. Hierarchical abstractions that yield suboptimal paths have been used to reduce the size of the search space by abstracting groups of vertices [Botea et al., 2004; Sturtevant and Buro, 2005]. More informed heuristics [Bj\u00f6rnsson and Halld\u00f3rsson, 2006; Cazenave, 2006; Sturtevant et al., 2009] guide the searches better to expand fewer states. Hierarchies can also be used to derive heuristics during search [Leighton et al., 2008; Holte et al., 1994]. Dead-end detection and other pruning methods [Bj\u00f6rnsson and Halld\u00f3rsson, 2006; Goldenberg et al., 2010; Pochter et al., 2010] identify areas of the graph that do not need to be searched to find shortest paths. Search with contraction hierarchies [Geisberger et al., 2008] is an optimal and extremely hierarchical method, as every level of the hierarchy contains only a single node. It has been shown to be effective on road networks but seems to be less effective on graphs with higher branching factors, such as gridbased game maps [Storandt, 2013]. Another approach is that of N-level graphs [Uras and Koenig, 2014] constructed from undirected graphs by partitioning the nodes into levels. The hierarchy allows significant pruning during search. A different approach that does not rely on preprocessing the graph makes use of some notion of a geometric distance between two nodes as a heuristic estimate of the shortest path distance between them. One such common heuristic that is used in gridworlds is the Manhattan Distance heuristic.1 For many gridworlds, A* search with the Manhattan Distance heuristic outperforms Dijkstra\u2019s algorithm. However, in complicated 2D/3D gridworlds like mazes, the Manhattan Distance heuristic may not be informed enough to efficiently guide A* search. Another issue associated with Manhattan In a 4-connected 2D gridworld, for example, the Manhattan Distance between two cells (x1, y1) and (x2, y2) is |x1\u2212x2|+|y1\u2212y2|. Similar generalizations exist for 3D and 8-connected gridworlds. ar X iv :1 70 6. 02 79 2v 1 [ cs .A I] 8 J un 2 01 7 Distance-like heuristics is that they are not well defined for general graphs.2 For a graph that cannot be conceived in a geometric space, there is no closed-form formula for a \u201cgeometric\u201d heuristic estimate for the distance between two nodes because there are no coordinates associated with them. For a graph that does not already have a geometric embedding in Euclidean space, a preprocessing algorithm can be used to generate one. As described before, at runtime, A* search would then use the Euclidean distance between any two nodes in this space as an estimate for the length of the shortest path between them in the given graph. One such approach is presented in [Rayner et al., 2011]. This approach guarantees admissiblility and consistency of the heuristic and therefore generates optimal paths. However, it requires solving a Semidefinite Program (SDP) in its preprocessing phase. SDPs can be solved in polynomial time [Vandenberghe and Boyd, 1996]; and in this case, additional structure is leveraged to solve them in cubic time [Rayner et al., 2011]. Still, a cubic preprocessing time limits the size of the graphs that are amenable to this approach. The Differential heuristic is another state-of-the-art approach that has the benefits of near-linear preprocessing time. However, unlike the approach in [Rayner et al., 2011], it does not produce an explicit Euclidean embedding. In the preprocessing phase of the Differential heuristic approach, some nodes of the graph are chosen as pivot nodes. The shortest path distances between each pivot node and every other node are precomputed and stored [Sturtevant et al., 2009]. At runtime, the heuristic distance between two nodes, a and b, is given by maxp |d(a, p)\u2212 d(p, b)| where p is a pivot node and d(, ) is the precomputed distance. The preprocessing time is linear in the number of pivots times the size of the graph. The required space is linear in the number of pivots times the number of nodes, although a more succinct representation is presented in [Goldenberg et al., 2011]. Similar preprocessing techniques are used in Portal-Based True Distance heuristics [Goldenberg et al., 2010]. In this paper, we present a new preprocessing algorithm that produces an explicit Euclidean embedding while running in near-linear time. It therefore has the benefits of the Differential heuristic\u2019s preprocessing time as well as that of producing an embedding from which heuristic estimates can be quickly computed using closed-form formulas. Our preprocessing algorithm, dubbed FastMap, is inspired by the Data Mining algorithm of the same name [Faloutsos and Lin, 1995]. It is orders of magnitude faster than SDP-based approaches for producing Euclidean embeddings. FastMap also produces admissible and consistent heuristics and therefore guarantees the generation of optimal paths. In comparison to other heuristics derived from closed-form formulas, like the Manhattan or the Octile Distance heuristics, the FastMap heuristic has several advantages. First, it is defined for general undirected graphs (even if they are not gridworlds). Second, we observe empirically that even in gridworlds, A* with the FastMap heuristic outperforms A* with the Manhattan or the Octile Distance heuristic. In comHenceforth, whenever we refer to a graph, we mean an edgeweighted undirected graph unless stated otherwise. parison to the Differential heuristic with the same memory resources, the FastMap heuristic is not only competitive with it on some graphs but even outperforms it on some others. This performance of FastMap is encouraging given that it produces an explicit Euclidean embedding that has other representational benefits like recovering the underlying manifolds of the graph and/or visualizing it. Moreover, we observe that the FastMap and the Differential heuristics have complementary strengths and can be easily combined to generate a more informed heuristic. The Origin of FastMap The FastMap algorithm [Faloutsos and Lin, 1995] was introduced in the Data Mining community for automatically generating geometric embeddings of abstract objects. For example, if we are given objects in the form of long DNA strings, multimedia datasets such as voice excerpts or images, or medical datasets such as ECGs or MRIs, there is no geometric space in which these objects can be naturally visualized. However, in many of these domains, there is still a well defined distance function between every pair of objects. For example, given two DNA strings, the edit distance between them3 is well defined although an individual DNA string cannot be conceptualized in a geometric space. Clustering techniques, such as the k-means algorithm, are well studied in Machine Learning [Alpaydin, 2010]; but they cannot be applied directly to domains with abstract objects as described above. This is because these algorithms assume that the objects are described as points in a geometric space. FastMap revives the applicability of these clustering techniques by first creating an artificial Euclidean embedding for the abstract objects. The Euclidean embedding is such that the pairwise distances are approximately preserved. Such an embedding would also help in the visualization of the abstract objects. This visualization, for example, can aid physicians in identifying correlations between symptoms or other patterns from medical records. We are given a complete undirected edge-weighted graph G = (V,E). Each vertex vi \u2208 V represents an abstract object Oi. Between any two vertices, vi and vj , there is an edge (vi, vj) \u2208 E with weight D(Oi, Oj). Here, D(Oi, Oj) is the given pairwise distance between the objects Oi and Oj . A Euclidean embedding assigns to each object Oi a Kdimensional point pi \u2208 R . A good Euclidean embedding is one in which the Euclidean distance between any two points, pi and pj , closely approximates D(Oi, Oj). One of the early approaches for generating such an embedding was based on the idea of multi-dimensional scaling (MDS) [Torgerson, 1952]. Here, overall distortion of the pairwise distances is measured in terms of the \u201cenergy\u201d stored in \u201csprings\u201d connecting each pair of objects. MDS, however, requires O(N) time (N = |V |) and hence does not scale well in practice. On the other hand, FastMap [Faloutsos and Lin, 1995] requires only linear time. Both methods embed the objects in a K-dimensional space for a user-specified K. The edit distance between two strings is the minimum number of insertions, deletions or substitutions that are needed to transform one to the other.", "creator": "LaTeX with hyperref package"}}}