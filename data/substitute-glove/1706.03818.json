{"id": "1706.03818", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings", "abstract": "Query - led - example find thought uses dynamic time warping (DTW) they negative pertinent and proposed placing segments. Recent there on yet that trends praising gamut by families them instead fixed - nonlinear invariant - - - instrument word orks - - - used 7.9 rest vector speed (e. h2o. , cosine angle) do discriminate during instance these calculate number DTW - based range. We consider an approach even query - he - example effort no embeds are set confusing work copy network according taken a neural used, when that home - neighbor search giving might first uniform feature. Earlier not followed embedding - has query - subsequently - example, using tool - example drum means voltammetry, accomplished efficient given. We seeing that our amphiphilic, 's as recurrent intelligences segments volunteered should tool word unfair, promise substantial provided during score for run - time improves into another previous create.", "histories": [["v1", "Mon, 12 Jun 2017 19:30:57 GMT  (477kb,D)", "http://arxiv.org/abs/1706.03818v1", "To appear Interspeech 2017"]], "COMMENTS": "To appear Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shane settle", "keith levin", "herman kamper", "karen livescu"], "accepted": false, "id": "1706.03818"}, "pdf": {"name": "1706.03818.pdf", "metadata": {"source": "CRF", "title": "Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings", "authors": ["Shane Settle", "Keith Levin", "Herman Kamper", "Karen Livescu"], "emails": ["klivescu}@ttic.edu,", "klevin@jhu.edu"], "sections": [{"heading": null, "text": "1. Introduction Query-by-example speech search (QbE) is the task of searching for a spoken query term (a word or phrase) in a collection of speech recordings. Unlike keyword search and spoken term detection, where the search terms are given as text, QbE involves matching audio segments directly. This task arises naturally when the search terms may be out-of-vocabulary [1, 2], in handsfree settings, or in low- or zero-resource settings [3].\nFor QbE in high-resource settings, one can train a model to map the audio query to a sequence of subword units, such as phonemes, and search for this sequence in a lattice built from the search collection [2, 4]. This approach requires very significant resources, since it involves much the same process as training a full speech recognition system.\nIn low-resource settings, typical approaches for this task use dynamic time warping (DTW) to determine the similarity between audio segments. Early approaches to low-resource QbE were based on performing DTW alignment of the query against a search collection either exactly [5, 6] or approximately [7\u20139].\nAn alternative to DTW for QbE, which we explore in this paper, is to represent variable-duration speech segments as fixeddimensional vectors and directly measure similarity between them via a simple vector distance. In this approach, shown in Figure 1, the query is embedded using an acoustic word embedding function, producing a vector representation of the query. All potential segments in the search collection are then represented as vectors using the same embedding function. The putative hits (matches) correspond to those segments in the search collection that are closest to the query in the fixed-dimensional embedding space. This type of approach requires preprocessing steps for learning the embedding function and generating the embeddings\nfor the search collection. At test time, efficient approximate nearest-neighbor search can greatly speed up computation.\nIn prior work, Levin et al. [10] used a template-based acoustic word embedding function, and showed that this type of embedding-based QbE search can greatly speed up search compared to a purely DTW-based system, while matching or improving performance. Their template-based embedding approach does not require any labeled supervision. However, in many practical settings, a limited amount of training data might be available. In this work we consider this low-resource setting; in particular, we use acoustic word embeddings based on neural models learned to discriminate between words given a limited (roughly 2-hour) training set.\nWe build on a growing body of work on neural networkbased acoustic word embeddings [11\u201315]. In several of these studies, neural approaches are shown to far outperform templatebased embeddings (such as those used in [10]) on an isolatedword discrimination task, which can be viewed as a proxy for QbE. Here, we use the neural embedding approach of [13], based on Siamese recurrent neural networks, and incorporate these into a complete QbE system using the embedding-based approach of Levin et al. [10]. We show that these neural embeddings, trained only on a small amount of labeled data, achieve large improvements in true QbE performance."}, {"heading": "2. Neural embedding-based QbE", "text": "As illustrated in Figure 1, embedding-based query-by-example (QbE) consists of an embedding method and a nearest neighbor search component. We first describe our neural acoustic word embedding approach, and then give details of the embeddingbased QbE search system in which the embeddings are used."}, {"heading": "2.1. Neural acoustic word embeddings (NAWEs)", "text": "An acoustic word embedding function g maps a variable-length speech segment Y = y1, y2, . . . , yT , where each yi is an acoustic feature frame, to a single embedding vector x \u2208 Rd. Ideally,\nar X\niv :1\n70 6.\n03 81\n8v 1\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n17\ng should map instances of the same word to nearby vectors, while different words are mapped far apart. Once segments are embedded, they can be compared by computing a vector distance between their embeddings, rather than using DTW.\nIn [16], Levin et al. proposed a template-based approach for the embedding function g. For a target segment, a reference vector is defined as the vector of DTW alignment costs to a set of template segments. Dimensionality reduction (based on Laplacian eigenmaps [17]) is then applied to the reference vector to obtain an embedding in Rd. This template-based embedding approach was subsequently used for unsupervised speech recognition in [18] and, more importantly, the full QbE system of [10], which we consider to be a baseline in our experiments.\nRecently, neural acoustic word embeddings (NAWEs) have been proposed as an alternative [11\u201315]. In this recent work, NAWEs have achieved much better performance than the template-based approach, but only in an isolated-word discrimination task that can be seen as a proxy for QbE [19]. Here, we specifically focus on the NAWE approach developed in [13], where it was shown that embeddings based on Long Short-Term Memory (LSTM) [20] networks outperform competing feedforward and convolutional methods. Rather than the proxy task, here we apply these NAWEs in a complete QbE system.\nConcretely, we use the concatenation of the hidden representations from a deep bidirectional LSTM network as our embedding function, i.e. x = g(Y ) = [ \u2212\u2192 hT ; \u2190\u2212 h1], where \u2212\u2192 hT , \u2190\u2212 h1 refer to the final hidden state vector from the forward and backward LSTMs, respectively. This LSTM is trained using a Siamese weight-sharing scheme [21] depicted in Figure 2 with a contrastive triplet loss [22, 23], lcos hinge(Ya, Ys), defined as\nmax { 0,m+ dcos(xa, xs)\u2212 max\nxd\u2208D dcos(xa, xd) } In this definition, Ya and Ys are two segments that have the\nsame word label, and xa, xs are their embeddings as output by the neural embedding network. The goal is to push the embeddings xa and xs together, until they are closer to each other by a margin m than the embedding xd of a negative example. Here dcos(x1, x2) = (1 \u2212 cos(x1, x2)) is the cosine distance between vectors x1 and x2. Rather than sampling a single negative example as in [12], or keeping track of confusion statistics as in [13], we sample a set of k embedded segments D from the whole training set with labels different from Ya and consider only the example embedding, xd \u2208 D, that most violates the margin constraint. This improves both performance and rate of convergence on the proxy task."}, {"heading": "2.2. Embedding-based QbE", "text": "Our system needs to quickly retrieve from a large collection those segments nearest to a given spoken query. For this, we use the Segmental Randomized Acoustic Indexing and LogarithmicTime Search (S-RAILS) system [10], an embedding-based QbE approach. Although S-RAILS was first applied using the template-based embedding method, it is agnostic to the embedding type, and here we apply it to our neural embeddings.\nS-RAILS provides a simple platform for performing approximate nearest neighbor search over vectors, relying on a version of locality-sensitive hashing (LSH) [24, 25]. Let X = {x1, x2, . . . , xN} \u2208 Rd be the search collection. LSH is a method for representing vectors in Rd as bit vectors, referred to as signatures, such that if two vectors xi, xj \u2208 X are close under the cosine distance, then their signatures si, sj \u2208 {0, 1}b will agree in most of their entries.\nS-RAILS uses LSH to replace the comparatively expensive d-dimensional cosine distance between embeddings with a fast approximation. S-RAILS arranges the signatures s(X ) = {si : 1 \u2264 i \u2264 N} into a lexicographically sorted list S. Given a query vector q \u2208 Rd, we map q to its LSH signature s = s(q) \u2208 {0, 1}b, and find its location in the sorted signature list S in O(log b) time. A set of (approximate) near neighbors to q can be read off this list by looking at the B entries appearing before s and the B entries after s. Bits appearing earlier in the signature have far more influence on whether or not two vectors xi, xj \u2208 X will be judged similar. To ameliorate this effect, S-RAILS performs this lexicographic lookup under P different permutations of the bits: \u03c01, \u03c02, . . . , \u03c0P \u2208 Sb.\nS-RAILS has three parameters: the signature length b, the beamwidth B, and the number of permutations P . Increasing any of these parameters will tend to improve performance either because it increases the fidelity of our approximation to the cosine distance (in the case of b and P ) or because it improves recall (in the case ofB). However, any such improvements come at the cost of increased memory required to store the index and the permuted lists (in the case of b and P ) and increased runtime (in the case of B and, to a lesser extent, b and P ). All told, building the index requires O(PbN logN) time in the worst case, and querying the index requires O(B + Pb logN) time."}, {"heading": "3. Experimental setup", "text": "We use data from the Switchboard corpus of (primarily American) English conversational telephone speech [26]. For training the NAWE model, we use a training set consisting of approximately 10k word segments covering less than 2 hours of speech taken from conversation sides distinct from those used to extract the query set and the evaluation collection. The size of this set is comparable to those used for training in prior work on acoustic word embeddings [13, 27, 28]. As acoustic features, we use 39-dimensional MFCC+\u2206+\u2206\u2206s. For QbE, we partition Switchboard into a 37-hour set from which to draw our query terms, a 48-hour development search collection on which to tune parameters of S-RAILS, and a 433-hour evaluation set. These partitions are identical to those used in prior work [8, 10] for the QbE task. We use a set of 43 query words previously used in [8, 10], which were chosen subject to the constraints that the median word duration of each type across the entire corpus is at least 0.5 seconds and the orthographic representation of each word type has at least six characters [8, 10, 16]. Each word type appears 20 to 162 times in the query set, 2 to 188 times in the development search collection, and 39 to 1386 times in the\nevaluation set. For our NAWE model (see Section 2.1), we use a stacked 3-layer bidirectional LSTM with 256 hidden units in each direction; the embeddings produced by the model are therefore 512-dimensional. Dropout is applied with probability 0.3 between LSTM layers. For the margin of the contrastive loss, lcos hinge, we use m = 0.5, and we sample k = 10 negative instances per anchor segment. We use the Adam optimization algorithm [29] with a batch size of 32, learning rate of 0.001, \u03b21 = 0.9, \u03b22 = 0.999, and = 1 \u00b710\u22128. We tuned these parameters based on development set performance on the isolated word discrimination task of [19]. For our QbE evaluation experiments, we trained all models for 100 epochs.\nWe evaluated the quality of search results according to three commonly used metrics: figure-of-merit (FOM), oracular term weighted value (OTWV), and precision at 10 (P@10). FOM is the recall averaged over the ten operating points at which the false alarm rate per hour of search audio is equal to 1, 2, . . . , 10. OTWV is a query-specific weighted difference between the recall and the false alarm rate (further explanation can be found in [30]). P@10 is the fraction of the ten top-scoring results that are correct matches to the query.\nSince the multiple query examples within each query type can have significant variation, we report average median example and average maximum example scores for each of these three metrics. That is, we compute the median and maximum score over all examples of each query type, and report an unweighted arithmetic mean across the 43 query types.\n4. Results We first present QbE performance on the development data in order to show how performance differs across parameter settings, and then give evaluation results. In this section, we refer to the QbE system that employs the original template-based embeddings simply as S-RAILS, and to the system with neural embeddings as S-RAILS+NAWE."}, {"heading": "4.1. Development set performance", "text": "Figure 3 shows development set performance, in terms of median P@10, for the baseline QbE system using template-"}, {"heading": "P FOM OTWV P@10 FOM OTWV P@10", "text": ""}, {"heading": "4 48.8 33.2 45.2 75.2 59.0 83.0", "text": ""}, {"heading": "8 60.9 41.0 50.3 80.3 63.8 85.0", "text": ""}, {"heading": "10000 74.6 49.5 54.2 86.3 67.9 84.8", "text": "based embeddings (S-RAILS) and our system using NAWEs (S-RAILS+NAWE). Tables 1, 2, and 3 show development set performance for S-RAILS+NAWE as the signature length b, permutations P , and beamwidth B are varied, respectively.\nFigure 3 shows that neural embeddings improve the performance of S-RAILS by large margins at all running time operating points. This figure also shows that increased signature length yields much larger improvements in P@10 for S-RAILS+NAWE than it does for the baseline S-RAILS system. Significant improvements in P@10 can be seen when holding fixed any combination of settings for P and B. Our performance on P@10 saturates with signatures around 1024 bits, while S-RAILS\u2019 saturates, for the most part, at 256 bits.\nAgain in contrast to the S-RAILS system, our method responds strongly to increases in the number of permutations used. In both Figure 3 and Table 2, adjustment to this parameter improves performance consistently across signature lengths. This is to be expected if the neural embeddings provide a better measure of speech segment distances, since the increased number of permutations helps provide a more exact estimate of the embedding distance. We note that performance as measured in Table 2 has not plateaued in any of the Median Example metrics. Further increasing the number of permutations may further improve these metrics, but this incurs a large cost in memory.\nFigure 3 and Table 3 show that, except for the cases with short signatures and few permutations, increasing beamwidth does not improve P@10 performance, while incurring significant cost. To obtain higher precision systems, it is more important to use computational resources for increasing the number of permutations or using longer signatures. However, as would be expected, the higher beamwidths help to significantly improve the FOM score, a metric concerned primarily with recall."}, {"heading": "4.2. Evaluation set performance", "text": "Based on development results, we find that an operating point of 16 permutations, beamwidth of 2000, and signature length of 1024 is close to optimal, in terms of both performance and query speed, for both the baseline S-RAILS and S-RAILS+NAWE. We use these settings for final evaluation. For a qualitative view, Figure 4 visualizes several queries and their top hits in the evaluation collection. This visualization shows some expected properties. For example, the two \u201cMassachusetts\u201d queries and their top hits are embedded close together. Two of the false alarms for \u201cMassachusetts\u201d are the similar-sounding \u201cmessages\u201d and \u201cmath is just\u201d, while the somewhat more distant \u201cmath and science\u201d is (correctly) not retrieved.\nFinal evaluation performance is shown in Table 4. Besides the S-RAILS baseline, we also compare to RAILS [8], a DTWbased system that is optimized for speed using LSH to get approximate frame-level near neighbor matches. RAILS evaluation scores are reproduced from [8]. We find that our approach improves significantly over both RAILS and S-RAILS in terms of all performance metrics at this operating point. Note that, based on Figure 3, the improvements should hold at most operating points, including ones with much higher query speeds. The biggest gains from S-RAILS+NAWE are seen in the Median Example results, where there is a relative improvement over S-RAILS of more than 55% across all measures. In terms of FOM and OTWV, we see relative improvements of over 40% in the Best Example case. Although the baselines obtain good\nP@10, we still find large improvements in this measure as well, from 87.1% to 95.1%."}, {"heading": "5. Conclusion", "text": "We have presented an approach to query-by-exmaple speech search using neural acoustic word embeddings, demonstrating the ability of these embedding models to improve over previous methods on a realistic task. The neural embeddings are learned from a very limited set of data; one interesting future direction is to study the limits of the approach as the amount of training data is varied, or to extend it to use no labeled data at all. Another interesting aspect of the approach is that the neural embeddings are learned from speech segments that have been pre-segmented at word boundaries, but they are then applied for embedding arbitrary segments that may or may not (and usually do not) correspond to words. It is encouraging that this approach works despite the lack of non-word examples in the training data, and an interesting avenue for future work is to attempt to further improve performance by explicitly training on both word and non-word segments. Additional future directions include training a QbE system end-to-end and extending our model to operate at the level of multi-word phrases."}, {"heading": "6. Acknowledgements", "text": "This material is based upon work supported by the National Science Foundation under Grant No. IIS-1433485 and by a Google faculty award.\n7. References [1] W. Shen, C. M. White, and T. J. Hazen, \u201cA comparison of query-\nby-example methods for spoken term detection,\u201d DTIC Document, MIT Lincoln Labs, Tech. Rep., 2009.\n[2] C. Parada, A. Sethy, and B. Ramabhadran, \u201cQuery-by-example spoken term detection for oov terms,\u201d in Proc. ASRU, 2009.\n[3] I. Szo\u0308ke, L. J. Rodr\u0131\u0301guez-Fuentes, A. Buzo, X. Anguera, F. Metze, J. Proenca, M. Lojka, and X. Xiong, \u201cQuery by example search on speech at mediaEval 2015.\u201d in Proc. MediaEval, 2015.\n[4] C. Allauzen, M. Mohri, and M. Saraclar, \u201cGeneral indexation of weighted automata: application to spoken utterance retrieval,\u201d in Proc. HLT-NAACL, 2004.\n[5] T. J. Hazen, W. Shen, and C. White, \u201cQuery-by-example spoken term detection using phonetic posteriorgram templates,\u201d in Proc. ASRU, 2009.\n[6] Y. Zhang and J. R. Glass, \u201cUnsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams,\u201d in Proc. ASRU, 2009.\n[7] \u2014\u2014, \u201cA piecewise aggregate approximation lower-bound estimate for posteriorgram-based dynamic time warping.\u201d in Proc. Interspeech, 2011.\n[8] A. Jansen and B. Van Durme, \u201cIndexing raw acoustic features for scalable zero resource search,\u201d in Proc. Interspeech, 2012.\n[9] G. Mantena and X. Anguera, \u201cSpeed improvements to information retrieval-based dynamic time warping using hierarchical k-means clustering,\u201d in Proc. ICASSP, 2013.\n[10] K. Levin, A. Jansen, and B. Van Durme, \u201cSegmental acoustic indexing for zero resource keyword search,\u201d in Proc. ICASSP, 2015.\n[11] Y.-A. Chung, C.-C. Wu, C.-H. Shen, and H.-Y. Lee, \u201cUnsupervised learning of audio segment representations using sequence-tosequence recurrent neural networks,\u201d in Proc. Interspeech, 2016.\n[12] H. Kamper, W. Wang, and K. Livescu, \u201cDeep convolutional acoustic word embeddings using word-pair side information,\u201d in Proc. ICASSP, 2016.\n[13] S. Settle and K. Livescu, \u201cDiscriminative acoustic word embeddings: Recurrent neural network-based approaches,\u201d in Proc. SLT, 2016.\n[14] W. He, W. Wang, and K. Livescu, \u201cMulti-view recurrent neural acoustic word embeddings,\u201d in Proc. ICLR, 2017.\n[15] K. Audhkhasi, A. Rosenberg, A. Sethy, B. Ramabhadran, and B. Kingsbury, \u201cEnd-to-end ASR-free keyword search from speech,\u201d in Proc. ICASSP, 2017.\n[16] K. Levin, K. Henry, A. Jansen, and K. Livescu, \u201cFixed-dimensional acoustic embeddings of variable-length segments in low-resource settings,\u201d in Proc. ASRU, 2013.\n[17] M. Belkin and P. Niyogi, \u201cLaplacian eigenmaps for dimensionality reduction and data representation,\u201d Neural Comput., vol. 15, no. 6, pp. 1373\u20131396, 2003.\n[18] H. Kamper, A. Jansen, and S. J. Goldwater, \u201cUnsupervised word segmentation and lexicon discovery using acoustic word embeddings,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.\n[19] M. A. Carlin, S. Thomas, A. Jansen, and H. Hermansky, \u201cRapid evaluation of speech representations for spoken term discovery,\u201d in Proc. Interspeech, 2011.\n[20] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[21] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. Sa\u0308ckinger, and R. Shah, \u201cSignature verification using a \u201cSiamese\u201d time delay neural network,\u201d Int. J. Pattern Rec., vol. 7, no. 4, pp. 669\u2013688, 1993.\n[22] S. Chopra, R. Hadsell, and Y. LeCun, \u201cLearning a similarity metric discriminatively, with application to face verification,\u201d in Proc. CVPR, 2005.\n[23] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng, \u201cGrounded compositional semantics for finding and describing images with sentences,\u201d Trans. ACL, vol. 2, pp. 207\u2013218, 2014.\n[24] P. Indyk and R. Motwani, \u201cApproximate nearest neighbors: Towards removing the curse of dimensionality,\u201d in Proc. STOC, 1998.\n[25] M. Charikar, \u201cSimilarity estimation techniques from rounding algorithms,\u201d in Proc. STOC, 2002.\n[26] J. J. Godfrey, E. C. Holliman, and J. McDaniel, \u201cSWITCHBOARD: Telephone speech corpus for research and development,\u201d in Proc. ICASSP, 1992.\n[27] H. Kamper, M. Elsner, A. Jansen, and S. J. Goldwater, \u201cUnsupervised neural network based feature extraction using weak top-down constraints,\u201d in Proc. ICASSP, 2015.\n[28] A. Jansen, S. Thomas, and H. Hermansky, \u201cWeak top-down constraints for unsupervised acoustic model training,\u201d in Proc. ICASSP, 2013.\n[29] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. ICLR, 2014.\n[30] D. R. H. Miller, M. Kleber, C. Kao, O. Kimball, T. Colthurst, S. A. Lowe, R. M. Schwartz, and H. Gish, \u201cRapid and accurate spoken term detection,\u201d in Proc. Interspeech, 2007.\n[31] L. van der Maaten and G. Hinton, \u201cVisualizing data using t-SNE,\u201d J. Mach. Learn. Res., vol. 9, pp. 2579\u20132605, 2008."}], "references": [{"title": "A comparison of queryby-example methods for spoken term detection", "author": ["W. Shen", "C.M. White", "T.J. Hazen"], "venue": "DTIC Document, MIT Lincoln Labs, Tech. Rep., 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Query-by-example spoken term detection for oov terms", "author": ["C. Parada", "A. Sethy", "B. Ramabhadran"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Query by example search on speech at mediaEval 2015.", "author": ["I. Sz\u00f6ke", "L.J. Rodr\u0131\u0301guez-Fuentes", "A. Buzo", "X. Anguera", "F. Metze", "J. Proenca", "M. Lojka", "X. Xiong"], "venue": "Proc. MediaEval,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "General indexation of weighted automata: application to spoken utterance retrieval", "author": ["C. Allauzen", "M. Mohri", "M. Saraclar"], "venue": "Proc. HLT-NAACL, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Query-by-example spoken term detection using phonetic posteriorgram templates", "author": ["T.J. Hazen", "W. Shen", "C. White"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Indexing raw acoustic features for scalable zero resource search", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. Interspeech, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Speed improvements to information retrieval-based dynamic time warping using hierarchical k-means clustering", "author": ["G. Mantena", "X. Anguera"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of audio segment representations using sequence-tosequence recurrent neural networks", "author": ["Y.-A. Chung", "C.-C. Wu", "C.-H. Shen", "H.-Y. Lee"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative acoustic word embeddings: Recurrent neural network-based approaches", "author": ["S. Settle", "K. Livescu"], "venue": "Proc. SLT, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-view recurrent neural acoustic word embeddings", "author": ["W. He", "W. Wang", "K. Livescu"], "venue": "Proc. ICLR, 2017.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2017}, {"title": "End-to-end ASR-free keyword search from speech", "author": ["K. Audhkhasi", "A. Rosenberg", "A. Sethy", "B. Ramabhadran", "B. Kingsbury"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["K. Levin", "K. Henry", "A. Jansen", "K. Livescu"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Comput., vol. 15, no. 6, pp. 1373\u20131396, 2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Rapid evaluation of speech representations for spoken term discovery", "author": ["M.A. Carlin", "S. Thomas", "A. Jansen", "H. Hermansky"], "venue": "Proc. Interspeech, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Signature verification using a \u201cSiamese\u201d time delay neural network", "author": ["J. Bromley", "J.W. Bentz", "L. Bottou", "I. Guyon", "Y. LeCun", "C. Moore", "E. S\u00e4ckinger", "R. Shah"], "venue": "Int. J. Pattern Rec., vol. 7, no. 4, pp. 669\u2013688, 1993.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "Proc. CVPR, 2005.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "Trans. ACL, vol. 2, pp. 207\u2013218, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "Proc. STOC, 1998.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M. Charikar"], "venue": "Proc. STOC, 2002.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "SWITCH- BOARD: Telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "Proc. ICASSP, 1992.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1992}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["H. Kamper", "M. Elsner", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Weak top-down constraints for unsupervised acoustic model training", "author": ["A. Jansen", "S. Thomas", "H. Hermansky"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "Proc. ICLR, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Rapid and accurate spoken term detection", "author": ["D.R.H. Miller", "M. Kleber", "C. Kao", "O. Kimball", "T. Colthurst", "S.A. Lowe", "R.M. Schwartz", "H. Gish"], "venue": "Proc. Interspeech, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "J. Mach. Learn. Res., vol. 9, pp. 2579\u20132605, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "This task arises naturally when the search terms may be out-of-vocabulary [1, 2], in handsfree settings, or in low- or zero-resource settings [3].", "startOffset": 74, "endOffset": 80}, {"referenceID": 1, "context": "This task arises naturally when the search terms may be out-of-vocabulary [1, 2], in handsfree settings, or in low- or zero-resource settings [3].", "startOffset": 74, "endOffset": 80}, {"referenceID": 2, "context": "This task arises naturally when the search terms may be out-of-vocabulary [1, 2], in handsfree settings, or in low- or zero-resource settings [3].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "For QbE in high-resource settings, one can train a model to map the audio query to a sequence of subword units, such as phonemes, and search for this sequence in a lattice built from the search collection [2, 4].", "startOffset": 205, "endOffset": 211}, {"referenceID": 3, "context": "For QbE in high-resource settings, one can train a model to map the audio query to a sequence of subword units, such as phonemes, and search for this sequence in a lattice built from the search collection [2, 4].", "startOffset": 205, "endOffset": 211}, {"referenceID": 4, "context": "Early approaches to low-resource QbE were based on performing DTW alignment of the query against a search collection either exactly [5, 6] or approximately [7\u20139].", "startOffset": 132, "endOffset": 138}, {"referenceID": 5, "context": "Early approaches to low-resource QbE were based on performing DTW alignment of the query against a search collection either exactly [5, 6] or approximately [7\u20139].", "startOffset": 132, "endOffset": 138}, {"referenceID": 6, "context": "Early approaches to low-resource QbE were based on performing DTW alignment of the query against a search collection either exactly [5, 6] or approximately [7\u20139].", "startOffset": 156, "endOffset": 161}, {"referenceID": 7, "context": "Early approaches to low-resource QbE were based on performing DTW alignment of the query against a search collection either exactly [5, 6] or approximately [7\u20139].", "startOffset": 156, "endOffset": 161}, {"referenceID": 8, "context": "[10] used a template-based acoustic word embedding function, and showed that this type of embedding-based QbE search can greatly speed up search compared to a purely DTW-based system, while matching or improving performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "We build on a growing body of work on neural networkbased acoustic word embeddings [11\u201315].", "startOffset": 83, "endOffset": 90}, {"referenceID": 10, "context": "We build on a growing body of work on neural networkbased acoustic word embeddings [11\u201315].", "startOffset": 83, "endOffset": 90}, {"referenceID": 11, "context": "We build on a growing body of work on neural networkbased acoustic word embeddings [11\u201315].", "startOffset": 83, "endOffset": 90}, {"referenceID": 12, "context": "We build on a growing body of work on neural networkbased acoustic word embeddings [11\u201315].", "startOffset": 83, "endOffset": 90}, {"referenceID": 13, "context": "We build on a growing body of work on neural networkbased acoustic word embeddings [11\u201315].", "startOffset": 83, "endOffset": 90}, {"referenceID": 8, "context": "In several of these studies, neural approaches are shown to far outperform templatebased embeddings (such as those used in [10]) on an isolatedword discrimination task, which can be viewed as a proxy for QbE.", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "Here, we use the neural embedding approach of [13], based on Siamese recurrent neural networks, and incorporate these into a complete QbE system using the embedding-based approach of Levin et al.", "startOffset": 46, "endOffset": 50}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In [16], Levin et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Dimensionality reduction (based on Laplacian eigenmaps [17]) is then applied to the reference vector to obtain an embedding in R.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "This template-based embedding approach was subsequently used for unsupervised speech recognition in [18] and, more importantly, the full QbE system of [10], which we consider to be a baseline in our experiments.", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "This template-based embedding approach was subsequently used for unsupervised speech recognition in [18] and, more importantly, the full QbE system of [10], which we consider to be a baseline in our experiments.", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "Recently, neural acoustic word embeddings (NAWEs) have been proposed as an alternative [11\u201315].", "startOffset": 87, "endOffset": 94}, {"referenceID": 10, "context": "Recently, neural acoustic word embeddings (NAWEs) have been proposed as an alternative [11\u201315].", "startOffset": 87, "endOffset": 94}, {"referenceID": 11, "context": "Recently, neural acoustic word embeddings (NAWEs) have been proposed as an alternative [11\u201315].", "startOffset": 87, "endOffset": 94}, {"referenceID": 12, "context": "Recently, neural acoustic word embeddings (NAWEs) have been proposed as an alternative [11\u201315].", "startOffset": 87, "endOffset": 94}, {"referenceID": 13, "context": "Recently, neural acoustic word embeddings (NAWEs) have been proposed as an alternative [11\u201315].", "startOffset": 87, "endOffset": 94}, {"referenceID": 17, "context": "In this recent work, NAWEs have achieved much better performance than the template-based approach, but only in an isolated-word discrimination task that can be seen as a proxy for QbE [19].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "Here, we specifically focus on the NAWE approach developed in [13], where it was shown that embeddings based on Long Short-Term Memory (LSTM) [20] networks outperform competing feedforward and convolutional methods.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "Here, we specifically focus on the NAWE approach developed in [13], where it was shown that embeddings based on Long Short-Term Memory (LSTM) [20] networks outperform competing feedforward and convolutional methods.", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "This LSTM is trained using a Siamese weight-sharing scheme [21] depicted in Figure 2 with a contrastive triplet loss [22, 23], lcos hinge(Ya, Ys), defined as", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "This LSTM is trained using a Siamese weight-sharing scheme [21] depicted in Figure 2 with a contrastive triplet loss [22, 23], lcos hinge(Ya, Ys), defined as", "startOffset": 117, "endOffset": 125}, {"referenceID": 21, "context": "This LSTM is trained using a Siamese weight-sharing scheme [21] depicted in Figure 2 with a contrastive triplet loss [22, 23], lcos hinge(Ya, Ys), defined as", "startOffset": 117, "endOffset": 125}, {"referenceID": 10, "context": "Rather than sampling a single negative example as in [12], or keeping track of confusion statistics as in [13], we sample a set of k embedded segments D from the whole training set with labels different from Ya and consider only the example embedding, xd \u2208 D, that most violates the margin constraint.", "startOffset": 53, "endOffset": 57}, {"referenceID": 11, "context": "Rather than sampling a single negative example as in [12], or keeping track of confusion statistics as in [13], we sample a set of k embedded segments D from the whole training set with labels different from Ya and consider only the example embedding, xd \u2208 D, that most violates the margin constraint.", "startOffset": 106, "endOffset": 110}, {"referenceID": 8, "context": "For this, we use the Segmental Randomized Acoustic Indexing and LogarithmicTime Search (S-RAILS) system [10], an embedding-based QbE approach.", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "S-RAILS provides a simple platform for performing approximate nearest neighbor search over vectors, relying on a version of locality-sensitive hashing (LSH) [24, 25].", "startOffset": 157, "endOffset": 165}, {"referenceID": 23, "context": "S-RAILS provides a simple platform for performing approximate nearest neighbor search over vectors, relying on a version of locality-sensitive hashing (LSH) [24, 25].", "startOffset": 157, "endOffset": 165}, {"referenceID": 24, "context": "We use data from the Switchboard corpus of (primarily American) English conversational telephone speech [26].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "The size of this set is comparable to those used for training in prior work on acoustic word embeddings [13, 27, 28].", "startOffset": 104, "endOffset": 116}, {"referenceID": 25, "context": "The size of this set is comparable to those used for training in prior work on acoustic word embeddings [13, 27, 28].", "startOffset": 104, "endOffset": 116}, {"referenceID": 26, "context": "The size of this set is comparable to those used for training in prior work on acoustic word embeddings [13, 27, 28].", "startOffset": 104, "endOffset": 116}, {"referenceID": 6, "context": "These partitions are identical to those used in prior work [8, 10] for the QbE task.", "startOffset": 59, "endOffset": 66}, {"referenceID": 8, "context": "These partitions are identical to those used in prior work [8, 10] for the QbE task.", "startOffset": 59, "endOffset": 66}, {"referenceID": 6, "context": "We use a set of 43 query words previously used in [8, 10], which were chosen subject to the constraints that the median word duration of each type across the entire corpus is at least 0.", "startOffset": 50, "endOffset": 57}, {"referenceID": 8, "context": "We use a set of 43 query words previously used in [8, 10], which were chosen subject to the constraints that the median word duration of each type across the entire corpus is at least 0.", "startOffset": 50, "endOffset": 57}, {"referenceID": 6, "context": "5 seconds and the orthographic representation of each word type has at least six characters [8, 10, 16].", "startOffset": 92, "endOffset": 103}, {"referenceID": 8, "context": "5 seconds and the orthographic representation of each word type has at least six characters [8, 10, 16].", "startOffset": 92, "endOffset": 103}, {"referenceID": 14, "context": "5 seconds and the orthographic representation of each word type has at least six characters [8, 10, 16].", "startOffset": 92, "endOffset": 103}, {"referenceID": 27, "context": "We use the Adam optimization algorithm [29] with a batch size of 32, learning rate of 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "We tuned these parameters based on development set performance on the isolated word discrimination task of [19].", "startOffset": 107, "endOffset": 111}, {"referenceID": 28, "context": "OTWV is a query-specific weighted difference between the recall and the false alarm rate (further explanation can be found in [30]).", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "Figure 4: Embeddings of queries and their top hits, visualized in two dimensions using t-SNE [31].", "startOffset": 93, "endOffset": 97}, {"referenceID": 6, "context": "Median Example Best Example System FOM OTWV P@10 FOM OTWV P@10 Query time (s) RAILS [8] 6.", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "Besides the S-RAILS baseline, we also compare to RAILS [8], a DTWbased system that is optimized for speed using LSH to get approximate frame-level near neighbor matches.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "RAILS evaluation scores are reproduced from [8].", "startOffset": 44, "endOffset": 47}], "year": 2017, "abstractText": "Query-by-example search often uses dynamic time warping (DTW) for comparing queries and proposed matching segments. Recent work has shown that comparing speech segments by representing them as fixed-dimensional vectors \u2014 acoustic word embeddings \u2014 and measuring their vector distance (e.g., cosine distance) can discriminate between words more accurately than DTW-based approaches. We consider an approach to queryby-example search that embeds both the query and database segments according to a neural model, followed by nearestneighbor search to find the matching segments. Earlier work on embedding-based query-by-example, using template-based acoustic word embeddings, achieved competitive performance. We find that our embeddings, based on recurrent neural networks trained to optimize word discrimination, achieve substantial improvements in performance and run-time efficiency over the previous approaches.", "creator": "LaTeX with hyperref package"}}}