{"id": "1706.04138", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing", "abstract": "In this work, why explore such auditory architectures translation for put responsibilities known drives before - editing now launcher mentions output. We key started neural take - when - end introduced been nuts have factors $ blackcomb $ then $ bmb $ also this same cell modeled, modeling $ \\ {cranmore, osca \\} \\ rightarrow pe $ generally. Apart from that, get investigate came religious more but - need models which ones them they well - suited for nonwhite tasks, recently well recently characteristics given between ideas. We report results this signal moves same during after WMT 2016 provided underway as automatic leaving - editing well can recognizing yet a - attention customized that ways we available accounts in into APE scenario in a single introduced establishing back the best shared our system, after three other writings results after before shared task. Double - comes available already are first, nothing serious nevertheless dominate despite applying five mean. part input.", "histories": [["v1", "Tue, 13 Jun 2017 15:55:02 GMT  (204kb,D)", "http://arxiv.org/abs/1706.04138v1", null], ["v2", "Sat, 30 Sep 2017 13:03:33 GMT  (233kb,D)", "http://arxiv.org/abs/1706.04138v2", "Accepted for presentation at IJCNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcin junczys-dowmunt", "roman grundkiewicz"], "accepted": false, "id": "1706.04138"}, "pdf": {"name": "1706.04138.pdf", "metadata": {"source": "META", "title": "An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing", "authors": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz"], "emails": ["junczys@amu.edu.pl", "rgrundki@exseed.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Given the raw output of a (possibly unknown) machine translation system from language src to language mt, Automatic Post-Editing (APE) is the process of automatic correction of raw MT output (mt), so that a closer resemblance to human postedited MT output (pe) is achieved. While APE systems that only model mt \u2192 pe yield good results, the field has always strived towards methods that also integrate src in various forms.\nWith neural encoder-decoder models, and multi-source models in particular, this can be now achieved in more natural ways than for previously popular phrase-based statistical machine translation (PB-SMT) systems. Despite this, results for\nmulti-source or double-source models in APE scenarios are incomplete or unsatisfying in terms of performance.\nIn this work, we explore a number of singlesource and double-source neural architectures which we believe to be better fits to the APE task than vanilla encoder-decoder models with soft attention. We focus on neural end-to-end models that combine both inputs mt and src in a single neural architecture, modeling {mt, src} \u2192 pe directly. Apart from that, we investigate the influence of hard-attention models, which seem to be well-suited for monolingual tasks. Finally, we create combinations of both architectures.\nWe report results on data sets provided during the WMT 2016 shared task on automatic postediting (Bojar et al., 2016) and compare our performance against the shared task winner, the system submitted by the Adam Mickiewicz University (AMU) team (Junczys-Dowmunt and Grundkiewicz, 2016), and a more recent system by Pal et al. (2017) with the previously best published results on the same test set.\nOur main contributions are: (1) we perform a thorough comparison of multiple end-to-end neural approaches to APE during which (2) we demonstrate that double-attention models that incorporate all available data in the APE scenario in a single model achieve the best reported results for the WMT 2016 APE task so far, and (3) show that models which incorporate a hard-attention mechanism reach competitive results although they execute fewer edits than model relying only on soft attention.\nThis remainder of the paper is organized as follows: Previous relevant work is described in Section 2. Section 3 summarizes the basic encoderdecoder with attention architecture that is further extended with multiple non-standard attention mechanisms in Section 4. These attention mecha-\nar X\niv :1\n70 6.\n04 13\n8v 1\n[ cs\n.C L\n] 1\n3 Ju\nn 20\n17\nnisms are: hard-attention in Section 4.1, a combination of hard attention and soft attention in Section 4.2, double soft attention in Section 4.3 and a combination of hard attention and double soft attention in Section 4.4. We describe experiments and results in Section 5 and conclude in Section 6."}, {"heading": "2 Previous work", "text": "Before the application of neural sequence-tosequence models to APE, most APE systems would rely on phrase-based SMT following a monolingual approach first proposed by Simard et al. (2007). B\u00e9chara et al. (2011) proposed a \u201csource-context aware\u201d variant of this approach: automatically created word alignments are used to create a new source language which consists of joined MT output and source token pairs. The inclusion of source-language information in that form is shown to be useful to improve the automatic post-editing results (B\u00e9chara et al., 2012; Chatterjee et al., 2015). The quality of the word alignments plays an important role for this methods, as shown for instance by Pal et al. (2015).\nDuring the WMT 2016 APE two systems relied on neural models, the CUNI system (Libovick\u00fd et al., 2016) and the shared task winner, the system submitted by the AMU team (Junczys-Dowmunt and Grundkiewicz, 2016). This submission explored the application of neural translation models to the APE problem and achieved good results by treating different models as components in a log-linear model, allowing for multiple inputs (the source src and the translated sentence mt) that were decoded to the same target language (postedited translation pe). Two systems were considered, one using src as the input (src \u2192 pe) and another using mt as the input (mt \u2192 pe). A simple string-matching penalty integrated within the log-linear model was used to control for higher faithfulness with regard to the raw MT output. The penalty fires if the APE system proposes a word in its output that has not been seen in mt. The influence of the components on the final result was tuned with Minimum Error Rate Training (Och, 2003) with regard to the task metric TER.\nThis system is of particular interest for this work due to multiple reasons:\n\u2022 it is one of the first applications of neural sequence to sequence models to the APE task and among these the most successful;\n\u2022 it is arguably a multi-source model, but the combination of mt and src inputs was achieved by ensembling of multiple singlesource models. Due to this it serves as an interesting baseline and contrastive approach to our end-to-end multi-source models;\n\u2022 and finally, the large amounts of artificial data provided by the authors make it feasible to explore the influence of model choices on post-editing quality without worrying about data scarcity \u2013 a problem suffered from by other neural approaches to the same task, see for instance Libovick\u00fd and Helcl (2017).\nFollowing the WMT 2016 APE shared task, Pal et al. (2017) published work on another neural APE system that integrates precomputed wordalignment features into the neural structure and enforces symmetric attention during the neural training process. The result is the best reported single neural model for the WMT-2016 APE test set prior to this work. With n-best list re-ranking and combination with phrase-based post-editing systems, the authors improve their results even further. None of their systems, however, integrates information from src, all model mt\u2192 pe."}, {"heading": "3 Attentional Encoder-Decoder", "text": "Implementations of all models explored in this paper are available in the Marian1 toolkit. The attentional encoder-decoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017). The model differs from the standard model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention. The summary provided in this section is based on the description in Sennrich et al. (2017).\nGiven the raw MT output sequence (x1, . . . , xTx) of length Tx and its manually post-edited equivalent (y1, . . . , yTy) of length Ty, we construct the encoder-decoder model using the following formulations.\nEncoder context A single forward encoder state\u2212\u2192 h i is calculated as:\n\u2212\u2192 h i = GRU( \u2212\u2192 h i\u22121,F[xi]),\n1https://github.com/marian-nmt/marian\nwhere F is the encoder embeddings matrix. The GRU RNN cell (Cho et al., 2014) is defined as:\nGRU (s,x) =(1\u2212 z) s+ z s, (1) s = tanh (Wx+ r Us) , r = \u03c3 (Wrx+Urs) ,\nz = \u03c3 (Wzx+Uzs) ,\nwhere x is the cell input; s is the previous recurrent state; W, U, Wr, Ur, Wz , Uz are trained model parameters2; \u03c3 is the logistic sigmoid activation function. The backward encoder state is calculated analogously over a reversed input sequence with its own set of trained parameters.\nLet hi be the annotation of the source symbol at position i, obtained by concatenating the forward and backward encoder RNN hidden states, hi = [ \u2212\u2192 h i; \u2190\u2212 h i], the set of encoder states C = {h1, . . . ,hTx} then forms the encoder context.\nDecoder initialization The decoder is initialized with start state s0, computed as the average over all encoder states:\ns0 = tanh ( Winit \u2211Tx i=1 hi Tx ) .\nConditional GRU with attention We follow the Nematus implementation of the conditional GRU with attention, cGRUatt:\nsj = cGRUatt (sj\u22121,E[yj\u22121],C) , (2)\nwhere sj is the newly computed hidden state, sj\u22121 is the previous hidden state, C the source context and E[yj\u22121] is the embedding of the previously decoded symbol yi\u22121.\nThe conditional GRU cell with attention, cGRUatt, has a complex internal structure, consisting of three parts: two GRU layers and an intermediate attention mechanism ATT.\nLayer GRU1 generates an intermediate representation s\u2032j from the previous hidden state sj\u22121 and the embedding of the previous decoded symbol E[yj\u22121]:\ns\u2032j = GRU1 (sj\u22121,E[yj\u22121]) .\nThe attention mechanism, ATT, inputs the entire context set C along with intermediate hidden\n2Biases have been omitted.\nstate s\u2032j in order to compute the context vector cj as follows:\ncj =ATT ( C, s\u2032j ) = Tx\u2211\ni\n\u03b1ijhi,\n\u03b1ij = exp(eij)\u2211Tx\nk=1 exp(ekj) ,\neij =v \u1d40 a tanh ( Uas \u2032 j +Wahi ) ,\nwhere \u03b1ij is the normalized alignment weight between source symbol at position i and target symbol at position j, and va,Ua,Wa are trained model parameters.\nLayer GRU2 generates sj , the hidden state of the cGRUatt, from the intermediate representation s\u2032j and context vector cj :\nsj = GRU2 ( s\u2032j , cj ) .\nDeep output Finally, given sj , yj\u22121, and cj , the output probability p(yj |sj , yj\u22121, cj) is computed by a softmax activation as follows:\np(yj |sj ,yj\u22121, cj) = softmax (tjWo) tj = tanh (sjWt1 +E[yj\u22121]Wt2 + cjWt3) .\nWt1 ,Wt2 ,Wt3 ,Wo are the trained model parameters.\nThis rather standard encoder-decoder model with attention is our baseline and denoted as ENCDEC-CGRU."}, {"heading": "4 Encoder-Decoder Models with APE-specific Attention Models", "text": "The following models reuse most parts of the architecture described above wherever possible, most differences occur in the decoder RNN cell and the attention mechanism. The encoders are identical, so are the deep output layers."}, {"heading": "4.1 Hard Monotonic Attention", "text": "Aharoni and Goldberg (2016) introduce a simple model for monolingual morphological reinflection with hard monotonic attention. This model looks at one encoder state at a time, starting with the left-most encoder state and progressing to the right until all encoder states have been processed.\nThe target word vocabulary Vy is extended with a special step symbol (V \u2032y = Vy \u222a {\u3008STEP\u3009}) and whenever \u3008STEP\u3009 is predicted as the output symbol, the hard attention is moved to the next encoder\nstate. Formally, the hard attention mechanism is represented as a precomputed monotonic sequence (a1, . . . , aTy) which can be inferred from the target sequence (y1, . . . , yTy) (containing original target symbols and Tx step symbols) as follows:\na1 = 1,\naj = { aj\u22121 + 1 if yj\u22121 = \u3008STEP\u3009 aj\u22121 otherwise.\nFor a given context C = {h1, . . . ,hTx}, the attended context vector at time step j is simply haj .\nFollowing the description by Aharoni and Goldberg (2016) for their LSTM-based model, we now adapt the previously described encoder-decoder model to incorporate hard attention. The encoder as well as the output layer of the previous model remain unchanged. Given the sequence of attention indices (a1, . . . , aTy), the conditional GRU cell (Eq. 2) used for hidden state updates of the decoder is replaced with a simple GRU cell (Eq. 1) (thus removing the soft-attention mechanism):\nsj = GRU ( sj\u22121, [ E[yj\u22121];haj ]) , (3)\nwhere the cell input is now a concatenation of the embedding of the previous target symbol E[yj\u22121] and the currently attended encoder state haj . This model is labeled ENCDEC-GRU-HARD.\nWe find this architecture compelling for monolingual tasks that might require higher faithfulness with regard to the input. With hard monotonic attention, the translation algorithm can enforce certain constraints:\n1. The end-of-sentence symbol can only be generated if the hard attention mechanism has reached the end of the input sequence, enforcing full coverage;\n2. The \u3008STEP\u3009 symbol cannot be generated once the end-of-sentence position in the source has been reached. It is however still possible to generate content tokens.\nObviously, this model requires a target sequence with correctly inserted \u3008STEP\u3009 symbols. For the described APE task, using the Longest Common Subsequence algorithm (Hirschberg, 1977), we first generate a sequence of match, delete and insert operations which transform the raw MT output (x1, \u00b7 \u00b7 \u00b7xTx) into the corrected\npost-edited sequence (y1, \u00b7 \u00b7 \u00b7 yTy)3. Next, we map these operations to the final sequence of steps and target tokens according to the following rules:\n\u2022 For each matched pair of tokens x, y we produce symbols: \u3008STEP\u3009 y;\n\u2022 For each inserted target token y we produce the same token y;\n\u2022 For each deleted source token x we produce \u3008STEP\u3009;\n\u2022 Since at initialization of the model a1 = 1, i.e. the first encoder state is already attended to, we discard the first symbol in the new sequence if it is a \u3008STEP\u3009 symbol."}, {"heading": "4.2 Hard and Soft Attention", "text": "While the hard attention model can be used to enforce faithfulness to the original input, we would also like the model to be able to look at information anywhere in the source sequence which is a property of the soft attention model.\nBy re-introducing the conditional GRU cell with soft attention into the ENCDEC-GRU-HARD model while also inputting the hard-attended encoder state haj , we can try to take advantage of both attention mechanisms. Combining Eq. 2 and Eq. 3, we get:\nsj = cGRUatt ( sj\u22121, [ E[yj\u22121];haj ] ,C ) . (4)\nThe rest of the model is unchanged; the translation process is the same as before and we use the same target step/token sequence for training. This model is called ENCDEC-CGRU-HARD."}, {"heading": "4.3 Soft Double-Attention", "text": "Neural multi-source models (Zoph and Knight, 2016) seem to be natural fit for the APE task, as raw MT output and original source language input are available. Although applications to the APE problem have been reported (Libovick\u00fd and Helcl, 2017), state-of-the-art results seem to be missing.\nIn this section we give details about our doublesource model implementation. We rename the existing encoder C to Cmt to signal that the first encoder consumes the raw MT output and introduce a structurally identical second encoder Csrc = {hsrc1 , . . . ,hsrcTsrc} over the source language. To\n3Similar to GNU wdiff.\ncompute the decoder start state s0 for the multiencoder model we concatenate the averaged encoder contexts before mapping them into the decoder state space:\ns0 = tanh ( Winit [\u2211Tmt i=1 h mt i\nTmt ;\n\u2211Tsrc i=1 h src i\nTsrc\n]) .\nIn the decoder, we replace the conditional GRU with attention, with a doubly-attentive cGRU cell (Calixto et al., 2017) over contexts Cmt and Csrc:\nsj = cGRU2-att ( sj\u22121,E[yj\u22121],Cmt,Csrc ) . (5)\nThe procedure is similar to the original cGRU, differing only in that in order to compute the context vector cj , we first calculate contexts vectors cmtj and c src j for each context and then concatenate4 the results:\ns\u2032j =GRU1 (sj\u22121,E[yj\u22121]) ,\ncmtj =ATT ( Cmt, s\u2032j ) = Tmt\u2211\ni\n\u03b1ijh mt i ,\ncsrcj =ATT ( Csrc, s\u2032j ) = Tsrc\u2211\ni\n\u03b1ijh src i ,\ncj = [ cmtj ; c src j ] , sj =GRU2 ( s\u2032j , cj ) .\nThis could be easily extended to an arbitrary number of encoders with different architectures. During training this model is fed with a triparallel corpus, and during translation both input sequences are processed simultaneously to produce the corrected output. This model is denoted as ENCDEC-M-CGRU."}, {"heading": "4.4 Hard Attention with Soft Double-Attention", "text": "Analogously to the procedure described in section 4.2, we can extend the doubly-attentive cGRU to take the hard-attended encoder context as additional input: sj = cGRU2-att ( sj\u22121, [ E[yj\u22121];hmtaj ] ,Cmt,Csrc ) .\n4Calixto et al. (2017) combine their two attention models by modifying their GRU cell to include another set of parameters that is multiplied with the additional context vector and summed in the GRU-components. Formally, both approaches give identical results, as for concatenation the original parameters have to grow in size to match the now longer input vector dimensions. The GRU cell itself does not need to be modified.\nIn this formulation, only the first encoder context Cmt is attended to by the hard monotonic attention mechanism. The target training data consists of the step/token sequences used for all previous hard-attention models. We call this model ENCDEC-M-CGRU-HARD."}, {"heading": "5 Experiments and Results", "text": ""}, {"heading": "5.1 Training, Development, and Test Data", "text": "We perform all our experiments with the official WMT16 (Bojar et al., 2016) automatic postediting data and the respective development and test sets. The training data consists of a small set of 12,000 post-editing triplets (src,mt, pe), where src is the original English text, mt is the raw MT output generated by an English-toGerman system, and pe is the human post-edited MT output. The MT system used to produce the raw MT output is unknown, so is the original training data. The task consist of automatically correcting the MT output so that it resembles human postedited data. The main task metric is TER (Snover et al., 2006) \u2014 the lower the better \u2014 with BLEU (Papineni et al., 2002) as a secondary metric.\nTo overcome the problem of too little training data, Junczys-Dowmunt and Grundkiewicz (2016) \u2014 the authors of the best WMT16-APE shared task system \u2014 generated large amounts of artificial data via round-trip translations. The artificial data has been filtered to match the HTER statistics of the training and development data for the shared task and was made available for download5. Table 1 summarizes the data sets used in this work.\n5The artificial filtered data has been made available at https://github.com/emjotde/amunmt/wiki/ AmuNMT-for-Automatic-Post-Editing.\nTo produce our final training data set we oversample the original training data 20 times and add both artificial data sets (they may overlap). This results in a total of slightly more than 5M training triplets. We keep the development set as a validation set for early stopping and report results on the WMT16 test set. The data is already tokenized. Additionally we truecase all files and apply segmentation into BPE subword units. We reuse the subword units distributed with the artificial data set. For the hard-attention models, we create new target training and development files following the LCS-based procedure outlined in section 4.1."}, {"heading": "5.2 Training parameters", "text": "All models are trained on the same training data. Models with single input encoders take only the raw MT output (mt) as input, double-encoder models use raw MT output (mt) and the original source (pe). The training procedures and model settings are the same whenever possible:\n\u2022 All embedding vectors consist of 512 units, the RNN states use 1024 units. We choose a vocabulary size of 40,000 for all inputs and outputs. When hard attention models are trained the maximum sentence length is 100 to accommodate the additional step symbols, otherwise 50.\n\u2022 To avoid overfitting, we use pervasive dropout (Gal, 2015) over GRU steps and input embeddings, with dropout probabilities 0.2, and over source and target words with probabilities 0.2.\n\u2022 We use Adam (Kingma and Ba, 2014) as our optimizer, with a mini-batch size of 64. All models are trained with Asynchronous SGD (Adam) on three to four GPUs.\n\u2022 We train all models until convergence (earlystopping with a patience of 10 based on devset cross-entropy cost), saving model checkpoints every 10,000 mini-batches. For different models we observed early stopping to be triggered between 600,000 and 900,000 minibatch updates or between 8 and 11 epochs.\n\u2022 The best eight model checkpoints w.r.t. devset cross-entropy of each training run are averaged element-wise (Junczys-Dowmunt et al., 2016) resulting in new single models with generally improved performance.\n\u2022 For the multi-source models we repeat the mentioned procedure four times with different randomly initialized weights and random seeds to later form model ensembles.\nTraining time for one model on four NVIDIA GTX 1080 GPUs or NVIDIA TITAN X (Pascal) GPUs is between one and two days, depending on model complexity. The ENCDEC-M-CGRU-HARD model is the most complex and trains longest."}, {"heading": "5.3 Evaluation", "text": "Table 2 contains a selection of most relevant results for the WMT16 APE shared task \u2014 during the task and afterwards. WMT 2016 BASELINE-1 is the raw uncorrected MT output. BASELINE-2 is the results of a vanilla phrase-based Moses system (Koehn et al., 2007) trained only on the official 12,000 sentences. Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task. Pal et al. (2017) SYMMETRIC is the currently best reported result on the WMT16 APE test set for a single neural model (single source), whereas Pal et al. (2017) RERANKING \u2014 the overall best reported result on the test set \u2014 is a system combination of Pal et al. (2017) SYMMETRIC with phrase-based models via n-best list re-ranking.\nIn Table 3 we present the results for the models discussed in this work. Unsurprisingly, none of the single attention models can compete with the better systems reported in the literature. The encoder-decoder model with only hard monotonic attention (ENCDEC-GRU-HARD) is the clear loser, while the comparison between ENCDECCGRU and ENCDEC-CGRU-HARD remains inconclusive. ENCDEC-CGRU-HARD seems to generalize slightly better, but would not have been chosen based on the development set performance where it is slightly worse.\nThe double-attention models, however, each outperform the best WMT16 system and the currently reported best single-model Pal et al. (2017) SYMMETRIC. The ensembles also beat the system combination Pal et al. (2017) RERANKING in terms of TER (not in terms of BLEU though). The simpler double-attention model with no hardattention ENCDEC-M-CGRU reaches slightly better results on the test set than its counterpart with added hard attention ENCDEC-M-CGRU-HARD, but the situation would have been less clear if only the dev set were used to choose the best model. It also seems that the hard-attention model with dou-\nble soft attention benefits less from ensembling; we maybe shed some light on this in the following section."}, {"heading": "5.4 On Faithfulness", "text": "One question remains: we postulated multiple times that the hard-attention models might have a potential for higher faithfulness. Since the APE task is a mostly monolingual task, we can verify this by comparing TER scores with regard to the reference post-edition (TER-pe) and TER scores\nwith regard to the raw MT output (TER-mt). The lower the TER-mt score the fewer changes have been made to the input to arrive at the output, thus resulting in higher faithfulness. Table 4 contains this comparison for the WMT 2016 APE test set.\nWe can indeed verify that the hard-attention models adhere to the input, making fewer changes than their soft-attention counterparts. This difference is especially dramatic for ENCDEC-M-CGRU and ENCDEC-M-CGRU-HARD, where only small differences in TER-pe occur but a gap of more than two TER points for TER-mt. This shows that hard-attention models can reach similar TER scores to soft-attention models while performing fewer changes. It might also explain why ensembling has a lower impact on the hard-attention models: higher faithfulness means less variety which results in smaller benefits from ensembles."}, {"heading": "6 Conclusions and Future Work", "text": "In this paper we presented several neural APE models that are equipped with non-standard attention mechanisms and combinations thereof. Among these, hard attention models have been applied to APE for the first time, whereas double-\nsoft attention models have been proposed before for APE tasks, but with non-conclusive results.\nThis is the first work to report state-of-the-art results for double-attention models that integrate full post-edition triplets into a single end-to-end model. The ensembles of double-attention models provide more than 1.52 TER points improvement over the best WMT 2016 system and 0.7 TER improvement over the best reported system combination for the same test set.\nWe also demonstrated that while hard-attention models for APE yield similar results to pure softattention models, they do so by performing less changes to the input. This might be a useful property in scenarios where conservative edits are preferred. Future work should investigate the influence of alignment quality for the hard-attention models and reinforcement-learning towards TER, the task metric. Although not reported in this work, we did conduct experiments with increased numbers of RNN-layers, however without significant impact on the final performance. A more thorough investigation of these hyper-parameters is planned."}, {"heading": "Acknowledgments", "text": "This research was funded by the Amazon Academic Research Awards program."}], "references": [{"title": "Sequence to sequence transduction with hard monotonic attention", "author": ["Roee Aharoni", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1611.01487 .", "citeRegEx": "Aharoni and Goldberg.,? 2016", "shortCiteRegEx": "Aharoni and Goldberg.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Statistical post-editing for a statistical MT system", "author": ["Hanna B\u00e9chara", "Yanjun Ma", "Josef van Genabith."], "venue": "Proceedings of the 13th Machine Translation Summit. Xiamen, China, pages 308\u2013315.", "citeRegEx": "B\u00e9chara et al\\.,? 2011", "shortCiteRegEx": "B\u00e9chara et al\\.", "year": 2011}, {"title": "An evaluation of statistical post-editing systems applied to RBMT and SMT systems", "author": ["Hanna B\u00e9chara", "Rapha\u00ebl Rubino", "Yifan He", "Yanjun Ma", "Josef van Genabith."], "venue": "Proceedings of COLING 2012. Mumbai, India, pages 215\u2013230.", "citeRegEx": "B\u00e9chara et al\\.,? 2012", "shortCiteRegEx": "B\u00e9chara et al\\.", "year": 2012}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Ma-", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Doubly-attentive decoder for multi-modal neural machine translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "CoRR abs/1702.01287. http://arxiv.org/abs/1702.01287.", "citeRegEx": "Calixto et al\\.,? 2017", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "Exploring the planet of the APEs: a comparative study of state-of-the-art methods for MT automatic post-editing", "author": ["Rajen Chatterjee", "Marion Weller", "Matteo Negri", "Marco Turchi."], "venue": "Proceedings of the 53rd Annual Meeting of the As-", "citeRegEx": "Chatterjee et al\\.,? 2015", "shortCiteRegEx": "Chatterjee et al\\.", "year": 2015}, {"title": "Learning Phrase Representations Using RNN EncoderDecoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "In", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarin Gal."], "venue": "ArXiv e-prints .", "citeRegEx": "Gal.,? 2015", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Algorithms for the longest common subsequence problem", "author": ["Daniel S. Hirschberg."], "venue": "J. ACM 24(4):664\u2013 675. https://doi.org/10.1145/322033.322044.", "citeRegEx": "Hirschberg.,? 1977", "shortCiteRegEx": "Hirschberg.", "year": 1977}, {"title": "Is neural machine translation ready for deployment? A case study on 30 translation directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."], "venue": "arXiv preprint arXiv:1610.01108 http://arxiv.org/abs/1610.01108.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the First Conference on Machine Translation. pages 751\u2013", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Attention strategies for multi-source sequenceto-sequence learning", "author": ["Jindrich Libovick\u00fd", "Jindrich Helcl."], "venue": "CoRR abs/1704.06567. http://arxiv.org/abs/1704.06567.", "citeRegEx": "Libovick\u00fd and Helcl.,? 2017", "shortCiteRegEx": "Libovick\u00fd and Helcl.", "year": 2017}, {"title": "CUNI system for WMT16 automatic post-editing and multimodal translation tasks", "author": ["Jind\u0159ich Libovick\u00fd", "Jind\u0159ich Helcl", "Marek Tlust\u00fd", "Ond\u0159ej Bojar", "Pavel Pecina."], "venue": "Proceedings of the First Conference on Machine", "citeRegEx": "Libovick\u00fd et al\\.,? 2016", "shortCiteRegEx": "Libovick\u00fd et al\\.", "year": 2016}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proc. of the Annual Meeting on Association for Computational Linguistics. pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Neural automatic post-editing using prior alignment and reranking", "author": ["Santanu Pal", "Sudip Kumar Naskar", "Mihaela Vela", "Qun Liu", "Josef van Genabith."], "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics. pages", "citeRegEx": "Pal et al\\.,? 2017", "shortCiteRegEx": "Pal et al\\.", "year": 2017}, {"title": "USAAR-SAPE: An English\u2013Spanish statistical automatic post-editing system", "author": ["Santanu Pal", "Mihaela Vela", "Sudip Kumar Naskar", "Josef van Genabith."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. Association for", "citeRegEx": "Pal et al\\.,? 2015", "shortCiteRegEx": "Pal et al\\.", "year": 2015}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Asso-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Nematus: a toolkit", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u00e4ubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Statistical phrase-based post-editing", "author": ["Michel Simard", "Cyril Goutte", "Pierre Isabelle."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "Simard et al\\.,? 2007", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "A Study", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul"], "venue": null, "citeRegEx": "Snover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Multisource neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "CoRR abs/1601.00710. http://arxiv.org/abs/1601.00710.", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": ", 2016) and compare our performance against the shared task winner, the system submitted by the Adam Mickiewicz University (AMU) team (Junczys-Dowmunt and Grundkiewicz, 2016), and a more recent system by Pal et al.", "startOffset": 134, "endOffset": 174}, {"referenceID": 11, "context": ", 2016) and compare our performance against the shared task winner, the system submitted by the Adam Mickiewicz University (AMU) team (Junczys-Dowmunt and Grundkiewicz, 2016), and a more recent system by Pal et al. (2017) with the previously best published results on the same test set.", "startOffset": 135, "endOffset": 222}, {"referenceID": 3, "context": "The inclusion of source-language information in that form is shown to be useful to improve the automatic post-editing results (B\u00e9chara et al., 2012; Chatterjee et al., 2015).", "startOffset": 126, "endOffset": 173}, {"referenceID": 6, "context": "The inclusion of source-language information in that form is shown to be useful to improve the automatic post-editing results (B\u00e9chara et al., 2012; Chatterjee et al., 2015).", "startOffset": 126, "endOffset": 173}, {"referenceID": 16, "context": "Before the application of neural sequence-tosequence models to APE, most APE systems would rely on phrase-based SMT following a monolingual approach first proposed by Simard et al. (2007). B\u00e9chara et al.", "startOffset": 167, "endOffset": 188}, {"referenceID": 2, "context": "B\u00e9chara et al. (2011) proposed a \u201csource-context aware\u201d variant of this approach: automatically created word alignments are used to create a new source language which consists of joined MT output and source token pairs.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "B\u00e9chara et al. (2011) proposed a \u201csource-context aware\u201d variant of this approach: automatically created word alignments are used to create a new source language which consists of joined MT output and source token pairs. The inclusion of source-language information in that form is shown to be useful to improve the automatic post-editing results (B\u00e9chara et al., 2012; Chatterjee et al., 2015). The quality of the word alignments plays an important role for this methods, as shown for instance by Pal et al. (2015).", "startOffset": 0, "endOffset": 515}, {"referenceID": 15, "context": "During the WMT 2016 APE two systems relied on neural models, the CUNI system (Libovick\u00fd et al., 2016) and the shared task winner, the system submitted by the AMU team (Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 77, "endOffset": 101}, {"referenceID": 11, "context": ", 2016) and the shared task winner, the system submitted by the AMU team (Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 73, "endOffset": 113}, {"referenceID": 16, "context": "The influence of the components on the final result was tuned with Minimum Error Rate Training (Och, 2003) with regard to the task metric TER.", "startOffset": 95, "endOffset": 106}, {"referenceID": 14, "context": "\u2022 and finally, the large amounts of artificial data provided by the authors make it feasible to explore the influence of model choices on post-editing quality without worrying about data scarcity \u2013 a problem suffered from by other neural approaches to the same task, see for instance Libovick\u00fd and Helcl (2017).", "startOffset": 284, "endOffset": 311}, {"referenceID": 17, "context": "Following the WMT 2016 APE shared task, Pal et al. (2017) published work on another neural APE system that integrates precomputed wordalignment features into the neural structure and enforces symmetric attention during the neural training process.", "startOffset": 40, "endOffset": 58}, {"referenceID": 20, "context": "The attentional encoder-decoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017).", "startOffset": 99, "endOffset": 122}, {"referenceID": 1, "context": "The model differs from the standard model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention.", "startOffset": 56, "endOffset": 79}, {"referenceID": 1, "context": "The model differs from the standard model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention. The summary provided in this section is based on the description in Sennrich et al. (2017).", "startOffset": 56, "endOffset": 251}, {"referenceID": 7, "context": "The GRU RNN cell (Cho et al., 2014) is defined as:", "startOffset": 17, "endOffset": 35}, {"referenceID": 0, "context": "Following the description by Aharoni and Goldberg (2016) for their LSTM-based model, we now adapt the previously described encoder-decoder model to incorporate hard attention.", "startOffset": 29, "endOffset": 57}, {"referenceID": 9, "context": "For the described APE task, using the Longest Common Subsequence algorithm (Hirschberg, 1977), we first generate a sequence of match, delete and insert operations which transform the raw MT output (x1, \u00b7 \u00b7 \u00b7xTx) into the corrected post-edited sequence (y1, \u00b7 \u00b7 \u00b7 yTy).", "startOffset": 75, "endOffset": 93}, {"referenceID": 23, "context": "Neural multi-source models (Zoph and Knight, 2016) seem to be natural fit for the APE task, as raw MT output and original source language input are available.", "startOffset": 27, "endOffset": 50}, {"referenceID": 14, "context": "Although applications to the APE problem have been reported (Libovick\u00fd and Helcl, 2017), state-of-the-art results seem to be missing.", "startOffset": 60, "endOffset": 87}, {"referenceID": 5, "context": "In the decoder, we replace the conditional GRU with attention, with a doubly-attentive cGRU cell (Calixto et al., 2017) over contexts C and C:", "startOffset": 97, "endOffset": 119}, {"referenceID": 11, "context": "Adapted from Junczys-Dowmunt and Grundkiewicz (2016).", "startOffset": 13, "endOffset": 53}, {"referenceID": 22, "context": "The main task metric is TER (Snover et al., 2006) \u2014 the lower the better \u2014 with BLEU (Papineni et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 19, "context": ", 2006) \u2014 the lower the better \u2014 with BLEU (Papineni et al., 2002) as a secondary metric.", "startOffset": 43, "endOffset": 66}, {"referenceID": 11, "context": "To overcome the problem of too little training data, Junczys-Dowmunt and Grundkiewicz (2016) \u2014 the authors of the best WMT16-APE shared task system \u2014 generated large amounts of artificial data via round-trip translations.", "startOffset": 53, "endOffset": 93}, {"referenceID": 8, "context": "\u2022 To avoid overfitting, we use pervasive dropout (Gal, 2015) over GRU steps and input embeddings, with dropout probabilities 0.", "startOffset": 49, "endOffset": 60}, {"referenceID": 12, "context": "\u2022 We use Adam (Kingma and Ba, 2014) as our optimizer, with a mini-batch size of 64.", "startOffset": 14, "endOffset": 35}, {"referenceID": 10, "context": "devset cross-entropy of each training run are averaged element-wise (Junczys-Dowmunt et al., 2016) resulting in new single models with generally improved performance.", "startOffset": 68, "endOffset": 98}, {"referenceID": 13, "context": "BASELINE-2 is the results of a vanilla phrase-based Moses system (Koehn et al., 2007) trained only on the official 12,000 sentences.", "startOffset": 65, "endOffset": 85}, {"referenceID": 11, "context": "Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task.", "startOffset": 0, "endOffset": 40}, {"referenceID": 11, "context": "Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task. Pal et al. (2017) SYMMETRIC is the currently best reported result on the WMT16 APE test set for a single neural model (single source), whereas Pal et al.", "startOffset": 0, "endOffset": 97}, {"referenceID": 11, "context": "Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task. Pal et al. (2017) SYMMETRIC is the currently best reported result on the WMT16 APE test set for a single neural model (single source), whereas Pal et al. (2017) RERANKING \u2014 the overall best reported result on the test set \u2014 is a system combination of Pal et al.", "startOffset": 0, "endOffset": 240}, {"referenceID": 11, "context": "Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task. Pal et al. (2017) SYMMETRIC is the currently best reported result on the WMT16 APE test set for a single neural model (single source), whereas Pal et al. (2017) RERANKING \u2014 the overall best reported result on the test set \u2014 is a system combination of Pal et al. (2017) SYMMETRIC with phrase-based models via n-best list re-ranking.", "startOffset": 0, "endOffset": 348}, {"referenceID": 17, "context": "The double-attention models, however, each outperform the best WMT16 system and the currently reported best single-model Pal et al. (2017) SYMMETRIC.", "startOffset": 121, "endOffset": 139}, {"referenceID": 17, "context": "The double-attention models, however, each outperform the best WMT16 system and the currently reported best single-model Pal et al. (2017) SYMMETRIC. The ensembles also beat the system combination Pal et al. (2017) RERANKING in terms of TER (not in terms of BLEU though).", "startOffset": 121, "endOffset": 215}, {"referenceID": 11, "context": "47 Junczys-Dowmunt and Grundkiewicz (2016) 21.", "startOffset": 3, "endOffset": 43}], "year": 2017, "abstractText": "In this work, we explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs mt and src in a single neural architecture, modeling {mt, src} \u2192 pe directly. Apart from that, we investigate the influence of hardattention models which seem to be wellsuited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT 2016 shared task on automatic postediting and can demonstrate that doubleattention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Double-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input.", "creator": "LaTeX with hyperref package"}}}