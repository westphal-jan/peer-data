{"id": "1706.04304", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Dueling Bandits with Weak Regret", "abstract": "We change ads different recommendation put transformational feedback and pre-determined comparisons, framework example still putting - called spotlighting bandit need. We experiment brought farewells conniving problem both the Condorcet finish difficult, part make, embodying of nothing: this already well - lecturer far regret, taken also 0 only went while full hit mainly. Condorcet finish; it the less both - lectured particularly desire, which if 23 come but jaw over important from Condorcet victory. We blueprint short major optimisation put of change, Winner Stays (WS ), rest these for rather makes mainly sympathy: WS included weak shame (WS - W) while expecting decreasing weak sincerely does goes $ O (N ^ 28) $, addition $ O (N \\ boxes (N) ) $ if gun have a 33 meant; WS taken also regret (WS - S) in raise minimum strong regret. $ O (N ^ 17 + N \\ log (T) ) $, took $ O (N \\ copy (N) + N \\ porch (T) ) $ nobody forces such has respectively orders. WS - W once place made litany ogre decoding back bullish impression example does constant began time. WS considered technique hold compute, not for focus a come counter, the we ensure outside generalization observed over anti-submarine bringing goes data turn WS has risk smaller regret while existing probabilistic in yet part inflation - making advantage - regret tools.", "histories": [["v1", "Wed, 14 Jun 2017 03:44:32 GMT  (1612kb,D)", "http://arxiv.org/abs/1706.04304v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bangrui chen", "peter i frazier"], "accepted": true, "id": "1706.04304"}, "pdf": {"name": "1706.04304.pdf", "metadata": {"source": "META", "title": "Dueling Bandits with Weak Regret", "authors": ["Bangrui Chen", "Peter I. Frazier"], "emails": ["<pf98@cornell.edu>."], "sections": [{"heading": "1. Introduction", "text": "We consider bandit learning in personalized content recommendation with implicit pairwise comparisons. We offer pairs of items to a user and record implicit feedback on which offered item is preferred, seeking to learn the user\u2019s preferences over items quickly, while also ensuring that the fraction of time we fail to offer a high-quality item is small. Implicit pairwise comparisons avoid the inaccuracy of user ratings (Joachims et al., 2007) and the difficulty of engaging users in providing explicit feedback.\n1Cornell University, Ithaca, NY. Correspondence to: Peter I. Frazier <pf98@cornell.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nWe study a model for this setting called the dueling bandit problem (Yue & Joachims, 2009). The items we may offer to the user are called \u201carms\u201d, and we learn about these arms through a sequence of \u201cduels\u201d. In each duel, we \u201cpull\u201d two arms and receive noisy feedback from the user telling us which arm is preferred. When an arm is preferred within a duel, we say that the arm has \u201cwon the duel\".\nWe study this problem in the Condorcet winner setting, in which we assume the existence of an arm (the Condorcet winner) that wins with probability at least 12 when paired with any of the other arms. In these settings, we consider two notions of regret: \u201cweak regret\u201c, in which we avoid regret by selecting the Condorcet winner as either arm in the duel; and \u201cstrong-regret\u201d, in which we can only avoid regret by setting both arms in the duel to the Condorcet winner.\nWeak regret was proposed by Yue et al. (2012) and arises in content recommendation when arms correspond to items, and the user incurs no regret whenever his most preferred item is made available. Examples include in-app restaurant recommendations provided by food delivery services like Grubhub and UberEATS, in which implicit feedback may be inferred from selections, and the user only incurs regret if her most preferred restaurant is not recommended. Examples also include recommendation of online broadcasters on platforms such as Twitch, in which implicit feedback may again be inferred from selections, and the user is fully satisfied as long as her favored broadcaster is listed. Despite its applicability, Yue et al. (2012) is the only paper of which we are aware that studies weak regret, and it does not provide algorithms specifically designed for this setting.\nStrong regret has been more widely studied, as discussed below, and has application to choosing ranking algorithms for search (Hofmann et al., 2013). To perform a duel, query results from two rankers are interleaved (Radlinski et al., 2008), and the ranking algorithm that provided the first result chosen by the user is declared the winner of the duel. Strong regret is appropriate in this setting because the user\u2019s experience is enhanced by pulling the best arm twice, so that all of that ranker\u2019s results are shown.\nOur contribution is a new algorithm, Winner Stays (WS), with variants designed for the weak (WS-W) and strong regret (WS-S) settings. We prove that WS-W has expected cumulative weak regret that is constant in time, with de-\nar X\niv :1\n70 6.\n04 30\n4v 1\n[ cs\n.L G\n] 1\n4 Ju\nn 20\n17\npendence on the number of arms N given by O(N2). If the arms have a total order, we show a tighter bound of O(N logN). We then prove that WS-S has expected cumulative strong regret that is O(N2 + N log(T )), and prove that a tighter bound of O(N log(N) + N log(T )) holds if arms have a total order. These regret bounds are optimal in T , and for weak regret are strictly better than those for any previously proposed algorithm, although at the same time both strong and weak regret bounds are sensitive to the minimum gap in winning probability between arms. We demonstrate through numerical experiments on simulated and real data that WS-W and WS-S significantly outperform existing algorithms on strong and weak regret.\nThe paper is structured as follows. Section 2 reviews related work. Section 3 formulates our problem. Section 4 introduces the Winner Stays (WS) algorithm: Section 4.1 defines WS-W for the weak regret setting; Section 4.2 proves that WS-W has cumulative expected regret that is constant in time; Section 4.3 defines WS-S for the strong regret setting and bounds its regret. Section 4.4 disusses a simple extension of our theoretical results to the utility-based bandit setting, which is used in our numerical experiments. Section 5 compares WS with three benchmark algorithms using both simulated and real datasets, finding that WS outperforms these benchmarks on the problems considered."}, {"heading": "2. Related Work", "text": "Most work on dueling bandits focuses on strong regret. Yue et al. (2012) shows that the worst-case expected cumulative strong regret up to time T for any algorithm is \u2126(N log(T )). Algorithms have been proposed that reach this lower bound under the Condorcet winner assumption in the finite-horizon setting: Interleaved Filter (IF) (Yue et al., 2012) and Beat the Mean (BTM) (Yue & Joachims, 2011). Relative Upper Confidence Bound (RUCB) (Zoghi et al., 2014) also reaches this lower bound in the horizonless setting. Relative Minimum Empirical Divergence (RMED) (Komiyama et al., 2015) is the first algorithm to have a regret bound that matches this lower bound. Zoghi et al. (2015) proposed two algorithms, Copeland Confidence Bound (CCB) and Scalable Copeland Bandits (SCB), which achieve an optimal regret bound without assuming existence of a Condorcet winner.\nWhile weak regret was proposed in Yue et al. (2012), it has not been widely studied to our knowledge, and despite its applicability we are unaware of papers that provide algorithms designed for it specifically. While one can apply algorithms designed for the strong regret setting to weak regret, and use the fact that strong dominates weak regret to obtain weak regret bounds ofO(N log(T )), these are looser than the constant-in-T bounds that we show.\nActive learning using pairwise comparisons is also closely related to our work. Jamieson & Nowak (2011) considers an active learning problem that is similar to our problem in that the primary goal is to sort arms based on the user\u2019s preferences, using adaptive pairwise comparisons. It proposes a novel algorithm, the Query Selection Algorithm (QSA), that uses an expected number of operations of d log(N) to sort N arms, where d is the dimension of the space in which the arms are embedded, rather than N log(N). Busa-Fekete et al. (2013) and Busa-Fekete et al. (2014) consider topk element selection using adaptive pairwise comparisons. They propose a generalized racing algorithm focusing on minimizing sample complexity. (Pallone et al., 2017) studies adaptive preference learning across arms using pairwise preferences. They show that a greedy algorithm is Bayesoptimal for an entropy objective. While similar in that they use pairwise comparisons, these algorithms are different in focus from the current work because they do not consider cumulative regret."}, {"heading": "3. Problem Formulation", "text": "We consider N items (arms). At each time t = 1, 2, . . ., the system chooses two items and shows them to the user, i.e., the system performs a duel between two arms. The user then provides binary feedback indicating her preferred item, determining which arm wins the duel. This binary feedback is random, and is conditionally independent of all past interactions given the pair of arms shown. We let pi,j denote the probability that the user gives feedback indicating a preference for arm i, when shown arms i and j. If the user prefers arm i over arm j, we assume pi,j > 0.5. We also assume symmetry: pi,j = 1\u2212 pj,i.\nWe assume arm 1 is a Condorcet winner, i.e., that p1,i > 0.5 for i = 2, \u00b7 \u00b7 \u00b7 , N . In some results, we also consider the setting in which arms have a total order, by which we mean that the arms are ordered so that pi,j > 0.5 for all i < j. The total order assumption implies transitivity.\nWe let p = minpi,j>0.5 pi,j > 0.5 be a lower bound on the probability that the user will choose her favourite arm.\nWe consider both weak and strong regret in its binary form. The single-period weak regret incurred at this time is r(t) = 1 if we do not pull the best arm and r(t) = 0 otherwise. The single-period strong regret is r(t) = 1 if we do not pull the best arm twice and r(t) = 0 otherwise. We also consider utility-based extensions of weak and strong regret in Section 4.4.\nWe use the same notation r(t) to denote strong and weak regret, and rely on context to distinguish the two cases. In both cases, we define the cumulative regret up to time T to be R(T ) = \u2211T t=1 r(t). We measure the quality of an algorithm by its expected cumulative regret."}, {"heading": "4. Winner Stays", "text": "We now propose an algorithm, called Winner Stays (WS), with two variants: WS-W designed for weak regret; and WS-S for strong regret. Section 4.1 introduces WS-W and illustrates its dynamics. Section 4.2 proves the expected cumulative weak regret of WS-W is O(N2) under the Condorcet winner setting, and O(N log(N)) under the total order setting. Section 4.3 introduces WS-S and proves that its expected cumulative strong regret is O(N2 + N log(T )) under the Condorcet winner setting, and O(N log(T ) +N log(N)) under the total order setting, both of which have optimal dependence on T . Section 4.4 extends our theoretical results to utility-based bandits."}, {"heading": "4.1. Winner Stays with Weak Regret (WS-W)", "text": "We now present WS-W, first defining some notation. Let qi,j(t) be the number of times that arm i has defeated arm j in a duel, up to and including time t. Then, define C(t, i) =\u2211 j 6=i qi,j(t)\u2212 qj,i(t). C(t, i) is the difference between the number of duels won and lost by arm i, up to time t. With this notation, we define WS-W in Algorithm 1.\nAlgorithm 1 WS-W Input: arms 1, \u00b7 \u00b7 \u00b7 , N for t = 1, 2, \u00b7 \u00b7 \u00b7 do\nStep 1: Pick it=arg maxi C(t\u22121, i), breaking ties as follows: \u2022 If t > 1 and it\u22121 \u2208 arg maxi C(t\u22121, i), set it = it\u22121. \u2022 Else if t > 1 and jt\u22121 \u2208 arg maxi C(t\u2212 1, i),\nset it = jt\u22121. \u2022 Else choose it uniformly at random from\narg maxi C(t\u2212 1, i). Step 2: Pick jt=arg maxj6=it C(t\u22121, j), breaking ties as follows: \u2022 If t > 1 and it\u22121 \u2208 arg maxi6=it C(t \u2212 1, i) \\ {it},\nset jt = it\u22121. \u2022 Else if t > 1 and jt\u22121 \u2208 arg maxi 6=it C(t\u22121, i)\\{it},\nset jt = jt\u22121. \u2022 Else choose jt uniformly at random from\narg maxj C(t\u2212 1, j) \\ {it}. Step 3: Pull arms it and jt; Step 4: Observe noisy binary feedback and update C(t, it) and C(t, jt);\nend\nWS-W\u2019s pulls can be organized into iterations, each of which consists of a sequence of pulls of the same pair of arms, and rounds, each of which consists of a sequence of iterations in which arms that lose an iteration are not visited again until the next round. We first describe iterations and rounds informally with an example and in Figure 1 before presenting our formal analysis.\nExample: At time t = 1, C(0, i) = 0 for all i, and WS-W pulls two randomly chosen arms. Suppose it pulls arms i1 = 1, j1 = 2 and arm 1 wins. Then C(1, i) is 1 for arm 1, \u22121 for arm 2, and 0 for the other arms. This first pull is an iteration of length 1, arm 1 is the winner, and arm 2 is the loser. This iteration is in the first round. We call t1 = 1 the start of the first round, and t1,1 = 1 the start of the first iteration in the first round.\nAt time t = 2, C(t\u22121, i) is largest for arm 1 so WS-W chooses i2 = 1. Since C(t \u2212 1, i) is \u22121 for arm 2 and 0 for the other arms, WS-W chooses j2 at random from arms 3 through N (suppose N > 2). Suppose it chooses arm j2 = 3. This pair of arms (1 and 3) is different from the pair pulled in the previous iteration (1 and 2), so t1,2 = 2 is the start of the second iteration (in the first round).\nWS-W continues pulling arms 1 and 3 until C(t, i) is \u22121 for one of these arms and 2 for the other. WS-W continues to pull only arms 1 and 3 until one has C(t, i) = 2 even though this may involve times when C(t, i) is 0 for both arms 1 and 3, causing them to be tied with arms 4 and above, because we break ties to prioritize pulling previously pulled arms. The sequence of times when we pull arms 1 and 3 is the second iteration. The arm that ends the iteration with C(t, i) = 2 is the winner of that iteration.\nWS-W continues this process, performing N \u2212 1 iterations on different pairs of arms, pitting the winner of each iteration against a previously unplayed arm in the next iteration. This sequence of iterations is the first round. The winner of the final iteration in the first round, call it arm Z(1), has C(t, Z(1)) = N \u2212 1 and all other arms j 6= Z(1) have C(t, j)=\u22121.\nThe second round begins on the next pull after the end of the first round, at time t2. WS-W again performs N \u2212 1 iterations, playing Z(1) in the first iteration. Each iteration has a winner that passes to the next iteration.\nWS-W repeats this process for an infinite number of rounds. Each round is a sequence of N\u22121 iterations, and an arm that loses an iteration is not revisited until the next round. Figure 1 illustrates these dynamics, and we formalize the definition of round and iteration in the next section."}, {"heading": "4.2. Analysis of WS-W", "text": "In this section, we analyze the weak regret of WS-W. After presenting definitions and preliminary results, we prove WS-W has expected cumulative weak regret bounded by O(N log(N)) when arms have a total order. Then, in the more general Condorcet winner setting, we prove WS-W has expected cumulative weak regret bounded by O(N2). We leave the proofs of all lemmas to the supplement.\nWe define t`, the beginning of round `, and Z(` \u2212 1), the\nwinner of round `, as the unique time and arm such C(t` \u2212 1, Z(`\u2212 1)) = (N \u2212 1)(`\u2212 1) and C(t` \u2212 1, i) = \u2212`+ 1 for all i 6= Z(`\u2212 1).\nWe define t`,k, the beginning of iteration k in round `, as the first time we pull the kth unique pair of arms in the `th round. We let T`,k be the number of successive pulls of this pair of arms.\nWe additionally define terminology to describe arms pulled in an iteration. In a duel between arms i and j with pi,j> 0.5, arm i is called the better arm and arm j is called the worse arm. We say that an arm i is the incumbent in iteration k iteration and round ` if C(t`,k\u22121,i)>0. A unique such arm exists except when ` = k = 1. When ` = k = 1, the incumbent is the better of the two arms being played. We call the arm being played that is not the incumbent the challenger.\nUsing these definitions, we present our first pair of results toward bounding the expected cumulative weak regret of WS-W. They bound the number of pulls in an iteration.\nLemma 1. The conditional expected length of iteration k in round `, given the arms being pulled, is bounded above by N(`\u22121)+k2p\u22121 if the incumbent is worse than the challenger, and by 12p\u22121 if the incumbent is better than the challenger.\nLemma 1 shows that iterations with a worse incumbent use more pulls. We then bound the number of iterations with a worse incumbent.\nLemma 2. Under the total order assumption, the conditional expected number of future iterations with an incumbent worse than the challenger, given history up to time t`,k, is bounded above by 2p 2\n(2p\u22121)3 (log(N) + 1) for any k, ` \u2265 1.\nLemma 2 implies that the incumbent is worse than the challenger in finitely many iterations with probability 1. We now bound the tail distribution of the last such round.\nLemma 3. Let L denote the smallest ` such that no round `\u2032 > ` contains an iteration in which the incumbent is worse\nthan the challenger. Then P (L \u2265 `) \u2264 (\n1\u2212p p\n)` .\nTo present our final set of preliminary lemmas, we define several indicator functions. Let B(`, k) be 1 when the incumbent in iteration k of round ` is better than the challenger. Let D(`) be 1 if arm 1 (the best arm) is the incumbent at the beginning of iteration 1 of round `. Denote B\u0304(`, k) = 1 \u2212 B(`, k) and D\u0304(`) = 1 \u2212D(`). Let V (`, k) be 1 if D(`) = 1 and arm 1 loses in any iteration 1 through k \u2212 1 of round `.\nWe may only incur weak regret during round ` iteration k if D\u0304(`) = 1, or if V (`, k\u2032) = 1 for some k\u2032 < k. We will separately bound the regret incurred in these two different scenarios. Moreover, our bound on the number of pulls, and thus the regret incurred, in this iteration will depend on whether B(`, k) = 1 or B\u0304(`, k) = 1. This leads us to state four inequalities in the following pair of lemmas, which we will in turn use to show Theorem 1. The first lemma applies in both the total order and Condorcet settings, while the second applies only in the total order setting. When proving Theorem 2 we replace Lemma 5 by an alternate pair of inequalities.\nLemma 4.\nE[D\u0304(`)B(`, k)T`,k] \u2264 1\n2p\u2212 1 ( 1\u2212 p p )`\u22121 ,\nE[V (`, k)B(`, k)T`,k] \u2264 1\n2p\u2212 1 ( 1\u2212 p p )` .\nLemma 5. Under the total order assumption:\n\u2022 E [\u2211N\u22121\nk=1 D\u0304(`)B\u0304(`, k)T`,k ] is bounded above by(\n1\u2212p p\n)`\u22121 2N`p2\n(2p\u22121)4 (log(N) + 1).\n\u2022 E [\u2211N\u22121\nk=1 V (`, k)B\u0304(`, k)T`,k ] is bounded above by(\n1\u2212p p\n)` 2N`p2\n(2p\u22121)4 (log(N) + 1).\nWe now state our main result for the total order setting, which shows that the expected cumulative weak regret is O ( N log(N) (2p\u22121)5 ) .\nTheorem 1. The expected cumulative weak regret of WS-W is bounded by\n[ 2p3\n(2p\u22121)6N(log(N) + 1) + N (2p\u22121)2\n] under\nthe total order assumption.\nProof. Iterations can be divided into two types: those in which the incumbent is better than the challenger, and those where the incumbent is worse.\nWe first bound expected total weak regret incurred in the first type of iteration, and then below bound that incurred in the second type. In this first bound, observe that we incur weak regret during round ` if D(`) = 0, or if D(`) = 1 but arm 1 loses to some other arm during this round. Under the second scenario, we do not incur any regret until arm 1 loses to another arm.\nThus, the expected weak regret incurred during iterations with a better incumbent is bounded by\nE [ \u221e\u2211 `=1 N\u22121\u2211 k=1 B(`, k)T`,kD\u0304(`) + \u221e\u2211 `=1 N\u22121\u2211 k=1 B(`, k)T`,kV (`, k) ] .\nThe first part of this summation can be bounded by the first inequality in Lemma 4 to obtain\nE [ \u221e\u2211 `=1 N\u22121\u2211 k=1 B(`, k)T`,kD\u0304(`) ]\n\u2264 \u221e\u2211 `=1 ( 1\u2212 p p )`\u22121 N 2p\u2212 1 =\npN\n(2p\u2212 1)2 .\nThe second part of this summation can be bounded by the second inequality in Lemma 4 to obtain\nE [ \u221e\u2211 `=1 N\u22121\u2211 k=1 B(`, k)T`,kV (`, k) ]\n\u2264 \u221e\u2211 `=1 N 2p\u2212 1 ( 1\u2212 p p )` = N(1\u2212 p) (2p\u2212 1)2 .\nThus, the cumulative expected weak regret incurred during iterations with a better incumbent is bounded by N(2p\u22121)2 .\nNow we bound the expected weak regret incurred during iterations where the incumbent is worse than the challenger.\nThis is bounded by\nE [ \u221e\u2211 `=1 N\u22121\u2211 k=1 B\u0304(`, k)T`,kD\u0304(`) + \u221e\u2211 `=1 N\u22121\u2211 k=1 B\u0304(`, k)T`,kV (`, k) ] .\nThe first term in the summation can be bounded by the first inequality of Lemma 5 to obtain\nE [ \u221e\u2211 `=1 N\u22121\u2211 k=1 B\u0304(`, k)T`,kD\u0304(`) ]\n\u2264 \u221e\u2211 `=1 2N`p(1\u2212 p) (2p\u2212 1)4 (log(N) + 1) ( 1\u2212 p p )`\u22121 = 2Np4\n(2p\u2212 1)6 (log(N) + 1).\nThe second term in the summation can be bounded by the first inequality of Lemma 5 to obtain\nE [ \u221e\u2211 `=1 N\u22121\u2211 k=1 B\u0304(`, k)T`,kV (`, k) ]\n\u2264 \u221e\u2211 `=1 2N`p2 (2p\u2212 1)4 (log(N) + 1) ( 1\u2212 p p )` =\n2p3(1\u2212 p) (2p\u2212 1)6 N(log(N) + 1).\nThus, the cumulative expected weak regret incurred during iterations with a worse incumbent is bounded by\n2p3\n(2p\u22121)6N(log(N) + 1).\nSumming these two bounds, the cumulative expected weak regret is bounded by\n[ 2p3\n(2p\u22121)6N(log(N) + 1) + N (2p\u22121)2\n] .\nWe prove the following result for the Condorcet winner setting in a similar manner in the supplement.\nTheorem 2. The expected cumulative weak regret of WSW is bounded by N(2p\u22121)2 + pN2\n(2p\u22121)3 under the Condorcet winner setting."}, {"heading": "4.3. Winner Stays with Strong Regret (WS-S)", "text": "In this section, we define a version of WS for strong regret, WS-S, which uses WS-W as a subroutine. WS-S is defined in Algorithm 2\nEach round of WS-S consists of an exploration phase and an exploitation phase. The length of the exploitation phase increases exponentially with the number of phases. Changing the parameter \u03b2 balances the lengths of these phases, and thus balances between exploration and exploitation. Our theoretical results below guide choosing \u03b2.\nAlgorithm 2 WS-S Input: \u03b2 > 1, arms 1, \u00b7 \u00b7 \u00b7 , N for ` = 1, 2, \u00b7 \u00b7 \u00b7 do\nExploration phase: Run the `th round of WS-W. Exploitation phase: Let Z(`) be the index of the best arm at the end of the `th round. For the next b\u03b2`c time periods, pull arms Z(`) and Z(`) and ignore the feedback.\nend\nWe now bound the cumulative strong regret of this algorithm under both the total order and Condorcet winner settings:\nTheorem 3. If there is a total order among arms, then for 1 < \u03b2 < p1\u2212p , the expected cumulative strong regret for WS-S is bounded by[\n2p3 (2p\u22121)6N(log(N) + 1) + N log\u03b2(T (\u03b2\u22121)) 2p\u22121\n] .\nProof. Suppose at time T, we are in round `. Then \u03b2+ \u00b7 \u00b7 \u00b7+ \u03b2` \u2264 T . Solving for `, we obtain ` \u2264 log\u03b2(T (\u03b2 \u2212 1)).\nWe bound the expected strong regret up to time T . The expected regret can be divided in two parts: the regret occuring during the exploration phase; and the regret occuring during the exploitation phase.\nFirst we focus on regret incurred during exploration. We never pull the same arm twice during this phase, and so regret is incurred in each time period. To bound regret incurred during exploration, we bound the length of time spent in this phase.\nThe length of time spent in exploration up to the end of round ` with a better incumbent is bounded by (N\u22121)`2p\u22121 . The length of time spent with a worse incumbent, based on the proof of Theorem 1, is bounded by 2p 3\n(2p\u22121)6N(log(N) + 1).\nNow we focus on regret incurred during exploitation. The\nprobability we have identified the wrong arm at the end of the ith round is less than (\n1\u2212p p\n)i . Thus, the expected regret\nincurred during this phase up until the end of the `th round is bounded by \u2211` i=1 ( 1\u2212p p )i \u00d7 \u03b2i \u2264 `.\nOverall, this implies that the strong expected regret up to time T (recall that T is in round `) is bounded by[\n2p3\n(2p\u2212 1)6 N(log(N) + 1) + `+ (N \u2212 1)` 2p\u2212 1 ] \u2264 [ 2p3\n(2p\u2212 1)6 N(log(N) + 1) + N log\u03b2(T (\u03b2 \u2212 1)) 2p\u2212 1\n] .\nThus, the expected strong regret up to time T is O(N log(T ) +N log(N)).\nTheorem 4. Under the Condorcet winner setting and for 1 < \u03b2 < p1\u2212p , the expected cumulative strong regret for\nWS-S is bounded by [\nN2p (2p\u22121)2 + N log(T (\u03b2\u22121)) (2p\u22121) log(\u03b2)\n] .\nProof. The proof mirrors that of Theorem 3, with the only difference being that we bound the length of exploration with a worse incumbent using the proof of Theorem 2 rather than Theorem 1, and the bound is O(N2). Due to its similarity, the proof is omitted.\nThese results provide guidance on the choice of \u03b2. If \u03b2 is too close to 1, then we spend most of the time in the exploration phase, which is guaranteed to generate strong regret. The last inequality in the proof of Theorem 3 suggests that asymptotic regret will be smallest if we choose \u03b2 as large as possible without going beyond the p/(1\u2212 p) threshold. Indeed, if \u03b2 is too large, then WS-S may incur large regret in early exploitation stages when we have finished only a few rounds of exploration. In our numerical experiments\nwe set \u03b2 = 1.1, which satisfies the p/(1 \u2212 p) constraint assumed by our theory if p > \u03b2/(1 + \u03b2) \u2248 .524. With a properly chosen \u03b2, the numerical experiments in section 5.2 suggest WS-S performs better than previously devised algorithms. At the same time, the best choice of \u03b2 is dependent on p. Modifying WS-S to eliminate parameters that must be chosen with knowledge of p is left for future work.\nOur regret bound grows as p, which is the minimal gap between two arms, shrinks, and p tends to decrease as the number of arms N increases. Other dueling bandit algorithm for strong regret, such as RUCB and RMED, have regret bounds with better dependence on the gaps between arms. Modifying WS-S to provide improved dependence on these gaps is also left for future work."}, {"heading": "4.4. Extension to Utility-Based Regret", "text": "We now briefly discuss utility-based extensions of weak and strong regret for the total order setting, following utilitybased bandits studied in Ailon et al. (2014). Our regret bounds also apply here, with a small modification.\nSuppose that the user has a utility ui associated with each arm i. Without loss of generality, we assume u1 > u2 > \u00b7 \u00b7 \u00b7 > uN , and as in the total order setting, we require that pi,j > 0.5 when i < j. Typically the pi,j would come from the utilities of arms i and j via a generative model. We give an example in our numerical experiments.\nThen, the single-period utility-based weak regret is r(t) = u1 \u2212 max{uit , ujt}, which is the difference in utility between the best arm overall and the best arm that the user can choose from those offered. The single-period utility-based strong regret is r(t) = u1 \u2212 uit+ujt 2 . To get zero regret under strong regret, the best arm must be pulled twice.\nOur results from Section 4 carry through to this more general regret setting. Let R = u1 \u2212 uN be the maximum single-period regret. Then, the expected cumulative utilitybased weak regret for WS-W is O ( RN log(N)(2p\u22121)5 ) , and the expected cumulative utility-based strong regret for WS-S is O(R [N log(T ) +N log(N)])."}, {"heading": "5. Numerical Experiments", "text": "In this section, we evaluate WS under both the weak and strong regret settings, considering both their original (binary) and utility-based versions. In the weak regret setting, we compare WS-W with RUCB and QSA. In the strong regret setting, we compare WS-S with 7 benchmarks including RUCB and Relative Minimum Empirical Divergence (RMED) by Komiyama et al. (2015). We also include an experiment violating the total order assumption in Section 11 in the supplement. WS outperforms all benchmarks tested in these numerical experiments."}, {"heading": "5.1. Weak Regret", "text": "We now compare WS-W with QSA and RUCB using simulated data and the Yelp academic dataset (Yelp, 2012)."}, {"heading": "5.1.1. SIMULATED DATA", "text": "In this example, we compare WS-W with RUCB and QSA on a problem with 50 arms and binary weak regret. Each arm is a 20-dimensional vector uniformly generated from the unit circle. We assume pi,j=0.8 for all i<j.\nThe results are summarized in Figure 2a. RUCB has approximately linear regret over the time horizon pictured. This is common in the dueling bandits literature, where many algorithms require \u223c 104 comparisons before they achieve log(T ) cumulative regret for 50 arms. WS-W finds the optimal arm after \u223c500 comparisons and has a regret that is consistent with our theoretically established constant expected cumulative weak regret."}, {"heading": "5.1.2. YELP ACADEMIC DATASET", "text": "In this example, we compare WS-W with RUCB and QSA using the Yelp academic dataset (Yelp, 2012) and utilitybased weak regret.\nWe choose 100 restaurants from Las Vegas as our arms. Associated with each arm (restaurant) i is a 20-dimensional feature vector Ai, calculated using doc2vec (Rehurek & Sojka, 2010) from its reviews. We select 49 users who have reviewed at least 20 of these 100 restaurants. For each user, we model their utility for restaurant i as ui = Ai \u00b7 \u03b8, where \u03b8 is a 20-dimensional vector of preferences. We infer \u03b8 for each user using linear regression.\nTo model pi,j , we then use the probit model. We let \u03c3\u03022 be the estimated variance of the residuals from the linear regression above. When presented with two restaurants, we model the user as taking independent random draws from a normal distribution with means ui and uj respectively and variances \u03c3\u03022, and choosing the restaurant with the larger draw. This gives pij = \u03a6(ui \u2212 uj), where \u03a6(\u00b7) is the cdf for the normal distribution with mean 0 and variance 2\u03c3\u03022.\nWe simulate performance for each user separately, and then average the results. These results are summarized in Figure 2b. WS-W outperforms RUCB and QSA, finding the optimal restaurant after \u223c 500 iterations."}, {"heading": "5.2. Strong Regret", "text": "In this section, we compare WS-S using binary and utilitybased strong regret with 7 benchmarks from the literature. We use the sushi and MSLR datasets, which were previously used by Komiyama et al. (2016) and Zoghi et al. (2015) respectively to evaluate dueling bandit algorithms.\nThe sushi dataset (Komiyama et al., 2016) contains 16 arms corresponding to types of sushi, with pairwise preferences inferred from data on sushi preferences from 5000 users in Kamishima (2003). The MSLR dataset has 5 arms, corresponding to ranking algorithms, with pairwise preferences provided in Zoghi et al. (2015). We give preference matrices (pi,j) for both datasets in the supplement. For utility-based regret, we define ui = 2(1\u2212 p1,i).\nWS-S has a user-defined parameter \u03b2. In our experiments we set \u03b2 = 1.1. The corresponding minimum p for which our theoretical bounds hold is \u03b2/(1 + \u03b2) \u2248 0.52. We recommend \u03b2 \u2248 1.1 for problems of 20 arms or fewer, and \u03b2 closer to 1 for those problems with more arms that are likely to have p closer to 1/2. We also conduct a sensitivity analysis of \u03b2 in the supplement.\nFigure 3 shows the results of our comparisons. WS-S outperforms all 7 benchmarks considered on both datasets using both variants of strong regret."}, {"heading": "6. Conclusion", "text": "In this paper, we consider dueling bandits for online content recommendation using both weak and strong regret.\nWe propose a new algorithm, WS, with variants designed for the weak regret (WS-W) and strong regret (WS-S) settings. We prove WS has constant weak regret and optimal strong regret in T . In numerical experiments, WS outperforms all benchmarks considered on both simulated and real datasets."}, {"heading": "Acknowledgements", "text": "The authors were partially supported by NSF CAREER CMMI-1254298, NSF CMMI-1536895, NSF IIS-1247696, NSF DMR-1120296, AFOSR FA9550-12-1-0200, AFOSR FA9550-15-1-0038, and AFOSR FA9550-16-1-0046."}, {"heading": "A. Gambler\u2019s Ruin Lemma", "text": "In our analysis of WS-W, we will use results from a special case of the Gambler\u2019s ruin problem (Karlin, 1968), stated as follows: suppose a gambler has m dollars initially. In each of a sequence of rounds, he loses 1 dollar with probability q 6= 12 and wins 1 dollar with probability 1 \u2212 q. He stops playing when he has either m+ 1 dollars or has no money left. We have the following result, with a proof available on Page 73 of Karlin (1968).\nLemma 6 (Gambler\u2019s Ruin Lemma). In the gambler\u2019s ruin problem: (1) the probability that the gambler reaches m+ 1 dollars before reaching 0 dollars is qm = ( 1\u2212qq ) m\u22121\n( 1\u2212qq ) m+1\u22121\n;\n(2) the expected number of steps before the gambler stops\nplaying is m1\u22122q \u2212 m+1 1\u22122q\n( 1\u2212qq ) m\u22121\n( 1\u2212qq ) m+1\u22121\n.\nObserve that the conditional distribution of T`,k and the winner of iteration k round `, given the two arms being pulled, is given by the result above for the Gambler\u2019s ruin problem. We leverage this in our proof."}, {"heading": "B. Proof of Lemma 1", "text": "Proof. Suppose we are comparing arm i versus arm j in this iteration with i > j and arm i is the incumbent. Then we know C(t`,k \u2212 1, i) = (N \u2212 1)(` \u2212 1) + k \u2212 1 and C(t`,k \u2212 1, j) = \u2212` + 1. We will keep playing these two arms until C(t`,k + T`,k \u2212 1, i) = (N \u2212 1)(` \u2212 1) + k or C(t`,k + T`,k \u2212 1, j) = (N \u2212 1)(` \u2212 1) + k. Further, since the winning probability of arm i over arm j is pi,j over this period, we know the dynamics of this iteration are the same as those of the Gambler\u2019s Ruin problem. Denote E = C(t`,k \u2212 1, i) \u2212 C(t`,k \u2212 1, j) + 1 = Nl + k \u2212 N . Then the expected length of time we spend in this iteration by Lemma 6 is\nE 1\u2212 2pi,j \u2212 E + 1 1\u2212 2pi,j\n( 1\u2212pi,j pi,j )E \u2212 1(\n1\u2212pi,j pi,j\n)E+1 \u2212 1\n\u2264 E 1\u2212 2pi,j \u2264 E 2p\u2212 1 .\nThe proof of second statement is similar. Using the same notation but now supposing pi,j \u2265 p > 12 , we have that the expected length of time we spend in this iteration is\nE 1\u2212 2pi,j \u2212 E + 1 1\u2212 2pi,j\n( 1\u2212pi,j pi,j )E \u2212 1(\n1\u2212pi,j pi,j\n)E+1 \u2212 1\n= 1 2pi,j \u2212 1 \u2212 E + 1 1\u2212 2pi,j pi,j(1\u2212 pi,j)E \u2212 (1\u2212 pi,j)E+1\n(1\u2212 pi,j)n+1 \u2212 pE+1i,j\n\u2264 1 2p\u2212 1 ."}, {"heading": "C. Proof of Lemma 2", "text": "In this section, we prove Lemma 2 from the main paper. This section is structured as follows: In section C.1, we provide two bounds for the incumbent\u2019s losing and winning probability; In section C.2, we consider a version of the problem in which better and worse incumbents have constant (but different) winning probabilities and provide a upper bound for the number of worse incumbents in a round before a better incumbent loses ; In section C.3, we use the results from the previous subsection to bound the expected number of iterations with a worse incumbent in a single round before a better incumbent loses, starting from within a round; In section C.4, we prove a similar bound on the expected number of iterations with a worse incumbent in this and future rounds before a better incumbent loses, starting from the beginning of a round; In section C.5, we complete the proof of Lemma 2.\nThroughout this section, we use a one to one correspondence between n and (`, k) defined by n = (`\u2212 1)(N \u2212 1) + k, 0 \u2264 k \u2264 N \u2212 1 and ` = dn/(N \u2212 1)e. We also denote p\u2217 = 2p\u22121p .\nC.1. Bounds on Win and Loss Probabilities\nWe first prove the following two lemmas, which give\n\u2022 a lower bound for the probability that a worse incumbent loses an iteration;\n10\n\u2022 an upper bound for the probability that a better incumbent loses an iteration.\nLemma 7. In iteration k of round ` conditioned on the identities of the incumbent and the challenger, if the incumbent is worse than the challenger, then the incumbent loses the iteration with conditional probability at least p\u2217 = 2p\u22121p .\nProof. Let i be the incumbent and j be the challenger, with i > j. C(i, t`,k) \u2265 0 and C(j, t`,k) \u2264 0. Let E = C(i, t`,k) + |C(j, t`,k)| + 1. The probability that arm i loses this iterations is the same as 1\u2212 qE in the Gambler\u2019s Ruin Lemma, Lemma 6, with q = pi,j < 0.5. This probability is:\n1\u2212 qE = 1\u2212\n( 1\u2212pj,i pi,j )E \u2212 1(\n1\u2212pi,j pi,j\n)E+1 \u2212 1\n\u2265\n( 1\u2212pi,j pi,j )E+1 \u2212 ( 1\u2212pi,j pi,j )E (\n1\u2212pi,j pi,j )E+1 = 1\u2212 2pi,j1\u2212 pi,j \u2265 2p\u2212 1\np .\nLemma 8. In iteration k of round ` conditioned on the identities of the incumbent and the challenger, if the incumbent is better than the challenger, then the incumbent loses the it-\neration with conditional probability at most (\n1\u2212p p\n)E , where\nE = N(`\u2212 1) + k.\nProof. This proof is similar to the previous one. Suppose we are pulling arm i and j with i < j and i is the incumbent. Then we know C(t`,k\u22121, i) = (N \u22121)(`\u22121) +k\u22121 and C(t`,k \u2212 1, j) = \u2212` + 1. The probability that arm i loses is equal to 1\u2212 qE from the gambler\u2019s ruin problem, where E = (N \u2212 1)(`\u2212 1) + k\u2212 1 + `\u2212 1 = N(`\u2212 1) + k. We have\n1\u2212 qE = 1\u2212\n( 1\u2212pi,j pi,j )E \u2212 1(\n1\u2212pi,j pi,j\n)E+1 \u2212 1\n=\n( 1\u2212pi,j pi,j )E [1\u2212 1\u2212pp ]\n1\u2212 (\n1\u2212pi,j pi,j )E+1 \u2264 (\n1\u2212 pi,j pi,j\n)E \u2264 (\n1\u2212 p p\n)E .\nC.2. Definition and Upper Bound for g(b,m)\nIn this section, we define a function g(b,m) as follows. First, we define g(0,m) = 0 for any m. We define g(b,m) for other integers b, m satisfying m > 0 and 0 \u2264 b \u2264 m recursively, as follows:\ng(b,m)\n= b\nm + b\u22121\u2211 b\u2032=0 1 m p\u2217g(b \u2032 ,m\u2212 1) + m\u22121\u2211 b\u2032=b 1 m g(b,m\u2212 1)\n+ b\u22121\u2211 b\u2032=0 1 m (1\u2212 p\u2217)g(b\u2212 1,m\u2212 1)\n= b\nm + b\u22121\u2211 b\u2032=0 1 m p\u2217g(b \u2032 ,m\u2212 1) + m\u2212 b m g(b,m\u2212 1)\n+ b\nm (1\u2212 p\u2217)g(b\u2212 1,m\u2212 1) (1)\nIntuitively, g(b,m) is the expected number of future iterations in which the incumbent is worse than the challenger, starting with m arms that have not dueled yet b of which are better than the incumbent, when we stop counting when we reach the end of the round or when an incumbent loses to a worse challenger, in a simplified problem in which worse incumbents beat better challengers with probability p\u2217. In our problem, this probability is not p\u2217, but is bounded below by this quantity, and in the next section we will show that g(b,m) is an upper bound on an analogous quantity in our problem.\nWe prove the following result about g.\nLemma 9. For 0 \u2264 b \u2264 m \u2264 N \u2212 1, we have\ng(b,m) = g(b, b) \u2264 log(b) + 1 p\u2217 .\nProof. Given the boundary conditions g(0,m) = 0 for all m, we know Equation (1) has a unique solution. In this proof,\n\u2022 We first assume g(b,m) = g(b, b) for all b \u2264 m and solve for g(b,m);\n\u2022 Then we show that this g(b,m) is indeed the solution for Equation (1), verifying that g(b,m) is as claimed;\n\u2022 Finally, we show g(b,m) \u2264 log(b)+1p\u2217 .\nFirst, we solve for g(b,m) with the assumption that g(b,m) = g(b, b) for b \u2264 m. Setting m = b in Equation (1)\nprovides\ng(b, b) = 1 + b\u22121\u2211 b\u2032=0 p\u2217g(b \u2032 , b) b + (1\u2212 p\u2217)g(b\u2212 1, b\u2212 1).\n(2)\nThus, we know\nb\u22121\u2211 b\u2032=1 p\u2217g(b \u2032 , b+ 1)\n= b\u22121\u2211 b\u2032=1 p\u2217g(b \u2032 , b) =b [g(b, b)\u2212 1\u2212 (1\u2212 p\u2217)g(b\u2212 1, b\u2212 1)] .\nTherefore, Equation (2) becomes\ng(b+ 1, b+ 1)\n=1 + b\nb+ 1 [g(b, b)\u2212 1\u2212 (1\u2212 p\u2217)g(b\u2212 1, b\u2212 1)]\n+ p\u2217g(b, b)\nb+ 1 + (1\u2212 p\u2217g(b, b).\nRe-organizing the terms, we have\ng(b+ 1, b+ 1)\u2212 g(b, b)\n= 1\nb+ 1 +\nb\nb+ 1 (1\u2212 p\u2217)[g(b, b)\u2212 g(b\u2212 1, b\u2212 1)].\nDenote F (b) = g(b, b)\u2212g(b\u22121, b\u22121). We know F (1) = 1. Thus, we have\nF (b) = 1 b + b\u2212 1 b (1\u2212 p\u2217)F (b\u2212 1)\n= 1\nb +\n1\u2212 p\u2217 b + b\u2212 2 b (1\u2212 p\u2217)2F (b\u2212 2)\n= 1\nb +\n1\u2212 p\u2217 b + \u00b7 \u00b7 \u00b7 (1\u2212 p \u2217)b\u22121 b .\nTherefore,\ng(b, b) = b\u2211 k=1 F (k)\n= b\u2211 k=1 [ 1 k + 1\u2212 p\u2217 k + \u00b7 \u00b7 \u00b7 (1\u2212 p \u2217)k\u22121 k ] .\nThus, if g(b,m) = g(b, b) for all b \u2264 m, we know\ng(b,m) = b\u2211 k=1 [ 1 k + 1\u2212 p\u2217 k + \u00b7 \u00b7 \u00b7 (1\u2212 p \u2217)k\u22121 k ] .\nNow we verify that this is the correct solution. We prove this by induction on b. For b = 1, Equation (1) becomes\ng(1,m) = 1 m + m\u2212 1 m g(1,m\u2212 1).\nSince g(1, 1) = 1, it is easy to check g(1, 2) = g(1, 3) = \u00b7 \u00b7 \u00b7 = g(1, N \u2212 1) = 1.\nSuppose this g(b,m) = g(b, b) are true for all b \u2264 m, b \u2264 k. For b = k + 1, Equation (1) becomes\ng(k + 1,m)\n= k + 1\nm + k\u2211 b\u2032=0 p\u2217 m g(b \u2032 ,m\u2212 1) + m\u2212 k \u2212 1 m g(k + 1,m\u2212 1)\n+ k + 1\nm (1\u2212 p\u2217)g(k,m\u2212 1)\n= k + 1\nm + k\u2211 b\u2032=0 p\u2217 m g(b \u2032 , b \u2032 ) + m\u2212 k \u2212 1 m g(k + 1,m\u2212 1)\n+ k + 1\nm (1\u2212 p\u2217)g(k, k).\nTo show g(k + 1,m) does not depend on m, we need to prove the following equation is true for m = k + 2, k + 3, \u00b7 \u00b7 \u00b7 , N \u2212 1.\nk + 1\nm + k\u2211 b\u2032=0 p\u2217 m g(b \u2032 , b \u2032 ) + k + 1 m (1\u2212 p\u2217)g(k, k)\n= k + 1\nm g(k + 1,m\u2212 1)\n\u21d0\u21d2 k + 1 + k\u2211\nb\u2032=0\np\u2217g(b \u2032 , b \u2032 ) + (k + 1)(1\u2212 p\u2217)g(k, k)\n=(k + 1)g(k + 1,m\u2212 1) (3)\nWe first check Equation (3) when m = k+ 2. Starting from the left hand side, we have\nk + 1 + k\u2211 b\u2032=0 g(b \u2032 , b \u2032 ) + (k + 1)(1\u2212 p\u2217)g(k, k)\n=k + 1 + (k + 1)[g(k + 1, k + 1)\u2212 1\u2212 (1\u2212 p\u2217)g(k, k)] (4)\n+ (k + 1)(1\u2212 p\u2217)g(k, k) =(k + 1)g(k + 1, k + 1),\nwhich equals to the right hand side. Equation (4) follows from Equation (2) (Equation (2) holds because g(b,m) = g(b, b) for all b \u2264 k).\nAgain, by induction, we know (3) is true for all m = k + 2, \u00b7 \u00b7 \u00b7 , N \u2212 1 and thus we concludes our induction.\nWe have shown that g(b,m) = g(b, b) for all b \u2264 m.\nFinally, we prove g(b, b) = g(b,m) \u2264 log(b)+1p\u2217 . This is\nbecause\ng(b,m) =g(b, b)\n= b\u2211 k=1 [ 1 k + 1\u2212 p\u2217 k + \u00b7 \u00b7 \u00b7 (1\u2212 p \u2217)k\u22121 k ]\n\u2264 b\u2211\nk=1\n[ 1\nk +\n1\u2212 p\u2217 k + \u00b7 \u00b7 \u00b7 (1\u2212 p \u2217)b\u22121 k\n]\n= b\u2211 k=1 1 k [ 1 + (1\u2212 p\u2217) + \u00b7 \u00b7 \u00b7+ (1\u2212 p\u2217)b\u22121 ] \u2264 log(b) + 1\np\u2217 ,\nwhich concludes our proof.\nC.3. Bound on the Number of Iterations in One Round with a Worse Incumbent, Starting from Within the Round\nLet B(n) denote an indicator function that equals 1 if we have a better incumbent at the nth iteration. The definition of B(n) is very similar to B(`, k) except B(`, k) tracks both round and iteration number. Similarly, we use B\u0304(n) = 1\u2212B(n) to denote an indicator function that equals 1 if we have a worse incumbent at the nth iteration.\nLet h(i, n,A) be the expected number of iterations with an incumbent that is worse than the challenger, between iteration n and the first time that a better incumbent loses to a challenger or the round ends, given that the incumbent arm at iteration n is i and A is the set of arms that have not yet previously dueled in the round. Formally, we define this quantity as:\nh(i, n,A) = E [ \u03c3\u22121\u2211 n\u2032=n B(n\u2032)|A, in = i ] ,\nwhere\n\u2022 Conditioning on A is understood to mean that we are conditoning on C(n\u2212 1, j) = \u2212`+ 1 \u2200 j /\u2208 A\u222a{in}, and C(n\u2212 1, j) = \u2212` \u2200 j \u2208 A, where ` = dn/(N \u2212 1)e is the round in which iteration n resides. In other words, it is understood to mean that A contains the set of arms that have not yet dueled in this round.\n\u2022 \u03c3 = min {n\u2032 > n : J(n\u2032) = 1, n\u2032 = Ndn/(N \u2212 1)e} where J(n) is an indicator that equals 1 when a better incumbent loses at iteration n, i.e., \u03c3 is the first time that either a better incumbent loses or the round ends.\nLemma 10. For any i, `, k and A, we have\nh(i, n,A) \u2264 g(b,m) \u2264 log(N) + 1 p\u2217 ,\nwhere m = N \u2212 k and b = |{u \u2208 A : u < i}|.\nProof. Denote qi,j(n) as the probability that incumbent arm i will beat challenger j at time n. We first write a recursive expression for h(i, n,A) that applies when n is not divisible by N :\nh(i, n,A) = \u2211\n{j\u2208A:i>j}\n[ 1 + qi,j(n)\nN \u2212 k h(i, n+ 1,A \u222a {j})\n+ 1\u2212 qi,j(n) N \u2212 k\nh(j, n+ 1,A \u222a {i}) ]\n+ \u2211\n{j\u2208A:i<j}\nqi,j(n) N \u2212 k h(i, n+ 1,A \u222a j). (5)\nWhen n is divisible by N \u2212 1, the only allowed value of A is \u2205 and h(i, n, \u2205) = 0.\nWe then prove the desired result via induction on the number of iterations in the round, i.e., on n (mod N \u2212 1). When n (mod N \u2212 1) = 0, we have h(i, n, \u2205) = 0, b = 0, and g(b,m) = 0. Thus the result holds in this case.\nThen suppose the result holds for all n with a particular value of n (mod N \u2212 1) and we show it holds for n\u2212 1.\nApplying the induction hypothesis to the right-hand side of (5), we have\nh(i, n,A) \u2264 \u2211\n{j\u2208A:i>j}\n[ 1 + qi,j(n)\nm g(bi,j ,m\u2212 1)\n+ 1\u2212 qi,j(n)\nm g(bj,j ,m\u2212 1) ] +\n\u2211 {j\u2208A:i<j} qi,j(n) m g(bi,j ,m\u2212 1), (6)\nwhere bu,j = #{u \u2032 \u2208 A \\ {j} : u\u2032 < u}.\nConsider the summand in the first sum in (6), dropping the constants 1 and 1m ,\nqi,j(n)g(bi,j ,m\u2212 1) + (1\u2212 qi,j(n))g(bj,j ,m\u2212 1). (7)\nThis is increasing in qi,j(n) when i > j since bi,j > bj,j , and since g(b,m) is increasing in b. Since i is an incumbent that is worse than the challenger when i > j, Lemma 7 shows that qi,j(n) \u2264 1 \u2212 p\u2217 = 1 \u2212 2p\u22121p in this situation. Thus, this summand is bounded above by (1\u2212p\u2217)g(bi,j ,m\u2212 1) + p\u2217g(bj,j ,m\u2212 1).\nSubstituting this into (6), along with the inequality qi,j(n) \u2264\n1 in the last term, we have\nh(i, n,A) \u2264 \u2211\n{j\u2208A:i>j}\n[ 1 + 1\u2212 p\u2217\nm g(bi,j ,m\u22121) +\np\u2217 m g(bj,j ,m\u22121) ] +\n\u2211 {j\u2208A:i<j} 1 m g(bi,j ,m\u2212 1)\n= b\nm +\nb\nm (1\u2212 p\u2217)g(b\u2212 1,m\u2212 1) + b\u22121\u2211 b\u2032=0 p\u2217 m g(b \u2032 ,m\u2212 1)\n+ m\u2212 b m g(b,m\u2212 1)\n=g(b,m)\nIn the second to last line we have used that {bi,j : j \u2208 A, i > j} = {0, . . . , b \u2212 1} and bi,j = b \u2212 1 when i > j; bi,j = b when i < j; and that the cardinality of {j \u2208 A : i > j} and {j \u2208 A : i < j} are b and m\u2212 b respectively. In the last line we have used the recursive definition of g(b,m) in terms of g(\u00b7,m\u2212 1).\nThis shows the first inequality in the statement of the lemma. The second inequality follows directly from Lemma 9.\nC.4. Bound on the Number of Iterations with a Worse Incumbent, Starting from a Round Beginning\nDenote f(i, `) to be the expected number of iterations with a worse incumbent in this and future rounds, stopping as soon as a better incumbent loses, giving that we have arm i as the incumbent at the start of round `.\nLemma 11. For any i and `, we have\nf(i, `) \u2264 log(N) + 1 (p\u2217)2 .\nProof. Let U(i, `) denote the expected number of iterations in round `with a worse incumbent before a better incumbent loses. We use V (`) to denote an indicator which equals to 1 if a better incumbent does not lose in the round `. Then for i > 1,\nf(i, `) = U(i, `) + E[f(Z(`), `+ 1)V (`)|Z(`\u2212 1) = i].\nThe first term is bounded by Lemma 10 by\nU(i, `) \u2264 log(N) + 1 p\u2217 ,\nfor all i and `.\nFor the second term, since f(Z(`), `+1) = 0 when Z(`) = 1, we know the second term is bounded by\nE[f(Z(`), `+ 1)V (`)|Z(`\u2212 1) = i] \u2264E[f(Z(`), `+ 1)|Z(`) 6= 1, V (`) = 1, Z(`\u2212 1) = i] \u00d7 P (Z(`) 6= 1, V (`)|Z(`\u2212 1) = i).\nLet sj = P (Z(`) = j|Z(`) 6= 1, V (`), Z(`\u2212 1) = i) to be the probability distribution over the integers from 2 through N . Then we know\nE[f(Z(`), `+ 1)|Z(`) 6= 1, V (`) = 1, Z(`\u2212 1) = i]\n= N\u2211 j=2 sjf(j, `+ 1)\n\u2264 max j=2,. . . ,N f(j, `+ 1).\nFurther, since if arm 1 wins its first duel as a challenger (which happens with probability at least p\u2217), then either Z(`) = 1 (it wins all subsequent duel in the round) or V (`) = 0 (it loses a subsequent duel), we have P (Z(`) 6= 1, V (`)|Z(`\u2212 1) = i) \u2264 1\u2212 p\u2217.\nThus, we know\nf(i, `) \u2264 log(N) + 1 p\u2217 + (1\u2212 p\u2217) max j=2,\u00b7\u00b7\u00b7 ,N f(j, `+ 1).\nLet f(`) = maxj=2,\u00b7\u00b7\u00b7 ,N f(j, `). Then,\nf(`) \u2264 log(N) + 1 p\u2217 + (1\u2212 p\u2217)f(`+ 1).\nThus,\nf(1) \u2264 log(N) + 1 p\u2217 + (1\u2212 p\u2217)f(2)\n\u2264 log(N) + 1 p\u2217 (1 + (1\u2212 p\u2217) + (1\u2212 p\u2217)2 + \u00b7 \u00b7 \u00b7 )\n= log(N) + 1\n(p\u2217)2 .\nC.5. Completing the Proof of Lemma 2\nWith the lemmas in the preceding subsections established, we now complete the proof of Lemma 2.\nProof. Let \u03c40 = 0 and \u03c4k = {n > \u03c4k\u22121 : J(n) = 1}. The expected number of iterations with a worse incumbent is\nE [ \u221e\u2211 n=0 B\u0304(n) ]\n=E \u221e\u2211 k=0 1{\u03c4k <\u221e} \u221e\u2211 n=\u03c4k 1{n < \u03c4k+1}B\u0304(n)\n= \u221e\u2211 k=0 P (\u03c4k <\u221e)E [ \u221e\u2211 n=\u03c4k 1{n < \u03c4k+1}B\u0304(n)|\u03c4k <\u221e ]\nwhere we have used Tonelli\u2019s theorem to exchange the expectation of an infinite sum of non-negative terms with an infinite sum of expectations of the same terms.\nConditioning on the history available at time \u03c4k, we have that the inner expectation can be written as,\nE [ \u221e\u2211 n=\u03c4k 1{n < \u03c4k+1}B\u0304(n)|\u03c4k <\u221e ]\n=E [ E [ \u221e\u2211 n=\u03c4k 1{n < \u03c4k+1}B\u0304(n)|H\u03c4k , \u03c4k <\u221e ] |\u03c4k <\u221e ] ,\nwhere Hn is the sigma algebra generated by (C(i, s) : s < t`,k\u2032 , i = 1, . . . , N), where ` = n (mod N \u2212 1), k \u2032 = dn/(N \u2212 1)e, and H\u03c4k is the filtration (Hn : n) stopped at \u03c4k. We further break this inner term E [\u2211\u221e\nn=\u03c4k 1{n < \u03c4k+1}B\u0304(n)|H\u03c4k , \u03c4k <\u221e\n] into two\nparts: the part that occurs during the round in which \u03c4k resides, and the part that occurs in future rounds. Let `k = d\u03c4k/(N \u2212 1)e. Then,\nE [ \u221e\u2211 n=\u03c4k 1{n < \u03c4k+1}B\u0304(n)|H\u03c4k , \u03c4k <\u221e ]\n=E [ `kN\u2211 n=\u03c4k 1{n < \u03c4k+1}B\u0304(n)|H\u03c4k , \u03c4k <\u221e ]\n+E [ \u221e\u2211 n=`kN+1 1{n < \u03c4k+1}B\u0304(n)|H\u03c4k , \u03c4k <\u221e ]\n\u2264 log(N) + 1 p\u2217 + log(N) + 1 (p\u2217)2\n\u22642(log(N) + 1) (p\u2217)2\nwhere the second to last inequality relies on Lemma 10 to show E [\u2211`kN n=\u03c4k 1{n < \u03c4k+1}B\u0304(n)|H\u03c4k , \u03c4k <\u221e ] is bounded above by log(N)+1p\u2217 and Lemma 11 to show E [\u2211\u221e\nn=`kN+1 1{n < \u03c4k+1}B\u0304(n)|H\u03c4k , \u03c4k <\u221e\n] is\nbounded above by log(N)+1(p\u2217)2 .\nThus,\nE [ \u221e\u2211 n=0 B\u0304(n) ] \u2264 2(log(N) + 1) (p\u2217)2 \u221e\u2211 k=0 P (\u03c4k <\u221e).\nNow we bound P (\u03c4k < \u221e) for a fixed k. Based on Lemma 8, we know J(n) is a Bernoulli random variable\nwith success rate less than (\n1\u2212p p\n)n (this is because of\nLemma 8 and n = (N \u2212 1)(` \u2212 1) + k < E), independent across n. Let Qn denote a Bernoulli random variable with success rate ( 1\u2212p p )n . Then we know:\nP (\u03c4k <\u221e) \u2264 P ( \u221e\u2211 i=1 J(i) \u2265 k )\n\u2264 P ( \u221e\u2211 i=1 Qi \u2265 k ) .\nLet Wm = \u2211m i=1Qi, which follows a Poisson Bernoulli distribution, and let W = limm\u2192\u221eWm. W follows a\nPoisson distribution with parameter \u2211\u221e i=1 ( 1\u2212p p )i = 1\u2212p2p\u22121 (Theorem 4, Wang (1993)). Thus,\nE [ \u221e\u2211 n=0 B\u0304(n) ] \u2264 2(log(N) + 1) (p\u2217)2 \u221e\u2211 k=0 P (W \u2265 k)\n= 2p2(1\u2212 p) (2p\u2212 1)3 (log(N) + 1)\n\u2264 2p 2\n(2p\u2212 1)3 (log(N) + 1)"}, {"heading": "D. Proof of Lemma 3", "text": "Proof. It is easy to see that at the last iteration which has a worse incumbent, the better arm is always arm 1. Thus, we only consider C(t, 1) in this proof. At the end of the `th round, ifC(t`+1\u22121, 1) < 0, we knowC(t`+1\u22121, 1) = \u2212`.\nLet us consider a simple random walk W(t) such that W (t+ 1) = W (t) + 1 with probability p > 12 and W (t + 1) = W (t) \u2212 1 with probability 1 \u2212 p for t \u2265 1. If we denote p\u2217` = P (\u2203t\u2217,W (t\u2217) = \u2212`) for ` > 0, then it is easy to\ncalculate that p\u2217` = ( 1\u2212p p )` .\nNow let us consider C(t, 1). If we pull arm 1 with some other arm i at time t, then C(t, 1) = C(t\u22121, 1)+1 happens with probability p1,i > p and C(t, 1) = C(t \u2212 1, 1) \u2212 1 with probability 1\u2212 p1,i < 1\u2212 p. If we do not pull arm 1 at time t, then C(t, 1) = C(t\u2212 1, 1) with probability 1.\nDefine \u03c41 = 1 and \u03c4k = mint{t > \u03c4k\u22121, C(t, 1) 6= C(\u03c4k\u22121, 1)}, for k = 1, 2, \u00b7 \u00b7 \u00b7 ,. Because \u03c4k is a nondecreasing right continuous stopping time, we know it\nis a valid random change of time (Barndorff-Nielsen & Shiryaev, 2015). Define R(k) a new stochastic process where R(k) = C(\u03c4k, 1). Then we know at every time k, R(k) = R(k \u2212 1) + 1 with probability greater or equal to p and R(k) = R(k \u2212 1)\u2212 1 with probability less than 1-p. Define p` = P (\u2203t\u2217, R(t\u2217) = \u2212`), then it is easy to prove\np` \u2264 p\u2217` = ( 1\u2212p p )` using first step analysis and induction (we leave the proof as an exercise for the reader), which\nmeans P (\u2203t\u2217, C(t\u2217, 1) = \u2212`) \u2264 ( 1\u2212p p )` ."}, {"heading": "E. Proof of Lemma 4", "text": "Proof. To show the first claimed equation, we have:\nE[B(`, k)T`,kD\u0304(`)] =E[B(`, k)T`,k|D\u0304(`) = 1]P (D\u0304(`) = 1). (8)\nThe first term E[B(`, k)T`,k|D\u0304(`) = 1] can be bounded by writing it as E[B(`, k)T`,k|D\u0304(`) = 1] = E[E[B(`, k)T`,k|D\u0304(`) = 1, A(`, k)]|D\u0304(`) = 1], where A(`, k) denotes the pair of arms being pulled in iteration k round `.\nWe focus on the inner term E[B(`, k)T`,k|D\u0304(`) = 1, A(`, k)]. B(`, k) is observable given A(`, k). If B(`, k) = 0 then this inner term is 0. If B(`, k) = 1 then this inner term is E[T`,k|A(`, k)] (where we note that T`,k is conditionally independent of D\u0304(`) given A(`, k)) and is bounded above by 1/(2p\u2212 1) by Lemma 1. In both cases, the inner term is bounded above by 1/(2p\u22121), and we have that E[B(`, k)T`,k|D\u0304(`) = 1] \u2264 1/(2p\u2212 1).\nThus, we have that (8) is bounded above by\n1 2p\u2212 1 P (D\u0304(`) = 1) \u2264 1 2p\u2212 1 ( 1\u2212 p p )`\u22121 ,\nwhere the final inequality follows from Lemma 3 and the fact that D\u0304(`) = 1 implies L \u2265 `\u2212 1.\nTo show the second claimed equation, we use the same proof technique used for the first and get:\nE[B(`, k)T`,kV (`, k)] \u2264 1\n2p\u2212 1 P (V (`, k) = 1).\nNow we just need to compute P (V (`, k) = 1). Given C(t` \u2212 1, 1) = (N \u2212 1)(` \u2212 1) at the beginning of round `, it loses only if there exists a t0 \u2265 t` and C(1, t0) = \u2212`. Using the results from Lemma 3, we know P (V (`, k) =\n1) \u2264 (\n1\u2212p p\n)` . This completes the proof of the second\nclaimed equation."}, {"heading": "F. Proof of Lemma 5", "text": "Proof. For the first inequality, we know\nE [ N\u22121\u2211 k=1 B\u0304(`, k)T`,kD\u0304(`) ]\n= N\u22121\u2211 k=1 E [ E[B\u0304(`, k)T`,k|D(`) = 0]D\u0304(`) ] . (9)\nMoreover,\nE[B\u0304(`, k)T`,k|D(`) = 0] =E[T`,k|B(`, k) = 0, D(`) = 0]P (B(`, k) = 0|D(`) = 0)\n\u2264 N` 2p\u2212 1 P (B(`, k) = 0|D(`) = 0),\nwhere the last equation follows from applying Lemma 1 and iterated conditional expectation. Thus, we know\n(9) = N\u22121\u2211 k=1 N` 2p\u2212 1 P (B(`, k) = 0|D(`) = 0)E[D\u0304(`)]\n\u2264 N\u22121\u2211 k=1 N` 2p\u2212 1 P (B(`, k) = 0|D(`) = 0) ( 1\u2212 p p )`\u22121 (10)\n\u2264 (\n1\u2212 p p\n)`\u22121 2N`p2\n(2p\u2212 1)4 (log(N) + 1),\nwhere equation (10) is because Lemma 2.\nThe proof of the second inequality follows very similarly, and is omitted."}, {"heading": "G. Proof of Theorem 2", "text": "In this section, we prove the cumulative expected weak regret of WS-W is bounded by O(N2) in the Condorcet winner setting. First, we want to give an example to illustrate why our algorithm will not have O(N log(N)) regret under the Condorcet winner setting.\nIn the Condorcet winner setting, Lemma 2 is no longer true. Here is a counter example to illustrate why Lemma 2 does not hold true anymore. Suppose we have N = 3k + 1 arms in total, which includes a Condorcet winner arm and three types of other arms: k type-A arms, k type-B arms and k type-C arms. Among these arms, we assume the user prefers type-A arms than type-B arms, type-B arms than type-C arms and type-C arms than type-A arms. Among each type of arms, there is a total order. In this setting, the expected number of iterations with a worse incumbent is\nO(N) instead of O(log(N)), which means Lemma 2 is no longer true.\nNow we start our proof for Theorem 2.\nProof. In the Condorcent winner setting, Lemmas 3 and 4 hold, but as explained earlier, Lemma 2 does not. Because the proof of Lemma 5 utilizes Lemma 2, Lemma 5 also no longer holds.\nOn the other hand, since we can have at most N \u2212 1 iterations in a round, we know the following statement is true: the conditional expected number of iterations with a worse incumbent is bounded by N in each round. Thus, we know Lemma 5 now becomes:\nE [ N\u22121\u2211 k=1 B\u0304(`, k)T`,kD\u0304(`) ] \u2264 ( 1\u2212 p p )`\u22121 N2` 2p\u2212 1 ,\nE [ N\u22121\u2211 k=1 B\u0304(`, k)T`,kV (`, k) ] \u2264 ( 1\u2212 p p )` N2` 2p\u2212 1 .\nThus, following the same reasoning as in the proof of Theorem 1, we know the expected weak regret in the Condorcet\nwinner setting is bounded by\nNR\n(2p\u2212 1)2 +\npN2\n(2p\u2212 1)3 ,\nwhich concludes our proof."}, {"heading": "H. Preference Matrices", "text": "In the sushi experiment, the user\u2019s preference matrix is given by Figure 4.\nIn the MSLR experiment, the ranker\u2019s preference matrix is given by:\n 0.5 0.535 0.613 0.757 0.765\n0.465 0.5 0.580 0.727 0.738 0.387 0.420 0.5 0.659 0.669 0.243 0.276 0.341 0.5 0.510 0.235 0.262 0.331 0.490 0.5\n"}, {"heading": "I. Condorcet Winner Experiment", "text": "In the main paper, we considered numerical examples in which the arms have a total order. This is common in the dueling bandits literature, where even work that considers more general settings theoretically test their methods on problems that satisfy the total order assumption (Komiyama et al., 2016; Urvoy et al., 2013).\nIn this section, we consider an additional example that has a Condorcet winner but does not have a total order among arms. The example has a cyclic struture, and is similar to the cyclic example in Komiyama et al. (2015).\nThe preference matrix is:\n 0.5 0.6 0.6 0.6 0.4 0.5 0.6 0.4 0.4 0.4 0.5 0.6 0.4 0.6 0.4 0.5  In the above example, arm 1 is the Condorcet winner. Arm 2 beats arm 3, arm 3 beats arm 4 and arm 4 beats arm 2.\nAgain, we consider both binary strong regret and the utilitybased strong regret. The utility-based strong regret is defined the same as the other two experiments. The result is summarized in Figure 5. WS-S outperforms all benchmarks considered in all time periods on binary regret, and outperforms them all in all time periods except T = 102 on utility-based regret."}, {"heading": "J. Sensitivity Analysis", "text": "In this section, we conduct a sensitivity analysis of \u03b2 in WSS using the MSLR dataset. In this analysis, we choose \u03b2 = 1.01, 1.05, 1.1, 1.2, 1.5 respectively and compare them with RMED and RUCB. The result is summarized in Figure 6.\nBased on Figure 6, WS-S with \u03b2 = 1.05, 1.1, 1.2 outperforms RMED and RUCB. When \u03b2 = 1.01, we spend too\nmuch time on the exploration period and do not exploit enough. Similarly, WS-S with \u03b2 = 1.5 over exploits and does not explore enough. In both cases, WS-S underperforms RMED and RUCB. However, as long as \u03b2 is within a reasonable range, WS-S can outperform existing state-of-art algorithms."}], "references": [{"title": "Reducing dueling bandits to cardinal bandits", "author": ["Ailon", "Nir", "Karnin", "Zohar Shay", "Joachims", "Thorsten"], "venue": "In ICML,", "citeRegEx": "Ailon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2014}, {"title": "Change of time and change of measure, volume 21", "author": ["Barndorff-Nielsen", "Ole E", "Shiryaev", "Albert"], "venue": "World Scientific Publishing Co Inc,", "citeRegEx": "Barndorff.Nielsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barndorff.Nielsen et al\\.", "year": 2015}, {"title": "Preference-based rank elicitation using statistical models: The case of mallows", "author": ["Busa-Fekete", "R\u00f3bert", "H\u00fcllermeier", "Eyke", "Sz\u00f6r\u00e9nyi", "Bal\u00e1zs"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2014}, {"title": "Fidelity, soundness, and efficiency of interleaved comparison methods", "author": ["Hofmann", "Katja", "Whiteson", "Shimon", "Rijke", "Maarten De"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Hofmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2013}, {"title": "Active ranking using pairwise comparisons", "author": ["Jamieson", "Kevin G", "Nowak", "Robert"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jamieson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2011}, {"title": "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search", "author": ["Joachims", "Thorsten", "Granka", "Laura", "Pan", "Bing", "Hembrooke", "Helene", "Radlinski", "Filip", "Gay", "Geri"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Joachims et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2007}, {"title": "Nantonac collaborative filtering: recommendation based on order responses", "author": ["Kamishima", "Toshihiro"], "venue": "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Kamishima and Toshihiro.,? \\Q2003\\E", "shortCiteRegEx": "Kamishima and Toshihiro.", "year": 2003}, {"title": "A First Course In Stochastic Processes", "author": ["Karlin", "Samuel"], "venue": null, "citeRegEx": "Karlin and Samuel.,? \\Q1968\\E", "shortCiteRegEx": "Karlin and Samuel.", "year": 1968}, {"title": "Regret lower bound and optimal algorithm in dueling bandit problem", "author": ["Komiyama", "Junpei", "Honda", "Junya", "Kashima", "Hisashi", "Nakagawa", "Hiroshi"], "venue": "In COLT,", "citeRegEx": "Komiyama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Komiyama et al\\.", "year": 2015}, {"title": "Copeland dueling bandit problem: Regret lower bound, optimal algorithm, and computationally efficient algorithm", "author": ["Komiyama", "Junpei", "Honda", "Junya", "Nakagawa", "Hiroshi"], "venue": "arXiv preprint arXiv:1605.01677,", "citeRegEx": "Komiyama et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Komiyama et al\\.", "year": 2016}, {"title": "Bayes-optimal entropy pursuit for active choice-based preference learning", "author": ["Pallone", "Stephen N", "Frazier", "Peter I", "Henderson", "Shane G"], "venue": "arXiv preprint arXiv:1702.07694,", "citeRegEx": "Pallone et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Pallone et al\\.", "year": 2017}, {"title": "How does clickthrough data reflect retrieval quality", "author": ["Radlinski", "Filip", "Kurup", "Madhu", "Joachims", "Thorsten"], "venue": "In Proceedings of the 17th ACM conference on Information and knowledge management,", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "Software framework for topic modelling with large corpora", "author": ["Rehurek", "Radim", "Sojka", "Petr"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. Citeseer,", "citeRegEx": "Rehurek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rehurek et al\\.", "year": 2010}, {"title": "On the number of success in independent trials", "author": ["Y.H. Wang"], "venue": "Statistica Sinica,", "citeRegEx": "Wang,? \\Q1993\\E", "shortCiteRegEx": "Wang", "year": 1993}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Yue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2009}, {"title": "Beat the mean bandit", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}, {"title": "The k-armed dueling bandits problem", "author": ["Yue", "Yisong", "Broder", "Josef", "Kleinberg", "Robert", "Joachims", "Thorsten"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Yue et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2012}, {"title": "Relative upper confidence bound for the k-armed dueling bandit problem", "author": ["Zoghi", "Masrour", "Whiteson", "Shimon", "Munos", "Remi", "Rijke", "Maarten de"], "venue": "In JMLR Workshop and Conference Proceedings,", "citeRegEx": "Zoghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2014}, {"title": "Copeland dueling bandits", "author": ["Zoghi", "Masrour", "Karnin", "Zohar S", "Whiteson", "Shimon", "De Rijke", "Maarten"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zoghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2015}, {"title": "section, we consider an additional example that has a Condorcet winner but does not have a total order among arms. The example has a cyclic struture, and is similar to the cyclic example", "author": ["Komiyama"], "venue": null, "citeRegEx": "Komiyama,? \\Q2015\\E", "shortCiteRegEx": "Komiyama", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Implicit pairwise comparisons avoid the inaccuracy of user ratings (Joachims et al., 2007) and the difficulty of engaging users in providing explicit feedback.", "startOffset": 67, "endOffset": 90}, {"referenceID": 3, "context": "Strong regret has been more widely studied, as discussed below, and has application to choosing ranking algorithms for search (Hofmann et al., 2013).", "startOffset": 126, "endOffset": 148}, {"referenceID": 11, "context": "To perform a duel, query results from two rankers are interleaved (Radlinski et al., 2008), and the ranking algorithm that provided the first result chosen by the user is declared the winner of the duel.", "startOffset": 66, "endOffset": 90}, {"referenceID": 12, "context": "Weak regret was proposed by Yue et al. (2012) and arises in content recommendation when arms correspond to items, and the user incurs no regret whenever his most preferred item is made available.", "startOffset": 28, "endOffset": 46}, {"referenceID": 12, "context": "Weak regret was proposed by Yue et al. (2012) and arises in content recommendation when arms correspond to items, and the user incurs no regret whenever his most preferred item is made available. Examples include in-app restaurant recommendations provided by food delivery services like Grubhub and UberEATS, in which implicit feedback may be inferred from selections, and the user only incurs regret if her most preferred restaurant is not recommended. Examples also include recommendation of online broadcasters on platforms such as Twitch, in which implicit feedback may again be inferred from selections, and the user is fully satisfied as long as her favored broadcaster is listed. Despite its applicability, Yue et al. (2012) is the only paper of which we are aware that studies weak regret, and it does not provide algorithms specifically designed for this setting.", "startOffset": 28, "endOffset": 732}, {"referenceID": 16, "context": "Algorithms have been proposed that reach this lower bound under the Condorcet winner assumption in the finite-horizon setting: Interleaved Filter (IF) (Yue et al., 2012) and Beat the Mean (BTM) (Yue & Joachims, 2011).", "startOffset": 151, "endOffset": 169}, {"referenceID": 17, "context": "Relative Upper Confidence Bound (RUCB) (Zoghi et al., 2014) also reaches this lower bound in the horizonless setting.", "startOffset": 39, "endOffset": 59}, {"referenceID": 8, "context": "Relative Minimum Empirical Divergence (RMED) (Komiyama et al., 2015) is the first algorithm to have a regret bound that matches this lower bound.", "startOffset": 45, "endOffset": 68}, {"referenceID": 10, "context": "(Pallone et al., 2017) studies adaptive preference learning across arms using pairwise preferences.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Yue et al. (2012) shows that the worst-case expected cumulative strong regret up to time T for any algorithm is \u03a9(N log(T )).", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "Relative Minimum Empirical Divergence (RMED) (Komiyama et al., 2015) is the first algorithm to have a regret bound that matches this lower bound. Zoghi et al. (2015) proposed two algorithms, Copeland Confidence Bound (CCB) and Scalable Copeland Bandits (SCB), which achieve an optimal regret bound without assuming existence of a Condorcet winner.", "startOffset": 46, "endOffset": 166}, {"referenceID": 7, "context": "Relative Minimum Empirical Divergence (RMED) (Komiyama et al., 2015) is the first algorithm to have a regret bound that matches this lower bound. Zoghi et al. (2015) proposed two algorithms, Copeland Confidence Bound (CCB) and Scalable Copeland Bandits (SCB), which achieve an optimal regret bound without assuming existence of a Condorcet winner. While weak regret was proposed in Yue et al. (2012), it has not been widely studied to our knowledge, and despite its applicability we are unaware of papers that provide algorithms designed for it specifically.", "startOffset": 46, "endOffset": 400}, {"referenceID": 7, "context": "Relative Minimum Empirical Divergence (RMED) (Komiyama et al., 2015) is the first algorithm to have a regret bound that matches this lower bound. Zoghi et al. (2015) proposed two algorithms, Copeland Confidence Bound (CCB) and Scalable Copeland Bandits (SCB), which achieve an optimal regret bound without assuming existence of a Condorcet winner. While weak regret was proposed in Yue et al. (2012), it has not been widely studied to our knowledge, and despite its applicability we are unaware of papers that provide algorithms designed for it specifically. While one can apply algorithms designed for the strong regret setting to weak regret, and use the fact that strong dominates weak regret to obtain weak regret bounds ofO(N log(T )), these are looser than the constant-in-T bounds that we show. Active learning using pairwise comparisons is also closely related to our work. Jamieson & Nowak (2011) considers an active learning problem that is similar to our problem in that the primary goal is to sort arms based on the user\u2019s preferences, using adaptive pairwise comparisons.", "startOffset": 46, "endOffset": 906}, {"referenceID": 2, "context": "Busa-Fekete et al. (2013) and Busa-Fekete et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 2, "context": "Busa-Fekete et al. (2013) and Busa-Fekete et al. (2014) consider topk element selection using adaptive pairwise comparisons.", "startOffset": 0, "endOffset": 56}, {"referenceID": 0, "context": "Extension to Utility-Based Regret We now briefly discuss utility-based extensions of weak and strong regret for the total order setting, following utilitybased bandits studied in Ailon et al. (2014). Our regret bounds also apply here, with a small modification.", "startOffset": 179, "endOffset": 199}, {"referenceID": 8, "context": "In the strong regret setting, we compare WS-S with 7 benchmarks including RUCB and Relative Minimum Empirical Divergence (RMED) by Komiyama et al. (2015). We also include an experiment violating the total order assumption in Section 11 in the supplement.", "startOffset": 131, "endOffset": 154}, {"referenceID": 8, "context": "We use the sushi and MSLR datasets, which were previously used by Komiyama et al. (2016) and Zoghi et al.", "startOffset": 66, "endOffset": 89}, {"referenceID": 8, "context": "We use the sushi and MSLR datasets, which were previously used by Komiyama et al. (2016) and Zoghi et al. (2015) respectively to evaluate dueling bandit algorithms.", "startOffset": 66, "endOffset": 113}, {"referenceID": 9, "context": "The sushi dataset (Komiyama et al., 2016) contains 16 arms corresponding to types of sushi, with pairwise preferences inferred from data on sushi preferences from 5000 users in Kamishima (2003).", "startOffset": 18, "endOffset": 41}, {"referenceID": 8, "context": "The sushi dataset (Komiyama et al., 2016) contains 16 arms corresponding to types of sushi, with pairwise preferences inferred from data on sushi preferences from 5000 users in Kamishima (2003). The MSLR dataset has 5 arms, corresponding to ranking algorithms, with pairwise preferences provided in Zoghi et al.", "startOffset": 19, "endOffset": 194}, {"referenceID": 8, "context": "The sushi dataset (Komiyama et al., 2016) contains 16 arms corresponding to types of sushi, with pairwise preferences inferred from data on sushi preferences from 5000 users in Kamishima (2003). The MSLR dataset has 5 arms, corresponding to ranking algorithms, with pairwise preferences provided in Zoghi et al. (2015). We give preference matrices (pi,j) for both datasets in the supplement.", "startOffset": 19, "endOffset": 319}, {"referenceID": 13, "context": "W follows a Poisson distribution with parameter \u2211\u221e i=1 ( 1\u2212p p )i = 1\u2212p 2p\u22121 (Theorem 4, Wang (1993)).", "startOffset": 89, "endOffset": 101}, {"referenceID": 9, "context": "This is common in the dueling bandits literature, where even work that considers more general settings theoretically test their methods on problems that satisfy the total order assumption (Komiyama et al., 2016; Urvoy et al., 2013).", "startOffset": 188, "endOffset": 231}, {"referenceID": 8, "context": "This is common in the dueling bandits literature, where even work that considers more general settings theoretically test their methods on problems that satisfy the total order assumption (Komiyama et al., 2016; Urvoy et al., 2013). In this section, we consider an additional example that has a Condorcet winner but does not have a total order among arms. The example has a cyclic struture, and is similar to the cyclic example in Komiyama et al. (2015). The preference matrix is: \uf8ef\uf8ef\uf8f0 0.", "startOffset": 189, "endOffset": 454}], "year": 2017, "abstractText": "We consider online content recommendation with implicit feedback through pairwise comparisons, formalized as the so-called dueling bandit problem. We study the dueling bandit problem in the Condorcet winner setting, and consider two notions of regret: the more well-studied strong regret, which is 0 only when both arms pulled are the Condorcet winner; and the less well-studied weak regret, which is 0 if either arm pulled is the Condorcet winner. We propose a new algorithm for this problem, Winner Stays (WS), with variations for each kind of regret: WS for weak regret (WS-W) has expected cumulative weak regret that is O(N), and O(N log(N)) if arms have a total order; WS for strong regret (WS-S) has expected cumulative strong regret of O(N + N log(T )), and O(N log(N) +N log(T )) if arms have a total order. WS-W is the first dueling bandit algorithm with weak regret that is constant in time. WS is simple to compute, even for problems with many arms, and we demonstrate through numerical experiments on simulated and real data that WS has significantly smaller regret than existing algorithms in both the weakand strong-regret settings.", "creator": "LaTeX with hyperref package"}}}